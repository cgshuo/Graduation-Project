 Food preference learning is an important component of well-ness applications and restaurant recommender systems as it provides personalized information for effective food target-ing and suggestions. However, existing systems require some form of food journaling to create a historical record of an in-dividual X  X  meal selections. In addition, current interfaces for food or restaurant preference elicitation rely extensively on text-based descriptions and rating methods, which can im-pose high cognitive load, thereby hampering wide adoption.
In this paper, we propose PlateClick , a novel system that bootstraps food preference using a simple, visual quiz-based user interface. We leverage a pairwise comparison approach with only visual content. Using over 10,028 recipes collected from Yummly, we design a deep convolutional neural net-work (CNN) to learn the similarity distance metric between food images. Our model is shown to outperform state-of-the-art CNN by 4 times in terms of mean Average Preci-sion. We explore a novel online learning framework that is suitable for learning users X  preferences across a large scale dataset based on a small number of interactions (  X  15). Our online learning approach balances exploitation-exploration and takes advantage of food similarities using preference-propagation in locally connected graphs.

We evaluated our system in a field study of 227 anony-mous users. The results demonstrate that our method out-performs other baselines by a significant margin, and the learning process can be completed in less than one minute. In summary, PlateClick provides a light-weight, immersive user experience for efficient food preference elicitation. H.4.m [ Information Systems Applications ]: Miscella-neous; I.2.6 [ Artificial Intelligence ]: Learning Food preference elicitation; visual interface; online learning c
The problem of capturing and understanding people X  X  food preferences has attracted substantial attention from indus-try (e.g., Yelp and Foursquare) and academia [12,13,17,38]. Food preferences guide our diet choices [32] which in turn have a strong effect on our personal health and social lives [32]. A recipe advice system [13, 27] could more effectively coach users to prepare healthier meals at home if alternative food suggestions provided were appealing to them. This is important because healthy diet recommendations are of no benefit if users don X  X  adopt them. Another application area that could leverage user preferences is commercial restau-rant recommender systems like Yelp and Foursquare. The recommendations will be more accurate and personalized if the system output is tuned to the user X  X  diet profile. How-ever, food preference is notoriously difficult to learn because of its dependence on context (e.g., evolving personal goals). Existing systems and algorithms suffer from several limi-tations that interfere with efficient learning and wide user adoption:
Data sparsity. Current food preference learning systems require longitudinal records of the meals that users have eaten [12, 13, 17, 38]. These historical data traces [11] typi-cally come from location sensing, which is not always related to food preference, or burdensome food journaling, which is often abandoned after short periods of adoption [9]. As a result, data points are too sparse to provide enough food preference information.

High cognitive load. Preference elicitation [29], in which users are asked to rate different food items explicitly (on a scale from 1 to 5) [12, 17], is an approach that is comple-mentary to the above longitudinal methods. While text-based instruments and rating methods [5, 10, 35, 43] effec-tively address the cold-start problem [23, 34, 42] in movie recommender systems, the counterpart for food preference is especially hard for users [9] as it is time consuming [28] and presents a high cognitive load [8].

Insufficient Visual Understanding. Traditional meth-ods mainly use food tags and other metadata for learning tasks [12,17]. However, as people X  X  diet decisions are greatly influenced by visual appearances of meals [9], analysis of im-age features can provide a valuable signal for diet profiling and food preference elicitation.

In this paper, we propose PlateClick ,anovelsystem for efficient food preference elicitation using a sim-ple, visual quiz-based user interface . PlateClick allevi-ates the limitations mentioned above with deep understand-ing of food images and a user-friendly visual interface. To Figure 1: PlateClick system pipeline. The system is divided into two major parts: Offline preprocessing , which is surrounded by green dotted line and On-line preference learning , which is surrounded by blue dotted line . the best of our knowledge, this is the first system and algo-rithm that learns users X  food preferences through real-time interactions without any requirements of diet history. We developed this system as a lightweight, easily accessible web service that can be completed within 60 seconds. Through a field study with 227 anonymous users in the wild, we show that our system is able to predict the food items that a user likes/dislikes with high accuracy. The system pipeline of PlateClick is shown in Fig. 1, which consists of offline and online stages with several components, as follows.
Food Items Harvesting. We pulled 12 , 000 main dish recipes with their images and metadata (ingredients, nutri-ents, tastes, etc.) via the Yummly API 1 and filter out image outliers. The final dataset has 10 , 028 food items across var-ious cuisines (American, Asian, Mexican, Italian, etc.)
Food Similarity Embedding. In order to tame a large number of food items and facilitate image understanding, we learn the image similarity distance metric based on la-bels from the Food-101 dataset [4] using a deep Siamese network [7]. With the trained network, we extract 1,000 dim visual features for each food image. The method we propose improves the performance of other state-of-the-art visual feature extraction approaches. We append 200 dim ingredients features to visual features, resulting in an em-bedding of the food items into a 1,200 dim space in which similar items are nearby one another and dissimilar ones are farther away.

Visual User Interface. The process of our food pref-erence elicitation consists of several iterations. We explore the advantages of image picking and pairwise comparison in interface design, both of which offer a potentially improved http://developer.yummly.com user experience [8,28]. In each of the first two iterations, we present ten images and users are asked to tap on all the ones they like . In each subsequent iteration, we present a pair of food images and ask users either to tap on whichever they prefer or click yuck , indicating a preference for neither.
Backend Online Learning. The backend of our system consists of a novel online learning algorithm that explores the similarity between food items. Our algorithm is inspired by label propagation [44] in locally connected graphs and the Exponentiated Gradient Algorithm for bandit settings (EXP3) [2]. We demonstrate that this algorithm is more effective in our proposed workflow than other baselines.
Compared to traditional food preference learning systems, our work offers 3 major contributions and points of novelty. We envision that PlateClick , a light-weight and efficient food preference elicitation system, will provide a personalized user experience capable of fueling a wide range of appli-cations in domains including health care, diet planning, and restaurant recommendation.
Collaborative Filtering. As one of the most popular al-gorithms adopted in current recommender systems, collabo-rative filtering (CF) [18] has been widely studied in a variety of applications. The main idea of this method is to predict and learn a user X  X  preferences based on similarity measures such as user-based CF [18] and item-based CF [33]. It has also been shown that latent factor models [21] and matrix factorization [31] are promising to predict users X  ratings for previously unobserved items.

A major limitation of collaborative filtering is its require-ment for historical user data. Although several techniques have been proposed to address the cold-start problem [23, 42], the performance of CF is still largely dependent on the number of active users, availability of contextual informa-tion [42] and observed ratings for different items [23]. In the case of food preference learning, it X  X  typically difficult to get access to a user X  X  diet history since meal journaling is burdensome [9]. Therefore, in the design of PlateClick ,we don X  X  assume any prior knowledge of the users.

Preference elicitation. To alleviate the cold-start prob-lem mentioned above, several models of preference elicitation have been proposed in recent years. The most prevalent method of elicitation is to train decision trees to poll users in a structured fashion [10,14,30,35,45]. These questions are either generated in advance and remain static [30] or change dynamically based on real-time user feedback [10,14,35,45]. In addition, another previous work explores the possibility of eliciting item ratings directly from the user [5, 43]. This process can either be carried at item-level [43] or within-category (e.g., movies) [5].

The preference elicitation methods we mentioned above largely focus on the domain of movie recommendations [5, 30, 35, 43] and visual commerce [10] (e.g., cars, cameras) where items can be categorized based on readily available metadata. When it comes to real dishes, however, categor-ical data (e.g., cuisines) and other associated information (e.g., cooking time) possess a much weaker connection to a user X  X  food preferences. Through the design of PlateClick , we leverage the visual representation of each meal so as to better capture the process through which people make diet decisions.

Food preference learning system. Most existing food preference learning approaches are hybrids of historical record mining and rating elicitation [12,13,17,38,40]. To avail one-self of these systems to promote healthy eating, one is often required to record daily meal intake and provide this infor-mation as a bootstrapping resource to a diet recommender system [12,13,38]. After that, several food items are selected for display based on matching scores between meal metadata and the user X  X  previous choices. For each item provided, the user is prompted to enter a rating on a scale from 1 to 5.
To the best of our knowledge, no existing systems take vi-sual features  X  arguably one of the most important factors in assisting people X  X  daily food choices [9]  X  into consideration. Additionally, the most common methods adopted in food preference elicitation (i.e. text based interface and numerical rating scale) impose a high cognitive load on the user [8] and are susceptible to noisy and unreliable responses. Inspired by elicitation strategies in other domains (e.g. crowdsourc-ing [6], housing [15]), we propose a simplified, purely visual interface that presents users with simple pairwise compar-isons. Through our field study with anonymous users, we show that this lightweight interface can promote efficient food preference learning.
For 12 , 000 main dishes recipes pulled from Yummly API, we filter out entries with unrelated (or missing) image con-tent, resulting in a final dataset S containing 10 , 028 food apart from visual features, we append 200 dim ingredient features as the representation of each food image. The ingre-dient feature vector is calculated according to the following pre-processing steps: 1. Keyword extraction and lemmatization. For each ingre-dient appearing in the metadata, we extract keywords from its description and apply lemmatization; see Table 1. 2. Aggregation and Filtering. We aggregate and count the occurrences of each ingredient appearing in our dataset. We select top 200 most frequent ingredients 2 as our list of ingredient features. 3. Feature Vector Calculation. For each food item s i  X  X  , its d ingr = 200 dim normalized ingredient feature vector
We will incorporate more sophisticated methods such as tf-idf and homonyms/synonyms handling in the future. ingredient j appears in food item s i  X  X  ingredient list Ingr as Equation (1) shows:
Table 1: Keyword extraction and lemmatization. Recent advances in similarity metric learning with Deep Convolutional Neural Networks (CNNs) have resulted in breakthroughs in areas including Face Verification [37], Im-age Retrieval [41], Geo-localization [24] and Product De-sign [3]. The CNN architectures in these works are based on the Siamese Network [7], which is trained on a large number of paired inputs of similar and dissimilar examples. In light of the prior efforts mentioned above, we adopt this approach to generate a distance embedding for meals.

The proposed CNN architecture (Food-CNN) is illustrated in Fig. 2. The inputs of Food-CNN are a pair of color food images x, y  X  X  , each of size 227  X  227  X  3. Then, each im-age proceeds through an identical feature extraction CNN containing several layers from Convolution to Batch Nor-malization. Finally, the outputs of the last layer are used as their low-dimensional feature embeddings f ( x ) ,f ( y ). The architecture from the first Convolution layer to the last Fully Connected layer (i.e. layers in dashed line bounding box in Fig.2) is the same as the architecture that achieved state-of-the-art image classification performance on ImageNet [22]. For each of the layers, the numbers at the top specify its window size and step size ( Convolution and Max Pooling); the numbers at the bottom specify the size of its output feature map. For example, the first convolution layer takes a 227  X  227  X  3 color image from image data layer as the input and convolves it with 96 filters. Each filter has a size of 11  X  11  X  3 and convolves the image on a grid with step size 4  X  4. In this sense, the output of the first Convolution layer is a 55  X  55 feature map with 96 channels. We add a final Batch Normalization layer to normalize the 1000 dim feature vector so that each dimension has zero mean and unit variance within a training batch. Batch Normalization provides a faster convergence rate and higher accuracy in practice [19].

Our goal of Food-CNN is to learn a low dimensional fea-ture embedding that pulls similar food items together and pushes dissimilar food items far away. Specifically, we want f ( x )and f ( y ) to have small distance (close to 0) if x and y are similar items; otherwise, they should have distance larger than a margin m . Therefore, we choose Contrastive Figure 3: Contrastive Loss function with m =1 .
 Loss proposed in [16] as the loss function to optimize Food-CNN, which can be expressed as: where similarity label l  X  X  0 , 1 } indicates whether the input pair of food items x , y are similar or not ( l = 1 for similar, l = 0 for dissimilar), m&gt; 0 is the margin for dissimilar items and D = f ( x )  X  f ( y ) 2 is the Euclidean Distance between f ( x )and f ( y ) in embedding space.

As illustrated in Fig. 3, Contrastive Loss function exactly matches our goal. Minimizing the loss in Eqn. (2) pulls sim-ilar food images closer and pushes dissimilar ones apart as it penalizes similar pairs by their distances quadratically and dissimilar pairs by their squared differences of the distances to the margin m if they are smaller than m .

Training a deep Siamese Network usually requires huge amount of training data that can X  X  fit in memory. To address this problem, we adopt Nesterov X  X  Accelerated Gradient De-scent method [26] with Momentum algorithm [36]. We use back-propagation to compute the gradient of the loss with respect to the parameters of each layer. Suppose we have an n -layer CNN: where g i ( . ) represents the computation of i -th layer (e.g., convolution, pooling etc.), x, y and f ( x ) ,f ( y ) represent the input and output pairs of the Siamese Network, respec-tively. We adopt function L ( . ) to calculate the loss of the input pairs x, y as L ( x, y, l ). To minimize loss by updat-ing parameters W i of i -th layer, we need the gradient of the loss with respect to W i :  X  L ( . )  X  X  back-propagation,  X  L ( . )  X  X  for k -th layer g k ( . ), we only need to calculate  X  X  k  X  X  k . We use the implementation of gradient descent and back-propagation in Caffe [20], an open source deep learning framework.

Since the food dataset S pulled from Yummly doesn X  X  in-clude categorical similarity annotations, we trained Food-CNN on the Food-101 dataset [4], which is the largest food image dataset so far and contains 101 food categories and 101 , 000 food images. We sampled 75 , 750 same pairs and 757 , 500 different pairs from the training set to train our Food-CNN. After the training process, we use the pre-trained Food-CNN to extract visual features from images in our dataset. For each item s i  X  X  , we feed the food image to pre-trained feature extraction layers (i.e. layers in blue dashed line bounding box in Fig. 2) and get the feature vector  X  v s the feature vectors before and after normalization, respec-tively; d vis =1 , 000 is the dimension of visual features.
We then concatenate the visual features v s i with ingre-dient features g s i to create a 1,200 dim feature embedding As discussed in previous sections, each food item s i  X  S has a 1,200 dim feature vector f s i in the embedding space. Building upon the offline feature extraction results, we model the interaction between user and our backend sys-tem at iteration t, ( t  X  X  + ,t =1 , 2 , ..., T ) as Fig. 4 shows. The symbols that will be used in our algorithms are defined as follows:
K t : Set of food items that are presented to user at itera-tion t ( K 0 =  X  ).  X  k  X  X  t , k  X  X  ;
L t  X  1 : Set of food items that user prefer(select) among { k food items s i ( i =1 , ..., |S| ), where p t 1 =1. p 0 is initial-
B t : Set of food images that have been already explored until iteration t ( B 0 =  X  ). B i  X  X  j ( i&lt;j );
Based on the workflow depicted in Fig. 4, for each iter-ation t , backend system updates vector p t  X  1 to p t and set Figure 4: User-system interaction at iteration t . B  X  1 to B t based on users X  selections L t  X  1 and previous im-age set K t  X  1 . After that, it decides the set of images that will be immediately presented to the user (i.e., K t ). Our food preference elicitation framework can be formalized in Algorithm. 1. The core procedures are update and select , which will be described in the following subsections for more details.
 Algorithm 1: Food Preference Elicitation Framework
Result : p T 1
B 0 =  X  , K 0 =  X  , L 0 =  X  , p 0 =[ 1 2for t  X  1 to T do 4 K t  X  select( t , B t , p t ) ; 5if t equals T then 6 return p T 7else 8 ShowToUser( K t ) ; 9 L t  X  WaitForSelection() ;
Based on user X  X  selections L t  X  1 and the image set K t  X  1 the update module renews user X  X  state from {B t  X  1 , p t to {B t , p t } . Our intuition and assumption behind following algorithm design is that people tend to have close preferences for similar food items in 1,200 dim space. 5.2.1 Preference vector p t
Our strategy of updating preference vector p t is inspired by Exponentiated Gradient Algorithm in bandit settings (EXP3) [2]. Specifically, at iteration t ,each p t i in vector where  X  is the exponentiated coefficient that controls update to adjust each preference value.

In order to calculate update vector u , we formalize user X  X  selection process as a data labeling problem [44] where for s y provided by user is:
For update vector u ,weexpectthatitisclosetola-bel vector y but with smooth propagation of label values y a =0 y i i =1 /
Figure 5: Locally connected graph with item s i . to nearby neighbors (For convenience, we omit superscript that denotes current iteration). The update vector u can be regarded as a soften label vector compared with y .To make the solution more computationally tractable, for each item s i with y i = 0, we construct a locally connected undi-rected graph G i as Fig. 5 shows:  X  s j  X  X  , add an edge ( s ,s j )if f s i  X  f s j  X   X  (  X  = 35 in our implementation). The labels y i for vertices s j in graph G i are calculated as y =0( j =1 ,..., |S| \ i ) ,y i i = y i .

For each locally connected graph G i ,wefix u i i value as u = y i i and propose the following regularized optimization method to compute other elements (  X  u i j ,j = i )ofupdate vector u i , which is inspired by the traditional label prop-agation method [44]. Consider the problem of minimizing following objective function Q ( u i ): min
In Eqn. (6), w ij represents the similarity measure between food item s i and s j :
The first term of the objective function Q ( u i )isthe smooth-ness constraint as the update value for similar food items should not change too much. The second term is the fit-ting constraint ,whichmakes u i close to the initial labeling assigned by user (i.e. y i ). However, unlike [44], in our al-gorithm, the trade-off between these two constraints is dy-namically adjusted by the similarity between item s i and s where similar pairs are weighed more with smoothness and dissimilar pairs are forced to be close to initial labeling.
With Eqn. (6) being defined, we can take the partial derivative of Q ( u i ) with respect to different u i j as follows:
As u i i = y i i , then:
After all u i are calculated, the original update vector u is then the sum of u i , i.e. u = i u i . The pseudo code for the algorithm of updating preference vector is shown in Algorithm.2 for details.
In order to balance the exploitation and exploration in image selection phase, we maintain a set B t that keeps track of all similar food items that have already been visited by user and the updating rule for B t is as follows:
With the algorithms designed for updating preference vec-tor p t and explored image set B t , the overall functionality of procedure update is shown in Algorithm.2.

Algorithm 2: User state update Algorithm -update 2 u =[0 , ..., 0] , B t = B t  X  1 , p t = p t  X  1 3for i  X  1 to |S| do 4 // preference update 5for s j in K t  X  1 do 8 // explored image set update 9if min( f s i  X  f s j ,  X  s j  X  X  t  X  1 )  X   X  then 10 B t  X  X  t  X  X  s i } 11 // normalize p t s.t. p t 1 =1 12 normalize( p t )
Algorithm 3: Kmeans++ Algorithm for Exploration 1 Function k-means-pp( S , n ) 2 K t = random( S ) 3 while |K t | &lt;n do 4 prob  X  [0 , ..., 0] |S| 5for i  X  1 to |S| do 7 sample s m  X  X  with probability  X  prob m 8 K t  X  X  t  X  X  s m }
After updating user state, the select module then picks food images that will be presented in the next round. In or-der to trade-off between exploration and exploitation in our algorithm, we propose different images selection strategies based on current iteration t .
For each of the first two iterations, we select ten differ-ent food images by using K-means++ [1] algorithm, which is a seeding method used in K-means clustering and can guarantee that selected items are evenly distributed in the feature space. For our use case, K-means++ algorithm is summarized in Algorithm.3.
Starting from the third iteration, users are asked to make pairwise comparisons between food images. To balance the Exploitation and Exploration in our algorithm design, we always select one image from the area with higher preference value based on current p t and another one from unexplored area, i.e. S\B t . (Both selections are random in a given subset of food items). With above explanations, the image selection method we propose in this application is shown in Algorithm 4.

Algorithm 4: Images Selection Algorithm -select 1 Function select( t, B t , p t ) 2 K t =  X  3if t  X  2 then 4 K t  X  k-means-pp( S ,10 ) // K-means++ 5else 6 // 99th percentile (top 1% ) 7 threshold  X  percentile( p t ,99 ) 8 topSet  X  X  s i  X  X | p t i  X  threshold } 9 K t  X  [ random( topSet ) , random( S\B t ) ] We examine and evaluate the clustering performance of Food-CNN model on Food-101 [4] dataset, where each image is tagged with a categorical label. We first extract 1,000 dim visual feature for each food image in the test set. After that, we explore k ,where k =1 , 2 ,...,N ( N is the size of the test set), nearest neighbors of each food image and calculate the Precision and Recall values for each k :
Suppose N i k is the set of k nearest neighbors of item i under category C i , then the Precision and Recall values are:
In order to measure the overall performance of our em-bedding method on Food-101 test set, we average the Preci-sion/Recall values over all food images for each method and plot Precision-Recall Curve (PR Curve) as Fig. 6 shows. We use mean Average Precision (mAP), which corresponds to the area under PR Curve, as the quantitative comparison metric. The mAP value of the ideal algorithm is equal to 1.
We compare our Food-CNN model with several state-of-the-art feature extraction methods: 1. Pretrained AlexNet deep neural network : This is the state-of-the-art feature ex-traction method using pretrained AlexNet [22]. We take 1,000 dim feature representation from the output of the last fully-connected layer. 2. SIFT+Bag of visual Words(BoW): As the most popular method among hand-crafted feature representations, SIFT [25] has been shown to be effective in several applications. We extract visual features using 1000 Figure 6: Precision-recall curve for food similarity embedding( mAP: mean Average Precision, which represents area under each curve).
 words so as to guarantee that it has the same feature dimen-sion with our Food-CNN.
 As can be seen in our results, although SIFT+BOW and AlexNet greatly outperform random guess baseline, they lack discriminative power for food images because these mod-els are mainly designed for the general clustering purpose. With Food-CNN, we can achieve 4 times performance im-provements over state-of-the-art models in terms of mAP value. Algorithm that with much better clustering power can help the whole system understand visual content and thus improve the efficiency of online preference learning.
To further verify and visualize the generalization power of our system, for each of the recipe images that collected from Yummly, we embed it into 1200 dim feature space by first using Food-CNN to extract 1000 dim visual representation and then concatenate it with 200 dim ingredients feature. We project all image representations to 2-D plane by using t-Distributed Stochastic Neighbor Embedding(t-SNE) [39] method. As shown in Fig. 7, we divide the 2-D plane into several blocks and for each block, we sample a representative food image resides in that area. The final embedding results clearly show that our method can effectively group similar food items (recipes) together and push dissimilar items away based on their visual appearances and ingredients metadata. For example, in Fig. 7, we show that burgers, noodles, piz-zas, and meat are grouped in different areas of the feature space.
We conducted field study among 227 anonymous users that recruited from social networks and university mailing lists. The experiment was approved by Institutional Review Board (ID: 1411005129) at Cornell University. All partic-ipants were required to use this system independently for three times. Each time the study consisted of following two phases: Training Phase. Users played with PlateClick for the first T iterations and the system learnt and elicited preference vector p T based on the algorithms proposed in this paper or baseline methods, which will be discussed later. We ran-domly picked T from set { 5 , 10 , 15 } at the beginning but made sure that each user experienced different values of T only once.

Testing Phase. After T iterations of training, users en-tered the testing phase, which consisted of 10 rounds of pairwise comparisons. We picked testing images based on preference vector p T that learnt from online interactions: One of them was selected from food area that user liked (i.e. item with top 1% preference value) and the other one from the area that user disliked (i.e. item with bottom 1% preference value) Both of the images were picked randomly among unexplored food items.
In order to show the learning performance of our algo-rithm, we compare it with several combinatorial baselines that mentioned next. Users encountered these online learn-ing algorithms randomly when they logged into the system:
LE+EE: This is the online learning algorithm proposed in this paper that combines the ideas of L abel propagation, E xponentiated Gradient algorithm for user state update and E xploitation-E xploration strategy for images selection.
LE+RS: This algorithm retains our method for user state update ( LE ) but R andom S elect images to present to user without any exploitation or exploration.

OP+EE: As each item is represented by 1200 dim feature vector, we can adopt the idea of regression to tackle this online learning problem (i.e. learning weight vector w such that wf s i is higher for item s i that user prefer). Hence, we compare our method with O nline P erceptron algorithm that updates w whenever it makes error, i.e. if y i wf s i assign w  X  w + y i wf s i ,where y i is the label for item s (pairwise comparison is regarded as binary classification such that the food item that user select is labeled as +1 and otherwise -1). In this algorithm, we retain our strategy of images selection (i.e. EE ).

OP+RS: The last algorithm is the O nline P erceptron that mentioned above but with R andom images S election strategy.

Among 227 participants in our study, 58 of them finally used algorithm LE+EE ,57used OP+RS .Fortherestof users (112), half of them (56) tested OP+EE and the other half (56) tested LE+RS . Overall, the participants for differ-ent algorithms are totally random so that the performances of different models are directly comparable.

After all users going through training and testing phases, we calculate the prediction accuracy of each individual user and aggregate them based on the context that they encountered (i.e. the number of training iterations T and the algorithm settings that mentioned above). The predic-tion accuracies and their cumulative distributions are shown in Fig. 8, 9 and 10 respectively.

Length effects of training iterations. As can be seen in Fig. 8 and Fig. 9, the prediction accuracies of our online learning algorithm are all significantly higher than the base-lines.The algorithm performance is further improved with longer training period. As is clearly shown in Fig. 9, when Figure 8: Prediction accuracy for different algo-rithms in various training settings (asterisks repre-sent different levels of statistical significance:  X  X  X  X  : p&lt; 0 . 001 ,  X  X  X  : p&lt; 0 . 01 ,  X  : p&lt; 0 . 05 ). the number of training iterations reaches 15 , about half of the users will experience the prediction accuracy that ex-ceeds 80%, which is fairly promising and decent consider-ing small number of interactions that system elicited from scratch. The results above justify that PlateClick ,asan online preference learning system, can adjust itself to ex-plore users X  preference area as more information is avail-able from their choices. For the task of item-based food preference bootstrapping, our system can efficiently balance the exploration-exploitation while providing reasonably ac-curate predictions.

Comparison across different algorithms. As men-tioned previously, we compared our algorithm with some obvious alternatives. Unfortunately, according to the results shown in Fig. 8 and Fig. 10, none of these algorithms works very well and the accuracy of prediction is actually decreas-ing as the user provides more information. Additionally, as is shown in Fig. 10, our algorithm has particular advantages when users are making progress (i.e. the number of training iterations reaches 15). The reasons why these techniques are not suited for our application is mainly due to the following limitations:
Random Selection. Within a limited number of interac-tions, random selection can not maintain the knowledge that it has already learned about the user (exploitation), nor ex-plore unknown areas (exploration). In addition, it X  X  more likely that the system will choose food items that are very similar to each other and thus hard for the user to make decisions. Therefore, after short periods of interactions, the system is messed up, and the performance degrades.
Underfitting. The algorithm that will possibly have the underfitting problem is the online perceptron ( OP ). For our application, each food item is represented by 1200 dim fea-ture vector and OP is trying to learn a separate hyperplane based on a limited number of training data. As each single feature is directly derived from deep neural network, the lin-earity assumptions made by perceptron might yield wrong predictions for the dishes that haven X  X  been explored before.
As another two aspects of online preference elicitation system, computing efficiency and user experience are also very important metrics for system evaluation. Therefore, we recorded the program execution time and user response Figure 9: Cumulative distribution of prediction accuracy for LE+EE algorithm (Numbers in the legend represent the values of T through training phase). time as a lens into the real-time performance of PlateClick . As shown in Fig. 11(b), the program execution time is about iterations afterwards 3 . Also, according to Fig. 11(a), the majority of users can make their decisions in less than 15 s for the task of comparison among ten food images while the payload for the pairwise comparison is less than 2  X  3 s . As a final cumulative metric for the system overhead, it is shown in Table 2 that even for 15 iterations of training, users can typically complete the whole process within 53 seconds, which further justify that PlateClick is a light-weight user-friendly visual interface for efficient food preference elicita-tion.
Table 2: Average time to complete training phase.
An interesting phenomenon that we observed from our field study is that there exists obvious correlation between total user response time and Precision of our model: Pref-erence learning of PlateClick tends to be more accurate if the user made decisions within shorter period of time. As is shown in Fig. 12, we plot scatter diagram that contains all data points that users generated when they used Plate-Click under LE+EE algorithm setting and with 15 number of training iterations ( T = 15). Apparently, most of the points (blue ones) are above the dotted line in Fig. 12 and the expected total response time is higher for those users with lower prediction accuracy. The possible reason behind this result is that if food pairs are hard to be distinguished with each other, the responses from user will likely to have large noise and uncertainty, 4 which in turn affects the per-formance of online learning system.
In this section we discuss limitations and future directions for PlateClick .
Our web system implementation is based on Amazon EC2 t2-micro Linux 64-bit instance
User engagement is a critical issue in our application. We will conduct studies/interviews on the effects of human be-havior in the future. Figure 12: Scatter diagram of total user response time under different precision of model prediction (Points in the graph represent experimental results under LE+EE algorithm setting and with 15 number of training iterations).

Special diets. We noticed in the user study that our current system can not be efficiently used by people with special diets (e.g., vegetarian, vegan, kosher and halal) due to the limits of our image corpus. It X  X  very unlikely that the system will select anything such a user would like from the current main-dishes dataset. It will degrade user experience to repeatedly show dishes that are not to their taste. In the future, we need to consider different diets and curate special food datasets accordingly.

Food courses and diversity. People X  X  food preferences are highly dependent on context as determined by factors such as time of day. It is unnecessarily difficult and possibly confounding to compare dishes across different courses (e.g., between desserts and dinner). In our current system, we mainly focus on food items from main dishes, which conve-niently constrains the space of choices available to the user. Future work will incorporate dishes across different courses (e.g., breakfast and lunch) and enrich the diversity of our dataset.

Healthy recommendation. One of the most important applications that could build on PlateClick is a healthy food recommendation system. Combining a user X  X  diet profile and recipe nutrient metadata, we could recommend appeal-ing but healthier food alternatives to users so that they are more likely to follow the system X  X  advice in their daily lives and choices. Suggestions guided by people X  X  preferences will be more effective and persuasive than a traditional strategy that lack awareness of what a given person actually likes. This was our initial motivation for developing PlateClick and we hope to pursue integration with existing nutritional behavior change apps.
In this work, we introduced PlateClick , a novel visual in-terface for real-time food preference elicitation from scratch. Compared with previous solutions and online learning sys-tems, we don X  X  assume any prior knowledge of the user. In addition, we greatly simplify the elicitation user interface by replacing traditional text-based instruments with visual contents and leveraging a pairwise comparison method. We demonstrated that these design choices reduce user burden and cognitive load when using the elicitation system.
Although our algorithmic framework was originally de-signed for visual content based food preference learning, the techniques proposed in this paper could be used to enhance the interplay between human hedonic and content similari-ties in solving general human-in-the-loop problems. We en-vision that PlateClick , an efficient and user-friendly food preference learning system, could be used to capture per-sonal diet profiles and fuel a wide range of applications in healthcare and commercial recommender systems.
We would like to thank Dr. Curtis Cole for suggesting the development of food preference modeling and Dr. Thorsten Joachims for discussion of ML algorithms. Also, we appreci-ate the anonymous reviewers for insightful comments. This research is partly funded by AOL-Program for Connected Experiences, NSF grant IIS-1344587 and further supported by the small data lab at Cornell Tech which receives funding from UnitedHealth Group, Google, Pfizer, RWJF, NIH and NSF.
