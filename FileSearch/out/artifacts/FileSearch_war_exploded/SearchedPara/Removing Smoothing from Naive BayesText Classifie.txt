 Text categorization (TC), the activity of labeling natural language texts with thematic categories from a predefined set, has gained a prominent status in the information systems field, largely due to the widespread and ever growing availability of digital documents and the consequential need on the part of the users to access them in flexible ways [1,2].
 has been gaining popularity lately. There are two event models of NB: multi-variate Bernoulli model and multinomial model. The latter is used more widely in TC [3]. Recently, the focus on NB is to improve the accuracy by alleviating the skewed data bias, performing heuristic feature transformations, normalizing by the length of the documents and taking the logarithms of the counts [4,5]. Smoothing is commonly used to avoid zero probability estimates, and plays an important role in NB. There are several smoothing methods for statistical lan-guage modeling, such as additive smoothing, the GT estimate, Katz smoothing etc [6]. Two most popular smoothing methods used in NB are additive smoothing and GT smoothing. Experiments show GT smoothing is more competent than additive smoothing in web document categorization [7]. William A. Gale intro-duced the Linear Good-Turing (LGT) estimate, proposed Simple Good-Turing (SGT) estimate which was proven to be a good scheme[8].
 fers many negative effects. First, smoothing doesn X  X  have firm theoretic base to rely on as ML estimate does [9]. In addition, some smoothing strategies used in Bayes classifier are neither efficient nor easy to understand. For example, GT smoothing is difficult to understand and not very efficient because of the com-plicated smoothing process [10]. Laplace additive smoothing does not consider out of vocabulary (OOV) features [11], and one can often do better by explicitly considering them. Hence, if we can avoid smoothing in NB without sacrificing categorization accuracy, t he classifier will be more effective and more efficient. smoothing from NB without sacrificing cl assification accura cy. Compared with Laplace smoothing and SGT smoothing, NB TS performs the best and NB TF outperforms Laplace smoothing but inferior to SGT.
 algorithm and related smoothing m ethods. In Section 3, we discuss NB TF and NB TS. Section 4 gives the experimental results and analysis. Finally, Section 5 concludes the paper. 2.1 Naive Bayes Algorithm NB computes the posterior probabilities that the document The computation of P ( t j | c k ) in (3) varies according to the event model chosen for document representation and the smoothing method adopted. 2.2 Smoothing Methods There are many smoothing methods for NB, among which additive smoothing and GT smoothing are used most widely.
 Additive Smoothing. The P ( t j | c k ) required in (3) is calculated as follows: In Laplace method,  X  is set to 1. In ELE and  X  X dd-tiny X  [8],  X  is set to 0.5 and 1 / | V | respectively. Add- X  smoothing determines  X  by cross validation [12]. Good-Turing Smoothing. Let r represent the frequency of a given feature, N r is the number of features with a frequency of r , N = r number of features observed, and the value r  X  is the estimated frequency. A precise statement of the theorem underlying the GT method is that The total probability of the unseen features is N 1 /N , and the probability of each feature that occurred at least once is r  X  /N . LGT estimate and SGT estimate are two improved versions of GT method [8]. Because smoothing suffers many negative effects, we propose two strategies that not only make NB avoid smoothing without sacrificing categorization accuracy, but also include the OOV features. 3.1 NB TF Strategy If we can make sure each feature appearing in the test document d i also appears in all classes, smoothing is not required in the categorization procedure of d i . 1. For each training document d x belonging to class c k ,wegeta | C | -dimension 3. We use the training set D and the test document d i to build the model, and 4. We use (1),(3),(6),(7) to predict the class of test document d i . } 3.2 NB TS Strategy NB TF which does well in online categorization is too time costly to be applied in bach categorization. In this section, we describe a new strategy that can do the batch categorization very efficiently and effectively. The new strategy, NB TS, is described in pseudocode as follows: 1. The same as step 1 in NB TF described in Sect. 3.1. 2. For each test document d i belonging to test set T ,wegeta | C | -dimension 3. We use the training set D and the test set T to build the model, and call 4. We use (1),(3),(6),(7) to predict the class of each test document d i in T . We carry out experiments on mini newsgroups corpus [13] over several settings and use the three-fold cross validation m ethod to evaluate the effectiveness of Laplace smoothing, SGT smoothing, NB TF and NB TS. The experimental re-sults are listed in Table 1, each bold figure in the table presents the winner of algorithms in one comparison. We explain the four settings as follows:  X  Setting 1: tokenization, stopwords removal, stemming.  X  Setting 2: tokenization, stopwords removal, stemming, length normalization.  X  Setting 3: tokenization, stopwords removal, stemming, feature selection.  X  Setting 4: tokenization, stopwords removal, stemming, feature selection, used widely in TC [1], and we introduce all of them in comparisons. the frequency of each feature be integ er, but after length normalization the frequency becomes decimal, which limits the application of SGT. By comparison, we find NB TS performs the best in all se ttings without exception. NB TF does better than Laplace smoothing in three settings out of four, except in Setting 3. SGT smoothing performs well in Setting 1 where it beats Laplace smoothing and NB TF, but ranks last in Setting 3, in which Laplace smoothing gets its only victory over SGT smoothing and NB TF.
 21578 collection [1,2]. After pruning documents having multiple topics, exper-iments are performed on the 25 most fre quent categories. The experimental results are listed in Table 2. Once again NB TS performs the best, followed by SGT smoothing and NB TF, and Laplace smoothing takes the last rank. not very encouraging. SGT smoothing is a good method, but its applicability is limited in TC. Many proven effective prepr ocessing strategies may make it not applicable. NB TF performs better than Laplace smoothing, the advantage is more distinct when length normalization is adopted. As a whole, NB TS out-performs all the others. It has many advantages such as simplicity, effectiveness and robustness, and it is as efficient as Laplace smoothing. In this paper, we propose two new strategies that make Naive Bayes classifier avoid smoothing without sacrificing categorization accuracy. NB TF does a good job in TC, and it is suitable for online document categorization. When there are many documents to classify, NB TF is not efficient enough, while NB TS performs better. From the experiments, we find NB TS outperforms Laplace smoothing, SGT smoothing and NB TF, and it has many advantages such as simplicity, effectiveness, efficiency and robustness. In a word, NB TF and NB TS strategies are suitable for Naive Bayes classifiers in TC.
 test set in a step. They are the two extr emes. In future work, we will do some research on combining NB TF and NB TS strategies to find whether there exists an optimal point between the two extremes.

