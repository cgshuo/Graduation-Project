 Keywords in a document provide important infor-mation about the content of the document. They can help users search through information more effi-ciently or decide whether to read a document. They can also be used for a variety of language process-ing tasks such as text categorization and informa-tion retrieval. However, most documents do not provide keywords. This is especially true for spo-ken documents. Current speech recognition system performance has improved significantly, but there is no rich structural information such as topics and keywords in the transcriptions. Therefore, there is a need to automatically generate keywords for the large amount of written or spoken documents avail-able now.

There have been many efforts toward keyword ex-traction for text domain. In contrast, there is less work on speech transcripts. In this paper we fo-cus on one speech genre  X  the multiparty meeting domain. Meeting speech is significantly different from written text and most other speech data. For example, there are typically multiple participants in a meeting, the discussion is not well organized, and the speech is spontaneous and contains disflu-encies and ill-formed sentences. It is thus ques-tionable whether we can adopt approaches that have been shown before to perform well in written text for automatic keyword extraction in meeting tran-scripts. In this paper, we evaluate several differ-ent keyword extraction algorithms using the tran-scripts of the ICSI meeting corpus. Starting from the simple TFIDF baseline, we introduce knowl-edge sources based on POS filtering, word cluster-ing, and sentence salience score. In addition, we also investigate a graph-based algorithm in order to leverage more global information and reinforcement from summary sentences. We used different per-formance measurements: comparing to human an-notated keywords using individual F-measures and a weighted score relative to the oracle system per-formance, and conducting novel human evaluation. Experiments were conducted using both the human transcripts and the speech recognition (ASR) out-put. Overall the TFIDF based framework seems to work well for this domain, and the additional knowl-edge sources help improve system performance. The graph-based approach yielded worse results, espe-cially for the ASR condition, suggesting further in-vestigation for this task. TFIDF weighting has been widely used for keyword or key phrase extraction. The idea is to identify words that appear frequently in a document, but do not occur frequently in the entire document collec-tion. Much work has shown that TFIDF is very ef-fective in extracting keywords for scientific journals, e.g., (Frank et al., 1999; Hulth, 2003; Kerner et al., 2005). However, we may not have a big background collection that matches the test domain for a reli-able IDF estimate. (Matsuo and Ishizuka, 2004) pro-posed a co-occurrence distribution based method us-ing a clustering strategy for extracting keywords for a single document without relying on a large corpus, and reported promising results.

Web information has also been used as an ad-ditional knowledge source for keyword extraction. (Turney, 2002) selected a set of keywords first and then determined whether to add another keyword hy-pothesis based on its PMI (point-wise mutual infor-mation) score to the current selected keywords. The preselected keywords can be generated using basic extraction algorithms such as TFIDF. It is impor-tant to ensure the quality of the first selection for the subsequent addition of keywords. Other researchers also used PMI scores between each pair of candidate keywords to select the top k% of words that have the highest average PMI scores as the final keywords (Inkpen and Desilets, 2004).

Keyword extraction has also been treated as a classification task and solved using supervised ma-chine learning approaches (Frank et al., 1999; Tur-ney, 2000; Kerner et al., 2005; Turney, 2002; Tur-ney, 2003). In these approaches, the learning al-gorithm needs to learn to classify candidate words in the documents into positive or negative examples using a set of features. Useful features for this ap-proach include TFIDF and its variations, position of a phrase, POS information, and relative length of a phrase (Turney, 2000). Some of these features may not work well for meeting transcripts. For exam-ple, the position of a phrase (measured by the num-ber of words before its first appearance divided by the document length) is very useful for news article text, since keywords often appear early in the doc-ument (e.g., in the first paragraph). However, for the less well structured meeting domain (lack of ti-tle and paragraph), these kinds of features may not be indicative. A supervised approach to keyword ex-traction was used in (Liu et al., 2008). Even though the data set in that study is not very big, it seems that a supervised learning approach can achieve reason-able performance for this task.

Another line of research for keyword extrac-tion has adopted graph-based methods similar to Google X  X  PageRank algorithm (Brin and Page, 1998). In particular, (Wan et al., 2007) attempted to use a reinforcement approach to do keyword ex-traction and summarization simultaneously, on the assumption that important sentences usually contain keywords and keywords are usually seen in impor-tant sentences. We also find that this assumption also holds using statistics obtained from the meeting cor-pus used in this study. Graph-based methods have not been used in a genre like the meeting domain; therefore, it remains to be seen whether these ap-proaches can be applied to meetings.

Not many studies have been performed on speech transcripts for keyword extraction. The most rel-evant work to our study is (Plas et al., 2004), where the task is keyword extraction in the mul-tiparty meeting corpus. They showed that lever-aging semantic resources can yield significant per-formance improvement compared to the approach based on the relative frequency ratio (similar to IDF). There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; De-silets et al., 2002; Rogina, 2002). (Wu et al., 2007) showed that keyword extraction combined with se-mantic verification can be used to improve speech retrieval performance on broadcast news data. In (Rogina, 2002), keywords were extracted from lec-ture slides, and then used as queries to retrieve rel-evant web documents, resulting in an improved lan-guage model and better speech recognition perfor-mance of lectures. There are many differences be-tween written text and speech  X  meetings in par-ticular. Thus our goal in this paper is to investi-gate whether we can successfully apply some exist-ing techniques, as well as propose new approaches to extract keywords for the meeting domain. The aim of this study is to set up some starting points for research in this area. We used the meetings from the ICSI meeting data (Janin et al., 2003), which are recordings of naturally occurring meetings. All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al., 2004), topics, and extractive sum-maries (Murray et al., 2005). The ASR output for this corpus is obtained from a state-of-the-art SRI conversational telephone speech system (Zhu et al., 2005), with a word error rate of about 38.2% on the entire corpus. We align the human transcripts and ASR output, then map the human annotated DA boundaries and topic boundaries to the ASR words, such that we have human annotation of these infor-mation for the ASR output.

We recruited three Computer Science undergradu-ate students to annotate keywords for each topic seg-ment, using 27 selected ICSI meetings. 1 Up to five indicative key words or phrases were annotated for each topic. In total, we have 208 topics annotated with keywords. The average length of the topics (measured using the number of dialog acts) among all the meetings is 172.5, with a high standard devi-ation of 236.8. We used six meetings as our devel-opment set (the same six meetings as the test set in (Murray et al., 2005)) to optimize our keyword ex-traction methods, and the remaining 21 meetings for final testing in Section 5.

One example of the annotated keywords for a topic segment is:
Note that these meetings are research discussions, and that the annotators may not be very familiar with the topics discussed and often had trouble deciding the important sentences or keywords. In addition, limiting the number of keywords that an annotator can select for a topic also created some difficulty. Sometimes there are more possible keywords and the annotators felt it is hard to decide which five are the most topic indicative. Among the three annota-tors, we notice that in general the quality of anno-tator I is the poorest. This is based on the authors X  judgment, and is also confirmed later by an indepen-dent human evaluation (in Section 6).

For a better understanding of the gold standard used in this study and the task itself, we thoroughly analyzed the human annotation consistency. We re-moved the topics labeled with  X  X hitchat X  by at least one annotator, and also the digit recording part in the ICSI data, and used the remaining 140 topic seg-ments. We calculated the percentage of keywords agreed upon by different annotators for each topic, as well as the average for all the meetings. All of the consistency analysis is performed based on words. Figure 1 illustrates the annotation consistency over different meetings and topics. The average consis-tency rate across topics is 22.76% and 5.97% among any two and all three annotators respectively. This suggests that people do not have a high agreement on keywords for a given document. We also notice that the two person agreement is up to 40% for sev-eral meetings and 80% for several individual top-ics, and the agreement among all three annotators reaches 20% and 40% for some meetings or topics. This implies that the consistency depends on topics (e.g., the difficulty or ambiguity of a topic itself, the annotators X  knowledge of that topic). Further studies are needed for the possible factors affecting human agreement. We are currently creating more annota-tions for this data set for better agreement measure and also high quality annotation. Our task is to extract keywords for each of the topic segments in each meeting transcript. Therefore, by  X  X ocument X , we mean a topic segment in the re-mainder of this paper. Note that our task is different from keyword spotting, where a keyword is provided and the task is to spot it in the audio (along with its transcript).

The core part of keyword extraction is for the sys-tem to assign an importance score to a word, and then pick the top ranked words as keywords. We compare different methods for weight calculation in this study, broadly divided into the following two categories: the TFIDF framework and the graph-based model. Both are unsupervised learning meth-ods. 2 In all of the following approaches, when se-lecting the final keywords, we filter out any words appearing on the stopword list. These stopwords are generated based on the IDF values of the words us-ing all the meeting data by treating each topic seg-ment as a document. The top 250 words from this list (with the lowest IDF values) were used as stop-words. We generated two different stopword lists for human transcripts and ASR output respectively. In addition, in this paper we focus on performing key-word extraction at the single word level, therefore no key phrases are generated. 4.1 TFIDF Framework (A) Basic TFIDF weighting
The term frequency (TF) for a word w i in a doc-ument is the number of times the word occurs in the document. The IDF value is: where N i denotes the number of the documents con-taining word w i , and N is the total number of the documents in the collection. We also performed L 2 normalization for the IDF values when combining them with other scores. (B) Part of Speech (POS) filtering
In addition to using a stopword list to remove words from consideration, we also leverage POS in-formation to filter unlikely keywords. Our hypothe-sis is that verb, noun and adjective words are more likely to be keywords, so we restrict our selection to words with these POS tags only. We used the TnT POS tagger (Brants, 2000) trained from the Switch-board data to tag the meeting transcripts. (C) Integrating word clustering
One weakness of the baseline TFIDF is that it counts the frequency for a particular word, without considering any words that are similar to it in terms of semantic meaning. In addition, when the docu-ment is short, the TF may not be a reliable indicator of the importance of the word. Our idea is therefore to account for the frequency of other similar words when calculating the TF of a word in the document. For this, we group all the words into clusters in an unsupervised fashion. If the total term frequency of all the words in one cluster is high, it is likely that this cluster contributes more to the current topic from a thematic point of view. Thus we want to as-sign higher weights to the words in this cluster.
We used the SRILM toolkit (Stolcke, 2002) for automatic word clustering over the entire docu-ment collection. It minimizes the perplexity of the induced class-based n-gram language model com-pared to the original word-based model. Using the clusters, we then adjust the TF weighting by inte-grating with the cluster term frequency (CTF): where the last summation component means the to-tal term frequency of all the other words in this docu-ment that belong to the same cluster C i as the current word w i . We set parameter  X  to be slightly larger than 1. We did not include stopwords when adding the term frequencies for the words in a cluster. (D) Combining with sentence salience score
Intuitively, the words in an important sentence should be assigned a high weight for keyword ex-traction. In order to leverage the sentence infor-mation, we adjust a word X  X  weight by the salience scores of the sentences containing that word. The sentence score is calculated based on its cosine sim-ilarity to the entire meeting. This score is often used in extractive summarization to select summary sen-tences (Radev et al., 2001). The cosine similarity between two vectors, D 1 and D 2 , is defined as: where t i is the term weight for a word w i , for which we use the TFIDF value. 4.2 Graph-based Methods For the graph-based approach, we adopt the itera-tive reinforcement approach from (Wan et al., 2007) in the hope of leveraging sentence information for keyword extraction. This algorithm is based on the assumption that important sentences/words are con-nected to other important sentences/words.

Four graphs are created: one graph in which sen-tences are connected to other sentences (S-S graph), one in which words are connected to other words (W-W graph), and two graphs connecting words to sentences with uni-directional edges (W-S and S-W graphs). Stopwords are removed before the creation of the graphs so they will be ineligible to be key-words.

The final weight for a word node depends on its connection to other words (W-W graph) and other sentences (W-S graph); similarly, the weight for a sentence node is dependent on its connection to other sentences (S-S graph) and other words (S-W graph). That is, where u and v are the weight vectors for sentence and word nodes respectively, U, V, W,  X  W represent the S-S, W-W, S-W, and W-S connections.  X  and  X  specify the contributions from the homogeneous and the heterogeneous nodes. The initial weight is a uni-form one for the word and sentence vector. Then the iterative reinforcement algorithm is used until the node weight values converge (the difference be-tween scores at two iterations is below 0.0001 for all nodes) or 5,000 iterations are reached.

We have explored various ways to assign weights to the edges in the graphs. Based on the results on the development set, we use the following setup in this paper:  X  W-W Graph: We used a diagonal matrix for  X  S-W and W-S Graphs: The weight for an  X  S-S Graph: The sentence node uses a vector
Similar to the above TFIDF framework, we also use POS filtering for the graph-based approach. Af-ter the weights for all the words are determined, we select the top ranked words with the POS restriction. Using the approaches described above, we com-puted weights for the words and then picked the top five words as the keywords for a topic. We chose five keywords since this is the number of keywords that human annotators used as a guideline, and it also yielded good performance in the development set. To evaluate system performance, in this section we use human annotated keywords as references, and compare the system output to them. The first metric we use is F-measure, which has been widely used for this task and other detection tasks. We compare the system output with respect to each human anno-tation, and calculate the maximum and the average F-scores. Note that our keyword evaluation is word-based. When human annotators choose key phrases (containing more than one word), we split them into words and measure the matching words. Therefore, when the system only generates five keywords, the upper bound of the recall rate may not be 100%. In (Liu et al., 2008), a lenient metric is used which ac-counts for some inflection of words. Since that is highly correlated with the results using exact word match, we report results based on strict matching in the following experiments.

The second metric we use is similar to Pyramid (Nenkova and Passonneau, 2004), which has been used for summarization evaluation. Instead of com-paring the system output with each individual hu-man annotation, the method creates a  X  X yramid X  using all the human annotated keywords, and then compares system output to this pyramid. The pyra-mid consists of all the annotated keywords at dif-ferent levels. Each keyword has a score based on how many annotators have selected this one. The higher the score, the higher up the keyword will be in the pyramid. Then we calculate an oracle score that a system can obtain when generating k keywords. This is done by selecting keywords in the decreas-ing order in terms of the pyramid levels until we obtain k keywords. Finally for the system hypoth-esized k keywords, we compute its score by adding the scores of the keywords that match those in the pyramid. The system X  X  performance is measured us-ing the relative performance of the system X  X  pyramid scores divided by the oracle score.

Table 1 shows the results using human transcripts for different methods on the 21 test meetings (139 topic segments in total). For comparison, we also show results using the supervised approach as in (Liu et al., 2008), which is the average of the 21-fold cross validation. We only show the maximum F-measure with respect to individual annotations, since the average scores show similar trend. In ad-dition, the weighted relative scores already accounts for the different annotation and human agreement. We notice that for the TFIDF framework, adding POS information slightly helps the basic TFIDF method. In all the meetings, our statistics show that adding POS filtering removed 2.3% of human anno-tated keywords from the word candidates; therefore, this does not have a significant negative impact on the upper bound recall rate, but helps eliminate un-likely keyword candidates. Using word clustering does not yield a performance gain, most likely be-cause of the clustering technique we used  X  it does clustering simply based on word co-occurrence and does not capture semantic similarity properly.
Combining the term weight with the sentence salience score improves performance, supporting the hypothesis that summary sentences and keywords can reinforce each other. In fact we performed an analysis of keywords and summaries using the fol-lowing two statistics: the normalized frequency of a keyword w i in the summary and the entire topic respectively; and where P S summary represents the percentage of the sentences containing at least one keyword among all the sentences in the summary, and similarly P S topic is measured using the entire topic segment. We found that the average k and s are around 3.42 and 6.33 respectively. This means that keywords are more likely to occur in the summary compared to the rest of the topic, and the chance for a summary sen-tence to contain at least one keyword is much higher than for the other sentences in the topic.

For the graph-based methods, we notice that adding POS filtering also improves performance, similar to the TFIDF framework. However, the graph method does not perform as well as the TFIDF approach. Comparing with using TFIDF alone, the graph method (without using POS) yielded worse re-sults. In addition to using the TFIDF for the word nodes, information from the sentences is used in the graph method since a word is linked to sentences containing this word. The global information in the S-S graph (connecting a sentence to other sentences in the document) is propagated to the word nodes. Unlike the study in (Wan et al., 2007), this infor-mation does not yield any gain. We did find that the graph approach performed better in the development set, but it seems that it does not generalize to this test set.

Compared to the supervised results, the TFIDF approach is worse in terms of the individual maxi-mum F-measure, but achieves similar performance when using the weighted relative score. However, the unsupervised TFIDF approach is much simpler and does not require any annotated data for train-ing. Therefore it may be easily applied to a new domain. Again note that these results used word-based selection. (Liu et al., 2008) investigated adding bigram key phrases, which we expect to be independent of these unigram-based approaches and adding bigram phrases will yield further per-formance gain for the unsupervised approach. Fi-nally, we analyzed if the system X  X  keyword ex-traction performance is correlated with human an-notation disagreement using the unsupervised ap-proach (TFIDF+POS+Sent weight). The correla-tion (Spearman X  X   X  value) between the system X  X  F-measure and the three-annotator consistency on the 27 meetings is 0.5049 (p=0.0072). This indi-cates that for the meetings with a high disagreement among human annotators, it is also challenging for the automatic systems.

Table 2 shows the results using ASR output for various approaches. The performance measure is the same as used in Table 1. We find that in gen-eral, there is a performance degradation compared to using human transcripts, which is as expected. We found that only 59.74% of the human annotated keywords appear in ASR output, that is, the upper bound of recall is very low. The TFIDF approach still outperforms the graph method. Unlike on hu-man transcripts, the addition of information sources in the TFIDF approach did not yield significant per-formance gain. A big difference from the human transcript condition is the use of sentence weight-ing  X  adding it degrades performance in ASR, in contrast to the improvement in human transcripts. This is possibly because the weighting of the sen-tences is poor when there are many recognition er-rors from content words. In addition, compared to the supervised results, the TFIDF method has sim-ilar maximum F-measure, but is slightly worse us-ing the weighted score. Further research is needed for the ASR condition to investigate better modeling approaches.
 Given the disagreement among human annotators, one question we need to answer is whether F-measure or even the weighted relative scores com-pared with human annotations are appropriate met-rics to evaluate system-generated keywords. For example, precision measures among the system-generated keywords how many are correct. How-ever, this does not measure if the unmatched system-generated keywords are bad or acceptable. We therefore performed a small scale human evaluation. We selected four topic segments from four differ-ent meetings, and gave output from different sys-tems to five human subjects. The subjects ranged in age from 22 to 63, and all but one had only basic knowledge of computers. We first asked the eval-uators to read the entire topic transcript, and then presented them with the system-generated keywords (randomly ordered by different systems). For com-parison, the keywords annotated by our three hu-man annotators were also included without reveal-ing which sets of keywords were generated by a human and which by a computer. Because there was such disagreement between annotators regard-ing what made good keywords, we instead asked our evaluators to mark any words that were definitely not keywords. Systems that produced more of these rejected words (such as  X  X asically X  or  X  X mm-hm X ) are assumed to be worse than those containing fewer rejected words. We then measured the percentage of rejected keywords for each system/annotator. The results are shown in Table 3. Not surprisingly, the human annotations rank at the top. Overall, we find human evaluation results to be consistent with the automatic evaluation metrics in terms of the ranking of different systems.

Note this rejection rate is highly related to the re-call/precision measure in the sense that it measures how many keywords are acceptable (or rejected) among the system generated ones. However, instead of comparing to a fixed set of human annotated key-words (e.g., five) and using that as a gold standard to compute recall/precision, in this evaluation, the human evaluator may have a larger set of accept-able keywords in their mind. We also measured the human evaluator agreement regarding the accepted or bad keywords. We found that the agreement on a bad keyword among five, four, and three human evaluator is 10.1%, 14.8%, and 10.1% respectively. This suggests that humans are more likely to agree on a bad keyword selection compared to agreement on the selected keywords, as discussed in Section 3 (even though the data sets in these two analysis are not the same). Another observation from the human evaluation is that sometimes a person rejects a key-word from one system output, but accepts that on the list from another system. We are not sure yet whether this is the inconsistency from human evalu-ators or whether the judgment is based on a word X  X  occurrence with other provided keywords and thus some kind of semantic coherence. Further investi-gation on human evaluation is still needed. In this paper, we evaluated unsupervised keyword extraction performance for the meeting domain, a genre that is significantly different from most pre-vious work. We compared several different ap-proaches using the transcripts of the ICSI meeting corpus. Our results on the human transcripts show that the simple TFIDF based method is very compet-itive. Adding additional knowledge such as POS and sentence salience score helps improve performance. The graph-based approach performs less well in this task, possibly because of the lack of structure in this domain. We use different performance measure-ments, including F-measure with respect to individ-ual human annotations and a weighted metric rela-tive to the oracle system performance. We also per-formed a new human evaluation for this task and our results show consistency with the automatic mea-surement. In addition, experiments on the ASR out-put show performance degradation, but more impor-tantly, different patterns in terms of the contributions of information sources compared to using human transcripts. Overall the unsupervised approaches are simple but effective; however, system performance compared to the human performance is still low, suggesting more work is needed for this domain.
For the future work, we plan to investigate dif-ferent weighting algorithms for the graph-based ap-proach. We also need a better way to decide the number of keywords to generate instead of using a fixed number. Furthermore, since there are multiple speakers in the meeting domain, we plan to incor-porate speaker information in various approaches. More importantly, we will perform a more rigorous human evaluation, and also use extrinsic evaluation to see whether automatically generated keywords fa-cilitate tasks such as information retrieval or meeting browsing. This work is supported by NSF award IIS-0714132. Any opinions expressed in this work are those of the authors and do not necessarily reflect the views of NSF.

