 Real world large-scale recommender systems are always dy-namic: new users and items continuously enter the system, and the status of old ones (e.g., users X  preference and items X  popularity) evolve over time. In order to handle such dy-namics, we propose a recommendation framework consisting of an online component and an offline component, where the newly arrived items are processed by the online component such that users are able to get suggestions for fresh infor-mation, and the influence of longstanding items is captured by the offline component. Based on individual users X  rating behavior, recommendations from the two components are combined to provide top-N recommendation. We formulate recommendation problem as a ranking problem where learn-ing to rank is applied to extend upon matrix factorization to optimize item rankings by minimizing a pairwise loss func-tion. Furthermore, to better model interactions between users and items, Latent Dirichlet Allocation is incorporated to fuse rating information and textual information. Real data based experiments demonstrate that our approach out-performs the state-of-the-art models by at least 61 . 21% and 50 . 27% in terms of mean average precision (MAP) and nor-malized discounted cumulative gain (NDCG) respectively. H.3.3 [ Information Search and Retrieval ]: Information filtering; H.4 [ Information Systems Applications ]: Mis-cellaneous top-N recommendation; learning to rank; matrix factoriza-tion; topic modeling
Most large-scale recommender systems are highly dynamic, where new users and items continuously enter the system, and existing users X  preference and items X  popularity may vary over time. In order to handle such dynamics for per-sonalized recommendation, a set of time-aware models have been proposed, which employed decay function to give more weight to recent observations [15], or considered only infor-mation in specific time-windows [14], or incorporated tem-poral effects into latent factor models [13, 25]. However, ex-isting solutions either only focus on local information (e.g., recent data or data in certain time-windows) thus deem-phasizing the influence of global information (e.g., the long-standing items) which might also reflect users X  current pref-erence (e.g., seasonal/periodic preference) [3], or suffer from the scalability issue, e.g., frequently re-training latent factor model over big data is computationally expensive.
In order to address the issues of existing time-aware rec-ommendation models, we propose a flexible framework that takes into account both fresh information and longstanding information but processes them separately. The framework consists of an online component and an offline component, where the online component frequently updates the online recommendation model such that users are able to timely re-ceive recommendations for continuously arrived items, and the offline component slowly updates the offline recommen-dation model to capture the influence of (typically huge vol-ume) longstanding items 1 . When a recommendation query is issued, two recommendation lists from the two compo-nents are generated and combined to provide the final top-N recommendation to the user based on her past rating behav-ior (i.e., preference on the freshness of items).
For each component (online or offline), in contrast to the state-of-the-art solutions that focus on achieving high rating prediction accuracy, we model the recommendation prob-lem as a ranking problem. Specifically, we apply learning to rank technique on the basis of matrix factorization to opti-mize recommendation ranking by minimizing a pairwise loss function. The rank of a pair of items is based on their rele-vance, where the relevance is determined by the users X  pref-erence on items, characterized by the corresponding user X  X  and items X  latent factors.

Furthermore, in addition to rating information, we con-sider textual contents which are pervasive in recommender systems (e.g., tags, descriptions of items) to more precisely model users X  preference on items. In this setting, each item is represented by a  X  X ag-of-words X  and the words of all items
In this work, we consider a generic recommendation sce-nario where every item is potentially a candidate for recom-mendation like movie and book recommendation. For the scenario where items have short lifespan (e.g., news recom-mendation), we can remove outdated items periodically or just use online component. construct a text corpus. We apply latent dirichlet allocation (LDA) [4] to extract a set of topics from the corpus, and assign each topic a latent factor vector that shares the same space with that of users and items. A user X  X  preference on an item is inferred by combining (1) the user X  X  affinity to topics, derived by multiplying user-specific and topic-specific latent factors and (2) the correlation between topics and the item, i.e., the topic distribution of the item inferred by LDA. This is different from the state-of-the-art approaches [22, 17] that reconcile the number of topics and the dimensionality of la-tent factors (i.e., implicitly assume that textual content is the dominant factor that influences the distribution of latent factors 2 ), thus is able to more flexibly incorporate textual contents into matrix factorization.

The contributions of this paper are summarized as follows: (1) We propose a recommendation framework consisting of an online component and an offline component to process the continuously arrived items and longstanding items sepa-rately. (2) We fuse learning to rank and latent factor model to provide top-N recommendation by optimizing a pairwise loss function. Additionally, we apply LDA to better model users X  preference for item ranking. To the best of our knowl-edge, this is the first work that integrates topic modeling into learning to rank model to improve top-N recommendation. Note that such a recommendation model is used by both online an offline components. (3) We cluster each user X  X  rated items based on their topic distribution such that the items in the same cluster can be more meaningfully com-pared for optimizing pairwise loss function. Moreover, such a clustering strategy removes the item pairs that are not meaningfully comparable, thus reducing the training data while keeping its informativeness. (4) We evaluate the per-formance of the proposed recommendation method over real datasets. Experimental results show that our approach sig-nificantly outperforms the state-of-the-art models. Traditional neighborhood-based algorithms handle temporal effect by either giving more weight to recent information or focusing on observations in specific time windows. For instance, in [25], a decay function f ( x ) = e  X   X t was utilized to reduce the effect of old ratings, where t is the time at which a rating was given and  X  is the decay rate. Liu. et al. [15] used the similar decay function for both similarity computation and rating prediction.

For latent factor models, Koren [13] tackled temporal ef-fect by using time function to model user and item biases in matrix factorization. In [25], the authors split time in equal time intervals and added the time dimension to rating matrix to form a tensor. Alternating least square algorithm was applied to optimize the tensor model.

Recently, a series of solutions were proposed to particu-larly process the highly dynamic data stream for social me-dia like Twitter. In [7], the authors proposed a collaborative ranking algorithm to recommend interesting tweets to users taking into account tweet topic level factors, user social re-lation factors, explicit features and the quality of tweets. In
This assumption is not always true in reality. For instance, users X  preference on items may be influenced by other factors like contexts, e.g., users X  emotion and social information [16]. [9], tweet topic recommendation was studied, and a ranking algorithm called Stream Ranking Matrix Factorization was proposed where a pairwise ranking approach was applied to optimize the personalized ranking of topics. Agarwal et al. [2] proposed FOBFM model for fast online recommendation learning through effective model initialization using histori-cal information.
 Recently, top-N recommendation that directly generates a list of items for users has attracted a lot of attention. In particular, learning to rank techniques have been applied to extend upon latent factor models for top-N recommen-dation. CofiRank [24] was the first solution in this trend, where NDCG was used as the loss function, and the model was trained by minimizing over a convex upper bound of the loss function. Kabbur et al. [12] proposed a factored item similarity based method, where a ranking loss function that optimizes the area under the curve (AUC) was used.
Shi et al. proposed a set of ranking-oriented algorithms by fusing learning to rank and latent factor models. TFMAP [20] directly maximized MAP to optimize a ranked list of items for individual users. Tensor factorization was ap-plied to model implicit feedback data (e.g., click) under each type of context. Similar solutions include CLiMF [21] and xCLiMF [19], which focused on implicit binary relevance data and explicit rating respectively.
 By fusing topic models and latent factor models, hybrid fil-tering recommendation methods have been improved. The basic idea is to assign a latent topic to each word of an item, which is represented by a  X  X ag-of-words X , and then generate item latent factors by aggregating the topics of all words of this item (i.e., each latent factor is characterized by a topic). A missing rating is predicted by combining the tar-get user X  X  affinity to the topics and the correlation between the topics and the target item. A set of algorithms have been proposed to incorporate topics into user/item latent factors [1, 18, 17] to better model user-item interactions for rating prediction. For instance, collaborative topic regres-sion [22] represented users X  latent factors by topic interests and assumed that items X  latent factors were generated by a topic model. In [17], a model called  X  X idden Factors as Topics X  (HFT) was proposed to combine ratings and reviews for product recommendation, where the topics were used as regularizers for the latent factors of users and products.
In this section, we present our dynamic top-N recommen-dation framework. We start with framework architecture and notation definition in Section 3.1. A top-N recommen-dation model, which is built on learning to rank, matrix factorization and LDA, is elaborated in Section 3.2. We pro-pose a heuristic algorithm to combine online and offline rec-ommendations in Section 3.3. Related issues are discussed in Section 3.4.
As illustrated in Fig. 1, the proposed recommendation framework consists of two main components: an online com-ponent that handles continuously arrived items and an of-fline component that processes longstanding items. Specif-ically, the online component stores the coming data in the online storage module (which is typically of small size, com-pared to the offline storage module) and updates the on-line recommendation model frequently such that the fresh information can be timely recommended to users. The up-date frequency depends on characteristics of the coming data (e.g., volume and velocity) as well as the capability of com-puting infrastructure. Periodically, the data in online stor-age module is merged to the offline storage module such that the online component only processes the most recent infor-mation timely. For instance, the online component only pro-cesses a window of 1000 recently entered items (for online recommendation); once the window is full, the 1000 items are moved to offline component and the online component starts processing new items. Due to the large volume of data in the offline storage module, the offline recommen-dation model is updated less frequently to capture users X  preference on the longstanding items. When a recommen-dation query for a user is issued, both online and offline component provide a recommendation list respectively and the recommendation aggregator serves to combine the two lists based on the user X  X  past rating behavior to provide final top-N recommendation.
 The notations used in this paper are introduced as follows. We denote the user set by U = { u 1 ,u 2 ,...,u m } , and the item set by V = { v 1 ,v 2 ,...,v n } . Any user can rate any item based on her preference. We assume that the value of a rating is a discrete variable in a range L = { L 1 ,L 2 ,...,L l } , e.g., five-point likert scale used by Netflix. A rating provided by user u to item v is denoted by R u,v . We assume each item v is associated with a  X  X ag-of-words X , denoted by T v , which is constructed from relevant textual metadata such as the description and tags of the item 3 . The summation of such  X  X ag-of-words X  constructs a text corpus T for LDA.
Latent factor models, e.g., matrix factorization (MF) have become the state-of-the-art method for recommendation. The rating matrix R  X  R m  X  n ( m is the number of users and n is the number of items) is factorized into one user-specific matrix U  X  R l  X  m and one item-specific matrix V  X  R l  X  n
This assumption holds in most online applications like e-commerce, review sites, social media, etc.  X  R  X  U T V , where l is the dimensionality of a latent factor vector that characterizes a user or an item. For user u , the elements of U (i.e., U u ) measure u  X  X  affinity to the corre-sponding latent factors; for item v , the elements of V (i.e., V ) measure the correlation between v and the correspond-ing latent factors. Accordingly, the resulting U T u V v captures the correlation between user u and item v , i.e., the predicted rating from u to v . In order to measure the accuracy of rat-ing prediction, an objective function is defined as follows: L = min where I u,v is 1 if user u has rated item v , and 0 otherwise. To avoid overfitting, regularization terms are added, where the parameter  X  controls the extent of regularization. Equa-tion 1 can be solved by stochastic gradient descent (SGD), which iteratively updates user-specific and item-specific la-tent factors to minimize the sum-of-squared-error.
Traditional approaches recommend items to users based on the predicted ratings. However, it is non-trivial to ac-curately interpret and infer a user X  X  preference simply us-ing ratings, in particular, when such rating information is sparse, which is common in most recommender systems. Recently, some effort has been made to integrate textual contents via topic modeling, e.g., LDA into MF to improve recommendation accuracy [22, 17]. However, in most exist-ing solutions, the dimensionality of latent factors in MF is the same with the number of topics in LDA, and each latent factor is initialized or bounded by the corresponding topic distribution. This implicitly assumes that the latent factors of users and items are completely characterized by textual contents, which is not always true in reality. For instance, a user X  X  preference may be affected by other factors like social relationships and users X  emotions [16]. We therefore propose a novel model to fuse MF and LDA by relaxing the arguably strong coupling between topics and items X  latent factors. We apply LDA on the text corpus T to extract k topics D = { d 1 ,...,d k } such that each word w has a probability  X  w,d of being assigned to topic d , and each item v is rep-resented by a topic distribution  X  v . Different from previous methods [22, 17] that reconcile the number of topics and the dimensionality of user/item latent factor vector, in our approach, the topic number k is not necessarily equal to the factor latent vector dimensionality l , and each topic d is also associated with and characterized by a latent factor vector X d that is sharing the same dimensional space with user/item latent factors.

Accordingly, the correlation between a user and an item, which was captured through the user specific and item spe-cific latent factors, is reformulated by incorporating the ef-fect of topics. Specifically, item specific latent factor matrix V is represented by the product of two matrices: where X  X  X  l  X  k is a topic specific latent factor matrix where each column is a latent factor vector that characterizes the corresponding topic. Y  X  X  k  X  n is a matrix that stores cor-relations between items and topics, where the correlation between topic d and item v is measured by the correspond-ing topic distribution  X  d,v over this item. Therefore, when inferring user u  X  X  preference on item v , u  X  X  preferences on the k topics are firstly measured through user specific and topic specific latent factors (i.e., dot product), and then the corre-lations between the k topics and item v are derived, finally the two aspects are fused to capture u  X  X  preference on item v . With topics, interactions between users and items can be more meaningfully modeled than traditional scenario where only rating information (less interpretable) is used. Fur-thermore, the dimensionality of latent factor vector is not bounded by the number of topics thus is more flexible to capture other latent effects besides texts. By incorporating topics the rating from user u to item v can be obtained by
In contrast to the state-of-the-art approaches that focus on improving the accuracy of rating prediction, we aim to provide top-N recommendation which is more realistic in real-world recommendation scenarios. To this end, we ap-ply learning to rank technique to directly optimize a ranking measure for recommendation. Among numerous loss func-tions employed by learning to rank algorithms, we choose the one used in RankNet [5], which is not only popular in academia, but is also used in a commercial search engine. Specifically, we employ a pairwise approach to learn top-N item ranking, where the ranking task is formulated as a pair-wise classification task, i.e., to rank a pair of items. Note that besides pairwise approach, another two alternatives for learning to rank are pointwise and listwise approaches. Pointwise approach is actually equivalent to rating predic-tion. For listwise approach, when training the recommenda-tion model, it is non-trial to meaningfully construct a list of ranked item (i.e., items are rated by a user individually thus cannot intuitively construct a list structure). We therefore choose a pairwise approach which can better structure each user X  X  rated items for learning at a finer granularity.
To provide top-N recommendation, existing pairwise based learning to rank models are learned by comparing the rele-vance of any pair of rated items of each user [9]. However, this strategy fails to distinguish characteristics of items thus missing the semantic comparability among items. For in-stance, it is useful to learn that a user prefers rock music to lyric music, but meaningless to compare rock music and programming books. To solve this issue, we propose to clus-ter a user X  X  ratings by leveraging LDA such that the rated items can be more meaningfully compared.

Recall that we extract k topics from corpus T , and each item v is represented by a topic distribution  X  v . Based on such topic distributions, we divide user u  X  X  rated items into k groups by applying k -mean clustering algorithm 4 . Note that if the size of a group is smaller than a threshold (e.g., 2), this group is merged into the nearest one. Within each group, pairs of items, where in each pair the first item is assumed to be ranked higher than the second one (based on ratings), are obtained, and we denote  X  u = { ( v i ,v j ) | v V u V v j  X  X  u V v i .v j } as a set of item pais of user u across all groups. Aggregating all users X  such item pairs constructs training data for pairwise learning to rank.

It is worth noting that a byproduct of such a topic based clustering strategy is that it removes the item pairs where
K-means clustering algorithm is applied due to its simplic-ity. More sophisticated clustering algorithms may be used, but the discussion on the tradeoff between the improved clustering quality and the increased computational complex-ity is beyond the scope of this work. the comparison of the two items is meaningless (instead of considering all possible item pairs, which is adopted by most existing approaches), thus reducing the volume of training data while keeping its informativeness.

Following its definition in RankNet [5], we apply a cross entropy cost function w.r.t. user u as follows: where P u i,j is the inferred posterior probability (by a logistic function) that item v i is ranked higher than item v j by user u : where f ( u,v i ) returns item v i  X  X  relevance score, which is the predicted rating from user u to item v i , obtained by our topic-boosted approach (see Equation 3).  X  P u i,j is the desired target value for the posterior probability (by a logistic func-tion), which is derived using the known ratings R u,i and R u,j provided by user u to the pair of items v i and v j :
By substituting P u i,j and  X  P u i,j with Eq. 5 and 6, the loss function Eq. 4 is reformulated as:
By considering all users X  item pairs, we define the objec-tive function that aims to minimize pairwise loss, taking into account the regularization for avoiding overfitting: L
To solve the objective function, which is non-convex, we apply Stochastic Gradient Descent (SGD) to try to find a local minima. We perform gradient descent with respect to user specific and topic specific latent factors respectively: The latent factors are updated iteratively: where  X  is the learning rate. By sorting the predicted ratings (i.e., relevance scores) of items (in descending order) that user u has not rated, we generate top-N items for user u .
To produce final top-N recommendation by combining recommendations from online and offline components 5 , we investigate a user X  X  past rating behavior to assess her prefer-ence on the freshness of items. We denote the freshness of an item v when it was rated by  X  v , which is a binary variable where 1 indicates that this item was fresh (i.e., processed by the online component) and 0 indicates that this item was longstanding (i.e., processed by the offline component).
For user u , we record the freshness of each item that u has rated: {  X  1 , X  2 ,... } . These freshness indicators are mod-eled as observations of independent Bernoulli trials. In each trial, the success probability (i.e., the probability of rating a fresh item) is modeled by Beta distribution with parameters  X  and  X  (we start with  X  =  X  = 1, which translates into complete uncertainty about the distribution of the param-eter, modeled by uniform distribution: Beta(1, 1) = U(0, 1)). After observing s successes in n trials, the posterior density of the probability is Beta(  X  =  X  + s ,  X  =  X  + n -s ). The expectation probability 6 of rating a fresh item is then obtained by  X  u =  X   X  +  X  . The higher the  X  u , the more likely user u will rate a fresh item.

Given the online recommendation list  X  u on and offline rec-ommendation list  X  u off for user u , as well as u  X  X  preference on the freshness of items  X  u , the final top-N recommenda-tion list  X  u is generated iteratively: at each iteration, the top fresh item v 0 is picked from  X  u on with a probability of  X  u ; otherwise, the top longstanding item v 00 is picked from  X  end of  X  u , and removed from  X  u on or  X  u off . This process continues until the size of  X  u increases to N .
To achieve personalized top-N recommendation, we pro-posed a recommendation framework by leveraging learning to rank, matrix factorization and LDA. Two important is-sues, scalability and cold-start, are discussed as follows.
The time complexity of the optimization procedure is O ( m  X   X  t ), where m is the number of users, and  X  t is the average number of items per user [5]. In practice, sampling methods might be applied to sample a subset of items for each users (i.e., decreasing the value of  X  t ), and hence reduce the compu-tational complexity. Furthermore, for each user, we cluster the rated items based on their topic distribution to make the pairwise item relevance comparison more meaningful 7 . This greatly reduces the number of comparisons (i.e.,  X  t 2 ), thus simplifying the model training. Another way to cope with the complexity of the model is to parallelize the optimization
Remind that online and offline components use the same recommendation model elaborated in Section 3.2.
A straightforward extension is to add a decay factor to emphasize users X  recent preference on items X  freshness to im-prove prediction accuracy. We leave as a future work a more detailed discussion on such extensions.
This may worsen the issue of data sparsity. In practice, we only apply clustering strategy to users who have rated sufficient number of items (e.g., at least 20) procedure, e.g., distributed SGD [11] or downpour SGD [8], which makes our approach applicable to even larger data.
It is also worth noting that  X  X old-start X  is still an open question in most recommender systems. For our approach, this issue is particularly important when fresh items are merged from online storage module to offline storage module (i.e., the input data for online recommendation model will be sparse). To address this issue, each user X  X  most recently rated items are not only merged to the offline storage module but also kept in the online storage module as the buffer to bootstrap newly arrived items 8 , e.g., by measuring the sim-ilarity between two items based on their topic distributions. Moreover, a user X  X  recent ratings reflect her current prefer-ence, so the derived online recommendation is expected to be more accurate. Regarding the completely new users, ex-isting solutions (e.g., [23]) can be applied on the basis of our recommendation approach, however, the detailed discussion about this issue is beyond the scope of this paper.
We use a dataset collected from Douban (www.douban.com), one of the largest Chinese based social platforms for sharing reviews and recommendations for books, movies and music. Users provide ratings in 5-star scale to indicate their prefer-ence on items. The dataset contains ratings submitted from 2007 to 2011. Table 1 summarizes the statistics of Douban dataset (shared by the first author of [26]).
 Book 1,097,148 33,523 381,767 Movie 2,828,585 33,561 87,081 Music 1,387,216 29,287 257,288 All 5,312,949 36,673 726,136
Besides rating information, we crawled each item X  X  textual metadata such as the summary and tags assigned by users for topic modeling. For our approach, we keep the items that enter the system before 2011 in offline storage module (see Fig. 1), and treat the items that enter the system in 2011 as continuously arrived data which will be handled by online component. The online recommendation model is updated when every 1000 new items enter the system and the offline recommendation model is updated when every 10,000 new items are merged from online storage module to offline stor-age module (i.e., the online model is updated 10 times more frequently than the offline model). Note that the ratings submitted in 2011 will be used as test data, and the ones submitted before 2011 will be used as training data. Since top-N recommendation is provided by combining online rec-ommendation and offline recommendation, so the two com-ponents cannot be evaluated separately. Besides Douban data, we also use the Delicious bookmarks data [6]. In this dataset, 1,867 users bookmarked 69,226 URLs with 53,388 tags (for topic modeling). In the experiments, the purpose of a recommendation model is to recommend URLs that are likely to be bookmarked by users (i.e., binary rating is used). Note that due to space limitation, Delicious dataset is only used in comparison study in Section 4.2.2.
The number of users X  most recently rated items that are kept in online component is a design parameter, which can be configured according to characteristics of the system or by cross validation. We set this number to 1 in our experiments.
We compare our approach with several representative base-lines: (1) BasicMF . This is the basic matrix factorization model. Top-N recommendation is provided by sorting the candidate items in descending order of the predicted ratings. The dimensionality of factors, regularization parameter and learning rate are set to 10, 0.1 and 0.01 respectively (by cross validation). (2) RMFX [10]. This approach uses a (hinge loss based) pairwise approach with matrix factoriza-tion to optimize the ranking for social streams like Twitter. When processing continuously arrived items, it applies ran-dom sampling with a reservoir to sample user-item pairs for model update. The dimensionality of factors, regularization parameter, learning rate, and reservoir size are set to 10, 0.1, 0.1 and 100 respectively (by cross validation). (3) CLiMF [21]. This is a ranking-oriented algorithm that directly op-timize a ranking measure Mean Reciprocal Rank (MRR). The dimensionality of factors, regularization parameter and learning rate are set to 10, 0.001 and 0.001 respectively (by cross validation). The implementation of this model is publicly available at http://dmirlab.tudelft.nl/users/yue-shi. (4) timeSVD ++. This is a matrix factorization model incorporating temporal dynamics proposed in [13]. User bias, item bias and user latent factors are modeled as func-tions of time. For each of these approaches, we use the ratings submitted from 2007 to 2010 as the training data to train the model and validate its performance using ratings submitted in 2011. To obtain time-aware recommendation, we updated these models (except RMFX) when every 5000 new items enter the system. 5000 is chosen as the compro-mise of the online and offline components of our approach (see previous paragraph for the settings of our approach). For RMFX, the model is updated when every 1000 items enter the system. This is to compare with our approach X  X  online component that is particularly designed to handle fresh items. Experiments are conducted 10 times and the averaged results are presented for each approach. Note that error bars are omitted in figures to avoid cluttering but in Tab. 2, we provide 95% confidence interval (all comparison results are statistically significant).

We measure the performance of recommendation models using Mean Average Precision (MAP) and Normalized Dis-counted Cumulative Gain (NDCG), two well-known metrics for measuring the performance of ranking algorithms.
As argued in Section 3.2.1, the number of topics in LDA should not be tied to the dimensionality of latent factors in matrix factorization such that textual contents can be more flexibly incorporated to model interactions between users and items. We thus first demonstrate the performance of our approach when different number of topics is set. Note that the dimensionality of latent factors, regularization parame-ter and learning rate are set to 5, 0.001 and 0.00001 which are optimized by cross validation. From Fig. 2 we observe the performance of our approach varies a lot with different number of topics. For every type of item (i.e., book, movie and music), the general trends are both NDCG and MAP first increase, when arriving at a certain point, they start de-creasing with the increasing number of topics. Specifically, for book data and movie data, 20 topics achieve the highest NDCG and MAP, while for music data, the optimal number of topics is 10. This observation supports our claim that topic number should not be tied to the dimensionality of la-tent factors (i.e., 5 in this case), and a suitable topic number significantly improves the performance of our approach.
We next validate the effectiveness of combining LDA and matrix factorization for modeling users X  preference on items for ranking. Fig. 3 shows the comparison results (with dif-ferent recommendation list size) of our approach and a vari-ant where textual contents are not considered. It is clear that when both textual contents and rating information are used, NDCG and MAP are significantly improved. This is because by deriving topic distribution for each item, users X  preference can be more accurately modeled, especially when the target user and/or item X  X  ratings are sparse (i.e., a single piece of textual content can reveal many of an item X  X  char-acteristics thus is more informative than a single rating).
Recall that one contribution of our work is to cluster a user X  X  rated items based on their topic distributions such that the training data (i.e., item pairs for optimizing pair-wise loss function) is more compact and more meaningful. We next demonstrate the effectiveness of such item cluster-ing. From Fig. 4 we notice that in all cases, considering topic based clustering evidently improves the performance of a variant of our approach where no clustering is con-ducted. This demonstrates that removing unnecessary item
Our approach 0.0799
BasicMF 0.0266
RMFX 0.0525
CLiMF 0.0585 timeSVD++ 0.0412 pairs indeed refines the training data thus improving ranking measures. On average, such a clustering strategy improves the performance of our approach in terms of NDCG/MAP, by 2.99%/4.89%, 5.37%/9.02% and 7.99%/5.92% for book data, movie data and music data respectively.
We compare the performance of our approach with that of the state-of-the-art approaches (see Section 4.1). Note that Figure 5: Performance comparison (Douban data). Figure 6: Performance comparison (Delicious data). for our approach, optimal topic number is applied for differ-ent type of items. Table 2 summarizes NDCG and MAP for all recommendation approaches when top-10 recommenda-tion is provided. Not surprisingly, BasicMF, which simply predicts a user X  X  rating to an item for ranking, produces the lowest NDCG and MAP. This shows that optimizing rat-ing prediction does not necessarily generates good recom-mendation ranking, in particular, when rating information is sparse. As a ranking-oriented recommendation approach, CLiMF significantly outperforms BasicMF. This proves the effectiveness of learning to rank for top-N recommendation. By optimizing a ranking measure, CLiMF directly gener-ates a recommendation list instead of focusing on improv-ing rating prediction accuracy. Similarly, RMFX, which ap-plies pairwise learning to rank also outperforms BaseMF. However, to better handle scalability issue, RMFX applies sampling techniques to process the incoming items, which inevitably lose information (and hence, the accuracy). This makes RMFX slightly less accurate than CLiMF. timeSVD++, although is not particularly designed for ranking, by incor-porating temporal effects into matrix factorization (via time-aware user/item bias and latent factors), still generates rea-sonable results: for music data, timeSVD++ is even better than CLiMF and RMFX. In all cases, our model consistently outperforms other methods due to four key designs: (1) learning to rank is applied to directly optimize item ranking instead of rating prediction; (2) textual contents are incor-porated (by fusing LDA and matrix factorization) to better model users X  preference; (3) topic distribution based item clustering refines the training data to more efficiently fit the model; (4) new items and longstanding items are modeled separately for providing personalized recommendation based on individual users X  preference on the freshness of items.
We also compare the performance of all recommendation approaches when recommendation list size varies. Fig. 5(a) and 5(b) summarize NDCG and MAP for each recommen-dation approach when top-5, 10, 15, 20, 30 recommenda-tions are provided. Note that we mix book data, movie data and music data to conduct comprehensive comparison study. Similar to previous comparison results, our approach generates the highest NDCG and MAP. In summary, by av-eraging performance when different recommendation list size is applied, our approach improves BasicMF, RMFX, CLiMF and timeSVD++ by 168.98%, 72.40%, 50.27% and 87.64% in terms of NDCG, 96.73%, 72.83%, 61.21% and 75.95% in terms of MAP. The comparison results using Delicious data (see Fig. 6) demonstrate the similar trends.
In this paper, we present a dynamic recommendation frame-work that consists of an online component that quickly pro-cesses the continuously arrived items and an offline com-ponent that captures the influence of longstanding items. We apply learning to rank to optimize a pairwise loss func-tion. LDA is integrated to combine rating information and textual contents to better model users X  preference on items. Furthermore, topic distribution based item clustering refines training data. A heuristic algorithm is proposed to combine online and offline recommendations based on users X  prefer-ence on the freshness of items. Two real datasets based ex-periments demonstrate that our approach significantly out-performs the state-of-the-art models.

As for the future work, we intend to explore more so-phisticated mechanisms to derive users X  preference on the freshness of items for recommendation. Another direction is to investigate to further reduce computational complexity to better handle scalability issue (e.g., by sampling). This work was partially supported by the grant Reconcile: Robust Online Credibility Evaluation of Web Content from Switzerland through the Swiss Contribution to the enlarged European Union. [1] D. Agarwal and B.-C. Chen. flda: matrix factorization [2] D. Agarwal, B.-C. Chen, and P. Elango. Fast online [3] M. Aly, S. Pandey, V. Josifovski, and K. Punera. [4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [5] C. Burges, T. Shaked, E. Renshaw, A. Lazier, [6] I. Cantador, P. Brusilovsky, and T. Kuflik. 2nd [7] K. Chen, T. Chen, G. Zheng, O. Jin, E. Yao, and [8] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, [9] E. Diaz-Aviles, L. Drumond, L. Schmidt-Thieme, and [10] E. Diaz-Aviles, L. Drumond, L. Schmidt-Thieme, and [11] R. Gemulla, E. Nijkamp, P. J. Haas, and Y. Sismanis. [12] S. Kabbur, X. Ning, and G. Karypis. Fism: factored [13] Y. Koren. Collaborative filtering with temporal [14] N. Lathia, S. Hailes, and L. Capra. Temporal [15] N. N. Liu, M. Zhao, E. Xiang, and Q. Yang. Online [16] X. Liu and K. Aberer. Soco: a social network aided [17] J. McAuley and J. Leskovec. Hidden factors and [18] S. Purushotham and Y. Liu. Collaborative topic [19] Y. Shi, A. Karatzoglou, L. Baltrunas, M. Larson, and [20] Y. Shi, A. Karatzoglou, L. Baltrunas, M. Larson, [21] Y. Shi, A. Karatzoglou, L. Baltrunas, M. Larson, [22] C. Wang and D. M. Blei. Collaborative topic modeling [23] F. Wang, W. Pan, and L. Chen. Recommendation for [24] M. Weimer, A. Karatzoglou, Q. Le, and A. Smola. [25] L. Xiong, X. Chen, T.-K. Huang, J. Schneider, and [26] E. Zhong, W. Fan, J. Wang, L. Xiao, and Y. Li.
