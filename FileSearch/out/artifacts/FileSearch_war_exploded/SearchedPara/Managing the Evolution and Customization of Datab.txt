 Information systems play an important role in the management of organizations. But in order to be effective, they often need to cover a wide range of functionality to support all relevant user roles effectively. For exa mple an information system for a university needs to address personnel and finances, but also lectures, exams, students, etc. Today software ecosystems are an important topic due to their business implications. Often in an ecosystem, a core development o rganization provides a platform, which third party vendors extend with additional f unctionality. This allows customers to inte-grate features from different vendors into their systems to create a product that fits their needs [9]. This concept can be applied to software product lines [2] and information system product lines [4].

In order to allow such extensions, features need to be encapsulated into, what we call, feature packs, which may consist of both an i mplementation and a partial data-model. When feature packs are installed, updated and removed by customers, the database structure and content must be adjusted acco rdingly. In this paper, we will describe an approach that supports this and evaluate it based on a number of case studies.
Our approach is based on a relational database and object/relational mapping. While our case study is an information system in the university management domain and im-plemented in Java, our approach is not specific to any particular programming language or domain. It can be used in other information system ecosystems.

The remainder of this paper is structured as follows: Section 2 describes the case study context and Section 3 defines the general problem. In Section 4 we describe our solution approach, which we evaluate in Section 5. In Section 6 related work is dis-cussed before we conclude the paper in Section 7.
 Our research is motivated by and based on experiences with a large-scale information system development. The approach we discuss in this article was driven by needs from this context and integrates parts that are al ready in use for several years as well as parts that have been created to address recent and upcoming needs. In this section, we will describe the case study context. In particular, we will introduce some features, which we will use as examples throughout this paper. 2.1 Information System HISinOne The HISinOne-system is the basis for our case study. We describe it and its evolution in this section. The producer Hochschul-Informations-System GmbH (HIS) is market leader in Germany for university management systems for 40 years. The HISinOne-product covers many areas that are relevant to universities like management of students, personnel, or lectures. It is a large information system under active development. Table 1 shows its growth in terms of lines of code, dat abase tables and columns, and foreign-key constraints over recent years. HISinOne us es O/R-mapping with rather small objects. This results in small tables with an averag e of less than eight columns per table.
HISinOne must be highly customizable due to diverse requirements originating from a number of sources. Laws regarding universities are defined by each of the 16 federal states in Germany. There are different sizes and categories of universities (e.g., univer-sities focusing on research, universities of applied science, and universities of sports or music). Further, even in a single university there exists diverse regulations, e.g., con-cerning examinations, that the system must handle simultaneously. An approach for handling the system configuration in this context was described in [4]. Customizations range from small modifications like fitting the look-and-feel to the corporate design over business processes adaptations to replacing complete functional areas like lecture management with products from other vendors. Figure 1 compares two different HISi-nOne Publication Management products, which differ in the number of features and in customizations of the user interface. These variations can be very significant as a previous study [11] showed.

HISinOne is at the core of its own information system ecosystem as customers and third-party vendors develop their own customizations and extensions as feature-packs. These extensions may consist of both program logic and additions to the database. Cus-tomers may install feature-packs by themselves at any time without support by HIS.
HIS releases about two major versions pe r year and additional service packs as required. Customers, however, face the problem that it is difficult to identify a good time-frame for updating the complete installation as at any time some part is under intensive use. Thus, it is desirable to support the evolution of individual feature packs without ramifications for other parts of the installation. Possible issues, which may arise in update situations, are described in [3]. 2.2 Features Used as Examples in This Paper In this section, we will introduce the research management domain of HISinOne and relevant feature packs as a reference example for the rest of this paper.

The Research Management domain consists of features for the management of re-search projects and publications as shown in Figure 2. The core Publication Manage-ment may be extended with the following features: Research Output Measurement (F1): Research projects document results by creat-Proceedings Publication Type (F2): Support for the additional publication types Pro-Publication Quality Assessment (F3): The possibility to assess the quality of publi-While these three features may be developed independently (even by third party ven-dors or customer), they interact with each ot her at runtime: end users expect a single dialog for publications, where they can enter all relevant data, without considering the structure of the system. Also, the additional publication types provided by the proceed-ings publication feature impact the behavior of the quality assessment and the output measurement features.

The ecosystem and evolution requirements of our context result in challenges that are relevant to other information system ecosystems, too. We discuss them below. While our research is motivated by problem s, which occur in practice in a specific com-pany as described in Section 2, the issues are of broader relevance to information system ecosystems. We distinguish two problem areas: in an ecosystem a global data schema can no longer be enforced, rather it needs to be broken down into feature-specific frag-ments. Further, when partial updates are done the database must be appropriately up-dated without a negative impact on existing data.
 The identified problems areas lead to the following research questions: RQ1 How can separate organization-wide data-models in small-scale feature-specific RQ2 How can features, developed by customers and third party vendors, be integrated RQ3 How can the database be modified at the cust omer site to fulfill the requirements The following constraints on our approach play an important role in our context: C1 It must scale to large systems with about 800 tables and tens of thousands of users. C2 It must not require a global c oordination because feature packs are developed by C3 It should be easy to use for developers at all participating organizations. This im-C4 It must be possible to modify attributes originating from multiple different feature Based on the requirements and constraints, which we defined in the previous section, we will now discuss the approach, that we developed to address these problems. While our focus is on the general approach, we will also illustrate its specific application in the case study. We will first discuss how the customizability impacts the object-model, then we will describe how this leads to the management of variability on the data-model and database level. Finally, we will discuss the life-cycle of a feature pack in an installation and the corresponding database adaptation.
 4.1 Object Model Design for Feature Packs In this section, we discuss our approach to manage the object-model as a basis for sup-porting the development of features by multip le, independent organi zations without full synchronization. Our approach to object model management indirectly also provides the basis for the customization of the database schema based on O/R-mapping as we will discuss in Section 4.2.

We use object oriented design (OOD) patterns to implement features. This is one of the common approaches to impl ement software product lin es according to [6]. Unlike conditional compilation and aspect oriented programming, OOD enables the definition of stable interfaces. This is important because the final product is assembled by the customer based on fragments provided by independent organizations.

Delegation based design patterns allow multiple features to enrich model-objects with additional attributes without having to be aware of other enrichments. For example the Project Output Measurement and the Publication Quality Assessment both enrich the Publication-entity independently of each other. Users may still access both features together to edit a Publication-object because delegation is used instead of inheritance.
Figure 3 shows a class diagram of these enrichments. The Publication-entity is en-riched independently by the Project Output Measurement on the left side and the Pub-lication Quality Assessment on the right side.

As a consequence of this implementation approach, there is no need at the database layer to support adding of columns to tables from another feature pack. The additional classes are mapped to new tables instead. 4.2 Managing the Data-Model Data modeling aims at developing a consistent database scheme that fulfills the domain requirements without introducin g redundancies. Ideally, this is achieved by enforcing a global data-model. In our case, this is not possible, as we need to support distributed development of integratable data-models without global coordination, as required by the nature of software ecosystems. As platform providers HIS often never learns about certain extensions of the data-model created by customers or third-party suppliers. Locally Coordinated Development of Data-Models: (Partial) database models are created by the core development organization, several third party vendors and cus-tomers. Although they are not coordinated globally except for the assignment of names-paces, they are typically coordinated on a pe r-organization-basis. For example, HIS consolidates all data for all its features in a single database scheme. While this inter-nal coordination is not necessary for our approach, it is useful in practice, as it reduces redundancies, ensures naming conventions and prevents conflicting designs.
 Splitting the Model: If organization-wide data-models are used, they are split into feature-related parts. Database tables are linked to their model classes by object/relation mapping definitions. As the model classes are part of exactly one feature pack, it is defined which database table belongs to wh ich feature pack. Thus the variability de-pendency management implies relations among the database tables and supports their consistent combination.
This mapping strategy requires that data is not stored in large monolithic tables, but tables are split up. Corresponding model classes, which only contain the relevant fields, must be introduced. (For example instead of having one huge table with many columns for all student data, several small tables such as person , address , student information , library id , fee credit are used). Features, that must access this information, need to mark the corresponding feature packs (and hence database tables) as required.

We usually attach foreign-key constraints to the tables they are defined on. But in some cases, a feature pack may introduce a constraint on a table from another feature pack. In such cases the definitions must be assigned to the feature pack explicitly. Views can also be attached to the first tables, in th e same way, and they too may be assigned to a different feature pack explicitly. While views are not commonly used in applications, which are based on object/relational mapping, some customers like to use them as an easy means to extract information from the database without writing program code. Example of the Global View on a Partially Coordinated Model: Figure 4 illustrates our approach based on the example from Section 2.2. The development organization maintains an organization-wide database model, which is shown on the left side of the figure.

The features Publication Management , Project Output Measurement (F1) and Re-search Project Management are managed by the development organization. The cor-responding data is stored in the tables author , publication , publication project and re-search project and maintained by the development organization. The foreign-key rela-tion between author and publication belongs to the feature Publication Management . The feature F1 contains the foreign-keys from publication project to publication and research project .

A third party vendor develops the feature Publication Quality Assessment (F3) , which contains the two tables publication qa and quality value .Thisisshowntothe right of Figure 4. F3 also provides a pair of forei gn-key constraints between publica-tion qa and publication in both directions to ensure an one-to-one relationship. Data Model Format: The data-models and partial dat a-models are described using a subset of Structured Query Language (SQL). Thus common modeling tools can be used as SQL is widely supported.

The subset consists of create statements for tables, views and indexes. For con-straints the syntax in SQL starts with alter table add constraint . All other alter table commands are explicitly disallowed. Triggers and stored procedures are not used in our context, but may be defined using create statements.

During update or installation of a feature pack, the SQL-statements are not executed, but analyzed as a description of the desire d reference model. The necessary actions to alter the structure of the existing database to the desired model are derived from a comparison of both definitions, as we will discuss in Section 4.4. This process also takes vendor-specific incompatib ilities of SQL dialects into account by creating appropriate SQL-statements internally. 4.3 Feature Lifecycle Support Over time feature packs may be added, removed or updated in a given installation, which may result in an adaptation of the data-model. Figure 5 shows the states and transitions of the feature life cycle in an installation. We will discuss the transitions in the context of the three processes of adding, removing and updating a feature pack. Adding a New Feature Pack: Initially the new feature pack does not exist in the instal-lation. The feature pack is uploaded by an administrator. This step makes the feature implementation and data-model definitions available to the system. During the install step, the partial data-model provided by the feature pack is integrat ed into the database. We will discuss this process in detail in Sec tion 4.4. After the feature pack was installed, it will be activated , which allows people to use it.
 Some features require data, that can only be provided by domain experts manually. Even when these features are in state active, they can only be used in a limited fashion until the necessary data is provided. For example the quality assessment of publica-tions is not visible to the general public until the domain experts have assessed the publications. This half activat ed state is managed by the feat ures themselves, similar to common domain-inspired situations in which some information is hidden from certain users. Removing a Feature Pack: Feature packs in any state may be removed from an instal-lation, which will result in the following transitions.
 A feature pack, which is in state active, is deactivated , resulting in the installed state. This transition does not influence the database and is therefore suitable for temporarily hiding a feature. An installed feature pack must be de-installed to adapt the database to the structure without the data-model part of the de-installed feature pack. This re-sults for the feature pack being in state available. An available feature pack may be deleted without any further impact on the datab ase, because the data and the structure has already been removed on de-installing the feature pack.

Mass deletion of data is a very dangerous action as it cannot be undone easily. Thus, it is desirable to be able to perform a preserving de-install , which keeps the data of a feature. This preserved data, however, will decay over time without constraints and without the program logic t o maintain it. But the ability to undo a de-installation by uploading the feature pack agai n is highly desirable, because the alternative of merging the current database with a version restored from a backup, is a highly complex task.
After it is ensured, that the preserved data will not be used anymore, it may be purged . Or the feature pack may be uploaded to make it available again.
 Updating a Feature Pack: An update of an available, installed or active feature pack is done by uploading a newer version of the f eature pack, which replaces the old one and makes the newer version available in the system. The transitions from available through installed to active are exactly the same as i n the process of adding a new feature pack. The main difference is that the old feature pack is not removed prior to integrating the new one. But the combined effect is determined and only those changes are applied to the database as part of the install-transition. The details of the activities leading to the adaptation of the data-model are described in the following section. 4.4 Customizing the Database The integration of the partial database models provided by the available features into the actual database is a process that consists of four phases: Combine partial data-models , initial structure adjustment , data modification and final structure adjustment . We will discuss these phases below, describing initial and final structure adjustment together. The database adjustment process is started by an administrator. The execution of the four phases is a fully automated process.
 Combining the Partial Data-Models: In the first phase of the database adaptation, the desired data-model for the complete datab ase is derived based on a combination of the partial data-models of feature packs in state data preserved and later. This combination is given as the set union of all tables, columns, constraints, and views defined by any of the partial data-models. A naming conflict, which can be the result of an element be-ing renamed or independent introduction of elements with the same name, is prevented by the coordination within an organization a nd namespaces across organizational bor-ders. As each column and table is defined by exactly one feature pack as discussed in Section 4.2, there is no redundancy that needs to be addressed on this level. As far as necessary, this is handled on the variability model level.

The initial version of the combined data-m odel is not necessarily consistent. It may contain foreign-key constraints for which the source or target table is not part of the combined model or views using non-existing tables. These dangling constraints and views are the result of unfulfilled optional dependencies on the feature pack level. This is illustrated by our example, where we implemented the Research Output Measurement as a new feature pack, which depends on the f eature packs Publica tion Management and Research Project Management. Alternatively the functionality could be implemented as part of Research Project Management with an optional dependency on Publication Management. Here, the Research Project Ma nagement would introduce a foreign-key constraint on Publication Management. However, this constraint would be eliminated in installations where Publicatio n Management would not be present.

For feature packs, which are in the state data preserved , only the information regard-ing required tables and columns of the database is included. Information on constraints are ignored, as the implementation, that would be observed is not part of the running system. For example, there might be a constraint enforcing that every publication is related to a project, but if Research Project Management would be in the state data pre-served , this could no longer be managed by the system, thus the constraint may not be enforced any longer.
 Structure Adjustment: The structure (tables, columns, constraints, indexes and views) of the actual database is adjusted to meet th e desired combined data-model. The struc-ture of existing data may change, for example a column may be moved into a new table. The data modification phase needs to have access to both the old data and the new target structure. This requires the extension of the existing database structure with new ele-ments during an initial structure adjustment phase before the data modification phase. The final structure adjustment narrows the database structure down to fit the desired data-model.

The adjustment is based on the results of a c omparison between the actual database structure and the desired model for all element types except views. Table 2 shows all possible situations.

An element may be absent in the actual datab ase structure but present in the desired one, which means an element is new. The opposite situation occurs when an element exists in the database, but not in the desired model. Finally an element may be present in both models. In this case both definitions of the element have to be compared in detail. For tables this means that the order of columns may need to be adjusted. For columns, changes of datatypes must be applied. Indexes and constraints are identified by their complete definition. So a modification of such an element type results in the treatment as a deletion and a creation. This is usef ul because common database systems do not allow the modification of constraints and indexes without their recreation.
For simplicity all views are dropped in the beginning and recreated in the end, be-cause views prevent modifications of the underlying tables in all common database systems. Since views do not contain any data directly and their creation and removal is very fast, this is a simple and effective approach.

In the initial structure adjustment phase additional tables, columns and non-unique indexes increase the possibilities of the data-model and are therefore created as shown in Table 2 a). Constraints and unique indexes, however, limit the possibilities of the data-model, therefore outdated ones are deleted as shown in Table 2 b). The order of columns will be fixed. If the datatype of a column does not match and the target type is wider, it will be adjusted.

In the final structure adjustment phase, deleted tables, columns and non-unique indexes are removed. Remaining datatype changes in columns are performed. All views, new constraints, and new unique indexes are created.

The creation of some new constraints may not be possible because existing data may not comply. In most cases default values will be set in the data modification phase before the constraints are defined. But sometimes expert knowledge is required to determine adequate data. In this case the constraints cannot be enforced during the installation of the feature pack, but must be added at a later time. The affected feature packs must pro-vide functionality for domain experts to enter the necessary information, while disabling functionality, which would require this data. When all data conforms to the constraint, the database adjustment process may be rest arted and will add the missing constraints. Data Modification: In the third phase of the database customization, the data is con-verted to fit the new structure and requirements. The data modification is defined as a series of actions, which rely on a set of standardized, common patterns, which were identified based on experience at HIS over r oughly a decade. They are listed in Table 3.
As special cases may arise, that cannot be addressed by the common standard pat-terns, the list also includes two actions which are rather generic. The first allows arbi-trary SQL-statements, while the second even allows arbitrary program code. However, using the patterns for common cases is preferable, as it improves maintainability. Fur-thermore it abstracts from incompatible SQL-dialects used by different vendors, for example for multi table update statements.
 The actions may have preconditions like the existence of a specific column or table. For example renaming of a column includes a Copy column step, that is executed, if both the old column and the new column exist during the data modification phase. The new column was created by the initial structure adjustment and the old column will be deleted by the final structure adjustment. In this section we will discuss the evaluation of our approach. We will first describe the different evaluation activities, we performed, and then discuss our findings in relation to the research questions from Section 3. 5.1 Evaluation Activities We evaluated our approach in several ways, as we will describe in this section. Multiple evaluation activities may relate to the same contribution of our approach, as we will describe below. We start our description of the activities with the most recent effort. E1: Over the last half year, we (the first two au thors of this paper) developed a proto-type to evaluate the possibility of partitioning the data model into fine-grained, feature-specific data models by decomposing the corresponding object structure. This prototype was implemented in the context of a current feature development at HIS: the Research Management domain. It resulted in the decomposition of the features as described in Section 2.2 while investing an effort of about two weeks of work. The evaluation was used on the one hand to understand better any (technical) limits on choosing fine-grained features and on the other hand as basis for discussions on the approach with developers, the software architects and product managers.
 E2: A fellow developer created a feature to man age test-case descriptions with an effort of about half a year distributed over the course of a year. The goal of the implementation was to show that combining data-models works in practice for large partial data-models without coordination. The feature has been in production internally for over a year, and was updated several times. With fifteen database tables, it is significantly larger than the prototype features.
 E3: We gathered the experience, HIS gained i n evolution and ecosystems with infor-mation systems in production over the last ten years, by talking to customers, support, product managers and fellow developers. According to [13] gathering expert opinion is the state of the art approach to estimate maintainability in database based applications. We looked especially at the database update process and the combination of multiple, partial data-models. Further, we wanted to learn about any existing issues. E4: We analyzed HISinOne to estimate the performance impact of the decomposition approach. HISinOne is a six year old University Management System, which is cur-rently in production by 53 customers. We collected and calculated metrics on the data-base structure as shown in Table 1. We meas ured the performance of the database ad-justment tool and looked for indications of a possible performance impact by having many small tables. 5.2 Separating Data-Models The decomposition of organization-wide data-models into small, feature-specific mod-els and the management on this level (RQ1) was evaluated by E1, E2 and E4.
 Observation and Discussion: A possible problem with using delegation to extend model objects from other feature packs (cf. S ection 4.1) is the increased complexity. E2 showed that this is not significant while E1 confirms this for small feature packs. We found no negative impact on performance by using many small tables based on E4. The core objects of HISinOne (e.g., applications, students, exams) have 7 to 15 columns with an average of 7.8 columns per table. At a university with about 40 000 students, the database contains an overall sum of 17 284 995 records for the student management domain. The largest ten tab les cover a range of about 180 000 to about 6 million records 1 . Typical installations in such a context use 2 to 20 application servers to handle the load during peak times, while only one database server is sufficient. Since the existing modeling is already based on small t ables without a performance bottleneck, we expect no problems from using smal l tables motivated by feature packs.
 Limits: The organization-wide coordination of data-models may be seen as a limit. In discussions E1 and E2 with developers, they welcomed a coordination across features and domains as it simplifies maintenance and prevents redundancies. But, our approach does not depend on organization-wide harm onization as feature-specific data-models would be sufficient.
 Threats to Validity: E1 was implemented by the first two authors of this paper, but it was extensively discussed with other developers and E2 was created by a different developer, reducing threats to external validity. E2 and E4 concern systems that have been in production for more than a year, respectively for many years, thus we don X  X  see any threats. In particular, based on the ecosystem experience we do not assume significant issues in terms of external validity. 5.3 Combining Partial Data-Models We evaluated the process of combining of partial data-models in production environ-ments at our customers (E3) and looked at the performance impact (E4).
 Observation and Discussion: Combining partial data-models from multiple sources in concrete installations, as discussed in Sec tion 4.4, works fine according to our experi-ence with E3. It was implemented three y earsagoandisusedbycustomersandthird party vendors in production since then.

The performance impact of the composition algorithm is negligible in practice. It takes less than 50 milliseconds to combine the data-models in the installation we looked at. The algorithm uses a simple set union for all elements, and then iterates over all views and foreign-key constraints doing a hash-set-lookup for elements, they depend on, to clean up the model. As this scales with O ( nlog ( n )) , we conclude that the perfor-mance impact even for a large amount of feature packs is still acceptable.
 Limits: In order to prevent conflicts across organizations, our approach requires the as-signment of namespaces for tab les, columns, indexes and constraints. The management of dependencies among partial data models n eeds to be handled with another technique, such as admin guide lines or feature pack dependencies.
 Threats to Validity: We did not actually test a large number of feature packs, but so far our experience is very positive that the performance only depends on the number of el-ements and is negligible in practice. This is supported by our analysis of the complexity of the composition algorithm. 5.4 Database Adjustment We evaluated whether the database adjustment fulfills the requirements of installing and updating feature packs in installations (RQ3).
 Observation and Discussion: The database adjustment p rocess from Section 4.4 is in use for ten years with only minor improvements since then (E3). The data modification patterns listed in Table 3 evolved over time, but are stable since four years. The tool is used about 200 times per year in production systems by 90 customers for two different, large-scale information systems. It is also used several hundred times in development and for prototype testing by customers. It is quality tested on Informix and PostgreSQL, but implementations for other database systems exist as well.

Compared with the previous approach the number of failed updates, especially for prototypes and hot -fixes, was reduced significantly by the new approach, according to customer support (E3).
 Lesson Learned: While we learned some lessons since the database adjustment tool was first used in production in 2002, they ha ve not required fundamental changes to the process: Support for views was added about five years ago, due to customer interest. Deletion of tables and column s is recently very rare, but occurred more often during the initial development of HISinOne about 4 years ago.
 Limits: Triggers and stored procedures are not used by HIS. If the same database struc-tures are changed repeatably and customers skip versions, data modification rules may need to be modified. We have not automated thi s, yet, because this situation is not rele-vant in practice according to E3.
 Threats to Validity: We do not assume significant threats as this approach is used extensively in production. Our approach allows the evolution of the database in an information system ecosystem, which is little covered by research yet.

In [7] the author discusses a set of migration patterns, which is suitable for the mi-gration from an older system to a newer system and is similar to the data-modification-patterns explained in Section 4.4, but without tailoring the database structure. An ap-proach for refactoring existin g database structures based on defect patterns is discussed in [8]. This approach especially aims at impr oving the database structure by refactoring, but the existing semantics of the model is kept.

A customizable database schema evolution a pproach for object-oriented databases is discussed in [12]. It offers support for both schema evolution and adjustment of existing data, but the tailoring of the schemes is not covered.

Several approaches for tailoring of database schema exist. In [14, 17] the use of database views for tailoring is proposed, yet no support for evolution of the scheme is offered. The use of a global database model, which is pruned to the actual needs, is dis-cussed in [1]. Another approach based on a global database model, whose components like columns are overloaded for customizing it, is discussed in [18], but evolution is not covered by this approach. In [10] Mahnke proposes the use of component-based data-base schema development, but does not cover evolution of the schema or of the data. Dyreson and Florez discuss a specialized approach for adding cross-cutting concerns to data-models with aspects [5]. Yet, it does not cover evolution and is focused on those cross-cutting concerns. Sabetzadeh et al. describe an approach for the composition of models from parts with the goal to enable global consistency checking, which does only cover models and does not consider evolution [15]. Finally Schaeler et al. split a global database model into parts with the help of SQL analysis and a manual clean up. The approach does not consider the evolution of the models and the corresponding data.
All related work has in common that the approaches do not origin from an ecosystem context. The ecosystem perspective seems to be mostly unique to our approach. In this paper we described an approach to database management and evolution, which supports customization and (partial) evolution of the database schema. This approach relies on core functionality that has been pr oduction use for many years, with nearly a hundred different customers. However, m ore recently, we extended it with capabilities that support the more fine-grained evolution and customization. Moreover, as opposed to other, related approaches, our approach addresses software ecosystems and works well in this context. We evaluated our approach using several independent evaluation approaches and discussed the conclusions we can draw from them.

In the future, we are going to further improve feature pack support on the application level.

In particular, we envision the integration of an explicit dependency management sup-port, which is so far not yet supported. We plan to achieve this by integrating our ap-proach with a corresponding dependency management approach that we described ear-lier [4]. It relies on the idea of a distributed variability model, as we discussed in [16].
