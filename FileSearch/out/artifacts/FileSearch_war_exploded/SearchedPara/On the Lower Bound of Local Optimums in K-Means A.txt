
The k -means algorithm is a popular clustering method used in many different fields of computer science, such as data mining, machine learning and information retrieval. However, the k -means algorithm is very likely to converge to some local optimum which is much worse than the desired global optimal solution. To overcome this problem, current k -means algorithm and its variants usually run many times with different initial centers to avoid being trapped in local optimums that are of unacceptable quality. In this paper, we propose an efficient method to compute a lower bound on the cost of the local optimum from the current center set. After every k -means iteration, k -means algorithm can halt the procedure if the lower bound of the cost at the future lo-cal optimum is worse than the best solution that has already been computed so far. Although such a lower bound com-putation incurs some extra time consumption in the itera-tions, extensive experiments on both synthetic and real data sets show that this method can greatly prune the unneces-sary iterations and improve the efficiency of the algorithm in most of the data sets, especially with high dimensionality and large k .
The k -means algorithm is a popular clustering method used in many different fields of computer science, such as data mining, machine learning and information retrieval. Given a data set P ,the k -means algorithm, also known as Lloyd X  X  algorithm [13], tries to find k centers in the space minimizing the cost , which is the sum of the squared Eu-clidean distance from every point in P to its nearest cen-ter. At the beginning of the algorithm, k random centers are chosen from the original data set. Then the algorithm keeps invoking k -means iterations. Every k -means iteration con-sists of two operations. First, every point in the data set is assigned to the nearest center. Second, points are divided into k groups according to the nearest center in the previ-ous step and the geometric centers of all groups form a new set of centers. This procedure continues until the centers do not move any more. The k -means algorithm is accepted and used in many different applications because of its simplicity and efficiency.

However, there are two main problems for k -means al-gorithm. First, in each iteration, much computation time is spent on assigning every point in the data set to its new nearest center. Second, the algorithm is easy to be trapped in some local optimum, which can be much worse than the global optimum. For the first problem, there have been sev-eral works on accelerating the nearest center search proce-dure based on triangle inequality [6] or indexing structures [10, 15], which can work well in low dimensional space. Compared with the first problem, the second problem has not been well addressed yet. Although there are some stud-ies on the choices of initial centers [3, 14] to avoid those local optimums, these methods do not show too much ad-vantage over the simple random selection in the data set.
To address the second problem as mentioned above, we propose a novel computation method on the lower bound of local optimums in the multi-procedure k -means algorithm, where every procedure is a running of k -means algorithm with a unique initial center set. With the current center set of a procedure, the method proposed in this paper is able to find out a lower bound on the cost of the local opti-mum, where the algorithm will converge to. As solution with smaller cost is favored, the current procedure can be terminated if such a lower bound is greater than the mini-mum cost achieved by other procedures. The computation time on the further iterations to reach those worse local op-timums can thus be saved. Here, we simply assume but not limit to the assumption that all procedures are run one by one on a single machine.

In order to lower bound the cost at the local optimum, our algorithm works in two steps. In the first step, it tries to find a maximal region inside which the local optimum will defi-nitely fall in. Despite that the general maximal region for a center set is hard to compute, we propose a special type of maximal region which is looser but easy to calculate. In the Algorithm 1 Original k -means algorithm (data set P , k ) 1: Randomly choose k points as the center set M 2: while M is not still do 3: M = SimpleIterate ( P, M ) //See Algorithm 2 // 4: end while 5: Return M second step, it calculates the lower bound on local optimum by estimating how the movements of the centers can affect the overall cost. Although the searching of such maximal region will result in extra computation time consumption, the benefit from early pruning by the lower bound can sig-nificantly speed up the whole k -means algorithm. This ef-fect can be verified by our extensive experiments on both synthetic and real data sets.

The rest of the paper is organized as follows. Section 2 defines the problem and discusses some intuition. Sec-tion 3 proposes the concept of maximal region and derive the lower bound based on a special type of maximal re-gion. Section 4 presents the new iteration algorithm and the new accelerated k -means algorithm by exploiting the lower bound on local optimums. Section 5 gives some experi-mental results, and Section 6 reviews some related work. Finally, we conclude this paper in Section 7.
In this paper, we put our focus on the k -means clus-tering algorithm in Euclidean space R d .Weuse p = distance defined in Euclidean space D ( p, q ) can thus be cal-culated by D ( p, q )= d i =1 ( p [ i ]  X  q [ i ]) 2 = p  X 
Given a center set M = { m 1 ,m 2 ,...,m k } , we define the cost of the center set with respect to data set P as the sum of squared Euclidean distance of points in P to their corresponding nearest center within M , i.e., C ( M, P )= C ( { q } ,P ) as C ( q, P ) .

Given a data set P ,weuse c ( P )= 1 | P | p  X  P p to denote the geometric center of P . It is well known that c ( P ) is the optimal solution of 1-mean on data set P , which has the following property [9, 12].
 Lemma 1 Given a data set P and an arbitrary point q in the same space, we have C ( q, P )= C ( c ( P ) ,P )+ |
P | D 2 ( q, c ( P )) . 2.1 K -Means Algorithm
In Algorithm 1, we present the detail of the original k -means algorithm. At the beginning, the original center set Algorithm 2 SimpleIterate (data set P , M ) 1: for every point p in P do 2: Assign p to the closest center in M 3: end for 4: for every m i in M do 5: Use the geometric center of all points assigned to m i 6: end for 7: Return M M is constructed by randomly choosing k points from the original data set P . Then, the algorithm keeps improving the cost of the solution by invoking the SimpleIterate (Al-gorithm 2) algorithm. This procedure stops when the cen-ters do not move any more, i.e., no point changes its assign-ment in the last iteration.

In SimpleIterate , it first assigns the points in P to the closest center in M (line 1-3), then updates the centers by replacing the old ones with the new geometric center of the clusters (line 4-6). It is easy to verify that such an iteration can always decrease the overall cost before the convergence of the k -means algorithm.

In this paper, we are trying to improve the efficiency of multi-procedure styled k -means algorithm, where several procedures with different initial centers are run and the best solution is returned as the final result. These procedures are assumed to run one by one on a single machine. 2.2 Local Optimums in K -Means Algo-
Just like the other geometrical optimization problems, k -means algorithm can not guarantee to converge to global optimum every time. The algorithm is very likely to stop at some local optimum much worse than the global optimum. To clearly illustrate the severity of the local optimum prob-lem in k -means algorithm, we give some statistics on some real data set here.

We run 20 procedures of k -means algorithm on KDD99 data set with parameter k =4 . The results show that 80% of random initial centers lead k -means algorithm to some local optimum with cost larger than 110,000, while 10% of procedures end with cost smaller than 100,000. Most of the computation time is wasted on the useless iterations if the initial centers are not well chosen!
To discover such bad initial centers as early as possible, we focus on deriving a lower bound on the local optimums achievable in the future iterations. In Fig 1, we give a run-ning example of the accelerated k -means algorithm with lower bound computation. The curve above shows the costs of the center set after every iteration, while the curve below shows the corresponding lower bounds on the local opti-mum computed after every iteration. Here the lower bound convergence cost. While the original k -means algorithm can stop only after totally converging to local optimum, the lower bound estimation can be used to terminate the proce-dure earlier. For example, if the cost of current best solution is below 100,000, there is no need to further iterate after the 5th iteration since it is impossible to find a better solution. The termination in such situation can save more than half of the computation time in this procedure since there are another 7 iterations before convergence.
There are two steps in the computation of the lower bound on local optimums. In the first step, we find a maxi-mal region within which the centers can move in the future iterations. In the second step, we bound the cost of any cen-ter set in the maximal region.
To facilitate the analysis on the movements of the centers in k -means algorithm, we define a kd -dimensional solution space S for all center sets of size k . Given a center set M = { m 1 ,m 2 ,...,m k } , we can find a corresponding point in S ,  X 
M =( m 1 [1] ,...,m 1 [ d ] ,...,m k [1] ,...,m k [ d ]) . Without ambiguity, we directly use M to denote both a center set and its position  X  M in S .
In Fig 2, we show an example of one 2-means iteration in a 2-dimensional space. With N = { n 1 ,n 2 } as the pre-are assigned to center n 1 while the others are assigned to center n 2 . The geometric centers of the two sets are m and m 2 , and thus M = { m 1 ,m 2 } replaces N as the new center set. Therefore, we say the center set moves from ( n 1 [1] ,n 1 [2] ,n 2 [1] ,n 2 [2]) to ( m 1 [1] ,m 1 [2] ,m in the solution space S . We call a center set T Intermediate Center Set between N and M , if every center t i in T lies on the line segment joining m i  X  M and n i  X  N . Obviously, T is also on the line segment between M and N in solution space S . T = { t 1 ,t 2 } is such an intermediate center set in Fig 2.

In the following, we denote the neighborhood of center m i by NH ( m i ,M,P ) , which contains points in P nearer to m i than any other center in M .
 Lemma 2 Assume N is the center sets before a k -means it-eration, and M is the center set after the k -means iteration. If T is an intermediate center set between M and N ,we have C ( N, P )  X  C ( T,P ) .

Proof: Since m i = c ( NH ( n i ,N,P )) , by Lemma 1, we have On the other hand, the cost of the center set T is
Combining (1) and (2), we have C ( N, P )  X  C ( T,P ) since D ( n i ,m i )  X  D ( t i ,m i ) .

The above lemma shows that when k -means algorithm moves the centers after one iteration, any solution on the line segments between them must be a better solution than the previous one. This further implies that if the current cen-ter set are surrounded, in the solution space S , by a group of center sets with higher costs, then the centers must con-verge at some local optimum in the  X  X asin X , which is proven rigourously in the following theorem.
 Theorem 1 Assume M is a k -means center set in S and is covered by a closed region R  X  S . If every center set T on the boundary of R has C ( T,P ) &gt;C ( M, P ) ,the k -means algorithm with M as initial centers must converge at some solution M  X  R .
Proof: If from current center set M , k -means algorithm can converge at some solution out of R , there must be one iteration from solution M 1 to solution M 2 , where M in R but M 2 is not. There must be an intermediate cen-ter set M 3 on the boundary of R between M 1 and M 2 . Since C ( M 1 ,D )  X  C ( M, D ) by the property of itera-tions, and C ( M 3 ,D ) &gt;C ( M, D ) by assumption, we have C ( M 3 ,D ) &gt;C ( M 1 ,D ) which contradicts lemma 2.
Based on the above theorem, we know that if we can find aregion R containing the current center set M in the solu-tion space S with costs larger than C ( M, P ) on the bound-ary, the movements of the centers are constrained in such aregion R . We call such a region Maximum Region of the k -means local optimum.
In Fig 3, we show an example of the solution space S for a 1-dimension 2-means clustering. In the figure, the vertical axis represents the position of the first center and the hori-zontal axis represents the position of the second center, so any point in the figure is a 2-point center set on 1-dimension space. We use contour lines to present the costs of the center sets in the solution space, and use the square point to repre-sent the local optimum in the space. Assume the circle point M is the current center set after the last iteration. Since M is on the contour line of cost 10, we have C ( M, D )=10 . By Theorem 1, any contour line of cost larger than 10 must form a boundary of a maximal region for M , such as the contour line of cost 20 or 30. The dashed rectangle, whose center is M , also forms a maximal region, since any center set on the boundary of the rectangle must have cost larger than 10. It is straightforward to verify that the local opti-mum L is enclosed by any of such maximal regions.
As is shown in the example of Fig 3, maximum re-gions can be of complex shape, which makes it impos-sible to exhaustively search for such regions in the kd -dimensional space S . We therefore propose a special type of maximum regions, which is easier to manipulate. Given a center set M , we define R ( M,  X ) = { M = { m 1 ,m 2 ,...,m k }| X  i, 1  X  i  X  k, m i  X  m i  X   X  } . That is, for any M  X  R ( M,  X ) , there is no center m i  X  M is away from m i  X  M by  X  distance. Therefore, the bound-ary of R ( M,  X ) is  X  X  ( M,  X ) =  X  B ( m i ,M,  X ) , where B ( m i ,M,  X ) = { M  X  R ( M,  X ) | m i  X  m i = X  } .By Theorem 1, R ( M,  X ) is maximum region if C ( M ,P ) &gt; C ( M, P ) for all M  X  B ( m i ,M,  X ) for any i .

Recall the 1-dimension 2-means example in Fig 3. The dashed rectangle forms the boundary of a maximal region R ( M,  X ) , since (1) the difference on either axis between M and any other center set in the rectangle is smaller than  X  , and (2) any center set on the rectangle must have cost larger than 10.

We u s e R ( M,  X ) as our maximal region not only for its easiness to represent but also for its simpler analysis on the costs of the center sets on the boundary. Assume we are at the beginning of a new iteration in k -means algorithm and L = { L 1 ,...,L k } is the point assignment after last iteration, i.e., all points in L i  X  P are assigned to m i and m i is the geometric center of the point set L i .Givena point p  X  L i , we define d 1 ( p ) to be the distance from p to its current cluster center m i , d 2 ( p ) to be the distance from p to its nearest center in M and d 3 ( p ) to be the distance to p  X  X  second nearest center in M .

To find out whether R ( M,  X ) is a maximal region pro-vided the value of  X  , we divide the data set P into three subsets P 1 , P 2 and P 3 according to the value of  X  used in R ( M,  X ) . P 1 contains all the points which will be as-signed to some other center after the current k -means it-eration, i.e., P 1 = { p  X  P | d 1 ( p ) &gt;d 2 ( p ) } tains all the points in P  X  P 1 that might be assigned to other cluster if centers move to M  X   X  X  ( M,  X ) , i.e., P 2 = { p  X  P | d 1 ( p ) &gt;d 3 ( p )  X  2 X  } . P 3 contains all the other points not in P 1 and P 2 . In Fig 2, when the centers move from N = { n 1 ,n 2 } to M = { m 1 ,m 2 } , q is in P 1 since d 1 ( q ) &gt;d 2 ( q ) , while p maybein P d ( p ) &gt;d 3 ( p )  X  2 X  , otherwise p is in P 3 .
Let X ( p )= d 2 1 ( p )  X  d 2 2 ( p ) for any point p  X  P the following lemma.
 Lemma 3 Given the assignment L and center set M before a k -means iteration, the cost of M with respect to P is
Proof: i C ( m i ,L i ) is the cost by assigning every point p  X  L reassigning p  X  P 1 to the nearest center in M instead of m i . The difference gives the cost of C ( M, P ) .
 We also define the following two functions for points in P 1 and P 2 respectively. For point p  X  P 1 , we define
Y ( p )= For point p  X  P 2 , we define
Z ( p )= Lemma 4 Given the current center set M , its point assign-ment L , and any center set M  X  B ( m i ,M,  X ) , C ( M ,P ) is lower bounded by
Proof: Given M = { m 1 ,...,m k } , L = { L 1 ,...,L k } , and M = { m 1 ,...,m k } , we first assign every p  X  L j to m j , whose cost is at least j C ( m j ,L j )+ is because m i must move to m i by distance  X  , and m j can stay at m j for all j = i .

For every point p  X  P 1  X  L j , 1  X  j  X  k , the maximum distance to m j is d 1 ( p )+ X  . The minimum distance to any minimum distance is 0. So, the cost reduction by reassign-ment for p can be no more than Y ( p ) .

For any point q  X  P 2  X  L j , the maximum distance to m j is still d 1 ( q )+ X  . But the minimum distance to m l = m d ( q )  X   X  if d 3 ( q ) &gt;  X  and is 0 otherwise. Z ( q ) can fully capture such cost reduction. So, the actual cost of M  X  B ( m i ,M,  X ) is lower bounded by the function above. Theorem 2 R ( M,  X ) is a maximum region if  X  2 min
Proof: M  X  B ( m i ,M,  X ) for 1  X  i  X  k con-sist of the whole boundary of R ( M,  X ) , R ( M,  X ) is a maximum region if C ( M ,P ) &gt;C ( M, P ) for any i and M  X  B ( m i ,M,  X ) . By combining Lemma 3 and Lemma 4, for M  X  B ( m i ,M,  X ) ,wehave
If we iterate all the boundary faces B ( m i ,M,  X ) 1  X  i  X  k , the only difference in the inequality above is the number of points in L i . If the inequality above can be sat-isfied for the smallest | L i | , it must be valid for all boundary faces. So, inequality (3) is a sufficient condition to prove R ( M,  X ) is a maximal region.

Since Y ( p ) and Z ( p ) are actually functions on  X  , we call left hand of inequality (3) the incremental function f ( X ) . Then R ( M,  X ) is a maximum region if f ( X ) &gt; 0 .Inthe following part of the section, we will look at how we can lower bound the cost of the local optimum in a maximal region R ( M,  X ) . 3.3 Bounding Local Optimum in R ( M,  X )
If a  X  is found to satisfy the condition of Theorem 2, we can lower bound the cost of local optimum from the current center set M by the following theorem.
 Theorem 3 Given a positive  X  satisfying f ( X ) &gt; 0 ,if k -means algorithm converges to a center set M , C ( M ,P )  X  C ( M, P )  X  X  P |  X  2 .

Proof: Since the distance between every pair of corre-sponding centers m i  X  M and m i  X  M is no more than  X  . The difference between the costs C ( M, P ) and C ( M ,P ) is less than | P |  X  2 by Lemma 1.

The previous theorem provides an obvious lower bound on the local optimums falling in the maximal region R ( M,  X ) . It is obvious that the smaller  X  is, the higher the lower bound can be. So, we should find the smallest  X  satisfying f ( X ) &gt; 0 to give the tightest lower bound. The details about how to find the smallest  X  will be covered in the next section.
In the section, we provide the new algorithms. The first one is the bound iteration algorithm as the substitute of sim-ple iteration algorithm, which can calculate the lower bound on the local optimum during the iteration process. The sec-ond one is the accelerated k -means algorithm as the substi-tute of the original k -means algorithm, which invokes the bound iteration algorithm or simple iteration algorithm on necessary.
To combine the lower bound computation and simple iteration algorithm, we first need to find the information about d 1 ( p ) , d 2 ( p ) and d 3 ( p ) for every p  X  P by simply calculating the distances during the search of p  X  X  nearest center.

What is more difficult is how to find the smallest  X  satis-fying f ( X ) &gt; 0 . There are two issues which make solving this inequality difficult. First, the sets of P 1 and P 2 namic with different  X  . Second, the values of Y ( p ) and Z ( p ) depend on  X  as well.

To facilitate the computation, we first remove the impact of  X  on the functions Y ( p ) and Z ( p ) by further dividing the sets P 1 and P 2 . We construct a subset of P 1 , P 1 P | struct another subset of P 2 , P 2 = { p  X  P | d 3 ( p )  X  2 X  &amp; d 3 ( p ) &lt;  X  } . By such construction, incremental function f ( X ) becomes f ( X ) = A  X  2  X  2 B  X   X  C , where the following parameter equations are easy to verify from Theorem 2.
 A = min | L i | X  X  P 1 | X  X  P 2 | B = C =
We note that a point can be in P 1 and P 1 at the same time and likewise for P 2 and P 2 . When the sets P 1 , P 1 , P P 2 are static, the incremental function is a pure quadratic function on  X  . Since there are positive solutions for  X  in f ( X ) &gt; 0 only when A&gt; 0 ,ifwehaveanintervalof  X   X  [ i 1 ,i 2 ] on which P 1 , P 1 , P 2 and P 2 are static, the maximal value of the incremental function must be achieved on either end of the interval. This gives us the intuition that we only need to test the extreme points on each interval with static P 1 , P 1 , P 2 and P 2 . Fortunately, the following lemma shows, the number of such intervals must be no more than 2 | P | .
 Lemma 5 There are at most 2 | P | possible configurations of P 1 , P 1 , P 2 and P 2 .
 Proof: For every point p , it can change the configurations of the sets at most twice. For p  X  P 1 , p is always in P no matter what  X  is, and it is in P 1 when  X  &gt;d 2 ( p ) .For p  X  P  X  P 1 ,itisin P 2 and P 2 when  X  &gt; ( d 3 ( p )  X  d and  X  &gt;d 3 ( p ) respectively. Since these changes on the configuration can be sorted by  X  , there are at most 2 configurations. The bound is tight when no point is in P which takes place at the convergence of the algorithm.
With the analysis above, we give a scan algorithm which can find the minimum  X  for f ( X ) &gt; 0 if such  X  ex-ists. Briefly speaking, the algorithm gradually increases the value of  X  from 0, keeping  X  jumping to next event for configuration update. This can be done by sorting all the configuration update events for every point p as is shown in the proof of Lemma 5.

The detail of the algorithm is listed in Algorithm 3. For every point p  X  P , the algorithm (Line 6 to 12) stores, in array Ar , the smallest  X  s at which p changes the configu-ration, and how such change can influence the parameters in the function f ( X ) . Such information is summarized in a 4-attribute element, where the first attribute is the  X  value when update happens and the rest three are the differences on the parameters { A, B, C } when update takes place.
By visiting the elements in Ar in ascending order on the first attribute, the parameters A , B and C are updated ac-cordingly. If at any moment, a  X  satisfying f ( X ) &gt; 0 is found, the algorithm returns the new center set M as well as the lower bound by Theorem 3, otherwise the lower bound is 0, which is the trivial lower bound.
 Algorithm 3 BoundIterate (center set M , data set P , cur-rent best cost C  X  ) 1: construct an array Ar 2: set A = min | L i | , B = C =0 3: for every point p  X  P do 4: assign p to its nearest center in M 5: compute d 1 ( p ) , d 2 ( p ) and d 3 ( p ) 6: if p changes its clustering then 7: [ d 2 ( p ) ,  X  1 ,  X  d 2 ( p ) ,d 2 2 ( p )] is inserted into Ar 8: B += d 1 ( p )+ d 2 ( p ) 9: else 12: end if 13: end for 14: recompute the centers of the clusters and store in M 15: sort all the elements in Ar in ascending order on the 16: for every element r =[ r 1 ,r 2 ,r 3 ,r 4 ] in Ar do 17: A += r 2 , B += r 3 C += r 4 and  X = r 1 18: if A&lt; 0 then 19: return ( M , 0) 20: end if 21: if A  X  2  X  2 B  X   X  C&gt; 0 then 22: return ( M ,C ( M ,P )  X  X  P |  X  2 ) 23: end if 24: end for 25: return ( M , 0)
Note that B and C must be positive and A is non-ascending with the increase of  X  because any element in-serted into Ar must have a non-positive second attribute. So, when A becomes negative, it is impossible to find any positive solution for  X  any more. The iteration stops here and returns ( M , 0) (Line 18).

The complexity of the bound iteration algorithm con-sists of four parts: points assignment, element insertion, element sorting and  X  searching. Points assignment can be finished in O ( | P | d ) time. The insertion and searching take at most O ( | P | ) time since there are at most O ( ments in Ar and every single operation in them can be done in constant time. The sorting on the elements in Ar takes O ( | P | log | P | ) time by simply invoking existing sorting al-gorithms such as quicksort [4]. So, the total complexity of the bound iteration is O ( | P | (log | P | + d )) . 4.2 Accelerated K -Means Algorithm
Here, we present the complete accelerated k -means al-gorithm in Algorithm 4 to exploit the benefit from bound iteration algorithm. Similar to the original k -means algo-Algorithm 4 Accelerated k -means algorithm (data set P , k , current best cost C  X  ) 1: Randomly choose k points as the center set M 2: while M does not change any more do 3: if C ( M, P ) &gt;C  X  then 4: ( M,  X  )= BoundIterate ( M, P, C  X  ) 5: if  X   X  C  X  then 6: Return NULL 7: end if 8: else 9: M = SimpleIterate ( M, P ) 10: end if 11: end while 12: if C ( M, P ) &lt;C  X  then 13: Return M and Update C  X  14: else 15: Return NULL; 16: end if rithm, it first randomly chooses k points from the data set as the initial center set. Then, the algorithm keeps iterating until any of the following three cases happens: (1) a lower bound larger than current best cost is found; (2) the algo-rithm converges to some better solution than ever seen; (3) the algorithm converges but no better solution is found.
The iteration procedure in this algorithm can run ei-ther bound iteration algorithm or simple iteration algorithm. current best cost seen so far, C  X  , the algorithm invokes the bound iteration algorithm (Line 4). If the lower bound re-turned by bound iteration is larger than C  X  , it immediately stops the computation and return NULL (Line 5-6). When the cost of the current center set becomes smaller than C the algorithm switches to the original simple iteration algo-rithm. This is because any lower bound in future iterations must be smaller than current best solution, which can not help to prune any more (Line 9). If the centers finally con-verge, the algorithm return the center set if its cost is smaller than C  X  , otherwise NULL is returned (Line 12-15).
Lemma 5 shows that we only need to test O ( | P | ) possi-ble values to find a  X  satisfying f ( X ) &gt; 0 . The number of possible  X  values can be further reduced. There are two types of  X  values that we do not need to test. The first type contains all  X  s whose corresponding B and C satisfying the condition  X  &lt; 2 B/A , since quadratic equation has no positive solution in such situation. Second,  X  can not be too large either. When  X  is large enough, the lower bound achieved by such  X  must be smaller than the best known cost C  X  . Such a bound is useless since we cannot prune any iteration if the lower bound is below C  X  . The combination of the two ideas leads to the following lemma.
 Lemma 6 Given the current optimal cost C  X  , to find  X  satisfying f ( X ) &gt; 0 and C ( M, P )  X  X  P |  X  2 &gt; C  X  , the algorithm only needs to test  X  in the interval
Proof: Since f (0) = 0 , 0 &lt;A&lt; min | L i | , B&gt; p  X  P 1 d 1 ( p ) and C is non-decreasing with the increase of  X  . The positive solution of  X  for f ( X ) = 0 must be larger  X   X  ( C ( M, P )  X  C  X  ) / | P | , the lower bound must be larger than C ( M, P )  X  X  P |  X  2 &gt;C  X  . We have no interests on such lower bound, since it can not be used to prune the current procedure any more.

With such a lemma, we only need to sort and test the  X  values in a certain interval, which can save much time in the calculation of the lower bound. This makes two changes on the bound iteration algorithms. First, only el-ements with first attribute smaller than the upper bound of interval are inserted. Second, the elements smaller than the lower bound of interval are directly used to update the pa-rameters in f ( X ) without testing f ( X ) &gt; 0 .
Since the result of k -means algorithm is fairly random, we run any single experimental item 5 times with different random seed for different algorithm. The average results are used in our performance evaluation. In this section, we use OKM to denote the original k -means algorithm and AKM to denote the accelerated k -means algorithm.

The generation of synthetic data sets follows the method used in previous studies on clustering problem, such as BIRCH [16]. We created 4 different data sets with 1,000,000 points on 4, 8, 16 and 32 dimensional spaces, respectively. The value of the points on every dimension is a float value between 0 and 1. Every data set consists of 40 small clusters, each of which occupies about 2.5% of the whole data set. Every cluster follows a Gaussian dis-tribution, whose center and variance follow some uniform distributions. There are also some noisy points uniformly distributed in the space, whose size is 5% of the whole data set.

There are also two algorithm parameters in considera-tion, the target cluster number k and the k -means algorithm procedure number. The procedure number is the time the algorithm chooses the random initial center set and recom-pute the k -means result.
The performances of the algorithms were measured on the total computation time, the total number of iterations invoked and the individual numbers of both types of itera-tions invoked (simple iteration and bound iteration). Since AKM always outputs the correct clustering result, we omit the comparison of costs in these two algorithms.

In addition, we also conducted experiments on KDD98 data set 1 and KDD99 data set 2 . KDD98 data set is a 32 di-mension data set with 95000 records and KDD99 data set is a 41 dimensional data set with 310000 records. Both of the two data sets are collected for previous KDD-cups. Before using them in our experiments, we pruned all the non-numerical attributes and normalized those numerical attributes to float number between 0 and 1.
 All experiments were carried out on a PC with 2GHz AMD Athlon processor and 2GB main memory. The pro-grams were compiled with gcc 3.4.3 in Linux system.
The synthetic data tests are tested with varying dimen-sionality ( D ), target cluster number ( K ) and procedure number ( R ). Unless otherwise stated, the default setting of the experiment is D =8 , K =16 and R =20 .

We first test the effect of dimensionality on the perfor-mance both on the original k -means algorithm and our ac-celerated algorithm. As is shown in Fig 4(a), AKM does not show too much advantage over OKM when the dimen-sionality of the data set is small. This is because the extra cost on maintaining the necessary information, and the time to compute the lower bound is more than the time we can save on reducing the number of iteration. This can be ver-ified in Fig 4(b) and Fig 4(c). The total iteration time of AKM is close to that of OKM and most of the iterations invoked by AKM on 2D data set are bound iterations. As the dimensionality grows, AKM becomes much faster than OKM since both the number of total iteration and the ratio of bound iteration decrease.

We also evaluate the impact of the parameter k in our experiments. The group of experiments are conducted on 16 dimensional data set, varying k from 8 to 32. These test results in Fig 5(a) and Fig 5(b) show that both the compu-tation time and iteration time of OKM increases with the growth of k , while those of AKM do not change too much. Although the bound iteration also takes more time to cal-culate in every iteration, the increase in the ratio of simple iteration ensures that the efficiency of AKM is better than OKM with large k , as shown in Fig 5(c).

In Fig 6, we present the experiment result when we vary the procedure number for both OKM and AKM. When we run more times of the procedures on the same data set, the computations of OKM and AKM both increase linearly. From Fig 6(b) and Fig 6(c), we can see that although the total iteration time of AKM is usually less than OKM, the ratio of bound iteration time to simple iteration time rises when procedure number increases.
For the tests on real data set, we only vary two algorithm parameter: target cluster number k and procedure number R . The default value of k and R are 4 and 20, respectively.
Our AKM algorithm shows large advantage over OKM algorithm in the tests on KDD 99 data set. As is shown in Fig 7, when k is larger than 8, AKM is about 2 times faster than OKM. Similar to the results on synthetic data set, the total number of iterations in AKM is much fewer than that in OKM, which implies the strong pruning ability of our lower bound computation method. When varying the pro-cedure number R on this data set, we see stable performance on AKM algorithm from Fig 8. This is also consistent with the test results on synthetic data sets.

However, when testing on KDD 98 data set, we see some results quite different. From Table 1, we can find that AKM algorithm can not be faster than OKM. The total iteration time in AKM is almost the same as the OKM. To find out the underlying reason for this phenomenon, we checked the procedures carefully and found that 90% of the procedures converge to the same global optimum in the space. This example implies the limitation of the algorithm proposed in this paper. Our algorithm can not estimate the number of lo-cal optimums in the space, which finally wastes much time on procedures converging to the same global optimum with more time-consuming bound iteration algorithm. We also argue that in such data set, there is no need to use multi-procedure k -means algorithm, since running k -means algo-rithm once is enough to find the optimal solution wanted.
With different criterions on the clustering result, there are several independent but classic clustering problems, such as k -centers, k -means and k -medians. Han and Kam-ber X  X  book [7] provides a good survey on the different clus-tering problems in data mining.

Lloyd X  X  work [13] is one of the earliest application of k -means algorithm. To speed up the k -means algorithm, there are many accelerating methods proposed before. These methods can be divided into two categories. In the first categories, the triangle inequality property of metric space is exploited in the calculation of nearest center. Elkan [6] showed that many distance computation can be skipped if the triangle inequality is applied in the nearest center update process. The second category contains different method based on indexing structure [10, 15]. The indexing struc-ture, such as kd -tree can be used to improve the efficiency of the nearest center search when a group of points have the same nearest center. However, all of the studies mentioned here do not perform well in high dimensional space because of the curse of dimensionality.

The initial centers of k -means algorithm are very impor-tant to the result quality. The method proposed by [3] is a typical initial center refinement algorithm, which chooses the center set with the minimum distortion from a group of clustering results on some small samples of the original data set.

There are also a few of studies on the convergent prop-erty of k -means algorithm. [2] showed that k -means al-gorithm works very similar to gradient descent algorithm, which always moves toward the direction reducing most of the cost. [8] first proved some bound on the conver-gence speed of k -means algorithm. They gave an  X ( n ) lower bound on the number of iterations in standard k -means method, a O ( n  X  2 ) upper bound on one-dimensional standard k -means method and a O ( kn 2  X  2 ) upper bound on a variant k -means method, where n and  X  are the size and the spread of the data set respectively. Recently, [1] proved that the lower bound of standard k -means iteration time is space.

Besides k -means algorithm, some theoretical computer scientists try to find good k -means clustering result with other techniques. [9] proposed a O ( n O ( kd ) ) algorithm to find the optimal solution and an -approximate 2-means al-gorithm with O ( n (1 / ) d ) complexity. [12] extended the idea by a (1 + ) -approximate randomized algorithm with linear complexity to both dimensionality and data size. [11] proposed an (9 + ) -approximate local search algorithm which keep swapping the centers with other points in the data set to improve the clustering result. Ding and He [5] presented the relationship between k -means and PCA, which can lead to a lower bound on global optimum.
In this paper, we derive a lower bound on the cost of the local optimum based on the current center set. The k -means procedure can be terminated if the lower bound of the cost at the future local optimum is higher than current best solution that has been computed so far. Experiments on both synthetic and real data sets reveal that such a method can greatly improve the efficiency in most of the data sets, especially with high dimensionality and large parameter k .
