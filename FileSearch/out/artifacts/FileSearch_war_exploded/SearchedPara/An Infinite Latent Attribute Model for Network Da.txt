 Konstantina Palla kp376@cam.ac.uk University of Cambridge David A. Knowles dak33@cam.ac.uk University of Cambridge Zoubin Ghahramani zoubin@eng.cam.ac.uk University of Cambridge Network data encoding pairwise relations between ob-jects appears in many fields. For instance, in biology, a protein network connects interacting partners, while in a social network, links among people indicate relations. We focus on the most common type of network data  X  X ets of observations represented as an unweighted, undirected graph X  X n the ensuing discussion. The mo-tivation behind the analysis of these networks is two fold. Firstly, there is a desire to understand the latent structure of the network; what are the features of the proteins that account for the observed interactions and what is the mechanism behind the links or non-links among groups of people. Second, the prediction of  X  X issing X  links in the network arises as an important challenge; how likely is it that a pair of proteins in-teract or that two social network members are friends. A prominent theme in machine learning is the use of latent variable methods, which approach this problem by extracting a simplified summary of the graph and predicting the presence or absence of links based on this latent representation. Latent class and latent fea-ture models are the two most common categories found in the literature.
 Latent class models assume that there are a number of clusters (classes) and that each object belongs to a single cluster. Under these models, the link probabil-ity between two objects depends only on their cluster assignments. Early work in this category includes the stochastic block model (SB) proposed in Nowicki and Snijders (2001). Instead of assuming a fixed number of clusters, the Infinite Relational Model (IRM) and the Infinite Hidden Relational Model (Kemp and Tenen-baum, 2006; Xu et al., 2006) use the Chinese restau-rant process (Pitman, 2002) to allow a potentially in-finite number of clusters. The Mixed Membership Stochastic Block Model (Airoldi et al., 2009) (MMSB) increases the expressiveness of the latent class mod-els by allowing mixed membership, associating each object with a distribution over clusters.
 Latent feature models increase the flexibility of the generative process by letting each object possess a vector of features and determine the link probabili-ties based on interactions among the features. In Hoff et al. (2001) the link probability between two objects is determined by the similarity of their real-valued fea-ture vectors. Miller et al. (2009) uses a vector of bi-nary features which can be interpreted as allowing ob-jects to belong to multiple clusters at the same time. Their model, the Latent Feature Infinite Relational Model (LFRM), assumes that the number of clusters is not known a priori and uses the Indian Buffet Process (IBP) (Griffiths and Ghahramani, 2005) to determine the number of latent clusters.
 The limitation of a single cluster membership makes the latent class models less flexible than the latent fea-ture models. As an intuitive example, consider a net-work of individuals at a collegiate University, in which a link denotes friendship or acquaintance. Here there will be multiple types of cluster, for instance colleges, departments and sports teams. A person might be a member of more than one cluster and his cluster-memberships determine his interaction with others. To capture this structure a single membership model, such as the IRM, must introduce a cluster for each possible combination of the types of cluster, which in our example would be to introduce clusters such as  X  X ryffindor college, Department of Mathematics, Football X . This results in an exponential explosion of clusters, making learning, inference and generalisation difficult. Latent feature models, e.g. the LFRM, can instead use the feature vector representation to im-plicitly account for the possible combination of clus-ters. Though powerful, these models only account for a flat clustering of the objects. In the context of the University social network, the  X  X ollege X  feature might be divided into many different subclusters, such as  X  X lytherin college X ,  X  X ryffindor college X  etc. The same for  X  X port X , with subclusters like  X  X asketball X ,  X  X ennis X , etc. The LFRM must represent each cluster with a new feature, which will result in feature vectors of greater size with a cost in interpretability. Allowing an explicit representation of the partitioning of each gen-eral class into subclasses would provide a more struc-tured representation of the data.
 Towards this end, we develop a new nonparametric la-tent feature model. We use a binary feature vector to indicate the features that an object has. If an object has a particular feature, then the object belongs to a particular subcluster of this feature. Equivalently, we can think of objects having several attributes (fea-tures) which have discrete values (the subcluster as-signments). Following our university example, a per-son might have the  X  X ollege X  attribute and belong to the  X  X ryffindor college X  subcluster, but cannot simultane-ously be a member of another college. We denote our model by ILA for Infinite Latent Attribute model. We use a nonparametric Bayesian approach to simultane-ously infer the number of features and number of sub-clusters inside each feature, while at the same time in-ferring what features are active for each object, which subcluster it belongs to and how subcluster member-ship influences the observed interactions.
 The paper is arranged as follows. In Section 2 we describe the generative process for our nonparametric model. Section 3 explains the relationship of our model to several recently proposed models. In Section 4 we derive the algorithm for performing approximate pos-terior inference, parameter estimation and link pre-diction. Section 5 gives some observations about the computational cost of our proposed model relative to others. Finally, in Section 6 we study our model X  X  per-formance on one synthetic and two real datasets. Let R be the N  X  N binary matrix that contains the links among the objects. In ILA, each object i = 1 ,...,N , is represented by a binary vector of la-tent feature values, z i . If there are M features, then Z is a N  X  M binary matrix indicating which features each object has active, with z im  X  Z ( i,m ) = 1 if the i th object has feature m and z im = 0 otherwise. Let C be a set of vectors, that is C = { c (1) ,..., c ( M ) } , that describe the subcluster assignments within each fea-ture, such that c ( m ) is a vector of length N where c ( m ) denotes the subcluster the i th object belongs to in the m feature m ). The number of subclusters present in the m th feature, which is also not known a priori, is de-let W be a set of M real-valued weight matrices of size K weight that affects the probability of there being a link from object i to object j , given that object i belongs to subcluster k and object j belongs to subcluster k 0 of the m th feature.
 Given the feature matrix Z , the set of the subcluster assignments C , and the set of the weight matrices W , the probability that there is a link from object i to object j is given by Pr( r ij = 1 | z i , z j , C , W ) =  X  where the sum ranges over all M features, s is a bias term, and  X  ( x ) = (1 + e  X  x )  X  1 is the sigmoid (lo-gistic) function that maps the input arguments from (  X  X  X  , +  X  ) to (0 , 1), ensuring that the result is a valid probability. Under this model, only features that are on for both objects influence the probability of a link between them. For these common features, the ap-propriate weight values are summed up, depending on the subcluster assignments of i and j . The weight val-ues are continuous variables which can be positive or negative allowing pairs of subclusters to encourage or discourage links between them correspondingly. We assume that given the Z , C and W , the probability of each link is independent and the likelihood is therefore as follows In order to allow flexible inference of the latent struc-ture from data, we set the number of possible features M and the number of subclusters in each feature K ( m ) to infinity by using an IBP prior on Z and CRP priors on the c  X  X . The hierarchical generative model is then: r | Z , C , W  X  Bernoulli  X  The ILA model is illustrated in Figure 1.
 The IBP parameter,  X  , affects the number of repre-sented features, whereas the CRP parameter,  X  , con-trols the number of subclusters inside each feature. To improve the flexibility of our model, we put Gamma priors on  X  and  X  , and a Gaussian prior on the bias term s as follows where  X  s and  X  s are the mean and standard deviation hyperparameters for the bias (we use  X  s =  X  1 , X  s = 4 unless otherwise stated). Here we examine three models that are closely related to ILA. The IRM model of Kemp and Tenenbaum (2006) and the LFRM of Miller et al. (2009) both use nonparametric Bayesian approaches to account for po-tentially infinite number of clusters in the data. In the IRM, the link probability between two objects depends only on the clusters they are assigned to: where the link probabilities for each pair of clusters, {  X  kk 0 : k,k 0 = 1 ,...,K } are given independent Beta priors, and the cluster assignments, c are given a CRP prior. The ILA and LFRM on the other hand, put a logistic-normal prior on the between feature and sub-cluster link probabilities. More specifically, the LFRM defines the link probability as where W is a K  X  K real valued weight matrix (with K being the number of features), given an element-wise Gaussian prior, and Z is an N  X  K matrix of binary feature vectors drawn from an IBP. Compar-ing Equations 4 and 1 for the ILA, we see how the two models differ. The LFRM defines a weight value for each possible pair of features, while ILA defines a weight matrix for each feature, whose elements corre-spond to every pair of subclusters in that feature. The link probability in LFRM depends on all the possible pairs of features that are on for both objects, while in the ILA model, the link probability is contributed to only by features that are simultaneously on for both objects. While subclusters within a feature can inter-act in ILA, subclusters from different features do not interact.
 Unlike the IRM, the ILA model does not partition the objects into a set of non-overlapping clusters; although it specifies non-overlapping subclusters for each fea-ture, it also allows each object to have multiple fea-tures, thus accounting for multiple membership. ILA more expressive than LFRM because it associates each feature with a set of subclusters.
 Interestingly, both the IRM and LFRM can be thought of as special cases of our model. If only one column of Z is switched on in ILA (i.e. there is only one feature which is on for every object) then this is equivalent to the IRM. In this case the ILA likelihood becomes Contrasting this to Equation 3 the ILA has a logistic-normal prior on the between subcluster link probabil-ities rather than a Beta prior, but this is a relatively minor difference.
 If the LFRM is constrained to have a weight matrix W with only diagonal non-zero elements, then its link probability becomes This is then equivalent to ILA in the case when there is only one subcluster in each feature, since the ILA link probability is then Pr( r ij = 1 | z i , z j , C = 1 , W ) =  X  The ILA model can also be seen as a extension of the Multiplicative Attribute Graph (MAG) model pro-posed in Kim and Leskovec (2011), where the link probability is where  X  is a set of M two by two matrices of probabili-ties with elementwise independent Beta priors, and the c  X  X  are equivalent to our subcluster assignment vari-ables but constrained to takes values in { 1 , 2 } . We ex-tend this model in three ways: 1) we learn the number of subclusters in each feature, rather than fixing it to two, 2) we learn the number of features M , and 3) we incorporate additional sparsity, in that an object need not have a particular feature active at all. We parame-terise our model in terms of real valued weights which contribute to the log odds of a link being on, rather than with probabilities that are multiplied together, but this entails no loss of flexibility. In fact this may be advantageous to ILA since the MAG suffers from each new feature decreasing all link probabilities. There are several models that have been proposed for discovering hierarchical structure in relational data (Girvan and Newman, 2002; Roy et al., 2007). In these models, each object is still a member of one out of many non-overlapping clusters. Our model is distinct in allowing each object to be a member of many sub-clusters as long as these subclusters are in different features. In the following, we present a method for inferring the latent variables of the model: the infinite bi-nary feature matrix Z , the subcluster assignments, c ( m ) for each feature m , and the weight matrices, W ( m ) . Simultaneously we recover the number of fea-tures and the number of subclusters inside each fea-ture. As with many other Bayesian models, exact inference is intractable so we employ Markov Chain Monte Carlo (MCMC), and follow an iterative proce-dure that achieves posterior inference over the latent variables. The sampler iterates as follows: Sampling the feature matrix, Z. We Gibbs sam-ple each element of Z in succession. For each object i , the sampler makes the following decisions: which of the current M available features should be turned on/off, and how many new features should be turned on. However, when turning on a feature the sampler must also sample a new subcluster assignment and, in case of adding a new subcluster, the related new weights.
 We use exchangeability of the rows of Z and assume that the i th object is the last to be added to Z after N  X  1 rows have already been added. For all the M features currently present in Z , the conditional poste-rior probability of an entry z im , m = 1 ,...,M follows a Bernoulli distribution: where Z  X  im is the Z matrix excluding the Z ( i,m ) ele-ment, n  X  im is the number of times feature m is present in Z  X  im and C  X  im excludes the subcluster assignment c i . To compute the probability in Equation 6, we need to sum over c ( m ) i , the space of the possible sub-clusters that the i th object may be assigned to if z im is to be turned on. This also includes integration over a possible new subcluster. However, the prior over the parameters W ( m ) related to a new subcluster is not conjugate because of the logistic link function, and thus the likelihood term cannot be computed exactly. To overcome this problem, we use the auxiliary vari-able approach proposed in Neal (2000) (Algorithm 8), both to facilitate the integration required in Equation 6, and to decide which subcluster to assign the i th ob-ject to in the m th feature if z im is turned on. We must also sample the number of new features unique to the i th row, M ( i ) new . Instead of consider-ing these features separately, we calculate the condi-tional posterior over M ( i ) new , using the fact that under the IBP the prior distribution over M new for the last row is Poisson(  X /N ). Combining the Poisson prior with the likelihood, we obtain the conditional poste-rior over M ( i ) new . However, to obtain the required likeli-hood term we need values for C ( m ) and W ( m ) for the proposed new features. Clearly c ( m ) i = 1 for any new features, since a feature active for only one object can only have one subcluster. Integrating over the weights is not straightforward because the prior over W ( m ) is not conjugate to the logistic likelihood. We therefore employ a Metropolis Hastings step, proposing values for w ( m ) 11 from the prior so that the acceptance ratio becomes simply the likelihood ratio for including the new features and associated C ( m ) and W ( m ) values in the model versus not including them.
 Sampling the subcluster assignments, C. We may choose to resample each C ( m ) in succession as a second step, again using Algorithm 8 of Neal (2000). In practice we found this unnecessary since C is sam-pled in the process of sampling Z .
 Sampling the weights, W. Given Z and C , the sampler successively resamples each of the weights do not have conjugacy (due to the logistic link func-tion), we cannot sample directly from the posterior over w ( m ) kk 0 . To overcome this problem we used both Metropolis Hastings and slice sampling (Neal, 2003) but found the later resulted in faster mixing. Hyperparameters. We use slice sampling for both the IBP hyperparameter,  X  , the CRP concentration parameter,  X  and the bias, s .
 IRM implementation. Our implementation of the IRM model of Kemp and Tenenbaum (2006) uses stan-dard single site Gibbs sampling along with the re-stricted Gibbs sampling split merge method of Jain and Neal (2000). In the IRM we are able to integrate out the parameters  X  analytically due to conjugacy, so we need only sample the cluster assignments and the CRP concentration parameter, for which we use slice sampling.
 LFRM implementation. For the LFRM of Miller et al. (2009), we Gibbs sample the IBP matrix Z and slice sample each element of the weight matrix W se-quentially, followed by the IBP concentration param-eter. 4.1. Sequential initialisation The Gibbs updates described above are the simplest moves we could make in a MCMC inference proce-dure for the ILA model. However, these updates are quite incremental, since only a single variable is up-dated at a time. Due to the extremely large number of possible configuration states, sampler can suffer from local modes and have some-what slow mixing. Non-incremental moves, like split-ting and merging features in the Z matrix or subclus-ter assignments in C can produce major changes in the configuration state in a single iteration and can help the sampler explore more efficiently. Split-merge sampling in the IBP has been previously described in Meeds et al. (2006). However, we found that a sequen-tial initialization of the sampler improved the perfor-mance, guiding the sampler closer to neighborhoods of higher probability.
 To sequentially initialise all parameters the objects are first randomly permuted and then added to the model as follows. Initially two objects are added to the model with no features active. Then a few (typically three) iterations of the MCMC sampler are run. Then the next object is added, with no features turned on, and another three iterations of the sampler are run. This procedure is iterated until all objects have been added. The sampler will naturally grow the number of features and subclusters within each feature as more data is added. The advantage of this method is that the ini-tialisation is appropriate for the model, the sampler is very fast initially due to the small number of objects, and the search space is small initially so it is easier for the Markov chain to find a relatively high probability region of parameter space. We also used sequential ini-tialisation for our implementation of LFRM, but not for IRM where we find split-merge sampling is able to better overcome local optima. 4.2. Prediction A principled way to evaluated a generative model is by its ability to predict missing data values given some observations. In our model, we collect T samples mate the predictive distribution of a missing link as the average of the predictive distributions for each of the collected samples. Assuming that we want to pre-dict the missing link r ij between objects i and j , the approximate predictive distribution will be as follows In general, the computational cost of latent feature models scales quadratically in the number of objects. In the LFRM, computing the likelihood has a com-plexity of O ( M 2 N 2 ), where M and N is the number of represented features 1 and the number of objects cor-respondingly. For ILA, the link probability between two objects given by Equation 1, results in computa-tional cost O ( MN 2 ) when calculating the likelihood across all pairs. The computational cost of the IRM scales linearly in the number of links in the network, L = probabilities  X  integrated out, can be written as where n ( a,b ) is the number of pairs of objects ( i,j ) where i  X  a and j  X  b and R ( i,j ) = 1,  X  n ( a,b ) is the number of such pairs where R ( i,j ) = 0, and Beta(  X  ,  X  ) is the Beta function. The computational cost of com-puting the likelihood in the IRM is therefore O ( K 2 L ). In Morup et al. (2011), it is observed that if a noisy-or likelihood model is used in the LFRM rather than the logistic Gaussian model, then the likelihood can be calculated in O ( K 2 L ) as for the IRM. This allows excellent scalability on typical sparse real world net-works where the number of links is much smaller than the number of non-links. This scalable variant is ap-plicable to our model, but comes with the significant restriction of only being able to have positive weights between clusters (homophily). As a result we leave this development to future work. We present results on a toy synthetic data set and on two real world datasets: the NIPS coauthorship network and a novel gene interaction network. 6.1. Synthetic data We first explored the ability of our model to recover the underlying structure of a network using synthetic data. We considered one simple synthetic dataset (Figure 2a) hand-constructed to have an unambiguous most parsimonious solution under each model. Under ILA this is the feature matrix shown in Figure 2(b) with two features. The first feature has three homophilic subclusters (i.e. individuals tend to have links if they are in the same cluster), whereas the second feature has two heterophilic subclusters (i.e. individuals tend to link if they are in different clusters). We ran ILA for 200 MCMC iterations following sequential initiali-sation. The sample with the lowest energy (highest log probability under the posterior) corresponds exactly to the expected  X  X rue X  structure, as shown in Figure 2b. The MAP sample found using LFRM is shown in Figure 2c. Again this is a passable explanation of the data but it is considerably more convoluted than the simple, interpretable but rich solution found using ILA. Note that running 2000 iterations (following se-quential initialisation) of LFRM no better solution was found. In contrast the IRM finds the flat clustering of six clusters shown in Figure 2d, which is an acceptable solution but does not capture the rich structure that ILA is able to. 6.2. NIPS coauthorship network We compare the performance of the IRM, LFRM and ILA on the NIPS coauthorship dataset (Globerson et al., 2007), where a link corresponds to two individ-uals being coauthors of a paper at one of the first 17 NIPS conferences (see Figure 3(a)). Following Miller et al. (2009) we use only the 234 most connected au-thors. We run 10 repeats, each time holding out a different 20% of the data (links and non-links) and using a different random initialisation. We run two versions of ILA: the first with a fixed number of fea-tures M = 6, and the second which learns M , denoted M =  X  . Note that even with M = 6 ILA is still ex-tremely flexible since it can learn the number of sub-clusters in each feature. We run 500 iterations for ILA and 1000 iterations for IRM and LFRM, and calculate evaluation metrics averaged over the last 300 samples. The results are shown in Table 1. We confirm the find-ing in Miller et al. (2009) that LFRM outperforms the IRM on this dataset. However, across all three eval-uation metrics both ILA versions significantly outper-form LFRM (for example, the t-test between the test log likelihoods for LFRM and ILA ( M = 6) shows the means to be significantly different with a p -value of 10  X  7 ). The fully infinite version of ILA performs slightly, but still statistically significantly, better than when we constrain M = 6 for training error and test error. For test log likelihood ILA ( M =  X  ) still ap-pears to perform slightly better than ILA ( M = 6) but the difference is not statistically significant based on a t-test. Under the ILA posterior M is concentrated around 7 or 8 features, with typically 2 to 4 subclusters per feature.
 In Figure 3 the link predictions for each of the three models are presented. Figures 3(b)-(d) visualize the belief of each model that there should be a link be-tween each pair of authors. The link matrices were constructed after running the three models on the NIPS 1-17 dataset for 500 iterations, using the same random seed and averaging over the last 160 samples. To facilitate interpretability, we ordered the authors by the clusters found by the IRM. It can be clearly seen that both the LFRM and ILA models outperform the IRM model by appearing more confident and re-producing the corresponding network more faithfully. Considering Figures 3(c)-(d), LFRM and ILA appear comparable, with ILA being slightly more confident. Quantitatively however, ILA gives a test log likeli-hood of  X  0 . 0295 as opposed to  X  0 . 0386 for the LFRM model. We also report the AUC metric, the area under the ROC (Receiver Operating Characteristic) curve, for the held-out data. 6.3. Gene interaction network Finally we present results on a subset of the interac-tion data presented in Jonikas et al. (2009) 2 . This is an example of a new class of high throughput gene interaction assays, in this case using the yeast S. cere-visiae . A range of  X  X eletion X  strains are created, each of which has a single gene deleted. Some phenotypic response is measured during the growth of each strain, in this case unfolded protein response (UPR), a mea-sure of how badly the cell is doing at correctly folding its membrane proteins.  X  X ouble mutants X  with two distinct genes deleted are then screened. Based on the single deletion strains, the expected UPR response for these double mutants can be predicted (see Jonikas et al. (2009) for details) assuming no interaction be-tween the two deleted genes. If the observed UPR response is significantly different from this predicted value then the genes must interact in some way, so we consider this as an edge in the network. We use the 156 genes with the least missing data. We run 10 repeats with a different 10% of the observed data held-out each time, and perform 500 MCMC iterations for ILA and 1000 for the IRM and LFRM. Again we find the ILA model significantly outperforms LFRM, which in turn outperforms the simple IRM (see Table 2). In this case the infinite version M =  X  has consider-ably better predictive performance than with M = 6, suggesting there is considerably more structure in this data so that allowing more features is beneficial. In fact ILA typically finds around M = 30 features with 3 to 5 subclusters per feature. We find significantly more features are associated with particular proper-ties of the genes as defined by Gene Ontology classes 3 than would be expected by chance ( p &lt; 10  X  3 calcu-lated by permutation testing), for example the three subclusters of one particular feature have very differ-ent proportions of ligand binding genes (10 / 41 , 21 / 27 and 2 / 20 respectively). Our experimental results on two very different datasets suggest that the network models proposed to date fail to capture the complex nature of real world net-works. We have introduced a hierarchical nonparamet-ric Bayesian model, ILA, which is able to naturally rep-resent this complexity, with corresponding gains in em-pirical performance. In principle ILA could be made even more flexible by allowing multiple membership of subclusters within a feature, corresponding to a hi-erarchical IBP. We leave investigating whether this is beneficial to future work.

