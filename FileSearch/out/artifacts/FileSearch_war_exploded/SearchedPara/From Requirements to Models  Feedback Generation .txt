 At the beginning of a software project, the requirements of different stakeholders are usually gathered in a document. The majority of these documents are written in natural language, as the survey by Mich et al. shows [1]. Diversity of stakeholders and insuf-ficient communication results in imprecise, incomplete, and inconsistent requirements documents, because precision, completeness an d consistency are extremely difficult to achieve using mere natural language as the main presentation means.

In software development, the later an error is found, the more expensive its correc-tion. Thus, it is one of the goals of requirements analysis to find and to correct the deficiencies of requirements documents. A practical way to detect errors in require-ments documents is to convert informal specifications to system models. In this case, errors in documents would lead to inconsistencies or omissions in models, and, due to the more formal nature of models, inconsistencies and omissions are easier to detect in models than in textual documents.

Although there exist a number of automatic approaches that analyze specifications written in natural language and provide a model, the existing approaches go in one direction only: they transform a textual specification into a formal model. However, in the case that the specification exhibits some deficiencies, they either heuristically compensate these deficien cies or fail silently.
 Contribution: The goal of the presented paper is to show, how the deficiencies of the models resulting from the text can be used to generate feedback for human analysts. The feedback can be presented in two forms: (1) in natural language and (2) by spe-cial markings on the produced models. The effectiveness of the generated feedback was evaluated in an experiment and it was found that the generated feedback can ad-dress genuine problems of requirements specifications that would be overseen by human analysts.
 Outline: The remainder of the paper is organized as follows: Section 2 presents our approaches to text-to-model translation, used as the basis for the presented work on feedback generation. Sections 3 and 4 are the technical core of the paper, they present the feedback generation and its evaluation. Finally, Section 5 gives an overview of related work and Section 6 summarizes the paper. In our survey of existing modeling techniques [2] it was shown that all existing in-dustrially relevant formalisms are based either on interaction sequences or on finite automata. For this reason, the target model types for the behavior modeling are either finite automata or Message Sequence Charts (MSCs), serving as a representative for interaction-based modeling techniques. The translation from text to MSCs is presented in Section 2.1, and the translation to finite automata in Section 2.2. 2.1 From Scenarios to Message Sequence Charts Translation of textual scenarios to message sequence charts was presented in [3,4]. For the translation we assume that every message sequence chart (MSC) consists of a set of actors , a sequence of messages sent and received by these actors, and a sequence of conditions (or assertions ) interleaved with the message sequence. This terminology is illustrated in Figure 1.

The basic idea of the scenario-to-MSC translation can be illustrated on the following scenario, taken from the Instrume nt Cluster Specification [5]: 1. The driver switches on the car (ignition key in position ignition on). 2. The instrument cluster is turned on and stays active. 3. After the trip the driver switches off the ignition. 4. The instrument cluster stays active for 30 seconds and then turns itself off. 5. The driver leaves the car.
 A possible manual translation of this scenario to an MSC is shown in Figure 2. There are two challenge/response interactions in this MSC: The instrument cluster replies to the requests of the main controller ( X  X ar X ).

To model challenge/response patterns in MSCs translated from textual scenarios, we organize messages in a stack: If a new message m represents an answer to some previously pushed message m , m and the messages above it are popped from the stack. Otherwise, the new message m is pushed onto the stack.

We assume that the actors involved in the MSC are provided before the actual text-to-MSC translation. They can be extracted from the requirements document (cf. [4]) or listed manually. The list of actors allows us to decide, which sentences should be translated to messages, and which to asserti ons: A sentence is translated to a message, if its subject is contained in the set of actors, the sentence is in active voice and contains a grammatical object. All other sentences are translated to assertions (cf. [4]).
For the sentences translated to messages, we assume that the sentence subject is the message sender. For the message receiver there are two possibilities: if the sentence ob-ject is contained in the set of actors, the s entence object becomes the message receiver. If the sentence object is not contained in the set of actors, we have to infer the message receiver from the message stack: Let m top be the message on the top of the stack and m new the message under analysis. Then, we assume that m new is the response to m top and, therefore, the receiver of m new is the sender of m top . In a similar way, we can infer missing messages: if the sender of m new is not equal to the receiver of m top , we assume that there is a missing message m from the receiver of m top to the sender of m new . We put this missing message m on the stack too. The details of stack management are presented in [3]. 2.2 From Automata Descriptions to Automata Similarly to the text-to-MSC translation, it is possible to translate text pieces describing automata to automata themselves, as presented in [6]. The difference lies in the relation between sentences that we have to model. This can be illustrated on the specification excerpt in Table 1 (from [7]). The header and the first sentence of this excerpt set the context ( X  X ormal mode X ), and further sentences refer to this context. Thus, instead of a message stack, we have to model the context setting and the usage of the set context.
We model the context by assigning every (sub)sentence to one of the four categories:  X  X tate transition X ,  X  X ransitio n condition X ,  X  X ontext setting X  , or  X  X rrelevant X . The assign-ment of sentence segments to categories takes place in the following steps: (1) splitting of every sentence to segments, (2) assignment of segments to categories on the basis of grammatical information only, and (3) re-assignment of segments to categories, by using context information. Each of these steps is described below.
 Sentence splitting: Punctuation symbols, the words  X  X f X  and  X  X hen X  as well as the conjunctions  X  X nd X  and  X  X r X  are used as splitting marks, unless they directly follow an adjective or a number. 1 A splitting example is shown in Table 2.
 Assignment of segments to categories on the basis of grammatical information: Identification of the four segment classes is possible on the basis of the Part-of-Speech (POS) tags: A POS tagger decides, for every word, if this word is a noun, verb, adjec-tive, ...The applied tagger has the precision of a bout 97% [8] which makes it unlikely to become an error source. Furthermore, we assume that the names of the automaton states are extracted from the specification bef ore the actual text-to-automaton transla-tion, cf. [6]. The assignment of the sentence segment to one of the four classes takes place in the following way:  X  If the sentence segment does not contain any reference to a state, it is marked as  X  If the sentence segment contains a reference to a state, but first occurrence of the  X  Otherwise, the sentence segment is marked as  X  X tate transition X .
 Here it is important to emphasize that in the first phase no sentence segment is marked as  X  X ransition condition X .
 Re-assignment of segments to categories, by using context information: To t a ke context into account, it is necessary to revise th e  X  X ontext setting X -marks first. Here, the following heuristics is applied: If, for a given sentence, any of its segments is marked as  X  X tate transition X , then all segments marked as  X  X ontext setting X  are relabeled to  X  X tate transition X . This compensates for potentially missing verbs in some sentence segments. In the case of the example shown in Table 2, it marks the fourth segment as  X  X tate transition X  and leaves the other marks unchanged.

When the marking of segments as  X  X tate transition X  is finished, it is possible to iden-tify transition conditions:  X  If a sentence segment is marked as  X  X rrelevant X  and directly precedes a segment  X  If a sentence segment is marked as  X  X rrelevant X  and directly follows a segment When this relabeling process is finished, tra nsitions are created from the sentence seg-ments marked as  X  X tate transition X . Trans ition conditions, as well as source and target state of transitions, are inferred from the adjacent  X  X ontext setting X  and  X  X ransition con-dition X  segments.

The presented approach is domain-independent and only relies on a special writing style with guidelines for industrial requirements specifications: The complete set of system states is known, sentences describing state transitions contain a reference to the target state, and context setting is stated explicitly (for details, see [6]). The translation approaches presented in Section 2 use different inference rules in order to complete information not explicitly stated in the text. The main idea of feedback gen-eration is to turn off the inference, and, wh enever the inference would become neces-sary, to generate feedback questions addressing the missing information. For MSCs, this missing information can be the unspecified m essage receiver or an unspecified interme-diate message that is inferred by the means of the stack model. Feedback generation for MSCs is presented in Section 3.1. For automata, the missing information is always the source state of a state transition (due to peculiarities of the algorithm presented in Section 2.2). Feedback generation for automata is presented in Section 3.2.
In general, the generated questions can refe r either to the specification text or to the extracted model. Reference to the model means that, in addition to the actual models, special markers are generated that pinpoint th e problematic model elements. Reference to the text means that hints in natural language are generated, in the following form:  X  Problematic sentence: sentence cited  X  Detected problem: problem description Both for automata and for MSCs, we implemented the generation of both representa-tions. 3.1 Feedback Generation for MSCs The algorithm for MSC generation presented in Section 2.1 can infer unspecified mes-sage receivers and whole missing message s. Correspondingly, the feedback questions that are generated for MSCs, address (1) unspecified message receivers or (2) messages that are necessary from the point of view of a continuous message flow, but not explic-itly specified in the document.

Every type of the feedback question (addressing the missing receiver or the missing messages) can refer both to the model and to the specification text. Reference to the text means that we generate questions or hints in natural language, addressing the detected problem. Reference to the model means that we use special markers in the generated MSCs in order to address the detected problems.

Missing message receivers result from active sentences only, as passive sentences are translated to MSC assertions, cf. [4]. Furthermore, an active sentence needs a gram-matical object to be a candidate for an MSC message. Nevertheless, if the sentence object is not contained in the set of potential MSC actors, inference rules sketched in Section 2.1 must be applied. Thus, in order to address missing message receivers, we generate schematic hints referring to the specification text, structured as follows:
The inference of missing messages becomes necessary when the sender (basically, grammatical subject) of the sentence under consideration does not coincide with the receiver of the last message. In this cas e we generate coherence questions:
Additionally to the two above question types, based completely on the inference rules presented in Section 2.1, we address a further problem, resulting from passive sentences: whenever we come across a passive sentence, we check if the subject of the sentence coincides with the receiver of the last message. If this is not the case, we generate the following question: As an example, the set of questions addressing the problems of the scenario on page 95 is presented in Table 3.

For the questions referring to the model, we further develop the idea shown in Fig-ure 2: there, the inferred missing message wa s represented as a  X ? X -marked message. To generate the same types of questions as ab ove, we introduce a special actor named  X ??? X  and send all the messages where the message receiver is not explicitly specified, to the  X ??? X -actor. Similarly, every missing messa ge is  X ? X -marked. Missing messages in the sense of Section 2.1 are split into two: a message from the inferred message sender to the  X ??? X -actor, and a message from the  X ??? X -actor to the inferred receiver.
The set of questions generated for the example on page 95 is shown in Figure 3. It is easy to see the correspondence to the questions referring to the specification: Question 1 from Table 3 is represented by two  X ? X -messages before the first assertions, Question 2a is represented by two messages between the assertions, and Question 2b by the  X ? X -messages after the second assertion. 3.2 Feedback Generation for Automata The algorithm for automata generation presented in Section 2.2 infers, if necessary, the source state of the transition from the discourse context. It is easy to turn this inference specified, we can generate a corresponding question.

To generate questions referring to the specification text, we create schematic hints structured as follows:  X  Sentence: sentence cited  X  Unspecified source state: transition transition condition to state target state For the specification excerpt shown in Table 1, this results in the set of questions pre-sented in Table 4.

To generate questions referring to the mode l, we use the same representation as used in [6] for generated automata. In [6], the gen erated automata are represented as a three-column table: Every line of the table represents exactly one state transition: it contains the source and the target state, and the tr ansition condition. The generated questions are represented in a similar table, with the only difference that the states that are not explicitly specified in the text are marked as  X  X nknown X . For the specification excerpt shown in Table 1, this results in the set of questions presented in Table 5. The presented approach to question generation was evaluated in an experiment. The goal of the experiment was to see, if the generated questions address genuine specifi-cation problems that would be overseen by human analysts. The experiment setting is presented in Section 4.1. In the experiment, it was found that the generated questions can address genuine specification problem s, not detected by human analysts. The re-sults of the experiment are presented in Section 4.2. It was found, furthermore, that the problems not addressed by the generated que stions can be detected by other techniques. The lessons learned in the experiment are presented in Section 4.3.
 4.1 Experiment Setting In order to evaluate the generated feedback questions, the following evaluation hypoth-esis was put forward: H : The automatically generated questions addr ess deficiencies of the specification that
In total, 9 PhD candidates/postdocs in computer science participated in the evalua-tion. The evaluation took place in the following way: 1. First, the experiment subjects were given the chapter of the Steam Boiler Specifica-2. Then, every subject was given a set of questions generated as presented in Sec-3. Similarly to step 1, every subject was given 15 (out of 41) scenarios from the In-4. Analogously to step 2, every subject was given a set of generated questions and In order to address different representations of the generated questions, the subjects were separated in two groups: One group was given questions referring to the specifi-cation text for the Steam Boiler and questions referring to the model for the Instrument Cluster. The other group, inversely, was given questions referring to the model for the Steam Boiler and questions referring to the specification text for the Instrument Cluster. Due to this setting, we avoid two potential threats to validity of the results:  X  Every subject was given one set of questions referring to the model and one set of  X  Every subject was given one set of questions about the Steam Boiler Specification, The evaluation was performed in groups, but the subjects were not allowed to commu-nicate with each other. In total, every subject spent approximately two hours performing the above steps 1-4. 4.2 Experiment Results The results of the experiment are presented in Table 6. The numbers in the cells repre-sent the number of manually marked problems or generated/confirmed questions. The number of questions generated for the Steam Boiler Specification (automaton-based) coincide for the questions referring to the model and to the text, as the differences in the representations are not too big. For the Instrument Cluster (MSCs), however, differ-ences in representations result in the diffe rent number of generated questions (cf. Sec-tion 3.1).

Questions generated for the Steam Boiler Specification (automaton-based) were con-sidered by the most subjects as spurious. With one notable exception, most subjects found that the generated questions do not address genuine problems of the specifica-tion. Thus, the experiment hypothesis has to be rejected for the automaton-based speci-fication. For the Instrument Cluster Specification (MSCs), however, the subjects found that many more generated questions address genuine problems of the specification text. The fraction of questions addressing genuine problems (column  X  X enerated confirmed  X  of Table 6) varies from 2% (1 out of 50) to 84% (73 out of 87). It looks like the number of generated questions addr essing genuine problems is higher for the questions refer-ring to the model than for the questions refe rring to the text: For the questions referring to the text, at most 31 out 50 questions (62%) are found to address genuine problems. For the questions referring to the model, though, this rate varies from 49% (43 out of 87) to 84% (73 out of 87). The size of the sample, though, is too small to claim that questions referring to the model better h elp to detect specification problems. The most interesting finding of the experiment is presented in the last two columns of Table 6: the manually found problems and the problems addressed by the automatically generated questions are almost always disjoint. The figures in both columns coincide, except for one line. This line results from the following situation: the subject marked two sentences as problematic. The tool ma rked the same two sentences as problematic too. However, the subject did not confirm the generated questions as addressing gen-uine problems. The most probable reason is that the subject saw different problems in these sentences than the tool. However, as the subject did not explicitly write down the identified problems, this remains solely our interpretation of the discrepancy.
Two last columns of Table 6, together with the number of generated questions that the subjects found to address genuine specification problems, allow us to claim that the hypothesis is confirmed for the Instrument Cluster Specification, namely, that the automatically generated questions address d eficiencies of the sp ecification that would be overseen by human analysts. 4.3 Lessons Learned The specification problems that were det ected by our subjects were highly different from the problems addressed by the automati cally generated questions. The problems most often marked by the human analysts were pertinent to unclear phrasing. The auto-matically generated questions, to the contra ry, addressed missing specification pieces. For example, our subjects marked following phrases as problematic:  X  For the Steam Boiler Specification:  X  For the Instrument Cluster Specification: These phrases are indeed problematic if we want to build a system model, but they represent fuzzy specification, not completely missing facts.

In order to address such fuzzy phrasings, it makes sense to apply the previously de-veloped tool that detects poor phrasings [9]. In its current version, the tool detects poor phrasings listed in the Ambiguity Handbook [10]. Adding further patterns for prob-lematic phrases, however, is a matter of minor extension of the configuration files. An integrated tool, detecting both missing speci fication pieces (as in the presented paper) and poor phrasings, would provide a reliable means of specification analysis. Work related to the presented paper can be subdivided in two areas: work on text-based modeling and work on natural language proce ssing (NLP) in requirements engineering. Both areas are presented below. 5.1 Text-Based Modeling Saeki at al. [11], Overmyer et al. [12], and Ermagan et al. [13] introduced tools provid-ing modeling approaches. The approach by Saeki et al. allows the user to mark words in the requirements documents, and then to assign the marked word to some noun or verb type. Then, the approach maps nouns to classes (in the sense of object oriented design), and verbs to operations. The approach can handle four predefined verb classes.
Overmyer et al. developed a tool allowing the user to mark words or word sequences and map them to classes, roles, and operations. As opposed to the approach by Saeki et al., they do not assume that the verb must fall into one of the four predefined categories.
Ermagan et al. developed the tool SODA, allowing to link textual use cases to behav-ior models. However, SODA sticks to manual modeling and does not provide automatic extraction of model elements.

Our approach has the important advantage that it does not assume the textual doc-ument to contain sufficient information to generate models. Thus, it can cope with in-complete documents and make the incompleteness apparent. 5.2 Natural Language Processi ng in Requirements Engineering There are three areas where natural language processing is applied to requirements engi-neering: assessment of document quality, identification and classification of application specific concepts, and analysis of system behavior.
 Approaches to the assessment of documen t quality were introduced, for example, by Rupp [14], Fabbrini et al. [15], Kamsties e t al. [16], and Chantree et al. [17]. These approaches define writing guidelines and measure document quality by the degree to which the document satisfies the guidelines. These approaches have a different focus from our work: their aim is to detect poor phrasing and to improve it, they do not target system modeling, and do not generate any mode l-related feedback, as our approach does.
Another class of approaches, as, for example, those by Goldin and Berry [18], Ab-bott [19], or Sawyer et al. [20] analyzes the requirements documents, extracts applica-tion-specific concepts, and provides an initial static model of the application domain. However, these approaches do not generate feedback addressing specification incom-pleteness either.

The approaches analyzing system behavior translate requirements documents to ex-ecutable models by analyzing linguistic patterns. Vadera and Meziane [21] propose a procedure to translate certain linguistic patterns into first order logic and then to the specification language VDM, but they do not provide automation for this procedure. Gervasi and Zowghi [22] go further and introduce a restricted language, a subset of English. They automatically translate textual requirements written in this restricted lan-guage to first order logic. Similarly, Breaux et al. [23] introduce a restricted language and translate this language to description logic. Avrunin et al. [24] translate natural lan-guage to temporal logic. Our work goes further than the above approaches, as we not only translate textual descriptions to models, but also use the resulting models to make the deficiencies of the textu al specification apparent.

To summarize, to the best of our knowledge, there is no approach to documents analysis, yet, able not only to translate model descriptions to models themselves, but also to generate feedback concerni ng the deficiencies of the document. Even though many formal and semi-formal specification techniques exist, requirements specifications in natural language remain a de-facto standard. Apart from being readable by all stakeholders, specifications in natural language entail a lot of problems with poor phrasings, inconsistencies, and missing information.

The approach presented in our paper analyzes missing information in behavior speci-fications and automatically generates ques tions addressing these findings. As evaluated in our experiment, the generated questions can address specification deficiencies that would otherwise be overseen by human analysts. Thus, the presented method should be one of the means to ensure the quality of requirements specifications.
 We want to thank the participants of the experiment: Mou Dongyue, Florian H  X  olzl, Maged Khalil, Alexander Krauss, Christian Leuxner, Daniel M  X  endez Fern  X  andez, David Trachtenherz, and Andreas Vogelsang.

