 Deguang Kong doogkong@gmail.com Chris Ding chqding@uta.edu Heng Huang heng@uta.edu Feiping Nie feipingnie@gmail.com Recently, there have been many algorithms pro-posed for nonlinear dimension reduction, which include Isomap ( Tenenbaum et al. , 2000 ), lo-cally linear embedding (LLE) ( Roweis &amp; Saul , 2000 ), kernel-LLE ( Ham et al. , 2004 ), Hessian LLE ( Donoho &amp; Grimes , 2003 ), local tangent align-ment ( Zhang &amp; Zha , 2004 ), Laplacian embedding ( Hall , 1971 ; Belkin &amp; Niyogi , 2001 ), and many varia-tions. Above dimension reduction algorithms usually cover two main steps: (A) for each data point, learn the local geometry information W . This W can be viewed as similarity between data points or the edge weights of a graph whose nodes are the data points. We call this W -learning, or learning the graph weights. (B) Using the learned W to embed the high-dimensional data points into a lower-dimensional space Y . We call this Y -learning, or learn the embedding. The performance of those algorithms are determined both by learning the local information and also by constructing the mapping relations. In past decades, many clustering algorithms have been proposed such as K-means, spectral cluster-ing and its variants ( Ng et al. , 2001 ), normalized cut ( Shi &amp; Malik , 1997 ), ratio cut ( Chan et al. , 1994 ), etc. Among them, the use of manifold information in graph cuts has shown the state-of-the-art clustering performance.
 One key observation is that both LLE and spectral clustering utilize the data manifold information. This motivates us to investigate deeper relations between the LLE Y -learning and spectral clustering in terms of Laplacian embedding (because the embedding is precisely the relaxed cluster indicators for the spec-tral clustering). Indeed, we discover that a properly modified formulation of Y -learning provides a solution which is identical to the normalized Laplacian embed-ding (see  X  2.3). We incorporate this improvement into our final iterative LLE algorithm.
 Another observation is that the data geometry infor-mation encoded in W also plays a central role in the performance of these algorithms. We investigate the W -learning process and propose a nonnegative, ker-nelized, sparse W -learning algorithm (see  X  4). Furthermore, we propose to iteratively repeat the two mains steps ( W -learning and Y -learning) to improve the results progressively. Here we use the learned em-bedding Y to augment the input data to learn a bet-ter W , which leads to a better Y in turn. This is repeated until the process converges (details are given in  X  3). This iterative procedure incorporates both the improved Y learning and the improved W learning into a coherent iterative LLE algorithm.
 The experiment results for clustering and semi-supervised learning tasks on 9 datasets show clear per-formance improvements.
 2.1. Brief overview of LLE LLE ( Roweis &amp; Saul , 2000 ) is a nonlinear dimension reduction approach. Suppose data X = [ x 1 , x 2 ,  X   X  dimensionality p . LLE expects each data point and its neighbors to lie on or close to a locally linear manifold, which governs how the weight coefficients W are constructed from Eq.( 1 ). It then reconstruct each data point (low k -dimensional embedding vectors { y i } ) from its neighbors via the same neighborhood re-lations by minimizing a quadratic cost function Eq.( 2 ), where weight W ij summarizes the contribution of the j th data point to the construction of i th data point. N is the kNN neighborhood of x i . The shift invariance of P 2.2. LLE Improvements in two directions In this paper, we propose improved formulations in both main steps in LLE. (A) In the W -learning step of Eq.(1), we propose new improved formulations to learn W . We first make W nonnegative in this section. We will further propose a kernelized sparse learning in  X  4. (B) In the Y -learning step of Eq.(2), we propose slightly modified formulation and prove that the solu-tion to Eq.(2) is identical to Normalized Cut or Lapla-cian embedding. Our iterative LLE algorithm is based on these improved formations in both LLE steps. To make a connection to graph embedding, we (1) re-strict W to be nonnegative, i.e., we add constraint W  X  0 to Eq.( 1 ) (as done in ( Wang &amp; Zhang , 2006 )); (2) we symmetrize W to obtain Z = 1 2 ( W + W T ) as the graph edge weight/similarity matrix; (3) we im-pose D -orthogonal constraint on Y , i.e., YDY T = I , where D = diag( Z e ) is a diagonal matrix containing node degrees.
 With these three changes, the LLE equations of Eqs.( 1 , 2 ) become where d i = D ii .
 We note several important changes here. In Eq.( 4 ), D  X  1 is inserted for two important reasons: (1) Note bors. Thus Eq.( 4 ) enforces the smoothness of function { y i } . (2) It also enforces the shift invariance of ob-tained Y , because P j ( D  X  1 Z ) ij = 1. This implies that a constant vector. Note that we add d i as the weight of each point y i , for reasons immediately clear below. 2.3. LLE Y-learning is identical to Normalized Now we show that LLE Y -learning formulation of Eq.( 4 ) is identical to normalized cut.
 In fact, this is a general result, not restricted to LLE. It holds for any symmetric nonnegative graph similarity function Z . More precisely we have theorem ( 1 ), Theorem 1. For any symmetric nonnegative graph similarity function Z of the formulation of Eq.( 4 ), the optimal solution of Y is identical to the optimal so-lution H of normalized cut spectral clustering, given graph weight matrix Z .
 In the following, we first briefly introduce normalized cut and present the proof of Theorem 1.
 Normalized Cut .
 Normalized cut ( Shi &amp; Malik , 1997 ) is an effective graph partitioning (clustering) technique to identify clusters inherent in the data, given the pairwise simi-larity matrix Z . It is well-known now multi-way nor-malized cut can be solved by the following problem, where  X  Z = D  X  1 2 ZD  X  1 2 . and G = [ g 1 , g 2 ,  X   X   X  , g relaxed cluster indicators. The optimal solution for G is the smallest k eigenvectors from ( I  X   X  Z ), i.e.,
The cluster indicator H = [ h 1 , h 2 ,  X   X   X  , h k ] is Relation to Laplacian Embedding .
 It is easy to see that H T = V  X  [ v 1 ,  X   X   X  , v n ] is identi-cal to the solution of This Laplacian embedding with degree normalization VDV T = I k is effective for clustering problems be-cause the embedding coordinates are the continuous relaxation of the cluster indicators of the multi-way normalized cut spectral clustering. Similarly, Lapla-cian embedding using coordinates with standard nor-malization VV T = I k is precisely the continuous re-laxation of the cluster indicators of multi-way ratio cut spectral clustering ( Chan et al. , 1994 ); The widely used linear embedding, Principal component analysis (PCA) is precisely the continuous relaxation of the cluster indicators of the multi-way K-means cluster-ing ( Zha et al. , 2001 ; Ding &amp; He , 2004 ). Theorem 1 can be equivalently expressed for Laplacian embedding. 2.4. Proof of Theorem 1 To prove the theorem 1 , we need Lemma 1 .
 Lemma 1. The optimal solution to Eq.( 4 ) is, vectors of ( I  X   X  Z ) 2 ,  X  Z = D  X  1 2 ZD  X  1 2 , i.e., Proof of Lemma 1 .
 Proof. Note Y = [ y 1 , y 2 ,  X   X   X  , y n ]  X  &lt; k  X  n . Let and then  X  Y = [  X y 1 ,  X y 2 ,  X   X   X  ,  X y n ]  X  &lt; k  X  Y = Y  X  YZD  X  1 . Now Eq.( 4 ) can be written as
Thus Eq.(4) becomes
To optimize Eq.( 12 ) is equivalent to optimize, k eigenvectors from ( I  X   X  Z ) 2 , i.e., Eq.(9). Thus the Proof of Theorem 1 .
 Proof. Because ( I  X   X  Z ) is semi-definite positive, the eigenvectors g k of Eq.( 6 ) can be uniquely mapped to eigenvectors g k of Comparing Eq.(6) of normalized cut against Eq.( 10 ) of LLE, one can see f i = g i ,  X  2 i =  X  i , F = G . Compared Eq.( 7 ) of normalized cut against Eq.( 9 ) of LLE, one can see H = Y T . This completes the proof. We now use the above results, coupled with two new schemes(A,B) to derive a new learning algorithm. 3.1. Motivation of iterative LLE (A) Iterative process of LLE In LLE, starting from X , we learn W , and then learn Y as the low-dimensional embedding of data X . In this paper, we propose to use Y as the new data and iterate this process to further improve the embedding. The key observation is that the class structure of the data is more clear in Y than in X (this is the original embedding purpose of LLE). Thus we use Y as the new data and repeat this process to learn an improved Y . (B) Kernel generalization From experiments on several datasets, the results of using linear formulation on X for learning W in Eq.( 1 ) are generally not as good as other state-of-art meth-ods. Here we use the kernel trick to generalize this to arbitrary nonlinear similarity function. We re-write Eq.( 1 ) as where  X  ( x i ) is a mapping to a higher dimensional space. The important thing here is that the exact form of the mapping function is not needed; only the inner product K ij =  X   X  ( x i ) ,  X  ( x j )  X  is needed. Using matrix notation, the LLE of Eq.( 1 ) can be writ-ten as min as This is useful, because once we compute Y from Eq.( 4 ), we can build a kernel from Y and substitute it into Eq.( 16 ) to learn a new W (and thus Z ). 3.2. Proposed algorithm By incorporating the above schemes of (A,B), we out-line our iterative LLE learning algorithm as follows. (1)Given kernel K t , solve for W t with Eq.( 16 ) or Eq.( 18 ) 1 . (2)Given pairwise similarity W t , solve for Y t using Lemma 1. either as the final result of our algorithm (both em-bedding Y t and kernel K t +1 ) or as input to step (1). Details of K t +1 construction is given in  X  3.3. Initially K 1 is obtained from data X , we repeat above 3 steps for serval iterations to obtain a better kernel. See Algorithm 1 for more details. Note in step(1), we have two alternatives to compute W t . Thus we have two versions of iterative LLE -one based on simply iterating LLE process, and the other based on learning a sparse kernel using algorithm of Eq.( 21 ) in  X  4. Discussion Here we did not give the global conver-gence proof of this iterative LLE algorithm. The algo-rithm is very intuitive and natural. It is motivated by a simple observation: class structure is more clear in embedding Y than in original data X . 3.3. Construction of the new kernel In step (3) of our algorithm, once the low-dimensional embedding Y t is obtained, we have the following choices. (a) Construct a new kernel from Y t . There are many way to construct kernel. One possible approach is to construct the kernel K Y by simply using the Gaussian parameter. Another way is to construct new kernel K Y as the linear kernel in low-dimensional space, i.e., K (b) Construct the kernel K t +1 either as the final result of our algorithm or as input to step (1). There are combination of K t Y and the previous kernel K t . There e.g., if C = A B , then C ij = A ij  X  B ij .
 In choice (b1), we simply ignore the previous kernel and set the new kernel K t +1 = K t Y . Note both additive and multiplicative operations in choices (b2) ensure the new kernel K t +1 is also semi-definite positive(s.d.p) if the original kernel K t is s.d.p.
 Discussion In our experiments, we tried different choices. We find the results obtained from (b2) are generally better than (b1), and the multiplicative com-bination usually achieves better results than additive combination. Thus in our experiment we use (b2) with multiplicative combination to construct the new kernel in step 3.
 Algorithm 1 Iterative LLE algorithm(ILLE) Here we propose an improvement to the W -learning step of LLE. So far for LLE of Eq.( 1 ) and the new kernel version of Eq.( 16 ), we maintain the original LLE convention that W preserves the k NN structure, i.e. W ij 6 = 0 for only j  X  N i (kNN of object i ). This constraint is too strong for constructing the data similarity matrix W . Thus, in our approach, we relax this to let W ij be nonzero even if j 6 X  N i . In other words, we bypass kNN entirely.
 We now present a new approach to learn the pairwise similarity matrix S  X  &lt; n  X  n , where S ij represents the i -th data X  X  contribution to reconstruct data point x j . We hope the newly learned S has much clear structure. We use the symbol S to emphasize that W is learned using the new approach. Our objective function for learning S is, where  X  and  X  are regularization parameters, || S || 1 , 1 = P ij | S ij | . The first term k X  X  XS k 2 = P tion error from the original data. The second term penalizes the complexity of S . The third term of L 1 norm is to promote the sparsity of the solution. Using mapping  X  : X  X   X  ( X ) to map data X to a higher dimensional space in kernel machine. Eq.( 17 ) becomes which is equivalent to, Eq.( 19 ) is identical to Eq.( 17 ) when K = X T X . Eq.( 19 ) is a convex optimization problem and S has a unique global solution. Furthermore, Eq.( 19 ) can be written as where E is a matrix of all ones. Because K is s.d.p., by adding  X  I with  X  &gt; 0, ( K +  X  I ) is a well-conditioned matrix. It can be solved efficiently (see below). Usu-ally L 1 norm term is difficult to handle. Here, however, it does not add any difficulty when handled together with the nonnegativity constraint. The L 1 term can be ignored entirely: || S || 1 , 1 = Tr( ES ). 4.1. Computational algorithm for Eq.( 18 ) Here we present an efficient algorithm to solve Eq.( 18 ) and prove its convergence rigorously.
 The algorithm starts with an initial guess of S = E ( E is a matrix of all ones), iteratively updates S according to This algorithm converges very fast. The computa-tional algorithm for Eq.( 18 ) is very simple and can be efficiently implemented. 4.2. Convergence of Updating rule of Eq.( 21 ) We have Theorem( 2 ) to prove the convergence of the algorithm when K is non-negative.
 Theorem 2. Updating S using the rule of Eq.( 21 ), the objective function of Eq.( 18 ) monotonically decreases. The proof of this theorem is lengthy and is similar to that in ( Ding et al. , 2010 ; Kong et al. , 2011 ). We therefore skip the proof in this paper. 4.3. Correctness of Updating Rule of Eq.( 21 ) We prove that the converged solution satisfies the Karush-Kuhn-Tucker condition of the constrained op-timization theory. We have Theorem 3 to prove it. Theorem 3. At convergence, the converged solution S of the updating rule of Eq.( 21 ) satisfies the KKT condition of the optimization theory.
 Proof. The KKT condition for S with constraints S ij  X  0 is The derivative of J ( S )(Eq. 18 ) is  X  X  ( S )  X  S (  X  2 K + 2 K S + 2  X  S +  X  E ) ij . Thus the KKT condi-tion for S is We perform the proposed algorithms on nie datasets. We do both semi-supervised learning and clustering on these datasets. We evaluate the proposed itera-tive LLE learning algorithm(  X  3) and sparse similarity learning algorithm(  X  4), and then show the embedding results from our approach.
 Dataset These data sets come from a wide range of domains, including three face datasets AT&amp;T, umist and yale ( Georghiades et al. , 2001 ), two digit datasets mnist ( Lecun et al. , 1998 ) and binalpha 1 , two im-age scene datasets Caltec101(Caltec) ( Dueck &amp; Frey , 2007 ) and MSRC ( Lee &amp; Grauman , 2009 ), and two text datasets Newsgroup 2 , Reuters 3 . Table 1 summa-rizes the characteristics of them. We show both the iterative LLE (algorithm 1 in  X  3) and the sparse similarity learning algorithm (  X  4) re-sults. Given original kernel K 0 , S is obtained from 1-time running of sparse similarity learning algorithm in  X  4. Then we obtain the final embedding results Y after repeating out iterative LLE algorithm for 4 times. For step 1 of algorithm 1(  X  3), we use kernel constructed from Eq.( 18 ) for the subsequent iterations. For step 3 of algorithm 1(  X  3), given current embedding Y t , we obtain the new kernel K t +1 using choice (b2) with mul-tiplicative combination in every iteration. 5.1. Clustering Results We use clustering algorithms to evaluate the learned Y in LLE. We compare three standard clustering algo-rithms: (1) normalized cut, which in the context of our iterative LLE, is simply K-means clustering on learned embedding Y ; (2) spectral clustering ( Ng et al. , 2001 ), which is K-means clustering on embedding Y normal-ized onto unit sphere. (3) symmetric NMF, which runs on the learned W in iterative LLE. All of results are the averages of 10 K-means clustering with random starts.
 We use accuracy, normalized mutual information (NMI) and purity as the measurement of the cluster-ing qualities and the results are shown in Table 2 . We show the clustering results obtained from using (1) the original/input kernel ( K 0 ), (2) LLE1: results on learned Y after 1 LLE iteration. (3) LLE4: results on learned Y after 4 LLE iterations.
 For image datasets, we use gaussian kernel K 0 ij = e We tune the graph construction parameter  X  to obtain the best results from kernel K 0 . From Table 2, we ob-serve that LLE1 and LLE4 consistently achieve better clustering results, as compared to the results obtained from original kernel K 0 .
 5.2. Semi-supervised learning results We use K 0 , LLE1 and LLE4 results (learned W ) as the input to run three semi-supervised methods: harmonic function( Zhu et al. , 2003 ), lo-cal and global consistency( Zhou et al. , 2004 ), green X  X  function( Ding et al. , 2007 ). We compare the classifi-cation accuracy of above three methods by using origi-nal kernel( K 0 ) and the results obtained from LLE1 and LLE4 on 9 data sets. For all the methods and datasets, we randomly select 10%, 20% of labeled data for each class, and use the rest as unlabeled data. We do 10 fold and 5 fold cross validation, respectively. Finally, we re-port the average of the semi-supervised classification accuracy in Table 3 . In all cases, we obtain higher clas-sification accuracy by applying iterative LLE learning algorithm (shown as LLE4 and LLE1). 5.3. Demonstration of embedding results We demonstrate the advantages of iterative LLE learn-ing algorithm (  X  3) and sparse similarity learning algo-rithm (  X  4) using two-dimensional visualization. We randomly select four digits from MNIST dataset ( X 0 X ,  X 3 X ,  X 6 X ,  X 9 X ). Given Gaussian Kernel as the input, the iterative LLE algorithm (  X  3) and sparse similar-ity learning algorithm (  X  4) are run. The other pa-rameters are set as mentioned before. The embed-ding results obtained from original Gaussian Kernel K 0 , 4-time running of iterative LLE learning algorithm (LLE4) and 1-time running of W -learning algorithm (LLE1) are shown in Figs.( 1(a) , 1(c) , 1(b) ). In original results from Gaussian Kernel, all images from different groups collapse together. For the results obtained from LLE4 and LLE1, the images from different groups are balanced and distributed more evenly. This indicates much better embedding results.
 Insights from experiment results . Overall, from initial/input kernel K 0 to LLE1, LLE4, both cluster-ing and semi-supervised learning results consistently improved. Comparing results obtained between LLE1 and initial/input kernel K 0 , the performance boost is from the learned W using the algorithm of  X  4. Com-paring results obtained between LLE1 and LLE4, the performance boost is from the iterative learning of LLE. From the statistics shown in Tables 2 , 3 , we ob-serve that the boost from LLE1 to LLE4 is usually higher than that from K 0 to LLE1, indicating that the iterative aspect contributes more. In summary, the main contribution of our paper is in three-fold. (1) We show that an improved Y -learning formulation of LLE is identical to normal-ized cut spectral clustering. (2) We present an im-proved W -learning algorithm that learns a nonnega-tive, sparse pairwise similarity from an input kernel function. (3) An iterative procedure of the above two steps is proposed to progressively refine/improve the solution. Experiments show that the iterative LLE in-corporating (1,2,3) leads to better clustering and semi-supervised learning results.
 Acknowledgments . This work is supported partially by NSF-CCF-0939187, NSF-CCF-0917274, NSF-DMS-0915228.

