 Bursty features in text streams are very useful in many text mining applications. Most existing studies detect bursty fea-tures based purely on term frequency changes without taking into account the semantic contexts of terms, and as a result the detected bursty features may not always be interesting or easy to interpret. In this paper we propose to model the contexts of bursty features using a language modeling approach. We then propose a novel topic diversity-based metric using the context models to find newsworthy bursty features. We also propose to use the context models to au-tomatically assign meaningful tags to bursty features. Using a large corpus of a stream of news articles, we quantitatively show that the proposed context language models for bursty features can effectively help rank bursty features based on their newsworthiness and to assign meaningful tags to an-notate bursty features.
 H.3.3 [ Information Search and Retrieval ]: Text Mining Algorithms, Experimentation bursty features, bursty features ranking, bursty feature tag-ging, context modeling
Pioneered by Kleinberg X  X  seminal work on bursty struc-tures in streams [5], a particularly important problem in temporal text mining is to find unusual surges of activities related to an event from text streams such as one X  X  incom-ing emails [5] or a search engine X  X  query logs [8]. These bursts are usually identified by tracing the time series of the frequencies of single terms or phrases [5, 8, 3, 4]. Follow-ing [3], we use bursty feature to refer to a sudden surge of the frequency of a single term or phrase in a text stream, and represent a bursty feature by the term or phrase itself together with the time interval during which the burst takes place. For example, ( Olympic , Aug-08-2008 , Aug-24-2008 ) can be regarded as a bursty feature. We also call the term or phrase in a bursty feature its bursty term .

Many effective techniques have been proposed for bursty feature detection, such as the moving average-based method in [8], the two-state automaton model in [5], and the parameter-free methods in [3] and [6]. What these techniques share in common is that they identify bursty features purely based on the frequency changes in the time series of the term or phrase under consideration. However, they do not try to capture the semantic meanings of these bursty features. In particular, they do not take into account the semantic con-text in which a burst of a term or phrase happens.
As a result, there are several limitations with the kind of bursty features identified by these methods: 1) A bursty feature identified purely through frequency changes may not correspond to an interesting or newsworthy event. For ex-ample, there may be a peak of the word Wednesday on ev-ery Wednesday, but it is clearly not an interesting burst. Including such bursty features may hurt the performance of downstream text mining applications such as event de-tection. 2) Current representation of bursty features is too simple. Although currently bursty features are usually used as input to downstream applications such as event detection and document search, in some situations we may also want to directly display bursty features to the end-users. For ex-ample, at microblogging sites such as Twitter, to capture the most recent trends in the online social space, we may want to constantly display and update a list of buzz words, which can be thought of as the the most recent bursty terms from the microblogging streams. However, existing work simply represents a bursty feature as a single term or phrase and its bursty interval. There is no additional semantic annota-tion attached to it, which makes it hard for an end-user to interpret and understand a bursty feature. 3) Downstream applications may not be able to make full use of the input bursty features if no context information is provided. An example is event detection through clustering of bursty fea-tures [3, 4]. In these methods, correlated bursty features are grouped together to represent an event. However, using only bursty terms to describe an event may not be sufficient to capture the background and context of the event. For ex-ample, the bursty terms bird and flu may be clustered to represent an event [3], but other closely related terms such as China and chicken are also important but unlikely to be included in the bursty feature cluster simply because they may not be detected as bursty terms.

To summarize, there is a lack of consideration of context in current bursty feature detection methods. We argue that it is important to take account of the context in order to address the limitations above. We therefore propose a lan-guage modeling approach to context modeling for bursty features, and show how the context models can be used for two novel tasks, namely, bursty feature ranking and bursty feature tagging .
In this paper, we define a bursty feature f as a triplet ( w; t s ; t e ), where w is the bursty term and t s and t start and end timestamps of the bursty interval. Conceptu-ally, a context is defined as X  X he situation within which some-thing exists or happens, and that can help explain it. X  1 a bursty feature, intuitively its context should help explain the background and details of the event this bursty feature is related to. Formally, we define the context language model f of a bursty feature f to be a multinomial distribution over the vocabulary V . In other words, for v 2V , f;v is the probability of generating word v from this context language model, and
We use the batch mode two-state automaton method from [5] for bursty feature detection. In this model, a stream of doc-uments containing a term v are assumed to be generated from a two-state automaton with a low frequency state q 0 and a high frequency state q 1 . Each state has its own emis-sion rate, and there is a probability for changing state. If an interval of high states appears in the optimal state se-quence of some term, this term together with this interval is detected as a bursty feature, and the weight of a bursty feature f = ( w; t s ; t e ) is defined as where ( ; r t ; s t ) is the cost function of the t  X  X h batch 0 and 1 represent the low and the high states respectively. This weight can be seen as an indicator of the strength of burstiness for this bursty feature. In Section 5.2, we will use this weight to rank bursty features as a baseline.
Given a bursty feature f , we assume that we have some text that can provide the context for f . We represent this text as a bag of words C f = f w f 1 ; w f 2 ; : : : ; w f that C f is generated from f .
 The simplest way to generate C f is to assume that words from C f are generated only from f . Based on maximum likelihood estimation, we have http://dictionary.cambridge.org/dictionary/british/context 1
The t  X  X h batch contain r t relevant documents out of a total of s t documents. One problem with this generative model is that it assumes all words are generated by a single underlying context language model, even though stop words and some other words may not be representative of the context for the bursty feature. Intuitively, the text that provides contextual information for a bursty feature can be decomposed into two main parts: contextual words closely related to the bursty feature and general background words. So we argue that a more reason-able generative model is to assume that a word in C f is gen-erated either from f or from a general background model . This kind of two-mixture models have been shown to be effective in pseudo relevance feedback for information re-trieval [9]. Formally, with the two-mixture model, we have where is the probability that a word in C f is generated from the general background model and c ( v; C f ) be the count of word v in C f .

We propose to use different background models for differ-ent time periods in order to adapt to the dynamic nature of text streams. Here we use the documents during the bursty interval of f to estimate the background model f B for f . Let B f denote the bag of words from all documents within the bursty interval of f (including those that do not contain the bursty term of f ). Let c ( v; B f ) denote the count of word v in B f . The estimation formula for f B is as follows:
After deriving f B for f and fixing , we can use the ex-pectation maximization (EM) algorithm [2] to estimate the context language model for f . The updating formulas of the E-step and the M-step are shown below: To select the context, we consider two options: 1) Document-level context , to use the words from all the documents within the bursty interval [ t s ; t e ] that also contain the bursty term w as C f . 2) Sentence-level context , to use the words from all the sentences within the bursty interval that also contain the bursty term as C f . To estimate the context language model for a bursty feature, we have the following four variations: SM-sen : Using the simple context gen-eration model with sentence-level context; SM-doc : Using the simple context generation model with document-level context; TM-sen : Using the two-mixture context genera-tion model with sentence-level context; TM-doc : Using the two-mixture context generation model with document-level context.
In this section, we make use of the context language mod-els of bursty features proposed in the previous section to-gether with topic analysis for bursty feature ranking. In-tuitively, a bursty feature is interesting to a general audi-ence if it is related to a specific event that does not happen very often, e.g. tsunami . On the other hand, bursty terms such as Wednesday and October are not interesting because their semantic contexts are likely to contain various topics and events that can generally happen on other days of a week and in other months of a year. In the context of topic analysis, an uninteresting bursty feature such as Wednesday is therefore likely to have a diverse distribution of topics. To capture this intuition, we can define a topic diversity measure for bursty features and then rank bursty features in increasing order of their topic diversity. The top-ranked ones are then likely to be interesting or newsworthy bursty features.

Formally, following a standard topic modeling approach, let E denote a set of topics, where each topic e is repre-sented as a multinomial word distribution over the vocabu-lary, denoted as e . Such topics can be obtained by applying standard LDA [1]. We first define a normalized similarity measure between a bursty feature f and a topic e as follows: Here div( f jj e ) denotes the KL-divergence between two word distributions f and e , and f is the context language model for the bursty feature f .

Since this similarity measure is normalized over all topics, we can also think of it as the probability of a topic given a bursty feature f , that is, we can define
Finally, we define the topic diversity (TopicDiv) of a bursty feature f to be the entropy of this topic distribution:
Our hypothesis is that a bursty feature with a small TopicDiv measure is likely to be related to a newsworthy event and therefore of high quality. As we will show in Section 5, this topic diversity measure can indeed help boost more interest-ing bursty features to the top of the list.
In this section we consider another task based on context language models: bursty feature tagging. In this tagging task, we consider the whole vocabulary V as our candidate tag set. Given a bursty feature f together with its estimated context language model f , we formulate the automatic tag-ging problem as a term ranking problem where we want to rank all terms in V and select the top-K terms as tags for f . Note that this tagging problem is different from the part-of-speech tagging problem in NLP. It is similar to social tagging in Web 2.0 except that it is done automatically.
 A na  X   X vely method is to rank candidate tags (all terms in based on their probabilities in the context language model of f , namely, to rank tags in decreasing order of the following scoring function for a candidate tag v given f : If we treat the context language model of a bursty feature as a topic, then the bursty feature tagging problem is simi-lar to the problem of labeling topics in topic modeling. We can therefore apply an existing method in [7] that has been shown to be effective in labeling topics using pointwise mu-tual information. Formally, we can rank a candidate tag v given f according to the expectation of the pointwise mu-tual information between v and a term v  X  under the context language model f of f , as defined below: where B f is background for f within its bursty interval as defined in Section 2.
We crawled all the articles from the news archives at http://english.peopledaily.com.cn in a time span of two and a half years from January 6, 2005 to July 1, 2007. In to-tal we obtained 179,672 articles. After pre-processing (stem-ming and stopword removal), we obtained a vocabulary of 30,718 terms. We applied the two-state automaton method described in [5] to obtain a list of bursty features. Articles from the same day were grouped into one batch. On average, there are about 200 articles per day. Parameter setting fol-lows [5]. We obtained 12,115 bursty features of which there are 1,024 unique bursty terms in total.
As a first step of our ranking method, we need to get a set of topics E from the whole text collection. We used the popu-lar open source topic modeling package GibbsLDA++ 3 . We selected top-200 bursty features discovered by the method in [5] without duplicate bursty terms. We asked two human judges to separately judge the interestingness or newswor-thiness of the 200 bursty features. A third human judge would give the final decision when there was disagreement. In the end we obtained 156 newsworthy bursty features and 44 noisy bursty features. For comparison, we consider a baseline method that ranks bursty features by their bursti-ness weights as defined in Equation (1).

To measure the ranking performance, we use two met-rics: P@k , defined as the percentage of bursty features that are labeled as newsworthy among the top-k bursty features ranked by a method; #IP , defined as the number of bursty feature pairs that are inversely ordered by this method, i.e. when a noisy bursty feature has been ranked higher than a newsworthy one. http://gibbslda.sourceforge.net Table 1: Comparison of the methods using #IP .
 Table 2: Comparison of the performances using P@k .
In Table 1 we show the #IP of the different methods and in Table 2 we show the P@k of the baseline and our TM-doc method for a range of k . In these results, we set the number of topics of LDA to 150, and for the two-mixture models we fixed = 0 : 3. We can see from Table 1 that there is clearly a big gap between the baseline method and our methods in terms of #IP . Our methods have much fewer inverse pairs, which shows that our ranking can push the interesting or newsworthy bursty features to the top of the ranked list. It also shows that the two-mixture model generally performs better than the simple model, and document-level context performs better than sentence-level context. Overall, TM-doc is the best among the four variations of our method. In Table 2, we can see that in terms of P@k , TM-doc also sig-nificantly outperforms the baseline. In particular, precision remains close to 1 up to at least P@100 for TM-doc , but P@25 is below 0.5 for the baseline.
In this section, we turn to the second task of automat-ically tagging bursty features and report our quantitative evaluation results. Similar to the ranking task, we also need to create a test set for the tagging task. We randomly se-lected 50 newsworthy bursty features from Section 5.2. We adopted the pooling strategy commonly used in information retrieval. For each bursty feature, we applied the different variations of our method and took the union of the top-50 tags assigned by the different variations. The human judges were asked to assign a relevance score to each tag given the same information of the bursty feature as they had in Sec-tion 5.2. We used three levels of relevance and their scores, namely, 2 for relevant , 1 for marginally relevant and 0 for non-relevant . We thus obtained a test set of 50 bursty fea-tures with their gold standard tags and corresponding scores.
The tagging task aims to generate interpretable, compre-hensive and discriminative tags. For each bursty feature, a tagging method returns a ranked list of tags, and the quality of the top few tags is the most important. It is similar to Web search where the relevance of the top few search results is the most important. We therefore use a modified version of the popular nDCG measure as our evaluation metric. In particular, given a bursty feature f and its ranked list of tags given by method M , we define our nDCG@k as follows: where M f;i is the i  X  X h tag for bursty feature f given by method M , score( ) returns the relevance score of a tag as defined above, and iDCG@k ( f ) is the maximum DCG score at k for f assuming an ideal ranking.
 Table 3: Performance of bursty feature tagging.

Given a set of bursty features F , we define nDCG@k ( M ) to be the average of nDCG@k ( f; M ) over all f 2F and use this as the overall performance measure for method M . We consider k = 5 and k = 10.

We show the basic results in Table 3. We set = 0 : 9 in the two-mixture models in this case because tagging task needs a more discriminative context language model to generates discriminative tags. We can see from the table that overall we achieved good nDCG@5 and nDCG@10 values. Among the four variations, again TM-doc performed the best among the four variations, confirming that document-level context coupled with two-mixture model gives the best context lan-guage model for a bursty feature for the purpose of tagging. We can also see that while the PMI-based tagging method consistently improved the performance for SM-sen , TM-sen and SM-doc , for TM-doc , it did not show any advantage. It suggests that the context language model estimated by TM-doc can already provide a good keyword representation of the bursty feature.
 The authors Xin Zhao, Hongfei Yan and Xiaoming Li are partially supported by NSFC under the grant No. 70903008 and 60933004, CNGI grant No. 2008-122, 863 Program No. 2009AA01Z143, and the Open Fund of the State Key Lab-oratory of Software Development Environment under Grant No. SKLSDE-2010KF-03, Beihang University. [1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [2] A. P. Dempster, N. M. Laird, and D. B. Rubin. [3] G. P. C. Fung, J. X. Yu, P. S. Yu, and H. Lu. [4] Q. He, K. Chang, and E.-P. Lim. Analyzing feature [5] J. Kleinberg. Bursty and hierarchical structure in [6] T. Lappas, B. Arai, M. Platakis, D. Kotsakos, and [7] Q. Mei, X. Shen, and C. Zhai. Automatic labeling of [8] M. Vlachos, C. Meek, Z. Vagena, and D. Gunopulos. [9] C. Zhai and J. Lafferty. Model-based feedback in the
