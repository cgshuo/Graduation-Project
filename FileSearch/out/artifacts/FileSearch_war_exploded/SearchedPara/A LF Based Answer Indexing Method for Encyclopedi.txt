 Question-Answering(QA) processing has been attracting a great deal of attention recently[1]. Many researches on question answering have been carried out to make up for the weak points in IR systems.[2,3]. We have implemented AnyQues-tion2.0(http://anyQ.etri.re.kr) which is a encyclopedia question-answering system following AnyQuestion1.0 in 2003[4]. AnyQuestion1.0 is a question-answering sys-encyclopedia domain. The method for finding answers is 3-step answer process(which is the combination of IE-supported QA technique and passage extraction method). However, it takes long time to extract an answer in AnyQuestion1.0 on account of passage extraction method. 
In AnyQuestion2.0, we adopt answer indexing method instead of passage extrac-tion method to solve this problem and improve IE-supported QA method. Moreover, we extend category from only person to entire domain of encyclopedia and add up on the question answering technique using answer indexing method among tech-niques of AnyQuestion2.0. This paper is structured as follows: In the second section, we review the related works. Section 3 des cribes answer indexing technique and sec-tion 4 explains our answer processing method. Results of our evaluation test are pre-sented in section 5. Finally, we draw our conclusions. In current techniques for QA, there are IE(Information Extraction)-supported QA technique[5], passage extraction method based on IR system[6], the technique of answer indexing[7,8], and so on. [5] is a typical IE-supported QA system. They de-fine templates about each entity(for example: person entity) and then fill up template values using IE techniques. In a closed domain such as encyclopedia texts or newspa-pers, IE-supported method is successful[6,9] but is an impractical solution in open-domain due to the dependency of IE systems on domain knowledge. Therefore, pas-sage extraction methods have been the most commonly used ones by many QA sys-tems. In the passage extraction methods, sentences or passages which are the most relevant to the question are extracted and then answers are retrieved by using lexico-syntactic information or NLP techniques[6]. However, it takes a long time to extract an answer in these QA systems because rules should be applied to each sentence in-cluding answer candidates on the retrieval time[7]. To overcome this problem, [7,8] uses a method for indexing answer candidates in advance. In [7,8], they define answer types(they called QA token or semantic category) and identify answer candidates in a text and then index them. Especially, [7] uses a predictive answer indexer based on 2-pass scoring method. However, [7,8] uses not high-level information(grammatical role or dependency structure etc.) but low-level information like the term frequencies and the distances. For the indexing module, we rely on natural language processing techniques including morphological process, word sense disambiguation, answer type tagging (similar to the extended named entity recognizer) and syntactic analysis. We define about 160 answer types in consideration of user X  X  asking points for finding answer candidates. They have 15 top levels and each top node consists of 2 or 4 layers. The base set of The AT-tagging engine annotates with answer types(AT) for each sentence and then the indexer generates AIU(Answer Index Unit) structures using the answer candi-dates(the AT annotated words) and the content words which can be founded within the same context boundary. We adopt LF(Logical Form) and sentence as the context boundary. 
First, the LF-based method extracts the AIU structures within the same LF rela-tions. We defined LF(logical form) as the syntactic relation between a verb and other predicate arguments in the same dependency structure[4]. (e.g. verb(x, y, z,...) ). We classify these predicate arguments and verbs into three sets; answer candidate word set(the AT annotated words), verb set and content word set(noun, compound noun, adverb, genitive phrase etc). We construct AIU structures based on the following formula and then add up LF information(S:subject, O:object, V:verb, A:adverb) for each AIU structures. 
Second, the sentence-based method constructs the AIU structures within a same sentence. In other words, we adopt AIU st ructures between answer candidate word set and content word set within a sentence boundary with the exception of same AIU structures which is extracted from the first method. In addition, we exclude verb set in this method because verbs produce side effects even if they appear with the answer candidates within the same sentence, so verb set is restricted in LF-based method. We also prevent effectively over-generation of AIU structures. We append distance in-formation between answer candidates and content words to AIU structures. Fig.1 contains as example of the data structure passed from the indexing module. The answer processing module searches the relative answer candidates from index This module is composed of question term-weighting part and answer-ranking part. potential answers. We choose essential terms among question terms according to the following equation. 
Equation 2 shows three features; title point(if question term is title of encyclope-dia), LF point(if term is LF arguments(Subject, Object, Adverb)) and AT point (if term is answer type). Especially, in case of AT point, if the term is PLO type(Person, Location, Organization), it will be added to more points than not PLO type. Essential term is a question term which has the highest scores. After selection of essential term, we assign term weight as in the following equation. After assigning term weight, we calculate the similarities between query terms and answer candidates. To compute similarities, we use AND operation of a p-Norm mode. 
Second, the input to the answer ranking part is results of relative answer candidates searched from index DB. The answer candidates are ranked accordingly to the follow-ing equation. In this system the final score is combination of first score( W i )and second ous section. Second score( Score(R i ) ) has following tree types of weight. To experiment on our system, we use ETRI QA Test Set[5] which consists of 402 pairs of question and answer in encyclopedia. Our encyclopedia currently consists of 163,535 entries, 13 main categories, and 41 sub categories in Korean. For each ques-tion, the performance score is computed as the reciprocal answer rank(RAR) of the first correct answer. To compute the overall performance of AnyQuestion2.0, we use the Mean Reciprocal Answer Rank(MRAR). We consider 5 answers in the highest ranks as the answer candidates. For this experiment, we used 402 pairs of the evalua-tion set. Table1 shows the result of AnyQuestion1.0(using passage retrieval system). The performance of the proposed method in this paper is shown in Table2. Table3 summarizes the result of another AnyQuestion2.0 except for LF based method. As shown in Table1 and Table2, the performance of AnyQuestion2.0(using LF, Table3) is similar to that of AnyQuestion1.0(using passage retrieval system, Table1). The problem of AnyQustion1.0 is that the average response time of this system ex-ceeds 5 second, while AnyQuestion2.0 takes less than the maximum 0.5 second. This result means AnyQuestion2.0 is more useful. Moreover we tested performance of LF-based indexing method. From Table2 and 3, we see that the accuracy of AnyQues-tion2.0 using LF method is higher than that of AnyQuestion2.0 excluding LF method. question-answering system. We presented a fast and effective question-answer system for encyclopedia domain. We focus on answer indexing method based on syntactic relation in this paper. For answer indexing method, we classified user X  X  asking points into 160 answer types. We explained how our system generated AIU(Answer Index Unit) structures within LF and sentence boundary in indexing process. For ranking the answer candidates, we chose essential terms using syntactic information. We have shown that our proposed method is more useful with some experiments. While these methods have improved our previous QA system, we note that more improvements may be pursed in future work. We have to construct more practical QA system not only for the encyclopedia domain but also for other domain. Further work includes the fine-tuning of current system and we plan to expand our AnyQuestion system to be able to process list type questions. 
