 Let R be a set of objects. An object o  X  R is an outlier ,ifthere exist less than k objects in R whose distances to o are at most r . The values of k , r , and the distance metric are provided by a user at the run time. The objective is to return all outliers with the smallest I/O cost.

This paper considers a generic version of the problem, where no information is available for outlier computation, except for objects X  mutual distances. We prove an upper bound for the memory con-sumption which permits the discovery of all outliers by scanning the dataset 3 times. The upper bound turns out to be extremely low in practice, e.g., less than 1% of R . Since the actual memory ca-pacity of a realistic DBMS is typically larger, we develop a novel algorithm, which integrates our theoretical findings with carefully-designed heuristics that leverage the additional memory to improve I/O efficiency. Our technique reports all outliers by scanning the dataset at most twice (in some cases, even once), and significantly outperforms the existing solutions by a factor up to an order of magnitude.
 Categories and Subject Descriptors: H3.3 [Information Storage and Retrieval]: Information Search and Retrieval General Terms: Algorithms, Experimentation Keywords: Mining, Outlier, Metric Data
Data mining aims at discovering interesting characteristics of a dataset, mainly in the forms of correlation (particularly, associa-tion rules) and clusters, in order to assist advanced decision making (e.g., classification of new objects, prediction of events). All these mining operations draw conclusions from a majority of the dataset, as in association rule mining, where a rule is useful only if it is sup-ported by a sufficiently large subset of the database. In this case,  X  X utliers X , i.e., objects differing in behavior with the majority, are harmful (and hence, must be ignored), since they may reduce the accuracy of the mined results.

Outliers, however, have their own merits, as recognized by Knorr and Ng in their pioneering paper [8]. The merits arise from the fact Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. that outliers typically indicate irregular patterns that deserve spe-cial attention. Such patterns are especially important in security systems, where, contrast to traditional mining tasks, the goal is not to understand the patterns of the  X  X ajority X , but rather, to capture abnormal  X  X inorities X . In fact, in t hese systems, the majority pat-terns may even have been obtained previously, and are taken as an input for assisting outlier mining.

Using a popular example in the outlier-analysis literature, con-sider a system that detects frauds in creditcard transactions. Over 99.9% of the transactions are ordinary, and their behavior conforms to certain patterns which have long been understood by experts. Based on such understanding, the experts hope to identify the re-maining (less than 0.1%) transactions demonstrating suspicious de-viation from normal behavior, in order to alert the company about possible investigatory actions.
Let R be a relation with n objects o 1 , o 2 , ..., o n . A user specifies a distance threshold r ,aninteger k by far smaller than n ,anda distance function d ( o i ,o j ) ( i, j  X  [1 ,n ] ). Function d a metric, i.e., it satisfies the triangle inequality.

D EFINITION 1. Given any o i , o j in R ,if d ( o i ,o j )  X  that o i is a neighbor of o j (likewise, o j is also a neighbor of o Specially, each object is a neighbor of itself.

D EFINITION 2. An outlier is an object that has less than k neighbors.
 The above  X  X istance-based X  outlier definition is proposed by Knorr and Ng [8], who provide solid justification about its use-fulness and importance in practice 1 [8, 9, 10].

We are interested in generic solutions that utilize only objects X  mutual distances, and do not place any constraint on data and the function d ( . ) , apart from the fact that d ( . ) should be a metric. Such a solution, therefore, is applicable in all outlier-detection applica-tions, regardless of the data types (pictures, movies, time series, ...) and d ( . ) (Euclidean distance, road network distance, edit distance, ...). This requirement excludes the existing solutions (surveyed in Section 5) which constrain the objects and/or d ( . ) to be in Euclid-ean space 2 , or assume the possibility of creating an index on R .
As an interesting property, the definition is compatible with the notion of  X  X are events X  in statistics. For instance, if the underlying objects are known to obey a Gaussian distribution, an object is a rare event if its value deviates from the mean of the Gaussian by more than 10 times the standard deviation. Such objects can be captured as outliers, by setting r and k appropriately (see a formula in [8])
For example, objects are multi-dimensional points, and d ( tures their L 2 -distance.

Our discussion focuses on large datasets that do not fit in mem-ory, rendering minimization of I/O cost to be a major concern in algorithm design. We do not demand any index structure on R ,but the objects should have been stored in a random order, as can be easily achieved by a simple randomization process.
Our objective is to discover all the outliers with I/O overhead linear to the database size. A similar attempt has been made by Bay and Schwabancher [3]. They show that the problem can be solved with linear CPU time. Unfortunately, their solution incurs quadratic I/O cost, as will be analyzed in the next section.
Motivated by this, we present a systematic study of the problem, and make two major contributions. First, we establish, through a probabilistic analysis based o n random sampling, an upper bound for the amount of memory required to retrieve all outliers by scan-ning the dataset 3 times. The upper bound indicates an important fact: the memory usually needs to hold only 1% of a practical dataset to achieve the 3-times-scan performance!
Since the memory capacity of a modern DBMS may be larger, as the second contribution, we develop a new algorithm, SNIF ( s can with pri oritized f lushing ), which integrates our theoretical findings with several carefully-designed heuristics that leverage the addi-tional memory to improve I/O efficiency considerably. Extensive experiments demonstrate that SNIF completes outlier mining by scanning the dataset at most twice, and sometimes, even once.
The rest of the paper is organized as follows. Section 2 dis-cusses the drawbacks of an existing method that deploys nested loop. Section 3 lays down the theoretical foundation for the pro-posed technique, based on which Section 4 explains the details of SNIF. Section 5 reviews the previous work that is related to ours. Section 6 verifies the efficiency of our method with extensive ex-periments. Section 7 concludes the paper with directions for future work. A straightforward solution to our problem is nested loop (NL). That is, for each object o  X  R , scan the database from the begin-ning, counting the number of objects within distance r from o .The scan is terminated as soon as the counter reaches k , i.e., o is not an outlier. A complete scan of the database is necessary only if o is an outlier.
 Despite the clear O ( n 2 ) complexity of the algorithm, Bay and Schwabancher [3] present a surprising, yet reasonable, result: the actual CPU time of NL is often linear to the dataset size. This phenomenon is due to the observation that, for most non-outlier objects in R , scan of the dataset terminates very early so that only a fraction of the dataset is examined.

To illustrate this specifically, denote x as the number of neigh-bors of o (i.e., x is the number of objects in R with distances at most r to o ). Remember that objects in R have been randomized, so that the next object scanned always has a probability x/n to be a neighbor of o .So,if o is not an outlier, in expectation, k/ need to be checked before k neighbors are found.

Therefore, if we assume that there are y outliers in R ,and average of x for all non-outliers, the e xpected number of scanned objects (in the entire execution of NL) equals where the term O ( n ) corresponds to the fact that the number of non-outliers is bounded by n .

The value of y is extremely small with respect to n , so the second term is roughly linear on n . In the first term, note that does not depend on n , but instead it is a constant related to the data distribution. To understand this, imagine that more objects are added to R following the same distribution; then, along with the increase of n ,thevalueof  X  x also increases, such that  X  equivalent to the probability that the distance between two objects is at most r . Hence, the first term in Formula 1 is also linear to n .
Unfortunately, the analysis of [3] fails to account for the fact that, in database environments, nested loop is performed in blocks  X  block nested loop (BNL). Assume that each disk page can ac-commodate b objects, and that the memory has m pages. Each scan of the database is performed with m  X  1 pages of objects in memory, and thus, the scan may terminate only when all the ( m  X  1)  X  b memory-resident objects have been confirmed as non-outliers. Next, we will show that, for typical values of b and m , each scan must cover a significant portion of the database, render-ing the overall I/O cost O (( n/b ) 2 ) (note that this does not con-tradict the earlier analysis, which only shows that the number of distance computations is linear to n ).

Let us use p to denote the average pr obability that a non-outlier object o  X  R can be verified without scanning 90% of the dataset. Then, in BNL, a scan does not need to examine 90% of R , only with a probability approximately p ( m  X  1)  X  b . Consider a dataset of 2D points where 0.05% of the objects are outliers. The value of p is at most 99.95%. In practice, ( m  X  1)  X  b can easily reach 10000 (e.g., m = 101, and b = 100 ) such that p ( m  X  1)  X  b evaluates to less than 1%! In other words, almost every scan must access 90% of the pages occupied by R , leading to quadratic I/O overhead.
This section justifies the possibility of discovering all outliers with I/O cost linear to n/b ,where n is the cardinality of R ,and b the number of objects in a disk page. Specifically, we show that a very small amount of memory (around 1% of the dataset) is usually sufficient for retrieving all the outliers by scanning the dataset 3 times. This motivates an algorithm presented in Section 4, which further improves performance by scanning the dataset even fewer times.
Let us randomly sample s objects from R . To allow rigorous analysis, we follow the strategy of  X  X ampling with replacement X  [12]. Specifically, each random sample is taken independently from all the objects in R , i.e., it is possible that multiple samples happen to be the same object.

We use the sample set to build s partitions PA 1 , PA 2 , ..., PA of R . Each sampled object is the centroid of a partition; hence, we denote the s samples as PA 1 .o , ..., PA s .o respectively, after the partitions they represent. Besides its centroid, a partition PA some i  X  [1 ,s ] ) includes all the objects o  X  R that satisfy two conditions: 1. the distance from o to PA i .o is no more than r/ 2 (i.e., half 2. o is nearer to PA i .o than to the centroid of any other partition
The union of the partitions may not be R , since an object is not in any partition if it is farther away from all centroids than r/
We compute a density , PA i .den , for each partition PA i i  X  s ). Specifically, PA i .den is the number of objects (including PA i .o itself) whose distances to PA i .o are at most r/ that an object contributing to the density of a partition does not necessarily belong to that partition. In particular, a single object o may contribute to the densities of multiple partitions, but o only belongs to a single partition, i.e. , the one whose centroid is the nearest to o .

All the partitions, including their centroids and densities, consti-tute a data summary of R . The next lemma show s that non-outliers may be verified directly from the data summary.

L EMMA 1. If PA i .den  X  k (for any i  X  [1 ,s ] ), none of the objects belonging to PA i can be an outlier.

P ROOF . The lemma follows the fact that function d fies the triangle inequality. Let o be an object belonging to PA and o an object that contributes to PA i .den . Thus, it holds that d ( o, P A i .o )  X  r/ 2 and d ( o ,PA i .o )  X  r/ 2 d ( o, o )  X  r . PA i .den  X  k means that there are at least k such o ; hence, o is not an outlier.

Let us divide the s partitions into two disjoint sets. The first one S good includes all the partitions whose densities are at least k . The second set S bad involves all the remaining partitions whose objects, therefore, ca nnot be asserted as non-outliers from the data summary.
 Assuming the memory has m pages, we have:
L EMMA 2. We can find all ou tliers by scanning the dataset 3 times, if m  X  1 pages can accommodate t he objects qua lifying one of these conditions: the object (i) is a partition centroid, (ii) does not belong to any partition at all, or (iii) belongs to a partition in S
P ROOF . The data summary, which includes the centroids and densities of all partitions, can be constructed by scanning R once. In particular, since objects are stored in a random order, the cen-troids can be simply set to the first s objects encountered in the scan; thus, it is not necessary to perform a separate sampling process for obtaining the centroids.

After the first scan, we identify the set S good of partitions with densities at least k . Then, we keep the partition centroids in mem-ory (but throw away partition densities), and perform a second scan over R . In this scan, a fetched object is discarded immediately if it belongs to any partition in S good (by Lemma 1). An un-discarded object is retained in memory. In this way, the amount of consumed memory gradually increases; but, given the condition in Lemma 2, this amount is expected to be less than m  X  1 pages at the end of the second scan. Keeping all the non-prunable objects in memory, we perform a third scan over R , using all the un-occupied memory pages as the input buffer (there is at least one such page), in order to determine whether each object is an outlier.

How large should m (i.e., the memory) be, in order to allow a 3-times-scan algorithm? To answer this queston, our first step is to quantify the number of objects satisfying condition (ii) of Lemma 2. For each object o i  X  R ( 1  X  i  X  n ), we use o to represent the number of objects in R (including o i itself) whose distances to o i do not exceed r/ 2 .

L EMMA 3. The expected number of objects in R that do not belong to any partition e quals
P ROOF . Since the centroids are obtained from R following the sampling-with-replacement scheme, there are totally n s centroid sets, each of which is taken with an equal probability. Let us denote them as CS 1 , CS 2 , ..., CS n s , respectively.
We construct a two-dimensional array with n rows and n columns, where the i -th ( 1  X  i  X  n ) row concerns object o R ,andthe j -th ( 1  X  j  X  n s ) column corresponds to CS j cell c ij at the i -th row and j -th column, we fill in  X 0 X  if o some partition when the set of sampled centroids is CS j ;otherwise (i.e., o i is not  X  X aptured X  by any partition), we fill in  X 1 X .
If we add up the cell-values at the j -th column, the sum, rep-resented as col j , equals the number of  X  X n-captured X  objects ac-cording to the centroid set CS j . Hence, the expected number of un-captured objects (given an arbitrary centroid set) is the average sum of all columns: 1
Note that n s j =1 col j in the above formula is exactly the number of 1 X  X  in the array. Next, we count the 1 X  X  in an alternative  X  X ow-oriented X  manner. Let row i be the number of 1 X  X  at the i -th row. Clearly, row i is the number of centroid sets that do not capture object o i . We call such a centroid set a  X  X on-capturing CS of o The distances between o i and all the centroids in a non-capturing CS are larger than r/ 2 .Since n  X  o i .n  X  r/ 2 objects in R are farther away from o than r/ 2 , every centroid in a non-capturing CS of o must originate from those n  X  o i .n  X  r/ 2 objects. Hence, there are ( n  X  o i .n  X 
Therefore, n s j =1 col j (the number of 1 X  X  in the array) equals Formula 2
Clearly, the chance that an obj ect belongs to no partition de-creases exponentially with s .Letusset s to 1000 (the centroids constitute a very small sample set of R ). As a result, for any non-outlier object o  X  R , as long as o.n  X  r/ 2 n is a non-trivial selectivity, the number of objects not captured by any partition is very small (we will demonstrate this in the next section).

Let us use n dense to denote the number of objects o  X  R whose o.n  X  n/ 2 is at least k . We arrive at a formal result regarding the memory size for fulfilling the condition in Lemma 2.

C OROLLARY 1. We expect to find all ou tliers by scanning the dataset 3 times, if s + objects can be stored in m  X  1 pages.

P ROOF . Lemma 2 says that, to achieve the designated query cost, m  X  1 pages should be sufficient for storing three types of objects (i), (ii), and (iii). The number of objects of type (i) is s ,and the number for type (ii) has been given in Lemma 3. To prove the corollary, it remains to show that the number of type (iii) objects is at most ( k  X  1)  X  s  X  1  X  n dense n in expectation.

Notice that a partition belongs to S bad if and only if its centroid is one of the n  X  n dense objects o whose o.n  X  r/ 2 is less than k . Since each centroid is randomly picked from R ,ithas 1  X  n probability to produce a  X  X ad partiti on X , or equivalently, the ex-pected number of bad partitions is s  X  1  X  n dense n . Finally, each bad partition contains at most k  X  1 objects, thus completing the proof.

The goal of this section is to identify the value of Formula 4 for practical data, since the formula indicates the minimum amount of memory for finding outliers by 3 scans of R . For this purpose, we examine the following datasets 3 popular in the literature:
In all cases, the data space is normalized such that each axis has a domain of [0, 10000]. The distance function d ( . ) corresponds to Euclidean distance.

Outlier formulation requires parameters r and k .Thevalueof k is easier to set [8]: it ranges between 0.01% and 0.1% of the dataset cardinality n . In this subsection, we fix k to the median value n . The setting of r is more difficult because an excessively small r leads to an unrealistically large number of outliers, whereas an overly-large r does not produce any outlier at all.

Therefore, for each dataset, we decide an interesting range [ r min ,r max ] of r as follows. Initially, r equals  X  , and obviously, no object qualifies as an outlier. Then, we gradually decrease r , until some object qualifies for the first time; the value of r at this point equals r max . Next, we continuously reduce r (so that the number of outliers increases), and stop as soon as there are ex-actly 0 . 1%  X  n outliers. The current value of r equals r a result, by investigating r  X  [ r min ,r max ] , we simulate the prac-tical environments [8, 10] where the number of outliers is below 0 . 1%  X  n . The interesting ranges of r for CA , Household ,and Server are [260 , 4585] , [2166 , 7201] , [1530 , 10004] , respectively.
In Figure 1a, the curve labeled with  X  Fo r m u l a 4  X  plots the value of the formula as a function of r (in its interesting range) for CA . The formula value is represented as a percentage of the dataset X  X  cardinality (e.g., 1% means 6.2k objects). Formula 4 contains 3 terms; except the first term s , the other two terms vary with r . Hence, Figure 1a also demonstrates the values of the second and third terms as a function of r , with curves labeled with  X  Fo r m u l a 2  X  CA can be downloaded at http://www.census.gov/geo/www/tiger , Server at http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html , and Household at https://www.ipums.org .
Semantics of these attributes are available here: http://kdd.ics.uci.edu/databases/kddcup99/task.html . and  X  num of bad objects  X , respectively (recall that the third term is the number of objects in partitions of S bad ). Again, all values are in percentages of the cardinality. Figures 1b and 1c illustrate the same information for Household and Server , respectively. In all figures, s equals 1000.

There are two important observations: 1. The value of Formula 4, which is the minimum memory size 2. The value of Formula 4 decreases nearly exponentially as
In fact, the above observations are general, and exist in a large number of real datasets. This is not surprising, because the value of r for meaningful outlier mining ca nnot be too small (see the values of r min in Figure 1), resulting in a non-trivial o.n  X  r/ majority of non-outlier objects o  X  R . This means (i) may easily reach a selectivity above 0.5%, rendering Formula 2 to descend to a tiny value (as mentioned in Section 3.1), and (ii) n dense is close to n (i.e., most objects o satisfy o.n  X  r/ so that there are very few partitions in S bad , leading to a small value for the third term of Formula 4. Finally, combining (i), (ii) and the fact that s accounts for a very small percentage of n ,we have shown that outlier mining with linear I/O overhead is possible, usually with a small amount of memory.
Motivated by the analysis in the last section, in the sequel, we propose a new algorithm, s can with pri oritized f lushing (SNIF), which finds all outliers by scanning R at most twice, as verified in our experiments.

The efficiency of SNIF owes to the fact that, the memory size is typically larger than the smallest size (often less than 1% of R as in Figure 1) necessary for a 3-times-scan solution. Thus, we can afford to retain more objects in memory during the first dataset scan, which allows us to claim a significant portion of R as non-outliers directly after the scan! As a result, the remaining objects that require further verification may fit in memory, so that another scan of R suffices to determine the exact outliers.

Based on this idea, SNIF deploys a novel prioritized flushing technique to minimize the chance of performing the third scan of R . Specifically, the technique associates each object with a  X  X rior-ity X , and, whenever the memory becomes full, flushes the objects with the lowest priorities. The priorities are designed in such a way that, objects in memory are those deserving  X  X ore attention X : (i) outliers, and (ii) non-outliers with relatively few n eighbors. Since a memory-resident object o is checked with every subsequently scanned object, we obtain a highly accurate count of the neighbors of o , and may be able to use it for deciding whether o is an outlier.
Next, we explain the components of SNIF in detail.
SNIF starts by reading R from the beginning, and retaining the retrieved objects in memory, until the memory becomes full for the first time  X  the critical moment of SNIF.

At this moment, we have obtained b  X  m objects, where b is the number of objects that can be accommodated in a page, and m the number of memory pages. For each of the these objects o ,we obtain a neighbor counter o.n nb , equal to the number of neighbors of o in the already-inspected part of the dataset.

Then, SNIF builds a data summary (as defined in the previous section) with respect to these b  X  m objects as follows. For each object o , the algorithm estimates the total number of its neighbors in the entire R as o.n nb  X  n b  X  m ( n is the cardinality of R ), utilizing the property that these b  X  m objects constitute a random sample set of R . From the objects with estimates at least k , we randomly sam-ple s (= 1000 in our implementation) different objects (i.e., sam-pling without replacement [12]) as the partition centroids PA ..., PA s .o . This way, none of the centroids is likely to be an outlier.
For each of the b  X  m memory-resident objects, SNIF decides the partition it bel ongs to, using the two conditions stated in Section 3.1 (some objects may not belong to any partition). Next, we set the density PA i .den of each partition ( 1  X  i  X  s ) to the number of objects (currently in memory) with no more than distance r/ PA i .o .

For each partition PA i , SNIF maintains a radius PA i .r ,which equals the maximum distance between the centroid PA i .o and any object belonging to PA i . Obviously, due to the fact that an object is assigned to PA i only if d ( o, P A i )  X  r/ 2 ,thevalueof PA never exceeds r/ 2 .

After the above operations, SNIF performs the first flushing, af-ter which only b  X  m/ 2 objects (including all the partition centroids) remain in memory. We will elaborate the details of flushing later in Section 4.3. For now, it suffices to note several facts: 1. Among the objects removed from memory, those with 2. For each object appended to the file, we keep with it, in the 3. For each partition PA i ( 1  X  i  X  s ), we record in memory 4. During the first scan of the database, the neighbor counter 5. At any time after the critical moment during the first scan, the
After the critical moment, SNIF continues to scan R . For each object o encountered, we compute the distances between o and the centroids of all partitions. For each i  X  [1 ,s ] ,if d ( r/ 2 , the density PA i .den is increased by 1. After this, the partition PA j (for some j  X  [1 ,s ] )towhich o belongs is also decided. If d ( o, P A j .o ) is larger than PA j .r (the radius of PA j ), then PA is set to d ( o, P A j .o ) . Hence, the data summary (particularly, the density and radius of each partition) is always precise with respect to the objects already scanned.

Next, we initiate the neighbor counter o.n nb of o as 0, and then compute its distance to every object o beingretainedinmemory If d ( o, o )  X  r , both o.n nb and o .n nb are increased by 1. Hence, it is clear that the longer o stays in memory, the more likely its neighbor counter can reach k , increasing the chance that it can be confirmed as a non-outlier before being removed from memory.
Now that we have calculated o.n nb using the memory-resident objects, we attempt to further increase it by incorporating the ob-jects that have been removed from the memory (i.e., either dis-carded or flushed to the verification file), using the data summary. There are two independent ways to achieve this purpose, leading to values v 1 , v 2 , both of which satisfy the lower-bound property of o.n nb (Fact 4 in Section 4.1). Naturally: Next we clarify the computation of v 1 and v 2 , respectively.
Deriving v 1 . Let v be the value of o.n nb so far (obtained with respect to only the memory-resident objects). We first set v and then inspect each partition PA i ( 1  X  i  X  s ) in turn. In case we add PA i .n removed (the number of objects in PA i from memory) to v 1 . Recall that PA i .r is the largest distance between PA i .o and any object o assigned to PA i . Hence, the validation of Inequality 6 indicates, by the triangle inequality, that d ( o, o )  X  r , implying that all the removed objects in PA neighbors of r .

Deriving v 2 . The formulation of v 2 is simpler. Specifically, if o belongs to a partition PA i (for some i  X  [1 ,s ] ), then v PA i .den , i.e., the number of objects o within distance r/ PA i .o among the objects already encountered ( o must have dis-tance at most r to o ). Note that PA i .den already includes both the objects of PA i in memory and those removed; hence, unlike v does not need to take into account v .

As the number of memory-resident objects increases, the mem-ory eventually becomes full again. When this happens, SNIF in-vokes another flushing, before resuming the scan of R . In the next section, we clarify the procedures of flushing.
Let n seen be the number of objects in R that have been scanned; these objects form a random sample set of R . From the current density PA i .den of each partition PA i , we estimate its final den-sity (after scanning the entire R )as PA i .den  X  n/n seen the estimates, we classify the partitions into two disjoint sets. The first one S good includes the potentially prunable partitions whose predicted final densities are larger than k (by Lemma 1, no object in such a partition can be an outlier). The second one S bad the remaining, potentially un-prunable, partitions.

Next, we divide the memory-resident objects (other than the partition centroids, which must stay in memory) into five disjoint types. 1. Objects whose current neighbor counters are  X  k ;
As an optimization, we perform the distance calculation only if the neighbor counter of either o or o is smaller than k . 2. Objects that are not of the previous type, and belong to a 3. Objects that are not of the previous types, and belong to a 4. Objects that are not of the previous types, do not belong to 5. Objects that are not of the previous types, do not belong to
We compute a priority for each object o as where x est is the estimated number of neighbors of o in the entire R . The second term of the above formula is a value in [0, 1), mean-ing that if an object has a smaller type-id, it has a lower priority, thus a higher chance to be eliminated from memory. Notice that the second term is only for distinguishing objects of the same type.
The priorities decided this way re flect the likelihood that objects are outliers: the smaller priority is, the less likely. To explain this, -5, respectively.

Clearly, o T 1 is definitely a non-outlier, and o T 2 most probably can be verified as a non-outlier usi ng Lemma 1, at the end of the first database scan (when the data summary about the entire R is ready). o T 3 may not be verified by Lemma 1, and thus, should have a greater priority than o T 1 and o T 2 . Nevertheless, compared to objects of types-4 and -5, o T 3 has a better chance of being in a cluster, since it belongs to a partition. As a result, o possess a lower priority than o T 4 and o T 5 . Finally, the difference between o T 4 and o T 5 is that, the neighbor counter of o cise (the distances between o T 5 and all scanned objects have been calculated) but that of o T 4 is not. Therefore, o T 5 is given a higher priority to stay in memory, so that, at the end of the first scan, we can claim it to be an outlier, if its neighbor counter is still lower than k .

Given two objects o , o of the same type, why should the one, say o , with a larger x est have a higher chance to stay in memory? This is justified by two reasons, both related to the fact that every un-scanned object has a larg er probability to be a neighbor of o than of o . The first reason is that if both o and o were kept in memory, the neighbor counter of o would increase faster, and hence, would have a better chance of being c onfirmed as a non-outlier. Second, o would be more likely to increase the neighbor counters of the subsequently scanned objects, thus increasing the probability that these objects are validated as non-outliers, too.
 After calculating the priorities of all memory-resident objects, SNIF sorts them in ascending order of the priorities, and removes the first b  X  m/ 2 objects in the sorted list. Specifically,  X  X emoving X  an object means discarding it if it is of type-1, or otherwise, append-ing it (in blocks) to the verification file together with its partition ID. Whenever an object in partition PA i (for some i  X  [1 appended, PA i .n remove is increased by 1. If the object belongs to no partition, we add 1 to n noP A removed . In fact, as will be demonstrated experimentally later, the verification file is usually empty, i.e., only type-1 objects are discarded at each flushing .

Now we clarify the computation of x est , the expected number of neighbors of o . For this purpose, we only need to maintain two ad-ditional values for o . The first one  X  1 equals the number of objects whose distances to o have been computed. The second value  X  is the number of these objects whose distances to o are at most r . Then, x est is calculated as  X  2  X  n/ X  1 . Note that  X  1 is at least b i.e., the estimation of x est isbasedonalargesampleset 6 . Specif-ically, if o was scanned before the critical moment, then  X  least b  X  m (the number of objects in memory at that moment). Oth-erwise, when o is read from the file, there must be at least b objects in memory (see Fact 5 in Section 4.1), whose distances to o are computed.

Finally, we point out that the contents of S good and S bad as the priorities of objects, may vary at different flushings, since they depend on the densities of the partitions at the time of the corresponding flushing.
After the file of R is exhausted, we have: (i) a complete data summary, (ii) a set of objects in memory, and (iii) a verification file containing objects whose qualification (being an outlier or not) could not be decided at their flushing time.

At this point, SNIF obtains sets S good , S bad , and classifies the memory-resident objects into the 5 types stated in Section 4.3. Type-1 and -2 objects are discar ded as non-outliers , and type-5 ob-jects are directly reported as outliers.

For each type-4 object o (which is usually an outlier), we attempt to verify it as follows. First, we count the number x mem bors of o among the objects currently in memory. Next, we collect the set S of partitions whose centroids have distances at most o (different o leads to different S ). No object belonging to a parti-tion outside S can have distance  X  r to o , since the radius of each partition is no more than r/ 2 . Then, o is an outlier if were removed from memory, and belong to PA i (or do not belong to any partition).

If o is indeed an outlier, in most cases we can verify it with In-equality 8 because (i) S is typically empty (an outlier tends to be faraway from all clusters), and (ii) n noP A removed is often 0. To under-stand (ii), remember that very few objects belong to no partition: the number of them is given by Formula 2, and its value is usu-ally less than 1% of n as shown in Figure 1. As long as the value does not exceed b  X  m/ 2 (the number of removed objects in each flushing), all the  X  X o-partition X  objects are necessarily retained in memory, due to their high priorities.

Provided that r is not very close to the lower end of its inter-esting range (defined in Section 3.2), Type-3 objects often do not exist (the third term of Formula 4 upper bounds the number of such objects under a very low value). Otherwise, it implies the presence of a small cluster whose number of objects is at the order of k . This is rare because the cardinality n is larger than k by orders of magnitude (more than 1000 times), and thus, the size of a cluster is not likely to be comparable to k . However, if type-3 objects do exist, there is no effective way we can verify them without another scan of R . This is reasonable because determining such objects as non-outliers demands extremel y accurate neighbor counters.
Having performed the above procedures, SNIF terminates if n and there is no type-3 object. Otherwise, we execute a verification step as follows.

We read the verification file, checking the partition-IDs of the retrieved objects. If the object belongs to a partition in S
If we use the optimization stated in Footnote 4, then we should revise the statement here: if the neighbor counter of o is less than k , x est is computed from a sample set with size at least b is discarded right away. In other words, the objects that remain in memory are those (i) in partitions whose densities are less than k , or (ii) in no partition at all. SNIF keeps scanning the verification file, until the file has been exhausted, or m  X  1 pages of mem-ory have been occupied by objects. In either case, we scan R for a second time to precisely decide whether these memory-resident objects are outliers. If the verification file has not been completely scanned (which is rare; the verification file is empty in almost all our experiments), we discard all the objects in memory, resume the scan, and repeat the above process.
Except BNL, no existing solution can solve our problem and sat-isfy the generality requirements in Section 1.1. However, for the restricted scenario where objects are multi-dimensional points with mutual distance measured by the L 2 norm, Knorr and Ng [8] de-velop an alternative method CELL 7 . Since we will compare SNIF with CELL for Euclidean datasets in the experiments, next we ex-plain the rationale of CELL.

Given a distance threshold r , CELL partitions the data space reg-ularly into a grid, where each cell is a multi-dimensional square whose diagonal has length r/ 2 . Then, CELL hashes the objects into cells, by reading and writing the dataset R once, respectively. Meanwhile, each cell is associated with a counter , equal to the number of points it covers.

Based on the grid and the counters, it is possible to quickly deter-mine some outliers and non-outliers . To illustrate this in 2D space, consider cell c in Figure 2, which shows part of a grid. The cells labeled with  X 1 X  constitute the level-1 cells with respect to c ,and those labeled with  X 2 X  the level-2 (note that the level-2 is  X  X hicker X  than level-1). CELL obtains two numbers n 1 and n 2 ,where n the total number of points in c and its level-1 cells, and n the number of points in c , its level-1, and -2 cells. It is not hard to observe that (i) if n 1  X  k , all points in c must be non-ou tliers, and (ii) if n 2 &lt;k , all points in c must be outliers. In either case, c is marked as  X  X olored X ; otherwise, c remains  X  X hite X , indicating that the identities of the points covered by c are currently unknown.
For each white cell c , CELL loads the points in it (from its hash bucket) into memory, and verify whether they are outliers, by scan-ning the data in its level-1 and -2 cells. Obviously, as long as there is available memory, multiple white cells can be processed together to improve I/O efficiency. Specially, if the points in all the white cells fit in memory (as is an assumption in [8]), CELL terminates by scanning R another time in the worst case.

The problems of CELL are two-fold. During hashing, at least one memory page must be allocated to each cell as an input buffer, so that a page of objects can be written to the hash bucket at a time. This seriously limits the range of r that can be supported. For example, for dataset CA (as shown in Figure 1a), a meaningful r
Knorr and Ng [8] also propose an index-based algorithm, which, however, is substantially slower than CELL, and hence, is omitted from our discussion. can be as small as 2.6% of a dimension. In this case, each cell in thegridhasasidelengthof 2 . 6% / (2 contains more than 11830 cells! Assuming a disk size of 1k bytes, CA (with 62k two-dimensional points) occupies around 720 pages Hence, CELL requires a memory size 16.4 times that of the dataset!
Unlike BNL and SNIF, CELL incurs a significantly larger num-ber of random accesses, due to its reliance on hashing. Specifically, every time a buffer is flushed, the disk head is forced to move from (and then back to) its original position in reading the dataset file, necessitating at least two random accesses. This problem is par-ticularly serious, if each cell X  X  input buffer has a single memory page (which, unfortunately, is usually true, as the number of cells is large). In this case, every I/O writing and most I/O reading are random.

CELL can be extended to higher dimensionalities, however, at the cost of severely aggravating the above defects. The reason is that, with the same r , the number of cells increases exponentially with the dimensionality l (each cell has a side length of r/ Furthermore, while the level-1 of a cell c still includes those cells adjacent to c , the level-2 becomes an l -dimensional  X  X ectangular ring X  with a thickness of 2 on either side of c along each dimension). As a result, as l grows, each white cell must be inspected against a higher number of hash buckets. Finally, CELL is clearly inapplicable to non-Euclidean domains, where grid partiti oning is simply undefined.

Although our algorithm SNIF is a lso based on  X  X artitions X , it significantly outperforms CELL both in applicability (SNIF can be applied as long as the distance function is a metric), and effi-ciency (SNIF is faster than CELL even in Euclidean space). This is achieved by leveraging several problem characteristics based on random sampling (see Section 3.1), and integrating these charac-teristics with prioritized flushing. In particular, unlike CELL, SNIF prunes a majority (more than 99%) of the objects directly after the first database scan, and for some datasets, even terminates right af-ter the first scan without missing any outlier.

It is worth mentioning that other definitions of outliers have also been proposed in the literature. T he earliest definition appears in statistics, where data values are known to obey a probability model, and a value is captured as an ou tlier if it should have occurred only with a very low probability [2] (cal culated from the model). John-son et al. [7] identify outliers as points on the convex hull (or, in general, the  X  X ut-most X  layers of convex hulls). Ramaswamy et al. [14] present a definition based on the distance between an object and its k -th nearest object in the dataset. Breunig et al. [4] pro-pose the concept of  X  X ocal outliers X , according to which an object is an outlier if it demonstrates behavior significantly different from the behavior of its nearby objects. This concept is extended by Jin et al. [6] to  X  X op-k local outliers X . Aggarwal and Yu [1] analyze Euclidean outliers in high-dimensional space, whereas Lazarevic and Kumar [11] approach this issue with a technique called  X  X ea-ture bagging X . Finally, Papadimitriou et al. [13] discuss outliers based on  X  X ocal correlation integrals X .
The organization of this section is as follows. First, we com-pare the proposed algorithm SNI F against BNL and CELL, de-ploying the real datasets CA , Household ,and Server described in Section 3.2 (they contain Euclidean points in a data space where all dimensions have a domain of [0, 10000]). Then, we examine the scalability of SNIF with respect to the dataset cardinality, using
For each point, 3 values must be stored, i.e., id and x-, y-coordi-nates.
 synthetic non-Euclidean data (whose generation will be clarified later). In the above experiments, the value of s (the number of centroids) for SNIF is fixed to 1000, because the behavior of our technique is not sensitive to s , as demonstrated in the last experi-ment.

The default memory size equals 10% of the space occupied by the underlying database. The default k is 0.05% of the dataset car-dinality n . As discussed in Section 3.1, for any k , there exists an  X  X nteresting range X  of r such that, as r distributes in the range, the number of outliers is between 1 and 0 . 1%  X  n .Wedefinethe median of r , also the default of r , as the value of this parameter when the number of outliers equals exactly 0 . 05%  X  n (as with the interesting range, the median of r also depends on k ). For k =0 . 05%  X  the default values of r are 375, 4200, 1688 for CA , Household ,and Server , respectively. We measure the performance of each method by its I/O cost, including the time of both random and sequential accesses. The disk page size equals 1024 bytes.

Performance vs. r . The first experiment inspects the efficiency of SNIF, BNL, and CELL with respect to the distance threshold r . For this purpose, we set k and the memory size to their default values, and measure the cost of all algorithms at 7 values of r that evenly partition the interesting range of each dataset.

Figures 3a-3c illustrate the cost as a function of r . There is no result of CELL for some experiments on CA , Household ,andall experiments on Server because, in these experiments, the memory requirement of CELL exceeds 10% of the database (e.g., for CA and r = 260, CELL requires memory 16 times larger than the data-base, as explained in Section 5). Each percentage along the curves indicates the percentage of random-access cost in the overall over-head. We will use the same style to illustrate the cost fraction of random I/Os in the following diagrams.

SNIF outperforms its competitors significantly, especially when r is small (i.e., more outliers are retrieved). Our technique termi-nates by scanning the dataset at most twice. In particular, for CA , when r is different from the lower end of its interesting range, SNIF returns all outliers by performing a single scan, as indicated by its cost decrease in Figure 3a. Furthermore, as analyzed in Section 5, most I/O accesses by CELL (up to 93%) are random, whereas SNIF and BNL perform (almost) only sequential I/Os.

The verification file produced by SNIF after the first scan is empty in the above experiments, i.e., all the objects removed from memory during prioritized flushing have neighbor counters (since this is true for most of the subsequent experiments, we will explicitly discuss the size of the verification file, only if it is not zero). Equivalently, the objects that are verified in the second scan are retained in memory at the end of the first scan. Figure 3d demonstrates the number of such objects (in percentages of the cor-responding dataset cardinality) in t he experiments of Figures 3a-3c. Observe that, in all cases, SNIF prunes at least 99% of a dataset af-ter the first scan .

Performance vs. k . Figures 4a-4c evaluate the efficiency of al-ternative solutions when k distributes from 0.01% to 0.1% of the cardinality, using default values for r and the memory size. CELL is applicable only to Household , again due to its excessively large memory consumption for the other datasets. The performance of all methods remains stable for the entire range of k tested. This phe-nomenon implies that each algorithm incurs similar cost as long as the number of fetched outliers is the same (remember that, for a median r , the number of outliers is always 0.05% of the cardinal-ity). Similar to Figure 3d, we present in Figure 4d the number of objects verified by SNIF in the second scan, confirming the obser-vation that the number is less than 1% of the dataset cardinality.
Performance vs. memory size. Next, we study the impact of memory size on the efficiency of outlier mining. Towards this, we use the default values for both k and r , but measure the perfor-mance of all algorithms, as the amount of memory changes from 1% to 20% of the database. The results are illustrated in Figure 5. CELL is inapplicable to CA and Server , and applicable to House-hold only if the memory accounts for at least 5% of the database. There is no result of CA at memory size 1%, because in this case the memory contains less than 10k bytes, which is unrealistically small.

It is clear that SNIF is by far the best method, if memory is scarce. Particularly, for Household and Server ,SNIFisfasterthan BNL by a factor over an order of magnitude at memory size 1%. The cost of our method is the same regardless of the memory ca-pacity, with one exception: Server and 1% memory. In this case, the verification file is not empty, but contains 16k objects, i.e., 3.2% of the database. As a result, SNIF incurs additional cost for scan-ning the file, and accordingly, the cost percentage of random I/Os increases, because random accesses must be performed whenever objects are appended to the verification file.

Performance vs. cardinality. To test the scalability of our so-lution with respect to the dataset size, we create several synthetic non-Euclidean Signature datasets with various cardinalities. Each object in Signature is a string containing 30 English letters. First, 50  X  X ivot X  strings are randomly generated. Each pivot defines a cluster, in which an object is obtained by modifying a number x of letters in the pivot, where x uniformly distributes in [1, 10]. We continuously generate objects this way (randomly picking a pivot for each object), until the number of objects reaches 99.95% of the target cardinality n . Finally, the remaining 0 . 05%  X  n objects are again randomly generated, i.e., they are outliers.
 The distance metric for Signature datasets is the edit distance. We vary the cardinality from 200k to 1 million, but fix the amount of available memory to 10% of the database with the smallest car-dinality 200k. Setting r and k to their default values, Figure 6a demonstrates the cost of SNIF and BNL as the cardinality grows (CELL cannot be applied to non-Euclidean data). As expected, the I/O-time of BNL demonstrates clear quadratic behavior (confirm-ing our analysis in Section 2), whereas that of SNIF increases lin-early (always terminating after 2 scans). At the highest cardinality, SNIF again outperforms BNL by more than an order of magnitude. Figure 6b shows the number of objects verified by SNIF in the sec-ond scan.
 SNIF sensitivity to s . Finally, we examine the performance of SNIF when the number s of centroids changes. In this experiment, we set r , k , and the memory size to their default values, but vary s from 1000 to 3000. Figure 6a plots the cost of SNIF for the real datasets, and a synthetic dataset Signature with cardi nality 500k.
The behavior of our algorithm is not affected as s changes: SNIF always scans a dataset twice. To explain this, recall that the num-ber of objects that need to be verified after the first scan is given by Formula 4, and a third scan is necessary only if these objects do not fit in memory. After s has reached a reasonably large value (in-dependent of the dataset cardinality), the second and third terms of Formula 4 are already very low, such that further increasing s leads to only marginal decrease of those ter ms. Figure 6b verifies this, by showing that, for all datasets, the number of objects for verification in the second scan decreases only slightly (by less than 0.15% of the cardinality) as s grows from 1000 to 3000. The phenomenon implies that selection of s is simple in practice  X  we recommend s = 1000 .
In this paper, we developed a novel algorithm SNIF, which re-ports all outliers by scanning the dataset at most twice. Our solu-tion is general, and can be applied as long as the distance function satisfies the triangle inequality, regardless of the function itself (it can be Euclidean distance, edit distance, etc.) and the types of data (e.g., points, strings, and so on). SNIF has solid theoretical justifi-cations, and can be easily implemented in a commercial DBMS.
This work also indicates several promising directions for future investigation. The first one concerns  X  X robabilistic outliers X , where the goal is to identify objects that may be outliers with at least a certain probability. Compared to  X  X xact outliers X  (as retrieved by SNIF), deriving probabilistic results may be achieved with lower cost (e.g., we may never have to scan the database twice). An-other exciting direction is to address outlier detection on streams [5], where the objects (credit card transactions) are received by the system at a fast rate, and the objective is to catch outliers contin-uously. In this scenario, the algorithm must operate in strict time bounds, in order to avoid jamming the subsequent data traffic. ACKNOWLEDGEMENTS. This work was accomplished when Yufei Tao and Xiaokui Xiao were with the City University of Hong Kong, and were supported by CERG Grant CityU 1163/04E from the Research Grant Council of the HKSAR government. Shuigeng Zhou was supported by grants numbered 60373019 and 90612007 from the National Natural Science Foundation of China, and the Shuguang Scholar Program of Shanghai Municipal Educa-tion Committee. We would like to thank the anonymous reviewers for their insightful comments.

