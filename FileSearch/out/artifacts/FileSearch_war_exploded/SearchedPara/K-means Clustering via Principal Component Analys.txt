 Chris Ding chqding@lbl.gov Xiaofeng He xhe@lbl.gov Data analysis methods are essential for analyzing the ever-growing massive quantity of high dimensional data. On one end, cluster analysis(Duda et al., 2000; Hastie et al., 2001; Jain &amp; Dubes, 1988) attempts to pass through data quickly to gain first order knowledge by partitioning data points into disjoint groups such that data points belonging to same cluster are sim-ilar while data points belonging to different clusters are dissimilar. One of the most popular and efficient clustering methods is the K -means method (Hartigan &amp; Wang, 1979; Lloyd, 1957; MacQueen, 1967) which uses prototypes (centroids) to represent clusters by op-timizing the squared error function. (A detail account of K -means and related ISODATA methods are given in (Jain &amp; Dubes, 1988), see also (Wallace, 1989).) On the other end, high dimensional data are often transformed into lower dimensional data via the princi-pal component analysis (PCA)(Jolliffe, 2002) (or sin-gular value decomposition) where coherent patterns can be detected more clearly. Such unsupervised di-mension reduction is used in very broad areas such as meteorology, image processing, genomic analysis, and information retrieval. It is also common that PCA is used to project data to a lower dimensional sub-space and K -means is then applied in the subspace (Zha et al., 2001). In other cases, data are embedded in a low-dimensional space such as the eigenspace of the graph Laplacian, and K -means is then applied (Ng et al., 2001).
 The main basis of PCA-based dimension reduction is that PCA picks up the dimensions with the largest variances. Mathematically, this is equivalent to find-ing the best low rank approximation (in L 2 norm) of the data via the singular value decomposition (SVD) (Eckart &amp; Young, 1936). However, this noise reduction property alone is inadequate to explain the effective-ness of PCA.
 In this paper , we explore the connection between these two widely used methods. We prove that principal components are actually the continuous solution of the cluster membership indicators in the K -means cluster-ing method, i.e., the PCA dimension reduction auto-matically performs data clustering according to the K -means objective function. This provides an important justification of PCA-based data reduction.
 Our results also provide effective ways to solve the K -means clustering problem. K -means method uses K prototypes, the centroids of clusters, to characterize the data. They are determined by minimizing the sum of squared errors, where ( x 1 ,  X  X  X  , x n ) = X is the data matrix and m k = P i  X  C k x i /n k is the centroid of cluster C k and n k is the number of points in C k . Standard iterative solution to K -means suffers from a well-known problem: as iteration proceeds, the solutions are trapped in the local minima due to the greedy nature of the update algorithm (Bradley &amp; Fayyad, 1998; Grim et al., 1998; Moore, 1998).
 Some notations on PCA. X represents the original data matrix; Y = ( y 1 ,  X  X  X  , y n ), y i = x i  X   X  x , repre-sents the centered data matrix, where  X  x = P i x i /n . The covarance matrix (ignoring the factor 1 /n ) is P i ( x i  X   X  x )( x i  X   X  x ) T = Y Y and principal components v k are eigenvectors satisfy-ing:
Y Y T u k =  X  k u k , Y T Y v k =  X  k v k , v k = Y T u k These are the defining equations for the SVD of Y : Y = P k  X  k u k v T k (Golub &amp; Van Loan, 1996). Elements of v k are the projected values of data points on the principal direction u k . Consider the K = 2 case first. Let be the sum of squared distances between two clusters , C  X  . After some algebra we obtain and J where y 2 = P i y T i y i /n is a constant. Thus min( J K is equivalent to max( J D ). Furthermore, we can show Substituting Eq.(4) into Eq.(3), we see J D is always positive. We summarize these results in Theorem 2.1 . For K = 2, minimization of K -means cluster objective function J K is equivalent to maxi-mization of the distance objective J D , which is always positive.
 Remarks . (1) In J D , the first term represents average between-cluster distances which are maximized; this forces the resulting clusters as separated as possible. (2) The second and third terms represent the aver-age within-cluster distances which will be minimized; this forces the resulting clusters as compact or tight as possible. This is also evident from Eq.(2). (3) The factor n 1 n 2 encourages cluster balance. Since J D &gt; 0, max( J D ) implies maximization of n 1 n 2 , which leads to n 1 = n 2 = n/ 2.
 These remarks give some insights to the K -means clus-tering. However, the primary importance of Theorem 2.1 is that J D leads to a solution via the principal com-ponent.
 Theorem 2.2 . For K -means clustering where K = 2, the continuous solution of the cluster indicator vector is the principal component v 1 , i.e., clusters C 1 , C 2 given by The optimal value of the K -means objective satisfies the bounds Proof . Consider the squared distance matrix D = ( d ij ), where d ij = || x i  X  x j || 2 . Let the cluster indicator vector be This indicator vector satisfies the sum-to-zero and nor-malization conditions: P i q ( i ) = 0 , P i q 2 ( i ) = 1 . One can easily see that q T D q =  X  J D . If we relax the re-striction that q must take one of the two discrete val-ues, and let q take any values in [  X  1 , 1], the solution of minimization of J ( q ) = q T D q / q T q is given by the eigenvector corresponding to the lowest (largest nega-tive) eigenvalue of the equation D z =  X  z .
 A better relaxation of the discrete-valued indicator q into continuous solution is to use the centered distance matrix D , i.e, to subtract column and row means of D . Let  X  D = (  X  d ij ), where where d i . = P j d ij , d . j = P i d ij , d .. = P ij d we have q T  X  D q = q T D q =  X  J D , since the 2nd, 3rd and fore the desired cluster indicator vector is the eigen-vector corresponding to the lowest (largest negative) eigenvalue of By construction, this centered distance matrix  X  D has a nice property that each row (and column) is sum-to-zero, P i  X  d ij = 0 ,  X  j. Thus e = (1 ,  X  X  X  , 1) T is an eigenvector of  X  D with eigenvalue  X  = 0. Since all other eigenvectors of  X  D are orthogonal to e , i.e, z T e = 0, they have the sum-to-zero property, P i z ( i ) = 0, a definitive property of the initial indicator vector q . In contrast, eigenvectors of D z =  X  z do not have this property.
 With some algebra, d i . = n x 2 i + n x 2  X  2 n x T i  X  x , d 2 n 2 y 2 . Substituting into Eq.(8), we obtain Therefore, the continuous solution for cluster indicator vector is the eigenvector corresponding to the largest (positive) eigenvalue of the Gram matrix Y T Y , which by definition, is precisely the principal component v 1 . Clearly, J D &lt; 2  X  1 , where  X  1 is the principal eigenvalue of the covariance matrix. Through Eq.(2), we obtain the bound on J K .  X   X  Figure 1 illustrates how the principal component can determine the cluster memberships in K -means clus-tering. Once C 1 , C 2 are determined via the principal component according to Eq.(5), we can compute the current cluster means m k and iterate the K -means until convergence. This will bring the cluster solution to the local optimum. We will call this PCA-guided K -means clustering. Above we focus on the K = 2 case using a single indi-cator vector. Here we generalize to K &gt; 2, using K  X  1 indicator vectors.
 Regularized relaxation This general approach is first proposed in (Zha et al., 2001). Here we present a much expanded and con-sistent relaxation scheme and a connectivity analysis. First, with the help of Eq.(2), J K can be written as The first term is a constant. The second term is the sum of the K diagonal block elements of X T X ma-trix representing within-cluster (inner-product) simi-larities.
 The solution of the clustering is represented by K non-negative indicator vectors: H K = ( h 1 ,  X  X  X  , h K ), where (Without loss of generality, we index the data such that data points within each cluster are adjacent.) With this, Eq.(9) becomes There are redundancies in H K . For example, P bination of others. We remove this redundancy by (a) performing a linear transformation T into q k  X  X :
Q K = ( q 1 ,  X  X  X  , q K ) = H K T, or q  X  = X where T = ( t ij ) is a K  X  K orthonormal matrix: T T T = I , and (b) requiring that the last column of T is Therefore we always have This linear transformation is always possible (see later). For example when K = 2, we have and q 1 = p n 2 /n h 1  X  p n 1 /n h 2 , which is precisely the indicator vector of Eq.(7). This approach for K -way clustering is the generalization of K = 2 clustering in  X  2.
 The mutual orthogonality of h k , h T k h  X  =  X  k X  (  X  k X  1 if k =  X  ; 0 otherwise), implies the mutual orthogo-nality of q k , Let Q K  X  1 = ( q 1 ,  X  X  X  , q K  X  1 ), the above orthogonality relation can be represented as Now, the K -means objective can be written as J Note that J K does not distinguish the original data { x i } and the centered data { y i } . Repeating the above derivation on { y i } , we have noting that Y e = 0 because rows of Y are centered. The first term is constant. Optimization of J K be-comes subject to the constraints Eqs.(15,16), with additional constraint that q k are the linear transformations of the h k as in Eq.(12). If we relax (ignore) the last constraint, i.e., let h k to take continuous values, while still keeping constraints Eqs.(15,16), the maximization problem can be solved in closed form, with the follow-ing results: Theorem 3.1 . When optimizing the K -means objec-tive function, the continuous solutions for the trans-formed discrete cluster membership indicator vectors Q
K  X  1 are the K  X  1 principal components: Q K  X  1 = ( v 1 ,  X  X  X  , v K  X  1 ). J K satisfies the upper and lower bounds where n y 2 is the total variance and  X  k are the principal eigenvalues of the covariance matrix Y Y T .
 Note that the constraints of Eq.(16) are automatically satisfied, because e is an eigenvector of Y T Y with  X  = 0 and the orthogonality between eigenvectors as-sociated with different eigenvalues. This result is true for any K . For K = 2, it reduces to that of  X  2. The proof is a direct application of a well-known the-orem of Ky Fan (Fan, 1949) (Theorem 3.2 below) to the optimization problem Eq.(19).
 Theorem 3.2 . (Fan) Let A be a symmetric ma-trix with eigenvalues  X  1  X  X  X  X  X  X   X  n and correspond-ing eigenvectors ( v 1 ,  X  X  X  , v n ). The maximization of Tr( Q T AQ ) subject to constraints Q T Q = I K has the solution Q = ( v 1 ,  X  X  X  , v K ) R , where R is an arbitary  X  K orthonormal matrix, and max Tr( Q T AQ ) =  X  +  X  X  X  +  X  K .
 Eq.(11) is first noted in (Gordon &amp; Henderson, 1977) in slghtly different form as a referee comment and promptly dismissed. It is rediscovered in (Zha et al., 2001) where spectral relaxation technique is applied [to Eq.(11) instead of Eq.(18)], leading to K principal eigenvectors of X T X as the continuous solution. The present approach has three advantages: (a) Direct relaxation on h k in Eq.(11) is not as desir-able as relaxation on q k of Eq.(18). This is because q k satisfy sum-to-zero property of the usual PCA com-ponents while h k do not. Entries of discrete indicator vectors q k have both positive and negative values, thus closer to the continuous solution. On the other hand, entries of discrete indicator vectors h k have only one sign, while all eigenvectors (except v 1 ) of X T X have both positive and negative entries. In other words, the continuous solutions of h k will differ significantly from its discrete form, while the continuous solutions of q k will be much closer to its discrete form. (b) The present approach is consistent with both K &gt; 2 case and K = 2 case presented in  X  2 using a single indicator vector. The relaxation of Eq.(11) for K = 2 would requires two eigenvectors; that is not consistent with the single indicator vector approach in  X  2. (c) Relaxation in Eq.(11) uses the original data, X
T X , while the present approach uses centered ma-trix Y T Y . Using Y T Y makes the orthogonality condi-tions Eqs.(15, 16) consistent since e is an eigenvector of Y T Y . Also, Y T Y is closely related to covariance matrix Y Y T , a central theme in statistics. Recovering K clusters Once the K  X  1 principal components q k are computed, how to recover the non-negative cluster indicators h k , therefore the clusters themselves? Clearly, since P i q k ( i ) = 0, each principal compo-nent has many negative elements, so they differ sub-stantially from the non-negative cluster indicators h k . Thus the key is to compute the orthonormal transfor-mation T in Eq.(12).
 A
K  X  K orthonormal transformation is equivalent to a rotation in K -dimensional space; there are K 2 ele-ments with K ( K +1) / 2 constraints (Goldstein, 1980); the remaining K ( K  X  1) / 2 degrees of freedom are most conveniently represented by Euler angles. For K = 2 a single rotation  X  specifies the transformation; for K = 3 three Euler angles (  X ,  X ,  X  ) determine the rota-tion, etc. In K -means problem, we require that the last column of T have the special form of Eq.(13); there-fore, the true degree of freedom is F K = K ( K  X  1) / 2  X  1 . For K = 2, F K = 0 and the solution is fixed; it is given in Eq.(14). For K = 3, F K = 2 and we need to search through a 2-D space to find the best solu-tion, i.e., to find the T matrix that will transform q k to non-negative indicator vectors h k .
 Using Euler angles to specify the orthogonal rotation in high dimensional space K &gt; 3 with the special constraint is often complicated. This problem can be solved via the following representation. Given arbi-trary K ( K  X  1) / 2 positive numbers  X  ij that sum-to-of freedom is K ( K  X  1) / 2  X  1, same as the degree of freedom in our problem. We form the following K  X  K matrix: where It can be shown that 2 P ij x i  X   X  ij x j = P ij  X  ij ( x for any x = ( x 1 ,  X  X  X  , x K ) T . Thus the symmetric matrix  X  is semi-positive definite.  X  has K eigen-vectors of real values with non-negative eigenvalues: ( z 1 ,  X  X  X  , z K ) = Z , where  X  z k =  X  k z k . Clearly, t n Eq.(13) is an eigenvector of  X  with eigenvalue  X  = 0. Other K  X  1 eigenvectors are mutually orthonormal, Z T Z = I . Under general conditions, Z is non-singular and Z  X  1 = Z T . Thus Z is the desired orthonormal transformation T in Eq.(12). Summerizing these re-sults, we have Theorem 3.3 . The linear transformation T of Eq.(12) is formed by the K eigenvectors of  X  specified by Eq.(21).
 This result indicates the K -means clustering is reduced to an optimization problem with K ( K  X  1) / 2  X  1 pa-rameters.
 PCA Connectivity Analysis Generally, computing the transformation T is difficult; so is computing h k . Here we propose a method that bypasses T . Instead of recovering H K from principal components q k as in previous section, we attempt to recover H K H T K . From Eq.(12), we have where q k , k = 1 ,  X  X  X  , K  X  1 are those discrete valued indicators in Eq.(12). Here the exact form of T is not needed. Now these q k are replaced by their continuous solutions, i.e., the principal components v k . Once they are computed, we can easily form where we have ignored the constant term q K q T K = Note that P = H K H T K = P k h k h T k has a clear diagonal block structure, which leads naturally to a connectiv-ity interpretation: if p ij &gt; 0 then x i , x j are in the same cluster, we say they are connected. We further associate a probability for the connectivity between i, j onal block structure is the characteristic structure for clusters.
 If the data has clear cluster structure, we expect P has similar diagonal block structure, plus some noise, due to the fact that principal components are approx-imations of the discrete valued indicators. For exam-ple, P could contain negative elements. Thus we set p ij = 0 if r ij &lt; 0. Also, elements in P with small positive values indicate weak, possibly spurious, con-nectivities, which should be suppressed. We set where 0 &lt;  X  &lt; 1 and we chose  X  = 0 . 5. In standard PCA expansion of Y T Y = P k  X  k v k v T k , if we truncate at K  X  1 terms and set eigenvalues as unit, we recover the connectivity matrix P . For this reason, we call this PCA connectivity analysis. Gene expressions 4029 gene expressions of 96 tissue samples on hu-man lymphoma is obtained by Alizadeh et al.(Alizadeh et al., 2000). Using biological and clinic expertise, Al-izadeh et al classify the tissue samples into 9 classes as shown in Figure 2. Because of the large number of classes and also highly uneven number of samples in each classes (46, 2, 2, 10, 6, 6, 9, 4, 11), it is a relatively difficult clustering problem. To reduce dimension, 200 out of 4029 genes are selected based on F -statistic for this study. We focus on 6 largest classes with at least 6 tissue samples per class to adequately represent each class; classes C2, C3, and C8 are ignored because the number of samples in these classes are too small (8 tis-sue samples total). Using PCA, we plot the samples in the first two principal components as in Fig.2. Following Theorem 3.1, the cluster structure are em-bedded in the first K  X  1 = 5 principal components. In this 5-dimensional eigenspace we perform K -means clustering. The clustering results are given in the fol-lowing confusion matrix where b k X  = number samples being clustered into class k , but actually belonging to class  X  (by human exper-tise). The clustering accuracy is Q = P k b kk /N = 0 . 875 , quite reasonable for this difficult problem. To provide an understanding of this result, we perform the PCA connectivity analysis. The cluster connec-tivity matrix P is shown in Fig.3. Clearly, the five smaller classes have strong within-cluster connectiv-ity; the largest class C 1 has substantial connectivity to other classes (those in off-diagonal elements of P ). This explains why in clustering results (first column in contingency table B ), C 1 is split into several clusters. Also, one tissue sample in C 5 has large connectivity to C 4 and is thus clustered into C 4 (last column in B ). Internet Newsgroups We apply K -means clustering on Internet news-group articles. A 20-newsgroup dataset is from www.cs.cmu.edu/afs/cs/project/theo-11/www /naive-bayes.html. Word -document matrix is first constructed. 1000 words are selected according to the mutual information between words and documents in unsupervised manner. Standard tf.idf term weight-ing is used. Each document is normalized to 1. We focus on two sets of 2-newsgroup combinations and two sets of 5-newsgroup combinations. These four newsgroup combinations are listed below: In A2 and A5, clusters overlap at medium level. In B2 and B5, clusters overlap substantially.
 To accumulate sufficient statistics, for each newsgroup combination, we generate 10 datasets, each is a ran-dom sample of documents from the newsgroups. The details are the following. For A2 and B2, each clus-ter has 100 documents randomly sampled from each newsgroup. For A5 and B5, we let cluster sizes vary to resemble more realistic datasets. For balanced case, we sample 100 documents from each newsgroup. For the unbalanced case, we select 200,140,120,100,60 doc-uments from different newsgroups. In this way, we generated a total of 60 datasets on which we perform cluster analysis: We first assess the lower bounds derived in this pa-per. For each dataset, we did 20 runs of K -means clustering, each starting from different random starts (randomly selecting data points as initial cluster cen-troids). We pick the clustering results with the lowest K -means objective function value as the final cluster-ing result. For each dataset, we also compute principal eigenvalues of the kernel matrices of X T X, Y T Y from the uncentered and centered data matrix (see  X  1). Table 1 gives the K -means objective function values and the computed lower bounds. Rows starting with Km are the J K optimal values for each data sample. Rows with P2 and P5 are lower bounds computed from Eq.(20). Rows with L2a, L2b are the lower bounds of the earlier work (Zha et al., 2001). L2a are for original data and L2b are for centered data. The last column is the averaged percentage difference between the bound and the optimal value.
 For datasets A2 and B2, the newly derived lower-bounds (rows starting with P2) are consistently closer to the optimal K -means values than previously derived bound (rows starting with L2a or L2b).
 Across all 60 random samples the newly derived lower-bound (rows starting with P2 or P5) consistently gives close lower bound of the K -means values (rows start-ing with Km). For K = 2 cases, the lower-bound is about 0.6% within the optimal K -means values. As the number of cluster increase, the lower-bound be-come less tight, but still within 1.4% of the optimal values.
 PCA-reduction and K -means Next, we apply K -means clustering in the PCA sub-space. Here we reduce the data from the original 1000 dimensions to 40, 20, 10, 6, 5 dimensions respectively. The clustering accuracy on 10 random samples of each newsgroup combination and size composition are av-eraged and the results are listed in Table 2. To see the subtle difference between centering data or not at 10, 6, 5 dimensions; results for original uncentered data are listed at left and the results for centered data are
Dim A5-B A5-U B5-B B5-U 10 0.90/0.90 0.89/0.88 0.74/0.75 0.67/0.71 20 0.89 0.90 0.74 0.72 40 0.86 0.91 0.63 0.68 1000 0.75 0.77 0.56 0.57 listed at right.
 Two observations. (1) From Table 2, it is clear that as dimensions are reduced, the results systematically and significantly improves. For example, for datasets A5-balanced, the cluster accuracy improves from 75% at 1000-dim to 91% at 5-dim. (2) For very small number of dimensions, PCA based on the centered data seem to lead to better results.
 These observations indicate PCA dimension reduction is particularly beneficial for K -means clustering. From Thereom 3.1, the eigen-space V K are the relaxed solu-tions of the transformed indicators Q K , i.e., K -means clustering in eigen-space V K are approximately equiv-alent to that in the transformed indicator space Q K . Because K -means clustering is invariant w.r.t. the or-thogonal transformation T (see Eq.12), K -means clus-tering in the Q K space is equivalent to K -means clus-tering in the unsigned indicator space H K . In H K space, all data points belonging to a cluster collapse into a single point, i.e., clusters are well separated. Hence clustering in H K space is particularly effective  X  our results provides a theoretical basis for the use of PCA dimension reduction for K -means clustering. Discussions Traditional data reduction perspective derives PCA as the best set of bilinear approximations (SVD of Y ). The new results show that principal components are continuous (relaxed) solution of the cluster member-ship indicators in K -means clustering (Theorems 2.2 and 3.1). These two views (derivations) of PCA are in fact consistent since data clustering also is a form of data reduction. Standard data reduction (SVD) happens in Euclidean space, while clustering is a data reduction to classification space (data points in same cluster are considered belonging to same class while points in different clusters are considered belonging to different classes). This is best explained by the vector quantization widely used in signal processing(Gersho &amp; Gray, 1992) where the high dimensional space of sig-nal feature vectors are divided into Voronoi cells via the K -means algorithm. Signal feature vectors are ap-proximated by the cluster centroids, the code-vectors. That PCA plays crucial roles in both types of data reduction provides a unifying theme in this direction. We thank a referee for pointing out the reference (Gor-don &amp; Henderson, 1977). This work is supported by U.S. Department of Energy, Office of Science, Office of Laboratory Policy and Infrastructure, through an LBNL LDRD, under contract DE-AC03-76SF00098.
 Alizadeh, A., Eisen, M., Davis, R., Ma, C., Lossos, I., Rosenwald, A., Boldrick, J., Sabet, H., Tran, T., Yu,
X., et al. (2000). Distinct types of diffuse large B-cell lymphoma identified by gene expression profiling. Nature , 403 , 503 X 511.
 Bradley, P., &amp; Fayyad, U. (1998). Refining initial points for k-means clustering. Proc. 15th Interna-tional Conf. on Machine Learning .
 Duda, R. O., Hart, P. E., &amp; Stork, D. G. (2000). Pat-tern classification, 2nd ed. Wiley.
 Eckart, C., &amp; Young, G. (1936). The approximation of one matrix by another of lower rank. Psychometrika , 1 , 183 X 187.
 Fan, K. (1949). On a theorem of Weyl concerning eigenvalues of linear transformations. Proc. Natl. Acad. Sci. USA , 35 , 652 X 655.
 Gersho, A., &amp; Gray, R. (1992). Vector quantization and signal compression . Kluwer.
 Goldstein, H. (1980). Classical mechanics . Addison-Wesley. 2nd edition.
 Golub, G., &amp; Van Loan, C. (1996). Matrix computa-tions, 3rd edition . Johns Hopkins, Baltimore. Gordon, A., &amp; Henderson, J. (1977). An algorithm for euclidean sum of squares classification. Biometrics , 355 X 362.
 Grim, J., Novovicova, J., Pudil, P., Somol, P., &amp; Ferri,
F. (1998). Initialization normal mixtures of densi-ties. Proc. Int X  X  Conf. Pattern Recognition (ICPR 1998) .
 Hartigan, J., &amp; Wang, M. (1979). A K -means cluster-ing algorithm. Applied Statistics , 28 , 100 X 108. Hastie, T., Tibshirani, R., &amp; Friedman, J. (2001). El-ements of statistical learning . Springer Verlag. Jain, A., &amp; Dubes, R. (1988). Algorithms for clustering data . Prentice Hall.
 Jolliffe, I. (2002). Principal component analysis . Springer. 2nd edition.
 Lloyd, S. (1957). Least squares quantization in pcm. Bell Telephone Laboratories Paper, Marray Hill . MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations. Proc. 5th Berkeley Symposium , 281 X 297.
 Moore, A. (1998). Very fast em-based mixture model clustering using multiresolution kd-trees. Proc. Neu-ral Info. Processing Systems (NIPS 1998) .
 Ng, A., Jordan, M., &amp; Weiss, Y. (2001). On spectral clustering: Analysis and an algorithm. Proc. Neural Info. Processing Systems (NIPS 2001) .
 Wallace, R. (1989). Finding natural clusters through entropy minimization. Ph.D Thesis. Carnegie-Mellon Uiversity, CS Dept.
 Zha, H., Ding, C., Gu, M., He, X., &amp; Simon, H. (2001). Spectral relaxation for K-means clustering.
Advances in Neural Information Processing Systems
