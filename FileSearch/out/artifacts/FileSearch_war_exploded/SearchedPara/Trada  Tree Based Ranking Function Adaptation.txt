 Machine Learned Ranking approaches have shown successes in web search engines. With the increasing demands on de-veloping effective ranking functions for different search do-mains, we have seen a big bottleneck, i.e., the problem of insufficient training data, which has significantly limited the fast development and deployment of machine learned rank-ing functions for different web search domains. In this paper, we propose a new approach called tree based ranking func-tion adaptation ( X  X ree adaptation X ) to address this problem. Tree adaptation assumes that ranking functions are trained with regression-tree based modeling methods, such as Gra-dient Boosting Trees. It takes such a ranking function from one domain and tunes its tree-based structure with a small amount of training data from the target domain. The unique features include (1) it can automatically identify the part of model that needs adjustment for the new domain, (2) it can appropriately weight training examples considering both lo-cal and global distributions. Experiments are performed to show that tree adaptation can provide better-quality rank-ing functions for a new domain, compared to other modeling methods.
 H.3.3 [ Information Search and Retrieval ]: Retrieval Models; I.2.6 [ Learning ]: Concept Learning Algorithms Web Search Ranking, Learning to Rank, Model Adaptation, Regression Tree
Learning to rank has been a promising method for contin-uously and efficiently improving relevance for web search. It applies novel machine learning algorithms [23, 7, 9, 21, 22, 16, 5] to a set of labeled relevance examples to learn a rank-ing function. Compared to the traditional ranking functions [2] developed in the information retrieval community, learn-ing to rank has several unique benefits: (1) it is convenient to incorporate new features to the ranking function without the need of manually tuning the function, which mainly relies on experts X  experience and heuristics; (2) although depending on the specific learning algorithms, with sufficient training data it can usually give better performance over manually tuned functions. Currently, machine learned ranking func-tions have been successfully applied to several major search engines.

Learning to rank requires sufficient amount of good-quality labeled training data. To obtain good-quality training data, we usually need trained editors (relevance experts) to judge the relevance of sampled web search results, i.e., (query, doc-ument) pairs, and cross-verify the judgments. Since this pro-cess has to be done manually, it is highly time-consuming and expensive. Although there are convenient methods to extract relevance judgments from implicit user feedbacks [16, 17], the expert-labeled data are still regarded as a more reliable source for training high-quality ranking functions. Due to the increasing demands from different web search domains, e.g., different regions or countries or topics, it has been urgent to develop effective domain-specific ranking functions and continuously improve them. However, when applying learning to rank to a new domain, we usually do not have sufficient amount of labeled training data.
One approach to addressing this problem is to utilize the training data from one major web search domain to help train a function for a new search domain. Apparently, the training data from one existing domain cannot be easily ap-plied to another domain, due to different joint feature  X  relevance distributions. Specifically, for the web search rank-ing problem, there are usually tens or hundreds of features designed for learning a ranking function. Even small distri-bution difference in each feature will aggregate to significant difference in the multidimensional feature space.

Although each search domain has its own characteristics, we observe that many of them share certain level of com-monality. In particular, we have seen that a ranking func-tion developed in one domain, though not the best function for another domain, works reasonably well crossing different domains. We name the domain having sufficient training data as the source domain , and that we do not have or have only small amount of training data as the target domain . How to adapt a good ranking function from the source do-main to a target domain and get a better ranking function, is the major problem we are going to tackle.

In this paper, we propose a tree-based ranking function adaptation approach (Trada) to address the problem of in-sufficient training data for target search domains. Although it can be applied to any regression-tree based ranking mod-els, we will use ranking functions trained with the gradient boosting trees (GBT) method [10] in this paper. Tree-based models have some known advantages over other kinds of models, such as the model interpretability. In our approach, we will also explore the structural benefit of regression tree over other models, since the tree structure is highly tun-able. Based on the characteristics of regression tree and the mechanism of training a regression tree, we design a few al-gorithms to tune the base-model trees with the small target training dataset. The tree-based adaptation algorithm has a couple of unique features: (1) it can automatically iden-tify the part of model that needs to adjust for the new do-main, (2) it can appropriately weigh training examples con-sidering both local and global distributions. By doing tree-adaptation, we can effectively tune a base model towards the domain-specific distributions indicated by the small dataset, thus incorporate the knowledge learned by the base model into the target domain. Experiments have shown that this approach is more effective than other methods.

In Section 2, we will briefly review the related work, mainly, the representative learning to rank algorithms and the model adaptation work done in other areas. In Section 3, we de-scribe some basic concepts and notations that will be used in tree adaptation. We will also analyze how a regression tree is generated, which helps understand the basic idea of tree adaptation. In Section 4, we first give intuitions how tree adaptation works and then present a few tree adapta-tion algorithms. Experimental results will be reported in Section 5 to validate the proposed algorithms and evaluate different algorithmic settings.
In recent years, several novel learning algorithms have been developed for the ranking problem. They can be roughly grouped into three categories. The first category works on training data labeled with absolute grades, typically two-level grades as  X  X elevant X  and X  X rrelevant X , or multilevel grades. Correspondingly, the learning problem is formalized as a classification problem [20] or ordinal regression problem [12, 6, 10, 23]. This category has known weaknesses since rank-ing cares about only the relative ordering between any pair of documents regarding to a specific query, rather than ac-curate prediction of categories. Therefore, the second cate-gory of algorithms proposes to take pairwise data as training data and develops pairwise ranking function. The represen-tatively algorithms includes Ranking SVM[16], RankNet[5], RankBoost[9], and GBRank[23], etc. The third category is the listwise approach, which tackles the ranking problem di-rectly by adopting listwise loss functions, or directly optimiz-ing information retrieval evaluation measures. The typical algorithms are LamdaRank[4], ListNet[7], and AdaRank[22].
Recently, model adaptation has been of great interest in other areas, in particular, natural language processing and speech recognition, for the similar training data problems we have encountered in learning to rank. The standard ap-proach is to treat the source domain data as  X  X rior knowl-edge X  and then to estimate maximum a posterior (MAP) values for the model parameters under this prior distribu-tion for the target domain. This approach has been applied successfully to language modeling [1], parsing [13] and tag-ging [3]. In speech recognition, the maximum likelihood lin-ear regression (MLLR) approach is also proposed for speaker adaptation [18]. The problem of distributional difference be-tween the source domain and the target domain is formally addressed by the paper [8], which is further decomposed as 1) the difference between the prior distribution of feature vectors and 2) the difference between the label distributions [15]. The paper [15] also used a simple data combination technique by appropriately overweighting the target domain data in training. As we will show, overweighting the en-tire target-domain data may not give satisfactory results for ranking adaptation. The challenge is to appropriately assign different weights to different examples in terms of the dis-tributional difference and importance. In contrast, our tree adaptation technique can automatically adapt to the fine-grained distribution difference between the source domain and the target domain.
Tree adaptation follows the general GBT training frame-work, while the major algorithms are more related to the mechanism of generating regression tree. In order to design effective adaptation algorithms on trees, we need to under-stand the structure of a regression tree and how the boosted trees are generated. In this section, we will first give the def-inition of training data for the ranking problem, and then briefly describe how a regression tree is generated, which is the main component of GBT. This section will also setup the notations used later in this paper.
In learning to rank approaches, the expert judged results (query, document, grade) are transformed to training ex-amples { ( x i ,y i ) } . Here, x i represents a feature vector de-scribing the features associated with the (query, document) pair. y i is the target value from a set of grades, e.g., a five-grade scheme, which represents different levels of relevance between the query and the document. The grade is deter-mined by the relevance expert for each (query, docuemnt) pair. The task of learning to rank is thus transformed to learning a function from the training examples { ( x i , y so that the learned function can predict the target value for any (query, document) pair if its feature vector is pro-vided. Since such a ranking function outputs a score for each (query, document) pair, we can simply sort the scores for a set of (query, document) pairs and display the sorted list of documents for the given query.

We briefly describe some of the typical features available for ranking. For each query-document pair, there are three categories of features:
With multi-grade labeled training examples, one straight-forward learning method is ordinal regression. Many algo-rithms can do this purpose and we will use gradient boosting trees in this paper for its superb modeling quality and flex-ible structure. The basic component of GBT is regression tree [11]. For better understanding of the tree adaptation algorithms, we will give sufficient details of learning a regres-sion tree in this section. Figure 1 shows a sample regression tree, which is a binary tree with one predicate at each inter-nal node. The predicate consists of a variable (feature) and a splitting value, typically in form of F&lt; X  ?. In such a tree, an internal tree node partitions the training data that reach the node into two parts, with the corresponding predicate defined in the node. The tree is grown with a top-down man-ner, i.e., starting from the root and terminating with certain satisfied condition, e.g., the fixed number of leaf nodes. In the following, we describe how the training algorithm de-cides which feature and splitting value are used for growing child nodes.

First, splitting a leaf node to grow a tree should give some kind of  X  X ain X , namely, optimizing the goal of regression, i.e., minimizing the square error between the predicted value and the target value. We assume that there are n i training records reaching the node i ,eachofwhich, x j , has a target value r ij to fit at the node i . r ij = y j if the current node is the root, otherwise, r ij is the residual by fitting the parent node. It represents how well this example is fit so far from the existing part of tree. As we have known, the best-effort predictor for all records falling onto the current node is the mean of all r ij , i.e.,  X  r i = 1 n i n i j =1 r ij [11]. With  X  r square error E for the current node is
Finding the Best Split for a Node. Let F p denote the feature and v p,q is a feasible value for F p .( F p partitions the data into two parts: those records with F p v p,q go to the left subtree and the rest records go to the right subtree. After performing this partition, similarly, we cangetthesquareerror E L for the left subtree, and E R for the right subtree. We define the gain by splitting this node as gain = E  X  E L  X  E R . By scanning through all possible features and feasible splitting values for each feature, we can find the best splitting condition, which satisfies
Finding the Best Node for Splitting With the above criterion, a greedy search procedure can be applied to deter-mine the leaf node that will bring the highest gain among all existing leaf nodes for splitting. node i for splitting = argmax i { gain i } , for all leaf nodes.
This is a hill-climbing procedure, which does not guaran-tee to get a globally optimal tree. Certainly, there are other strategies, but this one is very efficient especially when we have many features in the dataset, as the cost is linear to the number of features. In the sequel, we will use this tree grow-ing strategy by default. Figure 2 shows a perfectly fitted tree to the underlying target value distribution. To extend it to general multidimensional cases, we can understand that each node represents a  X  X ultidimensional bounding box X  de-fined by the disjointed partitioning conditions along the path from the root to that node. For example, the leaf node la-beled with R 2 in Figure 2 is defined by the bounding box F 1 &lt;a  X  F 2 &lt;b 0 .

Calculating Leaf Node Response. In the above al-gorithm, during the growing phase, the predicted value  X  r for the node i is recorded, and the residuals r ij  X   X  r used as the new target values for its child nodes. Let {  X  r node i in the path from the root to any node  X  } denote the predicted values along the path from the root to the leaf node t .Sinceeach X  r (  X  ) i fits the residual from its parent, the predicted response R  X  for the node  X  should be defined as Note that an equivalent way to computing the response is to simply find the mean of the target values for all points falling onto the leaf node. However, these two will result in quite different adaptation strategies, which will be discussed in  X  X esponse value adaptation X .
Gradient boosting trees can be used to model both clas-sification and regression problems. The boosted trees are a series of regression trees, denoted by h i ( x ). The final func-tion is based on these regression trees.
 where  X  i is the learning rate, which is often small, e.g., 0.05. A formal description of the training algorithm can be found in the literature [10]. The GBT learning method trains the k -th tree, based on the previous trees h j ,1  X  j&lt;k ,witha set of random samples from the training dataset ( Stochastic Gradient Boosting ). The steps can be briefly described as follows. 1. randomly sample the training data to get a subset of 2. set the target r i of the example in S k to the original 3. train the regression tree h k with the examples { ( x i
In this section, we first describe the basic challenge that the tree adaptation approach will address and also justify why this approach will work. Then, we will present several tree adaptation algorithms in details.
In what scenarios will domain adaptation work? To an-swer this question, we need to formally analyze the learning problem. Given a set of training examples, { x i ,y i } goal of training an effective model is to find a function ap-proximating the joint distribution p ( x ,y ). p ( x ,y )canbe decomposed into two parts, as p ( x ,y )= p ( y | x ) p ( x ). p ( x ) is the underlying unlabeled data distribution and p ( y | the label distribution. Let ( s ) denote the source domain and ( t ) the target domain. In general, the source and target domains differ in both p ( y | x )and p ( x ). Model adaptation works in the following scenarios.
Figure 3 and 4 illustrate the above scenarios. The small blocks in the figures represent the local areas in the high-dimensional space ( X  X he high-dimensional bounding boxes X ) that are covered by the training data. Different colors repre-sent different target values for the blocks. In regression tree modeling, each leaf node tries to approximately model one of these blocks, and each internal node groups a few nearby blocks with close target values.

The above illustration also shows that training with only the small amount of target training data may cause serious overfitting. A feasible solution could be combining the two sets of data or even two models. However, simply combin-ing the datasets may not work well. The major difficulty is to appropriately weight the samples from the target do-main: without sufficient understanding of the source data and the target domain, often, we can only roughly handle the weighting scheme. When inappropriately overweighting the small data from the target domain, we will get unsatis-factory models.

Tree adaptation provides us a convenient way to locate the part of the model that needs tuning, and to automatically weight different part of target data according to both source and target data distributions. As a result, tree adaptation models tend to be more robust and less possible to overfit the small data.
The basic idea of tree adaptation includes 1) using the base model to partition the new dataset, i.e., approximating  X  p ( t ) ( x )with  X  p ( s ) ( x ); 2) properly weighting the samples based on locality; 3) and finely tuning the partition based on both source and target data distributions.

The tree adaptation algorithms are closely related to the mechanism of regression tree modeling that we have dis-cussed. We can understand tree adaptation from the per-spective of multidimensional partitioning of the sample space  X  p ( x ). In a regression tree, each path from root to any node represents a multidimensional bounding box, which is a sub-space of  X  p ( x ). In particular, it is worth noting that from top down the parent bounding box also encloses the children bounding boxes, and the records falling to the same box will get the same predicted value (and response). By inherit-ing the tree structure from the base model, we try to tune the target data distribution  X  p ( t ) ( x ) based on the source data distribution  X  p ( s ) ( x ).

In tree adaptation, we will slightly tune the response (and also the boundary of the bounding box) based on the local distributions of the source and target data. This process will be done node by node, from the root to leaves. By doing so, we not only incorporate the distribution learned by the base model to the new model, but also take into consideration the subtle distributional differences represented by the target domain. Due to the complexity of the tree structure, there are probably numerous strategies for tuning the base model. According to the intensity level of changing the base model, we choose to present a few representative algorithms. The first strategy is fixing the bounding boxes and tuning responses only. This strategy and some of the later ones em-ploy the similar local-distribution-based tuning algorithm. Namely, we assume there are certain number of records, n 0 from the source domain D 0 , i.e., the training data for the base model, and n 1 from D 1 , the small training data for the target domain, falling onto the same bounding box, respec-tively. We allow the two populations to vote for the the final decision about the response. By appropriately aligning up the size difference between the two sets of data, we gener-ate the weight for each vote and then calculate the tuned response.

Concretely, we calculate the weights as follows. First, let a leaf node at the base model be associated with response R ,andthereare n 0 records from the source training data falling onto this node. Next, we apply the target domain data to the tree, by fixing the splitting condition, to get the response value R 1 . Similarly, we know n 1 target domain records falling onto that node. We assume the distribution that the two sets of points fall on this node follow a binomial distribution, and the corresponding probabilities are p 0 (1  X  p 0 ) for the source and target data, respectively. A balanced estimate of the combined value is calculated by f ( R 0 ,R 1 ,p 0 ) is used as the tuned response value for this leaf node. Now, we should estimate p 0 based on the two local sample populations on this node. Since these two original datasets have unequal size, we may need to scale up the small data with an appropriate factor  X  . Based on the sample populations and  X  ,weestimate p 0 with The appropriate  X  can be determined through cross-validation. This distribution-based estimation will also applied to bound-ary tuning later. Plugging 3 into Formula 2, we expand the parameters to f ( R 0 ,R 1 ,n 0 ,n 1 , X  ). Formula 3 says that, when n 1 n 0 , the original response is almost used as the response in the new model.

As we have mentioned, each node has a predicted value trying to fit the residual from its parent and Formula 1 cal-culates the leaf node response based on the series of residual prediction {  X  r i } on the path from root to the node. Alter-natively, we can adapt  X  r i on each node to get  X  r i .Let r and r 1 ,i be the predicted values at the node i by applying the source data and the target data, respectively. Also, let n 0 ,i , n 1 ,i be the number of source records and target records falling onto the node i , respectively. A layer-by-layer tuning strategy can be represented by Eq. 4, where the function f is the expanded form of Formula 2.

This layer-by-layer tuning strategy considers more global distribution of the two datasets, while the leaf-only tuning strategy (Formula 2) focuses more on the local distribution. It actually smoothes out the tuning process, making the change over nearby boxes less dramatic. In practice, we have observed that the layer-by-layer strategy indeed gives better results.
 This algorithm more aggressively tunes the base tree model. As we have described, each internal node in the path from the root to the leaf node is associated with a predicate, in form of feature F&lt; X  , which makes one of the dimensions of the bounding box represented by the path. In this algorithm, we still keep the the feature F unchanged, while tuning both the threshold  X  and the corresponding node response. Assume that the current node has a split with feature F , F&lt;v 0 . 1. calculate the weight  X  p 0 with Eq. 3; 2. partition the new data with the specific feature F and 3. adjust the split by 4. re-partition the new data with the condition F&lt;v comb 5. adjust the response for the current node with Eq. 2 6. move to the child nodes and repeat the above steps.
Figure 5 illustrates the basic steps in the Adaptation Al-gorithm 2 for one node adaptation. In the figure, the top left tree is the base model and the right process has the major split adaptation steps. There are two threads going on: one is generating and updating the tree for the new data on the right side, where the new data is the part of data that goes through the ancestor nodes. The other on the left is updat-ing the base tree, while still preserving the information of source data distribution. We will use the output of the left side as the final adapted tree. This process is repeated for any nodes in the subtrees and applied to all trees in the base model.
 Figure 5: A sample algorithm for tree adaptation
Note that in both Algorithm 1 and 2, some branches of the base model may not be reached by the target domain exam-ples. It would be difficult to assert whether trimming such branches will result in better model or not. Instead, we will evaluate both trimming and n on-trimming in experiments. Since the base model is trained for the source domain, which may not contain the features that are specific to the target domain. Tuning the base model with Algorithm 1&amp;2 can-not address this problem. Fortunately, the GBT training method can be used to uniquely address this problem  X  we can expend the adapted model by appending certain num-ber of additional trees. These additional trees are trained on the residuals from the adapted base model. This method is easy to implement and less likely overfitting the target domain data as it follows the general boosting framework.
One may raise a question: why not solely append trees without applying Algorithm 1 or 2 on the existing trees (as known as additive modeling ), to achieve the adaptation goal? Additive modeling is not sufficient because the work-ing mechanism of GBT limits the effect of the appended trees. The earlier trees dominate the predicted value, while the appended trees only finely tune the result. For signif-icantly different distributions, it is ineffective to catch up the difference with appended trees. We will show that ad-ditive modeling has limited advantage if not combined with Algorithm 1 or 2. There are several goals in this experimental evaluation. First, we want to see how the setting of  X  can affect the quality of adaptation with varying size of the small training data from the target domain; Second, we want to compare the effectiveness of different adaptation algorithms we have presented; Third, additional trees (the additive method) can further benefit adaptation and we study how much addi-tional gain we can get; Finally, the adaptation approach is compared to other existing methods, including 1) the base model only; 2) models that are trained only with the small data from the target domain; and 3) data combination mod-els that combine the two sets of data for training.
The data used for training the base model from the source domain D 0 is large, including around 150,000 (query, web document, grade) examples for about 6,000 sample web queries. The base model uses 200  X  300 features scattered in the three categories we have described in Section 3.

The target domain D 1 has about 38,000 examples from around 1,300 queries. This is not a typical small dataset. We will use samples from the target domain to simulate the scenario that we have only very small training data. These training examples are labeled by relevance experts and can be divided into 5 batches according to the time the labeling was done. A five-fold cross validation will use this natural split.

There are a few more datasets that come from the target domains D 2 -D 5 , respectively. We will focus on D 1 detailed study on the properties of tree adaptation, while using other domain data for comparing different modeling methods. Table 1 summarizes the size of the datasets.
Discounted Cumulative Gain (DCG) [14] is a metric de-signed for evaluating the quality of ranked list if the grades for items in the list are known. In our case, DCG is defined as follows. We use a five-grade labeling scheme for editorial judgment, which are mapped to integers {  X 10 X ,  X 7 X ,  X 3 X ,  X 1 X ,  X 0 X  } , corresponding to the most relevant to the most irrel-evant. Suppose there are k documents used for testing the query q and each query-document pair ( q, d i ) in the test set is labeled with a grade l i . The test result will give a list of the k documents that is sorted by the scores given by the ranking function H to each pair ( q, d i ). Let i =1 ,...,k be the order of the sorted documents. DCG k score is computed for the sorted list as follows.

By definition, when reverse orderings happen at earlier positions ( i is small), they will be punished more than those happening later. By doing so, we prefer that high quality results show up at the top of the ranked list.

Each model test will generate a list of DCGs correspond-ing to the list of testing queries. To compare the statistical significance between two results, we perform t -test [19] on the two DCG lists. If p -value &lt; 0 . 05, the results are signifi-cantly different. t -test is only performed on the comparison between different models. For parameter tuning of the same model, we will show only the average DCG of the multi-fold cross validation. Note that the relevance differences be-tween high-quality commercial web search engines are often less than 5%. Therefore, the small statistically significant improvements will have practical impact.
We are going to compare four types of modeling methods to the proposed adaptation approach. They are 1) the base model only, 2) the small-data model, 3) the additive model, and 4) the data combination model. We briefly describe how they are generated as follows.

The base model is trained with a large training dataset from the source domain. In experiments, we train a GBT model on the D 0 data with parameters: 300 trees, 12 leaf nodes, and 0.05 shrinkage rate. Testing results show this model works reasonably well for all target domains. We will use the ranking quality of this model on the target domain as the base line for comparison.

A small-data model is trained only with the small amount of training data from the target domain with the GBT method. Since the training data is small, it is highly possible that the model will be overfitting to the training data, which means it may not work well for new examples from the target do-main in the future although it works well in cross-validation on the existing data.

An additive model does not change the base model but appends a few new trees to the base model, which are trained on the residuals of the new data on the base model. The training method is the default GBT method.

A data combination model [15] uses the combination of two sets of training data: one from the source domain (in the above specific case, the 150K query-document-grade ex-amples) and the other from the target domain, with possi-bly overweighting the target domain data. The same GBT training method is applied to the combined data to generate the final model.

There are also a few settings for performing adaptation as we have described. For clear presentation, we setup the notations for these settings (Table 2). notation description R tuning responses layer by layer RA tuning aggregated responses at leaves S tuning splitting values T trimming branches that no new example reaches.

The target training dataset consists of batches of data, which was judged by experts during different periods. The cross validation is done based on the batches, i.e., we directly map the batches to the folds in cross validation. Depending on the number of batches, different domains ( D 1 -D 5 )may have different number of folds. In each fold of cross vali-dation, one batch is used for testing and the rest are used for training. Meanwhile, when we test the effect of training data size to the performance of adaptation with D 1 ,wealso sample specific amount of queries from the training folds in each round. When training adaptation models, the target domains use the same base model that is trained on domain D 0 (300 trees and 12 leaf nodes).
The size of target training data is an important factor in ranking function adaptation. We anticipate that we will not need adaptation when the size of target training data increases to certain amount. In the first set of experiments, we use Adaptation Algorithm 2, i.e., tuning both responses and splitting values to investigate the effect of training data size and  X  setting on adaptation.

When performing adaptation, we do not change the struc-ture of the base model trees, i.e., the number of trees and the number of leaf nodes per tree are not change. Figure 6 shows the result for different settings on Algorithm 2. With the increase of training data size, the overall performance increases as we expected. With larger  X  the performance tends to better as well. However, increasing  X  from 10 to 20 will not change the performance much. In fact, large  X  may be subject to over-tuning the local distributions, and thus overfitting the small data. In practice, we will use a smaller  X  setting if the two settings give similar performance, to reduce the chance of overfitting.
First, we want to compare the two response tuning algo-rithms: tuning the layer-by-layer residual-fitting values or tuning the final leaf response. Figure 7 shows that layer-by-layer tuning is much better than aggregated-response tun-ing. This matches our expectation that it is good to consider more global distribution.

However, we have not found that trimming branches will significantly affect the performance. Trimming branches as-sumes that the finer partition developed on the base model will not fit the new data  X  a more generalized model (with less deep branches) will work better. Non-trimming trusts the structure learned from the source domain more and as-sumes the branches will eventually work for future data in the target domain. As the small sample set is not so repre-sentative, we expect that non-trimming will work better for future data, assuming the similarity between the source and target domains is high. Without sufficient data, trimming or not can only be determined by certain prior beliefs or heuristics, which will be a part of extended study.
In addition, the algorithm 2, tuning both splitting values and responses, actually reduces the performance slightly for D 1 data. However,thisisnotsureforallcasesaswewill show later.
When testing the effect of additional trees, we first fix the number of additional trees (30 trees) to see the relationship between the size of training data and the performance of different models. Adapted models are compared to  X  X ddi-tive models X  that append trees to the base model without changing the base model trees. Figure 9 shows the com-parison. Additional trees improve the quality of the base model to certain extent. Adaptation plus additional trees give more gains and the resultant models are much better than the simple additive models. t -test shows that the per-formance difference between the additive models and the adapted models is statistically significant ( p -value &lt; 0 . 05)
Next, let X  X  keep appending more trees with fixed train-ing data size. Figure 10 shows that with the same training data size (600 queries), appending more trees will slightly increase the performance for both adapted models and ad-ditive models. However, the performance differences keep almost constant and they are also statistically significant.
Finally, we compare the adaptation approach to other methods: 1) small-data modeling; 2) data combination mod-eling; 3) the base model. For fair comparison, except the base model, all models have the same size: 330 trees and 12 leaf nodes per tree. The performance of adapted model is the best among all algorithms and settings. In particular, we indeed observed that when the training data is very small, e.g.,  X  600 queries, adaptation models clearly show advan-tages. We find that with the increase of training data, the performance of small-data model increases quickly and pars with the adapted models around the size of 1000 queries, in Figure 11. However, for train ingagoodrankingfunction, 1000 queries are often insufficient. It is highly possible that the small-data model with 1000 queries is still overfitting the incompletely distributed training data, as we illustrated in Figure 3. Further testing with future data will show the bias. Since adapted models consider both the training data for the base model and those from the new domain, in general, we consider they should have more generalization power than the small-data models. Table 3: significant test: adaptation vs. other meth-ods with varying training data size.  X  X  X  means sta-tistically significant in t -test.

In data combination modeling, we try two settings: sim-ply pooling the two sets of data with equal weight and over-weighting the small data. Note that overweighting the small data too much may result in a model very similar to a small-data model. As Figure 12 shows, by weighting 20 times (W=20), the curve of combination model is actually very close to that of small-data model, with some exception at extremely small data (200 queries). With simple pooling (W=1), the combination models perform worse than the adaptation models. As we have analyzed, appropriately weighting individual samples according to the local distri-bution is a challenging topic in data combination, which, however, is naturally addressed by the tree adaptation algo-rithms.
Finally, we summarize the settings of adaption algorithm and compare them to data combination models for a few more target domain s (Figure 12). All models expect the base model have 330 trees and 12 leaf nodes. However, the training data may vary according to different domains 1 . The combination models are selected among the different weight settings W =1 , 10 , 20. Among the five domains, we find that adaptation is better than data combination for D D ,and D 5 , while data combination is slightly better for the other two domains.
Training with insufficient data has been a major challenge for developing effective ranking functions crossing domains. Based on the observation that domains must share some similarity, we propose the tree adaptation approach, which is based on Gradient Boosting Trees. The tree structure of the base model from the source domain provides sufficient local and global information that enables tree adaptation to conveniently incorporate the small training data from the new domain. We present a few tree adaptation algorithms and perform extensive experimental study to show the char-acteristics of these algorithms. The result shows that the tree adaptation approach is robust and usually better than other methods. [1] Bacchiani, M., and Roark, B. Unsupervised [2] Baeza-Yates, R., and Ribeiro-Neto, B. Modern [3] Blitzer, J., McDonald, R., and Pereira, F.
 [4] Burges,C.,Le,Q.,andRagno,R. Learning to [5] Burges, C., Shaked, T., Renshaw, E., Lazier, A., [6] Cao, Y., Xu, J., Liu, T.-Y., Huang, Y., and Hon, [7] Cao, Z., Qin, T., Liu, T.-Y., Tsai, M.-F., and Li, [8] Daum  X  e III, H., and Marcu, D. Domain adaptation [9] Freund, Y., Iyer, R., Schapire, R. E., and
The numbers for D 1 are from previous 600-query results [10] Friedman, J. H. Greedy function approximation: A [11] Hastie, T., Tibshirani, R., and Friedmann, J. [12] Herbrich, R., Graepel, T., and Obermayer, K.
 [13] Hwa, R. Supervised grammar induction using [14] Jarvelin, K., and Kekalainen, J. IR evaluation [15] Jiang, J., and Zhai, C. Instance weighting for [16] Joachims, T. Optimizing search engines using [17] Joachims, T., Granka, L., Pan, B., and Gay, G. [18] Leggetter, C., and Woodland, P. Flexible [19] Lehmann, E. L., and Casella, G. Theory of Point [20] Nallapati, R. Discriminative models for information [21] Tsai,M.-F.,Liu,T.-Y.,Qin,T.,Chen,H.-H.,and [22] Xu,J.,andLi,H. AdaRank: a boosting algorithm [23] Zheng, Z., Chen, K., Sun, G., and Zha, H. A
