 } The Reverse k-Nearest Neighbors (RkNN) queries are important in profile-based marketing, information retrieval, decision support and data mining systems. However, they are very expensive and existing algorithms are not scalable to queries in high dimensional spaces or of large values of k. This paper describes an efficient estimation-based RkNN search algorithm (ERkNN) which answers RkNN queries based on local kNN-distance estimation methods. The proposed approach utilizes estimation-based filtering strategy to lower the computation cost of RkNN queries. The results of ex-tensive experiments on both synthetic and real life datasets demon-strate that ERkNN algorithm retrieves RkNN efficiently and is scal-able with respect to data dimensionality, k , and data size. H.2.4 [ Systems ]: Query Processing Performance, Algorithms Reverse k-nearest neighbours, kNN distance estimation
The reverse k-Nearest Neighbors (RkNN) query aims to find points in a data set that have the given query point as one of their k-nearest neighbors (kNN). It has many applications in profile-based marketing, information retrieval, decision support and data mining systems and therefore, has received considerable attention in the recent years[10, 15, 18, 14, 9, 11].

RkNN queries are much more complex than the traditional kNN queries because unlike kNN, RkNN are not localized to the neigh-borhood of the query point. We illustrate this behavior using the example in Figure 1 where p 2 is the query point and k =2. We ob-serve that p 2 is one of the 2-nearest neighbors of p 1 , p Hence, p 2  X  X  reverse 2-nearest neighbors are p 1 , p 3 , and p that p 4 is an R2NN of p 2 although it is far from the query point p . In contrast, p 5 and p 7 are not answers of the R2NN query of p although they are close to p 2 .
 Copyright 2005 ACM 1-59593-140-6/05/0010 ... $ 5.00.

The naive solution for RkNN search is expensive. It first com-putes the k-nearest neighbors for each point p in the dataset and retrieves points that have q as one of the k-nearest neighbors. The complexity of this naive approach is O ( N 2 ) for non-indexed data, and O ( NlogN ) for datasets that are indexed by some hierarchical structure such as the R-tree[4] ( N is the cardinality of dataset).
Various methods have been developed for the efficient process-ing of RkNN queries and can be divided into two categories: pre-computation and space pruning . Pre-computation methods [10, 18] pre-compute the nearest neighbors of each point in the datasets and store the pre-computed information in hierarchical structures. This approach cannot answer an RkNN query unless the corresponding k-nearest neighbor information is available. Space pruning meth-ods such as [13, 16, 14] utilize the geometry properties of RNN to find a small number of data points as candidates and then verify them with NN queries or range queries. However, these methods are expensive when data dimensionality is high or when the value k is large.
This paper presents an efficient estimation-based RkNN search algorithm called ERkNN utilizes the filter-and-refine framework. It retrieves RkNN candidates based on the estimated kNN-distance (kNN-distance is the distance from a data point to its k th-nearest neighbor). This estimation-based filter has the advantage that its computation cost is much lower than the filtering strategies em-ployed by space pruning methods. This improves the RkNN query speed by orders of magnitude. We provide two local kNN-distance estimation methods -the PDE method and the kDE method. Exten-sive experiments on both synthetic and real-world datasets demon-strate that ERkNN retrieves RkNN efficiently and is scalable with respect to data dimensionality, k , and data size.

The rest of the paper is organized as follows. Section 2 defines the problem and reviews related work. Section 3 describes the estimation-based RkNN search algorithm. Section 4 presents the results of the performance study, and we conclude in Section 5.
We focus on the conventional RkNN query [10] in our study. It is formally defined as follows.
 Definition 2.1 (Reverse k-Nearest Neighbors) Given a dataset DB , a query point q , a positive integer k and a distance metric ` () , reverse k -nearest neighbors of q , denoted as RkNN ( q ) , is a set of points S  X  DB such that  X  p  X  S, q  X  kNN ( p ) , where kNN ( p ) are the k -nearest neighbors of point p.

Data points in our study are multi-dimensional vectors p = &lt; x , x 2 , ..., x d &gt; . They could be locations and features vectors of documents and images etc. The distance metric in our considera-tion is the L  X  metric, where ` ( p, q ) = 1  X   X   X   X  . For illustrative purposes, we shall use the most commonly used metric, L 2 (the Euclidean distance). Table 1 sum-marizes the symbols used frequently in the paper.
Various methods have been proposed for efficient RkNN query processing. The majority have been designed for RNN queries (i.e., RkNN queries when k = 1 ). We divide these methods into two categories: pre-computation methods and space pruning methods.
Pre-computation methods [10, 18] pre-compute and store the nearest neighbor information of each point in a dataset in index structures, i.e., the RNN-tree [10] and the Rdnn-tree[18]. With the pre-computed nearest neighbor information, an RNN query is an-swered by a point enclosure query [10] that retrieves a set of points A such that for each point p  X  A , the query point falls within the sphere centered at p and of the radius dnn (the distance from p to its nearest neighbor). The RNN-tree and the Rdnn-tree can be used to answer RkNN queries by pre-computing and storing the k-nearest neighbor information in these structures and applying the same point enclosure search.

The drawback of pre-computation methods is that they cannot answer an RkNN query unless the corresponding k-nearest neigh-bor information is available. Since the values of k may vary greatly in many applications, storing the k-nearest neighbor information for all possible values of k is expensive and sometimes infeasible, and maintaining such a large amount of k-nearest neighbor infor-mation in the presence of frequent updates is even more costly.
Space pruning methods such as TPL [16], SFT [13], SAA [14] filters a large portion of data points based on the geometry proper-ties of RkNN and returns a small number RkNN candidate points for verification. SAA [14] makes use of the bounded output prop-erty, e.g. for an RNN query in the 2-dimensional space, a query point q has at most 6 RNNs [14]. SAA divides the data space into six equal regions by straight lines that intersect at the query point q . For each region, the nearest neighbors of q are retrieved as can-didates and then verified by NN queries. For an RkNN query, the number of outputs is bounded by 6  X  k in 2-dimensional space [16] and similar algorithm is applicable to RkNN queries. SAA is costly for high-dimensional data because the bounding number increases exponentially with respect to data dimensionality.

SFT [13] is based on the assumption that RkNN and KNN are correlated, that is, an RkNN of q is expected to be one KNN of q , where K is a value bigger than k . It retrieves K nearest points to q as candidates and then verifies the candidates with range queries. However, the correlation between RkNN and KNN is not strong. Hence, K should be set to be sufficiently big in order to reduce false misses (points that are RkNN but missed from the found answer set), which makes it expensive for k s of large values.

TPL [16] uses the half-planes pruning strategy to divide the data space into two half-planes by the perpendicular bisector between q and an arbitrary data point p . Then any point in the half plane of p cannot be an RNN of q . TPL traverse the R-tree to retrieve nearest neighbors incrementally as RkNN candidates and uses the candi-dates to prune tree nodes with the trim algorithm until all nodes of R-tree are either pruned or visited. The retrieved candidates are ver-ified by an I/O optimized refinement algorithm. For RkNN queries, the k-trim algorithm is used to prune R-tree nodes based on the extended half-planes strategy. TPL is efficient in low-dimensional space and for small values of k . However, the cost of k-trim algo-rithm increases rapidly with respect to k .
Algorithm 1 shows the estimation-based RkNN search (ERkNN) which has two main steps. The first step calls procedure Filter to retrieve a set of points p whose distance to the query point q is equal to or greater than p  X  X  estimated kNN-distance as RkNN can-didates. The second step calls procedure Refinement to verify the candidates with range queries.

The novelty of ERkNN lies in its efficient candidate retrieval based on the local kNN-distance estimation which accurately ap-proximates each point X  X  kNN-distance. It answers RkNN queries of arbitrary k efficiently without requiring the corresponding k -nearest neighbor information and outperforms space pruning meth-ods significantly especially when data dimensionality is high and k is big. The following subsections give the details of the estimation methods and each procedure.
 Algorithm 1 ERkNN( T , q , k ) 1: A =  X  ; /* A is the RkNN candidate set*/
Previous studies on the kNN-distance estimation [3] employ the global uniform assumption , that is, the data are uniformly distributed over the whole data space. We call these approaches the global kNN-distance estimation . The kNN-distance computed by the global kNN-distance estimation is the average of the kNN-distance of all the points in the dataset. However, the local density of each point in a dataset varies considerately and so does the kNN-distance of each point. RkNN candidate retrieval that is based on the average kNN-distance tends to produce a large number of false misses and false hits, which increases the refinement cost and as well as decreases the recall of the correct RkNN answers.

In this work, we introduce the idea of the local kNN-distance estimation which is based on the nonparametric density estima-tion [7]. The nonparametric density estimation estimates a point X  X  local density function by a small number of neighboring samples around the point. The resulting local density function gives a much better approximation of the data distribution compared to the global uniform assumption. Hence, the local kNN-distance estimation ap-proximates the kNN-distance of each point much more accurately than the global approach.
 We develop two local kNN-distance estimation methods -the PDE method and the kDE method. The PDE method is based on the parzen density estimator with uniform kernel [7], which is a most commonly-used non-parametric density estimator. The kDE method, as an alternative to the PDE method, is based on the in-teresting finding which is deduced from the kNN density estima-tor [7]. Our experiment study show that these methods produce similar estimation results, work effectively on both synthetic and real life datasets and outperform the global approach significantly.
The PDE method is based on the parzen density estimator with uniform kernel [7]. Let L ( p ) be a small sphere region centered at p . The Parzen Density Estimator counts the number of points falling in L ( p ) and estimates the local probability density function at p ,  X  X ( p ) as follows: where N is the cardinality of dataset, k is the number of points within L ( p ) and V is the volume of L ( p ) . L ( p ) is a d -dimensional hyper-sphere of radius r = dnn k ( p ) , so where,  X ( x + 1) = x  X ( x ) ,  X (1) = 1 ,  X (1 / 2) = ing Equation 1 and 2, we have
When knowing that p  X  X  K NN-distance is dnn K ( p ) , we have Equation 4 gives an approximation of local density around p .
From Equation 3, we also have
Combining Equation 4 and 5 and applying the assumption that the data density is uniform over p  X  X  kNN vicinity to K NN vicinity (the local uniform assumption ), we obtain
Thus, we have the PDE method which estimates kNN-distance of p using the following equation: where ednn k ( p ) is the estimated kNN-distance of p and dnn is K NN-distance, the distance between p and its K th nearest neigh-bor, which is pre-computed in advance. d is data dimensionality.
The kDE method is based on an interesting finding which is de-duced from the kNN density estimator [7, 8] 1 , that is, the ratio of ( k +1)NN-distance to the k NN-distance is as follows [7, 8]:
Thus, we have, Refer [7] for the detail of deduction Algorithm 2 Filter( T , q , k , A ) 1: Initialize queue Q with root of T ; 2: while Q is not empty do 3: Dequeue a node N from Q ; 4: if N is an internal node then 8: else 9: for each point p in N do 11: Insert p into A ;
So for any two integers k 1 and k 2 ( k 1 6 = k 2 ),
Therefore, we have the kDE method which estimates kNN-distance using Equation 8:
The kNN-distance estimated by the PDE or the kDE methods is an approximation of the real kNN-distance, so the candidate set retrieved by the filter procedure of ERkNN may contain false hits and miss true answers due to the estimation error. The false hits will be removed with the refinement procedure of ERkNN. The problem of false misses will be discussed in Section 3.3. For RkNN queries of different k values, ERkNN uses the same K NN-distance as the basic for estimation. It is observed that when k is far from K , the approximation becomes less accurate. This problem can be alleviated by a multiple K s version of ERkNN. That is, we store several K NN-distances ( K 1 NN-distance, K distance,... K m NN-distance) and estimate a point X  X  kNN-distance according to the K i NN-distance such that K i is closest to k . ERkNN with multiple K s is a straightforward extension of the single K case, so we will focus on the single K version ERkNN here.
The data dimensionality d can be evaluated by either the embed-ded dimensionality or the intrinsic dimensionality [17]. The em-bedded dimensionality is the length of the feature vector of data and the intrinsic dimensionality is the number of the effective features of data. Studies in query cost analysis and pattern recognition show that cost estimation and data analysis based on intrinsic dimension-ality are more accurate. This is same for the local kNN-distance estimation according to our experimental study. The PDE and kDE methods estimate the kNN-distance more accurately when the in-trinsic dimensionality is used in Equation 7 and 8. Approaches for intrinsic dimensionality computation are in [17]. We now present the the filter and refinement procedures of ERkNN. We use the Rdnn-tree [18] data structure for the search.
Data Structure: The Rdnn-tree is basically an R-tree that is augmented with the nearest neighbor distance (NN-distance). We store the data points and the K NN-distance in the leaf nodes of the Rdnn-tree. Each leaf node entry e has the form ( p , dnn where p is the data point and dnn K ( p ) is the K NN-distance of p .
Each entry e in the internal nodes of the Rdnn-tree has the form ( ptr , MaxDnn K , mbr ). ptr points to a sub-node N 0 ; mbr is the minimum bounding rectangle (MBR) of N 0 ; MaxDnn K is the maximal K NN-distance of all data points in the subtree rooted at N .
 where p 1 , ..., p m are all points within N 0 . We use the algorithm described in [18] to build the Rdnn-tree with K NN-distance, except that the NN queries are replaced by the K NN queries. The tree can also be constructed using the bulk approach proposed in [12].
In the filtering step, ERkNN retrieves a set of points p whose estimated kNN-distance is equal to or greater than distance from p to the query point q . The estimated kNN-distance ednn computed with either the PDE or the kDE method. This estimation-based filter has the advantage that its computation cost is much lower than the filtering strategies employed by space pruning meth-ods [15, 16, 13].

During the tree traversal, we apply the following pruning strat-egy : If MinDist ( N, q )  X  Max ED ( N ) , tree node N can be pruned from traversal, where MinDist ( N, q ) is the minimum distance between the query point q and the MBR of N and Max ED ( N ) is computed as follows: -The PDE method is employed for kNN-distance estimation: -The kDE method is employed for kNN-distance estimation:
Since MaxDnn K = Max m i =1 dnn K ( p i ) , according to the com-putation of Max ED ( N ) , Max ED ( N ) = Max m i =1 ednn where ednn k ( p i ) is the estimated kNN-distance of p i point in N . Therefore, a node N such that MinDist ( N, q ) &gt; Max ED ( N ) can be pruned from traversal.

Algorithm 2 presents the candidate retrieval procedure that tra-verses the Rdnn-tree in a breath-first manner. It utilizes a queue Q to store tree nodes that shall be visited. Q contains the root of the Rdnn-tree initially. While Q is not empty, the algorithm dequeues a node N from Q and processes it according to the node type: -If N is an internal node (line 4-7): for each sub-node N sented by an entry e in N , it calculates MinDist from the query point q to N 0 , computes Max ED ( N 0 ) and inserts N 0 such that MinDist ( N 0 , q )  X  Max ED ( N 0 ) into Q to be visited later. -If N is an leaf node (line 8-11): for each point p in N , it computes the distance between p and the query point q , estimates the kNN-distance of p and inserts points such that ` ( p, q )  X  ednn the candidate set A .

The algorithm stops when Q is empty, i.e., all the tree nodes have been visited or pruned. All the data points p such that ` ( p, q )  X  ednn k ( p ) are retrieved and stored in the candidate set A .
The filter procedure of ERkNN is equal to the point enclosure query, so its complexity is O ( logN ) [10], where N is the number of data points.
The candidate set A contains false hits due to the over-estimation of a point p  X  X  kNN-distance. A refinement step is needed to remove the false hits.
 Algorithm 3 Refinement( T , q , k , A ) 1: Initiate range queries; 2: Refine in memory ( &lt; , A ); 4: Output points p i in A ;
A point p is a reverse k-nearest neighbor of q if and only if there cording to this property, the refinement procedure removes candi-dates with a set of range queries. These range queries have the candidate points as the query points and the distances between the candidate points and the query point of the RkNN query q as query ranges. Candidates that have at least k points within their corre-sponding query ranges shall be removed from A .
 Algorithm 3 shows the four steps in the refinement procedure. Step 1: Initialization of queries: for each point p i in A , a range where p i is the query point, r i is the query range and r Step 2: A fast refinement in memory: the range queries are first evaluated among the candidates. That is, for each range query R it checks how many candidate points are within R i  X  X  query range. Candidate p i that has at least k points within its query ranges is removed from A .
 Step 3: Range queries: it performs the range queries on the Rdnn-tree and removes data points that has at least k points in their query ranges from the candidate set.
 Step 4: Points remains in A are output as RkNNs.

Steps 1-2 and 4 are straightforward. Step 3 dominates the cost incurred in the refinement procedure. In order to reduce both I/O and CPU cost, we apply the aggregation strategy in this step. The basic idea is to first compute an aggregated range query R before carrying out the individual range queries. The query range of R a , which is centered at p a and of radius r a , covers the search ranges of all R i in &lt; . Figure 2 gives an example. There are three candidates p 1 , p 2 and p 3 . The dashed circles are their query ranges. The solid circle is the aggregated query R a . The computation query sphere of R a is corresponding to the minimum enclosing ball prob-lems [6] whose complexity is lower bounded by O ( |A| ) , where | X | is the cardinality of a set.

We design the following pruning strategies based on the aggre-gated query R a : Node pruning : For a node N , if MinDist ( N, p a )  X  r a surely out of the query range of any R i in &lt; and can be cut off safely ( e.g. N 1 in Figure 2). If MinDist ( N, p a ) &lt; r check whether N intersects with at least one range query R If N intersects with none of them, N can also be pruned away ( e.g. N 2 in Figure 2).
 Point pruning : For a data point p , if ` ( p, p a )  X  r a of the query range of any R i in &lt; . ( e.g. p in Figure 2). Query pruning : When a node N is being visited, if MinDist ( N, p  X  r i , all the entries in N are surely out of the query range of R Thus, R i is marked ignored while N is being visited ( e.g. , when N 3 in Figure 2 is being visited, R 1 and R 2 are to be ignored). The above pruning strategies show that with the aggregated query R , a point p or a node N can be pruned away with a single distance computation of ` ( p, p a ) or MinDist ( N, p a ) instead of checking its distance to each query point p i . This saves a large amount of distance computation and reduces the CPU cost.

Algorithm 4 describes the procedure Range Queries. c i counts Algorithm 4 Range Queries( T , k , &lt; , R a , A ) 2: Initialize priority queue Q with root of T ; 3: while Q is not empty and &lt; is not empty do 4: Dequeue a node N from Q ; 5: Apply query pruning ; 6: if N is an internal node then 8: Apply node pruning ; 10: else 11: for each point p in N do 12: if p cannot be pruned by point pruning then 15: Increase c i by 1;
Figure 2: Query aggregation and illustration of pruning. the number of points within query range of R i . Q is a priority queue and sorts nodes in ascending order of their MinDist to p Initially, Q contains the tree root. When the first queue item N is dequeued, the query pruning strategy is applied to mark the queries such that MinDist ( N, p i )  X  r i as ignored (line 5). Node N is then processed according to its type: -If N is an internal node (line 6-9): for each sub-node N sented by an entry e in N , it applies the node pruning strategies. Node N 0 that cannot be pruned are inserted into the priority queue Q and shall be visited later. -If N is an leaf node (line 10-17): for each point p in N , it first applies the point pruning strategy. If p is not pruned, it checks whether p is within the query range of each not ignored query R If true, c i is increased by 1. Whenever there are k points inside of query range of R i , p i is identified as a false hit and removed from A and range query R i is also removed from &lt; .

The procedure stops when either Q is empty or &lt; is empty, im-plying that all the tree nodes that intersect with at least one range query R i in &lt; have been searched or the RkNN query has an empty answer set ( e.g. , the R2NN of p 8 in Figure 1 is empty).
The I/O cost of the refinement procedure is O ( logN ) since it carries out a set of range queries simultaneously, where N is the number of data points. The CPU cost is O ( |A| X  logN ) . The up-per bound of the I/O and CPU costs are O ( N ) and O ( |A|  X  N ) respectively even when the index fails.
ERkNN may miss some correct answers due to the estimation error. This section examines the recall of the RkNN answer set retrieved by ERkNN.
 Definition 3.1 Let A be the RkNN answer set retrieved by ERkNN and  X  be the complete answer set of the RkNN query, the recall of A is denoted as R A = |A| |  X  | , where | X | is the cardinality of a set. Theorem 3.1 The recall of the answer set retrieved by ERkNN is lower-bounded by tribution of the estimation errors.
 Proof: For a point p in the complete answer set  X  , p is falsely missed when both the following conditions are true: (1) ednn Let Pr { X } be the probability of an event.
 ednn k ( p )  X  dnn k ( p ) is the estimation error. Let err ( p ) = ednn k ( p )  X  dnn k ( p ) , and f ( x ) be the probability distribution of err ( p ) .
 Hence, we have Theorem 3.1.

The above study shows that the false misses are introduced by the under-estimation of the kNN-distance. Therefore, we can im-prove the recall by simply introducing a positive adjustment to the estimated kNN-distance. Here, we present two approaches, the lo-cal adjustment and the global adjustment. Let ednn k 0 ( p ) be p  X  X  estimated kNN-distance after adjustment.

Local adjustment : Let  X  be a real number and  X  &gt; 1 .  X  is called the local adjustment factor.

Global adjustment : Let  X  be a real number and  X  &gt; 0 .  X  is called the global adjustment factor. ednn k ( p ) in both Equation 10 and 11 shall be substituted with either Equation 7 or 8. ERkNN then retrieves a set of points p such that ` ( p, q )  X  ednn k 0 ( p ) in the filtering phase and then apply the same refinement algorithm to verify the candidates.

These adjustments reduces Pr { err ( p ) &lt; 0 } and hence increases the recall. At the same time, the adjustment makes the filtering step of ERkNN retrieve more data points as candidates which may in-crease the refinement cost. However, the in-memory refinement procedure filters a number of candidates effectively, so its impact on the overall performance is not significant. (c) Zipf Dataset (dim=8) (c) Recall (global adjustment) In this section, we present the experiments to evaluate ERkNN. We use both synthetic and real life datasets. The real life datasets are the Corel dataset from UCI KDD data repository [1] which con-tains 32 dimensional feature vectors of around 60K images, and the CA dataset from the Sequoia 2000 benchmark [2] which contains 62,556 locations in California.
 We compare ERkNN with TPL [16] and SFT [13]. We exclude SAA because its performance is significantly worse than SFT and TPL [16]. For both the R-tree (used by SFT and TPL) and the Rdnn-tree (used by ERkNN), the node size is 8192 bytes. By de-fault, SFT retrieves 5  X  k nearest neighbors as candidates in its fil-tering phase and K NN-distance used by ERkNN is 15NN-distance. The experiments are conducted on a Pentium 4 2.6GHz PC running WinXP. We measure the performance in terms of CPU time, num-ber of node accesses and total query cost which includes both CPU time and I/O overhead by charging each node access 20ms [5]. The results are the average of 200 RkNN queries. The query points are randomly picked from the datasets.
The first set of experiments study the proposed local kNN-distance estimation methods -the PDE method and the kDE method. We es-timate the kNN-distance of k=1,2,..., 30 and evaluate the estimation accuracy by the mean square error (MSE).
 where N is the number of data points in the dataset.
 Figure 4 shows the results on the uniform, Zipf and real life Coral and CA datasets. PDE-id ( or kDE-id) indicates PDE ( or kDE) method using the intrinsic dimensionality . PDE-ed ( or kDE-ed) is the PDE ( or kDE) method using the embedded dimensional-ity . GEM is a global estimation method that calculates the average kNN-distance using the method proposed in [3].

We observe that the PDE and the kDE methods have similar accuracies on all the datasets. Estimations using the intrinsic di-mensionality are better than the estimations using the embedded di-mensionality. The superiority of PDE-id and kDE-id over PDE-ed and kDE-ed is very clear on the Zipf dataset and the Coral dataset where the intrinsic dimensionality is much lower than the embed-ded dimensionality (see Table 2). As for the uniform and the CA datasets, their intrinsic dimensionality and embedded dimensional-ity are similar, so there is not much difference between the estima-tions using the intrinsic dimensionality and the embedded dimen-sionality.

The local estimation methods are more accurate than the global estimation method (see Figure 4). The MSE of GEM on Coral dataset is too large to be plotted. On average, the local estimations are 37 times better than GEM on the uniform dataset. The local estimations on the Coral and the Zipf datasets outperform GEM significantly than on uniform and CA datasets.

The study demonstrates that local estimations outperform the global approach significantly and yield more accurate approxima-tion of each point X  X  kNN-distance on both uniformly distributed datasets and real and skewed datasets. The study also confirms that the intrinsic dimensionality captures the effective data dimension-ality and leads to better estimations. Next, we evaluate the recall of the answer set retrieved by ERkNN. We query RkNN k = 10 and use the PDE method to estimate local kNN-distance. We first evaluate the local adjustment. Figure 5 (a) and (b) present the results on the 100K 8-dimensional Zipf datasets. Figure 5(a) exhibits the average recall when we vary the local ad-justment factor  X  from 1 to 1.06. As expected, the actual recall is always higher than the lower bound. As  X  increases, the recall ap-proaches 1 and the lower bound becomes tighter. Figure 5(b) shows the influence of the local adjustment on the performance of ERkNN in terms of the total query cost. The real number on top of the bars indicate the number of RkNN candidates retrieved. Both the cost of ERkNN and the number of RkNN candidates increase moderately with the increase of  X  .
 We evaluate the effect of global adjustment on various datasets. Figure 5 (c) and (d) show the results of the study on the 8-dimensional Zipf dataset. The global adjustment factor  X  is varied from 0 to 0.015. Our study finds that the global adjustment has the simi-lar influence on the recall and performance of ERkNN as the local adjustment on the uniform data but works more effectively on the skewed Zipf dataset. On the Zipf dataset, when ERkNN with local adjustment reaches 100% recall, only 15 RkNN candidates are re-trieved, while ERkNN with global adjustment needs to retrieve 22 candidates. The reason is the local adjustment is more adaptive to the data density distribution.
We now compare the performance of ERkNN with SFT and TPL on real datasets with varying values of k . Figure 6 presents the re-sults on the Coral datasets when we vary k from 1 to 30. Note that the lines of ERkNN have a break at k =15 because when k = K the RkNN queries are answered using the point enclosure query directly. Contrary to the experiment results in [16], SFT is more efficient than TPL in our experiments. The reasons are two-fold. First, SFT retrieves only 5  X  k points as candidates in our exper-iments, while SFT retrieves 10  X  d  X  k points as candidates in the experiments in [16]. Second, we use an optimized SFT with batch execution of the boolean range queries [13]. The batch execution of boolean range queries reduces I/O cost and speeds up the query performance considerably [13].

This study shows that ERkNN outperforms both TPL and SFT significantly. The speed-up factor on the Coral dataset in terms of total response time is 50.5 when k is 1 and 2024.45 when k is 3. The average speed-up of ERkNN over SFT is more than 20. TPL is expensive when k is large mainly because the k-trim algo-rithm used by TPL to prune an R-tree node requires to do ( times clippings [16], where n c is the number of RkNN candidates. ERkNN is more efficient than SFT because its low CPU cost for candidates retrieval and refinement. During the filtering phase, ERkNN performs the point enclosure query, while SFT performs the kNN query. kNN query is more expensive due to the additional CPU cost to sort and insert the kNN candidates. It is considerable especially when k is large. The refinement procedure of ERkNN is also more efficient because ERkNN retrieves much fewer candi-
Figure 7: Number of distance computation on Coral dataset dates than SFT and the aggregation strategy employed by ERkNN prunes a large number of distance computations, thus reducing the CPU cost greatly. Experiment results on the Coral dataset show that the aggregation strategy prunes around 75% distance computations on average (see Figure 7).
 We observe that ERkNN incurs more node accesses than SFT. This is because ERkNN accesses more nodes during the filtering phase (ERkNN accesses 1863.95 nodes for the Coral dataset while SFT accesses 1319.65 nodes on average). The reason is that ERkNN may access more tree nodes to retrieve some potential RkNNs which are far from the query point according to the estimated kNN-distance. Further, the lowest recall of ERkNN is 96.95% on the CA dataset and 97.12% on the Coral dataset.

The study also shows that for RkNN query, I/O cost is no longer the dominant cost for both SFT and ERkNN because both meth-ods execute a set of range queries simultaneously and traverse the index tree only once in the refinement procedure. CPU cost is more expensive because there are multiple candidates to be veri-fied. On Coral data, the average I/O overhead of ERkNN and SFT is 0.069 sec (3342.26 node accesses) and 0.055 sec (2763.46 nodes accesses) respectively, while the CPU cost is 0.19 sec and 4.68 sec. Similar results are obtained for the CA dataset.
In this section, we study ERkNN, SFT and TPL on synthetic datasets of various sizes and dimensions.
We first evaluate the effect of data dimensionality on the RkNN query by varying the number of dimensions from 2 to 32 ( k = 10). Figure 8 shows the results on the Zipf datasets. Similar trends are obtained for the uniform datasets.

ERkNN outperforms SFT by an average factor of 28.7 on Zipf datasets and 40.1 on uniform datasets. The recall of ERkNN re-mains higher than that of SFT. For the Zipf datasets, the recall of ERkNN remains steady at around 99%, while the recall of SFT de-creases from 91.03% to 70.79% when the data dimensionality in-creases from 2 to 32. The study demonstrates that ERkNN is more scalable to RkNN queries in high-dimensional spaces compared to TPL and SFT.
We examine the RkNN query performance on datasets of vary-ing sizes. We query RkNN k = 10 on the uniform and the Zipf datasets and vary the dataset size from 100,000 to 500,000 objects. Figure 9 shows the results on the uniform datasets. Similar trends are obtained for the Zipf datasets. We observe that ERkNN out-performs SFT and TPL significantly. In terms of total query cost, the average speed-up of ERkNN over SFT is 29.6 on the uniform datasets and 11.7 on the Zipf datasets. The average speed-up of ERkNN over TPL is over 10,000. Figure 9(d) compares the recalls of the RkNN answer sets retrieved by ERkNN and SFT. As in the other studies, ERkNN has a higher than SFT.
RkNN queries have important applications in many database sys-tems. However, existing methods are expensive and not scalable to RkNN queries in high dimensional space or of large values of k . In this paper, we propose an efficient estimation-based approach -ERkNN. It employs an estimation-based filter which requires much lower computation cost. In order to estimate the kNN-distance ac-curately, we propose the local kNN-distance estimation methods. Extensive experiments demonstrate that ERkNN is efficient, scal-able and outperforms pervious methods significantly.
This work was supported by A*STAR-NUS grant R-252-000-172-593. We thank authors of [16] and [18] for providing the codes.
