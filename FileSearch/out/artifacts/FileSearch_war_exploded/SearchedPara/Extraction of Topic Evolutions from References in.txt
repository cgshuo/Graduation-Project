 This paper provides a topic model for extracting topic evo-lutions as a corpus-wide transition matrix among latent top-ics. Recent trends in text mining point to a high demand for exploiting metadata. Especially, exploitation of reference re-lationships among documents induced by hyperlinking Web pages, citing scientific articles, tumblring blog posts, retweet-ing tweets, etc., is put in the foreground of the effort for an effective mining. We focus on scholarly activities and propose a topic model for obtaining a corpus-wide view on how research topics evolve along citation relationships. Our model, called TERESA, extends latent Dirichlet allocation (LDA) by introducing a corpus-wide topic transition proba-bility matrix, which models reference relationships as transi-tions among topics. Our approximated variational inference updates LDA posteriors and topic transition posteriors al-ternately. The main issue is execution time amounting to O ( MK 2 ), where K is the number of topics and M is that of links in citation network. Therefore, we accelerate the infer-ence with Nvidia CUDA compatible GPUs. We compare the effectiveness of TERESA with that of LDA by introducing a new measure called diversity plus focusedness (D+F). We also present topic evolution examples our method gives. H.2.8 [ Database Management ]: Database Applications X  Data Mining ; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation Topic modeling, Citation analysis, GPU
Intensive analysis of reference relationships among docu-ments, e.g. hyperlinking Web pages, citing academic arti-relationship helps us to find papers we can cite and to pre-dict what kind of research will follow ours. Our method is different from the existing approaches as is discussed below.
Nallapati et al. [10] propose a method for modeling topic transitions among documents and explicitly model transi-tions of topic distributions among documents as  X  X lows X  of topics. However, their probabilistic model has K flow pa-rameters at every link in citation network. Therefore, the complexity of the parameters amounts to O ( MK ), where K is the number of topics and M is the number of links in citation network. Further, this model aims at giving an accumulation of local transition patterns, and thus it is diffi-cult to obtain a global view on how topics are related to each other. We may repeat the same discussion with respect to [15], where relationships of topic distributions among docu-ments are modeled as Markov random fields. Further, Ren et al. [14] modify HDP [16] so that we can distill topic tran-sitions between adjacent time points along the time axis. They consider topic transitions not between linked docu-ments as in [10] but between adjacent time points. There-fore, the complexity of the parameter space is reduced. How-ever, this proposal also makes it difficult to obtain a global view on how topics interrelate. A similar discussion may also be valid for [2, 17]. In contrast, TERESA extracts a single K  X  K matrix of topic transition probabilities and thus has a smaller number of parameters for modeling a citation net-work of scientific articles. In this manner, we obtain a single corpus-wide view of topic interaction as a digraph whose vertices are latent topics and whose weighted arcs are tran-sitions among topics accompanied with their probabilities (Figure 1). The results of our analysis are easy to grasp.
Some existing topic models provide an analysis of refer-ence relationships by establishing links among topic multino-mial probabilities via logistic normal distributions [10, 18]. This approach is intricate, because the model has a part of its parameters in K  X  K dense covariance matrices, which need to be inverted in the inference. Therefore, covariance matrices are often assumed to be diagonal [2, 1, 6]. One im-portant reason this approach avoids dense matrices is that it considers topic transitions at many different places in ob-served data, e.g. at all citation links or at all pairs of adja-cent time points. Consequently, a large number of matrices should be taken into consideration. Therefore, the matri-ces should be assumed to be sparse for achieving a realistic Figure 1: A trimmed portion of a topic evolution network extracted from CORA when K = 300 .Each topic is presented by the most frequent 21 words. Thicker arrows give topic transitions of larger prob-ability. We can find interesting relationships among the topics related to ML, NLP, DM, etc. This result was obtained as a result of VB executed on GTX580. transition matrix R , which increases the computational bur-den. While the time complexity of the inference for LDA is only proportional to K , that for TERESA is proportional to K 2 . Therefore, we accelerate the inference. We employ variational Bayesian inference (VB) [3] for an approximated estimation of posterior parameters, because VB is suitable for an efficient parallelization. Zhai et al. discuss why VB can be efficiently parallelized [19]. However, we do not con-sider a parallelization with OpenMP and/or MPI, which is considered by Zhai et al. We consider a parallelization with Nvidia CUDA compatible GPUs, because our VB frequently performs the same operation on hundreds of different data simultaneously and favors SIMD architecture. In this paper, we assume that the input data can be stored on the device memory of GPU without being split into smaller subsets. Therefore, VB is not seriously affected by the latency dur-ing a data transfer between CPU and GPU, because such transfer does not occur in the course of the computation.
We give the update formulae without derivation due to space limitation. Let  X  jwk be the variational posterior prob-ability that a token of word v w expresses topic t k in docu-ment d j ,where k  X  jwk = 1 holds. Let Dir(  X  j )bethevaria-tional Dirichlet posterior for the topic multinomial Mul(  X  j ). Further, let Dir(  X  k ) be the variational Dirichlet posterior for the word multinomial Mul(  X  k ).Basedon[3],  X  jwk sand  X  j s  X  w + j n jw  X  jwk ,where X (  X  ) represents digamma function,  X  k 0 is defined to be w  X  kw ,and n jw is the number of the tokens of word v w in document d j .(  X  jw 1 ,..., X  jwK )canbe updated independently for different pairs of j and w .  X  kw can be updated independently for different pairs of k and w . Therefore, these updates are efficiently parallelized.
With respect to  X  jk , we use the following update for LDA:  X  TERESA, the generation of word tokens in d j is affected also by the topic distributions of the documents cited by d j . Therefore, we obtain a different update formula. Let Dir(  X  k ) be the variational Dirichlet posterior for the topic transition multinomial Mul( r k ). Then,  X  jk can be updated as +  X   X  to citation analysis. The other is the HEP-PH data set, a set of tex documents along with a citation graph avail-able at KDD Cup 2003 Web site 2 .Eachtexdocumentin the HEP-PH data set corresponds to a paper in the hep-ph portion of the arXiv. For both data sets, we removed stop words and applied a Porter X  X  stemmer. We checked the soundness of the inference by calculating perplexity [3] for 10% randomly chosen word tokens, whose number is denoted by N test . The perplexity is defined as exp  X  around 570 for CORA after CGS for LDA, and the perplex-ity was not significantly modified by VB for TERESA. For HEP-PH, we obtained a perplexity around 710 after CGS for LDA, and the perplexity was also not significantly mod-ified by VB for TERESA. These perplexities were obtained for K = 300. Since TERESA did not significantly modify the perplexity achieved by CGS for LDA, it can be said that TERESA could conduct a topic evolution analysis without affecting the generalization power of LDA.

As a quantitative measure for evaluating the quality of topic evolution analysis, we devise a new measure called di-versity plus focusedness (D+F) based on the posterior prob-abilities of topic transitions. D+F is defined by combining two measures: transition diversity and expected focusedness . P ( t k ) is the posterior probability of the occurrence of topic t ,and P Tr ( t k | t k ) is that of the transition from t k to t k . Then the posterior probability of the transition to topic t k is written as P To ( t k )= k P ( t k ) P Tr ( t k | t k ). We define transition diversity as TrDiv  X  X  X  k P To ( t k )log P To ( t k ). TrDiv is an entropy measure. When TrDiv is large, the transition to each topic occurs equally often. This means that the topic model can extract topics so that every topic joins a corpus-wide topic evolution equally. Next, we define is a negative entropy measure and is thus less than or equal to 0. When Foc ( t k ) is close to 0, only a limited number of topics are frequently reached from topic t k . Further, we ob-tain an expected focusedness as EFoc  X  k P ( t k ) Foc ( t k ). When EFoc is large, the probability distribution of the tran-sitions from any topic is highly skewed. In other words, only a limited number of transition edges starting from each node have a large probability. D+F is defined to be the sum of TrDiv and EFoc . When D+F is large, all topics are equally visited, and, at the same time, the choice of transition des-tination is highly selective. We contend that topic evolution analyses giving a larger D+F are better. We give a rationale for D+F by considering the three extreme cases as below.
First, assume that the transition probability matrix is the identity matrix. Then TrDiv =  X  k P ( t k )log P ( t k ) and EFoc = 0. Therefore, D+F is equal to the entropy  X  k P ( t k )log P ( t k ), which is larger when P ( t k )s show less differences. In this case, we extract the topics that are to-tally  X  X ndependent X  X n the sense that each topic only transits to itself. Needless to say, articles in reality may cite articles from heterogenous research fields. However, it is desirable to extract relatively independent components so that the tran-sitions among different components rarely occur. Therefore, it can be said that a larger D+F is better. Second, assume that the transition probability matrix is a matrix all of whose columns except one are zero vectors. Then TrDiv =0and http://www.cs.cornell.edu/projects/kddcup/datasets.html Figure 3: D+F scores for CORA (left) and HEP-PH (right) for K = 300 . to the difference between TERESA and LDA. As is shown in each graph, TERESA can give a better topic evolution analysis for smaller values of  X  . However, when  X  is close to 1, TERESA seems to put too much emphasis on citation data and thus fails to achieve a balanced inference over text data and citation data. We recommend 0.1 or 0.2 for  X  .
We have already given an example of the results of topic evolution analysis in Figure 1, where we can find several in-teresting topic transitions. For example, the topic seemingly corresponding to machine translation (represented by trans-lat, languag, system, english, machin, cross, automat ,etc) and the topic seemingly corresponding to speech recogni-tion (represented by speech, recognit, system, word, speaker, model, recogn , etc) point to each other with a thick arrow. That is, this figure reveals that machine translation has a close bi-directional relationship with speech recognition.
More examples are presented in Figure 2, which also con-tains the examples of topic evolutions extracted from CORA. On the top panel of Figure 2, the word network appears in several boxes. However, the boxes representing the topics related to network protocols (i.e., the boxes containing net-work, protocol, internet, servic, multicast , etc) and the boxes representing the topics related to neural networks (i.e., the boxes containing network, neural, learn, train ,etc)arenever connected. This conforms to our intuition. On the bottom panel of Figure 2, the topic seemingly related to Bayesian statistical analysis (represented by model, estim, data, dis-tribut, statist, bayesian, paramet , etc), has incoming arcs both from the topic seemingly related to MCMC inferences (represented by chain, markov, carlo, mont, converg , etc), and from the topic seemingly related to bioinformatics (rep-resented by sequence, protein, dna, molecular, align , etc). We can guess that some papers on Bayesian methods were written based on the preceding works related to MCMC in-ferences and also on the works related to bioinformatics.
This paper provides an LDA-like topic model for extract-ing a corpus-wide view on how latent topics interrelate in a given set of scientific articles. While many existing ap-proaches provide an accumulation of a large number of local views, our approach, TERESA, can give a single global view.
