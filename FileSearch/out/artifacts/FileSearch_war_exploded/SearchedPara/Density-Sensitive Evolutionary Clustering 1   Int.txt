 Many clustering approaches, such as the K-Means Algorithm[1], partition the data set into a specified number of clusters by minimizing certain criteria. Therefore, they can be treated as an optimization problem. As global optimization techniques, Evolutionary algorithms (EAs) have been used for clustering tasks commonly in literature.[2][3][4] The solution representation and dissimilarity measure are the main representation approach that borrows from the K-Means algorithm: the representation codes for cluster center only, and each data item is subsequently assigned to a cluster representative according to an appointed di ssimilarity measure.[5] The most popular dissimilarity measure is the Euclidean distance. By using Euclidean distance as a measure of dissimilarity, these evolutionary clustering methods as well as the K-Means algorithm have a good performance on the data set with compact super-sphere distributions, but tends to fail in the data set organized in more complex and unknown dissimilarity measure for clustering. Su and Chou [6] proposed a nonmetric measure based on the concept of point symmetry, according to which a symmetry-based cluster center if they present a symmetrical structure with respect to the cluster center. Charalampidis [7] recently de veloped a dissimilarity measure for directional patterns represented by rotation-variant vectors an d further introduced a circular K-Means algorithm to cluster vectors containing directional information. 
In this study, we propose a novel evolutionary algorithm-based clustering technique, named density-sensitive evolutionary clustering (DSEC), by using a novel representation method and a density-sensitive dissimilarity measure. In DSEC, each string is a sequence of the cluster representatives selected from all the data items. The complex non-convex clusters compared with the K-Means algorithm [1], a genetic algorithm-based clustering [3], and a modified K-Means algorithm with the density-sensitive distance metric [8]. For real world problems, the distribution of data points takes on a complex manifold structure, which results in the classical Euclidian distance metric can only reflect the locating in the same manifold structure will have a high affinity. We can illustrate this problem by the following example. As shown in Fig. 1(a), we expect that the affinity metric. In terms of Euclidian distance metric, however, point 1 is much closer to point fully reflect the characters of data clustering. 
Here, we want to design a novel dissimilarity measure with the ability of reflecting distribution in Fig. 1(a) that data points in the same cluster tend to lie in a region of points. We can design a data-dependent dissimilarity measure in terms of that character of local data density. 
At first, data points are taken as the nodes V of a weighted undirected graph = . Edges {} they can be linked by a path running along a region of high density, and a low affinity if they cannot. This concept of dissimilarity measure has been shown in experiments to lead to significant improvement in cl assification accuracy when applied to semi-supervised learning [9][10]. We can illustrate this concept in Fig 1(a), that is, we are looking for a measure of dissimilarity accordi ng to which point 1 is closer to point 3 cross low density regions, and simultaneously shorten those that not cross. triangle inequality under the Euclidean metric. In other words, a direct connected path between two points is not always the shortest one. As shown in Fig 1(b), to describe follows. Definition 1. The density adjusted length of line segment (, ) where factor. Obviously, this formulation possesses the property mentioned above, thus can be between two points can be elongated or shortened by adjusting the flexing factor  X  . 
According to the density adjusted length of line segment, we can further introduce a new distance metric, called density-sensitive distance metric, which measures the distance between a pair of points by searching for the shortest path in the graph. of length 1 lp = X  connecting the nodes 1 kp  X &lt; . Let density-sensitive distance metric between Thus ( , ) (, ) 0
Dx x  X  ; ( , ) ( , ) ( , ) and only if 
As a result, the density-sensitive distance metric can measure the geodesic distance along the manifold, which results in any two points in the same region of high density being connected by a lot of shorter edges while any two points in different regions of achieving the aim of elongating the distance among data points in different regions of local density, namely, what is called density-sensitive. 3.1 Representati on and Operators viewpoint. Each individual is a sequence of real integer numbers representing the sequence number of K cluster representatives. The length of a chromosome is K words, where the first gene represents the first cluster, the second gene represents the second cluster, and so on. As an illustration, let us consider the following example. being considered be 5. Then the individual (6, 19, 91, 38, 64) represents that the 6-th, 19-th, 91-th, 38-th, and 64-th points are selected to represent the five clusters, respectively. the data set is N and the number of clustering is K , then the search space is N K . 
Crossover is a probabilistic process that exchanges information between two parent [11] because it is unbiased with respect to the ordering of genes and can generate any uniform crossover on the encoding employed is shown in example 2. Example 2. Let the two parent individuals be (6, 19, 91, 38, 64) and (3, 29, 17, 61, 6), random generate the mask (1, 0, 0, 1, 0), then the two offspring after crossover are (6, 38, 6 ) because the 6 in bold is repeat, we keep it unchanged. Each individual undergoes mutation with probability p m as example 3. Example 3. Let the size of the clustered data set be 100 and the number of clustering 19+ floor ((100-19)* random +1), 91, 38, 64) or (6, 19-floor((19-1)* random +1), 91, 38, towards minus infinity. 3.2 Objective Function representative to the point is minimum. As an illustration, let us consider the following example. density-sensitive distance between the first point and the 38-th point is the minimum the points are assigned in the same way. 
Subsequently, the objective function is computed as follows: is the density-sensitive distance between the i -th data item of cluster k C and k  X  . 3.3 Density-Sensitive Evolutionary Clustering Algorithm The processes of fitness computation, roulette wheel selection with elitism [13], crossover, and mutation are executed for a maximum number of generations G max . The best individual in the last generation provides the solution to the clustering problem. number in [1, N ], where N is the size of the data set. This process is repeated for each of the P chromosomes in the population, where P is the size of the population. experimental results on seven artificial data sets, named Line-blobs, Long1, Size5, Spiral, Square4, Sticks, and Three-circles, with different manifold structure. The distribution of data points in these data sets can be seen in Fig. 3. The results will be compared with the K-Means algorithm (KM )[1], a modified K-Means algorithm using the density-sensitive dissimilarity measure (DSKM)[8], and the genetic algorithm-based clustering technique (GAC) [3]. In all the algorithms, the desired clusters number is set to be known in advance. The parameter settings used for DSEC and GAC in our experimental study are given in Table 1. For DSKM and KM, the maximum iterative number is set to 500, and the stop threshold 1e-10. Clustering quality is evaluated using two external measures, the Adjusted Rand Index [5] and the Clustering Error [8]. The adjusted rand Index returns values in the interval [0, 1] and is to be maximized. The clustering error also returns values in the interval [0, 1] and is to be minimized. 
We perform 30 independent runs on each problem. The average results of the two metrics, clustering error and adjusted rand index, are shown in Table 2. 
From Table 2, we can see clearly that DSEC did best on six out of the seven problems, while GAC did best only on the Square4 data set. DSKM also obtained the true clustering on three problems. KM and GAC only obtained desired clustering for the two spheroid data sets, i.e. Size5 and Square4. This is due to that the structure of the other five data sets does not satisfy convex distribution. On the other hand, DSEC and DSKM can successfully recognize these complex clusters, which indicate the structure. When comparisons are made between DSEC and DSKM, the two the Size5 and Square4 problems, DSEC did a little better than DSKM in both the clustering error and the adjusted rand index. The main drawback of DSKM is that it consistency. DSEC made up this drawback by evolutionary searching the cluster representatives from a combinatorial optimization viewpoint. In order to show the from DSEC are shown in Fig. 3. quality, DSEC outperformed GAC, DSKM and KM in partitioning most of the test problems. in clustering data with computational complexity. The main computational cost for the flexibility in detecting clusters lies in searching for the shortest path between each pair of data points which makes it slower than KM and GAC. Acknowledgements. This work was supported by the National High Technology Research and Development Program (863 Program) of China (No. 2006AA01Z107), the National Basic Research Program (973 Program) of China (No. 2006CB705700) and the Graduate Innovation Fund of Xidian University (No. 05004). 
