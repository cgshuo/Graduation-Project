 1. Introduction task that is present not only in many (if not all) industrial processes, but also in many other scenarios such as medicine and finance. Most of the time, the accuracy of the decisions made within a process is critical for its success as a whole. Due to this criticality and complexity, this task has been traditionally under-taken by experts in the relevant field, who had to make a choice based on the limited available information. While humans can handle complex tasks reasonably well, they are vulnerable to psychological biases, lack of consistency and are unable to consider large volumes of data simultaneously. With the advent of ever-cheaper computational resources and machine learning algorithms, experts can now rely on guidance from automatic systems which do not have the aforementioned drawbacks. These new decision making algorithms focus on achieving high accuracy at low computational cost. In this work we will strive to reduce computational cost while keeping state-of-the-art accuracy. and Shawe-Taylor, 2000 ) are accepted as a standard de facto in automatic classification. An SVM is a maximum margin linear classifier, that can be extended to nonlinear problems by the use find SVMs attractive because of their classification accuracy reported in the literature. But it should also be noticed that another major key behind SVMs success is that one only needs to tune one or two hyper-parameters before the main optimisation takes place. These hyper-parameters are the kernel design hyper-parameters and the omnipresent trade-off between regularisation and training set error minimisation, C . In this sense, most practitioners bear the following rule of thumb: use an SVM with a Radial Basis Function (RBF) as kernel and a 10-fold cross-validation process to determine C and the Gaussian kernel width s .
The above mentioned hyper-parameter selection involves a huge increase in the computational cost of the training: one has to a priori define a grid of  X  C , s  X  pairs and then solve 10 SVM problems of size 90% of the complete dataset per grid pair. Of course more sophisticated ways of surfing the grid can help reduce this computational burden ( Momma and Bennett, 2002 ;
Ortiz-Garc X   X  a et al., 2009 ), but the question about which range of values to explore for both parameters remains open.

Although many authors have explored the use of different parameter setting techniques in machine learning algorithms ( Shaheen et al., 2010 ; Pavo  X  n et al., 2008 ; Bengio, 2000 ) and, in particular, in SVMs ( Friedrichs and Igel, 2004 ; Samanta et al., 2003 ), most of these methods are based on high computational
Samanta et al., 2003 ). This fact has prevented these techniques from becoming mainstream, and most practitioners still prefer to cross-validate their algorithms X  parameters, see, for instance Kohavi and John (1995) .

In this paper we focus on the estimation of the Gaussian kernel width. The motivation for this choice is twofold: on the one hand, s determines the effective rank of the kernel matrix, and thus the degrees of freedom that we may use to build a linear classifier in the feature space. In this sense, very small values of s lead to ultra-localised kernels, resulting in a classifier equivalent to the 1-Nearest Neighbour; while very big values of s lead to a classifier that always predicts the most populated class. On the other hand, once s is fixed, the kernel matrix will not change with the variations on parameter C . This enables the use of a shared kernel matrix cache among all the explored values of C , saving computational cost.

Bearing in mind that the value of s relates to the locality of the dataset, we propose to explore three methods to estimate s based on the performance of two cheap non-linear classifiers that somehow bring out the local structure of the dataset, enabling us to choose a value for s that captures this locality. The first cheap classifier is based on K -Nearest Neighbours ( Cover and Hart, 1967 ). The second one is based on the distance from each data point to its nearest opposite-class neighbour. The third one employs a fast clustering algorithm to extract the local geometry of the problem.

The three methods compare favourably against 10-fold cross-validation in the selection of the s in several UCI classification tasks ( Blake and Merz, 1998 ).

The rest of the paper is organised as follows. Section 2 reviews the SVM and the algorithms that are used to implement the proposed s estimation methods. Section 3 motivates and presents in detail the three estimation algorithms. Section 4 includes an empirical evaluation of the SVM classification performance under the proposed model selection schemes; we use 10-fold cross-validation as baseline method. Finally, Section 5 closes the paper with the main conclusions and the description of our ongoing research. 2. Background
This section includes a brief review of all the techniques and algorithms employed in the proposed methods to estimate s . 2.1. Machine classification
Consider a classification problem defined in terms of a set of labelled examples f X  x i , y i  X g n i  X  1 , with observations x y
A f 1 ; 2 , ... , L g indicating the class to which observation x belongs. There are two general approaches to tackle the design of an automatic classifier for this problem ( Webb, 2002 )
Generative approach : Data are used to learn a probabilistic model for the distribution of each class. This model consists of the following probability density functions: p  X  x 9 y  X  l  X  and of the Bayes optimal classification rule
Discriminative approach : One selects a parametric classification rule (also denominated discriminant function) f w  X  x  X  whose output determines the class of instance x , and uses the data to fit the parameters w .
 Despite the optimality underlying the former, in practice the latter is more common since the fitting of the discriminant function posses an easier optimisation problem (with a smaller number of variables to optimise) than the learning of a more or less complex probabilistic model. This intuition can be also borrowed from daylife experience; it is well known that children are able to separate dogs from cats without coming up with a precise definition of each class.

A machine learning version of Occam X  X  Razor principle translates to stick to the simplest discriminant function able to solve the classification problem. This principle helps to prevent from a bad generalisation due to overfitting. One of the simplest discriminant functions for binary problems is linear classifiers o  X  x  X  X  sign f f  X  x  X g X  sign f w T x  X  b g X  1  X  since the number of parameters is equal to the size of the observa-tions (plus one). Moreover, it is well known from the machine learning community experience that linear classifiers work reason-ably well in a good number of applications. Linear classifiers as formulated in Eq. (1) solve binary problems (they are able to discriminate between two classes). The linear classification rule defines a separating hyperplane (or b oundary) that divides the input space in two halfspaces, each one associated with one output class (see Fig. 1 ). For one class, usually termed positive class, one expects that f  X  x  X  4 0, whilst for the other class, termed negative class, one expects that f  X  x  X  o 0. The quantity yf  X  x  X  measures how well is x classified if y is its label. A much greater than zero value of yf  X  x  X  indicates that x is on the correct side of the boundary and well away from it. Analogously, a negative value of yf  X  x  X  indicates that the pattern is on the wrong side of the boundary. The more negative the value of yf  X  x  X  is, the further from the boundary (and from being correctly classified) x lies.

Nevertheless, it is straightforward to reformulate any multi-class problem as a pool of binary classifications following one of these strategies:
One vs.
One vs. one :
The training of a linear classifier for a linearly separable problem consists in determining the value of its weight vector w and bias term b . It usually involves the optimisation of a functional that includes a penalty term favoring a reduced number of errors among the training data set plus some regularisation terms that ensure a good generalisation capability (good classification rates with testing different classification algorit hms. Among the most widely used functionals one can find: method due to their excellent performance in many diverse applications. However, this popularity is due mainly to their nonlinear version that will be reviewed in the next section. 2.2. Nonlinear SVMs and their hyper-parameters (3) is the soft margin concept to tackle problems that are not linearly separable. The soft margin consists in the addition of a set tion of errors in the training set, i.e., violations on the constraints of Eq. (3). The functional for the non-separable case becomes min subject to y  X  w T x i  X  b  X  Z 1 x i i  X  1 , ... , n  X  5  X  x
Z 0 i  X  1 , ... , n  X  6  X  where C is a hyper-parameter that trades-off between the max-imisation of the margin and the minimisation of the training errors.

Moreover, the SVM can be extended to a nonlinear version by means of the so-called kernel trick ( Sch  X  olkopf and Smola, 2002 ).
The kernel trick is a direct way of derivating nonlinear versions of algorithms where the input data appear exclusively inside scalar products. This trick consists in replacing all the scalar products between input patterns by evaluations of a kernel function (those satisfying Mercer X  X  Theorem Sch  X  olkopf and Smola, 2002 ). This procedure assumes that the data are nonlinearly mapped onto a feature space where the scalar product between two mapped examples can be computed by evaluating a kernel function on the examples in the input space.

Let us assume /  X  x  X  is the nonlinear map associated to a kernel function k  X  x i , x j  X  , i.e.,  X  x i , x j  X  X  /  X  x i  X  T /  X  x j  X 
Then, the linear classifier in feature space will result from the following optimisation min subject to y  X  w T /  X  x i  X  X  b  X  Z 1 x i i  X  1 , ... , n  X  8  X  x Z 0 i  X  1 , ... , n  X  9  X 
The introduction of the constraints of Eqs. (8) and (9) in the minimisation of Eq. (7) with Lagrange Multipliers a i and m the following functional: 1 2
J w J 2  X  C
At the optimum, the following Karush X  X uhn X  X ucker (KKT) conditions apply: w  X   X  m i  X  C  X  13  X 
Eq. (11) shows that the weight vector is a linear combination of some input data (those with a non-zero Lagrange Multiplier) called support vectors (see Fig. 2 ). Moreover, the KKT conditions transform the optimisation of Eq. (10) in the following (dual)
Quadratic Program: max subject to 0 r a i r Ci  X  1 , ... , n  X  15  X  and to the constraint in Eq. (12). Once the SVM training is finished, each training datum x i can be assigned to one of the following three categories according to the value of its corre-sponding a i :
Non-critical data: training samples with a i  X  0. These data are correctly classified and well away from the margin. They are not needed for the definition of the discriminating function. In fact, if any of these data were removed from the training set, the classification boundary would remain the same.

Non-bounded support vectors: training samples with 0 o a i o C . These data are correctly classified but lie on the the classification boundary.

Bounded support vectors: support vectors with a i  X  C . These data lie on the wrong side of the margin ( y i  X  w T x i  X  b  X 
Moreover, those bounded SVs with y i  X  w T x i  X  b  X  o 1 are con-sidered outliers by the final classifier (samples mostly sur-rounded by samples belonging to the other class). In this sense, the value of C limits the influence of these outliers in the classification of the data of the other class, since it imposes an upper bound on the weight of each outlier in the final classifier.

Therefore, the selection of C and s is critical in the training of an SVM, because they determine the optimisation that leads to the classifier w . This selection is ideally made using prior knowl-edge about the problem, since the importance of the regularisa-tion over the fitting of the training data (controlled with C ) and the resolution of the Gaussian kernel (more on this in Section 3 ) are particular of each problem. When prior information is not available, the selection is usually carried out by cross-validation. One has to fix a priori a set of pairs  X  C , s  X  . For each point in the grid, one gets the cross-validation error averaging the following N errors: the training set is partitioned in N folds and for each fold one obtains the classification error of an SVM trained with the N 1 left-out folds. The selected pair  X  C , s  X  is the one with the smallest averaged cross-validation error. The most common selection for the number of folds is N  X  10, therefore the cost of the hyper-parameter selection is roughly 10 times the number of tried pairs  X  C , s  X  multiplied by the training of an SVM with a 90% of the training examples. Since C and s could in principle take any positive real value, the most common way to determine the range of interest is to do an iterative refining of the grid. One starts with a logarithmic scale that is further refined in linear scales zooming into the area where the minimiser of the validation error lies. 2.3. K-Nearest Neighbours Another well-known automatic classifier is the simple K -Nearest Neighbours ( K -NN) algorithm ( Cover and Hart, 1967 ). It does not have a training stage and defers all computation to the testing stage. At this stage, all training samples are used to label the new point.

To classify a new sample, distances to every training sample are computed, and the labels of the K nearest samples are considered. In the standard K -NN approach, each neighbour means one vote for the neighbour label, and the test sample is classified according to the majority vote. In binary classification scenarios, K is usually an odd number in order to avoid ties. In this work we will use the soft variation, in which each neighbour vote is weighted by the inverse of its distance to the test sample, so that samples closer to our new candidate have a greater effect on the selected class.

The only free parameter for this classification method is K , the number of neighbours that will be considered for classification; it is usually selected by cross-validation. 3. Estimation of r for SVM
The value of parameter s determines the locality of the kernel in the sense that it somehow limits the effective area of influence of each sample. Fig. 3 shows the same data set classified by SVMs with noticeably different values of s . As this value decreases the
SVM is able to draw a more complex classification boundary, focusing on the local features of the problem. Notice how for the smallest value of s in Fig. 3 (c) the discriminating boundary perfectly encloses most samples of the class represented with red dots. In this last case the classification rule turns very local: classify as red dot any sample falling within the closed boundaries and as blue crosses the rest of the input space. This locality of the rule is specially remarked in the small closed boundary in the middle of the plot.

A small s enables a more detailed definition of the boundary in the region where the two classes overlap in exchange for a risky generalisation capability in areas far from this region. Test points belonging to the blue cross class that fall above the training examples will be misclassified in spite of being closer to the blue-crosses training samples (see Fig. 3 (b)). In addition, red dot test points falling towards the left-bottom or right-bottom corners will be misclassified in Fig. 3 (c) (in spite of being farther from the training blue crosses than from the red dots).

With this intuition in mind, we propose to find out a good value for s gathering information about the local structure of the dataset. The remainder of the section presents three methods to unveil this underlying local structure. The three methods share a common principle: they first determine for each training instance the value of s that constraints the area of influence of this instance as potential support vector to the classification of the closest points. Then, all these values are averaged to estimate the parameter s for the SVM. 3.1. Soft K-NN estimation of s To estimate s we can resort to the use of a cheaper classifier. Here we explore the possibility of using the soft K -NN algorithm to estimate a value for s . Using this soft version means that each sample label gets weighted by the corresponding inverse of the distance. We will select the value of K that minimises the Leave One Out (LOO) estimation of the error rate.
 training dataset to every other training sample, and sort them in ascending order. This takes O  X  n log n  X  per sample. Then, for any given value of K , the LOO estimation of the i th sample label is ^ y i  X  sign 1 ... K max o n in O  X  K max  X  time per sample, since we are just adding new terms to the summation in Eq. (16). This enables us to compute LOO error rates for every possible value of K and select the best one in O  X  n 2 log n  X  time, less than the time needed to solve the SVM problem. In general, there is no need to explore all possible values of K , since big values do not generally improve the error rates. In our experiments, the optimal resulting values for K were always equal or smaller than 100, despite considering several thousands of training samples.
 we consider the distances d ij  X  J x i x j J between each sample x kernel can be expressed as k  X  x i , x j  X  X  exp  X  d 2 ij = s s 2 normalises d 2 ij . If we want to select this normalisation accord-ing to the K -Nearest Neighbours, a reasonable choice is to set s 2  X  E  X  d 2 ij , where the expectation only takes into account dis-tances d ij between a sample and its K -Nearest Neighbours. distributed as a multivariate Gaussian with centre x i and covar-iance matrix v I , then d 2 ij is a random variable distributed as a scaled Chi-squared distribution with D degrees of freedom (see
The value of v is unknown, but we can use maximum likelihood to estimate it from data v which is the standard estimator for the variance of a multivariate isotropic Gaussian, and then set s 2  X  Dv ML  X  18  X 
This s 2 corresponds to the maximum likelihood estimation of the expected sample-to-neighbours squared distance. In this way, we are proposing to select an SVM kernel in which the influence of each sample over the rest is in accordance with the influence determined by the K -NN classifier. This computation does not increase the previously stated complexity and therefore we end up with a s selection procedure that incurs almost no overhead compared with training a single SVM (whose computational complexity is super-quadratic in the number of samples). 3.2. Nearest Enemy estimation of s
This method is built upon three assumptions about the structure of a dataset that is favorable to the use of a local kernel such as the RBF one 1. We can simplify the dataset to an equivalent separable dataset that results in the same classification boundary as the whole dataset ( Bak X r et al., 2005 ). 2. The clustering assumption: once we have this separable dataset, the data points belonging to each class are organised in clusters. 3. In the ideal case, the local kernel should limit the influence of each support vector to the nearest points belonging to its class. This assumption resembles the principles of the Set Cover Machine ( Marchand and Shawe-Taylor, 2002 ).

The first step consists in editing the training set and removing patterns likely to become bounded support vectors (they will end up on the wrong side of the classification boundary). To carry out this step, we have developed the following modification of the multiedit algorithm ( Devijver and Kittler, 1982 ): 1. Divide the training set in M folds. 2. Classify the patterns in the m th fold with the 1-NN classifier formed by the patterns in the remaining M 1 folds. 3. After processing the M folds remove the incorrectly classified patterns. 4. If the total number of iterations is not reached, return to step 1.

We carry out five iterations of the editing algorithm. The motiva-tion for the modification of multiedit is that we observe that the original multiedit presents problems when classes present unba-lanced populations. In the original multiedit each fold is classified with another fold and the iterations run until convergence (when all remaining patterns are correctly classify by the other fold).
When a class is much less populated than the other, the multiedit algorithm tends to eliminate all the patterns of the minority class.
The use of M 1 folds for the 1-NN classifier and the early stopping after five iterations circumvents this problem.
If we compute and store the sorted (in ascending order) distances between each pair of training samples the edition takes O  X  n 2 log n  X  time.

From the edited training data set, the Nearest Enemy approach to select parameter s consists of two single steps: 1. Obtain the distances between each data and the nearest data from the opposite class (Nearest Enemy). 2. Calculate s as the average of these distances.
 Since the sorted distance matrix is known beforehand, the computational cost is linear with n . Therefore, the whole Nearest Enemy takes O  X  n 2 log n  X  time. 3.3. Redundant Fast Clustering estimation of s
We can extend the Nearest Enemy method clusterwise, i.e., compute distances between clusters of data of different classes rather than between single points.

To cut the computational burden as much as possible, we have employed the following method to obtain clusters of data belong-ing to the same class. We start from the same edited training set as the Nearest Enemy. Note that we have stored the sorted distances between each pair of training points. Consider these distances are stored in a matrix D where the i th row contains the sorted distances between x i and the rest of the training set. indicates whether x j belongs (or not) to the same class as x 2. Form a cluster for each row of D including the patterns with a positive d ij until the first negative element is reached. 3. Compute the mean of each cluster (centroid). 4. Estimate s as the average of the distances between each centroid and the nearest centroid of the opposite class.
Notice that this method, that we term Redundant Fast Cluster-ing, gives more importance to data in large clusters, since a cluster appears in the average for s as many times as the number of its members. Although this method is costlier than the previous, it still takes O  X  n 2 log n  X  time. 3.4. About the proposed methods
These three proposals rely on different assumptions about data, so the performance of the final machine will depend on whether the problem at hand actually fulfills them.

Nearest Enemy and Redundant Fast Clustering assume that the data is transformable into an equivalent, separable data set which exhibits clustering; whereas soft K -NN estimation relies only on the smoothness of the regression surface being similar in all directions (isotropic) and a dense enough dataset. Since the soft K -NN assumptions are also implied by the use of the RBF kernel itself, this seems to be the safest of the three proposed options. We will confirm this in Section 4 .

Choosing the appropriate method is the same problem faced in practice when selecting among the available SVM kernel types: to determine a priori which data assumption will better describe a given problem. Two possible approaches are: (a) check which strategy works best with other data sets from the same domain and then select that one or (b) use cross-validation to test the different alternatives. As argued above, when in doubt, soft K -NN estimation seems to be the safest option. 4. Experiments 4.1. Experimental setup
We consider the UCI problems described in Table 1 in terms of number of samples and input dimension.

For the 10-fold cross-validation, the best pair has to be sought in a grid since there is no prior knowledge about the problems. This absence of prior knowledge determines a range of values for
C that serves for all the considered datasets. We opt for a logarithmic scale of 12 values that contains a reasonable good value of C for each dataset. It could be possible to obtain a better value of C for each dataset zooming in the proximities of the better values of this grid, but this would further increase the computational burden of the search, as well as the risk of over-fitting (since the data sets are not infinite in length an exhaus-tively searched best C for the training set can result non-optimal for the test set). The considered range for C is C
A
With respect to the range for the value of s , it is usual to consider multipliers of
In the Gaussian kernel, s 2 normalises the Euclidean distance between the kernel arguments. This distance is a sum of d elements, therefore it is reasonable to think in normalisations proportional to the number of summands. In this paper we have considered the a range for s of 15 multipliers k s so that s  X  k where k s A
The plots in Fig. 5 show contours of the test error as a function of the explored values of C and s . Notice how the a priori selected grids grab enough insight about the shape of the dependence of the error with the hyper-parameters.

For the proposed methods, the value of s for each partition is determined as indicated by the corresponding method. Once s is fixed, the value of C is obtained through 10-fold cross-validation using the same range described in the previous paragraph.
Once we have found the pair  X  C , s  X  for each one of the 10 partitions, we train an SVM with the whole training set and calculate the classification error on the test set. The classification errors for each dataset reported in the next tables correspond to an average over the 10 test errors. 4.2. Results and discussion
Table 2 shows the test error rates achieved by the SVM with s estimated using the different methods. According to the test error rates, the method based on Soft K -NN obtains the best s for three out of the six problems, cross-validation on the grid finds the best model in two of the problems, while the Nearest Enemy strategy finds the best s in one problem.

Though the performance of Redundant Clustering is not the best for any of the proposed datasets, it is very close for form and pima . It is also noticeable how this method gets completely lost in problem ringnorm . We believe that the dominance of large clusters is not a good strategy for this dataset, since the Nearest Enemy yields a performance comparable to the other two methods.
 distributions of test errors overlap, and therefore we can conclude that (with the above mentioned exception of ringnorm ) perfor-mances are comparable, whereas computation time is significantly reduced using any of the proposed schemes (see
Section 4.3 ). To gain more insight on the limitations due to searching within a grid, we display in Fig. 5 the contour plots of the Classification Error Rate in the test set for a partition of every dataset. The contours are obtained using the same grid as cross-validation. The proposed methods (specially soft K -NN) provide a s ) very close to wide areas of reduced test error according to the contours. This is specially noticeable in the plots corresponding to datasets waveform , pima and spam . The contour plots also show that for small values of s (left side of the plots) the error of the classifier is less dependent on the value of C that on the value of s .
For bigger values of s the error generally decreases as C decreases (this means that the margin of the SVM increases). This interplay between the values of C and s explains the acceptable test error rates obtained with the cross-validated selection of both para-meters in problems pima and spam , where the value for s found by cross-validation is far larger than the value suggested by the proposed methods (that do not have information about suitable values of C ). 4.3. Computational cost
In this section we provide detailed timings for each of the discussed model selection strategies and datasets.
 Table 3 shows the CPU time corresponding to the simulations of Table 2 . These simulations were executed in an Intel Xeon X5675 CPU, clock speed of 3.07 GHz, dual pr ocessor, six cores per processor and 48 Gb of RAM. The operative system was Linux Gentoo and the code of the s estimation methods was written in Matlab 7.5.0.338. We used the LIBSVM ( Chang and Lin, 2011 ) implementation of the Support Vector Machine. We have separated the CPU time corre-sponding to the estimation of the kernel parameter (under columns labelled as s ) and of the regularisation parameter (columns labelled as C ). Notice how the computational burden of the estimation of the kernel parameter is much lower than the cost of the cross-validation. In fact our experiments show a 15 speedup using the proposed methods (since the 15 candidate values for s are replaced with a single value estimation). 5. Conclusions
This paper has introduced three methods to estimate an appropriate value for the spread parameter of the Gaussian kernel in an SVM. The three methods exploit fast ways of getting information about the local structure of the problem and present a computational time of O  X  n 2 log n  X  , where n is the number of datapoints. Usage of these methods substantially speeds up the training process of an SVM, since s is not cross-validated, but directly computed without incurring any significant overhead. Another clear advantage of these methods with respect to cross-validation is that there is no need to a priori select a set of s values for exploration.

From the classification error rate point of view, the empirical work shows that the proposed algorithms compare favourably or exhibit no significant difference with respect to the standard hyper-parameter estimation based on 10-fold cross-validation.
Future lines of work include the identification of the particular features of each dataset that favour performance for each of the three methods, as well as exploiting the information obtained about the local structure of the problem to create a multi-kernel classifier, where every support vector is associated to a different value of s  X  s This can be achieved in a straight forward manner using a Gaussian kernel of the form k  X  x i , x j  X  X  exp f J x i x j J 2 =  X  2 s In this work, we have only dealt with binary classification. Another interesting research line would be to empirically study how well the presented approach works when dealing with multiple classes. As discussed in Section 2 , when several classes are present in a dataset, the standard approach is to decompose it as a set of binary classification problems, which can then be solved as described here. This can be done using the two discussed paradigms: one vs. all and one vs. one . In one vs. all, a specific binary SVM is trained for each class, so that it is able to distinguish between the given class and the rest. Test instances are then classified according to the SVM with outputs the largest value. In one vs. one, a different SVM is trained to distinguish each possible pair of classes. Test instances are evaluated by all the
SVMs, each casting a vote to the winning class. The instance is classified according to the class that received a maximum number of votes. In both cases, the approach discussed here is directly applicable.
 Acknowledgments
Authors X  work was partly supported by MICINN Grants TIN2011-24533, TEC2011-22480 and PRI-PIBIN-2011-1266 (Spanish Govern-ment). The first author was also partly supported by MICINN CONSOLIDER-INGENIO project C SD2008-00010 (COMONSENS). References
