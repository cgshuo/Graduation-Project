 Word2vec, a word embedding technique, has gained significant in-terest among researchers in natural language processing (NLP) in recent years. The embedding of the word vectors helps to identify a list of words that are used in similar contexts with respect to a given word. In this paper, we focus on the use of word embeddings for enhancing retrieval effectiveness. In particular, we construct a gen-eralized language model, where the mutual independence between a pair of words (say t and t 0 ) no longer holds. Instead, we make use of the vector embeddings of the words to derive the transformation probabilities between words. Specifically, the event of observing a term t in the query from a document d is modeled by two distinct events, that of generating a different term t 0 , either from the docu-ment itself or from the collection, respectively, and then eventually transforming it to the observed query term t . The first event of gen-erating an intermediate term from the document intends to capture how well a term fits contextually within a document, whereas the second one of generating it from the collection aims to address the vocabulary mismatch problem by taking into account other related terms in the collection. Our experiments, conducted on the stan-dard TREC 6-8 ad hoc and Robust tasks, show that our proposed method yields significant improvements over language model (LM) and LDA-smoothed LM baselines.
 H.3.3 [ INFORMATION STORAGE AND RETRIEVAL ]: Infor-mation Search and Retrieval X  Retrieval models, Relevance Feed-back, Query formulation Theory, Experimentation Generalized Language model, Word Embedding, Word2Vec
Word embedding as technique for representing the meaning of a word in terms other words, as exemplified by the Word2vec ap-proach [7]. The embedding of the word vectors enables the identifi-cation of words that are used in similar contexts to a specufic word. a list of words that are used in similar contexts with respect to a given word. While word Embedding has gained significant interest among researchers in natural language processing (NLP) in recent years, there has to date been little exploration of the potential for use of these methods in information retrieval (IR).

This paper explores the use of word embeddings of enhance IR effectiveness. We begin with a brief introduction to word embed-ding techniques and then motivate how can these be applied in IR.
A brief introduction to word embedding . Word embedding techniques seek to embed representations of words. For example, two vectors ~ t and ~ t 0 , corresponding to the words t and t in an abstract space of N dimensions if they have similar contexts and vice-versa, (i.e. the contexts in turn having similar words) [4]. Use of a cosine similarity measure on this abstract vector space of embedded words can be used to identify a list of words that are used in similar contexts with respect to a given word. These se-mantically related words may be used for various natural language processing (NLP) tasks. The general idea is to train moving win-dows with vector embeddings for the words (rather than training with the more conventional word count vectors), and classify the in-dividual windows [2]. This finds application for examples in appli-cations such as POS tagging, semantic role labeling, named-entity recognition and other tasks. The state-of-the-art word embedding approaches involve training deep neural networks with the help of negative sampling [7]. It is reported that this process of negative sampling (commonly known as word2vec 1 ) produces reliable word embeddings in a very efficient manner [7].

Potential use in IR . We now discuss how word embeddings can potentially be helpful in improving retrieval quality. In the context of IR, vocabulary mismatch , i.e. the inherent characteristic of us-ing different but semantically similar terms across documents about
The name word2vec comes from the name of the software tool released by Micholov et. al. ( https://code.google.com/ p/word2vec/ the same topic or between a query and its relevant documents, is a difficult problem to solve.

However, the working principle of most standard retrieval mod-els in IR involves an underlying assumption of term independence, e.g. the vector space model (VSM) assumes that the documents are embedded in a mutually orthogonal term space, while probabilis-tic models, such as the BM25 or the language model (LM) assume that the terms are sampled independently from documents. Stan-dard approaches in IR take into account term association in two ways, one which involves a global analysis over the whole collec-tion of documents (i.e. independent of the queries), while the other takes into account local co-occurrence information of terms in the top ranked documents retrieved in response to a query. The latter approach corresponds to the relevance feedback step in IR which we do not investigate in this paper. Existing global analysis meth-ods such as the latent semantic indexing (LSA) [3] or latent Dirich-let allocation (LDA) [1] only take into account the co-occurrences between terms at the level of documents instead of considering the context of a term. Since the word embedding techniques that we in-troduced in the beginning of this section, leverage the information around the local context of each word to derive the embeddings (two words have close vector representations if and only if they are used in similar contexts), we believe that such an approach can potentially improve the global analysis technique of IR leading to better retrieval effectiveness.

The rest of the paper is organized as follows. Section 2 discusses related work. In Section 3, we propose the generalized LM, which is evalaued in Section 4. Finally, Section 5 concludes the paper.
Latent semantic analysis (LSA) [3] is a global analysis tech-nique in which documents are represented in a term space of re-duced dimensionality so as to take into account inter-term depen-dencies. More recent techniques such as the latent Dirichlet allo-cation (LDA) represent term dependencies by assuming that each term is generated from a set of latent variables called the topics [1]. A major problem of these approaches is that they only con-sider word co-occurrences at the level of documents to model term associations, which may not always be reliable. In contrast, the word embeddings take into account the local (window-based) con-text around the terms [7], and thus may lead to better modeling of the term dependencies.

Moreover, most of these global analysis approaches, e.g. LDA, have been applied in IR in an ad-hoc way for re-assigning term weights without explicitly representing the term dependencies as an inherent part of an IR model. For example, an LDA document model (term sampling probabilities marginalized over a set of la-tent topic variables) is linearly added as a smoothing parameter to the standard LM probability [9], as a result of which the term de-pendencies are not clearly visible from the model definition. Con-trastingly, in this paper, we intend to directly model the term de-pendencies as a part of an IR model.
In this section, we propose the generalized language model (GLM) that models term dependencies using the vector embeddings of terms.
In LM, for a given query q , documents are returned as a ranked list sorted in decreasing order by the posterior probabilities P ( d | q ) . These posterior probabilities are estimated for each document d during indexing time with the help of the prior probability ( P ( q | d ) ) according to the Bayes rule [8, 6, 10].
 P ( d | q ) = P ( q | d ) .P ( d ) P = Y In Equation 1, the set C represents a universe of documents (com-monly known as the collection ),  X  P ( t | d ) and  X  P ( t | C ) denote the maximum likelihood estimated probabilities of generating a query term t from the document d and the collection respectively, using frequency statistics. The probabilities of these two (mutually exclu-sive) events are denoted by  X  and 1  X   X  respectively. The notations tf ( t,d ) , | d | , cf ( t ) and cs denote the term frequency of term t in document d , the length of d , collection frequency of the term t and the total collection size respectively.
As per Equation 1, terms in a query are generated by sampling them independently from either the document or the collection. We propose the following generalization to the model. Instead of as-suming that terms are mutually independent during the sampling process, we propose a generative process in which a noisy channel may transform (mutate) a term t 0 into a term t . More concretely, if a term t is observed in the query corresponding to a document d , according to our model it may have occurred in three possible ways, shown as follows.
Transformation via Document Sampling . Let P ( t,t 0 | d ) de-note the probability of generating a term t 0 from a document d and then transforming this term to t in the query.
 In Equation 2, P ( t 0 | d ) can be estimated by maximum likelihood with the help of the standard term sampling method as shown in Equation 1. For the other part, i.e. transforming t 0 to t , we make use of the cosine similarities between the two embedded vectors corre-sponding to t and t 0 respectively. More precisely, this probability of selecting a term t , given the sampled term t 0 , is proportional to the similarity of t with t 0 . Note that this similarity is independent of the document d . This is shown in Equation 3, where sim ( t,t the cosine similarity between the vector representations of t and t and  X ( d ) is the sum of the similarity values between all term pairs occurring in document d , which being the normalization constant, can be pre-computed for each document d .
 Consequently, we can write Equation 2 as Equation 4 favours those terms t 0 s that are not only tend to co-occur with the query term t within d , but are also semantically related to Figure 1: Schematics of generating a query term t in our pro-posed Generalized Language Model (GLM). GLM degenerates to LM when  X  =  X  = 0 . it. Thus, words that are used in similar contexts with respect to the query term t over the collection, as predicted by the vector embed-dings, are more likely to contribute to the term score of t . In other words, Equation 4 takes into account how well an observed query term t contextually fits into a document d . A term contextually fits well within a document if it co-occurs with other semantically sim-ilar terms. Terms, score high by Equation 4, potentially indicate a more relevant match for the query as compared to other terms with low values for this score.

Transformation via Collection Sampling . Let the complemen-tary event of transforming a term t 0 , sampled from the collection instead of a particular document, to the observed query term t be denoted by P ( t,t 0 | C ) . This can be estimated as follows. Now P ( t | t 0 ,C ) can be estimated in a way similar to computing P ( t | t 0 ,d ) , as shown in Equation 3. However, instead of consider-ing all ( t,t 0 ) pairs in the vocabulary for computation, it is reason-able to restrict the computation to a small neighbourhood of terms around the query term t , say N t because taking too large a neigh-bourhood may lead to noisy term associations. This is shown in Equation 6.
 While P ( t,t 0 | d ) measures the contextual fitness of a term t in a document d with respect to its neighbouring (in the vector space of embedded terms) terms t 0 in d , P ( t,t 0 | C ) , on the other hand, aims to alleviate the vocabulary mismatch between documents and queries in the sense that for each term t in d it expands the doc-ument with other related terms t 0 s. From an implementation per-spective, P ( t,t 0 | d ) reweights existing document terms based on their contextual fit, whereas P ( t,t 0 | C ) expands the document with additional terms with appropriate weights.

Combining the Events . Finally, for putting all the events to-gether in the LM generation model, let us assume that the proba-bility of observing a query term t without the transformation pro-cess (as in standard LM) be  X  . Let us denote the probability of sampling the query term t via a transformation through a term t sampled from the document d with  X  , and let and the complemen-tary probability of sampling t 0 from the collection be then  X  , as shown schematically in Figure 1. The LM term generation proba-bility in this case can thus be written as shown in Equation 7. This is a generalized version of the standard LM, which we now hence-forth refer to as generalized language model (GLM), that takes into account term relatedness with the help of the noisy channel trans-formation model, which in turn uses the word embeddings to derive the likelihood of term transformations. Note that the GLM degen-erates to standard LM by setting  X  and  X  to zero, i.e. not using the transformation model in the term generation process.

P ( t | d ) =  X P ( t | d ) +  X  X
An efficient approach to get the neighbours of a given term is to store a pre-computed list of nearest neighbours in memory for every word in the vocabulary. After this step, for each document d in the collection, we iterate over term pairs ( t,t 0 ) and assign a new term-weight to the term t representing the document sampling transformation according to Equation 4. Then we iterate again over every term t in d and use the pre-computed nearest neighbours of t ( N t ) to compute a score for the collection sampling transformation, as shown in Equation 6. To account for the fact that these transfor-mation probabilities are symmetrical, we add the term t 0 that it is not required to add the term t 0 in case of the document sampling transformation event because t 0 is already present in d .
Experimental Setup . Our experiments were conducted on the standard TREC ad hoc tasks from TREC 6, 7, 8 and the Robust track. Information about the document and the query sets is out-lined in Table 1. We implemented GLM using the Lucene framework. As one of our baseline retrieval models, we used stan-dard LM with Jelinek Mercer smoothing [6, 10], which is dis-tributed as a part of Lucene. Additionally, we also used LM with LDA smoothing [9] as our second baseline to compare against. In contrast to [9], which reports retrieval results with LDA smoothed LM (LDA-LM) on individual document subsets (and their corre-sponding relevance judgements) from the TREC collection as cat-egorized by their sources, i.e. the  X  X A Times X  and the  X  X inancial Times X , we instead executed LDA on the whole TREC collection. The rationale for using LDA as a baseline is that analogous to our model, LDA also attempts to model term dependencies by taking into account latent variables (called the topics). This baseline was also implemented in Lucene.

Parameters . The parameter  X  of the LM baseline was empiri-cally set to 0 . 2 (after varying it within a range of [0 . 1 , 0 . 9] ). This value of  X  for the TREC collection agrees with the observations reported in [6]. According to the findings of [9], the number of topics in LDA, i.e. K , was set to 800 . As prescribed in [5], we set the LDA hyper-parameters  X  and  X  (note that these are different from the GLM parameters) to 50 /K and 0 . 01 respectively. Ob-taining effective word embedding is an integral part of the GLM. The word embeddings for the experiments reported in this section were obtained on the TREC document collection with the param-eter settings as prescribed in [7], i.e., we embedded the word vec-tor in a 200 dimensional space, and used continuous bag-of-words http://lucene.apache.org/core/ Figure 2: Effect of varying the GLM parameters  X  and  X  on the MAP values for the TREC query sets. with negative sampling. The neighbourhood N t of the GLM (see Equation 7) was set to 3 , i.e., for each given term in a document, we consider adding at most 3 related terms from the collection.
Results . First, we varied the GLM parameters, namely  X  and  X  within the range [0 . 1 , 0 . 4] so as to ensure that  X  +  X  +  X  &lt; 1 (  X  being set to 0 . 2 ) for all the query sets used in our experiments. The results are shown in Figure 2. It can be seen that the optimal values of  X  and  X  depend on the query set, e.g. for the TREC 8 query set (Figure 2c, the optimal results are obtained for (  X , X  ) = (0 . 3 , 0 . 2) , whereas this combination does not produce the optimal results for the other query sets. It can be observed that a reasonable choice for these parameters is in the range [0 . 2 , 0 . 3] , which means imparting more or less uniform weights to all the term generation events, namely  X  ,  X  and  X  . In Table 2, we show the optimal results obtained with GLM for each individual query set and compare the results with the baselines, i.e. the LM and the LDA-LM. It can be observed that for each query set, GLM significantly 3 outperforms the baselines. It turns out that the LDA-LM (almost) consistently outperforms the standard LM. However, the results (as measured by the percentage gains in comparison to standard LM) do not seem to be as high as reported in [9] (about 3% as compared to about 8% ). We believe that the reason for this is due to the diversity in the LDA topics caused by the news articles from different sources.
From Table 2, we observe that GLM consistently and signifi-cantly outperforms both LM and LDA-LM for all the query sets. Not only does it increase the recall values in comparison to LM, but it also increases precision at top ranks by always outperforming LDA in terms of MAP. Although LDA achieves higher recall than GLM in two cases (TREC-6 and Robust), the higher recall in the case of LDA does not significantly increase the MAP, which is in-dicative of the fact that the precision at top ranks does not improve. For GLM however, an increase in the recall value is always asso-ciated with a significant increase in MAP as well, which indicates that precision at top ranks remains relatively stable in comparison to LDA. We proposed a generalized version of the language model for IR. Our model considers two possible cases of generating a term from
Measured by Wilcoxon statistical significance test with 95% con-fidence.
 Table 2: Comparative performance of LM, LDA and GLM on the TREC query sets.
 either a document or the collection and then changing it to another term after passing it through a noisy channel. The term transfor-mation probabilities of the noisy channel, in turn, are computed by making use of the distances between the word vectors embedded in an abstract space. We argue that this model has two fold advantage, firstly it is able to estimate how well a term fits in the context of a document, and secondly it is able to decrease the vocabulary gap by adding other useful terms to a document. Empirical evaluation shows that our method significantly outperforms the standard LM and LDA-LM. Possible future work will be to investigate composi-tionality of terms from the vector embeddings of words.

Acknowledgement . This research is supported by SFI through the CNGL Programme (Grant No: 12/CE/I2267) in the ADAPT Centre (www.adaptcentre.ie) at Dublin City University, and by a grant under the SFI ISCA India consortium.
