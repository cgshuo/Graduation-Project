 1. Introduction
Over recent years there has been growing interest in detection of opinions in online documents. One source of online doc-uments is web logs (blogs). A blog tracking company, Technorati, Inc., reported a dramatic rise in the number of blogs, track-ing 112.8 million blogs worldwide (About technorati, 2007 ), up from 4.2 million in October 2004 (Rosenbloom, 2004 ), which equates to more than 2500% growth.

In 2006, the Text Retrieval Conference (TREC) released the Blog06 document collection which included an opinion detec-tion task. Past opinion detection research has reported the detection of an opinion within a document without specifying the by asking participants to detect an opinion about a given topic. The detection of an opinion about a given topic complicates the opinion detection task because blogs usually have text covering multiple topics within the same blog document, some of which can be expressing an opinion about another topic (Oard et al., 2006; Yang, Yu, Valerio, &amp; Zhang, 2006 ). The lists of documents identified by the Blog06 participants as expressing an opinion on a given topic were assessed by
TREC assessors. A random selection of 100 blog documents from the documents assessed as being relevant to topics within the Blog06 corpus are used as a standard assessment in this study. These assessments are compared to the assessments of seven independent human assessors to determine the level of agreement between multiple assessors.
The results of the seven human assessors gives an indication of the subjectivity of opinion detection within blog docu-ments and allow the level of agreement between humans to be calculated. Knowing the level of agreement amongst humans is important because it sets an upper bound on the expected performance of an automated opinion detection system (AODS), and influences the process of building training and evaluation data sets. The Blog06 participants were able to determine their system X  X  relative performance using the rankings published in (Ounis, de Rijke, Macdonald, Mishne, &amp; Soboroff, 2006 ). The rankings enabled the participants to compare their system X  X  results to the results of the other participants. Those rankings are based on IR performance, and do not allow ready analysis of the opinion detection performance of systems, which our paper aims to address. Furthermore, the rankings do not give an indication of how well a system detects opinions in com-parison to the best possible performance. The research outlined in this paper discusses setting an upper bound on expected performance for each opinion detection corpus, to enable researchers to determine their system X  X  actual performance.
The remainder of this paper is structured as follows. Section 2 provides a discussion of opinion detection in blogs, spe-cifically the Blog06 corpus. Section 3 examines a study to determine the effect changes in assessments can have on the rel-ative performance of an information retrieval (IR) system. Section 4 describes the methodology applied in this inter-rater study, including the statistical reporting methods. The results are discussed in Section 5, including the level of agreement between the seven human assessors and a comparison between the human assessors and the runs submitted by the Blog06 participants. Conclusions from the study are presented in Section 7. 2. Opinion detection in blogs TREC provides large document collections to create a standardised international platform for studying a problem. In 2006,
TREC created a blog collection (Blog06), comprising of 3.2 million blog documents being monitored and downloaded over an 11-week period. The  X  X pinion Detection X  task in Blog06 asked participants to retrieve blog documents expressing an opinion on one of 50 given topics. Participants could submit up to five runs (lists of documents), which included retrieved documents expressing an opinion on a given topic ( Ounis et al., 2006 ).

A total of 56 runs consisting of the top 1000 opinion-bearing documents for each topic were submitted by the Blog06 participants. The participants assigned a priority value to each run. The two highest priority runs from each participant were pooled to a depth of 100 documents (27 runs) and the remainder of the runs were pooled to a depth of 10 documents ( Ounis et al., 2006 ), resulting in a list of 67,382 blog documents. These documents were assessed for the relevance of the document to the topic and also whether the document contained an opinion on the given topic.

A pool of five TREC assessors were used to assess the 67,382 blog documents documents per assessor. Table 1 (Ounis et al., 2006 ) shows the number of documents allocated to each of the assessment cat-egories. The Blog06 corpus is used as a test bed for this study due to the large number of assessed blog documents, which in-clude documents assessed as expressing opinions and non-opinions about the topic: 19,891 documents in 2006.
The TREC Blog06 assessment process is based on a similar process to previous TREC IR testbeds. However, opinion detec-tion systems and IR systems are quite different in their structure, indicating that the assessment process for an opinion detection task should also be different to the IR assessment process. In general, IR does not use training data in its process, while opinion detection systems generally need training data  X  none of the Blog06 participants used training data in their retrieval process, while all used training data in their opinion detection process. When detecting opinions within blog doc-uments, it is likely that training data specific to the query will not be available, due to the nature of a query. The standard TREC process for IR assessment is as follows:
Assessors create a set of topics (approximately 10 each);  X  The estimated number of relevant documents in the collection and load-balancing is used to determine which topics
Participants submit lists of the top 1000 documents for each topic (runs), ranked according to the relevance of the doc-ument to the topic as assessed by their system; Pools of documents are formed using the submitted runs to determine which documents are to be assessed;
The assessors who originally created the topic, assesses the documents in each topic pool.  X  Any documents not assessed are assumed to be not relevant to any topic (Voorhees, 1998 ).

The Blog06 data and opinion judgements will likely be used for future training data and testing of AODSs. This study dis-cusses the validity of using a single assessor on each document in the opinion detection judgements when the documents will be used as training data for an AODS. It is vital that the training data assessments are as valid and consistent as humanly possible. Analysis of the level at which humans can agree on the existence of an opinion will also reflect on the expected performance of an AODS.

The process of creating the topics for Blog06 differed slightly from the standard TREC process. Fifty topics were selected from a list of donated queries. The queries were used as the title fields of the topic and an interpretation of the query was used to develop the description and narrative (Ounis et al., 2006 ). Following is an example of a Blog06 topic. &lt;num&gt;Number: 877. &lt;title&gt;Sonic food industry. &lt;desc&gt;Description: how well are Sonic fast-food restaurants liked by those eating at fast food establishments. &lt;narr&gt;Narrative: a document should reveal the reason why Sonic fast food is enjoyed and what foods are liked best. The document may include why Sonic is chosen and what consistently brings the person back there. Documents which men-tion Sonic games are not relevant.

It is not stated whether the assessor who developed the topic is the same assessor who assessed the relevance of the doc-uments within the pool of documents created from the participant X  X  runs. The use of a single assessor in the assessment of IR document collections has been studied in the past (Voorhees, 1998 ), the results are discussed in the following section. 3. Information retrieval assessments
In 1998, the National Institute of Standards and Technology (NIST) investigated the effect differences in raters have on IR results ( Voorhees, 1998 ). The NIST investigation found that differences between raters do not affect the relative performance of IR systems when Mean Average Precision (MAP) is the reporting method. However, Lesk and Salton (1969) , cited in Voo-rhees (1998) reported that this is due to results being averaged across a number of queries, while the results of an individual query can change dramatically. The Voorhees (1998) findings, which used overlap (Jaccard similarity coefficient) to report agreement between assessors, are supported by the findings reported in Bailey et al. (2008) , which used the kappa statistic to measure the agreement between assessors.

Interestingly, Bailey et al. (2008) suggests that assessors not familiar with the topic or the task display greater variance in assessments and may be likely to affect system rankings. Having compared the variation between assessors in three classes standard as possible. The difficulty with opinion detection assessment in the context of the Blog06 corpus is the lack of gold standard assessors. There is no topic originator (an assessor with topic expertise) as the topics for the Blog06 corpus were se-lected from common search terms collected from a search engine. There is also a lack of expertise in opinion detection assess-ment as the task is more complex than that of IR assessment. Opinion detection assessments in Blog06 require the assessor to locate text relevant to the topic and then assess the text for the expression of an opinion about the topic.
Voorhees (1998) reported a higher level of instability in MAP results for systems that have an element of human inter-vention. In particular, systems that use relevance feedback in their design are more likely to vary in the results when there are different assessors used. This is particularly important to AODSs because it is likely that training data that has been as-sessed by different assessors will be used. Both Bailey et al. (2008) and Voorhees (1998) tested the effect of disagreements between assessors on information retrieval results. However, the research reported in this inter-rater study is attempting to develop a measure for setting an upper bound for automated opinion detection and to determine the affect differences in assessors have on the upper bound. The methodology for finding the possible level of inconsistency between human asses-sors and determining a upper bound for AODSs is reported in Section 4. The results are discussed in Section 5. 4. Methodology
This study determines the level of agreement between assessors, when given a topic and asked to indicate whether an opinion is being expressed about the topic within a given document. Blog documents were selected from the assessed por-tion of the Blog06 document collection. The selection of the documents was random within selected categories as indicated by the judgements provided with the document collection.
A total of 100 documents were selected from the documents assessed to be relevant to one of the topics. As there is no intention to test changes to the performance of an AODS or rankings within TREC participants and the documents are used to calculate the upper bound that could be expected of an AODS, only a small sample is needed. The number of documents se-lected from each category is shown in Table 2 . The 100 blog documents were randomly selected from the assessment cat-egories, resulting in 41 of the 50 topics being selected. Seven assessors were asked to read each document and judge whether the document was opinion-bearing on the given topic. If the judgement was opinion-bearing, the assessor was asked to determine whether the opinion was negative, mixed or positive. All seven assessors were given the same instructions as pro-vided to the original TREC Blog06 assessors. In keeping with the TREC assessment instructions, the seven assessors were not asked to rank the documents in order of strength of opinion or relevance. The seven assessors were made up of four male and three female assessors with six of the assessors having English as a first language.

The titles of the blog post were randomly displayed to the assessor to reduce the effect of the assessor X  X  perception chang-ing as they interacted with the documents (Voorhees, 1998 ). The documents were made available to the assessors via the
World Wide Web, thus allowing assessors to access the website at their convenience. To reduce the effect of assessors becoming tired during the assessment process, assessors were able to assess the documents in multiple sessions. The use of a website resulted in the assessors never meeting each other and so they were unable to discuss their responses.
Once an assessor selected a blog title, the topic, blog post and comments were displayed to the assessor. The document was assessed and placed into one of the assessment categories. Upon the completion of the 100 documents by all assessors, the judgements were collated and analysed. There are various statistical methods for analysing the resulting data, as dis-cussed in Section 4.1. 4.1. Statistical reporting methods
This study aims to measure seven individual X  X  ability to agree on the existence of an opinion on a given topic and applying their level of agreement to a wider population. Two reporting methods are applied to the results of the seven assessors: (1) be referred to as JA (Jaccard Agreement) in the remainder of this paper.

Kappa statistics are used to calculate the degree of agreement between raters, incorporating a correction for the level of agreement which would be expected by chance (Fleiss, 1971; Green, 1997 ). A kappa score indicates the level of agreement between assessors above the level that would be expected by chance. The score ranges from 0 (agreement is no more than by the agreement between two assessors that has been chance-corrected (Bailey et al., 2008 ).
 To determine the level of agreement between multiple assessors, Fleiss X  kappa (Chen, Zaebst, &amp; Seel, 2005; Eugenio &amp; 1998), where
P is the proportion of agreeing pairs, n is the number of ratings per subject, i is the subject, j is the category and k is the number of categories into which assignments are made, N is the total number of subjects, and p j is the mean proportion of agreement in category j .
Kappa is used to report the disagreements in this inter-rater agreement study as all disagreements are considered equal in seriousness. This includes disagreements in documents assessed as opinion and the documents assessed as non-opinion.
Weighted Kappa is not used in this study as the seriousness of disagreement could not be specified ( Fleiss, 1971 ). Many in-ever Cohen X  X  kappa is restricted to two raters. Fleiss X  kappa extends Cohen X  X  kappa to allow the kappa agreement of three or more raters to be reported. The original research using Fleiss X  kappa (Fleiss, 1971 ) considered the case of  X  X nweighted X  kappa on a constant number of raters, but not necessarily the same raters. This differs from the inter-rater agreement study de-tailed in this report, which asks all assessors to assess all documents.

The interpretation of kappa values suggested by Fleiss (1981) indicates that a kappa value below 0.40 indicates poor agreement and values above 0.75 indicate excellent agreement while values between 0.40 and 0.75 indicate a fair level of agreement. Bailey et al. (2008) simplified their interpretation of kappa scores by considering j P 0 : 6 to indicate substantial agreement. In contrast, Castillo et al. (2006) expanded on the interpretation of kappa values by providing a table to be used as a way to interpret kappa scores in their research (Table 3 ). These interpretations have been expanded to fit the inter-rater study and will be used when reporting kappa scores in Section (5). Table 4 details the  X  X greement X  categories that will be used throughout the remainder of this report to interpret the agreement results.

JA scores differ from kappa scores by reporting on the level of agreement on opinion assessments only. JA is also used to report the agreement between pairs of assessors. The formula (5) for calculating JA between two assessors is (Voorhees, 1998): where A 1 o and A 2 o are the documents assessed as being opinion documents according to the assessor. 5. Results and discussion
The level at which the human assessors can agree on the existence of an opinion on a given topic and their ability to agree with the TREC assessments will be discussed in this section using kappa and JA values. Two more formulas are introduced in
Section 5.2 which will be used to compare the level of agreement achieved by the Blog06 participant X  X  runs with the TREC assessments and the level of agreement achieved by the seven assessors with the TREC assessments.

Of the 100 blog documents assessed by the seven assessors in this study, total agreement was obtained in 27 of the doc-uments, six assessors agreed on 25, five assessors agreed on 31 of the documents, and four assessors agreed on 17 of the documents. Of the 27 blog documents obtaining total agreement, 18 (67%) were assessed as being opinion-bearing (op) and 9 (33%) were assessed as being non-opinion-bearing (nop).
 5.1. Agreement level between assessors
The 100 blog documents selected have a bias of 58 op/42 nop, according to the TREC assessments, and it would be ex-pected that the seven assessors in this study would achieve a similar bias. Overall, the documents were assessed 57 op/ 43 nop. However, a higher proportion of opinion documents (18 of 57, 31%) achieved total agreement by the seven assessors compared to total agreement on non-opinion documents (9 of 43, 21%). Fig. 1 illustrates the higher level of agreement on op blog documents, between the assessors. Inter-rater agreement studies of information retrieval assessments have found that assessors agree more readily on non-relevant documents (Bailey et al., 2008; Voorhees &amp; Harman, 1996 ). This is in contrast to the level of agreement achieved by the seven assessors, who achieved a higher level of agreement on the opinion docu-ments than non-opinion documents.

Of the seven assessors in this study, only one (Rater7) recorded the same op/nop bias as the TREC evaluations. However, this assessor only agreed with the TREC assessments on 50 of the 100 blog documents. Rater5 recorded the highest variation from the expected bias with 74 of the 100 documents being assessed as opinion. Rater7 and Rater5 are discussed later in this section as being the most dissenting assessors within the group of seven assessors in this study. Table 5 details the propor-tion assessed as opinion and non-opinion for each assessor.

Agreement was also calculated using a sevenfold calculation, leaving out one assessor each time. A level of  X  X air agree-ment X  (according to Table 4 ) is reached between the seven assessors. However, when one assessor was removed from the assessments, the level of agreement either increased or decreased depending on the assessor being excluded, as detailed in Table 6 .

The results indicate that the level of agreement between the raters increases when  X  X ater7 X  is removed, and to a lesser degree this is also true if  X  X ater5 X  is removed. Removing any of the other five raters reduces the level of agreement. One pos-sible explanation for Rater7 recording a lower level of agreement is that Rater7 is the only assessor not having English as their first language. However, this does not explain Rater5 who assessed 74% of the blog documents as being op. The various levels of agreement are detailed in Table 6 . For the kappa value measuring agreement amongst all seven assessors (as re-ported in Table 6 ), we can apply the method of Fleiss and Cuzick (1979) to estimate the standard error of this measurement at around 0.02. As the observed value of kappa was 0.36, we can be statistically confident that the true value of kappa lies within the  X  X air X  level of agreement indicated in Table 4 .

The level of agreement between each pair of assessors (including the TREC assessments as an eighth assessor) is detailed in Table 7 . Pairwise comparisons between the assessors reinforce the results in Table 6 , indicating that Rater7 and Rater5 are dissenting assessors, with the TREC assessments being the third most dissenting assessor. The percentage agreement value is calculated by counting the number of documents where agreement was achieved within the pair of assessors. Rater7 achieved the lowest mean agreement followed by the TREC and Rater5 means, with Rater2 and Rater6 achieving the highest mean agreement. A detailed record of the demographics of the seven assessors was not recorded, therefore it is difficult to determine reasons for higher and lower levels of agreement between particular assessors.

Rater2 and Rater6 achieved the highest level of agreement and JA between any pair of assessors, with the lowest being achieved between Rater7 and TREC. If Rater2 assessments were used as training data and Rater6 assessments were used as testing data, an AODS could not be expected to achieve above 84%. However, Rater7 and TREC assessments could only achieve approximately 50% agreement. This highlights the importance of knowing the reliability of the assessments before reporting the results of a system.

The pairwise comparisons shown in Table 7 highlight the importance of valid opinion assessments when reporting the results of an AODS. In terms of pairs of assessors, the levels of agreement range from 0.133 j to 0.682 j (if TREC assessments are included the low is 0.026 j ). This shows a substantial variation between pairs of assessors. When Rater6 is used as the standard assessment, the highest level of agreement is achieved with Rater2. However, using Rater7 as the standard assess-ment, Rater2 achieves the lowest level of agreement. Table 8 illustrates the difference in the ranking of assessors when dif-ferent assessors are used as the standard assessment. As discussed in Section 3,(Voorhees, 1998 ) concluded that the subjective nature of relevance assessments in IR tasks did not affect MAP measurements on systems. However, ( Voorhees, 1998) stated that MAP results are influenced by the level of human intervention in a system. This is particularly relevant when training data that has been assessed by a single human assessor is used in the opinion detection process, as there may be a difference between assessors.

For the pairwise measurements of agreement between raters reported in Table 7 , the standard errors can be estimated using the approach of Hanley ( 1987). The error values vary between pairs as they depend on the actual kappa values, and j 0.026 the proportion of documents assigned to each category by each rater, but they are generally in the range of 0.07 X 0.08. This is sufficiently accurate to allow us to establish that there are significant variations between some of the pairs of assessors (for example, the level of agreement between Rater1 and Rater2 is significantly higher than that between Rater2 and Rater7).
The results of the pairwise comparisons of the various assessors in this study demonstrate the impact that changes to training data assessments can have on assessing the results of an AODS. Some variations would be expected due to these ranks being calculated using the relatively small number of assessed documents. Improved results can be obtained when the remaining assessors form a constituency and are compared to the assessor being used as the standard assessment.
The results of the constituencies (const5 and const6) are included in Table 8 : const6  X  An assessor is used as the standard assessment and a constituency of the remaining six assessors is formed and included in the rankings; const5  X  Rater7 has been the most dissenting assessor compared to the other assessors in this inter-rater study, therefore constituencies of five remaining assessors (after Rater7 was removed) were also calculated to highlight the difference dis-senting assessors can make to the overall results.

Regardless of the assessor being used as the standard assessment, an improved result can be obtained using a constitu-ency of assessors. The comparisons between the various constituencies of assessors and TREC assessments indicate that a majority vote can increase the level of agreement and JA between the assessors and TREC, thus indicating that a constituency of assessors may be more reliable compared to a single assessor.

To determine the extent of agreement between the majority of the seven assessors and the TREC assessments, the 100 blog documents assessed in this study were divided into various subsets: Subset1  X  documents with total agreement (27), Subset2  X  documents with six assessors agreeing (25), Subset3  X  documents with five assessors agreeing (31), and Sub-set4  X  documents with four assessors agreeing (17). With the TREC assessments being considered the Gold standard for opin-ion detection, the four subsets of documents were compared to the assessments provided with the TREC data. 5.1.1. Subset1  X  seven assessors agreed on 27 blog documents
According to the seven assessors in this study, this subset is made up of 18 op and 9 nop blog documents. These assess-ments were compared to the TREC assessments (20 op and 7 nop) for the same documents, with 70.0% agreement. The Co-hen X  X  kappa on this level of agreement is 0.294 ( X  X light/Fair agreement X ), with a JA of 0.652. On this subset of documents, agreement was  X  X otal X  between the seven assessments, yet only a Slight/Fair level of agreement was achieved with the TREC assessments. 5.1.2. Subset2  X  six assessors agreed on 25 blog documents
The blog documents in this subset were selected by a majority vote by the seven assessors (six agreed). The dissenting assessor was not necessarily the same assessor for each of these documents. This subset is made up of 13 op and 12 nop documents (according to the six assessors). Comparing this to the TREC assessments (15 op and 10 nop) for the same doc-uments, 64.0% agreement was found with a Cohen X  X  kappa of 0.286 ( X  X light/Fair agreement X ), with a JA of 0.438. As could be expected, agreement between the six assessors and the TREC assessment achieved a lower level of agreement than that of
Subset1. 5.1.3. Subset3  X  five assessors agreed on 31 blog documents Blog documents in this subset were decided by a majority vote by the seven assessors (five assessors agreed). Similar to Subset2, the dissenting assessors were not necessarily the same assessors for each document. The documents were assessed as 17 op and 14 nop by the five agreeing assessors. The TREC assessments (19 op and 12 nop) agreed on 74.0% of the blogs, with a Cohen X  X  kappa value of 0.472 ( X  X air/Moderate agreement X ), with a JA of 0.636. The agreement by majority vote of five assessors achieved a much higher level of agreement with the TREC assessments compared to the Subset1 and Subset2. 5.1.4. Subset4  X  four assessors agreed on 17 blog documents Blog documents in this subset were decided by a majority vote by the seven assessors (four assessors agreed). Similar to
Subset2 and Subset3, the dissenting assessors were not necessarily the same assessors for each document. The documents were assessed as 9 op and 8 nop by the four agreeing assessors. The TREC assessments (9 op and 8 nop) agreed on 65.0% of the documents, with a Cohen X  X  kappa value of 0.292 ( X  X light/Fair agreement X ) which is similar to Subset1 and Subset2, with a
JA of 0.500. This subset recorded a slightly higher level of agreement with the TREC assessments compared to Subset2 which had six agreeing assessors.

The statistics for the three subsets of documents are detailed in Table 9 . The table compares the percentage of agreement between the human assessors and TREC assessments, and breaks down the disagreements between opinion and non-opinion assessments. Cohen X  X  kappa gives an indication of the level of agreement in opinion and non-opinion assessments above the expected level by chance, while JA results report on the level of agreement on opinion assessments only.

Comparing the agreement levels of the four subsets, it is interesting that Subset3 (five assessors agreeing) achieved the highest kappa with the TREC assessments, while the remaining three subsets all achieved similar kappa values to each other.
The highest JA was achieved in subset1 where all seven assessors agreed. The previous results indicate that the majority vote of assessors can improve their level of agreement with the TREC assessments. 5.2. Comparing human agreement and system agreement
One of the aims of this inter-rater study is to compare the level of agreement between human assessors and an automated system. A direct comparison between the human assessments and the runs submitted by the Blog06 participants is compli-cated by the different tasks carried out by these groups. In particular, the human assessors were only given blog documents assessed as being relevant to the given topic, whereas the Blog06 participants also had to carry out the retrieval task. This inter-rater study is designed to test the assessors on opinion detection and not retrieval of relevant documents. While the participants X  AODSs were expected to rank the documents, both the TREC assessors and the seven assessors in this inter-rater study were not asked to perform this task.

Calculating MAP values on the documents assessed by the seven assessors is not possible due to the assessors not ranking the documents they assessed. To enable the comparison of the level of agreement between human assessors and the level achieved by AODSs, the following formulas have been developed. The results of the formula will create an upper bound to indicate an achievable result for the automated systems. Formula (6) is designed to partially de-couple retrieval and opinion detection. The JA between the human assessments and the TREC assessments is calculated using formula (6), where H resents the documents assessed as being opinion-bearing by the human assessor and T as being opinion-bearing by the TREC assessors.

The level of agreement between the runs submitted by the Blog06 participants and the TREC assessments is calculated using the formula (7), which is restricted to documents assessed as opinion only. This formula is designed to reduce the retrieval influence on the results achieved by the automated systems and only test the ability to detect an opinion within a document: where S o represents the documents assessed as being opinion-bearing included in a run, T sessed as being opinion-bearing by the TREC assessor and T pic by the TREC assessor (opinion and non-opinion). Fig. 2 illustrates the formulas by highlighting the assessed documents used in the calculations.

The motivation for applying the above-mentioned formulas is to gauge the impact of retrieval on the results of AODSs, and to calculate a score that can be compared to human assessors. The human assessors in this inter-rater study were not asked to retrieve documents, instead they were shown a selection of blog documents that had been retrieved and as-sessed as being relevant (either opinion or non-opinion) to the given topic, and asked to assess those documents for the exis-tence of an opinion on the topic. Formula (2) was used to calculate the HAoT values for this comparison. As shown in Table 10, the HAoT scores range from 0.571 (Rater6) down to 0.398 (Rater7), whilst the majority vote of all assessors scored the highest HAoT score of 0.575. The HAoT results of 0.575 provides an estimate of the upper bound of system performance based on human performance. Based on the sample of 100 documents a 95% confidence interval for the bound is 0 : 575 0 : 09.

Blog06 participants submitted lists of documents (runs) that their systems retrieved as having opinions relevant to a gi-ven topic within them. The Blog06 results compared the documents listed in the runs to the TREC assessments and ranked the runs in order of the highest MAP results to the lowest. The ability of a run to agree with TREC assessments of opinion-bearing documents can be calculated using formula (7), which is designed to reduce the retrieval influence incorporated into to the HAoT results listed in Table 10 . All human assessors, including the majority vote of the seven human assessors, achieved a higher level of agreement with the TREC assessments compared to any of the systems.

Of the 56 SYSoT results, 4 only the highest 10 and lowest 10 runs are displayed to save space. The rank according to the MAP results (MAP Rank) is listed and the rank according to the SYSoT results (SYSoT Rank) is also listed to highlight the difference reporting methods can have on the perception of one system compared to another.

The changes in the rank of the runs indicates that the retrieval process affects the MAP result of an AODS. Failure to sep-results shown in Table 11 indicate that the runs submitted by the University of Maryland (Oard et al., 2006 ), which used an aggregated score using Wilson X  X  subjective lemma lexicon to detect opinions, achieved the highest opinion detection results. 6. Summary Early opinion detection research reported the existence of an opinion within text, regardless of the topic of the opinion.
The Blog06 corpus introduced opinion detection focussed on the expression of an opinion about a given topic, adding an ex-tra complication to the opinion detection and assessment task. This study tested the ability of seven human assessors to agree on the existence of an opinion about a given topic, and their ability to agree with the TREC assessments on the same blog documents. The overall  X  X air X  level of agreement found in this study indicates how difficult it is for humans to agree on whether an opinion is being expressed about a given topic. The highest level of pairwise agreement between assessors was 84% (Rater2&amp; Rater6, Table 7 ), with the agreement between any individual human assessor and the TREC assessments rang-ing from 50% to 67%.

The results discussed in Section 5 indicate that a constituency of assessors is more likely to agree with a set of standard assessments than a single assessor. We are aware that these improvements in results are not new and to some extent are expected in general. However, we are not aware of studies on the level of agreement for human assessment of opinion. Therefore, our findings support previous studies in improving IR results using techniques such as fusion.
The results reported in Section 5.1 found that assessors are more likely to agree on the assessment of an opinion docu-ment compared with a non-opinion document. This result is in contrast to the IR assessors who are more likely to agree on a non-relevant document compared with a relevant document (Bailey et al., 2008; Voorhees, 1998 ). This research has shown that separating retrieval and opinion detection reporting can result in a variation in the performance of some systems. The ability to combine opinion detection with different retrieval systems indicates that the two processes could be treated as separate systems and the results reported accordingly. Table 11 highlights the possible changes in rank by showing that the run previously ranked 1 is ranked 4 when the retrieval influence is removed from the performance calculation.
There are three reporting statistics discussed throughout this report: (1) MAP  X  rewards a system for listing documents assessed as expressing an opinion relevant to the topic at the top of an (2) kappa  X  reports the level of agreement above the level expected by chance. In the context of opinion detection, this (3) JA  X  reports the agreement between assessors on opinion assessments only.

The ability to, not only test an automated system X  X  results against other systems, but to test the results against an upper bound, would allow researchers to have a clear indication of their system X  X  ability to accurately detect opinions. As each opinion detection task in the TREC blog corpus has a unique set of topics and blog documents, it is recommended that each have an upper bound calculated and included in the results to enable such a comparison. 7. Conclusion
Out study has shown a wide range of variation between individual assessors therefore it is recommended that wherever possible, opinion detection training data be assessed by multiple assessors and the majority vote used as the assessment.
However, multiple assessors can be expensive in the time commitment required to assess a large number of documents, therefore it is recommended that a pre-test of assessors is conducted to remove the most dissenting assessors from the pool of assessors. Removing dissenting assessors will result in a higher level of consistency in the assessments, and a higher level of consistency in the training data. (Voorhees, 1998 ) reported that the MAP results of a system can be affected by the level of user interaction involved. Given that AODSs rely on training data for the opinion detection process, the high level of disagreement between human assessors raises concerns about the training data used. The results shown in Table 7 illustrate the impact that one assessor can have on the overall assessments. If Rater7 assessments are used as a system it would have rated very poorly against the TREC assess-ments. It is recommended that AODS training data be assembled from a variety of sources to lesson the impact of inconsis-tent training data.

A further intention of this paper is to determine a level of performance that can be expected by an AODS. This has been achieved by testing the ability of seven human assessors to agree with the TREC assessments. The mean of the results was calculated as a modified JA score for the assessors (HAoT) 0.494 and compared to the results of the runs (SYSoT). None of the 56 runs submitted by the Blog06 participants achieved this level. The lowest HAoT score is 0.398, which is still substantially higher than the highest SYSoT result of 0.296.

While a standard reporting statistic for TREC is MAP, which includes the retrieval process and the ordering of the runs, it is suggested that MAP may not give a clear indication of the efficiency of the opinion detection task. A JA calculation, scoring the ability of the automated system to agree on the assessment of the existence of an opinion within a document may give a clearer indication of the efficiency of an AODS.
 Acknowledgments
The authors would like to thank Associate Professor James Thom for many valuable comments on an earlier draft of this paper. Thanks also to the anonymous reviewers whose comments substantially improved the paper.
 References
