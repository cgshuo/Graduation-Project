 1. Introduction (ANN) has made it advantageous and popular for use in speech recognition. ANNs such as Multi-Layer Perceptron (MLP), Time-Delay NeuralNetwork(TDNN)andRecurrentNeuralNetwork(RNN)require fi xed length input vector in order for the speech signals to be trained andtestedunderthesameANN.Innormalpractice,onceaproper input layer size of ANN is selected based on the maximum length of the speech signals, speech signals with shorter lengths are then padded with zero before they are fed into the ANN for training and testing. Due to the dynamic characteristics of speech, speech signals come with different types and lengths; thus, in practice, ANN is not suf fi ciently fl exible to handle the timing variability of speech signals without adjustment to the speech length, analysis windows or analysis frames.
 Popular speech feature extraction methods such as Mel-Frequency Linear Predictive (PLP) ( Hermansky, 1990 ), and Linear Predictive Cepstral Coef fi cient (LPCC) ( Makhoul, 1975 ; Papamichalis, 1987 )have been used in the short time spectral analysis to represent spectral information. In order to deal with speech signals of variable lengths, various methods have been proposed to provide fi xed length input vectors. Yamauchi et al. (1995) proposed a speech recognition module that can control the speech signal length by introducing delay lines towards the input data. However, the approach was too static and it showed limited range of variance in real practical conditions where speech is more spontaneous. In another method, time scaling was proposed to expand and compress speech signals ( Duchen-Sanchez et al., 2009 ). Under this time scaling, short speech signals are expanded to longer signals while lo ng speech signals are shortened. However essential information of the speech may be lost during the compression of speech signals.

Rozman and Kodek (2007) proposed the use of asymmetric analysis window to extract the speech features according to the temporal and spectral functions. However, it was hard to deter-mine the part of the speech signal that should be extracted with longer or shorter lengths. Variable-length frame overlap between two consecutive frames was proposed by Ding (2010) to handle the transition between different speech analysis frames. A more accurate feature vector of speech signals was extracted and compared to that using conventional fi xed-length frame over-lapping. Tyagi et al. (2006) also proposed variable size analysis windows to improve the recognition rate of the Hidden Markov Model (HMM)-based ASR.
 On the other hand, HMM, which is currently the state-of-the-art in ASR, does not have problems with variable speech lengths. HMM is able to handle both temporal and spectral variations statistically ( O '
Shaughnessy, 2008 ). However, HMM still has some major limita-tions for applicability in ASR. Thus, hybrid systems of ANN and HMM were proposed to improve fl exibility and recognition performance ( Trentin and Gori, 2001 ). The tandem connectionist method was further proposed to improve the recognition rate of the ANN/HMM system ( Hermansky et al., 2000 ). The conventional HMM systems used likelihood assumption to predict how the phonations were produced while the Tandem method utilised the log posterior probabilities from several acoustics models to perform speech recog-nition on a large scale.
 theinputunits.TDNNwasabletolearnthetemporalstructureof acoustic events and the temporal relationships between such events.
Besides that, TDNN was translation invariant, which allows the acoustic features learned by the network to be insensitive to shifts in time. TDNN outperformed HMM in the recognition of the three
Japanese phonemes. Since then, improvements have been studied to increase the robustness of TDNN ( Hempshire and Waibel, 1990 ; Day and Davenport, 1993 ; Baldi and Atiya, 1994 ; Poo, 1997 ; Yazdizadeh and 2009 ). Although delay is introduced to the time series speech signals,
TDNN still suffers from the fi xed length problem and it is in handling speech signals with variable lengths.

Adjustable Neural Network (SANN) is proposed in this paper. The proposed SANN can automatically adjust its structure according to the input length and it enables different lengths of speech signals to be trained and tested together under the same network. This paper is organised as follows: Section 2 introduces the architecture of the
SANN structures. Section 3 explains the speech feature extraction using Mel-frequency Cepstral Coef fi cients (MFCCs). Section 4 explains the database, as well as the training and testing of SANN and the state-of-the-art speech recogniser, HMM. Section 5 presents the results and discussion on the recognition rate of the SANN and HMM. The paper is then concluded in Section 6 . 2. Self-Adjustable Neural Network 2.1. SANN architecture sented in this section. The explanation of the SANN architecture is based on the speech recognition of 6 different Malay vowels as an example. SANN is proposed based on the architecture of TDNN. TDNN is further modi fi ed so that SANN can allow speech signals with different lengths to be trained and tested under the same network. 4layers,whicharetheinputlayer, fi rst hidden layer, second hidden layer and output layer. The analysis window, which consists of a few analysis frames, is used in SANN so that the speech signals are fed into the SANN one segment at a time in order to be connected to one analysis frame at the next layer. The analysis window is then shifted or delayed by one analysis frame in order to be connected to the next analysis frame at the next layer. The analysis windows are shifted in a way that they overlap with each other with two analysis frames. frames, with each analysis frame consisting of 39 MFCC coef
However,theinputlayersizeofSANNatonetimeiscomprisedofan analysis window, with a size of 3 analysis frames. The number of input nodes at one time is equal to 117 MFCC coef fi cients. The window at the input layer is connected to the fi rst analysis frame at analysis frame in order to be connected to the second analysis frame at layer is repeated until reaching the last analysis frame at the input layer. A total of 13 analysis frames are formed at the fi based on 15 analysis frames at the input layer. Every analysis window has its own copy of weights when it is connected to the analysis frame at the next layer. Every neuron in one analysis window at the input layer is fully connected to all neurons in one analysis frame at the hidden layer while every neuron in one analysis frame at the hidden layer is fully connected to all neurons in one analysis window at the input layer.
 analysis frames. The fi rst hidden layer size at one time is again comprised of an analysis window, with a size of 5 analysis frames. The fi rst analysis window at the fi rst hidden layer is connected to the fi rst analysis frame at the second hidden layer. The analysis window is shifted by one analysis frame in order to be connected to the second analysis frame at the second hidden layer. The shifting of the analysis window at the fi rst hidden layer is repeated until reaching its last analysis frame. Thus, a total of 9 analysis frames are formed at the second hidden layer based on 13 analysis frames at the fi rst hidden layer. Every analysis window has its own copy of weights when it is connected to the next layer. Every neuron in one analysis window at the fi rst hidden layer is fully connected to all neurons in one analysis frame at the second hidden layer while every neuron in one analysis frame at the second hidden layer is fully connected to all neurons in one analysis window at the fi rst hidden layer.

The second hidden layer consists of a maximum size of 9 analysis frames, with each analysis frame consisting of 6 hidden neurons. The hidden neurons across all the analysis frames at the second hidden layer are connected to the corresponding output neurons at the output layer. Thus, the hidden neuron number at the second hidden layer is set in a way that equals the output neuron number at the output layer. The fi rst neuron at the second hidden layer across all the activated analysis frames is connected to the fi rst output neuron at the output layer. The same approach is applied to the second until the last output neuron.
The number of analysis frames at the layers of input, fi rst hidden layer and second hidden layers are determined respec-tively according to Eqs. below: N1  X  S = L ;  X  2 : 1  X  N2  X  N1  X  AWS1  X  1 ;  X  2 : 2  X  N3  X  N2  X  AWS2  X  1 ;  X  2 : 3  X  where N1 is the number of analysis frames at the input layer, N2 is the number of analysis frames at the fi rst hidden layer, N3 is the number of analysis frames at the second hidden layer, S is the speech length in millisecond, L is the analysis frame length in millisecond, AWS1 is the analysis window size at the input layer (in terms of number of analysis frames) and AWS2 is the analysis window size at the fi rst hidden layer (in terms of number of analysis frames). The AWS1 and AWS2 are set at 3 and 5 respec-tively. For speech sounds with any signal length that are fed into SANN, the size of the input, fi rst hidden and second hidden layers will be determined according to Eqs. (2.1)  X  (2.3) . The input layer of SANN consists of a minimum of 7 analysis frames and a maximum of 15 analysis frames. A minimum 7 analysis frames at the input layer are required in order to be connected to 5 analysis frames at the fi rst hidden layer and subsequently one analysis frame at the second hidden layer. This is the most basic structure of SANN. In other words, if MFCC coef fi cients are extracted using analysis frame of 25 ms with a shift of 10 ms, then a minimum speech length of 85 ms is required for SANN. Thus, the bigger network size will be activated for longer speech lengths compared to that of shorter speech lengths. 2.2. The learning algorithm
In TDNN, different shifted or delayed analysis windows shared the same copy of the weights. The weights at the same layer were updated using the same average values ( Waibel et al., 1989 ). In SANN, every analysis window and its shifted analysis window has its own copy of weights when it is connected to the next layer. The weights are updated for every analysis window based on the current weight values. SANN is trained using standard error back-propagation and the weights are updated using the standard gradient descent method.

The multiplication of weights and input nodes that are con-nected to each other is calculated, then the result is added with the biases and, lastly, multiplied by a non-linear sigmoid function to produce a neuron at the next layer. The same feed forward method is used for connections at all the layers. Results obtained at the output layer are compared with the desired output. For every training pattern, error information terms are then calculated and used to update the weights and biases in every layer as shown in Eqs. (2.4) and (2.5) . w  X  t  X  1  X  X  w  X  t  X  X   X  w  X  t  X  1  X  X  2 : 4  X   X  w  X  t  X  1  X  X   X  X  k  X   X   X  w  X  t  X  X  2 : 5  X  where w ( t + 1) is the new updated weight, w ( t ) is the current weight,  X  w( t + 1) is the updated weight correction term, the current weight correction term,  X  is the learning rate, momentum,  X  is the error information term at the output layer, second hidden layer and fi rst hidden layer; and k are the neurons at the second hidden layer, fi rst hidden layer and input layer.
The weights of the SANN are initialised and connected accord-ing to the maximum length of the speech length, i.e. 165 ms or 15 analysis frames with each analysis frame length of 25 ms and a shift of 10 ms. Therefore, when the speech length is less than the maximum length, the weights will be activated to the speech length while other weights will be not activated. The weights that are not activated will remain the same in the forward propagation and only those activated weights will be multiplied with the input values and feed-forwarded by the non-linear function to obtain the output. When the weight values need to be updated at the stage of error back propagation, the weights that are not activated will not be updated and will remain the same. This allows SANN to learn the information about variable lengths of the speech signals. 3. Speech feature extraction
A pre-emphasis is used to spectrally fl atten the speech tokens according to Eq. (3.1) . x  X  n
 X  X  s  X  n  X   X  0 : 95 s  X  n  X  1  X  X  3 : 1  X  where s ( n ) is the input speech token and x ( n ) is the pre-emphasised speech token. Then, the speech tokens are blocked into analysis frames. A Hamming window is used to window the analysis frame in order to minimise the discontinuities at its boundary. The windowed speech token is h  X  n  X  X  x  X  n  X  w  X  n  X  ; 0  X  n  X  N  X  1  X  3 : 2  X  where w( n ) is the Hamming window function ( Webster, 1978 ), and N is the total sample points in an analysis frame. A typical Hamming window is de fi ned in Eq. (3.3) . w  X  n  X  X  0 : 54  X  0 : 46 cos  X  2  X  n = N  X  1  X  ; 0  X  n  X  N  X  1
MFCC is used to extract the speech features based on this windowed speech token. MFCC consists of 39 orders, which includes 13 mel-frequency cepstral coef fi cients (12 MFCC plus 1 log energy term using Discrete Cosine Transform), 13 fi derivative and 13 second derivative features (Delta cepstrum). Fast Fourier Transform (FFT) is performed on the h ( n ) and the output spectrum f , which is then changed to log mel-spectrum according to Eq. (3.4) . Discrete Cosine Transform is then used to convert the log mel-spectrum to MFCC.
 Mel  X  f  X  X  2595 log 10 1  X  f 700  X  3 : 4  X  MFCC coef fi cients were directly extracted by using the Hidden Markov Toolkit (HTK) ( Young et al., 2006 ). MFCC were normalized between  X  1 and + 1 before being fed into SANN. 39 MFCC coef fi cients were extracted at every analysis frame, with each analysis frame length of 25 ms and a shift of 10 ms. 4. Methodology 4.1. Speech database 4.1.1. Malay vowels vowel speech database was compiled from 360 Malay children (180 males and 180 females) aged between 7 and 12 years. The vowels were sustained for a duration of between 3 and 5 s. The speech database consisted of 2160 (360 speakers 6 vowels) speech tokens. The speech samples were recorded using Goldwave audio editor, with a sampling rate of 20 kHz with 16 bit of resolution.
 speech database. Speech tokens of 240 and 120 speakers were used for the training and testing respectively each time. The experiments were conducted in three different training and testing sets. The speech tokens were extracted using MFCC. 39
MFCC coef fi cients were extracted at every analysis frame length of 25 ms with a shift of 10 ms. Variable lengths of between 85 ms and 165 ms per speech token were used for the training and testing set, with an increment of 10 ms. Thus, the total number of speech tokens of the training set consisted of 12,960 utterances. As for the testing set, the total number of speech tokens of the testing set was 6480. The description of the database is shown in Table 1 . 4.1.2. TIMIT isolated words tinuous speech corpus ( Garofolo et al., 1992 ) was used to evaluate and compare the performance of SANN with HMM. The TIMIT database was used because it is the standard database used in many studies and it contains speech sounds with variable lengths ( Pols et al., 1996 ).

TIMIT:  X  She had your dark suit in greasy wash water all year  X  Don ' t ask me to carry an oily rag like that  X  . Ten isolated words were selected, which were  X  dark  X  ,  X  wash  X  ,  X  water  X  ,  X  carry  X  ,  X  greasy  X  ,  X  like  X  ,  X  oily  X  and  X  that  X  consisted of 462 and 168 speakers, respectively. MFCC were extracted from the speech tokens with a minimum signal length of 85 ms in order to produce a minimum seven analysis frames.
One testing speech token in  X  don ' t  X  did not meet the minimum signal length, thus it was excluded. The details of the database are shown in Table 1 . MFCC was extracted using an analysis frame length of 25 ms with a shift of 10 ms. A maximum frame number of 63 was extracted per speech token. 4.2. Experimental setting coef fi cients were derived per analysis frame. The total number of MFCC coef fi cients per speech token depends on the speech length.
For Malay vowels, variable lengths of the speech token were used between 85 ms and 165 ms, with an increment of 10 ms. Analysis frame length of 25 ms with a shift of 10 ms (or an overlapping of 15 ms) was used for the speech feature extraction. Thus, a minimum of 7 analysis frames and a maximum of 15 analysis frames were extracted from these tokens. MFCC coef fi cients were extracted from the Malay vowels about 0.75 s from their start point while for TIMIT isolated words, the speech features were extracted based on the labelled start point.
 randomly between  X  0.3 and + 0.3. The training of the neural networks was stopped when they reached a maximum iteration of 100 for Malay vowels and 1000 for TIMIT isolated words, and a minimum root mean square error of 0.005. The root mean square error, E rms is de fi ned as where P is the number of training patterns and K is the number of output neurons, t pk is the target or desired output and y actual output. The momentum and learning rate were set at 0.9 and 0.1 respectively. For every vowel pattern, the target value is set in a way that only one output neuron is set to 0.9 while the rest of the output neurons are set to 0.1.

In SANN, the minimum and maximum number of analysis frames at the input layer was 7 and 15, respectively for the vowel speech tokens. Thus the minimum and maximum number of input neurons was 273 and 585, respectively. In recognition of Malay vowels, SANN was trained with fi rst hidden neuron number between 10 and 50, in a step of 2. As for TIMIT isolated words, SANN was trained with fi rst hidden neuron number between 10 and 20, in a step of 2. The second hidden neurons were set at 6 for the vowel recognition while 10 for the TIMIT isolated word recognition. The analysis window size was set at 3 and 5 for the input layer and fi rst hidden layer, respectively. The output neurons of SANN were set at 6 for the recognition of Malay vowels and at 10 for the recognition of TIMIT isolated words.

The HMM was implemented using the HMM Toolkit (HTK), which was developed by Cambridge University, United Kingdom ( Young et al., 2006 ). The whole word model was used for the vowel and TIMIT isolated word recognition. The HMM model consisted of 3 states of left to right HMM and each state consisted of 8 Gaussian mixtures. HInit and HRest tools were used for the training while for the testing, HCopy, HVite and HResults tools were used. 5. Results and discussion
The performance of SANN in Malay vowel recognition different fi rst hidden neuron number is summarised in Table 2 . SANN obtained the highest average recognition rate of 95.15% at the hidden neuron number of 22. SANN needed to be trained with the proper fi rst hidden neuron number. Lower or higher number of fi rst hidden neuron number could not guarantee a optimal performance of SANN. On the other hand, HMM achieved an average recognition rate of 90.09% with the accuracy of set 1, set 2 and set 3 were 88.72%, 90.34% and 91.20% respectively. Obviously, SANN outperformed HMM in recognizing Malay vowels. The confusion matrix for Malay vowel recognition using SANN is shown in Table 3 . Vowel /a/ was recognised with the highest accuracy, followed by /u/ and /i/, while /o/ was recognised with the lowest accuracy.

The performance of SANN and HMM for the TIMIT isolated word recognition is shown in Table 4 . The results showed that HMM outperformed SANN in recognizing the isolated words. The confusion matrix for TIMIT isolated word recognition using SANN is shown in Table 5 . Word  X  carry  X  was recognized with the highest accuracy while  X  wash  X  was recognized with the lowest accuracy.
The proposed SANN is able to deal with variable lengths of speech. SANN adjusts its network size according to the length of the speech. Though a maximum input layer size of 15 (equivalent to a speech length of 165 ms) was used for the vowel speech tokens, there is no limit on the input layer size of SANN. SANN can be extended to accommodate speech with longer length. In the recognition of TIMIT isolated words, the minimum input layer size was kept at 7, while the input layer size of SANN was extended to the maximum length of the speech tokens at 63. Larger network size will be activated for speech tokens with longer length while smaller network size will be activated for speech tokens with shorter length. This forces SANN to learn different lengths of the speech tokens. Compared to the conventional ANN such as MLP and TDNN, the input layer of the network needs to be initialized with the maximum signal length. Speech tokens with shorter signal length are padded with zero before they are fed into the ANN for training and testing. 6. Conclusion
A novel Self-Adjustable Neural Network is proposed in this paper with application in speech recognition. SANN is able to adjust its network size according to the speech length and allows speech tokens with variable lengths to be trained and tested under the same network. SANN has been successfully used to recognise Malay vowels and 10 TIMIT isolated words. SANN outperformed the state-of-the-art HMM in recognising Malay vowels but its accuracy was lower in recognizing the TIMIT isolated words. Acknowledgements The authors would like to thank the Ministry of Science, Technology and Innovation, Malaysia (MOSTI) and University of Malaya for funding this study under Science Fund (Project no. 06-01-03-SF0516) and University Malaya Research Grant (UMRG) (Project no. RG126/11AET).
 References
