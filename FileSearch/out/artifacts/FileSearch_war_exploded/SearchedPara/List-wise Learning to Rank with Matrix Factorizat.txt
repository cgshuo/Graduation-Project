 A ranking approach, ListRank-MF, is proposed for collaborative filtering that combines a list-wise learning-to-rank algorithm with matrix factorization (MF). A ranked list of items is obtained by minimizing a loss function that re presents the uncertainty between training lists and output lists produced by a MF ranking model ListRank-MF enjoys the advantage of low complexity and is ana-lytically shown to be linear with the number of observed ratings for a given user-item matrix. We al so experimentally demonstrate the effectiveness of ListRank-MF by comparing its performance with that of item-based collaborative recommendation and a re-lated state-of-the-art collabora tive ranking approach (CoFiRank). H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  Information Filtering Algorithms, Performance, Experimentation Recommender systems, collaborative filtering, learning to rank, matrix factorization, recommendation Recommender systems attract research attention because of their importance for handling the unprecedentedly large amount of content, e.g., movies, music, bo oks, currently available online to users [1] . Collaborative filtering (CF) has been regarded as one of the most successful recommender te chniques. It is based on the concept that a user would like an item if the item is liked by the users with similar preferences. The ultimate purpose of a recom-mender system is to provide users with a ranking or recommenda-tion list and this objective has been argued to be more crucial than the accuracy of rating prediction [1] [5] [7] [13] . For this reason, we focus on ranking or recommendation (also referred as top-N rec-ommendation [3] ) in this paper, rather than rating prediction. 
We propose a scalable extension to the matrix factorization ap-proach to collaborative filtering called ListRank-MF. Our ap-proach makes use of a list-wise l earning-to-rank technique to rank items for each user, in which users and items are represented as latent features learned using ma trix factorization (MF). The key contribution of our approach is twofold: it provides a recommen-dation performance improvement over the existing state-of-the-art matrix factorization approach and it keeps the complexity linear with the number of observed ratings in the given user-item matrix, meaning that it can scale-up to be used in very large collections. 
Learning to rank (LTR) is supervised machine learning ap-proach that automatically cons tructs a ranking model/function from training data [10] . Recently LTR has been the subject of intensive research effort, e.g., within the Yahoo! LTR Challenge LTR research benefits both information retrieval (IR) and recom-mendation, both fields that focus on the task of returning a rank list of items in response to an in formation need, expressed explic-itly as a query (IR) or implicitly in the user profile (recommenda-tion). Although much work has been carried out on LTR tech-niques for systems that return documents in response to a query [10] , little effort has been devoted to exploiting LTR in recom-mender systems, or specifica lly, collaborative filtering. In order to apply LTR to CF several challenges must be faced. First, user profiles and item s in the recommendation setting are not easy to represent in terms of explicit features. In this way, recommendation contrasts with IR applications, where queries and documents can be explicitly repres ented, e.g., using term weights derived from occurrence frequencies. Researchers in recom-mender systems frequently represen t users and items with their rating vectors, which are essentially different from query and document representations in that th ey have no direct relationship to user and item characteristics. To address this issue, we make use of matrix factorization to represent users and items by latent features. Second, not all LTR approaches are inherently suitable for CF application. Liu et al. [10] categorizes LTR into point-wise, pair-wise and list-wise. Poin t-wise approaches predict rank-ing scores for individual documents and can thus be regarded as conceptually equivalent to rating prediction problem in CF. A ranked list is created by ordering documents according to these scores. As discussed in the literature, the accuracy measure of rating prediction is difficult to interpret as a measure of ranking quality [11] [13] . Pair-wise LTR makes a prediction for every pair of documents concerning their relative ordering in the final list. This approach is com putationally intensive and this disadvantage prevents it from scaling well to large data collections typical for CF scenarios. Approaches to CF that are conceptually similar to pair-wise LTR have been presented in the literature [11] [12] [14] , but a scalable solution to the comp lexity problem remains elusive. For this reason, we turn to list-wise LTR. http://learningtorankcha llenge.yahoo.com/index.php 
The remainder of the paper is structured as follows. In the next section, we summarize related wo rk and position our approach with respect to it. Then, we present the ListRank-MF algorithm and validate it experimentally. Finally, we sum up the key aspects of ListRank-MF and briefly menti on directions for future work. This section briefly summarizes the existing approaches to col-laborative filtering and related LTR approaches. A comprehensive survey of collaborative filtering approaches can be found in [1] [7] [18] . Collaborative filtering can be memory-based or model-based. In gene ral, memory-based approaches make recommendations on the basis of similarities between users (user-based) [6] , or on the basis of similarities between items (item-based) [3] [16] . Further modification and enhancement has been devoted to improving eith er user-based approaches [4] [17] [22] , or item-based approaches [21] . Model-based approaches first fit prediction models based on training data and then use the model to predict users X  preference on items, e.g., latent semantic model [8] . Matrix factorization (MF) techniques have attracted much research attention, due to the advantages of scalability and accuracy [9] [15] , especially for large-scale data, as exemplified by the Netflix contest. Generally, MF techniques learn latent features of users and items from the observed ratings in the user-item ma-trix, which are further used to predict unobserved ratings. 
Recently, research attention in the area of CF has shifted away from the rating prediction problem and places more focus on the quality of the ranking or recommendation list that the recom-mender system produces [13] . Many approaches have been inves-tigated, including investigation of pair-wise preference between items for users, e.g., EigenRank [11] , probabilistic latent prefer-ence analysis [12] and Bayesian probabilistic ranking [14] . These approaches all suffer from expensive computation which limits their scalability. In contrast, Li stRank-MF proposed in this paper is of low complexity, i.e., complexity linear with number of the observed ratings in a given user-item rating matrix. A comprehensive survey of LTR can be found in [10] . The accu-mulated body of LTR literature is sizable and here we focus on list-wise LTR approaches as most relevant to our ListRank-MF approach. Under a list-wise approach, an individual training ex-ample is an entire list of items, rather than individual items or item pairs [2] . Loss functions for list-wise LTR are formulated to reflect the distance between the reference list and the output list from the ranking model. Various algorithms are applied to learn the optimal or local optimal ra nking model. The permutation probability was proposed to repres ent the ranked list, which could be further simplified to top one probability [2] . In this paper, we also take the concept of top one probability to represent the rec-ommendation list, making our wo rk closest to list-wise LTR. CoFiRank [20] was proposed to directly optimize ranking effec-tiveness metrics for collaborative ranking, motivated also mainly by LTR and is also close to the work reported here. In this section, we introduce the list-wise learning to rank with matrix factorization (ListRank-MF ). We first present the two key components related to ListRank-MF, i.e., probabilistic matrix factorization (PMF) framework and top one probability. Then, we present the formulation of ListRank-MF as a loss function and develop corresponding learning pro cess for local optimal solution. The PMF framework was proposed in [15] , where matrix factori-zation is formulated from probabilistic inference of conditional distribution of observed ratings, us er rating priors and item rating priors. And the ultimate fram ework is formulated as: ,argmin ( ) (1 U V I R g U V U V Supposing the user-item rating matrix R consists of M users and N items, PMF seeks to represent the user-item rating matrix R with two low-rank matrices, U and V. A d -dimensional set of latent features is used to represent both users (in U ) and items (in V ). Note that we use U i to denote a d -dimensional column feature vector of item j . R ij denotes the user i  X  X  rating on item j . I indicator function that equals 1 when R ij &gt; 0, and 0 otherwise. Finally,  X  U and  X  V are regularization coefficients. We usually set  X  =  X  V =  X  for simplicity . The g(x) is a logistic function to bound the range of U i T V j , i.e., g(x)= 1/(1+exp(-x)) . 
Note that PMF, and also some other matrix factorization ap-proaches as in [9] , are rating prediction oriented. Although we can use the predicted ratings to rank the items, the quality of ranking is not directly related to the pur pose of PMF (minimizing the error of rating prediction). In analogy, PMF is equivalent to a point-wise ranking model, while not modeling ranking directly. As proposed in [2] , the top one probability for an item rated R user i  X  X  ranking list l i (e.g., with K items) can be expressed as: where  X  (x) can be any monotonically increasing and strictly posi-tive function. For simplicity, we take the same form as used in [2] , i.e., the exponential function for  X  (x) . The top one probability indicates the probability of an item being ranked in the top posi-tion for a given ranking list. We formulate the ListRank-MF by using the cross-entropy of top-one probabilities of the items in the training example lists and the ranking lists from the ranking model (MF) as the loss function: 
The training example lists consis t of the training-set items in the profiles of each user. The output of the recommendation model is a recommendati on list for each user i and is generated by ranking items in collection in descending order according to the value U i T V . Items already contained among the training examples in the user i  X  X  profile are removed. 
The loss function reflects the uncertainty between the training lists and the output lists from th e ranking model, i.e., MF here. The regularization term serves to reduce over-fitting. The optimal ranking model should perform least uncertainty between lists of training ratings and lists of output predictions list-wisely. Note that here the MF is not optimized for rating prediction, but for ranking positions of items in the users X  lists. As the loss function is not convex jointly over U and V , we choose to use gradient descent with alternatively fixed U and V , from which a local minimum can be obtained. The gradients of L(U,V) with respect to U and V can be computed as: Note that g X (x) denotes the derivative of g(x) . Exploiting the sparseness of user-item matrix, the computation of the loss function in Eq. (3) is of complexity O(2dS+d(M+N)) , where S denotes the number of observed ratings in a given user-item matrix. The gradients in Eq. (4) and Eq. (5) are O(2dS+dM) and O(dS+pdS+dN) , respectively, where p denotes the average number of items rated per user a nd usually is a very small value compared to S in collaborative filtering. Considering we often O(dS+pdS) , which is linear in the number of observed ratings in the matrix. This analysis indica tes that ListRank-MF is computa-tionally efficient and can be applied to large-scale cases. In this section, we present the preliminary experiments to evaluate ListRank-MF. We first investigate the impact of regularization coefficient on the ListRank-MF and further validate its effective-ness that optimizing the loss function contributes to ranking per-formance. We finally demonstrate that ListRank-MF could be not only efficient in learning, but also promising in performance which could improve the state-of-t he-art approach CoFiRank. Our experiments are conducted on the MovieLens dataset, which consists of 100K ratings (scale 1 X 5) assigned by 943 users to a collection of 1682 items [6] . We adopted the evaluation protocol referred to as  X  X eak generaliza tion X  in the CoFiRank evaluation in [20] . We randomly select 10, 20 and 50 rated items for each user for training and use the remaining rated items in the user profile for testing. For each cond ition, users with less than 20, 30, or 60 rated items are removed in or der to ensure we can evaluate on at least 10 rated items per user. We report the average per-formance attained across all users and ten runs of this procedure. We choose the evaluation metric as Normalized Discounted Cu-mulative Gain (NDCG), which is more sensitive to the relevance of higher ranked items. As in CoFiRank [20] , we focus on NDCG@10. Note that we use late nt feature dimension of 5 and learning rate 0.01 in our implementation for ListRank-MF in all the experiments. The regularization coefficient  X  in ListRank-MF influences the convergence of the loss function and controls for overfitting. To investigate the impact of  X  , we use one data fold, from the condi-tion in which 10 randomly selected rated items from each user are chosen for training. Fig. 1 illustrates the relationship between  X  and the loss and show the danger of overfitting arises when  X  is lower than the level of 0.001. Note that for illustrative purposes, the loss has been normalized for each setting of  X  . Since no over-fitting effect is evident with  X  equal or larger than 0.01, we set  X  to 0.01 in all the following experiments for consistency. 
Fig. 1 The impact of regularization coefficient on the conver-The optimization of the loss functi on leads to minimizing the loss. However, investigation whether the minimizing of the loss could indeed lead to a good property of ranking is still needed, e.g., NDCG@10 used in this paper. Fo r this reason, we demonstrate the development of the loss and NDCG@10 simultaneously dur-ing the iterations of optimization, as shown in Fig. 2. As observed, it is effective that the ranking performance becomes both optimal and convergent when optimizing the loss function. Moreover, it indicates the ListRank-MF is efficient as the NDCG@10 becomes approximately optimal after around 250 iterations. Fig. 2 The effectiveness of ListRank-MF to achieve optimal In this section we compare the performance of ListRank-MF with the well-known item-based colla borative recommendation (Item-CR) [3] and the state-of-the-art CoFiRank [20] . Since we share exactly same experimental protocol as used in CoFiRank [20] , we can compare results directly, i.e., CoFiRank-NDCG and CoFiRank-Best reported in [20] . Note that CoFiRank-NDCG represents an approach that directly optimizes NDCG as a loss function and CoFiRank-Best represen ts the options that lead to best results by CoFiRank in each condition. As shown in Table 1, ListRank-MF achieves a perform ance improvement of ca. 15% over ItemCR and ca. 10% over CoFiRank-NDCG. The improve-ment is significant for all the conditions, according to Wilcoxon signed rank test with p &lt; 0.05. Li stRank-MF also achieves ca. 5% improvement over CoFiRank-Best (s ignificantly) fo r experimental conditions with 10 and 20 rated items per user for training, while it is slightly outperformed for the condition with 50 rated items per user for training. 
Table 1. NDCG@10 (mean  X  standard deviation across 10 runs) comparison between ItemCR, CoFiRank and ListRank-Note that since we do not have performance of CoFiRank for each user in each run, the significance test involving CoFiRank was conducted by generating 10 samples from a Gaussian distribution with the corresponding mean and deviation in each condition, which are further compared with 10 average performances in 10 runs by ListRank-MF and ItemCR. In this paper, we have introduced the ListRank-MF, an approach that exploits a list-wise learning to rank technique in order to real-ize improvement over the performan ce of the state-of-the art ma-trix factorization techniques for the task of recommendation. The experiments demonstrate the Li stRank-MF outperforms item-based collaborative recommendati on and the state-of-the-art CoFiRank, significantly in most cases. We also analyze the com-putational complexity of ListRank-MF and find it to be linear in the number of observed ratings in the given user-item matrix. Because ListRank-MF is computationally inexpensive, it can be scaled up for use on large-scale real-world collections. Future work involves several interesting directions. First, the ListRank-MF in this paper is based on the concept of top one probability. From the point of view of the recommender system, for which the top items are critical, this provides a sensible repre-sentation of the list. However, we would like to investigate whether performance improvement can be achieved via represen-tations that reflect information along the entire list without an undue increase in computational complexity. Second, the pro-posed ListRank-MF approach, like most of the current CF rec-ommendation algorithms, could be re garded as a variational rec-ommendation approach, where th e common evaluation metrics, e.g., mean reciprocal rank, mean average precision and NDCG, are not directly related to the model. CoFiRank [20] has made the first attempt to address this issue, and recent developments in this area in LTR research [10] [19] could be further exploited for im-proving recommendation performance by directly optimizing for evaluation metrics. The research leading to these results was carried out within the PetaMedia Network of Excellence and has received funding from the European Commission's 7th Framework Program under grant agreement n X  216444. [1] Adomavicius G., and Tuzhilin, A., 2005. Toward the next gen-[2] Cao, Z., Qin, T., Liu, T.-Y., Tsai, M.-F. and Li, H., 2007. [3] Deshpande, M., and Karypis, G., 2004. Item-based top-N rec-[4] Ding, S., Zhao, S., Yuan, Q., Zhang, X., Fu, R. and Bergman, [5] Gunawardana A., and Shani, G., 2009. A survey of accuracy [6] Herlocker, J., Konstan, J., Borche rs, A., and Riedl, J., 1999. An [7] Herlocker, J., Konstan, J., Terveen, L. G., and Riedl, J. 2004. [8] Hofmann, T., 2004. Latent semantic models for collaborative [9] Koren, Y., Bell, R., and Volinsky, C., 2009. Matrix factoriza-[10] Liu, T. -Y., 2009. Learning to rank for information retrieval. [11] Liu, N. N., and Yang, Q., 2008. EigenRank: a ranking-oriented [12] Liu, N. N., Zhao, M., and Yang, Q., 2009. Probabilistic latent [13] McNee, S. M., Riedl, J., and Konstan, J. A., 2006. Being accu-[14] Rendle, S., Freudenthaler, C., Gantner, Z., and Schmidt-[15] Salakhutdinov, R., and Mnih, A., 2008. Probabilistic matrix [16] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J., 2001. Item-[17] Shi, Y., Larson, M., and Hanjalic, A., 2009. Exploiting user [18] Su, X. and Khoshgoftaar, T. M., 2009. A survey of collabora-[19] Volkovs, M. N., and Zemel, R. S., 2009. BoltzRank: learning [20] Weimer, M, Karatzoglou, A., Le, Q., and Smola, A., 2007. [21] Yildirim, H., and Krishnamoorthy, M. S., 2008. A random [22] Zhang, J. and Pu, P., 2007. A recursive prediction algorithm for 
