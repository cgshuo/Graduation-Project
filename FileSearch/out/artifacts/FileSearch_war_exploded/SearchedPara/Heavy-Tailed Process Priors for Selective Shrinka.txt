 Gaussian process classifiers (GPCs) [12] provide a Bayesian approach to nonparametric classifica-tion with the key advantage of producing predictive class probabilities. Unfortunately, when training data are unevenly sampled in input space, GPCs tend to overfit in the sparsely populated regions. Our work is motivated by an application to protein folding where this presents a major difficulty. In particular, while Nature provides samples of protein configurations near the global minima of free energy functions, protein-folding algorithms, which imitate Nature by minimizing an estimated energy function, necessarily explore regions far from the minimum. If the estimate of free energy is poor in those sparsely-sampled regions then the algorithm has a poor guide towards the minimum. More generally this problem can be viewed as one of  X  X ovariate shift, X  where the sampling pattern differs in the training and testing phase.
 In this paper we investigate a GPC-based approach that addresses overfitting by shrinking predictive class probabilities towards conservative values. For an unevenly sampled input space it is natural to consider a selective shrinkage strategy: we wish to shrink probability estimates more strongly in sparse regions than in dense regions. To this end several approaches could be considered. If sparse regions can be readily identified, selective shrinkage could be induced by tailoring the Gaussian process (GP) kernel to reflect that information. In the absence of such knowledge, Goldberg and Williams [5] showed that Gaussian process regression (GPR) can be augmented with a GP on the and defining different kernel functions on each. Treed Gaussian process regression [6] and Treed Gaussian process classification [1] represent advanced variations of this theme that define a prior distribution over partitions and their respective kernel hyperparameters. Another line of research which could be adapted to this problem posits that the covariate space is a nonlinear deformation of another space on which a Gaussian process prior is placed [3, 13]. Instead of directly modifying the kernel matrix, the observed non-uniformity of measurements is interpreted as being caused by the spatial deformation. A difficulty with all these approaches is that posterior inference is based on MCMC, which can be overly slow for the large-scale problems that we aim to address. This paper shows that selective shrinkage can be more elegantly introduced by replacing the Gaus-sian process underlying GPC with a stochastic process that has heavy-tailed marginals (e.g., Laplace, hyperbolic secant, or Student-t ). While heavy-tailed marginals are generally viewed as providing ro-bustness to outliers in the output space (i.e., the response space), selective shrinkage can be viewed as a form of robustness to outliers in the input space (i.e., the covariate space). Indeed, selective shrinkage means the data points that are far from other data points in the input space are regularized more strongly. We provide a theoretical analysis and empirical results to show that inference based on stochastic processes with heavy-tailed marginals yields precisely this kind of shrinkage. The paper is structured as follows: Section 2 provides background on GPCs and highlights how selective shrinkage can arise. We present a construction of heavy-tailed processes in Section 3 and show that inference reduces to standard computations in a Gaussian process. An analysis of our approach is presented in Section 4 and details on inference algorithms are presented in Section 5. Experiments on biological data in Section 6 demonstrate that heavy-tailed process classification substantially outperforms GPC in sparse regions while performing competitively in dense regions. The paper concludes with an overview of related research and final remarks in Sections 7 and 8. A Gaussian process (GP) [12] is a prior on functions z : X  X  R defined through a mean function (usually identically zero) and a symmetric positive semidefinite kernel k (  X  ,  X  ) . For a finite set of locations X = ( x 1 ,...,x n ) we write z ( X )  X  p ( z ( X )) = N (0 ,K ( X,X )) as a random variable distributed according to the GP with finite-dimensional kernel matrix [ K ( X,X )] i,j = k ( x i ,x j ) . Let y denote an n -vector of binary class labels associated with measurement locations X 1 . For Gaussian process classification (GPC) [12] the probability that a test point x  X  is labeled as class y  X  = 1 , given training data ( X,y ) , is computed as The predictive distribution p ( z ( x  X  ) | X,y,x  X  ) represents a regression on z ( x  X  ) with a complicated observation model y | z . The central observation from Eq. (1) is that we could selectively shrink the prediction p ( y  X  = 1 | X,y,x  X  ) towards a conservative value 1 / 2 by selectively shrinking p ( z ( x  X  ) | X,y,x  X  ) closer to a point mass at zero. In this section we construct the heavy-tailed stochastic process by transforming a GP. As with the GP, we will treat the new process as a prior on functions. Suppose that diag ( K ( X,X )) =  X  2 1 . We define the heavy-tailed process f ( X ) with marginal c.d.f. G b as only consider the case when G b is the (continuous) c.d.f. of a heavy-tailed density g b with scale parameter b that is symmetric about the origin. Examples include the Laplace, hyperbolic secant and Student-t distribution. We note that other authors have considered asymmetric or even discrete distributions [2, 11, 16] while Snelson et al. [15] use arbitrary monotonic transformations in place in transferring the correlation structure encoded by K ( X,X ) from z ( X ) to f ( X ) . If we define chosen to be Gaussian, we would recover the Gaussian process. The predictive distribution p ( f ( x  X  ) | X,f ( X ) ,x  X  ) can be interpreted as a Heavy-tailed process regression (HPR). It is easy to see that its computation can be reduced to standard computations in a Gaussian model by nonlinearly transforming observations f ( X ) into z -space. The predictive distribution in z -space satisfies The corresponding distribution in f -space follows by another change of variables. Having defined the heavy-tailed stochastic process in general we now turn to an analysis of its shrinkage properties. By  X  X elective shrinkage X  we mean that the degree of shrinkage applied to a collection of estimators varies across estimators. As motivated in Section 2, we are specifically interested in selectively shrinking posterior distributions near isolated observations more strongly than in dense regions. This section shows that we can achieve this by changing the form of prior marginals (heavy-tailed instead of Gaussian) and that this induces stronger selective shrinkage than any GPR could induce. Since HPR uses a GP in its construction, which can induce some selective shrinkage on its own, care must be taken to investigate only the additional benefits the transformation G  X  1 b ( X  0 , X  2 (  X  )) has on shrinkage. For this reason we assume a particular GP prior which leads to a special type of shrinkage in GPR and then check how an HPR model built on top of that GP changes the observed behavior. In this section we provide an idealized analysis that allows us to compare the selective shrinkage obtained by GPR and HPR. Note that we focus on regression in this section so that we can obtain analytical results. We work with n measurement locations, X = ( x 1 ,...,x n ) , whose index set { 1 ,...,n } can be partitioned into a  X  X ense X  set D with | D | = n  X  1 and a single  X  X parse X  index s /  X  Assuming that n &gt; 2 we fix the remaining entry  X  K ( x s ,x s ) = / ( + n  X  2) , for some &gt; 0 . We interpret as a noise variance and let K =  X  K + I .
 Denote any distributions computed under the GPR model by p gp (  X  ) and those computed in HPR by p hp (  X  ) . Using K ( X,X ) = K , define z ( X ) as in Eq. (2). Let y denote a vector of real-valued measurements for a regression task. The posterior distribution of z ( x i ) given y , with x i  X  X , is derived by standard Gaussian computations as For our choice of K ( X,X ) one can show that  X  2 d =  X  2 s for d  X  D . To ensure that the posterior distributions agree at the two locations we require  X  d =  X  s , which holds if measurements y satisfy
A similar analysis can be carried out for the induced HPR model. By Eqs. (5) X (7) HPR inference y in f -space satisfy tribution (b) the hyperbolic secant distribution (c) a Student-t inspired distribution, all with scale parameter b . Each plot shows three samples X  X otted, dashed, solid X  X or growing b . As b increases the distributions become heavy-tailed and the gradient of G  X  1 b ( X  0 , X  2 ( x )) increases. To compare the shrinkage properties of GPR and HPR we analyze select pairs of measurements strongly convex on [0 , +  X  ) and has gradient &gt; 1 on R . To see intuitively why this should hold, |  X  eral choices of G b , provided b is large enough, i.e., that g b has sufficiently heavy tails. Indeed, it can Analyzing such y is relevant, as we are most interested in comparing how multiple reinforcing ob-servations at clustered locations and a single isolated observation are absorbed during inference. By ing element y 0 = G  X  1 b ( X  0 , X  2 ( y ))  X  X  hp then satisfies Thus HPR inference leads to identical predictive distributions in f -space at the two locations even to the GPR measurements y ( x s ) and y ( x d  X  ) . As this statement holds for any y  X  Y gp satisfying our earlier sign requirement, it indicates that HPR systematically shrinks isolated observations more an intuitive connection suggests itself when looking at inequality (8): the heavier the marginal tails, the stronger the inequality and thus the stronger the selective shrinkage effect.
 The previous derivation exemplifies in an idealized setting that HPR leads to improved shrinkage of predictive distributions near isolated observations. More generally, because GPR transforms mea-surements only linearly, while HPR additionally pre-transforms measurements nonlinearly, our anal-ysis suggests that for any GPR we can find an HPR model which leads to stronger selective shrink-age. The result has intuitive parallels to the parametric case: just as ` 1 -regularization improves shrinkage of parametric estimators, heavy-tailed processes improve shrinkage of nonparametric es-timators. We note that although our analysis kept K ( X,X ) fixed for GPR and HPR, in practice we are free to tune the kernel to yield a desired scale of predictive distributions. The above analysis has been carried out for regression, but motivates us to now explore heavy-tailed processes in the classification case. The derivation of heavy-tailed process classification (HPC) is similar to that of standard multiclass GPC with Laplace approximation in Rasmussen and Williams [12]. However, due to the nonlinear transformations involved, some nice properties of their derivation are lost. We revert notation and let y denote a vector of class labels. For a C -class classification problem with n training points we For each block c  X  { 1 ,...,C } of n variables we define an independent heavy-tailed process prior using Eq. (4) with kernel matrix K c . Equivalently, we can define the prior jointly on f by letting K be a block-diagonal kernel matrix with blocks K 1 ,...,K C . Each kernel matrix K c is defined by a (possibly different) symmetric positive semidefinite kernel with its own set of parameters. The following construction relaxes the earlier condition that diag ( K ) =  X  2 1 and instead views  X  0 , X  2 (  X  ) as some nonlinear transformation with parameter  X  2 . By this relaxation we effectively adopt Liu et al. X  X  [9] interpretation that Eq. (4) defines the copula. The scale parameters b could in principle vary across the nC variables, but we keep them constant at least within each block of n . Labels y are represented in a 1-of-n form and generated by the following observation model For inference we are ultimately interested in computing occur in p ( f  X  | X,y,x  X  ) , provided the prior marginals have sufficiently heavy tails. 5.1 Inference As in GPC, most of the intractability lies in computing the predictive distribution p ( f  X  | X,y,x  X  ) . We use the Laplace approximation to address this issue: a Gaussian approximation to p ( z | X,y ) is found and then combined with the Gaussian p ( z  X  | X,z,x  X  ) to give us an approximation to p ( z  X  | X,y,x  X  ) . This is then transformed to a (typically non-Gaussian) distribution in f -space using a change of variables. Hence we first seek to find a mode and corresponding Hessian matrix of the log posterior log p ( z | X,y ) . Recalling the relation f = G  X  1 b ( X  0 , X  2 ( z )) , the log posterior can be written as J ( z ) , log p ( y | z ) + log p ( z ) = y &gt; f  X  X Let  X  be an nC  X  n matrix of stacked diagonal matrices diag (  X  c ) for n -subvectors  X  c of  X  . With W = diag (  X  )  X   X  X  &gt; , the gradients are Unlike in Rasmussen and Williams [12],  X  X  X  2 J ( z ) is not generally positive definite owing to its first term. For that reason we cannot use a Newton step to find the mode and instead resort to a simpler gradient method. Once the mode  X  z has been found we approximate the posterior as and use this to approximate the predictive distribution by Since we arranged for both distributions in the integral to be Gaussian, the resulting Gaussian can be straightforwardly evaluated. Finally, to approximate the one-dimensional integral with respect to p ( f  X  | X,y,x  X  ) in Eq. (10) we could either use a quadrature method, or generate samples from by an average. We have compared predictions of the latter method with those of a Gibbs sampler; the Laplace approximation matched Gibbs results well, while being much faster to compute. Figure 2: (a) Schematic of a protein segment. The backbone is the sequence of C 0 ,N,C  X  ,C 0 ,N atoms. An amino-acid-specific sidechain extends from the C  X  atom at one of three discrete an-gles known as  X  X otamers. X  (b) Ramachandran plot of 400 ( X  ,  X ) measurements and corresponding rotamers (by shapes/colors) for amino-acid arginine ( arg ). The dark shading indicates the sparse region we considered in producing results in Figure 3. Progressively lighter shadings indicate how the sparse region was grown to produce Figure 4. 5.2 Parameter estimation imation of the marginal log likelihood is log p ( y | x )  X  log q ( y | x ) = J ( X  z )  X  We optimize kernel parameters  X  by taking gradient steps on log q ( y | x ) . The derivative needs to take into account that perturbing the parameters can also perturb the mode  X  z found for the Laplace approximation. At an optimum  X  J ( X  z ) must be zero, so that where  X   X  is defined as in Eq. (9) but using  X  f rather than f . Taking derivatives of this equation allows us to compute the gradient d  X  z/d X  . Differentiating the marginal likelihood we have The remaining gradient computations are straightforward, albeit tedious. In addition to optimizing the kernel parameters, it may also be of interest to optimize the scale parameter b of marginals G b . Again, differentiating Eq. (12) with respect to b allows us to compute d  X  z/db . We note that when perturbing b we change  X  f by changing the underlying mode  X  z as well as by changing the parameter b which is used to compute  X  f from  X  z . Suppressing the detailed computations, the derivative of the marginal log likelihood with respect to b is of HPC (hyperbolic secant and Laplace marginals) significantly outperform GPC in sparse regions while performing competitively in dense regions. of continuous backbone angles ( X  ,  X ) , one pair for each amino-acid, as well as discrete angles, so-called rotamers, that define the conformations of the amino-acid sidechains that extend from the backbone. The geometry is outlined in Figure 2(a). There is a strong dependence between backbone angles ( X  ,  X ) and rotamer values; this is illustrated in the  X  X amachandran plot X  shown in Figure 2(b), which plots the backbone angles for each rotamer (indicated by the shapes/colors). The dependence is exploited in computational approaches to protein structure prediction, where estimates of rotamer probabilities given backbone angles are used as one term in an energy function that models native protein states as minima of the energy. Poor estimates of rotamer probabilities in sparse regions can derail the prediction procedure. Indeed, sparsity has been a serious problem in state-of-the-art rotamer models based on kernel density estimates (Roland Dunbrack, personal communication). Unfortunately, we have found that GPC is not immune to the sparsity problem. To evaluate our algorithm we consider rotamer-prediction tasks on the 17 amino-acids (out of 20) that have three rotamers at the first dihedral angle along the sidechain 2 . Our previous work thus applies with the number of classes C = 3 and the covariates being ( X  ,  X ) angle pairs. Since the input space is a torus we defined GPC and HPC using the following von Mises-inspired kernel for d -dimensional angular data: where x i,k ,x j,k  X  [0 , 2  X  ] and  X  2 , X   X  0 3 . To find good GPC kernel parameters we optimize an ` 2 -regularized version of the Laplace approximation to the log marginal likelihood reported in Eq. 3.44 of [12]. For HPC we let G b be either the centered Laplace distribution or the hyperbolic secant distribution with scale parameter b . We estimate HPC kernel parameters as well as b by similarly maximizing an ` 2 -regularized form of Eq. (11). In both cases we restricted the algorithms to training sets of only 100 datapoints. Since good regularization parameters for the objectives are not known a priori we train with and test them on a grid for each of the 17 rotameric residues in ten-fold cross-validation. To find good regularization parameters for a particular residue we look up that combination which, averaged over the ten folds of the remaining 16 residues, produced the best test results. Having chosen the regularization constants we report average test results computed in ten-fold cross validation.
 We evaluate the algorithms on predefined sparse and dense regions in the Ramachandran plot, as indicated by the background shading in Figure 2(b). Across 17 residues the sparse regions usually contained more than 70 measurements (and often more than 150), each of which appears in one of the 10 cross validations. Figure 3 compares the label prediction rates on the dense and sparse Figure 4: Average rotamer prediction rate in the sparse region for two flavors of HPC, standard GPC well as CTGP [1] as a function of the average number of points per residue in the sparse region. regions. Averaged over all 17 residues HPC outperforms GPC by 5.79% with Laplace and 7.89% with hyperbolic secant marginals. With Laplace marginals HPC underperforms GPC on only two residues in sparse regions: by 8.22% on glutamine ( gln ), and by 2.53% on histidine ( his ). On dense regions HPC lies within 0.5% on 16 residues and only degrades once by 3.64% on his . Using hyperbolic secant marginals HPC often improves GPC by more than 10% on sparse regions and degrades by more than 5% only on cysteine ( cys ) and his . On dense regions HPC usually performs within 1.5% of GPC. In Figure 4 we show how the average rotamer prediction rate across 17 residues changes for HPC, GPC, as well as CTGP [1] as we grow the sparse region to include more measurements from dense regions. The growth of the sparse region is indicated by progres-sively lighter shadings in Figure 2(b). As more points are included the significant advantage of HPC lessens. Eventually GPC does marginally better than HPC and much better than CTGP. The values reported in Figure 3 correspond to the dark shaded region, with an average of 155 measurements. Copulas [10] allow convenient modelling of multivariate correlation structures as separate from marginal distributions. Early work by Song [16] used the Gaussian copula to generate complex multivariate distributions by complementing a simple copula form with marginal distributions of choice. Popularity of the Gaussian copula in the financial literature is generally credited to Li [8] who used it to model correlation structure for pairs of random variables with known marginals. More recently, the Gaussian process has been modified in a similar way to ours by Snelson et al. [15]. They demonstrate that posterior distributions can better approximate the true noise distribution if the transformation defining the warped process is learned. Jaimungal and Ng [7] have extended this work to model multiple parallel time series with marginally non-Gaussian stochastic processes. Their work uses a  X  X inding copula X  to combine several subordinate copulas into a joint model. Bayesian approaches focusing on estimation of the Gaussian copula covariance matrix for a given dataset are given in [4, 11]. Research also focused on estimation in high-dimensional settings [9]. This paper analyzed learning scenarios where outliers are observed in the input space, rather than the output space as commonly discussed in the literature. We illustrated heavy-tailed processes as a straightforward extension of GPs and an economical way to improve the robustness of estimators in sparse regions beyond those of GP-based methods. Importantly, because these processes are based on a GP, they inherit many of its favorable computational properties; predictive inference in regression, for instance, is straightforward. Moreover, because heavy-tailed processes have a parsimonious representation, they can be used as building blocks in more complicated models where currently GPs are used. In this way the benefits of heavy-tailed processes extend to any GP-based model that struggles with covariate shift.
 Acknowledgements We thank Roland Dunbrack for helpful discussions and providing access to the rotamer datasets. [1] Tamara Broderick and Robert B. Gramacy. Classification and Categorical Inputs with Treed [2] Wei Chu and Zoubin Ghahramani. Gaussian Processes for Ordinal Regression. Journal of [3] Doris Damian, Paul D. Sampson, and Peter Guttorp. Bayesian Estimation of Semi-Parametric [4] Adrian Dobra and Alex Lenkoski. Copula Gaussian Graphical Models. Technical report, [5] Paul W. Goldberg, Christopher K. I. Williams, and Christopher M. Bishop. Regression with [6] Robert B. Gramacy and Herbert K. H. Lee. Bayesian Treed Gaussian Process Models with an [7] Sebastian Jaimungal and Eddie K. Ng. Kernel-based Copula Processes. In Proceedings of the [8] David X. Li. On Default Correlation: A Copula Function Approach. Technical Report 99-07, [9] Han Liu, John Lafferty, and Larry Wasserman. The Nonparanormal: Semiparametric Esti-[10] Roger B. Nelsen. An Introduction to Copulas . Springer, 1999. [11] Michael Pitt, David Chan, and Robert J. Kohn. Efficient Bayesian Inference for Gaussian [12] Carl E. Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning . [13] Alexandra M. Schmidt and Anthony O X  X agan. Bayesian Inference for Nonstationary Spa-[14] John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis . Cambridge [15] Ed Snelson, Carl E. Rasmussen, and Zoubin Ghahramani. Warped Gaussian Processes. In [16] Peter Xue-Kun Song. Multivariate Dispersion Models Generated From Gaussian Copula.
