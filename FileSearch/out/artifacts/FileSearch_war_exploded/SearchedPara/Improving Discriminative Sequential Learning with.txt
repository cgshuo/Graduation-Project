 Discriminative sequential learning models like Conditional Random Fields (CRFs) have achieved significant success in several areas such as natural language processing or informa-tion extraction. Their key advantage is the ability to capture various non X  X ndependent and overlapping features of inputs. However, several unexpected pitfalls have a negative influ-ence on the model X  X  performance; these mainly come from an imbalance among classes/labels, irregular phenomena, and potential ambiguity in the training data. This paper presents a data X  X riven approach that can deal with such hard X  X o X  X redict data instances by discovering and empha-sizing rare X  X ut X  X mportant associations of statistics hidden in the training data. Mined associations are then incorpo-rated into these models to deal with difficult examples. Ex-perimental results of English phrase chunking and named entity recognition using CRFs show a significant improve-ment in accuracy. In addition to the technical perspective, our approach also highlights a potential connection between association mining and statistical learning by offering an al-ternative strategy to enhance learning performance with in-teresting and useful patterns discovered from large dataset. I.2.6 [ Artificial Intelligence ]: Learning; H.4 [ Information Systems Applications ]: Miscellaneous Algorithms, Experimentation Discriminative sequential learning, feature selection, infor-mation extraction, text segmentation, association rule
Conditionally-trained or discriminative models like Maxi-mum Entropy (MaxEnt) [3], Discriminative HMMs [5], Max-imum Entropy Markov Models (MEMMs) [18], and CRFs [13] have achieved significant success in many (sequential) labeling and segmenting tasks, such as part X  X f X  X peech (POS) tagging [24], text segmentation or shallow parsing [20, 25], information extraction [8, 22], object detection in computer vision [26], image analysis and labeling [10, 12], and bio-logical sequence modeling [27]. The noticeable advantage of these models is their flexibility to integrate a variety of arbitrary, overlapping, and non X  X ndependent features at dif-ferent levels of granularity from the observed data.
However, applications employing these models with fixed and hand X  X uilt feature templates usually generate a huge number of features, up to millions, e.g., in [25]. This is be-cause one usually utilizes complex templates including con-junctions of atomic context predicates, e.g., n X  X ram of words or POS tags, to cover as many combinations of statistics as possible without eliminating irrelevant ones. As a result, models using long and fixed conjunction templates are heav-ily overfitting and time X  X onsuming to train because they contains many teacher X  X pecific and redundant features. To reduce these drawbacks, McCallum [19] proposed a likelihood X  driven feature induction for CRFs that is based on a famous feature inducing strategy for exponential models [21]. This method iteratively adds the conjunctions of atomic obser-vational tests that most increase conditional log X  X ikelihood into the model until some stopping criteria are reached. In spite of attaining a trade X  X ff between the number of used features and model accuracy, t his strategy may ignore rare but sensitive conjunctions with smaller likelihood gains that are still critical to model performance. Also, when the num-ber of atomic context predicates is large, the number of con-junctions becomes explosive; and thus ranking all conjunc-tions by likelihood gain is very expensive.

In this paper, we propose a data X  X riven approach that can identify and emphasize rare X  X ut X  X mportant associations or co X  X ccurrences of statistics 1 hidden the training data to
In this paper, terms like  X (atomic) context predicates X ,  X (singleton) statistics X , or  X (atomic) observational tests X  are used interchangeably to refer to particular kinds of contex-tual information observed from the training data improve prediction accuracy fo r hard X  X o X  X lassify instances. The main motivation and the underlying idea of this ap-proach are based on the fact that (sequential) data, such as natural language or biological information, potentially contain the following phenomena that should be the major sources of prediction errors:
Data instances falling into the above situations should be hard examples. Thus, the prediction of their labels does not usually obey the frequently observed statistics. In other words, the simple aggregation of singleton context predicates may lead to misleading predictions because the common sta-tistics always overwhelm uncommon ones. To overcome this pitfall, a model should rely on rare X  X ut X  X mportant associa-tions or conjunctions of singleton context predicates to win the dominance of common decisions. In the first example, most contextual supports surrounding plans (e.g., trip is a singular noun, plan s ends with s )tendtosaythat plans is a singular verb rather than a part of a noun phrase. It is, how-ever, quite easy for the model to recognize plans as a plural noun if relying on an important association like  X  X f the word after plans for is capitalized, then plans should be a plural noun X . This association rule emphasizes a rare but impor-tant co X  X ccurrence of three factors: plans , for ,andthenext word is initially capitalized (such as a location like a city or a country). Although such kind of associations may only oc-cur several times in a whole dataset, their appearance is an important source of evidence to deal with difficult instances.
In spite of their benefit, mining all rare X  X ut X  X mportant associations of singleton statistics in big datasets is challeng-ing because the number of candidates is prohibitively large. Fortunately, we find that association rule mining techniques, such as FP-growth [11], are very useful for discovering such patterns. In our method, the set of rare X  X ut X  X mportant as-sociations is a special subset of rare but highly confident association rules discovered in the training data. Selected associations are then integrated into the learning process ac-cordingtooneofthreewaystoi mprove the prediction accu-racy for hard instances: (a) associations as normal features, (b) associations as normal features with weighted feature values, and (c) associations as constraints for the inference process.

Derived from a reasonable assumption about rare X  X ut X  important associations and the robustness of association rule mining techniques, our approach offers the following distinc-tive characteristics: (1) rare X  X ut X  X mportant associations are globally discovered from a huge number of candidates with any length and any combination of singleton statistics; (2) models with those associations can deal with difficult in-stances while preventing overfitting by avoiding long and fixed conjunction templates; (3) users can choose a suit-able way to incorporate selected associations into their mod-els. Particularly, 100% X  X o nfidence associ ations can be inte-grated into the model in terms of constraints for inference; and (4) our method can be used to improve any discrimi-native sequential learning application, especially for highly ambiguous and imbalanced data; (5) finally, our work also highlights a potential connection between pattern mining and statistical learning from large datasets.

The rest of this paper is organized as follows. Section 2 briefly introduces linear X  X hain CRFs, a typical sequen-tial learning model. Section 3 presents the proposed ap-proach. Section 4 describes and discusses the experimental results. Section 5 reviews related work. Finally, conclusions are given in Section 6.
The goal of labeling/tagging for sequential data is to learn to map observation sequences to their corresponding label sequences, e.g., the sequence of POS tags for words in a sen-tence. Discriminative HMMs [5], MEMMs [18], and CRFs [13] were intentionally designed for such sequential learning applications. In contrast to generative models like HMMs [23], these models are discriminative, i.e., trained to pre-dict the most likely label sequence given the observation sequence. In this paper, CRFs are referred to as the undi-rected linear X  X hain of model states, i.e., conditionally X  X rained finite state machines (FSMs) that obey the first X  X rder Markov independence assumption. The strength of CRFs is that they can combine both the sequential property of HMMs and the philosophy of MaxEnt as well as global normaliza-tion that can avoid the label X  X ias problem [13]. In our work, CRFs were used to conduct all experiments. Let o =( o 1 ,o 2 ,...,o T ) be some observed data sequence. Let S be a set of FSM states, each of which is associated with a label, l  X  X  .Let s =( s 1 ,s 2 ,...,s T )besomestate sequence, CRFs [13] define the conditional probability of a state sequence given an observation sequence as normalization summing over all label sequences. f k denotes a feature function in the language of maximum entropy mod-eling and  X  k is a learned weight associated with feature Each f k is either a per X  X tate or a transition feature: where  X  denotes the Kronecker X   X  . A per X  X tate feature (2) combines the label l of current state s t and a context predi-cate, i.e., the binary function x k ( o ,t )thatcapturesapartic-ular property of the observation sequence o at time position t . For example, the current label is JJ (adjective) and the current word is  X  sequential  X . A transition feature (3) rep-resents sequential dependencies by combining the label l of the previous state s t  X  1 and the label l of the current state s , such as the previous label l = JJ and the current label l = NN (noun).
Inference in CRFs is to find the most likely state sequence s given the observation sequence o ,
In order to find s  X  , one can apply a dynamic programming technique with a slightly modified version of the original Viterbi algorithm for HMMs [23]. To avoid an exponential X  time search over all possible settings of s , Viterbi stores the probability of the most likely path up to time t which accounts for the first t observations and ends in state s We denote this probability to be  X  t ( s i )(0  X  t  X  T  X  1) and  X  ( s i ) to be the probability of starting in each state s recursion is given by:
The recursion terminates when t = T  X  1 and the biggest unnormalized pr obability is p  X  =argmax i [  X  T ( s i )]. At this time, we can backtrack through the stored information to find the most likely sequence s  X  .
CRFs are trained by setting the set of weights  X  = {  X  1 ,... } to maximize the log X  X ikelihood, L , of a given training data where the second sum is a Gaussian prior over parameters with variance  X  2 , which provides smoothing to deal with sparsity in the training data [4].

When the labels make the state sequence unambiguous, the likelihood function in exponential models such as CRFs is convex, thus searching the global optimum is guaranteed [19]. However, the optimum can not be found analytically. Parameter estimation for CRFs requires an iterative proce-dure. It has been shown that quasi X  X ewton methods, such as L X  X FGS [15], are most efficient [17, 25]. This method can avoid the explicit estimation of the Hessian matrix of the log X  X ikelihood by building up an approximation of it us-ing successive evaluations of the gradient.

L X  X FGS is a limited X  X emory quasi X  X ewton procedure for unconstrained optimization that requires the value and gradient vector of the function to be optimized. Let s j note the state path of training instance j in training set then the log X  X ikelihood gradient component of  X  k is where C k ( s , o ) is the count of feature f k given s and o ,equal ues for all positions, t , in the training sequence. The first two terms correspond to the difference between the empir-ical and the model expected values of feature f k .Thelast term is the first X  X erivative of the Gaussian prior.
This section presents the proposed framework in detail: (1) how to discover rare X  X ut X  X mportant associations from the training data and (2) how to integrate those associations into discriminative sequential learning models, e.g. CRFs.
This section first presents the concept rare X  X ut X  X mportant associations in discriminative sequential learning based on the traditional association rules [1], and then describes a method to discover such patterns from the training data. Table 1: Transactional database of POS tagging data ( o ( k ) , l ( k ) ): ... highly RB ambiguous JJ data NNS ... ...
 RB, ... w t :highly, w t +1 :ambiguous, ...

JJ, w t  X  1 :highly, w t :ambiguous, w t +1 :data, suf2( w
NNS, w t  X  1 :ambiguous, w t :data, ..., suf2( w t  X  1 ):us ...

Recall that the training dataset for sequential learning the k th data observation and label sequences, respectively. Let A = { A 1 ,A 2 , ...,A M } be the set of context predicate templates in which each template A i captures a particular type of contextual information about data observations. In asense, A is similar to the set of attributes in a relational table. Applying all templates in A to each position in every training sequence ( o ( k ) , l ( k ) ) in the training data tain a transactional database T D in which each transaction consists of a label and a list of active context predicates.
For example, the first part of Table 1 shows the train-ing data D for POS tagging in which each training sequence ( o , l ) is an English sentence together with POS tags of words. The second part is a set of 4 context predicate templates: the identities of the previous word ( w t  X  1 ), the current word ( w t ), the next word ( w t +1 ), and the 2-character suffix of the previous word (suf2( w i  X  1 )). The third part is the transac-tional database T D after applying templates in A for D .
Let I = { x 1 ,x 2 ,...,x n } be the set of all possible context predicates in the transactional database T D ,let L be the set of all labels, and T = { t 1 ,t 2 ,...,t m } be the set of all trans-actions in T D . Our target is to examines every predictive association rule r having the form below, where the left hand side (LHS) of r , X = x i 1  X  x i 2  X  ...  X  x  X  X  , is a conjunction of p context predicates in I ,andthe right hand side (RHS) of rule r , i.e., l  X  X  , is a particular label. The support of the rule r , denoted as sup( r ), is the number of transactions in T containing { l } X  X ,andthe confidence of r , denoted as conf( r ), is the conditional prob-ability that a transaction in T has the label l given that it contains X , i.e., conf( r )=sup( X  X  X  l } ) / sup( X ). In a sense, this kind of rule is similar to the associative classifi-cation rules in [14, 16] except that our work mainly focuses on rare X  X ut X  X mportant associations as discussed in the next section.
Derived from the predictive association rules defined in (8) and the concepts of support and confidence factors, we present a descriptive definition of rare X  X ut X  X onfident asso-ciations below.

Definition 1. Let lsup and usup be two integers that are much smaller than the total number of transactions in T (i.e., lsup  X  usup |T| ), and let lconf be a real number that satisfies the condition 0  X  lconf  X  1andlconf 1. A predictive association rule r in (8) is called a rare X  X ut X  confident if:
All predictive association rules satisfying definition (1) are rare X  X ut X  X onfident. However, NOT all of them are impor-tant. This is based on the important observation that:  X  X f most context predicates in the LHS of a rare X  X ut X  X onfident rule r strongly support the label l , then the rule r is trivial X . In other words, if most context predicates in the LHS of r largely support label l in a separated manner, there is no need to examine the co-occurrence of all items in the LHS, and the model can still work properly without this rule. For example, in named entity recognition, the rule w t  X  1 :New  X  w t :York  X  w t +1 :weather  X  label t =LOCATION is not important because both  X  w t  X  1 :New X  and  X  w t :York X  strongly support the label  X  X OCATION X , and thus their conjunction should be unnecessary. In other words, the named entity recognizer can predict the label  X  X OCATION X  for the word  X  X ork X  without the above rule because both  X  X ew X  and  X  X ork X  are frequently observed in the training data as a lo-cation name, i.e.  X  X ew York X . Based on this observation, we define the concept of  X  X are X  X ut X  X mportant X  associations as follows,
Definition 2. A rare X  X ut X  X onfident rule r : X  X  l is con-sidered to be rare X  X ut X  X mportant if there exists at least another label l  X  X  such that the sum of support counts for the label l from the context predicates in the LHS of r is larger than that for the label l , i.e.,
Why are predictive association rules satisfying definition (2) important? Intuitively, if such a rule, r ,existsinthe training data but is not being discovered and emphasized, the model may predict the label l for any data transaction holding all context predicates in the LHS of r when the correct label is l . This is because most singleton context predicates in LHS of r tend to support the label l rather than l . This is why the appearance of predictive association rules satisfying definition (2) is important. There should be more sophisticated definitions and conditions of rare X  X ut X  important predictive association rules. However, we choose the above definition because of the trade X  X ff between the rigorousness and the simplicity of calculation.

For instance, the predictive association rule w t  X  1 :New  X  w t :York  X  w t +1 :University  X  label t =ORGANIZATION is important for recognizing the named entity type of the current word ( X  X ork X ) since there is another label,  X  X OCA-TION X , that should satisfy the condition addressed in defin-ition (2), i.e., x  X  X sup( x  X  LOCATION) &gt; x  X  X sup(  X 
ORGANIZATION). This is because both New and York strongly support the label  X  X OCATION X  rather than  X  X R-GANIZATION X . Thus, the appearance of the above rule can help the model to recognize  X  X ew York University X  as an organization rather than a location.
Mining rare X  X ut X  X mportant associations from the transac-tional database T D encounters the following problems: (1) the number of data items, i.e., the number of atomic con-text predicates and labels |I  X  X | , is relatively large; and (2) the support thresholds, i.e., lsup and usup, are very small compared to the number of transactions |T | . This means that there are a huge number of combinations of items that must be examined during the mining process.

Fortunately, FP X  X rowth [11], a frequent pattern mining algorithm without candidate generation, can discover such associations in an acceptable computational time. This is because FP X  X rowth employs a FP X  X ree (an extended pre-fix tree structure) to store crucial, quantitative information about frequent patterns in such a way that more frequently occurring items will have better chances of sharing nodes than less frequently occurring ones. All mining operations are then performed on the FP X  X ree in a partitioning, recur-sive fashion without candidate generation. See [11] for a complete description of this algorithm.

Taking the sequential training data D = { ( o ( k ) , l ( k ) the set of context predicate templates A = { A 1 ,A 2 ,...,A the lower and upper support thresholds lsup, usup (lsup  X  usup |T| ), and the lower confidence threshold lconf (0  X  lconf  X  1andlconf 1) as inputs, Rare X  X ut X  X mportant association mining includes the following steps: 1. Transforming the sequential training data D to a trans-2. Mining all itemsets with supports larger or equal to 3. Generating all rare X  X ut X  X onfident association rules in 4. Selecting all possible rare X  X ut X  X mportant association
In the fourth step, to determine whether or not a rare X  but X  X onfident rule, r : X  X  l , is rare X  X ut X  X mportant, we have to scan the database to compute the sums of supports of context predicates in the LHS of r for all other labels. This is an expensive operation. Fortunately, we can perform this on the FP X  X ree by traversing the node X  X inks of each label, starting at the header table, and looking upward and downward to count the supports from context predicates appearing in the LHS of r . See [11, 14] for the detailed description of FP X  X ree.
This section presents three ways to incorporate the rare X  but X  X mportant associations discovered from the training data into CRFs: (1) associations as normal features, (2) associa-tions as features with emphasized feature functions, and (3) associations as constraints for the inference process.
All rare X  X ut X  X mportant associations are in the form X  X  l ,inwhich X = x i 1  X  x i 2  X  ...  X  x ip (  X  X  ) is a conjunction of p context predicates and l  X  X  is a particular label. These associations can be integrated into CRFs in terms of normal per X  X tate features as follows.
These per X  X tate features are similar to those in (2) except that they capture a co X  X ccurrence of p atomic context pred-icates rather than a single one. The features are treated as normal features and are trained together.
It is noticeable that rare X  X ut X  X mportant features are infre-quently observed in the training data, and thus their learned weights should be small. This means that their contribu-tions, in several cases, may not be sufficient to win the dominance of common statistics, i.e., frequently observed singleton features. To overcome this drawback, we empha-size rare X  X ut X  X mportant features by assigning larger feature function values compared to normal features. k ( s t , o ,t )= where  X  ( s t ,l )and { x i 1 ( o ,t )  X  ...  X  x ip ( o ,t logic expressions, and v is larger than 1 (the feature value of normal features). v should be large if the occurrence frequency of the feature (also the support of the rare X  X ut X  important association) is small. Thus, for each feature gen-erated from a rare X  X ut X  X mportant association r , v is equal to (usup  X  sup( r ) + 2). This ensures that v is always bigger than 1 and inversely proportional to the support of r , i.e., the occurrence frequen cy of the feature.
Constrained CRFs are extensions of CRFs in which use-ful constraints are incorporated into the inference process (i.e., the Viterbi algorithm) to correct potential errors ex-isting in the most likely output state sequence for each in-put observation sequence. Kristjansson et al. [8] proposed this extension with the application to interactive form fill-ing, in which users can examine the filling process and make necessary corrections in term s of their own constraints. A re X  X orrection applied at a particular position will propagate though the Viterbi sequence to make automatic updates for labels at other positions, i.e., the correction propagation ca-pability.

This section presents the integration of rare X  X ut X  X mportant associations with 100% -confidence into the Viterbi algorithm in terms of data X  X riven constra ints to make corrections di-rectly to the inference process of CRFs. Unlike those used in [8], our constrain ts are 100% X  X o nfidence associations and are automatically discovered from the training data.
Normally, CRFs use a variant of the traditional Viterbi algorithm to find the most likely state sequence given an input observation sequence. To avoid an exponential X  X ime search over all possible settings of state sequence, this al-gorithm employs a dynamic programming technique with a forward variable  X  t +1 ( s i ) in definition (5).
Let R = { r 1 ,r 2 ,...,r q } be a set of q rare X  X ut X  X mportant associations with 100%  X  X onfidence, and each r u (1  X  u  X  q has the form x u 1  X  x u 2  X  ...  X  x up  X  l u ( l u  X  X  ). Each r u  X  R is considered to be a constraint for the inference process. At each time position in the testing data sequence, we check whether or not the set of active context predicates at the current position holds the LHS of any rule r u  X  R yes, the most likely state path must go though the current state with the label l u (i.e., the RHS or rule r u ), and the possibility of passing though other labels equals to zero. The constrained forward variable is re-defined as follows.  X 
The constraint applied at the time position t will propa-gate though the whole sequence and make some re X  X orrections for labels at other positions (mostly around the position
One problem is that when the number of constraints (i.e., the number of 100% X  X  onfidence rare X  X ut X  X  mportant predic-tive association rules) is large, the time for examining the LHS of every rule at each position in the testing sequence also becomes large. To overcome this obstacle, we propose the following algorithm for a fast checking for constraints at a particular time position t in the testing sequence.
Let R = { r 1 ,r 2 ,...,r q } be the set of 100%  X  X onfidence rules, also known as constraints, and let X = { x 1 ,x 2 ,...,x be the set of m active context predicates observed at the cur-rent position t . The target of the following algorithm is to check whether or not X holds the LHS of any constraint r u  X  R . If yes, choose the constraint with the longest LHS. 1. For each x i  X  X , lookup the set of constraints R i  X  R 2. For each constraint r j  X  R ,let c j be the sum of oc-3. Find the pair r j ,c j (1  X  j  X | R | ) such that c j is the
If this algorithm find a constraint r j , then apply this con-straint to the current position t with formula (9), otherwise, apply the normal Viterbi recursion as formula (5).
All the experiments were performed with our C/C++ implementation of CRFs  X  FlexCRFs 2  X  X n2.5GHz,1Gb RAM, Pentium IV processor with RedHat Linux. All CRF models were trained using the limited X  X emory quasi X  X ewton method for unconstrained optimization, L X  X FGS [15]. Un-like those used in [25], our CRF models are simpler and easier to implement by obeying the first X  X rder Markov prop-erty, i.e., the label of the current state depends only on the label of the previous state.

Training and testing data for English phrase chunking and named entity recognition can be found at the shared tasks of CoNLL2000 3 and CoNLL2003 4 , respectively.
Phrase chunking, an intermediate step toward full pars-ing of natural language, identifies phrase types (e.g., noun phrase  X  NP, verb phrase  X  VP, PP  X  prepositional phrase, etc.) in text sentences. Here is an example of a sentence with phrase marking:  X  X NP He] [VP reckons] [NP the cur-rent account deficit] [VP will narrow] [PP to] [NP only # 1.8 billion] [PP in] [NP September]. X 
The training and testing data for this task is available at the shared task for CoNLL X 2000. The data consist of the same partitions of the Wall Street Journal corpus (WSJ): sections 15 X 18 as training data (8936 sentences, 211727 to-kens) and section 20 as testing data (2012 sentences, 47377 tokens). Each line in the annotated data is for a token and consists of three columns: the token (a word or a punctu-ation mark), the part-of-speech tag of the token, and the phrase type label (label for short) of the token. The la-bel of each token indicates whether the token is outside a phrase (O), starts a phrase (B X  PhraseType ), or continues
The documents and source code of FlexCRFs are available at http://www.jaist.ac.jp/  X  hieuxuan/flexcrfs/flexcrfs.html http://cnts.uia.ac.be/conll2000/chunking/ http://cnts.uia.ac.be/conll2003/ner/ a phrase (I X  PhraseType ). For example, the label sequence of the above sentence is  X  X  X  X P B X  X P B X  X P I X  X P I X  X P I X  NP B X  X P I X  X P B X  X P B X  X P I X  X P I X  X P I X  X P B X  X P B X  X P O X . This dataset contains 11 phrase types as shown in the first column of Table 3. Two consecutive data sequences (sentences) are separated by a blank line.
On the phrase chunking dataset, we use feature templates as shown in Table 2. All transition features obey the first X  order Markov dependency that the label ( l ) of the current state depends on the label ( l ) of the previous state (e.g.,  X  l = I X  X P X  and  X  l = B X  X P X ). Each per X  X tate feature ex-presses how much influence a context predicate ( x ( o ,t served surrounding the current position t has on the label ( l ) of the current state. A context predicate captures a par-ticular property of the observation sequence. For instance, the per X  X tate feature  X  l = I X  X P X  and  X  X ord t  X  1 is the  X  X n-dicates that the label of the current state should be I X  X P (i.e., continue a noun phrase) if the previous word is the .
Table 3 describes both transition and per X  X tate feature templates. Context predicates for per X  X tate features are identities of words, POS tags of words surrounding the cur-rent position t , such as words and POS tags at positions t  X  2, t  X  1, t , t +1, t + 2 (i.e., window size is 5). Table 2: Feature templates for phrase chunking
We also employ 2 X  X rder conjunctions of the current word with the previous ( w t  X  1  X  w t ) or the next word ( w t and 2 X  X rder and 3 X  X rder conjunctions of two or three con-secutive POS tags within the current window to make use of the mutual dependencies among singleton properties.
With the feature templates shown in Table 2 and the fea-ture rare threshold of 1 (i.e., only features with occurrence frequency larger than 1 are included into the CRF model), 321526 context predicates and 152856 CRF features were generated from 8936 training data sequences.
Let I be the itemset of 321548 data items, i.e., the union set of 321526 context predicates and 22 phrase labels; T be the set of 211727 data transactions corresponding to 211727 tokens of the training data (the maximum transaction length is 20, i.e., 19 context predicate templates plus the label). Let the lower support (lsup) and upper support (usup) thresh-olds be 4 and 8, respectively; the lower confidence (lconf) threshold be 0.98 or 98%. In fact, all output rules have the confidence of 100% because lconf = 0 . 98 &gt; 7 8 and there-fore larger than all other confidence levels. We also confine the length of the LHS of all rare X  X ut X  X mportant associa-tions between 3 and 6. There are several reasons why we confine the LHS length between 3 and 6. First, although simple rules (i.e., with shorter LHS length) are usually use-ful for generalization, we only examine rules complex enough (with LHS length  X  3) because our main target is to deal with hard data instances which are not frequently observed in the training data. We also observed that the number of rare X  X ut X  X mportant with LHS length smaller than 3 was small. Second, rules with LHS length larger than 6 are usu-ally too specific and most of them are covered by rules with LHS length of 4, 5 or 6. Also, mining and generating all long rules is time X  X onsuming. For these reasons, we only considered rules with LHS length between 3 and 6.
The mining process for rare X  X ut X  X mportant associations took 2 hours using FP X  X rowth algorithm and the filter cri-teria presented in definitions (1) and (2). The output was a set of 10364 rare X  X ut X  X mportant associations with an LHS length between 3 and 6, support between 4 and 8, and con-fidence of 100%. This set of associations were integrated into the CRF model in terms of normal features, normal features with weighted feature values, and constraints for the inference process. Table 3: The performance of English phrase chunk-ing without rare X  X ut X  X mportant associations Phrase #Hm. #Ml. #Mt. Pr. Rc. F1.
 SBAR 535 538 459 85.32 85.79 85.55 ADJP 438 398 303 76.13 69.18 72.49 ADVP 866 864 686 79.40 79.21 79.31 PRT 106 95 75 78.95 70.75 74.63 INTJ 2 1 1 100.0 50.00 66.67 CONJP 9 16 6 37.50 66.67 48.00 UCP 0 0 0 0.00 0.00 0.00 Avg1. 74.12 70.64 72.34 Avg2. 23852 23833 22202 93.16 93.08 93.12 Table 4: The performance of English phrase chunk-ing with rare X  X ut X  X mportant associations as normal features of CRFs Phrase #Hm. #Ml. #Mt. Pr. Rc. F1.
 SBAR 535 522 473 90.61 88.41 89.50 ADJP 438 416 337 81.01 76.94 78.92 ADVP 866 850 726 85.41 83.83 84.62 PRT 106 104 76 73.08 71.70 72.38 INTJ 2 1 1 100.0 50.00 66.67 CONJP 9 11 7 63.64 77.78 70.00 UCP 0 0 0 0.00 0.00 0.00 Avg1. 81.11 75.39 78.14 Agv2. 23852 23845 22333 93.66 93.63 93.65 Table 5: The performance of English phrase chunk-ing with rare X  X ut X  X mportant associations as normal features with weighted feature values Phrase #Hm. #Ml. #Mt. Pr. Rc. F1.
 SBAR 535 524 473 90.27 88.41 89.33 ADJP 438 415 339 81.69 77.40 79.48 ADVP 866 853 726 85.11 83.83 84.47 PRT 106 103 79 76.70 74.53 75.60 INTJ 2 1 1 100.0 50.00 66.67 CONJP 9 10 6 60.00 66.67 63.16 UCP 0 0 0 0.00 0.00 0.00 Avg1. 85.32 78.64 81.84 Agv2. 23852 23842 22365 93.81 93.77 93.79 Table 6: The performance of English phrase chunk-ing with rare X  X ut X  X mportant associations as con-straints for inference Phrase #Hm. #Ml. #Mt. Pr. Rc. F1.
 SBAR 535 524 475 90.65 88.79 89.71 ADJP 438 419 342 81.62 78.08 79.81 ADVP 866 853 724 84.88 83.60 84.24 PRT 106 103 81 78.64 76.42 77.51 INTJ 2 2 1 50.00 50.00 50.00 CONJP 9 9 7 77.78 77.78 77.78 UCP 0 0 0 0.00 0.00 0.00 Avg1. 82.31 80.03 81.15 Agv2. 23852 23850 22377 93.82 93.82 93.82
Table 3 shows the highest performance (achieved at the 48 th L X  X FGS iteration) of the phrase chunking task trained on the original set of 152856 CRF features without rare X  but X  X mportant associations. In each line, the first column is the phrase type; the second (#Hm.) is the number of human annotated phrases; the third (#Ml.) is the number of phrases automatically marked by the CRF model; the fourth (#Mt.) is the number of correct phrases marked by the model; the last three columns are precision (Pr.), re-call (Rc.), and F1 X  X easure (F1), respectively. The last two lines are the average performance calculated in two ways: precision X  X ecall based and phrase based; the first is based on the precision and recall values of separated phrase types and the second is based on the average numbers of human X  annotated, model, and correct phrases. The first average F1 (72.34%) reflects the balance and the trade X  X ff among per X  label performances while the second average F1 (93.12%) reflects the total performance.

Table 4, which has the same format as Table 3, describes the performance of phrase segmentation when discovered rare X  X ut X  X mportant associations were integrated into the CRF model as normal features. The highest average F1 X  measure achieved at the 45 th L X  X FGS iteration is 93.65%, i.e., 0.53% higher than the original performance.
Table 5 shows the performance when all rare X  X ut X  X mportant associations were incorporated into the CRF model in the form of features with weighted feature values. The highest average F1 X  X easure (at the 47 th iteration) is 93.79%, i.e., 0.67% higher than the original performance.

Table 6 describes the perfor mance when all 100% X  X o nfidence rare X  X ut X  X mportant associations were used as constraints for the inference process. The highest average F1 X  X easure is 93.82%, i.e., 0.70% higher than the original performance.
Named entity recognition (NER), a subtask of information extraction, identifies names of persons (PER), organizations (ORG), locations (LOC), times (DATE/TIME), and quanti-ties (NUMBER, CURRENCY, PERCENTAGE) in natural language. Here is an example of an English sentence with named entities marked:  X  X LOC Germany]  X  X  representative to the [ORG European Union]  X  X  veterinary committee [PER Werner Zwingmann] said on Wednesday ... X 
The training and testing data for English named entity recognition are provided at the shared task for CoNLL X 2003. The dataset is a collection of news wire articles from the Reuters Corpus. The training set consists of 14041 sentences (203621 tokens), and the testing data contains two parts: the development test set (testa: 3250 sentences, 51362 to-kens) and the final test set (testb: 3453 sentences, 46435 tokens). The data files contain four columns separated by a blank space. Each token (a word or a punctuation mark) has been put on a separate line and there is an empty line after each sentence (sequence). The first item on each line is a token, the second is the part X  X f X  X peech tag of the token, the third is a phrase type tag (like the label in phrase chunking) of the token, and the fourth is the named entity label (label for short). The label of each token indicates whether the to-ken is outside a named entity (O), or inside a named entity (I X  NamedEntityType ). If two named entities of the same type immediately follow each other, the first token of the second named entity will have tag B X  NamedEntityType . For example, the named entity label sequence of the above sentenceis X  X  X  X OCOOOOI X  X RGI X  X RGOOOI X  X ER I X  X ER O O O ... X .
On the named entity recognition dataset, we used the fea-ture templates shown in Table 7. All transition features also conform to the first Markovian p roperty. Each context pred-icate for a per X  X tate feature is one of the following types: (1) the identities of words ( w t  X  2 , w t  X  1 , w t , w t +1 ple regular expressions or formats of words such as  X  X he first character of a word is capitalized X  (IsInitialCapitalized),  X  X ll chars of a word are capitalized X  (IsAllCapitalized), etc. Like the phrase chunking task, all context predicates are captured within a window with size of 5. Our feature templates are simpler than those used in the previous work presented at the CoNLL2003 shared task and in [19] in two ways: only five simple format propertie s were captured (compared to 16 regular expressions in [19]), and no external dictionaries were used such as the lists of people names, organization names, countries, cities, etc.

With the feature templates described in Table 7 and the feature rare threshold of 1, 125206 context predicates and 77826 features were generated from 14041 training sequences.
Current state: s t Previous state: s t  X  1
Current state: s t Context predicate: x ( o ,t )
Let I be the itemset of 125215 data items, i.e., the union set of 125206 context predicates and 9 named entity labels; T be the set of 203621 data transactions corresponding to 203621 tokens of the training data (the maximum transac-tion length is 41, i.e., 40 context predicate templates plus the label). Let the lower support (lsup) and upper support (usup) thresholds be 4 and 8, respectively; the lower confi-dence (lconf) threshold be 0.98 or 98%. We also examine rules with the LHS length between 3 and 6.

The mining process for rare X  X ut X  X mportant associations took about 1.5 hours using FP X  X rowth algorithm and the filter criteria described in definitions (1) and (2). The output was a set of 9023 rare X  X ut X  X mportant associations with an LHS length between 3 and 6, support between 4 and 8, and confidence of 100%. This set of associations was integrated into the CRF model in terms of normal features, normal features with weighted feature values, and constraints for the inference process. Table 8: The performance of English named en-tity recognition without rare X  X ut X  X mportant asso-ciations NEType #Hm. #Ml. #Mt. Pr. Rc. F1.
 ORG 1325 1254 1043 83.17 78.72 80.88 MISC 916 852 735 86.27 80.24 83.14 Avg1. 87.09 84.15 85.60 Avg2. 5902 5741 5030 87.62 85.23 86.40
Table 8 shows the highest performance (F1 of 86.40%, achieved at the 133 th L X  X FGS iteration) of the NER task trained on the original set of 77826 CRF features. This table has the same format as Table 3 except that the first column of each line is the named entity type.

Table 9, which has the same format as Table 8, displays the experimental results of NER when all rare X  X ut X  X mportant Table 9: The performance of English named entity recognition with rare X  X ut X  X mportant associations as normal features of CRFs NEType #Hm. #Ml. #Mt. Pr. Rc. F1.
 ORG 1325 1256 1104 87.90 83.32 85.55 MISC 916 855 757 88.54 82.64 85.49 Avg1. 89.14 86.22 87.66 Agv2. 5902 5747 5136 89.37 87.02 88.18 Table 10: The performance of English named entity recognition with rare X  X ut X  X mportant associations as normal features with weighted feature values NEType #Hm. #Ml. #Mt. Pr. Rc. F1.
 ORG 1325 1250 1108 88.64 83.62 86.06 MISC 916 853 757 88.75 82.64 85.59 Avg1. 89.52 86.43 87.95
Agv2. 5902 5739 5150 89.74 87.26 88.48 associations were integrated into CRF model in terms of normal features. The highest F1 X  X easure is 88.18%, i.e., 1.78% higher than the original performance. Table 10 shows the results of NER in the case rare X  X ut X  X mportant associ-ations were encoded into the model in the form of normal features with weighted values. The highest average F1 is 88.48%. Table 11 demonstrates the performance when all 100% X  X o nfidence rare X  X ut X  X mportant associations were in-tegrated into the inference process in terms of Viterbi con-straints. The highest F1 obtained in this case is 88.51%.
We can see that the integration of rare X  X ut X  X mportant associations into CRF models can improve the performance of both the phrase chunking and named entity recognition tasks. The F1 X  X easure (Avg2.) of phrase chunking in-creases from 93.12% to 93.65%, 93.79%, and 93.82% corre-sponding to three methods of encoding rare X  X ut X  X mportant associations. Similarly, the F1 X  X easure of NER increases from 86.40% to 88.18%, 88.48%, and 88.51%. The precision X  recall based F1 X  X easure (Avg1.) also increases from 72.34% to 78.14%, 81.84%, and 81.15% for phrase chunking and from 85.60% to 87.66%, 87.95%, and 87.99% for named en-tity recognition. This demonstrates that our approach can improve not only total performance but also the balance among classes/labels.

We can also draw some conclusions from the experimental results: (1) rare X  X ut X  X mportant associations as normal CRF features (the first method) can significantly enhance the to-tal performance; however, treating rare X  X ut X  X mportant as-sociations as normal features can not fully utilizes their ad-vantages; (2) rare X  X ut X  X mportant associations as constraints for inference (the third method) are sometimes too aggres-sive because they are globally true on one training dataset but may not be true on another; and (3) treating rare X  but X  X mportant associations as normal features with empha-sized values should be the favorable choice because they are neither too loosely nor too tightly integrated with the Table 11: The performance of English named entity recognition with rare X  X ut X  X mportant associations as constraints for inference NEType #Hm. #Ml. #Mt. Pr. Rc. F1.
 ORG 1325 1255 1112 88.61 83.92 86.20 MISC 916 855 758 88.65 82.75 85.60 Avg1. 89.49 86.53 87.99
Agv2. 5902 5746 5155 89.71 87.34 88.51 models. The experimental results show that this method achieves both high total performance and a balance among classes/labels.
 We also did the experiments with rare X  X ut X  X onfident rules. We observed two important points that (1) the numbers of rare X  X ut X  X mportant rules (both noun phrase chunking and named entity recogniztion) were much larger than those of rare X  X ut X  X mportant ones; and (2) the experimental results were sometimes worse because of the overfitting problem. This means that there is a large proportion of rare X  X ut X  important rules that are unnecessary for capturing difficult data instances.

The experimental results reported in this paper do not represent the best possible performances on phrase chunk-ing and named entity recognition because: (1) our feature templates are relatively simple to keep the set of features compact; this is convenient for mining associations, train-ing again and again during conduc ting the experiments; (2) unlike the CRF model in [25], all our CRF models obey the first X  X rder Markov property to reduce the number of fea-tures and the training time.
Discriminative (sequential) learning models have been ap-plied successfully in different natural language processing and information extraction tasks, such as POS tagging [24], text chunking [20, 25], information extraction [8, 22], com-puter vision and image analysis [10, 12, 26], and biological modeling [27]. Normally, one can extract features from se-quential data within a relatively large window size (i.e., the history size of contextual information) and make high X  X rder combinations of atomic observational tests (e.g., the con-junctions of two or three consecutive words in a sentence) in the hope that they will capture as many useful predictive clues as possible. Unfortunately, such useful conjunctions are sparsely distributed in the feature space, and thus one unintentionally includes a large number of redundant con-junctions into the model. Inspired by this obstacle, our work aims at picking up useful conjunctions from a large array of conjunction candidates while keeping the set of features simple. The data X  X riven search with respect to support and confidence factors based on association rule mining tech-niques can discover desired conjunctions with an acceptable computational time.

McCallum [19] proposed an automated feature induction for CRFs that can dramatically reduce the number of used features. This likelihood X  X riven approach repeatedly adds features with high likelihood gains into the model. The set of induced features contains both atomic observational tests and conjunctions of them. The main difference between this work and ours is that McCallum focuses on features with high likelihood X  X ains in order to reduce the number of used features as much as possible, while the main target of our method is to discover rare X  X ut X  X mportant associations or co X  X ccurrences of weak statistics from the training data to highlight difficult examples. Further, our method can ex-amine any combination or conjunction of context predicates because of the exhaustive working method of association rule mining techniques.

An error X  X riven method that combines boosting technique into the training process of CRFs [2] to minimize an up-per bound on the ranking loss that was adapted to label sequences. This method also focuses on hard observation sequences, but without integrating new useful conjunctions of basic features. Another boosting X  X ike training for CRFs is based on the use of  X  X radient tree X  [6] to learn many conjunctions of features. One problem is that this method requires adding many trees for the training process.
In this paper, we proposed a data X  X riven approach that can discover and highlight rare X  X ut X  X mportant associations or co X  X ccurrences of singleton context predicates from the sequential training data to deal with hard examples. Dis-covered associations are integrated into the exponentially-trained sequential learning models as normal features, fea-tures with weighted values, and constraints for the infer-ence process. The experimental results show that rare X  X ut X  important associations can improve the model performance by fighting against the dominance of singleton but common statistics in the training data.

Though rare X  X ut X  X mportant associations can enhance the prediction accuracy for hard examples, our approach is cur-rently based on the occurrence f requency of statistics and the existence of rare X  X ut X  X mportant associations in the train-ing data. We believe that there is an indirect theoretical relation between the occurrence frequencies of statistics and the learned weights of the model X  X  features. Our future work will focus on this potential relation to estimate the extent to which useful patterns (e.g., rare X  X ut X  X mportant associ-ations) discovered from the training data can improve the performance of discriminative (sequential) learning models.
We would like to thank Dr. Bart Goethals, Department of Math and Computer Science, Antwerpen University, for sharing his lightweight and efficient implementation of the FP X  X rowth algorithm. We would like to say thank you to Prof. Jorge Nocedal, Department of Electrical and Com-puter Engineering, School of Engineering and Applied Sci-ence, Northwestern University, the author of FORTRAN im-plementation of the L X  X FGS optimization procedure. We also would like to thank Prof. Sunita Sarawagi, KR School of Information Technology, IIT Bombay, the author of the Java CRFs package, which is the precursor of our C/C++ CRFs toolkit.
