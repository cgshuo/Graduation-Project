 Go is an ancient board game X  X ver 3,000 years old [6, 5] X  X hat p oses unique opportunities and chal-lenges for artificial intelligence and machine learning. Th e rules of Go are deceptively simple: two opponents alternatively place black and white stones on the empty intersections of an odd-sized square board, traditionally of size 19  X  19 . The goal of the game, in simple terms, is for each player to capture as much territory as possible across the bo ard by encircling the opponent X  X  stones. This disarming simplicity, however, conceals a formidable combinatorial complexity [2]. On a average, on the order of 200-300 possible moves at each step o f the game, preventing any form of semi-exhaustive search. For comparison purposes, the game of chess has a much smaller branching factor, on the order of 35-40 [10, 7]. Today, computer chess p rograms, built essentially on search techniques and running on a simple PC, can rival or even surpa ss the best human players. In contrast, and in spite of several decades of significant research effor ts and of progress in hardware speed, the best Go programs of today are easily defeated by an average hu man amateur.
 Besides the intrinsic challenge of the game, and the non-tri vial market created by over 100 million players worldwide, Go raises other important questions for our understanding of natural or artificial intelligence in the distilled setting created by the simple rules of a game, uncluttered by the endless complexities of the  X  X eal world X . For example, to many obser vers, current computer solutions to chess appear  X  X rute force X , hence  X  X nintelligent X . But is t his perception correct, or an illusion X  X s there something like true intelligence beyond  X  X rute force  X  and computational power? Where is Go situated in the apparent tug-of-war between intelligence a nd sheer computational power? Another fundamental question that is particularly salient in the Go setting is the question of knowl-edge transfer. Humans learn to play Go on boards of smaller si zed X  X ypically 9  X  9  X  X nd then  X  X rans-fer X  their knowledge to the larger 19  X  19 standard size. How can we develop algorithms that are capable of knowledge transfer? Here we take modest steps towards addressing these challeng es by developing a scalable machine learning approach to Go. Clearly good evaluation functions and search algorithms are essential in-gredients of computer board-game systems. Here we focus pri marily on the problem of learning a good evaluation function for Go in a scalable way. We do inclu de simple search algorithms in our system, as many other programs do, but this is not the primary focus. By scalability we imply that a main goal is to develop a system more or less automatically, using machine learning approaches, with minimal human intervention and handcrafting. The syst em ought to be able to transfer infor-mation from one board size (e.g. 9  X  9 ), to another size (e.g. 19  X  19 ).
 We take inspiration in three ingredients that seem to be esse ntial to the Go human evaluation process: the understanding of local patterns, the ability to combine patterns, and the ability to relate tactical and strategic goals. Our system is built to learn these three capabilities automatically and attempts to combine the strengths of existing systems while avoiding so me of their weaknesses. The system is capable of automatically learning the propensity of local p atterns from a library of games. Propensity and other local tactical information are fed into a recursiv e neural network, derived from a Bayesian network architecture. The network integrates local inform ation across the board and produces local outputs that represent local territory ownership probabil ities. The aggregation of these probabilities provides an effective strategic evaluation function that i s an estimate of the expected area at the end (or at other stages) of the game. Local area targets for train ing can be derived from datasets of human games. The main results we present here are derived on a 19  X  19 board using a player trained using only 9  X  9 game data. our systems at a given board size and use it to play at differen t sizes, both larger and smaller. Pure bootstrap approaches to Go where computer players are initi alized randomly and play large numbers of games, such as evolutionary approaches or reinforcement learning, have been tried [11]. We have implemented these approaches and used them for small board s izes 5  X  5 and 7  X  7 . However, in our experience, these approaches do not scale up well to larg er board sizes. For larger board sizes, better results are obtained using training data derived fro m records of games played by humans. We used available data at board sizes 9  X  9 , 13  X  13 , and 19  X  19 .
 Data for 9  X  9 Boards: This data consists of 3,495 games. We randomly selected 3,16 6 games (90.6%) for training, and the remaining 328 games (9.4%) for validation. Most of the games in this data set are played by amateurs. A subset of 424 games (12.13% ) have at least one player with an olf ranking of 29, corresponding to a very good amateur playe r.
 Data for 13  X  13 Boards: This data consists of 4175 games. Most of the games, however, are played by rather weak players and therefore cannot be used fo r training. For validation purposes, however, we retained a subset of 91 games where both players h ave an olf ranking greater or equal to 25 X  X he equivalent of a good amateur player.
 Data for 19  X  19 Boards: This high-quality data set consists of 1835 games played by p rofessional players (at least 1 dan). A subset of 1131 games (61.6%) are pl ayed by 9 dan players (the highest possible ranking). This is the dataset used in [12]. 3.1 Evaluation Function, Outputs, and Targets Because Go is a game about territory, it is sensible to have  X  X  xpected territory X  be the evaluation function, and to decompose this expectation as a sum of local probabilities. More specifically, let A ij ( t ) as 0.5. The same scheme with 0.5 for empty intersections, or m ore complicated schemes, can be used to represent ownership at various intermediate stages of the game. Let O the learning system at intersection ij at time t in the game. Likewise, let T training target. In the most simple case, we can use T the game. In this case, the output O time t , of owning the ij intersection at the end of the game. Likewise, P computed at time t , of the total expected area at the end of the game.
 Propagation of information provided by targets/rewards co mputed at the end of the game only, how-ever, can be problematic. With a dataset of training example s, this problem can be addressed because intermediary area values A we use a simple scheme w  X  0 is a parameter that controls the convex combination between the area at the end of the game and the area at some step t + k in the more near future. w = 0 corresponds to the simple case described above where only the area at the end of the game is us ed in the target function. Other ways of incorporating target information from intermediar y game positions are discussed briefly at the end.
 To learn the evaluation function and the targets, we propose to use a graphical model (Bayesian architecture. 3.2 DAG-RNN Architectures The architecture is closely related to an architecture orig inally proposed for a problem in a com-pletely different area  X  the prediction of protein contact m aps [8, 1]. As a Bayesian network, the architecture can be described in terms of the DAG in Figure 1 w here the nodes are arranged in 6 lat-tice planes reflecting the Go board spatial organization. Ea ch plane contains N  X  N nodes arranged on the vertices of a square lattice. In addition to the input a nd output planes, there are four hidden planes for the lateral propagation and integration of infor mation across the Go board. Within each hidden plane, the edges of the quadratic lattice are oriente d towards one of the four cardinal direc-tions (NE, NW, SE, and SW). Directed edges within a column of th is architecture are given in Figure 1b. Thus each intersection ij in a N  X  N board is associated with six units. These units consist of an input unit I In a DAG-RNN the relationships between the variables are det erministic, rather than probabilistic, and implemented in terms of neural networks with weight shar ing. Thus the previous architecture, leads to a DAG-RNN architecture consisting of 5 neural netwo rks in the form where, for instance, N addition, since Go is  X  X sotropic X  we use a single network sha red across the four hidden planes. Go however involves strong boundaries effects and therefore w e add one neural network N corners, shared across all four corners, and one neural netw ork N across all four sides. In short, the entire Go DAG-RNN archit ecture is described by four feedforward of these feedforward neural networks, we have experimented with several architectures, but we typically use a single hidden layer. The DAG-RNN in the main s imulation results uses 16 hidden nodes and 8 output nodes for the lateral propagation network s, and 16 hidden nodes and one output node for the output network. All transfer functions are logi stic. The total number of free parameters is close to 6000.
 Because the underlying graph is acyclic, these networks can be unfolded in space and training can proceed by simple gradient descent (back-propagation) tak ing into account relevant symmetries and weight sharing. Networks trained at one board size can be reu sed at any other board size, providing a simple mechanism for reusing and extending acquired knowl edge. For a board of size N  X  N , the training procedure scales like O ( W M N 4 ) where W is the number of adjustable weights, and each position, N 2 outputs O the positions within each selected game record are randomly selected during training. Weights are updated essentially on line, once every 10 game positions. T raining a single player on our 9  X  9 data takes on the order of a week on a current desktop computer, cor responding roughly to 50 training epochs at 3 hours per epoch.
 Figure 1: (a) The nodes of a DAG-RNN are regularly arranged in one input plane, one output plane, and four hidden planes. In each plane, nodes are arranged on a square lattice. The hidden planes contain directed edges associated with the square lattices . All the edges of the square lattice in each hidden plane are oriented in the direction of one of the four p ossible cardinal corners: NE, NW, SW, and SE. Additional directed edges run vertically in colu mn from the input plane to each hidden plane and from each hidden plane to the output plane. (b) Conn ection details within one column of Figure 1a. The input node is connected to four corresponding hidden nodes, one for each hidden plane. The input node and the hidden nodes are connected to th e output node. I inputs at intersection ij . O lattice neighbors within the same plane are also shown. 3.3 Inputs At a given board intersection, the input vector I first three components X  X tone type, influence, and propensit y X  X re associated with the corresponding intersection and a fixed number of surrounding locations. Influence and propensity a re described below in more detail. The remaining features correspond to group properties involving variable numbers of neighboring stones and are self explanatory for t hose who are familiar with Go. The group G connected to it. Neighboring (or connected) opponent group s of G that are directly connected (adjacent) to G [13]. O the second weakest neighboring opponent groups. Weakness h ere is defined in alphabetical order with respect to the number of eyes first, followed by the numbe r of liberties.
 are properties associated with the corresponding intersec tion and a fixed number of surrounding stones.

Feature Description b,w,e the stone type: black, white or empty influence the influence from the stones of the same color and th e opposing color propensity a local statistics computed from 3  X  3 patterns in the training data (section 3.3) N eye the number of true eyes
N 1 st the number of liberties, which is the number of empty interse ctions connected
N 2 nd the number of 2nd-order liberties, which is defined as the lib erties of the 1st-
N 3 rd the number of 3rd-order liberties, which is defined as the lib erties of the 2nd-
N 4 th the number of 4th-order liberties, which is defined as the lib erties of the 3rd-
O 1 st features of the weakest connected opponent group (stone typ e, number of liber-
O 2 nd features of the second weakest connected opponent group (st one type, number Influence: We use two types of influence calculation. Both algorithms ar e based on Chen X  X  method [4]. One is an exact implementation of Chen X  X  method. The oth er uses a stringent influence prop-agation rule. In Chen X  X  exact method, any opponent stone can block the propagation of influence. With a stringent influence propagation rule, an opponent sto ne can block the propagation of influ-ence if and only if it is stronger than the stone emitting the i nfluence. Strength is again defined in alphabetical order with respect to the number of eyes first, f ollowed by the number of liberties. Propensity X  X utomated Learning and Scoring of a Pattern Lib rary: We develop a method to learn local patterns and their value automatically from a da tabase of games. The basic method is illustrated in the case of 3  X  3 patterns, which are used in the simulations. Considering ro tation and mirror symmetries, there are 10 unique locations for a 3  X  3 window on a 9  X  9 board (see also [9]). Given any 3  X  3 pattern of stones on the board and a set of games, we then compu te nine numbers, one for each intersection. These numbers are local indicators of strength or propensity. The propensity S is defined as: where N B the end of the games in the data, and N W N W ij ( p ) are computed taking into account the location and the symmet ries of the corresponding window w . C plays a regularizing role in the case of rare patterns and is s et to 1 in the simulations. Thus S w quering the corresponding intersection in the local contex t provided by the corresponding pattern and window.
 In general, a given intersection ij on the board is covered by several 3  X  3 windows. Thus, for a given intersection ij on a given board, we can compute a value S w that contains the intersection. In the following simulatio ns, a single final value S by averaging over the different w  X  X . However, more complex schemes that retain more informat ion can easily be envisioned by, for instance: (1) computing als o the standard deviation of the S w a function of w ; (2) using a weighted average, weighted by the importance of the window w ; and (3) using the entire set of S w 3.4 Move Selection and Search For a given position, the next move can be selected using one-level search by considering all possible legal moves and computing the estimate at time t of the total expected area E = P end of the game, or some intermediate position, or a combinat ion of both, where O outputs (predicted probabilities) of the DAG-RNNs. The nex t move can be chosen by maximizing this evaluation function (1-ply search). Alternatively, G ibbs sampling can be used to choose the next move among all the legal moves with a probability propor tional to e E/T emp , where T emp is a temperature parameter [3, 11, 12]. We have also experiment ed with a few other simple search schemes, such as 2-ply search (MinMax). We trained a large number of players using the methods descri bed above. In the absence of training data, we used pure bootstrap approaches (e.g. reinforcemen t learning) at sizes 5  X  5 and 7  X  7 with results that were encouraging but clearly insufficient. Not surprisingly, when used to play at larger board sizes, the RNNs trained at these small board sizes yiel d rather weak players. The quality of most 13  X  13 games available to us is too poor for proper training, althou gh a small subset can be used for validation purposes. We do not have any data for size s N = 11 , 15 , and 17 . And because of the O ( N 4 ) scaling, training systems directly at 19  X  19 takes many months and is currently in progress. Thus the most interesting results we report are de rived by training the RNNs using the 9  X  9 game data, and using them to play at 9  X  9 and, more importantly, at larger board sizes. Several 9  X  9 players achieve top comparable performance. For concisene ss, here we report the results obtained with one of them, trained with target parameters w = 0 . 25 and k = 2 in Equation 1, Figure 2: (a) Validation error vs. game phase. Phase is define d by the total number of stones on the board. The four curves respectively represent the valid ation errors of the neural network after 1, 2, 33, and 38 epochs of training. (b) Percentage of moves made by professional human players on boards of size 19  X  19 that are contained in the m top-ranked moves according to the DAG-RNN trained on 9  X  9 amateur data, for various values of m . The baseline associated with the red curve corresponds to a random uniform player.
 Figure 2a shows how the validation error changes as training progresses. Validation error here is defined as the relative entropy between the output probabili ties produced by the RNN and the target probabilities, computed on the validation data. The valida tion error decreases quickly during the also how the error is smaller towards the end of the game due bo th to the reduction in the number of possible moves and the strong end-of-game training signal.
 An area and hence a probability can be assigned by the DAG-RNN to each move, and used to rank them, as described in section 3.4. Thus we can compute th e average probability of moves played by good human players according to the DAG-RNN or othe r probabilistic systems such as [12]. In Table 2, we report such probabilities for several sy stems and at different board sizes. For size 19  X  19 , we use the same test set used in [12]. Boltzmann5 and Boltzma nnLiberties are their results reported in the pre-published version of their NIPS paper. At this size, the probabilities in Table 2: Probabilities assigned by different systems to mov es played by human players in test data. the table are computed using the 80-83rd moves of each game. F or boards of size 19  X  19 , a random player that selects moves uniformly at random among legal mo ves assigns a probability of 1/281 to the moves played by professional players in the data set. Bol tzmannLiberties was able to improve this probability to 1/194. Our best DAG-RNNs trained using a mateur data at 9  X  9 are capable of bringing this probability further down to 1/15 (also a consi derable improvement over our previous 1/42 performance presented in April 2006 at the Snowbird Lea rning Conference). A remarkable example where the top ranked move according to the DAG-RNN co incides with the move actually played in a game between two very highly-ranked players is gi ven in Figure 3, illustrating also the underlying probabilistic territory calculations.
 Figure 3: Example of an outstanding move based on territory p redictions made by the DAG-RNN. For each intersection, the height of the green bar represent s the estimated probability that the inter-section will be owned by black at the end of the game. The figure on the left shows the predicted probabilities if black passes. The figure on the right shows t he predicted probabilities if black makes the move at N12. N12 causes the greatest increase in green are a and is top-ranked move for the DAG-RNN. Indeed this is the move selected in the game played b y Zhou, Heyang (black, 8 dan) and Chang, Hao (white, 9 dan) on 10/22/2000.
 fessional human player on boards of size 19  X  19 that are contained in the m top-ranked moves according to the DAG-RNN trained on 9  X  9 amateur data, for various values of m across all phases of the game. For instance, when there are 80 stones on the boar d, and hence on the order of 300 legal moves available, there is a 50% chance that a move selec ted by a very highly ranked human player (dan 9) is found among the top 30 choices produced by th e DAG-RNN. We have designed a DAG-RNN for the game of Go and demonstrated that it can learn territory predictions fairly well. Systems trained using only a set of 9  X  9 amateur games achieve surpris-ingly good performance on a 19  X  19 test set that contains 1835 professional played games. The methods and results presented clearly point also to several possible direction of improvement that are currently under active investigation. These include: ( 1) obtaining larger data sets and training systems of size greater than 9  X  9 ; (2) exploiting patterns that are larger than 3  X  3 , especially at the beginning of the game when the board is sparsely occupied and matching of large patterns is possible using, for instance, Zobrist hashing techniques [ 14]; (3) combining different players, such developing better, non-exhaustive but deeper, search meth ods.
 Acknowledgments The work of PB and LW has been supported by a Laurel Wilkening F aculty Innovation award and awards from NSF, BREP, and Sun Microsystems to PB. We would li ke to thank Jianlin Chen for developing a web-based Go graphical user interface, Nicol S chraudolph for providing the 9  X  9 and 13  X  13 data, and David Stern for providing the 19  X  19 data.
 [2] E. Berlekamp and D. Wolfe. Mathematical Go X  X hilling gets the last point . A K Peters, [3] B. Brugmann. Monte Carlo Go. 1993. URL: ftp://www.joy.ne.jp/welcome/igs/ [4] Zhixing Chen. Semi-empirical quantitative theory of Go part 1: Estimation of the influence of [5] W. S. Cobb. The Book of GO . Sterling Publishing Co., New York, NY, 2002. [6] K. Iwamoto. GO for Beginners . Pantheon Books, New York, NY, 1972. [7] Aske Plaat, Jonathan Schaeffer, Wim Pijls, and Arie de Br uin. Exploiting graph properties of [9] Liva Ralaivola, Lin Wu, and Pierre Balid. SVM and Pattern -Enriched Common Fate Graphs [10] Stuart J. Russell and Peter Norvig. Artificial Intelligence: A Modern Approach . Prentice Hall, [11] N. N. Schrauldolph, P. Dayan, and T. J. Sejnowski. Tempo ral difference learning of position [12] David H. Stern, Thore Graepel, and David J. C. MacKay. Mo delling uncertainty in the game [13] E. Werf, H. Herik, and J. Uiterwijk. Learning to score fin al positions in the game of Go. In [14] Albert L. Zobrist. A new hashing method with applicatio n for game playing. 1970. Technical
