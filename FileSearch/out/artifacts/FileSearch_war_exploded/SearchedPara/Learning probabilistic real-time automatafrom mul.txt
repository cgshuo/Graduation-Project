 Institut f X r Informatik  X  I12, TU M X nchen, M X nchen, Germany 1. Introduction
Recent years have seen a surge of interest in temporal data in all areas of science and industry. In the fi eld of molecular biology and medicine, for instance, data sets of time-labeled observations may pro-possible way of representing complex temporal phenomena is by timed automata [5], which are fi nite state models explicitly modeling time. They can be linked to domain concepts and help to reason about real-time processes. Until now, experts construct such models by hand, which can be time-consuming of an automatic extraction of meaningful, expressive and temporally ordered states, we propose a new algorithm based on fi nding real-time automata. Observations consist of a multi-dimensional attribute vector and a corresponding time point, denoting when the state was observed. The implicit modeling of time, e.g., by an untimed model like Hidden Markov Models (HMMs) would result in a combinatorial easier to understand.
 (Section 3) is presented and subsequently the work fl ow and how the state merging is conducted will be discussed. Section 4 shows the results and gives an interpretation of inferred automata for medical and biological data. Moreover, we present the results of a comparison with Multi-Output HMMs and Petri nets learned by standard process mining algorithms. The paper closes with an overall conclusion. 2. Related work
Today, temporal data become available in man y domains, where only little is known about the pro-cesses generating the data. Example domains include the internet (click paths), medicine (disease pro-gressions) and biology (life cycles of organisms, biochemical pathways). In this paper, a process is assumed, described by states (or events) that are labeled with a time stamp and a multi-attribute vector holding M variables. The problem is to identify a model that fully represents the data and additionally A potential solution of the problem comes from the fi eld of graphical models. They give a compact representation of the processes and are easy to understand and interpret by researchers. To show dif-ferent approaches to the problem setting, HMMs, Causal Nets, Dynamic Bayesian Networks and Petri nets with their bene fi ts and shortcomings are presented. Subsequently, the approach of using automata the bene fi ts of all presented models can be combined into an automaton based one is given. 2.1. Hidden Markov Models (HMMs)
HMMs [21] are a special case of graphical models. Algorithms for graphical models were extended to fi rst order HMMs (e.g., for the MAP and inference problem). First order HMMs consist of hidden state Hidden variables are connected by directed edges. Each hidden variable has exactly one predecessor and an associated observable variable, which is only dependent on the precedent variables (Markov prop-B an HMMs ( A, B,  X  ) is achieved by maximum likelihood estimation using the Forward-Backward algo-rithm [7]. Given the observation sequence S and the model parameters  X  , P ( S |  X  ) is to be maximized. The model learning task is another problem to be solved in the domain of HMMs. There exist four further tasks: Evaluating, predicting, smoothing and decoding an HMM. Evaluation identi fi es the proba-calculated with the Forward algorithm. The second task, prediction, can also be solved by the Forward decoding, calculates the most probable hidden state sequence that produces a given emission sequence S at time point t . The Viterbi algorithm was devised for this task.

In order to model time-labeled states of a process, where the states are described by a multi-dimensional attribute vector, a fully connected ergodic HMM could be used. Due to the fact that standard HMMs model only one-dimensional variables, there are two possibilities of what a state emits in this that are fully connected. Each state models one subset of variables. The resulting emission matrix B is easy to determine: only one entry of the matrix is set to one, because each state emits exactly one of expensive because there are | O | 2 connections within the model. Therefore, A will be large and there is no guarantee that each connection is represented in the data. The second HMM design for the given problem is to use only a certain number of states which may emit all events. Here, all parameters have follows inadequate hypotheses, the modeling will never be appropriate. However, in both cases the com-portional to the number of possible combinations of each variable dimension. Even in the easiest case, where each event is described by d binary variables (so each event is of dimensionality d ), the number makes multidimensional problems very hard for standard HMMs. To handle the dimensionality prob-lem, DT-HMMs [19] were proposed. However, they are still not capable of modeling variables with more than two dimensions. So, other approaches to the two main drawbacks (multidimensionality and struc-ture learning) of HMMs were presented. To handle multi-dimensi onal input and output, Multi-Output HMMs [8] were proposed. But to train them, a hand-made structure has to be given a priori as well. For every problem, a hand-tweaked model is assumed, wh ich strongly depends on the ideas and intuitions of an expert. Nevertheless, there are tools like MoCaPy, which can be used to model Multi-Output HMMs and to fi t models to given observation sequences. In Section 4.5 a comparison to Multi-Output HMMs is presented. To learn the structure of HMMs, algorithms have been studied extensively, but rely on 2.2. Causal networks (CNs)
Causal networks (CNs) are another way to solve the problem of structure learning [20]. They were applied to learn structures de fi ned by conditional independencies. Unfortunately, the FD2CN algo-rithm [31] can again only handle one-dimensional events. It fi rst extracts conditional independencies and then combines them via the chain and Markov boundary rule to infer a CN. Although this approach events. 2.3. Dynamic Bayesian Nets (DBNs)
Dynamic Bayesian Nets (DBNs) are a generalization of HMMs that use recursive HMMs to model time series. Here, several HMMs, possibly having the same structure, are combined via cross connec-tions, so that each HMM models one time step [10] of a multivariate timed sequence. Such networks consist of output nodes Y states H the case for HMMs). Thus, the output variable Y that one may have to know upfront how many timesteps to include in the model. If, e.g., the model only for this step. Moreover, the conditional dependency tables for such models may become very large, if Y training data, depending on the hidden states X  H Even for binary multi-dimensional variables, this may become intractable. Nevertheless, algorithms that learn the structure of a DBN were proposed, like Bayesian model averaging [13] that estimates whether features (e.g., an edge in a graph) exist. Moreover, standard feature selection algorithms, such as for-ward or backwards stepwise selection for the identi fi cation of edges in such a model, or the leaps and bounds algorithm [11] can be applied, if the system is fully observable. As Multi-Output HMMs can be considered as DBNs, they are also addressed in the experimental part (cf. Section 4.5). 2.4. Petri nets
One additional fact in the problem setting presented is that most systems are described by discrete dresses the problem of identifying models from event logs. These models can later be used to explain and transfer the acquired knowledge. In general, Petri nets will only present the structural dependency, used to learn log-based model structures. A Petri net is a pair ( N,s ) ,where N isatuple( P,T,F )of places P , transitions T ( P  X  T =  X  ) and directed edges F  X  P  X  T  X  T  X  P ( fl ow relation). The parameter s denotes the marking of the net. A marking is a function f : P  X  N that shows how many an overall model. By improving the original algorithms, the problem of short loops and implicit depen-dencies could be incorporated [18]. However, for more complex logs, e.g., numeric logs, the structure has to be fi xed such that the parameters of the net can be estimated [16,30]. Although Petri nets were successfully applied to model business processes and biological pathways, they are not applicable for the given problem setting. First, they are usually based on events that consist of one variable only and are therefore unable to model multi-dimensional e vents, and second, model selection and capacity con-trol remain essential open proplems. While one straightforward way to handle multi-dimensional events constraints like, e.g., liveliness, was not elaborated yet. 2.5. Requirements for the desired algorithm
Up to now, all the presented models can deal with one or more of the desired constraints:  X  produce a probabilistic generative graphical model  X  handle multi-dimensional variables  X  consider time stamps  X  allow for cycles  X  generalize similar events but all of them have limitations that are not easy to solve. However, an approach to combine selected properties of each graphical model was presented recently. 2.6. Automata
Automata [24,26] are learned to model a given event log. The main advantage is that the structure of the model can be learned automatically, thus it does not need to be speci fi ed manually. Addition-ally, time information is incorporated into the automaton to model time constraints on transitions: a deterministic real-time automaton (DRTA) [9] is created. Automata model event sequences  X  ,where  X  =( e 1 ,t 1 )( e 2 ,t 2 ) ... ( e time that has elapsed since the last event. A DRTA is de fi ned like a DFA, but additionally has a delay deterministic because ther e is exactly one transition q,q ,a, X   X  T for a state q , every symbol a and every time value t  X  N .

To identify the automaton that is the smallest one consistent with the input, a timed pre fi xtreeis created (cf. Section 3). Then the state merging method, which is currently considered the best solution states of the pre fi x tree are merged, which means that two states are combined into a single one. The merging procedure is mainly dependent on the fi nal structure of the automaton. Automata not including identi fi ed and then k  X  h +1 states of the paths are merged. However, for automata with delay guards, the deterministic constraint can be violated after a merge: There may be two transitions with the same symbol and delay guard. This mismatch is resolved by a splitting procedure that creates two paths in the that all input data are expected to be part of the language that is formed by the automaton and thus (merge or split). This operation is based on the red blue algorithm [27].

Comparing automata to HMMs,  X  (a tuple that describes the parameters of the HMM) of the automaton transition probabilities depending on multi-dimensional symbols and time: b and to compute the probability P ( S |  X  ) for each observation sequence S depending on the model param-equivalent in the world of automata. The same is true for smoothing and decoding. In general, it is not important to know in which state an automaton resides, but if the automaton accepts the sequence S . This shows whether a sequence S is part of the language that is modeled by the given automaton. More-conducted as in the domain of HMMs. However, as stated above, answering these questions is not the main task of automata. It is only of interest whether or not a sequence S is accepted by an automaton. 2.7. Combining the best of the presented approaches
All existing automaton based algorithms are made for one dimensional events only. Nevertheless, only few modi fi cations are required to deal with multi-attribute events. To do so, an approach to combine probabilities with automata was presented [25]. It models one-dimensional symbols and time by using interdependent probability distributions, transition intervals must be given in advance which leads to a model with many parameters. In contrast, we solve the problem of multi-dimensional attributes by during the merge operation. In this way, selected properties of HMMs, causal networks and automata are combined, to obtain the desired functionality. De fi nition and inference of such an automaton will be presented in the next section. Finally, for all presented solutions, two drawbacks remain. Noise and underrepresented events can affect the discovery process and lead to wrong results. 3. Probabilistic real-time automata (PRTA)
In this section, we present an algorithm for learning probabilistic real-time automata (PRTAs) which is, like the currently best method for learning automata [12], based on state merging in a pre-fi x tree. Our type of automaton models a discrete event system (DES) [26]. Let dataset D of in-stances I be given D = { I 1 ,...,I I =( e 1 ,t 1 )( e 2 ,t 2 ) ... ( e tory .Anevent e ( a created. Each event ( e t . 3 Let | e represents the time that has elapsed since the previous event of the instance has occurred. A PRTA is a directed graph with states Q and transitions T . Each state q simplicity of notation  X  annotated by a so-called pro fi le f vector of all events that are mapped to q In other words, a pro fi le is just a summary of these events. Transitions t states q minimal (maximal) number of time steps when this transition can be passed. Additionally, transitions t q . These changes are expressed in the so-called delta notation : T and  X  ( e the label is the set of differences between the elements of E of change vectors that are necessary to reach q and states incorporate the events and the correspondi ng differences. To complete the transition, it has assigned a probability p in the automaton ( S  X  Q  X  F  X  Q ). In a PRTA, S = Q and F = Q , because each state is allowed to be astartor fi nal state. A PRTA is then formally de fi ned as follows: De fi nition 1 A PRTA  X  is a tuple  X =( Q, ,T,S,F ) ,where  X  Qisa fi nite set of states  X   X  is a fi nite set of events to label the transitions  X  Tisa fi nite set of transitions  X  S = Q is the set of start states  X  F = Q is the set of fi nal states A state q tuple q,q ,T occurs. 3.1. Accepting words
One task of automata is to decide whether they accept a given word of a language. This section will describe how this problem can be solved for a PRTA using the  X  -notation. 3.1.1. De fi nition of words
Let  X = { e 1 ,..., e of the alphabet  X  and time points of N ,and ( e In such languages, a time point t example, consider language Further, let the  X  -notation for a word w =( e Again, it shows the difference from one event to the next, and the time that has elapsed. The problem of deciding whether an automaton  X  accepts a word w (if w is part of the language L modeled by  X  ) is transformed into a check if there exists a valid sequence G = q 0 T 1 T 2 ...T which comprises the word w in  X  -notation. succeeding transiti ons share states ( are adjacent): Source ( T i ) ( Target ( T i ) ) names the source (target) state of a transition T i . 3.1.2. Solving the word problem
The language of a PRTA is given by: L threshold. P the probability of a transition from q In the case of a PRTA, Q = Q Algorithm 1 ParseWord (PRTA  X  , ArrayList w) Algorithm 2 ParseRemainingWord (State q , ArrayList w) e is found, the automat on accepts the word and returns the proba bility of the word. 3.2. Induction of a PRTA
In the following, we describe how PRTAs can be learned. The top-level algorithm is shown in Algo-Algorithm 3 InducePRTA (Histories H , Parameter params ) history is put in the PTA, there are the following possibilities: 1. No pre fi x of the history is represented by an existing path in the PTA. 2. There is a path that represents a pre fi x of the history. 3. There is a path that represents the whole history.
 dated corresponding to the annotated proba bilities. Consid er a transition from state q path of the history and assume that q by the new history, th e according p robability p is updated to p = | q j | +1 | of a state. This ratio actually is the maximum likelihood estimation b duced in Section 2. For all remaining outgoing transitions t is recomputed by p of the existing transition, the delay guard  X   X  to as determinization). After creating the PTA with all input histories, the goal is to produce a PRTA that is minimal. Minimal means that a minimum number of states should be derived, but re fl ecting a maximum of information. This condition is heuristically motivated by Occam s Razor . The parameter that leads to the minimal model is usually given by the user, in our case, a certain distance threshold between mergeable states (the distance measure is discussed in Section 4.1).
 To obtain a compact model, merges of nodes in the pre fi x tree are performed. A merge is an operation where two states q cluster into one new state with a new pro fi le. A merge combines all pro fi les f merged into one single pro fi le f Which states are to be merged is identi fi ed via clustering. Therefore, a cluster assignment for each (or a distance function and the instances respectively). However, when constructing an automaton, the as a function c (cf. Eq. (7)) that maps each state q to a cluster identi fi er k  X  N . Then it is possible to evaluate each possible clustering c is the mapping c ( q )  X  that maximizes the quality function. Note that depending on the application domain, the user can decide which distance function, clustering creates for each cluster identi fi er k one new state in the pre fi xtreebymergingallstates q which are set of states that are mapped to it. The automaton is created by merging all states q of clusters k one after the other. To preserve consistency, update ope rations on transitions have to be performed. If two states q t = q to q deleted from the pre fi x tree. If there exist two transitions t share the same start but not end state), the transitions have to be merged additionally. This means that q and b = max (  X  merge procedure. Then, each state holds its set of events and the labels T calculating the difference between the two sets E Algorithm 4 CreateTransitionLabels ( q One additional property of PRTAs is that a label  X  transition of a state q cluster distance, respectively, it can be shown that  X  and q 2 = { y 2 } ,f 2 , with the transitions t 1 = q 0 ,q 1 , X  transitions, each holding label  X  us further assume, without loss of generality, that  X   X  exists on both transitions, we can specify constraints for x 2 and y 2 .First, x 1 for all j&lt;k , because there is onl y one possi bility for y setting. Second, y 2 least one x 1 so t 2 cannot exist. With these constraints: (1) x 1 in this clustering cannot be optimal. This leads to the conclusion that for all suitable distance based clusterings there is no  X  conclusion even allows to check each learned automaton whether the intra cluster distances are above a threshold to ensure that each  X  The delay guard generalization is motivated by the use of positive instances only. Remember that all input instances shall be accepted by the automaton, indicating that they are positive. Furthermore, we assume that when no continuous timeframe is present in the data, it is not because it does not exist, but because of lack of data. 8 However, the pro fi les of the states q their weighted mean. Transition t transitions of states q computed. Like in every clustering problem, the proportion of inter-and intra-cluster distances is of interest. The silhouette coef fi cient (SC) [14] evaluates this proportion independent of the number of q representing a cluster C is calculated in the following way: a ( center. The SC for the automaton is computed by averaging the SC for each state. It always holds that  X  1 SC 1 . Good results are expected above an SC of 0.5. However, a high SC does not necessarily re fl ect the best clustering, since SC instances. Generally, SC of one example only.
 The clustering method can be different for each use case. We decided to cluster with a divisive hierar-the clustering. Furthermore, correlations between states can be investigated, and upper and lower dis-to tell a priori how many states are to be expected. This is why k-means like clustering methods are not appropriate in our scenario. Diana [14] is a divisive hierarchical clustering algorithm which computes a constraint, the user can create a suitable clustering. The dendrogram is cut according to the distance in the section on experimental results.

In Fig. 2, a merge step during the construction of a PRTA is illustrated. We see here that the states merging step (without the root). The transitions an d delay guards are updated following the rules. The because there exists n o splitting t ransition. 3.3. Predicting with an automaton
In this section, we explain how such an automaton can be used to make predictions. With a PRTA, we cannot only map processes that are re fl ected in the data but also make predictions about how the task of the prediction for a new instance can be formalized as follows: Given an instance x denoted by its feature vector f that is most similar to the given event distribution of the instance. This is the start state q prediction: denote the states with incoming transitions from q ,and p 1 ,...,p tions. Moreover, let [ t 1 pro fi le given l time steps starting with state q is de fi ned as: summand represents the case where state q is not left, because it consumes all the  X  X emaining X  time. Thus, no other following state is considered for the prediction. The second summand represents the case used. If q does not have any outgoing transition, then f  X  ( q,d )= f obtain a prediction for the test instance, we apply f  X  to q to leave a state, the delay guard has to meet the time constraint l : l&gt; [ t time l for the next steps is reduced by the maximum of the delay guard l = l  X  t assumption would be that a transition is made as early as possible. Again, one de fi nite prediction is achieved. However, an arbitrary time consumption of each state could be desired as well. In this case, the prediction would be dependent on all (valid) possible time consumptions of each state. Accordingly, comparable algorithms only give one prediction. Consider a system that has alternatives for each state (e.g., medical therapy options and outcomes or biochemical pathways), meaning that a certain percentage output. The automaton aims at modeling such alte rnatives and their co rresponding pr obabilities. 4. Experimental results is conducted and subsequently an application to real-world medical data is presented to investigate the a comparison to Multi-Output HMMs is presented. To adapt the approach to a given application, we now such a measure for our medical application. 4.1. Distance measure for medical applications
In general, every distance measure can be applied to the learning algorithm but they should be adjusted presence of a disease is more important than its absence, because the development of diseases shall be Most people of a population are healthy (have no ch ronic conditions). This is why the presence of diseases should receive more weight than the absence of diseases. To design a distance measure with this property, assume two events e that are equal to one in e | e  X  e As | e by the Tanimoto coef fi cient. Nevertheless, if the proportion of the intersection of e the intersection of two other states e than the compared e to introduce an additional term that captures the size of | e compares unequal attributes and adjusts a lower similarity in this case. A distance function that meets all the discussed requirements can be expressed as a combination of the two distances Tanimoto and Hamming. Tanimoto captures the relative distance (magnitude of intersection) and Hamming re fl ects Tanimoto and Hamming on the fi nal distance can be controlled. The resulting distance function is: If  X  is one, we obtain a Tanimoto distance, if it is zero, we obtain the Hamming distance. We chose  X  to be equal to 0.75, so we will obtain a high in fl uence of Tanimoto Eq. (12) and a rather small one of Hamming Eq. (13): In Eqs (12) and (13), n 11 is the number of attributes set in both events, n y but not in x ). The resulting distances lie between zero and one. Another adaptation to the domain of afterwards. Otherwise, this indicates an error in the data. 4.2. Data In this section we brie fl y introduce the three datasets we used for the evaluation of the algorithm. the descriptive power. The real-world datasets encompass medical and biological data. 4.2.1. Synthetic data
To show that the algorithm rediscovers an automaton that is known, histories were produced by a synthetic automaton. To do so, we created an automaton with 10 states and 10 attributes. This automaton see if the automaton will be detected correctly although the states are not completely equal. With a brackets). Delay guards are always equal to one and can therefore be neglected in this case. 4.2.2. Real-world data
Diagnostic data: The fi rst real-world, proprietary dataset comprises diagnosis data from 1000 persons from four years. Each diagnosis was grouped into one of 106 medically motivated disease groups (DGs) 5% of the instances, 66% of all events just have one or two DGs set, and 99% of the events incorporate less than 11 DGs. Thus, only DGs that occur often (in more than 5% of the instances) were used to time point with an event e where | e | &gt; 0 were incorporated into the histories.

Yeast metabolism data: The second real-world data set holds the gene expression values of budding yeast [22] (GEO ID: GSE3431) that were recorded using Affymetrix chips (GPL90) for 36 time points with a delay of 25 minutes each. The data set covers the expression rates of 9335 genes of a synchro-metabolism, genes are expressed (used) at different time steps, revealing expression peaks in the data. These peaks give information about the current state of the cell, e.g., which metabolism is currently vectors (gene is off/on). Therefore, the expression pro fi les were discretized via a sliding window ap-proach. A peak is considered as a raise of the expression level compared to the surrounding time points. Following this intuition, a gene X  X  expression level was set to one (peak) at a time point t level L ( t behavior, and they expose peaks in a maximal 12 time steps frame. Thus the window size was set to 12, meaning that the average was taken of the 12 time points before and after t tion methods (as discretization by mean and standard deviation) may not be as expressive because some should clearly change over the monitored time points. Furthermore, each time step should be described different time steps. Because most genes are expressed at the same time steps, the selection only covers the lower part of Fig. 8. 4.3. Results
This section focuses on the results of the algorithm with Diana clustering on synthetic and real-world world data sets give results of how meaningful the automaton is from the application point of view, e.g. 4.3.1. Proof of concept
As described in Section 3 the induction of a PRTA is based on clustering. It therefore needs a distance dendrogram that is returned by Diana clustering. As the correct structure of the automaton is known in this experimental setting, a lower and upper limit for the cut-off can be set. A cut-off of 0.3 would Eq. (11)) is 0.3. To follow the requirement that they must not be merged, the cut-off must be lower than between 0.21 and 0.3. When exploring the resulting number of clusters and the SC for the clustering in to state 7 that is not present in the original automaton. This is due to the data. Remember that random SC shows that the states are homogeneous.

The annotation of the states in the PRTA shows that events  X  X ith errors X  were actually included in the that the algorithm works correctly because it has to group similar events. This makes the automaton less prone to data errors and produces stable results. Moreover, the user can identify the main properties of occur commonly, while exceptions are all attributes that have a low frequency in a pro fi le. Exceptions abilities on the transitions match those of the origin al automaton quite well. We encounter at most a an automaton.

To quantify the stability of the algorithm (not illust rated), we repeated this experiment 1000 times on different synthetic datasets that were obtained from the prede fi ned automaton. The correct number of states of the automaton was induced in 31.8% of the cases, while in 50% of all runs the automaton X  X  number of states was between 10 and 12. Altogether, the number of states of the fi nal automaton varied between 9 and 14 states. The Euclidean distance of the induced states to their closest original lied be-algorithm fi nds automata that are correct or very close to the original one. 4.3.2. Extraction of an automaton on real world data
Results on diagnostic data: To test our algorithm on real-world data, an automaton was built on 1000 and lower bound for the clustering cut-off. Here they were based on medical assumptions. States with | e for this constraint is that individuals with one disease shall be separated from those that have already acquired two or more diseases and are thus multimorbid. This distinction can show the beginning and the resulting states are not known in advance. Thus, meaningful automata are expected with a cut-off 0 . 4 . We computed the automaton for several cut-offs and extracted its corresponding silhouette coef fi cient drop of the SC of about approximately 18% and a drop of the number of clusters (20%). Therefore, one can assume that a meaningful cut-off threshold is reached and thus more states are merged. The same behavior can be observed when we apply the algorithm to only 100 instances. Therefore the cut-off 0.38 was chosen to create the PRTA. The resulting automaton contains 518 states and has an SC of 0.83. called hubs, cf. Section 4.4.1). They are mostly states with only one or two DGs (e.g., hypertension and why those states are connected to many others. Besides hubs, loops and paths are created. A loop is a path in the automaton where start and end states are the same. The inspected loops resemble individuals We conclude that the automaton shows meaningful medical patterns.

A second quality parameter is the predictive power of the automaton. To evaluate the predictive power as part of an industrial case study. For each of the k histories with n events of the test sample, n  X  1 to the true joint pro fi le of these instances by computing the quadratic loss. For the whole automaton, the average loss is computed by the weighted average of the quadratic loss of all states. Each loss is on the log function which is not de fi ned for zero.

We compared the results to the well-established method of logistic regression, which is considered a standard in this application domain. Here, the prediction of events is achieved by applying one logistic regression for each attribute (DG), which results in 28 logistic regression models. The input for the regression will lead to one prediction for each attribute of the output vector, which together form the because no validation sample is present. Note that in both methods the Markov assumption holds, both methods predict the next state depending only on the current feature distribution. The automaton X  X  loss lies between 0.89 and 0.97 for similarity cut-offs up to 0.4 (cf. Fig. 5). Logistic regression achieves an average loss of 1.42 between the true and predicted pro fi les. This indicates that the automaton may again. This is because there are so many different states in the clusters, that groups resembling the average population are created. Of course, they will also get predictions for the average population, which is better in the predictive power, but will not enable predictions on a person but only on a group level. However, predicting the pro fi les for single persons is a main use case, and thus higher losses must be accepted. Compared to the synthetic automaton, the real-world data automaton is worse in its in the data. So, smaller predictive errors can be observed for the synthetic automaton.
Investigating patterns of a (real-world) diabetes automaton: In this paragraph we illustrate the medical knowledge represented by an automaton. We focus on a qualitative, not a quantitative evaluation at this point. An automaton for patients suffering from diabetes was created on 100 randomly sampled patients. Only frequent (occurring in more than 5% of all patients) attributes were considered. The resulting states that form cycles as well as self-loops. That means that patients alternate between those states. This also re fl ects the normal coding behavior of physicians, which often diagnose the same diseases in successive quarters. However, we can now choose a state in the automaton, analyze its properties and check which development it can undergo. Because this automaton is built only for patients with diabetes, automaton. Therefore, we picked a path of the automaton. It shows possible developments of the health The path highlighted in the upper left part of Fig. 6 begins at node 1259 and ends at node 929. It shows how the patients X  diseases shift from a diabetes and rather mild heart disorder to severe heart problems infarction and coronary heart disease (CHD A84). As time progresses, a heart attack may occur (A83), but in every case it leads to CHD and further heart problems (A86). Additionally, renal problems (A131) are present when coagulation dysfunction persists. However, as more diseases appear, the coagulation anymore.

Regarding this path and the overall structure, we can conclude that the algorithm fi nds meaningful patterns that re fl ect current medical knowledge. When applying the algorithm to other data, equivalent medical patterns were found as well.

Results yeast metabolism data: In this paragraph, another application based on yeast cell data is pre-experiment, gene expression data of the budding yeast is modeled by an automaton, which is displayed in Fig. 7. It consists of 11 states and 21 transitions with an SC of 0.87. The cutoff was set to 0.224. it is a cycle. Thus, the automaton presents a periodic pattern that the organism passes through, which well nourished culture, yeast grows and divides in a constant manner. The process is divided into four coarse steps: the G1, S, G2 and M phase. During the G1 phase the cell is growing, and then the DNA is cell cycle and state 11 is its end.

But not only the structure of observed biological processes, also temporal characteristics can be re-time steps. Moreover, stages lasting more than one time step (e.g., G2) can be visited more than once, ton captures the possibility that expression peaks/drops may occur faster or slower. These alternatives in the former case. Comparing this to the original data in Fig. 8, the quick drop of gene SWE1 can be shows that genes SWE1 and CLN2 are expressed at the same time, while state 2 shows that gene SWE1 thus to be expected. Using such information, this experiment shows that the automaton can uncover bi-ological processes by their recorded characteristics. Even more, the automaton provides the possibility of combining data from different cell cultures that are not synchronized. Until now, cells have to be in the same state before the experiment can take place. Using automata, it would be possible to combine method. However, the method shows even more potential if the time resolution and recording length is further improved. With a higher time resolution, the states can be further explored (e.g., by substates) and an extended recording could show rare cases and alternative transitions. 4.4. Empirical evaluation
In this section, some empirical properties of the algorithm are explored. The behavior depending on its main parameters, the number of histories (input sequences) and cut-off values, are studied in detail. Additionally, a runtime analysis will be presented. First, the correlation between the number of investigation of those features depending on the number of input sequences is presented. 4.4.1. Cut-off dependencies
To evaluate the structural features of the resulting automata, the number of hubs , singletons , dead has transitions to at least 10% of the remaining states in the automaton. Singletons are states that do not have a transitio n to another node. Start and dead end states only have outgoing and incoming edges respectively. With an increasing cut-off, similar states are clustered and therefore the number of hubs increases as well. This is because the number of states drops while the number of transitions stays constant. However, for the highest cut-offs the number of hubs drops again, because even hubs are combined into fewer states. Singletons become fewer with increasing cut-off. Again, they are merged that are less similar will be clustered and thus form fewer states. 4.4.2. Dependency on the number of input sequences
Figures 9b to 9d show how the number of input sequen ces affects the resulting automaton. In Fig. 9b, of states does not grow linearly but seems to converge to an upper limit. This may be explained by rare disease characteristics which are only present after a certain sample size. For low sample sizes, many frequent patterns are discovered and so the number of states increases rapidly. But subsequently, mostly rare cases are incorporated into the model, which leads to a slower growth rate of the automaton. of hubs , Fig. 9c shows that with an increasing number of input sequences the number of hubs (left axis) that appear in the fi nal automaton is quite stable. Keeping in mind that a hub occurs when a state is connected to more than 10% of the remaining states, this indicates that a certain number of or a metabolic problem, which are both frequent, alone and in combination with other diseases. The frequency of singletons , dead ends and start states (right axis) drops with an increasing number of input sequences. That indicates that even rare cases are observed more frequently and therefore occur in predicted histograms with a growing number of input data is evaluated. The results of these experiments of the predicted histograms decreases with more input data. This can be explained by the inclusion of more accurately. 4.4.3. Runtime evaluation
We determined the runtime for a varying number of input sequences. All experiments were conducted on a 1.7 GHz machine with 2 GB RAM where the PRTA is implemented in Java with an interface to R and Weka to use the clustering algorithms. As Table 2 shows, PRTAs can be created within acceptable time frames. The runtime of the algorithm is mainly impacted by the clustering method. So, the decision where the runtime is illustrated. 4.5. Comparison with an Multi-Output HMM Although HMMs do not provide multi-a ttribute structure learning, a c omparison of the automaton to a Multi-Output HMM (MOHMM) [8] is presented here, which is a special case of DBNs [10]. We used the input vectors. The result is a 28-dimensional output vector described by 28 discrete output nodes. The output nodes are only dependent on the hidden node in one slice. The structure was chosen a priori, following a suggestion by one of the developers. Because we assume independence of attributes, no dependencies in the output layer were introduced. Furthermore, because the prediction is only dependent on the given event, one hidden node per slice is considered appropriate. The MOHMM was trained on for the best possible solution, we changed the internal node size of the hidden node from 2 to 50. The resulting loss is shown in Table 3. We calculated t he prediction by a 500-fold sampling of the MOHMM and selected the sequence with the best log likelihood. Again, the quadratic loss was computed. The best prediction was achieved with a node size of 20 which resulted in a quadratic loss of 3.22. This is due to the fact that the MOHMM has a higher probability of predicting the presence of attributes, although depends on the number of nodes per slice, MOHMMs with 2 and 3 nodes per slice were computed and evaluated as described before. No improvement over the given results could be achieved; the resulting loss ranges between 3.98 and 4.54. This may also be due to the higher number of parameters which cannot be adjusted accurately. 4.6. Comparison with process mining algorithms The detection of automata is not only closely related to HMMs but also to the fi eld of process mining. One standard algorithm is alpha ++ [28] that discovers Petri nets from event logs. It is implemented in the ProM framework [1], which was used for a comparison here. As described in section 2, Petri such a miner within the given problem setting. However, alpha ++ does not include an automatic event grouping mechanism like the clustering in the presented approach. The group an event belongs to has automata with process models and to use the original algorithms of Petri net detection. To derive the to be derived by the original log and the cluster assignments of each event. This is done by replacing clustering and then model construction. Following the experiments of Section 4.3, the fi rst step is to check whether a given structure can be rediscovered with the alpha ++ algorithm. Again, the synthetic model (cf. Section 4.2.1) was used to extract a log sequence that can be handled by the ProM framework. For a self-loop on event q loop structure between states q is expected to be recorded correctly. Therefore, all events e model is mainly dependent on the clustering and not of the alpha ++ algorithm itself.

The alpha ++ algorithm was applied to noise-free synthetic data; its resulting Petri Net is displayed in which is neither indicated in the model nor in the log. The reason why alpha ++ places this connection is that there is no other possib ility to connect 6 and 7 without risking that markers may become stuck in the net (e.g., by inserting an additional place between 6 and 7). This requirement is induced by the demand that only valid Petri nets shall be discovered. Moreover, in the alpha ++ algorithm there is no check if invalid connections exist. This leads for example to the connections between 8, 9 and 10 where all required connections are modeled but there is now an additional direct connection between 8 and 10, which is not present in the original model. In fact, this misplacement can be handled by the alpha # algorithm [29]. It was designed to detect invisible tasks that can model SIDE, SKIP, REDO and SWITCH constructs. To check the bene fi cial impact of the invisible tasks, alpha # was subsequently applied to the same log. The resulting Petri net is displayed in Fig. 11C, where white rectangles are the connections between 8, 9 and 10 are correctly discovered and the wrong connection from 7 to 4 is Only 3 to 4 and 4 to 5 are included in the model. Both two-loop structures between 1 and 4 as well as 4 and 10 are completely missing in the Petri net, although these connections were actually present in the log. Alpha # here fails to assign the correct connections. This is a serious error, because the model is simply incorrect and moreover, such two-loop structures are frequently present in the underlying domain users to infer information about unknown systems. The same can be detected at the 8 to 5 connection. Such connections can easily be overlooked or even misinterpreted by the user.

Summarizing these results, both algorithms are not able to correctly reveal a known structure in the domain of automata detection. The main reason for this is that they are designed to detect discrete distributed systems. Their main purpose is to model parallelism in a quite linear ordering, which might not be given in a more complex system. Additionally, Petri nets have to satisfy constraints regarding automaton, all transitions should be reachable from the start place. This will allow the modeled system the input place of 5 in Fig. 11C. There are four possib ilities of how th e input place can be reached from other transitions. However, there is only one connection from the place to transitions 5. Thus, the transition can only be labeled with one event. If delay guards and probabilities of the incoming connections are different, it cannot be properly deployed on one connection, because once the place was all discussed issues, we can conclude that Petri nets fi rst do not always discover all dependencies of events in complex systems (as in the presented medical domain), and second, are not yet adapted to constraints. Due to these limitations, Petri nets cannot yet be used for the given problem statement. 5. Conclusion
In this paper, we proposed a new method for learni ng process models in the form of probabilistic in which states are merged when similar enough. State merging is employed because it is currently the for which the true underlying process was known. Moreover, it was tested on real data from a medical and a biological application domain to examine the resulting structure. The experiments showed that the knowledge. To compare the ability of structure identi fi cation, standard process mining algorithms were applied on the same synthetic data set. However, they were not able to reveal the correct structure of was computed. The predictions of the PRTA were compared to the predictions of a combined logistic regression approach, which is considered a standard in this application domain. Additionally, a Multi-Output HMM was trained and tested for its predictive power. The results suggest that the automaton-based prediction performs favorably compared to both logistic regression and Multi-Output HMMs. In the future, we want to further explore the automaton X  X  properties (like the stability, representation of domain knowledge and predictive power) depending on different dataset characteristics. Moreover, we of states by predicates [15].
 Acknowledgements We would like to thank Sonja Ansorge for the suppor t in the stability analysis and moreover, Martin Paluszewski for his help with MoCaPy and his very useful suggestions.
 References
