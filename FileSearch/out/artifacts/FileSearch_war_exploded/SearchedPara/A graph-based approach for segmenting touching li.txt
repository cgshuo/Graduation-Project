 ORIGINAL PAPER David Fern X ndez-Mota  X  Josep Llad X s  X  Alicia Forn X s Abstract Text line segmentation in handwritten docu-ments is an important task in the recognition of historical documents. Handwritten document images contain text lines with multiple orientations, touching and overlapping charac-ters between consecutive text lines and different document structures, making line segmentation a difficult task. In this paper, we present a new approach for handwritten text line segmentation solving the problems of touching components, curvilinear text lines and horizontally overlapping compo-nents. The proposed algorithm formulates line segmentation as finding the central path in the area between two consec-utive lines. This is solved as a graph traversal problem. A graph is constructed using the skeleton of the image. Then, a path-finding algorithm is used to find the optimum path between text lines. The proposed algorithm has been eval-uated on a comprehensive dataset consisting of five data-bases: ICDAR2009, ICDAR2013, UMD, the George Wash-ington and the Barcelona Marriages Database. The proposed method outperforms the state-of-the-art considering the dif-ferent types and difficulties of the benchmarking data. Keywords Text line segmentation  X  Handwritten documents  X  Document image processing  X  Historical document analysis 1 Introduction There is an increasing interest to digitally preserve and pro-vide access to historical document collections in libraries, museums and archives. This kind of documents are valuable cultural heritage, as they provide insights into both tangi-ble and intangible cultural aspects. Historical archives usu-ally contain handwritten documents. Examples are manu-scripts written by well-known scientists, artists or writers, as well as letters, trade forms or administrative documents kept by parishes or councils that help to reconstruct historical sequences in a given place or time. While machine-printed documents, under minimum quality conditions, are easy to be read by OCR systems, handwritten document recognition is still a scientific challenge.

Layout segmentation and, in particular, line segmentation is a key step to guarantee a good performance of handwriting recognition. Not only for text transcription, but the segmen-tation of documents into text lines is an important process for several document analysis tasks, such as word spotting [ 42 , 44 , 51 ] or text alignment [ 26 ]. However, line segmenta-tion is not a trivial process. Historical documents have several difficulties that can complicate the segmentation of text lines. First, the physical lifetime degradation of the original docu-ments, related to the frequent handling and careless storage, produces holes, spots, broken strokes, ink bleed, winkles, etc. Second, if the scanning process has not been rigorous, it might introduce difficulties such as non-stationary noise due to illumination changes, show-through effect, low contrast and warping. Third, the inherent irregularity of handwriting is also a problem. Besides these general difficulties, the char-acteristics of the handwriting and the configuration of the text lines may provoke additional difficulties. First, a curvilinear baseline due to the non-straight pen movement. Second, lines of crowded writing styles, which are more difficult to seg-ment because they are close to each other and increase the overlapping. Third, the presence of touching and horizontally overlapped components [ 17 ] when ascenders and descenders exceed the lower and upper bounds. And finally, punctuation and diacritic symbols, which are located between lines and introduce confusion in the decoding process of the physi-cal structure. In Fig. 1 , we illustrate some of the difficulties described above.

Although accurate algorithms for locating text lines in machine-printed documents have been proposed [ 16 , 38 ], theyhaveshowedsomedrawbacks inhandwrittendocuments and there is still room for improvement. Several text line seg-mentation algorithms for handwritten documents have been proposed (see Sect. 2 ). The methods can be classified as fol-lows: projection-based [ 34 ], Hough-based [ 33 ], grouping-based[ 11 ],morphology-based[ 10 , 48 ]orothermethods[ 21 ].
Text segmentation in handwritten documents can be divided into two tasks: localization and segmentation. Local-ization means to find the position of the text line, for example, by its baseline or central axis. Segmentation refers to a pixel-wise labeling. Localization has a good performance in highly structured documents, when text lines are isolated (as they follow rule lines or form boxes [ 31 ]) or when the methods are designed ad hoc to a particular layout and document type. But when the stated difficulties of handwritten documents are present: touching lines, curvilinear text lines and hori-zontallyoverlappingcomponents,theperformancedecreases and the accurate segmentation is very difficult. Finer analy-sis processes are performed, especially in touch parts. Some methods use connected components to group the touching parts to the closest text lines [ 25 , 51 ], while other methods are more accurate and analyze touching parts [ 28 , 41 ]. The segmentation is done taking in account the properties and the shape of the studied area [ 52 ]. In this paper, we present a line segmentation approach that in addition to the general difficulties of historical documents, tackles with these prob-lems without loosing the generality, so the approach is writer independent, layout independent, and is able to cope with skew and warping disturb.

The main idea of our algorithm is, first, to estimate the localization of the text lines and, second, to segment the text lines. The accuracy of the location is not primordial because our algorithm is focused in finding the optimal path with min-imum cost in-between two consecutive lines in the image background. This is solved as a graph traversal problem. Hence, the skeleton of the background image is converted to a graph. After finding potential starting and ending nodes, min-imum cost paths between pairs of starting and ending nodes are searched. Local cost functions associated with the graph nodes are defined to find the best continuation path in terms of the potential configurations. The problem of touching text lines is solved adding virtual edges between the candidate nodes that are around the involved characters.

The rest of the paper is structured as follows. In Sect. 2 ,the state-of-the-art is reviewed. Section 3 describes the proposed method. Section 4 shows theexperimental results. Finally, we present the conclusions in the last Section of the paper. 2 Related work Handwritten text line segmentation has received high atten-tion over the last years [ 31 ]. In addition to relevant publica-tions, a series of competitions on this topic has been orga-nized in international events (e.g., the ICDAR2009 Hand-written Segmentation Contest [ 13 ], with 12 participants, or ICDAR2013 Handwritten Segmentation Contest [ 54 ], with 14 participants). Observing the existing methods to seg-menthandwrittendocuments,weproposeaclassificationinto five categories: projection-based, Hough-based, component grouping, morphology-based operations and other methods. 2.1 Taxonomy of methods Projection-based methods are based on projection profiles. Black pixels are projected on the vertical axis. The max-ima and the minima of the resulting histogram correspond to regions with large and low horizontal density of pixels. The lines are obtained computing the average distance between the peaks of the histogram [ 34 , 56 ]. Some techniques [ 2 , 41 ] can deal with variations in the text orientation, but they are sensitive to the size of characters and the gaps between successive words. To solve these problems, some methods [ 22 , 23 ] detect areas where two lines are merged due to long ascenders or descenders and compute local histograms to split the lines. The PA I S method [ 13 ] improves a line segmen-tation approach based on projections applying the knowledge of estimated line distance and reasonable black-to-white tra-versal numbers.

Most of these techniques are simple and easy to imple-ment, but they do not work efficiently with multi-skewed text lines, touching components and horizontally overlap-ping component configurations.
 Hough-based methods [ 33 ] describe parametric geometric shapes (straight lines, circles and ellipses are the most usual) and identify geometric locations that suggest the existence of the sought shape. They are proper methods to detect lines because text lines are usually parallel, and consequently, in the Hough space, they generate a configuration consisting of aligned peaks at a regular distance. Although these methods handle documents with variations in the skew angle between text lines, they are not accurate when the skew varies along the same text line, i.e., curvilinear lines. In addition, these methods cannot achieve an accurate segmentation of touch-ing or overlapping lines.
 Grouping-based methods, also known as bottom-up strate-gies, group components according to a specific property. Most of the works belonging to this category are based on searching for components that are horizontally aligned. In Yin and Liu [ 58 , 59 ], connected components are organized in a tree structure in terms of a metric distance and grouped by a minimal spanning tree (MST) algorithm. The CMM method [ 13 ] groups components that are horizontally aligned. The JadavpurUniv method [ 13 ] analyzes dimension features of the components to determine the handwriting style and to set the threshold values for inter-word spacing. Yi et al. [ 30 ] propose an approach based on density estimation criteria to cluster components. Although grouping methods usually present problems to segment touching text lines, the method described in Yi et al. [ 30 ] includes a post-process that detects andsplitsthem.FeldbachandTonnies[ 11 ]joinbaselinesseg-ments, computed in a pre-process step, in historical church registers, similar to the main experimental focus of this paper. Kumar et al. [ 27 ] present a method that computes the sim-ilarity between text components based on local orientation detection and shortest path in graphs. The proposed method can handle with printed documents and complex layouts in handwritten documents; however, like the other grouping-based methods, it fails to segment touching text lines. Morphology-based methods have been used in many works for layout analysis, especially when documents contain text blocks that are strictly oriented horizontally or vertically, i.e., columns and lines. Smearing-based operators can be seen as morphological methods with horizontal structuring elements. Particular examples are the methods described in Dos Santos et al. [ 10 ] and Roy et al. [ 48 ]. They combine the two fundamental morphological operations (dilation and erosion) with horizontal projections and run-length smearing algorithm (RLSA), respectively. Other methods [ 35 , 50 ]use anisotropic Gaussian kernel or local estimation count map.
In the LRDE method [ 13 ], a morphological watershed transform is computed once the document is smoothed using an anisotropic Gaussian filter. Shi et al. [ 52 ] propose a tech-nique based on a generalized adaptive local connectivity map which uses a steerable directional filter. In the ETS method [ 13 ], the text is smeared using a modified version of Weick-est X  X  coherence-enhancing diffusion filter to segment lines. Alaei et al. [ 1 ] use strip-like structures to decompose the text block in vertically parallel structures. Each one is labeled using their gray intensity and applying a morphological dila-tion operation. Nicolau et al. [ 37 ] shred text images into strips along the white gaps in between text lines. Saabni et al. [ 49 ] propose a method that computes an energy map of the input text block image and determines the seams that pass across text lines.

These kind of methods also have problems in docu-ments with overlapping of adjacent text lines. To overcome this problem, some morphology-based works define ad hoc heuristics [ 7 ] or min-cut/max-flow graph cut algorithm [ 24 ]. Graph based: Some approaches use graphs to compactly rep-resent the image structure keeping the relevant information on the arrangement of text lines. Energy (or cost) functions are used to establish the optimal path between nodes that segment the lines. In Kumar and Namboodiri [ 29 ], the seg-mentation is posed as a graph cut problem. The graph is built using either the pixels or the connected components of the image as nodes, which are linked to its neighbors through edges.

The PortoUniv method [ 13 ] represents the image as a graph, which is used to find the minimum energy paths between the borders of the page using an efficient dynamic programming approach. The robustness of projection-based methods is combined with the flexibility of graph-based methods by Wahlberg et al. in [ 55 ]. The graph is constructed using the foreground of the image.
 Other methods: There is a miscellanea of other methods that cannot be classified into any of the main categories described above. Kass et al. [ 21 ] use active contours to explore the bor-ders of the image objects with relevant differences between the foreground and the background in characteristics such as brightness or color. Bukhari et al. [ 4 , 5 ] adapt active contours (snakes) over the ridges of the geometry of the gray-level image to detect the central axis of parts of text lines. The method properly localizes the text lines in the documents, even with the difficulties explained above. In case of touch-ing components lying in two different text lines (a connected component lies over two text lines), they are horizontally of vertically cut depending on the slope of underlying ridges into equal number of parts. However, this can split words into different lines (e.g., ascenders or descenders of the words are split in the above or below text line). Liwicki et al. [ 32 ]use dynamic programming to find text lines, computing mini-mum cost paths in between consecutive text lines. Stafylakis et al. use a Viterbi algorithm to segment the text lines [ 53 ]. 2.2 Discussion In order to summarize the above-described methods for seg-ment lines in handwritten documents, Table 1 overviews their strengths and weaknesses and Table 2 shows the type of documents (binary or gray level) usually used as input. The graph-based methods are not included in the taxonomy because the methodologies used in these kind of approaches are too diverse and therefore they are unable to be general-ized under a common assessment. The rest of the methods are compared (Table 1 ) according to the following criteria: if the method works in printed and/or handwritten documents; if the method handles variations in the skew angle between text lines and when the skew varies along the same text line (curved lines) or not; if the method can solve the problem of horizontally overlapping components; and if the method can properly split touching lines or not.

Text line segmentation in printed documents is a problem that has been solved from different approaches with satisfac-tory results. However, when dealing with handwritten doc-uments, state-of-the-art methods, specially projection and Hough-based, present some difficulties to properly segment the lines. Errors in segmentation are usually due to noise and the non-rigid structure of this kind of documents. These irreg-ularities lead to the three main problems that are present in the handwritten text line segmentation: curved lines, horizon-tally overlapping lines and touching lines. Methods based on morphological operations and grouping-based methods are able to deal with curved and horizontally overlapping lines. However,thesegmentationoftwotouchinglinesstillremains as an unsolved problem among state-of-the-art methods.
Besides the taxonomy presented in Table 1 , an addi-tional criterion that is worth to be considered is whether the approach requires a learning process [ 20 , 60 ]ornot[ 6 ]. The methods based on projection profiles have good accuracy when they are applied to documents with the same structure layout and style. The main problem of these approaches is their adaptability. They have to learn their models for every new document, for those that present a new structure layout, or a new handwriting style or a different time period. This kind of methods need some samples of every type of docu-mentstolearnamodel.However,suchsamplesarenotalways easily available. In addition to this drawback, learning-based methods have a higher computational cost, even though the learning process is an off-line process. The methods with-out a learning process are more adaptive. They can robustly extract the lines of any kind of documents, and the compu-tational cost is lower. However, the performance decreases when dealing with a close collection because they are not adapted to the specificities of that set.

Most of the methods presented above localize text lines with a high accuracy, but only a few of them focus their methods to solve the problem of overlapping and touching components [ 18 , 40 , 47 ]. Even so, Kang et al. [ 18 ] require a learning process to define local configurations of touching components. Ouwayed et al. [ 40 ] split touching components following the descending parts of the characters, which is a common property of most characters in the Arabic alphabet. Rohini et al. [ 47 ] localize touching components extracting the core region (space between consecutive text lines) using horizontal projections profiles. Then, the method needs a pre-process to deskew the curved text lines.

We have also classified the above methods according to the kind of input image: binary or gray level. Usually, projection-based methods use binary images as input, except the method of Manmatha et al. [ 34 ] that uses a modified ver-sion of Ha et al. [ 14 ] extended to gray-level images. Hough-based and grouping-based methods use binary images in their approaches because they perceptually group basic primitives (key points or connected components). Morphological-based and other methods have a large diversity of algorithms, and each one uses a different type of images.

From the comparative shown in Table 1 , we can conclude that the main challenges are the segmentation of touching, horizontally overlapping and curvilinear lines. The main con-tribution of the approach proposed in this paper is its robust-ness and high performance when segmenting lines under the above-mentionedproblems.FromthecomparativeinTable 2 , we conclude that most methods use binary images as input. In our approach, we assume that images have been previously binarized so it simplifies the process. However, we will show how the method is robust to binarization noise so this process is not critical in the pipeline. 2.3 Contribution We present an approach inspired by graph representation methods. Graphs are a useful tool to capture the structure of the image objects (lines and words in our case). In addi-tion, graph theory offers solid and elegant methods. Graph vertices are usually constructed from pixels or connected components. Graph edges represent spatial relations between connected components and are usually weighted by the dis-tance between the connecting vertices [ 3 , 57 ].

The main objective of our work is to localize text lines and to solve the problem of touching lines adding new vir-tual edges to the graph. These characters are split using some heuristics which evaluate the spatial information around the area involved. This technique is not oriented to a specific writer, style or alphabet, and it is able to cope with multi-oriented text lines and historical documents. The approach presented in this work belongs to the group of methods which do not need a learning process to segment lines; there-fore, it does not need labeled samples. Next, we explain this approach in detail. 3 Line segmentation approach Humans tend to write text in blocks, and they usually use the samespacebetweenlines.Ina3Dviewoftheintensityimage, this characteristic can be seen as a valley: if we compute the distance function and hence see the topography of the image, the in-line space is seen as a valley and the words as crests. Usingthisobservation,wefirstcomputethedistancefunction in the input image, which corresponds to the skeleton of the background. Afterward, we detect paths through the valleys of the document. The paths consist of background points at equal distance to the words above and below. We use a path-finding algorithm to select which paths are the best to segment the lines. In Fig. 2 , we can observe a representation of this characteristic.

The goal of our method is to automatically locate and seg-ment text line regions in handwritten documents. The system consists of two big stages, as shown in Fig. 3 . The first stage is the enhancing of the documents and the localization of the text lines. Then, the second stage is the line segmentation. We compute the skeleton of the background image. All the possible pixel paths are computed using an iterative thinning function. Then, the paths are converted to a graph, which will be used to find the optimal paths that segment the text lines. Then, the best paths that segment the text lines are found. For this purpose, we adapt the A-star path-finding algorithm. Finally, the consistency checking step is applied. Let us fur-ther describe the different steps. 3.1 Localization The localization step includes an enhancing process of the documents. The main idea is to use the valleys that appear between the text lines to segment them. The words of the text lines represent the crest of the mountains, and the noise of the documents can introduce hills that produce the diversification of the valleys. This fact introduces new possible paths, and the computational cost increases proportionally to the noise. The number of valleys is reduced applying morphological operations to smooth the image and to reduce the hilltops. First, the image is binarized using the Otsu X  X  method [ 39 ]. Several binarization methods have been tested in our doc-ument images: Niblack (Fig. 4 a) generates noisy images, Sauvola (Fig. 4 c) loses important information and the charac-ters are thinned, and Bernsen (Fig. 4 b) generates good results butthecomputationaltimeisveryhigh.Otsu(Fig. 4 d)obtains a clear image, without noise, the characters do not loose pix-els and are well defined, and in addition, it is the fastest method.

The skeleton is a simplification of the topology of the image. In this work, the skeleton of the distance function applied to the background allows to obtain the seams between text lines (valleys of the distance transform image). Since the images are originally binary (white paper and black ink), computing the skeleton in the gray-level image would not give the same result so it would obtain distorted skeleton with extra segments in images due the scanning process. Conse-quently, the path-search algorithm has to analyze all these extra paths, so the computation cost increases exponentially. For this reason, we prefer to binarize the input images.
The scanning process introduces some distortions. One of them is the set of page margins (a black area around the page image). We delete these margins by applying morphological operations and selecting the biggest blobs in the periphery of the document. Then, a median filter is applied with a mask of size 4  X  4 to remove the speckle noise. The problem of using this kind of filters is that they provoke a thinning of the characters. An alternate sequential filter of opening and closing operations is applied to correct this problem.
Once the image is binarized, text lines are localized using a projection-based method. To get a best accuracy, a rough estimation of the skew of the document is computed using the Wigner X  X ille distribution [ 40 ]. 3.2 Graph construction The proposed method for segmenting lines in handwritten documents is based on searching the pixel paths of minimum cost on the skeleton of the background image between the left and the right margins of the previously localized text lines.
For the sake of efficiency, instead of directly processing the skeleton at pixel level, it is approximated by a graph G = ( V , E ) that preserves its structure. The set of vertices V of the skeleton graph represents characteristic points (ter-minal and interSection points), and the set of edges E rep-resents sequences of consecutive skeleton points between vertices. Formally, a skeleton graph G is represented as an attributed graph G = ( V , E , L V , L E ) where L V and L are two labeling functions that assign attributes to nodes and edges, respectively. The labeling functions L V and L E are defined as follows.

Given a vertex v  X  V , the attributes assigned to it are denoted as: L where N v denotes the number of neighbors, ( x v , y v ) are the coordinates of the pixel, and t v is the type of the node out of {  X  i , X  f , X  e , X  c , X  cb , X  ct } (Fig. 5 ). These types of nodes represent the following configurations:  X   X  bor) of the skeleton located at the left margin of the image (first column of the image pixels).  X   X  at the right margin of the image (last column of the image).  X   X  part of the image. It is a terminal pixel of the skeleton located at any place of the image except the first and the last column of the image pixels.  X   X  has two incident edges (skeleton paths) with an important change in the orientation.  X   X  edges). It is a pixel of the skeleton which has three neigh-bors.  X   X  edges). It is a pixel of the skeleton which has four neigh-bors.

An edge e = (v s ,v t )  X  E stores the current path of chain pixels joining the source vertex v s  X  V and the target vertex v  X  V ; the Euclidean distance between v type of edge: true edge (when there is a true path of pixels between v s and v t )or virtual edge .

The problem of touching text lines is solved by adding virtual edges. Due to the geometry of the distance func-tion image, two touching words provide a discontinuity in the path, i.e., the skeleton computed in this area creates two or more ending points around the place where the touching problem appears. These ending nodes areusedtosolvethis problem. We connect these nodes using new edges. These new edges are known as virtual edges (Fig. 6 ). These virtual edges are sub-path candidates. So they allow to reconstruct the broken path traversing the touching characters through the minimum path. A virtual edge is created between two ending nodes  X  e when they are very close. The threshold radius R v (see Eq. 1 ) of the area around an ending node to search for other connecting nodes is experimentally set proportional to the size of the image. The mean of the sepa-rations between the text lines is estimated in the localization process. When t e is a virtual edge, then p e is empty. R v = 3.3 Graph path search Once the skeleton of the background image has been con-verted to a graph, the problem of text line finding is trans-lated into searching for shortest paths in the graph according to some considerations. A-star (A*) is a computer algorithm that is widely used in path finding and graph traversal. Hart et al. [ 15 ] described the algorithm as an extension of the Dijkstra X  X  1959 algorithm [ 9 ].

The algorithm proposed in this work is a modified version of the classical A-star algorithm. In the classical algorithm, the starting and the target point should be established. In our problem, we do not know a priori which node, among the final nodes, is the target node. For each initial node  X  i the graph, the algorithm iteratively searches a minimum cost path until a final node  X  f is reached.

The objective of this step is to find the best path in the valley between two crests or text lines (Eq. 2 ). The solution is found as a minimum energy path in the graph G between an initial node v 1  X  X   X  i } 1 and a final node v z  X  X   X  f P contains a sorted list of nodes and edges. The path P can be seen as a representation of the valley path between two text lines calculated based on the skeleton. We denote a path P as follows: P where v j 1  X  X   X  i } and v j z  X  X   X  f } .

The cost of a path P j is the accumulated cost of the local transitions (local paths) between consecutive nodes. Formally, p ( Given an initial node v 1  X  X   X  i } , the algorithm searches for the minimum cost path that reaches a final node v z  X  X   X  f in the opposite side of the page.

The algorithm searches the best path in the state space S , where each state represents a partial path explored to this point. The algorithm explores in a state S i all the possible states [ S 1 i , S 2 i ,..., S m i ] to go. An intermediate state S responds to a graph node v j n , and the next possible states correspond to all the possible next graph nodes v j n + 1 connected to the node v j n .

The transition from a state S i to the next state S i + 1 computed in terms of some heuristic functions that model local configurations (explained with more details in the next paragraphs). Each transition has a cost of moving from a state to the next state according to a weighted combination of four predefined heuristics corresponding to four possible local configurations. Briefly, the heuristics give the cost of a path taking into account its trend, the bound of each text line, and also to avoid the possible backward paths and to solve the problems of touching components and horizontally overlapping objects. The next state corresponds to the mini-mum cost transition from the node v j n to the neighbor v j through the edge e j n (chosen among all the possible nodes + 1 connected to v j n ). The cost of the path step v j n through the edge e j n is denoted as c (v j n ,v j n + 1 , c where  X  i are the corresponding weights computed experi-mentally (  X  1 = 1 , X  2 = 0 . 5 , X  3 = 0 . 01 and  X  4 = 0 {  X  , X  the edge between v n and v n + 1 , which contains information as if it is virtual or real. To simplify the notation x v j denoted as x j n + 1 and y v j
Let us further describe the four heuristics that are consid-ered to model the cost function.
 H1.-Trend Heuristic. Humans write text lines following a uniform direction, without abrupt orientation changes. Although a text line presents a curvilinear orientation, the local orientation trend predicts the smoothest continuation path. This property is used to fix the path to this trend and to avoid the possibility of sharp curves in the computed paths.
We use the trend of the path, computed by a linear regres-sion, to compute the cost of the new node in the path taking into account the nearby nodes in the y axis. Given the esti-mated trend point v j n + 1 , the starting node v i  X  X   X  i source node v j n , the cost is the sum of the respective differ-ences between those three points and the target node v j n Formally: h all the nodes that compose the temporal path [ v j 1 , e j and v j n + 1 is an estimation of v j n + 1 .
 H2.-Bounds Heuristic. Humans tend to write text lines par-allel each other. We also take into account that it is not usual to cross lines when writing. The objective of this heuristic is to fix the path inside the upper and lower bounds of two text lines, defining a band along which the path cannot surpass (Fig. 7 ). We fix the path between the upper and the bottom limit of each line. Some documents contain multi-skewed lines. To correct this problem, the bounds of each line are adapted dynamically at each iteration in terms of the current trend of the path. The graph edges located between the words of the same text line have a higher cost than the edges located between different text lines. Formally, h limit estimated previously of this path.
 H3.-Back Heuristic. Following the premises of the last two heuristics (H1 and H2), paths cannot go back abruptly. They have to follow a trend, and it has to be inside a bound. Another premise is that the target of our path-finding algorithm is located on the right margin of the document, so paths that go backwards are penalized with a high cost.

The direction of the path is checked, and if it goes back, we increase the cost directly proportional to the retracted distance (Fig. 7 ). Although the cost of this path is high, in some cases the algorithm chooses this option because there is no alternative or it is too expensive. Formally, h where d e is the Euclidean distance.
 Algorithm 1 Path finding. H4.-Virtual Paths Heuristic. As we have explained before, the problem of touching text lines is solved by adding vir-tual edges to the graph. In the construction process of the graph, we introduce virtual edges between intermediate end-ing points (Fig. 7 ). We use this kind of edges when there is no alternative path (or the cost of the other paths is too high), or the alternative path has a high deviation passing through the words of the text lines above or below. Formally, h where e j n is a virtual path.
In summary, the algorithm computes the best path in the valley between two crests, or text lines, for each initial node are expanded, and the heuristic cost h is computed from v j to cost and the heuristic cost ) is chosen. The real cost is the cost computed from the initial node v j i to the current node Contrary, the heuristic cost is an estimation of the cost from n to 3.4 Consistency checking Although the proposed method is able to cope with noise, some documents may present high levels of degradation. It results in wrongly segmented lines, in particular two consec-utive paths may be overlapped (Fig. 8 ). To avoid this prob-lem, the consistency checking process is a post-process that looks for the overlapped paths and splits them accordingly. During the graph path-search step, the algorithm checks the overlapped edges. Then, paths that share edges are split.
Two kinds of overlapping can appear. The first one occurs when two paths are overlapped (Fig. 8 ). The second one occurs when a path is overlapped with two other paths (Fig. 8 ). In the first case, the overlapping is solved taking in account two facts: the high variability (in Y axis) in its nodes and the distance between the starting node and the closest text line estimated in the localization step. Text lines that start in the middle of an estimated text line will be penal-ized. In the second case, the overlapping is solved removing the path that is overlapped in the other two paths. 4 Experimental results and discussions We have experimented with five databases with increas-ing level of difficulty: the datasets from ICDAR2009 [ 13 ] and ICDAR2013 [ 54 ] Handwritten Segmentation Contest, the UMD Database [ 19 ], George Washington X  X  manuscripts [ 43 , 45 ]andtheBarcelonaMarriagesDatabase[ 12 ].Themet-rics used to evaluate the performance of our approach are the ones used in the ICDAR2013 Handwritten Segmentation Contest. 4.1 Metrics To make the results comparable, the performance evalua-tion method used in this work is the same that the used in ICDAR2013 Handwritten Segmentation Contest [ 54 ]. It is based on counting the number of matches between the enti-ties detected by the algorithm and the entities in the ground truth. A MatchScore(i, j) Table was used, representing the matching results of the j th ground truth region and the i th resulting region.
 MatchScore ( i , j ) = Let I be the set of all images points, G j the set of all points inside the j ground truth region, R i the set of all points inside the i result region and T ( s ) a function that counts the points of set s .

A region means the set of foreground pixels likely to belong to a text line in each segment. A match is only con-sidered if the matching score is equal to or above a specific acceptance threshold. Let N and M be the amount of pixels of the ground truth and result elements respectively, and o2o is the number of one-to-one matches, we calculate the detec-tion rate DR and recognition accuracy RA as in ICDAR2013 Handwritten Segmentation Contest [ 54 ]. Formally, DR = A performance metric, called F-Measure FM, is computed combining the values of the detection rate (DR) and recog-nition accuracy (RA): FM = 4.2 Datasets 4.2.1 ICDAR2009 and ICDAR2013 The documents of the ICDAR2009 text line segmentation contest, consisting of 200 document binary images with 4,034 text lines, came from several writers that were asked to copy a given text. None of the documents include any non-text elements (such as graphical lines and drawings) and were written in several languages (English, French, German and Greek). The documents of the ICDAR2013 text line segmen-tation contest are similar to the previous one. The main differ-enceisthattherearemorelanguagesinvolvedbyincludingan Indian language. It consists of 150 document binary images with 2,649 text lines.

These databases were specifically created for a competi-tion on line segmentation. The text lines are roughly straight and horizontal. There are only few documents that present multi-skewed text. The text lines are well separated, and, in a few cases, we observe overlapping between ascenders and descenders from adjacent lines. The touching line problem only appears in some few cases. A sample of a handwritten document image of these datasets can be seen in Fig. 9 a and b. 4.2.2 UMD The UMD data set was collected by the members of the Lan-guage and Media Processing Laboratory at the University of Maryland. It consists of 123 Arabic documents, containing 1670 text lines. The images in this dataset present complex layouts and different levels of noise and degradation. We can observe an example of this database in Fig. 9 . 4.2.3 George Washington This dataset consists of 20 pages and 715 text lines, from the George Washington collection. It is sampled from different parts of the original collection (at the Library of Congress). The images are in gray level and scanned from microfilms. There are two writers in the 20 selected document pages. It is a well-known real historical handwritten database, and, although the documents present good quality, some of the typical difficulties appear in some cases. The handwriting style in the Washington dataset is roughly straight and hori-zontal, and it contains ascenders and descenders from adja-cent lines that are touching each other. We can observe an example of this dataset in Fig. 9 . 4.2.4 Barcelona Marriages It is a collection of books written from the fifteenth to the nineteenth century. 2 It contains the marriage licenses cel-ebrated in Barcelona and surroundings. There are approx-imately 90,500 pages, written by 244 different writers. The documents of this collection are in color, and they are degraded by lifetime and frequent handling. We show two examples of these documents in Fig. 9 e and f. Infor-mation extraction from these manuscripts is of key rele-vance for scholars in social sciences to study the demo-graphical changes over the five centuries. This collection presents an old handwriting style with all the difficulties of a real historical handwritten database (see Sect. 1 ): touch-ing lines, large and big strokes with many overlapped char-acters between lines, horizontally overlapping components and multi-skewed text lines. We have used two datasets in order to show the performance of our method with different handwriting styles. The first one consists of 30 documents with 964 text lines from the nineteenth century. The second one contains 94 documents with 3,252 text lines from the seventeenth century. 4.3 Experiments and results We have performed five different experiments with the five datasets explained above, plus a synthetic dataset. Each experiment includes the results of the method presented in this work and the results of a classic method based on pro-jections [ 46 ]. The objective of this comparison is to show the difference between localizing and segmenting text lines. We prove that, even our localization is coarse, we obtain a high accuracy in the text line segmentation. The parameters used in our method have been experimentally computed, but it is important to notice that we have used the same configuration for all the experiments. Therefore, we show how robust is the method to different collections using the same configu-ration. The values are  X  1 = 0 . 61 , X  2 = 0 . 369 , X  3 = and  X  4 = 0 . 20. 4.3.1 ICDAR2009 &amp; ICDAR2013 experiments In the first experiment, we have compared the results of our approach with the results of the participants of the ICDAR2009 and ICDAR2013 Handwritten Line Segmenta-tion Contests and a classical line segmentation method based on projections [ 46 ] as baseline. The performance obtained using the ICDAR2009 and the ICDAR2013 datasets is shown in Tables 3 and 4 , respectively.

First, to have a baseline reference, the performance of a classical algorithm based on projections has been mea-sured. It obtains a FM of 85 . 86 % using the ICDAR2009 and 76 . 51 % using the ICDAR2013 database. This low perfor-mance is because projection-based methods have difficulties segmenting handwritten documents with skew or touching components and do not work properly when ascenders and descenders of two consecutive lines are horizontally over-lapped.

To better assess the performance of our method regarding the state-of-the-art, in Table 3 , we can see the comparison with all the methods presented in the ICDAR2009 Handwrit-ten Line Segmentation Contest. These results are reprinted from the contest report [ 13 ]. Here, we focus on the analy-sis of the most outstanding methods. The CUBS method is based on an improved directional run-length analysis. The ILSP-LWSeg-09 method uses a Viberti algorithm to segment lines. The PA I S method is based on horizontal projections. And the CMM method uses labels to identify words of the same line. For more details on the rest of the methods, the reader is referred to Sect. 2 .

The main problem of the CUBS method is the overlapping of adjacent text lines. The ILSP-LWSeg-09 method has the problem with the function that minimizes the distance is that it is different depending on the document. The PA I S is a projection-based method, and this kind of methods have a problem in highly multi-skewed text lines. Finally, the CMM method is a grouping-based method, so it fails to distinguish touching text lines.

Table 4 shows the comparison with all the methods pre-sented in the ICDAR2013 Handwritten Line Segmentation Contest. These results are reprinted from the contest report [ 54 ]. Here, we focus again on the analysis of the most out-standing methods. The INMC method is based on an algo-rithm of energy minimization using the fitting errors and the distances between the text lines. The NUS method uses a seam carving algorithm to segment lines. The GOLESTAN method divides the document in regions to compute the text lines using a 2D Gaussian filter. The CUBS method is based on a connectivity mapping using directional run-length analysis. And the IRISA method combines blurred images and connected components to segment the lines.

The INMC method has a main problem that needs a learn-ing process to estimate a cost function that imposes the con-strains on the distances between text lines and the curvilin-earity of each text line. The GOLESTAN method localize text lines, the segmentation is done obtaining the dilation of synthetic paths, which represents the localization of the text lines. Difficulties as touching lines and overlapping are not solved. The IRISA method localizes properly the handwritten text lines, the problems of crossing lines and overlapping are considered, but the touching lines problem is omitted.
Our method has obtained a FM of 96 . 67 % and a 95 . 43 % irrespective databases. It is worth noticing that the per-formance of some methods, having a high performance, decreases when they are applied to other document collec-tions, especially historical ones. An example is the CUBS method which in the ICDAR2009 got a FM of 99 . 53 % and a97 . 45 % in the ICDAR2013. But nevertheless, the method presents robustness in front of different databases. In the fol-lowing paragraphs, we will show how our method is robust under different conditions and how the performance keeps in a reasonable level even when the distortion is high in some other historical datasets.

The objective of the next experiments is to show that our method is addressed to segment lines accurately and not only the localization. 4.3.2 UMD experiment In this experiment, we have evaluated the performance of our method with respect the classical approach based on pro-jections, using the UMD dataset. The performance obtained using the UMD dataset is shown in Table 5 . Our approach obtained a FM of 87 . 63 % and the baseline method based on projections obtained a FM of 59 . 55 %. The performance of simple projection profile analysis methods fails also in Arabic documents, even more if the present touching lines in their documents. The results of the UMD database can be compared with the results of the work [ 5 ]. They compare their work with several datasets of the literature and with dif-ferent databases (included the UMD database). We observe low performance results ( 73 . 52 % ) using the UMD database. This method makes an accurate localization of the lines, but fails in the segmentation when the lines present touching components. 4.3.3 George Washington experiment In this experiment, we have evaluated the performance of our method with respect to the classical approach based on projections, using the George Washington dataset. The per-formance obtained using the George Washington dataset is shown in Table 6 . Our approach obtained a FM of 92 . 70 %, slightlylowerthanintheICDAR2009andintheICDAR2013 dataset. The baseline method based on projections obtained a poor FM of 46 . 70 %. 4.3.4 Barcelona Marriages experiment In the last experiment, we have compared the performance of our method with the results obtained from the classical approach based in projections, using the two Barcelona Mar-riages datasets. Tables 7 and 8 show the results obtained in the datasets of the nineteenth and seventeenth century, respec-tively. The results using this dataset are good taking into account the quality of the documents. We have obtained a FM of 82 . 20 % using the dataset of nineteenth century and a FM of 86 . 3 % with the dataset of seventeenth century. These FM rates are lower than the obtained using the George Wash-ington dataset. We have also computed the FM rate using the classic method based on projections, and the results are poor (56 . 10 and 63 . 60 % respectively). As it was expected, the performance of simple projection profile analysis methods, that are standard techniques in machine-printed documents, is very poor in historical manuscripts with variations in the script styles, lines that touch and overlap ones to the others, noise, etc.

In Fig. 10 , the reader can observe some qualitative results obtained in the five databases. As we can observe, the Barcelona Marriages datasets are more complex than the other two datasets, and sometimes touching components are not properly segmented. However, the problem of horizon-tally overlapping components is solved in most of the cases (Fig. 10 f). Our method can find a path through the ascenders and descenders of the text lines without any problem. We can observe that our method properly segments documents con-taining text lines of different length (Fig. 10 d, e) and skew (Fig. 10 a, c). 4.3.5 Other experiments To evaluate the robustness of our method in front of the typ-ical difficulties of handwritten documents, we have gener-ated some synthetic images (Fig. 11 ) from the ICDAR2009 dataset. The first difficulty appears when the document presents a high skew (Fig. 11 a). We have rotated  X  5 degrees document. The second one appears when the line spaces between text lines are not homogeneous (Fig. 11 b). We have spaced the lines using different sizes. Sometimes, the text lines present a falling curvature (Fig. 11 c), or text lines have different orientation between them (Fig. 11 d). Finally, some documents present several text blocks, even in different ori-entations (Fig. 11 e). These three images have been manually modified. As the objective of this work is the line segmen-tation in text blocks, and not the layout segmentation, we have used the approach developed by Cruz et al. [ 8 ] which is oriented to extract text blocks in historical documents. So, the segmentation method has been applied after segmenting each text block. In the same image, we have introduced text with 0, 90 and 45 degrees. We can observe that in all these cases, our approach properly segments the documents.
Our method is also able to cope with noisy documents as we can observe in Fig. 12 . The presence of noise influences on the computation of the skeleton of the background image. However, the variation in the skeleton does not influence on the segmentation of the text lines. We can observe the results obtained using a clean image in Fig. 12 a and a salt-and-pepper noisy image in Fig. 12 b. In this figure, we have simulated the case when the document is too much noise and the pre-process cannot clean the document completely. The result of the pre-process is a noisy image, but nonetheless, the segmentation is done in a proper way.

Some documents contain spots and show-through that can be a problem in the segmentation process. As we can observe in Fig. 13 , our method allows to solve this problem thanks to the pre-processing step. In Fig. 13 a and c, we observe some noise in the document (spots), which can slightly mod-ify the path, as we can observe in the Fig. 13 , but, in both cases, our approach finds a path to segment the lines. There are some cases where the documents present drawings and graphical lines (Fig. 13 b). Our method does not remove this kind of noise, but segments the lines searching a path between these graphical elements. The problem of show-through is showed in the Fig. 13 d. The pre-process solves this prob-lem, and the paths are properly computed. For more com-plex cases, we can use specific pre-processing methods to remove spots, graphical lines, drawings and show-through. We have removed all the steps to clean the image included in the pre-process in all the experiments presented above. We have only done the binarization of the images. The objective is to simulate the noisy documents where the pre-process cannot remove the noisy completely. We show the good per-formance of the approach presented in this paper in this kind of documents.

There are two main problems that can vary the number of initial nodes regarding the number of text lines (Fig. 14 ). The first problem appears when there are comments between the text lines. They are usually smaller than the size of the text lines and they are completely touching the above and below text lines (see Fig. 14 a). In these cases, the paths to segment the lines (upper, bottom and in-line text line) are almost overlapped, and the consistency checking process will remove the most overlapped path. Consequently, it joins one of the text lines with the in-line text. The second problem appears when the length of the text line is very short (com-pared to the rest of the lines) and it is horizontally overlapped (Fig. 14 b). Then, the segmentation of the text lines becomes ambiguous because it is difficult to determine whether text fragments belong to the same text line or not. These two prob-lems increase the difficulty of segmenting text lines, and it can be objective of specific works. However, since the num-ber of these specific cases is very low in the databases used in our experiments, this is not a critical issue.

Finally, the presence of non-text elements such as graphi-cal lines or drawings is not a handicap for our method because it does not really depend on a script or text like writing style, but only on the structure. In the Barcelona Marriages dataset, it is usual to end text lines with an horizontal stroke or to cross out wrong registers. Our approach is able to tackle with this difficulty as we can observe in Fig. 15 . 4.4 Discussion Theconfigurationusedintheexperimentshasbeencomputed experimentally using the ICDAR2009 dataset. This configu-ration has been used in all the experiments independently of the dataset. This fact shows the robustness of our method in front of different types of handwritten documents, whether they are historical or modern.

We can observe the evolution of the performance of the methods using the five databases: the two first ones are datasets where the problem of touching, skew and hori-zontally overlapping text lines is scarce (ICDAR2009 and ICDAR2013 dataset). The second one is dataset written in Arabic. It is written using a left justification and presents touching and overlapping in a few cases. The third one is a historical dataset with some noise introduced by the life time and with some of the problems explained before (George Washington dataset). The last one is a dataset with noisy and degraded documents, where the percentage of touching com-ponents and horizontally overlapping is very high (Barcelona Marriage dataset).

The last experiment shows the robustness of our method in front of the typical difficulties in handwritten documents. The method is able to cope with noisy documents and with documents that contain drawing lines or comment lines.
The overall conclusion is that a baseline method based on classical projection profile analysis does not work well with manuscripts. The main difficulties for this kind of methods were anticipated in the introduction: the more is the skew and degree of touching and overlapping between consecu-tive lines, the lower is the accuracy of the segmentation. In general, projection profiles detect the approximate position of the text lines, but do not allow an accurate segmentation. When analyzing other methods designed for handwritten line segmentation, the proposed approach is ranked in the top positions. The robustness of the method was proved when it was applied to databases with increasing levels of difficulty. Our method kept its performance over 80 % of FM. Hence, we can conclude that the proposed method is highly able to cope with the different types of problems that appear in historical documents, in particular with multi-oriented lines, overlapped components and touching lines. Conversely, as it is observed in Manohar et al. [ 36 ], the winner method of the ICDAR2009 Handwritten Segmentation Contest with a FM of 99 . 5 %, drastically decreases the performance to a FM of 56 . 1 % using low resolution and noisy handwritten documents. 5 Conclusions In this paper, we have presented a robust line segmentation approach, which segments lines in any kind of handwritten documents. It is not designed for a specific category of doc-uments, coping with both historical and modern ones. The proposed approach finds the path which is located at the same distance in the area between two consecutive text lines. The skeleton of the background image is used to convert it to a graph. This graph is used to find the best path to segment text lines using a minimum cost path-search algorithm.

One of the key contributions is the ability of the method to segment lines pixel wise and not only locate then, as some approaches in the literature.

We have tested the method in five databases with differ-ent difficulties: ICDAR2009, UCDAR2013, UMD, George Washington and Barcelona Marriages database. With this extensive experimentation, we have proved that our method is able to deal with different conditions of degradations and in front of modern and historical documents. Even in the worst scenario,suchastheBarcelonaMarriagedatasetsthatpresent many difficulties, our approach obtains a FM of 82 . 20 %, while other state-of-the-art methods dramatically decrease their performance when the documents contain skewed or multi-oriented, touching and overlapping lines.

In summary, this paper has presented a robust method to segment handwritten lines that outperforms the state-of-the-art (Table 1 ) in historical documents. The method works in an unsupervised framework so it is writer invariant. It tack-les with multi-skewed, touching and horizontally overlapped lines.
 References
