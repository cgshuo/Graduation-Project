 Finding domain specific key terms/phrases from a given set of documents is a challenging task. A domain may be de-fined as an area of interest over a collection of documents which may not be explicitly defined but implicitly observ-able in those documents. When considering a collection of documents related to academic research, examples of key terms/phrases may be  X  X nformation Retrieval X ,  X  X arine Biol-ogy X , etc. In this paper a technique for extracting important key terms/phrases in a considered topical domain is pro-posed using external evidence from the titles of Wikipedia articles and the Wikipedia category graph. We performed some experiments over the docu ment collection of Web sites of different post-graduate schools. Our preliminary evalu-ations show promising results for the detection of domain specific key terms/phrases from the given set of domain fo-cused Web pages.
 H.3.3 [ Information Search and Retrieval ]: Clustering, Query formulation, Retrieval models, Search process; I.2.7 [ Natural Language Processing ]: Text analysis Theory, Design Open-domain knowledge, Wikipedia, community detection, n-gram model
A collection of text documents is concerned with a spe-cific knowledge domain implicitly observable within those documents. Finding the specific knowledge domain that a given text collection deals with is a significant research challenge. Several approaches have been proposed for the extraction of knowledge from text and the goal of current approaches is the extraction of key terms/phrases that de-scribe the document content. Most of these use statistical learning techniques, which have the potential limitation of requiring manual annotation [10, 17]. Another popular ap-proach includes the use of latent variable topic models such as latent dirichlet allocations. The drawback of these ap-proaches is that they are computationally expensive, and operate on full-length documents. On the contrary, to ad-dress the task of key phrase extraction we utilize short text snippets (i.e. titles) from Wikipedia articles related to a specific knowledge domain. A recent line of research in-vestigates the use of open-domain knowledge bases such as Wikipedia as an external knowledge resource for the task of key terms extraction. We use a similar methodology built upon n-grams overlap between Wikipedia articles and doc-ument (Web pages) titles. The resulting method demon-strates high precision and recall, as we will explain in later sections of the paper.

The rest of the paper is organized as follows. In Sec-tion 2 we describe the background by giving an overview of the related work. In Section 3 we discuss the underly-ing methodology for extraction of the key terms/phrases of a domain using Wikipedia articles and Wikipedia category graph. In Section 4 we present experimental evaluations along with their results. Section 5 concludes the paper with a discussion of future work.
Several approaches have been proposed in the literature to address the term/phrase extraction problem. Approaches based on the tf-idf model and its variations [13, 15] are the oldest ones. The research community then investigated su-pervised learning methods [17, 18] whereby training data is used to provide syntactic and lexical features for key terms/phrase extraction. A more recent line of research utilizes features extracted from open-domain knowledge re-sources such as Wikipedia for improving the accuracy of supervised learning based keyphrase extraction systems [11, 12]. However, supervised learning is a laborious task and is not desirable for Web scale data.

An alternative class of approaches applies graph-based se-mantic relatedness measures for extracting key terms and phrases [3, 9]. Some variants of these algorithms use the graph generated from a Wikipedia ontology [7, 3, 16]. How-ever, these techniques operate at document level instead of identifying domain specific key terms/phrases which is the focus of our work. Similar to these techniques, we propose to use graph-based semantic relatedness methods in com-bination with the Wikipedia category graph [19] to achieve better precision and recall.

The Information Retrieval research community is increas-ingly making use of the information richness in open-domain knowledge sources for improving the effectiveness of Web search applications. The use of open-domain knowledge re-sources has been investigated for query intent identification, document analysis and understanding, and query expansion [5, 6, 8]. To the best of our knowledge, this paper is the first attempt for (the task of) extracting domain specific key terms/phrases from short text (where standard NLP techniques fail). Furthermore, we differ from previous ap-proaches [3, 7, 16] in that we use the relationship between Wikipedia articles and Wikipedia categories for semantic re-latedness, and we apply the infomap [14] algorithm for com-munity detection.
In this section we discuss our method for extracting do-main specific key terms/phrases from a set of Web pages (in our case the Web pages crawled from University Web sites). To this aim, we first create indexes of the titles of Wikipedia articles and of the titles of the crawled Web pages, then we take intersection between the two indexes. Finally, we reduce valid n-grams to single terms in order to deter-mine single significant keywords. In the following subsec-tions the steps applied by the proposed extraction process are explained.
We create an index of the Wikipedia articles X  titles having first removed stopwords from the titles. We call this index Index wiki . Similarly we also create an index of the titles of Web pages after removing the stopwords, followed by the generation of possible n-grams (from 2 to 5 grams) per ti-tle. While generating this index, we maintain the frequency counts for each n-gram. We call this index Index web .
We apply the intersection between the n-gram index found from the titles of Web pages and the index of titles of Wikipedia articles ( Index wiki  X  Index web ) while preserving the fre-quency count which was observed during the generation of the Web pages index. However, at this stage we filter out n-grams that contain numeric values (such as  X 2 May X ) by assuming that they are non-representative of a topic. We call this index Index inter .
From the above index (i.e., Index inter ) we now filter out all the articles X  titles (or n-grams) that fall under the Wikipedia category of people. To this aim, we search for the word X  X eo-ple X  in the names of Wikipedia categories; matches such as  X  X iving people X  are found and all the article titles under this category are then removed. This intuition makes sense, as the article titles that are mentioned in the categories re-lated to people are possibly about people, and they will not represent a domain specific knowledge (instead they would represent some person). We call this index Index noppl which is a subset of Index inter .

To the aim of discovering domain specific knowledge we further apply a community detection algorithm. An undi-rected graph of the Wikipedia categories is generated. In Wikipedia, an article falls into one or more categories and a category may or may not have a super category (i.e., hi-erarchical category structure but not strict), and therefore, based on this we constructed a category graph (note that Wikipedia category graph is not strictly a taxonomy [19] as there are rare cycles in it). In this graph, a node represents a category of Wikipedia, edges represent relationships among the categories, and the weight on an edge is defined as the sum of the number of articles belonging to the super cate-gory and to the sub category node. The number of articles mentioned in the category is estimated by the number of article titles mentioned in the Index noppl (i.e., only the ones we discovered from previous step). For the sake of commu-nity detection, we apply the undirected multi-level infomap algorithm [14] whose predecessor was found to be the best-known algorithm for the community detection problem[2].
The application of the infomap algorithm to the gener-ated graph yields an assignment of each category to exactly one community. Some communities may contain many cat-egories while other communities may only contain one cat-egory. We then find top-k communities on the basis of the number of unique articles that they contain. For example, a community containing  X  X  X  categories (discovered after the application of the infomap algorithm) may contain  X  X  X  total articles and the community will also contain  X  X -x X  unique ar-ticles that are not mentioned inside any other community (i.e., only specific to the considered community). As a sim-ple example, an article on chemistry may be unique to the community that contains the categories related to chemical sciences and it would not be mentioned in other communi-ties such as the community that contains categories related to political sciences. Likewise if a community contains many unique articles then it becomes a strong representative of the domain of interest. Similarly, the less the number of unique articles in the community the less its chance to be represen-tative of the domain of interest. At worst a community may contain zero or one unique article that may turn out to be an outlier community (having little or no association with the considered domain). For example, a community containing only a unique article in the Wikipedia category  X 1979 births X  implies to be a random outlier (in the context of academic documents). Therefore, we declare top-k communities as being the most representative of the domain. These top-k communities contain several Wikipedia categories and each category contains articles. We assume articles contained in the top-k ranked communities as being relevant to the do-main of interest. Based on this we further reduce Index noppl and we call this subset Index comm .
Finally, we extend our algorithm to ease the discovery of important single terms instead of just n-grams (as in Index noppl ). To this aim, n-grams from Index noppl are re-duced to a list of single terms while preserving the frequency count of each term in the n-gram in such a way that none of the terms are over-counted. For example let us consider there were only two n-grams in the index;  X  X  b X  with fre-quency  X  X  X  and  X  X  c X  with frequency  X  X  X . Upon reducing to single terms we can say  X  X  X  and  X  X  X  occurs with  X  X  X  frequency. However we can X  X  say with certainty that  X  X  X  has  X 2n X  fre-quency as it may not be correct in the case if in a stream of data it occurs in the order  X  X  b c X . Therefore in order to overcome the problem of over-counting, we maintain posi-tional indexes with each n-gram (i.e., position of n-gram per Web page title). So now for the stream  X  X  b c X  the positional index of  X  X  X  is one, of  X  X  X  is two and of  X  X  X  is three, whereas for the discovered n-grams  X  X  b X  and  X  X  c X  there is a same posi-tional index for both the  X  X  X  terms therefore its frequency is counted one time (this aids in avoiding over-counting prob-lem).

Last, we lemmatize all the exploded terms in order to use one conceptual representation per term (e.g., sciences becomes science). Finally, all the exploded terms over all n-grams are arranged via frequency count and the term having highest frequency represents the most important key term.
In this section, we present the employed dataset and eval-uation measure followed by a discussion on the results ob-tained.
We crawled the English Web pages of eight post-graduate school Web sites from five different countries as shown in Table 1. For each Web site, we crawled up to depth five from the root page in order to cover at least 80%-95% of im-portant Web pages from a Web site as shown in [1]. In addi-tion to avoid crawler traps i.e., infinitely many dynamic Web pages such as calendars, we put a policy to crawl a maximum of first 500 instances of each dynamic Web page. We man-aged the policy by preserving the base url i.e., without pass-ing variables to a dynamic Web page, e.g. abc.com/a.asp instead of abc.com/a.asp?a=value.

To the aim of performing the evaluations we used the met-ric of Precision at top-k (P@k) results. P@k is defined by the ratio of correctly matching results over the total results within the first top-k results.
We conducted three experiments for the evaluation of do-main specific key terms/phrases. For each experiment, we asked a human annotator to manually label top-10 results being relevant or irrelevant for every run of test; this man-ual annotation provides us with P@k metric. The annotator labeled (the key) terms/phrases as relevant if they repre-sent certain domain concept (i.e., research area of interest) or represent a unique domain attribute associated with the school such as  X  X itzwilliam museum X  (attribute that may be of interest from research point of view).
 Table 2: Valid phrases in the top 10 results for Milano-Bicocca
Before conducting experiments, we produced three vari-ants of our algorithm that are explained as follows: ngrams: our algorithm without applying intersection be-tween indexes as described in Section 3.2. wiki nofilter: our algorithm without applying community detection as described in Section 3.2.2. wiki filter: our full algorithm described in Section 3.
In the first experiment we evaluated the precision of the top-10 (P@10) results for a single domain specific term for all schools. We then compared our three algorithms across BM25, classical TF-IDF and TF-Norm (term frequency nor-malized) weighting schemes. We used these as baselines on account of the observation that tf-idf forms a stronger base-line when compared across different datasets [4]. Figure 1 clearly shows that the wiki filter outperforms other algo-rithms, and only in one case it performed slightly worse than the best case. In generality wiki filter can be considered as the best algorithm for the detection of single domain specific term.
 Figure 1: P@10 for domain specific single term
In the second experiment, we evaluated the capability of our algorithms to generate high quality n-grams. Figure 2 shows that the wiki filter outperforms the other two algo-rithms. The reading for FAST-NU-KHI shows 0.0 P@10 for all algorithms because the FAST-NU-KHI Web site contains trivial (n-gram) title information.

In the last experiment, we evaluated the effectiveness of filtering out the noise in the first top-10 noisy n-grams de-tected by the wiki filter and the wiki nofilter algorithms. Note that we use the term noisy for the dropped n-grams after application of wiki filter and wiki nofilter. We found no error, i.e., P@10 for the detection of noise was 1.0 for each school X  X  Web site using both algorithms.
To provide an illustration of typical results, Table 2 shows extracted data of the Milano-Bicocca Web site. In this ta-ble, we can see P@10 top single key terms and top n-grams detected by wiki filter. Similarly, we can see top-10 noisy n-grams detected by wiki filter.
In this paper we presented an approach for finding do-main specific key terms/phrases using Wikipedia; the pre-liminary evaluations have shown promising results. In future work we would like to investigate the following issues: 1) to further investigate community detection algorithm(s) (for example edge weights assignment problem), 2) to incorpo-rate single term detection within n-gram model. Currently Wikipedia article titles such as  X  X iology X  or  X  X tatistics X  are not used in our technique due to complexity, 3) to investi-gate the effects of using more open-domain knowledge other than Wikipedia, and 4) to investigate the effect of includ-ing flat text from the body part of Web pages across the (current) title-only approach. [1] R. Baeza-yates and C. Cas tillo. Crawlin g the infinite [2] S. Fortunato. Community detection in graphs. Physics [3] M. Grineva, M. Grinev, and D. Lizorkin. Extracting [4] K. S. Hasan and V. Ng. Conundrums in unsupervised [5] J. Hu, G. Wang, F. Lochovsky, J. tao Sun, and [6] A. Jain and M. Pennacchiotti. Open entity extraction [7] M. Janik and K. J. Kochut. Wikipedia in action: [8] A. Kotov and C. Zhai. Tapping into knowledge base [9] Z. Liu, P. Li, Y. Zheng, and M. Sun. Clustering to find [10] Y. Matsuo and M. Ishizuka. Keyword extraction from [11] R. Mihalcea and A. Csomai. Wikify!: linking [12] D. Milne and I. H. Witten. Learning to link with [13] S. E. Robertson, S. Walker, M. Hancock-Beaulieu, and [14] M. Rosvall and C. T. Bergstrom. Multilevel [15] G. Salton and C. Buckley. Term-weighting approaches [16] P. P. Talukdar and F. Pereira. Experiments in [17] P. D. Turney. Learning algorithms for keyphrase [18] I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, [19] T. Zesch and I. Gurevych. Analysis of the Wikipedia
