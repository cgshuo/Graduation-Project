
This paper studies how to incorporate side informa-tion (such as users X  feedback) in measuring node prox-imity on large graphs. Our method (ProSIN) is moti-vated by the well-studied random walk with restart (RWR). The basic idea behind ProSIN is to leverage side infor-mation to refine the graph structure so that the random walk is biased towards/away from some specific zones on the graph. Our case studies demonstrate that ProSIN is well-suited in a variety of applications, including neighb or-hood search, center-piece subgraphs, and image caption. Given the potential computational complexity of ProSIN, we also propose a fast algorithm (Fast-ProSIN) that ex-ploits the smoothness of the graph structures with/without side information. Our experimental evaluation shows that Fast-ProSIN achieves significant speedups (up to 49x) over straightforward implementations.
Measuring proximity (i.e., relevance/closeness) between nodes on large graphs is a very important aspect in graph mining and has many real applications in ranking, anomaly nodes indentification, connection subgraphs, pattern matc h-ing, etc. Despite the successes of many previous work, most existing proximity measurements only consider the link structure of the underlying graph, ignoring any possib le side information. For example, given an author-conference bipartite graph, existing proximity measurements may an-swer the question: What are the most similar conferences to KDD? However, for a particular user, s/he might have her/his own preferences: I dislike ICML or I like SIGIR . These preferences are typically localized to a particular search, and may not reflect a global sentiment by the user.
There are a wide range of scenarios where users X  feed-back, both implicit or explicit, can be naturally integrate d as side information. For instance, in recommendation systems , side information could be users X  ratings on items (e.g., I like Kung-Fu Panda ). In Blog analysis, it could be opinions and sentiments. Additionally, for many real applications, use rs X  preferences can be estimated from click-through data. That said, it is thus important to incorporate such side informa-tion in the proximity measurement so that search results are well-tailored to reflect a user X  X  individual preferences. I n the earlier example, the question will then become: What are the most similar conferences to KDD, but dissimilar to ICML?
In this paper, we address the above challenge by propos-ing a novel method, called ProSIN, that incorporates such like/dislike side information in measuring node proximity on large graphs. Our method is based on random walk with restart (RWR), where ProSIN uses the side information to refine the graph structure so that RWR is biased to avoid or to favor some specific zones on the graph according to the users X  preferences. Additionally, ProSIN inherits exi st-ing capabilities from RWR, such as the ability to summarize the multiple faceted relationships, to be interpreted from the perspective of steady-state probability , etc. Therefore, we expect ProSIN to enrich a broad-range of applications by replacing their original proximity measurement implemen-tation. We evaluate ProSIN in three case studies: neighbor-hood search, center-piece subgraph, and image caption. In all cases, we show that ProSIN naturally reflects the users X  preference and/or improves the quality of the existing ap-plications (e.g., boost the precision/recall of the image c ap-tions by more than 10%).

Because a straightforward implementation of ProSIN re-quires significant computation, we propose a fast algorithm (Fast-ProSIN) that computes the proposed proximity mea-surement, while radically reducing the computational over -head. Fast-ProSIN achieves the performance gains by ex-ploiting the smoothness of the graph structures with/witho ut side information. Our experimental results show that it achieves significant speedup ( up to 49x ) while maintaining high approximation accuracy (more than 93.0%).

The paper has three key contributions:  X  A novel method (ProSIN) to incorporate side infor- X  A fast algorithm (Fast-ProSIN) to compute the pro- X  Extensive experimental evaluations on several real Symbol Definition and Description A , B , . . . matrices (bold upper case) A ( i, j ) element at the i th row and j th column of A A ( i, :) i th row of matrix A
A (: , j ) j th column of matrix A a , b , . . . column vectors
I , J , . . . sets (calligraphic) n number of nodes in the graph n i number of out links of node i c (1  X  c ) is the restart probability r r P positive set P = { x 1 , ..., x n + }
N negative set N = { y 1 , ..., y n n n  X  number of negative nodes n  X  = |N| e i n  X  1
The rest of the paper is organized as follows. We in-troduce notations and formally define the problem in Sec-tion 2 . We present the proposed proximity measurement in Section 3 and the fast algorithm in Section 4 , respectively. We provide experimental evaluations in Section 5 and re-view the related work in Section 6 . Finally, we conclude in Section 7 .
Table 1 lists the main symbols that we use throughout the paper. We represent a general graph by its adjacency matrix. Following the standard notation, we use capital let -ters for matrices (e.g. A ), lower case for vectors (e.g. and calligraphic fonts for sets (e.g. I ). We use the symbol  X   X   X  to distinguish the setting with/without side information . For example, A is the normalized adjacency matrix of the graph without side information; and  X  A is the normalized adjacency matrix of the refined graph by side information.
We represent the elements in a matrix using a conven-tion similar to Matlab, e.g., A ( i, j ) is the element at the row and j th column of the matrix A , and A (: , j ) is the column of A , etc.

We use a running example, depicted in Fig. 1 (a), to de-scribe the problem statement. There, each node represents and the existence of edge represents some social contact be-tween the two corresponding persons (e.g., phone call). In traditional settings of proximity measurement, the goal is to quantify the closeness (i.e., relevance) between two nodes (the source and target) based on the link structure of the un-derlying graph. In our settings, we assume the existence of side information, focusing primarily on like/dislike us er feedback as side information. In our running example, a user might not want to see (i.e., dislike) node 6 but favors (i.e., like) node 4.
 Formally, we represent such side information by two sets P and N . The set P contains the node indices that users like (referred to as the positive set), where the correspond -ing nodes are referred as positive nodes. The set N contains the node indices that users dislike (referred as negative se t), where the corresponding nodes are referred to as negative nodes. In our running example, both the positive set P and the negative set N contain one single element, respectively: P = { 4 } and N = { 6 } . Our goal is to incorporate such side information to measure the node proximity (e.g., the proximity from node 1 to the node 3 in our running exam-ple).
With the above notations and assumptions in mind, our problem can be formally defined as follows: Problem 1 (Proximity with Side Information) Given: a weighted direct graph A , a source node s and a Find: the proximity score  X  r
In problem 1 , if the target node t is absent, we measure the proximity score  X  r s to all the other nodes in the graph. If we stack all these scores into a column vector  X r equivalent to saying that we want to compute the ranking vector  X r that there is no overlap between the positive set and negativ e set (i.e., P  X  N =  X  . 1 ) Also, the positive and negative side information do not need to exist simultaneously. For example, if we only have positive side information, we can simply set the negative set to be empty (i.e., N =  X  ).
In this section, we introduce our proximity measurement with side information, ProSIN. We begin by reviewing ran-dom walk with restart (RWR), which is a good proximity measurement for the case where there is no side informa-tion. We, then, extend RWR to properly account for side information.
Random walk with restart (RWR) is considered one of the most successful methods for measuring proximity and is receiving increased interest in recent years X  X ee Sectio n 6 for a detailed review. For a given graph, RWR is defined as follows. Consider a random particle that starts from node i . The particle iteratively transits to its neighbors with pr ob-abilities proportional to the corresponding edge weights. At each step, the particle can returns to node i with some restart probability (1  X  c ) . The proximity score from node i to node j is defined as the steady-state probability r ticle will be on node j [ 18 ]. Intuitively, r of time that the particle starting from node i will spend on each node j of the graph, after an infinite number of steps.
If we stack all the proximity scores r (referred to as the ranking vector for the node i ), the equa-tion ( 1 ) gives the formal definition of RWR: where A is the column normalized adjacency matrix for the graph and e
For our running example in Fig. 1 (a), its normalized ad-jacency matrix A is shown in Fig. 1 (b). If we ignore any side information, by setting the correct starting vector (e .g., e 1 = [1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0]  X  solve the corresponding ranking vector using equation ( 1 ). Fig. 2 (a) plots the ranking vector (sorted from highest to lowest) for node 1 of the running example. The scores are consistent with our intuition: nearby nodes (e.g., nodes 9, 2 and 5) receive higher proximity scores.
Basic Ideas. Our goal is to incorporate side informa-tion to measure the node proximity. Intuitively, for a given source node s , if positive nodes exist, the proximity score from the source node to such positive nodes as well as their neighboring nodes should increase, compared to the case where such side information is unavailable. In our running example, if we know node 4 belongs to the positive set P , we expect that the proximity score from the source node 1 to node 4 to increase and so will the proximity scores from node 1 to node 4 X  X  neighboring nodes (e.g., node 2 and node 3). Analogously, if negative nodes exist, the prox-imity scores from the source node to such negative nodes as well as their neighboring nodes should decrease, compared to the case where such side information is unavailable. In our running example, if we know that node 6 belongs to the negative set N , we expect the proximity score from node 1 to node 6 to decrease, and so will node 6 X  X  neighboring nodes (such as nodes 5 and 7).

The basic idea of ProSIN is then to use side information to refine the original graph structure so that the random par-ticle (1) has higher chances of visiting the positive nodes a s well as their neighboring nodes, and (2) has lower chances of visiting the negative nodes as well as their neighboring nodes.
Figure 3. Adjustment on the original graph in the running example in Fig. 1 .

Dealing with Positive Nodes. For each node x in the positive set ( P ), we create a direct link from the source node s to node x . As in the running example, we add a direct link from the source node 1 to node 4 (See Fig. 3 (a)). In this way, whenever the random particle visits (or restarts from) the source s , it has higher chances of visiting the nodes in the positive set. Note that we are also implicitly increasing th e normalized so that they sum up to 1.) chance that the random particle will visit the neighborhood of those positive nodes. The weight of each newly added link is set to 1 / ( n s + n + ) . For example, the newly added edge (1,4) for the running example will receive a weight of 0.25 (since n 1 = 3 and n + = 1 ).

Dealing with Negative Nodes. To deal with the nega-tive nodes, we introduce a sink into the graph, which has no out links. For each node y in the negative set ( N ), we put a direct link from node y to the sink. Thus, whenever the random particle visits this node, it can go to the sink and never comes back (since there is no out links from the sink). Therefore, this negative node y is penalized and its corresponding proximity score will decrease. In order to penalize the neighborhood of node y , we also put a direct link from its neighboring nodes to the sink. In our running example, besides the link from node 6 (the negative node) to the sink, we placed a link from nodes 5 and 7 (the neighbor-ing nodes of node 6) to the sink respectively (see Fig. 3 (a)).
There are two remaining questions: (1) how to choose the neighborhood of a negative node y and (2) how to de-termine the weights to the sink. Let the index of the sink node be n + 1 , the procedure is summarized in Alg. 1 . In Alg. 1 , we use random walk with restart (on the original graph) to determine (1) the neighborhood of the negative node y (steps 2-4), and (2) the weights of the newly added links to the sink (steps 5-6). Notice that we eventually (ste p 9) discard the last row/column (which corresponds to the sink node). We use it to simplify the description of the pro-posed method without affecting the ranking vector in accord to the property of a sink node.

ProSIN Algorithm. Based on the above preparations, the complete algorithm to measure proximity with side in-formation (ProSIN) is given in Alg. 2 . In Alg. 2 , after ini-tialization (step 1), we first use side information to refine 8-12 for negative nodes). Note that in step 10, we use the same A (i.e., the original graph) to add links for each nega-tive node y . This is because we assume that all the negative Algorithm 1 Add Links for One Negative Node Input: The adjacency matrix A , the negative node y , the Output: The updated adjacency matrix  X  A . 2: get the ranking vector for the negative node y by r y = 3: for each node i do 5: set  X  A ( n + 1 , l ) = r y,i / r y,y 6: set  X  A (1 : n, i ) = (1  X  r y,i / r y,y )  X  A (1 : n, i ) 7: end if 8: end for 9: output  X  A =  X  A (1 : n, 1 : n ) . nodes are obtained in a batch mode (i.e., there is no order-ing among different negative nodes). Then, we perform a random walk with restart on the refined graph (  X  A ) for the source node s (step 13) and output the corresponding steady state probability as the proximity score (step 14). For exam -ple, Fig. 2 (b) plots the ranking vector (sorted from highest to lowest) for node 1 of the running example with side in-foramtion ( P = { 4 } , and N = { 6 } ). Compared to the case without side information (Fig. 2 (a)), it can be seen that positive node (node 4) as well as its neighborhood (nodes 2 and 3) receives higher proximity scores; while the negative node (node 6) as well as its neighboring nodes (nodes 5 and 7) receives lowers scores. In this section, we introduce our fast solution for ProSIN. We start by reviewing NB LIN, which is a fast algorithm to compute random walk with restart (the proximity without side information) [ 25 ]. We then extend it to include side information.
According to the definition (equation ( 1 )), we need to in-vert an n  X  n matrix. This operation is prohibitively slow for Algorithm 2 ProSIN Input: The adjacency matrix A , the source node s and the Output: the proximity score  X r 1: initialize  X  A = A 2: if n + &gt; 0 then 3:  X  A (: , s ) = n s / ( n s + n + )  X  A (: , s ) 4: for each positive node x in P do 5:  X  A ( x, s ) =  X  A ( x, s ) + 1 / ( n s + n + ) . 6: end for 7: end if 8: if n  X  &gt; 0 then 9: for each negative node y in N do 10: update  X  A by Alg. 1 11: end for 12: end if 13: solve the equation  X r s = c  X  A  X r s + (1  X  c ) e s 14: output  X r s,t =  X r s ( t ) . large graphs. On the other hand, the iterative method (iter-ating equation ( 1 ) until convergence) might need many iter-ations, which is also not efficient. In [ 25 ], the authors solve this problem using a low-rank approximation, followed by a matrix inversion of size l  X  l (where l is the rank of the low-rank approximation) to get all possible proximity scores. Their solution, called NB LIN, is the starting point for our fast algorithm.

Alg. 3 summarizes NB LIN, where it is divided into two stages: NB LIN Pre() and NB LIN OQ() . In NB LIN Pre() (steps 1-3), a low-rank approximation is performed for the normalized adjacency matrix A and a matrix inversion  X  is computed. Next, in NB LIN OQ() (steps 4-5), only a small number of matrix-vector multipli-cations are computed to output the ranking vector. Algorithm 3 NB LIN Input: The normalized adjacency matrix A , the source Output: The ranking vector for source node r 1: Pre-Compute Stage ( NB LIN Pre() ) 2: do low-rank approximation for A = USV 3: pre-compute and store the matrix  X  = ( S  X  1  X  c VU )  X  4: On-Line Query Stage ( NB LIN OQ() ) 5: output r s = (1  X  c )( e s + c U X Ve s )
To incorporate side information, we need to solve ran-dom walk with restart in two places. First, we process the original graph A (step 10 in Alg. 4 ); and then we pro-cess the refined graph  X  A to get the ranking vector for the source node s (step 13 in Alg. 4 ). If we utilize NB LIN in a straightforward way, we have to call it twice (for and for  X  A , respectively). Unfortunately, this does not fit the expect usage model of side information, where it needs to reflect users X  real-time interests. Imagine a user is query-ing an author-conference bipartite graph, and s/he wants to know which conferences are most similar to KDD . After the system gives the initial search results, s/he might furt her give her/his own preference (e.g., dislike ICML ) and expect updated search results that matches her/his interests. Thi s basically implies that calling NB LIN Pre() on the refined graph  X  A is part of the on-line cost, which may pose a huge threat to the system X  X  performance.
 Algorithm 4 Fast-ProSIN Input: The adjacency matrix A , the source node s , the side Output: the ranking vector  X r 1: Pre-Compute Stage 2: call [ U ,  X  , V ] = NB LIN Pre( A , c ) 3: On-Line Query (Feedback) Stage 5: for each negative node y in N do 6: call r y = NB LIN OQ( c, U ,  X  , V , e y ) . 8: for each node i s.t. r y,i &gt; =  X  do 9: set  X  ( i 0 , 1) = i and  X  ( i 0 , 2) = 1  X  r y,i / r y,y 10: increase i 0 by 1 11: end for 12: end for 13: set  X  ( i 0 , 1) = s and  X  ( i 0 , 2) = n s / ( n s + n 14: set  X  U = U and  X  V = V 15: for i = 1 : kn  X  + 1 do 16: set X ( i, :) = U (  X  ( i, 1) , :) 17: set Y (: , i ) = V (: ,  X  ( i, 1))(  X  ( i, 2)  X  1) 18: set V (: ,  X  ( i, 1)) = V (: ,  X  ( i, 1))  X  ( i, 2) 19: end for 20: compute L = ( I  X  c X X Y )  X  1 21: update  X   X  =  X  + c  X YLX X  22: set e + = 0 n  X  1 , e + ( P ) = 1 / ( n s + n + ) 23: call  X r s = NB LIN OQ( c,  X  U ,  X   X  ,  X  V , e s ) 24: call u = NB LIN OQ( c,  X  U ,  X   X  ,  X  V , e + ) 25: output  X r s =  X r s + c  X r s ( s ) / (1  X  c  X  c u ( s )) u
To deal with such challenge, we propose Fast-ProSIN, which is given in Alg. 4 . Here, we assume that we want the whole ranking vector for a given source node s since a single proximity score can be read out from such rank-ing vector. Also, we consider the most general case, where both positive nodes and negative nodes are present. In Fast-ProSIN, it first calls NB LIN Pre() on the original adja-cency matrix A (step 2). Then it calls NB LIN OQ() to determine the influence of the negative nodes (steps 5-12) and partial influence (i.e., scaling the s th column of the ad-nodes (step 13), both of which are used to update the low-rank approximation (  X  U and  X  V ) as well as matrix  X  14 -21). This way, it avoids directly calling the function NB LIN Pre() on the refined graph  X  A , where it would need to do a low-rank approximation and a matrix inversion, both of which are not efficient as on-line costs. Finally, it calls NB LIN OQ() twice (steps 23-24) and combines them as the final ranking result (step 25). Note that the second call on e fluence of the positive nodes (i.e., adding new links from the source to the positive nodes).

The correctness of Alg. 4 is guaranteed by theo-rem 1 . By theorem 1 , Fast-ProSIN will not introduce ad-ditional approximation errors beyond the first time it calls NB LIN Pre() on the original graph. Therefore, Fast-ProSIN is expected to obtain ranking results similar to call -ing NB LIN Pre() twice (one for A and the other for  X  A ). On the other hand, Fast-ProSIN avoids the expensive steps (low-rank approximation on  X  A and a matrix inversion of size l  X  l ) in calling NB LIN Pre() . This, as we will show, leads to significant on-line running cost savings. Theorem 1 Correctness of Fast-ProSIN. If A = USV holds, then Alg. 4 gives the correct ranking vector for the source node s .
 Proof: let an n  X  n matrix  X  A s.t.,  X  A (: ,  X  ( j, 1)) = A (: ,  X  ( j, 1))  X  ( j, 1) ( j = 1 : kn  X  + 1)
First, we will show that  X r ranking vector on the matrix  X  A if A = USV holds.
By the construction of matrix  X  A , we have  X 
Thus, in the matrix form, we have  X  A =  X  US  X  V , where the matrices  X  U and  X  V are as defined in steps 14-19 in Alg. 4 .
Define the matrix  X  Q = (1  X  c )( I  X  c  X  A )  X  1 . By the prop-erty of NB LIN algorithm [ 25 ], we have where  X   X  = ( S  X  1  X  c  X  V  X  U )  X  1 .
 Next, we will relate  X   X  with the matrix  X   X  (step 21 of Alg. 4 ).

By the spectral representation, we have the following equation: where  X  satisfies  X  = where the matrices Y and X are defined as steps 16-17 of Alg. 4 .

Plugging equations ( 5 ) and ( 6 ) into the matrix  X   X  and ap-plying Sherman-Morrison Lemma [ 19 ], we have where the matrices  X   X  and L are defined as steps 20-21 of Alg. 4 .

Plugging equation ( 7 ) into equation ( 4 ), we can verify the  X r
Next, define the matrix  X  Q = (1  X  c )( I  X  c  X  A )  X  1 will try to relate  X  Q with matrix  X  Q .

By the construction of  X  A and  X  A , we have where vector e there is only a rank-1 difference between  X  A and  X  A .
Now, applying Sherman-Morrison Lemma [ 19 ] to  X  Q , we have where vector u is defined as in step 24 and the scale b satis-fies
Putting equations ( 7 ), ( 10 ) and ( 11 ) together, we have that the correct ranking vector for the source node s on ma-trix  X  A must satisfies: where  X r of theorem 1 .
In this section we present experimental results. All the experiments are designed to answer the following ques-tions:  X  Effectiveness: What data mining observations does the  X  Efficiency: How does the proposed Fast-ProSIN bal-
Datasets. We use three datasets in our experiments, which are summarized in Table 2 .

The first dataset ( AC ) is from DBLP. 3 It is an author-conference bipartite graph, where each row corresponds to an author and each column corresponds to a conference. An edge weight is the number of papers that the corresponding author publishes in the corresponding conference. On the whole, there are 421,807 nodes (418,236 authors and 3,571 conferences) and 1,066,816 edges in the graph.

The second dataset ( ML ) uses author-paper information from two major machine learning conferences ( X  X IPS X , and  X  X CML X ) to construct a co-authorship graph, where each node represents an author and an edge weight is the num-ber of co-authored papers between any two corresponding authors. On the whole, there are 4,563 nodes and 20,469 edges.

The third dataset ( CoMMG ) is used in [ 18 ], which con-tains around 7,000 captioned images, each with about 4 cap-tioned terms. There are in total 160 terms for captioning. In our experiments, 1,740 images are set aside for testing. The graph matrix is constructed exactly as in [ 18 ], which con-tains 54,200 nodes and 354,186 edges.
Parameter Settings. There are two parameters in the proposed ProSIN: c for random walk with restart, and for the neighborhood size of a given negative node. We set c = 0 . 95 (as suggested in [ 25 ]). To determine k , a para-sensitivity to k for a large range of settings (from k = 2 k = 10 ). For the experimental results in this paper, k is set to be 5.
 we report the wall-clock time. All the experiments ran on the same machine with four 3.0GHz Intel (R) Xeon (R) CPUs and 16GB memory, running Linux (2.6 kernel). For each experiment, we run it 10 times and report the average.
In both the proposed ProSIN and the original random walk with restart, the proximity score is defined as the steady-state probability . Therefore, we expect it to enric h a broad range of applications by replacing the original ran-dom walk with restart with our ProSIN. In this subsection, we present three applications as case studies: neighborhoo d search, center-piece subgraphs, and image caption.
Neighborhood Search. By incorporating the users X  feedback, we can allow interactive neighborhood search on the graph. Fig. 4 gives one such example, where we want to find the top 10 neighbors of  X  X DD X  conferences (i.e, the 10 most similar conferences as  X  X DD X ) from the AC dataset. In Fig. 4 (a), we plot the initial results when there is no side information (i.e, P =  X  and N =  X  ). Sub-jectively, the result makes sense, which reflects two ma-jor sub-communities in  X  X DD X : the AI/statistic community (e.g.,  X  X CML X ,  X  X IPS X , and  X  X JCAI X ) and the databases com-munity (e.g.,  X  X IGMOD X ,  X  X LDB X ,  X  X CDE X  etc). Then, if the user gives negative feedback on  X  X CML X  (i.e, P =  X  and N = {  X  ICML  X  } ), all the AI/statistic related confer-ences ( X  X IPS X  and  X  X JCAI X ) disappear (See Fig. 4 (b)). In Fig. 4 (c), we present the updated result if the user fur-ther gives some positive feedback on  X  X IGIR X , which is one of the major conferences on information retrieval. Again, the result confirms the effectiveness of ProSIN: positive feedback on  X  X IGIR X  brings more information retrieval re-lated conferences (e.g,  X  X REC X ,  X  X IKM X ,  X  X CIR X , X  X LEF X ,  X  X CL X ,  X  X CDL X , etc).

Center-Piece Subgraphs. The concept of connec-tion subgraphs, or center-piece subgraphs, was proposed in [ 7 , 22 ]: Given Q query nodes, it creates a subgraph that shows the relationships between the query nodes. The resulting subgraph should contain the nodes that have stron g connection to all or most of the query nodes. Moreover, since this subgraph H is used for visually demonstrating node relations, its visual complexity is capped by setting a n upper limit, or a budget on its size. These so-called con-
Figure 4. Interactive neighborhood search for  X  X DD X  conference. nection subgraphs (or center-piece subgraphs) were proved useful in various applications, but currently cannot handl e users X  interaction (i.e, feedback).

One of the building block in the original center-piece subgraphs [ 22 ] is to use RWR to measure the proxim-ity from the query nodes to the remaining nodes on the graph. Therefore, by replacing the original random walk with restart by the proposed ProSIN, we can naturally deal with the users X  interactions (for details of center-piece s ub-graph, please refer to [ 22 ]).

Fig. 5 plots an example to find the center-piece sub-graphs between two researchers ( X  X ndrew Mccallum X  and  X  X iming Yang X ) from ML dataset. In Fig. 5 (a), we plot the initial results when there is no side information (i.e, P =  X  and N =  X  ). It can be seen that there are two major connec-tions between  X  X ndrew Mccallum X  and  X  X iming Yang X : one connection is on text mining/information retrieval (throu gh  X  X ebecca Hutchinson X ,  X  X uerui Wang X ,  X  X om M. Mitchell X ,  X  X ean Slattery X  and  X  X ayid Ghani X ), and the other con-nection in on AI/statistics (throught  X  X ohn D. Laffterty X ,  X  X oubin Ghahramani X  and  X  X ian Zhang X ). Fig. 5 (b) gives the updated result if the user gives negative feedback on  X  X om M. Mitchell X . It can be seen that the whole connection on text mining/information retrieval disappears, and more connection on AI/statistics (e.g. through  X  X ndrew Ng X  and  X  X ichael I. Jordan X ) shows up.

Image Caption. Here, the goal is to assign some key-words for a given image as its text annotation. In [ 18 ], the authors proposed a graph based solution and showed its su-periority over the traditional methods in feature space. Th e key idea of [ 18 ] is to construct an image-keyword-region graph and use RWR to measure the relevance between the test image and the known keywords. Similar to center-piece subgraphs, replacing RWR by ProSINcan easily incorporate side information (if available) in such process.

Fig. 6 presents the average precison/recall on CoMMG dataset. Here, the side-information is simulated as follow -ing: for each test image, 5 keywords that are most rele-vant to the test image based on the current proximity mea-surement are returned for users X  yes/no (i.e., correct/wro ng
Figure 5. Interactive center-piece subgraphs between  X  X ndrew Mccallum X  and  X  X iming
Yang X . caption) confirmation. Here, we also compare two sim-ple strategies: (1)  X  X emNeg X , where the negative nodes are simply removed from the graph; and (2)  X  X inCom X  [ 13 ], where the proximity scores from positive/negative nodes ar e added/substracted from the score from the test image. From the figure, it can be seen that our ProSIN largely improves both precision/recall for image caption task by incorporat -ing such side information. For example, it improves the precision by 13.59% (44.02% vs. 30.43%) and the recall by 17.39% (57.54% vs. 40.15%) when the prediction length is 4. It is interesting to notice that if we simply remove the negative nodes from the graph, it will actually hurts the per -formance ( X  X emNeg X ). As for  X  X inCom X , it can be seen that (1) the improvement is limited compared with the proposed ProSIN for short prediction length; and (2) it might hurt the performance with the increase of the prediction length.
In this subsection, we study the quality/speed trade-off of the proposed Fast-ProSIN. We use the CoMMG dataset (since it is the only one with ground truth among the three datasets we used in this paper). Here, we fix the predic-tion length to be 4 (the results with other prediction length are similar and therefore skipped for brevity), and we com-pare the precision/recall between Fast-ProSIN and ProSIN where in ProSIN random walk with restart is performed by more parameter in Fast-ProSIN, the rank of the low-rank approximation for NB LIN Pre() . We vary this parame-ter from 100 to 600 (denoted as Fast-ProSIN(100), Fast-
Figure 6. Incorporate side information for im-age caption.
 Figure 7. Quality/speed trade-off of Fast-ProSIN.
 ProSIN(200), etc in Fig. 7 ). In order to put quality/speed in the same figure, we normalized (1) precision/recall by the largest value for ProSIN, and (2) time by the longest value for ProSIN.

From Fig. 7 , it can be seen that the proposed Fast-ProSIN achieves significant speedup while maintaining high qual-ity. For example, Fast-ProSIN(100) is 49x faster than ProSIN (the most right one) while it preserves 93.6% precision (41.2% vs. 44.0%) and 94.0% recall (54.1% vs. 57.5%); Fast-ProSIN(400) is 16x faster than ProSIN while preserving 96.1% precision (42.4% vs. 44.0%) and 96.7% recall (55.6% vs. 57.5%). Overall, Fast-ProSIN is 10  X  49x faster than ProSIN, while preserving more than 93.0% quality (for both precision and recall). Note that in all cases, Fast-ProSIN significantly improves the preci-sion/recall when compared with the initial case (the left-most dashed bar). As for the wall-clock time, ProSIN need 3.7 hours to annotate all the 1,740 images, while Fast-ProSIN(100) only needs 4.5 minutes.
In this section, we review the related work, which can be categorized into two parts: node proximity and matrix low rank approximation.

Node Proximity. One of the most popular proximity measurements is random walk with restart [ 13 , 18 , 25 ], which is the baseline of ProSIN. Other representative prox-imity measurements include the sink-augmented delivered current [ 7 ], cycle free effective conductance [ 16 ], surviv-able network [ 10 ], and direction-aware proximity [ 24 ]. All these methods only consider the graph link structure and ig-nore the side information. Although we focus on random walk with restart in this paper, our approach (i.e., to use th e side information to refine the graph structure) can be applie d to other random walk-based measurements, such as [ 7 , 24 ]. In term of dealing with the side information on ranking, our work is also related to [ 3 ], where the goal is to use partial order information to learn the weights of different types of edges. In term of computation, the fast algorithm (NB LIN) for random walk with restart in [ 25 ] is most related to the proposed Fast-ProSIN. Our Fast-ProSIN differs from that in [ 25 ] in the sense that the graph structure in our setting keeps changing by the side information, whereas it is fixed in [ 25 ]. The core idea behind the proposed Fast-ProSIN is to lever-age the smoothness between graph structure with/without side information. In [ 26 ], the authors has used the simi-lar idea to track the proximity/centrality on a time-evolvi ng skewed bipartite graph. Other remotely related work in-cludes [ 11 ], where the goal is to propagate the trust/distrust to predict the trust between any two persons.

Graph proximity is an important building block in many graph mining settings. Representative work includes con-nection subgraphs [ 7 , 16 , 22 ], neighborhood search in bipar-tite graphs [ 20 ], content-based image retrieval [ 13 ], cross-modal correlation discovery [ 18 ], the BANKS system [ 2 ], link prediction [ 17 ], pattern matching [ 23 ], ObjectRank [ 4 ], RelationalRank [ 8 ] and recommendation system [ 5 ]. Note that for the ranking-related tasks (such as neighborhood search, image retrieval, etc.), we can also use a linear com-bination strategy suggested in [ 13 ], which itself includes personalized PageRank [ 12 ] as a special case when neg-ative set is absent, to incorporate like/dislike type of sid e information. Our experimental evaluation on image cap-tion task shows that although it is effective for small predi c-tion lengths, its performance is not as good as the proposed ProSIN and sometimes it actually hurts the performance. What is more important, it is not clear how to use such strat-egy (linear combination) for more complicated application s (such as center-piece subgraphs, pattern match etc). This i s exactly one major advantage of the proposed ProSIN: it can be easily plugged into such applications by simply replac-ing the original proximity measurement by our ProSIN.
Low Rank Approximation. Low rank approxima-ing. For example, the low rank approximation structure is often a good indicator to identify the community in the graph. A significant deviation from such structure often implies anomalies in the graph. For the proposed Fast-ProSIN, we need the low rank approximation in the pre-computational stage (in function NB LIN OQ() ). The most popular choices include SVD/PCA [ 9 , 15 ] and random projection [ 14 ]. More recent methods includes CUR [ 6 ] and its improved version CMD [ 21 ] to deal with the sparseness of many real graphs. Notice that our Fast-ProSIN is orthog-onal to the specific method of low rank approximation.
In this paper, we study how to incorporate like/dislike type of side information in measuring node proximity on large graphs. Our main contributions are in two folds. First , we proposed a novel method (ProSIN) to incorporate side information in measuring node proximity on large graphs and showed its broad applicability through various case studies. Second, to enhance the efficiency of ProSIN, we also took advantage of the smoothness of the graph struc-tures with/without side information and proposed a fast al-gorithm (Fast-ProSIN). We demonstrated that Fast-ProSIN achieves significant speedup (up to 49x) in our evaluation on real datasets. Overall, we expect the proposed algorithm s to enrich a broad range of applications that receive online feedback/side information.

