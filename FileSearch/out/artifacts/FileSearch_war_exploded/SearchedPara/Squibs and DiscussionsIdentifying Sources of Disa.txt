 University of Oklahoma
Many annotation projects have shown that the quality of manual annotations often is not as good as would be desirable for reliable data analysis. Identifying the main sources responsible for poor annotation quality must thus be a major concern. Generalizability theory is a valuable
Theory and give an example for its application based on published data. 1. Introduction
Manual annotations are still a major source of information in many small-and large-scale projects in diverse areas of corpus and computational linguistics. Often, however, manual annotations are not reliable enough for a given application. Measures must and applications are not to suffer from low data quality. Because a multitude of fac-tors may be responsible for inadequate reliability, a method is needed that is able to simultaneously consider a variety of probable factors and indicate those that are mainly (Cronbach et al. 1972), is a methodological framework specifically designed for this purpose. Because it is not restricted to any type of data or study design, it can be of great use in any kind of manual annotation project that needs to systematically identify sources of annotator disagreement. In this article we provide an outline of the approach and its basic assumptions and demonstrate its application based on an annotation study done by Shriberg and Lof (1991). 1 2. The G-Theory Approach
Reliability in G-Theory is defined by the amount of variation or variance observed in annotations; the lower the total variance in the data, the higher is its reliability. G-Theory variation. 2 Sources of variation might be idiosyncratic behaviors of individual annota-tors or external influences like alterations in the tools used for annotations, increasing time pressure, removal or adding of rewards, or changes in the annotation scheme. Each of these influences can lead to systematic changes in an annotator X  X  behavior and so to higher disagreement among annotators. According to G-Theory each possible facet, annotator , tools , rewards , and so on, will have its own independent impact on the quality, single facets and determine the degree of their impact. 2.1 Basic G-Study Designs
The main distinction with respect to G-study designs is the choice between a crossed and a (partially) nested design. In crossed designs measurements are obtained for each possible combination of facet values. Given two facets, items and coders , each that each value of the item facet is measured on every value of the coders facet. Nested for instance, when limited resources determine that only some of the coders annotate the same objects on more than one occasion. In general, fully crossed designs require a higher number of observations, but also provide more information. To obtain a full picture of possible influences crossed designs should therefore be preferred. For a detailed discussion of G-study designs, including unbalanced designs or missing data, and random and fixed facets, see, for example, Brennan (2001). 2.2 Estimating Variance Components
In fully crossed designs the total variance in the data is a result of individual facets as well as their interactions. Because G-Theory assumes independence of facets, effects of components are additive. Given three facets a , b , c , the total variance  X  is calculated as where  X  2 refers to variance and the subscripts to the name of one or more facets.
The subscript e in the last variance component denotes error variance. In nested de-signs some facets cannot be determined as independent terms due to their confounding 4 values of c may be associated with different values of b . Here the effect for c will be confounded both with bc and the residual term abc , e so that no independent term for the c facet can be obtained. Instead of the seven variance components in the crossed design only five variance components can be calculated, again stressing the fact that nested designs provide less information than fully crossed designs
For information on the mathematical foundation of G-Theory and the derivation of estimates see Cronbach et al. (1972) and Brennan (2001). 2.3 Interpreting Variance Components
Based on the assumption that the total variance is a sum of single variance components, the total variance is 100%. The relative magnitude of each component with respect to the total variance is an indicator of the individual contribution of this component with respect to overall (un)reliability. A facet explaining 60% of the total variance would thus be considered a major source of variation in contrast to a minor facet explaining only 5% of the variance. For instance, given that the coder facet is the largest facet, variation can be explained through systematic differences in the annotation behavior of individual coders X  X or example, annotators differ in their tendency to set prosodic boundaries in utterances leading to systematic differences in the number of boundaries placed. In this case retraining of annotators to reach a more comparable behavior would be advisable.
Ahigh schema component indicates that there is systematic variation in the use of categories, whereas a high coder X  X chema interaction indicates systematic differences in annotators X  use of these categories; for example, coders annotating rhetorical (RST) relations could differ in the frequency with which they use individual relations such as  X  X ackground X ,  X  X oncession X ,  X  X vidence X , and so forth, pointing to possible problems with the interpretation of rhetorical relations and their application. Variation mainly due to the item facet indicates that certain materials are harder to annotate than others. Such a result would imply retraining or elimination of overly difficult material. In consequence, the identification of distinct sources of variation should lead to specifically designed steps for improvement. 3. A Re-Analysis of Shriberg and Lof (1991) As an illustration for the application of G-Theory we reanalyzed data provided by
Shriberg and Lof (1991), who studied the accuracy of broad and narrow phonetic tran-scriptions. In Set A of their study they investigated four facets: annotation scheme (type of consonant, C), granularity (broad vs. narrow transcription G), material (continuous speech vs. articulation test, M), and annotation team (T). Data in Set A were given as agreement percentages. Our G-study results are shown in Table 1.
 tators assuming that variation is due to incommensurable annotator behavior. In our case, however, the team facet explains only a very small percentage of variance both four annotation teams are comparable in their annotation quality. The major factors responsible for the observed variance are granularity and type of consonants. Material  X  does not exhibit a substantial individual influence on reliability, but becomes relevant in the CGM-interaction. Our G-study therefore reveals that unreliability in Shriberg and
Lof X  X  data is caused not by idiosyncrasies of individuals, but due to the characteristics of the task, namely, granularity and scheme.
 of these facets that are especially prone to produce disagreement. Because we operated with agreement data, this information can be easily obtained from the data entered into the analysis. Because neither team nor material are major sources for variance, we only have to examine the values for granularity and consonants. Due to the same reason we can base the comparison on mean values over teams and material types. For the granularity facet we find overall lower agreement in narrow transcriptions (64.15%) compared to broad transcriptions (89.46%). On the consonant facet we can differentiate critical phonemes such as / D /or/ S / from uncritical ones (e. g., /j/, /b/). Interpreting the CG-interaction in this light, disagreement on consonants in narrow transcriptions seems to be comparably higher than in broad transcriptions. Implications from this study would be that the selection of annotators and the training of annotation teams are successful in producing comparable results. For high reliability, however, transcriptions should be done on a broad level with specific training for difficult consonants and some special care for material from articulation tests (see CGM-interaction). 4. Practical Considerations in Planning G-Studies
In planning and conducting a G-study some deliberation is necessary to achieve inter-pretable results. Foremost, the overall quality of the G-study depends on the choice of factors that completely and accurately represent the situation of the annotators. As it is 6 quite easy to overlook relevant but rather inconspicuous factors like minor changes in the annotation tool or increasing time pressure due to upcoming project deadlines, the choice of correct facets relies heavily on the experience and knowledge of the researcher.
The statistical results, however, will give indications for likely misspecifications of facets number of facets that can be included in a G-study is unlimited. Having more than four or five facets in one study might make the final interpretation overly complex, however.
Even though there is no minimum necessary number of observations, missing data due to a low number of observations pose a problem for model interpretation. Approaches to deal with such unbalanced designs are given by Brennan (2001) and Chiu and Wolfe (2003). Additionally, there is no clear-cut rule when a component might be considered  X  X oo small X  to be of importance. As a rule of thumb a component of less than 8% might be considered  X  X mall X , but the decision remains one of  X  X elative importance X  depending on the distribution of explained variance across components. 5. G-Theory and Agreement Indices
Two well-known measures for capturing the quality of manual annotations are agree-ment percentages and the kappa statistic (Cohen 1960; Carletta 1996; Eugenio and
Glass 2004). Both measures provide a  X  X ummary index X  (Agresti 1992) that expresses the degree of (dis)agreement among coders. Where the calculation of percentages and facets), G-Theory has been designed to analyze multiple possible influencing factors in a single run and to compare the relative importance of components among each other.
Shriberg and Lof (1991), for instance, compare narrow and broad transcriptions using graphics that show the agreement percentages for each consonant for both transcrip-tion types, arguing that overall narrow transcriptions seem unreliable. Based on their experience with the study context they further assumed that these differences were not due to annotator training, behaviors, or experience. They did not provide any direct evidence, however. The G-study presented in this article could prove both assumptions in a single run. It further allows us to investigate the interactions and mutual influences of these factors, thus clearly exceeding the possibilities of summary statistics. As we have seen in the example, G-Theory, however, does not provide answers as to which values of a facet are responsible for higher or lower reliability. This information must be obtained by a review of the data. Agreement indices and G-Theory should thus not be seen as competing, but rather as complementary, approaches. Kappa can serve
G-Theory in a second step investigates the underlying reasons of inadequate reliability and subsequently guides efforts to improve reliability. 6. Final Remarks
Generalizability theory is a valuable approach for identifying problematic areas in annotation projects. The investigation of multiple facets at the same time can provide a clearer understanding of reasons underlying insufficient annotation quality and subse-quently offer avenues to its improvement. In this article we could not give more than a passing glance over the possibilities provided by the G-Theory approach. For the interested reader, Shavelson and Webb (1981) give a good introduction into the material.
Further references are provided throughout the article and in the reference section. References Statistical Software
