 In semi-supervised classification, graph-based methods have drawn great atten-tion recently. This kind of methods maps the data points into vertices of a graph, and a weighted graph is formed after defining a similarity function between points. Graph methods are nonparametric, discriminative, and transductive in nature. Many methods employ this graph representation of data [2, 6, 7, 10, 8, 9].
Although these works employ many variants of graph formation, e.g. k NN and e NN graphs, exponential and cosine weights, they are all formed directly from the data points. We know that all the data points are only the observation of some hidden variables, and the noise can not be avoided in observed values. To build a graph which reflects the data distribution better, we should filter the noise first, and then classify the unlabeled data.

In many practical applications of data classification and data mining, we usually make necessary assumptions to facilitate our analysis and help us to find more applicable methods. Labels smoothness [11] and cluster assumption [2, 3] are of this kind. As we know, nearby data points in the same cluster tend to share the same label, and the density of one cluster changes slowly inside one cluster. So we assume that the real distribution of data points in the same cluster consists of lower frequency compo nents, and higher frequency ones of the distribution are more likely to be the noise.

We utilize the Markov random walk model to construct such a graph. Based on our assumption, theoretical analysis shows that our graph has less high frequency noise, and a smoother transition matrix. After the filtering step, we use a graph-based classification metho d to classify data points.

In this paper, we introduce a new form of smooth graph in Sect. 2. In Sect. 3, we give the analysis of our graph from spectral theory, Markov chain, and reg-ularization theory. The classification and parameter learning method are given in Sect. 4. With the experiment analysis of the artificial and real world data sets, the results and evaluations are shown in Sect. 5. In Sect. 6, we give the concluding remark. 2.1 Data Representation In this paper, we use the Markov random walk on graph model to build our graph. Let the graph G =( V, E ) be a pair consisting of a set V of vertices, and aset E of edges joining some pa irs of vertices. For each x  X  V , we may consider the set N x of neighbors of x , formed by vertices y with an edge joining x to y . The random walk is based on this graph, where the step from x to y ( y  X  N x )has probability p xy . Under the assumption of Markovian property, such a random walk on graph can be viewed as a Markov chain.

In the Markov random walk for classification, data points are mapped into the vertices in a graph or states in a Markov chain. The transition probability can be seen as the similarity between data points. Given that a dataset consists we can construct a graph from input data x i . For instance, a typical weight of edge is defined as where d (  X  ) can be Euclidean distance or other distance measure, and  X  is the parameter for exponential weight. Then let the one-step transition probability be We can get the weight matrix W =[ w ij ] and transition matrix P =[ p ij ]. In the matrix form, we have P = D  X  1 W ,where D is a diagonal matrix with D ii = j w ij . Some semi-supervised classific ation methods [7, 9, 10] are based on such a basic representation. In this paper, we assume w ii =0,whichformsa non-lazy random walk. 2.2 Smooth Graph From the Markov random walk, we treat that the walker starts walking from some point according to the transition matrix as a diffusion process. And this process can smooth the graph as its step increases. We know that the step of the walk corresponds to the power of transition matrix P .Ifthematrix P is from graph G , then we propose the following transition matrix of graph G ( m ) for semi-supervised classification.

Therefore, P  X  P is the transition matrix of another graph G (2) ,and P  X  P  X  P is the transition matrix of graph G (3) ,andsoon. All the following discussions are based on the assumption that real world data has a smooth distribution, which can be also viewed as data density changes slowly in a connected region. 3.1 Spectral Analysis Spectral theory and harmonic analysis have been used to analyze high dimen-sion data [1, 4]. Dimension reduction, visualization, and many machine learning methods for high dimension data can be derived from this framework. Our anal-ysis is given from the frequency point of view.

The smoothness of distribution for some data sets can be captured by the transition matrix P , sparse region with lower transition probability and dense region with higher one. For a smooth distribution, the transition probabilities inside a cluster should be relative high, and should not change greatly from point to point. On the other hand, the probabilities between clusters should be relatively small. However, in real world data, for the reason of noise and high dimensionality of data point, the transition probabilities inside a cluster vary greatly. In semi-supervised classificatio n, there are fewer labeled data points, and the noise can easily result in wrong classification.

If the matrix P is composed from different frequencies, based on our assump-tion, we can filter the high frequency com ponents to smooth data distributions. From spectral theory we know that matrix P can be decomposed as P =  X  X  X   X  1 , where  X  is a diagonal matrix with eigenvalue  X  i of P ,  X  is composed of the eigen-vectors corresponding to each eigenvalue.

From spectral theory and harmonic analysis we know that the eigenfunctions can be interpreted as a gener alization of the Fourier harmonics on the manifold defined by the data points [1]. In our problem setting, smaller eigenvalues corre-spond to higher frequency eigenfunctions, and larger eigenvalues correspond to lower ones. By eigen decomposition, we can rewrite P ( m ) as:
It is easy to see: where  X  ik and  X  kj are the elements in  X  and  X   X  1 . For a probability matrix, we have  X  0 =1 &gt; X  1 &gt; X  2  X  X  X  &gt; X  n  X  0. From equation (5) we know that as the power increases, smaller eigenva lues decay greatly, and larger ones stay relatively larger. From the frequency view, that is to say, the power of P acts as a low-pass filter, reducing the higher frequency components while retaining lower ones. Furthermore, the filtering process can be controlled with the parameter m . 3.2 Markov Chain In this subsection, we show that the transition matrix will become smoother and smoother as m increases. We firstly map the graph representation of data into a Markov chain. The vertices set V of the graph is mapped into the state set I of the chain. The weight of the graph G defined in Sect. 2 is mapped into transition probability between states in Markov chain.

From the view of Markov chain, if P m exists when m  X  X  X  ,thereisan uniform distribution  X  j (1  X  j  X  n ) over all the data, where n = | I | , total number of states in I or vertices in the graph G . That means the probabilities from any data point to one fixed point are the same. In this case, we say the graph is  X  X lat X . We can define the smoothness of a graph according to its  X  X lat X  state if it exists. The smoothness function is defined as: Q m ) reflects the smoothness of the graph. The smaller value of Q ( m ) means a smoother graph. We can predict that graph G (2) is smoother than G (1) .This can be explained from the view of Markov chain analysis. We give a brief proof here.

By treating a connected graph G as the Markov chain, it is easy to satisfy the following conditions: it is a finite-state Markov chain with no two disjoint closed sets, and it is aperiodic. After mapping the graph into a Markov chain, we have the following result [5]: there exist a probability distribution {  X  j ,j  X  I } and numbers  X &gt; 0and0 &lt; X &lt; 1 such that, for all i, j  X  I , In particular, In (7), p ( n ) ij is an element in the matrix P n .From(7),weknowthatas n gets larger, p ( n ) ij gets closer to the fixed value  X  j . From (6) and (7), we have For a fixed graph, n is a constant. Therefore, Q ( m ) gets smaller and smaller as m grows. Then we can say that the graph gets smoother and smoother.

Different m values will result in different graphs. In special case when m =1, it is the original graph G . However, m should not get too large. When m  X  X  X  , P m will become an uniform distribution, which provides no information about classification.

The following example can illustrate this point. Assume that a is the transition matrix of some graph G .
We can see that Q (1) =1 . 00 ,Q (4) =0 . 39 ,Q (16) =0 . 01 ,Q (64) =0 . 00. As m increases, the graph gets flatter and flatter and at last becomes an uniform distribution for each column. 3.3 Regularization In graph-based semi-supervised classification, the key problem is to estimate the probability for each class. In order to solve the  X  X ll-posed problem X  in estimating the probability, regularization is proposed as a solution. Under this framework, many methods can be viewed as to estimate a smooth function f on the graph. This function should be close to the given values on the labeled data points, and at the same time it should be smooth on the whole graph.
 One typical method minimizes the following smoothness function: The solution for the graph G is vector f (0) , whose weight matrix is W (0) .Forthe graph G ( m ) ,solutionis f ( m ) with weight matrix W ( m ) . The minimization forces the f i and f j to be close with biger w ij . From (2) we know that w ij is associated with p ij .As m goes up, from the analysis above, some data points far away but in the same cluster will have a higher transition probabilities p ( m ) ij . This means that their corresponding weight w ( m ) ij increases. With Euclid ean distance, the path between i and j is shortened. From the smoothness function S ( f ), f i and f j are forced to be closer than before. Although we notice that as m increases, bigger weights will be reduced gradually, as long as we pick up a proper value for m ,it is feasible to both keep the local structure and introduce global information. Many graph based methods for semi-supervised classification can be viewed as Markov random walks [7, 9, 10]. These methods have clear explanations and have done well in semi-supervised classification. In order to take advantage of our graph representation, we employ a random walk related method, i.e. harmonic function in [10] to label those unlabeled data points. 4.1 Parameter Learning The parameters in our model are m ,and  X  in equation (1) if selected. Zhu [10] has proposed a method for learning  X  . We will focus on how to learn parameter m in our model.

There are many methods that can be used for parameter learning in the graph-based semi-supervised classification. We propose a graph volume based method. In semi-supervised classification, labeled data usually has a relatively small size, e.g. in case of only two labeled data points for binary classification, one for each class. In this case, we can not fully trust these two points, because they may be noises or biased by noises greatly. Therefore, many parameter learning methods, which rely on only labeled da ta, can not be used here. Based on the cluster assumption, we know that when two clusters are separated well, no lower density region exists in any cluster. The connection inside a cluster is stronger than that between clusters. Based on this intuition, we propose the following method to select a proper m :let C i be a subset of the vertices of the graph G . We define The following function can be used to measure the connection inside a class. We call g ( C k ) the cohesion factor of the class C k , and it can be viewed as the density of C k . Since our classification method can be viewed as a random walk, labels are propagated from the labeled data points to unlabeled ones. As long as the classification is wrong, there will be low density region inside one class, and vol ( C k ) will be small. When g ( C k ) becomes relative large and stable, that is  X  X  ( C k ) &lt; , we can stop the walk and pick up the value of m at this time.
Average distance and its variants are frequently used to find a proper m [7]. This kind of methods computes the average distance between each pair of data points in different classes. When th e classes are well separated, the average distance should be larger than other cases. However, this method is influenced by the shapes of clusters more than our method. For instance, average distance method might not be suitable to be used in the two moon data set in Fig.1. Because of the interwoven shape, when tw o classes are well separated, the average distance may not be the largest one. But our method is independent of the shapes of classes. 5.1 Artificial Data We test our method on the switch or two-moon data [7] with two labeled points, one for each class. The weight is formed using equation (1), and d (  X  ) is Euclidean distance. From Fig.1 we can see that, as the value of m gets larger and larger, the graph becomes smoother. Furthermore, when m =1and  X  =0 . 04  X  0 . 06, classification can be tota lly correct using the method in [10]. However, keeping the accuracy at 100%, the smooth graph enlarges the range of parameter  X  to  X  =0 . 04  X  0 . 45.

The smoothness here is different from ones in [9, 10]. In this paper, we use smoothness to describe the transition probabilities between points. A rough tran-sition probabilities between points might easily spread the errors. If the bridge noise has a high transition probability, it will bring more error. However, a smoother transition probability can reduce this error.
 5.2 Text Classification We also apply our method to text classification, with few labeled documents but many unlabeled ones. Text documents are represented with high dimension vectors, which are usually quite sparse. We expect to construct a smooth graph for the classification.

We test our method with real-world data set 20 Newsgroups. In order to make a comparison, the data and setting for Windows vs Mac are the same as [2, 7]. From 2 to 128 labeled data points are randomly selected to form X L .We test on 100 randomly splits balanced for class labels. The value of m ranges from 1 to 16. We show experimental results under different m values. From the experimental results, we have the following observations: 1.The smooth graph has a high classification accuracy.
 In Fig.2, Smooth is the accuracy of ou r method with learned parameter m . MAM is the result of [7] and CK is the result of [2]. We can see a clear advantage of our smooth graph methods, especia lly when the number of labeled examples is relatively small. Table 1 shows the classification results of our methods and original harmonic function [10], where m = 1 is the result of harmonic function and learned m is the result of our method. 2. Different m affects results greatly.

From Fig.3, we can see that different st eps affect the classification results greatly. When labeled examples are fewer, more steps give a better classification. But when labeled examples get more, accu racy after more steps falls slightly. Because when there are more labeled exam ples, fewer steps are needed to reach labeled examples, and more steps result in a flatter graph. If the graph is too  X  X lat X , it may not be good for classification. When steps become infinite, then the graph becomes an uniform distribution. In the case we can not make a classification. In this paper, we propose a new form of graph, i.e., smooth graph, which aims to filter the high frequency noise with smooth transition matrix. This graph is constructed using Markov random walk model. By the power of transition matrix P , we obtain a smooth graph. From spectral theory, Markov chain, and regularization theory, we show that the high frequency components of this graph are reduced.

Based on the smooth graph, a semi-supervised classification method that combines the smooth graph and the graph-based method has been developed and applied to text classification. Results from the artificial and real data are convincing and illustrate that smooth graph fits graph-based semi-supervised classification better, which has a clear advantage over several other methods. We are grateful to St  X  ephane Lafon for the helpful conversation on harmonic analysis. And we are also grateful to Olivier Chapelle for the data set Windows vs Mac. This work was supported by Chinese 973 Research Project under grant No. 2004CB719401.

