 Cheng Soon Ong cheng.ong@anu.edu.au Xavier Mary xavier.mary@ensae.fr ENSAE-CREST-LS, 3 avenue Pierre Larousse, 92240 Malakoff, France St  X ephane Canu scanu@insa-rouen.fr Alexander J. Smola alex.smola@anu.edu.au Optimization, Ill-posed Problems Almost all current research on kernel methods in ma-chine learning focuses on functions k ( x, x 0 ) which are positive semidefinite. That is, it focuses on kernels which satisfy Mercer X  X  condition and which conse-quently can be seen as scalar products in some Hilbert space. See (Vapnik, 1998; Sch  X olkopf &amp; Smola, 2002; Wahba, 1990) for details.
 The purpose of this article is to point out that there is a much larger class of kernel functions available, which do not necessarily correspond to a RKHS but which nonetheless can be used for machine learning. Such kernels are known as indefinite kernels , as the scalar product matrix may contain a mix of positive and neg-ative eigenvalues. There are several motivations for studying indefinite kernels:  X  Testing Mercer X  X  condition for a given kernel can  X  Sometimes functions which can be proven not to  X  There have been promising empirical reports on  X  In H  X  control applications and discrimination  X  RKKS theory (concerning function spaces arising  X  In recent work on learning the kernel, such as We will discuss the above issues using topological spaces similar to Hilbert spaces except for the fact that the inner product is no longer necessarily pos-itive. Section 2 defines RKKS and some properties required in the subsequent derivations. We also give some examples of indefinite kernels and describe their spectrum. Section 3 extends Rademacher type gen-eralization error bounds for learning using indefinite kernels. Section 4 shows that we can obtain a theorem similar to the representer theorem in RKHS. However, we note that there may be practical problems. Sec-tion 5 describes how we can perform approximation of the interpolation problem using the spectrum of the kernel also using iterative methods. It also shows pre-liminary results on spline regularization. Kre  X  X n spaces are indefinite inner product spaces en-dowed with a Hilbertian topology, yet their inner prod-uct is no longer positive. Before we delve into defini-tions and state basic properties of Kre  X  X n spaces, we give an example: Example 1 (4 dimensional space-time) Indefinite spaces were first introduced by Minkowski for the solution of problems in special relativity. There the inner product in space-time ( x, y, z, t ) is given by Clearly it is not positive. The vector v = (1 , 1 , 1 , belongs to the cone of so-called neutral vectors which satisfy  X  v, v  X  = 0 (in coordinates x 2 + y 2 + z 2  X  t 2 In special relativity this cone is also called the  X  X ight cone, X  as it corresponds to the propagation of light from a point event. 2.1. Kre  X  X n spaces The above example shows that there are several dif-ferences between Kre  X  X n spaces and Hilbert spaces. We now define Kre  X  X n spaces formally. More detailed ex-positions can be found in (Bogn  X ar, 1974; Azizov &amp; Iokhvidov, 1989). The key difference is the fact that the inner products are indefinite.
 Definition 1 (Inner product) Let K be a vector space on the scalar field. 1 An inner product  X  ., .  X  K on K is a bilinear form where for all f, g, h  X  K ,  X   X  R :  X   X  f, g  X  K =  X  g, f  X  K  X   X   X f + g, h  X  K =  X   X  f, h  X  K +  X  g, h  X  K  X   X  f, g  X  K = 0 for all g  X  K implies  X  f = 0 An inner product is said to be positive if for all f  X  K we have  X  f, f  X  K  X  0. It is negative if for all f  X  K  X  f, f  X  K  X  0. Otherwise it is called indefinite . A vector space K embedded with the inner product  X  ., .  X  K is called an inner product space . Two vectors f, g of an inner product space are said to be orthogonal if  X  f, g  X  K = 0. Given an inner product, we can define the associated space.
 Definition 2 (Kre  X  X n space) An inner product space ( K ,  X  ., .  X  K ) is a Kre  X  X n space if there exist two Hilbert spaces H + , H  X  spanning K such that  X  All f  X  K can be decomposed into f = f + + f  X  ,  X   X  f, g  X  K ,  X  f, g  X  K =  X  f + , g +  X  H This suggests that there is an  X  X ssociated X  Hilbert space, where the difference in scalar products is re-placed by a sum: Definition 3 (Associated Hilbert Space) Let K be a Kre  X  X n space with decomposition into Hilbert spaces H + and H  X  . Then we denote by K the associated Hilbert space defined by K = H +  X  X   X  hence  X  f, g  X  K =  X  f + , g +  X  H Likewise we can introduce the symbol to indicate that K = H + H  X  hence  X  f, g  X  K =  X  f + , g +  X  H Note that K is the smallest Hilbert space majorizing the Kre  X  X n space K and one defines the strong topology on K as the Hilbertian topology of K . The topology does not depend on the decomposition chosen. Clearly | X  f, f  X  K | 6 k f k 2 K is said to be Pontryagin if it admits a decomposi-tion with finite dimensional H  X  , and Minkowski if K itself is finite dimensional. We will see how Pontryagin spaces arise naturally when dealing with conditionally positive definite kernels (see Section 2.4).
 For estimation we need to introduce Kre  X  X n spaces on functions. Let X be the learning domain, and R X the set of functions from X to R . The evaluation func-tional tells us the value of a function at a certain point, and we shall see that the RKKS is a subset of R X where this functional is continuous.
 Definition 4 (Evaluation functional) Definition 5 (RKKS) A Kre  X  X n space ( K ,  X  ., .  X  K ) is a Reproducing Kernel Kre  X  X n Space (Alpay, 2001, Chap-ter 7) if K  X  R X and the evaluation functional is con-tinuous on K endowed with its strong topology (that is, via K ). 2.2. From Kre  X  X n spaces to Kernels We prove an analog to the Moore-Aronszajn theo-rem (Wahba, 1990), which tells us that for every ker-nel there is an associated Kre  X  X n space, and for every RKKS, there is a unique kernel.
 Proposition 6 (Reproducing Kernel) Let K be an RKKS with K = H + H  X  . Then 1. H + and H  X  are RKHS (with kernels k + and k  X  ), 2. There is a unique symmetric k ( x, x 0 ) with k ( x,  X  )  X  3. k = k +  X  k  X  .
 Proof Since K is a RKKS, the evaluation functional is continuous with respect to the strong topology. Hence the associated Hilbert Space K is an RKHS. It fol-lows that H + and H  X  , as Hilbertian subspaces of an RKHS, are RKHS themselves with kernels k + and k  X  respectively. Let f = f + + f  X  . Then T x ( f ) is given by
T x ( f ) = T x ( f + ) + T x ( f  X  ) In both lines we exploited the orthogonality of H + with H  X  . Clearly k := k +  X  k  X  is symmetric. Moreover it is unique since the inner product is non-degenerate. 2.3. From Kernels to Kre  X  X n spaces Let k be a symmetric real valued function on X 2 . Proposition 7 The following are equivalent (Mary, 2003, Theorem 2.28):  X  There exists (at least) one RKKS with kernel k .  X  k admits a positive decomposition, that is there  X  k is dominated by some positive kernel p (that is, There is no bijection but a surjection between the set of RKKS and the set of generalized kernels defined in the vector space generated out of the cone of positive kernels. 2.4. Examples and Spectral Properties We collect several examples of indefinite kernels in Ta-ble 1 and plot a 2 dimensional example as well as 20 of the eigenvalues with the largest absolute value. We investigate the spectrum of radial kernels using the Hankel transform.
 The Fourier transform allows one to find the eigenvalue decomposition of kernels of the form k ( x, x 0 ) =  X  ( x  X  x ) by computing the Fourier transform of  X  . For x  X  R n we have where  X  = 1 2 n  X  1 and H  X  is the Hankel transform of order  X  . Table 1 depicts the spectra of these kernels. Negative values in the Hankel transform correspond to H  X  , positive ones to H + . Likewise the decomposi-endre polynomials allows one to identify the positive and negative parts of the Krein space, as the Legendre polynomials commute with the rotation group.
 One common class of translation invariant kernels which are not positive definite are so-called condition-ally positive definite (cpd) kernels. A cpd kernel of order p leads to a positive semidefinite matrix in a subspace of coefficients orthogonal to polynomials of order up to p  X  1. Moreover, in the subspace of ( p  X  1) degree polynomials, the inner product is typically neg-ative definite. This means that there is a space of poly-nomials of degree up to order p  X  1 (which constitutes an up to n + p  X  2 p  X  1 -dimensional subspace) with negative inner product. In other words, we are dealing with a Pontyragin space.
 The standard procedure to use such kernels is to project out the negative component, replace the lat-Kernel 2D kernel 20 main Eigenvalues Fourier Transform ter by a suitably smoothed estimate in the polyno-mial subspace and treat the remaining subspace as any RKHS (Wahba, 1990). Using Kre  X  X n spaces we can use these kernels directly, without the need to deal with the polynomial parts separately. An important issue regarding learning algorithms are their ability to generalize (to give relevant predictions). This property is obtained when the learning process considered shows an uniform convergence behavior. In (Mendelson, 2003) such a result is demonstrated in the case of RKHS through the control of the Rademacher average of the class of function considered. Here we present an adaptation of this proof in the case of Kre  X  X n spaces. We begin with setting the functional frame-work for the result.
 Let k be a kernel defined on a set X and choose a de-composition k = k +  X  k  X  where k + and k  X  are both positive kernels. This given decomposition of the ker-nel can be associated with the RKHS K defined by its positive kernel k = k + + k  X  whose Hilbertian topol-ogy defines the strong topology of K . We will then consider the set B K defined as follows: Note that in a Kre  X  X n space the norm of a function is the associated Hilbertian norm and usually k f k 2 6 =  X  f, f  X  but always  X  f, f  X  K  X  k f k 2 .
 The Rademacher average of a class of functions F with respect to a measure  X  is defined as follows. Let x 1 , . . . , x m  X  X be i.i.d random variables sam-pled according to  X  . Let  X  i for i = 1 , . . . , m be Rademacher random variables, that is variables tak-ing values { X  1 , +1 } with equal probability. Definition 8 (Rademacher Average) The Rademacher average , R m ( F ) of a set of functions F (w.r.t.  X  ) is defined as Using the Rademacher average as an estimate of the  X  X ize X  of a function class, we can obtain general-ization error bounds which are also called uniform convergence or sample complexity bounds (Mendel-son, 2003, Corollary 3), that is for any  X  &gt; 0 and  X  &gt; 0, there is an absolute constant C such that if m &gt; C  X  2 max { R 2 m ( J ( B K )) , log 1  X  } , then, where J ( f ( x )) denotes the quadratic loss defined as in (Mendelson, 2003). To get the expected result we have to show that the Rademacher average is bounded by a constant independent of the sample size m . To control the Rademacher average, we first give a lemma regarding the topology of Kre  X  X n spaces putting empha-sis on both difference and close relationship with the Hilbertian case.
 Lemma 9 For all g  X  K : Proof It is trivial if g = 0.  X  g  X  K , g 6 = 0, let h = g/ k g k . By construction k h k = 1. In the unit ball of a RKKS, the Rademacher average with respect to the probability measure  X  behave the same way as the one of its associated RKHS.
 Proposition 10 (Rademacher Average) Let K be the Gram matrix of kernel k at points x , . . . , x m , If according to the measure  X  on X x 7 X  X  X  k ( x, x )  X  L 1 ( X ,  X  ) , then with The proof works just as in the Hilbertian case (Mendel-son, 2003, Theorem 16) with the application of lemma 9. As a second slight difference we choose to express the bound as a function of the L 1 ( X ,  X  ) norm of the kernel instead of going through its spectral rep-resentation. It is simpler since for instance, for the un-normalized gaussian kernel k ( x, y ) = exp (  X  ( x  X  y ) on X = R we have M = 1 regardless the measure  X  considered. Since we are back to the Hilbertian con-text (Mendelson, 2003, Corollary 4) applies replacing Hilbert by Kre  X  X n, providing an uniform convergence result as expected. In order to perform machine learning, we need to be able to optimize over a class of functions, and also to be able to prove that the solution exists and is unique. Instead of minimizing over a class of functionals as in a RKHS, we look for the stationary point. This is moti-vated by the fact that in a RKHS, minimization of the cost functional can be seen as a projection problem. The equivalent projection problem in RKKS gives us the stationary point of the cost functional. 4.1. Representer Theorem The analysis of the learning problem in a RKKS gives similar representer theorems to the Hilbertian case (Sch  X olkopf et al., 2001). The key difference is that the problem of minimizing a regularized risk functional be-comes one of finding the stationary point of a similar functional. Moreover, the solution need not be unique any more. The proof technique, however, is rather similar. The main difference is that a) we deal with a constrained optimization problem directly and b) the Gateaux derivative has to vanish due to the nondegen-eracy of the inner product. In the following, we define the training data X := ( x 1 , . . . , x m ) drawn from the learning domain X .
 Theorem 11 Let K be an RKKS with kernel k . De-note by L { f, X } a continuous convex loss functional depending on f  X  K only via its evaluations f ( x i ) with x i  X  X , let  X (  X  f, f  X  ) be a continuous stabilizer with strictly monotonic  X  : R  X  R and let C { f, X } be a continuous functional imposing a set of constraints on f , that is C : K  X  X m  X  R n . Then if the optimization problem has a saddle point f  X  , it admits the expansion Proof The first order conditions for a solution of (1) imply that the Gateaux derivative of the Lagrangian needs to vanish. By the nondegeneracy of the inner product,  X  f, g  X  K = 0 for all g  X  K implies f = 0. Next observe that the functional subdifferential of L{ f,  X  } with respect to f satisfies (Rockafellar, 1996)  X 
L{ f,  X  } = Here  X  is understood to be the subdifferential with respect to the argument wherever the function is not differentiable, since C and  X  only depends on f ( x i ), the subdifferential always exists with respect variational derivative needs to vanish, we have 0  X   X  f L{ f,  X  } and consequently f = P i  X  i k ( x i ,  X  ) for some  X  i  X   X  f ( x proves the claim.
 Theorem 12 (Semiparametric Extension) The same result holds if the optimization is carried out over f + g , where f  X  K , and g is a parametric addition to f . Again f lies in the span of k ( x i ,  X  ) . Proof [sketch only] In the Lagrange function the partial derivative with respect to f needs to vanish just as in (3). This is only possible if f is contained in the span of kernel functions on the data. 4.2. Application to general spline smoothing We consider the general spline smoothing problem as presented in (Wahba, 1990), except we are considering Kre  X  X n spaces. The general spline smoothing is defined as the function stabilizing (that is finding the station-ary point) the following criterion: The form for the solution of equation (4) is given by the representer theorem, which says that the solution (if it exists) is the solution of the linear equation where K ij = k ( x i , x j ) is the Gram matrix. The general spline smoothing problem can be viewed as applying Tikhonov regularization to the interpola-tion problem. However, since the matrix K is indef-inite, it may have negative eigenvalues. For values of the regularization parameter  X  which equal a negative eigenvalue of the Gram matrix K , ( K +  X  I ) is singu-lar. Note that in the case where K is positive, this does not occur. Hence, solving the Tikhonov regularization problem directly may not be successful. Instead, we use the subspace expansion from Theorem 11 directly. Tikhonov regularization restricts the solution of the radius 1 / X   X  f, f  X  2 K . Hence, it projects the solution of the equation onto the ball. To avoid the problems of singular ( K +  X  I ), the approach we take here is to set  X  = 0, and to find an approximation to the solution in a small subspace of the possible solution space. That is, we are solving the following optimization problem, We describe several different ways of choosing the sub-space L . Defining T : K  X  R m to be the evaluation functional (Definition 4), we can express the inter-polation problem f ( x i ) = y i given the training data ( x tem T f = y , where f  X  K , and K is a RKKS. Define T  X  : R m  X  K to be the adjoint operator of T such that  X  T f, y  X  =  X  f, T  X  y  X  . Note that since T operates on elements of a Kre  X  X n space, T T  X  = K is indefinite. 5.1. Truncated Spectral Factorization We perform regularization by controlling the spec-trum of K . We can obtain the eigenvalue decompo-sition of K , Ku i =  X  i u i , where u 1 , . . . , u m are the orthonormal eigenvectors of K , and  X  i are the associ-ated nonzero eigenvalues (assume K is regular). Let v tors for T  X  T . The solution of T f = y (if it exists), is given by Intuitively, we associate eigenvalues with large abso-lute values to the underlying function, and eigenvalues close to zero corresponds to signal noise. The Trun-cated Spectral Factorization (TSF) (Engl &amp; K  X ugler, 2003) method can be obtained by setting all the eigen-values of small magnitude to zero. This means that the solution is in the subspace 5.2. Iterative Methods Iterative methods can be used to minimize the squared error J ( f ) := 1 2 k T f  X  y k 2 . Since J ( f ) is convex, we can perform gradient descent. Since  X  f J ( f ) = T
T f  X  T  X  y , we have the iterative definition f k +1 = f k  X   X  ( T  X  T f  X  T  X  y ), which results in Landweber-Fridman (LF) iteration (Hanke &amp; Hansen, 1993). The solution subspace in this case is the polynomial A more efficient method which utilizes the Krylov sub-spaces is MR-II (Hanke, 1995). MR-II, which gen-eralizes conjugate gradient methods to indefinite ker-nels, searches for the minimizer of k K X   X  y k within the Krylov subspace where r 0 = y  X  K X  0 . The algorithm is shown in Figure 1. The convergence proof and regularization behavior can be found in (Hanke, 1995). 5.3. Illustration with Toy Problem We apply TSF, LF, and MR-II to the spline approx-imation of sinc( x ) and cos(exp( x )). The experiments was performed using 100 random restarts. The re-sults using a Gaussian combinations kernel are shown in Figure 2. The aim of these experiments is to show that we can solve the regression problem using itera-tive methods. The three methods perform equally well on the toy data, based on visually inspecting the ap-proximation. TSF requires the explicit computation of the largest eigenvalues, and hence would not be suitable for large problems. LF has been previously shown to have slow convergence (Hanke &amp; Hansen, 1993), requiring a large number of iterations. MR-II has the benefits of being an iterative method and also has faster convergence. The results above required 30 iterations for LF, but only 8 for MR-II. The aim of this paper is to introduce the concept of an indefinite kernel to the machine learning com-munity. These kernels, which induce an RKKS, ex-hibit many of the properties of positive definite ker-nels. Several examples of indefinite kernels are given, along with their spectral properties. Due to the lack of positivity, we stabilize the loss functional instead of minimizing it. We have proved that stabilization provides us with a representer theorem, and also gen-eralization error bounds via the Rademacher average. We discussed regularization with respect to optimizing in Kre  X  X n spaces, and illustrated the spline smoothing problem on toy datasets.
 Alpay, D. (2001). The schur algorithm, reproducing kernel spaces and system theory , vol. 5 of SMF/AMS Texts and Monographs . SMF.
 Azizov, T. Y., &amp; Iokhvidov, I. S. (1989). Linear opera-tors in spaces with an indefinite metric . John Wiley &amp; Sons. Translated by E. R. Dawson.
 Bogn  X ar, J. (1974). Indefinite inner product spaces . Springer Verlag.
 Engl, H. W., &amp; K  X ugler, P. (2003). Nonlinear in-verse problems: Theoretical aspects and some in-dustrial applications. Inverse Problems: Computa-tional Methods and Emerging Applications Tutori-als, UCLA.
 Haasdonk, B. (2003). Feature space interpretation of
SVMs with non positive definite kernels. Unpub-lished.
 Hanke, M. (1995). Conjugate gradient type methods for ill-posed problems . Pitman Research Notes in
Mathematics Series. Longman Scientific &amp; Techni-cal.
 Hanke, M., &amp; Hansen, P. (1993). Regularization meth-ods for large-scale problems. Surveys Math. Ind. , 3 , 253 X 315.
 Hassibi, B., Sayed, A. H., &amp; Kailath, T. (1999).
Indefinite-quadratic estimation and control: A uni-fied approach to h 2 and h  X  theories . SIAM. Lin, H.-T., &amp; Lin, C.-J. (2003). A study on sigmoid kernels for svm and the training of non-psd kernels by smo-type methods. March.
 Mary, X. (2003). Hilbertian subspaces, subdualities and applications . Doctoral dissertation, Institut Na-tional des Sciences Appliquees Rouen.
 Mendelson, S. (2003). A few notes on statistical learn-ing theory. Advanced Lectures in Machine Learning (pp. 1 X 40). Springer Verlag.
 Ong, C. S., &amp; Smola, A. J. (2003). Machine learn-ing with hyperkernels. International Conference of Machine Learning (pp. 568 X 575).
 Rockafellar, R. T. (1996). Convex analysis . Princeton Univ. Pr. Reprint edition.
 Sch  X olkopf, B., Herbrich, R., Smola, A., &amp; Williamson,
R. (2001). A generalized representer theorem. Pro-ceedings of Computational Learning Theory .
 Sch  X olkopf, B., &amp; Smola, A. J. (2002). Learning with kernels . MIT Press.
 Smola, A. J., Ovari, Z. L., &amp; Williamson, R. C. (2000).
Regularization with dot-product kernels. NIPS (pp. 308 X 314).
 Vapnik, V. N. (1998). Statistical learning theory . John Wiley &amp; Sons.
 Wahba, G. (1990). Spline models for observational
