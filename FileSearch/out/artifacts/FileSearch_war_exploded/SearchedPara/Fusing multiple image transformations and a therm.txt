 1. Introduction
This article describes the realization of a human detection system based on low-cost sensing devices. Recently, research on sensing components and software lead by Microsoft provide useful results for extracting the human kinematics (Kinect motion sensor device, http://www.en.wikipedia.org/wiki/Kinect ).

Service robots, now and in the near future, performing tasks as assistants, guides, tutors, or social companions in human popu-lated settings such as museums, hospitals, etc. pose two main challenges: by the one hand, robots must be able to adapt to complex, unstructured environments and, on the other hand, robots must interact with humans. While interacting with the environment, the robot must navigate, detect and avoid obstacles ( Morales et al., 2011 ). A requirement for natural Human Robot interaction is the robot ' s ability to accurately and robustly detect and localize the persons around it in real-time. This problem is a challenging one, quite dif fi cult when a low cost camera is the only available sensor ( Yao and Odobez, 2011 ; Breitenstein et al., 2011 ).
Within this article, the service proposed by the mobile robot is to approach the closer person in the room, i.e. to approach the person to a given distance and to verbally interact with him. This  X  engaging  X  behavior can be useful in potential robot services such a tour guide, health care or information provider. Once the target person has been chosen, the robot plans a trajectory and navigates to the desired position. To accomplish this the robot must be able to detect human presence in its vicinity and it cannot be assumed that the person faces the direction of the robot since the robot acts proactively.

Kinect offers a rich data set (RGB image and depth map) at a signi fi cantly low cost, and represents a great addition to robotics, but there are some limitations. First, the depth map is only valid for objects more than 80 cm away from the sensing device. Second, the Kinect uses an IR projector with an IR camera to obtain the depth map, and that means that sunlight could affect negatively, taking into account that the sun emits in the IR spectrum. Third, Kinect rely on the detection of human activities captured by static sensors. In mobile robot applications the sensors setup is assumed to be embedded in a robot that is usually moving. As a consequence the robot is expected to evolve in environments which are highly dynamic, cluttered, and frequently subjected to illumination changes. To cope upwith this, this work is based on the hypothesis that the combination of Kinect and a thermopile array sensor (low cost Heimann HTPA thermal sensor, http://www.heimannsensor.com/index.php )cansigni fi cantly improve the robustness of human detection. Thermal vision helps to overcome some of the problems related to color vision sensors, since humans have a distinctive thermal pro fi le compared to non-living objects and there are no major differences in appearance between different persons in a thermal image. Another advantage is that the sensor data does not depend on light conditions and people can also be detected in complete darkness. Therefore it is a promising research direction to combine the advantages of different sensor sources because each sensing modality has complementary bene fi ts and drawbacks.

This article outlines the design and development of a multi-modal human detection system. The chosen approach is
Use as input three types of images (optical grayscale, depth map and thermal).

Select some Image Transformations from the Computer Vision (CV) area.

Select some Supervised Classi fi ers from the Machine Learning (ML) area.
 Apply, to each image, a sequence of CV transformations.
Classify the images obtained after the sequence of transforma-tions using different ML Classi fi ers.

Select a (Transformations Sequence, Classi fi er) binomial for each image type using an evolutionary paradigm.
 Combine the best binomials obtained to classify each new image.
We have experimented this approach in a real manufacturing shop fl oor where machines and humans share the space in performing production activities. Experimental results obtained show that the percentage of wrong classi fi ed using only Kinect s detection algorithms is drastically reduced.

In order to stress the suitability of the Multiple Transformation approach, we have performed a second experimental part in which the approach is applied to a well known pedestrian detection image database (that corresponding to the NISIS 2007 challenge), obtain-ing as well an improvement in the classi fi cation results.
The rest of the paper is organized as follows: In Section 2 related work in the area of human detection is presented. We concentrate mainly on work done using machine learning for people detection. Section 3 describes the proposed approach and
Section 4 the experimental setup. Section 5 shows experimental results and Section 6 conclusions and future work. 2. Related work
People detection and tracking systems have been studied exten-sively due to the increase in demand of advanced robots that must integrate natural human  X  robot interaction capabilities in order to perform some speci fi c tasks for the humans or in collaboration with them. As a complete review on people detection is beyond the scope of this work, an extensive work can be found in Schiele (2009) and Cielniak (2007) , we focus on most related work.

People detection solutions that can be used on mobile robots should cope with several requirements:
Camera and other sensors are usually not static since they are mounted on a moving platform. As a consequence, many algorithms aimed at the surveillance applications are not applicable.

Fast (real-time). The computational load of the used algorithms should be low in order to perform real-time detection. Non-invasive (normal human activity is unaffected).

To our knowledge, two approaches are commonly used for detecting people on a mobile robot. One, vision based techniques, and another approach, combining vision with other modalities, normally range sensors such as laser scanners or sonars like in
Guan et al. (2007) and Kalyan et al. (2004) . Methods for people detection in color images extract features based on skin color, face, clothes and motion information such as Bellotto and Hu (2010) . All methods for detecting and tracking people in color images on a moving platform face similar problems and their performance depends heavily on the current light conditions, viewing angle, distance to persons, and variability of appearance of people in the image.
 tracking are laser sensors. One of the most popular approaches in this context is to extract legs by the detecting moving blobs that appear as local minimal in the range image. Martinez-Otzeta et al. (2009) presents a system for detecting legs and follow a person only with laser readings. A probabilistic model of a leg shape is implemented, along with a Kalman fi lter for robust tracking.
Mozos et al. (2010) addresses the problem of detecting people using multiple layers of 2D laser range scans. Other implementa-tions such as Bellotto and Hu (2007) also use a combination of face detection and laser-based leg detection.
 St-Laurent et al. (2006) , Hofmann et al. (2011) , Johnson and
Bajcsy (2008) , and Zin et al. (2011) , concern non-mobile applica-tions in video monitoring applications, and especially for pedes-trian detection where the pose of the camera is fi xed. Some works, ( Gundimada et al., 2010 ), show the advantages of using thermal images for face detection. They suggest that the fusion of both visible and thermal based face recognition methodologies yields better overall performance.
 thermal sensor information to detect humans on mobile robots.
The main reason for the limited number of applications using thermal vision so far is probably the relatively high price of this sensor. Treptow et al. (2005) show the use of thermal sensors and gray scale images to detect people in a mobile robot. A drawback of most of these approaches is the sequential integration of the sensory cues. People are detected by thermal information only and are subsequently veri fi ed by visual or auditory cues. prede fi ned body model features for the detection of people. Few works consider the application of learning techniques. Arras et al. (2007) propose to use supervised learning (AdaBoost) to create a people detector with the most informative features . Mozos et al. (2010) build classi fi ers that are able to detect a particular body part such as a head, an upper body or a leg using laser data. These classi fi ers are learned using a supervised approach based on
AdaBoost. The fi nal person detector is composed of a probabilistic combination of the outputs from the different classi fi ers. In the fi eld of people detection several authors have used multi-classi approaches: Oliveira et al. (2010) use histograms of oriented gradients (HOGs) and local receptive fi elds (LRFs), which are provided by a convolutional neural network, and are classi by multi-layer perceptrons (MLPs) and support vector machines (SVMs) combining classi fi ers by majority vote and fuzzy integral. 3. Proposed approach system is human detection. In this section, the elements and processing sequence that constitute the proposed approach (multimodal input, image transformations, classi fi ers, evolutionary paradigm...) are explained in detail.
 one hand, we propose a new multi-modal approach combining two low-cost sensors, a Kinect motion sensor device for the XBOX 360 and an HTPA thermal sensor developed by Heimann http:// www.heimannsensor.com/index.php . These two sensors provide as sensory cues three types of images (optical, thermal and depth map) that are concurrently processed. Sensors are mounted on top of an RMP Segway mobile platform, which is shown in Fig. 1 .
On the other hand, in order to perform image based classi tion, a new strategy combining Machine Learning paradigms with
Computer Vision techniques is presented. In this approach a combination of a classi fi er and a sequence of image transforma-tions is selected for each image type, by means of an evolutionary paradigm (Estimation of Distribution Algorithms, EDA, Larra X aga and Lozano, 2001 ), and the results of the three classi fi to obtain the fi nal decision. 3.1. Data sources
As stated before, three kind of images are used coming from the Kinect sensor and the thermopile array. 3.1.1. Kinect images
Kinect provides depth images, by using near infrared light to illuminate the subject and measuring the disparity between the information received by the two IR sensors. It provides a 640 480 depth map in real-time (30 fps). In addition to the depth sensor the Kinect also provides a traditional 640 480 RGB image. The restriction derived by the sensor sensibility reduces notoriously the valid range of detection to a distance of 1  X  3m. 3.1.2. Thermal images
The HTPA allows the measurement of temperature distribution of the environment, where very high resolutions are not necessary, such as person detection, surveillance of temperature critical surfaces, hotspot or fi re detection, energy management and security applications. The thermopile array can detect infrared radiation and convert this information into an image where each pixel corresponds to a temperature value. The sensor only offers a 32 31 image that allows a rough resolution of the temperature of the environment as it is shown in Fig. 2 . The bene fi ts of this technology are low costs, the very small power consumption, small size, as well as the high sensitivity of the system. 3.2. Image transformations and training database creation
We therefore have three image type data taken in parallel, and we want to build a classi fi er whose goal is to identify whether a person is in front of the cameras, and thus, in front of the robot or not. First of all, the Kinect images (640 480) are reduced in size and RGB image is converted to grayscale; the reason is to decrease the computational load needed to deal with the images in real time. Fig. 3 shows an example of the three grayscale images obtained. The set is hand-labeled, and the value of each pixel is considered as a predictor variable within the Machine Learning database construction, summing up n m features, being m the column number and n the row number in the image. Each image corresponds to a single row in the generated database.
In addition to the original data, and to obtain different views of the images, we have selected some of the most common image transformations offered by related software (edge detection, Gaussian fi lter, binarization, and so on) in order to show the bene fi ts of the proposed approach without making use of compli-cated algorithms. Table 1 presents the transformations used, as well as a brief description of each of them. It is worth to point out the fact that any other CV transformation could be used apart from the selected ones.

In this way, and for each of the three data sources, a set of equivalent images is obtained, and from the original training database, a new training database is created for each of the CV transformations. Fig. 4 shows an example. These databases are created only for comparison purposes. Our proposed approach uses sequences of transformations (not only one) to improve the accuracy of the fi nal system, and the results are compared in Section 5 .
 3.3. Machine learning classi fi ers
As classi fi ers we use fi ve well known ML supervised classi tion algorithms ( Mitchell, 1997 ) with completely different approaches to learning and a long tradition in different classi tion tasks: IB1, Naive-Bayes, Bayesian Network, C4.5 and SVM.
Then, the goal of our fusion process is to maximize the bene each modality by intelligently fusing their information, and by overcoming the limitations of each modality alone. 3.3.1. IB1 classi fi er. To classify a new test sample, a simple distance measure is used to fi nd the training instance closest to the given test instance, and then it predicts the same class as this nearest training instance. 3.3.2. Naive-Bayes theorem to predict the class for each case, assuming that the predictive genes are independent given the category. To classify a new sample characterized by d genes X  X  X  X 1 ; X 2 ; ... ; classi fi er applies the following rule: c N  X  B  X  arg max where c N  X  B denotes the class label predicted by the Naive classi fi er and the possible classes of the problem are grouped in
C  X f c 1 ; ... ; c l g . 3.3.3. Bayesian networks cal model is a probabilistic graphical model that represents a set of random variables and their conditional independencies via a directed acyclic graph (DAG). In this paper we have used Bayesian Networks as classi fi cation models ( Sierra et al., 2009 ). 3.3.4. C4.5
The C4.5 ( Quinlan, 1993 ) represents a classi fi cation model by a decision tree. It is run with the default values of its parameters.
The tree is constructed in a top  X  down way, dividing the training set and beginning with the selection of the best variable in the root of the tree. 3.3.5. Support vector machines (SVM)
SVM are a set of related supervised learning methods used for classi fi cation and regression. Viewing input data as two sets of vectors in an n-dimensional space, an SVM will construct a separating hyperplane in that space, one which maximizes the margin between the two data sets ( Meyer et al., 2003 ). 3.4. Combination of image transformations
In order to fi nally classify the targets as human or non-human, the estimation of the kinect based classi fi ers has to be combined with the estimation of the thermal based classi fi er. After building the individual classi fi ers (5 24  X  120 for each device) the aim is to combine different transformation sequences (as shown in Fig. 5 ) with the classi fi ers to obtain a more robust fi nal people detector.
To do that, the transformation sequences to be used has to be selected fi rst. It is not supposed that the same chain of transfor-mations would be the best for all the image types, hence a search for the appropriated sequence has to be done. Estimation of Distribution Algorithms are used to this end, as explained below. 3.4.1. Estimation of distribution algorithms
Estimation of distribution algorithms (EDAs) have successfully been developed for combinatorial optimization ( Larra X aga and Lozano, 2001 ). They combine statistical learning with population-based search in order to automatically identify and exploit certain structural properties of optimization problems. EDAs typically work with a population of candidate solutions to the problem, starting with the population generated according to the uniform distribution over all admissible solutions. The population is then scored using a fi tness function. This fi tness function gives a numerical ranking for each string, with the higher the number the better the string. From this ranked population, a subset of the most promising solutions are selected by the selection operator. An example selection operator is truncation selection with threshold  X   X  50 % , which selects the 50% best solutions. The algorithm then constructs a probabilistic model which attempts to estimate the probability distribution of the selected solutions. Once the model is constructed, new solutions are generated by sampling the distribu-tion encoded by this model. These new solutions are then incorpo-rated back into the old population, possibly replacing it entirely. The process is repeated until some termination criteria is met (usually when a solution of suf fi cient quality is reached or when the number of iterations reaches some threshold), with each iteration of this procedure usually referred to as one generation of the EDA. Fig. 6 shows the pseudocode of the algorithm. 3.5. Computer vision transformation sequence as a search problem
The transformation sequence can be searched in a similar way as a feature subset selection is done in some ML problems ( Inza et al., 2000 ). As reported by Aha and Bankert (1994) , the objective of feature subset selection in Machine Learning is to reduce the number of features used to characterize a dataset so as to improve a learning algorithm ' s performance on a given task . Our objective will be the maximization of the classi fi cation accuracy in a image based classi fi cation task for a certain learning algorithm; the transformation sequence selection task can be exposed as a search problem, each state in the search space identifying a subset of possible transformations.

Hence, an individual in the EDA algorithm will be de fi ned as a n -tuple of binary 0, 1 values, in which each position in the tuple refers to a concrete transformation, and the value indicates whether this transformation is used (1 value) or not (0 value); an example with 10 transformations can be seen in Fig. 7 ; it has to be noticed that the order of the transformations in the sequence is not unique, and thus different orders have to be considered. As we have selected 23 transformations in this paper (the number can vary to any other one), our individuals have 23 binary positions.
Once an individual has been bring up, it has to be evaluated. It is worth to be noted that the transformations present in that individual could be made in any order. For instance, if certain individual has 5 transformations active , there are 5! possible orders to be considered. As in general n ! is an unapproachable number, we have decided to randomly select 5 orders among all the possible ones (for n 4 2). This leads the evolutionary algorithm to perform a multi-goal search: by the one hand, the transforma-tions which offer better classi fi cation results, and by the other hand the order the transformations are to be used. 3.6. Combination of classi fi ers classi fi er obtained, one by each sensor. To do that, we use a bi-layer Stacked Generalization approach ( Wolpert, 1992 ; Sierra et al., 2001 ) in which the decision of each of the three single classi fi ers is combined by means of another method.
 tion with this multi-classi fi er approach. It has to be noticed that the second layer classi fi er or function could be any function, including a simple vote approach among the used classi fi classi fi ers found in each picture source. The reason to do that is mainly computation load: using only three classi fi ers, one for each sensor, we can make the needed preprocess in parallel, and so we obtain a fast answer. This manner, we aim at using the obtained model in real time, which is mandatory for the task to be accomplished. 4. Experimental setup
The manufacturing plant located at Tekniker-IK4 is a real manufacturing shop fl oor where machines and humans share the space in performing production activities. The shop fl Fig. 9 can be characterized as an industrial environment, with high ceilings, fl uorescent light bulbs, high windows, etc. The lighting conditions are very changing from one day to another and even in different locations along the path covered by the robot; the environment the robot moves in is a real manufacturing plant with external windows in some of the parts, and hence there is a huge difference between sunny and cloudy moments. 4.1. Method
These are the steps of the experimental phase: (1) Collect a database of images that contains three data types that (2) Reduce the image sizes from 640 480 to 32 24, and convert (3) For each image, apply 23 computer vision algorithms, obtain-(4) Build 120 classi fi ers, applying 5 machine learning algorithms (5) Apply 10 fold cross-validation using 5 different classi (6) Select a transformation sequence for each of the different (7) Combine each of the (sequence, classi fi er) found for the three 4.2. Data set
The training data set is composed of 1065 samples. The input to the supervised algorithms is composed of 301 positive and 764 negative examples. The set of positive examples contains people at different positions and dressed with different clothing in a typical manufacturing environment. The set of negative examples is composed of images without people in the image and with other objects in the environment such as machines, tables, chairs, walls, etc. Figs. 10 and 11 show some examples of images from the data base. It has to be noticed that the thermal images shown in the third column of the fi gures do not always discriminate the presence of a person, due to the existence of hot elements in the plant (which could give False Positives) or, on the contrary, the clothing of a given person who is not looking to the robot, which could give a False Negative case.

To obtain the positive and negative examples the robot was operated in an unconstrained indoor environment (the manufac-turing plant). At the same time, image data was collected with a frequency of 1 Hz. During robot motion the images were hand-labeled as positive examples if people was visually detected in the image, and as negative examples otherwise. 5. Experimental results terms of detection rates and false positives or negatives. We use 10-fold cross-validation ( Stone, 1974 ) to get a validated classi tion accuracy. In order to make a fast classi fi cation  X  response is expected  X  we fi rst transform the color images in gray-scale 32 24, and reduce as well the size of the infrared images to 32 24 size matrix. Hence we have to deal with 768 predictor variables, instead of 307200 (3 colors) of the original images taken by the Kinect camera.
 original databases (32 24 for Images and Distances, 31 31 for thermal pictures). Table 2 shows the 10 fold cross-validation accuracy obtained. The best obtained result is 92.11% for the thermal images original database, and using SVM as classi
The real time Kinect ' s algorithms accuracy among the same images was quite poor (37.50%), as the robot was moving around the environment and the Kinect has been made to be used as a static device. As a matter of fact, that has been the origin of the presented research.

The same accuracy validation process has been applied to each image transformation on each image format. Table 3 shows the results obtained by each classi fi er on the transformed 23 image databases. The best result is obtained by the C4.5 classi transforming the images using Transformation 7 (Gaussian one).
After performing the validation over the distance images, the results shown in Table 4 are obtained. The best result is obtained again by the C4.5 classi fi er after transforming the images using Transformation 7 (Gaussian one), with a 92.82 accuracy.
Finally, the classi fi ers are applied to the thermal images, obtaining the results shown in Table 5 . In this case we obtain the best result (93.52) for the SVM classi fi er, and for two of the used transformations (Transf. 8  X  Lat  X  and Transf. 9  X  Linear-stretch Moreover, the obtained results are identical for both paradigms, so there are redundant algorithms and, if selected, only one of them can be used in the fi nal combination obtaining indistinct results. 5.1. Multiple image transformation
In order to improve the obtained results, an automatic approach aimed at deal with consecutive image transformations has been implemented. To do that, a searching process is used which aim is twofold: choose the sequence of transformations, and the order that these transformations have to be applied; this searching process is done by means of an evolutionary paradigm called Estimation of Distribution Algorithm (EDA). In the searching process, several transformation chains are evaluated, and those which obtain better classi fi cation accuracy are maintained and evolved. As a result different chains of transformations have been obtained for each of the three types of images used. Table 6 shows the obtained results for each image type and classi fi er; the best results obtained are a well classi fi ed percentage of 93.62% for Images and Distances and a 95.12% for Temperatures image type.
Table 7 shows the corresponding transformations for the bests classi fi ers automatically selected by the EDA algorithm.
It has to be noticed that, although the learning time is time consuming (until the EDA search converges), the classi fi time is very short, and it can be done in real time. In this way, and for any dataset used, once the classi fi er is constructed it can be used at a very high frequency, as it is composed by fast classi
As it could be seen, all of them outperform the best result obtained by using a single transformation and after apply a single classi the RGB original images, the best result obtained in the previous phase was 91.83%, while with a combination of transformations a 93.62% cross-validated accuracy is obtain ed.Withdistancetypeimages,the previous result was 92.86% and it has been increased to 93.62%. Finally, the result obtained for the thermical images is now 95.12, which outperforms the previous best result of 93.52%. An increase in the classi fi cation results has been therefore obtained for each of the three image types using the list of transformations automatically obtained by the EDA algorithm. It is worth to mention that the image transformations performed is different for each of the image type used, as shown in Table 7 . 5.1.1. Evaluation of the signi fi cance of the transformation sequence order
Intuitively, it could be thought that selecting transformations which individually perform better for each of the image type can be a good option to start with the transformation chain. This hypothesis is not adequate in this case, as it reach the accuracy in a local optimum instead of the quasi-optimal solution given by the evolutionary computation algorithm. To support that, the follow-ing experiment has been done: (1) Select the best fi ve transformation for each of the image types, (2) Perform those fi ve transformation one after the other, and
Obtained results are shown in Table 8 . As it can be seen, none of them outperforms the results obtained by the chains found using the EDA approach. In the cases of RGB and Distance type images the results after the fi rst transformation decrease, and in the
Thermical image type there is a slight increase that fi nishes after the fourth transformation.

These results stress the bene fi ts of the proposed chain of transformations. Nevertheless, in order to improve the Kinect people detection technique, we aim to combine the best three results obtained, and thus the three devices will be used in the fi nal model to be used on the robot. 5.2. Final combination
The last step is to combine the results of the best classi obtained for each sensor. To do that, we fi rst use a Stacking classi fi er ( Wolpert, 1992 ) in which the decision of each single classi fi er is combined by means of another classi fi er (the so called metaclassi fi er). Table 9 shows the obtained results. As it can be seen, the best obtained accuracy is 96.43%, using a classi tree as metaclassi fi er. It signi fi cantly improves the result of the best (transformation sequence, classi fi er) pair, which has a 95.12 accuracy for the Thermal images. 5.2.1. Statistical tests results, a comparison among the best classi fi er for each image type and the multiclassi fi er is performed, as proposed in Demsar (2006) . Table 10 shows the accuracies obtained by each model for each of the 10 folds used in the validation process. As shown in
Table 11 , the best approach is MultiClassi fi er , having a rank mean (1.40). The obtained Fisher  X  Snedecor F value is 9.87, which allow to reject the null hypothesis  X  all algorithms perform the same and hence it can be said that there is a best algorithm over all the others: the proposed approach with the MultiClassi fi er outper-forms signi fi catively the best classi fi ers of each image type. of only 3 single classi fi ers, and that all the three sensors are used, i.e., that transformations and related single classi fi ers to each of the sensors are taken into consideration. Although the number of transformations could be seen as high for a real time image processing, it has to be taken into account that we use small size images which are fast transformed, and that the classi fi constructed, give the classi fi cation result in milliseconds. A classi-fi er parallelization could be used also to obtain a faster answer, as all of the single classi fi ers can be executed independently, but it is not really necessary in this case.
 drastically reduced with the classi fi cation algorithms and with the combination of the three information sources. As the Kinect algorithm provides a 62.5% of wrong classi fi ed, the fusion only a 3.57%, best RGB 6.38%, the same as best depth, and best thermal 4.88%. 5.3. Extended evaluation to other experimental sets realized an extra experiment using the well known NISIS 2007 competition ( Munder and Gavrila, 2006 ) dataset, in which the presence/absence of pedestrians in an image had to be identi obtained outperforms the results obtained with the classi using both the original images and the images obtained after a single transformation is applied. As with the previous results, this could be improved adding more so fi sticated transformation and applying other CV paradigms such as keypoints detectors (SIFT, ...) or geometrical shape constraints (wavelets...). 6. Conclusions and future works
In this paper, we have introduced a multimodal approach to detect and track people in indoor spaces from a mobile platform.
The detection system is composed of a set of classi fi ers that are built using a combination of Computer Vision and Machine
Learning paradigms. This approach has been designed to manage three kinds of input images, color, depth and temperature to detect people.

The contribution of this paper is twofold. By the one hand, the introduction of multi-sensor fusion combining machine learning and computer vision in the domain of people detection in mobile platforms. And by the other hand, an automatic selection of computer vision transformation sequences to better manage a large amount of transformations to be used before the classi tion is made.

The people detection system performance is evaluated in terms of classi fi cation accuracy. A thorough evaluation process in a manufacturing shop fl oor has been followed in order to analyse the main contributions of the proposed solution: as shown in the experimental evaluation, fusing complementary sensors using a multiclassi fi er results in robust and accurate person detection that outperforms individual sensor based people detection. Further-more, using an EDA algorithm to automatically obtain a set of transformations, an improvement in the classi fi cation results has been obtained for each of the three image types.
 With respect to the multiclassi fi er, classical approaches such as
Bagging ( Breiman, 1996 ) and Boosting ( Freund and Schapire, 1999 ) to improve the single classi fi ers performance have been tried in each of the image source (RGB, Distances and Thermal) but the results were not convincing.

In order to generalize the conclusions, the application of the transformation sequence has been evaluated using other experimental data, the well known NISIS 200. Results con fi that the multi transform sequence outperforms the results obtained with the classi fi ers using both the original images and the images obtained after a single transformation.
 In the near future we envisage
To make a deeper analysis of the use of a multiclassi fi er each image type, and then a combination of RGB, Distances and Thermal classi fi ers.

To develop improved detectors combining/fusing visual cues using particle fi lter strategies, including face recognition and motion information, in order to track people gestures.
To use other single classi fi er paradigms, and other image transformations.

To extend to other scenarios. The approach will be extended toward a museum scenario.

To use more complex computer vision approaches (SIFT, SFOP and so forth).

To integrate with robot navigation planning ability to explicitly consider human in the loop during robot movement.
 Acknowledgments
The work described in this paper was partially conducted within the ktBOT project and funded by KUTXA Obra Social, the Basque Government Research Team grant and the University of the Basque Country UPV/EHU, under grant UFI11/45 (BAILab). References
