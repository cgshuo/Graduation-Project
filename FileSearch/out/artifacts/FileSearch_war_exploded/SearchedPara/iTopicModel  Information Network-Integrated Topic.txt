
Document networks , i.e. , information networks associ-ated with text information, are ubiquitous and indispens-able nowadays due to the popular use of web, blogs, and various kinds of online databases. Examples of document networks are: co-author networks and citation networks with text extracted from publications for each author/paper in bibliographic databases like DBLP 1 ; social network with text extracted from blogs and posts for each user in social network sites like Facebook 2 ; actor cooperation network with text as movie plots they have stared in movie databases like IMDB 3 , and so on. In this paper, we study the problem of building topic models on arbitrary document networks, either weighted or unweighted, directed or undirected. Similar to traditional topic modeling methods, such as PLSA [1] and LDA [2], topic modeling on document net-works tries to soft clustering the documents into different clusters with the meaning of topics, and each topic is described using a multinomial distribution over words. Thus, each topic can be easily understood by browsing only several top probability words in the distribution. Moreover, by presenting topic membership probabilities (referred as topic distribution thereafter) for each document, people may understand the general content of that document. In traditional topic modeling methods, documents are assumed independent with each other, and no links among them will be considered in the modeling process. However, in real life, two documents can be linked together through all sorts of semantics. For example, two papers can be linked together via citations, two webpages can be linked together by their hyper links, and two authors can be linked together according to the co-author relationship. More importantly, intuitively, two closely related documents should have sim-ilar text information, which can be utilized to improve the topic modeling. For example, if two researchers co-author a lot, we can infer that they share similar topics. A set of ideal independent documents in traditional topic model, and a set of mutually dependent (or connected) documents in real case are illustrated in Figure 1 (a) and (b), respectively.
In the studies of information networks, clustering on networks [3] or graph partitioning [4], [5], has been a popular topic for over a decade. Similar to our problem setting, similar objects are grouped into clusters according to links among them. Unfortunately, most of these studies are unable to utilize the text information that each node may contain. Thus, clusters so detected may not be quite accurate, especially when the network structure is sparse, which is often the case in real life. Moreover, since the clusters can only be described using objects rather than text, it is not easy for users to understand the semantic meaning of clusters so obtained, especially when the network is large. Another weak point for merely using link information of a network in clustering is that, connectivity is often required for the network, otherwise an outlier node or an outlier node group is either partitioned into a separate cluster, or randomly assigned to one cluster. However, for example, in a co-author network, a small group of authors may never co-author with other authors, but we would still be interested in their research areas.

Obviously, if one can utilize both the link information among documents and text information for each document, the topic modeling will be significantly improved. Currently, there are some approaches proposed to improving traditional topic models by integrating the complex structural informa-tion with text. One is to design complex generative model for text considering the semantic meaning associated with links [6], [7]. Another is to add a graph regularization constraint to the original log-likelihood objective function [8]. For the first approach, the complex generative model requires lots of expert knowledge and may not be easy to migrate to other datasets; whereas for the second, the combined objective function requires a parameter to adjust the weight of two original objective functions, and it lacks a unified generative model explanation.

In this paper, we propose a unified generative model considering both text and structure information in a doc-ument network, i.e. , an information network with each node containing text information. According to this model, each topic is modeled with a multinomial distribution over words, and each document in the network is associated with a T dimensional random variable  X  i , representing a topic distri-bution vector, and the dependency relationships among the variables are modeled using a multivariate Markov Random Field, given the network structure. Based on the generative model for the text and the structure, we then give the parameter estimations by maximizing the log-likelihood of the joint distribution of the generative model. Moreover, we propose a Q -function-based topic number selection method, which can help users decide the best topic number T ,given the current network with text. Thus, the contributions of this study can be summarized as follows. The remaining of the paper is organized as follows. In Section 2, we give a brief introduction to the related work. In Section 3, we introduce our unified generative model and give the estimation formulas to its parameters. In Section 4, we discuss several related issues in building generative model for different real datasets for practical applications. Section 5 is the experiment study, which compares our model with several state-of-the-art models in two real datasets. Section 6 concludes this study.

Topic modeling over documents is to find T topics that best describe the given corpus by assuming each document is a mixture model of these topics, where T is given and each topic is described using a multinomial distribution over words. PLSA (Probabilistic Latent Semantic Analysis) [1] and LDA (Latent Dirichlet Allocation) [2] are two most well known topic modeling methods. However, both PLSA and LDA treat documents in a given corpus as independent to each other. For example, in LDA, the random variable of topic distribution parameter for each document  X  i is assumed i.i.d from a Dirichlet distribution. However, the independence assumption may not hold in real cases, es-pecially when documents are linked to each other via links in the networks.

In order to integrate the structure information of docu-ments into topic modeling, there are three lines of studies. The first is to build complex generative models for docu-ments, given the additional structural information. For ex-ample, in author-topic model [6], [7], a document is modeled as first choosing an author, then selecting a topic according to the specific author X  X  topic distribution, and then selecting a word from the corresponding topic. Such methods require expert knowledge on the semantic meanings of the links and is difficult to migrate to other datasets. The second line of study is to add a regularization constraint on networks to the traditional topic models, such as in the recently proposed NetPLSA [8]. This type of methods combines two objective functions into a new objective function, but lacks a generative explanation to such combinations. Also, NetPLSA can only deal with undirected networks. A newly proposed method called Relational Topic Model (RTM) [9] proposes another view and tries to model nodes and links separately. However, RTM can only model unweighted networks, where a link is either observed or unobservable. Different from the three methods, in this paper, we propose a unified generative model integrating both text information and structural information, which is able to be applied to any document networks, either weighted or unweighted, directed or undirected. Other related studies include document classi-fication and clustering that integrate both text and structural information, such as in [10], [11], however, they have not addressed the topic modeling problem, but aim at extracting good features for better clustering or classification.
Clustering on networks, which aims at clustering nodes of the network into different groups, has been studied extensively. The most well known family of such methods could be spectral clustering methods [5], and NCut [4] is one of the most popular criteria in such algorithms. However, such algorithms do not consider text information associated with each node to help clustering.

Markov Random Field (MRF) [12], [13], [14] provides a way to model the dependency among random variables, according to their structural information described in a graph. MRF has many applications in image processing, spatial data analysis, and so on. In this paper, we will define a novel multivariate MRF over the random variables of topic distribution for each document, and model their dependency using the links in the document network.

In this section, we build a unified generative model by integrating both structural and text information in a document network.
 A. Preliminaries
We first define some terms and notations that will be used in the following context.
 Definition 1. Document . A document x i in a document collection X = { x 1 ,x 2 ,...,x N } is comprised of a bag of words from a vocabulary Y = { y 1 ,y 2 ,...,y M } , and is represented with vector x i =( c i 1 ,c i 2 ,...,c iM ) , where c denotes the occurrence number of word y l in document x i Definition 2. Document Network . A document network G =
X, E, W is a graph defined on a document set X . E is the link set, and e = x i ,x j  X  E if there is a link from document x i to x j . W is the adjacency matrix denoting the weights of the links, w ij &gt; 0 if there is a link from node x i to x j , and the value of w ij is the strength of the link e = x i ,x j ; w ij =0 , otherwise.
 Definition 3. Neighborhood . The neighborhood of a given document x i in the document network G , denoted as N ( i ) , is defined as N ( i )= N out ( i )  X  N in ( i ) , where N { x j | x i ,x j  X  E } and N in ( i )= { x j | x j ,x i  X  E } representing the out-neighborhood and in-neighborhood re-spectively.

In this paper, we confine our study on document networks with nonnegative weights on links. For undirected networks, they will be transformed to directed networks by converting each undirected link into two directed links. For unweighted networks, the weights of links are defined either 1 or 0, representing the status of observed and unobserved respec-tively. An example of directed document network is given in Example 3.1, which is a paper citation network. Example 3.1 (Paper Citation Network) Let X = { x 1 ,x 2 ,...,x N } be the collection of all the papers in a bibliographic database, each paper x i is a document, which is comprised of a bag of words from a vocabulary Y = { y 1 ,y 2 ,...,y M } . The text information for each paper can be from titles, abstracts, or even full text, according to the information availability of the database. For example, in DBLP, only titles are available for each paper, while for ACM Digital Library 4 , abstracts can also be obtained. We build a network among these papers according to their citation relationship, i.e. ,if x i cites x j , a link e = x with the weight w ij =1 is then added to E .

For topic modeling, given the topic number T , each topic is modeled as a multinomial distribution over words, with the parameter  X  T  X  M = {  X  kl } and M l =1  X  kl =1 , denoting the probability of word y l in topic k . Each document then can be viewed as a mixture model over the T topics, and  X  = {  X  ik denotes the probability that x i belongs to topic k , with the constraints that T k =1  X  ik =1 . Our topic modeling is to find the best  X  and  X  that maximizes the joint distribution of a document network given the current observation of text information and structural information.

For the self containment of this paper, we give a brief introduction to Markov Random Field, mainly following the work of [14].
 Definition 4. Markov Random Field . Given a graph G =
V, E , where V = { 1 ,...,n } , with each number as the label for each node. Let F = { F i } n i =1 be a family of random variables defined on the node set V , i.e. , each node i is associated with a random variable F i . F is said to be a Markov Random Field on V with respect to graph G if and only if the following two conditions are satisfied: where f = { f 1 ,...,f n } is a configuration of F = { F i P ( f ) is the abbreviation of P ( F = f ) , P ( f i ) is the abbre-viation of P ( F i = f i ) , F is all the possible configuration set for F , { X  i } denotes the node set V  X  X  i } ,whichis shortened as  X  i , f  X  i = { f j | j = i } denotes the configuration of F  X  i = { F j | j = i } defined on the nodes V  X  X  i } , and N ( i ) denotes the neighbors of the node i .

When F i is a multivariate variable, we call such MRF multivariate Markov Random Fields. Eq. (2) is called the markovianity property of MRF, also called the local prop-erty . According to Hammersley-Clifford theorem, an MRF defined as above can be factorized into the form of P ( f )= exp { X  U ( f ) } , where Z = f  X  F exp { X  U ( f ) } , is called the partition function , and U ( f )= c  X  X  V c ( f ) , is called the energy function . V c ( f ) is defined over cliques c in the graph G , and is called clique potentials . Therefore, there are two equivalent methods to define an MRF: one is using the local property to specify conditional probabilities P ( f i | f and the other is to directly give the joint probability P ( f ) , according to its global property . Some major notations are summarized in Table I.
 B. Model Set Up
To integrate structure information in the network into topic modeling, we now propose a novel unified generative model, iTopicModel, for generating documents, which has considered the dependency relationships among documents given by the document network. The graphical model of iTopicModel is in Figure 2, which can be viewed as two layers. The top layer has the same topology as the document network G .Let  X  i be the T dimensional multivariate random variable associated with document node x i on G , and  X  i and  X  j be linked if and only if documents x i and x j are linked in network G .Let  X  be {  X  1 ,...,  X  N the family of multivariate random variables of  X  i , and a multivariate Markov Random Field is defined on  X  to model the dependency among documents, which will give different probabilities to different configurations of  X  for The bottom layer is composed of the traditional document generative models for each document, where each word is generated by first choosing a topic z with probability  X  iz according to  X  , which is the current configuration of  X  , and then choosing a word y l from the vocabulary following the distribution of topic z with  X  zl . The joint probability for both text and structure of documents is then defined as: p ( X,  X  | G,  X  )= p (  X  | G ) p ( X |  X ,  X  )= p (  X  | G ) where documents are conditional independent with each other given current configuration of  X  for MRF  X  .We can see that the joint distribution for text and structure is decomposed into two parts, structure part denoted as p (  X  | and text generative part for each document x i as p ( x i We will give definitions for each part in the following. 1) Structure Modeling: Now we will define Markov Random Field  X  on network G , and thus give the definition for the structure probability p (  X  | G ) . Intuitively, for each document, their topic distribution should be very similar to their neighboring documents. Therefore, we now try to model this similarity by specifying the probability of a con-figuration of a document using its neighbors X  configurations. The higher probability the configuration is, the more similar the document and its neighbors are. For each variable  X  i associated with document x i given its neighborhood, we define it as a Dirichlet distribution, with the parameters derived from the out neighbor variables  X  N out ( i ) and the weight of the links between them: prior vector, and B (  X  i )= beta function. We give such definition with the following intuitions: 1) In this definition, we only consider the out-2) A document X  X  topic distribution configuration  X  i should 3) The precision ([15]) about how close a configuration The prior  X  0 i is the prior knowledge we know about doc-ument x i . It can be viewed that, there are T additional documents in the network, with each document purely from one topic (say k th ), i.e. , the topic distribution having the value 1 at the k th component and all zeros at the remaining components. Document x i then has links to each of them, with the weight of  X  0 ik . When setting all  X  0 i as 1 , they can be viewed as a smoothing prior over the graph structure, i.e. , each document can be viewed as linking to the additional T documents with the weight of 1. This smoothing is especially useful if a document has no neighbors in the network, otherwise there will be computational problems. When setting all of  X  0 i as 0, it can be viewed as no prior on the network structure at all. Other settings can be viewed as a prior knowledge of topic distribution  X  i on each document. In the following experiments, we set  X  0 i as 1 .
Next, we will give an MRF definition over  X  , which is able to give the probability to a configuration over all the documents and reflect the intuitions stated above. According to the global property of MRF, the density function of  X  can be factorized into the form: where Z =  X  exp { X  c  X  X  V c (  X  ) } is the partition func-tion 5 , and c stands for the clique in graph G .Inthe following, we only consider the potential functions defined on single nodes and single edges.
 Theorem 1. Given the potential functions defined on nodes and edges as:
V
V define joint distribution p (  X  ) over  X  using the form of Eq. (5), then the joint distribution is p (  X  | G )= and  X  is an MRF.
 tained. To prove  X  is an MRF, two conditions of MRF need to be validated. Eq. (1) is easy to check. Markovianity of MRF denoted by Eq. (2) is also easy to check: p (  X  i |  X  p ( From Eq. (6), we can see that the joint distribution of  X  can be viewed as 1  X  i and p (  X  i |  X  N out ( i ) ) follow the definition in Eq. (4). This means a high probability global configuration for all the documents is the configuration with good agreements in the local structures. 2) Text Modeling: Now, we will consider the text part p ( x i |  X  i ,  X  ) of the joint distribution in Eq. (3). By assuming each document is a mixture model over T topics, i.e. , and each word is generated following multinomial dis-tribution, the probability to generate document x i given parameter  X  and  X  is: p ( x i |  X  ,  X  )= Notice that p ( x i |  X  i ,  X  )= p ( x i |  X  ,  X  ) , since x dependent on topic distribution of the same document  X  i . In traditional topic modeling methods, documents are then assumed to be independent to each other to derive the joint distribution. In PLSA [1], the joint probability is equivalent to the multiplication of probabilities of observing each doc-ument N i =1 p ( x i |  X  ,  X  ) . In LDA [2], the distributions over topics for each document  X  i =( X  i 1 ,...,  X  iT ) is assumed to follow the Dirichlet distribution with parameter  X  , which serves as a prior, independently, and the joint probability is the multiplication of the probabilities of generating each of the documents N i =1 p ( x i |  X  ,  X  ) .
 C. Parameter Estimation
In Section III-B, a generative model for document network called iTopicModel has been proposed and a joint probability function of both text and structure is thus defined. In this section, we will give the parameter estimation method by maximizing the log-likelihood of the joint distribution. log L = log p ( X,  X  | G,  X  )( replacing with Eqs. (6) and (7) ) where Z is a constant and can be neglected for maximiza-tion.

The parameters we are going to estimate are  X = (  X  NT + TM in total. We now use an approximate EM algorithm to get the best estimators by maximizing the log-likelihood in Eq. (8), with the constraints that T k =1  X  for all i and M l =1  X  kl =1 for all k . The hidden variable in the likelihood function is the topic indicator variable z for each word in each document, and p ( z = k | x i ,y l ,  X ) p ( z = k, y l | x i ,  X ) = p ( z | x i ,  X ) p ( y l | z,  X ) , thus:
In the E-step, conditional expectation of log L given current value of parameters and conditional distribution of z is: In the M-step, find the best  X  that maximizes Q function:  X  the parameters. By standard calculation with the help of Lagrange multipliers, the updated formulas for  X  are:
By iteratively applying Eqs. (9), (10) and (11), a local maximum of  X  will be achieved. By observing Eq. (10), it is interesting to see that at each iteration,  X  i uses two parts of information to update itself, one is from structural information and the other is from text information. What is more, the structural information used in the updating is just at each iteration out neighboring topic distributions are used as priors to derive posterior topic distribution for x i given the observation of text in the document x i .
 D. Discussions of MRF Modeling on Network Structure
In Section III-B, we have given a new topic model, iTopicModel, that integrates both text information and struc-tural information among documents. We decompose the joint distribution of text and structure into two independent components, one is the structure layer modeled with MRF, and the other is the text layer given the current structure pa-rameter modeled as traditional topic model. For the structure layer, we define an MRF using Eq. (6). Actually, different MRF models can be used in the structure layer of our framework, with different intuitions.

Now we give another MRF definition over the structural layer using both conditional probability and joint probability definition, and then relate it with a newly proposed graph regularization-based topic model method NetPLSA [8].
It can be proved that an equivalent joint distribution can be defined in the following global definition.
From the definition, we can see that the direction of links are not considered, and even directed networks will be con-sidered as indirected networks. Under this joint distribution of  X  , the log-likelihood can be derived as (with constant normalization part Z neglected): Comparing Eq. (12) with objective function that is to be maximized given by Eq. (6) in NetPLSA [8], which we rewrite using the notations in this paper in the following, we then can find that this log-likelihood (Eq. (12)) is a special case of Eq. (13) with a fixed parameter  X  = 1 2 .Itis easy to see, we can multiply potential functions V 2 (  X  i with constant c to get other  X  in Eq. (13). Usually, this constant is represented as 1 T in MRF, where T is the temperature of the system. When temperature T is very high, a higher energy state U (  X  ) is allowed, and in our case a more irregular network structure is allowed. Notice that, a similar parameter can be used in iTopicModel. As we can see, NetPLSA can be integrated into the framework of our model, with a different definition of Markov Random Field, but with the limits that can only deal with undirected networks. Other definitions may also be used under this framework, but need careful reasoning and tests.
In this section, we discuss several practical issues in real applications, such as how to decide the number of topics, how to build a concept hierarchy, and how to choose among several possible networks. Experiments are provided in the later section to demonstrate these issues.
 A. Deciding the Number of Topics
How to set the number of topics in topic modeling is always a challenging problem. One possible method is varying the topic number T and choosing the one that maximizes the log-likelihood log( L | T ) , such as in Griffiths X  work [16]. However, in our model, partition function Z is usually intractable to calculate, and it is a function of T as well. It is difficult to get the exact value of log( L In network clustering, there is a well known modularity function called Q -function [3], which provides a measure to evaluate the goodness of a clustering on the network. In the original work, they only considered the network with either 0 or 1 weights for edges. Now we extend it to networks with non-negative weights of edges. In our method, topic distribution for each document could be viewed as a soft clustering result. Then we map it to a hard clustering by assigning the cluster label with the highest probability to each x i .Let C N  X  T denote the hard clustering indicator matrix, we thus give the Q -function over network G given  X  as: where X k denotes the set of nodes belonging to cluster k , w ( X k ,X k ) denotes the total weights of links whose both nodes are in cluster k , w ( X k ,X ) denotes the total weights of links that contains at least one node in cluster k , and D = N i =1 N j =1 w ij is the total weights of all the edges. This formula calculates the difference between within-cluster percentage of edges and the percentage of edges in a random case. Q varies from 0 , when the net-work is totally random and has no clustering structure; to approaching 1 (actually 1  X  1 /T ), when there are no inter-cluster edges at all. According to [3], Q lies in the range of [0 . 3 , 0 . 7] for networks with strong community structure. By varying the number of topics, we can select the T that maximizes Q -function defined in Eq. (14).
 B. Building Concept Hierarchies
Concept hierarchies would be very useful for data analy-sis. For example, concept hierarchies such as ACM Classifi-cation System 6 , would help authors to classify their papers, and help users index and search papers in a large collection of bibliographic data. However, it requires a lot of human labor to build such concept hierarchies manually, when the dataset is large. Also, the concept hierarchies may be changing along with time, e.g. , ACM Classification System has been changed several times since its first appearance. Therefore, automatically building concept hierarchies that are described using topics, would be very useful for data analysis tasks, e.g. , OLAP service.

A key problem in building concept hierarchy using topic modeling in a heterogeneous network that contains multiple types of objects is that, we should carefully select different object types to be the documents in different scales. For example, a conference network should be enough to find the first level topics. While for finer level topics, conference will be too coarse. In contrast, it may not be wise to use papers to get the first level topics, since they contain too detailed information and are not able to generate the overall view of the data. We will illustrate how to build a concept hierarchy using the DBLP data as an example in the experiment part. It turns out that, by recursively applying iTopicModel on the sub document network, with Q -function as the branch selection measure, a concept hierarchy can be automatically built. C. Effects of Network Structures
For the same document corpus, there are multiple ways to construct networks among documents. For a collection of papers in bibliographic database, paper citation network introduced in Ex. 3.1 can be built. Similarly, networks can also be constructed based on co-author relationships, or through text similarity. Consider the following two extreme cases of the relation between network and text information: 1) The links of the network among documents are ran-2) The links of the network among documents are built In order to measure how close a network over the documents to the text information this document corpus contains, we propose a correlation measure between network and text information: where G, X, x i ,x j are defined as in Section III-A. sim ( x i ,x j ) is the similarity measure for two documents, and it is defined as cos( x i , x j ) , where x i is the word count vector defined in Section III-A. We can also use tfidf instead of word count in the representation vector. How correlation between network and text impacts the topic modeling performance will be studied in Section V-D.
Another question would be, shall we trust all the links in the network? The answer will be no. According to our experimental study, a network that has too many small weight links will degrade the performance of iTopicModel, since small weight links may happen occasionally, only link with a larger weight shows a consistent, strong relation among documents. So we transform our original networks into a KNN network, which means a document only keeps K most connected documents as neighbors, according to the weight of links.

In this section, we apply our model to several real datasets, and show its usefulness in real applications and its effectiveness over several state-of-the-art models. Also, we study the impacts of different network structures on the topic modeling, which could be served as a hint to choose network structures in real cases.
 A. Datasets
We use two datasets in the experiments, the DBLP dataset and the Cora Research Paper Classification dataset 7 .Forthe DBLP data, we extract two datasets: (i) the  X  X ll-area X  data set, which contains top 1000 conferences and top 50000 authors by their publication numbers, and all the publications of these authors; and (ii)  X  X our-area X  dataset, which includes 20 major conferences from four related areas, i.e. , database, data mining, machine learning and information retrieval, and all the 28702 authors and their publications in these conferences. For the Cora dataset, after preprocessing, we get 19396 papers with their citation lists, author lists and title information. Each paper in Cora has a classification label from total 70 classes. Notice that, for both datasets, we only have titles as text information for papers. B. Building Concept Hierarchies in DBLP
In this case study, we use the DBLP  X  X ll-area X  dataset to build concept hierarchies in Computer Science. Different document networks can be obtained from the data. For conferences, network is derived from shared author numbers between conferences, and document for each conference is the compacted titles of all the papers in that conference. For authors, networks is formed by the co-authorship, i.e. , the number of papers they co-authored, and the document for each author is the grouped titles of all the papers that author published. For papers, networks can be formed by either the similarity of text between paper titles or the shared author number between papers, and the title itself serves as a document. As discussed in Section IV-B, for the first level, we use conference network to generate the most coarse topics. By using the modularity measure Q -function defined as in Eq. (14), we found 7 is the best topic number. The Q -function measure varying with different topic number T is shown in Figure 3. The seven topics are summarized in Table II by top-10 words in each topic. For further modeling on a parent topic, we use conferences that have max probabilities in that topic as a constraint to select top 1000 authors appearing in those conferences and build the author network. The subtopics in the subarea of Topic 4 at the first level is shown in Table III. C. Performance Study on Topic Modeling
How to judge whether a topic modeling is good? One typical method is to print the top-ranked words and judge them by experience. We use two measures to compare the results of different topic modeling methods: (i) NMI (Normalized Mutual Information) [17] that measures the goodness of document clustering results mapped from topic distribution over documents  X  by comparing them with the human-labeled clustering results, and (ii) Q -function measure as defined in Eq. (14) that measures the consistency of the clustering results over the network. NMI is used to compare two clustering results, say C and C , without knowing the mapping relation between them: where i and j are cluster labels in C and C , p ( i, j ) is the percentage of shared common objects in both clusters i and j , p C ( i ) the percentage of objects in i in clustering p
C ( j ) the percentage of objects in j in clustering C .NMI is in the range of [0 , 1] , and higher value means higher agreement among two clusterings.

We use three different networks, conference net and author net from DBLP, and paper citation net from Cora. Conference net and author net (top 1000 authors used) are extracted from the DBLP  X  X our-area X  dataset using the methods described in Section V-B, named as  X  X onfNet X  and  X  X uthorNet X  respectively. For the paper citation net, we selected five classes in the level of whole computer science, named as  X  X aperNet-Cite X . We labeled the 20 conferences and 200 authors sampled from the 1000 authors to the four areas for  X  X onfNet X  and  X  X uthorNet X , and use the classification labels for papers from Cora data.
Four topic modeling methods, PLSA, LDA, netPLSA, and iTopicModel, are studied. We use topic modeling toolbox [18] for the LDA method. The results are summarized in Tables IV and V. All the results are based on 10 rounds running of each algorithm. The experiments show that, in the measure of NMI, iTopicModel outperforms or has compa-rable performance in all the datasets, and especially good at the document network that with very short text information in each document, such as  X  X aperNet X . We can see that, without the information of links among papers, PLSA and LDA can rarely get the right clusters for each document in  X  X aperNet X . But for NetPLSA and iTopicModel, since they have considered the network structure, the clustering results are much better; and the latter is even better than the former in most datasets, since it has a better MRF definition to model the dependency relation among documents. Notice that, the NMI is not that high partly because the classification data provided in Cora is not that accurate, warned by the data provider. Also, we actually can pick the best result among several runnings according to the final logL (the larger the better). In our case for dataset  X  X aperNet-Cite X , we can get results with NMI above 0.5. In the measure of Q -function, iTopicModel consistently provides better topics that are consistent with the network structure.
 D. Network Structure Study
We build three different networks for papers using the dataset of Cora, and study the relationship between their correlations to the text and the topic modeling performance. Besides the citation networks, we also construct networks for papers using the co-author number, i.e. ,  X  X aperNet-Author X , and co-text number, i.e. ,  X  X aperNet-Text X . Co-text number means the co-occurrence word number for any two papers. The results are summarized in Table VI. From the results we can see that, for paper network built from text, the clustering performance is very similar to topic modeling using only text information (see NMI result in Table IV for PLSA). Actually,  X  X onfNet X  also has a high correlation between network and text, and thus the performance for iTopicModel has not improved too much from PLSA for this network.
In this paper, a new framework of generative topic model called iTopicModel is proposed that integrates both network structure and text information for the popularly encountered document networks. This model has a two-layer graphical model structure. On the top, a reasonable multivariate Markov Random Field is defined to model the dependency relations among documents. On the bottom, a traditional document generative model is used, which is conditionally independent with each other given the current topic distribution configurations. A joint probability function is then defined based on the graphical model. We then propose an EM-based iterative solution to estimate the set of best parameters that maximizes the log-likelihood of the joint distribution. Our experiments show that this model is more effective than the state-of-the-art topic modeling methods, in both aspects: following human intuition and being consistent with the network structure. Also, we show that this model, with the help of Q -function, can help us automatically build concept hierarchy in online databases. The future work could be on how to build topic models that integrate different network structures with text information.
