 The problem of modeling relational information among objects, such as pairwise relations repre-sented as graphs, arises in a number of settings in machine learning. For example, scientific liter-ature connects papers by citation, the Web connects pages by links, and protein-protein interaction data connect proteins by physical interaction records. In these settings, we often wish to infer hidden attributes of the objects from the pairwise observations. For example, we might want to compute a clustering of the web-pages, predict the functions of a protein, or assess the degree of relevance of a scientific abstract to a scholar X  X  query. Unlike traditional attribute data measured over indi-vidual objects, relational data violate the classical independence or exchangeability assumptions made in machine learning and statistics. The objects are dependent by their very nature, and this interdependence suggests that a different set of assumptions is more appropriate.
 Recently proposed models aim at resolving relational information into a collection of connectivity motifs. Such models are based on assumptions that often ignore useful technical necessities, or im-portant empirical regularities. For instance, exponential random graph models [11] summarize the variability in a collection of paired measurements with a set of relational motifs, but do not provide a representation useful for making unit-specific predictions. Latent space models [4] project individ-ual units of analysis into a low-dimensional latent space, but do not provide a group structure into such space useful for clustering. Stochastic blockmodels [8, 6] resolve paired measurements into groups and connectivity between pairs of groups, but constrain each unit to instantiate the connec-tivity patterns of a single group as observed in most applications. Mixed membership models, such as latent Dirichlet allocation [1], have emerged in recent years as a flexible modeling tool for data where the single group assumption is violated by the heterogeneity within a unit of analysis X  X .g., a document, or a node in a graph. They have been successfully applied in many domains, such as doc-ument analysis [1], image processing [7], and population genetics [9]. Mixed membership models associate each unit of analysis with multiple groups rather than a single groups, via a membership probability-like vector. The concurrent membership of a data in different groups can capture its dif-ferent aspects, such as different underlying topics for words constituting each document. The mixed membership formalism is a particularly natural idea for relational data, where the objects can bear multiple latent roles or cluster-memberships that influence their relationships to others. Existing mixed membership models, however, are not appropriate for relational data because they assume that the data are conditionally independent given their latent membership vectors. Conditional inde-pendence assumptions that technically instantiate mixed membership in recent work, however, are inappropriate for the relational data settings. In such settings, an objects is described by its rela-tionships to others. Thus assuming that the ensemble of mixed membership vectors help govern the relationships of each object would be more appropriate.
 Here we develop mixed membership models for relational data and we describe a fast variational inference algorithm for inference and estimation. Our model captures the multiple roles that ob-jects exhibit in interaction with others, and the relationships between those roles in determining the observed interaction matrix. We apply our model to protein interaction and social networks. Observations consist of pairwise measurements, represented as a graph G = ( N ,Y ) , where Y ( p,q ) denotes the measurement taken on the pair of nodes ( p,q ) . In this section we consider observations consisting of a single binary matrix, where Y ( p,q )  X  X  0 , 1 } , i.e., the data can be represented with a directed graph. The model generalizes to two important settings, however, as we discuss below X  X  collection of matrices and/or other types of measurements. We summarize a collection of pairwise measurements with a mapping from nodes to sets of nodes, called blocks , and pairwise relations among the blocks themselves. Intuitively, the inference process aims at identifying nodes that are similar to one another in terms of their connectivity to blocks of nodes. Similar nodes are mapped to the same block. Individual nodes are allowed to instantiate connectivity patterns of multiple blocks. Thus, the goal of the analysis with a Mixed Membership Blockmodel (MMB) is to identify (i) the mixed membership mapping of nodes, i.e., the units of analysis, to a fixed number of blocks, K , and (ii) the pairwise relations among the blocks. Pairwise measurements among N nodes are then generated according to latent distributions of block-membership for each node and a matrix of block-to-block interaction strength. Latent per-node distributions are specified by simplicial vectors. Each node is associated with a randomly drawn vector, say ~ X  i for node i , where  X  i,g denotes the probability of node i belonging to group g . In this fractional sense, each node can belong to multiple groups with different degrees of membership. The probabilities of interactions between different groups are defined by a matrix of Bernoulli rates B ( K  X  K ) , where B ( g,h ) represents the probability of having a connection from a node in group g to a node in group h . The indicator vector ~z p  X  q denotes the specific block membership of node p when it connects to node q , while ~z p  X  q denotes the specific block membership of node q when it is connected from node p . The complete generative process for a graph G = ( N ,Y ) is as follows: Note that the group membership of each node is context dependent , i.e., each node may assume different membership when interacting with different peers. Statistically, each node is an admixture of group-specific interactions. The two sets of latent group indicators are denoted by { ~z p  X  q : p,q  X  N} =: Z  X  and { ~z p  X  q : p,q  X  X } =: Z  X  . Further, the pairs of group memberships that underlie asymmetric interaction networks. Equality may be enforced when modeling symmetric interactions. The joint probability of the data Y and the latent variables { ~ X  1: N ,Z  X  ,Z  X  } sampled according to the MMB is: p ( Y,~ X  1: N ,Z  X  ,Z  X  | ~ X ,B ) = Y Introducing Sparsity. Adjacency matrices encoding binary pairwise measurements often contain a large amount of zeros, or non-interactions; they are sparse . It is useful to distinguish two sources of non-interaction: they may be the result of the rarity of interactions in general, or they may be an indication that the pair of relevant blocks rarely interact. In applications to social sciences, for instance, nodes may represent people and blocks may represent social communities. In this setting, it is reasonable to expect that a large portion of the non-interactions is due to limited opportunities of contact between people in a large population, or by design of the questionnaire, rather than due to deliberate choices, the structure of which the blockmodel is trying to estimate. It is useful to account for these two sources of sparsity at the model level. A good estimate of the portion of zeros that should not be explained by the blockmodel B reduces the bias of the estimates of B  X  X  elements. We introduce a sparsity parameter  X   X  [0 , 1] in the model above to characterize the source of non-interaction. Instead of sampling a relation Y ( p.q ) directly the Bernoulli with parameter specified as the result of assuming that the probability of a non-interaction comes from a mixture, 1  X   X  pq = explained by the blockmodel B . A large value of  X  will cause the interactions in the matrix to be weighted more than non-interactions, in determining plausible values for { ~ X ,B,~ X  1: N } . Recall that { ~ X ,B } are constant quantities to be estimated, while { ~ X  1: N ,Z  X  ,Z  X  } are unknown vari-able quantities whose posterior distribution needs to be determined. Below, we detail the variational expectation-maximization (EM) procedure to carry out approximate estimation and inference. 2.1 Variational E-Step During the E-step, we update the posterior distribution over the unknown variable quantities { ~ X  1: N ,Z  X  ,Z  X  } . The normalizing constant of the posterior is the marginal probability of the data, which requires an intractable integral over the simplicial vectors ~ X  p , We appeal to mean-field variational methods [5] to approximate the posterior of interest. The main idea behind variational methods is to posit a simple distribution of the latent variables with free parameters, which are fit to make the approximation close in Kullback-Leibler divergence to the true posterior of interest. The log of the marginal probability in Equation 1 can be bound as follows, by introducing a distribution of the latent variables q that depends on a set of free parameters. We specify q as the mean-field fully-factorized family, q ( ~ X  1: N ,Z  X  ,Z  X  | ~ X  1: N ,  X   X  ,  X   X  ) , where { ~ X  1: N ,  X   X  ,  X   X  } is the set of free variational parameters that must be set to tighten the bound. We tighten the bound with respect to the variational parameters, to minimize the KL divergence between q and the true posterior. The update for the variational multinomial parameters is for g,h = 1 ,...,K . The update for the variational Dirichlet parameters  X  p,k is for all nodes p = 1 ,...,N and k = 1 ,...,K .
 Nested Variational Inference. To improve convergence, we developed a nested variational infer-ence scheme based on an alternative schedule of updates to the traditional ordering [5]. In a na  X   X ve iteration scheme for variational inference, one initializes the variational Dirichlet parameters ~ X  1: N and the variational multinomial parameters ( ~  X  p  X  q , ~  X  p  X  q ) to non-informative values, and then it-erates until convergence the following two steps: (i) update ~  X  p  X  q and  X  p  X  q for all edges ( p,q ) , and (ii) update ~ X  p for all nodes p  X  X  . At each variational inference cycle one needs to allocate NK + 2 N 2 K scalars. In our experiments, the na  X   X ve variational algorithm often failed to converge, or converged only after many iterations. We attribute this behavior to dependence between ~ X  1: N and B in the model, which is not accounted for by the na  X   X ve algorithm. The nested variational inference algorithm retains portion of this dependence across iterations by following a particular path to con-vergence. We keep the block of free parameters ( ~  X  p  X  q , ~  X  p  X  q ) at their optimal values conditionally on the other variational parameters. These parameters are involved in the updates of parameters in ~ X  1: N and in B , thus effectively providing a channel to maintain some dependence among them. From a computational perspective, the nested algorithm trades time for space thus allowing us to deal with large graphs. At each variational cycle we allocate NK + 2 K scalars only. The algorithm can be parallelized, and, empirically, leads to a better likelihood bound per unit of running time. 2.2 M-Step During the M-step, we maximize the lower bound in Equation 2, used as a surrogate for the like-lihood, with respect to the unknown constants { ~ X ,B } . In other words, we compute the empirical Bayes estimates of the hyper-parameters. The M-step is equivalent to finding the MLE using ex-pected sufficient statistics under the variational distribution. We consider the maximization step for each parameter in turn. A closed form solution for the approximate maximum likelihood estimate of ~ X  does not exist. We used linear-time Newton-Raphson, with gradient and Hessian to find optimal values for ~ X  , numerically. The approximate MLE of B is for every pair ( g,h )  X  [1 ,K ] 2 . Finally, the approximate MLE of the sparsity parameter  X  is Alternatively, we can fix  X  prior to the analysis; the density of the interaction matrix is estimated attributes all the information in the non-interactions to the point mass, i.e., to latent sources other than the block model B or the mixed membership vectors ~ X  1: N . It can be used, however, as a quick recipe to reduce the computational burden during exploratory analyses.
 Several model selection strategies exist for hierarchical models. In our setting, model selection translates into the choice of the number of blocks, K . Below, we chose K with held-out likelihood in a cross-validation experiment, on large networks, and with approximate BIC, on small networks. 2.3 Summarizing and De-Noising Pairwise Measurements It is useful to consider two data analysis perspectives the MMB can offer: (i) it summarizes the data, Y , in terms of the global blockmodel, B , and the node-specific mixed memberships,  X  s , (ii) it de-noises the data, Y , in terms of the global blockmodel, B , and interaction-specific single memberships, Zs . In both cases the model depends on a small set of unknown constants to be estimated:  X  , and B . The likelihood is the same in both cases, although, the reasons for including the set of latent variables Zs differ. When summarizing data, we could integrate out the Zs analytically; this leads to numerical optimization of a smaller set of variational parameters,  X  s . We choose to keep the Zs to simplify inference. When de-noising, the Zs are instrumental in estimating posterior expectations of each interactions individually X  X  network analog to the Kalman Filter. The posterior expectations of an interaction is computed as ~ X  p 0 B ~ X  q , and ~  X  p  X  q 0 B ~  X  p  X  q , in the two cases. We evaluated the MMB on simulated data and on three collections of pairwise measurements. Re-sults on simulated data sampled accordingly to the model show that variational EM accurately recov-ers the mixed membership map, ~ X  1: N , and the blockmodel, B . Cross-validation suggests an accurate estimate for K . Nested variational scheduling of parameter updates makes inference parallelizable and a typically reaches a better solution than the na  X   X ve scheduling.
 First we consider, whom-do-like relations among 18 novices in a New England monastery. The unsupervised analysis demonstrates the type of patterns that MMB recovers from data, and allows us to contrast the summaries of the original measurements achieved through prediction and de-noising. The data was collected by Sampson during his stay at the monastery, while novices were preparing to join the monastic order [10]. Sampson X  X  original analysis is rooted in direct anthropological observations. He made a strong case for the existence of tight factions among the novices: the loyal opposition (whose members joined the monastery first), the young turks (who joined later on), the outcasts (who were not accepted in the two main factions), and the waverers (who did not take sides). The events that took place during Sampson X  X  stay at the monastery supported his observations X  members of the young turks resigned or were expelled over religious differences (John and Gregory). Scholars in the social sciences typically regard the faction labels assigned by Sampson to the novices (and his conclusions, more in general) as ground truth to the extent of assessing the quality of results of quantitative analyses; we shall do the same here. Using the nested variational EM algorithm above, we fit an array of mixed membership blockmodels with different values of K , and collected model estimates {  X   X ,  X  B } and posterior mixed membership vectors ~ X  1:18 for the novices. We used an approximation of BIC to choose the value of K supported by the data. This criterion selects  X  K = 3 , the same number of proper groups that Sampson identified based on anthropological observations X  the waverers are interstitial members, rather than a group. Figure 2 shows the patterns that the mixed membership blockmodel with  X  K = 3 recovers from data. In particular, the top-left panel shows a graphical representation of the blockmodel  X  B . The block that we can identify a-posteriori with the loyal opposition is portrayed as central to the monastery, while the block identified with the outcasts shows the lowest internal coherence, in accordance with Sampson X  X  observations. The top-right panel illustrates the posterior means of the mixed membership scores, E [ ~ X  | Y ] , for the 18 monks in the monastery. The model (softly) partitions the monks according to Sampson X  X  classification, with Young Turks, Loyal Opposition, and Outcasts dominating each corner respectively. Notably, we can quantify the central role played by John Bosco and Gregory, who exhibit relations in all three groups, as well as the uncertain affiliations of Ramuald and Victor; Amand X  X  uncertain affiliation, however, is not captured. The bottom panels contrast the different resolution of the original adjacency matrix of whom-do-like sociometric relations (left panel) obtained with the two analyses MMB enables. Figure 2: Top-Left: Estimated blockmodel,  X  B . Top-Right: Posterior mixed membership vectors, ~ X  1:18 , projected in the simplex. The estimates correspond to a model with Numbered points can be mapped to monks X  names using the legend on the right. The colors identify the four factions defined by Sampson X  X  anthropological observations. Bottom: Original adjacency matrix of whom-do-like sociometric relations (left), relations predicted using approximate MLEs for ~ X  1: N and B (center), and relations de-noised using the model including Z s indicators (right). If the goal of the analysis if to find a parsimonious summary of the data, the amount of relational sociomatrix (central panel). If the goal of the analysis if to de-noising a collection of pairwise measurements, the amount of relational information that is revealed by  X   X ,  X  B and E [ Z  X  ,Z  X  | Y ] leads to a finer reconstruction of the original sociomatrix, Y  X  X elations in Y are re-weighted according to how much they make sense to the model (right panel). Substantively, the unsupervised analysis of the sociometric relations with MMB offers quantitative support to several of Sampson X  X  observations. Second, we consider a friendship network among a group of 69 students in grades 7 X 12. The analysis here directly compares clustering results obtained with MMB to published results obtained with competing models, in a setting where a fair amount of social segregation is expected [2, 3]. The data is a collection of friendship relations among 69 students in a school surveyed in the Na-tional Study of Adolescent Health. The original population in the school of interest consisted of 71 students. Two students expressed no friendship preferences and were excluded from the analysis. We used variational EM algorithm to fit an array of mixed membership blockmodels with different values of K , collected model estimates, and used an approximation to BIC to select K . This proce-dure identified  X  K = 6 as the model-size that best explains the data; note that six is the number of grade-groups in the student population. The blocks are clearly interpretable a-posteriori in terms of grades, thus providing a mapping between grades and blocks. Conditionally on such a mapping, we assign students to the grade they are most associated with, according to their posterior-mean mixed membership vectors, E [ ~ X  n | Y ] . To be fair in the comparison with competing models, we assign students with a unique grade X  X espite MMB allows for mixed membership. Table 1 computes the correspondence of grades to blocks by quoting the number of students in each grade-block pair, for MMB versus the mixture blockmodel (MB) in [2], and the latent space cluster model (LSCM) in [3]. The higher the sum of counts on diagonal elements is the better is the correspondence, while the higher the sum of counts off diagonal elements is the worse is the correspondence. MMB performs best by allocating 63 students to their grades, versus 57 of MB, and 37 of LSCM. Correspondence only partially partially captures goodness of fit, however, it is a good metric in the setting we con-sider, where a fair amount of clustering is present. The extra-flexibility MMB offers over MB and LSCM reduces bias in the prediction of the membership of students to blocks, in this problem. In other words, mixed membership does not absorb noise in this example, rather it accommodates variability in the friendship relation that is instrumental in producing better predictions. Table 1: Grade levels versus (highest) expected posterior membership for the 69 students, according to three alternative models. MMSB is the proposed mixed membership stochastic blockmodel, MSB is the mixture blockmodel in [2], and LSCM is the latent space cluster model in [3].
 Third, we consider physical interactions among 871 proteins in yeast. The analysis allows us to eval-uate the utility of MMB in summarizing and de-noising complex connectivity patterns quantitatively, using an independent set of functional annotations X  X onsider two models that suggest different sets of interactions as reliable; we prefer the model that reveals functionally relevant interactions. The pairwise measurements consist of a hand-curated collection of physical protein interactions made available by the Munich Institute of Protein Sequencing (MIPS). The yeast genome database provides independent functional annotations for each protein, which we use for evaluating the func-tional content of the protein networks estimated with the MMB from the MIPS data, as detailed below. We explored a large model space, K = 2 ... 225 , and used five-fold cross-validation to iden-tify a blockmodel B that reduces the dimensionality of the physical interactions among proteins in the training set, while revealing robust aspects of connectivity that can be leveraged to predict phys-ical interactions among proteins in the test set. We determined that a fairly parsimonious model, K = 50 , provides a good description of the observed physical interaction network. This finding supports the hypothesis that proteins derived from the MIPS data are interpretable in terms func-tional biological contexts. Alternatively, the blocks might encode signal at a finer resolution, such as that of protein complexes. If that was the case, however, we would expect the optimal number of blocks to be significantly higher; 871 / 5  X  175 , given an average size of five proteins in a protein complex. We then evaluated the functional content of the posterior induced by MMB. The goal is to assess to what extent MMB reveals substantive information about the functionality of proteins that can be used to inform subsequent analyses. To do this, first, we fit a model on the whole data set to estimate the blockmodel, B (50  X  50) , and the mixed membership vectors between proteins and blocks, ~ X  1:871 , and second, we either impute physical interactions by thresholding the posterior expectations computed using blockmodel and node-specific memberships (summarization task), or we de-noise the observed interactions using blockmodel and pair-specific memberships (de-noising task). Posterior expectations of each interaction are in [0 , 1] . Thresholding such expectations at q , for instance, leads to a collection of binary physical interactions that are at reliable with probabil-ity p  X  q . We used an independent set of functional annotations from the yeast database (SGD at www.yeastgenome.org) to decide which interactions are functionally meaningful; namely those between pairs of proteins that share at least one functional annotation. In this sense, between two models that suggest different sets of interactions as reliable, our evaluation assigns a higher score Figure 3: Functional content of the MIPS collection of protein interactions (yellow diamond) on a precision-recall plot, compared against other published collections of interactions and microarray data, and to the posterior estimates of the MMB models X  X omputed as described in the text. to the model that reveals functionally relevant interactions according to SGD. Figure 3 shows the functional content of the original MIPS collection of physical interactions (point no.2), and of the collections of interactions computed using ( B,  X  s ) , the light blue (  X  X  ) line, and using ( B,Zs ) , the dark blue (  X  +) line, thresholded at ten different levels X  X recision-recall curves. The posterior means of  X  s provide a parsimonious representation for the MIPS collection, and lead to precise protein interaction estimates, in moderate amount (  X  X  line). The posterior means of Zs provide a richer representation for the data, and describe most of the functional content of the MIPS collection with high precision (  X  + line). Importantly, the estimated networks corresponding to lower levels of recall for both model variants (i.e.,  X  and + ) feature a more precise functional content than the original network. This means that the proposed latent block structure is helpful in effectively de-noising the collection of interactions X  X y ranking them properly. On closer inspection, dense blocks of predicted interactions contain known functional predictions that were not in the MIPS collection, thus effectively improving the quality of the protein binding data that instantiate cellular activity of specific biological contexts, such as biopolymer catabolism and homeostasis. In conclusion, our results suggest that MMB successfully reduces the dimensionality of the data, while discovering in-formation about the multiple functionality of proteins that can be used to inform follow-up analyses. Remarks. A. In the relational setting, cross-validation is feasible if the blockmodel estimated on training data can be expected to hold on test data; for this to happen the network must be of reasonable size, so that we can expect members of each block to be in both training and test sets. In this setting, scheduling of variational updates is important; nested variational scheduling leads to efficient and parallelizable inference. B. MMB includes two sources of variability, B,  X  s , that are apparently in competition for explaining the data, possibly raising an identifiability issue. This is not the case, however, as the blockmodel B captures global/asymmetric relations, while the mixed membership vectors  X  s capture local/symmetric relations. This difference practically eliminates the issue, unless there is no signal in the data to begin with. C. MMB generalizes to two important cases. First, multiple data collections Y 1: M on the same objects can be generated by the same latent vectors. This might be useful, for instance, for analyzing multivariate sociometric relations simultaneously. Second, in the MMSB the data generating distribution is a Bernoulli, but B can be a matrix of parameterizes for any kind of distribution. For instance, technologies for measuring interactions between pairs of proteins, such as mass spectrometry and tandem affinity purification, which return a probabilistic assessment about the presence of interactions, thus setting the range of Y  X  [0 , 1] .
