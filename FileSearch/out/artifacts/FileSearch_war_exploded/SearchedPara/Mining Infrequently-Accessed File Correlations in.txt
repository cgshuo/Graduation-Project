 Nowadays, due to the dramatic growth of the number of Internet users, the response time has become the central performance barrier for many websites. Analysis of statistics of web sites has shown that the file system as one of the fundamental infrastructures of web applications, often becomes the bottleneck. With the development of high bandwidth devices, such as RAID and SAN, throughput of file systems has seen remarkable improvements. However, the access latency still remains a problem, which is not likely to improve significantly due to the physical limitations of storage devices and network transfer latencies [3]. 
Several techniques, such as caching, prefetching, disk layout optimization and so on, can be used to reduce access latency. Being automatic or not, these techniques all heavily depend on file access patterns. The automatic methods usually deploy techniques such as caching, prefetching, or layout optimization based on predictions of file access patterns. In comparison, non-automatic methods are based on access pattern hints given by applications. Simple access patterns, such as temporal locality, spatial locality and sequential locality, have been heavily exploited in commodity storage systems, file systems and database systems. However, file access patterns in real system may be more complex, making it difficult to be captured accurately. Moreover, access latency may be exacerbated, when the predicted access patterns mismatch access patterns of application. 
File correlations are one of the common access patterns in file systems. Obviously, example, C source code file is correlated to its corresponding header files, and correlated files are usually accessed together, correlations can be harnessed to improve storage cache hit ratio, optimize file layout, prefetch more accurately and reduce file access latency. Correlations are also helpful in file hoarding in the mobile environments, which lead to better support for disconnected operation [4][19]. 
There are already some techniques in the literature, which aims at mining file However, to the best of our knowledge, these methods are not designed for searching storage systems is more challenging: Firstly, the huge number of files desires fast and environments make it difficult to maintain global data structures required by most previous approaches. 
Another major disadvantage of previous methods is that they focus on high support correlated files. (Support is defined in Section 3, higher support indicates higher access frequency). However low support correlated files are more valuable for web applications, especially for web 2.0 appli cations. For example, a typical Bloger access his Blog once a day, which leads to low file access frequency. Consequently, the correlation supports of typical user X  X  files are low. Thereby, user feeling of Blog System can be improved dramatically by pr efetching correlated files of a typical user. prefetching useless. In Order to create better user feeling, we emphasize on higher correlation instead of higher correlation support. 
This paper proposes PFC-Miner (Parallel File Correlations Miner) algorithm to discover file correlations efficiently in distributed file systems using file access logs. The algorithm is based on M-LSH (Min Locality Sensitive Hash) mining algorithm threshold. The algorithm is parallel and designed for execution in shared nothing distributed clusters. Instead of maintaining global data structures, the algorithm communication overhead. The algorithm is both fast and scalable, making it a this algorithm can find file correlations with very low support which are more valuable for web applications. 
The main contributions of this paper are as following: 1. Analysis and definition of file correlations in distributed file systems. 2. A parallel correlation mining algorithm to discover file correlations without 3. Evaluation of the scalability and accuracy of the algorithm. 
The remainder of the paper is organized as follows. In Section 2, we introduce file correlation and related techniques. In Section 3, we present our correlation mining algorithm. The experiment results are given in Section 4. In Section 5, we discuss related works. Section 6 concludes the paper. 2.1 File Correlations and Block Correlation File correlations are ubiquitous in file systems. A set of files are correlated if they are  X  X inked X  or  X  X lued X  together semantically. Fo r example, an HTML file is correlated to images embedded in it. At the same time, the images are correlated to each other, because they are glued together by this web page. Other examples include images in same photo albums, C source files, etc. 
Correlated files tend to be accessed together during their life time, because correlations reflect application level semantics. So correlations can be exploited to improve storage system performance by directing prefetching, optimizing file layout in disk and increasing cache hit ratio. explore the semantic correlations between objects to enhance storage system performance. Obviously, the first difference between file correlations and block simple block access patterns, such as web applications. Neighboring blocks prefetc-sequentially accessed in most web applicatio ns. Conversely, block correlations are more useful in applications with complex block access patterns, such as databases. The second major difference is scalability. The number of blocks in storage system is much larger than the number of files. As a result, it is more difficult to discover block correlations in distributed environments. 2.2 Obtaining File Correlation There are two categories of file correlations obtaining approach, namely, informed and automatic approaches. In Informed appr oaches, applications inform the system about the set of correlated files. Dynamic Set [11] is an informed approach proposed dynamic sets which are created on demand to enhance performance. In automatic approach, system automatically reveals file correlations of application. Griffioen and Appleton presented a file prefetching scheme relying on dependency performance. Kuenning and Popek propose Semantic Distance method, which use temporal, sequence and lifetime semantic distance to measure correlation [4]. Yeh et al. through identification of the programs accessing them [14]. 
The approaches mentioned above work well in traditional file systems, however three reasons. First, it is difficult and error prone to maintain global data structures in distributed system. Second problem is scalability. Space and time overhead of graph-based and Semantic Distance methods is high when the number of files is huge [9]. Third, they are not efficient in finding correlated files with low support. Finally, it is difficult to obtain process information of application for Program-Based approach, because the application code may be running on other servers. 2.3 Interesting File Correlations Researchers have found that the object access model roughly follows Zipf-like estimated to be: As shown in formula 1, the access freque ncies of majority files are small. the probability that frequent accessed file is in cache is high. Secondly, these kinds of files represent the majority of users X  access behavior, for typical users accessing web server once or several times a day. Therefore we interest in Files with high correlations but with very low support. algorithm. The summary of notations used is listed in table 1. f k The number of random numbers generated per log record r The row count of M  X  X  submatrix l Sub-matrix count of M M The k -Min-Hash matrix 
T 3.1 File System Model system runs on commodity hardware, uses Object based Storage Node (OSN) for storage and metadata servers (MDS) for storing metadata. Unlike traditional files system, it has flat file naming method, that is, files are accessed using unique file ID instead of file names. To enhance performance, file data transfers directly between applications and OSNs. So naturally, file access logs are distributed on each OSN. 3.2 Correlation Measurements same record are regarded as accessed togeth er. Because files stored on different OSNs may be accessed together, a log record is naturally partitioned among all OSNs. Furthermore, each log record is assigned an ascending record ID starting from 1. 
Correlated files tend to be accessed together, so naturally, we regard files accessed together frequently as correlated files. In tuitively, we define correlation among a pair of files as: count of concurrent access / total access count of both file. Suppose there f is R j . Correlation of the two files Co(f i ,f j ) is defined as: 
The value of Co(f i , f j ) is between 0 and 1, larger value indicating higher correlation between f i and f j . To make correlation more meaningful, we define correlation support, CoS(f i , f j ), as minimum access count of f i and f j : Correlation make no sense unless CoS(i, j) is larger than a relative small threshold. For example, suppose f i is accessed only once, i.e. | R i | = 1. Obviously the correlation of f i is not meaningful, because there is not enough information about that file. 3.3 Preprocessing Association rule mining algorithms including M-LSH are designed to discover rules algorithms, preprocess is necessary to break logs into short access records. Figure 2 shows four cutting methods to preprocess access logs into access records. These methods can be explained in two dimensions. In one dimension, break by count tries to group log entries whose timestamps within a same time interval together into a record. In other dimension, non-overlapped method divides the log into non-overlapping records with same size. In contrast, consecutive records overlap in overlapping method. 
We use break by time and non-overlapping methods in this paper. As mentioned above, a single access record may be distributed on many OSNs, because files on different OSNs may be accessed together. Assuming that OSNs have loosely synchronized clocks (which can be easily synchronized using SNTP protocol), each OSN can preprocess its own log without inte racting with others in break by time method. Further, we choose non-overlapping to improve efficiency. With relative relative small. 3.4 Core Algorithm The algorithm for identifying correlated files consists of three stages: compute Min-Random values; compute hash signatures and create inverted signature index; perform signature join and generate intra-server results. In the first stage, each OSN scans its local log records, prunes files with too lo w access count and generates a small set of Min-Random values for each file. The set of Min-Random values is expected to be hash signatures from Min-Random values, create inverted signature index which files on different OSNs using hash signatures, generate candidates using inverted signature index, calculate candidates X  correlations and output correlated files. 
Although PFC-Miner is based on M-LSH [2], there are three major differences between them. First, it is a parallel algorithm working in distributed environments. Second, PFC-Miner uses pre-computed invert ed signature index and signature join to reduce the number of candidates. Third, to improve performance, we compute correlations using Min-Random values without another scan of access log to prune small, especially when correlation support is low. 
Algorithm 1. Hash signature computing 1 Input: k -Min-Random values matrix M 2 Output: inverted signature table T , hash signature H
The first phrase algorithm is similar to M-LSH, we choose k random functions, h ,..., h k . Then each OSN scans in parallel through local access records and generates k random values for each record using the k random functions. At the same time, we calculate k -Min-Random values for each file. Definition 1. Min-Random value, h x (f i ), is the minimum random value of the records containing file f i , which is generated by hash function h x . k-Min Random values of f i is defined as all k Min-Random values, i.e. h 1 (f i ), h 2 (f i ),...,h k (f i ). All k -Min-Random values of f 1 , f 2 , ..., f m make up a k -Min-Random matrix M , with k rows and m columns, where M ij = h i (f j ) . M can be viewed as a compact representation similarity S M (f i , f j ). relatively large . In second stage, we apply Locality Sensitive Hashing (LSH) technique to k -Min-Random values matrix M . As shown in Algorithm 1, the algorithm first splits M generated for each file f i using as hashing key the concatenation of all r values, then signature of f i . At the same time, hash signature, H , is computed as the set of distinct values, files with at least one same hash signature values are considered as candidates in the third stage. 
As shown in Algorithm 2, the third stage algorithm performs hash signature join between OSNs. Each OSN first broadcasts local hash signature, H x , to all other OSNs. searched using hash signature values in H y , inter-server correlation candidates C xy which is the set of files with at least one hash signature values in H y is generated, and is sent back to OSN y . When candidate set C yx is received from OSN y , correlated file pairs can now be generated using C yx and T x . Note that, it isn X  X  necessary to broadcast H to all other OSNs, because file correl ation is symmetric. When the number of even OSNs (more than two) in system, we add a fake node, messages broadcasted to whom are not really sent. These two cases are shown in figure 3. 
Signature join can significantly reduce search space for correlated files, with adjustable false negatives and false positives. As proven in M-LSH, both the false positive and false negative ratio are small by choose relatively large r and l . 
Algorithm 2. hash signature join 1 Input: k -Min-Random values matrix M We evaluated real world benefits, scalability and accuracy of PFC-Miner. In all tests, we use the same hardware and software environments: Xeon 2.0G CPU, 2 GB memories, 1 GB/s link, Debian Linux operation system. 4.1 Correlation Guided Prefetch To show the benefits of file correlation, we implemented a storage cache simulator replacement policy, and evaluated the improvements on cache hit ratio by using correlation guided prefetch. In this test, we use trace driven simulation with publicly available web access logs: ClarkNet, which is a week of web log of ClarkNet WWW server and is available at http://ita.ee.lbl.gov/html/contrib/ClarkNet-HTTP.html. We file correlations. Then, we trace both the two part of log guided by mined correlations. As shown in figure 4(a) and figure 4(b), colru ( lru with prefetch ) and colfu ( lfu with prefetch ) performance significantly better than lru and lfu, especially when cache size is relatively small. 
Figure 4(c) shows the distribution of cache hit ratio over time when cache size online continuously. can also see that the line if flat when cot &lt; 0.5, which implies that low correlations are i.e. l . Cache hit ratio grows quickly when l &lt; 20, and becomes flat afterward. This can be quite expected, as correlation accuracy increase quickly with l and approaches real value after some threshold. Figure (f) describes that r has negative effect on cache hit ratio, because the mining algorithm becomes more coarse-grained when r grows. 4.2 Performance and Scalability Our performance and scalability study uses synthetic data which is generated using Zipf distribution. In the scalability test, we use a data set consists of 50k different files capable to compute file correlation in ma in memory, and hence its performance can be directly compared to parallel versio n of algorithm. The access log is evenly distributed across all servers to balance workload among servers. Execution time and speed up with one to eight servers are shown in figure 5(a). As the charts indicate, the when server count is between 2 and 6. Speed up is relatively small when server count is small and large. When server count is small, the large size of hash signature leads to relatively large communication time. When server count is large, local computation performance by varying data set size under 8 servers. This result indicates that execution time grows linearly with data set size. 4.3 Correlation Accuracy As shown in Section 3, our algorithm estimates correlation using Min-Random values, which introduces additional false positives compared to M-LSH algorithm. Therefore we evaluate correlation accuracy by counting the number of extra false positives. The corresponding data sets consists of 1000 files, 500 correlated file pairs PFC-Miner with r = 5, l =20, cot =0.5 which is larger than c . All correlated pairs found will be regarded as false positives, because th ere are no file pairs in the data set with correlation larger than the threshold. False positive numbers with different parameter quickly when data set X  X  correlation c decreases, but remains stable when sup changes. c &lt; 0.36. In a word, the mine results contain few false positives with large cot  X  c , and modest number of false positives when cot-l is small, which is accurate enough for to be used in real world systems. File correlation has been studied extensively in file systems. SEER [4] cluster correlated file to support better disconnected operation. They record every file a semantic distance between several closest related files. Then, correlation among files is computed using the number of shared neighbors. Griffioen and Appleton [3] uses weighed probability graph to represent relationships between files. Ahmed Amer et al [16] investigate a group based file cached management approach. The main idea is many works [17][18] recording file relations and access patterns using trees with each node representing the sequence of consecutive file accessed from root to the node. Access tree [15] use access tree to capture relationships and dependencies between files of a user process. Several access trees are maintained for each program file systems with huge number of files. 
In the spectrum of block correlation, C-Miner [8][9] proposes an effective algorithm to find Block Correlation in file system. They use a modified version of CloSpan to mine frequent closed block access sequences. Then association rules generated from the found sequences are used to guide block prefetch, layout optimization. Semantically-Smart Disk System [10] can effectively discover Block Correlation in FFS-like file systems based on knowledge of file system data structures and operations. Block correlation is useful when block access pattern is complex, but is not meaningful in web applications with sequential block access. The research of file correlations is important for improving file system performance. However, existing methods for extracting correlations are challenged by the increasing volume of data available nowadays. We have proposed PFC-Miner systems. The algorithm computes file co rrelations efficiently using Min-Random Values, exchanges candidates between file servers using signature join with low communication overhead. Experiments demonstrate that the algorithm is fast and system with large volume of files. This paper pays mostly attention to performance and accuracy up to the present, future research will focus on how to apply PFC-Miner algorithm to real world distributed file systems. 
