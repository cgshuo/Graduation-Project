 Co-training, a paradigm of semi-supervised learning, has drawn considerable at-tentions and interests recently (see, for e xample, [1,2] for review). The standard two-view co-training [3] assumes that there exist two disjoint sets of features or views that descr ibe the data. 1 The standard co-training utilizes an initial (small) labeled training dataset and a (large) set of unlabeled data from the same dis-tribution, and it works roughly as follows [3]. Two separate classifiers are first trained on the initial labeled training da taset using the two views respectively. Then, alternately, each classifier classifies the unlabeled data, chooses the few unlabeled examples whose labels it predicts most confidently, and adds those examples and the predicted labels to the training dataset of the other classifier. The classifiers are retrained, and the pro cess repeats, until so me stopping crite-rion is met. That is, the two classifiers  X  X each X  each other with the additional examples whose labels are given by the other classifier to improve the classifi-cation accuracy, compared to a classifie r learned only from the initial labeled training data.

Two assumptions are proposed for co-training to work well [3]. The first one assumes that the views are sufficient; that is, each view (thus also the combined view) is sufficient to predict the class perfectly. We call it the sufficiency as-sumption . The second assumption requires that the two views be conditionally independent; that is, the two views are independent given the class. We call it the independence assumption . Theoretical results have shown that if the suf-ficiency and independence assumptions are satisfied, co-training is guaranteed to work well. (The assumptions can be relaxed for co-training to still work well [6,7]. Nevertheless, the sufficiency and independence assumptions are a  X  X ufficient condition X  for co-training to work well). In addition, the two-view co-training has been applied quite successfully to many real-world tasks, such as statistical parsing [8], noun phrase identification [9], and image retrieval [10].
However, the two assumptions that guarantee co-training to work well may not be true in most real-world applications. Given a real-world dataset with two views of attributes, how can we judge if t he two-view co-training would work well? How can we verify if the sufficiency and independence assumptions are satisfied to guarantee co-training to work well? If the real-world dataset has only one view, can the two-view co-training still work? This paper is our first attempt to answer these questions. Given a whole dataset (with labels) and two views of attributes ( X = x 1 ,...,x m and Y = y 1 ,...,y n ), how can we verify if the two assumptions on sufficiency and independence for the standard co-training are satisfied? If the assumptions are satisfied, co-training is guaranteed to work well, and thus can be applied. Note that sometimes the domain knowledge can ensure the satisfaction of the two as-sumptions, but in most real-world applications, such assumptions cannot be guar-anteed. Thus it is important that these assumptions be empirically verified based on the dataset given. Here we will use the whole labeled dataset (or a very large training set) that represents the learning task to verify the two assumptions. This is because the theoretical assumptions on sufficiency and independence are based on the whole domain (for example, it is assumed that there exist target functions that map from the single view, the X view, and the Y view perfectly [3]). The sufficiency assumption is relatively easy to verify. Sufficiency means that X  X  Y can accurately predic t the class, so can X and Y individually. We can build a classifier to estimate the accuracy on the whole dataset D using X  X  Y with the 10-fold cross-validation. We denote this accuracy as p . The sufficiency says that p should be close to 1. Note that the theoretical results assume that there exist (target) functions that map from X  X  Y , X ,and Y to the class label perfectly. As we are verifying the assumption empirically, we use learning algorithms on the whole dataset to establish if such functions exist or not. Similarly, we build a classifier using attributes in X to estimate the accuracy (call it p x )of X predicting the class, and accuracy (call it p y )of Y predicting the class.
Thus, the sufficiency assumption of co-training can be defined as: there exists asmallpositivenumber  X  1 (such as 0.1) such that p&gt; 1  X   X  p x &gt; 1  X   X  p y &gt; 1  X   X 
We call  X  1 the sufficiency threshold . In Section 3.2, we will discover ranges of  X  1 that make co-training work well.

Conditional independence is a bit harder to verify. It means that given the class, the two views are independent. One way to verify this is, for each class label, if each x i is independent of Y ,andeach y i is independent of X .Toverify if x i is independent of Y empirically, we build a classifier (or many classifiers) to predict x i using Y on the whole dataset. If x i is independent of Y ,then Y cannot predict x i well  X  not better than the default accuracy of x i . Again we establish empirically if Y can predict x i better than its default accuracy on the whole dataset. Assume that the 10-fold cross-validated accuracy of Y predicting x i on D is p x i , then it should not be much larger than the default accuracy of x  X  the accuracy (denoted as p x i ) of the majority value of the class. The same is true for using X to predict y j . Thus, the independence assumption can be defined as: there exists a small positive number  X  2 (such as 0.1) such that for each class value
We call  X  2 the independence threshold . We will establish the ranges of  X  2 to make co-training work in Section 3.2. In the previous section we describe an emp irical approach to verify, when given the whole dataset and two views, if the two views satisfy the sufficiency and independence assumptions for co-training to work well. However, the standard two-view co-training has lim ited success in most real-wo rld datasets with single views, such as most UCI datasets [11]. (O ne could also apply directly the single-view co-training on the datasets with single views, but other complications may be entailed.)
In this section we propose a simple heuristic to split single views into two views such that if the two views satisfy the sufficiency and independence assumptions, the two-view co-training is guaranteed to work well. The heuristic works as follows. We first calculate the entropy of each attribute in the single view based on the whole dataset D , similar to the entropy calculation for all attributes when deciding which attribute should be chosen as the root of the decision tree [12]. Intuitively, the larger the entropy, the more predictive of the class that the attribute would be. In order to distribute high-entropy attributes evenly in the two views, we simply assign attributes with the first, third, and so on (the odd number of), highest entropy to the first view. We then assign attributes with the second, fourth, and so on (the even number of), highest entropy to the second view. Our proposed method is clos ely related to [13], which also splits single-views into two views. However, it simply splits the attributes randomly into two views. Later in this section, we make a comparision between our entropy splitting approach and the random splitting approach. After the two views are formed, the two assumptions (sufficiency and independence) for co-training are verified using the approaches described in the previous section.

We choose 32 UCI datasets coming with the WEKA package [14] to see if we can split the single view into two views for co-training to work. The continuous attributes are discredited into 10 equal-width bins in order to utilize naive Bayes [15] for checking the sufficiency and independence assumptions. As most previous co-training researches are based on binary classification problem, datasets with multiple classes are converted to binary by using the majority class value as one class, and the rest of the other values as the other class. These datasets are named with  X  new X  appended on the end of their original names.

In order to study the range of the sufficiency and independence thresholds for co-training to work well, we set  X  1 =0 . 5 for now (a very relaxed value, as any weak binary classifier should predict better than 50%). We apply both entropy splitting and random splitting on these 32 datasets for comparison. Entropy splitting yields smaller  X  1 on most datasets (31 out of 32) and smaller  X  2 on about half datasets (15 out of 32) compared to random splitting, thus we utilize it to verify the working of co-training in the rest of the paper. The cross-validated accuracies on the single view, the X view, and the Y view using naive Bayes on the whole datasets are listed in Table 1. We use  X  X cc( X , Y ) X ,  X  X cc( X ) X , and  X  X cc( Y ) X  to denote them respectively in the table.

Our experiments of applying co-training on these UCI datasets are conducted in the following two high-level steps. In the first step, we run the standard co-training on these datasets to see if co-training would work. For each dataset we also obtain the tightest (smallest) sufficiency and independence thresholds that would make it pass the verification. In the second step, we apply a meta-learning algorithm [16] on the results of the first step to discover proper ranges of the thresholds that can predi ct when co-training works well. We describe these two steps in details below. 3.1 Applying Co-training on UCI Datasets To apply co-training on these 32 datasets, the whole datasets D are first split randomly into three disjoint subsets: the training set ( R ), unlabeled set ( U ), and test set ( T ). The test set T is always 25% of D . To make sure that co-training can possibly show improvement when the unlabeled data are added, we choose a small training set for each dataset such that the  X  X ptimal gain X  in accuracy when using the unlabeled data optimally is large enough (greater than 10%). The  X  X ptimal gain X , denoted as  X  X ptGain X  in Table 1, is thus the difference between the accuracy on the initial training set R plus all unlabeled data with correct labels and the accuracy on R alone (without any benefit of unlabeled examples). The  X  X ptimal gain X  reflects the upper bound that co-training can achieve in accuracy. The unlabeled set is t he whole dataset taking away the test set and the training set. The proper training set size (with the optimal gain greater than 10%) is also listed in Table 1. The standard co-training [3] is then applied. The process is repeated 20 times with different split of R , U ,and T .
The average accuracy before applying co -training (test accuracy of applying naive Bayes on the initial training set; denoted as  X  X niAcc X ), and the average accuracy after applying co-training (de noted as  X  X tAcc X ) are recorded in the table. A significance test, a paired t-test with 95% confidence, is applied to see if the test accuracy after co-training is si gnificantly better than the test accuracy wins, denoted by W in the  X  X tWorks? X  column; if it is significantly worse, then are presented together in Table 1 for easy viewing.

From Table 1, we can see that overall, co-training wins in 6 datasets, loses in 3, and ties in the rest 23 datasets. Of course this does not imply that co-training does not work well for most single-view r eal-world dataset s, as the sufficiency and independence thresholds (  X  1 and  X  2 ) are set very relaxed (  X  1 =0 . 5), thus the two views of these datasets may not be sufficient or independent. For each dataset, we can obtain the tightest (smallest) threshold values for the sufficiency and independence assumptions to pass. These threshold values (  X  1 and  X  2 )are also listed in Table 1. These values provide us with an opportunity to discover the hidden regularity of these thresholds that make co-training win. 3.2 Meta-learning Co-training Thresholds Results in Table 1 do seem to indicate that co-training would win when  X  1 and  X  2 are relatively small. In order to obtain a more precise range of use the idea of meta-learning to find hidden regularity of  X  1 and  X  2 that makes co-training work (win). We simply take, from Table 1, the numerical values in columns  X  1 and  X  2 as attributes, and W , L or T from  X  X tWorks? X  as the class label. We obtain 32 training examples on which we can apply meta-learning.
We first assign W (win) as one class, and group L (lose) and T (tie) as the  X  X thers X  class, to discover when co-training would win ( W ). As we expect simple rules for the thresholds, we apply WEKA X  X  j48, the standard decision-tree algorithm [12] on the 32 training examples with pruning. The decision tree found is surprisingly simple: d1 &lt;= 0.23 | d2 &lt;= 0.15: W (7.0/2.0) | d2 &gt; 0.15: others (8.0/1.0) d1 &gt; 0.23: others (17.0) The tree discovered by j48 clearly indicates that co-training would win if the sufficiency threshold (  X  1 ) is less than or equals to 0.23, and the independence tree, but the overall accuracy of this tree is quite high at 91%, much higher than the default accuracy of 81% (26/32).

It would also be interesting to find out when co-training would lose ( L )soit should be avoided. We use L (lose) as one class, and group win ( W )andtie( T ) as the  X  X thers X  class, and run j48 again. 2 The result is also surprisingly simple: d2 &lt;= 0.26: others (21.0) d2 &gt; 0.26 | d1 &lt;= 0.28: others (9.0/1.0) | d1 &gt; 0.28: L (2.0) This indicates that co-training would lose (so it should not be used) if the suffi-ciency threshold (  X  1 ) is greater than 0.28, and the independence threshold (  X  2 ) is greater than 0.26. Clearly our empirica l results coincide we ll with theoretical findings that if the two views are sufficient and independent, co-training must work well (win). However, theoretical guarantee on sufficiency and independence is often impossible to obtain. What is more important is that the actual range of the sufficiency and independence thresholds, though discovered empirically here, provides a simple guideline for deciding and applying the standard co-training in real-world datasets. To summarize, in this paper we propose em pirical verification of the sufficiency and independence assumptions of the standard two-view co-training algorithm. We design heuristic to split datasets with a single view into two views, and if the two views pass the sufficiency and independence verification discovered by meta-learning, co-training is highly likely to work well. Our conclusions coincide well with the previous theoretical results, but our work provides a practical guide as to when co-training can work in datasets with two views. Our current work is based on the whole dataset. In our future work, we will study co-training verification on small training data.
 Z.-H. Zhou was supported by NSFC (60635030, 60721002), JiangsuSF (BK2008018) and 863 Program (2007AA01Z169).

