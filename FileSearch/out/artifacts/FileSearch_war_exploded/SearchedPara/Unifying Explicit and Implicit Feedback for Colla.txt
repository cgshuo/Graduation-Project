 Most collaborative filtering algorithms are based on certain statisti-cal models of user interests built from either explicit feedback (eg: ratings, votes) or implicit feedback (eg: clicks, purchases). Explicit feedbacks are more precise but more difficult to collect from users while implicit feedbacks are much easier to collect though less ac-curate in reflecting user preferences. In the existing literature, sep-arate models have been developed for either of these two forms of user feedbacks due to their heterogeneous representation. However in most real world recommender systems both explicit and implicit user feedback are abundant and could potentially complement each other. It is desirable to be able to unify these two heterogeneous forms of user feedback in order to generate more accurate recom-mendations. In this work, we developed matrix factorization mod-els that can be trained from explicit and implicit feedback simulta-neously. Experimental results on multiple datasets showed that our algorithm could effectively combine these two forms of heteroge-neous user feedback to improve recommendation quality.
 H.3.3 [ Information Systems ]: Information Search and Retrieval X  Information Filtering Algorithms, Experimentation Collaborative Filtering, Ranking, Latent Class Model
Collaborative filtering(CF) is a popular approach for building recommender systems. It works by collecting user feedback , which could generally refer to any form of user actions on items that may convey relevance judgements on the items. User feedback data generally fall into two categories: explicit feedback and implicit feedback. Explicit feedback is more widely used and are often in the form of numeric ratings input by users to express their prefer-ence over the items. For example, Amazon let users rate books on a 1-5 star scale. Explicit feedback, like its name suggests, could capture user preferences in a direct and granular way. However, it requires a user to perform an extra "rating" action, which may lead to inconveniences. Given the huge volume of digital content that people normally interact with on a daily basis, it would be rather burdensome if one had to enter a rating for every item he liked or hated. Therefore, in recent years, CF researchers have started looking into inferring user preference from the much more easily available implicit feedback , which consists of user behaviors that indirectly express user interests. For example, an online store may record what products a user has purchased and an internet radio could track which songs a user has listened to. Other forms of im-plicit feedback include search history, browse history, mouse move-ment and even eye tracking. Implicit feedback mostly involves user actions that would naturally occur as a result of using a particular system and thus could be more easily collected. However, implicit feedback is much less accurate and granular than explicit feedback. For example, a user may rent a movie and only realizes it is a bad movie after watching it, so just knowing a user has rented a movie does not necessarily imply that he likes the movie.

We can see that there is clearly a trade off between quantity and quality when comparing explicit and implicit feedbacks. Explicit feedback is higher in quality but lower in quantity while implicit feedback is lower in quality but more abundant in quantity, so the two forms of feedback are indeed complementary to each other. However, the vast majority of existing works have solely consid-ered the use of explicit feedback whereas only recently had several algorithms specifically designed for implicit feedback been devel-oped. To the best of our knowledge, very few works have stud-ied the simultaneous use of both explicit and implicit feedback, al-though in most real world recommender systems, both explicit and implicit feedback can be collected.

In this paper, we develop collaborative filtering models that can be simultaneously learnt from both explicit and implicit feedback regarding the same set of users and items. The key challenge in uni-fying explicit and implicit feedback for collaborative lies in coping with the heterogeneous representations of these two forms of user feedback. Explicit feedback data are mostly numeric responses whereas implicit feedback data mostly consists of binary responses. As a result, the accuracy and granularity of these two forms of user feedback are highly distinct. Explicit feedback can more accurately reflect user preference with its magnitude precisely indicating the degree to which a user prefers an item. On the other hand, implicit feedback are much less accurate and granular. To unify implicit and explicit feedback, we have developed a matrix factorization model called co-rating that can effectively cope with the heterogeneity be-tween the two forms of feedback. Experimental results on multiple datasets demonstrated the effectiveness of the proposed approach.
The notational framework is defined as follows. We use U = { u 1 ; u 2 ; :::; u m } to denote the set of m users in the system and I = { i 1 ; i 2 ; :::; i n } to denote the set of n items. Both explicit and implicit feedback are observable interactions between U and I that could reflect a user X  X  preferences on the items.

Explicit feedback is usually in the form of numeric ratings as-signed to items by the users, the scale of which indicate the how much the user likes a particular item. We use x ui  X  R to denote user u  X  X  rating on item i and let S  X   X  U  X  I denote the set of user-item pairs for which explicit feedback are available. We use X  X  R m  X  n to denote the matrix of observed ratings.

Types of implicit feedback typically include various actions that users performed on items that are automatically tracked by the sys-tems. Examples of implicit feedback include purchases at an online store, clicks on a news Website, or plays on an online radio. In the most general case, implicit feedback can be captured binary vari-able y ui  X  X  X  1 ; +1 } such that if an user u had acted upon an item i (purchased a book) then y ui is equal to +1 and -1 otherwise. We let Y  X  (  X  1 ; +1) m  X  n to denote the matrix of observed implicit feedback and use S +  X  U  X  I and S  X   X  U  X  I denote the set of user-item pairs for which y ui are equal to -1 and +1 respectively. Unlike explicit feedback, y ui  X  X  are weak and ambiguous indicators of user preferences. For example, a user may not really like a song or a news story he just accidently played or read and similarly, it would be possible for him to actually like a song or a news story he has not played or read because he is not aware of such items. Without further information, it would be very hard to discern such ambiguities in implicit feedbacks and most existing works[3, 6] are based on the assumption that items for which y ui = +1 are in gen-eral more relevant to the user than those items for which y
There are two important differences between explicit and im-plicit feedbacks: 1. Given explicit feedbacks, x ui are only known for those ( u; i ) 2. Explicit feedbacks are numeric in the nature and the magni-
For convenience, we use the I  X  u to refer to the set of items i on which user u has explicit feedback (i.e. ( u; i )  X  S  X  ). Similar, we define the I + u and I  X  u to be the sets of items for which y and  X  1 respectively. Symmetrically, we may define the user sets U ; U + i and U  X  i that are associated with a particular item i .
Most collaborative filtering algorithms are based on designing either a parametric or a nonparametric form of a scoring function f ( u; i ) that determines each item X  X  relevance to each user. The rat-ing oriented approaches is essentially regression modeling in that the parameters of the function f ( u; i ) are fitted so that the predicted scores b r ui = f ( u; i ) would match the observed user feedback x and y ui  X  X  as closely as possible. Given the scores b r ui rank items by the scores to produce recommendations. Such a rat-ing oriented approach has been studied for collaborative filtering using explicit feedback[7, 5, 8, 2] and has recently been extended to cope with implicit feedback as well[6, 3].

In recent years, matrix factorization [5] models has gained popu-larity due to its widely publicized success on the Netflix Challenge. In addition to modeling ratings data, its effectiveness has also been demonstrated on implicit feedback datasets[6, 3]. With matrix fac-torization models, a matrix X : m  X  n of observations is approx-imated by the product of two low-rank matrices U  X  R k  X  m V
Under this model, each user u is modeled by a latent feature vector v u , the u -th column of V and similarly each item i is repre-sented by u i , the i -th column of U . The prediction formula for the scores b r ui thus have the following form:
Typically, the factors matrices U and V are found via solving: The loss function L (  X  ;  X  ) quantifies the how well the approximation is, the regularization term R (  X  ;  X  ) penalizes model complexity in order to avoid overfitting and the parameter controls the trade off between these two terms.

For explicit feedbacks, we want the factorization model to ap-proximate well the observed ratings in S R . This leads to the widely used squared error loss form for L X : where the squared loss are only computed over the observed rat-ings in S  X  in contrast to standard SVD formulation which would treat the unobserved ratings as 0 and approximate these zero en-tries along with the observed ratings. Other forms of loss functions such as hinge loss that are more suitable for the ordinal ratings have also been considered[8].
 A commonly used regularization term is the Frobenius norm: which is adopted by many matrix factorization based collaborative filtering algorithm[5, 8] due to its smooth differentiable property.
A similar squared error based loss for implicit feedback had also been proposed [6, 3] by approximating the two types of feedback -1 and +1 as numerical values: where the major difference from (6) is that rather than just summing over entries in S  X  , there is now a square loss over all the m n entries. This raises a major computational challenge in terms of efficiency. Fortunately, it is shown that by exploiting special structure of the hessian matrix in the Newton X  X  algorithm, efficient algorithms with complexity per iteration in the order of O ( rather than O ( m  X  n ) is achievable[3].
When both explicit and implicit feedbacks are available with re-spect to the same set of users and items, we would like the predicted scores b r ui to reflect user preferences expressed in both explicit and implicit feedbacks. A natural idea is to apply a common factoriza-tion model U T  X  V for both the explicit feedback matrix X and the implicit feedback matrix Y so that the same b r ui is used to fit both x ui and y ui . However, there are two complications: firstly, the nu-meric scales of x ui and y ui can be very different, for example, x may range from 1 to 10 while y ui only takes on either -1 or +1. secondly, the accuracy of explicit and implicit feedback are highly variant with implicit feedbacks being much less precise. To address these two problems, we normalize the original x ui and y ui a common scale ranging from 0 to 1 and assign different impor-tance to explicit and implicit feedback. This leads to the following co-rating model: where the square losses due to explicit and implicit feedbacks are added together and an additional parameter is introduced to weigh the relative importance of explicit and implicit feedbacks. The for-mulation (7) can be viewed as a special case of the more general collective matrix factorization model[9], that involves two relations about two entities.

In this paper, we extend the alternating least squares (ALS) al-gorithm for factorizing a single matrix[10, 3] for learning the co-rating model. To this end, we first fix U and set the gradient of the objective function in (7) with respect to v u to zero, which results in the following expression for the v u that minimizes the loss: v where U I u and U I + of U in I  X  u and I + u respectively, x : u is a | I  X  u | formed by ratings of u on items in I  X  u , and y + : u is a vector of 1 X  X . Symmetrically, we can derive the update equation for item factors by fixing V and minimize with respect to each u which is omitted due to space limitation.
The idea of complementing explicit feedback with implicit feed-back was first proposed in [1], where the author consider explicit feedback as how the users rated the movies and implicit feedback as what movies are rated by the users. The two forms of feedback are combined via a factorized neighborhood model, an extension of tra-ditional nearest item-based model in which the item-item similarity matrix is approximated via low rank factorization. In particular, the ratings are predicted via the following formula: b r where a u ; b i are user and item rating biases and each item i is rep-resented by 3 sets of latent factors u i ; p i ; q i . Like traditional ma-trix factorization, the model parameters are learnt by minimizing prediction errors on the set of observed ratings.
Our goal is to study the if recommendations can be made more accurate when both explicit and implicit feedback are present. To the best of our knowledge, there is no publicly available datasets containing both forms of feedback such as movie ratings data plus movie rentals records. Here we follow the idea proposed in [1] to create implicit feedback from explicit ratings data by by consid-ering "whether" a movie is rated by a user in addition to "how" a movie is rated by a user. Thus, for each ( u; i )  X  S  X  , we would have the corresponding y ui = +1 , and the set S  X  would corre-spond exactly to the set S + . So the implicit feedback can be re-garded as the log of the "rating" action indicting which user rated which movies. It should be noted that the implicit feedback created this way are rather noisy as high ratings and low ratings become indistinguishable and that ratings lower than 3 actually accounted for nearly 30% ratings in both the Netflix and EachMovie datasets. As presence of noise is also an inherent nature of most other forms of implicit feedback such as click logs and browse logs, so we be-lieve the findings on the implicit feedback transformed from ratings should be transferrable to other forms of implicit feedback.
In this work, we conducted the evaluation on movie ratings datasets including both Netflix 1 and MovieLens 2 . On both datasets, we uti-lize 80% of the ratings in training and evaluate performances on the remaining 20% of the ratings. When doing the training/testing split, we ensured the consistency between explicit and implicit feedback by making y ui =  X  1 for all the x ui that were used for testing.
We evaluate recommendation quality based on both rating and ranking quality, the former of which is measured by Rsidual Mean Squared Error (RMSE) while the later is measured by Normalized Discounted Cumulative Gain(NDCG). Both RMSE and NDCG mea-sures are computed on a subset e S  X  of the observed ratings. In choosing this kind of evaluation, we are essentially assuming that we already know which items a user would choose to rate and are only interested in evaluating how these items would rated. In real world recommender systems, the top-k recommendations need to be selected from a pool consisting of all items not in the training set. So it is desirable for the top-k recommendations to have more items with high ratings and contain less items that are rated low or not rated all.

Toward this end, we designed an evaluation framework that more realistically simulate the top-k recommendation scenario. For each user in the test set, both rated and unrated items are ranked alto-gether to produce the top-k list. Note some of the randomly se-lected unrated items might actually be of interest to the user, though it is safe to assume most of the unrated items are probably of no in-terest to the user at all. We still use NDCG to measure the quality of the ranked list except that the utilities of rated and unrated items in the list are defined differently: (1) utilities of items rated 4 or above are equal to their ratings. (2) utilities of both unrated items and items rated lower than 4 are equal to 0. We refer to this version of NDCG that could account for both rated and unrated items as NDCG+.
We compared the co-rating and co-ranking models with the fol-lowing baselines. The first baseline is the matrix factorization model(MF) designed for explicit feedback[10]. The second baseline is the binary matrix factorization (BINMF) designed for implicit feed-back[3]. The third baseline is the factorized neighborhood model (FN)[4] mentioned in section 2.3, which, to the best of our knowl-http://www.netflixprize.com http://www.grouplens.org/node/73 MF 0.9062 0.8015 0.8384 0.7019 0.7497 0.8759 0.7769 0.8271 0.6606 0.7254 BINMF -0.7171 0.7703 0.6864 0.7387 -0.6994 0.7640 0.6744 0.7382 FN 0.9032 0.8022 0.8391 0.7034 0.7507 0.8697 0.7779 0.8281 0.6657 0.7293 MF 0.8724 0.8183 0.8521 0.7645 0.8010 0.8340 0.7973 0.8427 0.7290 0.7797 BINMF -0.7170 0.7701 0.6864 0.7387 -0.6995 0.7640 0.6744 0.7382 FN 0.8678 0.8196 0.8531 0.7770 0.8114 0.8170 0.8022 0.8467 0.7432 0.7902 edge, is the only existing model that combines explicit and implicit feedback.

We consider the following two setting when comparing different models. The first setting is when only 30% of the total ratings in the training set are used as explicit feedback during training. This corresponds to the scenario when explicit feedback is sparse while implicit feedback is much more abundant. In the second setting, all of the ratings in the training set are used as explicit feedback.
From the results summarized in Table 1 and 2, we can make the following observations:
In this paper, we have developed collaborative filtering models that can be learnt from explicit and implicit feedback simultane-ously, namely the co-rating and the co-ranking models. In partic-ular, we have shown that the co-ranking model based on pairwise preferences provide a highly effective framework for unifying ex-plicit and implicit feedback. Experiments on both the Netflix and MovieLens datasets demonstrated consistent improvements over baseline methods. In the future, we would like to conduct further evaluation of our model on more diverse forms of user feedbacks such as click logs, browse history etc., in order to study the effec-tiveness of the co-ranking in a broader application context. [1] R. M. Bell and Y. Koren. Scalable collaborative filtering with [2] T. Hofmann. Latent semantic models for collaborative [3] Y. Hu, Y. Koren, and C. Volinsky. Collaborative filtering for [4] Y. Koren. Factor in the neighbors: Scalable and accurate [5] Y. Koren, R. M. Bell, and C. Volinsky. Matrix factorization [6] R. Pan and M. Scholz. Mind the gaps: weighting the [7] D. M. Pennock, E. Horvitz, S. Lawrence, and C. L. Giles. [8] J. D. M. Rennie and N. Srebro. Fast maximum margin matrix [9] A. P. Singh and G. J. Gordon. Relational learning via [10] Y. Zhou, D. Wilkinson, R. Schreiber, and R. Pan. Large-scale
