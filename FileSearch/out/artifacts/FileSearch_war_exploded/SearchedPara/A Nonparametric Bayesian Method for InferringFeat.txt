 human similarity judgments. The particular methods used to infer representions from similarity judgments depend on the nature of the underlying representations. For stimuli that are assumed to be represented as points in some psychological space, multidimensional scaling algorithms [1] can represented in terms of a set of latent features, additive clustering is the method of choice. The original formulation of the additive clustering (ADCLUS) problem [2] is as follows. Assume that we have data in the form of a n  X  n similarity matrix S =[ s between the i th and j th of n objects. Similarities are assumed to be symmetric (with s assumed to be well-approximated by a weighted linear function of common features. Under these feature has an associated non-negative saliency weight w =( w form, the ADCLUS model seeks to uncover a feature matrix F and a weight vector w such that S  X  FWF 0 , where W = diag ( w ) is a diagonal matrix with nonzero elements corresponding to required feature possessed by all objects. terms are i.i.d. Gaussian [3], yielding the model: Figure 1: Graphical model representation of the IBP-ADCLUS model. Panel (a) shows the hier-archical structure of the ADCLUS model, and panel (b) illustrates the method by which a feature matrix is generated using the Indian Buffet Process. where E =[ F is binary valued, W is necessarily diagonal and S is non-negative. In any case, if we define  X  where  X  2 is the variance of the Gaussian error distribution. However, self-similarities s improved upon, we leave this open for future research.
 In our approach, additive clustering is framed as a form of nonparametric Bayesian inference, in which Equation 2 provides the likelihood function, and the model is completed by placing priors feature matrices F is more difficult, since there is generally no good reason to assume an upper reason we use the  X  X onparametric X  Indian Buffet Process (IBP) [6], which provides a proper prior distribution over binary matrices with a fixed number of rows and an unbounded number of columns. The IBP can be understood by imagining an Indian buffet containing an infinite number of dishes. n  X  1 customers, the probability that the n th customer will also try that dish is where F customers that have sampled that dish. Being adventurous, the new customer may try some hitherto untasted meals from the infinite buffet on offer. The number of new dishes taken by customer n follows a Poisson(  X /n ) distribution. The complete IBP-ADCLUS model becomes, prior is shown in Figure 1(b). quite difficult, so a natural alternative is to use Markov chain Monte Carlo (MCMC) methods to for the Bayesian ADCLUS model using a combination of Gibbs sampling [7] and more general Metropolis proposals [8].
 saliency is w of except w With a Gamma prior, the Metropolis sampler automatically rejects all negative valued w  X  updated using a standard Gibbs sampler: the value of f distribution over f conditional probability by noting that where F is just the likelihood function for the ADCLUS model, and is simple to calculate. Moreover, since last. Given this, Equation 3 indicates that p ( f single-stimulus features with probability 1, since n among the manifest features F . When resampling feature assignments, some finite number of those currently-latent features will become manifest. When sampling from the conditional prior over fea-stimulus i ) from the conditional prior, which is Poisson(  X /n ) as noted previously. make a  X  X raw X  by recording the state of the sampler, leaving a  X  X ag X  of several iterations between successive draws to reduce the autocorrelation between samples. When doing so, it is important to with nine parallel chains used for comparison: the time series plot shows no long-term trends, and finements are possible for both the sampler [9] and the convergence check [10], we have found this has been largely overlooked, to our knowledge. One advantage of the IBP-ADCLUS approach is explore estimators based on computing the posterior distribution over F and w given S . This in-cludes estimators based on maximum a posteriori (MAP) estimation, corresponding to the value of Figure 2: Smoothed time series showing log-posterior probabilities for successive draws from the Gibbs-Metropolis sampler, for simulated similarity data with n =16 . The bold line shows a single chain, while the dotted lines show the remaining nine chains.
 Conditional MAP Estimation. Much of the literature defines an estimator conditional on the as-sumption that the number of features in the model m is fixed [3][11][12]. These approaches seek to estimate the values of F and w that jointly maximize some utility function conditional on this Estimating the dimension is harder. The natural (MAP) estimate for m is easy to state: where F heuristic method.
 MAP Feature Estimation. In the previous approach, m is given primacy, since F and w cannot be we first select Notice that  X  F estimated after  X  F where analytic approximations to p ( F | S ) are used for expediency.
 conditioning on m , yielding the MAP estimators, prior place some emphasis on parsimony. However, many theoretically-motivated priors (includ-ing the IBP) allow the researcher to emphasize parsimony, and some frequentist methods used in ADCLUS-like models apply penalty functions for this reason [15]. Figure 3: Posterior distributions (a) over the number of features p ( m | S m  X  S , where the target is either the observed training data S matrix S Approximate Expectations. A fourth approach aims to summarize the posterior distribution by look-particular feature f Although this approach has never been applied in the ADCLUS literature, the concept is implicit in is likely to be represented. Letting  X  r is manifest, we can construct a vector  X  r =[ X  r since the expected posterior similarities can be written as follows: where  X  w is represented (Equation 13 relies on the fact that features combine linearly in the ADCLUS model, typically report only those features for which  X  r make the largest contributions to E [ s expected posterior similarities. By using the IBP-ADCLUS framework, we can compare the performance of the four estimators in a reasonable fashion. Loosely following [12], we generated noisy similarity matrices with n =8 , possessed each feature with probability 0.5. Saliency weights w interval [1 , 3] , but were subsequently rescaled to ensure that the  X  X rue X  similarities S the noise accounted for approximately 10% of the variance in the  X  X bserved X  data matrix S the  X  X ew X  matrix S an IBP model follows a Poisson(  X H prior has a strong bias toward parsimony. The prior expected number of features is approximately 5 . 4 , 6 . 8 and 8 . 1 (as compared to the true values of 6 , 8 and 10 ).
 We approximated the posterior distribution p ( F , w | S and 1000 samples were drawn from each. The chains were burnt in for 1000 iterations, and a lag in the n =32 condition did not converge: log-posteriors were low, differed substantially from one Figure 4: Posterior distributions over the number of features when the Bayesian ADCLUS model is applied to (a) the numbers data, (b) the countries data and (c) the letters data. extracted using the Bayesian ADCLUS model. The first column gives the posterior probability that of a feature in the event that it is included. remaining chains.
 Figure 3(a) shows the posterior distributions over the number of features m for each of the three simulation conditions. There is a tendency to underestimate the number of features when provided 79% of the variance in the data matrix S ison is to look at the proportion of variance this estimate accounts for in the observed data S novel data set S ces, the  X  X deal X  answer for these three should be around 90%, 90% and 100% respectively. When have converged appropriately. For the smaller matrices, the conditional MAP and joint MAP esti-mators (  X  S though the difference is very small. The expectation method  X  S ADCLUS estimators, namely the direct estimates of dimensionality provided through Equation 8, and the features extracted via  X  X pproximate expectation X .
 models measures the conceptual similarity of the numbers 0 through 9 [17]. This data set is often used as a benchmark due to the complex interrelationships between the numbers. Table 1(a) shows an eight-feature representation of these data, taken from [3] who applied a maximum likelihood highest-probability features extracted by the Bayesian ADCLUS model. Each column corresponds associated with the additive constant is 0.035.
 shows the ten highest-probability features extracted by the Bayesian ADCLUS model. Each column average weight associated with the additive constant is 0.003.
 metic concepts and to numerical magnitude. Fixing  X  =0 . 05 , and  X  =0 . 5 , we drew 10,000 lagged feature matrices, 92.6% of sampled matrices had between 9 and 13 features. The modal number of represented features was  X  m similarities between Table 1(a) and Table 1(b), which reports the ten highest-probability features predictive similarities  X  S Featural representations of countries. A second application is to human forced-choice judgments most probable features are listed in Table 2. The  X  X pproximate expectation X  method explains 85.4% of the variance, as compared to the 78.1% found by a MAP feature approach [18]. The features are interpretable, corresponding to a range of geographical, historical, and economic regularities. Featural representations of letters. As a third example, we analyzed a somewhat larger data set, in the children X  X  similarity judgments. The posterior distribution over the number of represented ing provides a framework for learning featural representations of stimulus similarity, but remains underused due to the difficulties associated with the inference. By adopting a Bayesian approach similarity judgments. Moreover, by using nonparametric Bayesian techniques to place a prior dis-ments using only a finite, usually small number of properties that form part of our current mental representation. In other words, by moving to a Bayesian nonparametric form, we are able to bring the ADCLUS model closer to the kinds of assumptions that are made by psychological theories.
