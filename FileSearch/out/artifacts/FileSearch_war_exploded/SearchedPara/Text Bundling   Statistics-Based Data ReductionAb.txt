 Lawrence Shih kai@ai.mit.edu Jason D. M. Rennie jrennie@ai.mit.edu Yu-Han Chang ychang@ai.mit.edu David R. Karger karger@lcs.mit.edu There is a great need to find fast, effective algorithms for text classification. A KDD panel headed by Domin-gos (2002) discussed the tradeoff of speed and accuracy in the context of very large (e.g. one-million record) databases. Most highly accurate text classifiers take a disproportionately large time to handle a large num-ber of training examples. These classifiers may become impractical when faced with large data sets, like the Ohsumed data set with more than 170,000 training examples (Hersh et al., 1994).
 The standard way to deal with a million point data set is to reduce the size of the data to a more manage-able amount. Subsampling, for example, reduces the data set by randomly removes training points. Fea-ture selection, another data reduction technique, tries to retain only the best features of a data set. One way to understand a large body of data is to sum-marize them with statistics, which are functions over the data. Let ~x = { x 1 ,x 2 ,...,x n }  X  R n be a set of data. Then a statistic, s ( ~x ), is a function that maps the data to a single value in R . For example the mean statistic is simply the average value of the data points. Subsampling does not preserve the entire set of data, rather it preserves all statistics on a random subset of the data. In contrast, we propose to preserve cer-tain statistics of all the data. For example, in text we preserve the mean statistic. The mean statistic has proven useful in Rocchio (Rocchio, 1971) X  X  standard text-classification algorithm X  X nd we believe the mean statistic will be useful for other classifiers. Our  X  X ext bundling X  algorithm fully preserves the mean statis-tics of all the data. Text bundling generates a new, smaller training set by averaging together small groups of points. This allows bundling to preserve some of the overall distribution information beyond simply the mean of the data. A classifier using this reduced data set should be able to learn a decision boundary that is at least as good as Rocchio X  X , since it can use addi-tional information about the data.
 We believe this focus on mean statistics will prove to be a superior data reduction technique than subsam-pling or feature selection. For example, in a binary problem where we reduce the data to one point per class or  X  X aximal bundling, X  we are left with one  X  X v-erage X  point in each class. In this average point, the count for word i is the average number of times word i appeared in documents of the class. When these points are given to most learning algorithms, the result will be a decision boundary equivalent to the Rocchio classifier: the perpendicular bisector of the two aver-age points. Thus, due to our focus on statistics, even maximal bundling results in a good classifier. On the other hand, reducing to a single point per class via subsampling yields a single sample document. This gives almost no information about the nature of the class. Similarly, text classification with the single best word-feature is often ineffective.
 We provide empirical evidence that our bundling tech-nique works well by comparing it with three other data reduction techniques: bagging, feature selection, and subsampling. The speed of machine learning algorithms is important in making learning practical and widely used. Google X  X  results are good; but few would use it were it to take ten minutes to return results. In some fields like text, more training data usually corresponds to higher clas-sification accuracy. In some problems, it is easy to amass a training set numbering in the hundreds of thousands or millions. The best text classification al-gorithms are  X  X uper-linear X  X  X ach additional training point takes more time to train on than the previous point. When the number of examples is very large, such algorithms effectively do not complete. Those al-gorithms run faster when given a smaller set of inputs. Our focus is on finding a better way to select the set of data points that we hand to such an algorithm. One simple method of speeding up any classifier is to reduce the number of training points. A common ap-proach is subsampling, which retains a random subset of the original training data. Subsampling has the ad-vantage of being fast and easy implement. Our belief is that for text classification there are better ways to reduce the data set.
 Given a classification algorithm that is super-linear, another potential solution is bagging (Breiman, 1996). Bagging partitions the original data set and learns a classifier on each partition. A test document is labeled by a majority vote of the classifiers. This technique makes training faster (since each classifier has fewer examples), but slows down testing since it evaluates multiple classifiers for each test example. Hence the overall training and testing time does not always de-crease.
 We can also speed up classifiers by reducing the num-ber of features. Feature selection has been the focus of much work (Yang &amp; Pedersen, 1997; Mladenic, 1998). Note that all classifiers already perform a type of fea-ture selection: if the classifier sees a feature as irrel-evant, it simply ignores that feature by zeroing out a weight corresponding to that feature. Thus, an impor-tant contribution of feature selection is to have a fast pre-processing step that reduces the overall training time. It is unusual, however, to see empirical evidence comparing the empirical time reduction of feature se-lection with the resulting loss in accuracy. 2.1. The Rocchio Algorithm Several times in this paper, we mention the Rocchio classification algorithm (Rocchio, 1971). For com-pleteness we describe it in full detail here. Consider a binary classification problem. Simply put, Rocchio selects a decision boundary (plane) that is perpendic-ular to a vector connecting two class centroids. Let { ~x 11 ,...,~x 1 l 1 } and { ~x 21 ,...,~x 2 l 2 } be sets of training data for the positive and negative classes, respectively. Let ~c 1 = 1 l troids for the positive and negative classes, respec-tively. Then, we define the Rocchio score of an ex-ample as One selects a threshold value, b , which may be used to make the decision boundary closer to the positive or negative class centroid. Then, an example is labeled according to the sign of the score minus the threshold value, We can think of the tradeoffs between speed and accu-racy in an information sense: the less raw information we retain, generally the faster the classifier will run and the less accurate the results. Each data reduc-tion technique operates by retaining some information and removing other information. By carefully select-ing our statistics for a domain, we can optimize the information we retain.
 Bundling preserves a set of k user-chosen statistics, ~s = ( s 1 ,...,s k ), where s i is a function that maps a set of data to a single value. Bundling imposes a global constraint as follows. global constraint Let ~x be a set of data. Let ~x 0 be There are many possible reduced data sets, ~x 0 , that can satisfy this constraint. But, we don X  X  only want to preserve the global statistics. We also want to pre-serve additional information about the distribution. To get a reduced data set that satisfies the global constraint, we could generate several random points and then choose the remaining points to preserve the statistics. This does not retain any information about our data except for the chosen statistics. We can re-tain some of the information besides the statistics by grouping together sets of points and preserving the statistics locally: local constraint Assume that the data points, ~x , The bundling algorithm X  X  local constraint is to main-tain the same statistics between subsets of the training data.
 The focus on statistics also usually implies that the bundled data will not have any examples in common with the original data. This is a necessary consequence of our wish to fully preserve certain statistics rather than the precise examples in the original training set. The bundling algorithm ensures that certain global statistics are maintained, while also maintaining a re-lationship between certain partitions of the data in the original and bundled training sets.
 In the following section we will discuss how to im-plement bundling for text, where we preserve mean statistics. 3.1. Text Bundling The first step in bundling is to select a statistic or statistics to preserve. For text, we choose the mean statistic of each feature. Rocchio and Multinomial Naive Bayes perform classification using only the mean statistics of the data. As these classifiers perform well, the mean statistics are very important for text classi-fication.
 We assume that there are a set of training documents for each class. We apply bundling separately to each class, so we will only discuss what needs to be done for a single class. Let D = { ~ d 1 ,..., ~ d n } be a set of doc-uments. It is standard to use the  X  X ag-of-words X  rep-resentation (McCallum &amp; Nigam, 1998), where each word is a feature and a document is represented as a vector of word counts. We write ~ d i = { d i 1 ,...,d iV where the second subscript indexes the words and V is the size of the vocabulary. We use the mean statistic for each feature as our text statistics. So, we define the j th statistic as procedure Randomized Bundling 1: Let n be the number of documents. 2: Let m be the chosen partition size (we assume n/m 3: Let s 1 ,...,s V be the mean statistics as defined in 4: Randomly partition the set of documents D into 5: Compute d 0 ij = s j ( P i ), where ~ d 0 i is the i th 6: D 0 = { ~ d 0 1 ,..., ~ d 0 m } is the reduced data set. As an example, consider what happens when we do  X  X aximal bundling. X  We reduce to a single point with the mean statistics; the j th component of the single point is s j ( D ). Using a linear classifier on this  X  X aximal bundling X  will result in a decision bound-ary equivalent to Rocchio X  X  decision boundary. Both decision boundaries are the perpendicular bisector of the sample means of the two classes. Thus, bundling degrades well; even when reducing to a single point, we expect the training set to provide enough informa-tion to create an effective text classifier. Other data reduction techniques do not degrade as nicely. Bundling gives us the power to trade-off between the advantages of a mean statistic based classifier (e.g., Rocchio) and the advantages of one that uses the full data (e.g., an SVM). Thus bundling allows us to ex-plicitly adjust the level of classification speed and ac-curacy.
 For text bundling, we partition the full data set D into m equal-sized partitions P 1 ,...,P m . Each partition becomes a single data point in the bundled data set D 0 = { D 0 the mean statistics of the data in partition P i . We calculate the elements of D 0 i as D 0 ij = s j ( P i ). Note this method also satisfies the global constraint; the global mean is simultaneously conserved. We then feed D 0 to a classifier of our choosing.
 The remaining question is determining how to parti-tion the points. We present two algorithms, random-ized bundling and Rocchio bundling. In randomized bundling, we simply partition points randomly. This takes very little time: one pass over the n training points X  X he same as subsampling.
 Randomized bundling puts together random points, so it poorly preserves data point locations in feature space. Ultimately we would like to preserve as much location information as possible by bundling nearby points. We might try doing a bottom-up clustering where we combine elements closest to one another, but procedure Rocchio Bundling 1: Let n be the number of documents. 2: Let m be the chosen partition size (we assume n/m 3: Let s 1 ,...,s V be the mean statistics as defined in 4: Sort the document indices { 1 ,...,n } according to 5: Partition the documents according to the sorted 6: Compute d 0 ij = s j ( P i ), where d 0 i is the i th reduced 7: D 0 = { ~ d 0 1 ,..., ~ d 0 m } is the reduced data set. simply computing all pairs of distances is too time con-suming. An alternate, faster clustering algorithm is k -means, an algorithm that iteratively attracts points to k centroids. Empirically this method was not much more accurate than randomized bundling but it was much slower. Next, we describe a faster algorithm that preserves more location information than random, but runs faster than the two clustering algorithms. It is difficult to do any fast clustering while considering all dimensions of the data. If we can project the points onto one important dimension, then we can cluster as fast as we can sort. Rocchio bundling projects points onto a vector and then partitions points that are near one another in the projected space. For binary classi-fication, that vector is the one that connects the class centroids. For multi-class problems, we choose a vec-tor for each class that connects the class centroid with the centroid of all the other classes X  data.
 Let ~c the centroid of one class, and ~c 0 the other cen-troid. Let ~ d i be the data point. Our score is the pro-jection of ~ d i on to ~c 0  X  ~c : By sorting documents by their score, then bundling consecutive sorted documents, we concatenate similar documents.
 This provides a quick way to decide which points fall within a bundle. Further details on the algo-rithm are provided in the Rocchio bundling pseudo code. The pre-processing time for Rocchio bundling is O ( n log( m )).
 Train Size 12,000 4797 7,700 179,215 Test Size 7,982 4,822 3,019 49,145 Features 62,060 55,194 18,621 266,901 SVM time 6464 2268 408 ?
Accuracy 86.5% 92.8% 88.7% ? 3.2. Other Applications and Methods for Here we describe how bundling might be used in do-mains where more statistics need to be preserved or where statistics other than the sample mean are im-portant. This section is not relevant to text, but may be of interest to a wider audience interested in apply-ing it to different domains.
 Single, simple statistics like feature maxima, minima and means can be solved in a straightforward fashion like the text example. If each local bundle has the maximum of each feature, then the global maximum for each feature will also be conserved.
 One can also bundle with two or more statistics si-multaneously, though only in special cases. Instead of bundling a partition to one point, we bundle to two or more. One can preserve the minimum and maximum statistics by creating two points, one of which contains the minimum value for each feature, the other contain-ing the maximum. One can preserve mean and vari-ance statistics by converting each partition into two points that have the same mean and variance statis-tics as the partition. Both of these examples simulta-neously satisfy the local and global constraints. 4.1. Data Sets and Experimental Setup Our experiments try to quantify the relationship be-tween speed and accuracy for five different data re-duction techniques at varying levels of speed-ups. In order to perform these comparisons, we made a test bed as broad and fair as possible. We compared the various reduction techniques on SvmFu, a fast, chunk-ing, publicly available SVM implementation (Rifkin, 2000). We coded each pre-processing step in C++, and compared the total reported preprocessing, train-ing and testing time reported by the UNIX time com-mand, for user process time. We used a fast, relatively un-used machine (1GB RAM, 1.6GHz PIII processor) to perform all the experiments.
 We use four well-known text data sets: 20 Newsgroups (McCallum &amp; Nigam, 1998; Slonim &amp; Tishby, 1999; Berger, 1999), Industry Sector (Ghani, 2000), Reuters-21578 (Yang &amp; Liu, 1999; Joachims, 1997; Schapire &amp; Singer, 2000), and Ohsumed(Hersh et al., 1994). The data sets are summarized in Table 1. We use Rain-bow to pre-process the raw documents into feature vec-tors (McCallum, 1996); our pre-processing steps mimic those used by Rennie and Rifkin (2001) for 20 News-groups and Industry Sector; we use the Mod-Apte split for Reuters-21578. For Ohsumed, we used the top ten categories and split training and test by date; we com-pute accuracy as the average binary accuracy of those ten categories.
 We chose to base our tests on the Support Vector Ma-chine (SVM), a highly accurate, but slower (super-linear) algorithm for classification. In many text-classification tasks, it has consistently outperformed other algorithms (Yang &amp; Liu, 1999; Joachims, 1997). The SVM takes the positive and negative training points, and tries to place a hyper-plane between them so that it optimizes a tradeoff between classification error and margin width. It is more sensitive to the ex-act location of the training points than algorithms that simply use the features X  means. For more information about the SVM, see the Burges (1998) tutorial. For our experiments, we use the SMART ltc transform; the SvmFu package is used for running experiments (Rifkin, 2000). We set C , the penalty for misclassi-fying training points, at 10. We produce multi-class labels by training a one-versus-all SVM for each class and assigning the label of the most confident SVM. We use the linear kernel for the SVM since after applying the SMART ltc transform, the linear kernel performs as well as non-linear kernels in text classification (Yang &amp; Liu, 1999).
 We use one of the best performing feature selection algorithms used in (Mladenic, 1998), which ranks fea-tures according to | p ( f i | +)  X  p ( f i | X  ) | , where p ( f is empirical frequency of f i in class c training docu-ments. Subsampling is done by randomly removing training documents, and bagging is done as outlined in the related work section. The remaining two algo-rithms, Randomized bundling and Rocchio bundling are the focus of this paper. 4.2. Results In this section, we analyze the results from our em-pirical work found in Table 2 and in Figure 1 showing the exact tradeoffs between speed and accuracy on the various data sets. We discuss how each of the five speed-up techniques worked.
 Our results on Ohsumed explain how different data reduction techniques work on truly large data sets. Neither bagging nor feature selection were useful on the data set. No bagging runs completed within the allotted time (8 hours per run) and feature selection required reducing the feature set to 50 features, which yielded very poor results.
 In general, feature selection was a disappointment on both speed and accuracy grounds. The actual fea-ture selection algorithm is empirically (though not al-gorithmically) a small factor slower than the other al-gorithms: one must perform calculations for every fea-ture, and the number of features tends to be larger than the number of data-points. This effect is rela-tively minor. More importantly, the speedups in train-ing times for our SVM were relatively minor. In our tests, reducing the number of training points sped up the algorithm more than reducing the number of fea-tures.
 Our results on Ohsumed help explain why feature se-lection does so poorly in our empirical results. At the fast, inaccurate end, we choose the top 50 or so features. However, those 50 features are distributed among 170,000 training points, so most documents end up consisting of a single feature. If a document X  X  sin-gle feature is a somewhat common word, it will tend to appear at least once in both the class and its op-posite class. So a common feature will result in du-plicate documents in opposite classes, generally mak-ing that feature useless. If the document X  X  single fea-ture is somewhat rare, and only appears in one class, it generally can only help classify a small percentage of the test documents. So when feature selecting to only a few points, classification accuracy sometimes becomes near random. When we choose more features for Ohsumed (even 100 total features) we find that the SVM takes much longer than our eight hour time limit. Bagging was generally more accurate than feature se-lection, but was slow. As discussed before, splitting the training set into more bags reduces the training time, but increases the test time. The amount of train-ing time per number of bags is shaped like a  X  X  X : early speed increases are due to less training time, later speed decreases are due to more testing time. This means that bagging can only speed up an algorithm to a certain point that is slower than some of the other algorithms. On the Ohsumed database, there were no bagging sizes that came close to completion within our eight hour time limit.
 Bagging yielded good accuracies for Reuters, and ac-ceptable accuracies for 20 news and industry sector. Its accuracy is tied to subsampling X  X  accuracy: the sharp drop off visible in the Industry Sector graph is due to the fact that subsampling down to a certain level yields extremely low (nearly random) accuracies; and combining a series of almost random classifiers does not really help create a better classifier. Subsampling worked overall better than expected. Relative to the other algorithms, the process of ran-domly selecting documents is fast. For a given re-duced training set size, subsampling pre-processing runs faster than any of the other data reduction al-gorithms. More importantly, the data points that it produces are more sparse (more zero entries) than bundling, and hence an algorithm like the SVM will run faster with subsampled points than with denser bundled points. For example, on Ohsumed we could run the SVM with 18,000 subsampled points, but with only 12,000 bundled points.
 Subsampling also leads to surprisingly accurate clas-sification on the two binary problems, Reuters and Ohsumed. On Reuters, it appears that a small num-ber of documents can adequately define a class; so for a given amount of time, subsampling will often perform better than the other algorithms. On Ohsumed, the accuracy seems to level off in certain ranges, perform-ing worse than bundling for higher amounts of time, but better than bundling for intermediate amounts of time. Subsampling did not work well on the multi-class problems. We believe this is because the multi-class problems are more difficult and require more training points to generate a good classifier.
 In all of our data sets, subsampling has a steep drop off in accuracy; eventually at 1 point per class, it will obviously do poorly. The difficulty with subsampling is knowing when that drop off will occur. One might get lucky, like with Reuters, where we found the heavy drop off doesn X  X  occur until you remove 19 of every 20 documents. Or one might get unlucky, like with 20 News, where removing half of the documents causes an immediate drop in accuracy.
 The most consistent and often best performing algo-rithms were the two bundling algorithms. They had the best performances for 20 news and industry sec-tor, and alternated the lead on Ohsumed with sub-sampling. For most data sets, they had the highest scores at the slow/high accuracy end (bundling pairs of points for most; for Ohsumed, combining every 14 points into 1); and also did not drop as sharply as sub-sampling on the fast/low accuracy end. As mentioned before, full bundling combined with the SVM acts like the Rocchio classifier.
 The Rocchio and random bundling methods have dif-ferent strengths and weaknesses. As Table 2 shows, with minimal amounts of bundling (two points per bundle), Rocchio usually outperforms random and most other algorithms. This is because Rocchio has lots of freedom to choose the points that go in each bundle. At the other end of the spectrum, Rocchio has very few choices. For example, when bundling to one point Rocchio has no choices X  X t bundles identi-cally to random. However, Rocchio takes more time to complete. Thus, Rocchio works well when bundling to more points, but suffers from higher preprocessing times when less points are retained. We present a new data reduction technique, called bundling, that tries to maintain user-chosen statistics of the data. We focused on text bundling, where the chosen statistic is the mean of each word feature in the training documents. We gave empirical evidence that bundling performs well on a variety of text data sets. In the future, we would like to extend bundling in both a theoretical and empirical sense. It may be possible to analyze or provide bounds on the loss in accuracy due to bundling. We would like to construct general meth-ods for bundling sets of statistics. We are also inter-ested in extending bundling to other machine learning domains.
 Acknowledgements This work was supported in part by the MIT Oxygen Partnership and Graduate Research Fellowships from the National Science Foun-dation. We thank Nati Srebro and Poompat Saengu-domlert for comments on this paper.
 Berger, A. (1999). Error-correcting output coding for text classification. Proceedings of IJCAI-99 Work-shop on Machine Learning for Information Filter-ing . Stockholm, Sweeden.
 Breiman, L. (1996). Bias, variance, and arcing classi-fiers (Technical Report 460). Statistics Department, University of California.
 Burges, C. J. C. (1998). A tutorial on Support Vector Machines for pattern recognition. Data Mining and Knowledge Discovery , 2 , 121 X 167.
 Domingos, P. (2002). When and how to subsample:
Report on the KDD-2001 panel. SIGKDD Explo-rations , 3 .
 Ghani, R. (2000). Using error-correcting codes for text classification. Machine Learning: Proceedings of the Seventeenth International Conference .
 Hersh, W., Buckley, C., Leone, T., &amp; Hickam, D. (1994). OHSUMED: An interactive retrieval eval-uation and new large test collection for research. Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (pp. 192 X 201).
 Joachims, T. (1997). A probabilistic analysis of the
Rocchio algorithm with TFIDF for text categoriza-tion. Proceedings of the Fourteenth International Conference on Machine Learning .
 McCallum, A., &amp; Nigam, K. (1998). A comparison of event models for naive Bayes text classification.
Proceedings of the AAAI-98 workshop on Learning for Text Categorization .
 McCallum, A. K. (1996). Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering. http://www.cs.cmu.edu/  X  mccallum/bow.
 Mladenic, D. (1998). Feature subset selection in text-learning. Proceedings of the Tenth European Con-ference on Machine Learning .
 Rennie, J. D. M., &amp; Rifkin, R. (2001). Improving multiclass text classification with the Support Vector
Machine (Technical Report AIM-2001-026). Mas-sachusetts Insititute of Technology, Artificial Intel-ligence Laboratory.
 Rifkin, R. (2000). Svmfu. http://five-percent-nation.mit.edu/SvmFu/.
 Rocchio, J. J. (1971). Relevance feedback in informa-tion retrieval. In G. Salton (Ed.), The SMART re-trieval system: Experiments in automatic document processing , 313 X 323. Prentice-Hall.
 Schapire, R. E., &amp; Singer, Y. (2000). Boostexter: A boosting-based system for text categorization. Ma-chine Learning , 39 , 135 X 168.
 Slonim, N., &amp; Tishby, N. (1999). Agglomerative infor-mation bottleneck. Neural Information Processing Systems 12 (NIPS-99) .
 Yang, Y., &amp; Liu, X. (1999). A re-examination of text categorization methods. Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval .
 Yang, Y., &amp; Pedersen, J. O. (1997). A comparitive study on feature selection in text categorization.
Proceedings of the Fourteenth International Confer-
