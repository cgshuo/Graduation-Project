 Research on differential privacy [1,2] has shown that it is possible to carry out data analysis on sensitive data while ensuring strong privacy guarantees. Differ-privacy is defined as a property of a query answering mechanism, and a query answering mechanism satisfying differe ntial privacy must meet the requirement that the distribution of its noisy query responses change very little with the addition or deletion of any record, so that the analyst can not infer the presence or absence of some record from the responses. Formally, differential privacy uses a user-specified privacy threshold to bound the ratio of the probabilities of the noisy responses from any two neighbor databases (differing one record).
Recent work [3 X 5] showed the necessity of incorporating a us er X  X  background knowledge to improve the accuracy of estimates from noisy responses of his-togram queries. Various types of constraints (e.g., linear constraints, ordering constraints, and range constraints) may hold on the true (non-randomized) an-swers of histogram queries. So the idea was to apply the constraints over the noisy responses and find a new set of answers (called refinements) that are clos-est to the noisy responses and also satis fy known constraints. As a result, the refinements expect to boost the accura cy of final histogram query results.
However, there is one key question: is the ratio of the distributions of the results after refinements from any two neighbor databases still bounded? In this paper, we introduce a new definition,  X  -differential privacy on refinement, to quantify the change of distributions of refinements. We focus on one represen-tative refinement, the linear refinement with linear constraints and study the relationship between the classic -differential privacy (on responses) and our  X  -differential privacy on refinement. We demonstrate the conditions when the  X  -differential privacy on refinement achieves the same -differential privacy. We argue privacy breaches could incur when the conditions do not meet. We revisit the formal definition and the mechanism of differential privacy. We denote the original database as D , and its neighboring database as D . We will concentrate on pairs of databases ( D,D ) differing only in one row, meaning one is a subset of the other and the larger database contains just one additional row. Definition 1. ( -differential privacy) [1]. A mechanism K is -differentially pri-vate if for all databases D and D differing on at most one element, and any subsets of outputs S  X  Range ( K ) , Theorem 1. [1] For f : D  X  R d , the mechanism K f that adds independently generated noise with distribution Lap (  X f/ ) to each of the d output terms satis-fies -differential privacy, where the sensitivity,  X f ,is  X f = max D,D f ( D )  X  f ( D ) 1 for all D , D differing in at most one element.
 The mechanism for achieving differential privacy computes the sum of the true answer and random noise generated from a Laplace distribution. The magnitude of the noise distribution is determined by the sensitivity of the computation and the privacy parameter specified by the data owner. Differential privacy maintains composability, i.e., differential privacy guarantees can be provided even when multiple differentially-private releases are available to an adversary, and can extend to group privacy, i.e., changing a group of k records in the data set induces a change of at most a multiplicative e k in the corresponding output distribution [6]. based on background knowledge. We present definitions of unbiased refinement and constrained refinement. We finally introduce our key concept,  X  -differential privacy on refinement , and use an illustrating example to show the differ-ence between the proposed  X  -differential privacy on refinement and the classic -differential privacy.In a differentially private query answering mechanism, the analyst submits queries, the mechanism g enerates true values for the query, and perturbs them with calibrated noise to derive the responses, then returns the responses to the analyst. Usually, the analyst may possess some background knowledge about the database. With background knowledge, the analyst can refine the responses given by the mechanism, and may obtain more accurate values for his queries. 3.1 Definition We denote the original database as D , and its neighboring database as D which differs from the original database by a single record. The vector-valued query is database D for the query as X , X =( X 1 ,X 2 ,  X  X  X  ,X n ) T . The randomization mechanism satisfies -differential privacy, i.e., for an arbitrary set of integers S = { i,j,...,k } X  X  1 ,...,n } , Assume that the user knows some background knowledge about D and D denoted by B and B respectively. For database D , we denote the estimated value as % X derived by the analyst from the response using background knowl-( % X 1 , % X 2 ,  X  X  X  , % X n ) T .
 Definition 2. ( Refinement ) Given the background knowledge B on database D of query Q ( D ) based on the response X : % X =rf( X |B , D ) .
 Similarly, given response X from D , the refinement to estimate Q ( D )is % X = rf( X |B , D ).
 Definition 3. ( UnbiasedRefinement )Therefinement % X is unbiasedif E ( % X )=  X  stands for any  X  .
 Definition 4. ( Constrained Refinement ) The refinement % X is a constrained refinement if % X always satisfies the background knowledge B for any response X . D , may be mapped to two disjoint spaces by the refinement function rf(). In this case, either the numerator or the denominator of ratio Pr( X = x ) information.
 Definition 5. (  X  -differential privacy on refinement ) Given the refinements % X and % X and an arbitrary set of integers S = { i,j,...,k } X  X  1 ,...,n } , define The refinement satisfies differential privacy, if R S  X  X  S =  X  and for any subset  X   X  X  X  X  the following inequality stands 3.2 An Illustrating Example Example 1. The analyst submits a vector-valued query Q , Q =( q 1 ,q 2 ) T , max |  X   X   X  | =1,and  X  = 1 . The analyst has the background knowledge that  X  +  X  2 = c and  X  1 +  X  2 = c . One method to refine the response is shown in (4): Equivalently expressed in matrix: The refinement in (5) belongs to constrained refinement. So we can calculate the Without loss of generality, we assume that  X  1  X   X  1 =1.When x 1 is sufficiently large, we can simplify formulas (6) and (7) to formulas (8) and (9) respectively. The ratio of the two PDFs can then be calculated as shown in (10), which tends to be e 2 for large value of response X 1 . So we can conclude that the ratio between the distributions of refinements for databases D and D could be different from the rati o between the distributions of privacy on refinement. In this section, we formally the linear constraint based background knowledge and conduct theoretical analysis on how refinement strategies affect differential privacy on refinement. We will use the following scenario as a running example throughout this section. Consider that a data publisher (such as a school) has collected grade information about a group of students and would like to allow the third party to query the data while preserving the privacy of the individuals q grades A , B , C , D ,and F respectively; q p represents the number of passing students (grade D or higher) and q t represents the query for the number of all the students. The analyst may have the background knowledge in terms of the linear con-straints shown in (11). The first two constraints are by the definition and inde-pendent of the underlying database whereas the third constraint holds specifi-cally on the current database.

The analyst may have the background knowledge in terms of the ordering constraint, e.g.,  X  A  X   X  p . Ordering constraint can also be enforced when the user submits the vector query. For example, the analyst may submit a simple ascending ordering query that shows the number of students in each category. In other words, the analyst knows for sure that  X  1  X   X  2  X  ...  X   X  n although the responses may not hold the order constraints due to calibrated noises. Similarly the range constraint denotes the true answer of a particular query is within some post-process of noisy output. For example, users apply non-negative constraints We will study these types of background knowledge in our future work. 4.1 Refinement with Linear Constraint Assume that the user knows m linear combinations of the true answers: and c =( c 1 ,...,c m ) T .
 Definition 6. ( Linear Constraint ) The background knowledge with linear constraint can be expressed as where B is an n  X  m matrix and c is an m -dimensional constant vectors. Under the linear constraint based background knowledge, a constrained refine-ment % X must satisfy B T % X = c for any response X .
 Definition 7. ( Refinement with Linear Constraint ) The refinement % X is linear if it can be expressed as where A and D are n  X  n and n  X  m matrices respectively, and h is an n -dimensional constant vector. 4.2 A General Result Theorem 2. Suppose that the user possesses the linear background knowledge B
T  X  = c and B T  X  = c for database D and D respectively, and he implements some constrained linear refinement as shown in (12) to estimate  X  . Assume 00 , be the SVD of noise from distribution Lap (  X  ) ,  X  =  X  Q / ,wouldresultin where  X  and  X  are from the two databases that achieves  X   X   X  1 =  X  Q . Proof. Let  X  = {  X  1 ,..., X  k } X  X  1 , 2 ,...,n } ,and P  X  be the n  X  k matrix with matrix P  X   X  defined likewise. We can rewrite the refinement function to the PDF of Z is Let W = UZ , S = % X  X  Dc  X  h = U 1 Z 1 ,and T = U 2 Z 2 . Then, W = S + T , S  X  X   X  , S  X  X   X  ,andthePDFof W is given by Notice that S and T are actually the projection of W onto the space spanned by U 1 and U 2 respectively, and the two spaces are orthogonal. For any s  X  X   X  and t  X  X   X  , W = s + t would always give S = s . Therefore, if s  X  X   X  ,thePDF of S is given by Hence, the PDF of % X can be given by if
Similarly, the PDF of % X is given by if % X
 X  can be expressed as where D ( x  X  )= { x  X   X  : U T 2 ( P  X  x  X  + P  X   X  x  X   X   X  Dc  X  h )=0 } .
The PDF of % X  X  can be derived in a similar manner. When the ratio of the integral kernels in (13) is bounded, i.e., the ratio of the integrals, f X that f X and f X are the Laplace distribution p.d.f., and hence Therefore, when the noise is added accord ing to the classical schema , i.e., take  X  =  X  Q = 1  X   X   X  1 ,wecanhave f X A special case of the above result is that  X  = when c = c (no difference on constants of linear background constraints over two neighbor databases). However, in practice, c could be different from c (refer to the example shown in -differential privacy. A direct result from the above theorem is that, in order to guarantee e  X   X  f X
A  X  D ( c  X  c )  X  (  X   X   X  ) 4.3 The Best Linear Refinement Consider the following least square refinement based on the linear background knowledge: Theorem 3. The least square refinement from the optimization problem in (15) is given by The refinement shown in (16) is a constrained unbiased refinement. It has the minimum variance of % X i , i =1 ,...,n , among all linear unbiased refinements. Proof. The Lagrange function of (16) is Taking  X  L Equivalently, % X can be expressed as follows: Next, we show that % X is a unbiased constrained refinement: Next, we prove the minimal variance property. We use M to denote the matrix I  X  B ( B T B )  X  1 B T .Thenwehave M = M T , MM T = M .Wecanfurther show that ( A  X  I ) M = 0 . Notice that the following equalities stand for any  X  , We can thus have I  X  A = DB T and h = 0 . Therefore, of matrix AA T . With MM T = M and ( A  X  I ) M = 0 ,wecanhave entries are non-negative, and hence ( AA T ) ii  X  M ii with A = M minimizes (
AA T ) In this paper we have introduced a new definition,  X  -differential privacy on re-finement, to quantify the change of distributions of results after refinements. We focus on one representative refinement, the linear refinement with back-ground knowledge as linear constraints and investigate the relationship between refinement.

Three techniques were proposed to use constraints to boost accuracy of an-swering range queries over histograms [3 X 5]. The refinement approach (also called constrained inference) [3] focused on usin g consistency constraints, which should hold over the noisy output, to improve a ccuracy for a variety of correlated his-The proposed approach, the minimum least squares solution , was a special case -differential privacy. In our work, we in troduced the general linear refinement and showed the conditions on when the refinement based on the general linear constraints achieves the same -differential privacy as defined over distributions of responses. The authors extended to refine degree distribution of networks un-an approach based on the Haar wavelet. In [5], the authors unified the two ap-proaches [3, 4] in one general framework based on the matrix mechanism that can answer a workload of predicate counting queries.

One key question is whether background knowledge can be exploited by ad-versaries to breach privacy. It is well known that for the pre-processing based privacy preserving data mining models, several works [8, 9] showed the risks of privacy disclosure by incorporating a user X  X  background knowledge in the rea-soning process. In contrast, in the context of differential privacy, the authors in [1,2] stated that differential privacy provides formal privacy guarantees that do not depend on an adversary X  X  backgr ound knowledge (including access to other databases) or computational power. In [10], the authors gave an explicit formulation of resistance to background knowledge . The formulation follows the implicit statement: Regardless of external knowledge, an adversary with access to the sanitized database draws the same conclusions whether or not my data is included in the original data. They presented a mathematical formulation of background knowledge and belief. The belief is modeled by the posteriori distri-bution: given a response, the adversary draws his belief about the database using the refinement has no impact on the differential privacy guarantee. This is be-cause the analyst performs the refineme nt without access to the private data, using only the constraints and the perturbed responses. The perturbed responses are simply the output of a differentially private mechanism and post-processing of responses cannot diminish the rigorous privacy guarantee.

In [11], the authors examined the assumptions of differential privacy from the data generation perspective and proposed a participation-based guideline -does privacy. They showed that the privacy guarantee from differential privacy can degrade when applied to social networks or when deterministic statistics (of a contingency table) have been previously released. The deterministic statistics can be modeled as linear constraints with fixed c values. In this case, c could be different from c . Based on our Theorem 2, the  X  -differential privacy on refine-ment is different from the -differential privacy and we have to add larger noise to prevent privacy breaches. In practi ce, the adversary may possess any kind of background knowledge, which may even include the a-priori knowledge of the exact values of all other n  X  1 individuals. We refer readers to the example shown in Appendix where the adversary can exploit the background knowledge of the We argue that the privacy breach is caused by the combination of the random-ization mechanism and the background knowledge. In our future work, we would explore whether refinements with some particular background knowledge (e.g., ordering or range constraints) can incur privacy breaches, i.e., enabling the ad-versary to draw significantly different beliefs about the databases. Acknowledgments. This work was supported in part by U.S. National Science Foundation IIS-0546027 and CCF-0915059.
 A.1 Example When c = c Database D with n records is obtained by adding one record to database D 0 . Every record in D belongs to one of two categories. The attacker knows that in D 0 , k X  1 =  X  2 ,where  X  i denotes the count of category i in D 0 , i =1 , 2. The added record belongs to either of the two categories, denoted by D and D respectively. Let  X  =  X  1 respectively. The background knowledge can be expressed as: where B = k  X  1 .

Response X =( X 1 ,X 2 ) is obtained by adding noise Lap ( 2 ). Next, we show Consider the following refinement: Comparing (17) and (18) with the general linear refinement formula in (12), we can have For D , we can similarly have that when x 1  X   X  1 +  X  2  X  1 k +1 , With  X  = 2 (satisfying -differential privacy), we can have: Therefore, the ratio f X indicates the adversary can tell which database of D and D the response is from. In other words, the adversary ca n derive the value of the added record by
