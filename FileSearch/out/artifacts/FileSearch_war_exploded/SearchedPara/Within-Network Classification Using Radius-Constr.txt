 W ithin-N etwork C lassification (WNC) techniques are de-signed for applications where objects to be classified and those with known labels are interlinked. For WNC tasks like web page classification, the homophily principle suc-ceeds by assuming that linked objects, represented as ad-jacent vertices in a network, are likely to have the same la-bels. However, in other tasks like chemical structure comple-tion, recent works suggest that the label of a vertex should be related to the local structure it resides in, rather than equated with those of its neighbors. These works also pro-pose structure-aware vertex features or methods to deal with such an issue.

In this paper, we demonstrate that frequent neighborhood patterns , originally studied in the pattern mining literature, serve as a strong class of structure-aware features and pro-vide satisfactory effectiveness in WNC. In addition, we iden-tify the problem that the neighborhood pattern miner indis-criminately mines patterns of all radiuses, while heuristics and experiments both indicate that patterns with a large radius take much time only to bring negligible effectiveness gains. We develop a specially designed algorithm capable of working under radius threshold constraints, by which pat-terns with a large radius are not mined at all. Experiments suggest that our algorithm helps with the trade-off between efficiency and effectiveness in WNC tasks.  X 
W e thank the anonymous reviewers for their constructive comments. We are grateful to Chunbin Lin for his discus-sions and feedbacks, and Christian Desrosiers for his ad-vices on implementing the RL-RW-Deg algorithm. This work was partially supported by the National Key Basic Research Program (973 Program) of China under grant No. 2014CB340403, the Fundamental Research Funds for the Central Universities, the Research Funds of Renmin Uni-versity of China, an NSERC Discovery grant and a BCIC NRAS Team Project. All opinions, findings, conclusions and recommendations in this paper are those of the authors and do not necessarily reflect the views of the funding agencies. H.2.8 [ Database Management ]: Database Applications X  Data mining
Since the ninetieth of the last century, frequent pattern mining has been an active research theme, enabling knowl-edge discovery and mining in various kinds of data [10]. Be-sides applications like association rule mining [1] and index-ing [35], according to [6], frequent patterns are especially effective features for corresponding classification tasks [21, 7], because they tend to capture the rich underlying seman-tics of the hosting data.

For graph data, it is obvious that the properties of the data do not hide in any single vertex or edge, but in the way they are combined and organized. In other words, the semantics of graphs lie in their structures. In the literature of graph classification, feature-based approaches, such as [7], all embrace sub-structure patterns of graphs as the most significant cue for classification.

In fact, [20] clarified that graph data is actually mod-eled differently in two settings: the graph-transactional set-ting and the single-graph setting . In the first setting such as chemical structure databases, the data is viewed as a set of relatively small graphs, called transactions . Graph classification tasks, such as molecule property prediction, should be categorized into this setting because they essen-tially classify transactions. In the second setting such as the web graph and social networks, however, the data is viewed as a large network. The corresponding classification task, commonly referred to as Within-network classification (denoted by WNC for short), actually involves classifying vertices rather than transactions.

Since transactions have structural patterns as indicators of their property, two analogous questions are interesting in the single-graph setting: 1) do vertices have structural patterns? and 2) if so, can they serve as vertex classification features? Our earlier work [11] answered the first question, where we formulated F requent N eighborhood pattern M ining (FNM) to mine patterns for vertices from the local structure they reside in. For example, in an academic network, a neigh-borhood pattern may suggest that some papers cite another paper with a common author. In a social network, a neigh-borhood pattern may represent persons having a son and a daughter. In the structure graph of a molecule, a neighbor-hood pattern may indicate that some carbon atoms appear on a cycle of length 6. It is obvious that neighborhood pat-terns carry rich semantics about the behavior of vertices in a network. In the first half of this paper we answer the second q uestion based on the work of [11], by relating the label of a vertex to the neighborhood patterns it follows.

It is worth clarifying that our assumption is essentially different from the homophily principle, which states that the label of a vertex tends to follow the majority of the vertex X  X  neighbors. This principle undoubtedly applies to many scenarios, e.g., friends share hobbies, and web pages link to pages with similar topics. However, properties of vertices do not always propagate along edges. For example, a carbon atom in a chemical structure may be surrounded by many hydrogen ones. Instead, we relate the classification of a vertex to the structure of its neighborhood, which is not limited to the information of all neighbors X  labels. The  X  X omophily-or-structure X  issue has caught some attention in recent WNC studies [12, 8, 25]. But to the best of our knowledge, we are the first to address this issue under the context of frequent-pattern-based classification.

In the second half of this paper, we consider incorporating the Markov assumption [24] of WNC into our neighborhood-pattern-based classifier. According to the assumption, dis-tant structures tend to have negligible effects on the classi-fication of a vertex, thus can be neglected for the interest of efficiency. However, the neighborhood miner in [11] can-not directly control the radius of the patterns it finds. For example, it may produce a pattern of radius 3, saying that the center person has a friend whose father works for a com-pany with the label  X  X T X . This pattern may not be effective in classifying the person because it takes distant structures into consideration. We design an algorithm called r-FNM (radius-constrained frequent neighborhood mining), which accommodates a pre-defined radius threshold on the pat-terns. Compared to the na  X   X ve approach of post-filtrating large-radius patterns, r-FNM saves time by not generating them at all, thus making the trade-off more worthwhile.
We summarize our contributions as follows: 1) We empir-ically show that neighborhood patterns are effective vertex features in WNC tasks; 2) We propose an algorithm r-FNM to directly mine neighborhood patterns w.r.t. a given radius upper bound; 3) We experimentally show that r-FNM helps balancing effectiveness and efficiency in WNC tasks.
The remainder of this paper is organized as follows: Sec-tion 2 reviews related work. Section 3 introduces relevant preliminaries. In Section 4, we discuss how to apply neigh-borhood patterns to classify networked data. We incorpo-rate the radius constraints into the generation of neighbor-hood patterns in Section 5. We present experimental results in Section 6, followed by conclusions in Section 7.
The problem of within-network classification has been ex-tensively studied in recent years. The distinguishing chal-lenge of WNC problems is that the conventional independent and identical distribution (i.i.d.) assumption does not hold, because the labels of unclassified objects depend on each other. [24] points out that common within-network classi-fiers all comprise a local classifier, a relational classifier, and a collective inference procedure. The local classifier performs a rough classification using network-independent informa-tion (such as the text content of a web page or the age of a person) to provide an initial estimation. Given a vertex v , the relational classifier adjusts the prediction for v by consid-ering both the network structure and the predictions of the Figure 1: Random walk and degree information cannot dis-t inguish between the neighborhoods of vertex 1, 2, 3, . . . other vertices. The collective inference component repeat-edly calls the relational classifier on every vertex according to certain updating strategies, and returns the classification results when certain stopping criterion is met.

Our focus lies under the scope of the relational classi-fier component. We note that, the relational classifier actu-ally encodes our assumption for the specific problem. The simplest wvRN [23] method, which operates under the ho-mophily assumption, classifies a vertex using the weighted votes of its neighbors X  labels. Moreover, cdRN [28] and nLB [22] adopt a more general assumption, which relates (but not equals) a vertex X  X  label to the distribution of those of its neighbors. However, none of those classifiers exploits the rich structural information of the network. [8] proposes RL-RW-Deg, which characterizes the structure of a vertex X  X  neighborhood by its degree and the distribution of different label sequences obtained after starting a random walk from the vertex. RL-RW-Deg is reported to outperform various non-structural methods [23, 28, 22] in the chemical struc-ture completion task. However, we categorize RL-RW-Deg as a semi-structural method, because it is not enough to recover a vertex X  X  neighborhood structure using only its de-gree and random walk distribution information. Consider Figure 1. Here each connected component is a cycle of an even length, where vertices have alternating A and B la-bels. Let v 1 , v 2 , . . . , v i be an arbitrary A vertex in the component of length 2 i . Apparently, v 1 , v 2 , . . . , v in different neighborhood structures. However, they have equal degrees, and all of them only initiates random walks of the label sequence AaBaAaBa . . . . Therefore, RL-RW-Deg does not have a full discriminative power at least in this case. In this paper, we adopt RL-RW-Deg as the baseline and compete with it on the same task. It is worth noting that [12, 25] also proposes structure-dependent approaches. However, their methods cannot exploit edge label informa-tion, thus are only applicable to homogeneous networks. As Section 1 mentions, many WNC methods employ the Markov assumption, while ours is no exception. Besides [23, 28, 22], ego-net based graph mining methods [2, 12], which represent a vertex as the induced graph generated by its neighbors and itself, also adopt this assumption implicitly.
The collective inference component is critical to WNC tasks [15]. Common collective inference algorithms include iterative classification (ICA) [27], Gibbs sampling [4], and relaxation labeling [5]. In our method, we use ICA as our collective classifier. We refer readers to [29] for a summary of common collective classifiers and their comparison.
When evaluating classification algorithms, it is common to use cross-validation to fully exploit the labeled data. Mean-while, it is desirable to compare the effectiveness of different WNC algorithms with networks of varied label ratio (i.e., the proportion of labeled vertices). However, the above two options are contradictory, because the fold number of cross-validation determines the ratio of training set size and test-ing set size, thus fixing the label ratio. To cope with this issue, work such as [24] replaces cross-validation with a sim -ple resampling procedure, thus resulting in overlapped test-ing sets. [26] points out that applying paired t -test in such a scenario leads to potentially high type I error (i.e., per-formance improvements will be incorrectly concluded when there is none). They also propose N etwork C ross-V alidation (NCV) as a solution to enable cross-validation in WNC with-out introducing overlapped testing sets. In this paper, we adopt NCV in our experimental methodology. More details will be given in Section 6.1.

Like the single-graph-based WNC problem, graph classi-fication under the graph-transactional setting has also re-ceived much attention. Common graph classification meth-ods fall under two categories: pattern-based and kernel-based. Pattern-based methods, such as [7], employ frequent subgraph patterns as classification features, which inspires us to apply neighborhood patterns to vertex classification. Sophisticated feature selection methods [32, 18] are also pro-posed to select the most discriminative subgraphs for clas-sification. Kernel-based methods, however, propose various kernel functions to measure the similarity between different transactions by the structure indicators they share. Com-mon indicators include random walks [16], shortest paths [3], subtrees [30], cycles [13], and graphlets [31]. [9] provides an interesting hardness result, suggesting that any graph ker-nel is not complete , i.e., having a full discriminating power, unless it is as hard to compute as to test graph isomorphism. Definition 1. Let  X  V be the set of vertex label names, and  X 
E that for edge labels. An (undirected) labeled graph is a 3-tuple G = h V, E, l i , where V is a set of all vertices, and  X 
V  X  X  null } and E  X   X  E is a labeling function mapping vertices and edges to their labels. Note that a vertex v may have no label, i.e., l ( v ) = null .

In the rest of this paper, when modeling our data graphs, i.e., networks, we use symbol N instead of G . Although only undirected graphs are discussed, our work can be straight-forwardly extended to directed graphs.

Given a network N with some vertices having no labels, following [24], we let V U = { v | l ( v ) = null } be the set of vertices to be classified, and V K = V \ V U the set of vertices with known labels. Our task is to classify V U into  X  V based on the given information about V U and N .
In this section, we revisit some key points of [11], which will be used in this paper.

Definition 2. A pivoted graph is a tuple G = h G, v p i , where G is a labeled graph, and v p  X  V ( G ) is a special vertex in G , called the pivot of G .

A neighborhood pattern P is a connected pivoted graph, w here the pivot points out the vertex whose neighborhood the whole pattern describes. For example, Figure 2b shows 7 neighborhood patterns (referred by IDs), where P 2 stands for vertices associated with two a -edges. All non-null ver-tex labels and labeled edges are called elements of a pat-tern. The size of a pattern P is defined as |P| = |{ v | l ( v ) 6 = (b) Some neighborhood patterns of Figure 2a. Pivots are marke d gray. Not all patterns are listed.
 Figure 2: A network with some neighborhood patterns. We use uppercase letters for vertex labels and lowercase for edges. Null vertex labels are omitted. null }| + | E | , i.e., the number of elements. For example, in Figure 2b, the sizes of both P 2 and P 4 are 2, respectively.
Definition 3. A pivoted graph G 1 is pivoted subgraph isomorphic to G 2 , denoted by G 1  X  p G 2 , if there exists an injective mapping f : V ( G 1 )  X  V ( G 2 ) such that: 1) labels); 2)  X  ( v 1 , v 2 )  X  E ( G 1 ), l ( v 1 , v 2 ) = l ( f ( v serving edge labels); 3) f ( v p ( G 1 )) = v p ( G 2 ), where v the pivot of G (mapping pivot to pivot).

Using pivoted subgraph isomorphism, the match between a neighborhood pattern and a vertex in the network can be defined. P matches vertex v of a network N , if P is pivoted subgraph isomorphic to the pivoted graph derived by making v the pivot in N , i.e., P  X  p h N, v i . For instance, in Figure 2b, P 2 matches v 2 , v 3 , v 4 , and v 5 in Figure 2a, while P only matches v 3 and v 4 . We can also use  X  p to define a partial order on all neighborhood patterns to describe the sub/super pattern relationship. A neighborhood pattern P is called a sub-pattern of P  X  , if P  X  p P  X  . For example, in Figure 2b, P 1 and P 2 are both sub-patterns of P 5 .
Definition 4. Given a vertex set V 0  X  V ( N ), the set of vertices in V 0 that P matches is M V 0 ( P ) = { v  X  V 0 h N, v i} . The support of P in V 0 is defined as the size of M
V 0 ( P ). P is a frequent neighborhood pattern of V 0 , if its support is above a given threshold  X  .

For instance, in Figure 2a, M V ( P 2 ) = { v 2 , v 3 , v Therefore, the support of P 2 in Figure 2a is 4. With the support measure defined, the FNM problem is simply find-ing all frequent neighborhood patterns, w.r.t. a given vertex set V 0 and a threshold  X  .

Like conventional apriori-based algorithms, [11] generates and verifies patterns in the search space in a level-wise man-ner, where patterns are at levels corresponding to their sizes. After obtaining all frequent patterns at level k , we join each pair of patterns which share a size-( k  X  1) sub-pattern to generate candidates to be verified at the next level. For in-s tance, in Figure 2b, the join of P 2 and P 3 produces P we get P 7 if we join P 3 and P 4 . Note that self-join is allowed. A join may produce multiple candidates. For example, the self-join of P 3 produces P 5 and P 6 .

Like most frequent pattern mining problems, e.g., frequent itemset mining [1] and frequent subgraph mining [14, 19, 34], FNM also satisfies the anti-monotonicity property, i.e., for any patterns P  X  p P  X  , the support of P  X  does not exceed that of P . For example, in Figure 2b, the support of P 2 4, while for P 5 it is 2. Given a threshold  X  , say, 3, once P 5 is found infrequent, we removes it to avoid checking its super-patterns. The pruning is both sound and efficient.
Although the anti-monotonicity property of FNM ensures a complete result set for apriori-based algorithms, we em-phasize non-trivial building blocks as a particularity of FNM, which, if improperly treated, may harm the completeness of the results. We define building blocks as follows and give the following result:
Definition 5. A pattern is called a building block , if it cannot be obtained by joining smaller patterns at the pre-vious level.

Lemma 1. A pattern P is a building block, if and only if there do not exist two elements in it, by removing either we o btain a valid, but smaller pattern P  X   X  p P .
 For example, in Figure 2b, building blocks are marked by dotted lines. P 3 is a building block, because to obtain a connected sub-pattern of size 1, we have no other choice than removing the rightmost dangling edge b (the vertex on the right tail is simultaneously removed because this does not affect the size). P 4 is also a building block because the only choice to shrink it is to remove the vertex label A .
When designing an apriori-based algorithm, it is vital to know the shape of building blocks in advance and guide the algorithm to treat them specially, because 1) they cannot be obtained by the join operation, 2) they themselves are part of the mining result, and 3) the absence of any of them may cause higher-level patterns depending on them to be missed. For conventional pattern mining problems, it is safe to trivially initiate with all level-1 patterns, because building blocks only appear at level-1. However, this is apparently not enough in the FNM case, since as shown in the above example, P 3 and P 4 are found to be non-trivial building blocks at level-2, and there are potentially more at the upper levels. For the FNM problem, where do building blocks appear, and what do they generally look like?
Interestingly, [11] formally proved that the building blocks of FNM are only limited to path patterns as defined below:
Definition 6. A neighborhood pattern is a path pattern if 1) it is a path of labeled edges, where the pivot is on one end of the path; 2) it contains at most one vertex label, which (if exists) must appear on the other end of the path. As the search space of path patterns is tree-like, they are easy to be generated in advance. Therefore, the FNM al-gorithm follows the apriori framework, with an important difference. During initiation, the algorithm generates all fre-quent path patterns by X  X xtending X  X ather than joining, then an apriori-based search is performed. Note that, the join op-eration at each level misses all building blocks belonging to the current level. Therefore, before moving on to the next level, the building blocks of appropriate size must be added to allow for larger patterns that depend on them. In the rest of this paper, for application-specific requirements, some-times we use an additional parameter k to end the search at level-k to produce all patterns whose size are up to k .
In the literature of graph classification, there is a signif-icant branch of research (e.g., [7]) which explores the ef-fectiveness of frequent subgraph patterns as classification features. Given a collection of patterns S generated by a subgraph mining algorithm, they represent each graph g in-volved in the task as a vector x g = ( x 1 , x 2 , ..., x where x j = 1 if and only if g contains the j -th pattern as a subgraph. After all graphs are vectorized, common clas-sification algorithms like SVM are performed to learn from graphs with known labels and classify the others.

Inspired by this, we are interested in an analogous ques-tion: can frequent neighborhood patterns [11] help classify-ing vertices in the setting of within-network classification? To the best of our knowledge, this problem is never studied in the literature. In this section, we introduce this approach by aligning it to the framework described in Section 2, and preview some of our experimental results which suggest that this approach is not only feasible but also competitive. We also identify and analyze a drawback of this approach, which motivates the second and third contributions of this paper.
According to the framework described in [24], a within-network classifier commonly consists of three key compo-nents: a local model, a relational model, and a collective inference procedure. To use neighborhood patterns to assist WNC tasks, we have to first specify the role of neighborhood patterns in such a framework, and appropriately instantiate the three components.

A local model exploits network-independent information on a vertex to provide the whole task with a good initial guess. In our method, we simply keep the null labels of un-classified vertices as the initial guess. We make this choice for three reasons: 1) a null label does not mean a cold start since the edge structure of the network carries information and is always available, 2) the study of how local classi-fiers affect the overall performance is rather orthogonal to that of the other two components, and 3) the principle of constructing local classifiers has been well studied in con-ventional machine learning literature.

In the relational model component, the neighborhood pat-terns actually take effect. Given V K as training set, first, we run the FNM algorithm on V K with appropriate param-eters and obtain a list of patterns {P j } , j = 1 ...m . Each P describes the surrounding structures where a number of ver-tices reside 1 . Second, we represent each vertex v in V K
N ote that as  X  X eatures X , P j must not contain any infor-mation about the label of its pivot vertex. This could be done by a post-filtration on the mining results. However, it is provably correct and efficient to achieve this by simply blocking those size-1 patterns consisting of only a pivot with a label from entering level-2. Figure 3: Performance of RL-RW-Deg and FNM (k=4) on t he Protein dataset an m -dimensional vector x v = ( x 1 , x 2 , ..., x j , ..., x Clearly, the patterns P j serve as vertex features for training the relational model. In the following parts of this paper, we will not distinguish  X  X atterns X  from  X  X eatures X  as they are essentially equivalent in our task. Third, we use the true label of v to form the training label, i.e., y v = l ( v ). Finally, we collect all h x , y i pairs as training data to train the relational model M .
 Algorithm 1 I terative Classification Input: N etwork N , Neighborhood Features {P j } , relational Output: Predicted Labels y U . 1: while true do 2: x U  X  1 ( P j  X  p h N, v U i i ) // 1 ( p ij ) is a 0-1 matrix, 3: y U  X  Classify ( x U , M ) 4: if y U converges or n iter is reached then 5: Break 6: else 7: Update N with y U 8: end if 9: end while 10: return y U
When doing inference, we adopt ICA [27] as the collective i nference component because it is simple and effective. The pseudo code of our implementation is in Algorithm 1. Given an initial guess and a relational model, the algorithm per-forms multiple iterations over V U . In each iteration, the fea-tures of every vertex v are reconstructed because the labels of vertices that v depends on might have changed. Then, the relational model is applied to provide a newer version of prediction. The algorithm stops when the result converges or a given number of iterations n iter have been reached.
In practice, we find frequent neighborhood features quite effective. In Figure 3, we use the chemical structure comple-tion task [8] to compete our approach with the baseline on testing networks of different label ratio | V K | | V | . We carry out 10-fold network cross-validation (further explained in Sec-tion 6.1) and all results are averaged over the 10 runs. It turns out that our method outperforms RL-RW-Deg by at least 11.7% in terms of weighted F1 score. Our features are powerful because they remember the exact structures rather than a statistics or an aggregation of the structure, which leads to a definite classification of the residing vertex. We Table 1: Large-radius features bring negligible improve-m ents. All improvements over the baseline are statistically significant under the p &lt; 0 . 01 paired t -test. will present more detailed experimental methodology, pa-rameter setting, and results in Section 6.
In Figure 3, our relational model is trained using all neigh-borhood features of size up to 4. Generally speaking, fea-tures of large sizes increase the expressivity of the relational model, enabling it to capture more semantics of the data. However, a large k allows many weak features to pass, caus-ing the training &amp; inferencing to be rather slow. Therefore, it is critical to explore techniques to prune weak features while keeping the remaining ones as expressive as possible.
For a pattern P , there is no doubt that its size |P| char-acterizes some properties of itself. However, due to the par-ticularity of neighborhood patterns, another characteristic indicator, called radius , also deserves to be emphasized. The radius of P , denoted by r ( P ), is defined as where d ( u, v ) is the distance between vertex u and v in P .
A neighborhood pattern X  X  radius has rich connections with several concepts in the network mining and classification literature. For example, in within-network classification [24] and graphical model [17], the first-order Markov assumption (or property) relates the label of a vertex to those of its direct neighbors. In [2], the ego-net (the induced graph generated by a vertex and all its neighbors) of a vertex v is used to fully represent the behavior of v . In essence, they all recognize a nature of real networks that a vertex X  X  property tends not to be affected by distant vertices or edges.

Similarly, in our approach, a feature with a large radius tends to incorporate the influence from sub-structures far-away into the classification of a vertex, while small-radius features only consider information nearby. To investigate the contribution of features of different radius, we grouped all features in Figure 3 by their radius. We retrained the model with features of radius up to 1, 2, 3, and 4, respec-t ively, applied it in the classification task, and report the fea-ture number and performance (label rate = 50%) in the first two rows of Table 1. Two points are clearly observed from the table. First, under all feature combinations, our method outperforms RL-RW-Deg. Second, the classification perfor-mance in terms of weighted F1 almost stops increasing when we attempt to add features of r = 3 and 4 (weighted preci-sion and recall also follow the same trend). This reveals the fact that, compared to small-radius features, features with a large radius tend to be relatively weak.

Since large-radius features are weak, did we spend much time on mining them? In the FNM algorithm, the time spent on patterns of different radius cannot be directly recorded, because at each level, patterns of all radius are mixed and mined simultaneously. However, the time spent and the dis-tributions of patterns w.r.t. radius at each level are avail-F igure 4: (a) Zipper patterns with r = 3; (b) A real zipper able. Therefore, we estimate the time cost w.r.t radius as follows. We ignore the time spent on building blocks because it makes up only a small proportion of the whole procedure. We also assume that at each level, time is uniformly amor-tized among patterns. For each level, we calculate the time cost w.r.t. radius using the time cost and the radius distribu-tion at this level. Then, the radius-specified time cost at all levels are accumulated to obtain the time cost w.r.t radius. We transform the results to the time cost of different feature combinations and present them in the third row of Table 1. For comparison, we also attach the time RL-RW-Deg needs for a run. It is obvious from the estimation that we could have spent little time only on small-radius features to gain most of the potential performance.
The observation described in Section 4.2 indicates that, when applying neighborhood features to WNC, most of the classification performance is earned by features with a small radius. By contrast, features with relatively large radiuses cause large running time in the feature-mining stage and classification stage, but only bring a small effectiveness im-provement. We can prune all large-radius features from the mined features to reduce the classification time. However, the time spent on mining those large-radius features cannot be taken back. When the size of training data is large and complicated, e.g., the network N is highly dense or large, the wasted computation is even bigger. This raises an in-teresting question: can we directly mine neighborhood fea-tures w.r.t. a given radius threshold r max , without generat-ing those whose radius are above r max ?
Recall that the FNM algorithm without radius constraint is a two-stage algorithm. In the first stage, the algorithm searches for all frequent path patterns. In the second stage, a level-wise apriori-based search is performed, using all path patterns from the first stage as building blocks.

With the additional radius constraint at hand, let us con-sider modifying the original algorithm to produce a correct and efficient algorithm. One may thinks that we only need to generate paths whose length is no larger than r max . How-ever, this approach again misses some building blocks, thus resulting in an incomplete result set.

Take r max = 3 as an example, i.e., the length of any path building block should not exceed 3. We consider the first neighborhood pattern in Figure 4a, where each vertex is associated with an ID for reference. Please note that the word  X  X ead X ,  X  X lider X , and  X  X ail X  are not vertex or edge labels, and are only markers for future reference. Each edge is associated with a certain edge label, which is omitted to keep the figure clear. Intuitively, the size of this pattern is 5 and the radius is 3. Therefore, according to Lemma 1, if the pattern is not a building block, we can find two edges in t he pattern, by removing either of them we obtain a valid p attern of size 4 and radius no larger than 3.

We consider removing the five edges individually. Remov-ing edge (1 , 2) or (2 , 3) is not valid because it produces un-connected patterns. (3 , 4) or (3 , 5) are unremovable either, since the radius of the resulted pattern is 4. Edge (4 , 5) is the only choice. Therefore, we cannot find two removable edges, and the original pattern is actually a building block.
For r max = 3, there are three types of non-path building blocks. In addition to the one discussed above, the other two are also shown in Figure 4a. If we arrange them in an ap-propriate order as in Figure 4a, from left to right they look like an opening zipper (Figure 4b), with the slider (the only vertex of degree 3, if it is not overlapped with the pivot) ap-proaching the tail. Therefore, we call them zipper patterns .
Definition 7. A neighborhood pattern is a zipper pat-tern of radius r max if 1) it has no vertex labels; 2) there exists exactly one special edge (we name it the  X  X ead X ), af-ter removing which the pattern is a rooted tree of depth r max , with the pivot as the root; 3) the two ends of the  X  X ead X  are the only two leaves of the rooted tree, and they both have depth r max .
 Interestingly, we can prove that for any r max , all building blocks are either path patterns or zipper patterns.
Theorem 1. Given a FNM instance with a radius thresh-old r max , the building blocks consist of 1) all path patterns whose length is no greater than r max , and 2) r max types of zipper patterns, whose size ranges from r max +2 to 2 r max
Proof. It is easy to verify that path and zipper patterns are building blocks. Therefore we only prove that any build-ing block is a path or zipper pattern. We prove by making step-wise restrictions and contradictions. Given a building block P , we narrow down by proposing predicates on the structure of P : if P violates the predicates there will be two removable elements, suggesting it is not a building block. For each case discussed below, we attach a figure in Figure 5 to illustrate the case. Finally readers will see that the predicates converges to the three conditions in Definition 7. For all cases, P must have no more than 1 vertex label. Otherwise, denoting two of them as l 1 and l 2 , it turns out that l 1 and l 2 are two removable elements.

For all cases, noticing that r ( P )  X  r max , P must have a rooted spanning tree P T , whose root is at the pivot and depth is no greater than r max . Throughout Figure 5, edges on P T are marked bold. Let n e be the number of edges of P not on P T . n e must not exceed 1. Otherwise, denoting two of them as e 1 and e 2 , e 1 and e 2 are two removable elements. Note that the removal of e 1 or e 2 does not increase the radius of P . Next, we enumerate the possible values of n e . 1. n e = 0: in this case, P = P T is itself a rooted tree with at most one vertex label. This tree must have only one leaf. If there are two leaves, the two dangling edges they are associated with will be the two removable elements. In the case where a leaf carries the only vertex label, we only remove the label instead. 1.1. The tree with only one leaf now is actually a path. However, we still have to prove that the only vertex label, (d) Case 1.1 (g) Case 2.2 F igure 5: Cases in the proof of Theorem 1. Labels with no direct influence on the proof are omitted. if exists, must be on the only leaf. If any vertex other than the leaf carries the label, the label and the dangling edge are removable. This branch of proof finally converges to the case of path patterns in Definition 6. 2. n e = 1: we denote the only edge not on P T as e . In this case, P must not contain any vertex label l . Otherwise, e and l are two removable elements. We denote the number of P T  X  X  leaves as n f , and enumerate all possible cases. 2.1. n f  X  3: P must have at least one dangling vertex v because e can consume two leaves on P T at most. In this case, e and the dangling edge are removable. 2.2. n f = 1: in this case, P T is a path pattern. We denote the dangling edge associated with the only leaf as e f . e and e f are removable. Note that removing e f does not violate the radius constraint. 2.3. n f = 2: we denote the two leaves as v 1 and v 2 . We consider two possible locations of e . 2.3.1. e is not between v 1 and v 2 : w.l.o.g., we assume that v is not associated to e , i.e., v 1 is a dangling vertex. In this case, e and the dangling edge are removable. 2.3.2. e links v 1 and v 2 : in P T , v 1 and v 2 must both have depth r max . Otherwise, we assume that v 1  X  X  depth is d &lt; r max . We denote the dangling edge associated with v as e  X  2 . In this case, e and e  X  2 are two removable elements. Note that in P \{ e  X  2 } , the depth of v 2 is no greater than ( d + 1)  X  r max because it is linked to v 1 by e . This branch of proof finally converges to the definition of zipper patterns in Definition 7.
I n the last section, we graphically and formally describe the structure of zipper patterns for the radius-constrained FNM problem. Similar to the path patterns in the original FNM problem, the new zipper patterns cannot be gener-ated by joining smaller ones, and need special treatment in algorithm design. Next, we present an approach.

Again we assume r max = 3, and we find all zippers of the first type in Figure 4a, which belong to level-5. Sup-pose P is a frequent zipper pattern. According to the anti-monotonicity property of the mining problem, the X  X  X -shaped pattern P Y obtained by removing the head of P must also be frequent, thus appearing in the results at the 4-th level. It is relatively easy to identify them from the list of size-4 patterns. We only need to check whether a pattern is a tree, calculate the number of leaves, and find the depth of the leaves. Once a pattern P Y is discovered, we find all its matching instances in the network N , and find all possible labeled edges appearing at the position of the head edge. Only those edge labels that lead to a frequent zipper pat-tern are assembled into P Y to produce a valid P . From now on, we call the radius-constrained version of neighborhood mining algorithm r-FNM.
FNM and r-FNM share some similar optimizations as FSG [19] such as TID (transaction ID) lists and canonical labels. In our current implementation, we adopted the VID (ver-tex ID) lists optimization [11], which is similar to the TID [19] one under the subgraph mining scenario. VID lists not only reduce the mining time by avoiding unnecessary verifi-cations of unpromising patterns, but also provide the IDs of vertices each pattern matches as a by-product output. This by-product is particularly useful to our classification task because the 0-1 matrix for training the relational model can be directly built with it.

Compared to the apriori-based breadth-first mining meth-ods [14, 19], it is well recognized that depth-first methods, such as gSpan [34], achieve better performances in graph pattern mining tasks. The frequent neighborhood mining problem and its radius-constrained version are both solv-able in a similar depth-first manner. We leave them as our future work.
In this section, we carry out the chemical structure com-pletion task on three real-life datasets to empirically evaluate our approaches. A state-of-the-art structure-aware WNC method called RL-RW-Deg [8] 2 was employed as the base-line. We aim to empirically answer the following questions : 1) Does neighborhood features consistently outperform the state-of-the-art? 2) Does the r-FNM algorithm save time as estimated in Table 1? 3) Does the new parameter r max en-able an efficiency-effectiveness tradeoff? 4) Compared with tuning k , does r max provides a better tradeoff? We used the first three datasets in [8] (Table 2), namely Mutagenicity, AIDS, and Protein, provided in the IAM Graph T he paper also proposed another method called RL-RW. However, it is used as a baseline of RL-RW-Deg. Figure 6: Partitioning data for network cross-validation D atabase Repository 3 . We did not explore the other two datasets in [8] since they are homophily-based. Because the three datasets were originally introduced respectively as benchmarks for graph classification, we need to first convert them such that they can be utilized in our task.

We processed the three datasets in the same manner. For each dataset, we sampled a subset of graph transactions con-sisting of 100 molecules (or proteins). We merged all trans-actions into a network, where they were regarded as con-nected components of the network. We performed 10-fold network cross-validation [26] to generate training, valida-tion, and testing sets as Figure 6 shows. Specifically, we first divided the vertex set into 10 folds. Each time we used one fold for testing, and merged the other nine as  X  X on-testing set X . We sampled V K from the non-testing set as training set such that the label ratio | V K | | V | r anged from 10% to 80%. In the non-testing set, we also sampled 10% | V | vertices as the validation set, which was disjoint from the training set V
K . Following [26], we denote V U = V \ V K as the  X  X n-ference set X , whose labels were removed and predicted in each classification run. However, in V U , only vertices in the testing set were considered in evaluation.

In our methods, neighborhood features were mined on V K with the minimum support 4 set to 2. We used SVM multiclass [33] 5 with linear kernel to train models with neighborhood features. The parameter C of SVM, which controls the bal-ance between empirical loss and regularization, was chosen from { 10  X  1 , 10 0 , ..., 10 5 } using ground truths on the valida-tion set. For RL-RW-Deg, which does not involve a training phase, we used the validation set to tune its four parame-N for the specific meaning of the parameters. h ttp://www.iam.unibe.ch/fki/databases/ iam-graph-database
According to [6], an appropriate support threshold prevents features without statistical significance from causing over-fitting in training. However, currently we do not address techniques for automatically deciding the support threshold. Given that the linear kernel has a low risk of over-fitting and that an improperly high support threshold can potentially block useful features, we decided to take this safe choice and leave the study of this parameter for future work. http://www.cs.cornell.edu/People/tj/svm_light/ svm_multiclass.html
When running the collective inferencing component in both methods, the number of iteration was set to 3 because it ensures convergence by observation. We evaluated the ef-fectiveness of all methods using the metric of weighted F1 score , which is the average of the conventional F1 score of different classes weighted by their true distribution. Each reported F1 score was an average on all 10 testing runs.
All experiments were conducted on a PC with two Xeon 2.50GHz processors and 64GB memory. All algorithms were implemented in C# and run on a single core. We report all efficiency results in milliseconds.
We report experimental results through answering the four questions raised at the beginning of Section 6.

Question 1: In Section 4.1 we mentioned that, on one of our datasets, neighborhood features of size below 4 build a strong relational model. Equipped with ICA [27] as collec-tive inferencing component, our solution outperforms RL-RW-Deg by at least 11.7% in terms of weighted F1.

In fact, neighborhood-feature-based classifiers can com-pete with the baseline, even with weaker classes of features. In Table 3 we list the performance of RL-RW-Deg and neigh-borhood features of size up to 2, 3, and 4, respectively (de-noted by Nb (2 ,  X  ) and so on). When k = 1, our features only consider the types of adjacent edges, which cannot exploit vertex label dependency. This case is theoretically trivial and empirically weak, and is thus not reported here. Clearly, as k grows, the classification performance of neighborhood features increases. Equipped with features of sizes up to 3 and 4 respectively, our methods consistently outperform the baseline (passed the p &lt; 0 . 01 paired t -test). In the case of k = 2, the improvements are not statistically significant for some label ratios. Although there are two cases (marked bold) where our performance is worse in average, the perfor-mance difference is neither large nor statistically significant.
Question 2: In Table 1, we estimated the time spent on mining features of different radius by adopting some assump-tions. When the r-FNM algorithm is applied, it is natural to ask, does it really save time as estimated, by not generat-ing large-radius features? In Figure 7, we report the feature mining time on different datasets and under different param-eter settings. Again we use the notation Nb ( k,  X  ) to denote features mined using the original neighborhood mining al-gorithm FNM. We also use Nb ( k, r max ) to denote features mined by the r-FNM algorithm described in Section 5. For example, Nb (4 , 2) stands for features generated by r-FNM using k = 4 and r max = 2. All time costs are averaged over the 10 training sets.

From Figure 7 it is obvious that, when k is fixed, the fea-ture mining time generally decreases as r max decreases. This indicates that when we need to use r max to filter the features by their radius, it is beneficial to call r-FNM with r max stead of filtering the features after calling FNM. There are some exceptions, for example, in Figure 7b at ratio 50%, the time of (4 , 3) exceeds that of (4 ,  X  ). We note that the zipper generating component is actually an extra, though not significant, computation overhead of the entire task. In the cases where r max approaches k (e.g., r max = k  X  1), r-FNM may spend some time dealing with many zippers, only to avoid generating a small number of large-radius patterns. However, this does not deteriorate the significance of r-FNM because no other obvious methods can ensure such efficiency when r max takes small or medium values.

Question 3: Since r-FNM enables the choice of not gen-erating large-radius features, how will the classification ef-fectiveness and efficiency change when large-radius features are blocked by r max ? To study this issue, we fix the label ra-tio at 50%, and jointly plot the classification time (including feature mining and generating test data) and F1 scores of different parameter settings in Figure 8. We do not include RL-RW-Deg in the figures because it is generally several times slower than our methods, but only provides a similar performance as Nb (2 ,  X  ) does.

In Figure 8, we observe that if we connect the data points of Nb (4 ,  X  ), Nb (4 , 3), Nb (4 , 2) and Nb (4 , 1) in order, we end up with a polyline roughly from upper right to lower left. Since the lower left direction represents the trend of worse performance and less time, it is clear that r-FNM provides a valid tradeoff of effectiveness and efficiency, by allowing specifying the maximum feature radius r max . There may be exceptions when a polyline goes lower right at some point. For example, in Figure 8b, Nb (4 , 3) has lower F1 score than Nb (4 ,  X  ), but costs more time. Recall that, when answer-ing question 2, we mentioned that for large r max , r-FNM may not be efficient. Besides these  X  X ad X  exceptions, we also observe  X  X ood X  exceptions. For example, in Figure 8a, Nb (4 , 2) is both more efficient and slightly more effective than Nb (4 ,  X  ).

Question 4: Since r max encodes the Markov-assumption-based tradeoff, how about the other parameter k ? Through reading Table 3 and Figure 7, it seems that k also pro-vides means for an efficiency-effectiveness tradeoff. Gen-erally speaking, k controls the complexity of the features generated by the neighborhood pattern miner, which is ir-relevant from the Markov assumption. When a tradeoff is required, which parameter we should try first actually de-pends on the data and task.

In Figure 8, we also mark out the polyline (4 ,  X  )  X  (3 ,  X  )  X  (2 ,  X  ) to illustrate the tradeoff by varying k . We denote it as the k -line, and the previous one as the r max -line. We ob-serve that on the Mutagenicity dataset, the r max -line is to the upper left of the k -line, indicating that on this dataset, the r max parameter provides a better tradeoff than k . This is because the valency information of atoms on the Muta-genicity dataset serves as a strong cue in classification, which falls under the scope of first-order Markov assumption. How-ever, the two lines exchanges their positions on the other two datasets. This is because the valency information is noisy on the second dataset (bonds to hydrogen atoms are omit-ted) and unavailable on the third one (vertices represent secondary structures, not atoms). The different results of applying the first-order Markov assumption indicate that, pre-knowledge about the data and task should be incorpo-rated into the stage of choosing algorithms and parameters. In this sense, r-FNM does not always ensure a better trade-off, but actually enables Markov-based assumptions to be adopted when necessary.
In this paper, we proposed using neighborhood patterns of vertices to build relational classifiers for within-network classification. We experimentally showed that neighborhood features are not only feasible, but also competitive, in structure-dependent WNC tasks. Moreover, we formulated the prob-lem of incorporating the Markov assumption into feature mining as the radius-constrained version of FNM. We algo-rithmically solved it by identifying zipper patterns as new building blocks of the pattern space. The new r-FNM algo-rithm was shown to help providing more effective choices on the efficiency-effectiveness trade-off in WNC tasks.
