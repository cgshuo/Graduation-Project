 Electroencephalography (EEG) and magnetoencephalography (MEG) provide an instantaneous and non-invasive measure of brain activity. Let q , p , and t denote the number of sensors, sources and time points, respectively. Sensor readings Y  X  R q  X  t and source currents S  X  R p  X  t are related by where X  X  R q  X  p is a lead field matrix that represents how sources project onto the sensors and E  X  R q  X  t represents sensor noise.
 Unfortunately, localizing distributed sources is an ill-posed inverse problem that only admits a unique solution when additional constraints are defined. In a Bayesian setting, these constraints take the form of a prior on the sources [3, 19]. Popular choices of prior source amplitude distri-butions are Gaussian or Laplace priors, whose MAP estimates correspond to minimum norm and minimum current estimates, respectively [18]. Minimum norm estimates produce spatially smooth solutions but are known to suffer from depth bias and smearing of nearby sources. In contrast, mini-mum current estimates lead to focal source estimates that may be scattered too much throughout the brain volume [9].
 In this paper, we take the Laplace prior as our point of departure for Bayesian source localization (instead of using just the MAP estimate). The obvious approach is to assume univariate Laplace priors on individual sources. Here, in contrast, we assume a multivariate Laplace distribution over all sources, which allows sources to be coupled. We show that such a distribution can be represented as a scale mixture [2] that differs substantially from the one presented in [5].
 Our representation allows the specification of both spatio-temporal as well as sparsity constraints. Since the posterior cannot be computed exactly, we formulate an efficient expectation propagation algorithm [12] which allows us to approximate the posterior of interest for very large models. Ef-ficiency arises from the block diagonal form of the approximate posterior covariance matrix due to properties of the scale mixture representation. The computational bottleneck then reduces to com-putation of the diagonal elements of a sparse matrix inverse, which can be solved through Cholesky decomposition of a sparse matrix and application of the Takahashi equation [17]. Furthermore, mo-ment matching is achieved by one-dimensional numerical integrations. Our approach is evaluated on MEG data that was recorded during an oddball task. In a Bayesian setting, the goal of source localization is to estimate the posterior where the likelihood term p ( Y | S ) = Q t N ( y t | Xs t ,  X  ) factorizes over time and  X  represents sensor noise. The prior p ( S |  X  ) , with  X  acting as a proxy for the hyper-parameters, can be used to incorporate (neuroscientific) constraints. For simplicity, we assume independent Gaussian noise with a fixed variance  X  2 , i.e.,  X  =  X  2 I . Without loss of generality, we will focus on one time-point ( y t , s t ) only and drop the subscript when clear from context. 1 The source localization problem can be formulated as a (Bayesian) linear regression problem where the source currents s play the role of the regression coefficients and rows of the lead field matrix X can be interpreted as covariates. In the following, we define a multivariate Laplace distribution, represented in terms of a scale mixture, as a convenient prior that incorporates both spatio-temporal and sparsity constraints.
 The univariate Laplace distribution can be represented as a scale mixture of Gaussians [2], the scaling function being an exponential distribution with parameter  X  2 / 2 . The scale parameter  X  controls the width of the distribution and thus the regularizing behavior towards zero. Since the univariate exponential distribution is a  X  2 2 distribution, one can alternatively write Gaussian given by definite matrix, and z is drawn from a univariate exponential distribution. The work presented in [11] is based on similar ideas but replaces the distribution on z with a multivariate log-normal distribution.
 In contrast, we use an alternative formulation of the multivariate Laplace distribution that couples the variances of the sources rather than the source currents themselves. This is achieved by gener-alizing the representation in Eq. (4) to the multivariate case. For an uncoupled multivariate Laplace distribution, this generalization reads such that each source current s i gets assigned scale variables u i and v i . We can interpret the scale variables corresponding to source i as indicators of its relevance: the larger (the posterior estimate of) u 2 i + v 2 i , the more relevant the corresponding source. In order to introduce correlations between sources, we define our multivariate Laplace (MVL) distribution as the following scale mixture: L ( s |  X , J )  X  Figure 1: Factor graph representation of Bayesian source localization with a multivariate Laplace prior. The factor f represents the likelihood term N y | Xs , X  2 I . Factors g i correspond to the coupling between sources and scales. Factors h 1 and h 2 represent the (identical) multivariate Gaus-sians on u and v with prior precision matrix J . The g i are the only non-Gaussian terms and need to be approximated. where J  X  1 is a normalized covariance matrix. This definition yields a coupling in the magnitudes of the source currents through their variances. The normalized covariance matrix J  X  1 specifies the correlation strengths, while  X  acts as a regularization parameter. Note that this approach is defining the multivariate Laplace with the help of a multivariate exponential distribution [10]. As will be shown in the next section, apart from having a semantics that differs from [5], our scale mixture rep-resentation has some desirable characteristics that allow for efficient approximate inference. Based on the above formulation, we define the sparse linear model as The factor graph in Fig. 1 depicts the interactions between the variables in our model. Our goal is to compute posterior marginals for sources s i as well as scale variables u i and v i in order to determine source relevance. These marginals are intractable and we need to resort to approximate inference methods. In this paper we use a deterministic approximate inference method called expec-tation propagation (EP) [12]. For a detailed analysis of the use of EP in case of the decoupled prior, which is a special case of our MVL prior, we refer to [16]. EP works by iterative minimizations of the Kullback X  X eibler ( KL ) divergence between appropriately chosen distributions in the following way.
 We introduce the vector of all latent variables z = ( s T , u T , v T ) T . The posterior distribution on z given the data y (which is considered fixed and given and therefore omitted in our notation) can be written in the factorized form exp( z T h 0  X  z T K 0 z / 2) . It factorizes into Gaussian functions of s , u , and v such that K 0 has a the  X  t i ( z ) are Gaussian functions as well.
 Our definition of the MVL distribution leads to several computational benefits. Equation (6) intro-duces 2 p auxiliary Gaussian variables ( u , v ) that are coupled to the s i  X  X  by p non-Gaussian factors, thus, we have to approximate p terms. The multivariate Laplace distribution defined in [5] introduces one auxiliary variable and couples all the s i s j terms to it, therefore, it would lead to p 2 non-Gaussian terms to be approximated. Moreover, as we will see below, the a priori independence of u and v and the form of the terms t i ( z ) results in an approximation of the posterior with the same block-diagonal structure as that of t 0 ( z ) .
 respect to q  X  and setting  X  t  X  i  X  q  X  /q \ i . It can be shown that when t i depends only on a subset of variables z i (in our case on z i = ( s i ,u i ,v i ) ) then so does  X  t i . The minimization of the KL diver-moment matching, i.e., q  X  ( s i ,u i ,v i ) is a Gaussian with the same mean and covariance matrix as q to compute q \ i ( z i ) and could then use a three-dimensional (numerical) integration to compute all first and second moments of q i ( z i ) . Below we will explain how we can exploit the specific charac-teristics of the MVL to do this more efficiently. For stability, we use a variant of EP, called power explanation of standard EP corresponds to  X  = 1 . In the following we will give the formulas for general  X  .
 We will now work out the EP update for the i -th term approximation in more detail to show by exactly the same role, it is also easy to see that the term approximation is always symmetric in u i terms for s i , u i , and v i , e.g., we can write follows directly from the factorization of t 0 ( z ) into independent terms for s , u , and v . That is, for the first EP step, the factorization can be guaranteed. To obtain the new term approximation, we Since q i ( u i ,v i ) only depends on u 2 i and v 2 i and is thus invariant under sign changes of u i and v i , we must have E [ u i ] = E [ v i ] = 0 , as well as E [ u i v i ] = 0 . Because of symmetry, we further have only, this variance can be computed from (11) using one-dimensional Gauss-Laguerre numerical quadrature [15]. The first and second moments of s i conditioned upon u i and v i follow directly from (10). Because both (10) and (11) are invariant under sign changes of u i and v i , we must have also E [ s i ] and E s 2 i can be computed with one-dimensional Gauss-Laguerre integration. Summa-rizing, we have shown that if the old term approximations factorize into independent terms for s i , u i , must do the same. Furthermore, given the cavity distribution q \ i ( s i ,u i ,v i ) , all required moments can be computed using one-dimensional numerical integration.
 The crucial observation here is that the terms t i ( s i ,u i ,v i ) introduce dependencies between s i and ( u i ,v i ) , as expressed in Eqs. (10) and (11), but do not lead to correlations that we have to keep track of in a Gaussian approximation. This is not specific to EP, but a consequence of the symmetries and invariances of the exact distribution p ( s , u , v ) . That is, also when the expectations are taken with E u 2 i = E v 2 i . The variance of the scales E u 2 i + v 2 i determines the amount of regularization on the source parameter s i such that large variance implies little regularization.
 Last but not least, contrary to conventional sequential updating, we choose to update the terms  X  t i in requires the computation of the marginal moments q ( s i ) , q ( u i ) and q ( v i ) . For this, we need the diagonal elements of the inverse of the precision matrix K of q ( z ) . This precision matrix has the block-diagonal form where J is a sparse precision matrix which determines the coupling, and K s , K u , and K v = K u are diagonal matrices that contain the contributions of the term approximations. We can exploit the low-rank representation of X T X / X  2 + K s to compute its inverse using the Woodbury formula [7]. The diagonal elements of the inverse of  X  2 J + K u can be computed efficiently via sparse Cholesky decomposition and the Takahashi equation [17]. By updating the term approximations in parallel, we only need to perform these operations once per parallel update. Returning to the source localization problem, we will show that the MVL prior can be used to induce constraints on the source estimates. To this end, we use a dataset obtained for a mismatch negativity experiment (MMN) [6]. The MMN is the negative component of the difference between responses to normal and deviant stimuli within an oddball paradigm that peaks around 150 ms after stimulus onset. In our experiment, the subject had to listen to normal (500 Hz) and deviant (550 Hz) tones, presented for 70 ms. Normal tones occurred 80% of the time, whereas deviants occurred 20% of the time. A total of 600 trials was acquired.
 Data was acquired with a CTF MEG System (VSM MedTech Ltd., Coquitlam, British Columbia, Canada), which provides whole-head coverage using 275 DC SQUID axial gradiometers. A re-alistically shaped volume conduction model was constructed based on the individual X  X  structural MRI [14]. The brain volume was discretized to a grid with a 0.75 cm resolution and the lead field matrix was calculated for each of the 3863 grid points according to the head position in the system and the forward model. The lead field matrix is defined for the three x , y , and z orientations in each of the source locations and was normalized to correct for depth bias. Consequently, the lead field matrix X is of size 275  X  11589 . The 275  X  1 observation vector y was rescaled to prevent issues with numerical precision.
 In the next section, we compare source estimates for the MMN difference wave that have been obtained when using either a decoupled or a coupled MVL prior. For ease of exposition, we focus on a spatial prior induced by the coupling of neighboring sources. In order to demonstrate the effect of the spatial prior, we assume a fixed regularization parameter  X  and fixed noise variance  X  2 , as estimated by means of the L curve criterion [8]. Differences in the source estimates will therefore arise only from the form of the 11589  X  11589 sparse precision matrix J . The first estimate is obtained by assuming that there is no coupling between elements of the lead field matrix, such that J = I . This gives a Bayesian formulation of the minimum current estimate [18]. The second estimate is obtained by assuming a coupling between neighboring sources i and j within the brain volume with fixed strength c . This coupling is specified through the unnormalized precision matrix  X  This prior dictates that the magnitude of the variances of the source currents are coupled between sources.
 For the coupling strength c , we use correlation as a guiding principle. Recall that the unnormal-ized precision matrix  X  J in the end determines the correlations (of the variances) between sources. Specifically, correlation between sources s i and s j is given by For example, using c = 10 , we would obtain a correlation coefficient of r i,i +1 = 0 . 78 . Note that this also leads to more distant sources having non-zero correlations. The positive correlation between Figure 2: Spatial coupling leads to the normalized precision matrix J with coupling of neighboring source orientations in the x , y , and z directions. The (reordered) matrix L is obtained from the Cholesky decomposition of J . The correlation matrix C shows the correlations between the source orientations. For the purpose of demonstration, we show matrices using a very coarse discretization of the brain volume. neighboring sources is motivated by the notion that we expect neighboring sources to be similarly though not equivalently involved for a given task. Evidently, the desired correlation coefficient also depends on the resolution of the discretized brain volume.
 Figure 2 demonstrates how a chosen coupling leads to a particular structure of J , where irregularities in J are caused by the structure of the imaged brain volume. The figure also shows the computational bottleneck of our algorithm, which is to compute diagonal elements of J  X  1 . This can be solved by means of the Takahashi equation which operates on the matrix L that results from a sparse Cholesky decomposition. The block diagonal structure of L arises from a reordering of rows and columns using, for instance, the amd algorithm [1]. The correlation matrix C shows the correlations between the sources induced by the structure of J . Zeros in the correlation matrix arise from the independence between source orientations x , y , and z . Figure 3 depicts the difference wave that was obtained by subtracting the trial average for standard tones from the trial average for deviant tones. A negative deflection after 100 ms is clearly visible. The event-related field indicates patterns of activity at central channels in both hemispheres. These Figure 3: Evolution of the difference wave at right central sensors and event-related field of the difference wave 125 ms after cue onset. Figure 4: Source estimates using a decoupled prior (top) or a coupled prior (bottom). Plots are centered on the left temporal source.
 Figure 5: Relative variance using a decoupled prior (top) or a coupled prior (bottom). Plots are centered on the right temporal source. findings are consistent with the mismatch negativity literature [6]. We now proceed to localizing the sources of the activation induced by mismatch negativity.
 Figure 4 depicts the localized sources when using either a decoupled MVL prior or a coupled MVL prior. The coupled spatial prior leads to stronger source currents that are spread over a larger brain volume. MVL source localization has correctly identified the source over left temporal cortex but does not capture the source over right temporal cortex that is also hypothesized to be present (cf. Fig. 3). Note however that the source estimates in Fig. 4 represent estimated mean power and thus do not capture the full posterior over the sources.
 Differences between the decoupled and the coupled prior become more salient when we look at the relative variance of the auxiliary variables as shown in Fig. 5. Relative variance is defined here as posterior variance minus prior variance of the auxiliary variables, normalized to be between zero and one. This measure indicates the change in magnitude of the variance of the auxiliary variables, and thus indirectly that of the sources via Eq. (6). Since only sources with non-zero contributions should have high variance, this measure can be used to indicate the relevance of a source. Figure 5 shows that temporal sources in both left and right hemispheres are relevant. The relevance of the temporal source in the right hemisphere becomes more pronounced when using the coupled prior. In this paper, we introduced a multivariate Laplace prior as the basis for Bayesian source localiza-tion. By formulating this prior as a scale mixture we were able to approximate posteriors of interest using expectation propagation in an efficient manner. Computation time is mainly influenced by the sparsity structure of the precision matrix J which is used to specify interactions between sources by coupling their variances. We have demonstrated the feasibility of our approach using a mismatch negativity dataset. It was shown that coupling of neighboring sources leads to source estimates that are somewhat more spatially smeared as compared with a decoupled prior. Furthermore, visualiza-tion of the relative variance of the auxiliary variables gave additional insight into the relevance of sources.
 Contrary to the MAP estimate (i.e., the minimum current estimate), our Bayesian estimate does not exactly lead to sparse posteriors given a finite amount of data. However, posterior marginals can still be used to exclude irrelevant sources since these will typically have a mean activation close to zero with small variance. In principle, we could force our posteriors to become more MAP-like by replacing the likelihood term with N y | Xs , X  2 I 1 /T in the limit T  X  0 . From the Bayesian point of view, one may argue whether taking this limit is fair. In any case, given the inherent uncertainty in our estimates we favor the representation in terms of (non-sparse) posterior marginals. Note that it is straightforward to impose other constraints since this only requires the specification of suitable interactions between sources through J . For instance, the spatial prior could be made more realistic by taking anatomical constraints into account or by the inclusion of coupling between sources over time. Other constraints that can be implemented with our approach are the coupling of individual orientations within a source, or even the coupling of source estimates between different subjects. Coupling of source orientations has been realized before in [9] through an ` 1 /` 2 norm, although not using a fully Bayesian approach. In future work, we aim to examine the effect of the proposed priors and optimize the regularization and coupling parameters via empirical Bayes [4]. Other directions for further research are inclusion of the noise variance in the optimization procedure and dealing with the depth bias that often arises in distributed source models in a more principled way.
 In [11], fields of Gaussian scale mixtures were used for modeling the statistics of wavelet coefficients of photographics images. Our approach differs in two important aspects. To obtain a generalization of the univariate Laplace distribution, we used a multivariate exponential distribution of the scales, to be compared with the multivariate log-normal distribution in [11]. The Laplace distribution has the advantage that it is the most sparsifying prior that, in combination with a linear model, still leads to a unimodal posterior [16]. Furthermore, we described an efficient method for approximating marginals of interest whereas in [11] an iterative coordinate-ascent method was used to compute the MAP solution. Since (the efficiency of) our method for approximate inference only depends on the sparsity of the multivariate scale distribution, and not on its precise form, it should be feasible to compute approximate marginals for the model presented in [11] as well.
 Concluding, we believe the scale mixture representation of the multivariate Laplace distribution to be a promising approach to Bayesian distributed source localization. It allows a wide range of approximated efficiently even for very large models.
 Acknowledgments The authors gratefully acknowledge the support of the Dutch technology foundation STW (project number 07050) and the BrainGain Smart Mix Programme of the Netherlands Ministry of Economic Affairs and the Netherlands Ministry of Education, Culture and Science. Tom Heskes is supported by Vici grant 639.023.604. [1] P. R. Amestoy, T. A. Davis, and I. S. Duff. Algorithm 837: Amd, an approximate minimum [2] D. F. Andrews and C. L. Mallows. Scale mixtures of normal distributions. Journal of the Royal [3] S. Baillet and L. Garnero. A Bayesian approach to introducing anatomo-functional priors in the [4] J. M. Bernardo and J. F. M. Smith. Bayesian Theory . Wiley, 1994. [5] T. Eltoft, T. Kim, and T. Lee. On the multivariate Laplace distribution. IEEE Signal Processing [6] M. I. Garrido, J. M. Kilner, K. E. Stephan, and K. J. Friston. The mismatch negativity: A [7] G. Golub and C. van Loan. Matrix Computations . John Hopkins University Press, Baltimore, [8] P. C. Hansen. Rank-Deficient and Discrete Ill-Posed Problems: Numerical Aspects of Linear [9] S. Haufe, V. V. Nikulin, A. Ziehe, K.-R. M  X  uller, and G. Nolte. Combining sparsity and rota-[10] N. T. Longford. Classes of multivariate exponential and multivariate geometric distributions [11] S. Lyu and E. P. Simoncelli. Statistical modeling of images with fields of Gaussian scale [12] T. Minka. Expectation propagation for approximate Bayesian inference. In J. Breese and [13] T. Minka. Power EP. Technical report, Microsoft Research, Cambridge, 2004. [14] G. Nolte. The magnetic lead field theorem in the quasi-static approximation and its use [15] W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery. Numerical Recipes in C . [16] M. W. Seeger. Bayesian inference and optimal design for the sparse linear model. Journal of [17] K. Takahashi, J. Fagan, and M. S. Chen. Formation of a sparse bus-impedance matrix and its [18] K. Uutela, M. H  X  am  X  al  X  ainen, and E. Somersalo. Visualization of magnetoencephalographic data [19] D. Wipf and S. Nagarajan. A unified Bayesian framework for MEG/EEG source imaging.
