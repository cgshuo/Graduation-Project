 Recommender Systems based on Collaborative Filtering sug-gest to users items they might like. However due to data sparsity of the input ratings matrix, the step of finding sim-ilar users often fails. We propose to replace this step with the use of a trust metric, an algorithm able to propagate trust over the trust network and to estimate a trust weight that can be used in place of the similarity weight. An empir-ical evaluation on Epinions.com dataset shows that Recom-mender Systems that make use of trust information are the most effective in term of accuracy while preserving a good coverage. This is especially evident on users who provided few ratings.
 H.3.3 [ Information Storage and Retrieval ]: Information filtering Algorithms, Experimentation
Recommender Systems (RS) [10] have the goal of suggest-ing to every user the items that might be of interest for her. In particular, RSs based on Collaborative Filtering (CF) [1 ] rely on the opinions expressed by the other users. In fact CF tries to automatically finds users similar to the active one and recommends to her the items liked by these similar users. This simple intuition is effective in generating rec-ommendations and widely used [10]. However RSs based on CF suffer some inherent weaknesses that are intrinsic in the process of finding similar users. In fact, the process of com-paring two users with the goal of computing their similarity involves comparing the ratings they provided to items. And in order to be comparable, it is needed that the two users rated at least some items in common. However in a typical domain, for example in the domain of movies or books, the number of items is very large (in the order of the millions) while the number of items rated by every single user is in general small (in the order of dozens or less). This means that it is very unlikely two random users have rated any items in common and hence they are not comparable. An-other important and underconsidered weakness is related to the fact that RS can be easily attacked by creating ad hoc user profiles with the goal of being considered as similar to the target user and influence the recommendations she gets.
In order to overcome these weaknesses, we propose to exploit trust information explicitly expressed by the user s. Users are allowed to state how much they consider every other user trustworthy that, in the context of RSs, is relate d to how much they consider the ratings provided by a certain user as valuable and relevant. This additional information (trust statements) can be organized in a trust network and a trust metric can be used to predict the trustworthiness of other users as well (for example, friends of friends). The idea here is to not search for similar users as CF does but to search for trustable users by exploiting trust propaga-tion over the trust network. The items appreciated by these users are then recommended to the active user. We call this technique, Trust-aware Recommender System.

While in other papers we explored portions of this process, the goal of this paper is to present a complete evaluation of Trust-aware Recommender System, by comparing different algorithms, ranging from traditional CF ones to algorithms that utilise only trust information with different trust met -rics, from algorithms that combine both trust and similarit y to baseline algorithms. The empirical evaluation is carrie d on a real world, large dataset. We have also evaluated the different algorithms against views over the dataset (for ex-ample only on users or items satisfying a certain condition) in order to hightlight the relative performances of the diffe r-ent algorithms.

The paper is structured as follows. Section 2 presents in more details the motivations for our proposal while Section 3 actually describes the proposal, focusing on the concept of trust, introducing the proposed architecture of Trust-awa re Recommender Systems and commenting on related works. Section 4 is devoted to the experiments in which we com-pared different algorithms and the experimental results are then summarized and discussed in Section 5. Section 6 con-cludes the paper.
Recommender Systems are a technique able to cope with the Information Overload problem. Information Overload refers to the fact that, for example, there are too many books, movies or songs to be able to experience all of them and make an informed decision about which ones we should read, watch or listen to. RSs suggest to every user few items she might like. Collaborative Filtering is a RS technique that exploits a simple intuition: items appreciated by peop le similar to someone will also be appreciated by that person. While Content-based RSs require a description of the con-tent of the items, Collaborative Filtering has the advantag e to rely just on the opinions provided by the users expressing how much they like a certain item in the form of a rating. Based on these ratings, the CF system is able to find users with a similar rating pattern and then to recommend the items appreciated by these similar users. In this sense, it does not matter what the items are (movies, songs, scien-tific papers, jokes,. . . ) since the technique considers only the ratings of the users and so the technique can be applied in every domain and does not require editors to describe the content of the items.

The typical input of CF is represented as a matrix of rat-ings, in which the users are the rows, the items the column and the values in the cells represent the rating expressed by a user about an item. The CF algorithm can be divided in two steps. The first step is the similarity assessment and consists into comparing the ratings provided by a pair of users (rows in the matrix) in order to compute their similarity. The most used and effective technique for the similarity assess-ment is to compute the Pearson correlation coefficient [1]. The second step is the actual rating prediction and consists into predicting the rating the active user would give to a cer -tain item. The predicted rating is the weighted sum of the ratings given by other user to that item, where the weights are the similarity coefficient of the active user with the othe r users. In this way the rating expressed by a very similar user has a larger influence on the rating predicted for the active user. The formula for the second step is the following where p a,i represents the predicted rating what active user a would possibly provide for item i , r u is the average of the rating provided by user u , w a,u is the user similarity weight of a and u as computed in first step and k is the number of users whose ratings to item i are considered in the weighted sum (called neighbours).

According to Equation 1, a user can be considered as neighbour for the active user only if it is possible to comput e the similarity weight of her and the active user. In this sens e the first step is very important in order to be able to gen-erate recommendations. Two users can be compared with a correlation coefficient only if they have rated in common at least few items. Since the items can be millions (think for example, of all the books ever written or of all the movies ever filmed), it is often the case that a user has provided a rating only about a tiny percentage of the items. This re-sults in a high sparsity of the ratings matrix and no overlap between the ratings provided by two random users. As a consequence it is often not possible to compute the similar-ity between two users. Data sparsity causes the first serious weakness of Collaborative Filtering.

This weakness is especially evident on cold start users , users who provided few ratings, for instance users who just started using the system. For these users, CF tends to fail in generating recommendations since it is not able to compare them with other users and to find possible neighbours. This is a relevant weakness since it is especially important to provide good recommendations to these users in order to give them incentives to keep using the system and provide more ratings. Moreover, as we will see in the following, they tend to be a significant portion of the users.

Even when it is possible to compute a similarity weight, because of data sparsity, this is often derived from few over -lapping ratings and it is hence a noisy and unreliable value. This results in unaccurate ratings predictions.

The last weakness we briefly comment about here is re-lated to attacks to Recommender Systems [8]. The simplest attack is the copy-profile attack: the attacker can copy the ratings of target user and fool the system into thinking that the attacker is in fact the most similar user to target user. In this way every additional item the attacker rates highly will probably be recommended to the target user.

These weaknesses are described in more detail in [5]. We describe in next section how our proposal alleviates these weaknesses.
In this section we summarize our proposal that has been already presented elsewhere [5]. We start by introducing basic concepts about trust networks and trust metrics. We then present the logical architecture of Trust-aware Recom -mender Systems. We conclude this section by comparing our proposal with related work in literature.
In decentralized environments where everyone is free to create content and there is no centralized quality control entity, evaluating the quality of this content becomes an im -portant issue. This situation can be observed in online com-munities (for example, slashdot.org in which millions of us ers posts news and comments daily), in peer-to-peer networks (where peers can enter corrupted items), or in marketplace sites (such as eBay.com, where users can create  X  X ake X  auc-tions) [4]. On these environments, it is often a good strateg y to delegate the quality assessment task to users themselves . The system can ask the users to rate other users: in this way, a user can express her level of trust in another user she has interacted with, i.e. issue a trust statement such as  X  X , Alice, trust Bob as 0.8 in [0,1] X . The system can then aggregate all the trust statements in a single trust network s representing the relationships between users.

Trust metrics are algorithms whose goal is to predict, based on the trust network, the trustworthiness of X  X nknown  X  users, i.e. users in which a certain user didn X  X  express a trust statement. Their aim is to reduce social complexity by suggesting how much an unknown user is trustworthy. There are many different trust metrics [2, 11, 6, 9]. An im-portant classification of trust metrics is in global and loca l ones [6]. Local trust metrics take into account the very per-sonal and subjective views of the users and predict different values of trust in other users for every single user. Instead global trust metrics predict a global  X  X eputation X  value th at approximates how the community as a whole considers a certain user. In this way, they don X  X  take into account the subjective opinions of each user but average them across standardized global values. PageRank [9], for example, is a global trust metric.

In the next section we will see how trust metrics can play a role in the context of Recommender Systems, essentially we propose them for replacing or integrating the users X  sim-ilarity assessment of step 1.
In this section we present the architecture of our pro-posed solution: Trust-aware Recommender Systems. Fig-ure 1 shows the different modules (black boxes) as well as input and output matrices of each of them (white boxes). There are two input information: the trust matrix (repre-senting all the community trust statements) and the ratings matrix (representing all the ratings given by users to items ). The output is a matrix of predicted ratings that users would assign to items. The difference with respect to traditional CF systems is the additional input matrix of trust state-ments. The two logical steps of CF remain the same. The first step finds neighbours and the second step predicts rat-ings based on a weighted sum of the ratings given by neigh-bours to items. The key difference is in how neighbours are identified and how their weights are computed. The weight w a,i in Equation 1 can be derived from the user similarity assessment (as in traditional CF) or with the use of a trust metric. In fact in our proposed architecture for the first step there are two possible modules able to produce these weights: Trust Metric module or Similarity Metric mod-ule. They produce respectively the Estimated Trust matrix and the User Similarity matrix: in both, row i contains the neighbours of user i and the cell of column j represents a weight in [0 , 1] about how much user j is relevant for user i (trustable or similar). This is the weight w a,i in Equa-tion 1 and represents how much ratings by user i should be taken into account when predicting ratings for user a (second step). A more detailed explanation of the architecture can be found in [5]. In Section 4 we are going to present experi-ments we have run with different instantiations of the differ-ent modules. For the Trust Metric module we have tested a local and a global trust metric. As local trust metric we have chosen MoleTrust [6], a depth-first graph walking algo-rithm with a tunable trust propagation horizon that allows us to control the distance to which trust is propagated. As global trust metric we have chosen PageRank [9], probably the most used global trust metric. For the Similarity Metric module we have chosen the Pearson Correlation Coefficient since it is the one that is reported to be performed best in [1]. About the Rating Predictor module (second step), we experimented with selecting only weights from the Esti-mated Trust matrix or the User Similarity matrix and with combining them. For comparison purposes, we have also run simple and baseline algorithms that we will describe in next section.

Now we briefly comment on how our proposal alleviates the before mentioned weaknesses. Due to the propagation of trust over the social network it is possible to compute the trust weight in more users than if user similarity was used and hence the problem of data sparsity is reduced. This is especially evident on cold start users. In fact, users with just one expressed trust statement can benefit from ratings provided by trusted users, and users trusted by them, and accordingly recommendations are computable even if they Figure 1: Trust-Aware Recommender System Ar-chitecture. have provided very few ratings. Just providing a trust state -ment is hence an effective way of bootstrapping RSs for new users. While neighbours chosen according to the similarity computed on few overlapping ratings might be not the best predictors, this is not the case with users explicitly indic ated as trusted by the active user. Attacks are addressed by a trust-aware technique given that the fake identities used f or the attacks are not trusted explicitly by the active users (a nd by the users she trusts). Thus the ratings they have intro-duced in order to game the system are not considered and do not affect the recommendations generated for the active user. A more detailed description of weaknesses and how Trust-aware RSs alleviates them is in [5].
There have been some proposals to use trust information in the context of Recommender Systems. We report here the most significant ones.
 In a paper titled  X  X rust in recommender systems X  [7], O X  X onovan and Smyth propose algorithms for computing Profile Level Trust and Item Level Trust. Profile Level Trust is the percentage of correct recommendations that this pro-ducer has contributed. Item level trust is a profile level tru st that depends on a specific item. As also reviewers note, this quantity represents more a  X  X ompetence X  measure and in fact reflects a sort of global similarity value. While in thei r work trust values are derived from ratings (of the Movie-lens dataset), in our proposal trust statements are explici tly expressed by users.

The PhD thesis of Ziegler [11] concentrates on RSs from different points of research. About the integration of trust , he proposes a solution very similar to ours, i.e neighbours formation by means of trust network analysis. He has de-signed a local trust metric, Appleseed [11], that computes the top-M nearest trust neighbours for every user. He has evaluated algorithms against a dataset derived from All-Consuming (http://allconsuming.net), a community of 3400 book readers, with 9300 ratings and 4300 trust statements. Only positive trust statements are available. Ziegler foun d that hybrid approaches (using taxonomies of books and hence based on content-based features of books) outperforms the trust-based one which outperforms purely content-based on e. Performances on users who provided fewer than five ratings were not studied.

Golbeck X  X  PhD thesis [2] focus on trust in web-based social networks, how it can be computed, and how it can be used in applications. She deployed an online Recommender System, FilmTrust (http://trust.mindswap.org/filmTrust/) in whi ch users can rate films and write reviews and they can also ex-press trust statements in other users based on how much they trust their friends about movies ratings. Trust state-ments in FilmTrust are weighted: users could express their trust in other user on a ten level basis. Golbeck designed a trust metric called TidalTrust [2] working in a breadth-firs t fashion similarly to MoleTrust but without a tunable trust propagation horizon. Even if on a dataset of just 300 mem-bers, it is interesting to note that her findings are similar t o ours that will be reported in the next section.
In this Section we present experiments we have conducted for evaluating the performances of Trust-aware Recommende r Systems. In particular we compare different instantiations of the modules of our proposed architecture (see Figure 1), so that the evaluated systems range from simple algorithms used as baselines to purely Collaborative Filtering ones, f rom systems using only trust metrics, both global and local, to systems that combine estimated trust and user similarity information. First we describe the dataset used and intro-duce the evaluation strategy we followed, then we present the actual experiments results.
The dataset we used in our experiments is derived from the Epinions.com Web site. Epinions is a consumers opin-ion site where users can review items (such as cars, books, movies, software, . . . ) and also assign them numeric ratings in the range 1 (min) to 5 (max). Users can also express their Web of Trust, i.e. reviewers whose reviews and ratings they have consistently found to be valuable and their Block list, i.e. a list of authors whose reviews they find consistently of -Web of Trust equals to issuing a trust statement of value 1 in her while inserting her in the Block List equals to issuing a trust statement of value 0 in her. Intermediate values such as 0 . 7 are not expressible on Epinions.

In order to collect the dataset, we wrote a crawler that recorded ratings and trust statements issued by a user and then moved to users trusted by that users and recursively did the same. Note however that the block list is kept private in Epinions in order to let users express more freely so it is not available in our dataset. Our dataset consists of 49 , 290 users who rated a total of 139 , 738 different items at least once. The total number of reviews is 664 , 824. The total number of issued trust statements is 487 , 181. Rating matrix sparsity is defined as the percentage of empty cells in the matrix users  X  items and in the case of the collected dataset is 99 . 99135%. The mean number of created reviews is 13.49 with a standard deviation of 34.16. It is interesting to have a look at what we have called  X  X old start users X . They are the large majority of users. For example, 26,037 users expresse d less than 5 reviews and represent 52.82% of the population. The mean number of users in the Web of Trust (friends) is 9 . 88 with a standard deviation of 32.85. Another interesting point is the distribution of ratings. In our dataset, 45% of the ratings are 5 (best), 29% are 4, 11% are 3, 8% are 2 and 7% are 1 (worst). The mean rating is hence 3 . 99. Note that almost half of the ratings are a 5, i.e. the maximum possible value. (http://www.epinions.com/help/faq/?show=faq wot)
The characteristics we briefly described are very different used dataset for RSs evaluation. In particular, in Movielen s dataset all the users are guaranteed to have voted at least 20 items while in Epinions more than half of them have voted less than 5 items (cold start users). This also means the sparsity is much higher in Epinions and so finding overlap-ping on provided ratings between users and hence possible neighbours (step 1 of CF) is even harder. While on Epinions most of the rating values are 5 and 4, in Movielens all the dif-ferent values are more balanced. This affects how different algorithms perform as we will see in the following sections. The most used technique for evaluating Recommender Systems is based on leave-one-out . Leave-one-out is an of-fline technique that can be run on a previously acquired dataset and involves hiding one rating and then trying to predict it with a certain algorithm. The predicted rating is then compared with the real rating and the difference in absolute value is the prediction error. The procedure is re-peated for all the ratings and an average of all the errors is computed, the Mean Absolute Error (MAE) [3].

A first problem with MAE is that it weighs every error in the prediction of a rating in the same way. For example, let us suppose that our dataset contains only 101 users: one user provided 300 ratings while all the remaining 100 users provided just 3 ratings each. We call the first user a  X  X eavy rater X  and the other users  X  X old start users X . In this way our dataset contains 600 ratings. The leave-one-out methodol-ogy consists in hiding these 600 ratings one by one and then trying to predict them. Typically CF works well for users who already provided a lot of ratings and poorly on users who provided few ratings. A probable situation is that the error over the predictions of the heavy rater is small while the error over the predictions of the cold start users is high . However in computing the Mean Absolute Error, the heavy raters weigh just as much as all the other users since she pro-vided a very large number of ratings. This does not reflect the real situation in which actually there is one user that is probably satisfied with the prediction error (the heavy rate r) and 300 users who are not satisfied (the cold start users). For this reason, the first additional measure we introduce is Mean Absolute User Error (MAUE). The idea is straightfor-ward: we first compute the Mean Absolute Error for every single user independently and then we average all the Mean Absolute Errors related to every single user. In this way, every user has the same weight in the Mean Absolute User Error computation. This is really important since Epinions dataset contains a large share of cold start users.
Another important measure that is often not reported and studied in evaluation of RSs is coverage. Herlocker et al. in their solid review of Recommender Systems evaluation techniques [3] underline how it is important to go  X  X eyond accuracy X  in evaluating RSs and count coverage as one step in this direction but also note how few works have investi-gated it. Coverage simply refers to the fraction of ratings for which, after being hidden, the RS algorythm is able to produce a predicted rating. It might in fact be the case that some RS techniques are not able to predict the rating 2 Distributed by Grouplens group at the University of Minnesota and available at http://www.cs.umn.edu/Research/GroupLens/ a user would give to an item. Again we believe that cov-erage was understudied by many research efforts because in Movielens, the most used dataset for evaluation of RSs, the coverage over ratings tends to be close to 100%. This is due to the fact that all the users are guaranteed to have voted at least 20 items and that there are some items that are rated by almost every user. Instead on a very sparse dataset that contains a large portion of cold start users and of items rate d just by one user, coverage becomes an important issue since many of the ratings become hardly predictable. While the percentage of predictable ratings ( ratings coverage ) is an im-portant measure, it suffers the same problem we highlighted earlier for Mean Absolute Error, it weighs heavy raters more . Following the same argument as before, we introduce also the users coverage , defined as the portion of users for which the RS is able to predict at least one rating.

A possibility given by a very large dataset of ratings is to study performances of different RS techniques on different portions of the input data (called  X  X iews X ). It is possible f or example to compute MAE or Users coverage only on ratings given by users or items which satisfy a certain condition. The views we are going to report results about in this paper are the following: cold start users, who provided from 1 to 4 ratings; heavy raters, who provided more than 10 ratings; opinionated users, who provided more than 4 ratings and whose standard deviation is greater than 1 . 5; black sheep, users who provided more than 4 ratings and for which the average distance of their rating on item i with respect to mean rating of item i is greater than 1; niche items, which received less than 5 ratings; controversial items, which re -ceived ratings whose standard deviation is greater than 1 . 5. We introduced these views because they are better able to capture the relative merits of the different algorithms in di f-ferent situations and to better represent their weaknesses and strengths.
Every different instantiation of the Trust-aware Recom-mender System architecture is evaluated with regard to the measures we have defined (MAE, MAUE, ratings coverage, users coverage), also focusing the analysis on the different views previously introduced, such as, for example, cold sta rt users and controversial items. In the following we discuss t he experiments X  results which are condensed in Table 1 and 2. Figure 2 and 3 presents graphically just one of the measures reported in the tables, precisely the row labeled  X  X old user s X  (i.e. MAE and ratings coverage on predictions for cold start users and MAUE and users coverage) in order to give the reader a visual grasp of the relative benefits of the different techniques.
As a first step in our analysis we tried a very simple al-gorithm that returns always 5 as the predicted rating a user would give to an item. We call this algorithm Always5 . This trivial algorithm is not meaningful from a RS point of view since, for instance, it does not allow to differentiate and pr i-oritize the different items. However it allowed us to start ex -ploring which MAE a simple algorithm would achieve. The MAE over all the ratings is 1 . 008. This result is not too bad, especially if we compare it with more complex algo-rithms as we will do in the following. Another trivial algo-rithm that predicts as rating the mean of the ratings pro-vided by one user is very effective as well achieving a MAE of 0 . 9243. The reason for such good performances is that in our dataset most of the rating values are in fact 5 and this is a notable difference with respect to other datasets, for instance MovieLens, on which these trivial algorithms work very badly. But in our case we have two very sim-ple and not personalized algorithms that seem to performs enough well. This fact suggested to us that just presenting the Mean Absolute Error over all the ratings is not a use-ful way to compare different algorithms. We introduced the evaluations views explained in Section 4.2 in order to have an evaluation technique better able to capture the relative merits of the different algorithms in different situations an d to better represent their weaknesses and strengths. In fact on the controversial items view for instance, these trivial algorithms perform very badly.
Another trivial algorithm is the one that predicts as a rating for a certain item the unweighted average of all the ratings given to that item by all the users but the active user . It is a non-personalized technique that is like assigning 1 a s similarity or trust weight to all the users in the second step of CF (Equation 1 with w a,i always equal to 1). For this reason we call it TrustAll . To our surprise, TrustAll outper-formed standard Collaborative Filtering algorithms, achi ev-ing a MAE of 0 . 821 (against 0 . 843 of standard CF ). On the other hand, on MovieLens dataset, we observe the expected result: MAE of CF is 0 . 730 while MAE of TrustAll is 0 . 815. Moreover, the number of predictable Epinions ratings (the coverage) is 51 . 28% for CF and 88 . 20% for TrustAll, while on Movielens ratings they are both close to 100%. The reason for these important differences is in the datasets. The Epin-ions dataset contains mostly 5 as rating value and most of the users provided few ratings (cold start users). We believ e these facts, not observed in other RS datasets, allowed us to study certain characteristics of RS algorithms that were previously unexplored. The problem with CF in our dataset is that the Pearson correlation coefficient (similarity weig ht output of the first step of CF) is often not computable be-cause of data sparsity and hence only the ratings of a small percentage of the other users can be utilized when gener-ating a recommendation for the active user. Since there is not too much variance in rating values (most of them are 5), an unweighted average is usually close to the real value. On cold start users, the balance is even more for TrustAll. The coverage of CF on cold start users is only 3 . 22% while the coverage of TrustAll is 92 . 92% and the MAE of CF is 1 . 094 while the MAE of TrustAll in 0 . 856. Note that in the real-world Epinions dataset, cold start users make up more than 50% of total users. In fact for a cold start user the first step of CF fails almost always since it is very unlikely to find other users which have rated the same few items and hence the similarity weight is not computable. How-ever these results are not totally dismissive of CF, in fact, on controversial items, CF outperforms TrustAll (MAE of 1 . 515 against 1 . 741). In this case, CF is able to just consider the opinions of like minded users and hence to overcome the performances of TrustAll, a technique that, not being per-sonalized, achieves greater error. This means that when it is really important to find the like-minded neighbours CF is needed and effective. Also note that the error over rat-Views Algorithms
All 0.843
Cold users
Heavy raters
Contr. items
Niche items
Opin. users
Black sheep Table 1: Accuracy and coverage measures on rat-ings, for different RS algorithms on different views. ings received by controversial items is greater than the err or over all the ratings, meaning that it is harder to predict the correct ratings for these items.
In this subsection we start comparing performances of RS algorithms that use only trust information (top box in Fig-ure 1) with standard CF (bottom box). We start by using only the users explictly trusted by the active user, i.e. not propatagating trust or setting the propagation horizon at 1 for the local Trust Metric MoleTrust. We call this algorithm MT1 . In general, RSs based on trust propagation work bet-ter with cold start users. They don X  X  use the (few) ratings information for deriving a similarity measure to be used as weight for that user, but use the trust information explic-itly provided by the user. In this way, even for a user with just one friend, it is possible that her friend has rated the items she rated and hence a prediction is possible. It is also possibly the case that that friend has tastes very similar to the current user and hence the error is small. In fact, the MAE of MT1 over cold start users is 0.674 while the MAE of CF is, as already discussed, 1.094. The difference in error is very high and particulary relevant since it is important f or a RS to be able to provide personalized recommendations as soon as possible to users who have not yet provided many ratings so that these users appreciate the system and keep using it, providing more ratings. Moreover cold start users are a very large portion of the users in our dataset.
Let us now compare performances of CF and MT1 over all the ratings. The MAUE achieved by MT1 and CF is respectively 0 . 790 and 0 . 938. About prediction coverage, while CF is able to predict more ratings than MT1 (ratings coverage is 51 . 28% vs. 28 . 33%), MT1 is able to generate at least a prediction for more users (users coverage is 46 . 64% vs. 40 . 78%). Summarizing, MT1 is able to predict fewer ratings than CF but the predictions are spread more equally over all the users (which can then be at least partially satisfied) and , about errors, CF performs much worse than MT1 when we consider the error achieved over every single user in the sam e way and not depending on the ratings she provided. These Views Algorithms
All 0.938
Cold users
Heavy raters
Contr. items
Niche items
Opin. users
Black sheep Table 2: Accuracy and coverage measures on users, for different RS algorithms on different views. facts have the following reason: CF works well, both in terms of coverage and in terms of error, for heavy raters (users who already provided a lot of ratings) while it performs very poorly on cold start users. On many important views such as controversial items and opinionated users MT1 outperforms both CF and TrustAll.
In the previous section we analyzed performances of RS algorithms that consider only trust information but don X  X  propagate trust. Here we analyze how trust propagation us-ing the local Trust Metric Moletrust performs. We name MT2 , MT3 and MT4 the algorithms which propagate trust up to distance 2, 3 and 4 respectively. Of course propagating trust allows to reach more users and hence predict a trust score for more of them (see graphs and data in [5]). It is in-teresting to briefly report the number of neighbours that can be identified by the different first steps in the Trust-aware Recommender System architecture. The average number of directly trusted users (MT1) is 9 . 88, while the average number of comparable users (users for which the Pearson Correlation coefficient is computable) is 160 . 73. However just propagating trust a few steps helps to increase signifi-cantly the number of neighbours that can be considered for the rating predictions. Propagating at distance 2 (friends of friends) is possible to reach 399 . 89 users and increasing the trust propagation horizon to 3 and 4 allows to reach respec-tively 4 , 386 . 32 and 16 , 333 . 94 users [5]. This pattern is even more evident on cold start users [5].

Since by propagating trust it is possible to reach more users and hence to compute a predicted trust score in them and to count them as neighbours, the prediction coverage of the RS algorithm increases. In fact the larger the trust propagation horizon, the greater the coverage (see columns MT1, MT2 and MT3 of Table 1 and 2). For instance, on all ratings, the ratings coverage increases from 28.33% for MT1, to 60.47% for MT2, to 74.37% for MT3. By continu-ing to propagate trust (i.e. expanding the trust propagatio n horizon) it is possible to consider more and more users as possible neighbours and hence to arrive at 88.20%, the rat-Figure 2: MAE on cold start users for some repre-sentative algorithms. ings coverage of TrustAll which considers every user who provided a rating. The downside of this is that the error in-creases as well. For example, on cold start users, the MAUE is 0 . 674 for MT1, 0 . 820 for MT2 and 0 . 854 for MT3. These results say that by propagating trust it is possible to incre ase the coverage (generate more recommendations) but that it also considers users who are worse predictors for the curren t user so that the prediction error increases as well. The trus t propagation horizon basically represents a tradeoff betwee n accuracy and coverage.
An additional experiment we performed is about testing the performance of global Trust Metrics as algorithms for predicting the trust score of unknown users. A global trust metric predicts for every user the same trust scores in other users. This technique, like TrustAll, is hence not personal -ized. We have chosen to run PageRank [9] as global trust metric and to normalize the output value in [0,1]. We call the Recommender System that uses PageRank for its Trust Metric module, PR . PR performs similarly to TrustAll, even slightly worse (MAE of 0 . 847 and 0 . 821 respectively). This means that a global Trust Metric is not suited for a Rec-ommender System whose task is to leverage individual dif-ferent opinions and not to merge all of them into a global average. We also tried to restrict the neighbours to just the first 100 users as ranked by PageRank but this algo-rithm (called PR100 ) while of course reducing the coverage, reports even larger errors (MAE of 0 . 973). The reason be-hind these bad performances is that globally trusted users (as found by PageRank) tend to be peculiar in their rating patterns and provide more varied ratings so that averaging them generates larger errors. Summarizing we can say that global trust metrics are not suited in the task to find good neighbours, especially because the task of RSs is to provide personalized recommendations while global trust metrics a re unpersonalized.
In the architecture of Trust-aware Recommender Systems Figure 3: Ratings coverage on cold start users for some representative algorithms. (Figure 1), the X  X ating predictor X  X odule takes as input bot h the Estimated Trust matrix and the User Similarity matrix. The idea is that the weight of a neighbour used in Equation 1 can be derived both from the user similarity value computed by the Similarity Metric (Pearson Correlation Coefficient in our case) and the predicted trust value computed by a Trust Metric. We have already commented on the number of users for which it is possible to compute a similarity weight or a predicted trust in previous subsection [5]. However in orde r to devise a way of combining these two matrices, it is in-teresting to analyze how much they overlap. As previously reported, the number of users reachable in one step (the ones used by MT1) are on average 9 . 88 and the number of users in which a user similarity coefficient is computable are on average 160 . 73. The two matrix rows overlap only on 1 . 91 users on average, that is only for 1 . 91 users we have both a predicted trust and a user similarity. The number of users reachable propadating trust up to distance 2 is 399 . 89. Comparing it again with the number of users in which a simi-larity coefficient is computable (160 . 73), the average number of users present in both lists is 28 . 84. These numbers show how Pearson Correlation coefficient and MoleTrust address different portions of the user base in which they are able to compute a weight. So, in order to combine these weights, we tested the simple technique of computing a weighted average when there are two weights available and, in case only one is available, of using that. We call this technique CF+MTx : for example the systems that combine CF and MT1 is called CF+MT1. The results are not very good. When comparing CF+MT1 with CF and MT1 for example, we see that the coverage is greater than the coverage of the two techniques. This is of course the expected situation since CF+MT1 con-siders both the users in which it is possible to predict a trus t score (as MT1 does) and the users in which it is possible to compute a user similarity (as CF does). However the error of CF+MTx is in general in between of CF and MTx, that is worse than MTx and better than CF. The problem is that, as we reported earlier, CF is almost always not able to find good neighbours and hence making an average of the users who are similar and of the users that are trusted produces worse results that just considering trusted users. Since te ch-niques that used only trust were superior in previous tests to CF-based ones, we also try to just use the predicted trust score when both the weights were available but the results are very similar.
In this section we summarize and discuss the most impor-tant results of the presented experiments. The first impor-tant result it that considering only the ratings of directly trusted users is the technique that, in general, achieves th e smallest error with an acceptable coverage. The compara-tive improvement over the other techniques is particularly evident with regard to controversial items and black sheep, two of the most important and challenging views. With re-gard to cold start users, standard CF techniques totally fai l and are not able to generate any recommendation. Instead by considering ratings of trusted users we achieve a very small error and are able to produce a recommendation for almost 17% of the users. We can therefore state that provid-ing a single trust statement is an easy, effective and reliabl e way of bootstrapping the Recommender System for a new user. It is important to underline that the evidence is based on experiments carried on a real world, large dataset. In particular the Epinions datasets allowed us to explore top-ics which were not addressed before by research papers, such as cold start users and other views. Using our local Trust Metric MoleTrust in order to propagate trust allows users trusted by trusted users (at distance 2 from active user in the directed trust network), or even further away users, to be considered as possible neighbours. In this way, the cov-erage increases significantly, but the error increases as we ll. This means that ratings of users at distance 2 (or more) are less precise and less useful than ratings of users at distanc e 1, i.e. directly trusted by the active user. However it is an open issue to see if different local trust metrics are able to e x-tract just some of the other users such that their ratings are really effective in improving the recommendation accuracy. In fact, this method can be used to evaluate the quality of different trust metrics, i.e. a better trust metric is the one that is able to find the best neighbours and hence to reduce the prediction error. As last point we would like to highligh t how Collaborative Filtering, the state of the art technique , performed badly in our experiments, especially on cold star t users (which in our dataset are in fact more than 50%). The reason of this has to be found in the characteristics of the datasets used for evaluation. In previous research evalua-tions the most used dataset was MovieLens, while we used a dataset derived from the online community of Epinion.com. As we have already commented they present very different characteristics. It is still an open point to understand how much the different datasets influence the evaluation of dif-ferent algorithms X  performances. In this paper we have presented our proposal for enhancing Recommender Systems by use of trust information. We have presented a deep empirical evaluation on a real world, large dataset of the performances of different algorithms ranging from standard CF to algorithms powered with local or global trust metrics, from combination of these to baseline algo-rithms. We have also segmented the evaluation only on cer-tain views (cold start users, controversial items, etc.) ov er the dataset in order to better highlight the relative merits of the different algorithms. The empirical results indicate th at trust is very effective in alleviating RSs weaknesses. In par -ticular the algorithm powered with MoleTrust local trust metric is always more effective than CF algorithms which surprisingly performs even worse than simple averages when evaluated on all the ratings. This difference is especially large when considering cold start users, for which CF is to-tally uneffective. The trust propagation horizon represent s a tradeoff between accuracy and coverage, i.e. by increasing the distance to which trust is propagated by the local trust metric the prediction coverage increases but the error in-creases as well. Results also indicates that global trust me t-rics are not appropriate in the context of RSs. Given that the user similarity assessment of standard CF is not effectiv e in finding good neighbours, the algorithms that combines both user similarity weight and predicted trust weights are not able to perform better than algorithms that just utilize trust information. [1] J. Breese, D. Heckerman, and C. Kadie. Empirical [2] J. Golbeck. Computing and Applying Trust in [3] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and [4] P. Massa. A survey of trust use and modeling in [5] P. Massa and P. Avesani. Trust-aware collaborative [6] P. Massa and P. Avesani. Trust metrics on [7] J. O X  X onovan and B. Smyth. Trust in recommender [8] M. P. O X  X ahony, N. J. Hurley, and G. C. M. Silvestre. [9] L. Page, S. Brin, R. Motwani, and T. Winograd. The [10] P. Resnick and H. Varian. Recommender systems. [11] C.-N. Ziegler. Towards Decentralized Recommender
