 Opinion mining mainly deals with the summari-sation of opinions on the basis of their sentiment polarities (Liu, 2012). However, on a closer obser-vation of such opinionated text, we can discover other facets of opinions. For example, a hotel re-view sentence like, Make sure you bring plenty of sun tan lotion-very expensive in the gift shop , would be labeled as a neutral sentiment in cur-rent opinion mining methodologies, since it would only be interested in collecting opinions about the hotel. In the case of aspect based sentiment anal-ysis, the sentence does not comprise of hotel re-lated aspects, and thus will again be labeled as neutral/objective. While such sentences are gen-erally ignored in sentiment based opinion sum-marisation, these can be very useful information to extract from the reviews. In hotel reviews, such suggestions range from tips and advice on the re-viewed entity, to suggestions and recommenda-tions about the neighbourhood, transportation, and things to do. Similarly, in product reviews sugges-tions can be about how to make a better use the product, accessories which go with them, or avail-ability of better deals. We refer to such sentences as customer-to-customer suggestions (CTC).
 Another type of suggestions, which can appear in the reviews, are the ones aiming at manufactur-ers or service providers, suggesting new features and improvements in products or services. For ex-ample, An electric kettle in the room would have been a useful addition . Recently, there have been some works on extracting the suggestions for im-provements from reviews (section 3), but they did not focus on CTC suggestions. Also, suggestions for improvement discuss only about the reviewed entity and its aspects, unlike suggestions to cus-tomers, which might also include other topics of interest.
 Suggestion mining and retrieval can be a poten-tial new research area emerging from this kind of research. Industrial importance of suggestions to customers can be validated from the sections like to a business, and defines it as  X  X ey information X . Such tips are often suggestions, or some impor-tant information, a user wants to convey to others. These tips are manually entered by the users, in addition to the reviews. We note that using sug-gestion mining, such information can be automat-ically extracted from a large number of already ex-isting reviews. The recommendation type of sug-gestions are of great importance in the case, when there is no dedicated reviews available for small shops, cafe, restaurants etc. in the vicinity of a hotel/business. Suggestions extracted from a large number of reviews, can also be seen as a kind of summarisation, an alternative or complementary to sentiment summarisation over the reviews. In order to perform suggestion mining, expres-sions of suggestions should to be detected in a given text. The presence of a variety of linguistic strategies in the suggestion expressing sentences makes this task interesting from a computational linguistics perspective. Section 2 discusses this in more detail. The detection of suggestions in text goes beyond the scope of sentiment polarity detec-tion, and opens up new problems and challenges in the areas of subjectivity analysis, social media analysis, and extra-propositional semantics.
Our main contributions through this work can be listed as:  X  Proposition of the task of detection of CTC  X  A well formed problem definition and scope.  X  Preparation of benchmark dataset for two do- X  An approach to automatically detect CTC Since suggestion mining is a young problem, there is a need for problem analysis and definition. As indicated in the previous section, the tasks under suggestion mining may vary. Below, we propose three parameters whose value would help define such tasks, and the values for these parameters in the context of the current task of detection of CTC suggestions.  X  Who is the suggestion aimed at?
As we explained in section 1, suggestions can be aimed at one of the two kinds of stakeholders, customers and service providers. In this work, we perform the detection of suggestions for cus-tomers only.  X  What should be the textual unit of sugges-tion?
The previous works on extraction of suggestions for product improvement considered sentences as the unit of suggestions. In this work, we also consider sentences as units of suggestion.
However, we observe that sentences might miss the context, or refer to something mentioned in the previous sentence. Furthermore, punctua-tion marks are often erroneously used in social media text, so automatic sentence split does not work well with such text. The approach used in this work aims at the detection of expressions of suggestions, regardless of the presence or ab-sence of context in the same sentence. There-fore, we currently ignore the problems asso-ciated with using sentences as a unit for sug-gestions. The datasets used in this work are sentiment analysis review datasets from other works, in which reviews are already split into sentences. For future works, we assume that context can be determined by the neighbouring text once the expression of suggestion is suc-cessfully detected.  X  What kind of text should be considered as a suggestion?
Oxford dictionary defines suggestion as, An idea or plan put forward for consideration .
Some of the listed synonyms of suggestions are proposal, proposition, recommendation, advice, hint, tip, clue . In a general scenario, this defini-tion of suggestion easily distinguishes sugges-tions from other kind of text. However, when suggestions are required to be identified in the space of customer reviews, there seems to be a tendency to consider most of the sentences as suggestions to other customers. This observa-tion is based on a preliminary data annotation task, which was meant for data analysis and de-velopment of annotation guidelines. Asher et. al (2009) define 20 types of opinion expres-sions, including suggestions and recommenda-tions, which appear in opinionated text. In order to form a representative sample, we chose 20 sentences from reviews, corresponding to each of these types, and asked 10 people (layman) to decide if these sentences are CTC suggestions or not. Table 1 shows sentences for which 5 or more people agreed of CTC Suggestion label.
Asher et. al. (2009) observe that these types are not uniformly distributed in the reviews. Ac-cording to them, suggestions and recommenda-tions constitute about 10 % of the statements, while judgements (blame, praise) and senti-ments (love, fascinate, hate, disappoint, sad) constitute about 80 % . Later, when we annotate the review datasets for CTC sugggestions, they also shows a smaller percentage of suggestions (see table 2).

Except the suggest and recommend categories, suggestions in rest of the categories are in an implicit form and need to be inferred. Since human beings can inherently infer suggestions, the layman annotators considered both implicit and explicit form of suggestions as suggestions.
However, in a real case scenario, humans can-not go through the large amount of reviews and infer suggestions from all of them. In this work, we aim to automatically detect and extract the explicit expressions of suggestions, rather than inferring them.

For the ease of defining the scope of our work, we propose two form of suggestions:
Explicit: Directly suggests/recommends an en-tity or action,
Implicit: Only provides the information from which a suggestion can be inferred, but do not authoritatively suggest anything.
 Lastly, we frame the problem of CTC sugges-tion detection problem as a sentence classification problem: dict a label l i for each of statement in S, where l i  X  X  CTC suggestion, non CTC suggestion } , where CTC suggestion should be explicitly expressed. Only a few attempts have been made to study sug-gestion mining, and there is an unavailability of benchmark datasets. Therefore, suggestion min-ing still remains a young area of study.  X  Suggestion Mining from Customer Reviews
As mentioned in section 1, there have been some attempts to extract suggestions for im-provements in products from customer reviews.
Ramanand et. al. (2010) used manually for-mulated patterns to extract wishes regarding improvements in products. Brun and Hagege (2013) also used manually formulated rules to extract suggestions for improvements from the product reviews. Negi and Buitelaar (2015) studied linguistic nature of suggestions and wishes for improvements and performed exper-iments in order to assert that these contain sub-junctive mood. These works do not acknowl-edge the fact that reviews can also contain sug-gestions for other customers. One major draw-back of previous works on customer reviews is the public unavailability of evaluation datasets.  X  Other Domains
Two other lines of work extracted suggestions from domains other than reviews. Dong et. al. (2013) performed detection of suggestions for product improvement using tweets. They used a statistical classifier, with features comprising of bag of words, automatically extracted sugges-tion templates using sequential pattern mining and hashtags. (Wicaksono and Myaeng, 2012;
Wicaksono and Myaeng, 2013) extracted advice from discussion threads. They also used a su-pervised classification approach, where some of the features were domain dependent, like the similarity between original query post and a given sentence. They do not make any distinc-tion between implicit and explicit advice, since there is less ambiguity in the domain of discus-sion forum.
 None of the previous works study the complex and interesting nature of suggestions in opinion-ated text, and the relationship between suggestions and sentiments. Also, there is an unavailability of benchmark datasets of suggestions customer re-views. Since there is no available dataset of suggestions for customers, we prepare new datasets for this task. We consider two kinds of reviews for this task, hotel reviews and electronic product reviews. Hotel : Wachsmuth et al. (2014) provide a large dataset of TripAdvisor hotel reviews, where re-views are segmented into statement so that each statement has only one sentiment label. State-ments have been manually tagged with positive, negative, conflict and neutral sentiments. We take a smaller subset of these reviews, where each statement is an instance of our dataset. Each state-ment also bears a unique identity no., which is constituted of hotel identity no., statement num-ber and review identity no. Therefore, the reviews belonging to same hotel, and the statements of the same review can be identified.
 Electronics : Hu et. al.(2004) provide a dataset consisting of reviews of electronic products, which is also already split into sentences, and the corresponding sentiment for each sentence is man-ually tagged.
 In the next section, we further annotate the sen-tences from these two datasets, for the current task.
 Table 2: Statistics of Phase 1 and Phase 2 Annota-tions 4.1 Dataset Preparation We performed a two phase annotation using both crowdsourced and expert annotations. This re-duced the number of statements to be annotated by experts.
 Phase 1 -Crowdsourced Annotations: The sourced annotations. The platform provides a set of management and analytics tools for qual-ity management, as well as for interaction with the annotators. In order to qualify for the anno-tation task, a Crowdflower worker was required to obtain a score of 7 / 10 out of 10 test statements. The annotators were asked to choose one label out of  X  X uggestion to Customers X  or  X  X ther Statement X  for each sentence. The definition of suggestion was left entirely to the understanding of the an-notators. For each sentence, Crowdflower selects set the system not to seek more than 3 annotations for a statement if one of the labels attained a confi-dence score of 0.6 or more. At least 3, and at most 5 annotators labeled each statement. Confidence score for each label is the weighted sum of the Trust score is determined by annotator X  X  score in the test questions. Table 2 shows the variation in the number of statements tagged as suggestions with the corresponding confidence scores. These suggestions are a mixture of both implicit and ex-plicit types, since the definition of suggestions was not restricted for the annotators. We observed that with the increase of confidence score, the ratio of explicit suggestions among the tagged suggestions increases. Phase 2 -Expert Annotations: Since we target the extraction of explicitly expressed suggestions, two expert annotators further classified the sen-tences which were finally labelled as  X  X uggestion to Customers X  by the Crowdflower platform, as ex-plicit CTC suggestions and implicit CTC sugges-tions. Therefore, the number of sentences which experts annotated was much smaller than the total number of sentences in the two datasets. The key points of annotation guidelines for identifying the explicit suggestions are: 1. The intention of giving a suggestion and 2. The suggestion should have the intention of A kappa value of 0.86 was calculated between the annotations performed by the expert annotators. The datasets from both phase 1 and phase 2 anno-tations are freely available for research. The final dataset has 3 kinds of labels, Implicit CTC, Ex-plicit CTC, and others. Therefore, this dataset is also usable by the works which intend to extract implicit suggestions as well. We frame the task of CTC suggestion detection as a text classification problem. Our approach per-forms binary classification in a supervised fash-ion. Explicit CTC suggestions belong to the posi-tive class, and rest of the sentences to the negative class.
 Heuristic Features: A general notion about the task is that suggestions contain some distinc-tive keywords like, suggest, recommend etc, and should be easily detectable using them. Therefore, we use a set of manually selected features in order to test this notion.  X  Suggestion keywords: These include the  X  POS tag VB : Base form of Verb (VB) ap-Generic Features: Standard uni, bi, tri-grams, and uni, bi, tri-grams of Part of Speech tags (Penn Tree bank tagset). We consider the best perform-ing set of Heuristics and generic feature types as the baseline for this task (see Table 3). 5.1 Special Features We suggest a set of complementary features, which are motivated by the linguistic analysis of explicit CTC suggestions. 1. Imperative Mood Sequential Patterns: Im-Table 4: Sequential POS Patterns obtained from Imperative Mood Dataset 2. Sentiment Features: Given that suggestions 3. Information about the subject / s of a state-Figure 3: Manually labelled Sentiment Distribu-tion of Electronics Dataset Figure 4: Manually labelled Sentiment Distribu-tion of Hotel Dataset Experimental Setup: We use the Stanford Parser (Toutanova et al., 2003) for obtaining part of speech and dependency information. Stemming did not effect the results. Stopwords were used using a customised stopword list. We employ the LibSVM library (Chang and Lin, 2011) for Sup-port Vector Machine classifiers (SVM), as imple-mented in Weka machine learning toolkit (Hall et al., 2009). The parameter values of SVM classi-fiers are: SVM type = C-SVC, Kernel Function = Radial Basis Function. A 10 fold cross valida-tion is performed in order to evaluate the classifier model. Features are ranked using the feature se-lection algorithm InfoGain (Mitchell, 1997). The other attribute selection algorithms provided with the Weka toolkit were also experimented with, but InfoGain consistently performed best in all the runs. Only the features which posses a positive information gain value are retained. The best per-forming run has a feature vector of size 300. The problem of having a imbalanced dataset is handled by using a higher class weight of 5:1 (Akbani et al., 2004) for positive class. We evaluate the proposed features using 10-fold cross validation. As indicated in section 5, we consider best performing set of generic features as the baseline, which is: uni, bi grams and uni-grams of pos tags. Table 5 summarises the classi-fier performance with the addition of special fea-tures, measured in Precision, Recall, F1 score, and ROC Area Under Curve. The results indicate that special features improve the F score in both the domains. However, some of the generic features produced better precision and recall values (Table 3). Imperative patterns improve the baseline re-sults for electronics dataset, but not for the hotel dataset. Also, the special features produce better improvement over baseline for electronics dataset. We attribute this to the smaller size of electronics dataset.
 Table 7 shows how the top ranked features change on addition of special features. Heuristic fea-tures tend to appear as some of the top ranked generic features. On the addition of special fea-tures, some of the special features replace the top general features. This validates the importance of proposed special features. Normalised sentiment Table 7: Some of the Top Ranked Features in Dif-ferent Runs score prove to be better features than the other two types of sentiment features including the manu-ally labelled sentiments. This indicates that this method of sentiment calculation is capturing some universally used phrasing for suggestions, where real sentiment of the sentence fails to capture it. Observed Challenges: Table 6 shows some in-stances of Type 1 and Type 2 errors in the best performing feature set. Our error analysis reveals the challenges associated with this task. 1. Non-CTC Suggestions: Example #4 gives a 2. Complex sentences: Often, suggestion is 3. Sarcasm: The surface form of #6 is a sugges-4. Biased Datasets: Explicit CTC suggestion A general observation is that the text in the form of suggestions may not always be a suggestion, and vice versa. Therefore, syntactic and lexical fea-tures seem to be ineffective in many cases. This work serves as an introduction and analysis of the problem of the extraction of customer-to-customer suggestions from reviews. The task is useful for a number of practical applications. We observed that the layman perception of suggestion is very wide, especially in the case of reviews. Therefore, we defined and limited the scope of our work to explicit suggestions. Following this, we prepared a well-investigated benchmark dataset, which is freely available for research purposes. A pilot approach to the problem is presented, which analyses the performance of standard text classification features, and tests a set of comple-mentary/special features. The special features im-proved the baseline results for both service and product reviews.
 Analysis of classification errors highlighted the challenges associated with the task. The classi-fication results have scope for improvement, and therefore the task calls for advanced semantic fea-tures and dedicated models, which will be our fu-ture direction. Furthermore, the relation between sentiments and suggestions seem to be worth in-vestigating.
 This work has been funded by the European Unions Horizon 2020 programme under grant agreement No 644632 MixedEmotions, and the Science Foundation Ireland under Grant Number SFI/12/RC/2289 (Insight Center).
