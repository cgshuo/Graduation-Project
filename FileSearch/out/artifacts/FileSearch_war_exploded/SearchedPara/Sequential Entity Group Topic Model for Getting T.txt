 Analyzing documents on the Web is difficult due to the fast growing number of documents. Most of documents are not annota ted, leading us to prefer unsupervised methods for analyzing document, and topic mining is one such method. This method documents. Since techniques like Probabilistic Latent Semantic Indexing (PLSI) [1] and Latent Dirichlet Allocation (LDA) [2] were first introduced, many studies have [3, 4], to discover topic flows of documents in time dimension [5], or topic flows of segments in one document [6, 7], and so on. Capturing topic flows in one document (i.e., a fiction or a history) has special ch aracteristics. For instance, adjacent segments in one document would influence each other because the full set of segments (i.e., the document) as a whole has some story. Moreover, the readers probably want to see the models tried to get topics of entity groups, no model has been proposed to obtain the topic flow of each entity or each relationshi p in one document. The topic flow in one document should also be useful for the readers to grasp the story easily. 
In this paper, we propose two topic models, Entity Group Topic Model (EGTM) and Sequential Entity Group Topic Model (S-EGTM), claiming two contributions. document can be captured. To realize our proposal, we adopt collapsed gibbs sampling methods [8] to infer the parameters of the models. 
The rest of the paper is organized as follows. In the following subsection, we preview the terminology to set out the basic concepts. Section 2 discusses related experiments and results. Finally, Section 5 concludes. 1.1 Terminology basic concepts.  X  Entity: Something which the user want to get information about it. It can be a  X  Empty group (empty set): A group having no entity.  X  Entity group: A group having one or more entities.  X  Entity group size: The number of entities in the entity group.  X  Entity pair: A pair of two entities.  X  Topic (word topic): A multinomial word distribution.  X  Entity topic: A multinomial entity distribution of CorrLDA2.  X  Segment: A part of a document. It can be a paragraph, or even a sentence.  X  Topic flow: A sequence of topic distribution through segments of a document.  X  Relationship of entities: A topic distribution of the entity group. sequential topic mining . relationship of entities. Author Topic Model (ATM) [9] is a model for getting a topic used for getting topics of entities by just considering an entity as an author. However, studies about a model involving the process. The recent proposed model, named as each entity and of each entity pair. However, since it takes two kinds of topics topic models for analyzing entities were introduced in [3]. Especially, CorrLDA2 showed its best prediction performance. The model captures not only topics, but also entity topics. The entity topic is basically a list of entities, thus each entity topic plays relationship of a certain entity group. different dimensions. Dynamic Topic Model (DTM) [5] aimed to capture topic flows of documents in time dimension. Probabilistic way to capture the topic patterns on weblogs, in both of space dimension and time dimension, was introduced in [10]. Multi-grain LDA (MG-LDA) [11] used topic distribution of each window in a document to get the ratable aspects. Although it utilizes sequent topic distributions to deal with multi-STM and Sequential LDA tried to get a topic flow within a document. The both studies are based on a nested extension of the two-parameter Poisson-Dirichlet Process (PDP). The STM assumes that each segment is influenced by the document, while the Sequential LDA assumes that each segment is influenced by its previous segment except for the first segment. be used to obtain topic flow of each entit y and each relationship within one document. The topic flow of each entity or each relations hip should also be useful for the readers to grasp the story more easily. This sec tion introduces two topic models, Entity Group Topic Model (EGTM) and Sequential Entity Group Topic Model (S-EGTM). 3.1 Entity Group Topic Model A graphical model of EGTM is shown in Figure 1(a). The meaning of notations is apply the assumption into our model, we employ a power-set. For example, if an document, because it associates with every sentence. Formally, the generative process is represented in Figure 2. 
As a sentence has only one entity group or an entity, the size of power-set does not grow exponentially. If there is no observed entity in a sentence, then the sentence has an empty group . We developed a collapsed gibbs sampling. At each step of the Markov chain, the topic of the i th word is chosen using a conditional probability The notations are described in Table 1, with a minor exceptional use of notation that follows: 3.2 Sequential Entity Group Topic Model A graphical model of S-EGTM is Figure 1(b). Formally, the generative process is represented in Figure 3. As S-EGTM gets a topic flow in a document, the D must be 1. A topic distribution of each segment is affected by that of previous segment, except that the first segment is affected by the document X  X  topic distribution. To model this, Restaurant Process (CRP) notations, then a word is a customer. The topics are dishes by customers. The customers sitting around a table share a dish. Especially, in nested 
When we do a collapsed gibbs sampling for topics, removing i th topic z dgi = k affects the table counts and topic distributions of entity group e in the segment g . Therefore, following. 
First, when u dek =1, Second, when 1 &lt; u dek  X  g , Third, when g &lt; u dek , The notations are described in Table 1, with a minor exceptional use of notation that because a table count is affected by the number of words having the table X  X  topic and sampled as follows: The notation g =1 means the first term is active only if it is the first segment, 1-( g =1) determine the window size of table count to consider. Among four parameters, we describe the approximate probabilities of two parameters as follows: We used two data sets: the Bible and the fiction  X  X lice X . We removed stop-words and  X  X ewline X . After the deleting stop-words, the Bible has 295,884 words and the fiction  X  X lice X  has 11,605 words. As S-EGTM gets a topic flow in a document, it regards the Bible as a document consisting of 66 segments, and the fiction  X  X lice X  as a document consisting of 12 segments. In contrast, to compare EGTM with other models, we divided each document into separated files as segments. For every experiment, we set  X  =0.1,  X  =0.01,  X  =1, a =0.5, b =10, and the window size was 1. 4.1 The Size of Power-Set of Entity Groups When we input a list of entities to cons ider, then a preprocessing will make a power-set hierarchy of existing entity groups in a document. Since a sentence is restricted to have an entity group, each entity group usually does not have more than three entities. used data is the Bible. 4.2 Topic Discovery LDA and EGTM, are shown. The obtained topics of the models are similar to each other because the empty group of EGTM associates with every sentence. The topics are coherent and specific to understand. EGTM additionally gives entity lists and topics that each entity or entity group as sociates with. For example, the topic Mission work , which is about missionary acts of apostles, is mostly handled in the Act written by Paul who lived in different era with Abraham . Nevertheless, the relationship Paul  X  X  writing about the covenant between God and Abraham . Since the covenant is { God,Abraham } has the topic Mission work in Act . Thus, EGTM helps us to grasp the documents in perspective of an entity or relationship. 4.3 Entity Prediction We compared the EGTM with CorrLDA2 by entity prediction performance. We also the number of topics in sentences which have each entity, after LDA estimation. We used methods. For CorrLDA2, we set the number of entity topics same as the number of word topics, because we observed that the prediction results are similar with different numbers of entity topics. We did 10-fold cross validation for the comparison, and got the prediction results using the process in Figure 5. 
The test data consists of sentences which have at least one entity. If a sentence has multiple entities, then choosing one of them is regarded as a correct choice. As depicted in Figure 6(a), the CorrLDA2 shows fixed performance because the resampling makes P(t|d) to be fixed. EGTM outperforms other models because the topics of Entity-LDA have nothing to do with entities and CorrLDA2 does not get the topic distribution of each entity. The performance of EGTM grows as the number of topics grows because the Bible cove rs various topics. EGTM shows better performances than CorrLDA2 because of two reasons. First, CorrLDA2 does not directly get the topic distribu tion of each entity and it di sperses the topic distribution To be specific, the data already used for entity topics will not be used for word topics. 4.4 Entity Pair Prediction We compared the entity pair prediction performance between EGTM and CorrLDA2. exist in only unseen document, while the false pairs do not exist. The prediction number of total pairs. We prepared 50 true pairs and 50 false pairs. The models have different methods to get P(e i |e j ) which is obtainable from . Entity-LDA just counts the number of each topic. CorrLDA2 uses entity topic distributions. For example, where et means each entity topic. Figure 6(b) describes the prediction performance. Because the most entities of the Bible old testament usually do not appear in the Bible new testament, the overall prediction performances is low. EGTM outperforms CorrLDA2 and Entity-LDA, because EGTM directly takes a topic distribution of each entity. 4.5 Entity Group Prediction We do not compare the prediction performance with other models because the other models lack ability to get topic distributions of entity groups. Instead, we demonstrate prediction performance with different entity group sizes. The predictive distribution is , where eg represents the entity group, and d represents each training document. Figure 6(c) shows the prediction performance. The accuracy is the number of correct predictions divided by the number of total predictions. The prediction performance of smaller entity group is better than that of larger entity group, becau se it is harder to predict more entities. 4.6 Topic Flow We compare the topic flow of S-EGTM with the topic distributions of EGTM. To show the topic consistency between the two models, we trained S-EGTM boosted from the trained EGTM with 2,000 iterations. The Bible new testament and the fiction relationship { Jesus , God } with 20 topics. Figure 7 and Figure 8 show the topic flows of the entity Alice and the relationship { Jesus , God }, respectively. 
Figure 7(a) and Figure 8(a) show the confusion matrices of the topic distributions generated by EGTM and S-EGTM. The diagonal cells are darker than others, meaning that the corresponding topics have low Hellinger distance. Thus, the topics of two models are consistent. Other than the Figure 7(a) and Figure 8(a), the 7(c), we can see the pattern that the topic 8(pink color) flows through every segment. As the topic 8 is about Alice X  X  tracking the rabbit , its flow through every segment is detail. In Figure 8(b), the topic Gospel (topic 14) is dominant in four separated parts, distribution because it reflects only the sentences having the relationship. The separated appearance of the topic is not coherent with the Bible, because a purpose of the Bible new testament associates with the topic Gospel which is strongly about the appears like a flow from Acts to Revelation . This means the relationship { Jesus , God } entity or a relationship by smoothing the sparse topic distribution of EGTM. In this paper, we proposed two new generative models, Entity Group Topic Model (EGTM) and the Sequential Entity Group Topic Model (S-EGTM). S-EGTM reflects the sequential structure of a document in the hierarchical modeling. We developed collapsed gibbs sampling algorithms for the models. EGTM employs a power-set structure to get topics of entities or entity groups. S-EGTM is a sequential version of the EGTM, and employs nested two-parameter Poisson-Dirichlet process (PDP) to capture a topic flow over the sequence of segments in one document. We have analyzed the topics obtained from EGTM, an d showed that topic flows generated by S-EGTM are coherent with the original document. Moreover, the experimental results show that the prediction performance of EGTM is better than that of CorrLDA2. Thus, we believed that the intended mechanisms of the EGTM and S-EGTM models work. Acknowledgments. This work was supported by the National Research Foundation (NRF) grant (No. 2011-0018264) of Ministry of Education, Science and Technology (MEST) of Korea. 
