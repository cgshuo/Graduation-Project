 Recurrent Neural Networks (RNNs) (Elman, 1990; Mikolov et al., 2010) are remarkably powerful mod-els for sequential data. Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), a spe-cific architecture of RNN, has a track record of suc-cess in many natural language processing tasks such as language modeling (J  X  ozefowicz et al., 2015), de-pendency parsing (Dyer et al., 2015), sentence com-pression (Filippova et al., 2015), and machine trans-lation (Sutskever et al., 2014).

Within the context of natural language process-ing, a common assumption is that LSTMs are able to capture certain linguistic phenomena. Evidence sup-porting this assumption mainly comes from evaluat-ing LSTMs in downstream applications: Bowman et al. (2015) carefully design two artificial datasets where sentences have explicit recursive structures. They show empirically that while processing the in-put linearly, LSTMs can implicitly exploit recursive structures of languages. Filippova et al. (2015) find that using explicit syntactic features within LSTMs in their sentence compression model hurts the per-formance of overall system. They then hypothesize that a basic LSTM is powerful enough to capture syntactic aspects which are useful for compression.
To understand and explain which linguistic di-mensions are captured by an LSTM is non-trivial. This is due to the fact that the sequences of input histories are compressed into several dense vectors by the LSTM X  X  components whose purposes with re-spect to representing linguistic information is not ev-ident. To our knowledge, the only attempt to better understand the reasons of an LSTM X  X  performance and limitations is the work of Karpathy et al. (2015) by means of visualization experiments and cell acti-vation statistics in the context of character-level lan-guage modeling.

Our work is motivated by the difficulty in un-derstanding and interpreting existing RNN architec-tures from a linguistic point of view. We propose Re-current Memory Network (RMN), a novel RNN ar-chitecture that combines the strengths of both LSTM and Memory Network (Sukhbaatar et al., 2015). In RMN, the Memory Block component X  X  variant of Memory Network X  X ccesses the most recent input words and selectively attends to words that are rel-evant for predicting the next word given the current LSTM state. By looking at the attention distribution over history words, our RMN allows us not only to interpret the results but also to discover underlying dependencies present in the data.

In this paper, we make the following contribu-tions: 1. We propose a novel RNN architecture that 2. We perform an analysis along various linguis-3. We show that, with a simple modification, Recurrent Neural Networks (RNNs) have shown im-pressive performances on many sequential modeling tasks due to their ability to encode unbounded input histories. However, training simple RNNs is diffi-cult because of the vanishing and exploding gradi-ent problems (Bengio et al., 1994; Pascanu et al., 2013). A simple and effective solution for explod-ing gradients is gradient clipping proposed by Pas-canu et al. (2013). To address the more challeng-ing problem of vanishing gradients, several variants of RNNs have been proposed. Among them, Long Short-Term Memory (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (Cho et al., 2014) are widely regarded as the most successful variants. In this work, we focus on LSTMs because they have been shown to outperform GRUs on language mod-eling tasks (J  X  ozefowicz et al., 2015). In the follow-ing, we will detail the LSTM architecture used in this work.
 Long Short-Term Memory Notation : Throughout this paper, we denote matri-ces, vectors, and scalars using bold uppercase (e. g., W ), bold lowercase (e. g., b ) and lowercase (e. g.,  X  ) letters, respectively.

The LSTM used in this work is specified as fol-lows: where x t is the input vector at time step t , h t  X  1 is the LSTM hidden state at the previous time step, W  X  and b  X  are weights and biases. The symbol de-notes the Hadamard product or element-wise multi-plication.

Despite the popularity of LSTM in sequential modeling, its design is not straightforward to justify and understanding why it works remains a challenge (Hermans and Schrauwen, 2013; Chung et al., 2014; Greff et al., 2015; J  X  ozefowicz et al., 2015; Karpa-thy et al., 2015). There have been few recent at-tempts to understand the components of an LSTM from an empirical point of view: Greff et al. (2015) carry out a large-scale experiment of eight LSTM variants. The results from their 5,400 experimental runs suggest that forget gates and output gates are the most critical components of LSTMs. J  X  ozefowicz et al. (2015) conduct and evaluate over ten thousand RNN architectures and find that the initialization of the forget gate bias is crucial to the LSTM X  X  perfor-mance. While these findings are important to help choosing appropriate LSTM architectures, they do not shed light on what information is captured by the hidden states of an LSTM.

Bowman et al. (2015) show that a vanilla LSTM, such as described above, performs reasonably well compared to a recursive neural network (Socher et al., 2011) that explicitly exploits tree structures on two artificial datasets. They find that LSTMs can effectively exploit recursive structure in the artifi-cial datasets. In contrast to these simple datasets containing a few logical operations in their exper-iments, natural languages exhibit highly complex patterns. The extent to which linguistic assumptions about syntactic structures and compositional seman-tics are reflected in LSTMs is rather poorly under-stood. Thus it is desirable to have a more principled mechanism allowing us to inspect recurrent architec-tures from a linguistic perspective. In the following section, we propose such a mechanism. It has been demonstrated that RNNs can retain in-put information over a long period. However, exist-ing RNN architectures make it difficult to analyze what information is exactly retained at their hidden states at each time step, especially when the data has complex underlying structures, which is common in natural language. Motivated by this difficulty, we propose a novel RNN architecture called Recurrent Memory Network (RMN). On linguistic data, the RMN allows us not only to qualify which linguis-tic information is preserved over time and why this is the case but also to discover dependencies within the data (Section 5). Our RMN consists of two com-ponents: an LSTM and a Memory Block (MB) (Sec-tion 3.1). The MB takes the hidden state of the LSTM and compares it to the most recent inputs using an attention mechanism (Gregor et al., 2015; Bahdanau et al., 2014; Graves et al., 2014). Thus, analyzing the attention weights of a trained model can give us valuable insight into the information that is retained over time in the LSTM.

In the following, we describe in detail the MB ar-chitecture and the combination of the MB and the LSTM to form an RMN. 3.1 Memory Block The Memory Block (Figure 1) is a variant of Mem-ory Network (Sukhbaatar et al., 2015) with one hop (or a single-layer Memory Network). At time step t , the MB receives two inputs: the hidden state h t of the LSTM and a set { x i } of n most recent words including the current word x t . We refer to n as the memory size. Internally, the MB consists of
Figure 1: A graphical representation of the MB. two lookup tables M and C of size | V | X  d , where | V | is the size of the vocabulary. With a slight abuse of notation we denote M i = M ( { x i } ) and C i = C ( { x i } ) as n  X  d matrices where each row corresponds to an input memory embedding m i and an output memory embedding c i of each element of the set { x i } . We use the matrix M i to compute an attention distribution over the set { x i } : When dealing with data that exhibits a strong tem-poral relationship, such as natural language, an ad-bias attention with respect to the position of the data points. In this case, equation 1 becomes We then use the attention distribution p t to compute a context vector representation of { x i } : Finally, we combine the context vector s t and the hidden state h t by a function g (  X  ) to obtain the out-put h m t of the MB. Instead of using a simple addi-tion function g ( s t , h t ) = s t + h t as in Sukhbaatar et al. (2015), we propose to use a gating unit that decides how much it should trust the hidden state h t and context s t at time step t . Our gating unit is a form of Gated Recurrent Unit (Cho et al., 2014; Chung et al., 2014): where z t is an update gate, r t is a reset gate.
The choice of the composition function g (  X  ) is crucial for the MB especially when one of its in-put comes from the LSTM. The simple addition function might overwrite the information within the LSTM X  X  hidden state and therefore prevent the MB from keeping track of information in the distant past. The gating function, on the other hand, can control the degree of information that flows from the LSTM to the MB X  X  output. 3.2 RMN Architectures As explained above, our proposed MB receives the hidden state of the LSTM as one of its input. This leads to an intuitive combination of the two units by stacking the MB on top of the LSTM. We call this architecture Recurrent-Memory (RM). The RM ar-chitecture, however, does not allow interaction be-tween Memory Blocks at different time steps. To enable this interaction we can stack one more LSTM layer on top of the RM. We call this architecture Recurrent-Memory-Recurrent (RMR).
 Figure 2: A graphical illustration of an unfolded RMR with memory size 4. Dashed line indicates concatenation. The MB takes the output of the bot-tom LSTM layer and the 4-word history as its input. The output of the MB is then passed to the second LSTM layer on top. There is no direct connection between MBs of different time steps. The last LSTM layer carries the MB X  X  outputs recurrently. Language models play a crucial role in many NLP applications such as machine translation and speech recognition. Language modeling also serves as a standard test bed for newly proposed models (Sukhbaatar et al., 2015; Kalchbrenner et al., 2015). We conjecture that, by explicitly accessing history words, RMNs will offer better predictive power than the existing recurrent architectures. We therefore evaluate our RMN architectures against state-of-the-art LSTMs in terms of perplexity. 4.1 Data We evaluate our models on three languages: En-glish, German, and Italian. We are especially inter-ested in German and Italian because of their larger vocabularies and complex agreement patterns. Ta-ble 1 summarizes the data used in our experiments. Lang Train Dev Test | s | | V | En 26M 223K 228K 26 77K De 22M 202K 203K 22 111K It 29M 207K 214K 29 104K Table 1: Data statistics. | s | denotes the average sen-tence length and | V | the vocabulary size.

The training data correspond to approximately 1M sentences in each language. For English, we use all the News Commentary data (8M tokens) and 18M tokens from News Crawl 2014 for train-ing. Development and test data are randomly drawn from the concatenation of the WMT 2009-2014 test sets (Bojar et al., 2015). For German, we use the first 6M tokens from the News Commentary data and 16M tokens from News Crawl 2014 for train-ing. For development and test data we use the re-maining part of the News Commentary data con-catenated with the WMT 2009-2014 test sets. Fi-nally, for Italian, we use a selection of 29M tokens from the PAIS ` A corpus (Lyding et al., 2014), mainly including Wikipedia pages and, to a minor extent, Wikibooks and Wikinews documents. For develop-ment and test we randomly draw documents from the same corpus. 4.2 Setup Our baselines are a 5-gram language model with Kneser-Ney smoothing, a Memory Network (MemN) (Sukhbaatar et al., 2015), a vanilla single-layer LSTM, and two stacked LSTMs with two and three layers respectively. N-gram models have been used intensively in many applications for their ex-cellent performance and fast training. Chen et al. (2015) show that n-gram model outperforms a pop-ular feed-forward language model (Bengio et al., 2003) on a one billion word benchmark (Chelba et al., 2013). While taking longer time to train, RNNs have been proven superior to n-gram models.

We compare these baselines with our two model architectures: RMR and RM. For each of our mod-els, we consider two settings: with or without tem-poral matrix (+tM or  X  X M), and linear vs. gating composition function. In total, we experiment with eight RMN variants.

For all neural network models, we set the dimen-sion of word embeddings, the LSTM hidden states, its gates, the memory input, and output embeddings to 128. The memory size is set to 15. The bias of the LSTM X  X  forget gate is initialized to 1 (J  X  ozefowicz et al., 2015) while all other parameters are initialized uniformly in (  X  0 . 05 , 0 . 05) . The initial learning rate is set to 1 and is halved at each epoch after the forth epoch. All models are trained for 15 epochs with standard stochastic gradient descent (SGD). During training, we rescale the gradients whenever their norm is greater than 5 (Pascanu et al., 2013).
Sentences with the same length are grouped into buckets. Then, mini-batches of 20 sentences are drawn from each bucket. We do not use truncated back-propagation through time, instead gradients are fully back-propagated from the end of each sen-tence to its beginning. When feeding in a new mini-batch, the hidden states of LSTMs are reset to zeros, which ensures that the data is properly modeled at the sentence level. For our RMN models, instead of using padding, at time step t &lt; n , we use a slice 4.3 Results Perplexities on the test data are given in Table 2. All RMN variants largely outperform n -gram and MemN models, and most RMN variants also outper-form the competitive LSTM baselines. The best re-sults overall are obtained by RM with temporal ma-trix and gating composition (+tM-g).

Our results agree with the hypothesis of mitigat-ing prediction error by explicitly using the last n words in RNNs (Karpathy et al., 2015). We further observe that using a temporal matrix always bene-fits the RM architectures. This can be explained by seeing the RM as a principled way to combine an LSTM and a neural n -gram model. By contrast, RMR works better without temporal matrix but its Table 2: Perplexity comparison including RMN variants with and without temporal matrix (tM) and linear (l) versus gating (g) composition function. overall performance is not as good as RM. This sug-gests that we need a better mechanism to address the interaction between MBs, which we leave to fu-ture work. Finally, the proposed gating composition function outperforms the linear one in most cases.
For historical reasons, we also run a stacked three-layer LSTM and a RM(+tM-g) on the much smaller Penn Treebank dataset (Marcus et al., 1993) with the same setting described above. The respective per-plexities are 126.1 and 123.5. The goal of our RMN design is twofold: (i) to obtain better predictive power and (ii) to facilitate under-standing of the model and discover patterns in data. In Section 4, we have validated the predictive power of the RMN and below we investigate the source of this performance based on linguistic assumptions of word co-occurrences and dependency structures. 5.1 Positional and lexical analysis As a first step towards understanding RMN, we look at the average attention weights of each history word position in the MB of our two best model variants (Figure 3). One can see that the attention mass tends to concentrate at the rightmost position (the current Figure 3: Average attention per position of RMN history. Top: RMR( X  X M-g), bottom: RM(+tM-g). Rightmost positions represent most recent history. word) and decreases when moving further to the left (less recent words). This is not surprising since the success of n -gram language models has demon-strated that the most recent words provide important information for predicting the next word. Between the two variants, the RM average attention mass is less concentrated to the right. This can be explained by the absence of an LSTM layer on top, meaning that the MB in the RM architecture has to pay more attention to the more distant words in the past. The remaining analyses described below are performed on the RM(+tM-g) architecture as this yields the best perplexity results overall.

Beyond average attention weights, we are inter-ested in those cases where attention focuses on dis-tant positions. To this end, we randomly sample 100 words from test data and visualize attention distri-butions over the last 15 words. Figure 4 shows the attention distributions for random samples of Ger-man and Italian. Again, in many cases attention weights concentrate around the last word (bottom row). However, we observe that many long distance words also receive noticeable attention mass. Inter-estingly, for many predicted words, attention is dis-tributed evenly over memory positions, possibly in-Figure 4: Attention visualization of 100 word sam-ples. Bottom positions in each plot represent most recent history. Darker color means higher weight. dicating cases where the LSTM state already con-tains enough information to predict the next word.
To explain the long-distance dependencies, we first hypothesize that our RMN mostly memorizes frequent co-occurrences. We run the RM(+tM-g) model on the German development and test sen-tences, and select those pairs of ( most-attended-word, word-to-predict ) where the MB X  X  attention concentrates on a word more than six positions to the left. Then, for each set of pairs with equal dis-tance, we compute the mean frequency of corre-sponding co-occurrences seen in the training data (Table 3). The lack of correlation between frequency and memory location suggests that RMN does more than simply memorizing frequent co-occurrences. Table 3: Mean frequency (  X  ) of ( most-attended-word, word-to-predict ) pairs grouped by relative dis-tance ( d ).
 Previous work (Hermans and Schrauwen, 2013; Karpathy et al., 2015) studied this property of LSTMs by analyzing simple cases of closing brack-ets. By contrast RMN allows us to discover more interesting dependencies in the data. We manually inspect those high-frequency pairs to see whether they display certain linguistic phenomena. We ob-serve that RMN captures, for example, separable verbs and fixed expressions in German. Separable verbs are frequent in German: they typically consist of preposition+verb constructions, such ab+h  X  angen ( X  X o depend X ) or aus+schlie X en ( X  X o exclude X ), and can be spelled together ( abh  X  angen ) or apart as in  X  h  X  angen von der Situation ab  X  ( X  X epend on the sit-uation X ), depending on the grammatical construc-tion. Figure 5a shows a long-dependency exam-ple for the separable verb abh  X  angen (to depend) . When predicting the verb X  X  particle ab , the model correctly attends to the verb X  X  core h  X  angt occurring seven words to the left. Figure 5b and 5c show fixed expression examples from German and Italian, re-spectively: schl  X  usselrolle ... spielen (play a key role) and insignito ... titolo (awarded title) . Here too, the model correctly attends to the key word despite its long distance from the word to predict. (a,b) and second in (c).

Other interesting examples found by the RMN in the test data include:
German: findet statt (takes place ), kehrte zur  X  uck
Italian: sinistra destra (left right ), latitudine lon-5.2 Syntactic analysis It has been conjectured that RNNs, and LSTMs in particular, model text so well because they capture syntactic structure implicitly. Unfortunately this has been hard to prove, but with our RMN model we can get closer to answering this important question.
We produce dependency parses for our test sets using (Sennrich et al., 2013) for German and (At-tardi et al., 2009) for Italian. Next we look at how much attention mass is concentrated by the RM(+tM-g) model on different dependency types. Figure 6 shows, for each language, a selection of Dependency direction is marked by an arrow: e.g.  X  mod means that the word to predict is a modifier of the attended word, while mod  X  means that the White cells denote combinations of position and de-pendency type that were not present in the test data.
While in most of the cases closest positions are attended the most, we can see that some dependency types also receive noticeably more attention than the average ( ALL ) on the long-distance positions. In German, this is mostly visible for the head of separable verb particles (  X  avz ), which nicely sup-ports our observations in the lexical analysis (Sec-tion 5.1). Other attended dependencies include: aux-iliary verbs (  X  aux ) when predicting the second el-ement of a complex tense ( hat . . . gesagt / has said ); subordinating conjunctions ( konj  X  ) when predict-ing the clause-final inflected verb ( dass sie sagen sollten / that they should say ); control verbs (  X  obji ) when predicting the infinitive verb ( versucht ihr zu helfen / tries to help her ). Out of the Italian dependency types selected for their frequent long-distance occurrences (bottom of Figure 6), the most attended are argument heads (  X  arg ), complement heads (  X  comp ), object heads (  X  obj ) and subjects ( subj  X  ). This suggests that RMN is mainly captur-ing predicate argument structure in Italian. Notice that syntactic annotation is never used to train the model, but only to analyze its predictions.

We can also use RMN to discover which complex dependency paths are important for word prediction. To mention just a few examples, high attention on Figure 6: Average attention weights per position, broken down by dependency relation type+direction between the attended word and the word to predict. Top: German. Bottom: Italian. More distant posi-tions are binned. the German path [ subj  X  ,  X  kon,  X  cj ] indicates that the model captures morphological agreement be-tween coordinate clauses in non-trivial constructions of the kind: spielen die Kinder im Garten und singen / the children play in the garden and sing . In Italian, high attention on the path [  X  obj,  X  comp,  X  prep ] denotes cases where the semantic relatedness be-tween a verb and its object does not stop at the ob-ject X  X  head, but percolates down to a prepositional phrase attached to it ( pass ` o buona parte della sua vita / spent a large part of his life ). Interestingly, both local n-gram context and immediate depen-dency context would have missed these relations.
While much remains to be explored, our analysis shows that RMN discovers patterns far more com-plex than pairs of opening and closing brackets, and suggests that the network X  X  hidden state captures to a large extent the underlying structure of text. The Microsoft Research Sentence Completion Chal-lenge (Zweig and Burges, 2012) has recently be-come a test bed for advancing statistical language modeling. We choose this task to demonstrate the effectiveness of our RMN in capturing sentence co-herence. The test set consists of 1,040 sentences se-lected from five Sherlock Holmes novels by Conan Doyle. For each sentence, a content word is removed and the task is to identify the correct missing word among five given candidates. The task is carefully designed to be non-solvable for local language mod-els such as n -gram models. The best reported re-is far below human accuracy of 91% (Zweig and Burges, 2012).
 As baseline we use a stacked three-layer LSTM. Our models are two variants of RM(+tM-g), each consisting of three LSTM layers followed by a MB. The first variant (unidirectional-RM) uses n words preceding the word to predict, the second (bidirectional-RM) uses the n words preceding and the n words following the word to predict, as MB input. We include bidirectional-RM in the experi-ments to show the flexibility of utilizing future con-text in RMN.

We train all models on the standard training data of the challenge, which consists of 522 novels from Project Gutenberg, preprocessed similarly to (Mnih and Kavukcuoglu, 2013). After sentence splitting, tokenization and lowercasing, we randomly select 19,000 sentences for validation. Training and val-idation sets include 47M and 190K tokens respec-tively. The vocabulary size is about 64,000.
We initialize and train all the networks as de-scribed in Section 4.2. Moreover, for regularization, we place dropout (Srivastava et al., 2014) after each LSTM layer as suggested in (Pham et al., 2014). The dropout rate is set to 0.3 in all the experiments.
Table 4 summarizes the results. It is worth to mention that our LSTM baseline outperforms a de-pendency RNN making explicit use of syntactic in-formation (Mirowski and Vlachos, 2015) and per-forms on par with the best published result (Mikolov et al., 2013). Our unidirectional-RM sets a new state of the art for the Sentence Completion Challenge with 69.2% accuracy. Under the same setting of d we observe that using bidirectional context does not Table 4: Accuracy on 1,040 test sentences. We use perplexity to choose the best model. Dimension of word embeddings, LSTM hidden states, and gate g parameters are set to d . bring additional advantage to the model. Mnih and Kavukcuoglu (2013) also report a similar observa-tion. We believe that RMN may achieve further im-provements with hyper-parameter optimization. Figure 7 shows some examples where our best RMN beats the already very competitive LSTM baseline, or where both models fail. We can see that in some sentences the necessary clues to predict the correct word occur only to its right . While this seems to conflict with the worse result obtained by the bidirectional-RM, it is important to realize that prediction corresponds to the whole sentence prob-ability. Therefore a badly chosen word can have a negative effect on the score of future words. This ap-pears to be particularly true for the RMN due to its ability to directly access (distant) words in the his-tory. The better performance of unidirectional ver-sus bidirectional-RM may indicate that the attention in the memory block can be distributed reliably only on words that have been already seen and summa-rized by the current LSTM state. In future work, we may investigate whether different ways to com-bine two RMNs running in opposite directions fur-ther improve accuracy on this challenging task. We have proposed the Recurrent Memory Network (RMN), a novel recurrent architecture for language modeling. Our RMN outperforms LSTMs in terms of perplexity on three large dataset and allows us to analyze its behavior from a linguistic perspective. We find that RMNs learn important co-occurrences regardless of their distance. Even more interest-ingly, our RMN implicitly captures certain depen-dency types that are important for word prediction, despite being trained without any syntactic informa-tion. Finally RMNs obtain excellent performance at modeling sentence coherence, setting a new state of the art on the challenging sentence completion task. This research was funded in part by the Netherlands Organization for Scientific Research (NWO) under project numbers 639.022.213 and 612.001.218.
