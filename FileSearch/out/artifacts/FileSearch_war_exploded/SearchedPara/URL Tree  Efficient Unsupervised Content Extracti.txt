 The Web represents the largest, and an increasingly grow-ing, source of information. Extracting meaningful content from Web pages presents a challenging problem, already ex-tensively addressed in the offline setting. In this work, we focus on content extraction from streams of HTML docu-ments. We present an infrastructure that converts continu-ously acquired HTML documents into a stream of plain text documents. The presented pipeline consists of RSS readers for data acquisition from different Web sites, a duplicate re-moval component, and a novel content extraction algorithm which is efficient, unsupervised, and language-independent. Our content extraction approach is based on the observa-tion that HTML documents from the same source normally share a common template. The core of the proposed con-tent extraction algorithm is a simple data structure called URL Tree . The performance of the algorithm was evaluated in a stream setting on a time-stamped semi-automatically annotated dataset which was made publicly available. We compared the performance of URL Tree with that of several open source content extraction algorithms. The evaluation results show that our stream-based algorithm already starts outperforming the other algorithms after only 10 to 100 doc-uments from a specific domain.
 E.1 [ Data Structures ]: Trees; H.3.3 [ Information Stor-age and Retrieval ]: Information Search and Retrieval X  Information filtering, Retrieval models ; I.7 [ Document and Text Processing ]: Miscellaneous Algorithms, Design, Experimentation, Performance content extraction; boilerplate removal; stream data; Web content; unsupervised learning
The Web has become the largest source of information and heterogeneous data. The data is available in different repre-sentations (texts, graphs, knowledge stores, databases, time series, etc.), languages, and sizes (the concept of big data is becoming more and more important), and can have different dynamics (static data, slow or fast-paced streams). Consid-ering only news sites, blogs, and social media, new content is produced continuously at an increasing pace. From this perspective, we can view the Web as a generator of data streams that contain valuable information that is often hard to discover due to the information overload problem. Man-aging such amounts of unstructured streaming data in terms of information and knowledge discovery requires satisfying, among other things, two major conditions: (i) efficient on-line processing with near-real time information delivery and (ii) differentiation between the relevant content in HTML documents and the accompanying text called the boilerplate (template and navigation items, recommendations, adver-tisements, copyright notices, etc.).

Automated content extraction, template identification, or boilerplate removal for a general Web page proves to be a challenging task and has attracted much attention in the scientific literature and industry. Early approaches were mostly based on handcrafted rules and could only be ap-plied to Web pages from a limited number of sources. The main drawback of these approaches was the inability to eas-ily adapt to the changes that occur over time in the HTML structure of Web pages. In contrast, template detection al-gorithms [1] aim at finding invariant and changing sections of Web pages automatically. In [8], the Document Object Model (DOM) trees of a set of Web pages are analyzed to find the optimal mapping between the tree nodes and thereby identify the content nodes. In [6], the cooccurence of the terms in a set of Web pages is analyzed and the en-tropy of each text block is computed. The informative text blocks are then identified by thresholding. The approach in [2] excludes template items by detecting similar content with a common layout style during the index building pro-cess of a search engine. More recent approaches mainly focus on textual features. NCleaner [3] uses a character-level n -gram language model to distinguish between, as they call it, clean and dirty text. The boilerpipe algorithm [4] describes text blocks with  X  X hallow text features X  and builds a decision tree that is used to classify the text blocks of an arbitrary Web page as content or boilerplate. jusText [7] implements a set of rules that characterize text blocks as either  X  X ood X  or  X  X ad X . The algorithm first performs a context-free clas-sification of text blocks and then refines it with a set of context-aware rules. Readability , 1 a popular reading tool, manipulates the DOM tree of a Web page by employing several heuristics based on textual and structural features to remove the boilerplate. More exhaustive overviews of the content extraction field can be found in [7] and [5].
In contrast to the existing methods which are mostly su-pervised, language-dependent, and/or unaware of the stream setting, we propose an efficient, online, unsupervised, language-independent approach to content extraction from streams of HTML documents.

The main contributions of this work are as follows. First, we describe an infrastructure for content extraction from a continuous stream of documents from different Web sites (see Section 2). Second, we propose a URL normaliza-tion procedure for identifying duplicates (see Section 3). Third, we present the URL Tree data structure and the cor-responding stream-based content extraction algorithm (see Section 4). Fourth, we compare the performance of the proposed algorithm to 10 different open source algorithms (see Section 5). Finally, we publish the time-stamped semi-automatically annotated dataset that we used for the eval-uation purposes. With this, we provide the essential means for further research in this area. Several ideas for further work are presented in Section 6.
Most of the content extraction algorithms, incl. the meth-ods presented in Section 1, extract content in an offline set-ting. By  X  X ffline setting X  we refer to extracting the relevant text from each HTML document separately, independently of any other acquired documents, except perhaps using a static labeled dataset in the model training phase. Such content extraction algorithms do not take into account that a subset of the previously acquired documents potentially shares the same template and thus the same boilerplate texts that are part of this template. Our approach makes use of the fact that documents are continuously being acquired by RSS readers, which generate sub-streams of documents with the same template. The template can thus be identified as a set of repeatedly observed text segments in a sub-stream and therefore separated from the main content. We base our work on the assumption that it is easier to distinguish between relevant and irrelevant content given the history of documents from the same source. This is analogous to re-touching a damaged movie frame as opposed to retouching a damaged photo: the first task is easier because fragments from the preceding frame can be used. The content extrac-tion process on a stream of HTML documents is illustrated in Figure 1.

In our workflow, HTML documents are acquired by RSS readers , 2 which periodically check the corresponding Web sites for the most recent RSS documents. An RSS document is essentially an XML containing titles and short descrip-tions (summaries) of a certain number of the most recent posts. Each RSS item also provides a link (i.e., URL) to the Web page containing the full content and the RSS reader is responsible for downloading the corresponding HTML doc-http://www.readability.com/developers/api/reader
In our case, an RSS reader is a software component de-signed to retrieve content updates from a particular Web site. RSS stands for Really Simple Syndication.
 Figure 1: The content extraction process on streams of Web documents. ument. In our case, one RSS reader is acquiring documents from one Web site (e.g., BBC at http://www.bbc.co.uk ) through one or more RSS feeds provided by the site (e.g., BBC lists its RSS feeds at http://www.bbc.co.uk/news/ 10628494 ). Each RSS reader keeps the history of acquired documents by representing each document with a hash code computed out of the document X  X  title, description, and pub-lication date as observed in the RSS document. With this, the RSS reader avoids downloading the same document mul-tiple times when processing subsequent RSS documents from the same site.

Multiple copies of the same document that were not iden-tified by this caching mechanism in the RSS readers are filtered out by the URL normalization component. This process is more thoroughly described in Section 3.
The HTML tokenization component removes HTML mark-up, CSS styles, and JavaScript code, and extracts text blocks from an HTML document. Text blocks are defined as chunks of text delimited with at least one (opening or closing) HTML tag. Certain inline HTML tags (such as em , strong , i , font , a , etc.) are ignored when partitioning textual content into text blocks.

The content and boilerplate of an HTML document are identified by employing the proposed URL Tree data struc-ture which is the core of our content extraction algorithm. This process is more thoroughly discussed in Section 4.
In the content extraction pipeline, news content is ac-quired from different Web sites (e.g., http://www.reuters. com , http://www.bbc.co.uk , etc.) through their RSS feeds. The RSS feeds provide references to the latest news arti-cles or blog posts (HTML documents) in the form of Uni-form Resource Locators (URLs). Based on the response URL, 3 a document belongs to a specific domain. By the domain of an HTML document, we refer to the combina-tion of the top level domain (TLD) and the second-level do-main name of the document X  X  (response) URL, e.g., news.eu would be the domain of the HTML document at http:// www.news.eu/politics/europe/article1.html . Note that documents acquired from the same Web site can in fact orig-inate from different domains.

Although collecting duplicate content from a single Web site is avoided by the caching mechanism in the RSS reader, some HTML documents from the same domain may still be acquired multiple times (within-domain duplicates). One
The response URL is obtained from a request URL after the redirections (if any) have been resolved. reason is that the same HTML documents are served over multiple RSS feeds covering different topics of interest (e.g., politics and economy). The caching mechanism in the RSS reader, relying on the metadata contained in an RSS XML document (in our case, document title, description, and pub-lication date), often receives different metadata from differ-ent RSS feeds even though they reference the same HTML document. Another reason is news aggregators that refer-ence documents from various different sites/domains (e.g., Google News collects news from BBC, CNN, and many other news sites). The same document can thus be acquired through an aggregator (e.g., by the Google News RSS reader) and through its publisher X  X  RSS feed (e.g., by the BBC RSS reader).

As already said, RSS feeds provide references to HTML documents in the form of URLs. This would seem to imply that within-domain duplicates can be easily avoided, sim-ply by observing these URLs. However, it turns out that a URL of an HTML document, in its raw form, is not a unique identifier of that document. Consequently, it cannot be used as a tool to properly detect and avoid duplicates in the data acquisition process, but it is nevertheless a good starting point. Understanding how URLs are composed and how they are used enables us to construct a  X  X early unique X  document identifier.

A URL is usually composed in the following way: protocol://domainname/path1/.../pathN/file?query
For example: http://www.ft.com/world/2013/02/news.html?feed=3
In the following, we present a URL normalization pro-cess which results in a URL-based nearly-unique identifier of an HTML document, called the URL key . Each step of the process decreases the differentiation between documents and thereby increases the ability to avoid collecting dupli-cated content. The complete URL normalization process is as follows (input: a request URL as given in an RSS XML document): 1. Follow the redirections to convert the request URL into the corresponding response URL.

Example request URL: http://news.google.com/news/url?sa=t&amp;fd=R&amp;usg=
AFQjCNHEIIAoeLGPfbkX6IdaQ2xoYptq-w&amp;url=http:// abcnews.go.com/kabc/story?section%3Dnews/local/ los_angeles%26id%3D8691010
The corresponding response URL: http://abcnews.go.com/kabc/story?section=news/ local/los_angeles&amp;id=8691010 2. Convert the title of the corresponding document, as given in the RSS XML document, into a 128-bit hash code and append it as a query parameter to the URL, specifically: and sort the query parameter names alphabetically in the query part.

Example (URL after Step 2): http://abcnews.go.com/kabc/story?_cid_=090 c0a49e55c9734bc2311e08ff006d4&amp;id=8691010 &amp;section=news/local/los_angeles 3. Normalize the query with the query normalization rules. For each rule in the rule list, check if it is applicable to the
URL. A rule is defined with a regular expression and a list of query parameters (usually just one) that need to be retained. Several examples are given in Table 1. A rule is applicable if the regular expression matches the URL.
In that case, the query parameters are filtered according to the rule. If none of the rules apply, the query part is dropped completely.

Example (URL after Step 3): http://abcnews.go.com/kabc/story?id=8691010 Regular expression (trigger) 4 abcnews \ .go \ .com id www \ .fitchratings \ .com.*?/detail \ .cfm pr id bbs \ .chinadaily \ .com \ .cn/viewthread \ .php tid www \ .hurriyetdailynews \ .com/n \ .php n globeandmail \ .golfcanada \ .ca articleId podcast \ .ft \ .com/index \ .php pid www \ .aljazeera \ .com( \ ?.*)?$ cid www \ .dailymail \ .co \ .uk/home/index \ .html cid www \ .foxnews \ .com/on-air.*?/index \ .html cid
The resulting URL (after Step 3) represents the corre-sponding URL key. In Figure 2, we show how URL normal-ization contributes to the identification of duplicates, solely by comparing URL keys of documents. The presented re-sults were computed on 569,583 documents collected by our data acquisition pipeline described in Section 2. The doc-uments were collected from 31 Web sites over a period of eight weeks (Oct 24  X  Dec 19, 2011). Figure 2: Number of identified within-domain du-plicates after each step of the URL normalization process.

The results show that a significant amount of documents (almost 50%) can be filtered out in the data acquisition pro-cess if we apply URL normalization. The removal of within-domain duplicates is essential for content extraction with a URL Tree as it prevents perceiving content blocks as boiler-plate.
Due to space limitation, the start of each regular expression  X  X ttp:// X  is omitted.
In our approach, we assume that each HTML document with a unique URL key represents a unique document. Our content extraction algorithm is based on the observation that documents from the same domain, whose URL keys differ only in the document identifier, have a lot of boiler-plate in common. To determine which text blocks are boil-erplate, we count their occurrences in the stream. We store the occurrence counts in the URL Tree structure. When a new document arrives in the stream and passes the dupli-cate removal filter, its URL key is mapped to a branch in a URL Tree. If the branch does not yet exist in the tree, it is created. The nodes in the branch (i.e. parts of the URL key) hold statistics about the text blocks extracted from the document.

For a newly observed document, each node in the branch is updated as follows: (i) the number of observed documents ( n s ) is increased by 1, (ii) each text block is normalized (all non-alphabetic characters are removed from the string, and the string is made all-lowercase) and converted into an MD5 hash code (a set of unique text block hash codes B = { b i is used from here on), and (iii) the counter for each of the text blocks in B (let us denote it with c( b i )) is increased by 1. The URL Tree construction process is illustrated in Figure 3.

After the URL Tree has  X  X een X  enough documents at a spe-cific node, that node can be used as a classification model to distinguish between content and boilerplate. The classi-fication process can employ different heuristics. The default heuristic is as follows (input: a document and its URL key):  X  The leaf corresponding to the URL key is identified in the
URL Tree (note that the leaf always exists because the document is inserted into the tree before its text blocks are classified) and selected for the classification process.  X  If the number of observed documents (i.e., support) in the selected node is less than the predefined threshold, i.e., n s &lt; n min , the tree is traversed towards the root until a node satisfying n s  X  n min is found and selected. If such a node cannot be found, the root node is selected.  X  For each text block extracted from the document, the statistics at the selected node are examined to determine whether the text block is content or boilerplate as follows:
Unlike the content extraction algorithms that work in of-fline settings, our URL Tree-based algorithm is designed to work efficiently on real-time streams of Web documents and to benefit from the continuous inflow of new (evolving) data. In our experiments, we used a time-stamped dataset of Web documents and compared the performance of our URL Tree-based algorithm to 10 different content extrac-tion algorithms developed within four open source projects.
The experiments were performed on a stream of HTML documents acquired from 31 Web sites over the period from Oct 24 to Dec 19, 2011. The initial stream of 569,583 documents was reduced to 292,053 documents after within-domain duplicate removal (URL normalization) as presented in Section 3. This dataset is a part of the data acquired dur-ing the European project FIRST. 5
A total of 56,436 documents, sampled from the beginning (Oct 24, 2011  X  Oct 31, 2011), the middle (Nov 10, 2011  X  Nov 30, 2011) and the end of the stream (Dec 10, 2011  X  Dec 19, 2011), were annotated with manually designed regular expressions tailored for specific Web site templates. The annotated dataset is available for download and preview at http://first.ijs.si/urltreedataset .
We measured the performance of content extraction with basic measures from information retrieval, namely recall ( R ), precision ( P ), and their harmonic mean, the F 1 -score. We computed these measures for each annotated document as follows:
The quantity of  X  X xtracted relevant text X  in Eq. 1 and 2 was computed with DiffLib , 6 a software library that enabled us to compute the size of the intersections between the text labeled as content in the annotated dataset and the different text outputs produced by the employed algorithms. Each text is transformed into a sequence of words/tokens and the length of the longest common subsequence is the length of the extracted relevant text.

In the stream setting, we present the performance of an algorithm as its overall performance until and including the currently processed document j (also referred to as the cu-mulative moving average): http://www.project-first.eu http://difflib.codeplex.com
We evaluated 10 different algorithms from four open source projects and compared their performance with the proposed URL Tree algorithm. The selected algorithms are listed in Table 2.
 Algorithm Description Boilerpipe 7 [4] -DE The default extractor based on a decision -AE An extractor tuned towards news articles. -ASE Extracts only whole sentences. -LCE Extracts the largest text block. -NWRE An extractor based on predefined word-jusText 8 [7] Designed to preserve mainly text contain-
NCleaner 9 [3] -default Uses character-level n -gram models as -non-lexical Relies on non-lexical text features.
Readability 10 -.NET A .NET (C#) port of Readability. -Python A Python port of Readability.
To extract content with URL Tree, we used four different classification heuristics: Strict: The default heuristic (discussed in Section 4) with Strict-support-N : Similar to the strict heuristic, except Strict-at-domain: The strict heuristic, always applied at Relaxed-at-domain-N : The strict heuristic with an addi-
We conducted two separate experiments. In the first ex-periment we streamed our dataset of 292,053 HTML docu-ments (see Section 5.1) through the URL Tree content ex-tractor employing the strict classification heuristic. We mea-sured F 1 at each of the 56,436 annotated documents (i.e., gold standard), for each of the 33 domains separately. From the results presented in Figure 4 we see that the performance of URL Tree, as expected, increased over time. Furthermore, for the majority of the domains, URL Tree achieved good F https://code.google.com/p/boilerpipe https://code.google.com/p/justext http://sourceforge.net/projects/webascorpus/ files/NCleaner/NCleaner-1.0 http://code.google.com/p/nreadability , https://github.com/buriy/python-readability scores already after 10 to 100 documents from an individual domain. F 1 scores higher than 0 . 7 were observed for all the domains, scores higher than 0 . 84 for three quarters of the domains, and for more than a third of the domains, the F 1 scores exceeded 0 . 9. The best scores were achieved for the domains chosun.com , usatoday.com , and newyorker.com , whereas the lowest scores were obtained for the domains abcnews.go.com , cbsnews.com , and foxnewsinsider.com . Cumulative 
F Figure 4: URL Tree content extraction evaluation results for each domain separately.

In the second experiment, we compared the performance of URL Tree with that of the 10 selected open source content extractors (see Section 5.3). We employed the four heuris-tics discussed in Section 5.4. The first 35,321 documents (which included the first 10,000 gold-standard documents) were used as the tuning set to select the best value for N for the two parametrized heuristics. We varied N from 100 to 1,000 (by 100). According to our observation on the tun-ing set, N was set to 100 for the strict-support-N heuristic (termed strict-support-100 ) and to 500 for the relaxed-at-domain-N heuristic (termed relaxed-at-domain-500 ). With these settings, the experiment was re-run on the rest of the stream (i.e., the test set).
 The results are given in Figure 5. The figure shows the F 1 scores aggregated over all the domains. The best per-forming open source algorithms, in our specific experimen-tal setting, were the two Readability implementations ( F of approximately 0.84), followed by the five different flavors of Boilerpipe ( F 1 ranging from 0.76 to 0.81).

We can see that URL Tree outperformed the other algo-rithms relatively early in the process. The F 1 score of the strict heuristic reached over 0.85 after about 4,000 docu-ments (which corresponded to around 1,000 gold-standard documents) and remained over this value throughout the rest of the stream. It also proved beneficial to experiment with several different heuristics. The heuristics strict-sup-port-100 and strict-at-domain both outperformed the strict heuristic and exhibited comparable results with F 1 scores around 0.86. Furthermore, the relaxed-at-domain-500 heu-ristic clearly outperformed the other three heuristics, with an F 1 score of about 0.89.
In this paper, we discussed a URL Tree-based approach to content extraction from streams of HTML documents. The presented approach is efficient (in terms of process-ing speed), unsupervised, language-independent, and out-performs the other algorithms we evaluated in our RSS data acquisition setting. However, it has some drawbacks as well. One of its weaknesses is that it cannot be applied to small, diverse datasets of HTML pages. It is specifically designed to work in scenarios such as our RSS data acquisi-tion pipeline. It could potentially also be applied to crawling large and dynamic Web sites. In this case the URL normal-ization rules would need to be changed in order to get the value for the cid parameter from the HTML header rather than the RSS metadata. Another thing to point out is the memory consumption. Our implementation of URL Tree consumes around 6,5 MB of RAM for every 1,000 inserted documents. Even though this aspect was completely ignored in this paper, it needs to be handled in real-life applications.
We have employed a URL Tree-based content extractor in a data acquisition pipeline running continuously since April 2011. So far, we have collected and preprocessed over 15 million unique documents from 175 Web sites providing al-together around 2,500 RSS feeds. Let us also briefly note that the stream of plain text that the pipeline produces is used as an input for a range of analytical components that look for correlations between occurrences of extracted enti-ties and different financial indicators such as credit default swaps and stock market indices. To handle the memory con-sumption issue, the employed instance of URL Tree holds a maximum of 10,000 documents for each domain. In addi-tion, it removes documents older than 14 days (however, it always keeps at least the 100 most recent documents). This mechanism was defined rather ad-hoc and we plan to in-vestigate the impact of different strategies on the accuracy more thoroughly. Our preliminary results show that remov-ing outdated documents from the tree does not affect the accuracy.

In future work, we plan to explore how URL Tree and several other content extractors perform on different types of boilerplate (advertisements vs. menu items vs. copyright notices etc.). Furthermore, we plan to explore whether it would be better to use a similarity hashing scheme rather than representing text blocks with MD5 hash codes. Last but not least, we plan to explore the possibility of updating the URL normalization rule list in a semi-or fully automatic manner.
We would like to thank Marko Brakus for creating the ini-tial annotated dataset. This work was partially funded by the Slovenian Research Agency and by the European Com-mission in the context of the FP7 projects FIRST and FOC, under the grant agreements no. 257928 and 255987, respec-tively. [1] Z. Bar-Yossef and S. Rajagopalan. Template detection [2] L. Chen, S. Ye, and X. Li. Template detection for large [3] S. Evert. A lightweight and efficient tool for cleaning [4] C. Kohlsch  X  utter, P. Fankhauser, and W. Nejdl. [5] T. Kova X ci X c. Evaluating Web Content Extraction [6] S.-H. Lin and J.-M. Ho. Discovering informative [7] J. Pomik  X alek. Removing Boilerplate and Duplicate [8] K. Vieira, A. S. da Silva, N. Pinto, E. S. de Moura,
