 Real-time monitoring over data streams have been attracting much attention. Anomaly detection aims to d etect the points which are significantly different from others, or those which cause dramatic change in the distribution of under-lying data stream. It is a great challenge to accurately monitor and detect the anomalies in real time because of the uncer tainty of properties of the anomaly.
In general, existing anomaly detectio n methods detect abnormality based on a model derived from normal historical behavior of a data stream. They try to reveal the differences over short-term or long-term behaviors that are inconsis-tent with the derived model. However, the detected results largely depend on the model defined, as well as the data distribution and time granularity of the under-lying data stream. For the data stream whose distribution changes constantly, the existing methods will result in a large volume of false positives. On the other hand, due to the application-dependent definition of abnormal behavior, each existing method can only d etect certain types of abnormal behaviors. In real situation, it is meaningful to define abnormal behavior in a more general sense, and is highly desirable to have an efficient, accurate, and scalable mechanism for detecting abnormal behaviors over dynamic data streams.

Self-similarity is ubiquitous both in natural phenomena and social events. It represents the intrinsic nature of the data. Abnormal behavior defined based on the change of self-similarity naturally captures the essence of the event. Fractals, which are built upon the statistical self-similarity, have been found with wide variety of applications [14]. In this paper, we focus on modeling data using fractals based on recurrent iter ated function systems (RIFS).

Most existing data stream algorithms are proposed for specific monitoring tasks [10,16,19,5,15].

In [20], the authors use a Shifted Wavelet Tree (SWT) to detect bursts, which is an important kind of anomalies, on multiple windows. A more efficient aggre-gate monitoring method is put forward in [4]. However, they can only monitor monotonic aggregates, and both the maximum window size and the number of monitored windows are limited. The moving average, or Kalman-like filter [9] are also important measures to be monitored for anomaly detection. An adaptive change detecting method for such measurements, based on Inverted Histogram, is proposed in [19]. However, no any ov erall bound for the error is given.
Monitoring abnormal behaviors in the network traffic systems has been stud-ied in [7,11,10]. They can monitor the changes of the behavior on only one gran-ularity. The detection of trends and surprise in time series database is studied in [17]. In [6], the exception of trend is defined and detected.

Fractal geometry has been known to be a powerful tool to describe complex shapes in nature [12]. It has been successfully applied to modeling and com-pressing the data sets [18,13,14]. However, it is difficult to apply them directly in the data stream scenario, since the time complexity of constructing the fractal model is polynomial, while multi-pass scan of the whole data is required.
In this paper, we propose a fractal-based novel approach to detect anomaly in data streams. Our contributions are summarized below.  X  We propose a general-purpose and adaptable definition for abnormal behav- X  We introduce the fractal analysis based on self-similarity into anomaly detec- X  Both the theoretical analyses and experimental results illustrate that the If a quantitative property q is measured on a time scale s , then the value of q depends on s according to the q = ps r scaling relationship [12]. This is called power law scaling relationship of q with respect to s . p is a factor of proportion-ality and r is the scaling exponent. The value of r can be determined by the slope of the linear least squares fitting to the graph of data pairs (log s ,log q ): log q =log p + r log s . For discrete time series data, the power-law scaling rela-tionship can be transformed to q ( st )= s r q ( t ), where t is the basic time unit and
The power-law scaling relationship is also retained when q is a first order aggregate function (e.g. sum , count ) or some second order statistic variables (e.g. variance ).

Intuitively, objects satisfying self-si milarity property can be mapped (by a se-ries of maps) to part of itself. This mapping process can be conducted iteratively until a fixed point called attractor , is found. Given the attractor. by iteratively applying the inverse of the mapping, we can approximate the original objects. The mapping process introduced above is mathematically called the iterated function system (IFS).

More precisely, an IFS consists of a finite collection of contraction maps M i ( i =1 , 2 , 3 , ..., m ). Each M i can map a compact metric space onto itself, provided that the metric space is linear. A point ( x, y ) can be mapped/transformed by M i [ x, y ]withtheform M i
Assume A is the attractor, then A = m i =1 M i ( A ). By modeling the data with an IFS, we can use the attractor of the IFS to approximate the data. When a i 12 is zero, the mapping is called shear transformation [14]. Then, a i 22 &lt; 1 is called the contraction factor for map M i .

IFS can only be used to manipulate linear fractals, which have a fixed contrac-tion factor for all objects in all scales . However, the natural objects may be more complex. Thus recurrent iterated functi on systems (RIFS) are introduced. RIFS can be generalized for arbitrary metric sp aces [2]. Intuitively, different from IFS, in RIFS each map can be applied on part of the objects. The ranges of objects the maps can be applied on may overlap. Thus, different objects may be mapped to different parts and with different contraction scale.

In RIFS, given the contraction maps M i , the attractor A  X  R d is N i =1 A i where A i  X  R d ( i =1 , ..., N ) are possibly overlapping partitions. Further-more, M j ( A )= A j = &lt;i,j&gt;  X  G M j ( A i ). Here G is a directed graph, and A =( A 1 , ..., A N )  X  ( R d ) N .Eachedge &lt;i,j&gt;  X  G indicates that the map-ping composition M j , M i is allowed. The attractor can also be represented by A = M ( A ), in which M :( R d ) N  X  ( R d ) N is defined as ( M 1 ,M 2 , ...M N ). Thus, given a set vector D =( D 1 ,D 2 , ..., D N ) with D i  X  R d , A = lim i  X  X  X  M i ( D ). Similar to IFS, attractor A can be used to approximate the data. When the graph G is complete, the RIFS degrades to an IFS [8].

IFS and RIFS are often used to summari ze and compress fractal data. The problem lies in the determination of the attractor. Since the attractor can be computed given the mapping, the computation of the parameters of the mapping is usually called the inverse problem of IFS or RIFS [8].

Given the mapping M , D = M ( D ), we consider the condition that D  X  D in this paper. Under this condition, the open set property holds, so that the Collage Theorem can be applied to guarantee the precision of approximating the original data using the attractor [14,2]. This theorem provides a way to measure the goodness of fit of the attractor associated with an RIFS and a given function.
 Collage Theorem :Let( X, d ) be a complete metric space where d is a distance measure. Let D be a given function and &gt; 0 be given. Choose an RIFS, with . Then d ( D ,A )  X  1  X  s where A is the attractor of the RIFS.

Since no assumption that the data streams to be monitored are linear fractals can be made, we consider RIFS in the rest of this paper. Adatastream X is considered as a sequence of points x 1 , ..., x n .Eachelement in the stream can be a nonnegative value, which is denoted as a ( timestamp : i, value : y i )pairor i for simplicity. Function F being monitored can be not only the monotonic aggregates with respect to the window size, for example sum and count , but also the non-monotonic ones such as average and variance .One common property of the above functions is that they all retain the power-law scaling relationship [16].

Monitoring anomalies in data strea m is to detect the change of power-law scaling relationship, i.e., self-similarity, on the sequences with continuous incom-ing new value x i . Under such a data stream model, the problem about anomaly detection can be described as follows.
 PROBLEM STATEMENT :Adatastream X is represented as a sequence of points x 1 , ..., x n . An anomaly is detected if the self-similarity of X changes (i.e., the historic one is violated) when new value x n comes.

Our basic idea is to use the attractor to represent the data stream, and moni-tor events that change the attractor, which mean anomalies. Thus, the problem lies in  X  X ow to efficiently estimate the attractor in a streaming data environ-ment? X  ,  X  X ow good is it to use estimated attractor to approximate the original data stream? X  ,and  X  X ow to modeling the anomaly based on the attractor? X  . Constructing a piecewise fractal model is e quivalent to solving a inverse problem in data stream scenario. We investigate two approaches. One is L 2 error optimal, whose complexity is O ( n 2 )intimeand O ( n ) in space. The other is an approx-imate method with at most (1 + ) times the error for the optimal solution. It uses O ( n )timeand O ( m ) space, where m is the number of pieces needed, and m&lt; log n . For ease of understanding, we list the notations used in Figure 1. 4.1 Inverse Problem RIFS maps a partition D i of the whole metric space D to another partition D i under two constraints. The first constraint is: D i = M i ( D i ) ,D i  X  D i .
The second constraint which needs to be maintained in RIFS is that every transforms a larger interval D i of stream X to a smaller interval D i of the same stream. Thus the endpoints of D i are also the endpoints of the attractor of M i , that is, M i
Once the contraction factor a i 22 for each mapping is determined, the remaining parameters can be obtained using the endpoint constraint (4.1) as follows. The contraction mapping M i denotes the similarity between D i and D i .Pro-vided that D i is a subset of D i ,the M i denotes the self-similarity between D i and D i . With the self-similarity/ M i , we can reconstruct D i from D i .Moreim-portantly, we alarm anomaly when the self-similarity/ M i of the latest partition D i is violated by newly arrived data point . Piecewise fractal model is used to maintain such contraction mappings (self-similarity) over data streams.
Now the problem of constructing the pi ecewise fractal model for data streams is reduced to determining the appropriate value of contraction factor a i 22 . 4.2 Piecewise Fractal Model In a piecewise fractal model, the goal of the global optimal solution is to minimize the sum of error of each piece.
 Optimal Model. Let the start point of a piece P of stream X be ( s, y s ), and the end point of P be ( e, y e ), e&gt;s . A contraction mapping M can be defined piece of a data stream are mapped by M from P { ( s, y s ) , ( e, y e ) } to a smaller piece of the same stream P { ( s ,y s ) , ( e ,y e ) } ,where P  X  P and e  X  s&gt;e  X  s . Every two adjacent P s meet only at the endpoints and do not overlap. For all pieces of the whole data stream, how ever, there might be some overlaps, that is, P i = P i = X . Suppose the L 2 error between the data points in mapping M ( P ) and those in the original piece P is E ( M )=( M ( P )  X  P ) 2 , where j = -i  X  a 11 + b 1 . .Given P and P , the optimal mapping M opt is the one such that the minimum value of E ( M ) is reached.

For data stream X , it is ensured that each contraction mapping M maps the start point and end point of piece P to corresponding ones of piece P ,namely, ( s ,y s )= M ( s, y s )and( e ,y e )= M ( e, y e ). a 21 and b 2 can be obtained by equations 2 and 3. Therefore, we have E ( M )= e i = s ( A i a 22  X  B i ) 2 , in which A
The coefficient a 22 of the optimal mapping M opt can be computed using quently, the optimal contraction mapping M opt can be obtained by maintaining  X  for constructing the optimal piecew ise fractal model for a data stream. Theorem 1. Assume the length of a data stream is n . Algorithm 1 maintains optimal piecewise fractal model with O ( n + m ) space in O ( n 2 ) time. Approximate Model. It is impossible to store the whole data stream in mem-ory. Thus we cannot maintain the value of  X  e i = s A i B i in the data stream scenario, as Algorithm 1 requires. To address this problem, we construct a approximate piecewise fractal model to com pute the approximate value of  X  e i = s A i B i .
Denote the approximate value of a 22 to be a 22 .Wehave1) C i = y i  X  ( e  X  i e  X  s y s + on the data stream. In this way, the a 22 and corresponding approximate results M app can be computed.

The approximate piecewise f ractal model consists of m pieces which can be used to reconstruct the original data stream. The error of reconstruction is bounded by Collage Theorem. Each piece P i corresponds to a contraction map-ping M i and a pair of partition P and P . One value needed to be stored for P i is the suffix sum F ( n  X  si ) starting from ( si ,y si ), where a suffix sum of data stream is defined as F ( w i )=  X  n j = n  X  wi +1 x j . The other value needed to be stored for P i is the number of points included in this suffix sum. We first try to add each data point of the evolving stream into the current piece. If failed, then a new piece is created for this point. Because the end point of piece P i is the start point of piece P i +1 , it is not necessary to store the end point. The method of con-structing an approximate p iecewise fractal model for a data stream is described in Algorithm 2.
 Theorem 2. Algorithm 2 can maintain the approximate piecewise fractal model for a data stream with O ( m ) space in O ( n ) time. 4.3 Error Bound Theorem 3. Assume Algorithm 2 produces an approximate solution M app which maps the partition P to P with error E ( M app ) , and the optimal method maps the partition P to P with error E ( M opt ) ,then E ( M app )  X  2 E ( M opt ) . The error bound of Algorithm 2 to the optimal result is shown in Theorem 3. The proof is omitted due to space limitation. For some applications in which the bounded absolute error is wanted, we provide a parameter  X  to control the error. Then the error of mapping M app on partitions P and P can be guaranteed: E ( M app )  X   X   X  E ( P LSF ), where E ( P LSF ) is the Least Linear Fitting error on partition P . Therefore, the error produced by approximate piecewise fractal model on partition P is less than the smaller value of  X E ( P LSF )and2 E ( M opt ), which is E ( M app )  X  min {  X E ( P LSF ) , 2 E ( M opt ) } .

To control the parameter  X  , we only need to check whether E ( M app )  X   X E ( P LSF ) holds or not when a new x n comes. If E ( M app )  X   X E ( P LSF )holds, x n is put into the current partition; otherwise, a new partition is created for x n . The rest is the same as that of Algorithm 2. 5.1 History-Based Model The history-based model trains a model incrementa lly, and compares the new coming data against the model. The data that deviate from the model are re-ported as anomalies. Existing burst detection [20,4,16] and change detection [10] methods conform to this model. For instance, the threshold of bursty behaviors of aggregate function F on i -th window is set as follows. At first, the moving F ( w i ) is computed over some training data which could be the foremost 1% of the data set. The training data forms another time series data set, say, Y .Then the threshold of anomaly is set to be T ( w i )=  X  y +  X  X  y ,where  X  y and  X  y are the mean and standard deviation, respectively. The threshold can be tuned by varying the coefficient  X  of standard deviation. These methods then look for the significant difference in short-term or long-term behaviors which are inconsistent with the model.

To maintain the model, piecewise fractal model is used. For each piece, the line connecting end points of a piece is used to approximate the data within this piece. The error is denoted by E ( M ) and is guaranteed. The data generated by all pieces are summed up to approximate t he original data stream. Thus, the piecewise fractal model is used a s a synopsis data structure. 5.2 Self-similarity-Based Model It is natural for us to investigate the another mechanism for detecting abnormal behaviors in the data streams. The behaviors observed by the mechanism on the data streams can be depicted with the equation: Y = X + e ,where Y is either an aggregate function(e.g., sum, count, average ), measurement of trend and deviant (say, variance ), or distance of two distributions on the data stream. X is the result of Y in normal condition. e , the effect caused by abnormal behaviors on Y , can be seen as a noise. We assume that X and e have their own distributions and the uncertainty noise e is white Gaussian [9].

In the scenario of discrete data points, Gaussian white noise can only be modelled and processed in time series data. The constraints imposed on the data stream processing include limited memory consumption, sequentially lin-ear scanning, and on-line accurate result reporting over evolving data streams. Therefore, we build piecewise fractal mo del on the data streams to model the change of self-similarity induced by white Gaussian noise.

The occurrence of abnormal behaviors is rooted in the change of self-similarity in the data stream. The piecewise fractal model can find the pieces with self-similarity in a data stream. That a new piece is created means there is a change in current self-similarity. When a new piece is created for the incoming x n ,an alarm is reported on the anomaly of x n . If the bounded absolute error is wanted, a parameter  X  can be used in the algorithm to control the error.
 With this model, we can evaluate both the signal and noise in the data stream. Since the anomaly detection only focuses on the prominent noise which causes the change of self-similarity, the evaluation of anomaly on e is not affected by the evolving of normal signal X . 6.1 Experimental Setup The experiments are conducted on a platform with a 2.4GHz CPU, 512MB main memory on Windows 2000. We applied our algorithms to a variety of data sets. Due to the space limitation, only results on three real-life datasets are reported: 1) Stocks Exchange (D1) : This data set contains one year tick-by-tick trading records in 1998 from a corporation on Hong Kong main board. 2) Network Traffic (D2) : This data set BC-Oct89Ext4 is a network traffic tracing data set obtained from the Internet Traffic Archive [1,7]. 3) WebSiteRequests(D3) : It consists of all requests made to the 1998 World Cup Web site between April 26, 1998 and July 26, 1998 [1].

Two accuracy metrics, recall and precisi on, are considered. Recall is the ratio of true alarms raised to the total true alarms which should be raised; precision is the ratio of true alarms raised to the total alarms raised. 6.2 Experimental Results Detecting Anomalies in History-Based Model. The piecewise fractal model can be applied to reconstruct data and detect history-based anomalies defined by existing methods [20,4,16]. Here we present the results of comparison tests with these methods.

To set the threshold for F ( w i )on i -th window w i , we compute moving F ( w i ) over some training data. The training data is the foremost 1% of each data set. It forms another time series data set, called Y . The absolute threshold is set to be T ( w i )=  X  y +  X  X  y . The length of windows is 4  X  NW time units, where NW is the number of windows ranging from 50 to 1100. Similar testing results can be got from all the data sets.

Given NW = 70, and m =8,  X  being 7 (D1) or 9 (D3), Figure 3 shows that under any setting of  X  the precision and recall of our method are always above 97%. Larger  X  means larger threshold. For our method can model such significant differences more accurate with self-similar pieces, it can be concluded that with the increasing value of  X  , our method can guarantee better accuracy for detecting history -based anomalies.

While fixing the threshold parameter  X  of piecewise fractal model (7 on D1 and 5 on D3), Figure 4 shows that when the value of  X  increases, the precision and recall also increase in certain domain. When  X  exceeds certain value, both precision and recall decrease.  X  cannot be too small, because smaller  X  means a piece can only store less data points, h ence the length of the overlapping pieces for the piecewise fractal models becomes shorter when the number of pieces m is fixed. Thus the estimated aggregate value of larger windows is more error prone, which leads to the increase of the error of anomaly detection. On the other hand, it is easy to understand that larger  X  results in longer pieces, therefore larger reconstruction error.

Given  X  =5and  X  = 9, with the increase of number of monitored windows, the number of used pieces also increases. But the number of consumed pieces can still be guaranteed by m&lt; log 2 4  X  NW . It can be seen from Figure 5 that with only a few pieces the precision and recall are st ill high. The accuracy becomes better when NW increases. This is because the av erage size of monitored windows is larger with the increase of NW . The fractal approximation is more accurate over large time scales. So the error decreases with the increasing value of NW .This illustrate our algorithm X  X  ability to monitor large amounts of windows with high accuracy over st reaming data.
 Anomaly Detection in Self-similarity-Based Model. In Figure 7 ( m = 1 , X  =  X  ) (a) and (b), the solid red points are anomalies identified by our algorithm on two pieces of data stream s D1 and D3. Both the precision and recall are 100%. Such significant difference depicts the abnormal behavior in stock exchanging and web site requesting.

In this experiment, we also compare the self-similarity-based method with existing history-based anomaly detection methods [20,4,16]. Since they have the same definition on anomalies, we choose [20] to represent the rest. When de-tecting anomaly on data streams, existing methods will return large number of false positive results. In the experiment, we set NW = 100. It can be seen from Figure 6 that SWT returns large number of false positive results in every case. For example, in Figure 6.(b), 10 8 alarms are returned, which is definitely not informative to the users. Moreover, it is difficult to set the threshold for anoma-lies. A slightly smaller or larger  X  can either boost false alarms or false positive, respectively. Therefore, our self-simila rity-based method is more adaptable for providing accurate alarms in anomaly detection.
 Space and Time Efficiency. To show the time and space efficiency of our algorithm, we compare our method with Stardust [4] and the query-based method [16]. It can be observed from Figures 8 and 9 that our method uses much less memory, runs more than tens of times faster than other methods. We use the code provided by the authors of [4] and [16] to do the comparing test. The rest of setting is the same as before, except for the special setting for Stardust with box capacity c =2.

Figure 8 shows the time efficiency testing results. With the increasing of the number of monitored windows, the processing time saved by using self-similarity-based algorithm is getting larger and larger. Our method can process 400  X  10 7 tuples in 10 seconds. This means that it is capable of processing traffic rates on 100Mbs links, and with some work than 1Gbps or even higher are within reach.
Figure 9 shows the space efficiency testing results. The space cost of our piece-wise fractal model is only affected by the number of pieces maintained. However, Stardust has to maintain all the monitored data points and the index structure at the same time. The query-based method has to maintain an inverted histogram (IH) in memory. It can be seen that the space saved by the piecewise fractal model is considerable. Thus, our method is more suitable to detect anomalies over streaming data. In this paper, we incorporate the fractal analysis technique into anomaly de-tection. Based on the piecewise fractal model, we propose a novel method for detecting anomalies over a data stream. Fractal analysis is employed to model the self-similar pieces in the original d ata stream. We show that this fractal-based method can be used to detect not only the bursts defined before, but more general anomalies that cause the fra ctal characteristics change. Piecewise fractal model is proved to provide accurate monitoring results, while consuming only limited storage space and computatio n time. Both theoretical and empirical results show the high accuracy and efficiency of the proposed method. Further-more, this approach can also be used to reconstruct the original data stream with the error guaranteed.
 Acknowledgments. This work is partially supported by National Science Foun-dation of China under grant numbers 60925008 and 61170086.

