 This paper presents a new way of thinking for IR metric optimization. It is argued that the optimal ranking prob-lem should be factorized into two distinct yet interrelated stages: the relevance prediction stage and ranking decisio n stage. During retrieval the relevance of documents is not known a priori, and the joint probability of relevance is used to measure the uncertainty of documents X  relevance in the collection as a whole. The resulting optimization objectiv e function in the latter stage is, thus, the expected value of the IR metric with respect to this probability measure of rel -evance. Through statistically analyzing the expected valu es of IR metrics under such uncertainty, we discover and ex-plain some interesting properties of IR metrics that have no t been known before. Our analysis and optimization frame-work do not assume a particular (relevance) retrieval model and metric, making it applicable to many existing IR mod-els and metrics. The experiments on one of resulting ap-plications have demonstrated its significance in adapting t o various IR metrics.
 H.3 [ Information Storage and Retrieval ]: H3.1Content analysis and Indexing; H.3.3 Information Search and Re-trieval Algorithms, Experimentation, Measurement, Performance
In Information Retrieval Modelling, the main efforts have been devoted to, for a specific information need (query), automatically scoring individual documents with respect t o their relevance states. Representative examples include t he Probabilistic Indexing model that studies how likely a quer y term is assigned to a relevant document [17], the RSJ model that derives a scoring function on the basis of the log-ratio of probability of relevance [20], to name just a few. And yet, given the fact that in many practical situations relevance i n-formation is not steadily available, major developments ha ve shifted their focus to estimating text statistics in the doc -uments and queries and then building up the link through these statistics[12, 21, 34]. For example, scoring functio ns such as TF  X  IDF, Vector Space Model, and the Divergence from Randomness (DFR) model [1] have been developed [16]. A practical approximation of the RSJ model led to the popular BM25 scoring function [21]. Another direction in probabilistic modelling was to build a  X  X anguage model X  of a document and assess its likelihood of generating a given query [34]; a query language model is also covered under the Kullback-Leibler divergence based loss function [15].
Despite the efforts for retrieval, when in the evaluation phase, many IR tasks have evaluation criteria that go be-yond simply counting the number of relevant documents in a ranked list. Measuring IR effectiveness by different met-rics is critical because, for different retrieval goals, we n eed to capture different aspects of retrieval performance. In the case where the preference goes strongly towards early-retrieved documents, MRR (Mean Reciprocal Rank) is a good measure [28], whereas if we try to capture a broader summary of retrieval performance, MAP (Mean Average Precision) becomes suitable [13]. Thus, there is a gap be-tween the underlying (ranking) decision process of retriev al models and the final evaluation criterion used to measure success in a task. Ideally, it is desirable to have retrieval systems adapted to the specific IR effectiveness metrics.
In fact, IR researchers have already started to explore the opportunity. One extreme case is learning to rank ; it di-rectly constructs a document ranking model from training data, bypassing the step of estimating the relevance states of individual documents [8]. Under this paradigm, some at-tempts have been made to directly optimizing IR metrics such as NDCG (Normalized Discounted Cumulated Gain) and MAP [23, 33]. However, it is known that some evalua-tion metrics are less informative than others [4]. As argued in [32], some IR metrics thus do not necessarily summarize the (training) data well; if we begin optimizing IR metrics right from the data, the statistics of the data may not be fully explored and utilized.

A somewhat opposite direction is to focus still on design-ing a scoring function of a document, but with the acknowl-edgement of various retrieval goals and the final rank con-text. The  X  X ess is more X  model proposed in [10] is one of the examples. By treating the previously retrieved documents as non-relevant when calculating the relevance of document s for the current rank position, the algorithm is shown to be equivalent to maximizing the Reciprocal Rank measure. In [35], a more general and flexible treatment in this direction is proposed. In the framework, Bayesian decision theory is applied to incorporate various ranking strategies throu gh predefined loss functions. Despite its generality, the resu lt-ing IR models, however, lack the ability of directly incorpo -rating IR metrics into the rank decision.

In this paper, we argue that regarding the retrieval task solely as either optimizing IR metrics or deriving a (rele-Figure 1: The two distinct stages in the statistical document ranking process. vance) scoring function presents a partial view of the under -lying problem; a more unified view is to divide the retrieval process into two distinct stages, namely relevance predict ion and ranking decision optimization stages, and solve them sequentially. In the first stage, the aim is to estimate the relevance of documents as accurate as possible, and sum-marize it by the joint probability of documents X  relevance. Only in the second stage is the rank preference specified, possibly by an IR metric. The rank decision making is a stochastic one due to the uncertainty about the relevance. As a result, the optimal ranking action is the one that max-imizes the expected value of the IR metric. We shall show that statistical analysis of the expected value of IR metric s gives insight into the properties of the metrics. One of the findings is that AP (Average Precision) encourages docu-ments whose relevance is positively correlated with previo us retrieved documents, while RR (Reciprocal Rank) does oth-erwise. It follows that if a rank achieves superior results o n AP, it must pay with inferiority on RR. Apart from a the-oretical contribution, our experiments on TREC data sets demonstrate the significance of our probabilistic framewor k.
The remainder of the paper is organized as follows. We first establish our optimization scheme, and study major expected IR metrics and practical issues. We then provide an empirical evaluation, and finally conclude our work.
In this section, we present the framework of optimizing IR metrics in the situation where the relevance of documents is unknown. To keep our discussion simple, we consider binary relevance, while graded relevance can be extended similarl y. Given an information need, let us assume each document in the corpus is either relevant or non-relevant. We denote them jointly as a vector r  X  ( r 1 , ..., r k , ..., r N )  X  X  0 , 1 } where k = { 1 , ..., N } , N denotes the number of documents. r = 1 if document k is relevant; otherwise r k =0.

Our view is the following: firstly the IR model should fo-cus on estimating the relevance of documents. The relevance in this stage is the X  X rue X  X opical relevance [18], different from the user  X  X erceived X  relevance that will be qualified in the next stage. In statistical modelling, we assign to every pos si-ble relevance state r a number p ( r | q ), which we interpret as the probability that a user, who issues query q , will find the documents X  relevance states as r . Given the observation so far (the query, the user X  X  interaction etc), the posterior p rob-ability p ( r | q ) presents our (or the IR model X  X ) belief about the relevance states of the documents in the collection as a whole. Note that we use the joint distribution of relevance instead of the marginal distribution p ( r k | q ) to cover the de-pendency of relevance among documents.

It is argued that only in the second stage does the re-trieval model make a ranking decision under the uncertainty specified by the joint probability of relevance. To formu-late this, we follow the terminology in natural language processing [6]; a ranking order is represented by a vector k is in rank position i , then a i = k . The retrieval task is, thus, to find an optimal rank order a to maximize a cer-tain retrieval objective. Formally, an IR metric (measure) m ( a | r ) is defined as a score function of a given r . A good metric should be able to measure the user X  X  gain or utility of a rank order a when the true relevance states of all the doc-uments, r , are known. m ( a | r ) can be also seen as a measure of the user X  X  perceived relevance in the context of a ranked list. For example, Precision concerns a solution that finds relevant documents as many as possible in the list regardles s of their order, while Reciprocal Rank (inverse of the rank of the first relevant document retrieved) makes sure to retriev e the first relevant document as early as possible regardless o f the rank positions of remaining relevant documents.
Given the fact that different IR effectiveness metrics are useful for capturing different aspects of retrieval quality , it is desirable to optimize a with respect to the specific metric m . Bayesian decision theory suggests that the optimal rank order  X a is obtained by maximizing the expected IR metric:  X a = argmax where E [  X | q ] denotes an expectation with respect to a con-ditional distribution p (  X | q ). The subscript r indicates it is averaged over all possible r . Eq. (1) shows that: firstly the true relevance state of documents, r , is generated from probability p ( r | q ) estimated by an IR model. Under the relevance state r , the score of a given rank order a is calcu-lated. E r [ m | q ], the expected score of the rank order, is the one averaging over all possible relevance states of r . Finally, the optimal rank order is chosen by maximizing E r [ m | q ].
Although the formulation can be thought of as a special instantiation of the general retrieval decision framework in [15, 35], our underlying idea and development are quite dif-ferent from their instantiated models. The advantage is tha t, as illustrated in Figure 1, in our framework, the IR metric (utility) relies only on the true relevance and ranking orde r, while (relevance) IR models are for estimating the relevanc e. Decoupling them is essential to directly use any retrieval metric and plug it into the optimization procedure. More discussion can be found in Section 4.
 To obtain Eq. (1), we analyze the expected IR metrics E [ m | q ] in Section 2.1 and present a practical implementa-tion and maximization (search) method in Section 2.2.
Average Precision (AP) is a widely-adopted metric. For each query, it is the average of the precision scores obtaine d across rank positions where each relevant document is re-trieved; relevant documents that are not retrieved receive a precision score of zero [7]. The metric, in fact, is the area u n-der the Precision-Recall curve, capturing a broad summary of retrieval performance with a single value [4].

By definition, the Average Precision measure is as follows: where M  X  N ( P i  X  1 j =1 r a j  X  0 when i =1). N R is the num-ber of relevant documents, and its expected value equals P i =1 p ( r a i = 1), the summation of the marginal probability of relevance. For simplicity, we define p ( r a i = 1)  X  p ( R in the remainder of the paper. Because during retrieval r is hidden, m A ( a | r ) cannot be calculated exactly. Instead, its expected value under the joint probability of relevance is derived by making use of the properties of expectation (Throughout this paper the expectation is all conditioned on a given query q and with respect to r . For simplicity, we drop the subscript r and notation q in E [  X  ] from now on): = X = X where Cov ( r a i , r a j | N R ) denotes the correlation between the relevance values of documents at rank i and j given the num-ber of relevant documents is N R . Eq. (3) shows that the ex-pected AP can be interpreted as: for the given query, an IR model first estimates the number of relevant documents in the collection, and then estimates the expected AP for that number of relevant documents. The final expected measure is the average, weighted by p ( N R | q ), across all the possible numbers of relevant documents.

We can obtain more insight into the expected AP by mak-ing a simple approximation to the average over N R . By assuming that the posterior distribution of N R is sharply peaked around the most probable value (the mode)  X  N R , we can use the mode to approximate the average [5]. This gives: E R [ m A ]  X  1  X  where E [ r a i ] = P r ability of a document X  X  relevance at rank i . Note that the equation removes the dependency of  X  N R because the con-ditional expectation and variance are well approximated by the non-conditional ones when P (  X  N R | q )  X  1. To simplify regarded as an adaptive weight of rank i .

The first term in this simple approximation indicates that the expected AP is a weighted average of the scores across all rank positions, and as we increase the marginal probabil -ity of relevance p ( R a i ) in the ranked list, the expected AP increases. Furthermore, because the weight ratio: is in the range between i 1+ i and 2 i 1+ i .The ratio is adaptive to the expected relevance (defined as P i  X  1 j =1 p ( R a so far. To get the insight into it, we approximate the weight by setting p ( R a i ) all equal to p ( r ). We plot the weight ra-tio against the marginal distribution p ( r ) and rank position i in Figure 2 (a). It illustrates that when we have more confidence about the relevance of the early retrieved docu-ments ( p ( r ) approaches one), the weight ratio becomes near one. As a result, the metric is less worried about the early retrieved documents, thus putting equal weights to the late r-retrieved documents. This is similar to the Precision metri c. But once less confident documents ( p ( r ) approaches zero) are retrieved, particularly in the top ranked positions, th e weight ratio approaches its lower bound i i +1 . As a conse-quence, the weight penalizes more the later-retrieved rele -vant documents, and the ratio of the expected AP behaves more like that of the expected DCG, which will be discussed later.

The second term in Eq. (4) indicates that a document will contribute more to the expected AP if its relevance is more positively correlated with those of previous retrieve d documents. The consequence is that it will push positively correlated documents up in the ranked list. This is an in-teresting finding because it shows that the expected AP is in fact nonlinear  X  it models well the dependencies between documents X  relevance and incorporates them in deciding the preferred rank order. The rational of encouraging positive ly correlated relevant documents is that if a document is rele-vant, it is likely that its positively correlated documents are also relevant. It theoretically explains why pseudo releva nce feedback, i.e., the top ranked documents are generally like ly to be relevant, and finding other documents similar to these top ranked ones helps improve MAP [24].
Discounted Cumulative Gain (DCG) is another pop-ular measure for ranking effectiveness, especially in web se arch. DCG measures the usefulness, or gain, of a document based on its (graded) relevance[14] (for the moment, let us con-sider r a i to cover the graded relevance too); the gain is ac-cumulated from the top of the result list to the bottom. To penalize late-retrieved relevant documents, the gain of ea ch result is discounted by a function of its rank position. By definition, we have the DCG measure as: where w D i is the discount weight for rank position i , and g ( r a i ) is a gain function mapping the relevance value to the retrieval gain. Unlike the expected AP, the expect DCG is linear with respect to rank positions. We thus have: Since g ( r a i ) is infinitely differentiable in the neighborhood of the mean of r a i , i.e.,  X  r a i  X  E [ r a i ], the mean of g ( r be represented by a Taylor power series as: The expected DCG is thus approximated by: where V AR ( r a i ) denotes the variance of r a i . Eq. (9) shows that the expected value of DCG is determined by both the mean and variance of the relevance of documents at rank positions from 1 to M . Whether it should add variance or minus variance depends on the sign of the second deriva-tive of the gain function. In the case of graded relevance, if consider highly relevant documents more valuable than marginally relevant documents and give them more gain, we can then use a gain function like g ( r a i ) = 2 r a i  X  1. In this case, we need to add variance.

It is shown that when w D 1 &gt; w D 2 ... &gt; w D M , the docu-ment with the highest score of g ( X  r a i )+ 1 2 g  X  X  ( X  r retrieved first, the document with the next highest score is retrieved second, and so on. It is common to define w i  X  1 log 2 ( i +1) . Compared to the adaptive weight in the expected AP, it penalizes more the late-retrieved relevant documents. Figure 2 (c) compares their weight ratios.
Precision at M is a special case of DCG, where the discount is a constant and the gain function is linear. Thus, the expected Precision measure is
In the cases like web search and question answering tasks, we quite often expect a relevant document to be retrieved as early as possible [10, 28]. Expected Search Length and Reciprocal Rank (RR) are strongly biased towards early-retrieved documents. This section analyzes RR, while Ex-pected Search Length can be derived similarly. RR is the in-verse of the rank of the first relevant document and bounded between 0 and 1. It is formally defined as: where we define v i  X  Q i  X  1 j =1 (1  X  r a j ), a function of the rel-evance values of documents ranked above i ; ( v i  X  1 when i = 1). Conceptually, RR measure can be thought of as a weighted average of relevance values at different rank po-sitions, where the weights are adaptive to earlier retrieve d documents.
 The expected value of the RR measure is the following: where, similarly, we consider E [ v i ] i as an adaptive weight and denote it as w R i . It can be approximated by assuming that the irrelevance of documents above rank i is independent when calculating w R i , i.e., w R i  X  E [ v i ] i  X  1 i Q Thus w R i &gt; w R i +1 . On the one hand, similar to the ex-pected DCG, the weight w R i is a discount factor penalizing late retrieved relevant documents. As a result, maximiz-ing the measure intends to push documents that have high marginal distribution of relevance p ( r j ) to the top. However, the penalty is much larger than the ones in expected DCG and expected AP. To see this, let us again approximate the weight by setting p ( R a i )  X  p ( r ). The weight ratio is com-pared with those of the expected AP and expected DCG in Figure 2 (c). It shows that expected RR has the smallest weight ratio, while expected AP has the largest. Expected DCG is the one in the middle.

One the other hand, the weight is updated in a completely different way compared to expected AP. Figure 2 (b) plots the weight ratio against the marginal distribution p ( r ) and rank position i . Different from expected AP, the weight ratio of expected RR becomes larger when p ( r ) is larger, re-inforcing the discount further. As a consequence, it entire ly focuses on the quality of a few early retrieval documents. For example, the upper bound for w R 3 is 1 12 . If we consider p ( R a i ) &gt; 0 . 5 for i = { 1 , 2 , 3 } , while for DCG it usually
The covariance bit in Eq. (12) shows that overall the ex-pected value of RR increases when relevance of a document is more positively correlated with v i , the product of non-relevancies (1  X  r a j ) of the documents above. The effect is that negatively correlated documents will have higher ex -pected RR than positively correlated documents. Such effect will be discounted by a factor 1 /i at rank i . This is an en-tirely opposite preference compared to the expected AP. To see this, suppose we have two documents to rank: = E [ R a 1 ] + E [ R a 2 ] = p ( R a 1 ) + p ( R a 2 ) 2  X  Cov [ r a 1 , r a 2 ] + p ( R = p ( R a 1 ) + w R 2 p ( R a 2 )  X  Cov [ r a 1 , r a 2 ] document has a higher value of the expected RR, confirming the findings in [10, 29] that the RR metric is optimized by diversifying the ranked list of documents.
Through our analysis, it can be seen that the expected IR metrics roughly have two components. A unified definition is given as follows: E [ m ( a | r )]  X  where W i is the discount weight in position i , and V is a function defining the correlation between documents. The specific definitions with respect to different metrics are sum -marized in Table 1. Notice that for DCG, in the case of binary relevance, g ( r a i ) = 2 r a i  X  1 can be approximated as a linear function, and the variance bit vanishes in Eq. (9).
The first bit is a linear one with respect to the marginal probability p ( R a i ). Strictly speaking, this is untrue as W is adaptive to previously retrieved documents. But since the weight ratio W i +1 /W i is usually smaller than one, the maximum value of the first bit is still achieved by ranking in the decreasing order of the marginal probability of relevan ce. This is identical to what the Probability Ranking Principle has suggested [19]. We call it the general ranking preference . The second bit makes the IR metrics different from each other. It is called the specific ranking preference . A more detailed discussion and comparison about it is presented in Section 3.1 through a simulation.
Stack Search Maximizing Eq. (14) is a non-trivial task because it needs to search over all possible ranking combi-nations. We use stack search similar to [30], which keeps a list of the best n ranking combinations as candidates seen so far. These candidates are incomplete solutions till rank i . It then iteratively expands each of the best partial solutio ns by adding a document at rank i + 1. For each candidate, we select top-n documents that have the maximum increases of the expected IR metric in Eq. (14). We then put all re-sulting partial solutions (in this case, n  X  n ) onto the stack and then trim the resulting list of partial solutions to the top n candidates again. We repeat the loop until the end of the rank list is reached. The solution is the one having the maximum value among the candidate solutions. Such a sequential update may not necessarily provide a global optimization solution, but it provides an excellent trade o ff between accuracy and efficiency by adjusting n . When n is 1, it goes back to the greedy approach. When we increase n , better solutions may be found at the expense of more computational cost. For details refer to [30].

IR Model Calibration To calculate the expected IR metrics during retrieval, we need to estimate the joint prob -ability of relevance. An obvious solution is to directly est i-mate it from the (training) data [20]. Relevance informatio n is, however, not steadily available in many practical situa -tions to build a robust relevance model. In this paper, we intend to conduct an indirect estimation using existing IR models. It is observed that in many text retrieval experi-ments that the calculated ranking scores can serve as robust indicators of documents X  relevance with respect to queries . Thus, a mapping function can be developed to map from the ranking scores to the probability of relevance. Similar to [29], the joint probability of relevance p ( r | q ) is summa-rized by the marginal probability p ( r a i | q ) and covariance Cov [ r a i , r a j ].

Let us first look at p ( r a i | q ), and treat it as the utility of ranking scores. We expect the utility, defined as u , to be a non-decreasing function of the ranking score. Thus the first derivative u  X  &gt; 0. It is also expected that u has a maximum value as the ranking score increases. Thus the Figure 3: By adjusting the correlation between doc-uments from -0.2 to 1.0, the gain on performance for average precision, DCG, and RR, respectively. second derivative u  X  X  &lt; 0. Our experiment (Section 3.2) on TREC data has confirmed our intuition. Applying an exponential utility function ( u  X  &gt; 0 and u  X  X  &lt; 0) [2] gives the mapping function as: where u ( s ), in the range [0 , 1), is the utility of the ranking score s , where s  X  0. b denotes a constant. For the empirical study of the mapping, we refer to Section 3.2.
 The next question is how to estimate the covariance where V ar [ r a i ] = (1  X  p ( R a i )) p ( R a i ) if r distribution. The correlation coefficient  X  ( r a i , r a the dependency of relevance between documents at rank i and j . During retrieval, it is reasonable to use the docu-ments X  score correlation to estimate the relevance correla -score correlation is query-dependent. A practical solutio n is, however, to approximate it by sampling queries and cal-culating the correlation between documents X  ranking score s from an IR model. In our implementation, we construct each of these queries by randomly sampling query terms from the vocabulary of a data set.

For the expected RR, we need to compute the covari-ance between document a i and variable v i , where v i is the  X  X eta-relevance X  X f previously retrieved i  X  1 documents, i.e., v  X  Q i  X  1 j =1 (1  X  r a j ) as defined in Section 2.1.3. In our im-plementation, we aggregate the content of the top i -1 doc-uments as a meta document, and estimate the correlation between r a i and v i as 1 minus the correlation between the meta document X  X  ranking score and document a i  X  X  ranking score.
In this section, we carried out a simulation as a confir-mation of our analysis about the effect of correlation be-tween different documents X  relevance on a range of IR met-rics. The relevance states of documents were generated for 10,000 trials. At each trial, for each rank position i , we kept Figure 4: Probability that a result from each bin is relevant against the median of each bin. the marginal probability of relevance p ( R a i | q ) unchanged and generated the relevance/nonrelevance states of the doc -ument. The samples were then randomly perturbed so that the correlation between each pair of variables increases fr om negative to positive (x axis in Figure (3) ). For each sample in each trial we calculated the value of an IR metric. We then averaged the metric values across all the trials to obta in the average value. We used the value of the IR metric when the correlation is set as zero as the basis for calculating th e gain on the metric when the correlation changes. The results for AP, DCG, and RR are shown in Figure (3). It confirms our derivation of the expected DCG that it is insensitive to correlation. AP value increases when correlation increase s, whereas RR does otherwise.

We tried with different settings such as the number of doc-uments, and marginals etc, and got similar findings to the reported above. Previous empirical studies on TREC data have found out that one cannot optimize both the RR and AP metrics at the same time [24, 29]. The analytical forms and the simulation provide direct evidence that the AP met-ric encourage positively correlated documents whereas the RR metric encourages the opposite.
In this section, TREC data is used to get an insight into how the mapping function u looks like. Similar to the ex-perimental setup in [22], we measured the utility of ranking scores by the probability that documents given the ranking scores are judged relevant. Documents were binned based on their ranking scores for analysis; we judged the probabilit y that a randomly picked document from each bin is judged as relevant. More specifically, we ran the Jelinek-Mercer smoothing language model on the TREC2004 Robust Track 249 topics with the parameter  X  set as its typical value 0.1 [34]. The top 1000 documents were returned for each topic, and there were in total 241,606 results returned for these 24 9 queries, among which there are 7,029 relevant documents out of a total number of 17,412 relevant documents in the track. The queries contain different numbers of terms. To making the ranking scores comparable across queries, we normalize d the ranking scores for all results of each query by dividing these ranking scores by the number of terms in the query.
We sorted the 241,606 results in the descending order in terms of their scores, and divided this ranked list into bins of 1,500 results each, yielding 161 bins: the first 160 bins containing 1,500 results each, and the last bin containing the 1606 documents with the lowest scores. We selected the median score in each bin to represent the bin. In Figure 4, the utility of each bin, i.e., the probability that a randoml y chosen result from the bin is relevant, is estimated as the number of relevant documents in each bin divided by the bin size. The data points are based on the pairs of the median of each bin and probability of relevance, and the data points are connected by smoothed curves.

Figure 4 confirms our intuition that the mapping function is approximately a concave curve ( u  X  &gt; 0 and u  X  X  &lt; 0) and fitting Eq. (15) to the data in Figure 4 gives b = 9.133. Our experiments showed that the performance of our approach is robust with respect to the choice of b , and a value of b any-where between 7.0 and 12.0 results in negligible changes of the performance on all the test collections. For the remain-ing experiments, we fix the parameter b as 9, while bearing in mind that tuning it from training data might have poten-tials for further performance improvement.
We continued our empirical study of the proposed prob-abilistic retrieval framework, focusing on understanding its ability of optimizing IR metrics. Dirichlet and Jelinek-Me rcer smoothing language models were chosen as the two baseline IR models since they are frequently reported for good per-formance on TREC test collections [34]. For each query, the ranking score of each document, calculated by either of the two IR models, is normalized by dividing them over the number of terms in the query. It is used as the input to esti-mate the marginal probabilities and covariance on the basis of the discussion in Section 2.2. The stack search is then ap-plied to find an optimal ranking list that maximizes a given IR metric in Eq. (14). For the stack search, we simply set n =1, i.e., equivalent to a greedy approach, while leaving thi s line of research to future work.

Standard stemming and stopword removing were carried out for both queries and documents. The smoothing pa-rameters of the language models were tuned for the optimal performance for a metric on each data set. The results are reported on six TREC test collections, described in Table 2. TREC8, Robust 2004, and Robust 2004 Hard topics are three plain text collections, and TREC 2001 ad hoc task on WT10g data, TREC 2007 enterprise track document search task on CSIRO data, and TREC 2002 topic distillation task on .Gov data are on three Web collections.
 The results in Table 3 indicate that if we choose a certain IR metric to maximize, we obtained in most cases the best performance on this metric than optimizing other metrics and the baselines. More specifically, our approach always had the best performance with respect to MAP and MRR when the objective was to maximize the expected AP and RR, respectively. When we aimed to optimize the expected DCG, our approach improved the baseline on 8 out of 12 occasions in terms of NDCG. It is worth mentioning that no parameter was needed when optimizing the metrics. With-out any parameter tuning, our approach consistently out-performed the two baseline models, and eight improvements are statistically significant.

Recall the analysis in Section 2 that the expected AP and RR have a rather  X  X pposite X  rank preference (utility)  X  the expected AP favors a document whose relevance is positively correlated with those of the documents ranked above, whereas the expected RR suggests otherwise. Table 3 demonstrates that the optimization of the expected RR al-ways leads to better performance on MRR than optimization of the expected AP, and vice versa. The result supports our theoretical finding that RR and AP are two different types of metrics, and optimizing either of them cannot lead to the optimal performance of the other.

Table 3 also shows that optimization of AP can sometimes lead to better performance on NDCG than direct optimiza-tion of DCG. Similar finding appeared in the learning to rank paradigm, and it was argued that the reason is due to the fact that MAP is more informative than DCG [32]. Yet, we think that the informative explanation, although true in learning to rank, does not necessarily hold in our probabili s-tic framework since we do not use IR metrics to summarize the training data. Our belief is supported by the results from the simulation in Section 3.1 that the expected DCG is invariant to the changes of relevance correlation betwee n documents; and as a result, optimzing AP (prompting docu-ments whose relevance is positively correlated with previo us documents) shouldn X  X  do any better than directly optimizin g DCG for the NDCG metric. We thus believe the somewhat contradicted finding in the real data set may be attributed to the estimation of the joint probability of relevance, mor e specifically the relevance correlation, given the fact we us ed textual content to infer relevancy. As the cluster hypothesis suggests that relevant documents tend to be similar to each other to form clusters [25], a document is likely to be rele-vant if it is similar to relevant documents. As a result, the expected AP biases towards putting documents similar with each other in the top rank positions. When top ranked doc-uments are relevant, these other documents are also likely t o be relevant -their marginal probabilities of relevance mig ht be higher than the estimated. As a result, metrics such as NDCG and Precision are improved.

Finally, we provide a further account of RR and AP, the two differently behaving metrics. Recall that in Figure 2 the properties of the expected RR and AP were depicted by adjusting the weight functions w A i and w R i using a single parameter p ( r ). Figure (5) used TREC8 test collection to further show the effect of p ( r ) on the resulting MRR and MAP performance. For comparison, the performance of the baseline Dirichlet smoothing language model, and the exact optimization of RR, MAP and DCG was also plotted.

It shows that adjusting p ( r ) to approximate AP is very stable since the solution keeps roughly the same for all eigh t values of p ( r ). This could be explained by the fact that the weight ratio between w A i +1 and w A i saturates at 1 for all values of p ( r ) when i increases above 4. By contrast, the RR approximation is more volatile with respect to p ( r ). As p ( r ) increases from 0.1 to 0.5, the MRR performance increases whereas the MAP performance decreases. This is due to the fact that as p ( r ) decreases, the weight ratio of RR becomes similar to that of DCG and AP. p ( r ) can be used to trade off between the performance of MAP and MRR. When p ( r ) = 0.3 and 0.4, the performance on MRR even slightly exceeds that on the exact optimization of RR. This suggests that there might be still scope to improve our stack search algorithm by setting n higher than 1.
To complement Section 1, we continue the discussion of re-lated work. In the learning to rank paradigm, optimizing IR metrics is conducted in a discriminative manner where Sup-port Vector Machines or Neural Networks were commonly used [23, 33]. By contrast, we study the problem in a proba-bilistic framework where the intention is to combine both th e generative and discriminative processes. Our formulation of optimal ranking also fundamentally departs from the idea in [26], where a probability distribution over document permu -tations (rank) is defined, and the expectation of IR metrics is considered under this distribution. In this paper, we, ho w-ever, believe that the expectation of IR metrics should be with respect to a distribution of relevance, because the un-certainty comes only from the fact that we cannot know the relevance of documents with absolute certainty.

For the purpose of evaluation, the estimation of IR met-rics, particularly MAP, has been investigated in the past. For example, to reduce the variability of test collection, a normalization technique was introduced [11]; to deal with incomplete judgements, sampling approaches were proposed [3, 31]. Empirically, their error rates were measured [7]; a nd the uncertainty from the variability of relevance judgment s in TREC were also examined [27]. By contrast, our study is for the purpose of retrieval, and thus the IR metric estima-tion and optimization were explored in a complete different situation where the relevance is not known a priori.
The most relevant work can be found in [10, 15, 35]. The study in [10] argued that in some tasks users would be sat-isfied with a limited number of relevant documents, rather than requiring all relevant documents. The authors there-fore proposed to maximize the probability of finding a rele-vant document among the top n . By treating the previously retrieved documents as non-relevant ones, their algorithm is equivalent to optimizing Reciprocal Rank. A more gen-eral solution is proposed in [35] on the basis of the Bayesian rank decision framework in [15]. In their solutions, differe nt rank preferences are expressed by different utility functio ns and can be incorporated when calculating the score for each of the documents. The two ideas are close in spirit to the Maximal Marginal Relevance (MMR) criterion in [9], and can be called  X  X arginal relevance X  IR models because they are designed to calculate the additional information a doc-ument contributes in a result list. But unfortunately this framework does not allow the capacity to model and opti-mize different IR metrics.

This paper takes a rather different view, although similar to [15, 35] we also follow the Bayesian decision theory. We argue that the rank utility is nothing to do with the (rele-vance) model parameters but only with the hidden true top-ical relevance; and the relevance states of documents need t o be estimated before knowing any user (rank) utility. A good IR metric could be able to specify one type of rank utilities. Once we summarize our belief about the true relevance by the joint probability of relevance, the utility, expressed by an evaluation metric, can be estimated under such uncer-tainty, and the optimal decision is the one that optimizes that expected value. The two distinct retrieval steps do not assume a particular (relevance) retrieval model, making it applicable to many existing IR models and IR metrics.
Our work is also related to the portfolio theory of docu-ment ranking [29]. By an analogy with the financial prob-lems, they argued that an optimal rank order is the one that balances the overall relevance (mean) of the ranked lis t against its risk level (variance). This paper follows the id ea of using mean and variance to summarize a distribution and to analyze the expected IR metrics. Our analytical forms of expected IR metrics on the basis of the mean and variance reveal some interesting properties that have not been shown in the past.
In this paper, we have studied the statistical properties of expected IR metrics when the relevance of documents is unknown. An implementation based on our analysis and the two-stage framework has already shown its ability of opti-mizing major IR metrics in a probabilistic framework. In the future, it is of great interest to seek its usage in web search where click-through data can be viewed as indirect evidence of documents X  relevance. Also, during evaluation, the X  X ra n-field paradigm X  considers relevance as deterministic value s, either binary or graded ones. It is, however, more general to consider IR evaluation as a stochastic process too. Thus, despite the fact that our study of the expected IR metrics is for retrieval, the analysis and development are also rel-evant to evaluation if the disagreement between relevance assessors needs to be modelled.
