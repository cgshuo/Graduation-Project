 Distributed frameworks are gaining increasingly widespread use in applications that process large amounts of data. One important example application is large scale similarity search, for which Locality Sensitive Hashing (LSH) has emerged as the method of choice, specially when the data is high-dimensional. To guarantee high search quality, the LSH scheme needs a rather large number of hash tables. This entails a large space requirement, and in the distributed setting, with each query requiring a network call per hash bucket look up, also a big network load. Panigrahy X  X  En-tropy LSH scheme significantly reduces the space require-ment but does not help with (and in fact worsens) the search network efficiency. In this paper, focusing on the Euclidian space under l 2 norm and building up on Entropy LSH, we propose the distributed Layered LSH scheme, and prove that it exponentially decreases the network cost, while maintain-ing a good load balance between different machines. Our experiments also verify that our theoretical results. H.3.1 [ Content Analysis and Indexing ]: Indexing Meth-ods Similarity Search, Locality Sensitive Hashing, Distributed Systems, MapReduce
Similarity search is the problem of retrieving data objects similar to a query object. It has become an important com-ponent of modern data-mining systems, with wide ranging applications [14, 17, 7, 5, 3]
Research supported by NSF grant 0904314
Research supported by NSF grants 0915040 and NSF 0904314 Research supported by NSF grant 0915040
In these applications, objects are usually represented by a high dimensional feature vector. A scheme to solve the similarity search problem constructs an index which, given a query point, allows for quickly finding the data points similar to it. The index construction also needs to be time and space efficient. Furthermore, since today X  X  massive datasets are typically stored and processed in a distributed fashion, where network communication is one of the most important bottlenecks, these methods need to be network efficient.
An important family of similarity search methods is based on the notion of Locality Sensitive Hashing (LSH) [12]. LSH scales well with the data dimension [12, 15]. However, to guarantee a good search quality, it needs a large number of hash tables. This entails a rather large index space require-ment, and also in the distributed setting, a large network load, as each hash bucket look up requires a network call. By looking up a number of query offsets in addition to the query itself, Panigrahy X  X  Entropy LSH [18] significantly re-duces the number of required hash tables, and hence the LSH space complexity. But, it does not help with its net-work efficiency, as now each query offset lookup requires a network call. In fact, since the number of required offsets in Entropy LSH is larger than the number of required hash tables in conventional LSH, Entropy LSH amplifies the net-work inefficiency issue.

In this paper, focusing on the Euclidian space, we design the Layered LSH method which, compared to a straightfor-ward distributed implementation of LSH or Entropy LSH, results in an exponential improvement in the network load, while maintaining a good load balance across machines. Our experiments also verify our theoretical results.
In this paper, we focus on the network efficiency of LSH in distributed frameworks. Two main instantiations of such frameworks are the batched processing system MapReduce [9] (with its open source implementation Hadoop [1]), and real-time processing systems denoted as Active Distributed Hash Tables (Active DHTs), such as Twitter Storm [2]. The common feature in all these systems is that they process data in the form of (Key, Value) pairs, distributed over a set of machines. This distributed (Key, Value) abstraction is all we need for both our scheme and analyses to apply. How-ever, to make the later discussions more concrete, here we briefly overview the mentioned distributed systems. MapReduce: MapReduce [9] is a simple model for batched distributed processing, where computations are done in three phases. The Map phase reads a collection of (Key, Value) pairs from an input source, and by invoking a user defined Mapper function on each input element independently and in parallel, emits zero or more (Key, Value) pairs. The Shuf-fle phase then groups together all the Mapper-emitted (Key, Value) pairs sharing the same Key, and outputs each dis-tinct group to the next phase. The Reduce phase invokes a user-defined Reducer function on each distinct group, inde-pendently and in parallel, and emits zero or more values to associate with the group X  X  Key.
 Active DHT: A DHT (Distributed Hash Table) is a dis-tributed (Key, Value) store which allows Lookups, Inserts, and Deletes on the basis of the Key. The term Active refers to the fact that an arbitrary User Defined Function (UDF) can be executed on a (Key, Value) pair in addition to Insert, Delete, and Lookup. Twitter X  X  Storm [2] is an example of Active DHTs which is gaining widespread use. The Active DHT model is broad enough to act as a distributed stream processing system and as a continuous version of MapRe-duce [16]. All the (Key, Value) pairs in a node of the active DHT are usually stored in main memory to allow for fast real-time processing of data and queries.

In addition to the typical performance measures of total running time and total space, two other measures are very important for both MapReduce and Active DHTs: 1) total network traffic generated 2) the maximum number of values with the same key; a high value here can lead to the  X  X urse of the last reducer X  in MapReduce or to one compute node becoming a bottleneck in Active DHT.
We design a new scheme, called Layered LSH, to imple-ment Entropy LSH in the distributed (Key, Value) model. A summary of our results is: 1. We prove that Layered LSH incurs only O ( 2. Surprisingly, we prove that, in contrast with both En-3. We prove that despite network efficiency (requiring col-
In this section we briefly review the preliminaries required for the paper.
 Similarity Search: The similarity search problem is that of finding data objects similar to a query object. In many practical applications, the objects are represented by multi-dimensional feature vectors, and hence the problem reduces to finding objects close to the query object under the fea-ture space distance metric. Formally, in a metric space with domain T , similarity search reduces to the problem more commonly known as the ( c,r )-NN problem, where given an approximation ratio c &gt; 1, the goal is to construct an index that given any query point q  X  T within distance r of a data point, allows for quickly finding a data point p  X  T whose distance to q is at most cr .
 Basic LSH: To solve the ( c,r )-NN problem, Indyk and Motwani [12] introduced the following notion of LSH func-tions:
Definition 1. For the space T with metric  X  , given dis-tance threshold r , approximation ratio c &gt; 1 , and probabili-ties p 1 &gt; p 2 , a family of hash functions H = { h : T  X  U } is said to be a ( r,cr,p 1 ,p 2 ) -LSH family if for all x,y  X  T , LSH families can be used to design an index for the ( c,r )-NN problem as follows. First, for an integer k , let H { H : T  X  U k } be a family of hash functions in which any H  X  H 0 is the concatenation of k functions in H , i.e., H = ( h 1 ,h 2 ,...,h k ), where h i  X  H (1  X  i  X  k ). Then, for an integer M , draw M hash functions from H 0 , independently and uniformly at random, and use them to construct the index consisting of M hash tables on the data points. With this index, given a query q , the similarity search is done by first generating the set of all data points mapping to the same bucket as q in at least one hash table, and then finding the closest point to q among those data points.
 To utilize this indexing scheme, one needs an LSH family H to start with. Such families are known for a variety of metric spaces [6]. Specially, Datar et al. [8] proposed LSH families for l p norms, with 0 &lt; p  X  2, using p -stable distribu-tions. In particular, for the case p = 2, with any W &gt; 0, they consider a family of hash functions H W : { h a ,b : R d  X  such that where b  X  R is chosen uniformly from [0 ,W ] and a  X  N d ( 0 , 1). Here, N d ( 0 ,r ) denotes the normal distribution around the origin, 0  X  R d , where the i -th coordinate has the distribu-tion N (0 ,r ),  X  i  X  1 ...d . For this case, Indyk and Motwani [12] proved that with n data points, choosing k = O (log n ) and M = O ( n 1 /c ) solves the ( c,r )-NN problem with con-stant probability.

Although Basic LSH yields a significant improvement in the running time over both the brute force linear scan and the space partitioning approaches [20, 4, 13], unfortunately the required number of hash functions is usually large [5, 10], which entails a very large space requirement for the index. Also, in the distributed setting, each hash table lookup at query time corresponds to a network call which entails a large network load.
 Entropy LSH: To mitigate the space inefficiency, Pan-igrahy [18] introduced the Entropy LSH scheme using the same indexing as the Basic LSH scheme, but a different query search procedure. The idea here is that for each hash function H  X  X  0 , the data points close to the query point q are highly likely to hash either to the same value as H ( q ) or to a value very close to that. Hence, in this scheme, in addi-tion to q , several  X  X ffsets X  q +  X  i (1  X  i  X  L ), chosen randomly from the surface of B ( q,r ), the sphere of radius r centered at q , are also hashed and the data points in their hash buckets are also considered as search result candidates. Panigrahy [18] shows that for n data points, choosing k  X  log n (with p 2 as in Definition 1) and L = O ( n 2 /c ), as few as hash tables suffice to solve the ( c,r )-NN problem.
Hence Entropy LSH improves the space efficiency signifi-cantly. However, in the distributed setting, it does not help with reducing the network load of LSH queries. Actually, since for the basic LSH, one needs to look up M = O ( n 1 /c buckets but with this scheme, one needs to look up L = O ( n 2 /c ) offsets, it makes the network inefficiency issue even more severe [12, 18, 17] .
In this section, we will present the Layered LSH scheme and theoretically analyze it. Let S be a set of n data points available a-priori, and Q be the set of query points, either given as a batch (in case of MapReduce) or arriving in real-time (in case of Active DHT). Parameters k,L,W and LSH families H = H W and H 0 = H 0 W will be as defined in section 2. Since multiple hash tables can be obviously implemented in parallel, for the sake of clarity we will focus on a single hash table and use a randomly chosen hash function H  X  X  0 as our LSH function throughout. We will also assume that a (Key, Value) pair gets processed by the machine with id Key.

We start with a simple distributed implementation of En-tropy LSH. For any data point p  X  S a (Key, Value) pair ( H ( p ) ,p ) is generated. For each query point q , after gener-ating the offsets q +  X  i (1  X  i  X  L ), for each offset q +  X  a (Key, Value) pair ( H ( q +  X  i ) ,q ) is generated. Then, for any received query point q , the machine with id H ( q +  X  retrieves all data points p , if any, with H ( p ) = H ( q +  X  which are within distance cr of q. This is done via a UDF in Active DHT or the Reducer in MapReduce.

The amount of data transmitted per query in this im-plementation is O ( Ld ): L (Key, Value) pairs, one per off-set, each with the d -dimensional point q as Value. Both L and d are large in many practical applications with high-dimensional data. Hence, this implementation causes a large network load. In this subsection, we present the Layered LSH scheme. The main idea is to use another layer of locality sensitive hashing to distribute the data and query points over the machines. More specifically, given a parameter value D &gt; 0, we sample an LSH function G : R k  X  Z such that: where  X   X  N k ( 0 , 1) and  X  is chosen uniformly from [0 ,D ].
Then, denoting G ( H (  X  )) by GH (  X  ), for each data point ). This machine will then add p to the bucket H ( p ) by a Reducer in MapReduce or a UDF in Active DHT. Similarly, for each query point q  X  Q , after generating the offsets q +  X  (1  X  i  X  L ), for each unique value x in the set we generate a (Key, Value) pair ( x,q ). Then, machine x will have all the data points p such that GH ( p ) = x as well as the queries q  X  Q one of whose offsets gets mapped to x by GH (  X  ). Then, this machine regenerates the offsets q +  X  i (1  X  i  X  L ), finds their hash buckets H ( q +  X  i any of these buckets such that GH ( q +  X  i ) = x , it performs a similarity search among the data points in that bucket. Note that since q is sent to this machine, there exists at least one such bucket. Also note that, the offset regeneration, hash, and bucket search can all be done by either a UDF in Active DHT or the Reducer in MapReduce.

At an intuitive level, the main idea in Layered LSH is that since G is an LSH, and also for any query point q , we have equation 3.2 has a very small cardinality, which in turn im-plies a small amount of network communication per query. On the other hand, since G and H are both LSH functions, if two data points p,p 0 are far apart, GH ( p ) and GH ( p highly likely to be different. This means that, while locat-ing the nearby points on the same machines, Layered LSH partitions faraway data points on different machines, which in turn ensures a good load balance across the machines. Note that this is critical, as without a good load balance, the point in distributing the implementation would be lost.
In this section, we analyze the Layered LSH scheme pre-sented in the previous section. We first fix some notation. As mentioned earlier in the paper, we are interested in the ( c,r )-NN problem. Without loss of generality and to simplify the notation, in this section we assume r = 1 /c . This can be achieved by a simple scaling. The LSH function H  X  H 0 W that we use is H = ( H 1 ,...,H k ), where k is chosen as in [18] and for each 1  X  i  X  k : where a i is a d -dimensional vector each of whose entries is chosen from the standard Gaussian N (0 , 1) distribution, and b i  X  R is chosen uniformly from [0 ,W ]. We will also let  X  : R d  X  R k be  X  = ( X  1 ,...,  X  k ), where for 1  X  i  X  k : lemma, which follows from triangle inequality, in our analy-sis:
Lemma 2. For any two vectors u,v  X  R d , we have: ||  X ( u )  X   X ( v ) || X 
Our analysis also uses two well-known facts. The first is the sharp concentration of  X  2 -distributed random variables, which is also used in the proof of the Johnson-Lindenstrauss (JL) lemma [12], and the second is the 2-stability property of Gaussian distribution: Before proceeding to the analysis, we give a definition:
Definition 3. Having chosen LSH functions G,H , for a query point q  X  Q , with offsets q +  X  i ( 1  X  i  X  L ), define to be the number of (Key, Value) pairs sent over the network for query q .
 Since q is d -dimensional, the network load due to query q is O ( df q ). Hence, to analyze the network efficiency of Layered LSH, it suffices to analyze f q . This is done in the following theorem:
Theorem 4. For any query point q , with high probability, that is probability at least 1  X  1
Proof. Since for any offset q +  X  i , the value GH ( q +  X  is an integer, we have: f q  X  max where the last line follows from the Cauchy-Schwartz in-equality. For any 1  X  i,j  X  L , we know from lemma 2: || H ( q +  X  i )  X  H ( q +  X  j ) || X ||  X ( q +  X  i )  X   X ( q +  X  Furthermore for any 1  X  t  X  k , since we know, using 2-stability of Gaussian distribution, that  X  ( q +  X  i )  X   X  t ( q +  X  j ) is distributed as N (0 , ||  X  ( p 2 ) &lt; 1 for which, using the JL lemma [12]: with probability at least 1  X  1 /n  X (1) . Since offsets are cho-centered at q , we have: ||  X  i  X   X  j ||  X  2 /c . Hence since there are only L 2 different choices of 1  X  i,j  X  L , and L is only polynomially large in n : with high probability. Then, using equation 3.4, we get with high probability: Furthermore, since each entry of  X   X  R k is distributed as N (0 , 1), another application of JL lemma [12] gives (again with = ( p 2 )): with high probability. Then, equations 3.3, 3.5, and 3.6 together give f q  X  2(1 + 4 cW ) k D + 1 .

Remark 5. A surprising property of Layered LSH demon-strated by theorem 4 is that the network load is independent of the number of query offsets, L . To increase the search quality, one needs to increase the number of offsets for En-tropy LSH, and the number of hash tables for Basic LSH, both of which increase the network load. However, with Lay-ered LSH the network efficiency is achieved independently of the level of search quality.
 Next, we proceed to analyzing the load balance of Layered LSH. First, we define the function P (  X  ): and state the following lemma which is proved in the full version of the paper.

Lemma 6. For any two points u,v  X  R k with || u  X  v || =  X  , we have: Pr [ G ( u ) = G ( v )] = P ( D  X  2  X  )
One can easily see that P (  X  ) is a monotonically increasing function, and for any 0 &lt;  X  &lt; 1 there exists a number z = z such that P ( z  X  ) =  X  . Using this notation and the previous lemma, we prove the following theorem:
Theorem 7. For any constant 0 &lt;  X  &lt; 1 , there is a  X  such that  X   X  /W = O (1 + D  X  where o (1) is polynomially small in n .

Proof. Let u,v  X  R d be two points and denote || u  X  v || =  X  . Then by lemma 2, we have: As in the proof of theorem 4, one can see, using k  X  log n [18] and the JL lemma [12], that there exists an = ( p 2 ) =  X (1) such that with probability at least 1  X  1 Hence, with probability at least 1  X  1 Now, letting  X   X  = W 1  X  (1 + D  X  Pr [ GH ( u ) = GH ( v ) ||| H ( u )  X  H ( v ) || X   X  0 ]  X  P (  X  ] = o (1).

Theorems 4, 7 show the tradeoff governing the choice of parameter D . Theorem 7 shows that choosing D = o ( does not asymptotically help with the distance threshold at which points become likely to be sent to different machines. Intuitively, since H has a bin size of W , a pair of points p ,p 2 mapping to the same buckets as two offsets of a query point, most likely have distance O ( W ). Hence, D should be only large enough to make points which are O ( W ) away likely to be sent to the same machine. Then, we need to choose D such that O (1 + D  X  Then, by theorem 4, to minimize the network traffic, we choose D =  X (
Corollary 8. Choosing D =  X ( antees that the number of (Key,Value) pairs sent over the network per query is O ( yet points which are  X ( W ) away get sent to different ma-chines with constant probability.

Remark 9. Corollary 8 shows that, compared to the sim-ple distributed implementation of Entropy LSH and basic LSH, Layered LSH exponentially improves the network load, from O ( n  X (1) ) to O ( ance across the different machines.
In this section we describe our experimental setup and results. Further details are in the full version of the paper.
Summary of Results: Here we present a summary of the results of our experiments using MapReduce with re-spect to the network cost (shuffle size) and wall-clock run time:
Implementation Details: We use the English Wikipedia corpus from February 2012 as the data set. We compute TF-IDF vectors for each document after some standard pre-processing. Then, we partition the 3.75M articles in the corpus randomly into a data set of size 3M and a query set of size 750K. We solve the ( c,r )-NN problem with r = 0 . 1 and c = 2. In addition, we also perform experiments on other datasets, which yield similar results as presented in the full version of the paper.

We perform experiments on a small cluster of 13 compute nodes using Hadoop [1] with 800MB JVMs. Consistency in the choice of hash functions H,G (Section 3) as well as offsets across mappers and reducers is ensured by setting the seeds of the random number generators appropriately.
We choose the LSH parameters ( W = 0 . 5 ,k = 12) ac-cording to [18, 17, 19]. We optimized D , the parameter of Layered LSH, using a simple binary search to minimize the wall-clock run time.

Since the underlying dimensionality (vocabulary size 549532) is large, we use the Multi-Probe LSH (MPLSH) [17] as our first layer of hashing. We measure the accuracy of search results by computing the recall, i.e., the fraction of query points with at least one data point within a distance r , returned in the output.
