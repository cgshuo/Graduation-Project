 Many prediction problems in machine learning applications are structured prediction tasks. For example, in protein folding we are given a protein sequence and the goal is to predict the protein X  X  native structure [14]. In parsing for natural language processing, we are given a sentence and the goal is to predict the most likely parse tree [2]. In these and many other applications, we can formalize the structured prediction problem as taking an input x (e.g., primary sequence, sentence) and predicting maps any input and a candidate assignment to a feature vector, Y denotes the space of all possible assignments to the vector y , and  X  is a weight vector to be learned.
 This paper addresses the problem of learning structured prediction models from data. In particular, example m , y m = arg max y  X  X   X   X   X  ( x m , y ) , i.e. one which separates the training data. For many structured prediction models, maximization over Y is computationally intractable. This makes it difficult to apply previous algorithms for learning structured prediction models, such as structured perceptron [2], stochastic subgradient [10], and cutting-plane algorithms [5], which require making a prediction at every iteration (equivalent to repeatedly solving an integer linear program). Given training data, we can consider the space of parameters  X  that separate the data. This space can be defined by the intersection of a large number of linear inequalities. A recent approach to getting around the hardness of prediction is to use linear programming (LP) relaxations to approximate the maximization over Y [4, 6, 9]. However, separation with respect to a relaxation places stronger constraints on the parameters. The target solution, an integral vertex in the LP, must now distinguish itself also from possible fractional vertexes that arise due to the relaxation. The relaxations can therefore be understood as optimizing over an inner bound of  X  . This set may be empty even if the training data is separable with exact inference [6]. Another obstacle to using LP relaxations for learning is that solving the LPs can be very slow.
 In this paper we ask whether it is possible to learn while avoiding inference altogether. We propose a new learning algorithm, inspired by pseudo-likelihood [1], that optimizes over an outer bound of  X  . Learning involves optimizing over only a small number of constraints per data point, and thus can be performed quickly, even for complex structured prediction models. We show that, if the data is an example of how having the right data can circumvent the hardness of learning for structured prediction. deciding whether a given data set is separable is NP-hard, and thus learning in a strict sense is no easier than prediction. Thus, we should not expect for our algorithm, or any other polynomial time algorithm, to always succeed at learning from an arbitrary finite data set. To our knowledge, this is the first result characterizing the hardness of exact learning for structured prediction. Finally, we show empirically that our algorithm allows us to successfully learn the parameters for both multi-label prediction and protein side-chain placement. The performance of the algorithm is improved as more data becomes available, as our theoretical results anticipate. We consider the general structured prediction problem. The input space is denoted by X and the set of all possible assignments by Y . Each y  X  Y corresponds to n variables y 1 ,...,y n , each with k possible states. The classifier uses a (given) function  X  ( x , y ) : X , Y  X  R d and (learned) weights f ( y ; x ,  X  ) =  X   X   X  ( x , y ) . Our analysis will focus on functions  X  whose scope is limited to small sets of the y i variables, but for now we keep the discussion general.
 weights  X  that correctly classify the training examples. Consider first the separable case. Define the exponentially many constraints per example, one for each competing assignment.
 In this work we consider a much simpler set of constraints where, for each example, we only consider the competing assignments obtained by modifying a single label y i , while fixing the other labels to their value at y m . The pseudo-max set, which is an outer bound on  X  , is given by Here y m  X  i denotes the label y m without the assignment to y i .
 When the data is not separable,  X  will be the empty set. Instead, we may choose to minimize the be an upper bound on the training error [13]. When the data is separable, min  X  ` (  X  ) = 0 . Note that regularization may be added to this objective.
 The corresponding pseudo-max objective replaces the maximization over all of y with maximization over a single variable y i while fixing the other labels to their value at y m : 2 , 3 Analogous to before, we have min  X  ` ps (  X  ) = 0 if and only if  X   X   X  ps .
 The objective in Eq. 2 is similar in spirit to pseudo-likelihood objectives used for maximum likeli-hood estimation of parameters of Markov random fields (MRFs) [1]. The pseudo-likelihood estimate is provably consistent when the data generating distribution is a MRF of the same structure as used in the pseudo-likelihood objective. However, our setting is different since we only get to view the maximizing assignment of the MRF rather than samples from it. Thus, a particular x will always be paired with the same y rather than samples y drawn from the conditional distribution p ( y | x ;  X  ) . The pseudo-max constraints in Eq. 1 are also related to cutting plane approaches to inference [4, 5]. In the latter, the learning problem is solved by repeatedly looking for assignments that violate the separability constraint (or its hinge version). Our constraints can be viewed as using a very small subset of assignments for the set of candidate constraint violators. We also note that when exact maximization over the discriminant function f ( y ; x ,  X  ) is hard, the standard cutting plane algorithm cannot be employed since it is infeasible to find a violated constraint. For the pseudo-max objective, finding a constraint violation is simple and linear in the number of variables. 4 It is easy to see (as will be elaborated on next) that the pseudo-max method does not in general yield a consistent estimate of  X  , even in the separable case. However, as we show, consistency can be shown to be achieved under particular assumptions on the data generating distribution p ( x ) . In this section we show that if the feature generating distribution p ( x ) satisfies particular assump-tions, then the pseudo-max approach yields a consistent estimate. In other words, if the training minimum of the pseudo-max objective will converge to  X   X  (up to equivalence transformations). The section is organized as follows. First, we provide intuition for the consistency results by con-sidering a model with only two variables. Then, in Sec. 2.1, we show that any parameter  X   X  can be identified to within arbitrary accuracy by choosing a particular training set (i.e., choice of x m ). This in itself proves consistency, as long as there is a non-zero probability of sampling this set. In Sec. 2.2 we give a more direct proof of consistency by using strict convexity arguments. For ease of presentation, we shall work with a simplified instance of the structured learning setting. We focus on binary variables, y i  X  { 0 , 1 } , and consider discriminant functions corresponding to Ising models , a special case of pairwise MRFs ( J denotes the vector of  X  X nteraction X  parameters): The singleton potential for variable y i is y i x i and is not dependent on the model parameters. We could have instead used J i y i x i , which would be more standard. However, this would make the parameter vector J invariant to scaling, complicating the identifiability analysis. In the consistency analysis we will assume that the data is generated using a true parameter vector J  X  . We will show that as the data size goes to infinity, minimization of ` ps ( J ) yields J  X  .
 We begin with an illustrative analysis of the pseudo-max constraints for a model with only two vari-principles for when pseudo-max constraints may succeed or fail. Assume that training samples are outline the exact decision boundaries of f ( y ; x ,J  X  ) , with the lines being given by the constraints in
 X  that hold with equality. The red lines denote the pseudo-max constraints in  X  We can identify J  X  by obtaining samples x = ( x 1 ,x 2 ) that explore both sides of one of the decision boundaries that depends on J  X  . The pseudo-max constraints will fail to identify J  X  if the samples do not sufficiently explore the transitions between y = (0 , 1) and y = (1 , 1) or between y = (1 , 0) and y = (1 , 1) . This can happen, for example, when the input samples are dependent, giving only rise to the configurations y = (0 , 0) and y = (1 , 1) . For points labeled (1 , 1) around the decision line J  X  + x 1 + x 2 = 0 , pseudo-max can only tell that they respect J  X  + x 1  X  0 and J  X  + x 2  X  0 (dashed red lines), or x 1  X  0 and x 2  X  0 for points labeled (0 , 0) .
 Only constraints that depend on the parameter are effective for learning. For pseudo-max to be able to identify J  X  , the input samples must be continuous, densely populating the two parameter depen-dent decision lines that pseudo-max can use. The two point sets in the figure illustrate good and bad input distributions for pseudo-max. The diagonal set would work well with the exact constraints but badly with pseudo-max, and the difference can be arbitrarily large. However, the input distribution on the right, populating the J  X  + x 2 = 0 decision line, would permit pseudo-max to identify J  X  . 2.1 Identifiability of True Parameters In this section, we show that it is possible to approximately identify the true model parameters, up to model equivalence, using the pseudo-max constraints and a carefully chosen linear number of data points. Consider the learning problem for structured prediction defined on a fixed graph G = ( V,E ) node fields  X  i ( y i ) for i  X  V . We consider discriminant functions of the form where the input space X = R | V | k specifies the single node potentials. Without loss of generality, we remove the additional degrees of freedom in  X  by restricting it to be in a canonical form:  X   X   X  can y = 0 . As a result, assuming the training set comes from a model in this class, and the input fields x ( y we show that, for some data sets, the pseudo-max constraints are sufficient to identify  X   X  . rule out the trivial solution  X  = 0 .
 close to  X   X  .
 The proof is given in the supplementary material. To illustrate the key ideas, we consider the simpler binary discriminant function discussed in Eq. 3. Note that the binary model is already in the canon-ical form since J ij y i y j = 0 whenever y i = 0 or y j = 0 . For any ij  X  E , we show how to choose two input examples x 1 and x 2 such that any J consistent with the pseudo-max constraints for these gives the complete set of examples. The input examples we need for this will depend on J  X  . For the first example, we set the input fields for all neighbors of i (except j ) in such a way that x j &gt; | N account the label assignments for y 1 i and its neighbors, and by removing terms that are the same on that J ij + x 1 i  X  0 or J ij  X  J  X  ij +  X  0 . The second example x 2 differs only in terms of the input 2.2 Consistency via Strict Convexity In this section we prove the consistency of the pseudo-max approach by showing that it corresponds to minimizing a strictly convex function. Our proof only requires that p ( x ) be non-zero for all x  X  R n (a simple example being a multi-variate Gaussian) and that J  X  is finite. We use a discriminant function as in Eq. 3. Now, assume the input points x m are distributed according to p ( x ) and that y data, and its limit when M  X  X  X  , compactly as: 1 and y k ( x ) = 0 for k  X  N ( i ) \ j } . Eq. 6 can then be written as the remaining terms, i.e. where either zero or more than one neighbor is set to one. The function  X  g i since it is a sum over functions strictly convex in each one of the variables in J .
 the marginal p i ( x i | S ij ) over x i within S ij . After some algebra, we obtain: remaining term is (for brevity we drop P ( S ij ) , a strictly positive constant, and the ij index):  X  h ( this inequality holds but is not necessarily strict (since  X  h is always convex in J). We thus have after as required. Note that we used the fact that p ( x ) has full support when integrating over I . plus other convex functions of J , hence strictly convex. We can now proceed to show consistency. By strict convexity, the pseudo-max objective is minimized at a unique point J . Since we know minimizer. Thus we have that as M  X   X  , the minimizer of the pseudo-max objective is the true parameter vector, and thus we have consistency.
 As an example, consider the case of two variables y 1 ,y 2 , with x 1 and x 2 distributed according to N ( c 1 , 1) , N (0 , 1) respectively. Furthermore assume J  X  which is indeed a strictly convex function that is minimized at J = 0 (see Fig. 1 for an illustration). Most structured prediction learning algorithms use some form of inference as a subroutine. However, the corresponding prediction task is generally NP-hard. For example, maximizing the discriminant function defined in Eq. 3 is equivalent to solving Max-Cut, which is known to be NP-hard. This raises the question of whether it is possible to bypass prediction during learning. Although prediction may be intractable for arbitrary MRFs, what does this say about the difficulty of learning with a polynomial number of data points? In this section, we show that the problem of deciding whether there exists a parameter vector that separates the training data is NP-hard.
 Put in the context of the positive results in this paper, these hardness results show that, although in some cases the pseudo-max constraints yield a consistent estimate, we cannot hope for a certificate of optimality. Put differently, although the pseudo-max constraints in the separable case always give an outer bound on  X  (and may even be a single point),  X  could be the empty set  X  and we would never know the difference.
 Theorem 3.1. Given labeled examples { ( x m , y m ) } M m =1 for a fixed but arbitrary graph G , it is NP-hard to decide whether there exists parameters  X  such that  X  m, y m = arg max y f ( y ; x m ,  X  ) . Proof. Any parameters  X  have an equivalent parameterization in canonical form (see section Sec. 2.1, also supplementary). Thus, the examples will be separable if and only if they are sepa-given an undirected graph G , whether there exists a cut of at least K edges. Let G be the same graph as G , with k = 3 states per variable. We construct a small set of examples where a parameter vector will exist that separates the data if and only if there is no cut of K or more edges in G . y = y using the technique described in Sec. 2.1 (also supplementary material), which when restricted to the space  X  can , constrain the parameters to equal  X  . We then use one more example ( x m , y m ) where y m = 3 (every node is in state 3) and, for all i , x m two states encode the original Max-Cut instance, while the third state is used to construct a labeling y m that has value equal to K  X  1 , and is otherwise not used.
 Let K  X  be the value of the maximum cut in G . If in any assignment to the last example there is a variable taking the state 3 and another variable taking the state 1 or 2, then the assignment X  X  value will be at most K  X   X  n 2 , which is less than zero. By construction, the 3 assignment has value K  X  1 . Thus, the optimal assignment must either be 3 with value K  X  1 , or some combination of states 1 and 2, which has value at most K  X  . If K  X  &gt; K  X  1 then 3 is not optimal and the examples are not separable. If K  X   X  K  X  1 , the examples are separable.
 This result illustrates the potential difficulty of learning in worst-case graphs. Nonetheless, many problems have a more restricted dependence on the input. For example, in computer vision, edge potentials may depend only on the difference in color between two adjacent pixels. Our results do not preclude positive results of learnability in such restricted settings. By establishing hardness of learning, we also close the open problem of relating hardness of inference and learning in structured prediction. If inference problems can be solved in polynomial time, then so can learning (using, e.g., structured perceptron). Thus, when learning is hard, inference must be hard as well. To evaluate our learning algorithm, we test its performance on both synthetic and real-world datasets. We show that, as the number of training samples grows, the accuracy of the pseudo-max method im-e ( y i ,y m i ) = 1 { y i 6 = y m i } /n m ( n m is the number of labels in example m ): We will compare the pseudo-max method with learning using structural SVMs, both with exact inference and LP relaxations [see, e.g., 4]. We use exact inference for prediction at test time. P For a weight vector  X   X  (sampled once, uniformly in the range [  X  1 , 1] , and used for all train/test sets) we generate train and test instances by sampling x m uniformly in the range [  X  5 , 5] and then computing the optimal labels y m = arg max y  X  X  f ( y ; x m ,  X   X  ) .
 gorithms, and measure the test error for the learned weights (with 1000 test samples). For each train size we average the test error over 10 repeats of sampling and training. Fig. 2(a) shows a comparison of the test error for the three learning algorithms. For small numbers of training examples, the test error of pseudo-max is larger than that of the other algorithms. However, as the train size grows, the error converges to that of exact learning, as our consistency results predict.
 We also test the performance of our algorithm on a multi-label document classification task from the Reuters dataset [7]. The data consists of M = 23149 training samples, and we use a reduction of the dataset to the 5 most frequent labels. The 5 label variables form a fully connected pairwise graph structure (see [4] for a similar setting). We use random subsamples of increasing size from the train set to learn the parameters, and then measure the test error using 20000 additional samples. For each sample size and learning algorithm, we optimize the trade-off parameter C using 30% of the training data as a hold-out set. Fig. 2(b) shows that for the large data regime the performance of pseudo-max learning gets close to that of the other methods. However, unlike the synthetic setting there is still a enough to be in the consistent regime (note that exact learning has not flattened either), or because the consistency conditions are not fully satisfied: the data might be non-separable or the support of the input distribution p ( x ) may be partial.
 We next apply our method to the problem of learning the energy function for protein side-chain placement, mirroring the learning setup of [14], where the authors train a conditional random field (CRF) using tree-reweighted belief propagation to maximize a lower bound on the likelihood. 5 The prediction problem for side-chain placement corresponds to finding the most likely assignment in a pairwise MRF, and fits naturally into our learning framework. There are only 8 parameters to be learned, corresponding to a reweighting of known energy terms. The dataset consists of 275 proteins, where each MRF has several hundred variables (one per residue of the protein) and each variable has on average 20 states. For prediction we use CPLEX X  X  ILP solver.
 Fig. 3 shows a comparison of the pseudo-max method and a cutting-plane algorithm which uses an LP relaxation, solved with CPLEX, for finding violated constraints. 6 We generate training sets of remaining examples. 7 For M = 10 , 50 , 100 we average the test error over 3 random train/test splits, whereas for M = 274 we do 1-fold cross validation. We use C = 1 for both algorithms. The original weights ( X  X oft rep X  [3]) used for this energy function have 26.7% error across all 275 25.6% error (their training set included 55 of these 275 proteins, so this is an optimistic estimate). To get a sense of the difficulty of this learning task, we also tried a random positive weight vector, uniformly sampled from the range [0 , 1] , obtaining an error of 34.9% (results would be much worse if we allowed the weights to be negative). Training using pseudo-max with 50 examples, we learn parameters in under a minute that give better accuracy than the HCRF. The speed-up of training with pseudo-max (using CPLEX X  X  QP solver) versus cutting-plane is striking. For example, for M = 10 , pseudo-max takes only 3 seconds, a 1000 -fold speedup. Unfortunately the cutting-plane algorithm took a prohibitive amount of time to be able to run on the larger training sets. Since the data used in learning for protein side-chain placement is both highly non-separable and relatively little, these positive results illustrate the potential wide-spread applicability of the pseudo-max method. The key idea of our method is to find parameters that prefer the true assignment y m over assignments weak requirement is sufficient to achieve consistency given a rich enough input distribution. One extension of our approach is to add constraints for assignments that differ from y m in more than one variable. This would tighten the outer bound on  X  and possibly result in improved performance, but would also increase computational complexity. We could also add such competing assignments via a cutting-plane scheme so that optimization is performed only over a subset of these constraints. Our work raises a number of important open problems: It would be interesting to derive generaliza-tion bounds to understand the convergence rate of our method, as well as understanding the effect of one hand, it needs to explore the space Y in the sense that a sufficient number of labels need to be obtained as the correct label for the true parameters (this is indeed used in our consistency proofs). On the other hand, p ( x ) needs to be sufficiently sensitive close to the decision boundaries so that the true parameters can be inferred. We expect that generalization analysis will depend on these two properties of p ( x ) . Note that [11] studied active learning schemes for structured data and may be relevant in the current context.
 How should one apply this learning algorithm to non-separable data sets? We suggested one ap-proach, based on using a hinge loss for each of the pseudo constraints. One question in this context is, how resilient is this learning algorithm to label noise? Recent work has analyzed the sensitivity of pseudo-likelihood methods to model mis-specification [8], and it would be interesting to perform a similar analysis here. Also, is it possible to give any guarantees for the empirical and expected risks (with respect to exact inference) obtained by outer bound learning versus exact learning? Finally, our algorithm demonstrates a phenomenon where more data can make computation easier. Such a scenario was recently analyzed in the context of supervised learning [12], and it would be interesting to combine the approaches.
 Acknowledgments: We thank Chen Yanover for his assistance with the protein data. This work was sup-
