 In recent years, active learning methods based on experi-mental design achieve state-of-the-art performance in text classification applications. Although these methods can ex-ploit the distribution of unlabeled data and support batch selection, they cannot make use of labeled data which often carry useful information for active learning. In this paper, we propose a novel active learning method for text classifi-cation, called s upervised e xperimental d esign (SED), which seamlessly incorporates label information into experimental design. Experimental results show that SED outperforms its counterparts which either discard the label information even when it is available or fail to exploit the distribution of unlabeled data.
 G.3 [ Mathematics of Computing ]: Probability and Statis-tics X  Experimental Design ; H.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Clustering Algorithms, Theory Active Learning, Supervised Experimental Design, Text Clas-sification, Convex Optimization
There has been a long tradition of research on text classi-fication in both the information retrieval and machine learn-ing communities. In order to learn a good text classifier, a large number of labeled documents are often needed for clas-sifier training. However, labeling documents always needs domain knowledge and thus is difficult, time consuming and costly. On the other hand, it is much easier to obtain a large number of unlabeled documents, such as web pages, news-papers and journal articles. In recent years, a new approach called active learning [1, 3, 5, 6, 9, 11, 13, 14, 15, 16, 18, 20, 25] has been developed in the machine learning community with the goal of reducing the labeling cost by identifying and presenting the most informative examples from the un-labeled examples for the human experts to label.

Although a lot of work has been done in active learning re-search, most of the existing active learning methods are still far from satisfactory with apparent shortcomings. In par-ticular, many methods only take into consideration partial information to determine the informativeness of examples. Some methods consider information conveyed by the class boundaries, some consider information conveyed by the dis-tribution of unlabeled data, and some consider the disagree-ment between learners when multiple learners are involved. Unfortunately, none of these methods is consistently better than others in all situations. Another drawback is that most active learning algorithms select only one example at a time for labeling. Compared with a batch approach [7, 9, 10] which selects multiple examples in one iteration, this greedy incremental approach is at best suboptimal and is not suit-able for large-scale and parallel computing applications.
Experimental design [2, 20, 21], which is one of the state-of-the-art active learning approaches for text classification, can effectively exploit the distribution of unlabeled data while supporting batch selection at the same time. Despite their appealing properties, existing methods based on exper-imental design cannot make use of label information even when labeled data are available. Thus, these methods are intrinsically unsupervised in nature.

In this paper, we propose a novel batch mode active learn-ing algorithm, called s upervised e xperimental d esign (SED), which incorporates label information into the experimental design procedure. SED is a supervised extension of exper-imental design with a new regularization term that incor-porates label information added to the objective function. To the best of our knowledge, no work has been done so far to utilize label information in the experimental design pro-cedure. Some favorable properties of SED are highlighted here: The remainder of this paper is organized as follows. In Section 2, we will introduce the notations and some related work. In Section 3, we first introduce transductive exper-imental design and then present our SED model and algo-rithm in detail. Extensive empirical studies conducted on two real-world text corpora are presented in Section 4. Sec-tion 5 concludes our paper.
Throughout this paper, we use boldface uppercase letters (e.g. X ) to denote matrices and boldface lowercase letters (e.g. x ) to denote vectors. We use tr( X ) to denote the trace of X and X T to denote its transpose. Moreover, we use calligraphic letters (e.g. A ) to denote sets and  X A X  to denote the size of A .
 Given the whole data set represented as X P  X   X  M  X  D or P = { x 1 , . . . , x M } , in which each data point x i is a D  X  1 vector, a generic active learning problem [4, 11] can be defined as selecting a subset of unlabeled data points from a candidate set X C  X   X  N  X  D or C = { x 1 , . . . , x N that if the selected data points are labeled and added to the training set for re-training the classifier, the improvement of the classifier will be maximized. We call the subset of selected data the active set and denote it as X A  X   X  K  X  D or A = { x 1 , . . . , x K } . 1
The promise of active learning is appealing because it can help to alleviate the labeled data deficiency problem commonly encountered in many supervised learning appli-cations. Existing active learning algorithms for text classifi-cation either select the most uncertain data given the current classifier [11], select the data with the smallest margin [18], select the data on which multiple classifiers disagree most with each other [5, 14, 17], or select the data that optimize some information gain [6, 13, 16, 25].

Closely related to active learning is experimental design in statistics [2]. Conventionally, experimental design consid-ers the problem of learning a predictive function f ( x ) from experiment-measurement pairs ( x i , y i ). Given that conduct-ing an experiment is expensive, experimental design seeks to select the most informative experiments to conduct such that the number of measurements needed can be reduced.
Traditional experimental design considers the following linear regression model: where y is the measurement, x is the D  X  1 feature vector of the experiment, w is the D  X  1 model parameter vector and is the noise term. the same index are not necessarily the same point, although we require that the points in C should appear in P and the points in A should appear in C .

Given a set of labeled data { ( x i , y i ) } M i =1 , the maximum likelihood estimate (MLE) of the model parameter vector w can be obtained by minimizing the residual sum of squares: where X = [ x 1 , . . . , x M ] T is a matrix of the labeled data and
If we put a spherical Gaussian prior on the noise , i.e.,  X  N (0 , 2 ), it can be proved easily that  X  w is an unbiased estimate of w with covariance:
Traditional experimental design aims at minimizing the covariance of  X  w , which characterizes the model uncertainty in some sense. Three criteria have been commonly used in the literature:
Recently, Yu et al. [20] proposed a method, called trans-ductive experimental design (TED), which selects the most informative examples by reducing the model uncertainty on all of the unlabeled data and thus effectively exploits the distribution of the unlabeled data. He et al. [8] applied sim-ilar ideas to content-based image retrieval (CBIR), where a Laplacian regularization term is added and then the model uncertainty, represented by a new covariance matrix, con-siders the smoothness among data points.

Despite the appealing properties which include clear math-ematical formulation and the ability of batch selection, algo-rithms based on experimental design often have to deal with combinatorial complexity and are NP-hard. Since the opti-mization problems involved are non-convex, the solutions obtained may correspond to poor local minima. To address this problem, some approximation methods based on convex relaxation have been developed [21, 23].
Existing active learning methods based on experimental design, such as TED, are formulated under the setting that all available data are unlabeled. As such, they cannot make use of the label information even when it is available.
Since label information has been found very useful to ex-ample (or document) selection [5, 11, 14, 16, 18], incorporat-ing label information into the example selection procedure of experimental design is a very worthwhile direction to ex-plore.

In this section, we first briefly review TED in Section 3.1 and then present our method, SED, in Section 3.2. The algorithm for SED will be summarized in Section 3.3 and its complexity analysis will be presented in Section 3.4. though the term  X  X abel X  is more appropriately used for clas-sification problems.
TED [20] seeks to choose X A from X C such that a func-tion f learned from X A has the smallest predictive variance on X P . The goal can be achieved by solving the following optimization problem: where I is an identity matrix whose dimensionality is de-termined by the problem and K is the number of examples selected. The objective function may also be considered as model uncertainty over X P . We note that it only depends on the input features of the training examples and thus is inde-pendent of the labels. This is because in the error function J ( w ; X , y ) of the linear regression model in Equation (1), the model parameter vector w is only coupled with the la-bels y i linearly and hence a second derivative with respect to w makes all the y i terms disappear.

Since the TED optimization problem is non-convex and can easily get stuck in local optima, Yu et al. [21] proposed a convex relaxation of TED (Convex TED). The optimization problem of Convex TED is defined as follows: min where the variables j , j = 1 , . . . , N , control the inclusion of examples in X C into the training set X A , the  X  1 -norm  X  X  X   X   X  X  X  enforces the sparsity of  X  , and ij denotes the j th element of  X  i . According to [20, 21], TED and Convex TED tend to select examples representative of all the unlabeled data and hence exploit the distribution of the whole data space.
Since experimental design based methods do not use la-bel information, we call them unsupervised active learning methods here. In the next subsection, we will present our supervised extension, SED, which can effectively utilize the available label information to select the most informative examples.
Given a set of labeled data points (training set), we can learn a classifier f from the data. In a typical active learn-ing setting in which labeled data are scarce, f may not be accurate enough and hence it is desirable to select some un-labeled data points for labeling to enlarge the training set. However, although f is not accurate enough, it still carries some useful information about the data points. Let f be a vector of decision values on the candidate set X C and  X  the vector after taking the absolute value of each element of f . For example, in support vector machine (SVM),  X  f in-dicates the uncertainty of the current classifier about the labels of the examples. The smaller the j th element  X  f is, the less certain is the classifier about the example. Intu-itively speaking, the most informative examples should be those with the smallest  X  f j values.

Based on the above intuition and the formulation of TED, we can choose the most informative X A by solving the fol-lowing optimization problem, where  X  f A is similar to  X  f but is defined only on the active set X A and is a user-defined parameter controlling the contribution of model uncertainty due to the current labeled data. In other words, controls the contribution of label information.

Since the optimization problem in Equation (4) is NP-hard, non-convex and can easily get trapped in local minima, we borrow ideas from [21] to reformulate it in a convex form and define our SED problem as follows.

Definition 1. Supervised Experimental Design (SED) min  X  s . t . x i  X  X P ,  X  i  X   X  N ,  X   X   X  N  X  1 ,  X   X  0 . (5) We can further prove that SED is a convex problem. Theorem 1. SED is convex w.r.t.  X  and {  X  i } .
 Proof. Let the objective function of SED be F = F 1 + F and F 2 = 2  X  T  X  f . Because  X  f is constant, F 2 is linear in  X  . Thus F 2 is convex with respect to  X  . Since F 1 is also convex with respect to  X  and {  X  i } 3 and F 1 + F 2 is a convex combi-nation of two convex functions F 1 and F 2 , F is thus convex with respect to  X  and {  X  i } . This completes the proof.
It is convenient to find the local optimum of Problem (5), which is also the global optimum, by updating  X  and {  X  i iteratively. More specifically, we can find the analytical so-lution for updating one variable while fixing the other as follows:
The proposed algorithm is summarized in Algorithm 1. The main computation of SED is to update  X  and {  X  i } . The time complexity of updating  X  (Equation (6)) is O ( M N ) and that of updating {  X  i } (Equation (7)) is O ( N 3 ). Hence, the time complexity of one iteration is O ( N 3 + M N ). Though our algorithm converges very quickly in practice, it is inter-esting and worthwhile to explore more efficient techniques to solve the problem, and we leave it as future work.
We conduct several experiments to compare SED with some other related methods. We have the following ques-tions in mind while designing and conducting the experi-ments: 1. How does SED perform when compared with other 2. How effective is label information for experimental de-Algorithm 1 Algorithm for SED 1: INPUT: 2: for t = 1 to T do 3: Train classifier f based on  X  t  X  1 . 4: Compute absolute decision values  X  f . 5: Initialize {  X  i } . 6: repeat 7: Fix {  X  i } , update  X  using Equation (6). 8: Fix  X  , update {  X  i } using Equation (7). 9: until converge w.r.t. objective value of Problem (5). 10: Choose K examples with the largest  X  values into 12: end for 13: Train classifier f based on  X  T . 14: return f 3. How does varying the size of the candidate set affect
These questions are answered in separate subsections: ques-tion 1 in Section 4.3, question 2 in Section 4.4.1, and ques-tion 3 in Section 4.4.2.
We conduct experiments on two public benchmark data sets. The first one is a subset of the Newsgroups corpus [21], which consists of 3 , 970 documents with TFIDF features of 8 , 014 dimensions. Each document belongs to exactly one of the four categories: autos , motorcycles , baseball and hockey . The other one is the Reuters data set, which is a subset of the RCV1-v2 data set [12]. We randomly choose from the original data set 5 , 000 documents with TFIDF features of 6 , 451 dimensions. Each document belongs to at least one of the four categories: CCAT , ECAT , GCAT and MCAT . Some characteristics of the two data sets are summarized in Table 1 respectively.

In the experiments, we simply treat the multi-class/label classification problem as a set of binary classification prob-lems by using the one-versus-all scheme, i.e., documents from the target category are labeled as positive examples and those from the other categories are labeled as negative examples. We use area under the ROC curve (AUC) as the performance measure to measure the overall classification performance, because in our setting, each binary classifica-tion task is unbalanced (only about 25% of the documents in each Newsgroups data set and about 30% of the documents in each Reuters data set are positive). Note that a larger value of AUC indicates a better performance.

At each iteration of our experiments, an active learning method selects a set of K = 5 unlabeled examples from the candidate set. The selected examples are then labeled and added to the training set  X  . The classifier is then trained on the expanded training set and used to predict the class labels of all documents. An AUC score is then computed based on the predictions. In order to randomize the ex-periments as well as to reduce the computational cost, we restrict the candidate set to randomly cover only a fraction of all the unlabeled documents. Ten different candidate sets are generated for each experiment and the average AUC value, together with the standard deviation, is reported.
We compare SED with four popular active learning meth-ods for text classification:
We note that all the methods use kernel ridge regres-sion, which is essentially equivalent to least squares SVM (LS-SVM), as the base classifier. LS-SVM has been re-ported to give state-of-the-art performance in text classi-fication tasks [22, 24]. Since no labeled data exists in the beginning of each experiment, we use Convex TED to select the first K = 5 examples for SED and Margin.
We first compare the five methods on the Newsgroups data set. For each method, we restrict the candidate set to cover 50% of the unlabeled data and set the parameters as = 0 . 01 , 1 = 0 . 1 max , 2 = 1. 4
The AUC values averaged over four binary classification tasks are reported in Table 2, where each row corresponds to one iteration. We use boldface numbers to indicate the best results among the five methods. It is obvious that SED con-sistently outperforms the other methods. To evaluate how significant SED outperforms other methods, we have con-ducted paired t-tests [19] on the results of SED and the sec-ond best method, Convex TED. The p-value is 2 . 37  X  10  X  5 indicating that SED achieves a significantly better result. It is not surprising that Random Sampling performs the worst because the randomly selected examples may not pro-vide much useful information to the classifier. We also note tion for the cardinality constraint  X   X   X  0  X  1. The reader is referred to [21] for details. that the methods based on experimental design, i.e., Con-vex TED and Sequential TED, perform better than Margin. This indicates that exploiting the distribution of unlabeled data can provide more useful information than selecting only examples near the class boundary. We also observe that in the beginning of the learning procedure, examples selected by Margin actually degrade the performance. This is be-cause the labeled data are scarce at that time and hence the class boundary learned by training on the labeled data is not accurate enough and hence may be misleading for document selection. Figure 1: Learning Curves on Newsgroups Data We plot the learning curves of SED, Convex TED and Margin in Figure 1. As we can see, SED performs better than its two counterparts by a large margin. This obser-vation validates that considering label information and the distribution of unlabeled data together can provide more useful information for active selection than only considering either of them.

To further understand the properties of SED, we plot the learning curves of SED, Convex TED and Margin for four binary classification tasks in Figure 2. For three categories, i.e., autos , baseball and hockey , SED consistently outper-forms the second best, Convex TED, by a large margin. For the motorcycles category, SED and Convex TED perform similarly. We note that Margin is consistently the worst with the largest variance for all tasks. We conjecture that Margin always selects the outliers, which stay close to the class boundary but are not useful to the learner. On the other hand, SED and TED can exploit the distribution of unlabeled data and hence have a smaller chance to select the outliers. We now compare the five methods on the Reuters data set. Each candidate set covers 20% of the unlabeled documents. The parameters are set as = 0 . 01 , 1 = 0 . 1 max , 2
The AUC values averaged over the four tasks are reported in Table 3. Again the best results are shown in bold. As in the Newsgroups data set, SED significantly outperforms Convex TED (the p-value of paired t-test is 2 . 26  X  10  X  5 validating the effectiveness of label information. It is in-teresting to find that Margin performs better than Convex TED. This can be attributed to two reasons. First, the data are very balanced in this data set and Margin selects the most discriminative examples without querying the out-liers. Second, the representative examples selected by TED might not be as helpful as those discriminative ones. How-ever, SED can take advantage of both criteria and always performs the best, especially in the early stage.
The learning curves of SED, Convex TED and Margin are plotted in Figure 3. From the figure, SED outperforms Convex TED and Margin especially in the early stage. This observation again validates the contribution of label infor-mation to experimental design.

We also plot the learning curves of SED, Convex TED and Margin for the four tasks in Figure 4. SED again out-performs its counterpart, Convex TED, for all tasks. It is interesting to observe that when the data are rather bal-anced, such as in MCAT , Margin performs better than Con-vex TED. This is actually possible, because when the data are balanced, discriminative examples near the class bound-ary will provide the most useful information to the learner. Note that the effectiveness of SED can be further improved if we put more weight on the label information for this task. Nevertheless, we leave the issue of automatically learning the weight of label information to our future research.
As we have discussed in Section 3, the contribution of la-bel information is controlled by the parameter 2 . If 2 = 0, we do not use the label information at all and hence our method degenerates to Convex TED; as 2 increases, we put larger weight on the label information. To evaluate the contribution of label information, we carry out a set of ex-periments by varying the value of 2 in the autos task of the Newsgroups data set. As before, each candidate set covers 50% of the unlabeled documents and the parameters are set to be = 0 . 01 , 1 = 0 . 1 max .

The learning curves of SED with different 2 values are plotted in Figure 5. As we can see, using a large enough 2 value, e.g. 2 = 1, can greatly speed up the learning procedure, while using small values, e.g. 2 = [0 , 0 . 1], will not improve much.

This observation validates the effectiveness of label infor-mation for experimental design. It should be noted that if is too large, e.g. 2 = 10 or 100, the learning rate will be slower than that with moderate 2 values in the early stage of learning.

This is because the training set is too small in this stage and the class boundary learned is not very accurate, so adopting too large 2 values will mislead example selection by querying the outliers. The risk can be mitigated as the size of the training set increases. We also note that choos-ing 2 = 1 will achieve the best performance not only in the early stage but also in the later stage.

Similar experiments are conducted for the MCAT task of the Reuters data set. As before, the random candidate sets cover 20% of the unlabeled documents and the parameters are set to be = 0 . 01 , 1 = 0 . 1 max . The learning curves of SED with different 2 values are plotted in Figure 6.
From Figure 6, it is interesting to observe that, differ-ent from what we have found in Figure 5, adopting a larger value of 2 will always improve the active learning proce-dure. This is because in the MCAT task, about 50% of the documents are positive, but in the autos task, only 25% of the documents are positive. Since the data distribution is more balanced in the MCAT task, adopting a larger value of will always choose those discriminative examples without taking the risk of querying the outliers.
In this section, we conduct several experiments to investi-gate the effect of the candidate set size by randomly choos-ing 20%, 40%, 60% and 80% of the unlabeled documents to form different candidate sets. For the autos task, the param-eters are set to be = 0 . 01 , 1 = 0 . 1 max , 2 = 1, and the learning curves for different candidate set sizes are plotted in Figure 7. For the MCAT task, the parameters are set to be = 0 . 01 , 1 = 0 . 1 max , 2 = 10, and the learning curves are plotted in Figure 8.

As we can see from Figure 7, using a larger candidate set will greatly speed up the learning procedure. We note that as the size of the candidate set increases, the performance gap between the learned classifiers becomes smaller. How-ever, in Figure 8, the learning curves are not so sensitive to the candidate set size as those in Figure 7. This can again be explained by the distribution of data. The more balanced the data are, the less sensitive is the method to the candi-date set size. We should note that the candidate set size has great impact on the optimization problem. Specifically, the larger the candidate set is, the longer time we need to solve the problem. Thus, in practice, we should maintain a tradeoff between performance and the time cost and use a candidate set of a reasonable size.
In this paper, we have proposed a novel active learning method, SED, to seamlessly incorporate label information into the document selection procedure of experimental de-sign. To the best of our knowledge, SED is the first work that uses label information to improve experimental design. One promising property of SED is that it can effectively use label information and the distribution of unlabeled data in a Figure 7: Effect of Candidate Set Size on Autos Task Figure 8: Effect of Candidate Set Size on MCAT Task unified framework. In particular, SED can greatly speed up the learning procedure when the distribution of unlabeled data is balanced, while existing methods based on experi-mental design always perform badly in this case. Moreover, SED can greatly outperform margin-based active learning when the distribution of unlabeled data is unbalanced. As another promising property, SED is convex and thus global optimality is guaranteed. Experiments conducted on two text corpora demonstrate that SED outperforms state-of-the-art active learning algorithms, such as TED and margin-based methods, which take into consideration only partial information.

One of our future research directions is to automatically learn from data the contribution of label information, i.e. . Another possible research direction is to apply SED to other information retrieval applications.
The authors thank Wu-Jun Li and Ning Zhu for some helpful discussions. This research has been supported by General Research Fund 622209 from the Research Grants Council of Hong Kong. [1] D. Angluin. Queries and concept learning. Mach. [2] A. Atkinson and A. Donev. Optimum Experimental [3] D. Cohn, L. Atlas, and R. Ladner. Improving [4] D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active [5] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An [6] Y. Guo and R. Greiner. Optimistic active learning [7] Y. Guo and D. Schuurmans. Discriminative batch [8] X. He, W. Min, D. Cai, and K. Zhou. Laplacian [9] S. C. Hoi, R. Jin, and M. R. Lyu. Large-scale text [10] S. C. Hoi, R. Jin, J. Zhu, and M. R. Lyu. Batch mode [11] D. D. Lewis and W. A. Gale. A sequential algorithm [12] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. RCV1: [13] D. MacKay. Information-based objective functions for [14] A. McCallum and K. Nigam. Employing EM and [15] H. T. Nguyen and A. Smeulders. Active learning using [16] N. Roy and A. McCallum. Toward optimal active [17] H. S. Seung, M. Opper, and H. Sompolinsky. Query by [18] S. Tong and D. Koller. Support vector machine active [19] Y. Yang and X. Liu. A re-examination of text [20] K. Yu, J. Bi, and V. Tresp. Active learning via [21] K. Yu, S. Zhu, W. Xu, and Y. Gong. Non-greedy [22] J. Zhang and Y. Yang. Robustness of regularized [23] L. Zhang, C. Chen, W. Chen, J. Bu, D. Cai, and [24] T. Zhang and F. J. Oles. Text categorization based on [25] X. Zhu, J. Lafferty, and Z. Ghahramani. Combining
