 Betweenness centrality measures the importance of a vertex by quantifying the number of times it acts as a midpoint of the shortest paths between other vertices. This measure is widely used in network analysis. In many applications, we wish to choose the k vertices with the maximum adaptive betweenness centrality, which is the betweenness centrality without considering the shortest paths that have been taken into account by already-chosen vertices.

All previous methods are designed to compute the be-tweenness centrality in a fixed graph. Thus, to solve the above task, we have to run these methods k times. In this paper, we present a method that directly solves the task, with an almost linear runtime no matter how large the value of k . Our method first constructs a hypergraph that encodes the betweenness centrality, and then computes the adaptive betweenness centrality by examining this graph. Our tech-nique can be utilized to handle other centrality measures.
We theoretically prove that our method is very accurate, and experimentally confirm that it is three orders of mag-nitude faster than previous methods. Relying on the scal-ability of our method, we experimentally demonstrate that strategies based on adaptive betweenness centrality are effec-tive in important applications studied in the network science and database communities.
 E.1 [ Data ]: Data Structures X  Graphs and networks Adaptive betweenness centrality; Adaptive coverage central-ity; Randomized algorithm
The centrality of a vertex measures its relative importance within a graph. There is no unique way of measuring the importance of vertices. In fact, many criteria of a vertex have been used to define its centrality, such as its degree, distance to other vertices, and its eigenvalue [ 4].
In this paper, we consider centralities based on shortest paths. The most famous such centrality is the (shortest-path) betweenness centrality , which quantifies the number of times a vertex acts as a midpoint of the shortest paths be-tween other vertices [12 ]. Betweenness centrality has been extensively used in network analysis, such as for measuring lethality in biological networks [17 , 10], studying sexual net-works and AIDS [ 20], and identifying key actors in terrorist networks [18 , 9]. Betweenness centrality is also used as a primary routine for clustering and community identification in real-world networks [23 , 15 ].

We also study the (shortest-path) coverage centrality , which measures the importance of a vertex by counting the number of pairs of vertices with a shortest path passing through it. Coverage centrality naturally arises in the study of indexing methods for determining shortest-path distances [ 2, 25 , 1].
Although computing the betweenness centrality and cov-erage centrality is already quite costly, in many applications, we want to compute these centralities in an iterative way. Consider the problem of suppressing epidemics with immu-nization [16 ]. In this problem, a vertex is infected by a dis-ease, and the disease spreads to other vertices under some stochastic process. We have a limited number k of vaccines, and the task is to find an effective strategy to vaccinate ver-tices to suppress the epidemic. For this task, it is known that greedily choosing k vertices with the maximum adaptive be-tweenness centrality often shows a good performance [ 16 ]. Here, the adaptive betweenness centrality denotes the be-tweenness centrality without considering the shortest paths that have been taken into account by already-chosen ver-tices.

In the indexing method for determining shortest-path dis-tances proposed by [2 ], it is desirable to compute the vertex ordering obtained by greedily taking vertices with the max-imum adaptive coverage centrality, where adaptive coverage centrality is defined analogously.

An obvious drawback of using the top-k vertices, or the vertex ordering with respect to adaptive centrality, is that obtaining these is computationally expensive. Indeed, the fastest exact methods for computing the top-k vertices with respect to adaptive betweenness and coverage centrality take O ( knm ) time and O ( kn 2 m ) time, respectively, where n is the number of vertices and m is the number of edges. To ameliorate this issue, faster randomized approximation al-gorithms have been proposed for the betweenness central-gorithm k times to obtain the top-k vertices, we still need at least  X ( k ( n + m )) time. Hence, the effectiveness of the top-k vertices and the vertex ordering with respect to adap-tive centrality has only been confirmed for small graphs, and dealing with larger graphs remains a challenging problem.
The main contribution of this paper is to present an al-gorithm that approximately computes the vertex ordering (and hence the top-k vertices) with respect to the adaptive betweenness and coverage centrality in almost linear time. This is remarkable, as finding the (single) vertex with the maximum centrality requires linear time.

Let us explain the idea of our algorithm using the coverage centrality. In this paper, we only consider undirected and unweighted graphs, but it is fairly straightforward to extend our result for directed and weighted graphs. For a graph G = ( V,E ), the (shortest-path) coverage centrality of a vertex v  X  V is defined as where P st is the set of all vertices on shortest paths between s and t . In other words, C ( v ) is the number of pairs such that some shortest path between them goes through v . For a vertex v and a vertex set S  X  V , the adaptive coverage centrality (ACC) of v conditioned on (having chosen) S is defined as Hence, we ignore pairs ( s,t ) if some shortest path between them is already covered by a vertex in S . Note that C ( v ) = C ( v | X  ) for any v . Our goal is to keep choosing vertices with the maximum ACC conditioned on the set of already-chosen vertices.

To this end, we construct a hypergraph H , which we call the hypergraph sketch , that encodes all information for es-timating ACC as follows. First, we randomly sample M =  X (log n/ 2 ) pairs of vertices, where is an error parameter. For each sampled pair ( s,t ), we add to H a hyperedge with the vertex set P st . Then, our algorithm for obtaining the vertex ordering is very simple: we keep choosing vertices with the maximum degree in the hypergraph after remov-ing the hyperedges incident to already-chosen vertices. The runtime of this algorithm is almost linear, and we have a good accuracy if M is chosen to be sufficiently large.
Our idea is applicable for obtaining the vertex ordering based on the adaptive betweenness centrality. For a graph G = ( V,E ), the (shortest-path) betweenness centrality of a vertex v  X  V is defined as where  X  st is the number of shortest paths between s and t , and  X  st ( v ) is the number of shortest paths between s and t going through v . For a vertex v  X  V and a vertex set S  X  V , the adaptive betweenness centrality (ABC) of v conditioned on (having chosen) S is defined as where  X  st ( v | S ) is the number of shortest paths between s and t passing through v , but not passing through any vertex in S . Our goal is to keep choosing vertices with the max-imum ABC conditioned on the set of already-chosen ver-tices. Though we can use the hypergraph sketch again, the algorithm becomes more involved than the case of ACC, be-cause we must impose weights on the vertices and update them each time we choose a vertex. However, the resulting runtime remains almost linear.

For both centralities, we prove that our method has a high probability of estimating the adaptive centrality of any vertex with additive error of n 2 . This additive error should not be critical in most applications, as numerous real-world graphs actually have vertices of centrality  X ( n 2 ), and it often suffices to obtain the correct ordering among these.
For both centralities, we can define the centrality of a ver-tex set, and our method can be seen as an approximation of the problem of finding a set of k vertices with the maximum centrality. We prove that our method has almost the same approximation ratio as the exact greedy method, which is known to work quite well in practice. Hence, the quality of the output of our method should be very high.

We also empirically show that our method runs two or three orders of magnitude faster than previous methods.
Now that we have a method to obtain the vertex ordering based on adaptive centrality, we investigate the two applica-tions mentioned before: suppressing epidemics with immu-nizations, and constructing indices for determining shortest-path distances. Our experiments demonstrate that the strat-egy based on vertex ordering with respect to adaptive cen-trality shows a better performance than other heuristic strate-gies in large graphs, which we have not been able to confirm due to the prohibitively large runtime of previous methods.
As we have mentioned, there are plenty of studies on com-puting the betweenness centrality. Currently, the fastest known algorithm for exactly computing the betweenness cen-tralities of all the vertices is that proposed by Brandes [ 6], which first solves the single-source shortest-path problem (SSSP) from every vertex. An SSSP computation from a vertex s produces a directed acyclic graph (DAG) encoding all shortest paths starting at s . By backward aggregation, the contributions of these paths to the betweenness central-ity can be computed in linear time. In total, Brandes X  al-gorithm runs in O ( nm ) time for unweighted graphs, and O ( nm + n 2 log m ) time for weighted graphs.

Brandes and Pich [ 8] investigated how the exact algorithm can be turned into an approximation algorithm by sampling a small set of vertices, from which we solve the SSSP. A random sample of starting vertices turns out to work well. To obtain better accuracy and runtime, several sampling and aggregation methods have been studied [ 14 , 3, 24 ]. However, none of these methods is sufficiently fast when we wish to obtain the vertex ordering based on the adaptive centrality.
Lee et al. [19 ] considered the problem of updating the betweenness centrality when the graph changes. However, their method is not sufficiently fast to handle large graphs, say, millions of vertices.

Borgs et al. [ 5] considered the problem of influence maxi-mization . In this problem, vertices are iteratively influenced by neighbors under some stochastic process, and the objec-tive is to find a seed set of k vertices that maximizes the expected number of influenced vertices at the end of the process. To efficiently implement a greedy algorithm, they proposed a method that constructs a hypergraph so that the degree of a vertex represents its influence. Though their method has a similar flavor to ours, we consider completely unrelated problems.
We first explain our method for ACC, and present its the-oretical analysis in Section 2, because this is much simpler than for ABC. We then proceed to ABC in Section 3. In Sec-tion 4, we empirically demonstrate the accuracy and runtime of our method. Section 5 is devoted to confirming the effec-tiveness of the vertex ordering based on adaptive centrality using the two applications.

For a given graph, we always use the symbols n and m to denote the number of vertices and edges, respectively.
We formalize the problem of computing the top-k vertices with respect to ACC. Let G = ( V,E ) be a graph. For two vertices s and t , let P st be the set of all vertices on any shortest path between s and t . By orienting the edges in P st according to the distance from s , we can obtain a DAG rooted at s . We often identify P st with this DAG. We say that a pair ( s,t ) is covered by a vertex v  X  V if v  X  P Similarly, we say that a pair ( s,t ) is covered by a vertex set S  X  V if S  X  P st 6 =  X  . In other words, ( s,t ) is covered by v (resp., S ) if some shortest path between s and t goes through v (resp., some vertex in S ).
 The coverage centrality of a vertex v  X  V is which is the number of pairs of vertices ( s,t ) that is covered by v . We extend the notion to a set of vertices. For a set of vertices S  X  V , the coverage centrality of S is which is the number of pairs of vertices ( s,t ) covered by S .
Suppose that, given a parameter k &gt; 0, we want to find a set of k vertices with the maximum coverage centrality. We call this problem MCC k , which stands for the maximum coverage centrality problem with parameter k . A natural strategy for MCC k is to keep choosing vertices that cover the maximum number of pairs that have not been covered by already-chosen vertices. This strategy motivates us to define the following notion. For a vertex v and a vertex set S , we define the ACC of v conditioned on S as Related to this definition, we say that a vertex v covers the pair ( s,t ) conditioned on S if v  X  P st and S  X  P st The following proposition is obtained immediately from this definition.

Proposition 2.1. For any vertex v and vertex set S , we have The greedy strategy keeps choosing vertices with the maxi-mum ACC conditioned on the set of already-chosen vertices, or from Proposition 2.1 , we can say that it keeps choosing vertices that maximize the resulting coverage centrality.
Given a set of vertices S , an obvious way to exactly com-pute C ( v | S ) for all v  X  V is the following. First, for each pair ( s,t ), we obtain P st by performing a breadth-first search (BFS). If P st contains a vertex in S , then the pair is already covered. Otherwise, vertices in P st cover the pair ( s,t ) con-ditioned on S . In this way, we can compute C ( v | S ) for all v  X  V in O ( n 2 m ) time. If we want to run the greedy algorithm to obtain a solution for MCC k , we need O ( kn time. On the other hand, our method runs in almost linear time, even when k = n .
We describe our method for computing the top-k ver-tices with respect to ACC. Given a graph G = ( V,E ), we first build a hypergraph H that encodes all the information needed to compute the ACC as follows. The vertex set of H is V . Given a parameter M , we pick M pairs of vertices at random. Then, for each pair ( s,t ), we add P st to H as a hyperedge (Algorithm 1). Let E ( H ) denote the set of hyper-edges in H , and for a hyperedge e  X  E ( H ), let V ( e ) denote the set of vertices incident to e .

The degree d H ( v ) of a vertex v  X  V in H is the number of hyperedges incident to v . Similarly, we define the degree d ( S ) of a vertex set S  X  V in H as the number of hy-peredges incident to S . That is, d H ( S ) = # { e  X  E ( H ) | V ( e )  X  S 6 =  X  X  . Note that d H ( { v } ) = d H ( v ). The following observation is crucial.
 Lemma 2.2. For any vertex set S  X  V , we have
Proof. Each time we sample a pair ( s,t ), the probability that P st contains a vertex in S is exactly C ( S ) /n 2 . Hence, the lemma holds from the additivity of expectation. e C ( S ) is a random variable, and its expectation is C ( S ). We will later show that d H ( S ) is highly concentrated on its ex-pectation, and we can use e C ( S ) as a good approximation to C ( S ).

We can estimate the coverage centralities of vertices in G using the hypergraph H , and, in particular, find the vertex with the maximum coverage centrality. We now proceed to find the vertex v with the maximum C ( v | S ), given a set S of already-chosen vertices. Note that C ( v | S ) = C ( S  X  { v } )  X  C ( S ) from Proposition 2.1 . Hence, we can approximate C ( v | S ) by where H  X  S is the hypergraph obtained from H by removing S and all hyperedges incident to S . With this observation, to estimate C ( v | S ), we can simply use the degree of v in the hypergraph H  X  S . Algorithm 2 describes how to compute the top-k vertices with respect to ACC. We choose M = O (log n/ 2 ) to guarantee the quality of its output.
In this section, we first show that, for any set of vertices S , e C ( S ) has a high probability of being a good approximation Algorithm 1 Build-Coverage-Hypergraph ( G,M ) Input: A graph G = ( V,E ) and an integer M  X  1. 1: Initialize H = ( V,  X  ). 2: for i = 1 to M do 3: Pick a pair of vertices ( s,t ) at random. 4: Add a hyperedge with the vertex set P st to H . 5: return H .
 Algorithm 2 Top-k -ACC ( G,k ) Input: A graph G = ( V,E ) and an integer k  X  1. 1: M  X  O (log n/ 2 ). 2: H  X  Build-Coverage-Hypergraph ( G,M ). 3: for i = 1 to k do 4: v i  X  arg max v { d H ( v ) } . 5: Remove v i and all hyperedges incident to v i . 6: return the set { v 1 ,...,v k } . to C ( S ). Then, we examine the quality of the output of Top-k -ACC . We say that an algorithm is an (  X , X  ) -approximation algorithm to a maximization problem if, for any instance of the problem, it outputs a solution whose value is at least  X   X  opt  X   X  , where opt is the optimal value of the instance. When  X  = 0, this is simply called an  X  -approximation al-gorithm . We will show that Top-k -ACC is a (1  X  1 /e,n approximation algorithm to MCC k .

To show that e C ( S ) is a good approximation to C ( S ), we recall Hoeffding X  X  inequality:
Lemma 2.3 (Hoeffding X  X  inequality). Let X 1 ,...,X n be independent random variables in [0 , 1] and X = 1 n P n Then,
It is not hard to show that, with high probability over the construction of H , we can estimate the coverage centrality to within a small error: Lemma 2.4. For any set of vertices S  X  V , we have over the construction of the hypergraph H .

Proof. From Lemma 2.2 , we have E [ e C ( S )] = C ( S ). Be-cause e C ( S ) is the sum of M random variables whose values are in [0 , 1], Hoeffding X  X  inequality implies that = Pr n = Pr 1 By choosing M = 2 log(2 n 3 ) / 2 = O (log n/ 2 ), we have the desired result.

Now we show that our method is a (1  X  1 /e,n 2 )-approximation algorithm to MCC k . We note that the problem can be seen as a monotone submodular function maximization problem . A function f : 2 V  X  R is called monotone if and is submodular if f ( S  X  X  e } )  X  f ( S )  X  f ( T  X  X  e } )  X  f ( T )
In the monotone submodular function maximization prob-lem (MSFM for short), we are given a non-negative mono-tone submodular function f : 2 V  X  R and a parameter k , and the objective is to find a set S  X  V of k elements that maximizes f ( S ). A standard heuristic for this problem is a greedy algorithm, that is, we keep choosing elements v that produce the maximum marginal gain f ( S  X  X  v } )  X  f ( S ), where S is the set of already-chosen vertices. The following fact is well known.

Lemma 2.5 ([21 , 22 ]). The greedy algorithm is a (1  X  1 /e ) -approximation algorithm to MSFM, and obtaining a better approximation ratio requires an exponential number of queries in | V | .

We can exploit this result to solve MCC k . We need the following properties of coverage centrality.

Lemma 2.6. The function C : 2 V  X  Z is non-negative, monotone, and submodular.

Proof. As non-negativity and monotonicity are clear, we consider the submodularity. Let S  X  T  X  V and v  X  V . C ( v | T ). Hence, it suffices to show C ( v | S )  X  C ( v | T ), but this should be true because the number of pairs newly covered by v is larger when having chosen a smaller set of vertices.

Corollary 2.7. The greedy algorithm is a (1  X  1 /e ) -approximation algorithm to MCC k .

Of course, we do not want to run the greedy algorithm on the function C , as evaluating C would take a very long time. Instead, we show that Algorithm 2 has almost the same quality, that is, it is a (1  X  1 /e,n 2 )-approximation algorithm. We first show the following.

Lemma 2.8. The function e C : 2 V  X  Z is non-negative, monotone, and submodular for any realization of H .
Proof. As non-negativity and monotonicity are clear, we consider the submodularity. Let S  X  T  X  V and v  X  V . e C ( v | T ). Hence, it suffices to show e C ( v | S )  X  e but this should be true because the number of hyperedges newly incident to v is larger when having chosen a smaller set of vertices.
 Theorem 2.9. Let e S be the output of Algorithm 2, and S  X  be the optimal solution to MCC k . Then, with a probability of at least 1  X  1 /n ,
Proof. Let e S  X  = arg max S  X  V : | S | = k e C ( S ). By Lemma 2.2 , there is a probability of at least 1  X  1 n 3 that e C ( S n 2 / 2. In particular, e C ( e S  X  )  X  C ( S  X  )  X  n 2 / 2.
Algorithm 2 runs the greedy algorithm on the function e C , and outputs the set e S . Because e C is a non-negative submodular function by Lemma 2.8, we have e C ( e S )  X  (1  X  1 /e ) e C ( e S  X  ) by Lemma 2.5 .

Let e S i be the set of vertices chosen up to and including the i -th iteration (with e S 0 =  X  ). In particular, e S k = i -th iteration, we consider each set of the form e S i  X  1 where v is a vertex. There are at most n of these sets, and hence the union bound implies that C and e C differ by at most n 2 / 2 on each of these sets, with probability at least 1  X  1 /n 2 . In particular, | e C ( e S i )  X  C ( e S i ) | &lt; n the union bound over all iterations, we have that | e C ( C ( e
S k ) | &lt; n 2 / 2 with probability at least 1  X  1 /n . Therefore, we have conditioned on an event of probability 1  X  1 /n .
 Hence, there is a high probability that Algorithm 2 outputs a (1  X  1 /e,n 2 )-approximation. In Section 4, we will see that the output is close to that of the exact greedy algorithm. We finally consider the runtime of Algorithm 2.

Theorem 2.10. Algorithm 2 can be implemented to run in O (( n + m ) log n/ 2 ) time.

Proof. Build-Coverage-Hypergraph clearly runs in O (( n + m ) log n/ 2 ) time. For Top-k -ACC , we will maintain a list of vertices sorted by their degree in H ; this will enable us to iteratively choose the vertex with the maximum degree in constant time. We need O ( n log n ) time for the initial sort. We must bound the time needed to remove an edge from H and correspondingly update the sorted list. The sorted list is implemented as a doubly linked list of groups of vertices, where each group itself is implemented as a doubly linked list containing all vertices of a given degree (with only non-empty groups present). Each edge of H will maintain a list of pointers to its vertices. When an edge is removed, the degree of each vertex in the edge decreases by 1. We modify the list by shifting any decremented vertex to the preceding group (creating new groups and removing empty groups as necessary). The time taken to remove an edge from H and update the sorted list is therefore proportional to the size of the edge. As each edge in H can be removed at most once over all iterations of Top-k -ACC , the total runtime is at most the sum of degrees in H , which is at most O (( n + m ) M ) = O (( n + m ) log n/ 2 ). In this section, we describe our method for ABC.

We formalize the problem of computing the top-k vertices with respect to ABC. Let G = ( V,E ) be a graph. For a vertex v  X  V , we define the betweenness centrality of v as where  X  st is the number of shortest paths between s and t , and  X  st ( v ) is the number of shortest paths between s and t passing through v . For a vertex set S  X  V , we similarly define the betweenness centrality of S as where  X  st ( S ) is the number of shortest paths between s and t passing through some vertex in S \{ s,t } . The between-ness centrality of a set is also called the group betweenness centrality in [11 , 13 ].

Suppose that, given a parameter k &gt; 0, we want to find a set of k vertices with the maximum betweenness centrality. We call this problem MBC k , which stands for the maximum betweenness centrality problem with parameter k . Similar to the coverage centrality, a natural strategy for MBC to keep choosing vertices v that maximize the resulting be-tweenness centrality. It is convenient to introduce some def-initions. For vertices v  X  V , s,t  X  V \{ v } , and a vertex set S  X  V , let  X  st ( v | S ) denote the number of shortest paths passing through v but avoiding all vertices in S \{ s,t } . In particular,  X  st ( v | S ) is zero when v  X  S . For a vertex v and a vertex set S , the ABC of v conditioned on S is
Proposition 3.1. For any vertex v and vertex set S , we have Proof.

B ( S  X  X  v } ) = X = X By Proposition 3.1 , we can say that the greedy strategy keeps choosing vertices v that maximize the ABC condi-tioned on the set of already-chosen vertices.

For a vertex set S , we can exactly compute B ( v | S ) for all v simultaneously using the following variant of Brandes X  algorithm [ 6]. For s  X  V \{ v } , let  X  s ( v | S ) = P Because B ( v | S ) = P s  X  V \{ v }  X  s ( v | S ), it suffices to com-pute  X  s ( v | S ) for every s  X  V \{ v } . Let P s be the DAG representing the shortest paths from s , which can be con-structed by performing a BFS from s . Let succ s ( v ) be the set of successors of v in the DAG P s . In particular, dist( s,w ) = distance between u and v . We process vertices in the DAG P s in reverse topological order, that is, by non-increasing distance from s . We have the following recursion:  X  ( v | S ) = X The first term in the summand deals with the contribution to  X  s ( v | S ) of the pair ( s,w ). The second term in the summand deals with the contribution to  X  s ( v | S ) of pairs ( s,t ), where t ranges over all descendants of w in the DAG P . We need the scaling factor  X  sv ( v | S )  X  Algorithm 3 Build-Betweenness-Hypergraph ( G,M ) Input: A graph G = ( V,E ) and an integer M  X  1. 1: Initialize H = ( V,  X  ). 2: for i = 1 to M do 3: Pick a pair of vertices ( s,t ) at random. 4: Make a weighted hyperedge e = { ( v,  X  st ( v )  X  5: return H .
 Algorithm 4 Top-k -ABC ( G,k ) Input: A graph G = ( V,E ) and an integer k  X  1. 1: M  X  O (log n/ 2 ). 2: H  X  Build-Betweenness-Hypergraph ( G,M ). 3: for i = 1 to k do 4: v i  X  arg max v { w H ( v ) } . 5: for Each hyperedge e incident to v i do 6: Replace it with a new weighted hyperedge 7: return The set { v 1 ,...,v k } . the edge ( v,w ). We do not give a detailed proof of this here, as the only difference from Brandes X  original algorithm is that we exclude shortest paths passing through S , and the proof remains almost the same.

Let us consider the runtime of the algorithm explained above. First, we must compute P s for every vertex s , which takes O ( nm ) time. For each DAG P s , we need O ( m ) time to calculate the recursion. Hence, in total, we require O ( nm ) time. Moreover, if we want to run the greedy algorithm to obtain a solution for MBC k , we need O ( knm ) time. Though this runtime is much better than the case of MCC k , it is still quite large. On the other hand, our method runs in almost linear time, even when k = n .
In this section, we explain our method for ABC. The idea is similar to the case of ACC, but is technically more in-volved. Algorithm 3 describes how to construct a hyper-graph sketch H for the betweenness centrality. We pick a set of M pairs of vertices ( s,t ), and for each pair ( s,t ), we add a hyperedge with a weight on each vertex. Specifically, we add to H a set of pairs { ( v,  X  st ( v )  X  In what follows, we simply call the set a hyperedge . For a hyperedge e of H , let V ( e ) be the set of vertices in e . For v 6 X  V ( e ), we set w e ( v ) = 0. Let s ( e ) and t ( e ) denote the pair ( s,t ) used to make the hyperedge e .
 For a vertex v , we define the weight of v in H as w H ( v ) = P define the weight of S in H as w H ( S ) = P e  X  E ( H )  X  Note that w H ( { v } ) = w H ( v ).
 We often use the following observation.
 Lemma 3.2. For any vertex set S  X  V ,
Proof. If we have sampled a pair ( s,t ), then the contri-to w H ( S ) over a pair chosen at random is exactly B ( S ) /n The lemma follows from the linearity of expectation. We define e B ( v ) = n 2 M w H ( v ) and e B ( S ) = n 2 to the case of coverage centrality, we can show that w H ( S ) is highly concentrated on its expectation. Hence, we can use e B ( S ) as a good approximation to B ( S ).
 Hence, we can obtain the vertex with the maximum be-tweenness centrality by choosing that with the maximum sum of weights imposed by hyperedges in H . We now show how to modify the hypergraph H so that we can compute ABC. Suppose that we have chosen a vertex set S . Because B ( v | S ) = B ( S  X  X  v } )  X  B ( S ), we want to approximate it by e
B ( v | S ) := e B ( S  X  X  v } )  X  e B ( S ) The last expression suggests how we should modify the hy-pergraph H . That is, after choosing a vertex set S , for each hyperedge e and a vertex v  X  V ( e ), we need to change However, as we only consider shortest paths between s ( e ) and t ( e ), we have  X  st ( v | S ) =  X  sv ( v | S )  X   X  v  X  P st and v 6 X  S , and we have  X  st ( v | S ) = 0 other-wise. Hence, we can compute  X  st ( v | S )  X  linear time with respect to the number of edges in the DAG P
Algorithm 4 summarizes how to compute the top-k ver-tices with respect to ABC. We choose M = O (log n/ 2 ) to guarantee the quality of its output.
We can prove the accuracy of Algorithm 4 in a similar manner to the case of the coverage centrality:
Theorem 3.3. Let e S be the output of Algorithm 4 and S  X  be the optimal solution to MBC k . Then, with a probability of at least 1  X  1 /n , Hence, there is a high probability that Algorithm 4 outputs a (1  X  1 /e,n 2 )-approximation to MBC k .
As opposed to the coverage centrality, we do not have a linear runtime in the worst case, because we must keep updating the vertex weights. However, we can bound the runtime with the following parameter
Theorem 3.4. Algorithm 4 can be implemented so that its expected runtime is O (( n + m + h log n ) log n/ 2 ) .
Proof. Build-Betweenness-Hypergraph clearly runs in O (( n + m ) M ) time.

For Top-k -ABC , we first examine the total runtime caused by updating the vertex weights. Let e be a hyperedge in H , with corresponding DAG P st . When choosing a vertex v  X  V ( e ), we need O ( | E ( P st ) | ) time to update the weights of vertices in V ( e ). In addition, we need to update the weights of vertices in e at most | P st | times throughout the algorithm. As we sample M pairs of vertices at random when constructing H , the expected total runtime is O ( hM ).
Because we must find vertices with the maximum weight at most hM times, using a standard priority queue (note that we are dealing with real values), the total time re-quired to find vertices with the maximum weight is at most O ( hM log n ).

Hence, the expected total runtime is O (( n + m ) M + hM + hM log n ) = O (( n + m + h log n ) log n/ 2 .
 Corollary 3.5. With a probability of at least 99 / 100 , Algorithm 4 outputs a (1  X  1 /e,n 2 ) -approximation to MBC in O (( n + m + h log n ) log n/ 2 ) time.

Proof. By Theorem 3.5 and Markov X  X  inequality, Algo-rithm 4 stops in O (( n + m + h ) log n/ 2 ) time with a prob-ability of at least 199 / 200. By Theorem 3.3 , we have a probability of at least 199 / 200 of obtaining the desired ap-proximation. By the union bound, we have the desired re-sult.
 In Section 4, we empirically show that h is much smaller than n + m in real-world graphs, and hence our method runs in almost linear time.
We conducted experiments on a Linux server with an In-tel Xeon E5-2690 (2.90 GHz) processor and 256 GB of main memory. The experiments required, at most, 4 GB of mem-ory. All algorithms were implemented in C++.

We considered various types of network, including social, computer, and road networks. All datasets utilized in this paper are available from the Stanford Network Analysis Project ( http://snap.stanford.edu/index.html ). All datasets were treated as undirected and unweighted graphs. Table 1 shows the basic statistics of the datasets used in our experiments. To estimate the parameter h = E s,t  X  V [ | P st | X | E ( P sampled 100,000 pairs of vertices ( s,t ) at random, and took the average of | P st | X | E ( P st ) | .

For the exact greedy algorithm for MCC k (resp., MBC k ), we use that described in Section 2 (resp., Section 3), whose runtime is O ( kn 2 m ) (resp., O ( knm )).

In this section, the symbol K denotes 2 10 = 1024.
To confirm the accuracy of our method, we conducted the following experiment for both MCC k and MBC k using the p2p-Gnutella08 dataset: For each problem, we executed the exact method to obtain the set of k vertices. We then executed our method to obtain the set of k vertices, and recomputed its exact centrality.

Figure 1 shows the accuracy of our method with various choices of k and M . For MCC k , we could only use k  X  50, as the exact method for MCC k is very slow. Centralities are divided by n 2 so that the range becomes [0 , 1] and we can easily see how the addtive error of n 2 affects the quality. As can be observed from the figure, our method is very accurate when M = 4K and M = 16K. When k = n , the accuracy of our method deteriorates, as it outputs more vertices, es-pecially when M is small. This is because our method only approximates the centrality to within n 2 , and the ordering among vertices with an adaptive centrality of less than n could be random.

Figure 2 shows the relative error of our method against the exact method using the same experimental setting as in Figure 1. As we can confirm from the figure, the relative error is less than 1% in most cases when M = 16K.

Note that the set (and the ordering) of vertices obtained by the exact method and our method may be very different. From these experiments, however, we can confirm that our method is accurate in the sense of solving MCC k and MBC k
Tables 2 and 3 summarize the runtime of our method and previous methods for MCC k and MBC k , respectively. Be-sides the exact greedy algorithm, we also implemented a sampling method that considers a fixed number, say M , of pairs of vertices, instead of all pairs, when computing the centrality. This can be viewed as a greedy algorithm that uses the method of Brandes and Pich [8 ] when computing the centralities.

When k = 50, our method runs two or three orders of magnitude faster than the exact method, and at least one order of magnitude faster than the sampling method.
The difference is even larger when k = n . In many cases, previous methods did not finish the computation within 12 h. As our method finishes in less than 30 s for all cases, our method runs at least three orders of magnitude faster than previous methods. The main reason for this is that the runtime of previous methods is proportional to k , whereas our method always runs in nearly linear time, no matter how large the value of k . We can confirm this observation from Figure 3, which illustrates how the runtime increases as k increases for the p2p-Gnutella08 dataset. There is no
Dataset MCC n MBC n M = 1K 16K M = 1K 16K soc-Epinions1 4.75 s 75.7 s 10.4 s 156 s email-Enron 1.47 s 24.0 s 3.28 s 53.0 s ego-Twitter 18.3 s 268 s 31.3 s 526 s web-Google 148 s 2333 s 316 s 6131 s as-Skitter 201 s 3541 s 472 s 7501 s line corresponding to the exact method for MCC k , as the program did not finish in 12 h.

Table 4 summarizes the runtime of our method for larger graphs, which demonstrates its scalability.
In this section, we consider two applications of adaptive centralities that we have not previously been able to inves-tigate.
Suppose a person is infected with a disease, and the dis-ease spreads out through a social network. We have a lim-ited number of k vaccines, and can immunize at most k people. The objective is to determine an effective immu-nization strategy that prevents an epidemic. This problem is well-studied in the network science community. It has been reported that a strategy based on ABC, that is, immu-nizing the top-k vertices with respect to ABC, is the most effective natural strategy [16 ]. However, as it is quite costly to compute ABC, its effectiveness has only been confirmed for small graphs.

We demonstrate the effectiveness of ABC for larger graphs using our method. To measure the effectiveness of immu-nization, we use the size of the largest connected component in the graph that results from removing vaccinated vertices, which is standard in the network science community.
Figure 4 illustrates the performance of the strategy based on ABC. We compare this with strategies based on between-ness, degree, and adaptive degree, which means the degree of the resulting graph after removing selected vertices. As we can see, the strategy based on ABC is most effective in these datasets. This improvement could have a huge impact in the context of suppressing epidemics.
Computing the distance between two vertices is a funda-mental graph operation. To quickly compute the distance, it is natural to construct an index from the input graph, and use this when answering queries. Among many others, Ak-iba et al. [ 2] proposed an efficient method based on pruned landmark labeling . Given a vertex ordering v 1 ,...,v n approach performs a BFS from v i , and records the distances from v i for each i = 1 ,...,n in this order. A non-trivial claim of this method is that, when the BFS from v i reaches a vertex w along the shortest path passing through some of v ,...,v i  X  1 , there is no need to expand vertex w in the BFS. Hence, the runtime of the i -th BFS is proportional to
In other words, after performing (pruned) BFSs from ver-tices v 1 ,...,v i , we do not have to consider pairs ( s,t ) such
Figure 4: Size of the largest connected component that P st  X  X  v 1 ,...,v i }6 =  X  , and s and t appear after v in the ordering. With this observation, it is natural to utilize the vertex ordering based on ACC.

In [ 2], the authors used a vertex ordering based on de-gree. This strategy works well when the input graph is a web graph or a social network, as the shortest paths of many pairs pass through high-degree vertices. However, the degree-based strategy does not work well for other kinds of graphs, such as road networks. For such graphs, coverage centrality might be more plausible.

Table 5 summarizes the performance of each strategy for road networks. In all datasets, we set the number of bit-parallel BFSs to be 64 (see [2 ] for details). The label size of a vertex means the number of distances recorded to that vertex. Note that the index size is (roughly) (64 + LN) n , where LN is the average label size. We also chose M = 4K in our method.

The best criteria with respect to the indexing time and the label size are strategies based on adaptive coverage and betweenness centrality, respectively. Adaptive centralities are better than their corresponding centralities in terms of the label size and the indexing time, and are comparable in terms of the ordering time. Thus, a strategy based on adaptive centrality may be preferable to one based on the corresponding centrality, which justifies the importance of adaptive centrality.

The label size of the degree-based strategy is larger than that of other strategies. Though its ordering time is much faster than other strategies, the combined ordering and in-dexing time is slightly longer than that of the ACC strategy.
To conclude, the strategy based on ACC is the best of those studied here, as it has the lowest total indexing time, and the label size is half that of the strategy developed by Akiba et al. [ 2]. ordering, and the average label size of a vertex, respectively.
We have proposed an almost linear-time approximate method for obtaining the vertex orderings with respect to the adap-tive coverage and betweenness centralities. Our method is remarkable, because simply obtaining the vertex with the maximum coverage or betweenness centrality requires linear time. The output quality of our method against the exact method is high in the sense of centralities, and our method is three orders of magnitude faster than previous methods.
Our method opens the door to use the vertex orderings with respect to the adaptive coverage and betweenness cen-tralities for large graphs, say, millions of vertices. As illus-trating examples, we have empirically shown the effective-ness of strategies based on these orderings in applications that arise from the network science and database communi-ties.
 The author thanks T. Akiba, C. Seshadhri, and T. Tak-aguchi for helpful comments.

Y. Y. is supported by JSPS Grant-in-Aid for Young Scien-tists (B) (No. 26730009), MEXT Grant-in-Aid for Scientific Research on Innovative Areas (No. 24106003), and JST, ER-ATO, Kawarabayashi Large Graph Project. [1] T. Akiba, Y. Iwata, K. Kawarabayashi, and [2] T. Akiba, Y. Iwata, and Y. Yoshida. Fast exact [3] D. A. Bader, S. Kintali, K. Madduri, and M. Mihail. [4] S. P. Borgatti and M. G. Everett. A graph-theoretic [5] C. Borgs, M. Brautbar, J. Chayes, and B. Lucier. [6] U. Brandes. A faster algorithm for betweenness [7] U. Brandes. On variants of shortest-path betweenness [8] U. Brandes and C. Pich. Centrality estimation in large [9] T. Coffman, S. Greenblatt, and S. Marcus.
 [10] A. del Sol, H. Fujihashi, and P. O X  X eara. Topology of [11] M. G. Everett and S. P. Borgatti. Extending [12] L. C. Freeman. A set of measures of centrality based [13] T. Fushimi, K. Saito, T. Ikeda, and N. Mutoh. [14] R. Geisberger, P. Sanders, and D. Schultes. Better [15] M. Girvan and M. E. J. Newman. Community [16] P. Holme, B. Kim, C. N. Yoon, and S. K. Han. Attack [17] H. Jeong, S. P. Mason, A. L. Barabasi, and Z. N. [18] V. E. Krebs. Mapping networks of terrorist cells. [19] M.-J. Lee, J. Lee, J. Y. Park, R. H. Choi, and C.-W. [20] F. Liljeros, C. R. Edling, L. A. N. Amaral, H. E. [21] G. L. Nemhauser and L. A. Wolsey. Best algorithms [22] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An [23] M. Newman and M. Girvan. Finding and evaluating [24] M. Riondato and E. M. Kornaropoulos. Fast [25] Y. Yano, T. Akiba, Y. Iwata, and Y. Yoshida. Fast
