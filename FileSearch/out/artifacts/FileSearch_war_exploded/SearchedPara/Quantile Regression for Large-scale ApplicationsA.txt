 Jiyan Yang jiyan@stanford.edu ICME, Stanford University, Stanford, CA 94305 Xiangrui Meng ximeng@linkedin.com LinkedIn Corporation, 2029 Stierlin Ct, Mountain View, CA 94043 Michael W. Mahoney mmahoney@cs.stanford.edu Dept. of Mathematics, Stanford University, Stanford, CA 94305 Quantile regression is a method to estimate the quan-tiles of the conditional distribution of a response variable, expressed as functions of observed covari-ates (Koenker &amp; Bassett, 1978), in a manner anal-ogous to the way in which Least-squares regression estimates the conditional mean. The Least Absolute Deviations regression ( i.e. , ` 1 regression) is a special case of quantile regression that involves computing the median of the conditional distribution. In con-trast with ` 1 regression and the more popular ` 2 or Least-squares regression, quantile regression involves minimizing asymmetrically-weighted absolute residu-als. Doing so, however, permits a much more accu-rate portrayal of the relationship between the response variable and observed covariates, and it is more ap-propriate in certain non-Gaussian settings. For these reasons, quantile regression has found applications in many areas (Buchinsky, 1994; Koenker &amp; Hallock, 2001; Buhai, 2005). As with ` 1 regression, the quantile regression problem can be formulated as a linear pro-gramming problem, and thus simplex or interior-point methods can be applied (Koenker &amp; D X  X rey, 1993; Portnoy &amp; Koenker, 1997; Portnoy, 1997). Most of these methods are efficient only for problems of small to moderate size, and thus to solve very large-scale quantile regression problems more reliably and effi-ciently, we need new computational techniques. In this paper, we provide a fast algorithm to com-pute a (1 + ) relative-error approximate solution to the over-constrained quantile regression problem. Our algorithm constructs a low-distortion subspace embed-ding of the form that has been used in recent devel-opments in randomized algorithms for matrices and large-scale data problems; and our algorithm runs in time that is nearly linear in the number of nonzeros in the input data.
 In more detail, recall that a quantile regression prob-lem can be specified by a (design) matrix A  X  R n  X  d , a (response) vector b  X  R n , and a parameter  X   X  (0 , 1), in which case the quantile regression problem can be solved via the optimization problem where  X   X  ( x ) = P d i =1  X   X  ( x i ), for x  X  R d , where for z  X  R , is the corresponding loss function. In the remainder of this paper, we will use A to denote the augmented matrix b  X  A , and we will consider A  X  R n  X  d . With this notation, the quantile regres-sion problem of (1) can equivalently be expressed as a constrained problem with a single linear constraint, where C = { x  X  R d | c T x = 1 } and c is a unit vector with the first coordinate set to be 1. We will focus on problems with size n d .
 Our main algorithm depends on a technical result, pre-sented as Lemma 3 below, of independent interest. Let A  X  R n  X  d be an input matrix, and let S  X  R s  X  n be a random sampling matrix constructed based on the following importance sampling probabilities, where k X k 1 is the element-wise norm, and where U ( i ) is the i -th row of an ` 1 well-conditioned basis U for the range of A (see Definition 2 below). Then, Lemma 3 states that, for a sampling complexity s that depends on d but is independent of n , will be satisfied for every x  X  R d .
 Although one could use, e.g. , the algorithm of (Das-gupta et al., 2009) to compute such a well-conditioned basis U and then  X  X ead off X  the 1-norm of the rows of U , doing so would be much slower than the time allotted by our main algorithm. Thus, to apply the ideas from Lemma 3 for fast quantile regression, we provide two algorithms, Algorithm 1 and Algorithm 2 below. As quantified by Lemma 4 and Lemma 5, re-spectively, these two algorithms provide fast construc-tion of the well-conditioned basis U and fast estimation of the ` 1 norms of U ( i ) , respectively; and both run in O (nnz( A )  X  log n ) time, where nnz( A ) is the number of nonzero elements of A .
 Given these results, our main algorithm for quantile regression is presented as Algorithm 3. Our main theorem for this algorithm, Theorem 1 below, states that, with constant probability, this algorithm returns a (1 + )-approximate solution to the quantile regres-sion problem; and that this solution can be obtained in time O (nnz( A )  X  log n ) plus the time for solving the sub-problem, whose size is O (  X d 3 log(  X / ) / 2 )  X  d , where  X  =  X  1  X   X  , for  X   X  1 / 2, independent of n . Our empirical evaluation results show that the output of our algorithm is 2-digit accurate in terms of both objective value and solution to quantile regression by sampling, e.g. , about 0 . 001% of the data. It outper-forms other conditioning-based methods, and the run-ning time of the proposed algorithm is comparable to existing sampling methods in RAM. In addition, our algorithm can be implemented in MapReduce-like en-vironments and applied to terabyte-sized problems. The best previous algorithm for moderately large quantile regression problems is due to (Portnoy &amp; Koenker, 1997) and (Portnoy, 1997). Their algorithm uses an interior-point method on a smaller problem that has been preprocessed by randomly sampling a subset of the data. Their preprocessing step involves predicting the sign of each A ( i ) x  X   X  b i , where A ( i ) b i are the i -th row of the input matrix and the re-sponse vector, respectively, and x  X  is an optimal so-lution to the original problem. When compared with our approach, they compute an optimal solution, while we compute an approximate solution; but we provide worst-case analysis that with high probability our algo-rithm is guaranteed to work, while they do not. Also, the sampling complexity of their algorithm depends on the higher dimension n , while the number of sam-ples required by our algorithm depends only on the lower dimension d ; but our sampling is with respect to a carefully-constructed nonuniform distribution, while they sample uniformly at random.
 For a detailed overview of recent work on using ran-domized algorithms to compute approximate solu-tions for least-squares regression and related prob-lems, see the recent review (Mahoney, 2011). Most relevant for our work is the algorithm of (Dasgupta et al., 2009) that constructs a well-conditioned ba-sis by ellipsoid rounding and a subspace-preserving sampling matrix in order to approximate the solu-tion of general ` p regression problems, for p  X  [1 ,  X  ), in roughly O ( nd 5 log n ); the algorithms of (Sohler &amp; Woodruff, 2011) and (Clarkson et al., 2013) that use the  X  X low X  and  X  X ast X  versions of Cauchy Transform to obtained a low-distortion ` 1 embedding matrix and solve the over-constrained ` 1 regression problem in O ( nd 1 . 376+ ) and O ( nd log n ) time, respectively; and the algorithm of (Meng &amp; Mahoney, 2013) that con-structs low-distortion embeddings in  X  X nput sparsity X  time and uses those embeddings to approximate the solution of the over-constrained ` 1 regression problem in O (nnz( A )  X  log n + poly( d ) log(1 / ) / 2 ) time. In par-ticular, we will use the method in (Meng &amp; Mahoney, 2013) for constructing ` 1 -norm well-conditioned basis matrices in nearly input-sparsity time.
 A more detailed empirical evaluation and the proofs of our main (and additional) results may be found in the associated technical report (Yang et al., 2013). We use k X k 1 to denote the element-wise ` 1 norm for both vectors and matrices; and we use [ n ] to denote the set { 1 , 2 ,...,n } . For any matrix A , A ( i ) and A denote the i -th row and the j -th column of A , respec-tively; and A denotes the column space of A . For simplicity, we assume A has full column rank; and we always assume that  X   X  1 2 . All the results hold for  X  &lt; 1 2 by simply switching the positions of  X  and 1  X   X  . Although  X   X  (  X  ) is not a norm, since the loss function does not have the positive linearity, it satisfies some  X  X ood X  properties, as stated in the following lemma: Lemma 1. Suppose that  X   X  1 2 . Then, for any x,y  X  R ,a  X  0 , the following hold:  X   X  ( x + y )  X   X   X  ( x ) +  X  ( y ) ; (1  X   X  ) k x k 1  X   X   X  ( x )  X   X  k x k 1 ;  X   X  ( ax ) = a X  and |  X   X  ( x )  X   X   X  ( y ) | X   X  k x  X  y k 1 . The following notion of a low-distortion embedding will be crucial for our method. In this paper, the  X  X ea-sure functions X  we will consider are k X k 1 and  X   X  (  X  ). Definition 1 (Low-distortion embedding) . Given A  X  R n  X  d , a measure function of vectors f (  X  ) , a matrix S  X  R s  X  n is a (1  X  ) -distortion embedding matrix of ( A ,f (  X  )) if s = poly( d ) and for all x  X  R d , We will say that S is a (1  X  ) -distortion sampling matrix if S is a (1  X  ) -distortion embedding matrix and there is only one nonzero element per row in S . The following notion of a basis, originally introduced by (Dasgupta et al., 2009), that is well-conditioned for the ` 1 norm will also be crucial for our method. Definition 2 (Well-conditioned basis) . Given A  X  R n  X  d , a basis U of A is (  X , X  ) -conditioned if k U k 1 and for all x  X  R q , k x k  X   X   X  k Ux k 1 . We will say that U is a well-conditioned basis of A if  X  and  X  are low-degree polynomials in d , independent of n .
 For completeness, note that two important ingredients for proving subspace preservation are  X  -nets and tail inequalities. Suppose that Z is a point set and k X k is a metric on Z . A subset Z  X  is called as a  X  -net for some  X  &gt; 0 if for every x  X  Z there is a y  X  Z  X  such that k x  X  y k  X   X  . It is well-known that the unit ball of a d -dimensional subspace has a  X  -net with size at most (3 / X  ) d (Bourgain et al., 1989). Also, we use the standard Bernstein inequality to prove concentration results for the sum of independent random variables. Lemma 2 (Bernstein inequality) . Let X 1 ,...,X n be independent random variables with zero-mean. Sup-pose that | X i |  X  M , for i  X  [ n ] , then for any positive number t , we have
Pr Here, we present our main technical results on low-distortion subspace-preserving embeddings and our fast randomized algorithm for quantile regression. 3.1. Main technical ingredients We start with a result which says that if we sample suf-ficiently many (but still only poly( d )) rows according to an appropriately-defined non-uniform importance sampling distribution (of the form given in Eqn. (4) below), then we obtain a (1  X  )-distortion embedding matrix with respect to the loss function of quantile re-gression. Note that the form of this lemma, the proof of which may be found in (Yang et al., 2013), is based on ideas from (Dasgupta et al., 2009; Clarkson et al., 2013).
 Lemma 3 (Subspace-preserving Sampling Lemma) . Given A  X  R n  X  d , let U  X  R n  X  d be an (  X , X  ) -conditioned basis for A . For s &gt; 0 , define and let S  X  R n  X  n be a random diagonal matrix with S ii = 1 /  X  p i with probability  X  p i , and 0 otherwise. Then when &lt; 1 / 2 and s  X  with probability at least 1  X   X  , for every x  X  R d , Remark. It is not hard to see that for any matrix S satisfying (5), the rank of A is preserved.
 Algorithm 1 Fast Algorithm for Computing Well-conditioned Basis (Meng &amp; Mahoney, 2013) 1: Input: A  X  R n  X  d with full column rank. 2: Output: R  X  1  X  R d  X  d such that AR  X  1 is an 3: Construct a low-distortion embedding matrix  X  1  X  4: Construct  X  R  X  R d  X  d such that A  X  R  X  1 is a well-5: Compute a (1  X  1 / 2)-distortion embedding matrix 6: Compute R  X  R d  X  d by ellipsoid rounding such Remark. Given such a low-distortion subspace-preserving sampling matrix, it is not hard to show that, by solving the sub-sampled problem induced by S , i.e. , solving min x  X  X   X   X  ( SAx ), then one obtains a (1 + ) / (1  X  )-approximate solution to the original problem. For more details, see (Yang et al., 2013). In order to apply Lemma 3, we need to compute the sampling probabilities in Eqn. (4). This requires two steps: first, find a well-conditioned basis U ; and sec-ond, compute the row norms of U . We now present two algorithms that will perform these two steps in the allotted O (nnz( A )  X  log n ) time.
 Consider, first, Algorithm 1, which computes a well-conditioned basis for A . This algorithm originally ap-peared as first four steps of Algorithm 2 in (Meng &amp; Mahoney, 2013), but it is included here for com-pleteness. Our main result for Algorithm 1 is given in Lemma 4.
 Lemma 4. Given A  X  R n  X  d with full rank, Algo-rithm 1 takes O (nnz( A )  X  log n ) time to compute a matrix R  X  R d  X  d such that with a constant probabil-ity, AR  X  1 is an (  X , X  ) -conditioned basis for A with  X  X   X  6 d 2 .
 Remark. The output of Algorithm 1 is not the well-conditioned matrix U , but instead it is the matrix R , the inverse of which transforms A into U .
 Remark. A well-conditioned basis for A can also be computed by other (typically more expensive) ap-proaches, e.g. , the methods from (Dasgupta et al., 2009; Sohler &amp; Woodruff, 2011; Clarkson et al., 2013) or the other algorithm of (Meng &amp; Mahoney, 2013). Consider, next, computing  X  p i from U (or from A and R  X  1 ), and note that forming U explicitly is expensive both when A is dense and when A is sparse. In prac-Algorithm 2 Fast Construction of (1  X  )-distortion Sampling Matrix of ( A , X   X  (  X  )) 2: Output: Sampling matrix S  X  R n  X  n . 4: Compute R  X  1  X  2 and construct  X  = AR  X  1  X  2  X  5: For i  X  [ n ], compute  X  i = median j  X  [ r 2 ] |  X  ij 7: Let S  X  R n  X  n be diagonal with independent en-tice, however, we will not need to form U explicitly, and we will not need to compute the exact value of the ` 1 -norm of each row of U . Indeed, it suffices to get estimates of k U ( i ) k 1 , in which case we can adjust the sampling complexity s to maintain a small approxima-tion factor. Algorithm 2 provides a way to compute the estimates of the ` 1 norm of each row of U fast and construct the sampling matrix. The same technique was used in (Clarkson et al., 2013). Our main result for Algorithm 2 is presented in Lemma 5, the proof of which can be found in (Yang et al., 2013) Lemma 5 (Fast Construction of (1  X  )-distortion Sampling Matrix) . Given a matrix A  X  R n  X  d , and a matrix R  X  R d  X  d such that AR  X  1 is a (  X , X  ) -conditioned basis for A , Algorithm 2 takes O (nnz( A )  X  log n ) time to compute a sampling matrix S  X  R t  X  n (with only one nonzero per row), such that with prob-ability at least 0 . 9 , S is a (1  X  ) -distortion sampling matrix. That is for all x  X  R d , Further, with probability at least 1  X  o (1) , t = O  X  X  X d log (  X / ) / 2 , where  X  =  X  3.2. Main algorithm Here, we state our main algorithm for computing an approximate solution to the quantile regression prob-lem. Recall that, to compute a relative-error approxi-Algorithm 3 Fast Randomized Algorithm for Quan-tile Regression 1: Input: A  X  R n  X  d with full column rank,  X  2: Output: An approximate solution  X  x  X  R d . 3: Compute R  X  R d  X  d such that AR  X  1 is a well-4: Compute a (1  X  )-distortion embedding S  X  R s  X  n 5: Return  X  x  X  R d that minimizes  X   X  ( SAx ) with re-mate solution, it suffices to compute a (1  X  )-distortion embedding matrix S . To construct S , we first com-pute a well-conditioned basis U by Algorithm 1, and we then apply Algorithm 2 to approximate the ` 1 norm of each row of U . These procedures are summarized in Algorithm 3. The main quality-of-approximation result for this algorithm is stated in Theorem 1, the proof of which can be found in (Yang et al., 2013) Theorem 1 (Fast Quantile Regression) . Given A  X  R n  X  d and  X   X  (0 , 1 / 2) , Algorithm 3 returns a vector  X  x that, with probability at least 0.8, satisfies where x  X  is an optimal solution to the original problem. In addition, the algorithm to construct  X  x runs in time where  X  =  X  1  X   X  and  X  ( s,d ) is the time to solve a quan-tile regression problem of size s  X  d . Here, we present a brief summary of our empirical eval-uation of Algorithm 3. We considered both simulated data and real data, and we considered both medium-sized data as well as terabyte-scale data. Many more details may be found in (Yang et al., 2013). 4.1. Medium-scale Quantile Regression We start with a test on simulated data with size 1 e 6  X  12 which is generated in the following way. 1. Each row of the design matrix A is a canonical 2. The true coefficient vector x  X  is a vector of size 12 3. The noise vector is generated with independent Recall, as we point out in a remark after Lemma 4, that we can use other methods for the conditioning step, i.e. , for finding the well-conditioned basis U = AR  X  1 in the first step of Algorithm 3. Here, we will consider the empirical performance of five methods for doing so, namely, ISPC, SPC, SC, NOCO, and UNIF: ISPC is the implementation of Algorithm 3 where ISPC stands for the Improved Sparse Cauchy Trans-form (called SPC2 in (Yang et al., 2013)); SPC, SC and NOCO are the variations of Algorithm 3 by using, respectively, the Sparse Cauchy Transform (Meng &amp; Mahoney, 2013), the Slow Cauchy Transform (Sohler &amp; Woodruff, 2011), and no conditioning (for all these, we compute the row norms of the well-conditioned ba-sis exactly instead of estimating them, as this may reduce the error due to the estimating step); and, fi-nally, UNIF is the uniform sampling method, which we add here for completeness.
 Rather than determining the sample size from a given tolerance , we let the sample size s vary from 100 to 1 e 5 as an input of the algorithm. Consider Figure 1, where we show the results when  X  = 0 . 95, where we draw the first and the third quartiles of the relative errors of the objective value and solution measured in infinity norm from 50 independent trials. We limit the Y axis to 100 to show more details. The relative per-formance of each method doesn X  X  change substantially when  X  takes other values. |f X  X  |/|f | From the plots we may see, for the objective value, all the three conditioning-based methods perform simi-larly. They give 2-digit accuracy when the sampling complexity is about 1000. Among these, ISPC per-forms slightly better. As expected, the two naive methods UNIF and NOCO yield large relative error when the sampling complexity s is below 1 e 4. They are not reliable if we want to solve the problem quickly while maintaining reasonable accuracy.
 Although our theory is about estimating the objective value, our conditioning-based methods also yield high accuracy on the solution, i.e. , the vector achieving the optimum. See Table 1. As we can see, ISPC performs the best. NOCO is likely to sample the outliers and UNIF performs badly due the the imbalance measure-ments in the design matrix. In order to see better the difference in the behavior of these methods, we fix the sample size to be 5000 and record the relative errors on solutions measured in three different norms. Next, we consider a real data set consisting of a 5% sample of the U.S. 2000 Census data 1 , consisting of annual salary and related features on people who re-ported that they worked 40 or more weeks in the pre-vious year and worked 35 or more hours per week. The size of the design matrix is 5  X  10 6 by 11. The per-formance of the methods on objective value is similar to that on the simulated data. Here, we will explore more on the running times. In particular, we compare the running time of our method with some compet-ing methods when the data size increases. They are the primal-dual method, referred to as ipm , and that with preprocessing, referred to as prqfn ; see (Portnoy &amp; Koenker, 1997) for more details. Fix d = 11, we let n , the large dimension, changes from 5 e 5 to 5 e 6. For each n , we extract the leading n  X  d submatrix of the census data, and record the running time of each method. The result is shown in Figure 2. From the plot we can see, ISPC runs faster than prqfn in most cases and appears to have a linear rate. time 4.2. Large-scale Quantile Regression We continue our empirical evaluation with a terabyte-scale problem that we generate by stacking 2000 copies of the census data we used in the previous section. This leads to a problem of size roughly 10 10  X  12 whose optimal solutions at different quantiles are known. At this terabyte scale, ipm has two major issues: mem-ory requirement and running time. Though shared memory machines with more than a terabyte of RAM exist, they are rare in practice. When n = 10 10 , even we have enough RAM to use, it might take a very long time to solve the problem using ipm .
 The MapReduce framework is now the de facto stan-dard parallel environment for large data analysis. Apache Hadoop 2 , an open source implementation of MapReduce, is extensively used by many companies and institutions, e.g. , Facebook, LinkedIn, and Ya-hoo!. Since our sampling algorithm only needs a few passes through the data and is embarrassingly parallel, it is straightforward to implement it on Hadoop. In Table 2, we list part of the solution computed by our randomized algorithm with a sample size 1 e 5 at different quantiles, as well as the corresponding opti-mal solution. As we may see, for most coefficients, our algorithm provides 2-digit accuracy. The quantile regression result reveals some interesting facts. For example, marriage may entail a higher salary in lower quantiles. Education 2 , whose value ranged from 0 to 256, has a strong impact on the total income, espe-cially in the higher quantiles. Also, the difference in age doesn X  X  affect the total income much in lower quan-tiles, but becomes a significant factor in higher quan-tiles.
 To summarize, our algorithm can handle terabyte-sized quantile regression problems easily, e.g. , obtain-ing 2 digits of accuracy by sampling about 1 e 5 rows on a problem of size 1 e 10  X  11; and the running time is competitive with the best existing random sampling algorithms, and it can be applied in parallel and dis-tributed environments.
 Bourgain, J., Lindenstrauss, J., and Milman, V. Ap-proximation of zonoids by zonotopes. Acta Math , 162:73 X 141, 1989.
 Buchinsky, M. Changes is US wage structure 1963-87: an application of quantile regression. Econometrica , 62:405 X 408, 1994.
 Buhai, I. S. Quantile regression: Overview and se-lected applications. Ad Astra , 4, 2005.
 Clarkson, K. L., Drineas, P., Magdon-Ismail, M., Ma-honey, M. W., Meng, X., and Woodruff, D. P. The
Fast Cauchy Transform and faster robust linear re-gression. In Proc. of the 24th Annual SODA , 2013. Dasgupta, A., Drineas, P., Harb, B., Kumar, R., and
Mahoney, M. W. Sampling algorithms and corsets for ` p regression. SIAM J. Comput. , 38(5):2060 X  2078, 2009.
 Koenker, R. and Bassett, G. Regression quantiles. Econometrica , 46(1):33 X 50, 1978.
 Koenker, R. and D X  X rey, V. Computing regression quantiles. J, Roy. Statist. Soc. Sr. C(Appl, Statis.) , 43:410  X  414, 1993.
 Koenker, R. and Hallock, K. Quantile regression. J. of Economic Perspectives , 15(4):143 X 156, 2001. Mahoney, M. W. Randomized algorithms for matri-ces and data . Foundations and Trends in Machine
Learning. NOW Publishers, Boston, 2011. Also available at: arXiv:1104.5557.
 Meng, X. and Mahoney, M. W. Low-distortion sub-space embeddings in input-sparsity time and appli-cations to robust linear regression. In Proc. of the 45th Annual STOC , 2013.
 Portnoy, S. On computation of regression quantiles: Making the Laplacian tortoise faster. Lecture Notes-
Monograph Series, Vol. 31, L 1 -Statistical Procedures and Related Topics , pp. 187 X 200, 1997.
 Portnoy, S. and Koenker, R. The Gaussian hare and the Laplacian tortoise: Computability of squared-error versus absolute-error estimators, with discus-sion. Statistical Science , 12(4):279 X 300, 1997. Sohler, C. and Woodruff, D. P. Subspace embedding for the ` 1 -norm with applications. In Proc. of the 43rd Annual ACM STOC , pp. 755 X 764, 2011.
 Yang, J., Meng, X., and Mahoney, M. W. Quantile regression for large-scale applications. Technical re-
