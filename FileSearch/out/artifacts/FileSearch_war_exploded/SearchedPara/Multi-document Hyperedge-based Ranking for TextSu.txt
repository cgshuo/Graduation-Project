 In a multi-document settings, graph-based extractive sum-marization approaches build a similarity graph out of sen-tences in each cluster of documents then use graph central-ity approaches to measure the importance of sentences. The similarity is computed between each pair of sentences. How-ever, it is not clear if such approach captures high-order re-lations among more than two sentences or can differentiate between descriptive sentences of the cluster in comparison with other clusters. In this paper, we propose to model sen-tences as hyperedges and words as vertices using a hyper-graph and combine it with topic signatures to differentiate between descriptive sentences and non-descriptive sentences. To rank sentences, we propose a new random walk over hy-peredges that will prefer descriptive sentences of the cluster when measuring their centrality scores. Our approach out-perform a number of baseline in the DUC 2001 dataset using the ROUGE metric.
 I.2.7 [ ARTIFICIAL INTELLIGENCE ]: Natural Lan-guage Processing X  Text analysis Text summarization; Text hypergraph; Multi-document sum-marization; Topic signatures
Automatic text summarization of a document is the pro-cess of automatically creating a succinct version of the doc-ument. The need for accurate text summarizers is evident for a number of areas as in information retrieval and infor-mation visualization. For instance, search engines need to create snippets of the retrieved documents to be displayed  X  This author is sponsored by King Saud University, Saudi Arabia Figure 1: Hypergraph example: s represents sen-tences, w represents words, and w t represents topical words in the search result. Additionally, short summaries are im-portant for visualizing information as in news headlines.
Text summarization is generally categorized into either generic summarization where we summarize a document to cover all the important information or query-focused summa-rization where the goal is to summarize an answer to a spe-cific question called a query. Additionally, it can be classified as either single-document summarization where we generate the summary from a single document or multi-document summarization with the summary being for multiple docu-ments. In this paper, we focus on multi-document generic summarization.

A common approach to multi-document generic summa-rization is to extract a small number of sentences that cover all topic information discussed in a group of related docu-ments. One of the well-known approaches is modeling sen-tences in a similarity graph, then choosing sentences that are central in the graph to be included in the summary, as in LexRank[5] and TextRank[8]. These approaches have shown remarkable success in summarization. However, graph-based approaches only model the similarity between pairs of sen-tences with no clear representation of word relations either. Therefore, it is not clear if they adequately cover all topical information. For example, whether a sentence contains a large number of informative words or only common words is not clear in standard graph-based approaches. To overcome such limitations, we propose to model both words and sen-tences in a hypergraph that clearly captures the high-order relations between both sentences and words. For instance, if n &gt; 2 sentences share the same group of informative words as in phrases or proper nouns, they won X  X  be the same as if each pair of the n sentences shared different words.
In this paper, we propose to model words as vertices and sentences as hyperedges in a hypergraph and approach the problem as a random walk over hyperedges. A hypergraph is a generalization of graphs that relaxed the condition of edges being pair-wise and makes high-order relation explicitly rep-resented. We rank hyperedges by assessing how much time the random walker spends in each hyperedge traveling from one to another through words. Naturally, such approach will prefer hyperedges that cover more words. Additionally, we use topic signatures[6] as a hyperedge weight to make the random walk favors sentences that are descriptive of the cluster of documents. For instance in Figure 1, words as { w t 2 ,w t 3 ,w t 5 } are topic words that should effect the ran-dom walk. Our approach shows interesting improvements on the multi-documents summarization task compared to other standard graph-based methods when tested on the Docu-ment Understanding Conference DUC 2001 datasets.
Let HG ( V, E ,w ) be a weighted undirected hypergraph with the vertex set V and the hyperedge set E . A hyperedge e is a subset of V where  X  e  X  X  e = V and w : E  X  R |E| + the hyperedge weight. A hyperedge e is said to be incident to v when v  X  e . A hypergraph has an incidence matrix The vertex and hyperedge degree are defined as follows:
D e and D v are the diagonal matrices representing the de-grees of hyperedges and vertices, respectively. W e is the diagonal matrix with the hyperedge weights.
We propose a new approach for graph-based extractive summarization that models the high-order relation between words and sentences. The high-order relations found on text is modeled using a hypergraph where words are represented as distinct vertices, and sentences are represented as hy-peredges. We formulate the problem of ranking sentences as a random walk between hyperedges (sentences) and the transition is performed over vertices (distinct words). By ranking hyperedges, we can find the most salient sentences that describe the text and use them as a summary. However, simply using random walks will naturally be biased towards longer sentences. Therefore, we define hyperedge weights by measuring the constituent words X  informativeness using topic signatures[6]. First, we will describe the calculation of hyperedge weights using topic signatures, second, we will describe the hyperedge ranking model over hypergraphs to measure the saliency score of sentences.
We design our hypergraph-based random walk approach to be drawn to descriptive sentences. Descriptive sentences are not necessarily longer sentences. They are sentences that contain a large number of descriptive words. To find out which words are descriptive and which are not, we use the log likelihood ratio known as topic signatures[6] in the summarization literature. The log likelihood ratio  X  of a given word  X  ( w ) is calculated as the ratio between observ-ing the occurrence probability P ( w ), using a binomial dis-tribution, in both the input document D I , which is the doc-ument to be summarized, and the background corpus D B . The ratio is calculated between two hypothesis: H 1 being P ( w | D I ) = P ( w | D B ) which is the null hypothesis, and H observing the occurrence probability of the word in D I be-ing greater than its probability in D B , P ( w | D I ) &gt; P ( w | D which indicates that the word is descriptive of document D We use  X  2 log X  for statistical significance testing of words X  descriptiveness which is asymptotically approximated to the  X  2 distribution. We classify words to be descriptive if their dence level of 0 . 001. This makes it possible to set a cutoff threshold for descriptive words unlike other frequency-based weighting measures as in tf or tf-idf. Words with higher  X  2 log X  values indicate a higher chance of being descriptive of the document. The hyperedge (sentence) weight could be calculated as the density of topic signatures over total words in the sentence[4]. However, such approach does not penalize long sentence effectively since longer sentences have a higher chance of containing larger number of topic signa-tures than shorter sentences. Therefore, we calculate the hyperedge weights as the following: in the sentence e i , and  X  ( e i ) 2 is the square of the length of the sentence to penalize longer sentences more effectively. Now that we define the descriptiveness score of a sentence as a hyperedge X  X  weight, we move on to describe the random walk model over hyperedges in the next section.
Traditionally, random walks are defined over vertices while transitioning is done over edges. Similarly, we can define the random walk over edges and the transitioning over ver-tices. In this paper, we are interested in ranking hyperedges instead of vertices found in a hypergraph. To rank hyper-edges, we define a new random walk that transitions from a hyperedge to another. A random walk over hyperedges is the process of transitioning between hyperedges by starting at a given hyperedge and moving to a neighboring hyperedge through shared vertices every discrete time unit t . We define a Markov chain M with different states being hyperedges. The chain is represented as a transition matrix P  X  R |E| X |E| which captures all transitions between any pair of hyper-edges.

The random walk over hypergraphs is a generalization of random walks over graphs. In graphs, a random walk over edges is described as choosing a connecting vertex and tran-sitioning to another edge incident with that vertex. In a hy-pergraph, hyperedges can be connected through more than a single vertex. Therefore, the need for generalizing the ran-dom walk to be suitable for a hypergraph structure is clear. The random walk over hyperedges is formed as two steps. First, the surfer chooses a vertex v uniformly at random from the current hyperedge e i . Second, the surfer chooses a hyperedge e j proportional to the hyperedge weight w ( e satisfying v  X  e i  X  e j . The probability of transitioning be-tween two hyperedges e i and e j is defined as follows: Or in matrix notation: Where D e is the diagonal matrix of the hyperedge degree in Equation 3. H T is the hypergraph incident matrix where rows are hyperedges and columns are vertices. D v is the diagonal matrix of the weighted vertex degrees as in Equa-tion 2. W e is a diagonal matrix with the hyperedge weights. Note that the transition matrix P is stochastic where we have every row sums to 1.

To get the stationary distribution of the hypergraph from the transition matrix P , we use the power method until con-vergence. To ensure convergence, we need to guarantee that the Markov chain is irreducible , for any two states s i ,s they must satisfy P ( s i ,s j ) &gt; 0. Also, 2) the chain is ape-riodic , where the greatest common divisor of every state riodicity, we use the PageRank algorithm [2]. The algorithm uses the idea of teleporting which will restart the random walk process making it useful for the previous conditions. The teleporting is depicted with a small probability called the damping factor  X  . Let ~v be a vector of all hyperedges to be ranked. The ranking is calculated as follows: The damping factor  X  is set to 0.85. n is the number of hyperedges in the hypergraph. ~e  X  R n  X  1 is a vector of all elements being 1.  X P T ~v means that the random walker will choose to go with one of the incident vertices. (1  X   X  ) ~e/n represents a vector of an introductory probabilities with each entry being (1  X   X  ) /n to teleport the random walk to a new hyperedge.

The intuition of this approach for text summarization is that ranking hyperedges (sentences) by traversing their vertices (words) will explicitly represent information sub-sumption of sentences more than standard graph-based ap-proaches. For instance, if we have many short sentences represented in smaller sets of words and a larger set that contains most of the small sentences the random walk will naturally prefer the more informative sentence that contain them all. Also, the hyperedge ranking approach takes into account the high-order relation of words and naturally ranks sentences that share them higher than sentences that are only pair-wise similar.

After ranking all sentences, we order them in a descending order and choose the most central sentences to be included in the summary. However, we need to ensure that sentences are of diverse nature and do not contain redundant information. To eliminate the redundancy in the summaries, we employ a reranking approach known as Maximal Marginal Relevance (MMR)[3] similar to [11]. We start by having two lists A =  X  ,B = { S 1 ,...,S n } where list B contains ranked sentences in a descending order. Then, we move the highest ranked sentence S 1 from B to A , and penalize any sentence in B that shares a relation with S 1 . Assume the rank of sentence S 1 is R ( S 1 ), the reranking of any other sentence connected to S 1 is recalculated as R ( S i ) 0 = R ( S i )  X  w S i w i S 1 is the transition probability from sentence S i to S We stop when the list B =  X  .
We used the common Task 1 of the Document Under-standing Conference (DUC 2001) benchmark dataset to eval-uate all the systems. The task was designed for generic summarization of English news articles. The dataset con-tains 308 news documents that are clustered manually to 30 clusters that represent topics. Each cluster roughly contains from 7 to 11 documents and comes with 3 human reference summaries to compare against. Since our task is a multi-document summarization, we build a summarization system to each cluster separately 1 . The length of the summaries for each cluster was set to a 100 words. For the prepro-cessing, we lowercased all characters, removed punctuations, and stem the text. For calculating the topic signatures, we used the cluster to be summarized as D I and the rest of the clusters to be D B .

To measure the performance of the summarization sys-tems, we use the common recall metric ROUGE[7]. It mea-sures the quality of the system produced summaries by the percentage of overlap with the human summaries. ROUGE produces different scores depending on the size of the tex-tual elements used for comparison. The unigram measure, ROUGE-1, has been shown to agree with human judgment the most[7]. We also show ROUGE-2 for bigrams, ROUGE-3 for trigrams, ROUGE-4, and ROUGE-L which stand for the longest common subsequence of words.

To show the effectiveness of our proposed approach, we compare it with three different baselines. The baselines con-sist of graph-based and statistical approaches that are used for multi-document summarizations. The baselines are de-scribed as follows:
Cluster d31 has been excluded since the organizers made a mistake in its summaries making the collection 29 clusters
To help validate our hypergraph random walk approach, we compare it with ranking sentences by the topic signa-tures. The results in Table 1 show that combining the random walk with the topic signature ranking helps in se-lecting more succinct sentences and performs better than using the topic signature alone. Additionally, we compare our hypergraph-based approach with other graph-based ap-proaches to test the effectiveness of modeling high-order re-lations compared to pairwise similarity. The results demon-strate that the hypergraph-based approach outperformed both the TextRank and the NE-Rank approaches.

The effectiveness of the hypergraph-based approach in text summarization is in its ability to represent subsump-tion of information in an explicit way. When a number of short sentences represented in small hyperedges are a sub-set of a larger hyperedge, the random walk will naturally prefer the larger hyperedge more. This helps in summa-rization because it will choose sentences that convey more information from shorter uninformative sentences. There-fore, when the random walk is defined over hyperedges as in our proposed approach, the coverage of information is clearer than if we model sentences as nodes which is stan-dard in graphs-based methods. Another idea behind the hypergraph approach is that by modeling sentences as hy-peredges and vertices as words, the random walk will be able to recognize high-order relation of both words and sen-tences. For instance, the approach is capable of recognizing subparts of a sentence that connects it to multiple sentences as in sets of shared words between more than two sentences. In standard graphs, the similarity is only measured between two sentences which makes the hypergraph perform better in ranking sentences.
Graph-based summarization is a well-researched area where the centrality of sentences is based on a similarity graph. Mihalcea and Tarau propose the TextRank algorithm which builds a graph of sentences when they share words between them. The pair-wise similarity of sentences is based on the edge weights being the number of overlapping words over the length of both sentences[8]. Similarly, Erkan and Radev propose LexRank which similarly builds a graph of sentences with the relation being the cosine similarity between the pair of sentences[5]. Both TextRank and LexRank use a random walk approach for ranking nodes based on eigen-vector centrality as in PageRank [2]. Wan et al. propose to use a mutual ranking of words and sentences where the relations between words, relations between sentences, and inter-relations between both sides is used for more effective ranking[9].

Wang et al. propose a hypergraph-based model for query-focused summarization where their ranking uses a semi-supervised approach to rank sentences based on their re-lation to the labeled sentence which represent the query[10]. Similar sentences are joined in a hyperedge if they belong to the same cluster. Their approach uses the high-order re-lation between sentences effectively to measure the similar-ity to the query. However, the approach is tailored towards query-focused approaches and it is not clear how to use such approach for generic summarization.
In this paper, we proposed a new hypergraph-based ap-proach for multi-document summarization. By representing words as vertices and sentences as hyperedges, our approach showed an effective way of capturing high-order relations be-tween both words and sentences. We showed a new random walk approach to rank hyperedges by transitioning through common words. Additionally, we combine hypergraphs with a statistical topic signatures approach to draw the random walk towards sentences that contain a lot of topical words. The approach showed interesting notion of combining both corpus level as in topic signatures and cluster level as in the hypergraph indicators for multi-document summarization. For future work, we plan to test our approach in more DUC datasets to show its validity. We also plan to study the effect of topic signatures on the hypergraph in more detail. Additionally, we plan to compare our approach to other summarizer besides the graph-based approaches.
