 Dyadic data matrices, such as co-occurrence matrix, rat-ing matrix, and proximity matrix, arise frequently in var-ious important applications. A fundamental problem in dyadic data analysis is to find the hidden block structure of the data matrix. In this paper, we present a new co-clustering framework, block value decomposition(BVD), for dyadic data, which factorizes the dyadic data matrix into three components, the row-coefficient matrix R , the block value matrix B , and the column-coefficient matrix C .Un-der this framework, we focus on a special yet very popular case  X  non-negative dyadic data, and propose a specific novel co-clustering algorithm that iteratively computes the three decomposition matrices based on the multiplicative updat-ing rules. Extensive experimental evaluations also demon-strate the effectiveness and potential of this framework as well as the specific algorithms for co-clustering, and in par-ticular, for discovering the hidden block structure in the dyadic data.
 E.4 [ Coding and Information Theory ]: Data compaction and compression; H.3.3 [ Information search and Re-trieval ]: Clustering; I.5.3 [ Pattern Recognition ]: Clus-tering Algorithms Co-clustering, Clustering, Matrix Decomposition, Dyadic Data, Hidden Block Structure, Block Value Decomposition (BVD), Non-negative Block Value Decomposition (NBVD).
The clustering procedure arises in many disciplines and has a wide range of applications. In many applications, such as document clustering, collaborative filtering, and mi-croarray analysis, the data can be formulated as a two-dimensional matrix representing a set of dyadic data. Dyadic data refer to a domain with two finite sets of objects in which observations are made for dyads , i.e., pairs with one element from either set. For the dyadic data in these ap-plications, co-clustering both dimensions of the data matrix simultaneously is often more desirable than traditional one-way clustering. This is due to the fact that co-clustering takes the benefit of exploiting the duality between rows and columns to effectively deal with the high dimensional and sparse data that is typical in many applications. Moreover, there is an additional benefit for co-clustering to provide both row clusters and column clusters at same time. For example, we may be interested in simultaneously cluster-ing genes and experimental conditions in bioinformatics ap-plications [4, 5], simultaneously clustering documents and words in text mining [8], simultaneously clustering users and movies in collaborative filtering.

In this paper, we propose a new co-clustering framework called Block Value Decomposition (BVD). The key idea is that the latent block structure in a two-dimensional dyadic data matrix can be explored by its triple decomposition. The dyadic data matrix is factorized into three components, the row-coefficient matrix R , the block value matrix B ,and the column-coefficient matrix C . The coefficients denote the degrees of the rows and columns associated with their clus-ters and the block value matrix is an explicit and compact representation of the hidden block structure of the data ma-trix.

Under this framework, we develop a specific novel co-clustering algorithm for a special yet very popular case  X  non-negative dyadic data, that iteratively computes the three decomposition matrices based on the multiplicative updat-ing rules derived from an objective criterion. By intertwin-ing the row clusterings and the column clusterings at each iteration, the algorithm performs an implicitly adaptive di-mensionality reduction, which works well for typical high-dimensional and sparse data in many data mining appli-cations. We have proven the correctness of the algorithm by showing that the algorithm is guaranteed to converge and have conducted extensive experimental evaluations to demonstrate the effectiveness and potential of the frame-work and the algorithms.

We define the following notations in this paper. Capital-boldface letters such as R , B ,and C denote matrices; small-boldface letters such as r , b ,and c denote column vectors; lower-case letters such as w denote scalars.
This work is primarily related to two main areas: co-clustering in data mining and matrix decomposition in ma-trix computation.

Although most of the clustering literature focuses on one-sided clustering algorithms, recently co-clustering has be-come a topic of extensive interest due to its applications to many problems such as gene expression data analysis [4, 5] and text mining [8]. A representative early work of co-clustering was reported in [11] that identified hierarchical row and column clustering in matrices by a local greedy splitting procedure. The BVD framework proposed in this paper is based on the partitioning-based co-clustering for-mulation first introduced in [11].

Information-theory based co-clustering has attracted in-tensive attention in the literature. The information bottle-neck (IB) framework [16] was first introduced for one-sided clustering. Later, an agglomerative hard clustering version of the IB method was used in [15] to cluster documents af-ter clustering words. The work in [10] extended the above framework to repeatedly cluster documents and then words. An efficient algorithm was presented in [8] that monoton-ically increases the preserved mutual information by inter-twining both the row and column clusterings at all stages. A more generalized co-clustering framework was presented in [2] wherein any Bregman divergence can be used in the ob-jective function, and various conditional expectation based constraints can be incorporated into the framework.
There have been many research studies that perform clus-tering based on SVD-or eigenvector-based decomposition [7, 3, 9, 14]. The latent semantic indexing method (LSI) [7] projects each data vector into the singular vector space through the SVD, and then conducts the clustering using traditional data clustering algorithms (such as K-means) in the transformed space. Since the computed singular vec-tors or eigenvectors do not correspond directly to individ-ual clusters, the decompositions from SVD-or eigenvector-based methods are difficult to interpret and to map to the final clusters; as a result, traditional data clustering meth-ods such as K-means must be applied in the transformed space.

Recently, another matrix decomposition formulation, Non-negative Matrix Factorization (NMF) [6], has been used for clustering [17]. NMF has the intuitive interpretation for the result. However, it focuses on one-dimension of the data matrix and does not take advantage of the duality between the rows and the columns of a matrix. we start by reviewing the notion of dyadic data. The notion dyadic refers to a domain with two sets of objects, X = { x 1 ,...,x n } and Y = { y 1 ,...,y m } in which the ob-servations are made for dyads ( x,y ). Usually a dyad is a scalar value w ( x,y ), e.g., the frequency of co-occurrence, or the strength of preference/association /expression level. For the scalar dyads , the data can always be organized as an n -by-m two-dimensional matrix Z by mapping the row indices into X and the column indices into Y . Then, each w ( x,y ) corresponds to one element of Z .

We are interested in simultaneously clustering X into k disjoint clusters and Y into l disjoint clusters. This is equiv-alent to finding block structure of the matrix Z , i.e., finding Figure 1: The original data matrix (b) with a 2  X  2 block structure which is demonstrated by the per-muted data matrix (a). The row-coefficient ma-trix R, the block value matrix B, and the column-coefficient matrix C give a reconstructed matrix (c) to approximate the original data matrix (b). k  X  l submatrices of Z such that the elements within each submatrix are similar to each other and elements from dif-ferent submatrices are dissimilar to each other. The original data matrix and the permuted matrix with explicit block structure in Figure 1 give an illustrative example.
Since the elements within each block are similar to each other, we expect one center to represent each block. There-fore a k  X  l small matrix is considered as the compact rep-resentation for the original data matrix with a k  X  l block structure. In the traditional one-way clustering, given the cluster centers and the weights that denote degrees of ob-servations associated with their clusters, one can approxi-mate the original data by linear combinations of the cluster centers. Similarly, we should be able to  X  X econstruct X  the original data matrix by the linear combinations of the block centers. Based on this observation, we formulate the prob-lem of co-clustering dyadic data as the optimization prob-lem of matrix decomposition, i.e., block value decomposition (BVD).
 Definition 1. Block value decomposition of a data matrix Z  X  n  X  m is given by the minimization of subject to the constraints  X  ij : R ij  X  0and C ij  X  0, where  X  denote Frobenius matrix norm, R  X  n  X  k , B  X  k  X  l , C  X  l  X  m , k n ,and l m .

We call the elements of B as the block values; B as the block value matrix; R as the row-coefficient matrix; and C as the column-coefficient matrix. As is discussed before, B may be considered as a compact representation of Z ; R de-notes the degrees of rows associated with their clusters; and C denotes the degrees of the columns associated with their clusters. We seek to approximate the original data matrix by the reconstructed matrix, i.e., RBC , as illustrated in Figure 1.
Under the BVD framework, the combinations of the com-ponents also have an intuitive interpretation. RB is the matrix containing the basis for the column space of Z and BC contains the basis for the row space of Z . For example, for a word-by-document matrix Z , each column of RB cap-tures a base topic of a particular document cluster and each row of BC captures a base topic of a word cluster.
Comparing with SVD-based approaches, there are two main differences between BVD and SVD. First, in BVD, it is natural to consider each row or column of a data matrix as an additive combinations of the block values since BVD does not allows negative values in R and C . In contrast, since SVD allows the negative values in each component, there is no intuitive interpretation for the negative combi-nations. Second, unlike the singular vectors in SVD, the basis vectors contained in RB and BC are not necessar-ily orthogonal. Although singular vectors in SVD have a statistical interpretation as the directions of the variance, they typically do not have clear physical interpretations. In contrast, the directions of the basis vectors in BVD have much more straightforward correspondence to the clusters. In summary, compared with SVD or eigenvector-based de-composition, the decomposition from BVD has an intuitive interpretation, which is necessary for many data mining ap-plications.

BVD provides a general framework for co-clustering. De-pending on different data types in different applications, var-ious formulations and algorithms may be developed under the BVD framework. An interesting observation is that the data matrices in many important applications are typically non-negative, such as the co-occurrence tables, the perfor-mance/rating matrices and the proximity matrices. Some other data may be transformed into the non-negative form, such as the gene expression data. Therefore, in the rest of the paper, we concentrate on a sub-framework of BVD, the non-negative block value decomposition (NBVD).
 NBVD puts an extra non-negative constraint on the B . The formal definition is given as follows.

Definition 2. Non-negative block value decomposition of a non-negative data matrix Z  X  n  X  m (i.e.,  X  ij : Z ij  X  is given by the minimization of subject to the constraints  X  ij : R ij  X  0 , B ij  X  0and C l m .

Finally, we compare NBVD with NMF [6]. Given a non-negative data matrix V , NMF seeks to find an approximate factorization V  X  WH with non-negative components W and H . Essentially, NMF concentrates on the one-way clus-tering and does not take the advantage of the duality be-tween the row clustering and the column clustering. In fact NMF may be considered as a special case of NBVD in the sense that WH = WIH ,where I is an identity matrix. By this formulation, NMF is a special case of NBVD and it does co-clustering with the additional restrictions that the number of the row clusters equals to the number of the col-umn clusters and that each row cluster is associated with one column cluster.
The objective function in (2) is convex in R , B and C respectively. However, it is not convex in all of them si-multaneously. Thus, it is unrealistic to expect an algorithm to find the global minimum. We derive an EM [1] style algorithm that converges to a local minimum by iteratively updating the decomposition using a set of multiplicative up-dating rules.

First, we prove the following theorem which is the basis of NBVD algorithm.

Theorem 1. If R , B and C are a local minimizer of the objective function in (2), then the equations are satisfied, where denotes the Hadamard product or en-trywise product of two matrices.

Proof. Let  X  1 ,  X  2 ,and  X  3 be the Lagrange multipliers for the constraint R , B ,and C  X  0, respectively, where  X  function L ( R , B , C , X  1 , X  2 , X  3 ) becomes:
L = f ( R , B , C )  X  tr(  X  1 R T )  X  tr(  X  2 B T )  X  tr(  X  The Kuhn-Tucker conditions are: Taking the derivatives, we obtain the following three equa-tions from (7), (8), and (9), respectively. Applying the Hadamard multiplication on both sides of (13), (14), and (15) by R , B ,and C , respectively, and using con-ditions (10), (11), and (12), the proof is completed. Based on Theorem 1, we propose following updating rules.
The time complexity of NBVD algorithm can be shown as
O ( t ( k + l ) nm )where t is the number of iterations. The complexity is the same as that of the classic one-way clus-tering algorithm, k-means clustering whose time complexity is
O ( tknm ). Since NBVD algorithm is simple to implement and only involves basic matrix operations, it is easy to take the benefit of distributed computing when dealing with very large data set.

The conditions in Theorem 1 are the necessary conditions but not the sufficient conditions for a local minimum. To assure that the NBVD algorithm is correct, we need to prove that the objective function (2) is nonincreasing under the updating rules (16), (17) and (18). This can be done by making use the concept of an auxiliary function similar to that used in the EM algorithm [1] and NMF [13]. Due to the space limit, we omit the details here.

Finally, we consider a special case of NBVD. In practice, there exists a special type of data, symmetric dyadic data. The notion of symmetric dyadic refers to a domain with two identical sets of objects , X = { x 1 ,...,x n } , in which the ob-servations are made for dyads ( a,b ), where both a and b are from X and dyads ( a,b )= dyads ( b,a ). Symmetric dyadic data may be considered as a two-dimensional symmetric ma-trix. For example, a proximity matrix may be considered as a symmetric dyadic data.

NBVD algorithm cannot directly be applied to non-negative symmetric dyadic data, because the extra condition R = C T needs to be satisfied. Consequently, NBVD algorithm needs to be revised to accommodate this special case. The formal definition for symmetric NBVD is,
Definition 3. Symmetric non-negative block value decom-position of a symmetric non-negative data matrix Z  X  n  X  n (i.e.,  X  ij : Z ij  X  0) is given by the minimization of  X  ij : S ij  X  0and B ij  X  0, where S  X  n  X  k , B  X  k  X  k ,and k n .
 Given this definition, we derive the updating rules for sym-metric NBVD as follows.
 Note that the symmetric NBVD provides only one clustering result though it does clustering on both dimensions of the data matrix. The symmetric NBVD is not a trivial special case of NBVD. It has a very important application, graph partition.
We conduct the performance evaluation using the various subsets of 20-Newsgroup data ( NG20 ) [12]and CLASSIC3 data set [8]. The NG20 data set consists of approximately 20 , 000 newsgroup articles collected evenly from 20 different usenet newsgroups. We have exactly duplicated this data set that is also used in [8, 10] for document co-clustering in order to ensure the direct comparability in the evaluations. Many of the newsgroups share similar topics and about 4 . 5% of the documents are cross posted making the boundaries between some news-groups rather fuzzy. To make our com-parison consistent with the existing algorithms we have re-constructed various subsets of NG20 used in [8, 10] to all the subsets, i.e., removing stop words, ignoring file headers, and selecting the top 2000 words based on the mutual informa-tion. As in [10], we include the subject line in the articles. Specific details of the subsets are given in Table 1.
Since each document vector of word-by-document matrix is normalized to have unit L 2 norm, in the implementation of the NBVD, we normalize each column of RB to have the unit L 2 norm. Assume that RB is normalized to RBV . The cluster labels for the documents are given by V  X  1 C instead of C .

We measure the clustering performance using the accu-racy given by the confusion matrix of the obtained clusters and the  X  X eal X  classes. Each entry ( i,j ) in the confusion matrix represents the number of documents in cluster i that are in true class j . Specifically, we use the micro-averaged-precision.
This section provides empirical evidence to demonstrate that, as a general co-clustering algorithm, how NBVD im-proves the document clustering accuracy in comparison with NMF[6], and two other co-clustering algorithms, Information-theoretic Co-Clustering (ICC) [8] and Iterative Double Clus-tering algorithm (IDC) [10] .

In the experiments, initial matrices are generated as fol-lows. All the elements of R and C are generated from uni-form distribution between 0 and 1 and all the elements of B are simply assigned to the mean value of the data ma-trix. Since NBVD algorithm is not guaranteed to find the global minimum, it is beneficial to run the algorithm several times with different initial values and choose one trial with a minimal objective value. In reality, usually a few number of trials is sufficient. In the experiments reported in this paper, three trials of NBVD are performed in each test run and the final results are averages for twenty test runs. The experiments for NMF are conducted in the same way.
Table 2 records the two confusion matrices obtained on the CLASSIC3 data set using NMF and NBVD, respec-tively, with 3 word clusters that is the number of true word clusters. Observe that NBVD extracted the original clusters with micro-averaged-precision of 0 . 9879 and NMF led to a micro-averaged-precision of 0 . 9866. It is not surprising that NBVD and NMF have almost the same performance on the CLASSIC3 data set. This is due to the fact that when there exists perfect one-to-one correspondence between row clus-ters and column clusters, the block value matrix B is close to the identity matrix and the NMF is equivalent to NBVD. Table 3 shows a block value matrix for the data set CLAS-SIC3 . The perfect diagonal structure of Table 3 indicates the one-to-one correspondence structure of document clus-ters and word clusters for CLASSIC3 .
 Table 4 shows the two confusion matrices obtained on the Multi5 data set by NBVD and NMF respectively. NBVD and NMF yield micro-averaged-precision of 0 . 944 and 0 . 884 respectively. This experiment shows that NBVD has a bet-ter performance than NMF on the data set Multi5 .Com-pared with CLASSIC3 , Multi5 has more complicated hidden block structure and there is no simple one-to-one relation-ship between document clusters and word clusters. This demonstrates that by exploiting the duality of the row clus-tering and the column clustering, NBVD is more powerful to discover the complicated hidden block structure of the data than NMF.

Table 5 shows the micro-averaged-precision measures on Table 2: Both NBVD and NMF accurately recover the original clusters in the CLASSIC3 data set. Table 3: A normalized block value matrix on the CLASSIS3 data set. all data sets from NG20 data. All NBVD precision values are obtained by running NBVD on the number of the true document clusters and the corresponding optimal numbers of the word clusters which are found by extra experiments that evaluate the precision at varied number of word clusters (the details are omitted due to the space limit). The peaked ITC and IDC precision values are quoted from [8] and [10], respectively. On all data sets NBVD performs better than its one-sided counterpart NMF. This result justifies the need to exploit duality between the word clustering and the doc-ument clustering. Compared with other two state-of-the-art co-clustering algorithms, NBVD shows clear improvements on precision for almost all data sets. In particular more substantial improvements are observed on the complicated data sets with more clusters, which is the typical scenario in practice.
In this section we provide empirical evidence to demon-strate the potential of the symmetric NBVD on the impor-tant application, graph partition. We still concentrates on the task of document clustering; but this time it is formed as a graph partition problem. The whole document data collection is represented as an undirected graph. Each node of the graph represents a document, and each edge ( i,j ) Table 4: NBVD extracts the block structure more accurately than NMF on Multi5 data set.
 Table 5: NBVD shows clear improvements on the micro-averaged-precision values on different news-group data sets over other algorithms. Figure 2: The symmetric NBVD shows substan-tial improvements measured as micro-averaged-precision values on the newsgroup data sets with different cluster numbers over AA and NC. is assigned a weight w ij to reflect the similarity between documents i and j . We conduct experiments on document clustering based on graph partition and show that the sym-metric NBVD has superior performance to two state-of-the-art methods, the Average Association (AA) [18] and the Normalized Cut (NC) [14].

We use the same data set, NG20 , with the same pre-processing steps defined before. Since each column of the word-document co-occurrence matrix Z has been normal-ized to the unit L 2 norm, the proximity matrix for the doc-uments is simply determined as W = Z T Z . The similarity between two documents is the cosine similarity. Similarly, micro-averaged-precision is used as the measure metric.
At each run of the test, k news groups are randomly selected from twenty newsgroups and 250 documents are randomly selected from each selected newsgroup. For each given cluster number k , 20 test runs are conducted and the final precision value is the average of the twenty test runs. As we did for the general NBVD experiments, 3 trials of the symmetric NBVD are performed in each test run.

From the performance results reported in Figure 2, it is clear that as a graph partition algorithm, the symmetric Table 6: The block value matrix of the symmetric NBVD on the Multi5 data set with 6 document clus-ters. Table 7: The block value matrix of the symmetric NBVD on the Multi5 data set with 5 document clus-ters.
 NBVD improves the document clustering precision substan-tially over AA and NC.

Finally, we apply the symmetric NBVD to the Multi5 data set to demonstrate the nice property of the block value ma-trix under the symmetric NBVD. Since the row clusters and the column clusters are identical under symmetric NBVD, the block values under the symmetric NBVD have a very intuitive interpretation. They represent the similarity or distance between the clusters. When applying the symmet-ric NBVD to the proximity matrix, the resulting block value matrix B may be considered as the generic proximity matrix for the clusters, i.e., B ij denotes the similarity between the i th clusters and j th clusters. Consequently, the better diag-onal structure B has, the better clustering we obtain. In the block value matrix of Table 6, Cluster 1 and Cluster 3 are more similar to each other than any other pair of different clusters. Thus, Cluster 1 and Cluster 3 may be merged to make a better clustering. Applying the symmetric NBVD to Multi5 with 5 document clusters that is the true number of document clusters, we obtain a perfect diagonal block value matrix shown in Table 7. The nice property of the block value matrix not only provides the intuitive information for the distribution of the clusters and the quality of the cluster-ing, but also indicates some interesting and open questions under the BVD framework, e.g., how to enforce the nice property of the block value matrix within an algorithm to make the algorithm more robust and efficient.
In this paper, we have proposed a new co-clustering frame-work for dyadic data called Block Value Decomposition (BVD). Under this framework, we focus on a special but also very popular case, Non-negative Block Value Decomposition, and have presented the specific novel NBVD algorithm. We have also reported extensive empirical evaluations to demonstrate the effectiveness and the great potential of the BVD frame-work as well as the NBVD algorithms. This research is supported in part by a grant from AFRL/ Information Institute through the award number FA8750-04-1-0234. We acknowledge the advice from Dr. John Salerno at AFRL. [1] N. M. L. A. P. Dempster and D. B. Rubin. Maximum [2] A. Banerjee, I. S. Dhillon, J. Ghosh, S. Merugu, and [3] P. K. Chan, M. D. F. Schlag, and J. Y. Zien. Spectral [4] Y. Cheng and G. M. Church. Biclustering of [5] H. Cho, I. Dhillon, Y. Guan, and S. Sra. Minimum [6] D.D.Lee and H.S.Seung. Learning the parts of objects [7] S. C. Deerwester, S. T. Dumais, T. K. Landauer, [8] I. S. Dhillon, S. Mallela, and D. S. Modha.
 [9] C.H.Q.Ding,X.He,H.Zha,M.Gu,andH.D.
 [10] R. El-Yaniv and O. Souroujon. Iterative double [11] J.A.Hartigan. Direct clustering of a data matrix. [12] K. Lang. NewsWeeder: learning to filter netnews. In [13] D. D. Lee and H. S. Seung. Algorithms for [14] J. Shi and J. Malik. Normalized cuts and image [15] N. Slonim and N. Tishby. Document clustering using [16] N. Tishby, F. Pereira, and W. Bialek. The information [17] W. Xu, X. Liu, and Y. Gong. Document clustering [18] H. Zha, C. Ding, M. Gu, X. He, and H. Simon.
