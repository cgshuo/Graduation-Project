 Hugo Gon  X alo Oliveira  X  Paulo Gomes Abstract A wordnet is an important tool for developing natural language pro-cessing applications for a language. However, most wordnets are handcrafted by experts, which limits their growth. In this article, we propose an automatic approach to create wordnets by exploiting textual resources, dubbed ECO. After extracting semantic relation instances, identified by discriminating textual patterns, ECO discovers synonymy clusters, used as synsets, and attaches the remaining relations to suitable synsets. Besides introducing each step of ECO, we report on how it was implemented to create Onto.PT, a public lexical ontology for Portuguese. Onto.PT is the result of the automatic exploitation of Portuguese dictionaries and thesauri, and it aims to minimise the main limitations of existing Portuguese lexical knowledge bases.
 Keywords Information extraction  X  Lexical ontology  X  Wordnet  X  Clustering  X  Semantic relations 1 Introduction A substantial amount of data produced every day is available in natural language text. Understanding its meaning involves more than recognising words and their interactions, and typically requires access to external sources of knowledge. This fact lead to the creation of broad-coverage knowledge bases, which can be exploited in natural language processing (NLP) tasks that perform a semantic analysis of text. The previous resources include not only large repositories covering world knowledge, but also lexical knowledge bases (LKBs), which are more linguistically grounded and structured in words and meanings. Regarding their structure, the latter are often referred to as lexical ontologies, because they share properties of lexicons as well as properties of ontologies (Hirst 2004 ; Pre  X  vot et al. 2010 ).
Despite different interpretations of the concept of lexical ontology, the paradig-matic resource of this kind is the Princeton WordNet (Fellbaum 1998 ) (hereafter, WordNet.Pr). It is a resource structured in synsets X  X roups of synonymous word senses that can be seen as possible lexicalisations of a natural language concept X  X nd semantic relations connecting synsets X  X ncluding hypernymy (a concept is a kind of another), part-of (a concept is part of another), and others.

Besides its structure, suitable for being integrated and exploited by NLP applications, the public availability of WordNet.Pr played an important role in its success. WordNet.Pr was widely accepted by the NLP community and, today, there is no doubt that the existence of such a resource has a positive impact on the computational processing of a language. This fact is evidenced for English, where WordNet.Pr opened the range of capabilities of NLP applications. It was used in the achievement of tasks, including, but not limited to, determining similarities (Agirre et al. 2009 ), word sense disambiguation (Gomes et al. 2003 ), query expansion (Navigli and Velardi 2003 ), intelligent search (Hemayati et al. 2007 ), question-answering (Pasca and Harabagiu 2001 ), and text summarisation (Bellare et al. 2004 ).
The wordnet model was also very successful and adopted in the creation of broad-coverage lexical-semantic resources for other languages [see e.g. EuroWord-Net (Vossen 1998 ) or BalkaNet (Stamou et al. 2002 )]. But, as it happens for WordNet.Pr, a huge limitation of most wordnets is that they are handcrafted. Their construction thus involves much human effort, which is a bottleneck for the growth and evolution of the resource.

This problem was tackled by the NLP community, which studied how to automatise the creation of LKBs and minimise the information sparsity issues. Textual data has been exploited in the automatic creation of LKBs from scratch (Richardson et al. 1998 ), or in the enrichment of existing LKBs (Snow et al. 2005 ). Wordnets were also linked with other knowledge bases [e.g. Gurevych et al. ( 2012 ) or Hoffart et al. ( 2011 )]. A different approach was based on the translation of a target wordnet (usually WordNet.Pr) to other languages (de Melo and Weikum 2008 ). But a problem arises because different languages represent different socio-cultural realities, they do not cover exactly the same part of the lexicon and, even where they seem to be common, several concepts are lexicalised differently (Hirst 2004 ). Therefore, we believe that a wordnet for a language, whether created manually, semi-automatically or automatically, should be developed from scratch for that language.

Having this in mind, we propose a flexible approach for the automatic acquisition, organisation and integration of lexical-semantic knowledge in a unique wordnet-like resource. The ECO approach exploits both textual sources and existing lexical-semantic resources to generate a resource structured in synsets and semantic relations. This is performed in three steps X  X elation extraction, word clustering and ontologisation X  X hat combine several information extraction techniques.

ECO was applied in the creation of Onto.PT, a public domain wordnet-like lexical ontology for Portuguese that integrates knowledge extracted from electronic dictionaries and thesauri, and intends to minimise the limitations of existing Portuguese LKBs. Its last version, at the time of writing, contains about 109,000 synsets connected by about 173,000 relations of different types. Still, given its automatic creation, Onto.PT can be further improved, and integrate knowledge from alternative sources, for a low cost.
 After introducing some related work, this article gives an abstract overview of the ECO approach and each of its steps, while discussing some of the options that can be taken for its application. Then, we present a concrete implementation of ECO, by describing how it was applied to the creation of Onto.PT, including some details on the structure of this resource and on its evaluation. We end with some concluding remarks. 2 Related work Since the 1970s, researchers have been exploiting textual resources and developing techniques towards the automatic acquisition of lexical-semantic knowledge, which could be used in the automatic creation of a broad coverage LKB. Electronic dictionaries were the primary resources exploited for this task (Calzolari et al. 1973 ; Amsler 1981 ), because they already provide an extensive coverage on words and meanings, they are created by experts on describing word senses, and they typically use systematic definitions, suitable for being exploited by automatic procedures. The first approaches to the automatic extraction of taxonomies from dictionaries were soon developed (Chodorow et al. 1985 ), but human input was still critical to perform word sense disambiguation (WSD). Continued work on information extraction (IE) from dictionaries led to to the creation of MindNet (Richardson et al. 1998 ), an independent LKB created automatically.

A few problems about semantic networks extracted from dictionaries have however been pointed out (Ide and Ve  X  ronis 1995 ). For instance, information in dictionaries differs considerably in amount and kind. So, the creation of a broad-coverage LKB requires the combination of multiple dictionaries or other sources of knowledge. Another problem is that electronic dictionaries are not always available for researchers.
 Despite several automatic attempts to the creation of a broad-coverage LKB, for English, WordNet.Pr, a manual effort, ended up being the leading resource of this kind (Sampson 2000 ). But, as the manual creation of knowledge bases is a time-consuming and tedious task, research on lexical-semantic IE from text proceeded, whether it was for the creation of new LKBs, or for the enrichment of wordnets. In fact, as a handcrafted resource covering mainly linguistic knowledge, researchers using WordNet.Pr as their only knowledge base soon had to deal with information sparsity issues.

Regarding that there is a huge amount of text available on most domains, researchers turned on to IE from less structured sources. While targeting textual corpora, unsupervised procedures have been developed for clustering words according to their distributional similarity (Lin 1998 ; Turney 2001 ), which could be used to identify synonyms and to create thesauri. On the automatic extraction of semantic relations from corpora, most research is based on Hearst ( 1992 ) X  X  method for discovering discriminating lexical-syntactic patterns. Starting with a set of seed relation instances of a certain type (e.g. hyponymy), this method identifies sequences of text that occur systematically between the related arguments. The discovered patterns can be used for the automatic extraction of new relations. Besides the combination of the previous idea with distributional measures to improve extraction (Caraballo 1999 ), fully supervised approaches were also applied to the extraction of semantic relations from text (Snow et al. 2005 ). Moreover, pattern discovery was integrated in weakly-supervised approaches that score the discovered patterns automatically, according to their reliability and frequency, and use only the higher ranked patterns (Pantel and Pennacchiotti 2006 ).

Weakly supervised approaches were also followed for extracting semantic relations from the Web (Agichtein and Gravano 2000 ). On this scope, more complex systems were developed in recent years for learning great amounts of various kinds of facts from the Web and for creating large knowledge bases [e.g. Banko et al. ( 2007 ); Carlson et al. ( 2010 ); Fader et al. ( 2011 )].
An alternative to the enrichment of a wordnet with information from text is to integrate, or link, several resources in a unique knowledge base. For instance, WordNet.Pr was linked to other lexical-semantic resources, such as FrameNet and VerbNet (Shi and Mihalcea 2005 ), to the collaborative encyclopedia Wikipedia (Suchanek et al. 2007 ; Navigli and Ponzetto 2012 ), to all the previous and Wiktionary, in English and in German (Gurevych et al. 2012 ), to the upper ontology SUMO (Pease and Fellbaum 2010 ) and to the descriptive ontology DOLCE (Gangemi et al. 2010 ).

Most of the previous approaches either extract knowledge to enrich an existing resource, or merge existing resources. The other few are not exactly concerned with the creation of a wordnet from scratch. For instance, some are not restricted to lexical-semantic knowledge, and most of them do not structure the extracted knowledge as a wordnet. Alternatively, we propose an approach that, although may take advantage of existing resources, can be used to create a wordnet automatically, completely from scratch, and in an unsupervised way. 3 The ECO approach ECO is the abstract model we propose for creating a lexical ontology, structured as a wordnet, automatically from heterogeneous textual sources of a target language. It combines several IE techniques in order to acquire, organise and integrate lexical-semantic knowledge. For this purpose, ECO encompasses three main steps, as shown in the diagram of Fig. 1 , performed in the following order: 1. Extraction instances of semantic relations, held between lexical items, are 2. Clustering clusters of synonymous lexical items are discovered from the 3. Ontologising the lexical items in the arguments of the non-synonymy relation
Each step is independent of each other and can alternatively be used in the achievement of its own task. This is an interesting way of coping with information sparsity, since it allows for the extraction of knowledge from different heteroge-neous sources (e.g. dictionaries, thesauri, corpora), and provides a way to harmoniously integrate all the acquired information in a common knowledge base. In the rest of this section, we describe each step in more detail and relate them to other works that addressed them individually. 3.1 Extraction In order to integrate lexical-semantic knowledge, ECO requires that this information is represented as relational triples, denoting instances of semantic relations. These structures are a common representation in most works on IE. They consist of two arguments connected by a relation, as in:
The extraction step deals with the automatic acquisition of relational triples connecting lexical items (terms), so we will hereafter refer to them as term-based triples (tb-triples). As mentioned earlier, this task is typically based on a set of discriminating patterns that occur in text, and indicate certain semantic relations. Depending on the relation to extract, some discriminating patterns might be intui-tive, but less intuitive patterns can be discovered automatically, in a similar fashion to Hearst ( 1992 ).

As referred in Sect. 2 , different techniques have been proposed to extract relations from text. Despite differences regarding their implementation effort and resulting coverage, the choice of technique is also dependent on the text to process. For instance, in dictionaries, where text is more structured and vocabulary is more controlled, several authors (Chodorow et al. 1985 ; Gonc  X alo Oliveira et al. 2009 ) have used symbolic pattern-based techniques with some success. But when it comes to more complex and less structured text, the identification of discriminating patterns and the creation of the extraction rules might involve an undesired amount of manual effort. In this case, weakly-supervised bootstrapping approaches, as in Pantel and Pennacchiotti ( 2006 ), are more common, at least when the relation types to extract are known a priori. Learning not only relation arguments, but also relation types, as in Fader et al. ( 2011 ), is out of the scope of ECO. Even though ECO could be adapted for that task, our main goal is to integrate a fixed set of the most common lexical-semantic relations, as in a wordnet.

We should nevertheless refer that, as long as the extracted relation instances are represented as tb-triples, the extraction techniques used do not affect the following steps. 3.2 Clustering The main problem about resources structured on lexical items, identified only by their orthographical form, is that they are not practical for several computational applications. This happens because words have different senses that go from tightly related, as in polysemy (e.g.  X  X ank X , institution and building) or metonymy (e.g.  X  X ank X , the building or its employers), to completely different, as in homonymy (e.g.  X  X ank X , institution or slope). Moreover, there are words with completely different forms denoting the same concept (e.g.  X  X ar X  and  X  X utomobile X ).

ECO handles word senses, but only after extraction. In the clustering step, word senses are discovered by exploiting the extracted information. Given that different resources cover different word senses, and word senses in different resources do not always match (Dolan 1994 ; Peters et al. 1998 ), this option provides more flexibility. The main idea of this step is to group similar words, as Lin ( 1998 ) proposes. However, Lin ( 1998 ) computes the similarity according to the context where words occur, while ECO does not store the context of the extracted tb-triples. Once again, this is beneficial to its flexibility, because it avoids having to define what is the context of a word in each kind of resource (e.g. thesaurus, dictionary, corpus).
On the other hand, it is desirable that synonymy relations are extracted. So, instead of clustering words based on their context, words are clustered regarding the similarity of their synonyms. This way, more than clustering similar words, the clusters should only contain words that are related, directly or indirectly, by synonymy, which means that they can be seen as synsets. On other words, this step results in the establishment of both concepts and word senses, and can thus be seen as a kind of word sense induction.

A suitable clustering algorithm would discover clusters in a network, established by the synonymy triples (hereafter, synpairs). This network has lexical items as nodes, while a connection between two nodes indicates that a synpair between them was extracted from some resource. Synonymy networks extracted from dictionaries actually tend to have a clustered structure (Gfeller et al. 2005 ), suitable for the application of the aforementioned kind of algorithms.

Also, as words might have different senses, they should belong to different synsets accordingly. Therefore, the clustering algorithm should account for overlapping clusters. Figure 2 is an ideal partitioning of a synonymy sub-network, centered on the word dog , into clusters. There, dog belongs to three clusters, each one with a different meaning: (A) a domestic pet; (B) a sausage; (C) a morally reprehensible person.

The main drawback of this approach is that it cannot differentiate between synsets with different meanings but the same synonymous words. But this problem can be minimised by using larger synonymy networks, extracted from different resources, and by using a set of pre-existing synsets as a starting point. The starting synsets can be, for instance, obtained from an existing thesaurus or wordnet, and will also improve the coverage of the discovered synsets. For this purpose, extracted synpairs can be first integrated in the existing synsets, which would be augmented, while the remaining synpairs can still be used to discover completely new synsets. 3.3 Ontologising While concepts, represented by synsets, are discovered in the previous step, all non-synonymy tb-triples still connect plain lexical items. The main goal of this step is to move from knowledge structured in terms towards an ontological structure, organised in concepts. This task, originally baptised as ontologising (Pantel 2005 ), consists of associating terms to a representation of their meaning which, in our case, are the discovered synsets. Table 1 illustrates how the tb-triple { man hypernym-of dog } should be ontologised in the available synsets. There, a  X  marks the appropriate choice of synsets.

Ontologising is similar to knowledge-based WSD, but the only available context consists of the arguments of the tb-triple. Given this limitation, an alternative is to exploit also the resource where we are ontologising, which is where the available meanings are represented. For instance, Pennacchiotti and Pantel ( 2006 ) propose two algorithms for ontologising tb-triples in WordNet.Pr. For such, they exploit the resource structure, including synsets and, especially, existing relations between synsets (hereafter, sb-triples).

However, although synsets are available in this step, there are no sb-triples yet, unless ECO is used to enrich an existing wordnet. On the other hand, ECO aims to create a whole resource from scratch, which means that it is expected that, in this step, there are many tb-triples to ontologise. So, to minimise the absence of sb-triples, the set of extracted tb-triples is exploited. Briefly, the selection of the most suitable pair of synsets can be achieved by using similarities between candidate synsets as an indicator. 4 ECO in the creation of a lexical ontology for Portuguese Regarding that the existing Portuguese LKBs have several limitations, we used ECO to create Onto.PT (Gonc  X alo Oliveira et al. 2012 ), a new lexical ontology for Portuguese, where some of the issues were minimised. After a brief discussion on the current limitations of Portuguese LKBs, this section presents how ECO was implemented to create Onto.PT. We refer what resources were exploited and what algorithms were implemented, and we provide an overview on the current results and their evaluation. 4.1 Limitations of Portuguese LKBs There have been attempts to create a wordnet [e.g. MWN.PT 1 , WordNet.PT (Marrafa 2002 ), WordNet.Br (Dias-da-Silva 2006 ) and OpenWN-PT (de Paiva et al. 2012 )] or a related resource for Portuguese, including synset-based thesauri (TeP (Dias-Da-Silva and de Moraes 2003 ; Maziero et al. 2008 ) and OpenThesaurus. PT 2 ) and a lexical-semantic network [PAPEL (Gonc  X alo Oliveira et al. 2009 , 2010 )]. Even though they all intend to be broad-coverage LKBs, all of them have limitations, especially regarding their structure, contents, creation and availability. Following, we refer some of these issues together with the resources where they are present: With the development of Onto.PT, we wanted to tackle most of these limitations. So, Onto.PT would: (1) be structured as a wordnet; (2) cover a wide range of semantic relations; (3) be created automatically by exploiting available textual resources and LKBs in Portuguese; (4) be public domain. 4.2 Exploited resources Despite the earlier problems concerning IE from dictionaries, these resources are still used in tasks such as the acquisition of ontologies [e.g. Nichols et al. ( 2005 )] and the extraction of semantic relations (Gonc  X alo Oliveira et al. 2009 ). Moreover, Wiktionary, a collaborative dictionary, has been exploited in several IE tasks [e.g. Zesch et al. ( 2008 ); Navarro et al. ( 2009 ); Henrich et al. ( 2011 )].
The main reasons for using dictionaries is their organisation in words and meanings, as well as their systematic definitions and vocabulary. They are one of the main sources of knowledge used in the creation of Onto.PT, which integrates: (1) the lexical-semantic network PAPEL, automatically extracted from a proprietary dictionary; (2) the electronic dictionaries Diciona  X  rio Aberto [DA, (Simo  X  es et al. 2012 )] and Wiktionary.PT. 3 Two public synset-based thesauri were also exploited, namely: TeP 2.0 (Maziero et al. 2008 ), hadcrafted by experts, and OpenThesaurus. PT, created collaboratively. 4.3 Implementation of ECO We implemented ECO according to the available resources for Portuguese. Figure 3 illustrates the result of each step of our implementation. It starts with a dictionary definition, from which two tb-triples are extracted from. Then, one TeP synset is augmented with the synonymy tb-triple. In the last step, the arguments of the non-synonymy tb-triple are attached to synsets ( synset 1 and synset 2 ). The rest of this section describes the implementation of each step in more detail. 4.3.1 Semantic relations from dictionaries The first step dealt mainly with the acquisition of tb-triples from dictionaries. For such, we took advantage of available handcrafted grammars, 4 originally created in the scope of PAPEL (Gonc  X alo Oliveira et al. 2009 ). The grammars were compiled after an exhaustive analysis of the patterns in the definitions of a commercial dictionary. They contain 371 non-terminal symbols and 1,714 productions that identify several relations in definitions, including synonymy, hypernymy, purpose-of, causation, property-of, and various kinds of meronymy. After comparing the structure of the definitions in DA and Wiktionary.PT, we concluded that most regularities were preserved across dictionaries. We thus reused the grammars for extracting relations from them too.

The new relations were merged with those of PAPEL, which resulted in a larger lexical-semantic network for Portuguese (Gonc  X alo Oliveira et al. 2011 ), with about 326,000 distinct tb-triples, connecting about 155,000 lexical items. Hypernymy was the relation with more instances (  X  98,000), followed by synonymy (  X  68,000 between nouns,  X  32,000 between adjectives,  X  28,000 between verbs) and property-of (  X  28,000 between adjectives and verbs,  X  11,000 between adjectives and nouns). The most representative relations were manually evaluated by two human judges (J1 and J2). For each evaluated type of relation, Table 2 shows the size of the evaluated samples, the accuracy according to relation and judge, the judge  X  -agreement and an example of a tb-triple. 4.3.2 Clustering for synsets The goal of the clustering step is to discover synsets in the network established by the synpairs extracted previously, N syn . In a first approach to this step, we used the complete N syn , which contained the synpairs extracted from all the dictionaries. After comparing the results of the first approach with TeP, the largest public synset-based Portuguese thesaurus, we noticed that, to some extent, the obtained synsets were complementary. Therefore, in a second approach, we used TeP as a starting point and enriched it with the synpairs in N syn . This way, the quality of the synsets is also higher. Currently, the second approach is followed in the creation of the Onto. PT. Following, we present both approaches to this step:
First approach : The main idea of this approach is that, in the full synonymy network, a node (lexical item) and its neighbourhood define a potential cluster [full details of this part of the work are described in Gonc  X alo Oliveira and Gomes ( 2011 )]. The cosine between the adjacencies (synonyms) of each node and all the others is computed, and all nodes with cosine higher than a predefined threshold  X  establish a new cluster. Clusters can be overlapping and, if a cluster contains only one item, or if it is completely included in a larger cluster, it is discarded. As clusters group lexical items linked by synonymy, they can be seen as synsets. In Table 3 , we put some properties of TeP side-by-side to a thesaurus obtained with  X  = 0.075.

The thesaurus obtained automatically is substantially larger than TeP, both in terms of lexical items and synsets. When it comes to ambiguity, it is more regular. Its average number of senses per lexical item is comparable to TeP X  X  and its synsets have, on average, one more item than TeP, except for verbs. In fact, the verbs of TeP are more ambiguous on average, and its verb synsets are larger. Moreover, the automatic thesaurus covered about 64 % of the lexical items of TeP, and there was about 53 % overlap between the synsets of TeP and those of the automatic thesaurus. After evaluating a sample of 440 noun synsets, manually, we concluded that about 75 % of the synsets discovered automatically were correct.

Second approach : Given that, to some extent, the previous thesauri are still complementary, we decided that it would be more fruitful to integrate both. So, since the synsets of TeP were created by humans, we can use them as a starting point and then enrich TeP it with the synpairs extracted from the dictionaries. As a handcrafted resource is virtually 100 % reliable, by doing this, we can expect to have a higher synset correction rate, as compared to that of the first approach. OpenThesaurus.PT was not used for this task because it is four times smaller than TeP, and it is not created by experts. Instead, we transformed the former into a set of synpairs, and added them to N syn .

This approach is divided into two tasks, namely synpair attachment and clustering: (a) Synpair attachment : this task deals with the assignment of each synpair of N syn to (b) Clustering : the remaining synpairs were either those with similarities always
Table 4 shows the properties of the thesaurus obtained with this second approach, which we used in Onto.PT. As compared to using the full synonymy network, this thesaurus covers more words and has more and larger synsets. It is also more ambiguous which is shown both by the higher average number of senses per lexical item and synset size. 4.3.3 Ontologising tb-triples into discovered synsets For the last step of ECO, we developed several algorithms for attaching the arguments of the tb-triples to suitable synsets. All the algorithms exploit the whole lexical network, N , extracted during the first step. After a performance comparison (Gonc  X alo Oliveira and Gomes 2012 ), we decided to use different algorithms for ontologising different relations, namely related proportion (RP) combined with average cosine (AC), for all tb-triples but the hypernymy ones, which were ontologised using only AC. For ontologising a tb-triple { aRb }, the algorithms are briefly explained as follows:
RP To attach the argument a , b is fixed. Then, for each synset A i 3 a ; n i is the number of items t k  X  A i such that { t k Rb }  X  N . The related proportion rp is computed as follows:
Argument a is attached to the candidate(s) synset(s) maximising rp , unless rp \  X  , a predefined threshold. Argument b is attached using the same procedure, but fixing a . The combined algorithm (RP+AC) consists of using RP with a high  X  , to guarantee higher precision and, if it cannot select a suitable synset for a or b , AC is used.
AC As related concepts tend to be described by words related to the same concepts, AC exploits all the tb-triples of all relations in N to select the most similar pair of candidate synsets. To ontologise a and b , a pair of synsets A i 3 a and B j 3 b is thus selected according to the adjacencies, in N , of the items they include. Similarity between A i and B j , represented by the adjacency vectors of their items, [ A average similarity of each item t k with each item u l :
While this expression has been used to find similar nouns in a corpus (Caraballo 1999 ), we adapted it to measure the similarity of two synsets, represented as the adjacency vectors of their lexical items.

For both algorithms, when there are ties for the bestselection,all the tied elementsare chosen. The performance comparison showed that, using AC, the accuracy of ontologising 210 hypernymy tb-triples was 60.1 %. The combination of RP+AC was used to ontologise 175 part-of and 67 purpose-of tb-triples, with accuracies respectively of64.1and63.4%.Thesenumbersshowthat,atleastinthecreationofOnto.PT,thisstep is the most challenging and less reliable of ECO. Its accuracy is highly dependent on the ambiguity of the synsets, the relation type and, especially, the quantity of extracted information (size of N ). We later used the same algorithms to ontologise a random sample of 800 antonymy tb-triples between adjectives, from TeP. Using RP and the same N as the previous comparison, which contained only about 700 antonymy tb-triples, precision was 99.4%,in a trade-off of 40.8% recall. Using RP+AC, whichwe remindthat,whenRPcannotselectanattachment,exploitsalsonon-antonymytb-triples with AC, recall increased to 69.7 %, but precision decreased to 69.3 %.
In the end of this step, we still added all the 8,886 antonymy sb-triples of TeP directly to the final resource, Onto.PT. This enlarges the set of antonymy sb-triples in Onto.PT at a low cost and, as TeP was created by humans, we have high confidence on this information. 4.4 Resulting resource Onto.PT is now freely available as a RDF/OWL model, based on a similar representation of WordNet.Pr (van Assem et al. 2006 ). It can also be queried through a web interface. 5 We see it as a viable alternative or a complement to Portuguese LKBs, and as an important contribution for advancing the state-of-the-art of Portuguese NLP. 4.4.1 Quantitative data Onto.PT v.0.35 contains about 109,000 synsets, of which about 105,000 are involved in at least one sb-triple. Besides the discovered synsets, Onto.PT contains about 79,000 synsets with only one lexical item, resulting from arguments of tb-triples not covered by the synset-base.

As for relations, Onto.PT v.0.35 contains about 173,000 sb-triples of the extracted relations, which are the same as in PAPEL. Almost half of the sb-triples are hypernymy (  X  80,300). Property-of is the second relation with most sb-triples (  X  25,100 between adjectives and verbs, and  X  9,700 between adjectives and nouns). In descending order of quantity, the remaining covered relations are: purpose-of (  X  15,300), causation (  X  9,800), part-of (  X  8,500), member-of (  X  7,000), antonymy (  X  6,000), manner-of (  X  3,400), producer-of (  X  2,100), quality-of (  X  2,000), con-tained-in (  X  600), state-of (  X  500), place-of (  X  300), manner-without (  X  200).
Onto.PT is larger than WordNet.Pr 3.0 and has more relation types, which highlights an advantage of the automatic construction approach. Regarding that all the relations in Onto.PT can be inverted, it contains 2  X  173,000 = 346,000 sb-triples against the 285,000 of WordNet.Pr 3.0. The number of synsets is just slightly higher, more precisely 109,000 against 105,000.

Of course this number is insufficient to quantify the coverage of Onto.PT. So, to have a better idea on this, we evaluated its coverage of concepts that should be included in wordnets. The Global WordNet Association 6 provides a list with 164 base concepts, referred to as the  X  X ost important X  in the wordnets of English, Spanish, Dutch and Italian. 7 They are divided into 98 abstract and 66 concrete concepts, and are represented as WordNet.Pr 1.5 synsets. Based on rough matches we defined for the Onto.PT synsets, we concluded that this resource covers most of the concepts in the list, more precisely 92 abstract and 61 concrete synsets (93 %). 4.4.2 Evaluation Onto.PT v.0.35 was the target of an extensive two step manual evaluation, where the synsets were, first, considered alone and, then, in the sb-triples. Briefly, this evaluation encompassed the following steps: 1. Selection of two random samples of 300 sb-triples: one with only hypernymy 2. Reduction of the arguments of the previous sb-triples, in order to simplify 3. Establishment of a set containing all the reduced synsets with more than one 4. Classification of each synset as correct or incorrect, by two judges X  X  correct 5. Removal, from the initial samples of 300 sb-triples, of those connecting one 6. Classification of each of the remaining sb-triples as correct or incorrect. Once In the 600 sb-triples, there were 774 distinct synsets with more than one item. From those, 572 (73.9 %) and 58 (7.5 %) were respectively classified as correct and incorrect by both judges. For the remaining 144 (18.6 %), the judges did not agree.
Table 5 presents the results of the manual evaluation of sb-triples. For each sample, it shows the proportion of correct sb-triples, both considering that those with incorrect confidence interval of 95 %, the margin of error for the results of set A is also presented (ME A ), as well as the judge agreement, given by the number of matches ( IAA ), and by the  X  coefficient.

The values in the column  X  X orrect X  and sub-column  X  X  X  can be seen as an approximation of the correct sb-triples of Onto.PT. This value is lower for hypernymy than for other relations. A possible explanation for this is that, for most hypernymy relations, the more generic argument is actually very generic, which means it is frequently vague and with several different senses. The following are some of the most frequent hypernyms: pessoa (person), planta (plant),  X rvore (tree), indiv X duo (individual), instrumento (instrument), subst X ncia (substance), lugar (place), pe X a (piece).

As for the non-hypernymy relations, their average accuracy is between 78 and 82 %. We cannot take specific conclusions for each relation type, because not enough sb-triples of each were classified. Still, considering only the relations with at least 10 classified sb-triples, we can say that the best relations are member-of, causation (between verbs and nouns), has-quality (between adjectives and nouns), and property-of, all with more than 90 % sb-triples classified by both judges (J1 and J2) as correct. Moreover, in a number highly influenced by TeP X  X  information, all antonymy sb-triples classified between verbs, adjectives and adverbs were correct. On the other hand, part-of was the worst relation, with only 50 and 70 % sb-triples classified as correct by J1 and J2 respectively. The same explanation given for the low accuracy of the hypernymy relations can be given for part-of, as it also tends to have one very generic argument.

These results led to recent corrections in Onto.PT, so its reliability is now expected to be higher. Furthermore, in order to assess the utility of Onto.PT, this resource was used in an information retrieval task, where it provided information for synonymy expansion (Rodrigues et al. 2012 ). Using Onto.PT, the quality and quantity of the baseline approach were improved. 5 Concluding remarks We have presented ECO, a flexible approach to create a wordnet automatically, by exploiting textual resources. ECO is an alternative to the time-consuming manual creation of wordnets, and intends to minimise information sparsity issues by exploiting several heterogeneous resources. Of course, this is performed in a trade-off of lower reliability than that of handcrafted resources.

ECO is an abstract model that intends to extract knowledge and integrate it harmoniously, by combining different IE techniques. Briefly, it starts with the acquisition of semantic relations from text, which is usually done by exploiting indicative discriminating patterns. Then, synsets are discovered by the identification of clusters in the synonymy relations. Finally, the arguments of the relations acquired in the first step are attached to suitable synsets, selected after exploiting all the extracted information. The implementation of ECO is quite flexible and should have in mind the available resources and data.
 ECO was followed, with relative success, in the creation of Onto.PT, a Portuguese wordnet-like resource. Given the limitations of Portuguese LKBs, we believe this resource will help to advance the state of the art of Portuguese NLP. Onto.PT has already shown to be useful for synonymy expansion, and we are currently using it for other NLP tasks, including WSD, and multiple-choice question answering. Nevertheless, the evaluation results show that there is still a long way to go until we have a highly-reliable resource.

One limitation of the ECO generated wordnets towards most wordnets is that there are no guarantees that there is a well-formed hypernymy tree. In fact, most paths from the more specific synsets to the top-level are not more than three edges long (e.g. dalmatian  X  dog  X  mammal  X  animal ), some synsets are attached directly to top-level synsets (e.g. gazelle  X  animal ), and there might be a minor number of cycles as well. This problem should be quantified in the future, and solutions to re-organise the relations should be studied. It might involve, for instance, a set of rules for changing the attaching points automatically. Another limitation is the absence of synset glosses. We intend to tackle this issue by following a similar approach to Henrich et al. ( 2011 ), who automatically assign dictionary definitions to synsets.

We would like to conclude by reminding that any resource created by ECO cannot be seen as static. This means that, in the future, we can create new instances of Onto.PT, not only by improving issues identified along the performed evaluations, but also by exploiting other resources. For instance, we are planning to study the impact of using other related words in the clustering context; and we see the Portuguese Wikipedia as a possible target for future extractions, this time using preferably weakly-supervised bootstrapping techniques. Furthermore, other resources can be explored for the application of ECO in the creation of wordnets in other languages, or even on closed domains. Therefore, we see it as a general contribution to the automatic creation of knowledge bases.
 References
 Hugo Gon  X alo Oliveira  X  Paulo Gomes Abstract A wordnet is an important tool for developing natural language pro-cessing applications for a language. However, most wordnets are handcrafted by experts, which limits their growth. In this article, we propose an automatic approach to create wordnets by exploiting textual resources, dubbed ECO. After extracting semantic relation instances, identified by discriminating textual patterns, ECO discovers synonymy clusters, used as synsets, and attaches the remaining relations to suitable synsets. Besides introducing each step of ECO, we report on how it was implemented to create Onto.PT, a public lexical ontology for Portuguese. Onto.PT is the result of the automatic exploitation of Portuguese dictionaries and thesauri, and it aims to minimise the main limitations of existing Portuguese lexical knowledge bases.
 Keywords Information extraction  X  Lexical ontology  X  Wordnet  X  Clustering  X  Semantic relations 1 Introduction A substantial amount of data produced every day is available in natural language text. Understanding its meaning involves more than recognising words and their interactions, and typically requires access to external sources of knowledge. This fact lead to the creation of broad-coverage knowledge bases, which can be exploited in natural language processing (NLP) tasks that perform a semantic analysis of text. The previous resources include not only large repositories covering world knowledge, but also lexical knowledge bases (LKBs), which are more linguistically grounded and structured in words and meanings. Regarding their structure, the latter are often referred to as lexical ontologies, because they share properties of lexicons as well as properties of ontologies (Hirst 2004 ; Pre  X  vot et al. 2010 ).
Despite different interpretations of the concept of lexical ontology, the paradig-matic resource of this kind is the Princeton WordNet (Fellbaum 1998 ) (hereafter, WordNet.Pr). It is a resource structured in synsets X  X roups of synonymous word senses that can be seen as possible lexicalisations of a natural language concept X  X nd semantic relations connecting synsets X  X ncluding hypernymy (a concept is a kind of another), part-of (a concept is part of another), and others.

Besides its structure, suitable for being integrated and exploited by NLP applications, the public availability of WordNet.Pr played an important role in its success. WordNet.Pr was widely accepted by the NLP community and, today, there is no doubt that the existence of such a resource has a positive impact on the computational processing of a language. This fact is evidenced for English, where WordNet.Pr opened the range of capabilities of NLP applications. It was used in the achievement of tasks, including, but not limited to, determining similarities (Agirre et al. 2009 ), word sense disambiguation (Gomes et al. 2003 ), query expansion (Navigli and Velardi 2003 ), intelligent search (Hemayati et al. 2007 ), question-answering (Pasca and Harabagiu 2001 ), and text summarisation (Bellare et al. 2004 ).
The wordnet model was also very successful and adopted in the creation of broad-coverage lexical-semantic resources for other languages [see e.g. EuroWord-Net (Vossen 1998 ) or BalkaNet (Stamou et al. 2002 )]. But, as it happens for WordNet.Pr, a huge limitation of most wordnets is that they are handcrafted. Their construction thus involves much human effort, which is a bottleneck for the growth and evolution of the resource.

This problem was tackled by the NLP community, which studied how to automatise the creation of LKBs and minimise the information sparsity issues. Textual data has been exploited in the automatic creation of LKBs from scratch (Richardson et al. 1998 ), or in the enrichment of existing LKBs (Snow et al. 2005 ). Wordnets were also linked with other knowledge bases [e.g. Gurevych et al. ( 2012 ) or Hoffart et al. ( 2011 )]. A different approach was based on the translation of a target wordnet (usually WordNet.Pr) to other languages (de Melo and Weikum 2008 ). But a problem arises because different languages represent different socio-cultural realities, they do not cover exactly the same part of the lexicon and, even where they seem to be common, several concepts are lexicalised differently (Hirst 2004 ). Therefore, we believe that a wordnet for a language, whether created manually, semi-automatically or automatically, should be developed from scratch for that language.

Having this in mind, we propose a flexible approach for the automatic acquisition, organisation and integration of lexical-semantic knowledge in a unique wordnet-like resource. The ECO approach exploits both textual sources and existing lexical-semantic resources to generate a resource structured in synsets and semantic relations. This is performed in three steps X  X elation extraction, word clustering and ontologisation X  X hat combine several information extraction techniques.

ECO was applied in the creation of Onto.PT, a public domain wordnet-like lexical ontology for Portuguese that integrates knowledge extracted from electronic dictionaries and thesauri, and intends to minimise the limitations of existing Portuguese LKBs. Its last version, at the time of writing, contains about 109,000 synsets connected by about 173,000 relations of different types. Still, given its automatic creation, Onto.PT can be further improved, and integrate knowledge from alternative sources, for a low cost.
 After introducing some related work, this article gives an abstract overview of the ECO approach and each of its steps, while discussing some of the options that can be taken for its application. Then, we present a concrete implementation of ECO, by describing how it was applied to the creation of Onto.PT, including some details on the structure of this resource and on its evaluation. We end with some concluding remarks. 2 Related work Since the 1970s, researchers have been exploiting textual resources and developing techniques towards the automatic acquisition of lexical-semantic knowledge, which could be used in the automatic creation of a broad coverage LKB. Electronic dictionaries were the primary resources exploited for this task (Calzolari et al. 1973 ; Amsler 1981 ), because they already provide an extensive coverage on words and meanings, they are created by experts on describing word senses, and they typically use systematic definitions, suitable for being exploited by automatic procedures. The first approaches to the automatic extraction of taxonomies from dictionaries were soon developed (Chodorow et al. 1985 ), but human input was still critical to perform word sense disambiguation (WSD). Continued work on information extraction (IE) from dictionaries led to to the creation of MindNet (Richardson et al. 1998 ), an independent LKB created automatically.

A few problems about semantic networks extracted from dictionaries have however been pointed out (Ide and Ve  X  ronis 1995 ). For instance, information in dictionaries differs considerably in amount and kind. So, the creation of a broad-coverage LKB requires the combination of multiple dictionaries or other sources of knowledge. Another problem is that electronic dictionaries are not always available for researchers.
 Despite several automatic attempts to the creation of a broad-coverage LKB, for English, WordNet.Pr, a manual effort, ended up being the leading resource of this kind (Sampson 2000 ). But, as the manual creation of knowledge bases is a time-consuming and tedious task, research on lexical-semantic IE from text proceeded, whether it was for the creation of new LKBs, or for the enrichment of wordnets. In fact, as a handcrafted resource covering mainly linguistic knowledge, researchers using WordNet.Pr as their only knowledge base soon had to deal with information sparsity issues.

Regarding that there is a huge amount of text available on most domains, researchers turned on to IE from less structured sources. While targeting textual corpora, unsupervised procedures have been developed for clustering words according to their distributional similarity (Lin 1998 ; Turney 2001 ), which could be used to identify synonyms and to create thesauri. On the automatic extraction of semantic relations from corpora, most research is based on Hearst ( 1992 ) X  X  method for discovering discriminating lexical-syntactic patterns. Starting with a set of seed relation instances of a certain type (e.g. hyponymy), this method identifies sequences of text that occur systematically between the related arguments. The discovered patterns can be used for the automatic extraction of new relations. Besides the combination of the previous idea with distributional measures to improve extraction (Caraballo 1999 ), fully supervised approaches were also applied to the extraction of semantic relations from text (Snow et al. 2005 ). Moreover, pattern discovery was integrated in weakly-supervised approaches that score the discovered patterns automatically, according to their reliability and frequency, and use only the higher ranked patterns (Pantel and Pennacchiotti 2006 ).

Weakly supervised approaches were also followed for extracting semantic relations from the Web (Agichtein and Gravano 2000 ). On this scope, more complex systems were developed in recent years for learning great amounts of various kinds of facts from the Web and for creating large knowledge bases [e.g. Banko et al. ( 2007 ); Carlson et al. ( 2010 ); Fader et al. ( 2011 )].
An alternative to the enrichment of a wordnet with information from text is to integrate, or link, several resources in a unique knowledge base. For instance, WordNet.Pr was linked to other lexical-semantic resources, such as FrameNet and VerbNet (Shi and Mihalcea 2005 ), to the collaborative encyclopedia Wikipedia (Suchanek et al. 2007 ; Navigli and Ponzetto 2012 ), to all the previous and Wiktionary, in English and in German (Gurevych et al. 2012 ), to the upper ontology SUMO (Pease and Fellbaum 2010 ) and to the descriptive ontology DOLCE (Gangemi et al. 2010 ).

Most of the previous approaches either extract knowledge to enrich an existing resource, or merge existing resources. The other few are not exactly concerned with the creation of a wordnet from scratch. For instance, some are not restricted to lexical-semantic knowledge, and most of them do not structure the extracted knowledge as a wordnet. Alternatively, we propose an approach that, although may take advantage of existing resources, can be used to create a wordnet automatically, completely from scratch, and in an unsupervised way. 3 The ECO approach ECO is the abstract model we propose for creating a lexical ontology, structured as a wordnet, automatically from heterogeneous textual sources of a target language. It combines several IE techniques in order to acquire, organise and integrate lexical-semantic knowledge. For this purpose, ECO encompasses three main steps, as shown in the diagram of Fig. 1 , performed in the following order: 1. Extraction instances of semantic relations, held between lexical items, are 2. Clustering clusters of synonymous lexical items are discovered from the 3. Ontologising the lexical items in the arguments of the non-synonymy relation
Each step is independent of each other and can alternatively be used in the achievement of its own task. This is an interesting way of coping with information sparsity, since it allows for the extraction of knowledge from different heteroge-neous sources (e.g. dictionaries, thesauri, corpora), and provides a way to harmoniously integrate all the acquired information in a common knowledge base. In the rest of this section, we describe each step in more detail and relate them to other works that addressed them individually. 3.1 Extraction In order to integrate lexical-semantic knowledge, ECO requires that this information is represented as relational triples, denoting instances of semantic relations. These structures are a common representation in most works on IE. They consist of two arguments connected by a relation, as in:
The extraction step deals with the automatic acquisition of relational triples connecting lexical items (terms), so we will hereafter refer to them as term-based triples (tb-triples). As mentioned earlier, this task is typically based on a set of discriminating patterns that occur in text, and indicate certain semantic relations. Depending on the relation to extract, some discriminating patterns might be intui-tive, but less intuitive patterns can be discovered automatically, in a similar fashion to Hearst ( 1992 ).

As referred in Sect. 2 , different techniques have been proposed to extract relations from text. Despite differences regarding their implementation effort and resulting coverage, the choice of technique is also dependent on the text to process. For instance, in dictionaries, where text is more structured and vocabulary is more controlled, several authors (Chodorow et al. 1985 ; Gonc  X alo Oliveira et al. 2009 ) have used symbolic pattern-based techniques with some success. But when it comes to more complex and less structured text, the identification of discriminating patterns and the creation of the extraction rules might involve an undesired amount of manual effort. In this case, weakly-supervised bootstrapping approaches, as in Pantel and Pennacchiotti ( 2006 ), are more common, at least when the relation types to extract are known a priori. Learning not only relation arguments, but also relation types, as in Fader et al. ( 2011 ), is out of the scope of ECO. Even though ECO could be adapted for that task, our main goal is to integrate a fixed set of the most common lexical-semantic relations, as in a wordnet.

We should nevertheless refer that, as long as the extracted relation instances are represented as tb-triples, the extraction techniques used do not affect the following steps. 3.2 Clustering The main problem about resources structured on lexical items, identified only by their orthographical form, is that they are not practical for several computational applications. This happens because words have different senses that go from tightly related, as in polysemy (e.g.  X  X ank X , institution and building) or metonymy (e.g.  X  X ank X , the building or its employers), to completely different, as in homonymy (e.g.  X  X ank X , institution or slope). Moreover, there are words with completely different forms denoting the same concept (e.g.  X  X ar X  and  X  X utomobile X ).

ECO handles word senses, but only after extraction. In the clustering step, word senses are discovered by exploiting the extracted information. Given that different resources cover different word senses, and word senses in different resources do not always match (Dolan 1994 ; Peters et al. 1998 ), this option provides more flexibility. The main idea of this step is to group similar words, as Lin ( 1998 ) proposes. However, Lin ( 1998 ) computes the similarity according to the context where words occur, while ECO does not store the context of the extracted tb-triples. Once again, this is beneficial to its flexibility, because it avoids having to define what is the context of a word in each kind of resource (e.g. thesaurus, dictionary, corpus).
On the other hand, it is desirable that synonymy relations are extracted. So, instead of clustering words based on their context, words are clustered regarding the similarity of their synonyms. This way, more than clustering similar words, the clusters should only contain words that are related, directly or indirectly, by synonymy, which means that they can be seen as synsets. On other words, this step results in the establishment of both concepts and word senses, and can thus be seen as a kind of word sense induction.

A suitable clustering algorithm would discover clusters in a network, established by the synonymy triples (hereafter, synpairs). This network has lexical items as nodes, while a connection between two nodes indicates that a synpair between them was extracted from some resource. Synonymy networks extracted from dictionaries actually tend to have a clustered structure (Gfeller et al. 2005 ), suitable for the application of the aforementioned kind of algorithms.

Also, as words might have different senses, they should belong to different synsets accordingly. Therefore, the clustering algorithm should account for overlapping clusters. Figure 2 is an ideal partitioning of a synonymy sub-network, centered on the word dog , into clusters. There, dog belongs to three clusters, each one with a different meaning: (A) a domestic pet; (B) a sausage; (C) a morally reprehensible person.

The main drawback of this approach is that it cannot differentiate between synsets with different meanings but the same synonymous words. But this problem can be minimised by using larger synonymy networks, extracted from different resources, and by using a set of pre-existing synsets as a starting point. The starting synsets can be, for instance, obtained from an existing thesaurus or wordnet, and will also improve the coverage of the discovered synsets. For this purpose, extracted synpairs can be first integrated in the existing synsets, which would be augmented, while the remaining synpairs can still be used to discover completely new synsets. 3.3 Ontologising While concepts, represented by synsets, are discovered in the previous step, all non-synonymy tb-triples still connect plain lexical items. The main goal of this step is to move from knowledge structured in terms towards an ontological structure, organised in concepts. This task, originally baptised as ontologising (Pantel 2005 ), consists of associating terms to a representation of their meaning which, in our case, are the discovered synsets. Table 1 illustrates how the tb-triple { man hypernym-of dog } should be ontologised in the available synsets. There, a  X  marks the appropriate choice of synsets.

Ontologising is similar to knowledge-based WSD, but the only available context consists of the arguments of the tb-triple. Given this limitation, an alternative is to exploit also the resource where we are ontologising, which is where the available meanings are represented. For instance, Pennacchiotti and Pantel ( 2006 ) propose two algorithms for ontologising tb-triples in WordNet.Pr. For such, they exploit the resource structure, including synsets and, especially, existing relations between synsets (hereafter, sb-triples).

However, although synsets are available in this step, there are no sb-triples yet, unless ECO is used to enrich an existing wordnet. On the other hand, ECO aims to create a whole resource from scratch, which means that it is expected that, in this step, there are many tb-triples to ontologise. So, to minimise the absence of sb-triples, the set of extracted tb-triples is exploited. Briefly, the selection of the most suitable pair of synsets can be achieved by using similarities between candidate synsets as an indicator. 4 ECO in the creation of a lexical ontology for Portuguese Regarding that the existing Portuguese LKBs have several limitations, we used ECO to create Onto.PT (Gonc  X alo Oliveira et al. 2012 ), a new lexical ontology for Portuguese, where some of the issues were minimised. After a brief discussion on the current limitations of Portuguese LKBs, this section presents how ECO was implemented to create Onto.PT. We refer what resources were exploited and what algorithms were implemented, and we provide an overview on the current results and their evaluation. 4.1 Limitations of Portuguese LKBs There have been attempts to create a wordnet [e.g. MWN.PT 1 , WordNet.PT (Marrafa 2002 ), WordNet.Br (Dias-da-Silva 2006 ) and OpenWN-PT (de Paiva et al. 2012 )] or a related resource for Portuguese, including synset-based thesauri (TeP (Dias-Da-Silva and de Moraes 2003 ; Maziero et al. 2008 ) and OpenThesaurus. PT 2 ) and a lexical-semantic network [PAPEL (Gonc  X alo Oliveira et al. 2009 , 2010 )]. Even though they all intend to be broad-coverage LKBs, all of them have limitations, especially regarding their structure, contents, creation and availability. Following, we refer some of these issues together with the resources where they are present: With the development of Onto.PT, we wanted to tackle most of these limitations. So, Onto.PT would: (1) be structured as a wordnet; (2) cover a wide range of semantic relations; (3) be created automatically by exploiting available textual resources and LKBs in Portuguese; (4) be public domain. 4.2 Exploited resources Despite the earlier problems concerning IE from dictionaries, these resources are still used in tasks such as the acquisition of ontologies [e.g. Nichols et al. ( 2005 )] and the extraction of semantic relations (Gonc  X alo Oliveira et al. 2009 ). Moreover, Wiktionary, a collaborative dictionary, has been exploited in several IE tasks [e.g. Zesch et al. ( 2008 ); Navarro et al. ( 2009 ); Henrich et al. ( 2011 )].
The main reasons for using dictionaries is their organisation in words and meanings, as well as their systematic definitions and vocabulary. They are one of the main sources of knowledge used in the creation of Onto.PT, which integrates: (1) the lexical-semantic network PAPEL, automatically extracted from a proprietary dictionary; (2) the electronic dictionaries Diciona  X  rio Aberto [DA, (Simo  X  es et al. 2012 )] and Wiktionary.PT. 3 Two public synset-based thesauri were also exploited, namely: TeP 2.0 (Maziero et al. 2008 ), hadcrafted by experts, and OpenThesaurus. PT, created collaboratively. 4.3 Implementation of ECO We implemented ECO according to the available resources for Portuguese. Figure 3 illustrates the result of each step of our implementation. It starts with a dictionary definition, from which two tb-triples are extracted from. Then, one TeP synset is augmented with the synonymy tb-triple. In the last step, the arguments of the non-synonymy tb-triple are attached to synsets ( synset 1 and synset 2 ). The rest of this section describes the implementation of each step in more detail. 4.3.1 Semantic relations from dictionaries The first step dealt mainly with the acquisition of tb-triples from dictionaries. For such, we took advantage of available handcrafted grammars, 4 originally created in the scope of PAPEL (Gonc  X alo Oliveira et al. 2009 ). The grammars were compiled after an exhaustive analysis of the patterns in the definitions of a commercial dictionary. They contain 371 non-terminal symbols and 1,714 productions that identify several relations in definitions, including synonymy, hypernymy, purpose-of, causation, property-of, and various kinds of meronymy. After comparing the structure of the definitions in DA and Wiktionary.PT, we concluded that most regularities were preserved across dictionaries. We thus reused the grammars for extracting relations from them too.

The new relations were merged with those of PAPEL, which resulted in a larger lexical-semantic network for Portuguese (Gonc  X alo Oliveira et al. 2011 ), with about 326,000 distinct tb-triples, connecting about 155,000 lexical items. Hypernymy was the relation with more instances (  X  98,000), followed by synonymy (  X  68,000 between nouns,  X  32,000 between adjectives,  X  28,000 between verbs) and property-of (  X  28,000 between adjectives and verbs,  X  11,000 between adjectives and nouns). The most representative relations were manually evaluated by two human judges (J1 and J2). For each evaluated type of relation, Table 2 shows the size of the evaluated samples, the accuracy according to relation and judge, the judge  X  -agreement and an example of a tb-triple. 4.3.2 Clustering for synsets The goal of the clustering step is to discover synsets in the network established by the synpairs extracted previously, N syn . In a first approach to this step, we used the complete N syn , which contained the synpairs extracted from all the dictionaries. After comparing the results of the first approach with TeP, the largest public synset-based Portuguese thesaurus, we noticed that, to some extent, the obtained synsets were complementary. Therefore, in a second approach, we used TeP as a starting point and enriched it with the synpairs in N syn . This way, the quality of the synsets is also higher. Currently, the second approach is followed in the creation of the Onto. PT. Following, we present both approaches to this step:
First approach : The main idea of this approach is that, in the full synonymy network, a node (lexical item) and its neighbourhood define a potential cluster [full details of this part of the work are described in Gonc  X alo Oliveira and Gomes ( 2011 )]. The cosine between the adjacencies (synonyms) of each node and all the others is computed, and all nodes with cosine higher than a predefined threshold  X  establish a new cluster. Clusters can be overlapping and, if a cluster contains only one item, or if it is completely included in a larger cluster, it is discarded. As clusters group lexical items linked by synonymy, they can be seen as synsets. In Table 3 , we put some properties of TeP side-by-side to a thesaurus obtained with  X  = 0.075.

The thesaurus obtained automatically is substantially larger than TeP, both in terms of lexical items and synsets. When it comes to ambiguity, it is more regular. Its average number of senses per lexical item is comparable to TeP X  X  and its synsets have, on average, one more item than TeP, except for verbs. In fact, the verbs of TeP are more ambiguous on average, and its verb synsets are larger. Moreover, the automatic thesaurus covered about 64 % of the lexical items of TeP, and there was about 53 % overlap between the synsets of TeP and those of the automatic thesaurus. After evaluating a sample of 440 noun synsets, manually, we concluded that about 75 % of the synsets discovered automatically were correct.

Second approach : Given that, to some extent, the previous thesauri are still complementary, we decided that it would be more fruitful to integrate both. So, since the synsets of TeP were created by humans, we can use them as a starting point and then enrich TeP it with the synpairs extracted from the dictionaries. As a handcrafted resource is virtually 100 % reliable, by doing this, we can expect to have a higher synset correction rate, as compared to that of the first approach. OpenThesaurus.PT was not used for this task because it is four times smaller than TeP, and it is not created by experts. Instead, we transformed the former into a set of synpairs, and added them to N syn .

This approach is divided into two tasks, namely synpair attachment and clustering: (a) Synpair attachment : this task deals with the assignment of each synpair of N syn to (b) Clustering : the remaining synpairs were either those with similarities always
Table 4 shows the properties of the thesaurus obtained with this second approach, which we used in Onto.PT. As compared to using the full synonymy network, this thesaurus covers more words and has more and larger synsets. It is also more ambiguous which is shown both by the higher average number of senses per lexical item and synset size. 4.3.3 Ontologising tb-triples into discovered synsets For the last step of ECO, we developed several algorithms for attaching the arguments of the tb-triples to suitable synsets. All the algorithms exploit the whole lexical network, N , extracted during the first step. After a performance comparison (Gonc  X alo Oliveira and Gomes 2012 ), we decided to use different algorithms for ontologising different relations, namely related proportion (RP) combined with average cosine (AC), for all tb-triples but the hypernymy ones, which were ontologised using only AC. For ontologising a tb-triple { aRb }, the algorithms are briefly explained as follows:
RP To attach the argument a , b is fixed. Then, for each synset A i 3 a ; n i is the number of items t k  X  A i such that { t k Rb }  X  N . The related proportion rp is computed as follows:
Argument a is attached to the candidate(s) synset(s) maximising rp , unless rp \  X  , a predefined threshold. Argument b is attached using the same procedure, but fixing a . The combined algorithm (RP+AC) consists of using RP with a high  X  , to guarantee higher precision and, if it cannot select a suitable synset for a or b , AC is used.
AC As related concepts tend to be described by words related to the same concepts, AC exploits all the tb-triples of all relations in N to select the most similar pair of candidate synsets. To ontologise a and b , a pair of synsets A i 3 a and B j 3 b is thus selected according to the adjacencies, in N , of the items they include. Similarity between A i and B j , represented by the adjacency vectors of their items, [ A average similarity of each item t k with each item u l :
While this expression has been used to find similar nouns in a corpus (Caraballo 1999 ), we adapted it to measure the similarity of two synsets, represented as the adjacency vectors of their lexical items.

For both algorithms, when there are ties for the bestselection,all the tied elementsare chosen. The performance comparison showed that, using AC, the accuracy of ontologising 210 hypernymy tb-triples was 60.1 %. The combination of RP+AC was used to ontologise 175 part-of and 67 purpose-of tb-triples, with accuracies respectively of64.1and63.4%.Thesenumbersshowthat,atleastinthecreationofOnto.PT,thisstep is the most challenging and less reliable of ECO. Its accuracy is highly dependent on the ambiguity of the synsets, the relation type and, especially, the quantity of extracted information (size of N ). We later used the same algorithms to ontologise a random sample of 800 antonymy tb-triples between adjectives, from TeP. Using RP and the same N as the previous comparison, which contained only about 700 antonymy tb-triples, precision was 99.4%,in a trade-off of 40.8% recall. Using RP+AC, whichwe remindthat,whenRPcannotselectanattachment,exploitsalsonon-antonymytb-triples with AC, recall increased to 69.7 %, but precision decreased to 69.3 %.
In the end of this step, we still added all the 8,886 antonymy sb-triples of TeP directly to the final resource, Onto.PT. This enlarges the set of antonymy sb-triples in Onto.PT at a low cost and, as TeP was created by humans, we have high confidence on this information. 4.4 Resulting resource Onto.PT is now freely available as a RDF/OWL model, based on a similar representation of WordNet.Pr (van Assem et al. 2006 ). It can also be queried through a web interface. 5 We see it as a viable alternative or a complement to Portuguese LKBs, and as an important contribution for advancing the state-of-the-art of Portuguese NLP. 4.4.1 Quantitative data Onto.PT v.0.35 contains about 109,000 synsets, of which about 105,000 are involved in at least one sb-triple. Besides the discovered synsets, Onto.PT contains about 79,000 synsets with only one lexical item, resulting from arguments of tb-triples not covered by the synset-base.

As for relations, Onto.PT v.0.35 contains about 173,000 sb-triples of the extracted relations, which are the same as in PAPEL. Almost half of the sb-triples are hypernymy (  X  80,300). Property-of is the second relation with most sb-triples (  X  25,100 between adjectives and verbs, and  X  9,700 between adjectives and nouns). In descending order of quantity, the remaining covered relations are: purpose-of (  X  15,300), causation (  X  9,800), part-of (  X  8,500), member-of (  X  7,000), antonymy (  X  6,000), manner-of (  X  3,400), producer-of (  X  2,100), quality-of (  X  2,000), con-tained-in (  X  600), state-of (  X  500), place-of (  X  300), manner-without (  X  200).
Onto.PT is larger than WordNet.Pr 3.0 and has more relation types, which highlights an advantage of the automatic construction approach. Regarding that all the relations in Onto.PT can be inverted, it contains 2  X  173,000 = 346,000 sb-triples against the 285,000 of WordNet.Pr 3.0. The number of synsets is just slightly higher, more precisely 109,000 against 105,000.

Of course this number is insufficient to quantify the coverage of Onto.PT. So, to have a better idea on this, we evaluated its coverage of concepts that should be included in wordnets. The Global WordNet Association 6 provides a list with 164 base concepts, referred to as the  X  X ost important X  in the wordnets of English, Spanish, Dutch and Italian. 7 They are divided into 98 abstract and 66 concrete concepts, and are represented as WordNet.Pr 1.5 synsets. Based on rough matches we defined for the Onto.PT synsets, we concluded that this resource covers most of the concepts in the list, more precisely 92 abstract and 61 concrete synsets (93 %). 4.4.2 Evaluation Onto.PT v.0.35 was the target of an extensive two step manual evaluation, where the synsets were, first, considered alone and, then, in the sb-triples. Briefly, this evaluation encompassed the following steps: 1. Selection of two random samples of 300 sb-triples: one with only hypernymy 2. Reduction of the arguments of the previous sb-triples, in order to simplify 3. Establishment of a set containing all the reduced synsets with more than one 4. Classification of each synset as correct or incorrect, by two judges X  X  correct 5. Removal, from the initial samples of 300 sb-triples, of those connecting one 6. Classification of each of the remaining sb-triples as correct or incorrect. Once In the 600 sb-triples, there were 774 distinct synsets with more than one item. From those, 572 (73.9 %) and 58 (7.5 %) were respectively classified as correct and incorrect by both judges. For the remaining 144 (18.6 %), the judges did not agree.
Table 5 presents the results of the manual evaluation of sb-triples. For each sample, it shows the proportion of correct sb-triples, both considering that those with incorrect confidence interval of 95 %, the margin of error for the results of set A is also presented (ME A ), as well as the judge agreement, given by the number of matches ( IAA ), and by the  X  coefficient.

The values in the column  X  X orrect X  and sub-column  X  X  X  can be seen as an approximation of the correct sb-triples of Onto.PT. This value is lower for hypernymy than for other relations. A possible explanation for this is that, for most hypernymy relations, the more generic argument is actually very generic, which means it is frequently vague and with several different senses. The following are some of the most frequent hypernyms: pessoa (person), planta (plant),  X rvore (tree), indiv X duo (individual), instrumento (instrument), subst X ncia (substance), lugar (place), pe X a (piece).

As for the non-hypernymy relations, their average accuracy is between 78 and 82 %. We cannot take specific conclusions for each relation type, because not enough sb-triples of each were classified. Still, considering only the relations with at least 10 classified sb-triples, we can say that the best relations are member-of, causation (between verbs and nouns), has-quality (between adjectives and nouns), and property-of, all with more than 90 % sb-triples classified by both judges (J1 and J2) as correct. Moreover, in a number highly influenced by TeP X  X  information, all antonymy sb-triples classified between verbs, adjectives and adverbs were correct. On the other hand, part-of was the worst relation, with only 50 and 70 % sb-triples classified as correct by J1 and J2 respectively. The same explanation given for the low accuracy of the hypernymy relations can be given for part-of, as it also tends to have one very generic argument.

These results led to recent corrections in Onto.PT, so its reliability is now expected to be higher. Furthermore, in order to assess the utility of Onto.PT, this resource was used in an information retrieval task, where it provided information for synonymy expansion (Rodrigues et al. 2012 ). Using Onto.PT, the quality and quantity of the baseline approach were improved. 5 Concluding remarks We have presented ECO, a flexible approach to create a wordnet automatically, by exploiting textual resources. ECO is an alternative to the time-consuming manual creation of wordnets, and intends to minimise information sparsity issues by exploiting several heterogeneous resources. Of course, this is performed in a trade-off of lower reliability than that of handcrafted resources.

ECO is an abstract model that intends to extract knowledge and integrate it harmoniously, by combining different IE techniques. Briefly, it starts with the acquisition of semantic relations from text, which is usually done by exploiting indicative discriminating patterns. Then, synsets are discovered by the identification of clusters in the synonymy relations. Finally, the arguments of the relations acquired in the first step are attached to suitable synsets, selected after exploiting all the extracted information. The implementation of ECO is quite flexible and should have in mind the available resources and data.
 ECO was followed, with relative success, in the creation of Onto.PT, a Portuguese wordnet-like resource. Given the limitations of Portuguese LKBs, we believe this resource will help to advance the state of the art of Portuguese NLP. Onto.PT has already shown to be useful for synonymy expansion, and we are currently using it for other NLP tasks, including WSD, and multiple-choice question answering. Nevertheless, the evaluation results show that there is still a long way to go until we have a highly-reliable resource.

One limitation of the ECO generated wordnets towards most wordnets is that there are no guarantees that there is a well-formed hypernymy tree. In fact, most paths from the more specific synsets to the top-level are not more than three edges long (e.g. dalmatian  X  dog  X  mammal  X  animal ), some synsets are attached directly to top-level synsets (e.g. gazelle  X  animal ), and there might be a minor number of cycles as well. This problem should be quantified in the future, and solutions to re-organise the relations should be studied. It might involve, for instance, a set of rules for changing the attaching points automatically. Another limitation is the absence of synset glosses. We intend to tackle this issue by following a similar approach to Henrich et al. ( 2011 ), who automatically assign dictionary definitions to synsets.

We would like to conclude by reminding that any resource created by ECO cannot be seen as static. This means that, in the future, we can create new instances of Onto.PT, not only by improving issues identified along the performed evaluations, but also by exploiting other resources. For instance, we are planning to study the impact of using other related words in the clustering context; and we see the Portuguese Wikipedia as a possible target for future extractions, this time using preferably weakly-supervised bootstrapping techniques. Furthermore, other resources can be explored for the application of ECO in the creation of wordnets in other languages, or even on closed domains. Therefore, we see it as a general contribution to the automatic creation of knowledge bases.
 References
