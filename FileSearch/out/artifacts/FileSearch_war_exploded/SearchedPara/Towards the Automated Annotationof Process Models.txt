 Henrik Leopold 1( Nowadays, many organizations use business process models for documenting and improving their operations. However, only a few have recognized the full poten-tial their process models offer. In particular semantic technologies facilitate a wide range of possibilities that go beyond the documentation of business opera-tions [ 27 ]. For example, there are techniques available that use process models of business processes [ 10 ], and for discovering semantic weaknesses in business processes [ 2 ]. However, the limitation of all these approaches is that they build on an existing annotation of the process model activities, for instance, with concepts from a taxonomy. Recognizing this drawback, user-friendly approaches for semantic annotation have been proposed [ 4 ]. Still, the manual effort that is required for annotating process models is considerable and, in many cases, even hardly manageable taking into account that taxonomies often contain hundreds or even thousands of concepts.
 In this paper, we present the first approach for automatically annotating pro-cess models with the concepts of a taxonomy. At this stage, we focus on activity-based taxonomies such as the Supply-Chain Operations Reference-model (SCOR) [ 24 ], the MIT process handbook [ 17 ], and the Process Classification Framework (PCF) [ 1 ]. To this end, we define an approach that combines semantic similarity measurement with probabilistic optimization. In particular, we use different types of similarity between the process model and the taxonomy as well as the distance between the taxonomy concepts to guide the matching with a Markov Logic for-malization. In contrast to prior approaches in the domain of process modeling, we do not measure the similarity using WordNet, but build on the more powerful corpus-based approach of second-order similarity. An evaluation of our approach with a set of 12 process models consisting of 148 activities and the PCF taxonomy with 1,131 concepts shows that our technique performs significantly better than a naive baseline and indeed produces satisfying results.
 The rest of the paper is structured as follows. Section 2 illustrates the problem of automatically annotating process models with taxonomy concepts. Section 3 introduces the similarity functions we use for computing the input for our prob-abilistic optimization. Section 4 introduces Markov Logic Networks and defines the probabilistic optimization problem using a Markov Logic formalization. Section 5 presents the evaluation of our approach. Section 6 discusses related work before Section 7 concludes the paper. The goal of this paper is to present an approach for the automated annotation of process models with the concepts of an activity-based taxonomy. It builds on two types of input: a process model P consisting of a set of activities an activity taxonomy T , which is specified as follows: Definition 1 (Activity Taxonomy). An activity taxonomy is a tuple (
A , r , H ) such that  X 
A t is a finite and non-empty set of activities. We refer to them as concepts .  X  r  X  A represents the taxonomy root.  X 
H  X  A  X  ( A \{ r } ) is the set of parent-child relationships such that (  X  H if a  X 
H is an acyclic and coherent relation such that each concept exactly one direct parent.
 We further use C = { a | ( r, a )  X  H } to refer to the direct children of the taxonomy root r . They represent the roots of what we refer to as taxonomy categories. The function c ( a )= { c a | c a  X  C  X  ( c a a concept a  X  A t belongs to. Note that the taxonomy categories are disjoint and, hence, | c ( a ) | =1.
 omy T are captured by the relation A : A p  X  A t . An element ( that the activity a p is annotated with the concept a t , i.e., they both represent similar semantics. Note that each activity is annotated with at most one concept. However, one concept can be used as annotation for several activities. by showing a simple hiring process and its annotations with the concepts from the PCF taxonomy. In total, the process consists of five activities. First, the job offer is distributed. Afterwards, the records of the applicant are checked and an interview is conducted. Based on the result of the interview, the applicant is either rejected or accepted. Once the corresponding letter was sent, the pro-cess is finished. The grey shades visualize the annotations of the activities of the process model with the taxonomy concepts. We observe that all activities belong to the category Develop and Manage Human Capital . Considering the annotations in more detail, it becomes clear that the automatic identification of these annotations is by no means trivial. While the activity Distribute job offer and the corresponding concept Post job requisition at least share the common word job , the connection between Check records of applicants and Identify and deploy candidate selection tools is purely semantic. A similar situation can be observed for the activities Send letter of rejection and Send letter of acceptance , which are both annotated with the concept Select and reject candidates . Beyond the challenge of recognizing semantic relationships between activities and concepts, we have to take the large number of taxonomy concepts into account. In total, version 5.2 of the PCF taxonomy contains 1,131 concepts. While the exemplary process from Figure 1 only contains activities relating to concepts from the same category, this is no justifiable assumption for a usable approach. In practice, we may encounter cross-sectional processes, which are related to several categories. Hence, we have to consider all concepts from the taxonomy as potential annotation candidates.
 To the best of our knowledge, there is currently no technique available that is capable of automatically annotating process model activities with the concepts of a taxonomy. Hence, we define such an approach in the subsequent sections. In this section, we introduce the similarity functions we use for generating the input for our optimization problem. In total, we define three separate functions: a function for capturing the similarity between activities and concepts, a function for capturing the similarity between the process model and a taxonomy category, and a distance cost function capturing the distance between taxonomy concepts. 3.1 Similarity Between Activities and Taxonomy Concepts To measure the similarity between a process model activity and a taxonomy concept, we automatically decompose them into their semantic components. As pointed out in [ 18 ], activities can be characterized by three components: an action, a business object on which the action is performed, and an optional addi-tional information fragment that is providing further details. As an example, consider the process model activity Perform interview with employee .Itcon-sists of the action perform , the business object interview , and the additional information fragment with employee . The same procedure can be applied to the concepts of an activity taxonomy. In order to accomplish this decomposition in an automated way, we employ the technique defined in [ 15 ].
 Building on the decomposition, we compute the semantic similarity between the actions, business objects, and additional information fragments of the con-sidered activity-concept pair. A challenge in this context is the usage of specific terminology from business settings, which is often not fully captured by standard natural language tools such as WordNet [ 19 ]. Hence, we determine the similarity between two components using a corpus-based method called second-order sim-ilarity [ 11 ]. The approach of second-order similarity is based on the statistical analysis of co-occurrences in large text collections and has been implemented in several tools such as NLS [ 5 ]orDISCO[ 13 ]. In comparison to WordNet, second-order similarity has the advantage that it is not restricted to a set of manually predefined term relations and, hence, is more powerful for our purposes. In order to calculate the semantic similarity between a process model activity taxonomy concept a t , we introduce three functions: a component similarity func-tion cpsim , a coverage function cov , and a activity-concept similarity function sim , combining the latter two into a final result.
 nents cp 1 and cp 2 derived from an activity-concept pair. In general, the second-order similarity sim SO is returned. In case one or both of the concepts represent an empty string, cpsim returns zero. of an activity or a concept a  X  A p  X  A t . Note that the index act in the defini-tion denotes an action, bo a business object and add an additional information fragment. introduce the function sim . It calculates the arithmetic mean of the similarity values for action, business object, and the additional information fragment. This is accomplished by dividing the sum of cpsim act , cpsim bo maximum coverage among the input activity-concept pair a p As a result, we obtain the overall semantic similarity for an activity-concept pair. ity values. These values form the basis for our automatic annotation approach. 3.2 Similarity Between Process Models and Taxonomy Categories For activities containing frequently occurring words the sole consideration of the similarity function sim may not be sufficient for identifying the best fitting concept. As an example, consider the activity Develop strategy , whose business object strategy occurs in six categories of the PCF taxonomy. In such a situation, it would be helpful to quantify the similarity between the entire process model and the different taxonomy categories. The resulting values could complement the individual similarity scores derived from sim in order to truly identify the best fitting candidate.
 we adopt the general idea of term frequency-inverse document frequency (tf-idf) and the vector space model from the domain of information retrieval [ 23 ]and modify them to meet the characteristics of our problem. Using the tf-idf it is possible to determine the discriminative power of a word. In the context of a taxonomy, the tf-idf assigns high weights to words that frequently occur in a particular category but rarely in the entire taxonomy. Contrarily, words that occur in many or even all categories have hardly any discriminative power and hence receive a low weight. Let A c denote the concepts from a category and f ( w, c ) the frequency of a word w among the concepts of c is defined as follows: Based on the tf-idf values, we can create a vector representation for the entire process model as well as for taxonomy categories. Therefore, we calculate the tf-idf values for each word from A p and A c and store them in the vectors and v c . As a result, we obtain a vector representation of the process model and the category in a vector space. Using the cosine similarity, it is now possible to measure the distance between these vectors and to quantify the similarity between a process model and a category. Accordingly, we introduce the similarity function rel , which we define as follows: By calculating rel for each category c  X  C , we receive a set of similarity values which complement the similarity values from sim . 3.3 Distance Costs Between Taxonomy Concepts Besides the semantic perspective, we also need to take the structure of the process model and the taxonomy into account. Assuming that process models describe the underlying process in a rather coherent fashion, we would not expect large  X  X eaps X  between the annotations of two neighboring activities. For instance, we would assume that two subsequent activities are rather annotated with concepts 6.2.1 and 6.2.6 than with 7.1 and 3.2.1.5 . To penalize such leaps, we introduce a distance cost function dc based on concept similarity introduced by Wu and Palmer [ 28 ]. It quantifies the distance between two concepts based on the graph structure of the taxonomy and their least common superconcept concepts a 1 ,a 2  X  A t , we define dc as follows: where N1 is the number of concepts on the path from a 1 to of concepts on the path from a 2 to a s , and N3 is the number of concepts on the path from a s to the taxonomy root r .
 By calculating dc for each concept pair, we obtain a set of distance cost values in the interval [-1,1], i.e., only big leaps are penalized. Together with the previously introduced similarity values they form the input for our probabilistic annotation model. Markov Logic (ML) is a formalism that combines first order logic with undirected probabilistic models, i.e., Markov Networks [ 21 ]. A Markov Logic formalization of a given problem consists of a set of weighted and unweighted logical formulae. These formulae describe observations relevant to the concrete problem instance and general constraints that have to hold for each instance of the problem class. By replacing variables with concrete values, which is called grounding, it is pos-sible to transform the Markov Logic formalization into a Markov Logic Network. For technical details we refer to [ 21 ].
 marginal inference and maximum a-posteriori (MAP) inference. In the context of our work, we are interested in MAP inference. MAP inference computes the most probable assignment of truth values to the ground atoms of the given formalization. The MAP state, which is the result of applying MAP inference, corresponds in our setting to the most probable annotation of activities with concepts. Since the underlying probabilistic model is log linear, the MAP state is the solution which is maximal with respect to the sum of weights attached to the formulae.
 relatedness, and distance costs. To incorporate these functions into our Markov Logic formalization, we introduce a predicate for two of these functions and weigh each grounded atom with the value that results from applying the corresponding similarity function. For the sake of simplicity, we use the same names for these predicates as introduced for the corresponding similarity measures, i.e., we use  X  sim ( a p ,a t ) to express that activity a p and concept  X  dc ( a t ,a t ) to express that a t and a t are located close to each other. by computing all possible groundings. Additionally, we add unweighted formulae that describe the sequence flow in the given process model and the structure of the taxonomy. In particular, we use the predicates  X  suc ( a p ,a p ) to express that activity a p directly succeeds activity  X  cat ( c, a t ) to express that a t belongs to the category predicate. The groundings of the annotate predicate correspond to the solution of the annotation problem. First of all, we add a constraint that enforces to anno-tate each activity with only one concept, i.e., we define the predicate to be functional. Note that we represent a weighted formula as a pair, where the first element is the formula itself and the second element is the associated weight. Using we refer to a hard (unweighted) formula that has to be true in every possible world. Now we link the activity-concept similarity to the This formula means that the weight attached to sim ( a p ,a objective of our optimization problem if annotate ( a p ,a The model we defined so far will create a functional mapping as MAP state that is optimal with respect to the similarity weights given in our evidence with the guarantee that the mapping is functional. We extend this model by taking the relatedness score into account. For each category c we add one weighted formula using the following schema. If c is the category to which a t belongs to and if a p is annotated with relevance score rel ( c ) of category c is added to the objective. By adding these weighted rules, we ensure that concepts from a category that are more relevant with respect to the given process model are preferred over concepts from less relevant categories.
 Finally, we want to penalize pairs of annotations where two consecutive activ-ities a p and a p are annotated with two concepts a t and a t that are located at different places in the taxonomy. Since a p and are a p are directly connected by a sequence flow, we would expect that their counterparts are located closely to each other in the taxonomy. The following constraint enforces that we have to add the distance cost, which is the weight associated to dc consecutive activities a p and a p are annotated with a t annotate ( a p ,a t )  X  annotate ( a p ,a t )  X  suc ( a p The positive impact of this constraint can be best explained with the help of Figure 1 . Suppose that the concepts 6.2.3.2 and 6.1.2.3 have similarly high sim-ilarity values for the activity Conduct Interview with Applicant . Further suppose that our approach detected a high similarity between Send letter of rejection and concept 6.2.3.4 . Due to Formula 10 , 6.2.3.2 will now be preferred over 6.1.2.3 as annotation for Conduct Interview with Applicant , because there is a large dis-tance between 6.1.2.3 and 6.2.3.4 , while 6.2.3.2 and 6.2.3.4 are located close to each other. This illustrates nicely that our approach solves the annotation problem as a whole taking interdependencies between potential annotations into account. To demonstrate the applicability of our approach, we conduct an evaluation with a set of manually annotated process models and the PCF taxonomy. The goal of the evaluation is to learn how well our approach can approximate the manual annotation. Section 5.1 introduces the data set we use for the evaluation. Section 5.2 introduces the details of the evaluation setup. Finally, Section 5.3 presents the evaluation results. 5.1 Test Collection As we are the first to present an automated approach for annotating process models with taxonomy concepts, there is currently no commonly accepted test sample available. Hence, we use a set of BPMN process models that was created during a PCF case study by students from the University of Osnabr  X  uck, Ger-many. In the context of this case study, groups of students were asked to model a set of three fictitious business processes from the area of change management, product development, and human resources using BPMN. In addition, they had to annotate the activities of the models with the corresponding PCF concepts. We use the original annotations created by the students without any modifi-cations. The resulting model set comprises twelve manually annotated BPMN process models consisting of a total of 148 activities.
 of activities, the main topic of the model, and the PCF categories the activities of the process model were assigned to. It shows that the models vary in size as well as their coverage of the different PCF categories. Moreover, some models are cross sectional (i.e., models 5, 6, and 7) while others only belong to a single PCF category. Thus, we believe that the test set is well-suited to demonstrate the applicability of our annotation approach. 5.2 Setup For evaluating the approach presented in this paper, we implemented it in the context of a prototype. The prototype is based on the activity analysis tech-nique from [ 15 ], the second-order similarity implementation DISCO [ 13 ], and the Markov Logic Network implementation RockIt [ 20 ]. We used our prototype to automatically generate the annotations for the models from our test set and the text-based PCF taxonomy version 5.2. We then compared the automatically generated annotations A with the manual annotation from the students on this comparison, we can assess the quality of the annotation by computing the metrics precision and recall: In our context, precision is the number of correct annotations computed by our approach divided by the number of annotations our approach proposed. Recall is the number of correct annotations computed by our approach divided by the total number of annotations according to the manually created gold standard. However, the drawback of the standard precision and recall metrics for our context is that they only consider annotations that are correct up to the last sub category. As an example, consider an activity a p that was manually annotated with the concept 6.2.1.4 . If our approach proposes to annotate concept 6.1.2.2 , this would be simply considered as incorrect although the first three levels of the manually annotated concept were actually identified correctly. To provide for a more fine granular perspective on our results, we introduce a level-based form of precision and, recall which is in line with the approach presented in [ 8 ].
 To this end, we introduce a function parent i ( a t ), which returns the of a concept a t  X  A t . We define parent i ( a t )= parent parent i ( a t )= a t for all i  X  0. Thus, for instance, parent 6.2 for the input concept 6.2.1.4 . We further introduce a function which returns the level of a concept a t in the taxonomy. It, for example, returns 4 for the concept 6.2.1.4 and 2 for the concept 6.2 . Based on these definitions, we introduce a function l n , which maps a set of annotations fine-grained annotations from level n : As an example, consider the set of annotations { ( Distribute job offer , 6.2.1.4 ), ( Recruit employees , 6.2.2 ) } . For these annotations, l job offer , 6.2 ), ( Recruit employees , 6.2 ) } and l 1 would return level-based form of precision and recall. four levels, we accordingly use four levels of precision and recall to evaluate our results. In addition, we report the f-measure fm n for each level, which is the harmonic mean of pre n and rec n . Respectively, the metrics provide information about annotations that are correct up to the forth level and pre with respect to the main category. 5.3 Results To demonstrate the applicability of the approach presented in this paper, we tested different configurations:  X  Baseline : As baseline configuration, we annotated each activity  X  ML with Wu &amp; Palmer : For this configuration we used our ML formaliza- X  ML with Category Weighting : For this configuration we used our ML  X  Full ML Configuration : For the full configuration we included all previ-configuration without Markov logic yields quite low results. The value of 0.20 for the metric pre 1 indicates that only one out of five activities is annotated with a concept from the correct main category. The consideration of the additional similarity measures in the context of our Markov implementation improves the The f-measure fm 1 of the category weighting configuration rises from 0.20 to 0.76 and fm 2 rises from 0.18 to 0.38. Apparently, the additional category weight helps to rule out candidates that have high values for sim , but generally cannot be related to the process model. The positive effect of the Wu &amp; Palmer distance can be observed for the full configuration. The combination of the category weighting and the Wu &amp; Palmer distance causes an additional increase of 0.45. The value of fm 1 , however, remains identical. This effect can be explained by the fact that the Wu &amp; Palmer distance does not cause the approach to consider additional (and potentially correct) concepts. It rather improves the coherence of the annotations among the already considered candidates. Hence, it improves fm 2 without affecting fm 1 . In addition, it does not compromise fm the Wu &amp; Palmer distance on the third and fourth level where the would have actually indicated the correct annotation.
 Altogether, the results suggest that both the category weights as well as the Wu &amp; Palmer distance are helpful for finding correct and ruling out incorrect annotations. Taking the huge complexity of the annotation problem into account -each activity has more than thousand potential annotations -the results have to be considered as good. About 76% of all activities were annotated with a concept from the correct main category and 44% of all activities were annotated with the correct subcategory. Figure 2 gives an indication of where our approach could be improved by showing the values of fm n for each model separately. From the numbers from Figure 2 we can learn that particularly the zero values for models 3 and 8 negatively affect the overall result. The reason for these numbers can be found in the use of highly specific words for which we were not able to obtain a similarity value. As a result, the weight the small values from sim and causes an erroneous annotation of the entire process model. In fact, the use of specific words also causes a major share of the incorrect annotations among the other models. As an example, consider the activity Sales contact OEM for details from model 8. Due to the domain specific abbreviation OEM , the values sim do not help us to identify the correct concepts from the taxonomy.
 Besides the problem with specific words, the reason for the sub optimal results on the levels three and four is given by the fact that some concepts are seman-tically quite close. As an example, consider the activity Recommend candidate from model 9. Our approach annotated this activity with the concept 6.2.4.3 Recommend/not recommend candidate whereas the manual annotation assigned it to 6.2.3.4 Select and reject candidates . Although the manual annotation is an arguably better choice than the computed one, both concepts represent seman-tically similar choices. The work presented in this paper relates to two major streams of research: pro-cess model annotation and process model matching.
 general guidelines and strategies [ 16 ] or the benefits and potentials associated with the annotation [ 25 ]. Some approaches also automatically filter relevant con-cepts from the considered ontology [ 3 , 4 , 7 ]. The final decision about the anno-tation is, however, still taken by the user. Hence, we are, to the best of our knowledge, the first who present an automatic approach for annotating process models.
 dences between two process models. In prior work, a plethora of process model matching approaches has been proposed [ 6 ]. Typically, they build on a com-bination of structural or behavioral properties with different types of textual similarity. Some rely on rather simplistic techniques such as the Levenshtein dis-so far, no approach has considered the use of second-order similarity. Besides these conceptual differences, it is worth noting that the overall complexity of automated annotation is considerably higher. While a process model typically does not consist of more than 30 activities, taxonomies often contain more than thousand concepts [ 1 ]. In this paper, we presented the first approach for automatically annotating pro-cess models with concepts of a taxonomy. Our approach uses a Markov Logic formalization to combine the results of different similarity functions. In contrast to prior approaches from the area of process modeling, we did not use WordNet for measuring the relatedness of words but the corpus-based method of second-order similarity. An evaluation of our approach with 12 process models consisting of 148 activities and the PCF taxonomy consisting of 1,131 concepts showed that our approach performs significantly better than a naive baseline and is able to compute satisfying results.
 As for future work, we consider two main directions. First, we aim at improv-ing the performance of our approach. Promising directions for accomplishing this goal include the consideration of additional information of the process model such as the control flow and the improvement of the similarity measurement by training DISCO with domain-specific corpora. Second, we plan to study to what extent our approach can be transferred to other types of ontologies as, for instance, the MIT Process Handbook, which uses both part-of as well as is-a relations to structure business activities.

