
Naive Bayes(simply NB)[12] has been widely used in machine learning and data mining as a simple and effective classification algorithm. Since its conditional independence assumption is rarely true, researchers have made a substan-tial amount of effort to improve naive Bayes. The related research work can be broadly divided into two approaches: eager learning and lazy learning, depending on when the major computation occurs. Different from eager approach, the key idea for extending naive Bayes from the lazy ap-proach is to learn a naive Bayes for each testing example. In recent years, some lazy extensions of naive Bayes have been proposed. For example, SNNB[18], LWNB[7], and LBR[19]. All are aiming at improving the classification ac-curacy of naive Bayes. In many real-world machine learn-ing and data mining applications, however, an accurate ranking is more desirable than an accurate classification. Responding to this fact, we present a lazy learning algo-rithm called instance greedily cloning naive Bayes (simply IGCNB) in this paper. Our motivation is to improve naive Bayes X  ranking performance measured by AUC[4, 14]. We experimentally tested our algorithm, using the whole 36 UCI datasets recommended by Weka[1], and compared it to C4.4[16], NB[12], SNNB[18] and LWNB[7]. The exper-imental results show that our algorithm outperforms all the other algorithms used to compare significantly in yielding accurate ranking.
Classification is one of the most important tasks in ma-chine learning and data mining. Learning Bayesian clas-sifiers is a process of constructing a special Bayesian net-works from a given set of preclassified instances, each of which is represented by a vector of attribute values. As-sume A i ,i =1 , 2 ,...,n are n attributes which take values a ,i =1 , 2 ,...,n respectively. Those attributes will be used collectively to predict the value c of the class C. Thus, the Bayesian classifier represented by a Bayesian network can be defined as: Assume all attributes are independent given the class. That is:
The resulting classifier is called a naive Bayesian classi-fier, or simply naive Bayes:
Naive Bayes is a probability-based classification model which is based on the assumption that attributes are condi-tionally independent given the class label. Although naive Bayes has many advantages, such as conceptual and compu-tational simplicity, its unrealistic attribute independence as-sumption leads to that its probability estimation is poor[3], if there exist strong attribute dependencies.

To overcome the limitation of naive Bayes, a natural way is to represent attribute dependencies. It is well-known that Baysian networks can represent arbitrary attribute depen-dencies, and thus can be used to overcome the unrealistic attribute independence assumption of naive Bayes. Un-fortunately, however, it has been proved that learning an optimal Bayesian networks is NP-hard[5]. In order to es-cape Bayesian networks X  X  learning complexity, learning im-proved naive Bayes has attracted much attention from re-searchers. Related work can be broadly divided into two approaches: eager learning and lazy learning, depending on when the major computation occurs[2].

In classification, the predictive ability of a classifier is typically measured by its predictive accuracy on the testing examples. In fact, most classifiers (including naive Bayes) can also produce probability estimation or  X  X onfidence X  of the class prediction. That is the class probability which is the probability that example E is in class c . Unfor-tunately, however, this i nformation is completely ignored in classification.

In many data mining applications, however, the classifi-cation accuracy is often not enough. For example, in direct marketing, we often need to promote the top X% of cus-tomers during gradual roll-out, or we often deploy differ-ent promotion strategies to customers with different likeli-hood of buying some products. To accomplish these tasks, a ranking of customers in terms of their likelihood of buy-ing is more useful than merely a classification of buyer or non-buyer.

If we are aiming at accurate ranking, we need a better method than the predictive accuracy to evaluate the clas-sifiers that also produce ranking. Notice that we are only given a set of training examples with class labels in many scenarios. In recent years, the ROC (Receiver Operating Characteristics) curve[4, 14] is widely used for this purpose. A ROC curve can be used to compares the performance of classifiers cross the entire range of class distributions and error costs. Figure 1 shows a plot of four ROC curves, each representing one of the four classifiers, A through D .A ROC curve X is said to dominate another ROC curve Y if X is always above and to the left of Y . This means that the classifier of X always has a lower expected cost than that of
Y , over all possible error costs and class distributions. In this example, A and B dominate D .

However, often there is no clear dominating relation be-tween two ROC curves. For example, curves A and B are not dominating each other in the whole range. In those sit-uations, the area under the ROC curve, or simply AUC, is a good  X  X ummary X  for comparing the two ROC curves. Hand and Till [9] show that, for binary classification, AUC is equivalent to the probability that a randomly chosen ex-ample of class  X  will have a smaller estimated probability of belonging to class + than a randomly chosen example of class +. They present a simple approach to calculating the AUC of a classifier G below. where n 0 and n 1 are the numbers of negative and positive examples respectively, and S 0 = r i ,where r i is the rank of i th positive example in the ranked list. From Equation 4, it is clear that AUC is essentially a measure of the quality of a ranking. For example, the AUC of a ranking is 1 (the maximum value of AUC) if there is no positive example preceding a negative example.

In this paper, we investigate the performance on rank-ing of some existing lazy improved algorithms (SNNB[18], LWNB[7] and LBR[19]) and find that they can not meet our needs when an accurate ranking is more desirable. Our motivation is to develop a new algorithm to improve naive Bayes X  performance on ranking measured by AUC. In or-der to achieve our goal, we present a lazy learning algo-rithm called instance greedily cloning naive Bayes (simply IGCNB). The experimental results show that our algorithm outperforms all the other algorithms used to compare sig-nificantly in yielding accurate ranking.

The rest of the paper is organized as follows. In Section 2, we introduce the related work on improving naive Bayes and on improving decision tree for ranking. In Section 3, we present the new lazy learning algorithm IGCNB and make a simple analysis on its advantages and disadvantages. In Section 4, we describe the experimental setup and results in detail. In Section 5, we make a conclusion and outline our main directions for future research.
Naive Bayes is a simple but effective classifier. Although its conditional independence assumption is often violated, it performs surprisingly well in classification[6]. This fact raises the question of whether a Bayesian classifier with less restrictive assumptions can perform even better. In the recent decade, learning Bayesian networks from data has become a rapidly growing field of research. Unfortu-nately, however, it has been proved that learning an optimal Bayesian networks is NP-hard[5].

Therefore, researchers have made a substantial amount of effort to improve naive Bayes. Related work can be broadly divided into two approaches: eager learning and lazy learning, depending on when the major computation occurs[2]. Eager learning does major computation at train-ing time. For example, SBC[13], TAN[8], and NBTree[11]. Different from eager learni ng, lazy learning spends little or no effort during training and delaying computation un-til classification time. For example, SNNB[18], LWNB[7], and LBR[19]. Our algorithm described in Section 3 is a lazy learning algorithm, so we investigate the performance on ranking of some other lazy improved algorithms.
Xie et al.[18] propose a model called selective neigh-borhood naive Bayes (simply SNNB). At first, SNNB con-structs multiple naive Bayes on multiple neighborhoods by using different radius value s for a test instance. Then, it selects the most accurate one to classify the test instance. Their experimental results show that SNNB outperforms not only the naive Bayes and NBTree, but also several other state-of-art classification methods in terms of accuracy. In this paper, we are more interested in its performance on ranking. Motivated by our in terest, we implement SNNB within the Weka framework[17] and investigate its perfor-mance on ranking in Section 4. However, unfortunately, SNNB can X  X  improve naive Bayes X  performance on ranking significantly measured by AUC.

Frank et al.[7] present an model to combine instance-based k-nearest neighbor with naive Bayes, called locally weighted naive Bayes(simply LWNB). In LWNB, each of nearest neighbors is weighted in terms of its distance to the test instance. Then a local naive Bayes is built from the weighted training instances. Their experimental results show that LWNB outperforms naive Bayes significantly. Although their experiments show that LWNB is not particu-larly sensitive to the choice of value of K as long as it is not too small, it is a K -related algorithm and its performance affected by the parameter of K to some extent.
 Zheng and Webb [19] propose an approach called Lazy Bayesian Rule(simply LBR). LBR generates a rule most appropriate to the test instance before classifying a test in-stance. The training instances that satisfy the antecedent of the rule are chosen as the train ing data for the local naive Bayes, and this local naive Bayes only uses those attributes that do not appear in the antecedent of the rule. Their ex-periments show that LBR obtains lower error rates signifi-cantly than naive Bayes etc. We use the implementation of weka.classifiers.lazy.LBR.java in the Weka-3-4 version to investigate its performance on ranking measured by AUC and find that LBR performs even worse than naive Bayes in ranking significantly.

Decision tree learning algorithms are a major type of ef-fective learning algorithms in many applications. However, unfortunately, traditional decision tree algorithms, such as C4.5, have been observed to produce poor ranking[15]. Aiming at this fact, Provost and Domingos[16] presented an algorithm called C4.4 to improve C4.5 X  X  performance on ranking measured by AUC. In detail, they used two tech-niques to improve the AUC of C4.5. 1) Smooth probability estimates by Laplace correction: Assume that there are p examples of the class at a leaf, N total examples, and C to-tal classes. The frequency-based estimation calculates the estimated probability as p N . The Laplace estimation calcu-lates the estimated probability as p +1 N + C . 2) Turn off pruning: Provost and Domingos[16]show that pruning a large tree damages the probability estimation. Thus, a simple strategy to improve the probability estimation is to build a large tree without pruning. It has been observed that combining naive Bayes with K -nearest neighbors achieves a s ignificant improvement over naive Bayes in accuracy [7]. The key idea is to deploy a naive Bayes within the k nearest neighbors of the testing instance, instead of deploying a naive Bayes on the whole training data set. In our previous work, we proposed to use instance cloning to expand the training set of the k nearest neighbors [10]. The algorithm is called ICLNB. However, one problem in ICLNB is that the value of k must be deter-mined. In addition, a search process for a better expanded training set would be helpful. These two observations are the key ideas for our new algorithm.
 At first, we define a similarity function for two instances. Assume that instance x is described by the feature vector where a i ( x ) denotes the value of the i th attribute of the in-stance x . Then the similarity between two instances x and y is defined as: where s ( x, y ) is a function that simply counts the number of iden-tical attributes of x and y . Roughly speaking, this func-tion measures the extent of similarity between these two in-stances.

Given a training data set T and a test instance x ,we firstly use Equation 5 to calculate the similarity between the test instance x and each instance in T . Then, we conduct a greedy search process to expand the current training data set T by cloning each instance to get a new training data set NT on which naive Bayes yields higher AUC. The proce-dure is greedy since it stops whenever the AUC on the new NT is not higher than the current one. At last, we deploy a naive Bayes on NT to produce probability estimates.
It is obvious that the main procedure of our algorithm is to acquire new training data set NT for the test instance by greedily expanding the current training data set T .We call our algorithm instance greedily cloning naive Bayes, simply IGCNB. Our IGCNB algorithm is a lazy algorithm because it spends no effort during training and delays com-putation until classification time. The IGCNB algorithm is depicted below in detail.
 Algorithm : IGCNB( T , x ) Input : a training data set T , and a test instance x . Output : the probability estimation  X  p ( c | x ) .
In summary, Our lazy algorithm relaxes naive Bayes X  unrealistic attribute independence assumption by greedily cloning training instances to produce a new training data set. As a result, our algorithm improve naive Bayes X  per-formance on ranking measured by AUC significantly. Since our algorithm is lazy algorithm, it has relatively higher time complexity.
We ran our experiments on 36 UCI data sets recom-mended by Weka[1], which are listed in Table 1. We down-load these data sets in format of arff from main web of Weka[17]. Currently, we don X  X  handle the missing and nu-meric values of attributes. Instead, we applied the following four preprocessing processes in Weka system. 1. We use the filter of ReplaceMissingValues in Weka to 2. We use the filter of Discretize in Weka to discretize the 3. We notice that, if the number of values of an attribute 4. Due to the relatively high time complexity of SNNB
We conducted experiments to compare IGCNB on AUC with C4.4, NB, SNNB, and LWNB. Multi-class AUC has been calculated by M-measure[9]. The AUC of each clas-sifier was measured via the ten-fold cross validation for all data sets. Runs with the various classifiers were carried out on the same training sets and evaluated on the same test sets. In particular, the cross-validation folds are the same for all the experiments on each data set. Throughout, we compare our algorithm with each other algorithm via two-tailed t-test with significantly different probability of 0.95, because we speak of two results for a data set as being  X  X ignificantly different X  only if the difference is statistically significant at the 0.05 level according to the corrected two-tailed t-test.
Table 2 show the AUC and standard deviation of each classifier on the test sets of each data set, the average AUC and standard deviation are summarized at the bottom of the table. Table 3 shows the results of two-tailed t-test with confidence level of 95% between each pair of algorithms in terms of accuracy. each entry w / t / l in Table 3 means that the algorithm at the corresponding row wins in w data sets, ties in t data sets, and loses in l data sets, compared to the algorithm at the corresponding column.

The detailed results displayed in Table 2 and Table 3 show that our algorithm outperforms all the other algo-rithms used to compare measured by AUC significantly. Now, let X  X  summarize the highlights as follows: 1. IGCNB also outperforms C4.4 significantly measured 2. IGCNB outperforms NB significantly measured by 3. IGCNB not only outperforms NB but also outperforms 4. Besides having better performance on ranking, IGCNB
In our experiments, we also observe the classification ac-curacy of each algorithm, shown in Table 4. Table 5 shows the results of two-tailed t-test with confidence level of 95% between each pair of algorithms in terms of accuracy. We can see that our experiments show the same experimental results by other researchers, in the sense that both SNNB and LWNB improve the classification performance of NB significantly. It is also interesting to notice that IGCNB also slightly outperforms all the other algorithms in terms of classification accuracy, while its ranking performance is significantly better than others.
In this paper, we investigate some state-of-the-art lazy algorithms for extending naive Bayes on ranking perfor-mance and find that they can not meet our needs when an accurate ranking is more desirable. Motivated by this fact and aiming at improving NB with accurate ranking, we present a new lazy learning algorithm called instance greed-ily cloning naive Bayes (simply IGCNB). We experimen-tally test our algorithm measured by AUC, using the whole 36 UCI data sets recommended by Weka[1], and compare our algorithm (IGCNB) with C4.4[16], NB[12], SNNB[18] and LWNB[7]. The experimental results show that our al-gorithm outperforms all the other algorithms significantly in yielding accurate ranking. In a word, we believe that our work provides an effective machine learning and data mining algorithm especially when a ranking are more desir-able. A potential problem with IGCNB is that IGCNB has relatively higher time complexity. So, enhancing our algo-rithm X  X  efficiency is our main directions for future research. Weka.
  X  11.25 96.67  X  10.54 98.33  X  5.27 naive Bayes(K=30); IGCNB: instance greedily cloning naive Bayes.
