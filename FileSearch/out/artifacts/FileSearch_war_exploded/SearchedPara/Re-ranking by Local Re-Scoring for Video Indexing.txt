 Video retrieval can be done by ranking the samples accord-ing to their probability scores that were predicted by clas-sifiers. It is often possible to improve the retrieval perfor-mance by re-ranking the samples. In this paper, we proposed a re-ranking method that improves the performance of se-mantic video indexing and retrieval, by re-evaluating the scores of the shots by the homogeneity and the nature of the video they belong to. Compared to previous works, the proposed method provides a framework for the re-ranking via the homogeneous distribution of video shots content in a temporal sequence. The experimental results showed that the proposed re-ranking method was able to improve the sys-tem performance by about 18% in average on the TRECVID 2010 semantic indexing task, videos collection with homoge-neous contents. For TRECVID 2008, in the case of collec-tions of videos with non-homogeneous contents, the system performance was improved by about 11-13%.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Abstracting methods, Indexing me-thods ;I.2.6[ Artificial Intelligence ]: Learning X  Concept learning Algorithms, Experimentation Video Indexing and Retrieval, Re-ranking
Semantic indexing and retrieval for video databases has been a very active research field over the past few years. The global goal is to automatically describe the videos, then to index them by their contents. Nevertheless, retrieving rel-evant samples and easily navigating within large collection, are still very difficult tasks.
 Generally, semantic indexing is achieved by supervised learn-ing approaches, in which it is based on training classifier on positive and negative samples of a target concept (the devel-opment set). This classifier will generate a model, which will be used to predict the likeliness of new samples (the test set) to contain the target concept. The likeliness is often com-puted homogeneously to a probability for each data sample to contain the concept. Retrieval can then be done by rank-ing the samples according to their probability scores. Such ranking is initially done with a score independently for each sample using only information from the development set. It is often possible to improve the indexing or retrieval perfor-mance by re-ranking the samples, considering the results of the initial ranking on the whole test collection. Thus, re-ranking may lead to retrieve more relevant samples at the top of the ranked list. Recently, several methods have been proposed and developed for re-ranking. We review below some of these methods.
 In context fusion [3, 5], the results of different searching models (concept-based search model, text-based search mod-el and query by example) are used to re-rank the ranked lists. In fact, the focus here is on fusing output scores of different models. This method needs to train new classifiers on new descriptors. Since we also use, in our work, the fusion of output scores obtained by multiple models, we took this as a baseline approach.
 Classification-based re-ranking [4], where the initial results of a baseline system are used to discover the co-occurrence patterns between the target semantics and extracted fea-tures. This is very similar to  X  X earning to rank X  [2], which is based on training a ranking model which can precisely predict the ranking lists in the dataset. In [4], the authors used the top-ranked and low-ranked samples respectively, as pseudo-positive and pseudo-negative examples to train a new classification model for ranking, and the classifica-tion margin for a target concept is regarded as its (new) re-ranked. The use of SVM as the classification model, leads to the method called RankSVM [2]. Ordinal re-ranking as it was proposed in [8], where the author re-ranks an initial results by using the co-occurrence patterns via the ranking functions. The final score is the weighting combination of the original score and the re-ranked scores. They adopted a training method to train the Re-ranking algorithm on some concepts, and the re-ranking algorithm was applied to re-rank the remaining concepts. In the case of video collections: the retrieval units are of-ten some video shots, rather than the whole videos them-selves. Our contribution in this paper is to re-rank the video shots according to their initial scores, which were obtained from initial classifiers, and according to the video knowl-edge and nature. Compared with the work in [7], where the authors re-ranked the initial results of shots using the video knowledge score, which was estimated by calculating the arithmetic mean on the initial scores of all shots within the same video. This paper goes further: the generalized mean rule was adopted to calculate global score for each shot, depending on the knowledge obtained from the scores of its neighbors within the video, and it has been proved to be more efficient. Moreover, we studied the effective-ness of the re-ranking when applied on homogeneous and non homogeneous databases. Two windowing functions, the Rectangular and the Gaussian, were used on the neighbors of each shot to calculate its global score.
 The paper is organized as follows: Our re-ranking method is presented in section 2, section 3 describes the experimental results and section 4 presents our concluding remarks.
In multimedia systems based video retrieval, we need to rank the video shots according to an estimation of their rel-evance to what the user is likely to want to see. This esti-mation can be the prediction score obtained by the trained model, as the likeliness of a shot to contain a target concept. Usually the order of the samples in the ranked lists contains some irrelevant samples, where in this case we can use a re-scoring method in order to minimize the error within these ranked lists.
 The method we proposed here, considers the hypothesis that videos have rather homogeneous contents, and that the pres-ence of a given concept in a video depends a lot on the nature of the video itself, and that the estimated scores are com-puted independently for all video shots in the corpus. The proposed re-ranking method is done by re-scoring the video shots, and this is done in two steps: First, for each shot, we compute global score z , this is calculated through the ini-tial scores of its predefined neighbors within the same video. Then this global score will be used to re-evaluate the initial score of each shot. Let the test collection consists of a set of videos V =( v 1 ,v 2 ,...,v m ), m being the number of videos in the collection. Each video v i composed of a sequence of shots v i =( s i 1 ,s i 2 ,...,s in i ), n i being the number of shots of v i . For each shot s ij , an initial classification score x computed from supervised learning on the development set. Many options  X  including (arithmetic mean, min, max, ge-ometric mean, harmonic mean and root mean square)  X  are possible for the computation of a global score z ij for the shot x ik in video v i , from its neighbor shots. We considered the formula of a generalization mean rule, equation 1, to be the method to calculate the global scores of each shot in the video, since all the above methods can be inherited from this rule, by evaluating different parameters of  X  . where x ik indicates the score of shot k in video i ,and  X  defines the used function, and it has to be tuned by cross-validation. Hence, different values of  X  leads to different functions, such as:(  X  =  X  X  X  :Min;  X  =  X  :Max;  X  =0: Geometric Mean;  X  = 1 : Arithmetic Mean;  X  =  X  1:Har-monic Mean and  X  =2:RootMeanSquare). f  X  ( j, k )works as a window around the current shot j , to its neighbor shots in the video i . In this paper, two kinds of windowing func-tions are considered: the rectangular ( X  X ard X ) and the Gaus-sian ( X  X oft X ). In both cases, the size of the window is defined by a parameter  X  . For the rectangular window, the number of neighbors of each shot in video i is given by 2  X  +1.For the Gaussian window, we have applied  X  =  X  (  X  +1) / 3so that both windowing functions have the same variance for the same value of  X  .This  X  parameters has also to be tuned within the training (or development set).  X  =0givesthe baseline, it correspond to the initial ranking and  X  =  X  uses a global score of the video itself which is calculated from all the shots belonging to it, in other words ( z ij = z i ). After these global scores z ij arecalculated,thescoreofeach shot is updated according to its previous score and its global score obtained from the video (its neighbors) knowledge. Again, many options were possible and we chose a weighted multiplicative fusion: where  X  is a parameter that controls the  X  X trength X  of the re-ranking method. This parameter also has to be tuned by cross-validation within the development collection.
In this section, we present our experiments in which we have evaluated the proposed re-ranking method on semantic indexing task. The experiments were conducted on TRECV-ID 2008 and 2010 databases. Each database consists of two large sets; the development and the test set. Table. 1 shows general information about these two databases. The TREC-VID 2010 development set (2010d) consists of 119685 shots of 3173 videos with average of 37 shots per video, and the test set (2010t) consists of 146788 shots of 8467 videos with average of 17 shots per video, which might tells that videos in this database are homogeneous. The TRECVID 2008 development set (2008d) consists of 43616 shots of 219 videos with average of 199 shots per video, and the test set (2008t) consists of 42461 shots of 219 videos with average of 193 shots per video, videos are not homogeneous.

Collection Concepts Shots/Videos Min/Mean/Max 2010 dev 130 119685/3173 1/37/1381 test 30 146788/8467 1/17/1423 2008 dev 20 43616/219 19/199/1003 test 20 42461/219 14/193/1029
This experiment was conducted on TRECVID 2010, which provided 130 concepts with ground truth labels in a training set. The evaluation was done by calculating the Mean Aver-age Precision (MAP) on only 30 concepts that were chosen by NIST. We have evaluated the re-ranking method on four different initial classification results, which have been sub-mitted to TRECVID 2010, including different fusion strate-gies such as weighted and direct optimized weighted fusion (Fusion MAP and Fusion OPT) , also the combination of these two fusion types with the genetic fusion (Fusion GA -MAP and Fusion GA OPT) . These fusion strategies were applied on score vectors obtained by training different sys-tems on 45 different descriptors including audio and visual descriptors, which have been produced by the partners of the IRIM project of the GDR-ISIS [1]. Each of these fu-sion ways -of the classification results-can be considered as the context fusion method, which we took as the baseline method to our re-ranking algorithm.
The tuning of  X  ,  X  and  X  parameters (Eq. 1 and Eq. 2 in section 2), was conducted using the aforementioned initial classification results, which are calculated on the TRECVID 2010 development set. The aim of this tuning is to get the best values of  X  ,  X  and  X  that gives the best performance of our system.
 Figure 1 shows the results of tuning  X  and  X  ,inwhichwe report the performance of the system in function with  X  . We used the MAP on the 130 concepts as evaluation met-ric. Each plot, in the (top) sub-figure, is related to different value of  X  , and it shows the MAP with different values of  X  (including  X  =0and  X  =inf). Foreachvideo,wehaveset  X  to be the number of all shots related to it, and we have used the initial scores of Fusion MAP for evaluation. From theplots,aswecansee,  X  =1and  X  = 2 are performing better that the others, and the best result can be obtained with  X  = 2 (Root Mean Square) and  X  =0 . 4. Moreover, in Figure 1 (bottom), we show the performance of the system, on the same collection, using the four used initial scores with  X  = 2. As we can see, the highest performance, on each of the initial scores, was achieved when applying the re-ranking method with  X  =0 . 4. Figure 1: Tuning  X  (top) and  X  (bottom) parameters on TRECVID 2010 development set.
 Let X  X  consider now the  X  parameterinEquation1.Asmen-tioned before, this parameter controls the range on which we expect the video to have homogeneous content. The optimal value for this range is likely to depend upon the collection contents. We rerun the previous evaluations with different values of  X  , including the baseline  X  =0and  X  =  X  which means that the global score of each video is assigned to all the shots belonging to it ( z ij = z i ). Figure 2 shows the MAP calculated on the 130 concepts on the Fusion GA OPT run, which we consider as the best run as shown in figure 1 (bot-tom). The evaluations were done using the Rectangular and Gaussian windows with different  X  -equivalent parameters for the re-ranking method. We have applied a sliding window of size 2  X  + 1, as the neighbors of shot j using rectangu-lar function, and  X  =  X  (  X  +1) / 3 using Gaussian window. Thus, the two windowing functions have the same variance for the same value of  X  . Aswecansee,thebestresultwas obtained when  X  =  X  for the two window functions. This is due to the fact that videos in the TRECVID 2010 col-lection are quite short (a few minutes in average), and they have homogeneous contents. Thus, local re-scoring does not perform better than global re-scoring. Figure 2: Tuning  X  -equivalent parameters on TRECVID 2010 development set, using Fu-sion GA OPT run.
We have applied the proposed method on the TRECVID 2010 test set; with the best parameters obtained by the cross-validation,  X  =2,  X  =0 . 4and  X  =  X  with the two windowing functions (Rectangular and Gaussian. We have compared the new results  X  obtained after re-ranking  X  with the results of the initial scoring methods obtained using the best run; the Fusion GA OPT run.
 We report, in table 2, the results (MAP on the 30 concepts) of the evaluation of the re-ranking method on TRECVID 2010 test set. As we can see, our proposed method has significantly improved the performance of the initial scoring methods; on this collection the proposed re-ranking method, with the fully homogeneity  X  =  X  , was able to improve the system performance with about 18% in average. The absolute MAP values are significantly different than in cross-validation (on the development set); this is mostly due to the fact that the set of concepts is different (30 only out of 130). Table 2: Results of the re-ranking method on the test set of TRECVID 2010.
We have conducted the second experiment on TRECVID 2008 High-Level Feature Extraction task (HLF). Consider-ing the Mean Average Precision (MAP) on these 20 con-cepts to be the performance metric. The evaluation of the re-ranking method has been conducted using the simple late fusion of four types of image descriptors taken from IRIM GDR-ISIS partners [1], (including a combination of color his-togram and Gabor transform, texture patterns, quaternionic wavelets and bag of SIFTs). The Multiple-SVM classifiers with RBF kernel was used as classification system, and it was implemented as in [6]. Since, TRECVID 2008 sets are not as homogeneous as TRECVID 2010 sets (see table. 1), we have fixed the optimal parameters  X  =2and  X  =0 . 4, taken from section 3.1.1. The goal was to find the best value of  X  for the re-ranking method, when dealing with non-homogeneous videos . Figure 3: Tuning  X  -equivalent parameter on TREC-VID 2008, using the fusion of four descriptors.
 We have evaluated our method on TRECVID 2008 develop-ment set using the late fusion of the four aforementioned descriptors, with different values of  X  -equivalent parame-ter, within the same conditions as in section 3.1.1. We present the performance of the systems in Figure 3, which shows the MAP (calculated on the 20 concepts) with dif-ferent values of  X  -equivalent in both functions, the rectan-gular and Gaussian. As we can see, the Gaussian perfor-mance better than the rectangular function, and the perfor-mance using the two windowing functions was significantly enhanced when  X  -equivalent is small and the best result was given when  X  = 3. In the Gaussian function when  X  =3  X   X  = 3(3 + 1) / 3=2.
 Furthermore, we have evaluated the re-ranking method with the optimal values  X  =2 , X  =0 . 4and  X  =3,onTRECVID 2008. It was evaluated using the two windowing functions. We report the final results in Table 3, in which we show the performance using deferent values of  X  :  X  = 0 is the baseline,  X  =  X  corresponds to applying the re-ranking on the whole videos, and the optimal  X  -equivalent values(  X / X  ) which de-fines respectively the rectangular and Gaussian functions. As we can see, the re-ranking with the optimal  X  can sig-Table 3: Results of the re-ranking method on the Test set of TRECVID 2008. nificantly enhance the performance of the retrieval system. As expected, this collection is not homogeneous and there is not much enhancement when re-ranking by a global score on the whole video. When applying the re-ranking with (  X  =0 . 4 , X  =0 . 4 , X  = 3), the performance of the system was enhanced in average by about 11-13% on the late fu-sion of the used descriptors with both the Gaussian and the rectangular windows.
Video retrieval can be done by ranking the samples ac-cording to their probability scores that were predicted by classifiers. It is often possible to improve the retrieval per-formance by re-ranking the samples. In this paper, we pro-posed a re-ranking method that improves the performance of semantic video indexing and retrieval, by re-evaluating the scores of the shots using the homogeneity and the nature of the video they belong to.

The experimental results showed that the proposed re-ranking method was able to improve the system performance by about 18% in average on the TRECVID 2010 semantic indexing task, videos collection with homogeneous contents. For TRECVID 2008, in the case of collections of videos with non-homogeneous contents, the system performance was im-proved by about 11-13%.
 This work was partly realized as part of the Quaero Program funded by OSEO, French State agency for innovation. [1] D. Gorisse, F. Precioso, P. Gosselin, L. Granjon, [2] R. Herbrich, T. Graepel, and K. Obermayer. Support [3] W.Jiang,S.-F.Chang,andA.C.Loui.Context-based [4] L. S. Kennedy and S.-F. Chang. A reranking approach [5] J. Liu, W. Lai, X.-S. Hua, Y. Huang, and S. Li. Video [6] B. Safadi and G. Qu  X  enot. Active learning with multiple [7] F. Wang and B. Merialdo. Eurecom at trecvid 2009 [8] Y.-H. Yang and W. H. Hsu. Video search reranking via
