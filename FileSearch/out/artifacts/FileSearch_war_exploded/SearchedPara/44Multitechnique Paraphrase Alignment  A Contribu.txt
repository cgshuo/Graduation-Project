 HOUDA BOUAMOR, AUR  X  ELIEN MAX, and ANNE VILNAT , LIMSI-CNRS &amp; University Going from raw words to their intended meaning in context is one of the major dif-ficulties of natural language processing. Grouping words, or more generally phrases, into sense clusters is an intermediary step towards this goal. While it does not yield a concrete sense representation per se, it can be used to establish some semantic rela-tionship between text units. Such relationships, expressed, for instance, as similarity measures, can then be effectively integrated into useful applications, and can possibly improve the recall of an information retrieval system, facilitate the translation of a text for a given machine translation system, or assist in simplifying a text intended for language-impaired readers.

Language variation , or the fact that messages can be conveyed in a great variety of ways by means of linguistic expressions, is one of the most challenging and certainly fascinating features of language for natural language processing, with wide applica-tions in language analysis and generation. The term paraphrase is now commonly used to refer to textual units of equivalent meaning, and may refer to sub-sentential fragments 1 . For instance, the phrases six months and half a year constitute a good paraphrase pair applicable in many different contexts, as they would denote the same concept 2 . Although one can envisage to manually build high-coverage lists of synonyms, enumerating meaning equivalences at the level of phrases is too daunting a task for humans. Consequently, acquiring this type of knowledge by automatic means has at-tracted a lot of attention [Madnani and Dorr 2010], and significant research efforts have been devoted to this objective [Callison-Burch 2007; Bhagat 2009; Madnani 2010].
Central to acquiring paraphrases is the need of assessing the quality of the candidate paraphrases produced by a given technique. But this of course poses a problem of cir-cular definition, as extracting paraphrases requires the ability of determining whether two text units meet the requirements for being paraphrases. Consequently, most works to date have resorted to human evaluation of paraphrases on the levels of grammat-icality and meaning equivalence . Human evaluation is, however, often criticized as being both costly and nonreproducible, and the situation is even more complicated by the inherent complexity of the task that can produce low inter-judge agreement. Task-based evaluation involving the use of paraphrasing into some application thus seems an acceptable solution, provided the evaluation methodologies for the given task are deemed acceptable. This, in turn, puts the emphasis on observing the impact of paraphrasing on the targeted application and is rarely accompanied by a study of the intrinsic limitations of the paraphrase acquisition technique used.

We believe that significant progress on the acquisition and use of paraphrases will only be possible if the nature of these complex linguistic objects is better comprehended. This relies on the availability of appropriate corpora, human annotations, and evalua-tion metrics. We argue that monolingual parallel corpora are the most suited resources for performing such a study for better comprehending sub-sentential paraphrases, as they do contain by nature a high proportion of such paraphrases, and we show in this article that they do contain a wide variety of paraphrasing phenomena, including some that are currently out of reach of current acquisition techniques. We base our study on the general principles set out by Cohn et al. [2008]: monolingual corpora, consist-ing of sentential paraphrases, are manually annotated at the word and phrase level and are subsequently used to measure the ability of some automatic techniques to find those gold standard paraphrases. Given such a setting, we propose to explore the problem of paraphrase acquisition along several lines: what type of knowledge and / or processes are needed, what is their complementarity? What is the impact of the lan-guage and comparability of the pairs of sentences used? What are the characteristics of the hard-to-align paraphrases? The outcome of our study is a descriptive and quantita-tive account of the characteristics of those paraphrases that currently defy automatic acquisition. All our experiments are conducted on two languages, English and French, using the same algorithms and resources: this is the first time, to our knowledge, that results are reported for such a study on paraphrase acquisition.

This article will be organized as follows. We will first review the various types of corpora that are available for research on paraphrasing (Section 2). We will then de-scribe in some details the task of sub-sentential paraphrase acquisition on which we focus in this work (Section 3). After presenting previous works addressing this task, we will introduce our experimental settings (Section 4.1) and describe the types of techniques that we have studied (Section 4.2). These techniques have been selected as representative of a particular kind of knowledge or process, as well as for their potential complementarity with the other techniques. They are respectively based on:  X  X tatistical word alignment models;  X  X ranslational equivalence;  X  X andcoded rules of term variation;  X  X yntactic similarity;  X  X nd some optimal transformation of word sequences.
 In order to attempt to generalize our results, we have conducted all our experiments in English and French, using comparable resources when relevant. We will report evalu-ation results for these individual techniques (Section 4.3), and describe and evaluate methods for combining their results (Section 4.4) and use automatic classification for validating candidate paraphrases (Section 4.5). A significant part of our study will be devoted to presenting further analyses by first considering each technique X  X  perfor-mance relative to some notion of sentential paraphrase comparability in our initial parallel monolingual corpus scenario (Section 5.1). We will further expand our study to consider the achievable performance of our systems when using as input pairs of sentences at varying degrees of semantic equivalence, corresponding to corpus types of various origins (Section 5.2):  X  X entence pairs resulting from multiple independent translation of a text;  X  X airs resulting from multiple independent translation of a speech;  X  X airs resulting from multiple independent descriptions of a visual scene;  X  X nd pairs resulting from multiple independent descriptions of some news event. Then, we will take a closer look at paraphrases that no technique was able to find and provide a detailed typology in which each class is quantified for the two languages (Section 5.3), and a pilot study on manual acquisition of such paraphrases will be introduced (Section 5.4). We will finally conclude by summarizing the main findings of our study and describing how it might be useful for further research on paraphrasing (Section 6). Contrarily to text translation, which produces vast amounts of translation examples, human activities do not seem to produce explicitly large quantities of naturally occur-ring textual paraphrasing from which models could be learned. Automatic techniques therefore have mostly relied on the indirect observation of partially equivalent text units, which raises a number of issues, including the fact that the extracted pairs of textual variants may not always be contextually related. Machines can nonetheless process massive quantities of text, up to Web scale [Pasc  X a and Dienes 2005; Bhagat and Ravichandran 2008], which is of course out of reach to fine-grained human studies and computationally expensive automatic techniques.

Numerous paraphrase acquisition approaches have thus consisted in first restrict-ing the corpora from which acquisition is performed to pairs of related sentences, borrowing heavily from typical definitions for comparable corpora where texts are often paired based on common subject and time frame using information retrieval techniques as a first step. Barzilay and Elhadad [2003] learn rules that exploit topic structure and local alignment to extract sentence pairs from news stories about the same events from different press agencies, yielding significant proportions of useful data. Dolan et al. [2004] combine string edit distance selection of sentence pairs from the same type of corpus with a heuristic strategy, and report a useful account of the types of paraphrases found in their resource: elaborations, synonymy, phrasal paraphrases, spelling variants, anaphora, and reorderings.

Multiple human translations of the same texts can be used to produce independent sentential paraphrases, and numerous works have used such corpora for sub-sentential paraphrase acquisition (e.g., use of multiply-translated novels [Barzilay and McKeown 2001]). This approach can be used to obtain paraphrases with varying degree of com-parability. In previous work [Bouamor et al. 2010], we measured the impact of the original language of a human translation on the comparability of the paraphrases obtained. Volunteer contributors translated from one European language into French (they had at least a near-native command of the source language), and some lexical overlap was measured on groups consisting of at least 20 paraphrase pairs. Proportions of lexical overlaps are shown in Table I. Numbers shown as indices represent the num-ber of sentential paraphrases (common translations) obtained from two languages. For instance, the 172 paraphrases obtained from English have an average of 90% common tokens. The main finding is that paraphrases obtained from the same language can be very similar (average up to 90% words in common), whereas they contain much more lexical variety when they are obtained from different languages (between 36% and 42% different tokens), resulting in possibly more useful corpora for recall-oriented paraphrase acquisition.

There are in fact other human activities, some even more common and easier to take part in, which produce text pairs that are in some instance very closely related to sen-tential paraphrasing. Bernhard and Gurevych [2008] exploit similar questions found on social Q&amp;A Web sites that have been manually aligned because they can share their answers. Mining revision histories of collaborative resources such as Wikipedia [Nelken and Yamangil 2008; Max and Wisniewski 2010] is likely to extract useful rewritings, as confirmed by our detailed annotation study on French Wikipedia revision histo-ries [Dutrey et al. 2011]: we show that a significant amount of such edits can be consid-ered as useful paraphrases, even though some instances clearly defy current automatic paraphrase identification techniques and are sometimes of the less conservative type commonly referred to as textual entailment [Zanzotto and Pennacchiotti 2010], where textual entailment is not necessarily bidirectional. It is also possible to acquire directly target paraphrases from monolingual contributors, for instance to provide machine translation systems with alternatives for difficult-to-translate phrases [Resnik et al. 2010].

Lastly, the field of automatic paraphrase generation [Madnani and Dorr 2010] has produced various techniques that have been used for generating sentential paraphrases that can be used for learning statistical machine translation systems [Nakov 2008] or optimizing their parameters [Madnani et al. 2008], and various potential uses including uses by humans [Quirk et al. 2004; Zhao et al. 2008a, 2010; Max 2009]. In addition to the fact that the current performance of these techniques is limited, it should be noted that an issue of circular dependency arises, as these techniques rely on the availability of paraphrase resources; most of these techniques are, however, able to produce paraphrases that did not belong to their original resources.
 Examples of paraphrases obtained in some representative studies are illustrated in Figure 1.
 In order to use paraphrases in new contexts, paraphrases at a finer level than that of sentences must be extracted, possibly from the resources described in the previous section. Sub-sentential paraphrases, ranging from pairs of synonyms to pairs of phrases with equivalent meaning in context, can be exploited in a more generic way by either paraphrase identification or paraphrase generation techniques. In this section, we describe some influential paraphrase acquisition techniques, but we direct interested readers to recent surveys for a broader panorama of these techniques [Madnani and Dorr 2010; Androutsopoulos and Malakasiotis 2010].

The hypothesis that if two words or, by extension, two phrases, occur in similar contexts then they may be interchangeable has been extensively tested. This distri-butional hypothesis , attributed to Zellig Harris, was, for example, applied to syntactic dependency paths in the work of Lin and Pantel [2001]. Their results take the form of equivalence patterns with two arguments such as { X asks for Y , X requests Y , X X  X  request for Y , XwantsY , Y is requested by X , ... } .

Using comparable corpora, where the same information probably exists under various linguistic forms, increases the likelihood of finding very close contexts for sub-sentential units. Barzilay and Lee [2003] proposed a multisequence alignment algorithm that takes structurally similar sentences and builds a compact lattice repre-sentation that encodes local variations. The work by Bhagat and Ravichandran [2008] describes an application of a similar technique on a very large scale. Wang and Callison-Burch [2011] recently introduced a minimally supervised paraphrase fragment acqui-sition technique working on comparable corpora.

The hypothesis that two words or phrases are interchangeable if they share a com-mon translation into one or more other languages has also been extensively studied in works on sub-sentential paraphrase acquisition. Bannard and Callison-Burch [2005] described a pivoting approach that can exploit bilingual parallel corpora in several lan-guages. The same technique has been applied to the acquisition of local paraphrasing patterns in Zhao et al. [2008b]. The work of Callison-Burch [2008] has shown how the monolingual context of a sentence to paraphrase can be used to improve the quality of the acquired paraphrases.

Another approach consists in modeling local paraphrasing identification rules. For instance, the work of Jacquemin [1999] on the identification of term variants, which ex-ploits rewriting morphosyntactic rules and descriptions of morphological and semantic lexical families, can be extended to extract the various forms corresponding to input patterns from large monolingual corpora.

All the previous approaches can produce or recognize inappropriate pairs that do not correspond to paraphrastic variants in context. This is largely due to the fact that the corpora that they use never explicitly encode paraphrasing relationships between text units. For instance, a paraphrase obtained by pivoting through another language may not have been observed in the context of the original phrase: the phrase it is too early to could be automatically extracted for the original phrase this is not the time to by pivoting through the French phrase il est trop t X  ot pour , an acceptable translation for some occurrences of the two English phrases, but it would clearly not be appropriate with a context such as this is not the time to be negative . Likewise, extracting a phrase that appears in contexts very similar to that of an original phrase is limited by the effectiveness of the modeling of context used: for instance, several occurrences of Spain defeated France and Spain lost to France should not be used as evidence for establishing a paraphrasing relationship between the two verbs.

In contrast, whenever parallel monolingual corpora aligned at the sentence level are available, the task of sub-sentential paraphrase acquisition can be cast as a problem of word alignment between two aligned sentences [Cohn et al. 2008]. Previous works have exploited multiple translations, which as we said in the previous section occur very infrequently naturally. But such translations are sometimes produced, albeit in small quantities, for example, as multiple reference translations for evaluating ma-chine translation outputs automatically. Barzilay and McKeown [2001] applied the distributional hypothesis on such parallel sentences, and Pang et al. [2003] proposed an algorithm to align sentences by recursive fusion of their common syntactic constituents. Marsi and Krahmer [2010] proposed an approach that matches syntactic trees of sen-tences from comparable corpora and labels resulting alignments with semantic simi-larity relations. More generally, most alignment techniques may be adapted to the task of word and / or phrase alignment for parallel monolingual corpora, based on statisti-cal [Och and Ney 2004] or differential associative methods [Lardilleux et al. 2011].
An important observation we can make is that most of the proposed approaches to the task of paraphrase acquisition have a tight relationship to the type of text corpora that they use. In fact, in most instances, the type of corpora used directly induces the technique, and no higher model of what paraphrases are seems to have emerged. In this study, we aim to identify the characteristics of paraphrases that are difficult to acquire automatically for state-of-the-art techniques, and answer intermediary questions on the type of knowledge or processes that are needed depending on the characteristics of the resources used for acquiring paraphrases. To this end, it is necessary to make use of a clear definition of what constitute correct paraphrase pairs in some corpus, as well as of the set of all such paraphrases present in this corpus. We first describe the data and evaluation methodology that we have chosen for our ex-periments (Section 4.1) and the different techniques that will be studied (Section 4.2), and we then report the performance of each individual technique (Section 4.3). We then study the potential complementary of each pair of techniques, and two ways of combining their results are described and evaluated (Section 4.4). Finally, paraphrase validation is performed by using automatic classification using various models that introduce new types of information (Section 4.5). This last experiment will allow us to report significant improvements over all individual techniques and combinations on both English and French. For our main experiments, we chose to use parallel monolingual corpora, the most direct resource type for acquiring sub-sentential paraphrase pairs as they are by nature dense in paraphrases. This allows us to define acceptable references for the task and resort to the most consensual evaluation technique for paraphrase acquisition to date. Using such corpora, we expect to be able to extract precise paraphrases, which will be natural candidates for further validation, a problem we will address in Section 4.5.
We used the main aspects of the methodology described by Cohn et al. [2008] for constructing evaluation corpora and assessing the performance of techniques on the task of paraphrase acquisition. Pairs of sentences (here, sentential paraphrases from monolingual parallel corpora) are hand-aligned to define a set of reference atomic paraphrase pairs at the level of words or phrases. Reference composite paraphrase pairs (denoted as R ) are then obtained by joining adjacent atomic paraphrase pairs (null alignments are not authorized between paraphrases) up to a given length, imposed on both paraphrases from the pair. This set is composed of sure paraphrase pairs (this subset will be denoted R sure )and possible paraphrase pairs 3 .
Figure 2 illustrates a reference alignment obtained on a pair of English sentential paraphrases and the list of sub-sentential paraphrase pairs up to a length of 6 tokens that can be extracted from it, against which acquisition techniques will be evaluated. Note that we do not consider pairs of identical units during evaluation, so after com-position we filter them out from the list of reference paraphrase pairs 4 .
The example in Figure 2 shows different cases that point to the inherent complexity of this task of word alignment, even for human annotators. For instance, we observe a direct impact of typographical choices or absence of appropriate tokenization in the phrase US $ 9 , which imposes that the phrase US $ 9 billion be aligned as a block to the phrase 9 billion US dollars . Also, the fact that the preposition of has been left unaligned prevents the extraction of composite possible paraphrases such as ( foreign trade volume  X  volume of import and export ). This latter case may be explained by the fact that annotators were asked to identify the smallest possible atomic paraphrases. Inter-annotator agreement values are reported in Table II and will be discussed shortly.
Evaluated techniques have to output word alignments for each pair of sentences in a test set, from which a set of hypothesis composite paraphrase pairs (denoted as H ) is extracted. The usual measures of precision, recall, and F-measure (considering precision and recall as equally important) can then be defined in the following way.
Note that the way that the sets R and R sure of reference paraphrase pairs are defined ensures that paraphrase pair hypotheses that include possible reference paraphrases will not penalize precision while not increasing recall. Furthermore, as said previously, in all our experiments we will only consider paraphrase pairs that are not identity pairs (e.g., ( nation  X  nation )).
 We conducted experiments using two monolingual parallel corpora in English and French. In each case, a held-out development corpus of 150 sentence pairs was used for development and tuning, and all techniques were evaluated on corpora of 500 sentence pairs. For English, we used the MTC corpus described in Cohn et al. [2008], consisting of multiply-translated Chinese sentences into English. For French, we used the CESTA corpus of news articles 5 obtained by translating into French from English. Manual inspection of the two corpora reveals that the French corpus tends to contain more literal translations, possibly due to the original language of the sentences, which is closer to the target language than Chinese is to English.

Our annotation was performed by 2 annotators per language, and inter-annotator agreement values were computed on sets of 50 sentences independently annotated by the 2 annotators. The annotators used the Y AWAT [Germann 2008] online annota-tion tool 6 . The main guidelines that they had to follow were that sure and possible paraphrases had to be distinguished, smaller alignments were to be preferred but any-to-any alignments may be used, and sentences should be aligned as much as possible.
Statistics for our main corpora are given on Table II. The two corpora are of a comparable size, with French sentences slightly longer (24.0 tokens per sentence against 21.0 for English). Interestingly, inter-annotator agreement values are very close for sure paraphrases in the two languages (66.1 for English and 64.6 for French), which are in an acceptable range. However, inter-annotator agreement on possible paraphrases are much lower (20.4 for English and 16.6 for French). This was expected as possible paraphrases are by nature less certain from the point of view of each anno-tator. This is not considered an issue here, because, as we explained before, hypothesis paraphrases proposed by a system will simply not penalize its precision, while not increasing its recall. Lastly, we observe that our French corpus contains significantly more sure paraphrases (29.2% against 18.6% for English), but significantly fewer possible paraphrases (6.2% against 12.3% for English). This may be explained by two main facts: first, as said previously, the French corpus is obtained by translating from a much more related language (from English) than the English corpus (from Chinese). Consequently, we would expect the French corpus to contain more semantically equiv-alent sentences. It may also be the case that our annotators for English and French had different interpretations of the distinction between the two types of paraphrases.
All performance values reported in the remainder of this article will be obtained using ten-fold cross-validation and averaging the results on each individual test set so as to report values that are more stable across all available annotated data. Statistical significance as given by the t -test at the 5% level will be indicated between pairs of individual systems when relevant. As discussed in Section 3, the acquisition of sub-sentential paraphrases is a challenging task that has previously attracted a lot of works. In this work, we consider the scenario where sentence pairs from monolingual parallel corpora are available and words and phrases from one sentence can be aligned to words and phrases from the other sen-tence to extract candidate paraphrase pairs. We now describe several techniques that we used to perform the task of sub-sentential unit alignment. We have selected and implemented five techniques which we believe are representative of the type of knowl-edge that these techniques use, and have reused existing tools, initially developed for other tasks, when possible.

The choice of our techniques was mainly motivated by the type of knowledge that these techniques use and their potential complementarity. In particular, one technique ( Giza ) learns statistical word alignment models, one exploits parallel bilingual cor-pora ( Pivot ), one exploits existing manual resources ( Fastr ), one uses syntactic parses ( Synt ), and one uses edit distance ( TER p ). Our subsequent experiments on validation (Section 4.5) will later allow us to integrate other sources of knowledge (e.g., large mono-lingual corpora and sentence similarity measures). Another criterion for selecting indi-vidual techniques included our ability to use existing tools or to reimplement them, and importantly to be able to use comparable settings for experiments in the two languages. Evaluation figures for these techniques will be provided in Section 4.3, while Section 4.4 will be concerned with combining the results of these techniques and Section 4.5 with validating their combined outputs. 4.2.1. Exploitation of Statistical Learning of Word Alignments (Giza). The G IZA tool [Och and Ney 2004] computes statistical word alignment models of increasing complexity from parallel corpora. While originally developed in the bilingual context of statistical machine translation, nothing prevents building such models on monolingual corpora. However, in order to build reliable models, it is necessary to use enough training material including minimal redundancy of words. To this end, we used monolingual corpora made up of multiply-translated sentences, allowing us to provide G IZA with all possible sentence pairs to improve the quality of its word alignments (note that we used symmetrized alignments from the alignments in both directions). This constitutes a significant advantage for this technique that techniques working on each sentence pair independently do not have. 4.2.2. Exploitation of Translational Equivalence (Pivot). Translational equivalence can be exploited to determine that two phrases may be paraphrases. Bannard and Callison-Burch [2005] defined a paraphrasing probability in the following way.

We used the Europarl corpus 7 of parliamentary debates in English and French, consisting of approximately 1.7 million parallel sentences: this allowed us to use the same resource to build paraphrases for English, using French as the pivot language, and for French, using English as the pivot language. The G IZA tool was used for word alignment and the M OSES Statistical Machine Translation toolkit [Koehn et al. 2007] was used to compute phrase translation probabilities from these word alignments. For each phrase of a sentence pair, we built its set of paraphrases, and extracted its paraphrase from the other sentence with highest probability. We repeated this process in both directions, and finally kept for each phrase its paraphrase pair from any direction with highest probability. 4.2.3. Exploitation of Linguistic Knowledge on Term Variation (Fastr). The F ASTR tool [Jacquemin 1999] was designed to spot term variants in large corpora. Variants are described through metarules expressing how the morphosyntactic structure of a term variant can be derived from a given term by means of regular expressions on word morphosyntactic categories. Paradigmatic variation can also be expressed through constraints between words, imposing that they be of the same morphological or se-mantic family. Both constraints rely on preexisting repertoires available for English and French. To compute candidate paraphrase pairs using F ASTR , we first consider all phrases from the first sentence and search for variants in the other sentence, then do the reverse process and finally take the intersection of the two sets. 4.2.4. Exploitation of Syntactic Similarity at the Level of Constituents (Synt). The algorithm introduced by Pang et al. [2003] takes two sentences as input and merges them by top-down syntactic fusion guided by compatible syntactic substructure. A lexical blocking mechanism prevents sentence constituents from fusing when there is evidence of the presence of a word in another constituent of one of the sentences. We use the Berkeley Probabilistic parser [Klein and Manning 2003] to obtain syntactic trees for English and its adapted version for French [Candito et al. 2010]. Because this process is highly sensitive to syntactic parse errors, we used in our implementation k -best parses and retained the most compact fusion from any pair of candidate parses. 4.2.5. Exploitation of Minimal Transform Edit of Word Sequences (TER p ). T ER p (Translation Edit Rate plus) [Snover et al. 2010] is a score designed for the evaluation of machine translation output. Its typical use takes a system hypothesis to compute an optimal set of word edits that can transform it into some existing reference translation. Edit types include exact word matching, word insertion and deletion, block movement of contiguous words (computed as an approximation), as well as optionally variants sub-stitution through stemming, synonym, or paraphrase matching. 8 Each edit type is parameterized by at least one weight which can be optimized using, for example, hill climbing. T ER p is therefore a tunable metric. In our experiments, T ER p systems were tuned towards F-measure. 9 Results for the 5 individual techniques are given on Table III. It is first apparent that all techniques but F ASTR fared better in terms of F-measure on the French corpus than on the English corpus. This can certainly be explained by the fact that the former results from more literal translations (from English to French, compared with from Chinese to English), which should be consequently easier to word-align. On the other hand, F ASTR , which obtained very close results on both languages, is not expected to be sensitive to sentence comparability.

The two linguistically-aware techniques, F ASTR and S YNT , have a good precision on both corpora 10 , but fail to achieve an acceptable recall on their own. This is not sur-prising: F ASTR metarules are focused on term variant extraction, and S YNT requires two syntactic trees to be highly comparable to extract sub-sentential paraphrases. When these constrained conditions are met, these two techniques appear to perform quite well in terms of precision.

G IZA and T ER p perform roughly in the same range, and obtain the highest F-measure values, the former achieving a higher precision and the latter a higher recall. G IZA performs better in F-measure, with a 2.14 advantage on English and a 2.65 advantage on French, both results statistically significant.

P IVOT achieves the highest precision of all individual techniques, but obtains a comparatively much lower recall, although much higher than those of F ASTR and S YNT . This may be due to the use of a bilingual corpus from the domain of parliamentary debates to extract paraphrases when our test sets are from the news domain: we may be observing differences inherent to the domain, and possibly facing the issue of numerous  X  X ut-of-vocabulary X  phrases, in particular for named entities which frequently occur in the news domain.

Importantly, we can note that we obtain at best a recall of 66.47 on English and of 61.45 on French, both with T ER p , which underlines the complexity of the task consider-ing that we are using parallel monolingual corpora, and provides further justification for initially focusing on such corpora, albeit scarce, for conducting fine-grained studies on sub-sentential paraphrasing.

A number of facts must be considered when analyzing the obtained results. First, our metrics do not consider identity paraphrases (e.g., at the same time  X  at the same time ). Second, our results are influenced by the fact that the metrics count as false small variations of gold standard paraphrases (e.g., missing function word): the acceptability or not of such candidates could be either evaluated in a scenario where such acceptable variants would be taken into account, and / or could be considered in the context of some actual use of the acquired paraphrases in some application. Finally, we can derive from inter-judge agreement values from Table II that gold standard alignment of sub-sentential paraphrases is a difficult task by itself.

Designing new individual techniques was not one of the objectives of our study, but we have proposed original reuse of existing techniques developed with different objectives (bilingual parallel corpora word alignment (G IZA ), term variant recognition (F
ASTR ), machine translation evaluation (T ER p )). Before considering combining the outputs of the techniques presented in the previous section, it is instructive to look at some notion of  X  X omplementarity X  between tech-niques, in terms of how many correct paraphrases a technique would add to a combined set. The following formula was used to account for the complementarity between the set of candidates from some technique t i , and the set for some technique t j .
Results on the test set for the two languages are given in Table IV. A number of pairs of techniques have strong complementarity values, the strongest one being for G IZA and T ER p for both languages. According to these figures, P IVOT identifies paraphrases which are slightly more similar to those of T ER p than G IZA . Interestingly, F ASTR and S
YNT exhibit a strong complementarity relatively to their respective recall. Considering the set of all other techniques, T ER p provides the more new paraphrases in English and both T ER p and G IZA in French.

We first consider a naive combination obtained by taking the union of all techniques, as illustrated on the top part of Figure 3. Results are given in the first column of Table V. The first result is quite encouraging: in both languages, around 8 paraphrases from the gold standard out of 10 are found by at least one of our techniques, which, given our previous discussion, constitutes a good result and provides a clear justification for combining different techniques for improving performance on this task. Precision is mechanically lowered to account for between 3.5 (in English) and 4 (in French) over 10 candidates for both languages. F-measure values do not improve over G IZA and T ER p , the two best-performing techniques, indicating that the union of all techniques is only interesting for recall-oriented acquisition. In Section 4.5, we will show how the results of the union can be validated using machine learning to improve these figures.
The second strategy that we have tried, illustrated on the bottom part of Figure 3, is to adapt the search of the edit distance technique, T ER p , by informing it with candidate paraphrase pairs extracted by the other techniques. T ER p can exploit a table of para-phrase pairs, and defines the cost of a phrase substitution as  X  X  function of the probabil-ity of the paraphrase and the number of edits needed to align the two phrases without the use of phrase substitutions X  [Snover et al. 2010]. Intuitively, the more parallel two sentential paraphrases are, the more atomic paraphrase pairs will be reliably found, and the easier it will be for T ER p to correctly identify the remaining paraphrases. But in the general case, and considering less apparently parallel sentence pairs, its work can be facilitated by the incorporation of candidate paraphrase pairs in its paraphrase table.

While this metric was not designed explicitly for the acquisition of word alignments, it produces as a by-product of its approximate search a list of alignments involving either individual words or phrases, fitting with the previous definition of atomic para-phrase pairs. When applying it on a MT system hypothesis and a reference translation, it computes how much  X  X ffort X  would be needed to obtain the reference from the hy-pothesis, possibly independently of the appropriateness of the alignments produced. However, if we consider instead a pair of sentential paraphrases, it can be used to reveal what sub-sentential units can be aligned. Of course, this relies on information that will often go beyond simple exact word matching.

We have therefore tested 4 hybrid configurations by providing T ER p with the output of the other individual techniques (see the second group of columns in Table V). Except for two exceptions, any hybridation involving one technique improves the F-measure (results statistically significant) over both T ER p and the technique used in both languages. The two exceptions are the use of G IZA and S YNT on French, which could be attributed to the inability of our optimization technique to escape from locally optimum points.

The addition of P IVOT yields the highest result in F-measure (difference with any other combination statistically significant), corresponding to an improvement of + 8.76 and + 21.48 over, respectively, T ER p and P IVOT alone in English, and + 3.61 and + 15.21 in French 11 . Interestingly, we observe that, contrarily to individual systems, our T ER p -based hybrid systems perform better in English than in French. Differences are, how-ever, mostly explained by recall, which was significantly stronger for T ER p alone in English than in French (respectively, 66.47 and 61.45). Lastly, adding all individual systems ( + ALL ) fails to improve over the best results so far, which may be explained by the amount of combined noise. In the previous section, we showed that T ER p could improve its performance by exploit-ing the results of another technique. However, we concluded that the amount of noise when considering a larger set of paraphrases was too high to allow to benefit from the union of the results of all other techniques. A natural improvement would therefore consist in validating candidate paraphrases from the union of all systems by using several models relevant to our task. If the fact that combining the outputs from sev-eral systems can improve performance is an established fact in many domains (e.g., in statistical machine translation [Matusov et al. 2009] or information retrieval [Shaw et al. 1994]), this has not been shown so far for paraphrase acquisition to the best of our knowledge. We cast our problem as one of biclass classification ( X  X araphrase X  versus  X  X ot paraphrase X ) for candidate paraphrases in the union of the results of all techniques 12 . We have used a maximum entropy classifier 13 with the following features. (1) sources . These are features indicating which technique or combination of techniques (2) part-of-speech sequences . These are features indicating the sequence of part of (3) context similarity . These are features indicating how similar the contexts of occur-(4) character-based distance . This is the edit distance between the two phrases of a (5) stem similarity . This is a feature indicating whether the stemmed phrases of a (6) token set identity . This is a feature indicating whether the unordered set of tokens (7) phrase length ratio . We use the phrase length ratio. (8) sentence pair similarity . These are features indicating similarity between sentences (9) sentential context . These are features indicating relative position of phrases in their
Positive examples were taken from the candidate paraphrase pairs from any of our 5 techniques which belong to the gold standard, and we used a corresponding number of negative examples from candidate pairs not in the gold standard. The last column of Table V presents the results of this validation experiment.

We obtain our best results for this study using the output of our validation classi-fier over the set of all candidate paraphrase pairs, differences with any other system being statistically significant. On English, the improvement in F-measure is of, re-spectively, + 12.86 over the best individual system (G IZA ), + 6.24 over the best system so far (T ER p +P IVOT ), and + 16.31 over the naive union. On French, improvements are respectively of + 11.7 (over G IZA ), + 10.74 (over T ER p +P IVOT ), and + 13.53 (over the naive union). While these results, which are very comparable for the two languages studied, are already very satisfying given the complexity of our task, further inspection of false positives and negatives may help us to develop additional models that will help us obtain a better classification performance.

Each individual technique was selected because of its characteristics and its potential complementarity relative to the set of the other techniques. It is therefore instructive to consider the results that are obtained when each individual technique does not par-ticipate in the validation experiment. Thus, if one particular technique employs some specific knowledge source (e.g., bilingual parallel corpora for P IVOT ), removing it pro-vides some insight into its contribution relative to the set of the other techniques used. Results are given in Table VI. We observe three groups of results: first, removing F
ASTR or S YNT from the set of techniques does not produce a drop of result that is statis-tically significant. The removal of any other techniques produces a drop in F-measure which is statistically significant. Among these techniques, G IZA and T ER p produce no statistically significant results, while the contribution of P IVOT is significantly better than all other techniques. This result emphasizes the positive contribution of the use of bilingual parallel corpora for the acquisition of sub-sentential paraphrases from mono-lingual corpora. Interestingly, P IVOT was not the technique with the highest comple-mentarity relative to the other techniques (see Table IV); the observed result certainly lies in its very good precision, where F ASTR and S YNT , also with high precision values, probably fail to play a significant role due to their low recall values (see Table III). In the previous sections, we have described experiments in sub-sentential paraphrase acquisition from monolingual parallel corpora. We now turn to a set of important complementary questions to further refine our previous conclusions. First, we have provided so far evaluation results at the level of our complete test set; we will now look at results obtained at various levels of sentence comparability (Section 5.1). We will then look further into the issue of sentence comparability by considering other corpus types (Section 5.2), using sentence pairs of various origins, including a corpus type much more readily available than the scarce monolingual parallel corpora considered so far. Then, we will take a closer look at those paraphrases that our best-performing systems were unable to extract, and organize them in a bilingual typology where each type will be quantified (Section 5.3), providing useful information for subsequent studies. Finally, we will present a pilot study aiming at demonstrating the feasibility of manual acquisition for the previously identified paraphrases that are difficult to acquire (Section 5.4). The sentential paraphrase pairs of the corpora that we have used in our study represent problems of different difficulties. Indeed, some sentences may be very similar, differing only by a few paraphrase pairs, while others may correspond to different syntactic structures and / or be lexically very different. Figure 4 gives an indication of how well each individual technique performs depending on the difficulty of the task, which we estimate here as the value (1  X  TER(sent 1 , sent 2 )). Thus, low values correspond to sentences which are costly to transform into the other from their paraphrase pair using T ER .

Not surprisingly, T ER p and G IZA 16 ,andS YNT to a lesser extent, perform better on  X  X ore parallel X  sentential paraphrase pairs. Conversely, F ASTR and P IVOT do not seem to be much affected by the degree of parallelism between sentences, and manage to extract paraphrases at any level of difficulty. While the quality of the extracted paraphrases may not be very dependent on the similarity of two sentential paraphrases, the more they differ the less likely it becomes to find paraphrases from the gold standard as the paraphrases may result from deeper syntactic rewritings. Lastly, we observe that our validation system follows the behavior of T ER p and G IZA : the more similar two sentential paraphrases, the better the performance on sub-sentential acquisition becomes, a result hardly surprising. In the previous section, we have shown that the performance of sub-sentential para-phrase acquisition can vary greatly with the comparability of sentence pairs. As we have acknowledged from the outset of this article, the monolingual parallel corpora that we have used are very scarce, and are thus not necessarily representative of all corpus types that may be used for actual, large-scale paraphrase acquisition. It is therefore informative to look at other types of corpora of sentence pairs from which to acquire paraphrases. In our study, we have considered the 3 following additional corpus types 17 :  X  X  PEECH : text span pairs resulting from multiple translations of the same speech;  X  X  CENE : sentence pairs resulting from multiple descriptions of a video scene;  X  X  VENT : sentence pairs resulting from multiple short descriptions of the same or a related news event. The monolingual parallel corpus type that we have used in Section 4, which results from multiple translations of the same text, will henceforth be referred to as  X  X  EXT  X . We now describe how we built each of our additional corpora.
 S PEECH . For English, we used two freely available subtitle files 18 of the French movies Le Fabuleux Destin d X  X m  X  elie Poulain and Les Choristes , and for French we used two subtitle files from the Desperate Housewives TV series. We first aligned each parallel corpus using the algorithm described in Tiedemann [2007], based on time frames and developed for bilingual subtitles, we then filtered out sentence pairs below a minimal edit distance threshold, and manually removed obvious errors made by the algorithm.
S CENE . We used the multiple video description corpus [Chen and Dolan 2011] ob-tained from multiple descriptions of short videos. We selected sentence pairs from clusters by minimal edit distance above a threshold. An important fact is that for English we were able to use what is described as  X  X erified X  descriptions (i.e., whose contributors had been manually selected). There were, however, far fewer descriptions available for French, and none had the  X  X erified X  status. We decided to use this corpus nonetheless, but with the knowledge that this source for French is of a significantly lower quality. 19
E VENT . We used titles of news article clusters from the Google News 20 news aggrega-tion service. We further refined the clustering algorithm by filtering out article pairs whose publication dates differed from more than one day. The selection procedure ensures maximal cluster coverage and selects more similar pairs first.

Examples of each corpus types are given in Figure 5 21 . The same annotation proce-dure was applied by the same pair of annotators per language, and agreement values were computed. Detailed statistics on all corpus types are given in Table VII.
We briefly sum up the main characteristics of the new corpora that are relevant for our study. All additional corpus types contain shorter sentences than those of T EXT , in particular S CENE and E VENT . Inter-annotator agreement values on sure paraphrases are comparable or higher than for the longer sentences of T EXT for all corpora, except for the low-quality case of S CENE in French. S PEECH and E VENT have a comparable density of sure paraphrases, which is very close to that of T EXT for English, but significantly lower for French.

We conducted anew our validation experiments from Section 4.5 for all our corpus types 22 , and present our results in Table VIII, in which results for T EXT have been copied to facilitate comparison.

With regards to individual techniques, we mostly observe the same ordering on per-formance as for T EXT , with the notable exception that G IZA now performs significantly worse than T ER p on E VENT , a reflection of the fact that it was not provided with enough training data for this corpus type, involving many rare words and named entities. Not surprisingly, all techniques achieve their best performance on the more parallel T EXT corpus type.
With regards to the best-performing system, we see that our validation system has a significantly better performance than any other technique on all corpus types for both languages. In terms of relative performance, we observe 3 groups: T EXT obtains significantly higher results, then S PEECH and S CENE obtain comparable results 23 , and finally E VENT appears as the most challenging corpus type. This last point is important, as E VENT is arguably the corpus which is available in the largest quantities, increasing every day. Nonetheless, the achieved performance of 46.02 in English and 46.80 in French on F-measure means that we already obtain acceptable results at this level of difficulty. From the outset of our study, one of our main objectives was to characterize the types of paraphrases that are particularly difficult to acquire automatically considering a set of different techniques. We decided to focus on the configuration where we take the union of all techniques on T EXT (see Table V) and take the subset of the gold standard that is not in the intersection with the union set. Therefore, we obtain the subset of reference paraphrase pairs (both sure and possible ) that no individual technique was able to capture.

We have determined a set of broad classes by iterative annotation with two annota-tors. Table IX describes this classification by means of representative examples. Classes have been ordered by decreasing frequency in our English corpus.

It can first be noted that there is a clear correlation between English and French, with the notable exception of typographical variations which account for many more instances in English than in French 24 . There seems to be more synonymy in our French corpus but more pragmatic variation in the English corpus.

Lexical and phrasal synonyms are by far the most represented class, with almost half of the instances in French. Some pairs involving long phrases, such as ( at a rapid rate  X  fast ), will be difficult to identify for all our techniques, except S YNT when they appear in compatible syntactic positions. The statistical approach may find particularly difficult aligning rare elements in a paraphrase pair, as in ( ignore  X  be blind to ). Some examples could have been captured given sufficient a priori lexical knowledge, such as ( stronger  X  firmer ).

The class of pragmatic variation, accounting for more than 20% of examples in both languages, corresponds to phrases that are only paraphrases in the very specific context of their sentence pair and are consequently very difficult to capture without deeper analysis (e.g., to south korea  X  home ). It is therefore not surprising that those phenomena, resulting from different choices from their respective human translator, are difficult to acquire automatically, although techniques such as T ER p or S YNT may be able to capture them under certain circumstances. Nevertheless, these paraphrases are much more difficult to reuse frequently into concrete applications, and consequently seem at first more interesting for the study of contextual paraphrasing phenomena rather than for large-scale acquisition.

It should also be noted that some cases correspond to difficult choices that the human annotators of the reference corpora had to make, or their inability to correctly follow annotation guidelines on a complex task. One may therefore argue that further studies could instead concentrate on sure paraphrases only. One of the main conclusions that can be drawn from this study is that some types of sub-sentential paraphrases are difficult to acquire given current techniques. Some types of paraphrases rely on information that is so complex that it is unlikely that automatic techniques will be able to capture this knowledge in the near future. We have therefore been led to the opinion that such knowledge is a good candidate for manual acquisition when justified (e.g., to improve automatic translation through the contribution of monolinguals for the source language only [Resnik et al. 2010]).
In order to demonstrate the feasibility of manual acquisition of sub-sentential para-phrases by lay users, we have conducted the following medium-scale experiment on French: 22 volunteers 25 contributed paraphrases through a Web interface for 20 in-stances of hard-to-acquire paraphrase pairs. Half of them were given the first sentence of a sentential paraphrase pair for context, and had to paraphrase the appropriate phrase, while the other half were given the second sentence. The guidelines that they were given included that they should attempt to propose paraphrases differing as much as possible from the original phrases, as well as paraphrases that were not necessarily the first solution that comes to mind.

An example of the results obtained is shown on Figure 6. From this example, we can first note that all 22 contributors managed to propose a paraphrase that differed from the original phrase: humans are indeed very good at rephrasing. Second, the rich variety observed (12 unique paraphrases for 22 contributors) did not prevent some candidates from being proposed more frequently ( au plus t X ot ( as soon as possible ) and au plus vite ( as quickly as possible ) were both proposed 4 times). Manual inspection, however, reveals that all candidate paraphrases here seem acceptable. Interestingly, none of our contributors proposed the paraphrases expected from the reference: this illustrates appropriately some of the reasons why some paraphrase pairs may be more difficult to align.

Looking at the complete results for the 20 sentence pairs, we find that the expected reference paraphrase was obtained in both directions in 6 cases, and in one direction in 8 cases. In the 6 cases where the reference paraphrase (i.e., the aligned phrase from the other sentence) was not obtained, a common paraphrase was proposed 3 times. Results on this scale confirm the great potential for paraphrasing in context short text units. We expect these results to be confirmed on most other natural languages, but collecting data on a large scale and for several languages through a Web-based game is part of our future work. This work is concerned with the study of sub-sentential paraphrases, and has focused on the task of their acquisition from monolingual parallel corpora. While this type of re-source is extremely scarce, we have argued that it is nonetheless the most suitable type of resource for studying paraphrase phenomena and algorithms for paraphrase acqui-sition. We further argued that a better understanding of the paraphrases that still defy state-of-the-art acquisition techniques is necessary to foster advances in paraphrasing research.

We have therefore proposed an exploration of different techniques for the task of sub-sentential paraphrase acquisition. We selected five techniques for their comple-mentarity in terms of resources and algorithms, and applied them on two corpora for two languages and four corpus types of various origins. We have reported detailed results on the performance of each of these individual techniques depending on some notion of the complexity of the task. Our explorations also led us to study two different technique combinations, which in some cases improved over all our previous baselines. Paraphrase validation via automatic classification allowed us to obtain significant improvement over all other configurations. Finally, we have exhibited and classified instances of difficult-to-acquire paraphrase pairs.

Here is a brief review of our main findings regarding sub-sentential paraphrase acquisition under our experimental conditions.  X  X n terms of techniques, statistical word alignment (G IZA ) and edit rate computation (T ER p ) achieved high performance.  X  X n terms of resources, parallel bilingual corpora (P IVOT ) and large monolingual cor-pora ( VALIDATION ) provided useful knowledge.  X  X hen combining individual systems, it is possible and useful to use machine learning with additional features that are not and / or cannot be integrated into individual techniques.  X  X onolingual parallel corpora (T EXT ) are a very appropriate corpus type for studying paraphrase acquisition, in spite of being scarce.  X  X cceptable performance can be achieved on monolingual comparable corpora which are much more readily available for large-scale acquisition (E VENT ).  X  X mprovements on subsentential paraphrase acquisition will be the most important with techniques and / or resources targeting lexical and phrasal synonymy, as well as pragmatic variation; this calls for a tighter integration with research on textual entailment [Androutsopoulos and Malakasiotis 2010].  X  X ll our experiments showed that comparable results can be achieved on English and French; we expect similar conclusions to be drawn for related languages.
 Although we have identified the lack of appropriate and sufficient data as an impor-tant bottleneck for paraphrasing research, we believe that the availability of very large monolingual corpora already allows for pushing our current work into several direc-tions. In particular, our current work includes focusing on sub-sentential paraphrase acquisition from comparable corpora, and the characterization of the contexts in which two paraphrases can be substituted.

