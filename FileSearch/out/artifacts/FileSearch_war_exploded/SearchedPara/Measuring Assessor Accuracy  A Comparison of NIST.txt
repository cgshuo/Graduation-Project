 In many situations, humans judging document relevance are forced to trade-off accuracy for speed. The development of better interactive retrieval systems and relevance assess-ing platforms requires the measurement of assessor accu-racy, but to date the subjective nature of relevance has prevented such measurement. To quantify assessor perfor-mance, we define relevance to be a group X  X  majority opinion, and demonstrate the value of this approach by comparing the performance of NIST assessors to a group of assessors representative of participants in many information retrieval user studies. Using data collected as part of a user study with 48 participants, we found that NIST assessors discrim-inate between relevant and non-relevant documents better than the average participant in our study, but that NIST assessors X  true positive rate is no better than that of the study participants. In addition, we found NIST assessors to be conservative in their judgment of relevance compared to the average participant.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval General Terms: Experimentation Keywords: Relevance, assessors, signal detection theory
Whether it is assessors judging the relevance of documents or users performing document triage in the midst of a search, humans are faced with a speed accuracy trade-off. When ac-curacy must be traded off for speed, relevant documents will be missed and judged non-relevant, and non-relevant docu-ments will be judged to be relevant. Better user interfaces (UIs) will allow assessors and users to judge faster and more accurately than poorer interfaces.

To build better relevance assessing UIs, we need to be able to measure the performance of the users of these UIs. While it is easy to measure the rate at which assessors make judg-ments, the subjective nature of relevance makes measuring the quality of the judgments difficult.

To sidestep the issue caused by the subjective nature of relevance, and inspired by the work of Alonso and Miz-zaro [1], we define relevance as the majority opinion of a group of qualified assessors. Alonso and Mizzaro [1] found Assessor Relevant (Pos.) Non-Relevant (Neg.) Relevant TP = True Pos. FP = False Pos.
 Non-Relevant FN = False Neg. TN =TrueNeg.
 Table 1: Confusion Matrix.  X  X os. X  and  X  X eg. X  stand for  X  X ositive X  and  X  X egative X  respectively. that the majority vote of a group of 10 non-NIST assessors correctly judged the relevance of 28 of 29 documents for a single TREC topic, and that the non-NIST assessors discov-ered mistakes made by the NIST assessors. We expand on Alonso and Mizzaro X  X  work by considering multiple topics and by using more than twice as many documents.

We demonstrate the usefulness of this approach by com-paring NIST assessors to a group of user study participants and in doing so gain a better understanding of both groups.
We use a group X  X  majority opinion to locate instances where NIST assessors may have made mistakes and establish a goldstandard for a set of relevance judgments.

In making relevance assessments, an assessor has to make a classic signal detection yes/no decision. Once viewed as a signal detection task, it becomes clear that we should ana-lyze the performance of assessors in terms of their true pos-itive rate (TPR) and their false positive rate (FPR), where TPR = | TP | / ( | TP | + | FN | )andFPR= | FP | / ( | FP (see Table 1).

Signal detection theory says that an assessor X  X  relevance judging task may be modeled as two normal distributions and a criterion [2]. Given this model of the signal detection task, we can characterize the assessor X  X  ability to discrimi-nate as d = z ( TPR )  X  z ( FPR ), and compute the assessor X  X  criterion c =  X  1 2 ( z ( TPR )+ z ( FPR )), where the function z is the inverse of the normal distribution function [2]. A pos-itive c represents a conservative judging behavior where the assessor misses relevant documents to keep the false positive rate low, and a negative c is liberal, respectively.
As part of a study that we conducted [3], 48 participants judged the relevance of documents for 8 different topics. The topics were from the 2005 TREC Robust track, which used the AQUAINT collection of newswire documents. All the results here are based on phase 1 of the study.

An intentional by-product of our study was that we would obtain a large number of judgments on the same documents.
Topic Docno R N % qrel qrel 336 NYT19980817.0234 10 1 91 N R 336 NYT20000927.0154 8 4 67 N R 362 NYT19990313.0150 11 1 92 N R 362 NYT19980805.0250 9 1 90 N R 362 XIE20000821.0238 2 8 20 R N 383 NYT19991214.0159 7 5 58 N R 427 NYT19990614.0216 5 7 42 R R 436 XIE19980303.0229 7 4 64 N R Table 2: Documents on which participants disagreed with NIST assessors. R=relevant. N=non-relevant.
 After conducting our study, we found that some documents were duplicates of each other. For both NIST assessors and study participants, we found cases where the same assessor had conflicting judgments for the same document.

After removing documents found to be duplicates, we had 71 documents with 10 or more judgments. There were 8 documents where the majority opinion of relevance differed from the NIST assessors X  and 2 documents where there was a tie. For these 10 documents, we and an undergraduate assistant examined each document to determine whether the majority opinion or the NIST assessor was correct. We in effect acted as  X  X wing votes X  to determine the final majority in these few cases.

Both ties were from topic 426 and the NIST assessor had judged them relevant. At first, the relevance of these two documents was not obvious to us, but after looking at other examples of the topic X  X  relevant documents, we set the gold standard judgment to agree with the NIST assessor.
For the remaining 8 of 10 documents, we determined that the majority opinion was correct in 7 of the 8 cases. Ta-ble 2 shows the 8 documents and the count of judgments by the participants. For each of the 6 documents judged non-relevant, we found relevant material in the document.
The one case where we found NIST assessors to have made a false positive mistake, it was because the on-topic docu-ment failed to describe a specific incident, which was re-quired by the description of relevance for topic 362.
For the seemingly off-topic document where the majority was wrong, the final paragraph contained relevant content.
For both the computation of d and c , a false positive or true positive rate of 0 or 1 will result in infinities. Rates of 0 and 1 are most often caused by these rates being estimated based on small samples. To better estimate the rates and avoid infinities, we employ a standard correction of adding a pseudo-document to the count of documents judged. Thus, the estimated TPR = ( | TP | +0 . 5) / ( | TP | + | FN | +1) and the estimated FPR = ( | FP | +0 . 5) / ( | FP | + | TN | +1).
Of the 8 total search topics, each study participant worked on different sets of 4 topics. As such, it is possible that one participant could have worked on easier topics or sets of documents than another participant. To correctly compare between participants and NIST assessors, we compute the NIST assessors X  performance on 48 sets of documents. Each set of documents corresponds to the set of documents that a participant judged. We measure statistical significance using a paired, two-sided, Student X  X  t-test. The measures for each participant are based on an average of 16.8 judgments.
Table 3: Results. Please see text for explanation.
The averages reported in Table 3 all use estimated TPR and FPR. For the NIST assessors, we average performance over the 48 sets of documents determined by the 48 partic-ipants (see Section 2). The NIST performance on the 71 documents is similar with an overall TPR of 0.81 (26 / 32) and a FPR of 0.03 (1 / 39).

While our results are not based on a random sample of documents or topics, two observations stand out.

First, our study participants did as well as NIST assessors at judging relevant documents to be relevant. Both study participants and NIST assessors had an average a TPR of 0.77. We hypothesize that the TPR of human assessors is limited by the amount of time that can be devoted to search-ing a document for relevant material. Unless an assessor reads documents in their entirety, relevant material will be missed and a TPR of less than 1 will result.

Second, NIST assessors rarely judged non-relevant docu-ments as relevant (a false positive). NIST assessors had a much lower FPR of 0.07 compared to the participants X  0.18. There are many possible reasons for this difference including different training, instructions, and user interfaces.
Taken together, the NIST assessors are better able to dis-criminate relevant from non-relevant documents than the study participants ( d of 2.3 vs. 1.9). While there is consider-able variation in the criterion used by participants, on aver-age the participants show little bias with a criterion c =0 . 10. The NIST assessors were found to have a conservative bias with a c =0 . 37.
By defining relevance to be a group X  X  majority opinion, we were able to quantitatively compare the relevance judging performance of NIST assessors and user study participants. We found that while both groups effectively have the same true positive rate, NIST assessors have a significantly lower false positive rate. Special thanks to Gordon Cormack, Ian Soboroff, and Ellen Voorhees for their helpful feedback. As a URI, Michael Tatham did a preliminary analysis of this data. This work was supported in part by NSERC, in part by Amazon, and in part by the University of Waterloo.
