 Product reviews have become an important resource for cus-tomers before they make purchase decisions. However, the abundance of reviews makes it difficult for customers to di-gest them and make informed choices. In our study, we aim to help customers who want to quickly capture the main idea of a lengthy product review before they read the details. In contrast with existing work on review analysis and document summarization, we aim to retrieve a set of real-world user questions to summarize a review. In this way, users would know what questions a given review can address and they may further read the review only if they have similar ques-tions about the product. Specifically, we design a two-stage approach which consists of question retrieval and question diversification. We first propose probabilistic retrieval mod-els to locate candidate questions that are relevant to a re-view. We then design a set function to re-rank the questions with the goal of rewarding diversity in the final question set. The set function satisfies submodularity and monotonicity, which results in an efficient greedy algorithm of submodu-lar optimization. Evaluation on product reviews from two categories shows that the proposed approach is effective for discovering meaningful questions that are representative for individual reviews.
The first two authors made equal contributions to this pa-per  X  Information systems  X  Summarization; Information retrieval diversity; Review summarization; Question retrieval; Diversification
With the rapid growth of online review sites, more people rely on advices from fellow users before they make purchase decisions. Unfortunately, finding relevant information from large quantities of user reviews in a short time is a huge chal-lenge. Thus, review analysis with the goal of extraction of useful information has become an important way to improve user experience of online shopping.

Existing techniques for review analysis include review rat-ing prediction [33, 17], sentiment polarity classification [13, 22], and aspect-based review summarization [11, 32, 28]. The first two techniques aim to predict numerical ratings and sentiment orientations of reviews. They do not summa-rize the main points discussed in reviews. Review summa-rization is beneficial for aggregating user opinions towards a product through the generation of a short summary from a set of product reviews. However, the generated summary may not be of interest to end users since it may contain lit-tle relevant information that addresses the specific questions that are in the user X  X  mind.

In our study, we seek an approach to help customers quickly comprehend a product review through questions. Questions are often more attractive for customers to read than plain opinion sentences are. In other words, we aim to find a con-cise set of questions that are addressed by a given review as well as cover the main points of it. Many users have certain questions about a product in mind and want to look at on-line reviews to see if their questions can be answered; but examining all lengthy reviews is too time-consuming. Given the concise set of questions for a review, users can quickly understand the review and may further read it only if they have similar questions in their minds.
 Directly synthesizing such questions is too intimidating. Thanks to the emergence of Community Question Answer-ing (CQA), large e-commerce websites now offer CQA ser-vices for their products. A notable example is Amazon X  X  Customer Questions &amp; Answers service 1 . In this paper, our goal is to retrieve real-world user questions to summarize in-dividual reviews. Take the following segment of a real-world review 2 from Amazon as an example: As we can see, this segment of review describes some per-sonal experience with the camera X  X  autofocus feature and compares it with another camera a7s . On the other hand, a real relevant question 3 was asked and answered on Amazon X  X  CQA service as shown below: This question asked about autofocus feature and can well represent the semantic of the segment of the above review. Meanwhile, since it is a question, users with similar ques-tions in their minds would be very interested in further read-ing the review if they see this question as part of the sum-mary of the review. Thus, this question would be a good candidate to retrieve for this review. Moreover, directly re-trieving this question could be challenging given the short length of the question, but we can exploit the answers of the question. For example, this particular answer also dis-cussed the comparison with a7s . Using it would be helpful to measure the relevance between the question and review.
This task of summarizing a product review through user questions is a challenging task. First of all, user generated reviews are usually long, ranged from hundreds to thousands of words, while questions are much shorter. Directly match-ing questions to a review may lead to unsatisfactory results. Second, a product review often discusses multiple aspects of a product. The set of retrieved questions for a given review should cover as many aspects as possible so that customers have a comprehensive understanding of the review. Last but not the least, the questions should not be redundant.
To tackle these challenges, we develop a two-stage frame-work to achieve the goal of retrieving a set of non-redundant questions to represent a product review. We first employ a probabilistic retrieval model to retrieve candidate questions http://www.amazon.com/gp/forum/content/qa-guidelines.html http://www.amazon.com/Sony-ILCE7SM2-Full-Frame-Mirrorless-Interchangeable/product-reviews/B0158SRJVQ/ http://www.amazon.com/Sony-ILCE7SM2-Full-Frame-Mirrorless-Interchangeable/dp/B0158SRJVQ/ based on their relevance scores to a review. We further lever-age answers to a question to bridge the vocabulary gap be-tween a review and a question. To remove redundancy in the candidate question set, we propose a set function that is used to re-rank the retrieved questions with the goal of diversifying the questions. Particularly, the set function sat-isfies monotone submodularity such that it can be efficiently optimized by a greedy algorithm. The main contributions of this paper can be summarized as follows:
Automatic review summarization has been a hot research topic over the past decade. Different from standard text summarization [7], which aims to generate a concise sum-mary for a single [31] or multi-document [8], review sum-marization aims to integrate users X  opinions for a large col-lection of reviews with respect to a product [23, 36]. The key idea is to identify the key specifications of a product and opinion sentences towards each specification. Detailed analysis of state-of-the-art literature can be found in [26, 14, 21]. Our problem of aligning questions to a review is similar to text summarization problem, with the goal of finding rel-evant and non-redundant questions (summary) for a review (document). It is also similar to review summarization, but the difference is that opinion-based summarization focuses on sentence or phrase extraction from reviews, while ours focuses on using relevant questions to represent the main points discussed in a review. By doing this, we are able to create more  X  X elevant X  summaries of reviews for potential buyers.
Our goal of finding a set of representative questions to summarize reviews is similar to question retrieval in the field of community question answering (CQA). The key problem is to quantify the similarity between newly generated user questions and curated questions so that corresponding an-swers can be used to answer those newly generated ques-tions. Examples of work include Zhou et al. [39] who firstly proposed a context-aware model for addressing the lexical gap problem between questions; and Zhou et al. [40] who designed an elegant study to model the question represen-tations with metadata powered deep neural networks. How-ever, question retrieval in CQA is different from our problem in two aspects. First, newly generated user questions and historical questions are  X  X arallel texts X , while user reviews and questions are highly asymmetric on the information they convey. Second, newly generated questions that are used to retrieve similar questions are usually short (i.e., less than 20 words), while user reviews are longer (usually more than 100 words).

Our problem also relates to automatic question generation from text data. Zhao et al., [38] developed a method to automatically generate questions from short user queries in CQA. Chali et al., [4] developed a method to generate all possible questions in regards a topic. One limitation of these studies is that questions are generated based on template, so they might not be representative of real user questions. Our study is different, as we aim to select relevant questions that can be used to summarize user reviews from real-user question archives.
As our goal is to use long reviews to find short repre-sentative questions as summaries, our problem relates to the problem of information retrieval with verbose queries [10]. Due to term redundancy, query sparsity, and difficulty in identifying key concepts, verbose queries often result in null results. In tackling these challenges, recent studies have developed techniques to re-compose queries. Examples in-clude query reduction [15, 12], query reformulation [5, 35], and query segmentation [1, 27]. However, the errors accu-mulated during the query transformation process cannot be corrected during the retrieval phase. In our study, we do not split long reviews into sentences or phrases, and use the text chunks to retrieve relevant questions. Instead, we utilize a two-stage framework: 1) use the entire review as a query to retrieve relevant questions; and 2) after retrieving a set of questions, we employ a diversity objective function to en-courage question diversity. To the best of our knowledge, no existing work attempts to retrieve non-redundant questions to summarize a product review.
Our task is to use a set of questions to summarize a prod-uct review. The review in turn is supposed to contain the answers to those questions. Introducing this feature to e-commerce platforms is beneficial for customers who want to quickly capture the main idea of lengthy reviews before reading the details. Consider a product database with m products. Each product i is associated with a set of reviews R for product i . Each review can be represented by a bag of words. Meanwhile, we have a question database/corpus Q = { q (1) ,...,q ( n ) } where the questions are crawled from Community Question Answering (CQA) sites. Given a re-view r ( i ) j of product i , our task is to select a small subset of questions S  X  Q to summarize the review.

Similar to other text summarization tasks [24], the quality of selected questions can be quantified by a set function F : 2
Q  X  R . In addition, the selected subset S should satisfy certain constraints. Formally, our task is to find the optimal question subset S  X  defined as the following combinatorial optimization problem: where c (  X  ) is a constraint function defined on q , and b  X  0 is a constant threshold. For example, if we want to enforce that the total length of all the selected questions should not exceed 50 words, we can define c (  X  ) as a function to calculate the length of each question and set b = 50. Similarly, we can define a constraint to restrain the total number of questions in the set.

The set function F in Eqn.(1) measures the quality of the selected question subset S . The choice of F depends on the property of the questions that we desire. In gen-eral, Eqn.(1) would be an NP-hard problem. Fortunately, if F satisfies non-decreasing submodular [6], the optimiza-tion problem can be solved by efficient greedy algorithms with a close approximation. We introduce the background on submodular functions in Section 3.2.

It is worth noting that we do not solve Eqn.(1) directly over all the possible questions in the database. Otherwise, it would be too time-consuming given the sheer size of all avail-able questions on CQA. Instead, we retrieve a set of poten-tially relevant questions first by using information retrieval techniques, e.g., obtaining the top 100 questions based on their relevance to a given review. We will introduce the question retrieval models in Section 4.2. Given these ques-tions, we then apply Eqn.(1) to select a few questions (e.g., 5) as the final results by considering both question coverage and diversity. Thus, this module can be viewed as re-ranking for achieving diversified results. We present our formulation of Eqn.(1) in Section 4.3.
Submodular functions are discrete functions that model laws of diminishing returns [30]. They have been used in a wide range of applications such as sensor networks [16], information diffusion [9], and recommender systems [29]. Recently, it has been explored in multi-document summa-rization [19, 20]. Following the notations introduced in the previous section, some basic definitions of submodular func-tions are given as follows.

Definition 1. A set function F : 2 Q  X  R is submodular if for any subset S,T  X  Q ,
Definition 2. A set function F : 2 Q  X  R is modular if for any subset S,T  X  Q , Modular set functions also satisfy submodularity according to Definition 1.
Definition 3. A set function F : 2 Q  X  R is monotone, if for any subset S  X  T  X  Q ,
The class of submodular functions enjoys a good property with concave functions as follows.

Theorem 1. If F : 2 Q  X  R is a submodular function, g ( S ) =  X  ( F ( S )) , where  X  (  X  ) is a concave function, is also a submodular function [30].
 In Section 4.3, we discuss the construction of F ( S ) and demonstrate that it is submodular and monotone based on Theorem 1. These properties enable efficient greedy approx-imation algorithms [25] for the optimization problem.
In order to provide customers with  X  X ints X  of a review, the questions should be representative of the review. For example, if a review discusses image quality and battery life of a camera, relevant questions would be related to these two features, e.g.,  X  X oes the camera take high quality macro images? X  or  X  X ow many days of battery life can you get with this camera? X  . In addition, the questions are expected to be dissimilar to each other such that there is little redundant information covered in the question set. For example, the question  X  X ow is the battery life? X  is redundant as it con-tains similar semantic information with the aforementioned question related to battery life .

With the dual goal of relevancy and diversity, we design a two-stage framework to find a set of questions that can be used to summarize a review. We first utilize a proba-bilistic retrieval model to select a smaller set of candidate questions that are relevant to a given review from a large pool of questions crawled from the CQA website. Consider-ing the possible semantic mismatch between the review and question corpus, we incorporate answers into the retrieval model to resolve the vocabulary gap between them. After obtaining the top ranked relevant questions, we design a set function to re-rank questions in the candidate list with the goal of removing redundant questions. The final question set is derived through the measurement of a trade-off between the relevance of selected questions to the review as well as the diversity of the questions.

In the following sections, we present the query likelihood language models to generate a candidate question list (Sec-tion 4.2) and introduce our set function to re-rank candidate questions (Section 4.3) with an efficient greedy algorithm (Section 4.4) for optimization.
To retrieve candidate questions that are relevant to a given review, we employ query likelihood language model [2]. We assume that before drafting a review, a user would think about what questions he/she would like to answer. There-fore, the relevance score of a question q retrieved by a review r is computed as the log-likelihood of the conditional prob-ability P ( q | r ) of the question given the review:
Similar to other text retrieval tasks, a review can be re-garded as a sample drawn from a language model built on a question pool. Formally, using the Bayes X  theorem, the conditional probability can be calculated by: In Eqn.(3), P ( r ) denoted the probability of the review r , which can be ignored for the purpose of ranking questions because it is a constant for all questions. Thus, we only need to compute P ( r | q ) and P ( q ). P ( r | q ) represents the conditional probability of review r given question q . We can apply the unigram language model to calculate P ( r | q ): where P ( w | q ) is the probability of observing word w in a question q . The word probability can be estimated based on maximum likelihood estimation (MLE) with Jelinek-Mercer smoothing [37] to avoid zero probabilities of unseen words in q : where  X  is a smoothing parameter and C denotes the whole question corpus. The MLE estimates for P ml ( w | q ) and P are: where count( w,q ) and count( w,C ) denote the term frequency of w in q and C , respectively. | X | denotes the total number of words in q or C .

P ( q ) in Eqn.(3) denotes the prior probability of the ques-tion q regardless of review. It can encode our prior prefer-ence about questions. In order to summarize a review, we prefer shorter questions so that users can digest information faster. Hence, we reward shorter questions by making the prior probability inversely proportional to the length of the question as follows: P ( q ) can also be computed by other ways. For example, if there exists rating information of the questions on the CQA website, we can use it to prefer questions with higher ratings.
By plugging Eqn.(4) and Eqn.(8) into Eqn.(3), we can obtain the relevance scores for all questions in the question corpus.
Since questions and reviews are not  X  X arallel texts X , there exists vocabulary gap between the two corpus. As shown in the real-world example in Section 1, directly retrieving this question could be challenging given the short length of the question. To address this issue, we incorporate the corresponding answers of the question corpus to estimate the parameters in the language model defined in Eqn.(5) [34]. After including all the answers a of question q , the relevance score becomes: Based on the Bayes X  theorem, we have: The above derivation is based on the following reasoning. Similar to Eqn.(3), P ( r ) is a constant for all the questions, and thus it can be ignored. We further assume the proba-bility of answers a given a question q is uniform, and thus p ( q,a ) is proportional to p ( q ).
 We then leverage both question and answers to estimate P r | ( q,a ) :
P r | ( q,a ) = Y where C 0 denotes the whole question and answe corpus, and P ml w | C 0 is the collection language model which is estimated based on Eqn.(7).  X  is a smoothing parameter. P mx w | ( q,a ) denotes the word probability estimated from the question and answers. It takes a weighted average of maximum-likelihood estimates from question and answers, respectively:
P mx ( w | ( q,a )) = (1  X   X  ) P ml ( w | q ) +  X P ml ( w | a ) where  X   X  [0 , 1] is a trade-off cofficient.

The prior probability P ( q ) can be calculated in the same way as in Eqn.(8). By plugging P r | ( q,a ) and P ( q ) in Eqn.(10), we can obtain the relevance scores in Eqn.(9). The top-k questions are then retrieved as candidates and to be re-ranked by promoting diversity among them.
Similar to other text summarization tasks, the final ques-tions presented to users should avoid redundancy as much as possible. At the same time, these questions are still rele-vant to the review and can convey the main information in the review. In other words, we aim to achieve a dual goal in the final question set: relevancy and diversity. Mathemati-cally, we formulate our objective function as a combinatorial optimization problem by following Eqn.(1) as follows: where V is the candidate question set obtained by the ques-tion retrieval component. L ( S ) measures the relevance of the final question set S with respect to the review. R ( S ) measures the diversity of the final question set.  X  is a constant for diversity regularization. The constraint P q  X  S length( q )  X  b requires that the word count of all the ques-tions is less than a threshold b , which is usually a small number because a concise summary is desirable for users.
The set function L ( S ) is defined to encourage the selection of questions with high relevance scores. Specifically, we use the logarithm of sum of offset relevance scores of questions in the final question set S . Formally, where score( q ) is the relevance score of question q . It can be calculated based on the query likelihood language models without (Eqn.(2)) or with (Eqn.(9)) incorporating answers (for convenience of presentation, we omit argument r and a ). c = min q  X  V (score( q )) is a constant to ensure the argument of log (  X  ) is always positive.

The set function R ( S ) is designed to select as  X  X iverse X  questions as possible. The function will score a set of ques-tions high if those questions do not semantically overlap with each other. Formally, where P i ,i = 1 ,...,T indicates a partition of the candidate question set V into T disjoint clusters, and r q indicates the reward of selecting question q in the final summary set. Specifically, r q = 1 | V | P v  X  V w qv , where w qv is the similarity score between question q and v [20]. Applying the logarithm function will make one cluster have diminishing gain if one question has been chosen from it. In this way, R ( S ) rewards question selection from a cluster in which none of the ques-tions have been selected. Addition of a small positive value to the argument of the logarithm function guarantees the argument is positive.

Theorem 2. Both L ( S ) and R ( S ) are monotone sub-modular functions.

Proof. The logarithm function is non-decreasing con-cave function. The functions inside each logarithm func-tion are non-negative modular functions (see Definition 2), so they are monotone (see Definition 3). Applying the loga-rithm function, which is a concave function, to non-decreasing modular functions yield submodular functions (see Theorem 1). For R ( S ), the summation of submodular functions re-sults in a submodular function as well. Hence, the set func-tion F ( S ) satisfies monotonicity and submodularity. The submodular optimization problem in Eqn.(13) is still NP-hard, but Nemhauser et al. [25] has proven that the ap-proximated solution achieved by a greedy algorithm is guar-anteed to be within (1  X  1 /e ) of the optimal solution. It is worth noting that this is a worst case bound, and in most cases the quality of the solution obtained would be much bet-ter than this bound suggests. Hence, we describe an efficient approximation algorithm by utilizing monotone submodular properties of F ( S ). Algorithm 1 shows a greedy algorithm that finds approximation solution to the optimization prob-lem in Eqn.(13). The algorithm selects the best question q that brings maximum increase in F ( S ) at stage i , as long as the total length of questions l in the selected question set S does not exceed the threshold b . It terminates when none of the questions in the candidate set V satisfy the length threshold constraint l + length( q ) &lt; b .
Algorithm 1: The Greedy Algorithm
One of the fundamental challenges is the lack of ground-truth data available for evaluating the quality of retrieved questions. Since the proposed task is a document summa-rization problem, we follow the same evaluation method and metric that are used for text summarization task in NIST Document Understanding Conferences (DUC) 4 .

We choose to focus on products from Amazon 5 , as it dis-plays various kinds of products with associated reviews and question and answering (QA) data contributed by real end users. We first decide on which product category to focus in our experiment. We select products from two categories, camera and TV, and download their QA data. We rely on NLTK 6 to preprocess the content of the data, including sen-tence segmentation, word tokenization, lemmatization and stopword removal. We remove meaningless questions whose lengths are shorter than 3 words. We also discard questions that are longer than 25 words, which are supposed to con-vey detailed information, as they might not be general to summarize many product reviews. The preprocessing step yields 331 products in the digital camera category and 226 in the TV category. Table 1 summarizes the questions and answers of products for each category.

After obtaining the QA data, we need to create a review dataset for evaluation. We first select the top 100 prod-ucts retrieved from the two product categories, each for 50 products. For each product, we select the top 5 reviews ranked by Amazon X  X  Helpfulness voting system, and retain http://duc.nist.gov/duc2004/ http://www.amazon.com/ http://www.nltk.org/ Table 1: Statistics of Question Data for Camera and TV Category only reviews whose length is between 200 and 2 , 000. After obtaining the 500 reviews for the two product categories, we follow the guidelines for summary generation of NIST DUC 7 . Specifically, we request 10 graduate students to read the reviews and generate questions for each of them. The questions, which is regarded as a summary, should cover all the product features that are discussed in a product review, but not overlap with each other with respects to product fea-tures. To ensure the generated questions are representative for real-user questions, we ask students to first select ques-tions from the question pool obtained through the crawling process. If no question can be selected, they are allowed to write their own questions. For each review, a student can generate up to 10 questions. The maximum of total length of all questions is 100. In order to accomplish the annotation task, 10 students are equally divided into two groups. The students from the first group select or write questions for 100 reviews, and the students from another group examine the quality of questions. The students from the two groups will do one more round of annotation together to resolve any conflicts. It usually takes 50 minutes to finish question generation and examination for a single review, which is a very time-consuming process since the annotators should consider both relevancy and diversity. We apply the same preprocessing steps (as we did for the QA data) to process the annotated review data. The averaged review length for camera dataset is 814 . 976 and the averaged review length for TV dataset is 582 . 932.
In order to evaluate the performance of our proposed ap-proach, we implement the following six summarization sys-tems based on the variant of our approach: (1) Query Likelihood Model: The query generation proba-bility is estimated based on question corpus (Eqn.(3)). (2) Combined Query Likelihood Model: The query genera-tion probability is estimated based on question and answer corpus (Eqn.(10)). (3) Query Likelihood Model with Maximal Marginal Rele-vance (MMR): re-rank retrieved questions by query likeli-hood model (system (1)) using MMR [3], which is designed to remove redundancy while preserving the relevance by using a trade-off parameter  X  . Note that MMR is non-monotone submodular, so a greedy algorithm is not theo-retically guaranteed to be a constant factor approximation algorithm [20]. (4) Combined Query Likelihood Model with Maximal Marginal Relevance: re-rank retrieved questions by combined query likelihood model (system (2)) using MMR. (5) Query Likelihood Model with Submodular Function: re-rank retrieved questions by query likelihood model (system http://duc.nist.gov/duc2004/t1.2.summarization. instructions (1)) using submodular function (Eqn.(13)). (6) Combined Query Likelihood Model with Submodular Function: re-rank retrieved questions by combined query likelihood model (system (2)) using submodular function.
For system (1) and (2), we choose the Jelinek-Mercer smoothing parameter  X  between 0 . 1 and 0 . 3 (Eqn.(5)). For system (3) and (4), we choose the trade-off parameter  X  be-tween 0 . 1 and 1 . 0. For system (5) and (6), we set the number of questions in the candidate set V (Eqn.(13)) as 100, the length threshold b as 50, 75, and 100, and the number of clusters (Eqn.(15)) as 10. We rely on K-means clustering al-gorithm to partition V , which leverages IDF-weighted term vector for each question. We also experiment with different settings of smoothing parameter  X  (Eqn.(12)) and diversity regularizer  X  (Eqn.(13)), which will be shown in Section 6.3.
We follow the evaluation of conventional summarization systems to measure the performance of the aforementioned six systems for finding questions to summarize a product re-view. Specifically, we rely on ROUGE [18] (Recall-Oriented Understudy for Gisting Evaluation), which measures how well a system-generated summary matches the content in a human-generated summary based on n-gram co-occurrence. In our experiment, we compare unigram and bigram-based ROUGE scores.

One limitation of ROUGE score is that it assumes all words play equally important roles in a document. How-ever, the words related to product aspects such as  X  X mage X  or  X  X creen X  are more important than stopwords such as  X  X oes X  or  X  X t X , which are frequently occurred in questions. There-fore, we also use TFIDF cosine similarity, which rewards important words by inverse document frequency. The defi-nition of cosine similarity function can be found in [20].
We first show the feasibility of our method to retrieve non-redundant questions that can be used to summarize a review. We take one review 8 from the digital camera category from Amazon as an example. The review length is around 700 tokens after preprocessing. The following segment shows the main aspects that the author talks about: Table 2 shows the questions edited by a human annotator. The first five questions are selected from the question corpus, while the last two are created by the annotator. Basically, the questions correspond to the top features highlighted in the review segment, and covers all the aspects that are dis-cussed in the review, including RAW files, 4K recording, shutter, stabilization, EVF, low light performance, and sen-sor. The last two aspects are not mentioned in the segment but are discussed in the main body. Table 3 shows the top-10 questions retrieved by query likelihood language model smoothed by answers. They cover the following aspects, http://www.amazon.com/gp/customer-reviews/ R360W96STA0KUI?ASIN=B0158SRJVQ
Table 3: Questions Retrieved by Query Likelihood Model
Table 4: Questions Reranked by Submodular Function camera X  X  performance in low light (the 1st, 3rd, 5th, and 7th question), comparison between different camera models (the 2nd question), lens adaption (the 4th question), video recording (6th question), shutter (the 9th and 10th ques-tion), and a general one (the 8th question). It shows that three of the top-5 results are redundant with respect to low light performance, and the last two questions overlap with each other with respect to shutter noise.

Table 4 shows the top-10 questions selected by the sub-modular function. The re-ranked questions cover the fol-lowing aspects: camera X  X  performance in low light (the 1st, 6th, 8th, and 10th question), comparison between differ-ent camera models (the 2nd question), shutter (the 3rd and 9th question), video recording (4th question), lens adaption (the 5th question), and RAW files (7th question). Com-pared with questions retrieved by query likelihood model, even though there still exist four questions that are rele-vant to low light performance, three of the related questions are demoted from the top due to their redundancy with the top-1 question. The questions asking camera model com-parison and shutter noise are promoted because they are semantically dissimilar to the top-1 question. There are non-redundant questions in top-5 positions of the re-ranked list. The re-ranking function is able to promote one question re-lated to RAW files, which is not included in the candidate question set retrieved by query likelihood model. In addi-tion, it also demotes the general question which was ranked at the 8th position, because it is not representative of ques-tions asking product aspects.

By comparing the human annotation with retrieved/ranked question set, there are overlaps such as low light perfor-mance, RAW files, 4K video recording, and shutter noise. Still, there are three aspects annotated by annotator that are not covered in the reranked question list: image sta-bilization, sensor, and EVF. It is not surprising that the retrieved questions do not cover the last two aspects, sensor and EVF, as the annotator does not select relevant questions from the question pool either. Meanwhile, the questions re-lated to comparison between different models and adaption of lenses are not selected by annotator. However, if we take a close look at the review, we can find some relevant sen-tences that can be used to answer the retrieved questions regarding the two questions: Considering the nature that summarizing a review is highly subjective, the questions generated by the proposed auto-matic retrieval and reranking method are reasonable and cover most of the aspects discussed in a product review.
The results on the two datasets (introduced in Section 5.1) achieved by different summarization systems (introduced in Section 5.2) are shown in Table 5 and 6. We set the total length threshold as 50, 75, and 100, respectively. Boldface stands for the best performance per column with respect to each length threshold. We conduct paired t-test for all comparisons of results achieved by two different methods.  X  indicates the corresponding method outperforms the simple query likelihood baseline statistically significantly, and  X  in-dicates the corresponding method outperforms all the other methods significantly.

On the TV dataset, the combined query likelihood lan-guage model (QL( Q,A )) yields better results than simple query likelihood language model (QL( Q )) does in terms of all evaluation metrics for different length threshold settings. Using MMR to rerank questions achieves competitive re-sults against QL( Q,A ) and QL( Q ) do. Using the submod-ular function to re-rank the questions retrieved by simple and combined query likelihood language model (denoted as QL( Q ) +sub and QL( Q,A ) + sub, respectively) show better results over corresponding retrieval models for all evaluation metrics. QL( Q,A ) + sub achieves significant better results than all the other systems do at 0 . 01 level for all evaluation metrics, except for bigram-ROUGE precision score when b = 50 and TFIDF cosine similarity score when b = 100.
On the camera dataset, unfortunately, incorporating an-swer corpus in the query likelihood language model does not bring improvement on the ROUGE and TFIDF cosine similarity scores. One possible reason is that the vocab-ulary size of answer collections for the camera category is larger than that of the TV category according to Table 1. Incorporating an answer collection might add many irrel-evant words to the language model, such that the results retrieved by QL( Q,A ) contain more noises than that by QL( Q ). After promoting diversity in the retrieved question set using MMR, QL( Q ) + MMR is able to achieve slightly higher or competitive results against QL( Q ) except for bi-gram ROUGE scores when b = 100; but QL( Q,A ) + MMR yields slightly inferior results against QL( Q,A ).
Even though the combined retrieval model does not help increase the ROUGE and TFIDF cosine similarity scores, QL( Q,A ) + sub yields the highest unigram-ROUGE scores. The precision and F 1 scores are significantly higher than that by QL( Q ) ( p &lt; 0 . 01). QL( Q ) + sub achieves the best TFIDF cosine similarity scores without significant difference with that by QL( Q ). The results on bigram-ROUGE scores are mixed. The highest bigram-ROUGE scores achieved by either QL( Q ) or QL( Q,A ) are significantly better than the score achieved by simple query likelihood at level 0 . 01, ex-cept the bigram-ROUGE recall score and F 1 score when b is set to 100.

In summary, query likelihood model incorporating an-swers is able to yield better summarization performance when the vocabulary size of the answer collection is mod-erate. The results achieved by query likelihood models with the submodular function are promising compared with con-ventional diversity promotion technique. The combined query likelihood model with submodular function yields signifi-cantly better performance on the TV dataset for both ROUGE and TFIDF cosine similarity metrics. This model also shows the potential ability to correct the order of a question list by promoting diversified results on the camera dataset.
In order to examine the impact of the smoothing parame-ter  X  of the answer collection (Eqn.(12)) and diversity regu-larizer  X  for the sumbodular function (Eqn.(13)), we examine the summarization performance achieved by system (2) and (6) (introduced in Section 5.2) with different settings of  X  and  X  on the TV and camera datasets. Figure 1 shows the unigram ROUGE F 1 scores achieved by different  X  between 0 and 1 with an interval of 0 . 1 when the length threshold is set to 50. The ROUGE curves achieved with other threshold settings follow similar patterns so we leave them out. For the TV dataset, as shown in the previous section, incorporating answers benefits the simple query likelihood language model estimated on the question collection. When  X  is between 0 and 0 . 3, the unigram ROUGE F 1 scores increase with the benefit of the integration of the answer collection. After that, the scores decrease when  X  is getting larger, mean-ing that imposing too much weights on the estimates from the answer collection is harmful to the performance of re-trieval model. For the camera dataset, results have shown Figure 1: ROUGE-1 F 1 Scores on TV and Camera Datasets with different Weights of Answer Collection when b = 50 Figure 2: ROUGE-1 F 1 Scores on TV and Camera Datasets with Different Diversity Regularizer when b = 50 that the answer collection does not help increase the uni-gram ROUGE F 1 scores. With larger  X  values, the scores are getting smaller.

Figure 2 shows the impact of diversity regularizer  X  on the combined query likelihood language model. With the increasing  X  values, the unigram ROUGE F 1 scores increase on both datasets. These figures are consistent with previous findings that adding submodular function to retrieval models will improve the summarization results. It shows that  X  = 5 . 0 is a good choice for both datasets.
This paper addresses a new task: summarizing a review through questions. Questions are often more attractive for customers to read than plain opinion sentences. They can serve as  X  X ints X  for customers to decide whether they want to further read the review. To the best of our knowledge, no prior work has studied this task. We propose a two-stage approach consisting of question retrieval and question diversification. Submodular optimization is used to consider both question coverage and non-redundancy. To evaluate the proposed approach, we create and annotate a dataset by manually locating and editing questions for reviews in two product categories. The experiments demonstrate the proposed approach can effectively find relevant questions for review summarization.

This work is an initial step towards a promising research direction. In future work, we will utilize more information about products such as product specifications and question ratings to enrich the proposed question retrieval compo-nent. Regarding question diversification, we will explore other submodular functions. We also would like to deploy the proposed method to a real-world review system and mea-sure the satisfaction of real users.
The authors would like to thank Miao Jiang for data col-lection, anonymous reviewers for valuable comments, and TCL Research America for generous funding support.
