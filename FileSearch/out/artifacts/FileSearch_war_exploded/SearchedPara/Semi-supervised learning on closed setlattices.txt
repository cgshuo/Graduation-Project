
Graduate School of Informatics, Kyoto University, Yoshida Honmachi, Sakyo-ku, Kyoto, Japan Japan Society for the Promotion of Science, Japan 1. Introduction
In various research domains from biology to economics, numerous mixed-type data including both dis-treats mixed-type data in a semi-supervised manner.

Semi-supervised learning is a special form of classification [48,50]; a learning algorithm uses both such large amount of unlabeled data together with labeled data in short supply.
To effectively use unlabeled mixed-type data for learning, we in this paper propose a novel semi-supervised learning algorithm, called SELF (SE mi-supervised L earning via F ormal Concept Analysis), which can directly treat mixed-type data. SELF adopts a popular semi-supervised learning strategy, cluster using labeled data. One of the remarkable features of SELF is that it performs the clustering process using Formal Concept Analysis (FCA) [8,14], which is a mathematical theory for data analysis and knowledge representation introduced by Wille [45]. Recently, Pasquier et al. [30] proposed to use closed patterns (itemsets) obtained by FCA as condensed  X  X ossless X  representations of patterns. This new approach has been the subject of further research and extensions [1,31,36,46]. In SELF, the label-them for each unlabeled datum. Furthermore, FCA and closed set lattices enable us to naturally treat incomplete data including missing values.

To summarize, this paper provides contributions to the respective fields of: 1. To semi-supervised learning : we present a novel approach based on an algebraic framework without 2. To FCA : we study a novel application, semi-supervised learning, using FCA and closed set lattices.
The behavior of SELF is outlined as a flowchart in Fig. 1, and this paper is organized along it after discussing about related work in Section 2. The data preprocessing phase to construct a context from a given dataset to apply FCA is explained in Section 3.1. Missing values are handled in this phase. The and, finally, key points and future work are summarized in Section 5. 2. Related work
Many studies have used FCA for machine learning and knowledge discovery [26], such as classifica-of real-valued data and proposed algorithms based on the JSM-method that produces hypotheses (classi-priori , while SELF automatically discretizes them along with the learning process and no background knowledge and assumption about data are needed.

On the other hand, in machine learning context, decision tree-based algorithms such as C4.5 [32,33] most of such approaches are designed for only continuous variables and, to the best of our knowledge, no semi-supervised learning algorithm based on cluster-and-label can treat mixed-type data including discrete variables appropriately. Since SELF uses FCA for clustering, it needs no distance calculation and no data distribution, which is one of the remarkable features of SELF.

There exists a study by Kok and Domingos [24] which is related to the idea of putting original data on lattices. They proposed a learning algorithm via hypergraph lifting, which constructs clusters by hypergraphs and learns on them. Their idea is thus similar to ours since we also  X  X ift X  raw data to approach, thereby our approach can be more useful for machine learning and knowledge discovery from mixed-type data.

SELF achieves not only semi-supervised learning but also label ranking using the preference for each by weighting each cla ssification rul e through closed set lattices. 3. SELF algorithm We present the SELF algorithm in this section, which is the main part of this paper. The behavior of second it constructs concept lattices by FCA, and third it learns the preference for each class label. Notations used in this paper are summarized in Table 1. 3.1. Data preprocessing
The aim of data preprocessing is to construct a (formal) context, a binary matrix specifying a set of objects and their attributes, to apply FCA to training data.
 An element of the header h  X  H is called a feature 1 and the domain of h is denoted by Dom ( h ) .The { the set of real numbers.
 table  X  =( H , X ) , we identify the set of objects G with set ( X )= { x 1 , x 2 ,..., x n } .
In the data preprocessing, for each feature h  X  H of a table  X  , we independently construct a context ( database systems literature [15].
 { h . m | m  X  Dom ( h ) \{ X }} and, for each value x performs this translation.
 Example 1 Given a table  X  =( H , X ) with H = { 1 , 2 , 3 } and X = x 1 , x 2 such that This table can be represented in the following manner.
 The domains are given as Dom (1) = Dom (2) = { T , F } and Dom (3) = {A, B, C}. Here we have Thus we have the context ( G , M , I ) such that It is visualized as a cross-table as follows:
Second, we make a context from continuous variables using discretization. This process is embedded in the learning process (see Fig. 1) and discretizing resolution increases along with the process. The value y ( h ) such that Next, we discretize values in [ 0 , 1 ] and make a context using the binary encoding of real numbers, ( x , h . a )  X  I for making a context from continuous variables.
 Example 2 Given a table  X  =( H , X ) with H = { 1 , 2 , 3 , 4 } and X = x 1 , x 2 such that It can be represented as follows: third and forth are continuous. Assume that discretization level k = 1. We have visualized as a cross-table as follows: 3.2. Clustering and making lattices by FCA
From a context obtained by the data preprocessing, we generate closed sets as clusters of data points always assume that a given table  X  is converted into a context ( G , M , I ) by Algorithms 1 and 2. For subsets A  X  G and B  X  M ,wedefine Using these mappings, we define a concept as follows: a pair ( A , B ) with A  X  G and B  X  M is called Each operator is a Galois connection between the power set lattices on G and M , respectively, hence (
A if and only if A = A . Each object usually belongs to more than one cluster, hence this method is not  X  X risp X  clustering.
 set of maximal elements of C by Max C .

Many algorithms are available for constructing closed set lattices, or concept lattices, and the algo-rithm proposed by Makino and Uno [28] is known to be one of the fastest algorithms. Their algorithm degree of the given bipartite graph, that is, we can use their algorithm directly. For empirical experiments, we use the program LCM [40] provided by the authors to enumerate all concepts and construct the closed set lattice.
 Example 3 Given the following context: ( { { 2 , 4 } ) , ( { x 3.3. Learning classification rules
Here we present the main learning algorithm of SELF in Algorithm 3, which obtains a set of classifi-algorithm of classification in the next subsection.
  X  (
First SELF performs data preprocessing and makes the context ( G , M , I ) from a given table at each (objects). Third it outputs the sets of classification rules such that above procedure for the remaining objects.
 is defined as follows: the rule. Using the weight of rules, label ranking is realized (see the next subsection). Example 4 Given a dataset  X  =( H , X ) and its labels as follows: where Dom ( 1 )= { T , F } X  X  X } ,Dom ( 2 )= { A , B , C } X  X  X } ,andDom ( 3 )= R  X  X  X } . At discretization level 1, we have the following context: x the following context: The right-hand side in Fig. 3 shows the closed set lattice of the above context, and we obtain R 2 = A, and 0 . 5 &lt; x ( 3 ) 0.75, its class label is 1 X . The weight are 2 for both rules.
We show that SELF always stops in finite time if there are no conflicting objects. Namely, for a each object x with  X  ( x ) =  X  must be contained in some consistent concept, and the algorithm stops. as a supervised classification method.
 The time complexity of learning by SELF is O ( nd )+ O (  X  3 N ) such that rules takes less than O ( N ) . 3.4. Classification mixed-type data including labeled and unlabeled data using Algorithms 1, 2, and 3. In this section, we show how to classify a new unlabeled datum using the rules. We assume that such a new datum is given as a table  X  =( H , y ) , where the body y consists of only one tuple.
 it outputs the preference of the label  X  , which is defined as if L . Thus the task of label ranking is achieved by the preference  X  . Moreover, if we pick up the label multiclass classification is also performed directly.
 Example 5 Let us consider the case discussed in Example 4. A tuple y such that tively. A tuple z with satisfies only the rule ( { 1 . F , 2 . A , 3 . 3 } ) , hence  X  ( 1 )= 0and  X  ( 2 )= 2. 4. Experiments
We empirically evaluate SELF. Our experiments c onsist of two parts: one is about multiclass classifi-cation, and the other is about label ranking. 4.1. Methods 4.1.1. Environment
SELF was implemented in R version 2.12.1 [34] and all experiments were performed in the R envi-LCM 2 distributed by Uno [40], which was implemented in C. 4.1.2. Datasets
We collected ten mixed-type datasets from UCI Machine Learning Repository [11]: abalone , allbp , ing algorithms, we ignored all tuples which have missing values since they cannot treat such datasets appropriately.
 4.1.3. Control learning algorithms implemented in R supplied in the tree package [35], SVM with the RBF kernel ( C = 5and  X  = 0.05) in the kernlab package [21], and the k nearest neighbor algorithm ( k = 1 and 5) in the class package. Notice that only the decision tree-based algorithm can treat mixed-type data directly, which is one of typical such learning algorithms. All discrete values were treated as continuous in SVM and k NN. 4.1.4. Evaluation
In classification, for each dataset, the following procedure was repeated 20 times and the mean and was fixed, where the range was from 10 to 100 and 2 to 10, respectively, 2) labeled training data were 4) the accuracy was obtained.

The Eq. (1) was used to determine the most preferable label for each unlabeled datum. If there exists more than two such labels, we chose the smallest one.

We adopted two criteria: correctness and completeness , used in the literature [5] to evaluate partial the normalized difference between the number of correctly ranked pairs and that of incorrectly ranked set of labels L . Assume that  X  is a predicated partial order. Here we define Then, the correctness is defined by  X  completeness of a predicted partial order, we use the completeness defined as follows: The completeness takes a value in [ 0 , 1 ] and should be maximized. 4.2. Results 4.2.1. Multiclass classification
We evaluated SELF in multiclass classification. Specifically, we examined SELF X  X  behavior with re-spect to the number of labeled data and the number of features; the number of labeled data was varied from 10 to 100, and the number of features from 2 to 10. When we fixed the number of labeled data, amined two cases in which the number of labeled data for training were 10 or 100. Such small amount of labeled data is typical in semi-supervised learning; for example, the numbers 10 and 100 were adopted in benchmarks in the literature 3 [50,  X 21].

To analyze effectivity of unlabeled data in the semi-supervised manner, we trained SELF in two ways; of semi-supervised learning methods [50,  X 21].
 For control, three learning algorithms were adopted: the decision tree-based classifier, SVM with the supervised learning and hence did not use unlabeled data in training.

Figures 4 and 5, 6 show the accuracy with respect to changes in the number of labeled data and the number of features, respectively. In every case, the accuracy of SELF was much better than that of SELF (w/o), and the accuracy was g etting better accordin g as the number of labeled data increases. Moreover, SELF X  X  performance is getting better with increase in the number of features. SELF therefore can effectively use unlabeled data and features for learning.

In comparison with the tree algorithm which can treat mixed-type data directly, SELF showed better performance in all datasets in Fig. 4. Moreover, compared to other learning algorithms of SVM and k NN, SELF also achieved the best performance in abalone , anneal ,and horse colic . When the number of labeled data is small (about 10 X 40), SELF outperformed other learning algorithms in all datasets except allbp , as shown in Figs 4 and 5. 4.2.2. Label ranking
We examined effectivity of SELF for label ranking. In consideration of the lack of benchmark data for label ranking, we adopted the following procedure for label ranking: We trained SELF using all labeled data on the respective dataset and obtained the ranking for each datum, and used them as the of SELF, hence their approach is not appropriate to our case.

Figures 7 and 8 show the results of label ranking by SELF with varying the number of labeled data, and Figs 9 X 12 show those with respect to the number of features, where the number of labeled data is 10 for Figs 9 and 10, and 100 for Figs 11 and 12. The correctness of SELF is better than SELF completeness of SELF is much higher than that of SELF (w/o) in most cases. The main reason might be that lots of data are not classified to any class in SELF (w/o). 4.3. Discussion
Our experiments about classification (Figs 4, 5, 6) show that SELF has competitive performance com-pared to other machine learning algorithms, where unlabeled data can be used effectively in training. 12), SELF outperformed SELF (w/o) in most cases in terms of completeness, and the performance got higher with increase of the number of labeled data. Our results therefore show that unlabeled data are also effectively used in SELF in the task of label ranking. 5. Conclusion
We have proposed a novel semi-supervised learning method, called SELF, for mixed-type data in-cluding both discrete and continuous variables, and experimentally showed its competitive performance. semi-supervised learning. Moreover, we can directly treat missing values on SELF, meaning that SELF can be used for various practical datasets. To date, many semi-supervised learning methods use data ground knowledge. Our results with lattice-based data analysis provide new insight to machine learning and knowledge discovery.

There are two future works; one is analysis of SELF from FCA point of view. Refinement of dis-Thereby analysis of mathematical connection between them is a future work. The other is theoreti-cal analysis in the computational learning theory context. de Brecht and Yamamoto [3] have proposed Alexandrov concept space for learning from positive data. Our proposed method might be an instance of framework is also a future work.
 Acknowledgments We would like to thank Marco Cuturi for his helpful comments. This work was partly supported by Grant-in-Aid for Scientific Research (A) 22240010 and for JSPS Fellows 22  X  5714. References
