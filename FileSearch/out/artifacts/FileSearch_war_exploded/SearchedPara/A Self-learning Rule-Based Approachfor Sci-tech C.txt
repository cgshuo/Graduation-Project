 Named entity recognition (NER), as a fundamental task of natural language processing, seeks to locate and classify elements in texts into pre-defined cate-gories, such as the names of persons, organizations, locations etc. However, much less work have been done on the recognition of a special type of named entity, namely sci-tech compound phrase entity recognition (STCPER). Sci-tech com-pound phrase entities (STCPE) are phrases that contain one or more modifiers and known entities in the fields of science and technology. Project names, book names and patent names are typical examples in the type. STCPER is an impor-tant problem in the field of science and technology data processing and discovery, which needs to be addressed urgently in many applications, such as scholar data discovery, open-source intelligence [18], intellectual property protection [17].
In this paper, we describe the STCPER problem as follows: for a given text t that contains a STCPE named p ,wewanttoextractasubstringof t , denoted as  X  p hereafter, that is similar with p as much as possible. Obviously, the best result we want to get is that  X  p and p are the same. As text t has too many substrings, the biggest technique challenge we need to address is how to find the correct answer from so many substrings.

We first observe the structures of STCPE, and conclude three unique charac-teristics. Based on our observations we propose a rule-based approach, which is the first work to address the problem to our best knowledge. The main idea is to exclude noise contents in t gradually, which are located by the characters when describing STCPE. Our approach consists of three stages, namely rule-based text truncation, BlackPOS-based text split and WhiteKey-based confirmation. Our approach is self-learning, because all the rules used in our approach are automatically learning from a training set, without any human involvement. We prove that constructing the best WhiteKey list is NP-hard. We design two algo-rithms, namely the dynamic programming algorithm and the greedy algorithm, to address the problem.

We make three key contributions in this paper. First, we introduce the prob-lem of locating the names of STCPE from web texts, which is the first of its kind to the best of our knowledge. Second, we give an efficient approach to ad-dress the problem. And we argue that our approach is universal and orthogonal to prior NER work, such as HMM-based (Hidden Markov Models) and CRF-based (Conditional Random Fields) approaches. That means prior work can be integrated into our approach to improve the identification of STCPE names. Third, we implemented our algorithm in JAVA and conducted experiments on real-world project name sets and web text sets. Our experimental results show that our approach achieves 94.78% precision rate, 89.19% recall rate and 91.9% F 1 measure in average. Research on NER starts at 1990s. In 1991, Rau first described the concept of information extraction and recognition, which implements heuristic algorithms and manually managed rules to extract targeted information. In 1996, the NER problem was import as track in MUC-6. In the following competition such as MUC-7, CoNLL-2002 and CoNLL-2003, Chinese NER was introduced. English NER does not need word segmentation, which makes it easy to achieve better result. According to the MUC and ACE tracks record, English NER can achieve as high as 90% precision rate, recall rate and F-measure.

Although much NER work have been proposed, little of them try to address the STCPER problem. Prior NER work mainly focus on identifying the names of persons, organizations, locations etc. , which can be classified into two categories: rule-based approaches and stochastic-based approaches.
 Rule-Based Approaches. [3,8,11,5] utilize pattern-matching techniques as well as heuristics derived either from the morphology or the semantics of the input sequence. The conceit of handling NER is to look for clues within the structure and grammars of the text which is the prompt of some named entities. These insights can be used as classifiers in machine-learning approaches, or as candidate taggers words in gazetteers. Some applications can also make effective use of stand-alone rule-based systems, but they are prone to be both overreach and skipping over named entities. Nadeau et al. provide a table of word-level features to recognize a person X  X  name or etc. in [6]. For example, the word is capitalized or all capital letters (or indeed, includes a mixed case), punctuation, morphology. Features like these, especially capitalization, form the basis for most heuristics. Word-level features can then be combined with contextual features to further identify names, called sure-fire rules as described in [12], which rely on contextual trigger words either prepe nded or appended to a series of capitalized tokens, whose Part Of Speech (POS) is unknown or are generically referenced as nouns.

Stochastic-based approach is the current dominant technique for address-ing the NER problem. Stochastic-based techniques include Hidden Markov Mod-els (HMM) [1], Decision Trees [13], Maximum Entropy Models (ME) [2], Support Vector Machines (SVM) [15], and Conditional Random Fields (CRF) [10]. These work are all variants of the stochastic-based approaches that typically consist of a system that reads a large annotated corpus, memorizes lists of entities, and creates disambiguation rules based on discriminative features.

Besides the above work, Cucerzan et al. [4] presented a large-scale system for the recognition and semantic disambiguation of named entities based on informa-tion extracted from Wikipedia and Web search results. Mann and Yarowsky [9] developed an unsupervised method for personal name disambiguation that ex-ploits a rich set of biographic features, such as birth date, birth place and others.
Our approach is universal and orthogonal to prior NER work. Thus we can integrate prior work into our approach to improve the precision rate and the recall rate of the STCPER problem. The approach we explore in this paper is illustrated in Figure 1. Our STCPER process consists of three stages. First, we use linguistic cues to truncate the input text t ; then, we split the substring outputs of the first stage by BlackPOSes; in the final stage, we use a white list of STCPE keywords to confirm whether the output substrings of the second stage are STCPE or not.

In this section, we first show our observations on the characteristics of STCPE, which are the starting points of our apporach. Then we describe the above three stages in detail in the following three subsections. At last, we introduce how to automatically generate the black list of STCPE POSes and the white list of STCPE keywords. 3.1 Characteristics of STCPE We observe that STCPE have three novel characteristics that are significantly different from traditional named en tities, such as personal names. Multi-entities Compound: each STCPE consists of multiple noun entities Limited Missing POSes: We observe that the number of the types of Part-Multiple Indicative Keywords: We note that people usually follow strong 3.2 Rule-Based Text Truncation From a linguistic view, the names of STCPE in the texts of natural language are expressed either implicitly or explicitly. The difference is that implicit STCPE names are expressed without obvious cue phrases. In order to reduce the in-fluence of unrelated text segments in cl assifying STCPE names, we use explicit connective markers as linguistic cues to truncate the original input text. Figure 3 shows an example that describes the specific details for three different projects respectively. For the example in Figure 3, the linguistic cues are  X  X on [*] award X ,  X  X as awarded X  and  X  X as reached [*] level X  respectively.

According to prior research experien ces, we focus on the most frequent and least ambiguous cues. The rules used in our work to truncate texts are written by Perl Compatible Regular Expressions (PCRE). We use these linguistic cues to find texts that we are interested in, and use the capturing parentheses and back references functions of PCRE to capture the text segments that are likely to contain STCPE names. In this way, we can use a few PCRE rules to achieve our goal by the help of the powerful expressive, efficient, and flexible capacity of PCRE. And we do not need to enumerate each linguistic cue in a sperate rule.
Obviously, the recall of texts that contain STCPE names can be improved if we use more comprehensive and more rel axed cues. However, one thing should be noted that using more comprehensive and more relaxed cues will retrieve more texts that do not contain any STCPE name mistakenly at the same time. 3.3 BlackPOS-Based Text Split This stage is motivated by the limited missing POSes characteristic. We call these missing POSes BlackPOSes which means that they are  X  X rohibited X  to appear in STCPE names. All the BlackPOSes constitute a black list of STCPE POSes. However, BlackPOSes may frequently appear in web texts. Thus, we can shorten the substring outputs of the rule-based text truncation step further to make  X  P more similar with P in the following method: we can split the substring outputs of the rule-based text truncation step by BlackPOSes, and use the split results as the substring outputs of the BlackPOS-based text split step, which are the inputs of the next and last step, namely WhiteKey-based confirmation.
Note that we do not use keywords to split texts here, because the keywords that appear and not appear in STCPE names are both huge. It is impossible to enumerate them.

The detail of choosing which POSes as BlackPOSes will be described in Sec-tion 3.5. 3.4 WhiteKey-Based Confirmation The main idea of our WhiteKey-based confirmation step is based on the multi-ple indicative keywords characteristic. M otivated by the characteristic, we can choose some keywords as Whitekeys and use them to constitute a white list of STCPE keywords. If a substring output of the BlackPOS-based text split step contains more than one word in the white list, we regard the substring as a STCPE name.

Note that we need to exclude the exist ence of noise words to avoid making a mistake. For example, if the substring is  X  X enior researcher Alex will give us a technical report tomorrow X , and word  X  X  esearch X  is exactly a WhiteKey, then we will mistakenly regard the substring as a STCPE name. We can use the negative lookahead assertion function of PCRE to achieve the goal. The PCRE for the above example is  X  X esearch(?!er) X .

The detail that which words are consid ered as WhiteKeys will be described in the next subsection (Section 3.5). 3.5 BlackPOS and WhiteKey Generation There are still two open problems to be addressed in our approach: how to generate the black list of STCPE POSes and the white list of STCPE keywords. The input of the two problems are the same: a training set that consists of the names of n STCPE. We denote the training set as P = { p 1 , p 2 ,  X  X  X  , p n } ,where p isthenameofthe i -th STCPE.

The the first problem can be addressed in a simple and intuitive way: we conduct word segmentation and POS t agging on each STCPE name and get the black list which is composed of all POSes that do not appear in any tagging result. We admit that the black list may contain POSes that appear in the tagging results of STCPE names that are not in the training set P , especially when P is small. However, we argue that the black list may be accurate enough as long as P is large enough. Moreover, we can seek the help of linguists to adjust the black list.

To address the second problem, we also need to conduct word segmentation on each STCPE name first. Note that each keyword is a segmented word. As there are lots of segmented words, how to select some appropriate members from them to constitute the white list are the biggest challenge to be addressed. On one hand, we want the white list has as few words as possible, as more words mean much more substrings of input texts will be identified as STCPE names mistakenly. On the other hand, we want these selected keywords cover as many STCPE as possible, where a STCPE name i s covered if at least one member in the white list appears in the STCPE name. Because more STCPE are covered means much more substrings that are really STCPE names will be correctly identified. These two goals are unfortunately conflicting. With the white list consisting of all segmented words, all STCPE names in the training set are covered, but the white list is the biggest. With the white list being empty, the white list is the smallest, but it cannot out identify any STCPE name.

For the given training set P = { p 1 , p 2 ,  X  X  X  , p n } , we can get m words after word segmentation, which constitute a set W = { w 1 , w 2 ,  X  X  X  , w m } ,where w i is the i -th word. The problem of selecting W ords with the LeAst Number (denoted as WLAN problem hereinafter) can be defined as follows: we want to find a minimum subset S  X  W that satisfies the following two limitations at the same time. First, S covers P , which means that each STCPE name in P is covered by at least one word in S . Second, | S | is the least where | X | represents the number of members in set X . The following Theorem 1 shows the computing complexity of WLAN problem.
 Theorem 1. The WLAN problem is NP-hard.
 Proof. For each word w  X  W , we can construct a set A ( w )  X  P which consists of all STCPE names that are covered by w . Then we can infer that A ( W )= { the WLAN problem becomes the classi cal minimum set cover problem: find a subset AS  X  A ( W ) such that every element in P belongs to at least one member of AS and minimize | AS | , as shown in [16].

In this paper, we use the dynamic programming strategy to obtain the opti-mal result and use the greedy strategy to obtain a near-optimal result. Before describing the two algorithms, we first introduce some symbols. We can con-struct a m  X  n boolean matrix M , where each row represents an element in W andeachcolumnrepresentsanelementin P .Ifthe i -th word w i appears in the j -th STCPE name p j ,then M ij =1;otherwise M ij = 0. We define a new binary operator  X   X  with a boolean matrix as the first argument and a word as the second argument. The operational result is another binary matrix got by re-moving the corresponding rows of the second argument from the first argument, and only retaining these columns that are not covered by the second argument. For example, assuming P = { p 1 , p 2 , p 3 } , four words will be obtained after word segmentation: W = { w 1 , w 2 , w 3 , w 4 } ,and M is a 4  X  3 boolean matrix as shown in Figure 4. The output of M w 2 is a 3  X  2 boolean matrix: the w 2 row and the p 1 column are removed from
We use the function F with a boolean matrix as the input to get the optimal result of the WLAN problem. Then we can infer that
Based on Equation 1, we can design a dynamic programming algorithm to get the optimal result of WLAN problem.

We can also design a greedy algorithm to get a near-optimal result. Initially, we set M = M . For the current boolean matrix M , we choose the word w that has the biggest number of value  X 1 X  in the corresponding row of M ,andset M = M w . The above step repeats until M becomes a 0  X  0 boolean matrix. For the example in Figure 4, the output of our greedy algorithm is { w 3 } ,which is also the optimal result of the corresponding WLAN problem.

Note that selecting words of the least number directly may not be the best strategy for different training sets. Next , we try to define the problem of selecting Words with a given Fixed Number k (denoted as k -WFN problem hereinafter) and discuss how to choose an appropriate value k .

The k -WFN problem can be defined as follows: we want to find a minimum subset S  X  W that satisfies the following three limitations at the same time. First, S covers P , which means that each STCPE in P is covered by at least one word in S . Second, | S | X  k .Third, w namely the goal is to select no more than k indicative words that appear in as many STCPE names as possible.

In this paper, we also use the dynamic programming strategy to obtain the optimal result and use the greedy strategy to obtain a near-optimal result. We use the function H with a boolean matrix and an integer as the inputs to get the optimal result of the k -WFN problem. We can infer that: where H ( M ,k  X  1 | w i )=max {| A ( w i ) | + H ( M w i ,k  X  1) | 1  X  i  X  m } , means that the word w i must be selected. Note that H ( M ,k )mustbemorethan H ( M ,k  X  1) for k -WFN problem when k is no more than the number for rows in
M and no less than F ( M ). Thus, we know that max {| A ( w 1) | 1  X  i  X  m } must be more than H ( M ,k  X  1), otherwise H ( M ,k ) will be equal to H ( M ,k  X  1). Then Equation 2 becomes as follows:
Based on Equation 3, we can design a dynamic programming algorithm to get the optimal result of k -WFN problem.

We can also design a greedy algorithm to get a near-optimal result of k -WFN problem. Note that using the same greedy strategy (as shown in the above greedy algorithm of WLAN problem) directly does not generate to give a result that satisfies the above three limitations at the same time. The result may have bigger than k members even if the greedy algorithm terminates when it finds that the first limitation is satisfied. Thus we design a new greedy algorithm for k -WFN problem. First, we try to obtain less than k words to constitute S which can satisfy the first two limitations at the same time. Then, we add some other words into S to maximize w be satisfied, namely | S | X  k .
 Next, we discuss how to choose an appropriate value for k . We can think H ( M ,k +1)  X  H ( M ,k ) as the marginal benefit of adding a new word into set S of k elements. Theoretica lly, the marginal benefit H ( M ,k +1)  X  H ( M ,k ) should decrease with the increase of k . Thus, we introduce a marginal benefit threshold  X  , and we want to choose value k such that H ( M ,k +1)  X  H ( M ,k ) &lt; X  and H ( M ,k )  X  H ( M ,k  X  1)  X   X  . 4.1 Experimental Setup Project names are typical STCPE names. I n our experiments, we evaluate our approach on real-world project name se ts and web text sets that are collected form the Internet.

The training set used to generate the white list of project keywords and the black list of project POSes consists of 1119 project names. Each element in the training set is the name of a real project supported by the National Natural Science Foundation of China in 2014. All the 1119 projects are undertaken by 8 universities.

We collect 1522 project names that each project won the National Science and Technology Progress Award between 2005 and 2014. We take advantage of search engines to retrieve web texts. Ea ch search uses a different project name as the search keyword. Finally, we get 734 web texts that each of them contains the project name used to search it. We a lso collect 766 web texts that each of them do not contain any project name from Sogou Lab Data [14]. All the 1500 (734+766) web texts consist of text set T .
 We evaluate our approach on the metrics of precision rate, recall rate and F-measure. We denote the i -th text in T as t i , and denote the corresponding project name as p i , and denote the project name that our approach gives for t i as  X  p i .Fortext t i , its precision rate is defined as the ratio of the length of the longest common s ubstring between p i and  X  p i and the length of  X  p i , denoted as and  X  p i ; its recall rate is defined as the ratio of the length of the longest common and Re ( t i ) = 0. Then for text set T , its precision rate is defined as the ratio of the total precision rates of its elements and the size of T ,namely and its recall rate is defined as the ratio of the total recall rates of its elements and the size of T ,namely F-measure is the harmonic mean of precision and recall. The general formula of F-measure for positive real  X  is: F  X  =(1+  X  2 )  X  Pr  X  Re  X  2  X  Pr + Re .Inthispaper,we use F 1 =2  X  Pr  X  Re Pr + Re to evaluate our approach, as the way used in many papers.
We implement our approach in Java. All the word segmentation and POS tagging operations are conducted with the widely-used ICTCLAS [7] tool. 4.2 Effect of Our Approach For the training set of 1119 project names, there are totally 61 different kinds of POSes appear in the elements of the training set. We know that there are 96 different kinds of POSes in ICTCLASS. Thus 35 POSes do not appear in any of the 1119 project names. In our experiments, we use these 35 POSes to constitute the black list.
 After word segmentation, the 1119 project names are divided into 3397 words. In our experiments, we remove the stop words form the 3397 words. We also remove the words of length 1, in order to avoid the white list being over-matched. Our experimental result shows the least number of words required to cover the training set is 72. Figure 5 shows the change of H ( M ,k +1)  X  H ( M ,k ) with the increase of k where k  X  72. The values of H ( M ,k +1)  X  H ( M ,k )for different k are got by our greedy algorithm. From Figure 5 we can find that: 1) H ( M ,k +1)  X  H ( M ,k ) is always bigger that 0, namely H ( M ,k + 1) is bigger than H ( M ,k ) for any k that is less than the number of rows in M ; 2) the bigger the value of k , the smaller the marginal benefit we can get. If the marginal benefit threshold  X  is set to be 30, then the value of k is 102.

We use the 35 BlackPOSes and 102 WhiteKeys to perform our project name recognition. Our experimental result shows that our approach can achieve 90.97% precision rate, 77.9% recall rate and 83.93% F 1 measure over the 734 web texts that each text contains a predefined project name. Note that the recall rate is a little low, because of the following two reasons. First, the white list of project keywords is incomplete, as the training set does not contain all kinds of project names, especially when the training set and the testing set come from different sources. Second, the results of POS tagging are not entirely accurate, thus some project names are fortunately cut off by the black list of project POSes.
Experimental results show that our approach can achieve 98.43% precision rate, 100% recall rate and 99.21% F 1 measure over the 766 web texts that do not contain any project name. The primary reason that the precision rate is close to but not 100% is that some substrings contain the elements in the white list of project keywords, and are identified as project names.

Overall, our approach achieves 94.78% precision rate, 89.19% recall rate and 91.9% F 1 measure over the set T of 1500 web texts. In this paper, we propose the first work to address the problem of recognizing sci-tech compound phrase entities. The process of our STCPER approach consists of three phrases. First, we use linguistic cues to truncate the input text; then, we split the substring outputs of the first stage by BlackPOSes; in the final stage, we use a white list of STCPE keywords to confirm whether the substring outputs of the second stage are STCPE names or not. We reduce the problem of generating the white list of STCPE keywords into the classical minimum set cover problem. And the we design two algorithms, namely the dynamic programming algorithm and the greedy algorithm, to address the problem. All the rules in our approach are automatically learning from the training set without any human involvement. Experimental results on real-world project names and web texts shows that our approach achieves quite high precision rate and recall rate on this complex problem.

Our approach is universal and orthogonal to prior NER work. In the future, we will integrate HMM-based NER approach and CRF-based NER approach into our work to improve the precision rate and recall rate of identifying STCPE names. Mover, we will extend our work from multiple views, such as the depen-dency trees of STCPE names.
 Acknowledgement. Supported in part by the Strategic Priority Research Pro-gram of the Chinese Academy o f Sciences under G rant No. XDA06030200.
