 In this paper, we address the problem of expert finding in commu-nity question answering (CQA). Most of the existing approaches at-tempt to find experts in CQA by means of link analysis techniques. However, these traditional techniques only consider the link struc-ture while ignore the topical similarity among users (askers and answerers) and user expertise and user reputation. In this study, we propose a topic-sensitive probabilistic model, which is an ex-tension of PageRank algorithm to find experts in CQA. Compared to the traditional link analysis techniques, our proposed method is more effective because it finds the experts by taking into account both the link structure and the topical similarity among users. We conduct experiments on real world data set from Yahoo! Answers. Experimental results show that our proposed method significantly outperforms the traditional link analysis techniques and achieves the state-of-the-art performance for expert finding in CQA. H.3.3 [ Information Systems ]: Information Storage and Retrieval -information filtering , selection process ; H.3.5 [ Information Sys-tems and Applications: ]: Web-based services Algorithms, Experimentation, Performance Expert Finding, PageRank, Yahoo! Answers
Community question answering (CQA) is a particular form of online service for leveraging user-generated content, which has gained increasing popularity in recent years. These online services, such as Yahoo! Answers 1 and Live QnA 2 , provide a platform for users to ask and answer questions. http://answers.yahoo.com/ http://qna.li ve.com/
Unfortunately, the quality of answers has high variance: ranging from very high to low quality, sometimes abusive content or even spam [1]. Although CQA provides many mechanisms for com-munity feedback ( X  X humbs up" and  X  X humbs down" votes), such community feedback requires some time to accumulate, and often remains sparse for obscure or unpopular topics. We analyze a large sample of Yahoo! Answers data, fewer than 20% of all questions have any user votes cast for any of the answers. Therefore, it is desirable to automatically find experts in CQA, so as to route the newly posted questions to the appropriate experts, who can pro-vide high quality answers to these questions [3, 7, 14]. Finally, the overall answer quality can be substantially improved.

Expert finding in CQA is the task of finding users who can provide a large number of high quality, complete, and reliable an-swers [18], which has recently gained a wide interest in NLP and IR communities [3, 7, 8, 24]. These existing approaches identify the experts by means of link analysis techniques such as PageR-ank [16] and HITS [9], or their variants. However, the traditional link analysis techniques only consider the link structure while ig-nore the topical similarity between askers and answerers .
In this paper, we propose a topic-sensitive probabilistic model for expert finding in CQA. Given a set of users in CQA, we first automatically distill the topics that users are interested in by ana-lyzing the content of their asked questions and answered questions. Based on the topics distilled, topic-sensitive question-answer rela-tionships between askers and answerers are constructed. Then, we measure the expert saliency score by taking into account both the link structure and the topical similarity between askers and answer-ers.

To the best of our knowledge, it is the first extensive and empiri-cal study of expert finding in CQA by taking into account both the link structure and the topical similarity between askers and answer-ers. Although topical similarity information has been proved very effective for web search [6, 15, 23], our goal is to capture the topi-cal similarity between askers and answerers rather than to calculate the topical similarity of web contents. We focus on a substantially different task and model formulation. To date, little work has been made regarding topical similarity among users in studies of expert finding in CQA, which remains an under-explored research area. This paper is thus designed to fill the gap. Specifically, we make the following contributions: Figur e 1: The graphical model for the user-topic model using plate notation.
The rest of this paper is organized as follows. Section 2 presents our proposed method. Experimental results are presented in Sec-tion 3. Finally, we conclude with ideas for future work in Section 4.
Topic distillation aims to automatically identify the topics that users (askers and answerers) are interested in based on the user profiles. 3 Because our data set is large, it is only feasible to use fully unsupervised or weakly supervised methods to automatically discover topics. In this paper, we use the widely studied topic model  X  X  X  Latent Dirichlet Allocation (LDA) [2] to identify the la-tent topic information from the large scale question-answer collec-tion.

Although we could also apply LDA to distill the topics from questions by treating each question as a single document, this di-rect application would most likely not work well because questions are very short (average 11.2 words for each question), often con-taining only a single sentence [25]. To overcome this difficulty, we propose a user-topic model shown in next subsection.
Figure 1 illustrates the generative process with a graphical rep-resentation of user-topic model. For readers not familiar with plate notation, shaded and unshaded variables indicate observed and la-tent variables, respectively. An arrow indicates a conditional de-pendency between variables and plates (the boxes in the figure) indicate repeated sampling with the number of repetitions given by the variable in the bottom. In our user-topic model, D in the figure refers to all user profile, and N d refers to the number of words in a specific user profile. From the figure, we can see that the main characteristic of user-topic model is that it is user-centric in a general generative manner. Each user is considered as a pseudo-document which represents the user profile. Each user is associated with a multinomial distribution over topics, represented by  X  . Each topic is associated with a multinomial distribution over words, rep-resented by  X  . The multinomial distribution  X  and  X  have a sym-metric Dirichlet prior with hyperparameters  X  and  X  .

It is interesting to note the difference between our user-topic model and the previous author-topic model in [20, 22]. In author-topic model, the authorship of an arbitrary word in the multi-author
Here, a user profile refers to the questions asked and answered by the user. document is not known so that author-topic model assumes a uni-form contribution of all documents authors. However, in our prob-lem setting, the user of each user profile in CAQ is explicitly rep-resented. In fact, the user information of word is important to pre-cisely identify user interests.
The user-topic model includes two sets of parameters  X  X  X  the K user-topic distributions  X  , and the T topic distribution  X  as the latent variables corresponding to the assignments of individ-ual words to topics z and user u . The Expectation-Maximization (EM) algorithm [4] is a standard technique for estimating param-eters. However, this method is susceptible to local maxima and computationally inefficient [2]. We use an alternative parameter estimation strategy, proposed by Griffiths and Steyvers [5], using Gibbs sampling. Instead of estimating the parameters directly, we evaluate the posterior distribution on just u and z and then use the result to infer  X  and  X  . For each word, the topic and user assign-ment are sampled from: where z i = j and u i = k represent the assignments of the i th word in the user profile to topic j and user k respectively, w represents the observation that the i th word is the i th word in the lexicon, and z  X  i , u  X  i represent all topic and user assignments not including the i th word. Furthermore, C W T wj is the number of times word w is assigned to topic j , not including the current user profile, and C UT kj is the number of times user k is assigned to topic j , not including the current user profile, and |V| is the size of the lexicon.
After parameter estimation, the algorithm only needs to keep track of a |V| X  T (word by topic) count matrix, and a K  X  (user by topic) count matrix, both of which can be represented effi-ciently in sparse format. From these count matrices, we can easily estimate the word-topic distributions  X  and user-topic distribution  X  as follows: where  X  wj is the probability of using word w in topic j , and  X  the probability of using topic j by user k . These values corresponds to the predictive distributions over new words w and new topics z conditioned on w and z .

In these two matrices, we can row normalize user-topic matrix as  X   X  such that  X   X   X  k  X  1 = 1 for each row  X   X  k . Each row of matrix  X  is the probability distribution of k  X  X  interest over the T topics, e.g., each element  X   X  kj denotes the probability that user k is interested in topic j ( p ( j | k ) =  X   X  kj ) . Based on the topics distilled in subsection 3.1, a directed graph G = ( V, E ) is formed with the topic-specific question-answer re-lationships among users. V is a set of nodes representing users (askers and answerers). A directed edge e  X  E where e = ( u u  X  V and u j  X  V , indicates that user u j answers the questions of user u i . Each edge e ij  X  E is associated with an affinity weight f ( i  X  j ) between u i and u j . The weight is computed as follows: where Q ( i ) is the set of questions asked by u i , A ( j ) is the set of questions answered by u j . Two users are connected if their affinity weight is larger than 0 and we let f ( i  X  i ) = 0 to avoid self transition. 4
The transition probability from u i to u j is then defined by nor-malizing the corresponding affinity weight as follows: where p ( i  X  j ) is usually not equal to p ( j  X  i ) , and j ) = 1 . We use the row-normalized matrix f M = [ f M ij ] describe G with each entry corresponding to the transition proba-bility.
In order to make the graph fulfill the property of being aperiodic and f M be a stochastic matrix, the rows with all zero elements are replaced by a smoothing vector with all elements set to 1 /
Based on the matrix f M , the saliency score R ( u i ) for u deduced from those of all other users linked with it and it can be formulated in a recursive manner as in the PageRank algorithm. where  X   X  [0 , 1] is a damping factor. The damping factor indicates that each vertex has a probability of (1  X   X  ) to perform random jump to another vertex within this graph. The saliency score are obtained by running equation (7) iteratively until convergence.
In equation (7), the second term is set to be the same value 1 / |
V | for all vertices within the graph, which indicates that there are equal probabilities of random jump to all vertices. However, Haveliwala [6] and Nie et al. [15] proposed a topical PageRank-like algorithm (TPR) and argued that the second term in equation (7) should be set to be non-uniformed. The assumption is that if we assign larger probabilities to some vertices, the final saliency score will prefer these vertices.
 The idea of TPR is to run PageRank for each topic separately. Each topic-specific PageRank prefers those users with high rele-vance to the corresponding topic. Formally, for a specific topic z , we will assign a topic-specific preference value p z ( u ) to each user u as its random jump probability are interested in topic z will be assigned larger probabilities when performing the PageRank. Given a topic z , the TPR-like saliency score are defined as follows:
The setting of preference value p z ( u i ) in equation (8) will have great influence to TPR. In this paper, we set p z ( u i ) = p ( z  X  . A large R z ( u i ) indicates a user u i is a good candidate expert in topic z .
The TPR ignores the topical similarity among users when setting the affinity weight. The affinity weight is set by counting the num-ber of questions answered by the two users for a given user X  X  asked In CQA, the users cannot answer their own questions.
 questions. For example in Figure 2, the nodes in the figure corre-spond to users (askers and answerers) and the directed edges rep-resent the question-answer relationships, where | Q ( a ) 500 , | Q ( a )  X  A ( c ) | = 300 . In this case, the transition probabil-ity from u a to u b is 1.67 times of that of u a to u c . As a result, u may get the higher ranks than u c by this topical similarity-free affinity weight although u b is not interested in topic  X  X offee mak-ing". In other words, topical similarity-free propagation may cause the scores to be off-topic.

In this paper, we propose a topic-sensitive random surfer model (TSPR) by considering the topical similarity among users when set-ting the affinity weight. The topic-sensitive random surfer model on graph G computes the expert as follows: the random surfer vis-its each user with certain probability by following the appropriate edge in G . The topic-sensitive propagation method differentiates itself from PageRank and PTR in that the random surfer performs a topic-sensitive random walk (e.g., the transition probability from one user to another is topic-sensitive). By doing so, we can es-sentially construct a topic-sensitive question-answer relationships between askers and answerers. Given a topic z , the transition prob-ability from the asker u i to the answerer u j is defined as: p z ( i  X  j ) = where u to u j in topic z . In this paper, we propose to use normalized Kullback Leibler (KL) divergence [10], which is an asymmetric measure. The KL-divergence from u i to u j in topic z is computed we calculate sim z ( i  X  j ) as follows:
The larger sim z ( i  X  j )  X  [0 , 1] , the more similarity from u and u j in topic z .

Then the new row-normalized matrix f M  X  is defined as follows:
Given a topic z , the final TSPR-like saliency score is computed based on the following iterative form:
For implementation, the initial scores of all users are set to 1 and the iteration algorithm in equation (12) is used to compute the new scores of the users. Usually the convergence of the iteration is achieved when the difference between the scores computed at two successive iterations for any users falls below a given threshold ( 0 . 00001 in this paper).

After ranking the users by using the TSPR or other methods, we select top N users for each topic as topical candidate experts. In this paper, we empirically set N to 1, 5, and 10 shown in the experimental section.
Yahoo! Answers web service supplies an API to allow web users to crawl the existing question answer archives and the correspond-ing user information from the website. We crawl the data set from Yahoo! Answers, the data set consists of 237,083 resolved ques-tions, and 593,107 answers posted by 286,053 users. In this pa-per, for all resolved questions, the information of each question includes: (1) Texts of question and the associated answers, with stop words being excluded 5 and the words being stemmed. 6 (2) User X  IDs of all questions and answers. (3) Users X  rating information (e.g., thumbs up, thumbs down, the best answers and so on.).

Since there is no available benchmark for expert finding for a given topic in CQA, we manually inspect the expert finding results. For each candidate expert u for topic z , we ask two annotators to check whether u is a real expert for the given topic. In this process, the annotators are given the top topic words and user profile. Each identified expert is voted by two annotators with label Yes (the user is a real expert for the given topic) or No (the user is not a real ex-pert for the given topic). If a conflict happens, a third person will make judgement for the final result. The Cohen X  X  Kappa coeffi-cients of the T topics range from 0.51 to 0.77, showing fair to good agreement.
To evaluate the performance of expert finding, we use the three widely studied metrics in information retrieval.

Mean Average Precision (MAP): This metric is the mean of the average precision scores for each topic.

Mean Reciprocal Rank (MRR): This metric is the multiplicative inverse of the rank of the first retrieved expert for each topic.
Average Precision@n (Avg. P@n): This metric denotes the av-erage ratio of the relevant experts in top n identified experts for each topic.
We have several parameters: i.e., Dirichlet hyper-parameters  X  ,  X  , topic number T , damping factor parameter  X  used in PageRank; In this paper, we set Dirichlet priors  X  = 50 /T , and  X  = 0 . 05 as Griffiths and Steyvers [5]. We run LDA with 200 iterations of Gibbs sampling. After trying a few different numbers of topics, we empirically set T = 15 . We choose these parameter settings because they give coherent and meaningful topics for our data set.
For parameter  X  , we conduct an experiment on a small develop-ment set to determine the best value among 0 . 1 , 0 . 2 , terms of MAP. This set is also extracted from Yahoo! Answers, and it is not included in the evaluation set (described in subsection 4.1). We find that  X  = 0 . 2 is the optimal parameter for PR, TPR and TSPR. http://truereader .com/manuals/onix/stopwords1.html http://tartarus.org/martin/PorterStemmer/
T able 1: Comparison of expert finding for different methods.
To demonstrate the effectiveness of our proposed TSPR method, comparisons against some previous work are also included:
Table 1 presents the comparison of expert finding for different methods. From this table, we can find that our proposed method significantly outperforms all previous works (row 1, row 2, row 3, row 4, row 5, and row 6 vs. row 7). 7 The results show the effective-ness of the propose method by considering the topical similarity among users, user expertise score and reputation score. Besides, we also note that incorporating the topical preference value into PageRank, the performance can be further improved (row 1 vs. row 6).
In order to further evaluate the effectiveness of our approach, we look at the identified experts and manually evaluated their answers. We expect experts to provide high quality answers for their inter-ested topics, thus answer quality is an indirect evaluation metric. In this paper, we use the quality metric described in Agichtein et al. [1] as the  X  X old standard" for evaluation. This metric is the con-fidence score of a binary classifier trained on high and low quality instances. The value of quality score is always between 0 and 1. To avoid manually labeling, we adopt the community and askers X  choices used in Li and King [12] to automatically construct a large number of  X  X igh quality" and  X  X ow quality" instances.

Figure 3 shows the average answer quality scores provided by the identified top 10 experts in three specific topics. As we can see
W e perform a significant t -test. The comparisons between our method and previous works are significant at p &lt; 0 . 05 . Figur e 3: Average answer quality scores provided by the iden-tified experts in three topics. from this figure, the average answer quality scores of experts using our proposed methods for each topic are generally between 0.74 and 0.83, which is a relatively high quality score . These results represent another source of confirmation concerning the suitability of our approach for finding the experts that contribute significantly to generate high quality answers in CQA. Moreover, such results also indicate that askers are very selective in choosing experts. We can thus recommend the open questions to these experts and en-hance the overall quality of content in CQA.
In this paper, we propose a topic-sensitive probabilistic model for expert finding in CQA. Compared to the traditional link anal-ysis techniques, our proposed method is more effective because it finds the experts by taking into account both the link structure and the topical similarity between askers and answerers. We conduct experiments on real world data set from Yahoo! Answers. Exper-imental results show that our proposed method significantly out-performs the traditional link analysis techniques and achieves the state-of-the-art performance.
 There are some ways in which this research could be continued. First, we will investigate the proposed method to the full system of CQA (26 categories) or other kinds of data set (e.g., forums and FAQ sites). Second, users X  relative saliency scores change over time, so it is necessary to take into account the temporal dimension of questions and answers.
This work was supported by the National Natural Science Foun-dation of China (No. 61070106), the National Basic Research Pro-gram of China (No. 2012CB316300), Tsinghua National Labo-ratory for Information Science and Technology (TNList), Cross-discipline Foundation and the Opening Project of Beijing Key Lab-oratory of Internet Culture and Digital Dissemination Research (No. 5026035403). We thank the anonymous reviewers for their insight-ful comments. [1] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and G. [2] D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet [3] M. Bouguessa, B. Dumoulin, and S. Wang. 2008. Identifying [4] A. P. Dempster, N. M. Laird, D. B. Rubin. 1977. Maximum [5] T. Griffiths and M. Steyvers. 2004. Finding scientific topics. [6] T. H. Haveliwala. 2002. Topic-sensitive pagerank. In WWW . [7] P. Jurczyk and E. Agichtein. 2007. Discovering authorities in [8] W. Kao, D. Liu, and S. Wang. 2010. Expert finding in [9] J. Kleinberg. 1999. Authoritative sources in a hyperlinked [10] S. Kullback and R. A. Leibler. 1951. On information and [11] J. Lafferty and C. Zhai. 2003. Probabilistic relevance models [12] B. Li and I. King. 2010. Routing questions to appropriate [13] Z. Liu, W. Huang, Y. Zheng, and M. Sun. 2010. Automatic [14] J. Liu, Y. -I. Song, and C. -Y. Lin. 2011. Competition-based [15] L. Nie, B. D. Davison, and X. Qi. 2006. Topic link analysis [16] L. Page, S. Brin, R. Motwani, and T. Winograd. 1998. The [17] A. Pal and S. Counts. 2011. Identifying topical authorities in [18] A. Pal and J. Konstan. 2010. Expert identification in [19] I. Porteous, D. Newman, A. Ihler, A. Asuncion, P. Smyth, [20] M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth. 2004. [21] D. Schall and F. Skopik. 2011. An Analysis of the Structure [22] M. Steyvers, P. Smyth, and T. Griffiths. 2002. Probabilistic [23] J. Weng, E. -P. Lim, J. Jiang, and Q. He. 2010. TwitterRank: [24] J. Zhang, M. Ackerman, and L. Adamic. 2007. Expertise [25] G. Zhou, L. Cai, J. Zhao, and K. Liu. 2011. Phrase-based
