 1. Introduction
Total suspended solids (TSS) are considered to be one of the major pollutants that contributes to the deterioration of water quality, contributing to higher costs for water treatment, decreases in fish resources, and the general aesthetics of the water ( Bilotta and Brazier, 2008 ). The activities associated with wastewater treatment include control of water quality, protection of the shoreline, and identification of economic life of protective structures ( Mamais et al., 1998 ). Predicting suspended sediments is important in controlling the quality of waste water. TSS is an important parameter, because excess TSS depletes the dissolved oxygen (DO) in the effluent water. Thus, it is imperative to know the values of influent TSS at future time horizons in order to maintain the desired characteristics of the effluent.

Industrial facilities usually measure the water quality para-meters of their influents two or three times a week, and the measurements include CBOD, pH, and TSS ( Choi and Park, 2002 ; Cartensen et al., 1996 ). Thus, the infrequently recorded data must be modified to make it suitable for time-series analysis. Sufficient associated parameters must be available to develop accurate TSS prediction models. Wastewater treatment involves complex phy-sical, chemical, and biological processes that cannot be accurately represented in paramedic models. Understanding the relationships among the parameters of the wastewater treatment process can be accomplished by mining the historical data. A detailed description of various waste water treatment plant (WWTP) modeling approaches is described in Gernaey et al. (2004) . Their review work is mainly focused on application of white-box modeling, and artificial intelligence to capture the behavior of numerous WWTP processes. Poch et al. (2004) developed an environmental decision support system (EDSS) to build real world waste water treatment processes. In another research, Rivas et al. (2008) utilized mathe-matical programming approach to identify the WWTP design parameters.
 Data-mining algorithms are useful in wastewater research.
Examples of data-mining applications reported in the literature include the following: (1) prediction of the inlet and outlet biochemical oxygen demand (BOD) using multi-layered percep-trons (MLPs), and function-linked, neural networks (FNNs) ( Patricia et al., 2004 ); (2) modeling the impact of the biological treatment process with time-delay neural networks (TDNN) ( Zhu et al., 1998 ); (3) predicting future values of influent flow rate using a k -step predictor ( Tan et al., 1991 ); (4) estimation of flow patterns using auto-regressive with exogenous input (ARX) filters ( Lindqvist et al., 2005 ); (5) clustering based step-wise process estimation ( Gibert et al., 2010 ); and (5) rapid performance evaluation of WWTP using artificial neural network ( Raudly et al., 2007 ).

In the research reported in this paper, the influent flow rate and the influent CBOD were used as inputs to estimate TSS. Due to the limitations of the industrial data-acquisition system, the TSS values are recorded only two or three times per week. The data must be consistent in order to develop time-series prediction models. Thus, we established two research goals: (1) to construct
TSS time series using influent flow rate and influent CBOD as inputs and (2) to develop models that can predict TSS using the TSS values recorded in the past.
 the dataset used in the research. In Section 3 , the TSS time-series models are discussed. In Section 4 , data-mining models are constructed for predicting TSS. The computational results are discussed in Section 5 . Section 6 concludes the paper with topics suggested as future research. 2. Data preparation obtained from a wastewater treatment plant (WTP) located in Des
Moines, Iowa. The plant processes over 50 million gallons of raw wastewater per day. The influent flow rate is calculated at 15 min intervals, whereas influent CBOD and TSS are measured only two or three times per week based on the daily concentration values.
A five-year data record, collected from 1/1/2005 to 12/31/2010, was available for the research reported in this paper. To deter-mine the association between the TSS (output) and the inputs (influent flow rate and the influent CBOD), the Spearman correla-tion coefficient is computed ( Table 1 ).
 correlation between the input and output parameters. Based on the non-linear relationship between the influent flow rate and CBOD and TSS, non-parametric approaches were explored. discussed. 2.1. Identification of outliers removed. Fig. 1 presents the box plot of TSS values with the outliers identified. In general, the TSS values remain between 32 mg/l and 530 mg/l). However, the outlier data points occur due to errors in the measurements.

A normal, two-sided, outlier-detection approach was used. In two-sided outlier detection, values that exceed  X  3 s and values that are smaller than 3 s are considered to be outliers. Almost 4% of the data points have been determined to be outliers and removed from the analysis. Fig. 2 provides the box plot of TSS after the outliers are removed.

In the next section, methods are discussed for constructing time-series data for TSS. 3. Construction of time-series data for TSS
Models that can approximately determine TSS values have been developed using influent flow rate and influent CBOD as input parameters. First, the most relevant parameters are selected to obtain robust models. It is also essential for the reduction of the dimensionality of the data. Approaches for selecting parameters, such as the boosting-tree algorithm, corre-lation coefficient, and principal component analysis, are often used for this purpose.

The frequency of the measurement of output TSS is once per day, whereas the flow rate of the influent is recorded every 15 min. Considering the influent flow rate recorded in a day, the input data-dimension becomes 96. In the first approach for reducing the dimensionality of the data, the boosting-tree para-meter selection approach and the correlation coefficient approach were used to identify the best time of day for estimating the values of TSS. The approach uses the total squared error com-puted at each split of the input parameters ( Kudo and Matsumoto, 2004 ). The parameter with the best split is assigned a value of 1, and the less-preferred parameters are assigned values smaller than 1. The boosting-tree algorithm computes the relative influ-ence of the parameters using ~
J  X  T  X  X  where ~ J 2 j  X  T  X  is the relative significance of parameter j , i is the index of the tree, v t is the splitting feature associated with node t ,
L is the number of terminal nodes in the tree, and ~ I 2 t improvement of the squared error ( Fig. 3 ).

The Spearman correlation coefficient (Eq. (2) ) reflects the non-linear correlation between the input and output variables ( Choi, 1977 ). It is a form of the Pearson coefficient with the data converted to rankings  X  1 where y is the predictor, x is the input variable, and n is the total number of observations. The boosting-tree algorithm ranks the parameter in the range 0 to 1, whereas the correlation coefficients of the parameters can be in the range of 1to  X  1. Fig. 4 provides the ranking of the parameters generated by the boosting-tree algorithm and the Spearman correlation coefficient (absolute value). Both metrics point to the significance of the flow rate of the influent in the time window from 12:00 AM to 5:15 AM.
In the second approach, a principal component analysis (PCA) was used to reduce the dimensionality of the dataset. In PCA, the data undergo an orthogonal, linear transformation to a new coordinate system so that the greatest variance by any projection of the data is realized on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on ( Jolliffe, 2002 ).

Table 2 presents the five principal components when applied to the 96 dimensional dataset. With an aim to retain 95% variability of the original dataset, two principal components (i.e., PC1 and PC2 were selected). Influent recorded at 2:00 PM X  2:30 PM, 3:15 PM, and 5:45 PM contributed the most to the first principal component (i.e., PC1).

Based on the number of input parameters, data frequency, and parameter selection, five different scenarios were investigated and reported in this paper ( Table 3 ). 3.1. Metrics for model selection
Two commonly used metrics, i.e., the mean absolute error and the mean relative error (%), were used to evaluate the perfor-mance of the data models (Eqs. (3) and (4) ) MAE  X  MRE  X  %  X  X 
In this paper, neural networks (NNs) are employed to model the data scenarios listed in Table 3 . Due to the complex, non-linear behavior of the data used in modeling, 500 neural networks were trained by varying the number of hidden units and activa-tion functions. The number of hidden layers is heuristically selected after some trial runs. The criteria of the selection are the computational time and error improvement. Based on the trial runs, the hidden layer equal to one was found to be the best. The number of neurons in a hidden layer varied from 5 to 25. Five nential , X  and  X  identity , X  were used. For each of the five scenarios mentioned in Table 3 , two-thirds of the data were used to derive the model, whereas the remaining one-third of the data was used for testing. Table 4 summarizes the testing results obtained for the five scenarios.

While most of the data models discussed in this paper have rather high error rates, the results obtained in Scenario 4 are promising. The reported results indicate the significance of high-frequency data and the appropriate selection of parameters in 12:00 PM 1:00 PM 2:00 PM 3:00 PM 4:00 PM 5:00 PM 6:00 PM 7:00 PM 8:00 PM 9:00 PM improving the accuracy of the predictions. The error can be further minimized by increasing the frequency of the TSS data collection process. However, the results obtained using the approach described in the paper are are of better quality than the results reported in the literature ( Sosiak, 2001 ; Susfalk et al., 2008 ).
 to construct the time series for TSS. Fig. 4 compares the actual and predicted values of TSS using the MLP model of Scenario 4. The results in Fig. 4 indicate a high coefficient of determination ( r 2  X  0.803).
 86.66% accuracy. These values are used to fill almost 60% of the data needed to construct a five-year TSS time series for the period from January 2005 through December 2010. Fig. 5 presents the run chart of the actual and predicted values of TSS values over a period of five years. The TSS data displayed in Fig. 5 were used to build the time-series prediction model discussed in the next section. 4. Predicting the TSS in the influent
Considering the univariate nature of the data, the past recorded values of TSS were used as the input to predict the current and future values of TSS. Such past values of the parameters are known as the memory values of the parameters.
Memory values have been used extensively to improve the accuracy of the predictions of various models developed for different applications ( Kusiak and Li, 2010 ; Kusiak and Song, 2006 ; Kusiak and Verma, 2012 ). The values of TSS over the past 10 day were used as input parameters in the expression shown in
The autocorrelation and the boosting-tree algorithm were used to rank the 10 memory parameters. The coefficients pro-duced by the two approaches reflect a similar ranking of the input parameters ( Fig. 6 ). As anticipated, the immediate past value is the best predictor, but the values recorded a week in the past are more significant than the values recorded two or three days in the past. The ranking of parameters is expressed in z  X  TSS  X  t T  X  4 z  X  TSS  X  t 7 T  X  4 z  X  TSS  X  t 6 T  X  4 z  X  TSS  X  t 8 T  X  4 z  X  TSS  X  t 2 T  X  4 z  X  TSS  X  t 9 T  X  where B [.] is the significance of the parameter.

The five best predictors from Eq. (6) were selected to develop the model for predicting day-ahead values of TSS. Descriptions of the selected data-mining algorithms for model construction are provided in the next section. Days Parameter importance 4.1. Algorithm selection
Five data-mining algorithms, i.e., the k -nearest neighbors ( k -NN) ( Hall et al., 2008 ); multi-variate adaptive regression spline (MARS) ( Friedman, 1991 ; Hastie et al., 2009 ); neural network (NN) ( Bishop, 1995 ); support vector machine (SVM) ( Kecman, 2005 ); and random forest (RF) ( Breiman, 2001 ) algorithms, were consid-ered to predict future values of TSS. A back-propagation algorithm determines the best fit NN. SVM constructs a set of hyper planes in high-dimensional space, which can be used for classification and regression. RF is an ensemble learning method in which multiple trees are generated. It selects n input parameters randomly to split the tree nodes. MARS is a non-parametric procedure for regression analysis. It constructs the functional relationship between input and output variables from a set of coefficients and basis functions, all driven by regression data. The k -NN approach is an instance-based learning method in which the function is approximated locally. For regression models, k -NN output is the average of the k -nearest neighbors X  outcomes.

An algorithm predicting day-ahead values of TSS with mini-mum error was selected to construct models for seven-day-ahead predictions. NN was trained with 100 multi-layered perceptron (MLPs) by varying hidden, output activation functions and the number of neurons in the hidden layers. Activation functions, e.g., ered for both hidden and output nodes. A single hidden-layer was used in this network, while the number of neurons varied from 5 to 25. SVM was trained using four different kernels, i.e., RBF, polynomial, linear, and sigmoid kernels. The number of nearest neighbors in the k -NN algorithm was varied from 2 to 10 in training, while the Euclidean distance was used as a distance metric. MARS was trained on a number of basis functions, with the maximum equal to 500. RF was trained by setting the number of random predictors to three, while the maximum number of trees was 500. Table 5 presents the 10-fold cross-validation result obtained using five data-mining algorithms.

Based on the results in Table 5 , the NN algorithm (MLP 5-24-1, hidden activation: tanh, output activation: exponential) outperforms the other algorithms by providing the lowest MAE and MRE errors.
Fig. 7 illustrates the run chart of the actual and MLP-predicted TSS values. The results in Fig. 7 show that the MLP algorithm is the most accurate predictor of future values of TSS. 4.2. Iterative learning
Even though the results produced by MLPs ( Table 5 ) were promising, the prediction error can be reduced further by updat-ing the prediction model iteratively for the next time-step prediction. A sliding window was utilized with NN models to predict future values of TSS iteratively. The value of TSS predicted by NN model at the current time (TSS( t )) was used as the input to predict the values of TSS at some future time (TSS( t  X  1)). The least significant parameter was replaced with the predicted out-put to keep the dimensions of the input data constant. Fig. 8 illustrates the concept of iterative learning. After each iteration, the least-significant memory parameter was replaced with the parameter predicted in the previous iteration. Thus, for predicting the values of TSS two days ahead, the one-day ahead predicted value of TSS was used as an input, and this process was repeated until it was ended.

TSS ( t 1)}, were used as inputs to predict the current value {TSS( t )}. Seven MLP models were constructed iteratively from the training data using 10-fold cross validations. Table 6 presents the results obtained by the MLP at each learning step. predict the future values of TSS. The prediction results obtained using basic and iterative learning are compared. 5. Computational results the MLP models developed in Section 4 ( Table 5 ). Table 7 presents the results obtained using MLP at seven time steps, spaced at one-day intervals. MAE was found to be in the range of 41 X 55 mg/l, whereas the MRE ranges from 22% X 32% for seven-day prediction.
The results in Table 7 indicate that the week-ahead values of TSS can be predicted with almost 68% accuracy. 5.1. Predictions based on iterative learning
In this section, the models constructed by seven MLP algorithms ( Table 6 ) are applied iteratively to the test data. Table 8 provides the
MAE and MRE statistics for the test dataset used for prediction. The computational results in Table 7 indicate that TSS can be predicted a week ahead with accuracy up to 73%, with the MAE in the range of 40.95 X 52.30 mg/l and the MRE in the range of 21.85% X 27.55%.
Fig. 9 illustrates the error improvement over time for the dynamic learning scheme. By applying the iterative NN learning scheme, a 5% improvement in the MRE and a 4% improvement in the MAE were obtained. The results shown in Fig. 9 indicate that the iterative learning scheme can be useful in making long-term predictions. 6. Conclusions
Data-mining algorithms were applied to predict total suspended solids (TSS). Numerous scenarios involving carbonaceous biochem-ical oxygen demand (CBOD) and influent flow rate were investi-gated to construct the TSS time-ser ies. The multi-layered perceptron (MLP) model performed best among the five different data-mining models that were derived for predicting TSS. The accuracy of the predictions was improved furthe r by an iterative construction of
MLP algorithm models. The values of TSS were predicted seven days in advance with accuracies that ranged from 73% to 79%.
The research reported in this paper indicated that accurate predictions are feasible if sufficient data are available. The performance of the models constructed in the research will be tested for long term prediction once more data becomes available.
Future research will involve application of the proposed models to optimize performance of wastewater treatment plants (WTPs). Acknowledgment
This research was supported by the Iowa Energy Center, Grant no. 08-01.
 Error improvment References
