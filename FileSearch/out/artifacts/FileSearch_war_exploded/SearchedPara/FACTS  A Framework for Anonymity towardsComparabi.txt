 In the recent past, many anonymization a pproaches have been proposed, i.e., an-onymity notions, anonymization schemes (subsequently referred to as  X  X chemes X ), and measures for their evaluation. It is difficult to compare them and to decide when to use which scheme. It is hard to answer important questions, e.g.:  X  Given a data set, a scheme, and an attack, how much information can the  X  Given a data set and a set of queries, wh ich scheme maximizes query accuracy
We see three dimensions that must be considered when comparing approaches, namely attacks, measures and benchmarks: Attacks. Schemes have to make assumptions on the data set to be anonymized, and on the capabilities required to break anonymization. This allows to state if and to which degree the schemes give protection. Example 1 introduces our running example. In order to ease presentation, we use well-known approaches with well-researched vulnerabilities.
 Example 1: Each row in Table 1a describes one individual. The data set con-tains attributes (quasi-identifiers) which might allow to identify a person ( X  X ip X ,  X  X ge X  and  X  X ex X ). It further has a sensitive attribute ( X  X isease X ). Table 1b shows how a k -Anonymization scheme has transformed this data. This trans-formation cannot shield from attacks that disclose the sensitive attribute value for an individual. This is because, for all tuples with the same values of iden-tifying attributes, the value of the sensitive attribute is the same. However, k -Anonymization implicitly assumes that there is no correlation between quasi-identifying and sensitive attributes. The so-called homogeneity attack can exploit this to break the anonymization of Table 1b. Another scheme is required; see for instance Table 1c for an anonymization outcome with l -Diversity.
 [10] has shown that any scheme that preserves some utility has to rely on assump-tions. Attacks in turn exploit such assumptions. This may result in new schemes to shield against them, i.e., we observe a stream of new attacks and countermea-sures, for many scenarios. We say that approaches belong to the same scenario if they share certain basic requirements. For example, in scenario data publishing of microdata ( S pub ) one releases modified data sets without any means to undo these modifications. With scen ario database-as-a-service ( S DaaS ) in turn, a re-quirement is to have de-anonymization mechanisms for authorized individuals. Besides S pub and S DaaS there are many more scenarios, e.g., statistical databases ( S stats ) and data mining ( S mining ). Differential Privacy [7] for example assumes independent database records  X  [11] then describes an attack exploiting depen-dencies between records, together with a respective new scheme. To find out if a scheme can be used in a certain real-wor ld context, it is important to test the anonymized data against such attacks.
 Measures. Besides formal proofs of anonymity and complexity analyses, quan-titative measures are needed to assess the applicability of a scheme for real-world applications. An example is the probability that the anonymity of a data set can be broken if it has been anonymized with a certain scheme. Regarding perfor-mance, it is interesting to know if there is an optimal scheme that can anon-ymize a certain data set in reasonable tim e, or if a heuristic scheme is needed. Further measures consider data quality and query accuracy  X  we address them later in this article. However, the sheer number of schemes, attacks, and appli-cation requirements makes it hard to identify the best scheme for a given setting. Making the right choice is important to account for high-level privacy requirements, cf. [3].
 Benchmarks. Schemes may be related in that they aim at the same kind of protection, e.g., against linking values of sensitive attributes to individuals. How-ever, related schemes typically have been evaluated with different experiments. For example, [12] uses the UCI Adult data set, while the related scheme [17] uses an IPUMS census data set: One ca nnot compare their measurements of data quality or of query accuracy.
 Example 2: Queries on non-anonymized da ta sets may need to be modified to be executed on the anonymization output. Query-processing techniques then must be tailored to schemes. With our running example, the query SELECT * FROM Table 1a WHERE Age BETWEEN 22 AND 28 needs to be modified so that values of  X  X ge X  map to the generalized intervals in Table 1b. Different measures for the loss of accuracy exist. To have ex periments that are comparable, not only the data must be identical, but also those modifications of the queries and the accuracy measures.
 The three dimensions of evaluation problems described above call for a frame-work that supports a detailed compariso n of schemes based on the requirements of real-world applications. Such requirements exist in various categories that are orthogonal to each other. At first, technical requirements must be considered, e.g., the memory footprint or the scalability of an anonymization scheme re-garding the number of input tuples. Secondly, the anonymization scheme must consider the eventual use of the anony mized data. For example, if a scheme removes all data values that deviate from the average, but the use case needs to perform outlier detection, this scheme is inappropriate. The third category considers privacy preferen ces, attacker models and how sensitive information is represented in the data. For example, sensitive information could be material-ized as set-valued data, e.g., from a shopping cart analysis, and an adversary might know typical shopping carts. Thus, FACTS must be flexible enough to implement a wide range of differen t schemes, attacks, and measures.
Designing such a framework is challenging, given the wide variety of possible attacks, measures and benchmarks. Although in this paper we limit examples and discussions to S pub and S DaaS , we strive for a framework that also works for other scenarios, e.g., S stats and S mining . In this context, the heterogeneity of scenarios is challenging. For instance, data quality is not important for S DaaS , but for S pub ,itis.
 In this paper, we propose FACTS, a F ramework for A nonymity towards C omparability, T ransparency, and S haring. It allows to compile benchmarks together with the implementations of schemes and of attacks, data sets, query-processing techniques etc. When design ing FACTS, we have devised standards for the anonymization application, namely for interfaces that researchers propos-ing new approaches must implement and for data they must provide. Users can then define, share, update, and execute benchmarks for anonymization that refer to the standards. FACTS addresses comparability, as Example 3 illustrates. Example 3: An author of a query must implement two methods whose interfaces are given by FACTS: one for the query on anonymized data sets, another one for the query on the original data. In S pub , this allows to measure the loss of query accuracy (cf. Example 2). In S DaaS , it allows to quantify the performance costs of decrypting query results and to verif y that results are the same as without encryption.
 Our evaluation is twofold. On the one hand, we have developed various bench-marks, including one S pub and one for S DaaS , described in a complementary technical report [9]. Here, we report on a user study with 19 participants that has continued for three months. The evaluation shows that FACTS addresses its objectives well, e.g., FACTS standards allow to compare approaches fairly by enforcing compliance with benchmark s. We have implemented the framework and the benchmarks in full and make everything available under a free license on our website [1]. The vision is that over time it will become common among anonymization researchers to refer to suitable benchmarks. We now introduce our terminology and discuss how schemes have been evaluated. Anonymization. Our understanding of the term anonymity is broad and in-cludes approaches such as encryption, see below. Any scheme takes an original data set as input, with original values in its cells. With S stats , a set of functions that operate on the data is input as well. Schemes generate an anonymization output . With S pub and S DaaS , this output is the entire anonymized data set, with S stats it is anonymized query results. Any scheme seeks to protect against a certain kind of disclosure of sensitive information. The protection model states which information to protect. Anonymity notions state characteristics the out-put of or the information processed by schemes must have. An anonymity notion may refer to a certain protection model , i.e., any scheme compliant with the notion protects the information specifie d by the protection model. Adversaries execute attacks that try to break the protection. Adversary models describe the adversary, i.e., her capabilities and her background knowledge. An anonymity notion may include a reference to an ad versary model: A scheme complies with the anonymity notion iff the adversary of the referenced adversary model cannot get to the information it aims to protect. Finally, anonymity is given if a scheme protects the information specified by the protection model against adversaries as defined by the adversary model. Thus, schemes such as pseudonymization or partitioning [2] can offer  X  X nonymity X  according to this definition. Example 4: With S pub , the original data sets contain quasi-identifying and sen-sitive attributes. An assumption is that each tuple belongs to one individual. A protection model is that any sensitive attribute value must not be linked to the respective individual. The anonymity notion k -Anonymity [15] specifies the fol-lowing rule for anonymization output: For any tuple, there are at least k  X  1other tuples with the same values for the quasi-identifying attributes. S k-Anonymity ,a scheme for k -Anonymity, with k set to 2, computes the output in Table 1b. It generalizes original values, to build so-called QI blocks . This anonymization however cannot protect from adversary Alice who wants to disclose the disease Bob has. The adversary model is that Alice has knowledge about individuals, as follows. Alice knows that Bob is in the database, lives in a zip-code area be-ginning with 130, and is 21 years old. She concludes that Bob has AIDS. That is, she executes the so-called homoge neity attack [13]. We refer to it as A HG . l -Diversity [13] protects against A HG ,seeTable1c.
 Experiments.  X  X pproach X  is our generic term for any new concept an anonym-ization researcher might propose. Approaches include schemes, attacks, query processing, and measures. Next to anonymity, schemes may have further goals: With S pub , a goal is to maximize data quality for subsequent analyses. For S stats , the data set is hidden, and the user can enter a given set of statistical queries  X  a goal is to maximize the accuracy of their results. For S DaaS , a goal is to maximize performance of query processing. In general, researchers strive to find schemes that are good regarding a combination of goals, as quantified by measures .Mea-sures used in the literature are anonymity , data quality , query accuracy ,and performance . For instance, anonymity measures quantify to which degree an ad-versary can break the anonymization, i.e., disclose the information specified in the protection model, and data-quality measures quantify how much the an-onymized data set differs from the original one. An experiment to evaluate an approach has experiment parameters , at least an original data set and a measure. With our terminology, a benchmark is a set of experiments. A benchmark suite bundles benchmarks with schemes that use them, and contains their parameters and runs such bundles. It yields measure values , i.e., values from the respective experiments as output. One benchmark may be used in several suites. We differ-entiate between benchmark specificatio n and benchmark execution with suites. This is because one might have an interesting benchmark, e.g., containing a new data set, but might not have a scheme using it. In general, we see two user roles: researchers and users . Researchers are inventors/implementer of an approach. Users deploy approaches. They do not necessarily know the inner structure of the approach they use. A researcher can also be a user.
 Example 5: Continuing Example 4, we illustrate how the measure of [12] (we refer to it as M Anon-Dist ) quantifies the threat posed by A HG . M Anon-Dist is the maximum distance between (a) any distribution of values of the sensitive attribute of a QI block in the anonymized data set and (b) the distribution of values of the sensitive attribute of the original data set. A HG can conclude that an individual has a sensitive attribute value if the distance is large, as we now explain. Experiment e quantifies anonymity with M Anon-Dist :
Benchmark B contains e as the only experiment, i.e., B = { e } .Benchmark suite B runs B for two schemes.
 B executes and generates Tables 1b and Table 1c as the anonymization output. For Table 1b, the distributions (a) are { Lupus, Lupus } and { AIDS, AIDS } .For Table 1c, the distributions (a) are { Lupus, AIDS } , both times. Distribution (b) is {
Lupus, Lupus, AIDS, AIDS } . The distributions (a) for Table 1b have a greater distance to distribution (b) than the distributions (a) for Table 1c. M Anon-Dist thus calculates a higher degree of disclosure for Table 1b. We now present FACTS, our framework for easy comparability for anonymiza-tion research. In this section we give an overview, describe the key concepts, and say how to implement benchmarks. 3.1 Overview FACTS is a framework for easy comparison of anonymization approaches. A core issue when designing FACTS has b een to come up with class models of approaches. Class models are our standardizations of behavior and of processes in the context of anonymization. In FACTS, researchers provide implementa-tions of class models, by implementing them against interfaces we, the designers of FACTS, have specified. Researchers further have to comply with the stan-dards class models specify for data generation. Users configure benchmarks and benchmark suites within FACTS that refer to these implementations. Bench-mark suites bundle all data, i.e., data sets, the implementations of class models, and experiment results. FACTS stores ev erything in a central repository. Users can execute benchmark suites to compare the state of the art with ease. The idea is that users who are experts of an anonymization sub-domain create benchmark suites for approaches where a comparison is interesting.
 3.2 Aspects FACTS covers four aspects. First, users define benchmark suites, i.e., the speci-fication which approaches to compare based on which data, parameter settings etc. Benchmark suites refer to implemen tations of class models. We have defined class models as the second aspect, and th ere are class models of schemes, attacks, and queries. The third aspect is that FACTS executes these class models and performs the benchmarking. FACTS stores all results and protocols of executing benchmark suites in a repository, this is the fourth aspect. See Figure 1. 3.3 Benchmarks We now describe how to realize benchmarks by means of the four aspects. For specifics of our implementation in Java, we refer to the documentation on the FACTS website [1].
 Aspect A1: Input In this aspect, a user configures benchmark suites. Stan-dardized interfaces and data representations ensure that all input plays well together, e.g., the scheme knows how to access the input data set. Benchmark suites refer to one or more experiments. An experiment has the following pa-rameters: 1. An original data set D . 2. A scheme anon , possibly with parameters, referred to as params ( anon ). 3. An attack attack . It may have parameters, referred to as params ( attack ). 4. A set of queries Q where each q  X  Q may have parameters params ( q ). 5. A measure M .

Users may omit (3) or (4) if the experiment does not make use of attacks or queries, e.g., experiments on the performance of schemes.
 Aspect A2: Class Models Class models let researchers model approaches with a set of interfaces they need to impl ement and standardized formats of the data they need to generate. For example, there is an interface for attacks that lets researchers make background knowledge explicit, and methods accessing such background knowledge return it in a format standardized within FACTS. This for example allows authors of anonymity measures to use the knowledge.
Our evaluation will show that the FACTS interfaces are on the one hand suf-ficiently generic and, on the other hand, specific enough to make comparisons indeed easier. Further, F ACTS allows to compose complex schemes, attacks, and queries from so-called operations. Operations can be used individually, or they can be combined by means of so-called macros. Operations and macros allow to encapsulate and combine logical operations such as encryption or ran-domization, to reduce the necessary implementation work.
 Aspect A3: Execution This aspect performs the benchmarking, with mea-surements. FACTS instantiates the implementations of class models of As-pect A2 with the data of Aspect A1 . That is, FACTS runs the schemes, attacks, and queries. Experiments are logged, including time, date of execution, and the input data set.
 Aspect A4: Data This aspect stores all data, i.e., benchmark suites ( A1 ), the class models and their implementations ( A2 ), and the measurement results and execution logs ( A3 ). FACTS stores all data sets and implementations of approaches for later runs of the same suite. This is transparent to the users;
FACTS takes care of the data storage. For instance, users and researchers do not need to know the schema of the database or other internals of the framework.
They do not need to concern themselves with logging or with the storage of implementations. They only have to comply with a few standardizations for data generation. One example of such a standardization is that a user has to provide a name for a benchmark suite. 3.4 Illustration We now exemplarily describe how to implement an anonymity benchmark in FACTS. Our benchmark lets an adversary attack a copy of the data that has been anonymized with some scheme one wants to test. The benchmark specifies a fragment of the original data as background knowledge of the adversary, and it allows to quantify the effect of various parameters of interest. In the following, we discuss the implementation of the four aspects of FACTS.
 Aspect A1 :Input Listing 3 shows how to configure the input of the anonymity benchmark suite by invoking the respective methods implemented in FACTS. First, we import the input data from a file (Lines 1-3) and set up up a new experiment (Line 4-5). We further specify a scheme anon (Line 6) and an attack attack (Line 8), and link them to the experiment and the benchmark suite (Lines 7 and 9). Note that anon and attack must have been modeled in Aspect 2. Finally, we tell FACTS to use the anonymity measure AnonDist (Line 10). AnonDist implements the distance measure M Anon-Dist , as introduced in Example 5. Aspect A2 :ClassModels With this aspect, we provide implementations of anon and attack , which inherit from the FACTS classes AnonymityClassModel and AttackClassModel . Listing 4 illustrates the implementation of background knowl-edge, which is part of attack . In our example, we consider the distribution of the attribute  X  X isease X  of the original data. Thus, backgroundKnowledge (Line 1) has the original data set as one parameter. We use methods implemented in FACTS to count each value of the attribute named  X  X isease X  (Lines 3-6). An-other FACTS method adds the background knowledge to the framework (Line 7). Aspect A3 :Execution The third aspect is about running our anonymity bench-mark (Listing 5). Line 1 runs the scheme anon , which produces the anonymized data set anon ( D ). Next, Line 2 executes attack . Finally, Line 3 executes the anonymity measure. It accesses the values guessed by attack and compares them to the original values specified during anonymization anon . Aspect A4 :Data The final aspect is storing and logging of all classes, models and test data in a relational database. Since FACTS handles this internally, no additional code is required. In this section, we describe important f eatures of our framework, namely com-parability, reproducibility, workability, collaboration, and understandability, to-gether with respective use cases. These use cases will form the basis of our evaluation in the next section.
 Feature 1( Comparability ) . Comparability means quantifying anonymity, data quality, query accuracy, and performance of approaches. This is to decide which approach is best for a given real-world problem.
 FACTS gives way to comparability by means of benchmarks.
 Use Case ( U benchmark ) . FACTS lets users define, update, and access bench-marks for anonymization. For anonymization approaches that are related, e.g., approaches that aim for the same protection model, a user creates a benchmark suite that compares them, together with attacks and queries, under a set of mea-sures. When a researcher proposes a ne w attack, users can update benchmark suites to include it, or create new ones.
 Feature 2( Reproducibility ) . Reproducibility lets unbiased third parties repeat and verify experiments.
 Experts in their respective scientific fie lds have stressed the importance of re-producibility. For example, [4] states that more research is necessary to get to good experiment tools. FACTS supports reproducibility use cases such as the following one: Use Case ( U committee ) . Authors of a new scheme, attack, or query-processing technique add an implementation of their approach to the FACTS repository and to a benchmark suite. They use this benchmark suite to evaluate their technique. A respective conference co mmittee can later retrieve the benchmark suite. The committee can rerun measurements without difficulty and award a reproducibility label.
 Feature 3( Workability ) . Workability lets one explore effects of modifications of evaluation parameters.
 Workability allows to evaluate if an approach achieves good results solely because experiment parameters were chosen to its advantage. Parameter values however may be hidden in an implementation. It can be hard to identify and to vary them subsequently. FACTS addresses this: Use Case ( U workability ) . Alice is developing a new approach. FACTS requires Alice to specify the parameters with int erfaces she has to implement, be it for an-onymization, attacks, or queries. Bob now wants to evaluate this new approach. He retrieves and changes parameters of any benchmark with the approach. To this end, he can use FACTS methods that we have already implemented. He does not need to search for parameters in the code. This lets Bob observe how parameters affect benchmark results with ease.
 Feature 4( Collaboration ) . Collaboration within the community allows for faster development of new approaches.
 In publications, details such as the concrete data set, initialization or termination procedures and the values of parameters are not always given [16]. This makes it hard for researchers to build upon existing work, i.e., when implementing a new approach by reusing some of the implementation of an existing one. Use Case ( U sharing ) . FACTS gives way to sharing of operations. Suppose that researchers have developed a new scheme for S DaaS that protects against ad-versaries trying to find out the order of tuples. The authors search the FACTS repository and find an operation which randomizes a data set. It might have been developed for schemes of S pub originally.
 Another use case of collaboration is to let the community assist in solving a task: Use Case ( U assistance ) . A user wants to find out if her data set can be anon-ymized such that her quality criteria are met. The community helps her to find suitable schemes. For example, suppose that Alice wants to outsource her data to a S DaaS provider. She wants to know if there exists an anonymization that allows executing certain queries in under one second on her data set. Alice cre-ates a benchmark suite with her data set a nd queries. Other users can retrieve it and add schemes and query-processing techniques.
 Feature 5( Understandability ) . Understandability lets the user perceive the impact of all input parameters of an experiment on the experimental results. Given an experimental result such as a diagram, it can be hard to understand how exactly it has been computed, e.g., why one value is larger than another one: For example, there may be several (p arametrized) schem es and attacks, op-erating on different background knowledge. FACTS supports understandability by allowing the user to execute series of anonymization experiments with vary-ing parameters, by providing logs of intermediate results that one can analyzes with data-analyitcs tools, and by providing convenience methods to generate diagrams. A use case for understandability, but also for reproducibility and col-laboration, is as follows: Use Case ( U diagram ) . Researcher Carl is developing a new scheme. He uses FACTS to implement it and creates a ne wbenchmarksuitew ith performance experiments. Carl wants to graph anonymization performance, to find settings where the scheme is slow. His workflow is to implement the scheme, generate the graph, and to refine the implementation. Our final use case is to simplify benchmarks for understandability: Use Case ( U simplify ) . Tony has a large data set with activities of his waste-management business. He wants his busin ess associate Silvio to access the data, but conceal it from the authorities. This is a S DaaS scenario and requires an-onymization. However, Silvio complains that certain queries are slow. Tony lets Christopher evaluate which data the problem occurs with. To this end, Christo-pher gradually reduces the data set size an d measures query-execution times. With FACTS, he can use methods already implemented to retrieve an evalua-tion data set, to reduce its size, and to start measurements. Christopher observes that processing is slow if a certain client is in the data set. Tony is now able to eliminate the problem, once and for all.
 Discussion Our evaluation will show that FACTS is general enough to be ap-plicable to the very different scenarios S pub and S DaaS . Furthermore, FACTS is directly applicable to many other scenarios where input and output data can be represented as relations, e.g., association-rule mining of shopping carts, search histories, location-based services, social networks, or statistical databases. An-onymization scenarios for continuously changing data, e.g., data streams or in-crementally updated databases, would re quire to adapt our interfaces and their implementations of Aspects A1 and A4. H owever, generic benchmark function-ality, e.g., performance meas urements, should work as is. We evaluate FACTS by means of an explo ratory study. We declare success if FACTS allows to model state-of-the-art schemes, attacks, and measures, and if FACTS allows to execute and to compare them by means of benchmark suites. Further, we seek confirmation that the framework indeed has the features we have identified earlier.

We have conducted a user study to evaluate how well FACTS realizes repro-ducibility and collaboration. We reenact the use cases U assistance and U committee with this study. It is based on an instance of U benchmark and a benchmark suite, B
DaaS , for scenario S DaaS . We stress however that our main contribution is not one specific benchmark suite but the idea of a framework to build and share such suites. Additionally, our technical report [9] includes a benchmark suite B pub , the use case U diagram and an alternative instance of U benchmark . Supplementary evaluation material is available on the project website [1].
 We now describe the user study that was realized as an instance of use case U assistance and has led to the development of benchmark suite specified three tasks with U assistance : (1) Anonymize specific data sets in a S DaaS scenario and produce anonymization output, (2) develop query-processing tech-niques for each anonymization (cf. Example 2), and (3) attack the anonymization output of other study participants. In a user experiment, we have let participants solve these tasks. After the solutions were handed in, we have verified their repro-ducibility ( U committee ). We have further compared the different solutions with performance and anonymity measures ( U benchmark ).
 Evaluation Setup. Our experiment consists of three phases where users solve different tasks with FACTS. After the three phases were completed, we handed out a user survey regarding FACTS. It is available on our website [1]. We de-signed the survey with care so as to not enforce positive results with the way of asking questions. Likert-scale questions did not follow patterns, i.e., positive answers have been sometimes to the left , sometimes to the right. Further, our participants answered the survey anonymously, and they knew that we could not trace negative answers back to them. We now describe the tasks, followed by a description of the participants, and incentives. Our three tasks are: Task 1. Folksonomies [14] let users annotate digital objects with free-text la-bels. For example, with Last.fm, users annotate music, with Flickr photos.
Folksonomies contain data that is sensitive regarding privacy. A user study [5] confirms that users see a significant b enefit in being able to control who is allowed to see which data. Schemes l et users only access data when the data creators have given the respective autho rization. Thus, the first task is to de-velop schemes for CiteULike folksonomies of varying size.
 Task 2. Users issue queries against folksonomies for various reasons, e.g., per-sonal organization or communication with other users. We have identified seven types of common folksonomy queries [8]. For example, one type of query is  X  X e-trieve all tags applied to a specific object X . B DaaS includes parameters suitable for each of these seven query types for each CiteULike folksonomy data set. To continue the example, B DaaS computes the most frequent object as one of the query parameters for each data set. This is because the most frequent object results in a large query result and thus a long query-processing time. This is an interesting extreme case that should be included in a meaningful benchmark.
Thus, the second task is to develop fast processing techniques for each query type given and its parameters. Task 3. The frequency of attribute values in folksonomies follows a power-law distribution. With improper anonymization, this leaves room for statistical at-tacks [6]. B DaaS specifies as the adversary model someone with statistical back-ground knowledge. B DaaS computes this knowledge from the original data sets and makes the frequency of values of each attribute of the original data set available to an adversary. Thus, the third task of B DaaS is develop attacks against the schemes developed in Task 1, given this adversary model. Participants. We have let 19 students of computer science solve the tasks. We divided the students into four groups where three groups had five members and one group had four members. We instructed them in the fundamentals of (i) database anonymization, (ii) query processing on anonymized data, and (iii) statistical attacks. To test their understanding regarding (i) to (iii), we issued assignments to them. Two of originally 21 students did not pass them, and we did not let them participate in the subsequent evaluation.
 Incentives. The participants joined the experiment as part of a practical course. Their main incentive for participation was to pass the course. To do so, partici-pants had to earn points. Completion of the three tasks (i)-(iii) had earned them points. We had issued bonus points if participants committed their implemen-tations of FACTS class models to the repository, or if they had developed and shared FACTS operations.
 Results Comparability with B DaaS One outcome of the study has been the FACTS bench-mark suite B DaaS . We have imported the data set, queries, and adversary model (along with the data representing statistical background knowledge) from a pre-vious research project of ours [8] into FACTS. B DaaS thus allows us to compare the approaches of students in an evaluation setup actually used in research. We have observed that FACTS allows us, the conductors of the study, to compare approaches with ease. We justify this claim in different ways. (1) The final result of queries on the anonymization output does always equal that of the queries on the respective original data set, for all approaches by different participants. (2) The same set of queries executes for each approach. In the past years, we had lectured this practical course without FACTS. There have been many com-parison tasks that were cumbersome without the standardizations. Participants had submitted query-processing techniques that returned fewer result tuples, and they had used other query parameters than what we had specified. With FACTS, (1) its benchmarking checks correctness of results, and (2) always runs the same queries.
 Reproducibility with B DaaS We evaluate reproducibility by letting participants upload solutions and then letting them rerun them.

Our first indicator for reproducibility is if participants are able to execute ap-proaches without errors. We say that schemes are without error if they produce an anonymization output. We say that query-execution techniques are without error if they terminate, and if they com pute the correct result for all queries. We say that attacks are without error if they write their guesses for original values for each anonymized cell in the proper place for FACTS, and anonymity measures compute. A scheme writing on ly zeros to all cells would thus be error-free, but query execution based on it would fail. To evaluate if participants were able to reproduce the results of approaches by other participants, we have asked respective questions in the survey about the total number of schemes, queries, and attacks that participants had executed, and for how many of them partici-pants have observed no errors. By means of answers to these questions, we have calculated the share of error-free execu tions, cf. Table 2. Our apriori expecta-tions have been that the values are close to our measurements. The numbers reflect that one group has had errors with queries and attacks with our bench-mark runs. The values calculated with the survey are lower, but relatively close to ours. We conclude from these observations that FACTS allows users to run approaches from the FACTS repository without difficulty and that FACTS stan-dardizations allow to observe implementation errors that would be in the way of (fair) comparisons and reproducibility otherwise.
 Approach Study Answers Our Measurements Schemes 85 % 100 % Query-Processing
Techniques Attacks 61 % 75 %
Our second indicator for reproducibility is if measurement values from several experiment runs on varying platforms lead to similar results. To do this com-parison, we could rely on our execution of B DaaS and the executions of B DaaS by each group. We did observe similar resul ts. For example, all performance mea-surements have had Group 4 as the fastest before Group 1 and Group 3 and have reported errors for Group 2. Results are not identical however because execution times depend on the computational power of clients.

We state that there is reproducibility with three of four groups ( U committee ) because we were able to execute all of their approaches without error, and our measurement results were similar to theirs. We thus see strong indications that FACTS does allow for reproducibility.
 Collaboration with B DaaS . To evaluate the collaboration feature, we have asked respective questions in the survey. In a nutshell, users deem that the concept to collaborate with anonymization operations through the FACTS repository is useful. Our complementary technical rep ort provides more details, also on other aspects of our evaluation. Nowadays, a broad variety of anonymization approaches exists. We observe that requirements, goals, adversary models, implementations, or evaluation parame-ters are publicly available only for a few of them. It is very difficult to answer which approach is best regarding anonymity, data quality, query accuracy, and performance. To deal with this situation, we have proposed a framework, FACTS, that allows to compare anonymization approaches with ease. Researchers can im-plement their approaches within FACTS against so-called class models. We have systematically devised interfaces of class models that ease comparing and bench-marking approaches. Besides comparability, FACTS has other useful features, e.g., to support researchers in the documentation and presentation of experi-ment results. Our evaluation shows that FACTS allows to define comprehensive benchmark suites for anonymization scenarios, and that it addresses user needs well. Our vision is that FACTS will give way to a higher degree of comparability within the research area.

