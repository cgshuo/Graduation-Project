 Multi-Classifier System (MCS) has gained much attention in pattern recognition, machine learning, and data mining [11, 20]. The key concept of MCS is a combination predictions [9] and so does an ensemble. MCS is gaining popularity because of the limits of every individual classification algorithm. Techniques proposed to create MCSs or ensembles can be broadly categori zed into three categor ies, according to how base classifiers are constructed. The first category is to use various subsets of training data to train base classifiers, such as boosting [10, 21] and bagging (bootstrap classifiers, such as random forest (RF) [7]. Finally, the third category of techniques for the MCS or ensemble construction is to employ different algorithms to build systems composed of heterogeneous classifiers. In fact, these categories represent not only ways to combine classifiers but also ways to achieve diversity among them. 
It has been shown that in practice diversity is an important factor that makes them successful. Diversity between two classifiers indicates how different the predictions made by one classifier will be from those made by the other. In this paper, we present theoretical and empirical analyses of clas sifier combination that is composed of heterogeneous classifiers. We introduce the use of heterogeneous classifiers in MCSs or ensembles, since the classifiers are expected to be uncorrelated and to behave independently from each other. We provide evidence supporting that the diversity among heterogeneous classifiers is better than that among homogeneous ones. We consider four synthetic data sets as well as six real data sets, utilize three of the most important data mining algorithms [25] with or without alternative parameter sets, and evaluate the results using ten well-known diversity measures. 
To the best of our knowledge, there exist quite a few papers that theoretically examine the diversity of heterogeneous clas sifiers and make empirical comparisons of the diversity between homogeneous classifiers to that between heterogeneous classifiers. In our theoretical analysis, we introduce two definitions for diversity based on disagreements. These definitions distinguish themselves from others in the sense well. Neglecting these two factors, other definitions and measures for diversity failed in investigating effects of the use of heterogeneous classifiers in ensembles. They could not explain the phenomena presented in Section 3. Our contributions are listed below: 1). Two definitions for diversity introduced in this paper could assist researchers in understanding the diversity in MCSs or ensembles more comprehensively. 2) We present a lower bound of the probability that using heterogeneous classifiers would give better diversity when underlying algorithms are different enough. 3) We show that using heterogeneous classifiers consistently provides better diversity regardless of the diversity measures employed in experiments. 4) It is demonstrated that using alternative parameters lead to changes of diversity between homogeneous classifiers but changes are not as significant as those of diversity between heterogeneous classifiers. 
The remainder of this paper is organized as follows. In Section 2 we will present the theoretical analysis for diversity between heterogeneous classifiers, while in Section 3 we will report the experiments and results. Next, we will discuss related work in Section 4. Finally, conclusions will be given in Section 5. denote a symmetric classification algorithm and a data set. The i-th sample in a data set S is ) , (  X  and S  X  are respectively the indicator function and the expectation operator with intra-algorithm diversity, which is the normalized number of disagreements caused by the nature of the algorithm when dealing with different training sets. Definition 1. D is an underlying distribution (which is unknown in practice) and three algorithm A is the test set S . S . Assume N S S = = Definition 2. A symmetric classification algorithm A is (  X  ,  X  )-stable with respect to the intra-algorithm diversity if The (  X  ,  X  )-stability for the intra-algorithm diversity can be interpreted as follows:  X  is the upper bound for the normalized number of disagreements under a probability at least  X  . The second type of diversity is inter-algorithm diversity. It is defined as the normalized number of disagreements due to diverse natures of individual algorithms. Definition 3. D is an underlying distribution (which is unknown in practice) and two subsets drawn from it are denoted as S and S  X  . If S is a training set and S  X  is a test set, the inter-algorithm diversity of two symmetric classification algorithms 1 A and sample in S  X  , between the prediction given by a classifier from applying 1 A on S and that given by a classifier from applying 2 A on S . In other words, we apply two algorithms on the whole data set S to build two classifiers, and the diversity between them in this context is defined as the disagreements between these two classifiers for an arbitrary non-empty subset S  X  of S . What is more, we employ the inter-algorithm diversity to define the differentiability for two classification algorithms. Definition 4. Two s ymmetric classification algorithms 1 A and 2 A are (  X  ,  X  )-differentiable if 
A and 2 A are (  X  ,  X  )-differentiable if  X  is the lower bound for the number of disagreements under a probability at least  X  . This definition is used to set an combinations of homogeneous and heterogeneous algorithms. We denote a combination of classifiers as b a A A + where subscripts a and b indicate the underlying algorithms. Diversities of 1 1 A A + and 2 2 A A + are [ ] Proposition. If symmetric classification algorithms 1 A and 2 A are respectively (  X  , and 2 A are ( 2  X  , 2  X  )-differentiable, then the following holds: Eq. (1) shows the difference between the diversity from using heterogeneous algorithms and that from using homogeneous ones. First of all, let us focus on the first 1
We assume 1 A is ( 1  X  , 1  X  )-stable for intra-algorithm diversity and get the probability that the second term will be larger than or equal to 1 2  X   X  is at least 1  X  . 1  X 
Therefore, the diversity given by heterogeneous classifiers is larger than that given by homogeneous classifiers by at least 1 2 2  X   X   X   X  with a probability at least 1 2  X   X   X  . In order to make the above inequality interesting and useful, we need 1 2 2  X   X   X   X  . That is, we need to make sure that the inter-algorithm diversity is at least twice larger than the intra-algorithm diversity of 1 A . In summary, according to the above analysis, if we want to obtain better diversity by using heterogeneous classifiers, we should focus on finding an algorithm that is as different from the first algorithm as possible. In this paper, we perform experiments with synthetic and real data sets. We exploit two data generators, RDG1 and RandomRBF built in Weka [23], to individually generate breast-w , credit-a , credit-g , diabetes , kr-vs-kp , and sick [3, 12] . Diversity measures are proposed to assist in MCS or ensemble design [17]. In [14], authors give a comprehensive summary of diversity measures. Here, we employ ten popular measures to establish the quantitative determination of diversity between classifiers. Diversity measures considered here are summarized below: Q-statistic ( Q ), correlation coefficient (  X  ), disagreement measure ( DIS ), double-fault measure ( DF ), entropy ( E ), Kohavi-Wolpert variance ( KW ), interrater agreement (  X  ), measure of difficulty (  X  ), generalized diversity ( GD ), and coincident failure diversity ( CFD ). Furthermore, we consider three disparate al gorithms: C4.5 decision tree [19] (named J48 in Weka), Na X ve Bayes [13] (NB for short), and nearest neighbor [1] (named IBk in
E
KW  X  make correct prediction for a random sample; That is, } 1 , 5 . 0 , 0 { = X Weka). We select any two of them to create a pair of heterogeneous classifiers so that, for each data set, we set up six experimental sets in each of which we compare diversity of the combination of homogeneous classifiers with that of heterogeneous ones. 
As for experiments, we adopt the following procedure to perform experiments, samples from D without replacement and generate two training datasets. For synthetic datasets, the ratio of a training dataset to the whole dataset ( D ) is 0.1; for real datasets, it is 0.5. Next, we use one training datasets to create the first classifiers based on an algorithm 1 A , which could be J48, NB, or IBk. We denote this dataset as 1 C . Afterwards, we use the other training dataset to create the second classifier, 2 C , based on the same algorithm 1 A . Next, we create the third classifier, 3 C , by using the second training dataset and another algorithm 2 A (where 2 1 A A  X  ). Following that, we draw samples from D with replacement and produce ten testing datasets. Then, for each diversity measures. Finally, we average the diversity values over ten testing datasets. 
In the following, we present the results obtained by using the above three classification algorithms with default parameters first and then we present the results obtained by using the algorithms with various parameter sets. 
Applying the above procedure to synthetic an d real data sets, we collect the diversity values obtained from heterogeneous and homogeneous classifiers. Table A1 and Table Table A1 and Table A2, the first column exhibits a group of six experimental sets with respect to data sets. The second column shows the algorithms used in an experimental set. For instance, J48+J48 presents a pair of homogeneous classifiers, while J48+NB presents a pair of heterogeneous classifier s where J48 is the first employed algorithm and NB (i.e., Na X ve Bayes) is the second one. For convenience, symbols  X  and  X  mean respect to some measure. The results clearly present that using heterogeneous classifiers (shaded rows) leads to better diversity regardless of the diversity measures used in experiments. 
According to the above theoretical analysis , the heterogeneity primarily comes from parameters. Here, we provide empirical support for this argument. However, it is impractical and unnecessary to study all possible combinations of parameters. Thus, we consider five quite different parameter sets, as listed in below, to increase the variability of classifiers that are from the same algorithm. Such a selection, or parameter tuning, is a common exercise in data mining. 1) J48: unpruned tree; NB: kernel density estimator; IBK: 3-nearest neighbor 2) J48: minimum 5 instances per leaf; NB: supervised discretization; IBk: 5-nearest neighbor 3) J48: 5-fold reduced error pruning; NB: kernel density estimator; IBk: 5-nearest neighbor, weighted by the inverse of distance 4) J48: confidence threshold 0.2 for pruning, minimum 5 instances per leaf; NB: supervised discretization; IBk: 5-nearest neighbors, weighted by 1-distance 5) J48: unpruned tree, binary splits, minimum 5 instances per leaf; NB: supervised discretization; IBk: 5-nearest neighbors, hold-one-out evaluation for training, minimizing mean squared error 
We applied algorithms with these parameter sets on all data sets we mentioned earlier. However, due to the limitation of space, we do not report all results; rather, we present here the results for a synthetic data set and a real data set. Table A3 and Table A4 give the results of applying such pairs of homogeneous classifiers to a synthetic data set and the real data set diabetes , respectively. The first column shows parameter sets and the second column shows experiments for homogeneous and heterogeneous classifiers. The superscript asterisk means that, the pair of homogeneous classifiers is constructed with different parameters while one of them (the second one) comes with alternative parameters corresponding. The results presented in Tables A3 and A4 are not as optimistic as those reported in Tables A1 and A2. For the combination of homogeneous classifiers, using different parameters indeed gives better diversity. Changing parameters does not mean changing the nature of an algorithm but the way the algorithm searches the hypothesis space, if we interpret it in the classical language of machine learning. However, the diversity among classifiers that are based on the same algorithm but come with different parameters would not be good eno ugh to differentiate them. From these results, in the setting considered here, the primary source algorithm with different parameters. Nevertheless, the conclusion by no means indicates that employing different parameters has no effect on the diversity. It will be interesting to study the theoretical relationship between diversity and this factor. The study of diversity in ensemble has gained increasing attention, even though in theory there is no strong connection between diversity and the overall accuracy [5, 8, 14, 15, 16, 18, 20, 23].In [18] authors indicate that, in general, diversity compensates for errors made by individual classifiers. However, diversity itself is not a strong predictor of the overall accuracy of an en semble [17]. In [15] authors discuss the relationship between diversity and accuracy, while in [23] it is indicated that an effective ensemble requires each of individual classifiers to offer high accuracy and to generate diverse errors. Additionally, in [22], authors demonstrate that boosting requires stronger diversity than does bagg ing while bagging does not depend only on diversity, and they argue that diversity depends on the size of training data set. Moreover, in [2] authors consider using different feature sets argue that using different feature sets is the only way to achieve diversity in a system of homogeneous classifiers. However, the argument is not necessarily true because using homogeneous classifiers with different parameters would lead to the change of diversity, as we can see in tables. Furthermore, we connect heterogeneity to diversity without considering the effects of using different feature sets. In [4] authors study the combination of heterogeneous classifiers with the focus on some combination methods. Nevertheless, neither a theoretical analysis nor an empirical investigation of the source of heterogeneity is performed in the paper. This paper studied theoretically and empirically the relationship between heterogeneity and diversity. We performed a rich set of experiments to provide empirical evidence. benchmark data sets, utilized three classification algorithms without and with five different parameter sets, and employed ten popular diversity measures. Consequently, we built a foundation for the use of heterogeneous classifiers in MCSs or ensembles. This is particularly essential because th ere are quite a few papers theoretically examining the diversity of classifier combinations and, at the same time, empirically comparing the diversity of the combination of homogeneous classifiers with that of heterogeneous classifiers. Two important observations in this paper will make substantial contributions to the future MCS or ensemble design. First, the diversity among heterogeneous classifiers is higher than that among homogeneous ones. Second, the heterogeneity mainly results from using different classification algorithms instead of using the same algorithm with different parameters. Future work includes the study of the relationship between heterogeneity and accuracy. 
