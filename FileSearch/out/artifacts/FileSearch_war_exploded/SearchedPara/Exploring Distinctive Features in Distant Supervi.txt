 Relation Extraction is the task of extracting semantic relations between entity pairs given a set of sentences containing both entities. It gains much interest for its poten-tial effects on constructing large scale knowledge bases and supporting many other applications like question answering [12], textual entailment [15] etc. Traditional su-pervised approaches for rela tion extraction [7][22] need to label training data, which is expensive and biased towards the domain of labeled data. Due to the problems of supervised approaches, an attractive paradig m called distant supervision (DS) [10] is employed. It automatically produces labeled training data by aligning entities in a knowledge base with relation facts (such as Freebase 1 ) to sentences. However, it suffers from noisy labeled data which will bring poor extraction results. For example, in Figure 1, r ( e 1 ,e 2) =  X  BornIn ( Y ao Ming,Shanghai ) X  is a relation in the knowledge base. After automatic labeling, we get the sentences containing both e 1= X  YaoMing  X  and e 2= X  Shanghai  X  . The upper sentence truly express the relation r = X  BornIn  X  be-tween two entities. However, the lower one does not. It is a noisy labeled sentence. In this paper, we focus to address the noisy labeling problem in DS for relation extraction.
To overcome the problem of noisy labeled data in DS, work in [13][9][14] attempted to model the noisy data with multi-instan ce learning methods. They assume that at least one of the sentences containing both e 1 and e 2 expresses r ( e 1 ,e 2) .However, this at-least-one assumption can fail, for that Takamatsu et al. [16] showed 91.7% of entity pairs only have one labeled sentence in Wikipedia articles which do not fit for the multi-instance learning assumption. Moreover, they used binary features in their model. This setting will enforce some frequent indistinctive features. For ex-ample, labeled sentence  X  Life of Pie, by Ang Lee, can be said to be the best...  X  X or r(e1, e2) =  X  X irectorOf(Ang Lee, Life Of Pi) X  has one lexical feature  X  e2 by e1  X  where e 1 and e 2 are placeholders for two entities. However, it can also be found in the  X  AuthorOf  X  relation like the sentence  X  One Hundred Years of Solitude, by Gabriel Gar-cia Marquez, is a novel that tells...  X  X or X  AuthorOf(Gabriel Garcia Marquez, One Hun-dred Years of Solitude)  X . As a result, entity pairs of AuthorOf are probably mistaken for relation DirectorOf . Although the feature is from a positive sentence, it is still noisy. Binary features can not discriminate between the distinctive features and noisy ones.
In the paper, due to the deficiencies of the at-least-one assumption and the binary feature setting, we propose a novel approach to solve the noisy labeling problem. In-stead of using binary features which take no difference to all features, we explore the distinctive features and assign distinctive features higher weight than the noisy ones. In this way, we do not use the at-least-one assumption as the multi-instance learning does and can solve the noisy feature problem in the non-noisy sentences that indeed express the target relations caused by the binary feature setting mentioned above.
 Specifically, we employ a new method to calcu late the distinctiveness of each feature. Our intuition is that the noisy features tend to appear in several different relations. It means that if a feature is used to indicate sever al different relations, it would be less dis-tinctive. To obtain the feature-relation distribution, instead of only using the sentences labeled by a knowledge base with the DS assumption ( KB-matched instances 2 ) like previous work, we use the united instances that combine the KB -matched instances with the instances generated by entity pairs in training data but not in the knowledge base ( KB-not-matched instances ). For that, the KB-matched instances are only a small part of the training data, instances in which are biased to the relations used as labeling sources and not sufficient to discover the distribution of features to latent relations. And then we employ a topic model to model the generating process of instances in united instances where the feature can be consider ed as  X  X ord X , instances as  X  X ocuments X  and relations as  X  X opics X . After estimating the parameters, We can get the feature-relation distribution (or the  X  X ord-topic X  distribution) via this model. After that, we compute the distinctiveness of each feature based on the feature-relation distribution, and as-sign features different weights according to t heir distinctiveness. Finally, we use these weighted features to train a classifier for discovering relations in new instances.
This paper mainly makes the following contributions:  X  To solve the noisy training data problem, we propose a method to assign fea- X  To discover the proba bilities of features belonging to latent relations, we model  X  We conduct experiments to evaluate our method with Wikipedia articles and Free-
The remainder of the paper is organized as follows. Section 2 introduces the related work. Section 3 describe the relation topic model. Section 4 introduces the method to weigh features. Section 5 illustrates our experiments and evaluation. Finally, we con-clude our paper with the future work. Distant supervision (also known as weak supervision or self supervision) is used to a broad class of methods in information extr action which aims to automatically generate labeled data by aligning with data in knowledge bases. It is introduced by Craven and Kumlien [4] who used the Yeast Protein Database to generate labeled data and trained a naive-Bayes extractor. Bellare and McCallum [2] used BibTex records as the source of distant supervision. The KYLIN system in [18] used article titles and infoboxes of Wikipedia to label sentences and trained a CRF extractor aiming to generate infoboxes automatically. The Open IE systems TEXTRUNNER [21] and WOE [19] trained their extractors with the automatic labeled data from Penn Treebank and Wikipedia infoboxes respectively. Yao et al. [20] trained a CRF cons idering selectional preference constraints of entity types with weak supervision.

Our work was inspired by [10] which performed distant supervision for relation ex-traction. It used Freebase as the knowledge base to label sentences in Wikipedia as training data and trained a logistic regression classifier to extract relations between en-tities. Distant supervision supplied a method to generate training data automatically, however it also bring the problem of noisy labeling. After their work, a variety of meth-ods focused to solve this problem. Work in [16] predicted negative patterns using a gen-erative model and remove labeled data containing negative patterns to reducing noise in labeled data. In [13][9][14], they propos ed multi-instance learning methods with the assumption that at least one of the labeled sentences truly expressed their relation. How-ever, this assumption does not fit for the entity pair with only one labeled sentence. We employ an alternative approach without the mentioned assumptions. Different from the previous work using binary features, we assi gn different weight to features according to their distinctiveness to target relations.
 Algorithm 1. Unite KB-matched instances w ith KB-not-matched instances Aiming to discover the distribution of features to latent relations and due to , in this sec-tion, we first use features of KB-matched in stances to combine w ith KB-not-matched instances, and then use a topic model for modeling features in united instances. At last we obtain the feature-relation distribution. 3.1 Generating United Instances Given the training data set, previous work trained their models only using the sentences labeled by a knowledge base with the DS assumption (KB-matched instances2). How-ever the KB-matched instances are only a small part of the training data, instances in them are biased to the relations used as labeling sources and not sufficient to discover the distribution of features to latent relations. As a result, we employ a method to unite KB-matched instances and K B-not-matched instances: (a) First, after labeling with the DS assumption, we extract features for each entity pair (b) Second, we collect all entity pairs in tra ining data except those generating KB-(c) Third, we united the two part of instan ces by features of KB-matched instances. (d) At last, we remove the features with frequencies below 5, for two reasons, one is 3.2 Modeling United Instances with Topic Model Topic model (or LDA) [3] is a generative graphical model. It has achieved great success in finding the latent topic for documents. In this paper, we use it to model the generative process of each instance which has a set of features (Figure 2). We can consider a instance as an  X  X ocument X , features f in the  X  X ocument X  as  X  X ords X  and latent relations r as latent  X  X opics X .

The generation process of topic mode l for each instance is as following: 1. Choose N  X  Poisson(  X  ). 2. Choose  X   X  Dirichlet(  X  ). 3. Choose  X   X  Dirichlet(  X  ). 4. For each of the N features f n :
Based on the generative graphical model depicted in Figure 2, the joint distribution of  X  , r and f is given by:
And the likelihood of an instance: Finally, taking the product of the likelihood of each instance, we get the probability of a relation corpus:
We estimate its parameters with Gibbs Sampling [8][11] and set number of relations as 50 and iteration times as 2000 in our experimetns. After estimation, we obtain a matrice  X  K  X  N representing the feature-relation distribution. The probability p ( f i | r k ) of a feature f i conditioned on a target relation r k in  X  K  X  N is computed as follows: Where n ( i ) k is the number of times that the i th feature is assigned to the k th relation. V is the size of features.

We will use the distribution to compute the distinctiveness of features in the next section. In this part, we use the obtained feature-relation distribution  X  K  X  N and the feature dis-tribution in united instances to compute features X  distinctiveness. Intuitively, if features has equivalent probabilities among several latent relations, they are less dis-tinctive than the ones which have significant probabilities in only one latent relation. We call it clarity . We measure the clarity for each feature by the following equation: Where p ( f i | r k ) is the probability of the i th feature f i in the k th relation r k from the relation-feature distribution  X  K  X  N . If a feature is only observed once, its clarity is 1. If a feature can not be observed in features of the relation-feature distribution, its clarity is also 1. The reason is that the unobserved features are those with low frequencies, we consider they are less likely belonging to several relations.

Besides the clarity, intuitively, we think features with more information will tend to be less noisy. Our features are composed of lexical and syntactic pathes between two entities the same with [10]. More words in the pathes, more information the features will contain. For example, two feature  X  e 2 by e 1  X  X nd X  e 2 directed by e 1  X  for the relation  X  DirectorOf ( Ang Lee, Life Of Pi ) X  , the latter one is more informative than the former one and it can better predict the target relation. And more, if a feature has a low frequency in united instances, it tends to be more specific to the relation containing this feature and be more predictable to this relation. As a result, less frequent features are more informative than more frequent ones. We measure features X  informativeness with the following equation considering both the length and frequency mentioned above: the max number of words in features, freq ( f i ) is the frequency of the feature f i in united instances. We use  X  ( 0 &lt; X &lt; 1 ) to avoid values of features with high frequency or short length being too small. In the experiments, we set  X  as 0.25.

We compute the distinctiveness of a feature by combining clarity and informativeness . Based on the theory of Discriminative Category Mathcing (DCM) [6][1], we have the following equation, where
We assign the distinctiveness to each feature in KB-matched instances as its fea-ture value, and then train a multi-class logistic classifier with Gaussian regularization as the extractor. Our extractor takes an entity pair and its feature vector as in put, and return a relation name and its corresponding confidence score based on the probability it belongs to that relation. At last, we rank the extracting result based on their confidence to generate n most likely new relation instances and evaluate our method comparing to previous methods. 5.1 Data We conduct our experiments on articles of Wikepedia with Freebase as the knowledge base. We randomly sample 900,000 Wikipedia articles from Freebase Wikipedia Ex-traction (WEX) 4 data dump of 2012. In them, 600,000 articles are used as training data, and 300,000 are used as testing.

For preprocessing, we segment each article to sentences by XML tags in the WEX dump. To find entities in sentences, we first do NER tagging with Stanford NER [5]. We tag tokens into 5 categories: PERSON, ORGANIZATION, LOCATION, MISC and NONE where MISC means name entities not be longing to the first three categories. Adjacent name entities with the same NER tag are combined to one name entity. Then for entity pairs in sentences, we extract their features (see Section 3). The feature types are the same with [10] which mainly consis t of lexical Part-O f-Speech (POS), name entity and syntactic features (paths between two entities in the dependency parsing tree). We use the Stanford POS tagger [17] to assign the Pos tags and Stanford parser 5 to parse the sentences. .

To distant supervision for relation extraction, we evaulate 9 of the most frequent relations in Freebase from three categories: people, location and film (see Table 1). To train our extractor, we need negative instances. As a result, we randomly sample 10% of the entity pairs that appear in the same sentence labeled by the DS assumption but are not contained in Freebase, and then use them to label negative instances. 5.2 Baselines We compare our method ( PROP ) against two methods:  X  Mintz : this method is implemented based on [10]. We use their aggregate feature  X  MULTIR : this is the  X  X t-least-one X  model (a form of multi-instance learning) 5.3 Evaluation Following the work in [10][9], we evaluate our method in two ways: the held-out evaluation and the manual evaluation. The held-out evaluation only compared the newly discovered relation instances against Fr eebase relation data, it would suffer from false negatives. Thus, besides the held-out evaluation, we further conduct the manual evaluation.
 Held-Out Evaluation. In held-out evaluation, the extracted relation instances from testing data are automatically compared w ith those in Freebase. We rank the predicted relation instances by their confidences. Then we traverse this ranked list from high to low and measure precision and recall at each position.

Figure 2 shows the precision and recall curves for Mintz , MULTIR and our pro-posed method PROP . At the head of the curves, MULTIR outperforms the other two methods. However, it drops quickly below other two curves. PROP is consistently out-performing Mintz and it also achieve a better curve than MULTIR . Manual Evaluation. In manual evaluation, we remove the relation instances existing in Freebase and pick the top ranked 50 relatio n instances for each of the 9 relations. We manually label instances whether the relations indeed holds.

Table 2 shows the top 50 precisions of the 9 relations. Our approach PROP out-performs Mintz in 8 relations and outperforms MULTIR in 4 relations. All the three methods fail in extracting the film.film.country relation with no correct instance in its top 50 instances. Among the three methods, PROP achieve the best average precision.
 Analysis. The experiment results show the advantage by exploring distinctive features and weighing features based on their distinctiveness. Mintz used aggregate features which aggregates sentential binary faetures and MULTIR used binary features. Their feature settings enforces some frequent noisy features in the labeled data generated with the distant supervision assumption like  X  e 2 by e 1 X  for the DirectorOf relation. Our method overcomes this problem.

MULTIR learns a model driven by sentence-level features and aggregated sentence-level extracting results as a form of multi-instance learning. It alleviates the noisy labeling problem to some extent and achieves better results in some relations. However, because of the problem caused by the binary feature setting mentioned above, it performs quit bad in several relations. Taking the relation people.deceased person.place of death as an example. We inspect its extracting re-sult, it emphasizes the feature  X  e 1 of e 2 X  like  X  Barack Obama of Illinois  X  X hich hurts its precision much.

The three methods failed in extracting the film.film.country relation. The reason information that can predicate this relati on. The mistaken sentences are as follows:  X  ...to unite with [ Czechoslovakia ] e 2 , [ Harvard Ukrainian Research Institute ] e 1 .  X  and  X  [ Mohatta Palace ] e 1  X  ([ Karachi ] e 2 ) .  X  X tc. In this paper, we propose a new approach to address the noisy labeling problem in DS for relation extraction. Our method does not use the at-least-one assumption which can fail when there is only one labeled sentence, and it is able to handle the problem of noisy features in non-noisy instances. We explore distinctive features and assign distinctive features more weight than the noisy ones. We employ unsupervised topic model to discover feature-relation distribution in both KB-matched instances and KB-not-matched instances (united instances). And the feature-relation distribution are used to compute features X  distinctiveness for weighing features. At last, we use the weighed features to train a classifier to discover relations of new instances.

In the future work, we will try to explore the features in all the training data that related to the labeled part of training data but not appeared in them. We expect they can help to improve the extracting performance.
 Acknowledgments. This work was supported by the National Natural Science Foun-dation of China (No. 61070106, No. 61272332 and No. 61202329), the National High Technology Development 863 Program of China (No. 2012AA011102), the National Basic Research Program of China (No. 2012CB316300) and the Opening Project of Beijing Key Laboratory of Internet Culture and Digital Dissemination Research (ICDD2 01201).

