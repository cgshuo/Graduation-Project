 University of Virginia Michigan State University annotated a corpus of implicit arguments for ten predicates from NomBank. Through analysis the task, and discusses future directions for work on implicit argument identification. 1. Introduction
Recent work has shown that semantic role labeling (SRL) can be applied to nominal predicates in much the same way as verbal predicates (Liu and Ng 2007; Johansson and
Nugues 2008; Gerber, Chai, and Meyers 2009). In general, the nominal SRL problem is formulated as follows: Given a predicate that is annotated in NomBank as bear-ing arguments, identify these arguments within the clause or sentence that contains the predicate. As shown in our previous work (Gerber, Chai, and Meyers 2009), this problem definition ignores the important fact that many nominal predicates do not bear arguments in the local context. Such predicates need to be addressed in order for nominal SRL to be used by downstream applications such as automatic question answering, information extraction, and statistical machine translation.
 nominal predicates that bear arguments in the local context. This makes the nominal
SRL system applicable to text that does not contain annotated predicates. The system does not address a fundamental question regarding arguments of nominal predicates, however: If an argument is missing from the local context of a predicate, might the argument be located somewhere in the wider discourse? Most prior work on nominal and verbal SRL has stopped short of answering this question, opting instead for an approach that only labels local arguments and thus ignores predicates whose arguments argument identification for nominal predicates.

TreeBank (Marcus, Santorini, and Marcinkiewicz 1993): The NomBank (Meyers 2007) role set for requirement is shown here:
In Example (1), the predicate has been annotated with the local argument labels pro-vided by NomBank. As shown, NomBank does not annotate an arg the requirement predicate; a reasonable interpretation of the sentence, however, is that SEC is the entity that is requiring something. 1 This article refers to arguments such as
SEC in Example (1) as implicit. In this work, the notion of implicit argument covers any argument that is not annotated by NomBank. 2
Example (1) in the corresponding TreeBank document: The NomBank role set for change is as follows:
Similarly to the previous example, Example (2) shows the local argument labels pro-vided by NomBank. These labels only indicate that rules have been changed. For a full interpretation, Example (2) requires an understanding of Example (1). Without 756 the sentence from Example 1, the reader has no way of knowing that the agency in
Example (2) actually refers to the same entity as SEC in Example (1). As part of the reader X  X  comprehension process, this entity is identified as the filler for the arg
Example (2). This identification must occur in order for these two sentences to form a coherent discourse.
 spans sentence boundaries. Thus, if one wishes to recover implicit arguments as part of the SRL process, the argument search space must be expanded beyond the traditional, single-sentence window used in virtually all prior SRL research. What can we hope to gain from such a fundamental modification of the problem? Consider the following question, which targets Examples (1) and (2):
Question (3) is a factoid question , meaning it has a short, unambiguous answer in the targeted text. This type of question has been studied extensively in the Text Retrieval
Conference Question Answering (QA) Track (Dang, Kelly, and Lin 2007). Using the evaluation data from this track, Pizzato and Moll  X  a (2008) showed that SRL can improve the accuracy of a QA system; a traditional SRL system alone, however, is not enough to recover the implied answer to Question (3): SEC or the agency . Successful implicit argument identification provides the answer in this case.
 cates. 3 The following section surveys research related to implicit argument identifica-tion. Section 3 describes the study X  X  implicit argument annotation process and the data it produced. The implicit argument identification model is formulated in Section 4 and evaluated in Section 5. Discussion of results is provided in Section 6, and the article concludes in Section 7. 2. Related Work
The research presented in this article is related to a wide range of topics in cognitive science, linguistics, and natural language processing (NLP). This is partly due to the discourse-based nature of the problem. In single-sentence SRL, one can ignore the dis-course aspect of language and still obtain high marks in an evaluation (for examples, see
Carreras and M ` arquez 2005 and Surdeanu et al. 2008); implicit argumentation, however, forces one to consider the discourse context in which a sentence exists. Much has been said about the importance of discourse to language understanding, and this section will identify the points most relevant to implicit argumentation. 2.1 Discourse Comprehension in Cognitive Science
The traditional view of sentence-level semantics has been that meaning is composi-tional. That is, one can derive the meaning of a sentence by carefully composing the meanings of its constituent parts (Heim and Kratzer 1998). There are counterexamples to a compositional theory of semantics (e.g., idioms), but those are more the exception than the rule. Things change, however, when one starts to group sentences together to form coherent textual discourses. Consider the following examples, borrowed from
Sanford (1981, page 5):
Examples (4) and (5) describe three events: bounce , rush ,and get . These events are intricately related. One cannot simply create a conjunction of the propositions bounce , rush ,and get and expect to arrive at the author X  X  intended meaning, which presumably involves Jill X  X  becoming injured by her fall and Harry X  X  actions to help her. The mutual dependence of these sentences can be further shown by considering a variant of the situation described in Examples (4) and (5):
The interpretation of Example (6) is vastly different from the interpretation of Exam-ple (4). In Example (4), Jill becomes injured whereas in Example (6) she is quite happy. sitional interpretation; rather, a sentence X  X  interpretation depends on the surrounding context. The standard compositional theory of sentential semantics largely ignores con-textual information provided by other sentences. The single-sentence approach to SRL operates similarly. In both of these methods, the current sentence provides all of the semantic information. In contrast to these methods X  X nd aligned with the preceding discussion X  X his article presents methods that rely heavily on surrounding sentences to provide additional semantic information. This information is used to interpret the current sentence in a more complete fashion.
 comprehension. Researchers in cognitive science have proposed many models of reader knowledge. Schank and Abelson (1977) proposed stereotypical event sequences called scripts as a basis for discourse comprehension. In this approach, readers fill in a dis-course X  X  semantic gaps with knowledge of how a typical event sequence might unfold.
In Examples (4) and (5), the reader knows that people typically call on a doctor only if someone is hurt. Thus, the reader automatically fills the semantic gap caused by the ambiguous predicate bounce with information about doctors and what they do. Similar observations have been made by van Dijk (1977, page 4), van Dijk and Kintsch (1983, page 303), Graesser and Clark (1985, page 14), and Carpenter, Miyake, and Just (1995).
Inspired by these ideas, the model developed in this article relies partly on large text corpora, which are treated as repositories of typical event sequences. The model uses information extracted from these event sequences to identify implicit arguments. 2.2 Automatic Relation Discovery
Examples (4) and (5) in the previous section show that understanding the relationships between predicates is a key part of understanding a textual discourse. In this section, we review work on automatic predicate relationship discovery, which attempts to extract these relationships automatically. 758 similar to the following: This relationship creates a mapping between the participants of the two predicates.
One can imagine using such a mapping to fill in the semantic gaps of a discourse that sense of the fact that X left a large tip for the waiter, however.
 variation of the so-called  X  X istributional hypothesis X  posited by Harris (1985), which states that words occurring in similar contexts tend to have similar meanings. Lin and
Pantel applied the same notion of similarity to dependency paths. For example, the inference rule in Example 8 is identified by examining the sets of words in the two X positions and the sets of words in the two Y positions. When the two pairs of sets are similar, it is implied that the two dependency paths from X to Y are similar as well. In Example (8), the two dependency paths are as follows:
One drawback of this method is that it assumes the implication is symmetric. Although this assumption is correct in many cases, it often leads to invalid inferences. In Exam-Y implies X likes Y  X  X s more plausible but not certain.
 handle cases of asymmetric relationships. The basic idea proposed by Bhagat, Pantel, and Hovy is that, when considering a relationship of the form x , p occurs in significantly more contexts (i.e., has more options for x and y )than p is likely to imply p 1 but not vice versa. Returning to Example 8, we see that the correct implication will be derived if likes occurs in significantly more contexts than eats .The and is more likely to be implied by the specific concept (i.e., eat ). As shown by Bhagat,
Pantel, and Hovy, the system built around this intuition is able to effectively identify the directionality of many inference rules.
 identifying asymmetric relationships between verbs. For example, the asymmetric en-tailment relationship Xwins  X   X  X plays holds, but the opposite ( X plays not. This is because not all those who play a game actually win. To find evidence for this automatically, the authors examined constructions such as the following (adapted from Zanzotto, Pennacchiotti, and Pazienza [2006]):
The underlying idea behind the authors X  approach is that asymmetric relationships such as Xwins  X   X  X plays are often entailed by constructions involving agentive, nominalized verbs as the logical subjects of the main verb. In Example (9), the agentive nominal  X  X layer X  is logical subject to  X  X on X , the combination of which entails the asymmetric relationship of interest. Thus, to validate such an asymmetric relationship, Zanzotto,
Pennacchiotti, and Pazienza (2006) examined the frequency of the  X  X layer win X  colloca-tion using Google hit counts as a proxy for actual corpus statistics.
 conducted that are similar to that work. In general, such work focuses on the automatic acquisition of entailment relationships between verbs. Although this work has often been motivated by the need for lexical X  X emantic information in tasks such as automatic question answering, it is also relevant to the task of implicit argument identification because the derived relationships implicitly encode a participant role mapping between two predicates. For example, given a missing arg 0 for a like predicate and an explicit arg 0 = John for an eat predicate in the preceding discourse, inference rule (8) would help identify the implicit arg 0 = John for the like predicate.
 task of implicit argument identification is that previous verb relations are not defined in terms of the arg n positions used by NomBank. Rather, positions like subject and object are used. In order to identify implicit arguments in NomBank, one needs inference rules between specific argument positions (e.g., eat : arg 0 and like : arg we propose methods of automatically acquiring these fine-grained relationships for verbal and nominal predicates using existing corpora. We also propose a method of using these relationships to recover implicit arguments. 2.3 Coreference Resolution sion refers. Coreference, therefore, is the condition of two linguistic expressions having the same referent. In the following examples from the Penn TreeBank, the underlined spans of text are coreferential: (10)  X  Carpet King sales are up 4% this year, X  said owner Richard Rippe. (11) He added that the company has been manufacturing carpet since 1967.
Non-trivial instances of coreference (e.g., Carpet King and the company ) allow the author to repeatedly mention the same entity without introducing redundancy into the dis-course. Pronominal anaphora is a subset of coreference in which one of the referring expressions is a pronoun. For example, he in Example (11) refers to the same entity as Richard Rippe in Example (10). These examples demonstrate noun phrase coreference.
Events, indicated by either verbal or nominal predicates, can also be coreferential when mentioned multiple times in a document (Wilson 1974; Chen and Ji 2009).
 uations (NIST 2008) has provided a test environment for systems designed to identify these and other coreference relations. Systems based on the ACE data sets typically take a supervised learning approach to coreference resolution in general (Versley et al. 2008) and pronominal anaphor in particular (Yang, Su, and Tan 2008).
 of Japanese anaphora resolution, where a missing case-marked constituent is viewed as a zero-anaphoric expression whose antecedent is treated as the implicit argument of the predicate of interest. This behavior has been annotated manually by Iida et al. (2007), and researchers have applied standard SRL techniques to this corpus, resulting 760 in systems that are able to identify missing case X  X arked expressions in the surrounding discourse (Imamura, Saito, and Izumi 2009). Sasano, Kawahara, and Kurohashi (2004) conducted similar work with Japanese indirect anaphora. The authors used automati-cally derived nominal case frames to identify antecedents. However, as noted by Iida et al., grammatical cases do not stand in a one-to-one relationship with semantic roles inJapanese(thesameistrueforEnglish).

Centering Theory (Grosz, Joshi, and Weinstein 1995) focuses on the ways in which referring expressions maintain (or break) coherence in a discourse. These so-called  X  X en-tering shifts X  result from a lack of coreference between salient noun phrases in adjacent sentences. Discourse Representation Theory (DRT) (Kamp and Reyle 1993) is another prominent treatment of referring expressions. DRT embeds a theory of coreference into a first-order, compositional semantics of discourse. 2.4 Identifying Implicit Arguments Past research on the actual task of implicit argument identification tends to be sparse.
Palmer et al. (1986) describe what appears to be the first computational treatment of implicit arguments. In that work, Palmer et al. manually created a repository of knowl-edge concerning entities in the domain of electronic device failures. This knowledge, along with hand-coded syntactic and semantic processing rules, allowed the system to identify implicit arguments across sentence boundaries. As a simple example, consider the following two sentences (borrowed from Palmer et al. [1986]):
Example (13) does not specify precisely which entity has select lock. The domain knowl-edge, however, tells the system that only disk drive entities can have such a property.
Using this knowledge, the system is able to search the local context and make explicit the implied fact that the disk drive from Example (12) has select lock.
 (1991), who offer the following example of implicit argumentation (page 21): In Example (14), the buy event is not associated with an entity representing the seller.
This entity is introduced in Example (15) as the salesman , whose semantic properties satisfy the requirements of the buy event. Whittemore, Macpherson, and Carlson (1991) build up the event representation incrementally using a combination of semantic prop-erty constraints and DRT.
 and Carlson (1991) are quite similar. They both make use of semantic constraints on arguments, otherwise known as selectional preferences . Selectional preferences have received a significant amount of attention over the years, with the work of Ritter,
Mausam, and Etzioni (2010) being some of the most recent. The model developed in the current article uses a variety of selectional preference measures to identify implicit arguments.
 ployed due to their reliance on hand-coded, domain-specific knowledge that is difficult to create. Much of this knowledge targeted basic syntactic and semantic constructions that now have robust statistical models (e.g., those created by Charniak and Johnson [2005] for syntax and Punyakanok et al. [2005] for semantics). With this information accounted for, it is easier to approach the problem of implicit argumentation. Subse-in statistical implicit argument identification.
 basis for understanding written text. In their case study, Fillmore and Baker manually build up a semantic discourse structure by hooking together frames from the various sentences. In doing so, the authors resolve some implicit arguments found in the dis-course. This process is an interesting step forward; the authors did not provide concrete methods to perform the analysis automatically, however.
 phrase ellipsis. Consider the following sentences: (16) John kicked the ball. (17) Bill [did], too.

The bracketed text in Example (17) is a placeholder for the verb phrase kicked the ball in Example (16), which has been elided (i.e., left out). Thus, in Example (17), Bill can be thought of as an implicit argument to some kicking event that is not mentioned. If one resolved the verb phrase ellipsis, then the implicit agent (Bill) would be recovered.
Nielsen (2004) created a system able to detect the presence of ellipses, producing the bracketing in Example (17). Ellipsis resolution (i.e., figuring out precisely which verb phrase is missing) was described by Nielsen (2005). Implicit argument identification for nominal predicates is complementary to verb phrase ellipsis resolution: Both work to make implicit information explicit.
 frames in a text could be linked to form a coherent discourse interpretation (this is similar to the idea described by Fillmore and Baker [2001]). The linking operation causes two frame elements to be viewed as coreferent. Burchardt, Frank, and Pinkal (2005) propose to learn frame element linking patterns from observed data; the authors did not implement and evaluate such a method, however. Building on the work of
Burchardt, Frank, and Pinkal, this article presents a model of implicit arguments that uses a quantitative analysis of naturally occurring coreference patterns. predicates that take no local arguments (Gerber, Chai, and Meyers 2009). This approach leads to appreciable gains for certain nominals. The approach does not attempt to actually recover implicit arguments, however. 762
Events and Their Participants in Discourse, X  which evaluated implicit argument iden-tification systems over a common test set. The task organizers annotated implicit arguments across entire passages, resulting in data that cover many distinct predi-cates, each associated with a small number of annotated instances. As described by
Ruppenhofer et al. (2010), three submissions were made to the competition, with two of the submissions attempting the implicit argument identification part of the task. Chen et al. (2010) extended a standard SRL system by widening the candidate window to include constituents from other sentences. A small number of features based on the
FrameNet frame definitions were extracted for these candidates, and prediction was performed using a log-linear model. Tonelli and Delmonte (2010) also extended a stan-dard SRL system. Both of these systems achieved an implicit argument F than 0.02. The organizers and participants appear to agree that training data sparseness was a significant problem. This is likely the result of the annotation methodology: Entire documents were annotated, causing each predicate to receive a very small number of annotated examples.
 presented in this article focused on a select group of nominal predicates. To help prevent data sparseness, the size of the group was small, and the predicates were carefully chosen to maximize the observed frequency of implicit argumentation. We annotated a large number of implicit arguments for this group of predicates with the goal of training models that generalize well to the testing data. In the following section, we describe the implicit argument annotation process and resulting data set. 3. Implicit Argument Annotation and Analysis
As shown in the previous section, the existence of implicit arguments has been rec-ognized for quite some time. This type of information, however, was not formally annotated until Ruppenhofer et al. (2010) conducted their SemEval task on implicit argument identification. There are two reasons why we chose to create an independent data set for implicit arguments. The first reason is the aforementioned sparsity of the
SemEval data set. The second reason is that the SemEval data set is not built on top of the Penn TreeBank, which is the gold-standard syntactic base for all work in this article.
Working on top of the Penn TreeBank makes the annotations immediately compatible with PropBank, NomBank, and a host of other resources that also build on the TreeBank. 3.1 Data Annotation the field. To effectively use our limited annotation resources and allow the observation of interesting behaviors, we decided to focus on a select group of nominal predicates.
Predicates in this group were required to meet the following criteria: 1. A selected predicate must have an unambiguous role set. This criterion 2. A selected predicate must be derived from a verb. This article focuses 3. A selected predicate should have a high frequency in the Penn TreeBank 4. A selected predicate should express many implicit arguments. Of course, (18) *John loaned (the money to Mary). (19) *John invested (his money).
 (20) John X  X  loan was not repaid. (21) John X  X  investment was huge.

Predicates were filtered according to criteria 1 and 2 and ranked according to the product of criteria 3 and 4. We then selected the top ten, which are shown in the first column of Table 1. The role sets (i.e., argument definitions) for these predicates can be found in the Appendix on page 790. 3.1.2 Annotation Procedure. We annotated implicit arguments for instances of the ten se-lected nominal predicates. The annotation process proceeded document-by-document.
For a document d , we annotated implicit arguments as follows: 1. Select from d all non-proper singular and non-proper plural nouns that are 764 2. By design, each selected noun has an unambiguous role set. Thus, 3. For each missing argument position, search the current sentence and 4. When possible, match the textual bounds of an implicit argument
In the remainder of this article, we will use iarg n to refer to an implicit argument position n . We will use arg n to refer to an argument provided by PropBank or NomBank.
We will use p to mark predicate instances. Example (22) provides a sample annotation for an instance of the investment predicate: (22) [ iarg 0 Participants] will be able to transfer [ iarg
NomBank does not associate this instance of investment with any arguments; one can easily identify the investor ( iarg 0 ), the thing invested ( iarg thing invested in ( iarg 2 ) within the surrounding discourse, however.

Consider the following contrived example: (23) People in other countries could potentially consume large amounts of (24) Because of this, there are [ p plans] to expand [ iarg
Example (24) contains one mention of the iarg 0 (the agentive planner). It might be tempting to also mark Coke in Example (23) as an additional iarg interpretation of Coke in 23 is as a consumable fluid, however. Fluids cannot plan things, so this annotation should not be performed. This is a case of sense ambiguity between
Coke as a company and Coke as a drink. In all such cases, the annotator was asked to infer the proper sense before applying an implicit argument label.
 ments. PropBank and NomBank do not allow argument extents to overlap. Tra-ditional SRL systems such as the one created by Punyakanok, Roth, and Yih (2008) model this constraint explicitly to arrive at the final label assignment; as the 766 following example shows, however, this constraint should not be applied to implicit arguments:
Despite its embedded nature, the pronoun their in Example (25) is a perfectly reasonable implicit argument (the seller) for the marked predicate. Systems should be required to identify such arguments; thus, we included them in our annotations. 3.1.3 Inter-annotator Agreement. Implicit argument annotation is a difficult task because it combines the complexities of traditional SRL annotation with those of coreference annotation. To assess the reliability of the annotation process described previously, we compared our annotations to those provided by an undergraduate linguistics student who, after a brief training period, re-annotated a portion of the data set. For each miss-ing argument position, the student was asked to identify the textually closest acceptable implicit argument within the current and preceding sentences. The argument position was left unfilled if no acceptable constituent could be found. For a missing argument position iarg n , the student X  X  annotation agreed with our own if both identified the same implicit argument or both left iarg n unfilled. The student annotated 480 of the 1,247 predicate instances shown in Table 1.
 ment (Cohen 1960), which is based on two quantities:
The quantity 1  X  p c indicates the probability of a chance disagreement. The quantity p  X  p alone. Finally, Cohen defines  X  as follows:
Cohen X  X  kappa thus gives the probability that a chance-expected disagreement will not occur. When agreement is perfect,  X  = 1. If the observed agreement is less than the expected chance agreement, then  X  will be negative. As noted by Di Eugenio and Glass (2004), researchers have devised different scales to assess  X  . Many NLP researchers use the scale created by Krippendorff (1980):
Di Eugenio and Glass (2004) note, however, that this scale has not been rigorously defended, even by Krippendorff (1980) himself. defined as follows: where N is the total number of missing argument positions that need to be annotated, agree is equal to 1 if the two annotators agreed on iarg n are the observed prior probabilities that annotators A and B assign the argument label n toafiller,and random agree is equal to the probability that both annotators would select the same implicit argument for iarg n when choosing randomly from the discourse. In
Equation (1), terms to the right of + denote the probability that the two annotators agreed on iarg n because they did not identify a filler for it.

According to the scale of Krippendorff (1980), this value is borderline between low and moderate agreement. Possible causes for this low agreement include the brief training period for the linguistics student and the sheer complexity of the annotation task. If one considers only those argument positions for which both annotators actually located an implicit filler, Cohen X  X  kappa indicates an agreement of 93.1%. This shows that much of the disagreement concerned the question of whether a filler was present. Having agreed that a filler was present, the annotators consistently selected the same filler.
Subsequently, we demonstrate this situation with actual data. First, we present our annotations for two sentences from the same document: (26) Shares of UAL, the parent of [ iarg 1 United Airlines], were extremely active (27) And 10 minutes after the UAL trading halt came news that the UAL group
In Example (27), the predicate is marked along with the explicit arg task is to locate the implicit iarg 1 (the entity bid for) and the implicit iarg of the bid). We were able to locate these entities in a previous sentence (Example (26)).
Next, we present the second annotator X  X  (i.e., the student X  X ) annotations for the same two sentences: (28) Shares of UAL, the parent of [ iarg 1 United Airlines], were extremely active (29) And 10 minutes after the UAL trading halt came news that the UAL group 768
As shown in Example (28), the second annotator agreed with our identification of the iarg 1 ; the second annotator did not mark an implicit iarg it can be inferred. We believe this type of error can be addressed with additional train-ing. The student X  X  annotations were only used to compute agreement. We performed all training and evaluation using randomized cross-validation over the annotations we created. 3.2 Annotation Analysis
We carried out this annotation process on the standard training (2 X 21), development (24), and testing (23) sections of the Penn TreeBank. Table 1 summarizes the results. In this section, we highlight key pieces of information found in this table. 3.2.1 Implicit Arguments are Frequent. Column (3) of Table 1 shows that most predicate instances are associated with at least one implicit argument. Implicit arguments vary across predicates, with bid exhibiting (on average) more than one implicit argument per instance versus the 0.5 implicit arguments per instance of the investment and fund predicates. It turned out that the latter two predicates have unique senses that preclude implicit argumentation (more on this in Section 6). 3.2.2 Implicit Arguments Create Fuller Event Descriptions. Role coverage for a predicate instance is equal to the number of filled roles divided by the number of roles in the predicate X  X  role set. Role coverage for the marked predicate in Example (22) is 0/3 for
NomBank-only arguments and 3/3 when the annotated implicit arguments are also considered. Returning to Table 1, the fourth column gives role coverage percentages for NomBank-only arguments. The seventh column gives role coverage percentages when both NomBank arguments and the annotated implicit arguments are considered.
Overall, the addition of implicit arguments created a 71% relative (20-point absolute) gain in role coverage across the 1,247 predicate instances that we annotated. 3.2.3 The V p  X  N p Predicate Selection Metric Behaves as Desired. The predicate selection method used the V p  X  N p metric to identify predicates whose instances are likely to take implicit arguments. Column (5) in Table 1 shows that (on average) nominal predicates have 1.1 arguments in NomBank; this compared to the 2.0 arguments per verbal form of the predicates in PropBank (compare columns (5) and (6)). We hypothesized that this difference might indicate the presence of approximately one implicit argument per predicate instance. This hypothesis is confirmed by comparing columns (6) and (8):
When considering implicit arguments, many nominal predicates express approximately the same number of arguments on average as their verbal counterparts. 3.2.4 Most Implicit Arguments Are Nearby. In addition to these analyses, we examined the location of implicit arguments in the discourse. Figure 1 shows that approximately 56% of the implicit arguments in our data can be resolved within the sentence containing the predicate. Approximately 90% are found within the previous three sentences. The remaining implicit arguments require up to 4 X 6 sentences for resolution. These obser-vations are important; they show that searching too far back in the discourse is likely to produce many false positives without a significant increase in recall. Section 6 discusses additional implications of this skewed distribution. 4. Implicit Argument Model 4.1 Model Formulation
Given a nominal predicate instance p with a missing argument position iarg is to search the surrounding discourse for a constituent c that fills iarg argument model conducts this search over all constituents that are marked with a core argument label ( arg 0 , arg 1 , etc.) associated with a NomBank or PropBank predicate.
Thus, the model assumes a pipeline organization in which a document is initially analyzed by traditional verbal and nominal SRL systems. The core arguments from this stage then become candidates for implicit argumentation. Adjunct arguments are excluded.
 in the discourse. Consider the following abridged sentences, which are adjacent in their
Penn TreeBank document: (30) [Mexico] desperately needs investment. (31) Conservative Japanese investors are put off by [Mexico X  X ] investment (32) Japan is the fourth largest investor in [ c Mexico], with 5% of the total
NomBank does not associate the labeled instance of investment with any arguments, but it is clear from the surrounding discourse that constituent c (referring to Mexico) is the thing being invested in (the iarg 2 ). When determining whether c is the iarg 770 one can draw evidence from other mentions in c  X  X  coreference chain. Example (30) states that Mexico needs investment. Example (31) states that Mexico regulates investment.
These propositions, which can be derived via traditional SRL analyses, should increase our confidence that c is the iarg 2 of investment in Example (32).

We defined a binary classification function Pr ( + | p , iarg ity that the entity referred to by c fills the missing argument position iarg instance p . In the remainder of this article, we will refer to c as the primary filler , differentiating it from other mentions in the coreference chain c . This distinction is for each missing argument position. In the following section, we present the feature set used to represent each three-tuple within the classification function. 4.2 Model Features
Appendix Table B.1 lists all features used by the model described in the previous section. For convenience, Table 2 presents a high-level grouping of the features and the resources used to compute them. The broadest distinction to be made is whether a feature depends on elements of c .Featuresin Group 3 do not, whereas all others do.
The features in Group 3 characterize the predicate X  X rgument position being filled ( p and and 35% of those in the top 20. 7 The remaining features depend on elements of c in some way. Group 1 features characterize the tuple using the SRL propositions contained in the text being evaluated. Group 2 features place the p , iarg constructed ontology and compute a value based on the structure of that ontology.
Group 4 features compute statistics of the tuple within a large corpus of semantically analyzed text. Group 5 contains a single feature that captures the discourse structure properties of the tuple. Group 6 contains all other features, most of which capture the syntactic relationships between elements of c and p . In the following sections, we provide detailed examples of features from each group shown in Table 2. 4.2.1 Group 1: Features Derived from the Semantic Content of the Text. Feature 1 was often selected first by the feature selection algorithm. This feature captures the semantic properties of the candidate filler c and the argument position being filled. Consider the following Penn TreeBank sentences: (33) [ arg 0 The two companies] [ p produce] [ arg 1 market pulp, containerboard
Here we are trying to fill the iarg 0 of shipping .Let c contain a single mention, The two companies , which is the arg 0 of produce . Feature 1 takes a value of produce arg 0 . This value, which is derived from the text itself, asserts that producers are also shippers. To reduce data sparsity, we generalized the predicates to their WordNet synset IDs (creating Feature 4). We also generalized the predicates and arguments to their
VerbNet thematic roles using SemLink (creating Feature 23). Although the generalized features rely on ontologies, they do so in a trivial way that does not take advantage of the detailed structure of the ontologies. Such structure is used by features in the next group. 4.2.2 Group 2: Features Derived from Manually Constructed Ontologies. Feature 9 captures the semantic relationship between predicate X  X rgument positions by examining paths between frame elements in FrameNet. SemLink 8 maps PropBank argument positions to their FrameNet frame elements. For example, the arg 1 position of sell maps to the Goods frame element of the Sell frame. NomBank argument positions (e.g., arg be mapped to FrameNet by first converting the nominal predicate to its verb form. By mapping predicate X  X rgument structures into FrameNet, one can take advantage of the rich network of frame X  X rame relations provided by the resource.
 (34) Frame 1 . FE 1
This path describes how the frame elements at either end are related. For example, consider the frame element path between the arg 1 of sell and the arg which denote the goods being transferred: (35) Sell.Goods Inherits  X   X  X  X  X   X  Give.Theme Causes  X  X  X  X  X  X  Get.Theme
This path can be paraphrased as follows: things that are sold (Sell.Goods) are part of a more general give scenario (Give.Theme) that can also be viewed as a get scenario (Get.Theme) in which the buyer receives something (Buy.Goods). This complex world knowledge is represented compactly using the relationships defined in FrameNet. In our experiments, we searched all possible frame element paths of length five or less that use the following relationships: 772 In Example (37) we are looking for the iarg 1 (thing sold) of sale . The path shown in
Example (35) indicates quite clearly that the candidate cars from Example (36), being the entity purchased, is a suitable filler for this position. Lastly, note that the value for Feature 9 is the actual path instead of a numeric value. When c contains multiple
Ultimately, these instantiations are binarized into the LibLinear input format, so the existence of multiple feature values does not pose a problem.
 tures the distance between predicate X  X rgument positions within the VerbNet hierarchy.
Consider the following VerbNet classes: 13.2 lose, refer, relinquish, remit, resign, restore, gift, hand out, pass out, shell out 13.5.1.1 earn, fetch, cash, gain, get, save, score, secure, steal
The path from earn to lose in the VerbNet hierarchy is as follows: other X  X hey describe two possible outcomes of a financial transaction. The VerbNet path tion can be used to identify implicit arguments in situations such as the following from the Penn TreeBank (abridged):
In Example (40) we are looking for the iarg 0 (i.e., entity losing something) for the loss predicate. According to SemLink, this argument position maps to the 13.2.Agent role in
VerbNet. In Example (39), we find the candidate implicit argument Monsanto Co. , which is the arg 0 to the earning predicate in that sentence. This argument position maps to the 13.5.1.1.Agent role in VerbNet. These two VerbNet roles are related according to the
VerbNet path in Example (38), producing a value for Feature 59 of four. This relatively small value supports an inference of Monsanto Co. as the iarg note that a VerbNet path only exists when the thematic roles are identical. For example, a
VerbNet path would not exist between 13.5.1.1.Theme and 13.2.Agent because the roles are not compatible. Lastly, recall that c might contain multiple coreferential elements.
In such a situation, the minimum path length is selected as the value for this feature. 4.2.3 Group 3: Filler-independent Features. Many of the features used by the model do not depend on elements of c . These features are usually specific to a particular predicate.
Consider the following example: (41) Statistics Canada reported that its [ arg 1 industrial X  X roduct] [ p price] index
The  X  X  p price] index X  collocation is rarely associated with an arg an iarg 0 in the annotated data (both argument positions denote the seller). Feature 25 accounts for this type of behavior by encoding the syntactic head of p  X  X  right sibling.
The value of Feature 25 for Example 41 is price:index . Contrast this with the following: (42) [ iarg 0 The company] is trying to prevent further [ p price] drops. distinction between the two uses of price : the former cannot easily take an iarg the latter can. Many other features in Table B.1 depend only on the predicate and have values that take the form predicate:feature value . 4.2.4 Group 4: Features Derived from Corpus Statistics. Feature 13 is inspired by the work of Chambers and Jurafsky (2008), who investigated unsupervised learning of narrative event sequences using pointwise mutual information (PMI) between syntactic positions. We extended this PMI score to semantic arguments instead of syntactic dependencies.
Thus, the value for this feature is computed as follows:
We computed Equation (2) by applying verbal SRL (Punyakanok, Roth, and Yih 2008), nominal SRL (Gerber, Chai, and Meyers 2009), and coreference identification (OpenNLP) to the Gigaword corpus (Graff 2003); because these tools are not fast enough to process all 1,000,000 documents in the corpus, however, we selected subsets of the corpus for each p 1 / p 2 combination observed in our implicit argument data. We first indexed the Gigaword corpus using the Lucene search engine. this index using the simple boolean query  X  p 1 AND p 2 , X  which retrieved documents relevant to the predicates considered in Equation (2). Assuming the resulting data have N coreferential pairs of arguments, the numerator in Equation (2) is defined as follows:
In Equation (3), # coref returns the number of times the given argument positions are found to be coreferential. In order to penalize low-frequency observations with 774 artificially high scores, we used the simple discounting method described by Pantel and Ravichandran (2004) resulting in the following modification of Equation (3):
Thus, if two argument positions are rarely observed as coreferent, the value be small, reducing the PMI score. The denominator in Equation (2) is computed with a similar discount factor:
Thus, if either of the argument positions is rarely observed as coreferent with other Equation (2) large, reducing the PMI score. In general, the discount factors reduce the PMI score for argument positions that are not frequent in the corpus.
 been chosen specifically for the calculation at hand. Table 3 shows a sample of targeted
PMI scores between the arg 1 of loss and other argument positions. There are two things to note about this data: First, the argument positions listed are all naturally related to the arg 1 of loss . Second, the discount factor changes the final ranking by moving the less frequent recoup predicate from a raw rank of 1 to a discounted rank of 3, preferring instead the more common win predicate.
 suggests that the marked candidate c , being the arg 1 of win , would be a suitable filler for this position. Lastly, note that if c were to form a coreference chain with other constituents, it would be possible to calculate multiple PMI scores. In such cases, the targeted PMI feature uses the maximum of all scores.
 the strength of attraction for a predicate X  X rgument position to a particular word or class of words. To calculate the value for this feature, we used the information X  X heoretic model proposed by Resnik (1996), which is defined as follows:
In Equation (6), Pref calculates the preference for a WordNet synset s in the given predicate X  X rgument position. Prior and posterior probabilities for s were calculated by examining the arguments present in the Penn TreeBank combined with 20,000 docu-ments randomly selected from the Gigaword corpus. PropBank and NomBank supplied arguments for the Penn TreeBank, and we used the aforementioned verbal and nominal
SRL systems to extract arguments from Gigaword. The head word for each argument was mapped to its WordNet synsets, and counts for these synsets were updated as suggested by Resnik (1996). Note that a synset s that is not observed in the training data will receive a score of zero because Pr ( s | p , arg a single word can map to multiple synsets if its sense is ambiguous, however. Given a w is defined as follows:
That is, the preference for a word is computed as the average preference across all possi-ble synsets. The final value for Feature 27 is computed using the word-based preference score defined in Equation (7). Given a candidate implicit argument c comprising the primary filler c and its coreferent mentions, the following value is obtained: In Equation (8), each f is the syntactic head of a constituent from c . The value of
Equation (8) is in (  X  X  X  , +  X  ), with larger values indicating higher preference for c as the implicit filler of position iarg n .
 implicit arguments might be identified using observed coreference patterns in a large corpus of text. Our implementation of this feature uses the same data used for the previous feature: arguments extracted from the Penn TreeBank and 20,000 documents randomly selected from Gigaword. Additionally, we identified coreferent arguments in this corpus using OpenNLP. Using this information, we calculated the probability of coreference between any two argument positions. As with Feature 13, we used 776 discounting to penalize low X  X requency observations, producing an estimate of coreference probability as follows: erential with p 2 , arg j given p 1 , arg i is coreferential with something. X  For example, we observed that the arg 1 for predicate reassess (the entity reassessed) is coreferential with six other constituents in the corpus. Table 4 lists the argument positions with which this argument is coreferential along with the raw and discounted probabilities.
The discounted probabilities can help identify the implicit argument in the following contrived examples: candidate (an arg 1 to rethink ) is likely to fill this missing argument position. When c forms a coreference chain with other constituents, this feature uses the minimum coreference probability between the implicit argument position and elements in the chain. the discourse relation (if any) that holds between the candidate constituent c and the filled predicate p . Consider the following example:
In this case, a comparison discourse relation (signaled by the underlined text) holds be-tween the first and second sentence. The coherence provided by this relation encourages name of the discourse relation (e.g., comparison ) whose two discourse units cover the candidate ( iarg 0 above) and filled predicate ( p above). Throughout our investigation, we used gold-standard discourse relations provided by the Penn Discourse TreeBank (Prasad et al. 2008). 4.2.6 Group 6: Other Features. A few other features that were prominent according to our feature selection process are not contained in the groups described thus for. Feature 2 encodes the sentence distance from c (the primary filler) to the predicate for which we are filling the implicit argument position. The prominent position of this feature agrees with our previous observation that most implicit arguments can be resolved within a few sentences of the predicate (see Figure 1 on p. 770). Feature 3 is another simple yet highly ranked feature. This feature concatenates the head of an element of c with p and iarg n . For example, in sentences (45) and (46), this feature would have a value of this feature by replacing the head word with its WordNet synset. 4.2.7 Comparison with Features for Traditional SRL. The features described thus far are quite different from those used in previous work to identify arguments in the traditional nominal SRL setting (see the work of Gerber, Chai, and Meyers 2009). The most impor-tant feature used in traditional SRL X  X he syntactic parse tree path X  X s notably absent.
This difference is due to the fact that syntactic information, although present, does not play a central role in the implicit argument model. The most important features are those that capture semantic properties of the implicit predicate X  X rgument position and the candidate filler for that position. 4.3 Post-processing for Final Output Selection
Without loss of generality, assume there exists a predicate instance p with two missing argument positions iarg 0 and iarg 1 . Also assume that there are three candidate fillers c , c 2 ,and c 3 within the candidate window. The discriminative model will calculate the probability that each candidate fills each missing argument position. Graphically:
There exist two constraints on possible assignments of candidates to positions. First, a candidate may not be assigned to more than one missing argument position. To enforce 778
Second, a missing argument position can only be filled by a single candidate. To enforce this constraint, only the top-scoring cell in each column is retained, leading to the following:
Having satisfied these constraints, a threshold t is imposed on the remaining cell prob-final assignment would be as follows:
In this case, c 3 fills iarg 0 with probability 0.6 and iarg outcome is desirable because not all argument positions have fillers that are present in the discourse. 5. Evaluation 5.1 Data
All evaluations in this study were performed using a randomized cross-validation configuration. The 1,247 predicate instances were annotated document by document.
In order to remove any confounding factors caused by specific documents, we first randomized the annotated predicate instances. Following this, we split the predicate instances evenly into ten folds and used each fold as testing data for a model trained on the instances outside the fold. This evaluation set-up is an improvement versus the one we previously reported (Gerber and Chai 2010), in which fixed partitions were used for training, development, and testing.
 system identified missing argument positions and generated a set of candidates for candidate implicit argument c (the primary filler) was annotated as filling the missing argument position; otherwise, the candidate three-tuple was given a negative label.
During testing, the system was presented with each predicate instance and was required to identify all implicit arguments for the predicate.
PropBank and NomBank information in all documents. This factored out errors from traditional SRL and affected the following stages of system operation: We also assumed the existence of gold-standard syntactic structure when possible.
This was done in order to focus our investigation on the semantic nature of implicit arguments. 5.2 Scoring Metrics
We evaluated system performance using the methodology proposed by Ruppenhofer et al. (2010). For each missing argument position of a predicate instance, the system was required to either (1) identify a single constituent that fills the missing argument position or (2) make no prediction and leave the missing argument position unfilled. To give partial credit for inexact argument bounds, we scored predictions using the Dice coefficient, which is defined as follows: 780 Predicted contains the tokens that the model believes fill the implicit argument position.
True is the set of tokens from a single annotated constituent that fills the missing argument position. The model X  X  prediction receives a score equal to the maximum Dice overlap across any one of the annotated fillers ( AF ):
Precision is equal to the summed prediction scores divided by the number of argument positions filled by the model. Recall is equal to the summed prediction scores divided by the number of argument positions filled in the annotated data. Predictions not cover-ing the head of a true filler were assigned a score of zero. the following true and predicted labelings:
In the ground-truth (49) there are three implicit argument positions to fill. The hypo-thetical system has made predictions for two of the positions. The prediction scores are: Score ( iarg 2 investment funds) = max { Dice (investment funds, other investment funds), Precision, recall, and F 1 for the example predicate are calculated as follows:
We calculated the F 1 score for the entire testing fold by aggregating the counts used in the above precision and recall calculations. Similarly, we aggregated the counts across all folds to arrive at a single F 1 score for the evaluated system.

Tibshirani (1993) to test the significance of the performance difference between various systems. Given a test pool comprising M missing argument positions iarg performance difference as follows: 1. Create r random resamples from M with replacement. 2. For each resample R i , compute the system performance difference 3. Find the largest symmetric interval [ min , max ] around the mean of D that 4. The exact p-value equals the percentage of elements in D that are not in
Experiments have shown that this simple approach provides accurate estimates of significance while making minimal assumptions about the underlying data distribution (Efron and Tibshirani 1993). Similar randomization tests have been used to evaluate information extraction systems (Chinchor, Lewis, and Hirschmant 1993). 5.3 LibLinear Model Configuration
Given a testing fold F test and a training fold F train , we performed floating forward feature subset selection using only the information contained in F algorithm similar to the one described by Pudil, Novovicova, and Kittler (1994). As part of the feature selection process, we conducted a grid search for the best c and w
LibLinear parameters, which govern the per-class cost of mislabeling instances from a particular class (Fan et al. 2008). Setting per-class costs helps counter the effects of class size imbalance, which is severe even when selecting candidates from the current and previous few sentences (most candidates are negative). We ran the feature selection model parameters are slightly different for each fold. 12 logistic regression solver and a candidate selection window of two sentences prior. As shown in Figure 1, this window imposes a recall upper bound of approximately 85%.
The post-processing prediction threshold t was learned using a brute-force search that maximized the system X  X  performance over the data in F train 5.4 Baseline and Oracle Models We compared the supervised model with the simple baseline heuristic defined below:
The normalization allows, for example, an existing arg 0 for the verb invested to fill an iarg 0 for the noun investment . This heuristic outperformed a more complicated heuristic that relied on the PMI score described in Section 4.2. We also evaluated an oracle model that made gold-standard predictions for candidates within the two-sentence prediction window. 782 5.5 Results
Table 5 presents the evaluation results for implicit argument identification. Overall, the discriminative model increased F 1 performance by 21.4 percentage points (74.1%) compared to the baseline (p &lt; 0.0001). Predicates with the highest number of implicit arguments ( sale and price ) showed F 1 increases of 13.7 and 17.5 percentage points, all predictions, and the F 1 difference between the discriminative and oracle systems is results and a listing of features and model parameters used for each fold. assistant X  X  annotations against a small portion of the evaluation data comprising 275 filled implicit arguments. The assistant achieved an overall F same two-sentence candidate window used by the baseline, discriminative, and oracle models. Using an infinite candidate window, the assistant increased F to 64.2%. Although these results provide a general idea about the performance upper bound, they are not directly comparable to the cross-validated results shown in Table 5 because the assistant did not annotate the entire data set. 6. Discussion 6.1 Training Set Size As described in Section 3.1, implicit argument annotation is an expensive process.
Thus, it is important to understand whether additional annotation would benefit the ten predicates considered. In order to estimate the potential benefits, we measured the effect of training set size on system performance. We retrained the discriminative model for each evaluation fold using incrementally larger subsets of the complete training set for the fold. Figure 2 shows the results, which indicate minimal gains beyond 80% of the training set. Based on these results, we feel that future work should emphasize feature and model development over training data expansion, as gains appear to trail off significantly. 6.2 Feature Assessment
Previously (Gerber and Chai 2010), we assessed the importance of various implicit argument feature groups by conducting feature ablation tests. In each test, the discrimi-native model was retrained and reevaluated without a particular group of features. We summarize the findings of this study in this section. 6.2.1 Semantic Roles are Essential. We observed statistically significant losses when ex-missing argument position. For example, Feature 1 appears as the top-ranked feature in eight out of ten fold evaluations (see Appendix Table C.1). This feature is formed by concatenating the filling predicate X  X rgument position with the filled predicate X  argument position, producing values such as invest.arg 0 -lose.arg that the entity performing the investing is also the entity losing something. This type of commonsense knowledge is essential to the task of implicit argument identification. 6.2.2 Other Information is Important. Our 2010 study also found that semantic roles are significant losses. This indicates that other features contribute useful information to the task. relations (Feature 67) from the model. Discourse structure has received a significant amount of attention in NLP; it remains a very challenging problem, however, with state-of-the-art systems attaining F 1 scores in the mid-40% range (Sagae 2009). Our 2010 work as well as the updated work presented in this article used gold-standard discourse relations from the Penn Discourse TreeBank. As shown by Sagae (2009), these relations are difficult to extract in a practical setting. In our 2010 work, we showed that removing discourse relations from the model did not have a statistically significant effect on performance. Thus, this information should be removed in practical applications of the model, at least until better uses for it can be identified. 6.2.4 Relative Feature Importance. We extended earlier findings by assessing the relative importance of the features. We aggregated the feature rank information given in Ap-pendix Table C.1. For each evaluation fold, each feature received a point value equal to its reciprocal rank within the feature list. Thus, a feature appearing at rank 5 for a arriving at the values shown in the final column of Appendix Table B.1. The scores confirm the earlier findings. The highest scoring feature relates the semantic roles of the candidate argument to the missing argument position. Non-semantic information such as the sentence distance (Feature 2) also plays a key role. Discourse structure is consistently ranked near the bottom of the list (Feature 67). 6.3 Error Analysis
Table 6 lists the errors made by the system and their frequencies. As shown, the single most common error (type 1) occurred when a true filler was classified but an incor-rect filler had a higher score. This occurred in approximately 31% of the error cases.
Often, though, the system did not classify a true implicit argument because such a candidate was not generated. Without such a candidate, the system stood no chance of making a correct prediction. Errors 3 and 5 combined (also 31%) describe this behavior.
Type 3 errors resulted when implicit arguments were not core (i.e., arg to other predicates. To reduce class imbalance, the system only used core arguments as candidates; this came at the expense of increased type 3 errors, however. In many cases, the true implicit argument filled a non-core (i.e., adjunct) role within PropBank or NomBank.
 the candidate window. Oracle recall (see Table 5) indicates the nominals that suffered most from windowing errors. For example, the sale predicate was associated with the highest number of true implicit arguments, but only 72% of those could be resolved within the two-sentence candidate window. Empirically, we found that extending the candidate window uniformly for all predicates did not increase F additional false positives were identified. The oracle results suggest that predicate-specific window settings might offer some advantage for predicates such as fund and bid , which take arguments at longer ranges.
 The former would be reduced by increasing t and thus filtering out bad predictions.
The latter would be reduced by lowering t and allowing more true fillers into the final output. It is unclear whether either of these actions would increase overall performance, however. 6.4 The Investment and Fund Predicates index X  collocation. We observed that this collocation is rarely associated with either an fund predicates. Although these two predicates are frequent, they are rarely associated with implicit arguments: investment takes only 52 implicit arguments and fund takes such as  X  X  p investment] banker, X   X  X tock [ p fund], X  and  X  X utual [ p fund], X  which use predicate senses that are not eventive and take no arguments. Such collocations also violate the assumption that differences between the PropBank and NomBank argument structure for a predicate are indicative of implicit arguments (see Section 3.1 for this assumption).
 such as investment and fund because the incorrect prediction of implicit arguments for 786 them can lower precision. This is precisely what happened for the investment predicate ( P = 33%). The model incorrectly identified many implicit arguments for instances such as  X  X  p investment] banker X  and  X  X  p investment] professional, X  which take no arguments.
The right context of investment should help the model avoid this type of error; however in many cases this was not enough evidence to prevent a false positive prediction.
It might be helpful to distinguish eventive nominals from non-eventive ones, given the observation that some non-eventive nominals rarely take arguments. Additional investigation is needed to address this type of error. 6.5 Improvements versus the Baseline
The baseline heuristic covers the simple case where identical predicates share argu-ments in the same position. Because the discriminative model also uses this information (see Feature 8), it is interesting to examine cases where the baseline heuristic failed but the discriminative model succeeded. Such cases represent more difficult inferences.
Consider the following sentence:
Neither NomBank nor the baseline heuristic associate the marked predicate in Exam-ple (51) with any arguments; the feature-based model was able to correctly identify the marked iarg 2 as the entity being invested in, however. This inference relied on a number of features that connect the invest event to the sell event (e.g., Features 1, 4, and 76). These features captured a tendency of investors to sell the things they have invested in.
 argument. Consider the following adjacent sentences: was able to correctly identify Olivetti from Example (54) as the implied filler of this argument position. The inference involved two key steps. First, the model identified coreferent mentions of Olivetti in Examples (52) and (53). In these sentences, Olivetti participates in the marked exporting and shipping events. Second, the model identified a tendency for exporters and shippers to also be sellers (e.g., Features 1, 4, and 23 made large contributions to the prediction). Using this knowledge, the system extracted infor-mation that could not be extracted by the baseline heuristic or a traditional SRL system. 6.6 Comparison with Previous Results
In a previous study, we reported initial results for the task of implicit argument identi-fication (Gerber and Chai 2010). This article presents two major advancements versus our prior work. First, this article presents a more rigorous evaluation set-up, which was not used in our previous study. Our previous study used fixed partitions of training, development, and testing data. As a result, feature and model parameter selections overfit the development data; we observed a 23-point difference in F development (65%) and testing (42%) partitions. The small size of the testing set also led to small sample sizes and large p-values during significance testing. The cross-validated approach reported in this article alleviated both problems. The F between training and testing was approximately 10 percentage points for all folds, and to directly compare the evaluation scores in the two studies; the methodology in the current article is preferable for the reasons mentioned, however.
 described in our previous study. In particular, we experimented with corpus statistics derived from sub-corpora that were specifically tailored to the predicate instance under consideration. See, for example, Feature 13 in Appendix B, which computed PMI scores between arguments found in a custom sub-corpus of text. This feature was ranked highly by a few of the evaluation folds (see Appendix B for feature rankings). 7. Conclusions
Previous work provided a partial solution to the problem of nominals with implicit arguments (Gerber, Chai, and Meyers 2009). The model described in that work is able to accurately identify nominals that take local arguments, thus filtering out predicates whose arguments are entirely implicit. This increases standard nominal SRL perfor-mance by reducing the number of false positive argument predictions; all implicit arguments remain unidentified, however, leaving a large portion of the corresponding event structures unrecognized.
 inal predicates. The study was based on a manually created corpus of implicit argu-ments, which is freely available for research purposes. Our results show that models can be trained by incorporating information from a variety of ontological and corpus-based sources. The study X  X  primary findings include the following: 1. Implicit arguments are frequent. Given the predicates in a document, 2. Implicit arguments can be automatically identified. Using the annotated 788 3. Much work remains. The study presented in the current article was very Appendix A: Role Sets for the Annotated Predicates Listed here are the role sets for the ten predicates used in this article. 790 Appendix B: Implicit Argument Features 792 Appendix C: Per-fold Implicit Argument Identification Results Acknowledgments References 796
