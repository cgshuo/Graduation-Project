 Low-rank Matrix Factorization (MF) methods provide one of the simplest and most effective approaches to collabo-rative filtering. This paper is the first to investigate the problem of efficient retrieval of recommendations in a MF framework. We reduce the retrieval in a MF model to an apparently simple task of finding the maximum dot-product for the user vector over the set of item vectors. However, to the best of our knowledge the problem of efficiently finding the maximum dot-product in the general case has never been studied. To this end, we propose two techniques for efficient search  X  (i) We index the item vectors in a binary spatial-partitioning metric tree and use a simple branch-and-bound algorithm with a novel bounding scheme to efficiently obtain exact solutions. (ii) We use spherical clustering to index the users on the basis of their preferences and pre-compute recommendations only for the representative user of each cluster to obtain extremely efficient approximate solutions. We obtain a theoretical error bound which determines the quality of any approximate result and use it to control the approximation. Both these simple techniques are fairly in-dependent of each other and hence are easily combined to further improve recommendation retrieval efficiency. We evaluate our algorithms on real-world collaborative-filtering datasets, demonstrating more than  X  7 speedup (with re-spect to the naive linear search) for the exact solution and over  X  250 speedup for approximate solutions by combining both techniques.
 H.4 [ Information Systems Applications ]: Miscellaneous Algorithms inner-product, fast retrieval, collaborative filtering
Recommender systems based on Matrix Factorization (MF) models have repeatedly demonstrated better accuracy than other methods such as nearest neighbor models and restricted Boltzmann machines [1, 8]. However, large scale MF models for real world recommender systems (e.g., [5, 12, 7]) run into a difficulty rarely discussed in the academic literature  X  the computational cost of finding the top-rated items for every user in the system once the model has been trained.
In MF models, the predicted rating of a user for an item boils down to a dot-product between two vectors represent-ing the user and the item (we will discuss this explicitly in section 2). Constructing the entire #USERS  X  #ITEMS preference matrix (or even just for the top-rated items for every user) requires heavy computational (and space) re-sources. For example, the recently published Yahoo! Mu-sic dataset [8] has 1,000,990 users and 624,961 music items. Generating the optimal recommendations in this dataset re-quires over 6  X  10 12 dot-products using a naive algorithm. A 50-dimensional model required 134 hours (over 5 days) to find optimal recommendations for all the users 1 . In terms of storage, saving the whole preference matrix requires over 5TB of disk-space. Moreover, this dataset of 10 6 users is just a small sample of the actual Yahoo! Music dataset and the problem worsens with larger numbers.

The problem of finding the maximum dot-product for a given query p (in this case, a user) over a set of points S (in this case, the items) is to find the point q  X  S such that: Surprisingly, we did not find any technique to efficiently solve this problem; a linear search over the set of points appears to be the state-of-the-art! Moreover, the number of queries in our application is very high (possibly higher than the number of points). The focus of our paper is to develop algorithms to solve (1) for multiple queries more efficiently than the linear-search algorithm . Our contributions are:  X  A simple branch-and-bound algorithm on a tree index  X  An approximate scheme that pre-computes solutions for  X  A theoretical error bound which determines the quality
U sing an Intel Xeon (E7320) CPU running at 2.13GHz.
The computational cost of recommendation retrieval can be mitigated by parallelization. One possible way of par-allelizing involves dividing the users across cores/machines  X  each worker can compute the recommendations for a sin-gle user (or a small set of users). However, this is wasteful in resources and requires complex setups. Moreover, this form of parallelization does not mitigate the high latency of computing recommendations for a single user. Map-reduce parallelization can reduce the single user latency. However, a single map-reduce can at best take O ( for the retrieval task of a single user.

Our proposed techniques are orthogonal to paralleliza-tion, and can be parallelized to improve the scalability. The branch-and-bound algorithm reduces single query latency (and can have O (log( #ITEMS )) retrieval time). Our ap-proximate scheme reduces the number of users by only choos-ing a small set of representative users.

The problem of efficient Retrieval of Recommendations (RoR) in collaborative filtering has been previously stud-ied. Large-scale recommender systems use techniques like min-hash clustering of users, probabilistic latent semantic indexing, and co-visitation counts to achieve fair scalabil-ity [5]. A more recent method based on multidimensional scaling embeds both the users and items in a common Eu-clidean space [11], reducing the retrieval task to the problem of k -nearest-neighbor search. The plethora of algorithms for nearest-neighbor search can then be used for efficient RoR. While these methods show significant improvements in retrieval times, they deviate from the more accurate MF framework. Our proposed methods are the first efficient re-trieval algorithms in the MF framework.

In section 2, we introduce the MF framework and reduce the task of RoR to a relatively simpler task of finding the best-matched items with respect to the dot-product of their representative vectors with the vector representing the user. Section 3 contrasts this dot-product based best matching problem to existing best matching problems in literature. In section 4, we index the items as a Metric tree and then propose a novel branch-and-bound algorithm to efficiently obtain the e xact top predicted preferences for a single user. To obtain further scalability, section 5 presents an approxi-mated method by clustering users with X  X imilar tastes X . The efficiency is obtained by pre-computing the top recommen-dations for the representative users (the cluster centers). We also present the theoretical worst-case error bound used to control the approximation. For further improvement, we combine both techniques and evaluate our proposed meth-ods in section 6 on prominent collaborative filtering datasets.
We reserve special indexing letters for distinguishing users from items: for users u and v , and for items i and j . A rating r ui indicates the rating given by user u to item i . We denote by  X  x;y the angle between the vectors x and y at the origin. Finally, we denote the l 2 -norm of a vector x as  X  x  X  .
In MF models, each user u is associated with a user-traits vector p u  X  R D , and each item i with an item-traits vector q  X  R D . Predicted ratings are obtained using the rule: where  X  is the overall mean rating value and b i and b u are scalers that represent the item and user biases respectively.
A user bias models a user X  X  tendency to rate on a higher or lower scale than the average rater, while the item bias captures the extent of the item popularity. The user X  X  trait vector p u represents the user X  X  preferences or  X  X aste X . Sim-ilarly, the item X  X  traits vector represent the item X  X  latent characteristics. The dot-product p  X  u q i is the personalization component which captures user X  X  u affinity to item i .
A significant strength of MF models is their natural ability to easily incorporate additional information. For example, temporal dynamics and taxonomy components can be easily incorporated into the MF model [6]. In such models the pre-diction equation takes a more complex form. Without loss of generality, we will only discuss the basic model (equation 2). However, the more complex models have more parameters which are usually additive. Hence, the proposed reduction can be easily extended and applied to these model as well. In fact, we use one such complex MF model (described in [6]) for evaluating our proposed methods.

There are various techniques for training MF models. Gen-erally a cost function is defined on the prediction error (e.g., RMSE) and optimization is followed by Stochastic Gradient Descent or an Alternating Least Squares algorithm. The reader is referred to [13] for more details on those learn-ing techniques. The training produces estimates of the MF model parameters (the trait vectors and the biases). Given these parameters, we will demonstrate that the user X  X  pref-erence for any item can be reduced to a simple dot-product.
RoR in a trained MF model involves finding the set of K items for a user u with maximum predicted ratings. Equa-tion 2 implies that for a given user u , ordering the items is independent of  X  and the user X  X  bias b u . Thus, we can ignore these parameters without affecting the preferred ordering of the items and obtain an effective rating: It is important to note that this is only applicable during the RoR phase (after the training). Ignoring these components during the training will inevitably result in poor accuracy.
By appending the item bias to the item vector, the effec-tive rating (equation 3) reduces to a simple dot-product: where  X  p u = [ p  X  u 1]  X  ,  X  q i = [ q  X  i b i ]  X  and  X  between  X  p u and  X  q i at the origin.

Equation 4 implies that for a given user  X  p u , the items ordering is independent of the norm  X   X  p u  X  and only depends on the user vector through the angle  X  ~ p u ; ~ q i . Hence, without loss of generality, we normalize the user vector to a unit vector  X  p u =  X  p u /  X   X  p u  X  to further simplify equation 4 to: Note that while we normalize the concatenated user vectors, the concatenated item vectors can not be normalized without Fi gure 1: The most popular musical tracks and gen-res in the Yahoo! Music dataset are embedded into a 2-dimensional. The open cone suggests the region of highly preferred items for the user (her logo is at the center of the cone). Note how the learned embedding separates Rock and similar items from Hip-Hop and similar items. The low dimensional-ity (which is required for a visualization), causes a small number of items to be wrongly folded near less related items (e.g., Bob Marley). loss of accuracy. If the item vectors were normalized, the RoR problem would have been reduced to the well studied nearest neighbor search (as explained in section 3). How-ever, such a normalization will introduce a distortion on the balance between the original item trait vector and the item bias which constitute the concatenated vector  X  q i . This would evidently result in an incorrect solution.

Denoting the concatenated user and item vectors as p u and q i respectively, and the effective rating for the task of retrieval as r ui , RoR reduces to the following task: Given a user query p u , we want to find an item q i  X  S such that: Hence the RoR task is equivalent to the problem of finding the best-match for a query in a set of points with respect to the dot-product (described in equation 1). A very simplistic visualization of this task is depicted in Figure 1. For the given user, the best recommendations (in this case songs) lie within the open cone around the user vector (maximizing the cos (  X  p u ;q i ) term) and are as far as possible from the origin (maximizing the  X  q i  X  term).
Efficiently finding the best match using the dot-product (equation 6) appears to be very similar to much existing work in the literature. Finding the best match with re-spect to the Euclidean (or more generally L p ) distance is the widely studied problem of nearest-neighbor search in metric spaces [4]. The nearest-neighbor search problem (in metric space) can be solved approximately with the popular Locality-sensitive hashing (LSH) method [9]. LSH has been extended to other forms of similarity functions (as opposed to the distance as a dissimilarity function) like the cosine similarity [3]. In this section, we show that the problem stated in equation 6 is different from these existing prob-lems.

The problem of finding the nearest-neighbor in this setting is to find a point q i  X  S for a query p u such that: q i = arg min If all the points in S are normalized to the same length, then the problem of finding the best match with respect to the dot-product is equivalent to the problem of nearest-neighbor search in any metric space. However, without this restric-tion, the two problems can yield very different answers.
Finding the best match with respect to the cosine simi-larity is to find a point q i  X  S for a query p u such that As in the previous case, the best match with cosine similar-ity is the best match with dot-products if all the points in the set S are normalized to the same length. Under gen-eral conditions, the best matches with these two similarity functions can be very different.

LSH involves constructing hashing functions h which sat-isfy the following for any pair of points q, p  X  R D : where sim( q, p )  X  [0 , 1] is the similarity function of inter-est. For our situation, we can scale our dataset such that  X  q  X  S,  X  q  X   X  1 and assume that the data is in the first quadrant (such as in non-negative matrix factorization mod-els [19]). In that case, sim( q, p ) = q  X  p  X  [0 , 1] is our simi-larity function of interest.

For any similarity function to admit a locality sensitive hash function family (as defined in equation 7), the distance function d ( q, p ) = 1  X  sim( q, p ) must satisfy the triangle inequality (Lemma 1 in [3]). However, the distance function d ( q, p ) = 1  X  q  X  p does not satisfy the triangle inequality. Hence LSH cannot be applied to the dot-product similarity function even in restricted domains (the first quadrant).
Unlike the distance functions in metric space, dot-products do not induce any form of triangle inequality (even under some assumptions as mentioned in the previous section). Moreover, this lack of any induced triangle inequality causes the similarity function induced by the dot-products to have no admissible family of locality sensitive hashing functions. Any modification to the similarity function to conform to widely used similarity functions (like Euclidean distance or Cosine-similarity) will create inaccurate results.
Mo reover, dot-products lack the basic property of coinci-dence  X  the self similarity is highest. For example, the Eu-clidean distance of a point to itself is 0; the cosine-similarity of a point to itself is 1. The dot-product of a point q to itself is  X  q  X  2 . There can possibly be many other points v ( i = 1 , 2 , . . . ) in the set such that q  X  v i &gt;  X 
Without any assumptions, this problem of obtaining the best match with respect to the dot-product is inherently harder than the previously addressed similar problems. This is possibly the reason why there is no existing work for this problem without any restrictions on the domain.
In this section, we describe metric trees and develop a novel bound to use with a simple branch-and-bound algo-rithm to provide the first method to efficiently obtain the exact best-matches with respect to the dot-products.
Metric trees [16] are binary space-partitioning trees that are widely used for the task of indexing datasets in Eu-clidean spaces. The space is partitioned into overlapping hyper-spheres (balls) containing the points (figure 2). We use a simple metric tree construction heuristic that tries to approximately pick a pair of pivot points farthest apart from each other [15] 2 , and splits the data by assigning points to their closest pivot. The tree T is built hierarchically and each node in the tree is defined by the mean of the data in that node ( T. center) and the radius of the ball around the mean enclosing the points in the node ( T. radius). The tree has leaves of size at most N 0 . The splitting and the recur-sive tree construction algorithm is presented in Algorithms 1 &amp; 2.

The tree is space efficient since every node only stores the indices of the item vectors instead of the item vectors them-selves. Hence the matrix for the items is never duplicated. Another implementation optimization is that the vectors in the items X  matrix are sorted in place (during the tree con-struction) such that all the items in the same node are ar-ranged serially in the matrix. This avoids random memory access while accessing all the items in the same leaf node. Fi gure 2: Metric-trees  X  note that while all the points in a child node lie also inside the parent ball, the child ball itself does not necessarily lie within the parent ball.
Metric trees are used for efficient nearest neighbor search and are fairly scalable in moderately high dimensions [15,
Th e intuition behind this heuristic is that these 2 points farthest from each other might lie in the principal direction (the direction of the principal eigenvector of the data). Al gorithm 1 MakeMetricTreeSplit(Data S ) Al gorithm 2 MakeMetricTree(Set of items S ) Fi gure 3: Metric-tree Construction: The object Q.S denotes the set of items in the node Q , Q. center denotes the Euclidean mean of the items in the node Q and Q. radius denotes the minimum radius of the ball centered around Q. center enclosing all the items in the node Q . 14]. The search employs a depth-first branch-and-bound al-gorithm. A nearest-neighbor query is answered by traversing the tree in a depth-first manner X  going down the node closer to the query first and bounding the minimum possible dis-tance to items in other branches with the triangle-inequality. If this branch is farther away than the current neighbor can-didate, the branch is removed from computation.

Since the triangle inequality does not hold for the dot-product, we present a novel analytical upper bound for the maximum possible dot-product of a user vectors with points (in this case, items) in a ball. We then employ a similar branch-and-bound algorithm for the purposes of searching for the K -highest dot-products (as opposed to the minimum pairwise distance in K -nearest-neighbor search).
Let B r q c be the ball of items centered around q c with radius r . Suppose that q  X  is the best possible recommendation in the ball B r q c for the user represented by the vector p be the Euclidean distance between the ball center q c and the best possible recommendation q  X  (by definition, r  X   X  r ). Let  X  be the angle between the vector  X  X  c and the vector  X  q  X  u ;q c and  X  q  X  ;q c be the angles between the vector  X  X  vectors  X  X  u and  X  q  X  respectively (see figure 4). The distance of q  X  from q c is r  X  sin  X  and the length of the projection of q onto q c is  X  q c  X  + r  X  cos  X  . Therefore we have: gives the following inequality regarding the angle between the user and the best possible recommendation (we assume that the angles lie in the range of [  X   X , +  X  ] instead of the usual [0 , 2  X  ]) : which implies since cos(  X  ) is monotonically decreasing in the range [0 ,  X  ]. Using this equality we obtain the following bound for the highest possible affinity between the user and any item within that ball: where the last inequality follows from equation 10. Substi-tuting equations 8 &amp; 9 in the above inequality, we have The second inequality comes from the definition of maxi-mum, and the next equality comes from maximizing over  X  giving us the optimal value for  X  =  X  p u ;q c . Simplifying the final inequality gives us the following upper bound: Al gorithm 3 SearchMetricTree(User p u , Item Tree Node Q ) Al gorithm 4 FindExactRecommendations(User p u , Item Tree Node Q ) Fi gure 5: Metric-tree Search: The object p . candidates contains the set of current best K can-didate items and p u . ub denotes the lowest affinity between the user and its current best candidates.
Using this upper bound (11) for the maximum possible dot-product, we present the depth-first branch-and-bound algorithm to search for the K -highest dot-products in Al-gorithm 3. The algorithm begins at the root of the tree of items. At each subsequent step, the algorithm is at a tree node. Using the bound in equation 11, the algorithm checks if the best possible item in this node is any better than the current best candidates for the user. If the check fails, this branch of the tree is not explored any more. Otherwise, the algorithm recursively traverses the tree, exploring the branch with the better potential candidates in a depth-first manner. If the node is a leaf, the algorithm just finds the best candidates within the leaf with the simple naive search. This algorithm ensures that the exact solution (i.e., the best candidates) is returned by the end of the algorithm.
We do not have any runtime guarantees for the algorithm presented in this section. However, we can conjecture pos-sible runtime bounds. If the metric-tree constructed with Algorithm 2 has a depth of O (log | Q | ) (where Q is the set of items), then the runtime bound for the construction of the tree is O ( D | Q | log | Q | ) (since you only require O ( D o perations at each level of the tree and D is the dimension-ality of the data). During the tree-search algorithm (Alg. 4), let us assume that the user visits L leaves. If L is much smaller and in fact independent of the number of items | Q we can say that the runtime bound for the search process for a single user is O ( DL log | Q | ). However, if L depends on as well, then the best possible runtime bound is O ( D | Q
Since algorithm 2 does not enforce that the splits be bal-anced, it is quite possible that the depth of the tree might end up being O ( | Q | ), in which case, the worst case runtime for the search process is O ( D | Q | ). However, in practice, the tree depths have been seen to be way less than O ( | Q | ).
The efficiency of the exact algorithm can be limited, and some applications may require even faster retrieval while al-lowing for some suboptimal recommendations. To this end, we propose a scheme to cluster the users into cones of sim-ilar  X  X aste X , pre-compute the recommendations for the cone centers (representative user tastes), and use these recom-mendations as approximate recommendations for incoming users with tastes similar to some existing user cluster.
Equation 5 specifies that the user preferences depend only on the angle (direction) of the corresponding user vectors. The smoothness of the cosine function implies that two users with vectors in similar directions will have very similar pref-erences. Hence, we partition the space into cones that ag-gregate users with similar taste. Let P c be a set of cone centers where each p c  X  P c is a unit vector. The direction of p is the taste of the cone which can be used to pre-compute recommendations for the users in that cone.

For a new user query p u , its best cone p  X  c is: After finding p  X  c we retrieve the pre-computed recommen-dations of p  X  c as the approximated recommendations for p Figure 6 depicts a user X  X  vector p u and its best cone X  X  vector p . If  X , the angle between p u and p  X  c , is small enough, then the approximated recommendations will be close to the op-timal recommendations. The speedup is achieved since the number of cones is much smaller than the number of users or items, thus finding p  X  c is significantly easier than computing the exact recommendations for each user .

The approximation is controlled by using an upper bound on the relative approximation error in terms of  X  (this is presented in section 5.1). This bound evaluates the quality of the approximation. By defining a threshold T r on the maximum acceptable error, we adaptively accept the pre-calculated approximate results when the error is below the threshold, or compute the exact results otherwise. The de-tails of the approximate RoR algorithm are given in figure 7. The threshold T r introduces a tradeoff between speedup and accuracy where maximal speedup is achieved when T r =  X  .
In general, one would like to choose a set of user clusters (cones) which appropriately fits the distribution of possible queries. This can be efficiently achieved by spherical cluster-ing of the user vectors. Spherical clustering defines groups of users with similar preferences or taste. Unit vectors in the direction of the clusters X  centers define the cone centers. Fi gure 6: Here p u is the user X  X  vector, p  X  c is the cone X  X  central direction, q i is the item X  X  vector and  X  is the angle between p  X  u and p c .
 Al gorithm 5 PrepareCones(Users P , Items S ) Al gorithm 6 FindApproxRecommendations(User p u , User Cones P c , Threshold T r , Item Tree Q ) Fi gure 7: Approximate RoR: The subroutine ChooseCones chooses a set of cones that fits the set of user vectors P in the dataset. Then, the optimal recommendations are computed for each cone X  X  cen-ter using the metric tree of section 4.
 In FindApproxRecommendations , the subroutine ComputeErrorBound computes the error bound ac-cording to equation 14. The approximated recom-mendations are used for every query with an error bound below the error threshold T r , otherwise exact recommendations are computed.
 We chose spherical clustering because of its computational efficiency. Furthermore, the clustering already assigns the user vectors into cones and there is no need to search for the best matching cone for the existing users.

A requirement for a good clustering is that  X  &lt; 2 . Oth-erwise the dot-product between the user and an item in the direction of the cone X  X  center can be negative, which implies that the user does not like the items that fit the cone X  X  vec-tor. Note that clustering assumes the presence of groups of users common tastes. This is a very natural assumption in every collaborative filtering algorithm.
In this subsection we present a theoretical error bound on the relative approximation error for any user. This is used by the adaptive algorithm to control the approximation error.
The vector q i in figure 6 depicts an item X  X  vector that was chosen as an optimal recommendation based on the cluster X  X  center p  X  c . Intuitively, as  X  decreases, the approximation er-ror should decrease as well. We define the approximation error err = exp  X  real as the difference between the ex-pected rating based on the cluster exp = q  X  i p  X  c and the real dot-product with the user X  X  trait vector real = q  X  i p u relative error is then:
We assume here that for every cone exp &gt; 0; otherwise it means that there are no fitting recommendation for that cone, which is very unlikely and we never encountered this
Since we want the worst-case bound, we ignore the case where real &gt; exp since this situation means that the affinity between p u and q i is better than expected. Hence, assuming that real &lt; exp , we have the following: The inequality follows from the fact that  X  p  X  c ;q i  X  2 exp  X  0) and  X   X  2 (a requirement of the clustering). Since cos(  X  ) is monotonically decreasing in the range [0 ,  X  ], we get we get the following upper bound on the relative error: Note that this bound is a tight bound. Namely, when  X   X  0 we get err  X  0.
We begin this section by presenting the datasets and the evaluation metric used. Then we present the results of the exact RoR algorithm (section 4). In the following subsec-tion, we present the performance of the approximate RoR method (section 5), demonstrating its efficiency-error trade-off. Finally, as a thought experiment, we also present the inaccuracies introduced by using existing best-match algo-rithms (nearest-neighbor search in Euclidean space and best-match with respect to cosine similarity) for the task of RoR.
We used the following publicly available datasets: 1. MovieLens  X  It consists of 1,000,206 ratings of 3,952 2. Netflix [2]  X  It consists of 100,480,507 ratings of 17,770 3. Yahoo! Music [8]  X  This dataset is the largest of the
Ev en if exp &lt; 0 it is still possible to bound the error by following a very similar process to the one shown here. Currently the Yahoo! Music dataset is the largest publicly available collaborative filtering dataset. Both our algorithms perform best on this dataset. In fact, in most of our eval-uations the results seems to improve with the size of the dataset. This is expected as overhead times become negli-gible when the number of queries (users) increase. All the above datasets were in fact sampled from real datasets which were possibly much larger. It is therefore likely that the re-sults presented in this paper will further improve when the proposed algorithms are implemented in real world systems.
For the MovieLens and Netflix datasets, we built and trained a basic MF model (equation 2) using stochastic gra-dient descent minimization of the mean squared error. For the Yahoo! Music dataset, we used the model presented in [6] that incorporates music taxonomy and temporal effects. All models have 50-dimensional vectors to represent the user and item traits. The root mean squared errors of these three models were 0 . 839 in MovieLens, 0 . 899 in Netflix, and 22 . 592 in Yahoo! Music.

We quantify the improvement of an algorithm A over an-other (baseline) algorithm A 0 by the following term: Since there are no efficient search algorithms for maximum dot-products, our baseline is a naive algorithm that searches over all items to find the best recommendations for every user. We denote by T naive the time taken by the naive al-gorithm. It is obvious that where D is the dimensionality of the vector (here D = 50).
As expected, the naive algorithm is extremely time con-suming. For example, the baseline execution time for re-trieving optimal recommendations for the Yahoo! Music dataset is 135.1 hours 4 (over 5 days). The mean latency for a single user query is 0.482 seconds. Using our proposed com-bined method (figure 7), we achieve up to  X  258 . 08 speedup, which is equivalent to just 31.4 minutes for the entire compu-tation or an average single user latency of 1.87 milliseconds. It is important to note that while the overall computation time can also be reduced by means of parallelization, the latency for a single user might be harder to improve upon.
We used the Cluto clustering toolkit [10] for spherical clus-tering of the user vectors. We used 500, 1000, and 2000 clusters (cones) for the MovieLens, Netflix and Yahoo! Music respectively, because these values showed a good balance be-tween performance and speedup. In Alg. 2, we used N 0 = 2 in all our experiments. In general, these parameters can be optimized using a cross-validation process.
The time taken by the exact algorithm of Section 4 can be broken up into two parts as follows: where T tree building is the time taken by Alg. 2 to build the tree on the set of item vectors, and T tree search is the total time taken by all the users to find their respective best
U sing an Intel Xeon (E7320) CPU running at 2.13GHz T able 1: Speedups of Alg. 3 over naive search for different number of top recommendations ( K ) . recommendations using Alg. 3. The speedup is therefore: We present the speedups obtained for different numbers of top recommendations in Table 1. The results indicate that the exact algorithm can be up to  X  7 faster than the naive algorithm. Another advantage of this method is its space efficiency  X  only the tree (which consists solely of point-ers) has to be stored. The complete (# USERS  X  # ITEMS ) user-preference matrix does not have to be stored and the recommendations for a user can be obtained when required.
An important thing to note is that the tree-building task is extremely time efficient  X  for example, for the Yahoo! Music dataset, the time taken to build the metric-tree on the set of items (of size 624961  X  50) was less than 16 seconds (the time required to load the whole data into memory took more than 40 seconds!). The tree-building process is a one-time cost which is amortized by the more expensive tree-search process. Moreover, new items can be easily added to this metric-tree index 5 . Nevertheless, we include the tree-building times in our computation for completeness. It is important to note that the search time increases with K (the number of top recommendations returned). This is because the bound for the best recommendations for the user ( p u . ub in Alg. 3) becomes smaller with increasing K (Line 8 in Alg. 3). This increases the number of nodes that have potential (Line 1 in Alg. 3), hence also increasing the number of leaves finally visited.

However, some applications may require more than just the top 50 items. In that case, the tree-based exact search does not provide any significant improvement over the naive algorithm. Therefore, we present further improvements in computational performance in the next subsection with the proposed approximate algorithm.
The time taken by the approximate algorithm of Section 5 can be broken up into four parts as follows: where T clustering is the time taken by the clustering al-gorithm, T tree building is the metric-tree-construction time, T search cones is the search time for optimal recommendations for all the cones and T search queries is the time taken to com-pute exact recommendations for queries that are above the threshold. The speedup of the approximate algorithm is: We define two terms to quantify the quality of the top K rec-ommendations retrieved by the approximated method. The first quantity ( P recision ) denotes how similar the approxi-mate recommendations are to the actual top K recommen-Effic ient item insertion is inherent to tree data structures. T able 2: Speedups of Alg. 6 over the naive algorithm for different values of K and the error threshold. dations (which are retrieved by the naive approach): where L rec ( u ) and L opt ( u ) are the lists of the top K ap-proximate and the top K optimal recommendations for the user u , respectively. Our evaluation metrics only care about the items at the top of the approximated and optimal lists ( L rec ( u ) and L opt ( u )). In that case there is no real meaning to compute Recall because its natural definition would be identical to the P recision .

In addition, we define a secondary metric ( M edianRank ) which denotes the preference of the approximated recom-mendations with respect to the rest of the items:
M edianRank ( K ) , median { X  u Rank ( L rec ( u )) } , (19) where the function Rank ( L ( u )) returns a list of the opti-mal ranks for the items in L ( u ) for user u (for example, Rank ( L opt ( u )) = { 1 , 2 , . . . , K  X  1 , K } ).
A high value for P recision implies that the approximate recommendations are very similar to the optimal recommen-dations, and a low value of M edianRank implies that the approximate recommendations are highly preferred by the users. In many practical applications, it is very likely to have a low value for P recision as well as for M edianRank . This implies that the items recommended by the approximate al-gorithm are generally different from the optimal items for the users, but the items recommended are still very highly preferred by the users.

The speedups of the approximate algorithm for differ-ent values of the error bound threshold are summarized in Table 2. The results indicate that the approximate RoR method can be up to  X  258 faster than the naive approach. The approximation quality for different levels of speedup is depicted in figures 8 &amp; 9.

Figure 8 shows the tradeoff between precision and speedup achieved by using different values of the error bound thresh-old. When the threshold is high, the approximated result is less likely to be rejected. In this case, the precision is lower, speedup is higher, and performance is better for higher val-ues of K . The latter is a result of the fact that precision values can be retrieved from table 2. in a finite set is easier to achieve as K is higher. When the threshold is low, the approximated result is more likely to be rejected. In this case, the precision is higher, speedup is lower, and performance is better when K is lower. The lat-ter is a result of the fact that we are more likely to fall back to using the metric tree and the fact that the tree performs worse on higher values of K (as explained earlier). Figure 9 presents the M edianRank for different values of K and different values of the error bound threshold. Speedup values can be retrieved from table 2. We see that even when P recison values are low (e.g., when T r =  X  ) the M edianRank values are also relatively low, which indicate that the approximated recommendations are still highly pre-ferred by the users. T able 3: Precision of the top ( K ) best matches with respect to the l 2 distance
In this subsection we find the top recommendations for a user with respect to the Euclidean ( l 2 ) distance and with re-spect to the cosine similarity. The first returns the K items closest to the user (in terms of the l 2 distance), and the sec-ond returns the K items making the smallest angles with T able 4: Precision of the top ( K ) best matches with respect to the cosine similarity the user at the origin (hence returning best matches with respect to the cosine similarity). The reason for this experi-ment is to demonstrate that existing nearest-neighbor search algorithms (like LSH) cannot be applied directly to the task of RoR in the existing MF framework without introducing high levels of error.

Tables 3 &amp; 4 report the precision of the exact best-matches obtained with respect to Euclidean distance and cosine sim-ilarity respectively. As expected from our discussion in sec-tion 3, the precision of these results are very low (especially on the larger Yahoo! Music dataset). Contrasting these numbers to the precision of the approximate solutions ob-tained from Alg. 6 (figure 8), we see that our approximate algorithm performs as accurately (if not better) with signifi-cant amount of speedup. For example, for the Yahoo! Music data set with K = 50, the best-matches with l 2 distance and cosine similarity have a precision of 0 . 112 and 0 . 033 respec-tively. In contrast, our proposed algorithm shows a speedup of about  X  200 while achieving a precision level of around 0 . 4 (figure 8(c)).

It is important to note that these returned recommenda-ti ons in both cases ( l 2 distance and cosine-similarity) are the exact best-matches with respect to their corresponding sense of similarity . If the exact results are so inaccurate (in terms of recommendation quality), it is hard to expect good results once approximate techniques for these best-match problems like LSH is used. This indicates that any form of modifi-cation done to the RoR task in MF framework (equation 1 and hence equation 6) to fit into existing best-matching problems can introduce a high level of inaccuracies.
In this paper we address the problem of efficient retrieval of recommendations (RoR) within the MF framework. This problem is inherent in a myriad of online services and re-quires added attention with the current influx of users (and items) on the internet. The RoR task in MF frameworks can be formulated as finding best matches with respect to the dot-product similarity measure. However, there are no known solutions to this problem. We thus present an exact and an approximate novel algorithms to improve the scala-bility of this task. Efficient algorithms to find the maximum dot-product can possibly have impacts beyond the realm of collaborative filtering.

The exact method uses an existing indexing scheme to in-dex the set of items, and the branch-and-bound algorithm with a novel bounding scheme to provide significant speedup (over  X  7 faster) over the naive algorithm, while having min-imal space requirements. It can be easily adapted to include new items (or new users) into the system. However, being an exact algorithm, it shows limited improvement in computa-tional performance. Hence we relax the problem of RoR and present an approximate algorithm based on the novel idea of grouping the users using spherical-cones. The method sub-sequently stores the best recommendations of each of the cone centers as the approximate best recommendations for all the users within that cluster. This method shows much better scalability (up to  X  258 speedup) with the trade-off of deviating from the otimal list of recommendations. How-ever, we demonstrate that even when precision is low, the approximated items are still highly preferred by the users.
MF based models have demonstrated impressive perfor-mance in terms of scalability at training time as well as pre-dictive accuracy. However, less accurate algorithms are often used in large scale web-services. This may be attributed to the computational bottleneck of retrieval of the recommen-dations. Long latency times are unacceptable in online ser-vices, and pre-computing recommendations to all the users is expensive in terms of computational time as well as stor-age. The methods presented in this paper alleviate this last obstacle, making the MF framework more approachable to large scale recommender-systems. This paper also gives the system X  X  architects a choice of an exact algorithm with sig-nificant but limited scalability or an approximate algorithm with a favorable trade-off between quality and scalability.
The problem of fast RoR discussed in this paper inspired the solution to the general problem of fast maximum inner-product search [17]. Possible extensions of this work will be to develop approximate algorithms with user-specified bounded approximation. For example, the system can ap-proximate the retrieval task to obtain any K -recommendations from among the best  X  -recommendations where K &lt;  X  (sim-ilar to the approximation of the nearest-neighbor search problem in [18]). [1] R. M. Bell and Y. Koren. Lessons from the netflix [2] J. Bennett and S. Lanning. The netflix prize. In Proc. [3] M. S. Charikar. Similarity estimation techniques from [4] K. Clarkson. Nearest-neighbor searching and metric [5] A. S. Das, M. Datar, A. Garg, and S. Rajaram. [6] G. Dror, N. Koenigstein, and Y. Koren. Yahoo! music [7] G. Dror, N. Koenigstein, and Y. Koren. Web scale [8] G. Dror, N. Koenigstein, Y. Koren, and M. Weimer. [9] P. Indyk and R. Motwani. Approximate Nearest [10] G. Karypis. CLUTO a clustering toolkit. Technical [11] M. Khoshneshin and W. N. Street. Collaborative [12] N. Koenigstein, N. Nice, U. Paquet, and N. Schleyen. [13] Y. Koren, R. M. Bell, and C. Volinsky. Matrix [14] T. Liu, A. W. Moore, A. G. Gray, and K. Yang. An [15] S. M. Omohundro. Five Balltree Construction [16] F. P. Preparata and M. I. Shamos. Computational [17] P. Ram and A. Gray. Maximum inner-product search [18] P. Ram, D. Lee, H. Ouyang, and A. G. Gray.
 [19] S. Zhang, W. Wang, J. Ford, and F. Makedon.

