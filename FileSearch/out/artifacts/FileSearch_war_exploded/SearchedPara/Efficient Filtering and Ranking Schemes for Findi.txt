 Data integrity constraints are fundamental in various applications, such as data management, integration, cleaning, and schema ex-traction. In this paper, we address the problem of fi nding inclusion dependencies on the Web. The problem is important because (1) applications of inclusion dependencies, such as data quality man-agement, are bene fi cial in the Web context, and (2) such depen-dencies are not explicitly given in general. In our approach, we enumerate pairs of HTML/XML elements that possibly represent inclusion dependencies and then rank the results for veri fi cation. First, we propose a bit-based signature scheme to ef fi ciently select candidates (element pairs) in the enumeration process. The signa-ture scheme is unique in that it supports Jaccard containment to deal with the incomplete nature of data on the Web, and preserves the semiorder inclusion relationship among sets of words. Sec-ond, we propose a ranking scheme to support a user in checking whether each enumerated pair actually suggests inclusion depen-dencies. The ranking scheme sorts the enumerated pairs so that we can examine a small number of pairs for simultaneously verifying many pairs.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval inclusion dependencies, data quality
Data integrity constraints are fundamental in computer data management. They have been used in many applications, such as data integrity management, data integration, data cleaning, and schema extraction [7] [10]. In particular, two types of constraints, functional and inclusion dependencies , are widely used. A func-tional dependency states that a set of values that appear in some  X 
Current Af fi liation: NTT Docomo  X  Current Af fi liation: NTT corporation part determines values in other places. An inclusion dependency states that a set of data items must be a subset of the set of data items in another place. In other words, it states that if a data item appears in a place, the same item must appear in another place.
We focus on the problem of supporting the discovery of inclusion dependencies among HTML/XML elements on the Web. Inclu-sion dependencies are important both from practical and theoretical viewpoints. First, we see many inclusion dependencies among data in Web sites, such as among lists of publications or members, and those with sets of sentences taken from an original document. For example, Figure 1 shows a pair of Web pages that show an inclusion dependency on the Web. They are taken form ACM SIGWEB and SIGIR Web sites, each of which lists the winners of Vannevar Bush best paper award. The SIGWEB page on the left has a (nested) element, which contains a part of the complete list shown in the SIGIR page on the right. The elements are structured with differ-ent tags (table and items) but they are similar to each other in the hierarchical structure. It is common to see such inclusion depen-dencies appear in the Web sites maintained by different adminis-trators. Second, an inclusion dependency is theoretically important because it is a generalization of the equivalence constraint, which is a universal constraint that pervasively appears in many applica-tions involving Web sites having the same data but maintained by different administrators. The problem is that the data often violate the dependencies because of ill maintenance. In fact, the existence of related data maintained by different administrators is one of the causes to degrade the data quality on the Web. For example, [18] reports that in a set of real estate Web sites, they found about 60% of data items with some inconsistency.

Applying data integrity constraints to fi x such inconsistencies have been widely discussed in various data integration and man-agement problems [6]. Assume that we know inclusion depen-dencies among the Web data that are related to each other but are maintained by different administrators. Then, we can use them to maintain the quality of the contents by (1) fi nding the portions that violate the inclusion dependency, (e.g., a name does not appear in another place) and (2) fi xing the violations by updating the values. Therefore, inclusion dependencies can be one of the key technolo-gies to improve data quality on the Web.

Since data integrity constraints have such important applications, there have been many attempts to explicitly deal with data integrity constraints on the Web. For example, XML Schema [9] introduces the key and foreign key constraints in XML elements, which are variations of functional and inclusion dependencies.

A widely known problem related to data integrity constraints is that they are not always given in an explicit manner [11]. There-fore, many studies have addressed the problem of helping users fi nd integrity constraints from an existing data instance. However, most existing techniques address the problem of supporting the discovery of data integrity constraints in the context of relational databases [1] [2]. To the best of our knowledge, only a few papers address the problem of supporting the discovery of data integrity constraints in the Web context. One such study discusses how to fi nd functional dependencies in an XML document [14].

This paper is the fi rst to show the results of a comprehensive study on fi nding inclusion dependencies on the Web. Because it is inevitable that fi nding all inclusion dependencies in a given data set yields false positives, a common approach is fi rst to enumerate all possible candidates (including false positives) [2] and then to verify the enumerated candidates. This paper discusses algorithms for this two-phase approach in the Web context.

Our challenge is to develop ef fi cient schemes that can deal with the characteristics of Web content, i.e., we need to address both of the following requirements: (1) Ef fi ciency. We want the scheme to be ef fi cient, because the number of Web pages can be large. (2) Dealing with the characteristics of Web content. We want the scheme to be able to deal with the characteristics of Web con-tent, because Web pages have hierarchical structures and their data are not necessarily clean. To our knowledge, there have been no schemes that address both of the requirements.

The contributions of this paper are as follows. First, this pa-per introduces a bit-based signature scheme to ef fi ciently deal with Jaccard containment [1], which is an asymmetric version of the or-dinary Jaccard coef fi cient, in order to enumerate inclusions with the incomplete data on the Web. In general, a bit-based signature is a bit sequence associated to each data item, and has been used for ef-fi ciently determining whether each item satis fi es a given condition. In our context, a bit-based signature is associated to each HTML element to concisely represent information required to compute the inclusion relationship with other elements. The proposed mecha-nism for Jaccard containment is unique in that the signature has a fi xed length and is designed to preserve a semiorder represented by the inclusion relationship. To our knowledge, there have been no such signature schemes.

Second, we discuss a ranking scheme to aid in verifying the can-didate inclusion dependencies. We introduce the notion of covers to ef fi ciently examine the enumerated inclusions, and prove that there is an ef fi cient algorithm to compute probabilities of inclusions that are compatible with the de fi nition of covers. Then, we propose a re-ordering scheme for the algorithm X  X  outputs in order to obtain better rankings.

Due to space limitations, proofs of the presented theorems and experimental results are given in [13].
In the context of relational databases, there are already numer-ous studies about computing inclusions, i.e., asymmetric set con-tainments. Bauckmann and others [2] proposed an algorithm that takes as input a set of relations and ef fi ciently enumerates all pairs of relational attributes one of which includes the other. The algo-rithm is designed to minimize the amount of I/O over the sets of attribute values. Sergey Melnik and others [12] proposed two hash-based partitioning algorithms called the Adaptive Pick-and-Sweep Join (APSJ) and the Adaptive Divide-and-Conquer Join (ADCJ), to ef fi ciently compute set containment joins. The algorithms above are designed to compute strict inclusions in fl at relations under the assumption that the data is clean.

Recently, fi nding inclusions based on Jaccard containment is at-tracting attention in the research community. This is because there are many applications in which we need to relax the assumption that the data is clean. An approach is to use the pre fi x fi ltering [4] [17], which is a run-time optimization technique for similarity joins. It uses the pre fi x of each data set sorted in some order in the to be used as the basis of the design of structured index for Jaccard containment and proposes to use the notion of minimal infrequent sets to construct the index. The size of the index can be exponen-tial in the record size, but it is reported that the size is often much smaller in practical applications. To our knowledge, there are no signature schemes to deal with Jaccard containments although both of the structured and signature-based indices are known important to support various types of applications. Again, the pre fi x fi ltering is not appropriate to be used as the basis of the design of the sig-natures, since it requires the threshold for Jaccard containment to determine the size of pre fi x. Another approach is to use estimators. For example, in [20], bottom-k sketches are used as estimators for Jaccard containment and then, foreign key constraints were deter-mined using a criterion called randomness. The bottom-k sketches are similar to the pre fi x fi ltering but do not guarantee 100% recall because k is determined independent of Jaccard containment.
Bit-based signatures to ef fi ciently support (exact) set-containment queries were studied in [8]. However, they can deal only with ex-act set containments, and as discussed in Section 4, it is not trivial to develop a signature scheme to deal with non-exact set contain-ments. Our bit-based signature scheme is the fi rst one that can deal with Jaccard containment and is unique in that it has all the fol-lowing properties: (1) It employs fi xed-length signatures, (2) the signature is general enough to support any Jaccard containments, and (3) it guarantees 100% recall.

Although asymmetric measures like Jaccard containment have been discussed mainly in the database context, symmetric simi-larities have been discussed in the literature in the context of the Web. [16] showed that Charikar X  X  simhash [3] is practically use-ful for identifying near-duplicates in Web pages. [19] proposes the positional fi ltering to support ef fi cient similarity joins for near du-plicate detection, which can be used to fi nd near-duplicate Web pages. However, symmetric similarity cannot capture asymmet-ric measures (such as containment) in general (Note that there are many cases in which X is contained in Y but X and Y do not have a high similarity score.) As suggested in [1], there are scenarios in which an asymmetric measure is more appropriate. Finding inclu-sion dependencies is one such scenario.

There are studies on the ranking of XML elements in the XML search context. In general, XML search needs to take into consid-eration the hierarchical structure of XML elements in the ranking, because XML elements of any granularity are potential answers to a query [15]. Our ranking scheme is unique in that the ranking is for pairs of HTML/XML elements. However, the idea of the cover in our context can be considered as a generalization of the removal of overlapping answers in XML search [5]. Inclusion Dependencies among Web Page Elements. This pa-per deals with inclusion dependencies among Web page elements. We model the target Web data as a triple ( P, elem, words ) P ( = { p 1 ,p 2 ,... } ) denotes a set of Web pages, and elem words are functions to represent components of each Web page; tained in Web page p k ,and words ( e j ) ( = {| w 1 ,w 2 the multiset words ( e j ) of words contained in the element e need one constraint to represent the hierarchical structure of page elements; If e i is a sub-element of e j , words ( e i ) has to be a sub-set of words ( e j ) . For example, assume that elem ( p k a set of HTML elements in Web page p k and let words ( e j multiset of words in each element e j  X  elem ( p k ) . Then, the map-ping satis fi es the constraint. Note that as long as the constraint is satis fi ed, the following discussion is independent of the mapping. For example, each p k does not necessarily have to be an actual Web page; it can be an XML document created from an HTML page by a wrapping process.

Now, we de fi ne an inclusion dependency among page elements as follows: Let e i and e j be page elements. Then, e i e j is an inclusion dependency between e i and e j meaning that words ( e i )  X  words ( e j ) should always be satis fi ed on the Web. In the rest of the paper, we often use e i to denote words ( e operations, when the meaning is clear from the context.
 Jaccard Containment and Weak Inclusions. To fi nd inclusion dependencies, we need to fi nd inclusions , i.e., pairs of page ele-ments that have the inclusion relationship in the current data in-stance. However, automatic discovery of such inclusions poses a problem, especially in the Web context. In general, Web content is error-prone and has variations in expression . Therefore, if we search for exact inclusions, we would miss many inclusion depen-dencies.
 To deal with such a situation, we employ an approach based on Jaccard containment. Jaccard containment [1] is an asymmetric version of the Jaccard coef fi cient, which is a measure of set simi-larities. Given two sets s 1 and s 2 , the Jaccard containment of s , denoted by JaccCont ( s 1 ,s 2 ) ,isde fi ned as follows.
For example, Jaccard containment for s 1 = { apple , peach and s 2 = { apple , banana , grape } is 0.5. If s 1 and s the bag intersection is used to compute their Jaccard containment.
Let e i and e j be page elements and c be a value s.t. 0  X  Then, we de fi ne e i  X  c e j as follows.
We read e i  X  c e j as  X  e i is included in e j with the inclusion ratio c when c =1 , it equals to e i  X  e j . In particular, an inclusion with a ratio c&lt; 1 is called a weak inclusion when we need the distinction.
We de fi ne e i  X   X  c e j as a natural extension; the pair ( an inclusion s.t. e i  X   X  c e j ,if JaccCont ( words ( e i in Equation (2) is greater than or equals to c . Inclusions with ratios for other inequalities are de fi ned in a similar manner. The Problem. Our problem is to fi rst enumerate every inclusion ( e ,e j ) s.t. e i  X   X  c e j for a given c , and then sort the enumerated inclusions for veri fi cation to fi nd the actual inclusion dependencies among page elements on the Web.
 Formally, let a set E of all page elements be pairs = { ( e i ,e j ) | e i ,e j  X  E } ,and c beavalues.t. 0  X  Then, the fi rst step is to compute and output a set of inclusions, denoted by inclusions ( pairs, c ) ,where inclusions ( pairs, c )= { the size of pairs can be large ( O ( | E | 2 ) ) and checking if a pair ( e ,e j ) is an inclusion s.t. e i  X   X  c e j requires costly strict com-parisons, we want to fi lter out irrelevant pairs fi rst and then conduct strict comparisons. The fi rst step will be addressed in Section 4.
Although the fi rst step enumerates all inclusions, they are can-didates for inclusion dependencies; each of them does not neces-sarily imply the existence of inclusion dependency. Therefore, the second step is to sort the inclusions in inclusions ( pairs, c ) veri fi cation to fi nd the actual inclusion dependencies. However, ordering the inclusions enumerated in the fi rst step to support the are challenges. This will be addressed in Section 5.
In the fi rst step, for the ef fi cient computation of inclusions we fi rst use a fi lter that removes the pairs of page elements that are guaranteed not to be inclusions. Then, we apply strict comparisons
Let filter ( e i ,e j ,c ) be a predicate that returns false only when e  X   X  c e j is guaranteed not to hold. Assume that we compute the following set of pairs: pairs = { ( e i ,e j ) | ( e i ,e j )  X  pairs  X  filter ( e Then, filter ( e i ,e j ,c ) should be designed to lead to the following results: (1) | pairs | X | pairs | and (2) inclusions ( pairs, c )= inclusions ( pairs ,c ) . This means that for a given c , the results with pairs and pairs are identical.
The idea of the bit-based signature scheme is as follows: First, we compute a bit-based signature b ( e ) for each e  X  E .Next,given b ( e i ) , b ( e j ) ,and c , we perform a simple computation to decide whether filter ( e i ,e j ,c ) holds.

The question is whether it is possible to develop a scheme to evaluate e i  X  &gt;c e j for any given c . This section shows that there exists such a signature scheme. Let sigsize be the ( fi xed) size of the signature. Given e i  X  E andaninteger t&gt; 0 ,let b ( e bit sequence computed in the following manner. 1. For each word w  X  words ( e i ) , compute the hash value 2. If the number of words having the same h ( w ) is greater than
Here, t should be chosen such that the distribution of signatures is not skewed. Note that because the set of page elements consti-tute a tree structure, we have a large number of small elements and a smaller number of large elements. In other words, the distribution of | words ( e i ) | for e i  X  E is biased to smaller values. Assuming that the distribution of word occurrences is uniform, it is reason-able to de fi ne t so that 1s and 0s are uniformly distributed in the signatures of small elements. Consequently, we de fi ne t Let minsize be the smallest size of words ( e i ) for e i
Then,
Note that if we de fi ne t on the basis of the larger words ( e many bits of the signatures of small elements would be 0, which means that the distribution of bits is more skewed in many signa-tures.
 Example. Assume that we have the following set E of page ele-ments.
 Let sigsize =8 and h ( w ) be the code of the fi rst character of w modulo eight. Then, b ( e 1 ) , b ( e 2 ) ,and b ( e 3 ) are computed as follows. 1. Let H ( e i ) be the multiset of hash values for words ( e 2. Because minsize = | words ( e 1 ) | =5  X  sigsize , t =1
Here, we de fi ne b -filter ( e i ,e j ,c ) , which is a kind of page element pairs.

The idea is to compute the possible maximum inclusion ratio c max for e i  X  c e j on the basis of b ( e i ) and b ( e j introduce a theorem on b ( e i ) and b ( e j ) .

T HEOREM 1. Let e i and e j be page elements. Then, the pos-sible maximum inclusion ratio c max for e i  X  c e j is computed as follows. Here, X is the number of integers k s.t. the k -th bit of and the k -th bit of b ( e j ) is 0.

Given Theorem 1, we de fi ne b -filter ( e i ,e j ,c ) , that produces no false negatives, as follows.
For each ( e i ,e j ) that survived the fi lter, the fi rst step conducts a strict comparison to compute Jaccard containment and determine whether it is an inclusion. To the best of our knowledge, the com-plexity of the fastest algorithm to check if a pair ( e i clusion is in O ( n ) for the size of the sets [2] under the assumption that we sort the words in e i and e j before the calculation.
Let m = | E | . Then, | pairs | is m C 2 (i.e., O ( m 2 ) ). Let n be the average of | words ( e ) | for all e  X  E . The simple computation needs to (1) sort the words included in every element in E computational complexity is O ( mn log n ) , and (2) conduct strict comparisons to fi nd inclusions ( O ( m 2 n ) ).

If we use b -filter , we need an additional computation of the bit signatures for all page elements, whose computational complexity is
O ( n ) for each signature. However, the additional computation reduces other costs. We do not need to sort the words eliminated by b -filter . The cost of the fi ltering is cheap (its computational complexity is O (1) ).

Let m b ( &lt;&lt; m ) be the number of pairs that are not eliminated by b -filter .Let n b be the average of | words ( e ) | for all elements of the survived pairs. Then, the computation needs to (1) create the signatures for every e  X  E ( O ( nm ) ), (2) apply b -pairs ( O ( m 2 ) ), (3) sort words ( e i ) and words ( e survived b -filter ( O ( m b n b log n b ) ), and (4) conduct strict com-parisons to fi nd inclusions ( O ( m 2 b n b ) ). Because m shown by the results of our experiment [13], b -filter dramatically reduces the cost of inclusion computation compared with the sim-ple computation without the fi lter, although n b is often larger than n .
Because all inclusions enumerated for a data instance do not nec-essarily result in actual inclusion dependencies, the next step is to verify whether each inclusion implies an inclusion dependency. The method to rank the enumerated inclusions is important because typically, the veri fi cation is performed manually. This section dis-cusses the method to rank the element pairs in inclusions ( pair, c ) (Section 3), which represent the enumerated inclusions. For the discussion, we ignore the inclusion ratio in each  X   X  c , since the purpose of introducing the ratio is to consider the problem as an approximation of fi nding exact inclusions. Now, the problem can be modeled as how to rank the inclusions in inclusions ( pairs )= { ( e i ,e j ) | ( e i ,e j )  X  pairs, e i  X  e j } .

Let  X  and  X  be inclusions s.t.  X ,  X   X  inclusions ( pairs ) say that  X  covers  X  , if we can verify  X  in parallel with the veri fi ca-tion of  X  . We write  X   X   X  to denote that  X  covers  X  .

The notion of covers can be considered as a generalization of the removal of overlapping answers in XML search. In XML search, it is often the case that the element hierarchy of XML data allows us to see an answer e 1 in parallel with seeing e 2 , because some of the elements (e.g. e 1 ) satisfying a query condition are often included in the others ( e 2 ). We extend the idea to deal with the relationships among element pairs in the context of the veri fi cation of inclusion dependencies. We de fi ne two types of cover relationships, namely, deductive covers that represent logical overlaps and regional cov-ers that represent physical overlaps. Note, our purpose is to ver-ify whether each inclusion represents an inclusion dependency, not whether each element pair is an inclusion.
 In the following, we fi rst de fi ne two types of cover relationships. Cover relationships de fi ne the semiorder among inclusions (ele-ment pairs). However, how to ef fi ciently compute the relationships is not straightforward. Interestingly, there exists an ef fi cient algo-rithm based on the pr obabilitie s of word occurrences that produces the total order among inclusions compatible with both cover rela-tionships (i.e., the output is one of its topological sorts). We present the algorithm. Finally, we show that re-ordering of the outputs of the probability-based algorithm gives better ranking results.
The cover relationship (  X  ) consists of deductively cover rela-tionship (  X  d )and regionally cover relationship (  X  r ). Formally,  X   X   X  iff  X   X  d  X   X   X   X  r  X  .

D EFINITION 1. When we have two inclusions  X  =( e 1 ,e 4 ) and  X  =( e 2 ,e 3 ) ,wesay  X  deductively covers  X  (denoted by  X   X  ) if and only if the following conditions hold.
This is illustrated by Figure 2, in which two element hierarchies are shown: (1) e 1 and e 2 and (2) e 3 and e 4 . Then, e 1 tively covers e 2  X  e 3 because the latter is deduced from the former and the inclusions e 2  X  e 1 and e 4  X  e 3 , which are derived from the hierarchical structure.

When  X  deductively covers  X  , we can easily check whether  X  suggests an inclusion dependency in parallel with checking whether  X  does. This is because (1) the elements in  X  overlap those in  X  , and (2) the existence of inclusion  X  =( e 1 ,e 4 ) suggests the place of inclusion  X  =( e 2 ,e 3 ) . Namely, (a) e 2 exists inside and (b) e 3 can be every ancestor of e 4 . The existence of ever, does not imply that of  X  . Therefore, the user who was fi rst told that  X  exists, could not know where and even whether related inclusions exist.

D EFINITION 2. When we have two inclusions  X  =( e 1 ,e 3 ) and  X  =( e 2 ,e 4 ) ,wesay  X  regionally covers  X  (denoted by  X   X  ) if and only if the following conditions hold.
Figure 3 shows an example in which there are two element hier-archies: (1) e 1 and e 2 and (2) e 3 and e 4 . Then, e 1  X  covers e 2  X  e 4 , because e 2 is a descendent element of a descendent element of e 3 .

A typical scenario encountered is the situation where there are two pages maintaining two lists of items and one list is a sublist of the other; for example, a publication list of a lab and one of its members. In the example, e 1 and e 3 are lists of publications, and e and e 4 represent publications.

When  X  regionally covers  X  , we can easily check whether  X  gests an inclusion dependency in parallel with checking whether does, for the same reason as the deductive covers: (1) the elements in  X  are overlapped to those in  X  , and (2) the existence of inclusion  X  =( e 1 ,e 3 ) suggests the place of inclusion  X  =( e 2 ,e (a) e 2 exists inside e 1 ,and(b) e 4 exists in e 3 .
A question arises as to whether there exist algorithms to ef fi -ciently compute the deductive and regional covers among inclu-sions. Our fi nding is that there exists such an algorithm.
As shown later (Theorem 2), we found that the notion of de-ductive and regional covers is compatible with the probabilities of occurrences of inclusions. Formally, let  X  (  X  ) be an inclusion and P (  X  ) ( P (  X  ) ) be the probability that the inclusion appears in page elements. Then, we prove that P (  X  )  X  P (  X  ) if  X   X   X  .Be-cause of the antisymmetric nature of the order, this implies that  X   X   X  if P (  X  )  X  P (  X  ) when  X   X   X  . Therefore, when we sort the inclusions according to their probabilities, the result becomes a topological sort of inclusions with the cover relationship. We compute the probabilities on the basis of a simpli fi ed model. In [13], we show that the model works well even in the real Web setting. Let WORDS denote the set of all words that can appear in Web pages. We assume that each word independently occurs in page elements and the distribution of word occurrence is uniform. In addition, we show that the size of WORDS is suf fi ciently large s.t. | WORDS | &gt;&gt; | words ( e ) | for every e  X  E .
Then, given elements e 1 and e 2 (s.t. | words ( e 1 ) | X  | words ( e 2 ) | ), the probability that e 1  X  e 2 appears, denoted by P ( e 1  X  e 2 ) , is computed as follows.
 The expression computes the probability that words ( e 1 ) is a subset of words ( e 2 ) in the simpli fi ed model. Note that the de fi nition is very simple and does not require complex computations.
 The following theorem holds (the proof is given in [13]).
T HEOREM 2. For any two inclusions  X  and  X  , P (  X  )  X  P (  X  ) if  X   X   X  .
In the ranking result of inclusions we want to the top-ranked in-clusions to cover as many other inclusions as possible. A simple approach is to sort the inclusions in the ascending order of their probabilitie s because the sorting result is guaranteed to be a topo-logical sort of inclusions with the cover relationship  X  .
Interestingly, the simple approach does not necessarily yield a good ranking and other possibilities exist. F or instance, assume that we have four inclusions  X  ,  X  ,  X  and  X  s.t.  X   X   X  and Obviously, the good rankings would have  X  and  X  in the fi rst two inclusions, and  X  and  X  in the last two inclusions, since we need to verify only the fi rst two inclusions that cover the remaining in-clusions. However, the simple ranking does not necessarily yields such a ranking result, because it is possible that we have such prob-abilities that P (  X  ) &lt;P (  X  ) &lt;P (  X  ) &lt;P (  X  ) to examine the fi rst three inclusions to cover the all inclusions. Group-Conscious Algorithm. In the above example, a better ranking result has  X  and  X  in the fi rst two inclusions and  X  in the last two inclusions, because we need to verify only the fi rst two inclusions that cover the remaining inclusions. The group-conscious algorithm generalizes the idea. First, we de fi ne a group of inclusions as follows:
D EFINITION 3. A group of inclusions on  X  is a set of inclu-sions, for any two inclusions  X  and  X  of which, either of  X   X   X  holds.
The group-conscious algorithm fi rst divides inclusions into groups, then sorts inclusions in each group by the probabilities and fi nally merges the sorted results on the basis of the rank in each group. In the fi nal step, the inclusions with the same rank in their groups are ordered according to the ir probabilities. Note that the algorithm outputs a sequence [  X  ,  X  ,  X  ,  X  ] for the above example, in which the fi rst two inclusions cover the remainder.

T HEOREM 3. The group-conscious algorithm sorts inclusions in a topological order with the cover relationship.
 It is not easy to fi nd an ef fi cient algorithm to group inclusions. We developed an ef fi cient group-conscious algorithm, that exploits the following theorem.

T HEOREM 4. For any inclusions  X  =( e 1 ,e 2 ) and  X  = ( e 3 ,e 4 ) , e 1 is an ancestor of e 3 if  X   X   X  .

Figure 4 shows the ef fi cient algorithm to identify inclusion groups. For simplicity, the algorithm deals with inclusions related to only one Web page, i.e., we assume that we have one element tree. It is easy to extend it to the case where we have more than one element tree.

The prerequisite of the algorithm is that the inclusions (element pairs) are sorted by the depth-fi rst order of the left element ( ( e ,e j ) ) in the element tree. Note that we do not need explicit sort-ing if the algorithm to produce element pairs traverses the element tree in the depth-fi rst order.

Interestingly, Theorem 4 guarantees that the sequence of inclu-sions that are sorted in the order already clusters the inclusion groups. In addition, it is guaranteed that the left element of every left element of the inclusions in the same group.
The algorithm in Figure 4 exploits the property and scans the se-quence of inclusions from the beginning, assigning an incremental group identi fi er to each group. In the scan, the algorithm maintains (1) the group identi fi er kept in the variable currentGroupID (2) the left element of the fi rst inclusion of each group kept in the variable firstElem . The latter is used to determine when the scan enters the next group. It is easy to determine when the scan enters the next group if we use the post-order assigned to each element. When each group ends in the scan, it updates currentGroupID and firstElem for the next group.

The group-conscious algorithm is as ef fi cient as the simple al-gorithm. Let n be the length of the inclusion sequence. The sim-ple algorithm fi rst computes the probabilities of inclusions whose computational complexity is O ( n ) and then sorts the inclusions ac-cording to the probabilities. Overall, the computational complexity is O ( n log n ) .

Although the group-conscious algorithm needs to identify inclu-sion groups, the scan can be performed in parallel with the com-putation of probabilities ( O ( n ) ). Next, it sorts inclusions in each group and fi nally merges the results of all sorts. Therefore, the computational complexity is again O ( n log n ) .
In this paper, we addressed the problem of fi nding inclusion de-pendencies on the Web. First, we introduced a bit-based signature scheme to ef fi ciently reduce the number of pairs of page elements that are irrelevant to inclusion dependencies. The signature scheme containment, in order to cope with the incomplete nature of Web data. Second, we discussed ranking schemes for the enumerated inclusions to support the veri fi cation process to fi nd inclusion de-pendencies. We introduced the notion of covers to ef fi ciently look through the enumerated inclusions, and showed that there are ef fi -cient algorithms to compute probabilities that are compatible with the de fi nition of covers.
 Acknowledgments. The authors are grateful to Prof. Tetsuo Sak-aguchi and Prof. Mitsuharu Nagamori for the discussion in semi-nars. This research was partially supported by the Grant-in-Aid for Scienti fi c Research (#22240023) from the Ministry of Education, Culture, Sports, Science and Technology (MEXT), Japan.
