 University of Edinburgh University of Rochester formalisms such as combinatory categorial grammars, head grammars, linear indexed grammars, and tree-adjoining grammars, which can be parsed in time O ( n complexity of parsing. 1. Introduction
The problem of grammar recognition is a decision problem of determining whether a string belongs to a language induced by a grammar. For context-free grammars (CFGs), recognition can be done using parsing algorithms such as the CKY algorithm (Kasami 1965; Younger 1967; Cocke and Schwartz 1970) or the Earley algorithm (Earley 1970).
The asymptotic complexity of these chart-parsing algorithms is cubic in the length of the sentence.
 tion is no more complex than Boolean matrix multiplication for a matrix of size m  X  m matrix multiplication, this means that CFG recognition can be done with an asymptotic complexity of O ( n 2 . 38 ). (LCFRS) recognition can also be reduced to Boolean matrix multiplication. Current chart-parsing algorithms for binary LCFRS have an asymptotic complexity of O ( n where f is the maximal fan-out of the grammar. 1 Our algorithm takes time O ( n  X  where the complexity of n  X  n matrix multiplication is M ( n ) = O ( n  X  ). The parameter d can be as small as f , meaning that we reduce parsing complexity from O ( n grammars.
 ings of Rajasekaran and Yooseph (1998), who showed that tree-adjoining grammar (TAG) recognition can be done in time O ( M ( n 2 )) = O ( n
LCFRS with d = 2). As a result, combinatory categorial grammars, head grammars, and linear indexed grammars can be recognized in time O ( M ( n show that inversion transduction grammars (ITGs; Wu 1997) can be parsed in time
ITGs. 1.1 Matrix Multiplication State of the Art Our algorithm reduces the problem of LCFRS parsing to Boolean matrix multiplication.
Let M ( n ) be the complexity of multiplying two such n  X  n matrices. These matrices can between the corresponding row and column in the input matrices (each such product is an O ( n ) operation). Strassen (1969) discovered a way to do the same multiplication in
O ( n 2 . 8704 ) time X  X is algorithm is a divide and conquer algorithm that eventually uses only seven operations (instead of eight) to multiply 2  X  2 matrices.
 plexity of matrix multiplication, relying on principles similar to Strassen X  X  method: a reduction in the number of operations it takes to multiply sub-matrices of the original matrices to be multiplied. Coppersmith and Winograd (1987) identified an algorithm that has the asymptotic complexity of O ( n 2 . 375477 ). Others have slightly improved that algorithm, and currently there is an algorithm for matrix multiplication with M ( n ) =
O ( n  X  ) such that  X  = 2 . 3728639 (Le Gall 2014). It is known that M ( n ) =  X  ( n 2002).
 stant factors lurking in the O -notation, Strassen X  X  algorithm does not, and is widely used in practice. Bened  X   X  and S  X  anchez (2007) show speed improvement when parsing natural language sentences using Strassen X  X  algorithm as the matrix multiplication subroutine for Valiant X  X  algorithm for CFG parsing. This indicates that similar speed-ups may be possible in practice using our algorithm for LCFRS parsing. 1.2 Main Result
Our main result is a matrix multiplication algorithm for unbalanced , single-initial binary LCFRS with asymptotic complexity M ( n d ) = O ( n  X  422 number of combination points in all grammar rules. The constant d can be easily determined from the grammar at hand: where A  X  B C ranges over rules in the grammar and  X  ( A ) is the fan-out of nonterminal
A . Single-initial grammars are defined in Section 2, and include common formalisms such as tree-adjoining grammars. Any LCFRS can be converted to single-initial form by increasing its fan-out by at most one. The notion of unbalanced grammars is introduced in Section 4.4, and it is a condition on the set of LCFRS grammar rules that is satisfied with many practical grammars. In cases where the grammar is balanced, our algorithm can be used as a subroutine so that it parses the binary LCFRS in time O ( n  X  context-free grammars. See more discussion of this in Section 7.5.
 do not give explicit grammar constants. For other work that focuses on reducing the grammar constant in parsing, see, for example, Eisner and Satta (1999), Dunlop, Boden-stab, and Roark (2010), and Cohen, Satta, and Collins (2013). For a discussion of the optimality of the grammar constants in Valiant X  X  algorithm, see, for example, Abboud,
Backurs, and Williams (2015). 2. Background and Notation
This section provides background on LCFRS, and establishes notation used in the remainder of the paper. A reference table of notation is also provided in Appendix A.
For a set X , we denote by X + the set of all sequences of length 1 or more of elements from X .
 larger string. The endpoints are placed in the  X  X paces X  between the symbols in a string.
For example, the span (0, 3) spans the first three symbols in the string. For a string of length n , the set of potential endpoints is [ n ] 0 .

LCFRS and their relationship to other grammar formalisms, see Kallmeyer (2010). A binary LCFRS is a tuple ( L , T , R ,  X  , S ) such that:
The language of an LCFRS G = ( L , T , R ,  X  , S ) is defined as follows: first choosing, top X  X own, a production to expand each nonterminal, and then, bottom X  up, applying the composition functions associated with each production to build the string. As an example, the following context-free grammar: corresponds to the following (binary) LCFRS:
The only derivation possible under this grammar consists of the function application g ( g 2 (), g 3 ()) =  X  ab  X  .
 that operates on nonterminals A , B , and C , we define variables from the set S = {  X  , ... ,  X   X  ( B ) ,  X  1 , ... ,  X   X  ( C ) } . In addition, we define variables  X  i  X  [  X  ( A )], taking values from S + . We write an LCFRS function as: the i th string of the function X  X  result tuple. For example, for the rule in Equation (1),  X  424 the article. We write the rule: as: where  X  consists of a tuple of strings from the alphabet { this notation,  X  is always the tuple  X   X  1 , ... ,  X   X  ( B ) include  X  and  X  in the rule notation merely to remind the reader of the meaning of the symbols in  X  .
 indicating that B and C each have one span, and are concatenated in order to form A . 1994). Figure 1 demonstrates how the adjunction operation is done with binary LCFRS. nonterminal B and concatenates it to the first span of nonterminal C (to get the first span of A ), and then takes the second span of C and concatenates it with the second span of B (to get the second span of A ). For TAGs, rules have the form: covers. The fan-out of CFG rules is 1, and the fan-out of TAG rules is 2. The fan-out of the grammar, f , is the maximum fan-out of its nonterminals: is just the context-free rule A  X  B C , omitting the variables. In that context, a logical B C
A statement such as A  X  B C  X  R is true if there is any rule A [  X  ]  X  B [ some  X  ,  X  , and  X  .
 re-ordered by the rule, and similarly we assume that  X  1 , ... ,  X  this is not the case in some rule, then the grammar can be transformed by introducing a new nonterminal for each permutation of a nonterminal that can be produced by the grammar. We further assume that  X  1,1 =  X  1 , that is, that the first span of A begins with material produced by B rather than by C . If this not the case for some rule, B and C can be exchanged to satisfy this condition.
 internal to a span of A , and dual-initial if the leftmost endpoint of C is the beginning of a span of A . Our algorithm will require the input LCFRS to be in single-initial form, meaning that all rules are single-initial. We note that grammars for common formalisms including TAG and synchronous context-free grammar (SCFG) are in this form. If a grammar is not in single-initial form, dual-initial rules can be converted to single-initial form by adding an empty span to B that combines with the first spans of C immediately to its left, as shown in Figure 2. Specifically, for each dual-initial rule A  X  B C , if the first span of C appears between spans i and i + 1 of B , create a new nonterminal B with  X  ( B 0 ) =  X  ( B ) + 1, and add a rule B 0  X  B , where B of length zero between spans i and i + 1 of B . We then replace the rule A  X  B C with
A  X  B 0 C , where the new span of B 0 combines with C immediately to the left of C  X  X  first span. Because the new nonterminal B 0 has fan-out one greater than B , this grammar transformation can increase a grammar X  X  fan-out by at most one.
 power of our results. Any LCFRS with arbitrary rank (i.e., with an arbitrary number potentially a larger fan-out). See discussion in Section 7.6.
 Example 1 Consider the phenomenon of cross-serial dependencies that exists in certain languages.
It has been used in the past (Shieber 1985) to argue that Swiss X  X erman is not context-free. One can show that there is a homomorphism between Swiss X  X erman and the regular language a  X  b  X  c  X  d  X  gives the language L = { a not context-free, this implies that Swiss-German is not context-free, because context-free languages are closed under intersection with regular languages.
 malisms that can handle such cross-serial dependencies in languages (where the a s B C A 426 are aligned with c s and the b s are aligned with the d s). For example, a tree-adjoining grammar for generating L would include the following initial and auxiliary trees (nodes marked by an asterisk are nodes where adjunction is not allowed):
Here we have one unary LCFRS rule for the initial tree, one unary rule for each ad-junction tree, and one null-ary rule for each nonterminal producing a tuple of empty strings in order to represent TAG tree nodes at which no adjunction occurs. The LCFRS shown here does not satisfy our normal form requiring each rule to have either two or zero nonterminals with a composition function returning fixed strings of terminals.
However, it can be converted to such a form through a process analogous to converting a CFG to Chomsky Normal Form. For adjunction trees, the two strings returned by the composition function correspond the the material to the left and right of the foot node. material produced by internal nodes of the tree at which adjunction may occur. ments of the composition function in any order. 3. A Sketch of the Algorithm multiplication for the goal of LCFRS recognition.
 with a specific non-associative multiplication and addition operator such that multiply-ing T by itself k times on the left or on the right yields k -step derivations for a given string. The row and column indices of the matrix together assemble a set of spans in the string (the fan-out of the grammar determines the number of spans). Each cell in the matrix keeps track of the nonterminals that can dominate these spans. Therefore, nonterminals that can dominate the assembled indices X  spans for the specific string at hand.
 rows correspond to the left endpoints of a span and the columns correspond to its right endpoints. Our matrix indexing scheme can mix both left endpoints and right endpoints at either the rows or the columns. This is necessary because with LCFRS, spans for the right-hand side of an LCFRS rule can combine in various ways into a new set of spans for the left-hand side.
 in the matrix T (or its matrix powers) are equivalent and should consist of the same nonterminals. The reason we need such an over-complete scheme is again because of the possible ways spans of a right-hand side can combine in an LCFRS. To address this over-completeness, we introduce into the multiplication operator a  X  X opy operation X  that copies nonterminals between cells in order to maintain the same set of nonterminals in equivalent cells.
 into the following sequence of matrix transformations. We will start with the following matrices, T 1 and T 2 : and (2, 7) for column denotes that B spans the constituents (1, 2) and (7, 8) in the string B C
A 428 multiplication). Similarly, with T 2 , C spans the constituents (2, 4) and (5, 7). because B and C share these two endpoints, they can combine to form A . In the matrix representation, (2, 7) appears as the column address of B and as the row address of C , meaning that B and C appear in cells that are combined during matrix multiplication.
The result of multiplying T 1 by T 2 is the following: and the merging of the spans (5, 7) and (7, 8) (right span of C and right span of B ) into (5, 8). Finally, an additional copying operation will lead to the following matrix: (4, 5) into the address with the row (1, 4) and column (5, 8). Both of these addresses cor-respond to the same spans (1, 4) and (5, 8). Note that matrix row and column addresses can mix both starting points of spans and ending points of spans. 4. A Matrix Multiplication Algorithm for LCFRS
We turn next to give a description of the algorithm. Our description is constructed as follows: 4.1 Matrix Structure
The algorithm will seek to compute the transitive closure of a seed matrix T ( d ), where d is a constant determined by the grammar (see Section 4.3). The matrix rows and columns are indexed by the set N ( d ) defined as: where n denotes the length of the sentence, and the exponent d
Cartesian product. Thus each element of N ( d ) is a sequence of indices into the string, it is marked or unmarked. Marked indices will be used in the copy operator defined later. Indices are unmarked unless specified as marked: We use  X  x to denote a marked index ( x , 1) with x  X  [ n ] 0 .
 set of all elements in the first coordinate of each element in the sequence (ignoring the additional bits). As such, 430 k by analogy to the variables in the CKY parsing algorithm, and also because we fine m ( i , j ) to be the set of f 0 = 1 2 | i  X  j | pairs { ( ` ` &lt; ` k + 1 for k  X  [2 f 0  X  1] and ( ` k , 0)  X  i  X  j for k  X  [2 f as input the two sequences in matrix indices, merges them, sorts them, then divides undefined.
 if min i &lt; min j . We assume that the rows and columns of our matrices are arranged in as T and N ( d ) as N .
 where , , , , , and are six special pre-defined symbols. 2 such that T ij  X  M .
 nonterminal are never re-ordered, meaning that it is not necessary to retain informa-tion about which indices demarcate which components of the nonterminal, because the second two indices as delimiting the second span, and so on. The two additional ditional , , , , , symbols are symbols that indicate to the matrix multipli-cation operator that a  X  X opying operation X  should happen between equivalent cells (Section 4.2).
 of the algorithm correspond to entries in the LCFRS parsing chart that can be derived immediately from terminals in the string. Entries added in Step 3 of the algorithm do not depend on the input string or input grammar, but rather initialize elements used in the copy operation described in detail in Section 4.2. Because the algorithm only initializes take advantage of in Section 4.2. 4.1.1 Configurations. Our matrix representation requires that a nonterminal appears configuration of a nonterminal in a rule. The concept of a configuration is designed to represent which endpoints of spans of the rule X  X  right-hand side (r.h.s.) nontermi-nals B and C meet one another to produce larger spans, and which endpoints, on nal A .
 of endpoints in the row address of the nonterminal X  X  matrix cell. To make this precise, for a nonterminal B with fan-out  X  ( B ), we number the endpoints of spans with integers of [2  X  ( B )] of endpoints of B that do not combine with endpoints of C in order to form a single span of A . The endpoints will form the row address for B . Formally, let  X   X  combination endpoints of B is defined as: 432 where the first set defines right ends of spans of B that are right ends of some span of A , and the second set defines left ends of spans of B that are left ends of some span of A .
For example, given that CFG rules have the form endpoint of A . For the TAG rule t shown in Figure 1, config four endpoints, the first and fourth are also endpoints of A .
 endpoints in the row address for C , which are the endpoints that do combine with B : config 3 ( r ) = { 2 i |  X  i =  X  j , k for some 1  X  k &lt; n where the first set defines right ends of spans of C that are internal to some span of A , and the second set defines left ends of spans of C that are internal to some span of A . For of C is internal to A . For the TAG rule t shown in Figure 1, config of C  X  X  four endpoints, the first and fourth are internal A .
 in the matrix cell where the row address corresponds to the endpoints from B , and the column address corresponds to the endpoints from C . To capture this partition of the endpoints of A , we define second set defines left ends of spans of A that are formed from B . For example, any derived from B . For the TAG rule t shown in Figure 1, config four endpoints, the first and fourth are derived from B . 4.2 Definition of Multiplication Operator
We need to define a multiplication operator  X  between a pair of elements R , S  X  M . Such a multiplication operator induces multiplication between matrices of the type of T , just by defining for two such matrices, T 1 and T 2 , a new matrix of the same size T that:
We also use the  X  symbol to denote coordinate-wise union of cells in the matrices it operates on.
 that for R , S 1 , S 2  X  M it holds that: maintains the upper-triangularity of the transitive closure of T .
 rithm is composed of two components. The first component (Step 2 in Figure 5) adds respectively, such that there exists a rule A  X  B C and the span endpoints denoted by k are the points where the rule specifies that spans of B and C should meet. of configurations defined earlier. To apply a rule r : A [  X  ]  X  B [ of B selected by config 2 ( r ) and k is a set of indices corresponding to the endpoints of corresponding to the endpoints of C selected by config corresponding to the endpoints of C selected by [2  X  ( C )] \ config by Step 2d. Finally, the spans defined by B and C must not overlap in the string. To 434 guarantee that the spans do not overlap, we sort the endpoints of A and check that each position in the sorted list is derived from either B or C as required by the configuration of A in r . This check is performed in Step 2e of Figure 5.
 cation guarantee that all matrix powers of T are upper-triangular. We now proceed to show that upper-triangular matrices are sufficient in terms of the grammar. In particular, we need to show the following lemma: Lemma 1
A by multiplying two upper-triangular matrices T 1 and T 2 for B , and T 2 contains an entry for C .
 Proof
A nonterminal B appears in a cell above the diagonal if its row address is smaller than its column address, which in turn occurs if the leftmost endpoint of B appears in the row address rather than the column address. The row address for B contains the endpoints of B that are also endpoints of A . Our normal form for LCFRS rules ensures that the leftmost endpoint of B forms the leftmost endpoint of A . Therefore the leftmost endpoint of B is in B  X  X  row address, and B is above the diagonal.
 bine with endpoints of B . For single-initial rules, these endpoints include the leftmost endpoint of C , guaranteeing that C appears above the diagonal.
 that are above the diagonal, each instance of A can be produced by multiplying two upper-triangular matrices. 4.2.1 Copy Operations. The first component of the algorithm is sound, but not complete.
If we were to use just this component in the algorithm, then we would obtain in each m ( i , j ). The reason this happens is that our addressing scheme is  X  X ver-complete. X  This terminals from one cell to its equivalents. This is done by the second component of the algorithm, in Steps 3 X 6. The algorithm does this kind of copying by using a set of six special  X  X opy X  symbols, { , , , , , } . These symbols copy nonterminals from one cell to the other in multiple stages.
 string. We must move the indices in i  X  ` from the row address to the column address, and we must move the indices in j  X  k from the column address to the row address. We will move one index at a time, adding nonterminals to intermediate cells along the way. to a column address (moving from column to row is similar). Let x indicate the index we wish to move, meaning that we wish to copy a nonterminal in cell ( i , j ) to advantage of fast matrix multiplication, we accomplish the copy operations through
T 1 (2, 7) (1, 8) { ... , B , ... }
T 3 (1, 8) (1) { ... , , ... }
T 3 T 1 T 2 (2, 7,  X  8) (1) { ... , B , ... }
T 3 T 1 T 2 T 4 (2, 7, 8) (1) { ... , B , ... } 436 the row and index  X  x appears in the column address. To remove x from the row address, we multiply on the left with a matrix containing the symbol in cell (remove( i , x ), i ), multiply by a third matrix to replace the marked index  X  x with the unmarked index x .
This is done by multiplying on the right with a matrix containing the symbol in cell (insert( j ,  X  x ), insert( j , x )).
 another through intermediate cells. In matrix multiplication, only cells that share a row or a column index actually interact when doing multiplication. Therefore, in order to copy a nonterminal from ( i , j ) to another cell which represents the same set of spans, we equivalent addresses, the seed matrix contains the special symbol only in cells ( j , k ) column address was originally present in the row address. In addition, the condition guarantee that only one index is marked in the address of any non-empty matrix cell. one index at a time. Furthermore, when in cell ( i , k ) combines with a nonterminal A in all the original indices, meaning that any index we remove from the row address is still present as a marked index in the column address.
 mark from  X  x does not take place until after x has been removed from the row address. operation followed by one operation and one operation. The conditions on these three special symbols are analogous to the conditions on , , and outlined earlier, some x .
 nonterminals between all equivalent cells above the diagonal.
 Lemma 2
Let ( i , j ) and ( k , ` ) be unmarked matrix addresses, in a seed matrix T indexed by row insert( i , x ) and ` = remove( j , x ) for some x . If A appears in cell ( i , j ) of T into any other cells with unmarked addresses.
 Proof
The condition on d guarantees that we can form row and column addresses long enough to hold the redundant representations with one address shared between row and column. This condition is only relevant in the case where i , j , k , and ` are all of the same length; in this case we need to construct temporary indices with length one greater, as in the example in Figure 6.
 by combining with symbols , , and or with , , and . Because T
The condition min i = min k implies that we are not moving this leftmost index from most index of i is not modified, the copy symbols that are required are all above the
T are the only symbols that introduce nonterminals into unmarked addresses. They can only apply when a marked index is present, and when the total number indices is 2  X  ( A ).
This can only occur after either has introduced a marked index and removed the corresponding unmarked index, or has introduced a marked index and removed the corresponding unmarked index.
 following lemma: Lemma 3 min {| k | , | ` |} . Then, for any nonterminal A in cell ( i , j ) in T ( k , ` ) of the power matrix T ( n + 6 d ) .
 Proof
Nonterminal A can be copied through a series of intermediate cells by moving one index at a time from i to ` , and from j to k . We begin by moving indices from either the row address otherwise. We must move up to d indices from row to column, and d indices matrix multiplications. 4.3 Determining the Contact Rank points from the first multiplicand column address with endpoints from the second all possible sequences of endpoints that could potentially combine with a given fixed LCFRS.
 points . For example, in the simple case of a CFG with a rule S  X  NP VP, there is one combining point where NP and VP meet. For the TAG rule shown in Figure 1, there are two combining points where nonterminals B and C meet. For each rule r in the
LCFRS grammar, we must be able to access the combining points as row and column addresses in order to apply the rule with matrix multiplication. Thus, d must be at least 438 the maximum number of combining points of any rule in the grammar. The number of combining points  X  ( r ) for a rule r can be computed by comparing the number of spans on the l.h.s. and r.h.s. of the rule: be denoted by  X  ( A  X  B C ). 3 the combination points in one dimension (either row or column), and the other points in the other dimension of the matrix. For r.h.s. nonterminal B in rule A  X  B C , the number of non-combination endpoints is: addresses needed are of length: of a CFG is 1, while the contact rank of a TAG is 2. A simple algebraic manipulation shows that the contact rank can be expressed as follows:
Because the process of converting an LCFRS grammar to single-initial form increases its fan-out by at most one, the contact rank is also increased by at most one. 4.4 Balanced Grammars
We define the configuration set of a nonterminal A to the the set of all configurations (Section 4.1.1) in which A appears in a grammar rule, including both appearances in the r.h.s. and as the l.h.s. as shown in Section 4.1.1, nonterminals are always used in the unique configuration { 1 } . For TAG, the configuration set of any nonterminal is {{ 1, 4 }} because, as in CFG, nonterminals are always used in the same configuration.
 number of contact points and non-contact points are the same.
 long as we move the first index from row to column, rather than from column to row, the intermediate results will require addresses no longer than the length of i . impossible to copy entries for B between the cells using a matrix of size (2 n ) because we cannot move indices from row to column or from column to row without creating an intermediate row or column address of length greater than d as a result of the first or operation.
 is 1. Similarly, TAG is not balanced, because each nonterminal has only one configura-tion. ITGs are balanced, because, for each nonterminal B ,  X  ( B ) = d = 2, and nontermi-nals can be used in two configurations, corresponding to straight and inverted rules. for the top level of our parsing algorithm.
 Condition 4.1
Unbalanced Grammar Condition There is no nonterminal B such that  X  ( B ) = d and | config( B ) | &gt; 1.
 multiplication: Lemma 4 for any nonterminal A with | config( A ) | &gt; 1 that appears in cell ( i , j ) in T appear in cell ( k , ` ) of the power matrix T ( n + 6 d ) 440 Proof
A will appear in cell ( k , ` ) of the power matrix T ( n + 6 d ) 4.5 Computing the Transitive Closure of T define where T ( i ) is defined recursively as: over the input string, each node in t must appear in the transitive closure ma-trix T + . Specifically, for each node in t representing nonterminal A spanning end-points { ( ` 1 , ` 2 ), ( ` 3 , ` 4 ), ... , ( ` 2  X  ( A )  X  1 m ( i , j ) = { ( ` 1 , ` 2 ), ( ` 3 , ` 4 ), ... , ( ` 2  X  ( A )  X  1 result: Lemma 5
Under Condition 4.1, the transitive closure of T is such that [ T nonterminals that are derivable for the given spans in m ( i , j ).
 Proof derivations consisting of a single rule A [  X  ]  X  B [ j corresponding the non-combination points of B and C . For all other i and j such that m ( i , j ) = { ( ` 1 , ` 2 ), ( ` 3 , ` 4 ), ... , ( `
Lemma 4. By induction, T s (6 d + 2) contains entries for all LCFRS derivations of depth s , and T + contains entries for all LCFRS derivations of any length.
 valid LCFRS derivation of nonterminal A spanning endpoints m ( i , j ). This can be shown by induction over the number of matrix multiplications. During each multiplication, entries created in the product matrix correspond either to the application of an LCFRS rule with l.h.s. A , or to the movement of an index between row and column address for a previously recognized instance of A .
 which takes a matrix T 0 of the same type of T , and sets  X  ( T procedure: 2. Set [  X  ( T 0 )] ij = [ all equivalent addresses in T 0 . Note that the  X  operator can be implemented such that it operates in time O ( n 2 d ). All it requires is just taking O ( n to the sentence length (i.e., the size is only a function of the grammar), and each union is over O (1) sets.
 satisfy Condition 4.1 (we also assume that these binary LCFRS would not have unary through transitive closure steps and copying steps until convergence. When we take the transitive closure of T , we are essentially computing a subset of the derivable non-terminals. Then, the copying step (with  X  ) propagates nonterminals through equivalent cells. Now, if we take the transitive closure again, and there is any way to derive new nonterminals because of the copying step, the resulting matrix will have at least one new nonterminal. Otherwise, it will not change, and as such, we recognized all possible derivable nonterminals in each cell.
 Lemma 6
For any single-initial LCFRS, when Step 2 of the algorithm in Figure 9 converges, T is such that [ T ] ij represents the set of nonterminals that are derivable for the given spans in m ( i , j ).
 Proof
Any LCFRS derivation of a nonterminal can be decomposed into a sequence of rule applications and copy operations, and by induction over the length of the derivation, all derivations will be found. Each matrix operation only produces derivable LCFRS nonterminals, and by induction over the number of steps of the algorithm, only deriv-able nonterminals will be found. multiplication operator similar to ours, can be reduced to the problem of Boolean matrix multiplication. His transitive closure algorithm requires as a black box this two-matrix multiplication algorithm.
 tation of the transitive closure, because our multiplication operator is distributive (with respect to  X  ). To complete our argument, we need to show, similarly to Valiant, that the product of two matrices using our multiplication operator can be reduced to Boolean matrix multiplication (Figure 7).
 reduce it to Boolean matrix multiplication, we create 2 | R | pairs of matrices, G 442 G
B (2, 7) (1, 8) 1 I
BC = G B H C (4, 5) (1, 8) 1 where r ranges over R . The size of G r and H r is N  X  N . If r = A [  X  ]  X  B [ [ G r ] ik to be 1 if the nonterminal B appears in [ T 1 ] ik and C , k , and j meet the conditions of Step 2d. All other cells, in both G to 0. Note that G r and H r for all r  X  R are upper triangular Boolean matrices. of nonterminals L . We set [ G A ] ik to be 1 if the nonterminal A appears in [ T the conditions of Step 2c of Figure 5. Similarly, we set [ H appears in [ T 2 ] kj , regardless of the conditions of Step 2d. All other cells, in both G H A , are set to 0. Again, G A and H A for all A  X  L are upper triangular Boolean matrices. , , , } . These matrices indicate the positions in which each symbol appears in the seed matrix T defined in Figure 4: 1. G , for which [ G ] ij = 1 only if ( , i , j )  X  T ij . 2. H , for which [ H ] ij = 1 only if ( , i , j )  X  T ij . 3. H , for which [ H ] ij = 1 only if ( , i , j )  X  T ij . 4. G , for which [ G ] ij = 1 only if ( , i , j )  X  T ij . 5. H , for which [ H ] ij = 1 only if ( , i , j )  X  T ij . 6. G , for which [ G ] ij = 1 only if ( , i , j )  X  T ij . matrix multiplications required is | R | , which is constant in n . Now, T by multiplying these matrices, and applying the conditions of Figure 5: 1. For each A  X  L , for each rule r = A  X  B C , check whether [ I 2. For each A  X  L , compute J A = G A H . For each ( i , j ), add A to [ T 3. For each A  X  L , compute J A = G H A . For each ( i , j ), add A to [ T 4. For each A  X  L , compute J A = G A H . For each ( i , j ), add A to [ T 5. For each A  X  L , compute J A = G H A . For each ( i , j ), add A to [ T 6. For each A  X  L , compute J A = G A H . For each ( i , j ), add A to [ T 7. For each A  X  L , compute J A = G H A . For each ( i , j ), add A to [ T Lemma 7
The matrix product operation for two matrices of size (2 n ) time O ( n  X  d ), if two m  X  m Boolean matrices can be multiplied in time O ( m Proof
The result of the algorithm above is guaranteed to be the same as the result of matrix multiplication using the  X  operation of Figure 5 because it considers all combinations of i , j , and k and all pairs of nonterminals and copy symbols, and applies the same set of conditions. This is possible because each of the conditions in Figure 5 applies either to a matrix multiplication, or to the pair ( i , j ), in which case we apply the condition to the result of the Boolean matrix multiplication. Crucially, no condition in Figure 5 involves i , j , and k simultaneously.
 while the pre-and post-processing steps for each matrix multiplication take only O ( n
The number of Boolean matrix multiplications depends on the grammar, but is constant with respect to n , yielding an overall runtime of O ( n  X  d symbol appears in a cell with an address that spans the whole string. If so, the string is in the language of the grammar. 5. Computational Complexity Analysis and  X  used in our matrix multiplication distribute. The  X  operator takes the cross 444 take the cross product of the union, or the union of the cross product. However, unlike general, x  X  ( y  X  z ) 6 = ( x  X  y )  X  z , because the combination of y and z may be allowed by the LCFRS grammar, whereas the combination of x and y is not.
 Lemma 8
The transitive closure of a matrix of size (2 n ) d  X  (2 n ) if 2 &lt;  X  &lt; 3, and two m  X  m Boolean matrices can be multiplied in time O ( m Proof
We can use the algorithm of Valiant for finding the closure of upper triangular matrices under distributive, non-associative matrix multiplication. Because we can perform one matrix product in time O ( n  X  d ) by Lemma 7, the algorithm of Valiant (1975, Theorem 2) can be used to compute transitive closure also in time O ( n such multiplication was Strassen X  X  algorithm, with M ( n ) = O ( n attempts to further reduce  X  , or find lower bounds for M ( n ).
 Theorem 1
A single-initial binary LCFRS meeting Condition 4.1 can be parsed in time O ( n where d is the contact rank of the grammar, 2 &lt;  X  &lt; 3, and two m  X  m Boolean matrices can be multiplied in time O ( m  X  ).
 Proof
By Lemma 8, Step 2 of the algorithm in Figure 8 takes O ( n of Step 2 gives all nonterminals that are derivable for the given spans in m ( i , j ). plexity of LCFRS chart parsing techniques is O ( n p ). We can now ask the question: In which case the algorithm in Figure 8 is asymptotically more efficient than standard chart parsing techniques with respect to n ? That is, in which cases is n condition for that is that for any rule A  X  B C  X  R it holds that: result of this article for arbitrary LCFRS: Theorem 2
A single-initial binary LCFRS can be parsed in time O ( n rank of the grammar, 2 &lt;  X  &lt; 3, and two m  X  m Boolean matrices can be multiplied in time O ( m  X  ). 446 Proof
The algorithm of Figure 9 works by iteratively applying the transitive closure and the copying operator until convergence. At convergence, we have recognized all derivable nonterminals by Lemma 6. Each transitive closure has the asymptotic complexity of
O ( n  X  d ) by Lemma 8. Each  X  application has the asymptotic complexity of O ( n such, the total complexity is O ( tn  X  d ), where t is the number of iterations required to converge. At each iteration, we discover at least one new nonterminal. The total number of nodes in the derivation for the recognized string is O ( n ) (assuming no unary cycles or rules). As such t = O ( n ), and the total complexity of this algorithm is O ( n 6. Applications
Our algorithm is a recognition algorithm that is applicable to binary LCFRS. As such, our algorithm can be applied to any LCFRS, by first reducing it to a binary LCFRS. We discuss results for specific classes of LCFRS in this section, and return to the general binarization process in Section 7.6.
 (1975) focused on. Valiant showed that the problem of CFG recognition can be reduced to the problem of matrix multiplication, and, as such, the complexity of CFG recognition in that case is O ( n  X  ). Our result generalizes Valiant X  X  result. CFGs (in Chomsky Normal
Form) can be reduced to a binary LCFRS with f = 1. As such, d = 1 for CFGs, and our therefore we can use a single transitive closure step.) grammar formalisms, some of which were discovered or developed independently of
LCFRS. Two such formalisms are tree-adjoining grammars (Joshi and Schabes 1997) and synchronous context-free grammars. In the next two sections, we explain how our algorithmic result applies to these two formalisms. 6.1 Mildly Context-Sensitive Language Recognition sensitive grammar formalisms. They subsume four important mildly context-sensitive formalisms that were developed independently and later shown to be weakly equiv-alent by Vijay-Shanker and Weir (1994): tree-adjoining grammars (Joshi and Schabes 1997), linear indexed grammars (Gazdar 1988), head grammars (Pollard 1984), and combinatory categorial grammars (Steedman 2000). Weak equivalence here refers to the idea that any language generated by a grammar in one of these formalisms can be also generated by some grammar in any of the other formalisms among the four. It can be verified that all of these formalisms are unbalanced, single-initial LCFRSs, and as such, the algorithm in Figure 8 applies to them.
 with an asymptotic complexity of O ( M ( n 2 )) = O ( n 4 . 76 that, the weak equivalence between the four formalisms mentioned here implies that all of them can be parsed in time O ( M ( n 2 )). Our algorithm generalizes this result. We now give the details.
 specific case of linear context-free rewriting systems, not just in the formal languages they define X  X ut also in the way these grammars are described. They are described using concatenation production rules and wrapping production rules, which are di-rectly transferable to LCFRS notation. Their fan-out is 2. We focus in this discussion on  X  X inary head grammars, X  defined analogously to binary LCFRS X  X he rank of all production rules has to be 2. The contact rank of binary head grammars is 2. As such, our work shows that the complexity of recognizing binary head grammar languages is O ( M ( n 2 )) = O ( n 4 . 76 ).
 tually be reduced to binary head grammars. Linear indexed grammars are extensions of CFGs, a linguistically motivated restricted version of indexed grammars, the latter of which were developed by Aho (1968) for the goal of handling variable binding in programming languages. The main difference between LIGs and CFGs is that the with LIGs copy the stack on the left-hand side to one of the nonterminal stacks in the right-hand side, 5 potentially pushing or popping one symbol in the new copy of the stack. For our discussion, the main important detail about the reduction of LIGs to head grammars is that it preserves the rank of the production rules. As such, our work shows that binary LIGs can also be recognized in time O ( n 4 . 76 tory categorial grammars to LIGs. The combinators they allow are function application and function composition. The key detail here is that their reduction of CCG is to an
LIG with rank 2, and, as such, our algorithm applies to CCGs as well, which can be recognized in time O ( n 4 . 76 ).
 natory categorial grammars. The TAGs they tackle are in  X  X ormal form, X  such that the auxiliary trees are binary (all TAGs can be reduced to normal form TAGs). Such TAGs can be converted to weakly equivalent CCG (but not necessarily strongly equivalent), and as such, our algorithm applies to TAGs as well. As mentioned earlier, this finding supports the finding of Rajasekaran and Yooseph (1998), who show that TAG can be recognized in time O ( M ( n 2 )).
 multiplication, see Satta (1994). 448 6.2 Synchronous Context-Free Grammars
SCFGs are widely used in machine translation to model the simultaneous derivation of translationally equivalent strings in two natural languages, and are equivalent to the syntax-directed translation schemata of Aho and Ullman (1969). SCFGs are a subclass of
LCFRS where each nonterminal has fan-out 2: one span in one language and one span in the other. Because the first span of the l.h.s. nonterminal always contains spans from both r.h.s. nonterminals, SCFGs are always single-initial. Binary SCFGs, also known as widely used model in syntax-based statistical machine translation.
 the three nonterminals in a rule has fan-out of two. ITGs, unfortunately, do not satisfy Condition 4.1, and therefore we have to use the algorithm in Figure 9. Still, just like with TAG, each rule combines two nonterminals of fan-out 2 using two combination points.
Thus, d = 2, and we achieve a bound of O ( n 2  X  + 1 ) for ITG, which is O ( n current state of the art for matrix multiplication.

Generalizing ITG to allow two nonterminals on the right-hand side of a rule in each of k languages, we have an LCFRS with fan-out k . Traditional tabular parsing has an asymp-totic complexity of O ( n 3 k ), whereas our algorithm has the complexity of O ( n the best well-known result for is that of binary synchronous TAGs (Shieber and Schabes reduced to a binary LCFRS. A tabular algorithm for such grammar has the asymptotic asymptotic complexity in that case is O ( n 9 . 52 ). 7. Discussion and Open Problems
In this section, we discuss some extensions to our algorithm and open problems. 7.1 Turning Recognition into Parsing
The algorithm we presented focuses on recognition: Given a string and a grammar, actual derivation tree, if it identifies that the string is in the language. asymptotic complexity of O ( n  X  d + 1 ). Once the transitive closure of T is computed, we can backtrack to find such a parse, starting with the start symbol in a cell spanning the whole string. When we are in a specific cell, we check all possible combination points (there are d of those) and nonterminals, and if we find such pairs of combination points and nonterminals that are valid in the chart, then we backtrack to the corresponding cells. The asymptotic complexity of this post-processing step is O ( n than O ( n  X  d ) (  X  &gt; 2, d &gt; 1).
 pre-calculated chart. If the chart was not already available when our algorithm finishes, the asymptotic complexity of this step would correspond to the asymptotic complexity of a na  X   X ve tabular parsing algorithm. It remains an open problem to adapt our algo-
Section 7.3. 7.2 General Recognition for Synchronous Parsing
Similarly to LCFRS, the rank of an SCFG is the maximal number of nonterminals that the LCFRS grammar can be larger than 2. This happens because binarization creates intermediate nonterminals that span several substrings, denoting binarization steps of the rule. These substrings are eventually combined into two spans, to yield the language of the SCFG grammar (Huang et al. 2009).
 ing over tabular methods. For example, Figure 10 shows the combination of spans for the rule [ S  X  A B C D , B D A C ], along with a binarization into three simpler LCFRS rules. A na  X   X ve tabular algorithm for this rule would have the asymptotic complexity of
O ( n 10 ), but the binarization shown in Figure 10 reduces this to O ( n a rule with d = 4. 7.3 Generalization to Weighted Logic Programs
Weighted logic programs (WLPs) are declarative programs, in the form of Horn clauses such as CKY and other types of dynamic programming algorithms or NLP inference algorithms (Eisner, Goldlust, and Smith 2005; Cohen, Simmons, and Smith 2011). semiring) over a set of possible values in the free variables in the Horn clauses. With
CKY, for example, this sum will be performed on the mid-point concatenating two spans. This join operation is also the type of operation we address in this paper (for LCFRS) in order to improve their asymptotic complexity.
 arbitrary weighted logic programs. In order to create an algorithm that takes as input 450 a weighted logic program (and a set of axioms) and  X  X ecognizes X  whether the goal is achievable, we would need to have a generic way of specifying the set N , which was specialized to LCFRS in this case. Not only that, we would have to specify N in such a way that the asymptotic complexity of the WLP would improve over a simple dynamic programming algorithm (or a memoization technique).
 unweighted grammars. Bened  X   X  and S  X  anchez (2007) showed how to generalize Valiant X  X  algorithm in order to compute inside probabilities for a PCFG and a string. Even if we were able to generalize our addressing scheme to WLPs, it remains an open question to see whether we can go beyond recognition (or unweighted parsing). 7.4 Rytter X  X  Algorithm
Rytter (1995) gives an algorithm for CFG parsing with the same time complexity as Valiant X  X , but a somewhat simpler divide-and-conquer strategy. Rytter X  X  algorithm string and entirely within the second half of the string. The combination step uses a shortest path computation to identify the sequence of chart items along a spine of the that cross the midpoint of the string, forms a single path from the root to one leaf of the derivation tree. This property does not hold for general LCFRS, because two siblings in the derivation tree may both correspond to multiple spans in the string, each containing material on both sides of the string midpoint. For this reason, Rytter X  X  algorithm does not appear to generalize easily to LCFRS. 7.5 Relation to Multiple Context-Free Grammars
Nakanishi et al. (1998) develop a matrix multiplication parsing algorithm for multiple context-free grammars (MCFGs). When these grammars are given in a binary form, they can be reduced to binary LCFRS. Similarly, binary LCFRS can be reduced to binary
MCFGs. The algorithm that Nakanishi et al. develop is simpler than ours, and does not directly tackle the problem of transitive closure for LCFRS. More specifically, Nakanishi et al. multiply a seed matrix such as our T by itself in several steps, and then follow up with a copying operation between equivalent cells. They repeat this n times, where n is the sentence length. As such, the asymptotic complexity of their algorithm is identical for both balanced and unbalanced grammars, a distinction they do not make. balanced, then both our algorithm and their algorithm give a complexity of O ( n the grammar is unbalanced, then our algorithm gives a complexity of O ( n the asymptotic complexity of their algorithm remains O ( n al. X  X  algorithm does not generalize Valiant X  X  algorithm X  X ts asymptotic complexity for context-free grammars is O ( n  X  + 1 ) and not O ( n  X  ).
 of T without the extra O ( n ) factor that their algorithm incurs. In our paper, we provide a solution to this open problem for the case of single-initial, unbalanced grammars. The core of the solution lies in the matrix multiplication copying mechanism described in
Section 4.2. 7.6 Optimal Binarization Strategies
The two main grammar parameters that affect the asymptotic complexity of parsing with LCFRS (in its general form) are the fan-out of the nonterminals and the rank of the total fan-out of all nonterminals in the rule. For binary rules of the form A  X  B C , p =  X  ( A ) +  X  ( B ) +  X  ( C ).
 equivalent to another non-binary LCFRS, we would want to minimize the time com-in the resulting binary grammar. Gildea (2011) has shown that this metric corresponds to the tree width of a dependency graph that is constructed from the grammar. It is not known whether finding the optimal binarization of an LCFRS is an NP-complete problem, but Gildea shows that a polynomial time algorithm would imply improved approximation algorithms for the treewidth of general graphs.
 optimal binarization for parsing with our algorithm based on matrix multiplication. In order to optimize the complexity of our algorithm, we want to minimize d , which is the maximum over all rules A  X  B C of plexity, and, hence, the optimal d  X  over binarizations of an LCFRS is always less than the optimal p  X  for tabular parsing. However, whether any savings can be achieved with our algorithm depends on whether  X  d  X  &lt; p  X  , or  X  d  X  grammars. Our criterion does not seem to correspond closely to a well-studied graph-theoretic concept such a treewidth, and it remains an open problem to find an efficient algorithm that minimizes this definition of parsing complexity.
 lower bound on the time complexity of our algorithm relative to tabular parsing using 8. Conclusion
We described a parsing algorithm for binary linear context-free rewriting systems that the grammar (the maximal number of combination points in the rules in the grammar generalizes the algorithm of Valiant (1975), and also reinforces existing results about mildly context-sensitive parsing for tree-adjoining grammars (Rajasekaran and Yooseph 1998). Our result also implies that inversion transduction grammars can be parsed in time O ( n 2  X  + 1 ) and that synchronous parsing with k languages has the asymptotic complexity of O ( n  X  k + 1 ) where k is the number of languages. 452 Appendix A. Notation Table A.1 provides a table of notation for symbols used in this article. Acknowledgments References 454
