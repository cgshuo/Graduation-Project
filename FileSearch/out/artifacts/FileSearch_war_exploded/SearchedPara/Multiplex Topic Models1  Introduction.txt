 Documents contain not only content information but also relational information such as coauthors, citations and geographic locations, leading to the multiplex net-work structures. For instance, the bibliographic field of a scientific paper has a list of entries: authors, venues, time, and refer ences, representing different types of re-lations between papers. Citation and coau thor links exist widely in both scientific papers and web pages, and thus attract considerable interest in the topic model-ing community [1 X 5]. Intuitively, the papers cited each other, written by the same author, or published in the same venue, will have stronger topic correlations. Pre-vious topic models consider only equal-weighted multiplex structures in document networks, i.e., citation and coauthor links have been treated equally in topic for-mation. However, in real-world applications it is obvious that different links may play different roles in topic modeling. For example, two authors who often col-laborate on some papers tend to be interested in the same topics, while the cited paper may be from an interdisciplinary area with a quite different topic. How to quantify and balance different types of links in document networks for a better topic modeling performance still remains a challenging and unsolved problem.
In this paper, we propose a novel multiplex topic model (MTM) for multi-plex document networks. MTM uses a factor graph [6, 7] to combine multiple types of links into the topic modeling process. The factor graph provides a nat-ural graphical description that factorizes the joint probability of MTM into a product of local functions. The topic label for each word is represented as the variable node in the factor graph, and different links between documents can be encoded by the factor nodes with parameterized functions to encourage or penalize specific topic labeling configurations through links. To maximize the joint probability of MTM, we develop a multiplex belief propagation (MBP) al-gorithm for approximate inference and parameter estimation. MBP is a message passing algorithm [8] based on multiple links. We quantify and balance the link influence by consistency of messages passed through the link. Intuitively, if the passed messages are more consistent, they will influence the topic labeling config-uration more through the link. To summarize, the methodological contributions of our work are: (1) MTM that incorporates different types of links using the factor graph, (2) MBP that maximizes the joint probability of MTM by passing messages through multiple links, and (3) balance of link influences by the consis-tency of messages passed through links. Experimental results confirm that MTM has the superior performance in two applications, document clustering and link prediction, when compared with several state-of-the-art link-based topic models including author-topic models (ATM) [1], relational topic models (RTM) [2] and multirelational topic models (MRTM) [3]. As one of the simplest topic models, latent Dirichlet allocation (LDA) [9] has achieved great successes in text analysis due to its ability in reducing the di-mensionality of text data. Since LDA ignores multiple types of links in multi-plex document networks, there have been many link-based topic models that fall broadly into two categories. The first is to build generative models that generate both content and link such as ATM [1], RTM [2] and topic-link LDA (TLLDA) [4]. ATM and RTM are typical single-link topic models for coauthor and citation links, respectively. Neither of them combines coauthor and citation links together. TLLDA generates citation links by incorporating the coauthor link contribution. But it does not clearly show how these links mutually influ-ence each other. The second focuses on depicting topic influences between doc-uments through links. For example, NetPLSA [10] regularizes the topic labeling configuration with the citation links. Fur thermore, TMBP [11] develops a joint regularization framework to combine the multiplex network structure with topic modeling. MRTM [3] and iTopic model [5] describe multiple dependencies be-tween documents within the Markov random field (MRF) [8] framework. MRTM combines both coauthor and citation links in either the parallel or the cascade manner. iTopic model transforms multiple types of links into one single type, so that it cannot directly deal with multiplex document networks. Note that most previous studies face the same unsolved problem: how to automatically determine the influence of different types of links for a better topic modeling performance? This motivates the MTM model in the following sections. In this section, we will address three qu estions: 1) how to represent the joint probability of MTM by a factor graph, 2) how to perform approximate inference based on the factor graph, and 3) how to learn the weights of different types of links. 3.1 Factor Graph Representation for MTM Probabilistic topic modeling can be interpreted as a labeling problem [7]. LDA [9] assigns a set of thematic topic labels z W  X  D = { z k w,d } to explain the observed non-zero elements in the document-word matrix x W  X  D = { x w,d } , where the notation 1  X  k  X  K is the topic index, the notations 1  X  w  X  W and 1  X  d  X  D are the word index in the vocabulary and the document index in corpus. The topic label satisfies k z k w,d =1and z k w,d = { 0 , 1 } . The non-zero element x w,d =0 denotes the number of word counts at index { w,d } . The document-specific topic proportion  X  = {  X  d ( k ) } generates the topic label z k w,d , and the topic-specific multinomial distribution  X  = {  X  w ( k ) } generates the word token w forming the word counts x w,d . In this paper, the Dirichlet hyperparameters  X , X  in LDA are assumed to be fixed for simplicity [12]. The best topic labeling configuration z  X  is inferred by maximizing the joint probability p ( x , z |  X  ,  X  ;  X , X  )intermsof z .Note that LDA does not consider the multiplex network structure that influences the topic labeling configuration z .

To address this problem, MTM takes the observed multiplex network struc-ture G into account. So, the objective of MTM turns to maximizing the joint depends not only on x but G as well. We assume that x and G are condi-tionally independent with regard to z . According to the Bayes X  rule and LDA assumption [9], this joint probability can be decomposed into three parts: where the first two parts are standard LDA objective function [7], and the third part p ( G | z ) shows that the multiplex document network structure is conditioned only on z for simplicity. Unlike LDA, MTM (1) explains how the content x and the link G influence the topic labeling configuration z .

Using the Multinomial-Dirichlet conjugacy [13], we can integrate out {  X  ,  X  } in (1) and obtain the objective function in the collapsed z space, The first term of (2) is the joint probability of LDA [14], where  X  (  X  ) is the gamma function. The second term of (2) is still the likelihood function of G given the topic labeling configuration z .

The collapsed LDA in (3) can be represented by a factor graph in Fig. 1A without looking at the factors  X  c and  X  a . In the collapsed space { z , X , X  } ,the factors  X  d and  X  w are denoted by squares, and t heir connected variables z w,d are denoted by circles. The factor  X  d connects the neighboring topic labels { z the factor  X  d describes the dependency between z k w,d and z k  X  w,d in the first term  X  ( w x w,d z k w,d +  X  ) of (3). The factor  X  w connects the neighboring topic labels { z the factor  X  w describes the dependency between z k w,d and z k w,  X  d in the second of (3) is a normalization factor in terms of k [7], we do not explicitly connect  X  w with all hidden variables z k for a better illustration. The hyperparameters {  X , X  } can be viewed as pseudo counts having the same layer with hidden variables z [7].
The second part p ( G | z ) of (2) is the likelihood function of the topic labeling configuration under the constraints of multiple links. Without loss of generality, we consider two types of links in this paper: citation and coauthor links, i.e., G = { c,a } . Thus, we can re-write p ( G | z )as citation link c ,and z A d is the topic labeling configuration of all documents written by the coauthors a  X  X  d of the document d . The notations 1  X  c  X  C and 1  X  a  X  A denote citation link and author indices in corpus. Eq. (4) shows that each citation link c is generated by the pairwise topic configurations of two cited documents z c , while each author a is conditioned on the topic configurations from all coauthors z A d in the document d . As a result, Eq. (4) describes the multiplex document network structure in terms of citation and coauthor links. In the next section, we shall design the citation likelihood as a function of similarity between two linked documents, and design the coauthor likelihood as a function of similarity among coauthor topic proportions.

Incorporating (4), we can represent the complete MTM by the factor graph as shown in Fig. 1A. The factors  X  c and  X  a denote the citation and coauthor links, respectively. For example, the factor  X  c connects topic labels z  X  , 1 and z  X  ,d on different word indices if the document pair { 1 ,d } has a citation link, and the factor  X  a connects topic labels z  X  ,d and z  X  ,D on different word indices if the document pair { d,D } has the same author a .FromFig.1A,weseethatMTM assigns a set of thematic topic labels, z = { z k w,d } , to explain the nonzero el-ements in the document-word matrix x = { x w,d } by taking multiple types of links into consideration. The factor  X  d connects topic labels z  X  ,d on different word indices within the same document d , while the factor  X  w connects topic labels z w,  X  on the same word index w but in different documents. Through four document indices except for w and d . Because the factor graph Fig. 1A contains loops, we usually can make only approximate inference for the conditional pos-In the next subsection, we propose a novel MBP algorithm for passing messages over the factor graph Fig. 1A, which maximizes the joint probability (2) in the collapsed space. 3.2 Multiplex Belief Propagation To maximize the objective (2) with hidden variables z ,weoftenusetheitera-tive expectation-maximization (EM) algorithm [8]. Fortunately, the factor graph representation in Fig. 1A facilitates a special EM algorithm called MBP for ap-proximate inference and parameter estimation. There are two important steps in MBP within the EM framework. In the E-step, MBP infers the conditional The message is a K -tuple vector satisfying 0  X   X  w,d ( k )  X  1 , K k =1  X  w,d ( k )=1. In the M-step, MBP uses the messages to update parameters in MTM, including the document-specific topic proportions  X  and topic-specific multinomial param-eters  X  . These two steps repeat for several iterations until convergence.
Fig. 1B shows the message passing process based on the factor graph. Mes-sages are passed from factor nodes to variables, and the message  X  w,d ( k )is and  X   X  a  X  z w,d ( k ). The factors  X  d ,  X  w and  X  a are parameterized functions, and their values can be estimated using EM algorithm. The message  X   X  d  X  z w,d ( k )is from the neighboring words in the same document d by the factor  X  d , implying that different words in the same document tend to have similar topic labels. The message  X   X  w  X  z w,d ( k ) is associated with the messages from the same word in different documents, which indicates that the same word in different documents are likely to be assigned the same topic label. The message  X   X  c  X  z w,d ( k ) receives the messages from all cited documents from the factor node  X  c , which encour-ages citing and cited documents to have similar topics. The message  X   X  a  X  z w,d ( k ) is from the neighboring words written by the same author a , which encourages topic smoothness among topic labels z w,d attached to the author a . Generally, adocument d has multiple coauthors a  X  X  d . So, we sum and pass all messages from coauthors attached to the document d , i.e., a  X  X  lating the influence from all coauthors. Note that the normalization factor in (3) prevents all words from having the same topic label, which forms different the-matic topic groups.

In the E-step, according to the standard sum-product algorithm [6, 8], the message  X  w,d ( k ) is proportional to the product of all incoming messages from factors in Fig. 1B, However, in practice, the direct product o peration cannot balance the messages from different sources. For example, the message  X   X  d  X  z w,d ( k ) is from the neigh-boring words within the same document d , the message  X   X  c  X  z w,d ( k )isfromall cited documents, and the message  X   X  a  X  z w,d ( k ) is from the words in different doc-uments written by the same author a . If we use the product of these three types of messages, we cannot distinguish which one influences more on the topic label z w,d . To quantify and balance different types of messages, we use the weighted sum of the three types of message, we have to determine the weight vector  X  = {  X  d , X  c , X  a } and compute the four messages sending to the variable node z w,d from its connected factor nodes. If  X  c =  X  a = 0, Eq. (6) reduces to the following message update equation, which is the message passing algorithm for LDA [7]. Comparing (6) with (7), we find that MTM is a natural extension of LDA by considering multiplex doc-ument structure G = { c,a } . We shall discuss how to automatically learn the weight vector  X  to achieve the desired topic modeling performance in the next subsection.

Eq. (6) transforms the standard sum-product [6] to the sum-sum message update equation. In practice, such a t ransformation often works because the weighted sum is a widely used method to combine different sources of infor-mation [15]. Eq. (6) also follows our intuition on topic modeling. The message  X  d  X  z w,d ( k ) reflects the content information from the document d , the message  X  c  X  z w,d ( k ) encodes the citation information from cited documents, and the mes-the message  X   X  w  X  z w,d ( k ) is vocabulary word-specific rather than document-specific, it does not need to have a balancing weight as the above document-level messages. Therefore, we still multiply  X   X  w  X  z w,d ( k ) by the weighted messages as the standard sum-product algorithm.

Following [7], the messages from factors {  X  d , X  w } to variables are the normal-ized sum of all incoming messages from the neighboring variables. are the factor functions that encourage or penalize the incoming messages. Based on the topic smoothness prior, we follow [7] and design f  X  d and f  X  w as follows: f d =1 / [ k [  X   X  w,d ( k )+  X  ]], f  X  w =1 / [ w [  X  w,  X  d ( k )+  X  ]].
In Fig. 1A, the citation link c connects a document pair { d,d c } , and the factor  X  connects word topic labels z  X  ,d and z  X  ,d c . We assume that the cited documents are more likely to have similar topics, and use the similarity matrix L ( d,d c )to encourage similar topics. The similarity matrix is built using the standard cosine similarity between two vectors x  X  ,d and x  X  ,d c . The higher similarity L ( d,d c )the more influence of the passed message. Note that L is a pre-computed similarity matrix as one of inputs for the MBP algorithm. The message  X   X  c  X  z w,d ( k ) receives all the incoming message sending to the factor  X  c from the variable node z  X  ,d c , which can be calculated as follows, where the denominator is a normalization function in terms of k .
 Fig. 1A shows that coauthor links can be also encoded into the factor graph. For example, the document pair { d,d a } is connected by the author index a .The factor  X  a connects the documents written by the same author, which in turn indicates that the research expertise of an author could be characterized by the document topics. We assume that documents written by the same author are more correlated than other documents, so we design the factor function f  X  a as follows, where the notation  X  denotes the Hadamard (element-wise) product [2] between two vectors, D a is a set of documents that are associated with author a , | d,d a  X  D a . The Hadamard product captures similarity between topic proportions of the connected documents with the author a . Consequently, Eq. (11) is the average Hadamard product of all pairs of documents connected with the author a ,which encourages that documents written by the same author tend to have similar topic proportions. Based on (11), the message from factor  X  a is calculated as follows, where d a  X  D a \ d denotes all connected documents with the author a except the current document d in Fig. 1A.

In (8),(9),(10) and (12), we have shown how to calculate the four types of messages from factors to variables in Fig. 1B. Then, we update the message  X  w,d ( k ) using (6). Note that we have to estimate the weight vector different sources of incoming messages.

In the M-setup, we estimate the parameters  X  d and  X  w basedontheinferred messages. To estimate parameters  X  and  X  , we follow [7], which are actually the normalized sum of messages flowing to factors  X  d and  X  w . Similar to the document-specific proportion  X  d , we can view the factor  X  a as the author-specific topic proportion, The underlying intuition behind the factor  X  a is that the topic proportion of each author is determined by the average topic proportions of his/her written documents. 3.3 Learning Link Weights Learning link weights is a challenging problem because different links play dif-ferent roles in multiplex document networks. One possible method is to optimize the objective (2) in terms of the weight vector  X  . But this strategy does not work in practice for two main reasons. First,  X  introduces more free parameters in (2) that make the optimization problem complicated. Second,  X  often varies during the topic modeling process to reflect the dynamical change of influence from different types of links. This dynamic property of  X  may make the optimization problem even harder.

Here we propose a consistency-based method to quantify  X  at each learn-ing iteration. Our intuition is that if the messages received by the factor node are more consistent, their influence weight should be also larger. For exam-ple, if two cited papers have quite different topics implying inconsistency, their influence to the citing document will be small. If two coauthored papers have quite similar topics implying consistency, their influence to the current document will be large. As a result, the link weight is proportional to the consistency of messages.

Inspired by [16 X 18], we use the absolute difference of messages at successive learning iterations t  X  1and t to quantify the consistency of messages, The larger accumulated difference me ans that the passed messages are more consistent owing to the fast convergenc e speed. So, we can update the weights  X  of different types of links by (16), (17) and (18). Note that we need to normalize weights to satisfy  X  d +  X  a +  X  c = 1 at each learning iteration. Our experiments confirm that the consistency-based link weights work very well in practice.
Fig. 2 summarizes the MBP algorithm for MTM. For each learning iteration t , the E-step updates the message  X  w,d ( k ), and in the meanwhile estimate the link weight  X  . The M-step estimates the parameters including the document-specific topic proportion  X  d ( k ), the topic-specific distribution  X  w ( k ), and the author-specific topic proportion  X  a ( k ). The computational complexity of MBP is
O ( TKDCA ), where T is the number of learning iterations, K the number of topics, D the number of documents, C the number of citation links, and A the number of authors. Theoretical proof of the convergence of MBP is out of the scope of this paper. Generally, MBP can be viewed as a generalized EM algorithm [8]. By repeatedly running E step and M step, MBP can converge to a local maximum of objective (2) under a fixed number of iterations.
 We compare MTM with some sate-of-the-art link-based topic models such as RTM [2], ATM [1] and MRTM [3] on two publicly available data sets CORA [19] and DBLP 1 . For a fair comparison, the open source codes are implemented using MATLAB/MEX C++ platform [20]. For simplicity, we use the same hyperpa-rameters  X  =50 /K, X  =0 . 01, where K is the number of topics. Using the same T = 500 learning iterations, we compare these topic models in two tasks: document clustering and link prediction.

Table 1 summarizes the statistics of the two data sets, where CL is the number of document categories, D is the number of documents, W is the vocabulary size, C is the total number of citation links and A is the total number of authors. In the following experiments, we randomly divide the entire CORA and DBLP in halves, and choose one half as the training set and the other one as the test set for the link prediction task. 4.1 Document Clustering Topic modeling techniques can be used as dimensionality reduction methods. The reduced document-specific topic proportions could be viewed as a soft clus-tering result. We can directly extract the clusters by assigning the cluster label with the highest topic proportion to each document, i.e., C =arg max k  X  d ( k ). We use two performance metrics to compare the document clustering results: normalized mutual information (NMI) [21] and Q-function [5, 22]. The former compares the predicted cluster labels C with the correct labels manually assigned by experts, and the latter measures the consistency of the clustering results over the network without using pre-existing labels. The value of NMI is in the range [0 , 1]. It is higher if the predicted cluster labels are more consistent with the cor-rect class labels. The value of Q-function lies in the range [  X  1 , 1]. It is positive if the number of edges within groups ex ceeds the expected number on the basis of chance.

Fig. 3 shows the document clustering results of different link-based topic mod-els. In Fig. 3A, on the CORA data set, MTM outperforms ATM and RTM around 23% and 10% in terms of NMI, respectively. Also, on the DBLP data set, MTM outperforms ATM and RTM more than 20% in terms of NMI, respectively. Such a salient improvement has been largely attributed to the ability of MTM in handling multiple types links in document networks. Although MRTM can also handle coauthor and citation links it has a sightly bad overall performance on document clustering. One reason is that MRTM is very sensitive to the hyper-parameters  X  and  X  , and it is difficult to determine the best hyperparameters to achieve the overall good performance. Another reason is that MRTM is unable to balance topic influences from citation and coauthor links. In contrast, MTM dynamically tunes the link weight vector  X  to reflect the truth that different links play different roles in topic modeling. Fig. 3B shows the Q-function on CORA and DBLP. Clearly, MTM achieves twice or three times higher Q-function values than those of RTM, ATM and MRTM, which implies that MTM can yield much more consistent document clustering results. As a summary, in the document clustering application, MTM significantly outperforms several state-of-the-art link-based topic models like RTM, ATM and MRTM. 4.2 Link Prediction In this subsection, we perform two tasks of link prediction: citation link predic-tion and article recommendation. For the first task, we examine all algorithms on CORA and DBLP and compare MTM with RTM, ATM and MRTM. We fol-low the experimental setup in RTM [2] by first fitting the model to the training set with citation links, and then use a logistic regression model to predict the links on the test set. The input to the logistic regression model is the Hadamard product of the topic proportions of each pair of documents. Citation link predic-tion results in terms of F-measure [3] are presented in Fig. 4A. MTM performs better than ATM, RTM and MRTM with 13%, 16% and 5% higher F-measure on CORA. Also, MTM yields 7%, 5% and 22% higher F-measure than ATM, RTM and MRTM on DBLP. The reason why the performance improvement on DBLP is much higher than that on CORA is that DBLP contains more citation links and authors than CORA, which shows the advantage of MTM in handling multiple types of links. Overall, the citation link prediction result demonstrates the effectiveness of MTM for modeling multiplex document networks.

Besides citation link prediction, MTM can address the out-of-matrix predic-tion problem [23] in recommending scientific articles to researchers very well. In this task, we consider how to recommend the newly published articles to the readers. We assume that the author a writes a paper d indicating that the author a likes reading the paper d . Thus, we form the prediction of whether an author a will like a paper d with the inner product between their topic proportions, r a,d =  X  T a  X  d . In our model,  X  a denotes the author-specific topic proportion and  X  d is the document-specific topic proportion. We will present each author with M articles sorted by their predicted ratings r a,d and evaluate based on which of these papers were actually wr itten by the authors. We use the recall @ M [23] metric to evaluate the article recommendation performance. For each author, the definition of recall @ M is The above equation calculates the user-specific recall, and the overall recall for the entire system can be summarized using the average recall from all authors. We only consider the rated articles within the top M articles. A higher recall with lower M indicates a better system. When we use MTM, ATM and MRTM to recommend articles, we set K =50and M  X  X  20 , 40 , 60 ,  X  X  X  , 200 } .Notethat RTM can only process citation links so it cannot do article recommendation task.

Fig. 4B shows the performance of different models for out-of-matrix predic-tion. When compared with ATM and MRTM, MTM works much better for out-of-matrix prediction with 16% and 21% recall enhancement on CORA. Also, MTM has a 43% and 44% higher recall than ATM and MRTM on DBLP, when the number of recommended documents M is small. The reason why MTM is superior to ATM and MRTM is that weighted citation links are incorporated into article recommendation. Although MRTM also uses citation links, it does not quantify and balance the influence of citation and coauthor links accounting for a relatively worse performance. 4.3 Analysis of Link Weights One of major contributions of this work is to automatically estimate weights to balance citation and coauthor link information. Fig. 5A shows the clustering accuracy progresses along with the chang es of the weights of different types of links. Fig. 5B shows how the learned weights change at different iterations and finally converge to the fixed values for the two data sets. We see that the content information has the highest weight in the topic modeling process for the CORA data set, while the citation has the highest weight for the DBLP data set. The major reason lies in that CORA uses the paper abstract but DBLP uses the paper titles as its content. Clearly, the content information of CORA is more important than that of DBLP. Moreover, DBLP has a much more number of citation links than CORA, so that the citation link information may play a major role in DBLP during the topic modeling process. Since the two metrics F-measure and Q-function can capture the characteristic of the citation network, the higher citation link weight for DBLP explains the much better citation link prediction results in Fig. 4A. As we se e in Fig. 4B, the learned weights reflect the multiplex structure of document networks. In this paper, we propose a novel MTM for multiplex document networks. This model has the following advantages. First, MTM naturally represent both cita-tion and coauthor relations using the factor graph, which clearly illustrates the topic labeling dependencies in the multiplex document networks. Furthermore, this factor graph can be extended to represent more types of relational infor-mation of scientific papers such as venue and time. Second, this factor graph facilitates efficient MBP algorithm for approximate inference and parameter es-timation within the EM framework. In the E-step, we infer the posterior topic distribution over each word. In the M-step, we estimate parameters based on the inferred messages. Finally, MBP uses a con sistency-based method to automati-cally learn the link weights that can balance different types of link information during the message passing process. Experimental results on document clustering and link prediction show that MTM achiev es the best performance among several state-of-the-art link-based topic models. Since our model has shown promising results on article recommendation, we are interested in designing more accurate recommendation system by incorporating more meta-data into the proposed MTM based on the factor graph representation.
 Acknowledgements. This work is supported by NSFC (Grant No. 61003154), Natural Science Foundation of the Jiangsu Higher Education Institutions of China (Grant No. 12KJA520004), and a grant from Baidu to JZ, and General Research Fund (HKBU210410) from the Research Grant Council of the Hong Kong Special Administrative Region, China to WKC.
 Information network analysis is an increasingly important direction in data mining in the past decade. Many analytical techniqu es have been developed to explore struc-tures and properties of information networks, among which clustering and ranking are two primary tasks. The clustering task [1] partitions objects into different groups with similar objects gathered and dissimilar objects separated. Spectral method [1,4] is widely used in graph clustering. The ranking task [6,10,12] evaluates the importance of objects based on some ranking function, such as PageRank [12] or MultiRank [10]. Clustering and ranking are often regarded as two independent tasks and they are ap-and ranking makes more sense in many applications [2-3,11]. On one hand, the know-ledge of important objects in a cluster helps to understand this cluster; on the other hand, knowing clusters is benefited to make more elaborate ranking. Some prelimi-nary works have explored this issue [11]. 
Although it is a promising way to do clustering and ranking together, previous ap-proaches confine it to a  X  X ure X  heterogeneous information network which does not consider the homogeneous relations among same-typed objects. For ex ample, RankClus [2] only considers relations between two-typed objects; NetClus [3] just considers relations among center type and attribute types. However, in many applica-tions, the networked data are more complex. They include heterogeneous relations among different-typed objects as well as homogeneous relations among same-typed objects. Taking bibliographic data as an example which is shown in Fig. 1(a), papers, venues, authors and their relations construct a heterogeneous information network. Simultaneously, the network also includes the citation relations among papers and the social network among authors. It is important to cluster on such a hybrid network which includes heterogeneous and homogeneous relations at the same time. The hybr-information from heterogeneous and homogeneous relations is promising to promote the performance of clustering and ranking. 
Although it is important to integrate clustering and ranking on the hybrid network, it is seldom studied due to the following challenges. 1) It is difficult to effectively organize networked data. The hybrid network is more complex than either of them. The way to organize the network not only needs to effectively represent objects and integrate information from heterogeneous and homogeneous relations to improve clustering and ranking performances. It is obvious that more information from differ-ent sources can help to obtain better performances. However, we need to design an effective mechanism to make full use of information from these two networks. 
In this paper, we study the ranking based clustering problem on a hybrid network and propose a novel ComClus algorithm to solve it. A star schema with self loop is applied to organize the hybrid network. The ComClus employs a probability model to represent the generative probability of objects and the experts model and generative method are used to effectively combine the information from heterogeneous and ho-mogeneous relations. Moreover, through applying the probability information of objects, we propose ComRank to identify the importance of objects based on ComClus. Experiments on DBLP show that ComClus achieves better clustering and ranking accuracy compared to well-established algorithms. In addition, ComClus has better stability and quicker convergence. In this section, we give the problem definition and some important concepts used in this paper. ing to the  X  - X  X  type. An information network can be represented as a weighted net-weight mapping from an edge e  X  E to a real number w  X  X   X  . If  X 2 X  the informa-tion network G is heterogeneous information network ; and homogeneous informa-tion networ k when  X 1 X  . For a network with multiple types of nodes, K -partite network [7,9] and star schema among different-typed nodes, without considering the homogeneous relations among same-typed nodes. However, real networked data are more complex hybrid networks where links exist not only in heterogeneous nodes but also in homogeneous nodes. So we propose the star schema with self loop for this kind of networks. Definition 2. Star schema with self loop network. An information network  X  X  X   X   X  X , X , X  on K+1 types of nodes  X  X   X   X   X   X   X  X  X  X  is called star schema with self loop  X   X   X  X  X  X  X  is the links set among the same-typed nodes (called homo-link) and  X   X  X  X  X  X  X  is node. The homo-link is the link between two same-typed nodes, which is denoted as e&lt;  X   X   X  ,  X   X   X  &gt; or e&lt;  X   X   X  ,  X   X   X  &gt;. 
Fig. 1 shows such an example. For a comp lex bibliographical data (see Fig. 1(a)), we can organize it as a hybrid network which includes heterogeneous network among different layers and homogeneous network on the same layer in Fig.1 (b). As shown in Fig. 1(c), the hybrid network can be represented with a star schema with self loop where  X  X aper X  is the center type, while  X  X enue X  and  X  X uthor X  are dependent types. 
Now, we can formulate the problem of clustering on hybrid network. Given a net-work  X  X , X , X  X  X  X  X  ,  X  X   X   X   X   X   X  X  X  X  and the cluster number N, our goal is to find a G,  X   X   X , X  X   X   X  X  and  X  X  X  X   X  X   X   X   X ,  X   X   X  X  X  X   X  . The probability function  X   X   X   X   X   X (  X  X  X  X  = 1. In our solution, we restrict probability function of center node  X  from 0 to 1. After introducing the basic framework of ComClus, this section describes the Com-Clus in detail and then proposes ComRank for estimating the importance of objects. 3.1 The Framework of ComClus The basic idea of ComClus is to determine the memberships of center nodes and then estimate the memberships of dependent nodes by center nodes. We consider that the probability of center node is estimated by two probabilities: homogeneous probability and heterogeneous probability. The homogeneous probability of center node depends on its homo-links. The heterogeneous probability of center node is generated by the dependent nodes that are correlated with it. In order to co-consider the heterogeneous and homogeneous probability for center nodes, generative method and experts model are used to mix these two types information. Finally, we estimate the posterior proba-bility for center node according to the Bayesian rule and reassign the memberships of center nodes. The ComClus will iteratively calculate posterior probability until the memberships do not change. Algorithm 1 shows the basic framework of ComClus. 3.2 Homogeneous Probability for Center Node The homogeneous probability of  X   X   X  depends on its homo-links and denotes as connects to other center nodes on G. This idea is inspired by a general phenomenon  X ( X  X   X   X  ) X | . which will be used to rank (in Sect.3.7 Eq. (11)) and filter the unimportant nodes (in benefit from the homogeneous information. 3.3 Conditional Probability for Dependent Node which can be represented as  X  X  X   X   X   X  X  X  X ) X | X ( X  X  X  X  X  X   X   X   X  X ,  X  X  . The probability of dependent type  X  being selected is  X ) X |  X ( X  | X   X  | we use  X  as a filter factor to expand the  X  X  X  X  X  X  X  X  X  X  (  X   X   X   X | ) gap among ent  X   X   X  . Repeat calculating (4) and (5) until the convergence is obtained. dependent nodes and the  X  X arren X  nodes can be distinguished obviously. Normaliza-tion method can be used when necessary. 3.4 Heterogeneous Probability for Center Node After conditional probability of dependent nodes being figured out, we can estimate that the dependent nodes generate the heterogeneous probability of center node inde-neous probability of center node  X   X   X  can be denoted as  X  ( X   X   X   X |  X  ) X , . 3.5 Mixed Probability for Center Node Until now, we obtain the homogeneous and heterogeneous probability of center node  X   X   X  . Next, the major difficulty in estimating the probability measure is how to jointly consider the homogeneous and heterogeneous distribution of center nodes. To mix the two distributions, we employ two methods: a generative method of center node and a mixture of experts model [5]. parts: the homogeneous and h eterogeneous information of  X   X   X  . The former is on hybrid network G as follows: In experts model, we regard the homogeneous and heterogeneous information of  X   X   X  as  X  X omogeneous expert X  and  X  X eterogeneous expert X . Then we can evaluate mixed probability of center node according to its own distribution. The mixture of experts model is denoted as follows: where  X 2  X  represents the number of experts. If , X 1 X  the homogeneous expert vated as:  X   X  (  X   X   X  |  X  )  X   X  (  X   X   X   X  X   X   X , ) .  X   X   X   X   X   X  X   X  weight of corresponding expert, and we adopt Softmax function to compute it.  X ( X   X  ) is the weight of expert m , which is proportional to the number of heter-links or homo-the following formula:  X  (  X   X  ) =  X  X  X  X  X  X  X  X  X   X  for each  X   X   X  . 
Both methods can evaluate the conditional probability of center node, which can be applied to different scenarios. The generative method equally treats the homogeneous and heterogeneous information, because it simply products homogeneous and hetero-geneous probability. Therefore, the generative method is suitable for the hybrid net-work with the same scales of homogeneous and heterogeneous relations. The mixture of experts model can dynamically adjust the weights of distributions (by  X   X  ). As a result, the method is more suitable for the hybrid network of which the homogenous and heterogeneous parts have different size. 
Besides, to avoid zero probabilities, we smooth the distribution by the following meter. G is the whole hybrid network and  X   X  is the n-th subnet. 3.6 Posterior Probability for Nodes In the previous subsection, we get the conditional probability of center node  X   X   X  by mixing two distributions. Now, we need to calculate the posterior probability purpose of getting the  X ( X   X  ) , the EM algorithm can be used to get the local optimum  X ( X   X  ) by maximizing the log likelihood of center nodes in different areas. We set  X   X  (  X   X  )  X   X   X  X  X  X  . Finally, we will have a  X  dimensional indicator vector  X  membership for each center node with K-means. 
After the iterative process is finished, the posterior probability of dependent node  X |  X  | is the size of set  X   X  . 3.7 Ranking for Nodes As an additional benefit for ComClus, the posterior probabilities of nodes can be used for ranking nodes. Once the cluster process is finished, we can further figure out the rank of nodes in their cluster. We proposed a function (called ComRank) to evaluate the importance of nodes. Taking bibliographic network as an example, the goodness of a paper is decided by the number of citations to a large extent. Another factor of rank function is the post-erior probability, which can be seen as a cluster coefficient and represents the degree of membership in that cluster. The rank of dependent node  X   X   X  can be computed ac-cording to the rank of center nodes connecting with it. In this section, we evaluate the effectiveness of our ComClus algorithm, and compare it with the state-of-the-art methods on two data sets. 4.1 Data Set The DBLP is a dataset of bibliographic information in computer science domain. We use it to build a hybrid network with three-typed nodes: papers (center type), venues (dependent type) and authors (dependent type). Homo-links among authors form a co-author network, and homo-links among papers form a paper citation network. Hete-links are the writing relation between authors and papers and the publication relation between venues and papers. We extract venues from different areas according to the categories of China Computer Federation (http://www.ccf.org.cn). Moreover, CCF provides three levels for ranking venues: A, B, C. The class A is top venues, such as KDD in data mining (DM). The class B is some famous venues such as SDM, ICDM. The class C is admitted venues such as WAIM. In the experiments, we extract two different-scaled subsets of the DBLP which are called DBLP-L and DBLP-S. 
The DBLP-S is a small size dataset and it includes three areas in computer domain: each area, covering three levels), 25,020 papers and 10,907 authors in DBLP-S. Two or three venues for each level are picked out. 
The DBLP-L is a large dataset. There are eight areas included, which are computer (35 venues for each area), 275,649 papers, and 238,673 authors. For each area, five venues are in A level and fifteen venues are selected in B or C level. In these two datasets, venues are labeled with their research areas. Moreover, in DBLP-S, we randomly label 1031 papers and 1295 authors with three research areas, runnings, and average results are shown. 4.2 Clustering Accuracy Comparison Experiments For accuracy evaluation, we apply our method to cluster on both DBLP-S and DBLP-L. We compare ComClus with the representative ranking-based clustering algorithm NetClus which can be applied in heterogeneous networks organized as star schema. The smoothing parameter  X  is fixed at 0.7 in both two algorithms. The filter factor  X  in ComClus is 3. The clustering accuracy of paper is the fraction of nodes identified correctly. For author and venue nodes, the accuracy is the posterior probability frac-tion of nodes identified correctly. Results are shown in Table 1. The two different mixture methods of ComClus both have higher accuracy than NetClus. The lower show that, the additional homogeneous relation utilized by ComClus is helpful for improving its accuracy as well as stability. In addition, ComClus with experts model achieves better perf ormance than ComClus with generative method. We think the reason is that experts model considers the weight of heterogeneous and homogeneous information. In the following experiments, we use ComClus with experts model as the standard version of ComClus. 
Since the hybrid network includes homogeneous network, we compare ComClus with those clustering algorithms on homogeneous network, where a repres entative spectral clustering algorithm Normalize Cut [4] is employed. We design the similarity i . The result is shown in Table 2, which clearly illustrates that ComClus is better than Normalized Cut. ComClus combines the information from homogeneous and hetero-geneous relations. It makes ComClus outperform Normalized Cut which only uses homogeneous network information. 4.3 Ranking Accuracy Comparison Experiment On DBLP-L, we make a ranking accuracy comparison between ComRank and Autho-rithyRank which is a rank method in NetClus[3]. In this application, it is hard to defi-nitely compare the goodness of two venues, whereas we can roughly distinguish their levels. For example, it is difficult to compare the ranking of SDM and ICDM. But we can safely say that SDM and ICDM are on the same level and they are worse than the top level venues (e.g., KDD) and better than the common level venues (e.g., WAIM). Inspired by RankingLoss measure [8], we define LevelRankingLoss to evaluate the as follows. can be seen as one LossPair for  X  X  X  X  X  X  X  X  X   X  . 
We select the top 5 and top 10 venues in different areas and then calculate LRLoss ComRank and NetClus. Results are shown in Fig2. 
The results clearly show that ComRank better ranks these venues, since its LRLoss is lower than that of AuthorityRank on all research areas. We think the additional homoge-neous information utilized by ComRank contributes to its better ranking performance. 4.4 Case Study In this section, we further show the performance of ComRank with a ranking case study. Table 3 sho ws the top 15 venues ranked by ComRank and AuthorityRank on DBLP-S. The results show that the ranks of venues generated by ComRank are all consistent with the recommended level by CCF. However, there are some disordered venues in AuthorityRank, which implies that AuthorityRank is sensitive to the num-ber of papers. That is, AuthorityRank tends to rank a venue publishing many papers with a higher value. For example, AuthorityRank ranks PODS with a low value and DEXA with a relatively high value because PODS published not many papers and DEXA published so many papers. In contrast, ComRank considers the citation infor-mation from homogeneous network. So ComRank avoids these shortcomings. 4.5 Convergence and Stability Experiments For observing the convergence, we compare each cluster probability distribution with global distribution by average KL divergence [3]. Next, we use entropy to measure the unpredictability of cluster and prove the algorithm stability. As shown in Fig. 3(a) and (b), the convergence of our algorithm is faster than Net-Clus. From the results shown in Fig. 3(c), (d) and (e), we can observe that ComClus achieves lower  X  X  X  X  X  X  X  X  . The reason is that ComClus prevents the negative effects of unimportant paper by the factor  X  . Besides, in ComRank, the distribution informa-tion of objects comes from heterogeneous and homogeneous relations. However, the distribution information of objects in NetClus is only from heterogeneous network. More information helps ComClus fast converge and achieve steady solution. In this paper, we proposed a new ranking-based clustering algorithm ComClus on heterogeneous information networks. Different from conventional clustering methods, ComClus can group different-typed objects on a hybrid network which includes the homogeneous network and heterogeneous relations together. Through applying prob-ability information in ComClus, ComClus can also rank the importance of objects. The experiments on real datasets have dem onstrated that our algorithm can generate more accurate cluster and rank with quicker and steadier convergence. Acknowledgments. It is supported by the National Natural Science Foundation of China (No. 60905025, 61074128, 71231002). This work is also supported by the National Basic Research Program of China (2013CB329603) and the Fundamental Research Funds for the Central Universities. With the recent advent of social web services, the data can now be shared and pro-cessed by a large number of users. As a consequence, researchers are faced with data sets that are labeled by multiple users. For example, Wikipedia provides a feedback tool worthy X ,  X  X bjective X ,  X  X omplete X  and  X  X ell-written X . The Amazon Mechanical Turk is an online system that allows the requesters to hire users from all over the world to perform crowdsourcing tasks. Galaxy Zoo is a website where visitors label astronomi-cal images. While providing large amounts of cheap labeled data in a short time, these can vary widely, and in some cases may even be adversarial. A natural question to ask is how to integrate opinions from multiple users for obtaining an objective opinion. The commonly used  X  X ajority vote X  and  X  X ake the average X  heuristics completely ignore the individual expertise and may fail in the settings with non-Gaussian or adversarial noise. This casts a challenge of learning from multiple sources for the machine learning and data mining researchers [2].
 Despite these web applications, one can f nd this problem in wide range of domains. Recently, sensor networks have been deployed for the scientif c monitoring of remote and hostile environments. For example, researchers deployed a 16 -node sensor network on a tree to study its elevation under different weather fronts [9]. Each node samples in this manner presents many novel challenges, such as fusing noisy readings from sev-eral sensors, detecting faulty and aging sensors. Importantly, it is necessary to use the trends and correlations observed in previous data to predict the value of environmental parameters into the future, or to predict the reading of a sensor that is temporarily un-available (e.g. due to network outages). However, these tasks may have to be performed with only limited knowledge of the location, reliability, and accuracy of each sensor.
In this work, the labeler (including user, annotator and sensor) mentioned above is vided by an observer is called the response . Unlike the conventional supervised learn-ing scenario, in our setting each instance is associated with a set of responses, yet the ground truth is unknown as some responses may be subjective or come from unreliable observers. We concentrate on the regression problem with continuous responses from multiple observers. Specificall , our method provides a principled way to answer the following questions: 1. How to learn a regression function to predict the ground truth precluding the prior 2. How to estimate the expertise of each observer without knowing the ground truth? There is a number of studies dealing with the setting involving multiple labelers, yet most of them focus on the classificatio problem. Early work such as [3,4,8] focus on estimating the error rates of observers. In the machine learning community, the prob-lem of estimating the ground truth from multiple noisy labels is addressed in [7]. In-has shifted towards on learning classif ers directly from such data. Authors of [2] pro-vide a general theory of selecting the most informative samples from each source for model training. Later, a probabilistic framework is presented by [5,6] to address the classification regression and ordinal regr ession problem with multiple annotators. The framework is based on a simple assumption that the expertise of each annotator does not depend on the given data. This assumption is infringed in [10,13] and later is extended to the active learning scenario [12]. There are some other related work that focus on different settings [1,11].

The above studies paid little attention to t he regression problem under multiple ob-servers, which is the main core of this paper. Moreover,our work differs fromthe related work in various aspects. First, we employ a less-parametric method, i.e. the Gaussian process (GP), to model the observers and the regression function. This allows us to associate the observer X  X  expertise with both ground truth and input instance. Moreover, our model is presented in an extensible probabilistic framework. The missing data and prior knowledge can be straightforwardly incorporated into the model.

The rest of this paper is organized as follows. Section 3 formulates the problem and introduces a probabilistic framework. The framework consists of two parts. The re-gression model is introduced in Section 3.2. A linear and a non-linear observer model is proposed in Section 3.3 and Section 3.4, respectively. Section 4 reports the exper-imental results on both synthetic and real-world data sets. Conclusions are drawn in Section 5.
 Denote the instance space X X  R L and the response space Y X  R D and the ground truth space Z X  R D .Given N instances x 1 ,..., x N where x n  X  X  , denote the ob-jective ground truth for x n as z n  X  X  . In our setting, the ground truth is unknown. Instead, we have multiple responses y n, 1 ,..., y n,M  X  X  for x n provided by M dif-ferent observers. For compactness, the N  X  L matrix of instance x n,l is represented as X : =[ x 1 ,..., x N ] .The N  X  M  X  D tensor of observers X  responses y n,m,d is denoted z n,d is denoted by Z : =[ z 1 ,..., z N ] .

Given the training data X and Y , our goal is threefold. First, it is of interest to get an estimate of the unknown ground truth Z . The second goal is to learn a regression function f : X X  X  which generalizes well on uns een instances. Finally, for each observer we want to model its expertise as a function of the input instance and the ground truth, i.e. g : X X Z X  X  . 3.1 Probabilistic Framework To formulate this problem from the probabilistic perspective, we consider the training data X and Y as random variables. The ground truth Z is unknown and hence is a latent variable. In general, the observed response Y depends both on the unknown ground truth and the instance. That is, observers may exhibit varying levels of expertise on different instances. On Wikipedia the assumption is particularly true for the novice readers, whereas the rating from an expert r eader is consistent across different types of articles. Figure 1 illustrates the conditional dependence between X , Y and Z with a graphical model. As a consequence, the joint conditional distribution can be expressed as where the term p ( X ) is dropped as we are more interested in the other two conditional distributions. There are two underlying assumptions in this model. First, each dimen-sion of the ground truth is independent, but is not identically distributed. Second, all observers respond independently.

Note that the f rst term in (1) indicates the probabilistic dependence between the ground truth and the input instance, whereas the second term characterizes the ob-servers X  expertise. Previous work have explored different parametric methods to model these two conditional distributions [10,13,5,12,6]. A distinguishing factor in this pa-per is that, we employ the Gaussian process as the backbone to construct the model. Specif cally, the generative process of Y can be interpreted as follows where and  X  is independent identically distributed Gaussian noise, respectively. Note every d . Therefore, our goal can be understood as searching { f d } and { g m,d } given corresponding ground truth should be close in Z through the mapping of { f d } ,which in turn restricts the searching space of { g m,d } when Y is known. 3.2 Regression Model We f rst concentrate on Eq. (2) and represent functions { f d } by the Gaussian process with some non-linear kernel. Specificall , t he conditional distribution of the ground truth given the training instances is assumed to be kernel matrix K d that depends on X , where each element is given by the value of a composite covariance function k d : X X X X  R 0+ , made up of several contributions as follows k covariance function involves an exponentia l of a quadratic term, with the addition of a constant bias, a linear and a noise terms. Fo r each dimension, the parameters need to be learned from the data are  X  1 ,d ,..., X  5 ,d . 3.3 Linear Observer Model To model the observer X  X  expertise, we now concentrate on (3) and assume that { g m,d } is a linear mapping from Z to Y , which does not depend on the instance at all. observer. The second conditional distribution in (1) is assumed to be where 1 is an all-ones vector with length N and I is a N  X  N identity matrix. Each observer is characterized by 3  X  D parameters, i.e. w m,d , X  m,d , X  m,d  X  R . Parameter Estimation. Now we can combine Eq. (6) with Eq. (4) and estimate the variable Z can be marginalized out, which yields The maximum likelihood estimator of  X  m,d is given by - X  m,d = 1 hereinafter use the short-hand y likelihood function is given by where C : = w 2 the partial derivatives of F LOB with respect to the parameters and obtain where B : = C  X  1 y we resort to L-BFGS quasi-Newton method to maximize F LOB . Essentially, in each it-eration the gradients are computed by Eqs. (8) to (10) and the parameters are updated accordingly.
 Estimate of Ground Truth. Note that the ground truth Z is marginalized out from By using the property of Gaussian distribution, one can show that the posterior of z : ,d follows N ( u , V ) ,where The above computation is repeated D times on every dimension to obtain the estimate of ground truth -Z .
 Prediction on New Instance. Given a new instance x  X  , we are interested in predicting the ground truth z  X  by using the learned regression function. This can be derived from the joint distribution a Gaussian distribution. Hence, the best estimate for the ground truth is and the uncertainty is captured in its variance As a consequence, the response from an observer can be also predicted by with variance - X  m,d .
 Priors on Parameters. Note that w m,d is an important indicator of the observer X  X  expertise. On the one hand, a genuine observer would have w m,d close to 1 , whereas an adversarial observer gives w m,d close to  X  1 . On the other hand, we encourage w m,d to be a small value unless supported by the data. Without any knowledge on observers, we can only expect that w m,d takes value either around 1 or  X  1 , which inspires the following penalty function where  X  controls the value of penalty as shown in Fig. 2 (see  X  X eneral X ). When w m,d takes value between [  X  1 , 1] , there is no penalty and the gradient is given by Eq. (8) directly. When | w m,d | &gt; 1 we penalize w m,d and keep it from being too large. This allows our model to search a reasonable solution for w m,d without over-fittin on the training data.

In the case that observers are highly reliable, the learned w m,d should be close to 1 and  X  m,d , X  m,d close to 0 . One can add a Laplacian prior for observers X  parameters, which leads to an L 1 regularization. The penalty term induced by the Laplacian prior for w m,d is  X  ( 1 observer is more reliable. The maximization of F LOB can be carried out by computing the sub-gradient of w m,d , X  m,d and  X  m,d , respectively.
The relationship between observers can be incorporated into the model as well. For example, the demographic information of users or the geographic location of sensors can be represented as a M  X  M proximity matrix P . In particular, we expect two ob-servers have similar parameters if they are highly correlated in P . Assuming P is a illustrates different penalty functions of w m,d .
 Missing Responses. The model can be extended to handle the training data with miss-ing responses. First of all, we partition the responses Y =( Y o , Y u ) ,where Y o rep-the latent variables in our model consists of Z and Y u .The expectation maximization (EM) algorithm can be developed for estimating the model parameters. In the E-step, we fi the model parameter  X  and compute the suff cient statistics of -Z by Eq. (11) and reaches a local maximum. 3.4 Non-linear Observer Model The assumptions behind the linear observer model may not be appropriate in some scenarios. For instance, if the thermistor i s being used to measure the temperature of the environment, due to the self-heating effect the electrical heating may introduce a signif cant error, which is known as a nonlinear function of the actual environment tem-perature. Moreover, the observers X  responses may depend on the input instance. With these considerations in mind, we propose a more sophisticated model which assumes that { g m,d } is a nonlinear mapping from X X Z to Y . By representing { g m,d } as the Gaussian process, the second conditional distribution in (1) has the form of where Y is connected with X and Z by a N  X  N kernel matrix S m,d .The ( i,j ) th element in S m,d is given by form as Eq. (5), but with the addition of an automatic relevance determination kernel on X . By incorporating a separate parameter  X  m,l,d for each input dimension l , we can optimize these parameters to infer the relati ve importance of different dimensions of an instance from the data. One can see that, as  X  m,l,d becomes small, the response y n,m,d substantially affect the observer X  X  response.
 Parameter Estimation. The observer model in Eq. (17) can be combined with Eq. (4) to form our new model, rameters to be inferred from the data. Unf ortunately, such marginalization of Z in-tractable as the latent variable z appears nonlinear in the ker nel matrix. Instead, we seek a maximum a posterior (MAP) solution by maximizing with respect to Z and  X  . Substituting Eq. (17) and Eq. (4) into Eq. (19) gives
F NLOB : =log p ( Z ,  X  | Y , X )=  X  The partial derivative of F NLOB with respect to the latent variable is given by The gradients with respect to the parameters of kernel matrix can be likewise derived as in the linear observer model. Finally, these gradients are used in the L-BFGS algorithm for maximizing F NLOB .

When the algorithm converges, the estimate of ground truth is directly given by the stationary point of F NLOB . Predicting the response of a new instance can be carried out in the same way as in Eq. (11). Moreover, the estimation of the m th observer X  X  response is given by Initialization. Note that seeking the MAP solution of Z and  X  simultaneously may lead to a bad local optimum. Specificall , the model may stuck in a solution where { f non-linear), which contradicts our intuition. To mitigate this problem, we f rst f t the model as the initialization of the ground trut h, and train the nonlinear observer model to further ref ne { f d } and { g m,d } . To evaluate the performance of our algorithm on predicting the ground truth and the ob-is demonstrated on the synthetic data. The s econd experiment is conducted on the real-world data. In both experiments, the ground truth is known and observers X  responses are simulated by mapping the ground truth with some random nonlinear functions. As a consequence, the performance can be evalua ted straightforwardly. Two metrics are considered here, i.e. the mean absolute normalized error (MANE) and the Pearson cor-relation coeff cient (PCC). In MANE, we f rst rescale the actual value and its predicted value into [0 , 1] respectively, and then measure the mean absolute error. MANE value close to 0 and PCC value close to 1 indicate that the algorithm performs well. In partic-ular, the expected MANE of a random predictor is 0 . 5 .

The proposed linear observer model ( LOB ) and nonlinear observer model ( NLOB ) are compared with several baselines. We f rst refer SVR and GPR as the Support Vec-tor Regression and Gaussian Process Regression trained with the ground truth, respec-tively. Then we combine responses from multiple observers by taking the average and then using it for training, which we denote as SVR-AVG and GPR-AVG , respectively. For a fair comparison, the covariance function of x in GPR and GPR-AVG has the same composite form as in Eq. (5). In addition to these non-parametric methods, Raykar refers to the model in which both p ( Z | X ) and p ( Y | Z ) are Gaussian in the spirit of [6].
 4.1 Synthetic Examples To create one-dimensional synthetic data (i.e. L : =1 and D : =1 ), we set f ( x ):= in [0 , 2  X  ] from the uniform distribution. The test instances are obtained using a dis-lated observers are obtaine d by setting the corresponding { g m } as a random nonlinear monotonicfunction.For a training instance x ,the m th observerprovidesits response by g Fig. 3(a). Figure 3(b, c, d, e) shows the results given by the baselines and our method. Not surprisingly, taking the average of observers X  responses is not an effective solu-tion. In contrast, our LOB and NLOB models outperform baseline methods signif cantly, which yield lower MANE and higher PCC. Moreover, the observers X  functions learned by NLOB are very close to those predefine { g m } in Fig. 3(a). 4.2 On Real-World Data We download four real-world data sets from UCI Machine Learning Repository,namely AUTO , COMMUNITY , CONCRETE and WINE . On each data set, we randomly select 500 instances and generate 20 observers in the same manner as in Section 4.1. The number of adversarial observers is fi ed to 6 . The experiment is conducted with 10 -fold cross-validation. The prediction result of the ground truth and observers X  X esponses is summa-rized in Table 1. It is notable that the proposed LOB and NLOB signif cantly outperform SVR/GPR-AVG and Raykar on inferring the ground truth. In general, additional im-provementsare observed when NLOB is used. Comparing it with the SVR/GPR column, one can see that the regression function learned by NLOB is almost as good as the one trained using the ground truth. We remark that the promising performance of NLOB is achieved by merely learning from a set of observers without any prior knowledge of their expertise and the ground truth. Furthermore, LOB and NLOB also show encourag-ing performance on predicting responses of observers, which can be proved useful in many applications such as the recommendation system.
 This paper investigates the regression pr oblem under multiple observers providing re-sponses that are not absolutely accurate. The problem involves learning a regression function and observers X  expertise from such data without any prior information of the observers. Based on the Gaussian process, we propose a probabilistic framework and develop two models. Our approach provides an estimate of the ground truth and also predicts the responses of each observer given new instances. Experiments show that the proposed method outperforms several baselines and leads to a performance close to the model trained with the ground truth.

There are many opportunities for future res earch. One possible direction is to extend our model with multiple kernel learning . The idea is to let the algorithm pick or com-posite different covariance functions instead of f xing the combination in advance. As a consequence, the algorithm may learn complex f ts for the observers by selecting mul-tiple kernels in a data-dependent way. Moreover, it would be highly benef cial to design active sampling methods for selecting which instance and whose response should be learned next.

