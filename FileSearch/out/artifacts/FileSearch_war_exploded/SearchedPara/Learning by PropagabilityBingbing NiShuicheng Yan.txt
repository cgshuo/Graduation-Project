
In this paper, we present a novel feature extraction framework, called learning by propagability . The whole learning process is driven by the philosophy that the data labels and optimal feature representation can constitute a harmonic system, namely, the data labels are invari-ant with respect to the propagation on the similarity-graph constructed by the optimal feature representation. Based on this philosophy, a unified formulation for learning by propagability is proposed for both supervised and semi-supervised configurations. Specifically, this formulation of-fers the semi-supervised learning two characteristics: 1) unlike conventional semi-supervised learning algorithms which mostly include at least two parameters, this formu-lation is parameter-free; and 2) the formulation unifies the label propagation and optimal representation pursuing, and thus the label propagation is enhanced by benefiting from the graph constructed with the derived optimal representa-tion instead of the original representation. Extensive ex-periments on UCI toy data, handwritten digit recognition, and face recognition all validate the effectiveness of our proposed learning framework compared with the state-of-the-art methods for feature extraction and semi-supervised learning.
Learning tasks (e.g., classification and regression) in computer vision research often encounter two interactional issues, namely high feature dimensionality and insufficient training data. Thus it is often necessary to transform the high dimensional d ata into a low dimensional feature space. Many algorithms have been proposed for this purpose [11], and among them, Principal Component Analysis (PCA) [7] and Linear Discriminant Analysis (LDA) [1] are the two most popular ones.

Without class information, PCA simply attempts to project the high dimensional data into a low dimension lin-ear subspace according to the criterion that the preserved dimension captures the most of the variability of the origi-nal data. On the other hand, if the class label information is provided, LDA aims to maximizing the inter-class scat-ter and at the same time minimizing the intra-class scatter. Unlike PCA and LDA whose objective functions are not di-rectly related with the final classification performances, the recently proposed algorithm, called Neighborhood Compo-nent Analysis (NCA) [5], was designed to directly optimize the expected leave-one-out cla ssification error on the train-ing data.

However, the second issue of insufficient training data often degenerates the perfo rmances of these algorithms. Despite of the insufficient labeled training samples, sam-ples without labels are relatively much easier to obtain. Known as semi-supervised learning, a variety of algorithms have been proposed to improve the algorithmic learnabil-ity by using additional unlabeled training data. Most al-gorithms of this category are based on graph [2], e.g., LapSVM and LapRLS [3], and local and global consistency [12], etc. Although these algorithms have proved to be ef-fective and achieved great success in many applications, much more research is still demanded for semi-supervised learning owing to the following observations: 1) almost all these algorithms are based on graphs defined on the orig-inal high dimensi onal feature space, which is unnecessary to be best for characterizing the similarities of the sam-ple pairs, since unfavorable features and noise may exist within the original representation; 2) most previous semi-supervised algorithms were directly designed for two-class classification problems, and e xtra process is required for handling multi-class classification problems; 3) most semi-supervised learning algorithms result in optimization prob-lems with extra parameters whi ch lack elegant ways for au-tomatic setting; and 4) the objective function does not di-rectly target the algorithmic discriminating power, instead is often the trade-off between the algorithmic discriminat-ing power and the label smoothness.

Recently, there existed one attempt to tackle the sec-ond issue as above-mentioned, an algorithm, called Semi-supervised Discriminant Analysis (SDA) [4], was proposed to learn a linear subspace by taking the graph constraints into consideration. However, SDA is still based on the graph defined on the original high dimensional feature space, which also has extra parameters to determine.
In this paper, we present a unified parameter-free learn-ing framework for both supervised and semi-supervised learning configurations, where the above four issues are re-solved within in the scenario of semi-supervised configura-tion. The whole framework is based on the philosophy that the class labels and the optimal feature representation can construct a harmonic system, namely the class labels are invariant with respect to the propagation on the similarity-graph defined on the optimal representation. For supervised learning tasks, the pursuing of the optimal feature trans-formation matrix targets to build such a harmonic system, while for semi-supervised configurations, it achieves the propagation of the labels from the labeled data to the un-labeled data in the same time. Based on this unified frame-work, an iterative procedure is presented for seeking the so-lution.

Here we would like to highlight some aspects of our pro-posed unified learning framework of Learning by Propaga-bility (FLP): 1. For label propagation, the graph is finally constructed 2. FLP is parameter-free and hence the results are more 3. Unlike most previous semi-supervised algorithms 4. The objective of FLP still characterizes the discrim-
This paper is organized as follows. Section 2 introduces the learning by Propagability framework followed by the it-erative procedure for seekin g the solution in Section 3. Sec-tion 4 presents the extensive experimental results. Section 5 concludes this paper followed by the discussion of future work.
For notational consistency, in this paper, we use lower case alphabets to represent scalars, lower case bold alpha-bets to represent vectors, upper case alphabets to represent matrix and upper case bold alphabets to represent data sets.
For a classification problem, we assume that the train-ing sample data (both labeled and unlabeled) are given as X =[ X l , X u ]=[ x 1 ,..., x N 1 , x N 1 +1 ,..., x N ] ,where x and N 1 is the number of labeled training samples. We use  X  =[  X  l ,  X  u ]=[  X  1 ,  X  X  X  ,  X  N 1 ,  X   X  N 1 +1 ,  X  X  X  to represent the corresponding label/class probability dis-tribution information, namely, for labeled data X l = [ x 1 ,..., x N 1 ] , each corresponding K dimensional vec-tor  X  i =[  X  1 i ,..., X  k i ,..., X  K i ] T ,i =1 ,...,N 1 a 1 -of-K representation in which a particular element  X  i is equal to 1 and all other elements are equal to 0 , indicating its class label; for unlabeled data X u = [ x
N 1 +1 ,..., x N ] , each corresponding K dimensional vec-tor  X   X  i =[  X   X  1 i ,...,  X   X  k i ,...,  X   X  K i ] T ,i = N sents its probability distribution of belonging to each class, with all its elements taking some real value in the inter-val [0 , 1] , which are unknown and need to be updated by some learning algorithm. Ther efore, for both labeled and unlabeled data, the sum-to-1 constraints are always satis-fied, i.e., K k =1  X  k i =1 ,  X  i =1 ,...,N 1 and K k =1  X  1 ,  X  i = N 1 +1 ,...,N . Note for notational convenience, we will ignore the notational difference between all the  X  and  X   X  in the rest of the paper, i.e.  X  i =  X   X  i ,i = N 1 +1 ,...,N . Since in practice the feature dimension m is often very large and unfavorable features may exist for certain task, it is usu-ally necessary to transform the original high-dimensional data into a low-dimensional feature space for facilitating and enhancing subsequent process.
Dimensionality reduction under supervised or semi-supervised learning configurationshas attracted much atten-tion in the past decade. Our wor k in this paper is motivated from the following three aspects. 2.1.1 Harmoniousness between Label and Representa-For supervised learning problems, the harmoniousness be-tween label and representation is easy to characterize, e.g., Linear Discriminant Analysis (LDA) [1] searches for a low-dimensional representation that minimizes the intra-class scatter and at the same time max imizes the inter-class scat-ter. However, for semi-supervised learning with unlabeled data, the criterion based on class labels is not applicable in modeling the harmoniousness between the class label and the representation any more. Instead, in the literature many semi-supervised learning algorithms without feature extrac-tion, namely with only label propagation, are proposed for utilizing these unlabeled data. These algorithms essentially can be considered as special classification methods instead of learning processes. These semi-supervised learning al-gorithm are mostly based on graphs built from the original high-dimensional features, which may include unfavorable features and noises. Intuitively the semi-supervised learn-ing may benefit from the feature extraction process which may remove these unfavorable features and noises.
In this work, we aim to conduct label propagation and feature extraction simultaneously. More specifically speak-ing, a low-dimensional feature space and the class labels for the unlabeled data are learned within a unified formulation, and the ultimate target is that the class labels are invariant after the propagation on the graph constructed on the de-rived optimal low-dimensional feature space. 2.1.2 Parameter-free Semi-supervised Learning: Semi-supervised learning has proved to be effective in uti-lizing the unlabeled data for enhancing the algorithmic per-formance. However, a common issue suffered by most pre-vious semi-supervised learning algorithms is that they have extra parameters within the formulation, and these param-eters may greatly affect the fi nal algorithmic performance. Moreover, the optimal parameters for different algorithms and different data sets may be different. It is desirable to de-velop a parameter-free semi-supervised learning algorithm for the purpose of robust systems.

In this work, the unlabeled data are used to propagate expected label information to the labeled data, such that the prediction confidence for the labeled data is maximized, and the derived formulation is parameter-free. 2.1.3 Consistency between Feature Extraction and Dimensionality reduction or feature extraction has been widely studied in computer vision literature, but most al-gorithms are based on certain intuitive motivations, which do not directly maximize the consequent classification ac-curacy. As a consequence, the feature extraction and fi-nal classification are separated as two independent steps, and thus the overall optimum cannot be guaranteed. All unsupervised learning algorithms belong to this category. Among the few exceptions, Neighborhood Component Analysis (NCA) [5] learns a Mahalanobis distance mea-sure to be used in the KNN classification algorithm, and the derived transformation is optimal in the sense of soft k -nearest-neighbor classification. However, NCA is designed for supervised learning, and cannot be directly used for semi-supervised learning.

In this work, we aim to conduct feature extraction which is directly based on the classification criteria used in the classification stage and appli cable for both supervised and semi-supervised configurations.
In this subsection, we introduce our solution to feature extraction based on graph propagability. The task of feature extraction is to find a matrix A =[ A 1 , A 2 , ..., A d ] to transform the original high-dimensional data x into a low-dimensional form y  X  R d (usually d m )as 2.2.1 Similarity Graph: Based on the low-dimensional representation transformed by matrix A , a similarity graph G = {  X  , W } with vertices as label set  X  and edge weights as matrix W can be defined as 2.2.2 Propagation on Graph: The underlying philosophy of classification tasks is to pre-dict the class label of a new datum based on the label obser-vations of other data, namely the label of the new datum is propagated from other observed data. The weight W ij mea-sures the relative similarity between different sample pairs. Intuitively, for the datum x i , the larger the weight W ij the more contribution the label of sample x j offers to the prediction of the label for sample x i . Since the sum of the predicted class probability vector of the sample x i should be 1 , the propagation coefficients are normalized as p p ii =0 . (3) Then, we have j p ij =1 ,  X  i . to p 5 . After label propagation, the uncertain labels (  X  1 ,  X  X  X  ,  X  4 ) are updated in (c) via graph harmo-niousness.

Here we denote p i =[ p 1 i ,...,p k i ,...,p K i ] T ,i = 1 ,...,N as the propagated class probability vector for sam-ple x i ,thenwehave Note that the propagated label satisfies the conditions 1) p on the class label propagation on the normalized similarity graph. 2.2.3 Objective Function: Before formally describing the objective function for learn-ing by propagability, we first define a concept called graph harmoniousness .

Definition For a graph G = {  X  , W } , the graph harmo-niousness is defined as H ( G )= 1 2 N 1 i =1 p i  X   X  i 2 p i is defined in Eqn. (4).

The graph harmoniousness characterizes the label propagability on the graph, which on the other hand mea-sures the classification capability of the derived feature rep-resentation based on the soft nearest neighbor method. Note that the supervised configuration can be considered as a special case of semi-supervised configuration by setting N 1 = N , namely all the samples are with labels. For sim-plicity, we only present the formulation for semi-supervised learning afterwards.

For semi-supervised learning, we propose to simultane-ously learn the transformation matrix A and the undeter-mined labels  X  u =[  X  N 1 +1 ,  X  X  X  ,  X  N ] by seeking the best harmoniousness, namely,
We shall note that 1) we do not include the terms on the unlabeled data in the objective function regarding that the number of the unlabeled data is much larger than the la-beled data and therefore the term on the unlabeled data will dominant the objective function, which in turn will degrade the algorithmic performance. 2) We do not use the cross-entropy as in [5] since some  X  k j on the unlabeled data can be or nearly be zero , which makes the numerical computation unstable. We also would like to highlight some aspects of this formulation: 1) this formulation is for multi-class clas-sification problems, not only for two-class problems, and there exist two constraints for the class labels; and 2) it in-tegrates feature extraction and label propagation within a unified formulation.
The objective function H ( G ) is nonlinear and the opti-mization problem is constrained, and hence there does not exist a closed-form solution. Naturally, we present a proce-dure to optimize the transformation matrix A and the unde-termined labels  X  u iteratively. 2.3.1 Optimize A for given  X  u : Assume that the values of  X  u are known, the optimization problem defined in (5) is casted as:
It is an unconstrained optimization problem with non-convex objective function, any gradient descent based opti-mization method could converge to a local minimum. The gradient of the cost function is calculated as,  X  X   X  X  where x ij = x i  X  x j . 2.3.2 Optimize  X  u for given A : When the transformation matrix A is fixed, the optimization problem defined in (5) is a constrained optimization prob-lem. To make it more computationally tractable, we convert this constrained optimization problem into an unconstrained one by setting and then the constraints defined in (5) are naturally satis-fied. Similarly, there does not exist closed-form solution, and we utilize the gradient descent method to obtain a local optimum. The gradient with respect to the new variable c k is
The above two steps are iteratively conducted to obtain the stepwise result ( A t ,  X  u t ) until satisfying the following stop criteria, where  X  is a manually defined threshold and empirically set to be 10  X  6 in this work.
In this section, we discuss two related works, and then analyze the algorithmic convergency property.
Zhu et al. [13] proposes an approach for semi-supervised learning based on a Gaussian random field model defined with respect to a weighted grap h representing labeled and unlabeled data. Our work is different from this work in many aspects: 1) Zhu X  X  work propagates the labels based on the graph defined on the original feature representation, while our work seeks the best feature representation and la-bels for unlabeled data simultaneously; 2) Zhu X  X  work was designed for two-class classification problems, while our work is directly designed for multi-class classification prob-lems; and 3) Zhu X  X  work is applicable for semi-supervised learning, while our work is general and applicable for both semi-supervised and supervised learning configurations. In our experiment part, we shall further show the advantages of simultaneous feature extraction and label propagation.
To the best of our knowledge, Cai et al. [4] presented the first work for simultaneous feature extraction and semi-supervised learning (SDA). Our work has the advantages over SDA in the following aspects: 1) Similar to Zhu X  X  work, Cai X  X  work is also based on the graph defined on the original feature representation, and hence may suffer from the unfavorable features and noises; and 2) Cai X  X  work is based on the LDA algorithm, the effectiveness of which is limited by the strong assumption that the samples of each class follow some Gaussian distribution and the covariance matrices for different classes are the same; while our algo-rithm does not have such assumptions and hence is more general.
 There also exists some recent extension on NCA by Salakhutdinov and Hinton [9], however, our work is sub-stantially different from theirs, i.e., [9] mainly focuses on the nonlinear mapping function learning under the super-vised configuration, and an extra regularization parameter is used for extension to semi-supervised learning. How-ever, our proposed method could learn a linear transforma-tion matrix as well as label propagation in a unified graph harmoniousness framework without any parameters.
The optimization problem in Eqn. (5) is non-convex due to the non-convexity of the objective function, and hence we cannot guarantee that the solution will be globally opti-mal. Here, instead we prove that the iterative procedure will converge to a local optimum. Denote the objective function
H ( A t ,  X  u t )  X  H ( A t +1 ,  X  u t )  X  H ( A t +1 ,  X  Therefore, the objective function is non-increasing, and we have H ( A,  X  u )  X  0 , which means that the objective func-tion has a lower-bound. Then we can conclude that the ob-jective function will converge to a local optimum according to  X  X auchy X  X  criterion for convergence X  [8].
In this section, we systematically evaluate the effective-ness of our proposed framework of learning by propagabil-ity (FLP) under the semi-supervised configuration. First, we examine the algorithmic properties, including algorith-mic convergency,advantage of the integration of label prop-agation and feature extraction, and parameter sensitive-ness. Then we give an extensive comparison of FLP with the state-of-the-art feature extraction and semi-supervised learning algorithms including Linear Discriminant Analysis (LDA) [1], Neighborhood Component Analysis (NCA) [5], and Semi-supervised Discriminant Analysis (SDA) [4], on three UCI toy data sets 1 , the USPS handwritten digit dataset [6], and the CMU PIE [10] face dataset. Note that the super-vised version of FLP is very similar to NCA although their objective functions are slightly different in this case, and our offline experiments show their performancesare similar, therefore we only report the results of NCA in this work.
Thedatasetsusedinoure xperiments are summarized as follows.  X  UCI iris dataset: It includes 3 classes and 150 samples  X  UCI wine dataset. It includes 3 classes and 178 sam- X  UCI yeast dataset. It includes 8 classes and 1459 sam- X  USPS handwritten digit dataset: It includes 10 classes  X  CMU PIE face dataset: The original database includes
If it is not otherwise specified, in each run, for the iris, wine, yeast, and handwritten digit, the data set is divided into three subsets. Two samples of each class are randomly selected as the labeled set, and other 20 samples of each class are randomly selected as the semi-set, and then all the remaining data are used as the testing set. For the face database, following the configuration in [4] for SDA eval-uation, we use 1 sample each subject as labeled set, and hence only semi-supervised algorithms SDA and FLP can used for the experiments. We use 20 images each subject as the semi-set for computational efficiency, and the rest im-ages are used for testing.

In the classification stage, the nearest neighborhoodclas-sifier is used to evaluate the recognition accuracies of LDA and SDA algorithms for both the semi-set and the testing set. NCA and our proposed FLP directly use their own probabilistic ways for final classification. The offline exper-iments show that after feature e xtraction, the performances of NCA (or FLP) with nearest neighbor and probabilistic way are similar, and hence in this work we do not report the results from nearest neighbor method for them separately.
In this subsection, we examine some properties of our proposed FLP and SDA: 1) convergencyproperty, 2) advan-tage of integrating label propagation and feature extraction, and 3) parameter sensitivity.
UCI wine dataset and the USPS handwritten digit dataset. 4.2.1 Algorithmic Convergency: As proved in the previous section, the iterative procedure guarantees a local optimum solution for our objective func-tion in Eqn. (5). In Figure 2 we show how the objective function value decreases with increasing iterations on the UCI wine dataset and USPS handwritten digit dataset. Our offline experiments show that generally FLP convergesafter about 6 -10 iterations. 4.2.2 Advantages of Integrating Label Propagation Most previous semi-supervised learning algorithms only consider label propagation, and the graph is built directly based on the original feature representation. SDA is a semi-supervised algorithm for feature extraction, but it does not directly consider label propagation. In this subsec-tion, we show the necessity to integrate the label propaga-tion and feature extraction for better semi-supervised learn-ing. More specifically, we compare our proposed semi-supervised FLP algorithm under the scenarios with feature extraction and without feature extraction (namely, directly set A = I ). This means that the labels can only be propa-gated on the original similarity graph by simply optimizing Eqn. 5 with respect to  X  u . The comparison experiments are conducted on the UCI iris dataset and USPS handwrit-ten digit dataset. Figure 3 shows the recognition accuracy of FLP with different reduced feature dimensions, and the results on both datasets indicate that the integration of la-bel propagation and feature extraction can boost the perfor-mance compared with the counterpart without feature ex-traction. 4.2.3 Sensitivity to Parameter for SDA: Figure 4 shows the classification accuracies of SDA on USPS handwritten digit dataset and the CMU PIE face dataset with respect to different values of the parameter  X  , which balances the term on dis criminating power and an-other term for label smoothness in SDA [4]. From the com-parison, we can observe that the recognition accuracy can be significantly affected by the selection of different values of  X  , moveover, the trends of the recognition accuracy on different datasets with repect to  X  can be totally different. It indicates that SDA is sensitive to the selection of this pa-rameter. As a parameter-free approach, our proposed FLP algorithm however has no such problems.
The detailed comparisons on recognition accuracies of different algorithms over diff erent datasets are summarized in Table 1. For each dataset and each algorithm, we ex-plore all possible dimensions for the transformation matri-ces, and then report the best result. For each configuration, we randomly generate 10 splits/runs, and the reported re-sults are given in terms of means and standard deviations, which is of sufficient statistical importance. For SDA, dif-ferent values for parameters as in Figure 4 are tested and the best result is reported. For FLP, we randomly ini-tialize the transformation matrix A and unknown variables c ,i = N 1 +1 ,...,N,k =1 ,...,K for each run. The de-tailed recognition accuracies over different feature dimen-sions for all the evaluated algorithms are displayed in Fig-ure 5, where the results are obtained from the UCI wine dataset and the CMU PIE face dataset.

From the results in Table 1 and Figure 5, we can have a set of interesting observations: 1. For the supervised algorithms, NCA algorithm shows 2. From the comparison of LDA and SDA, we can con-3. Our proposed semi-supervised learning algorithm FLP 4. In this work, we do not compare our algorithm with
An additional experiment is performed to demonstrate the performances of our algorithm under different unla-beled:labeled partitions for the training data. For each 10 runs, and the right one is the standard deviation. constraints. dataset, we use 20 samples for training (including both la-beled and unlabeled data) and the rest for testing. We vary the ratio between the number of l abeled and unlabeled train-ing data (e.g., 2  X  18 means 2 labeled samples and 18 unla-beled samples from each cla ss) and for each configuration, we randomly generate 10 splits. We report the classification results in terms of both means and standard deviations and compare the results from all the learning algorithms. Note we report the best result for SDA by varying its parameters as before. Fig. 6 shows the results on the UCI wine dataset and the USPS handwritten digit dataset (Note we do not show the results from all datasets due to the space limita-tion, however, the un-reported results are similar). We could observe that our proposed FLP algorithm consistently per-forms best under all different training data configurations, i.e., highest recognition rates with low deviations, which validate the robustness of FLP in terms of different training data configurations. Note that the result of NCA approaches FLP in the 10  X  10 setting, where the number of labeled training data is relatively high, and extra benefit from unla-beled data vanishes.
In this paper, we have proposed a criteria to measure the optimality of a graph with class probability vectors as vertices and similarity graph constructed from the pursu-ing of the optimal feature representation. Then, a gen-eral learning philosophy of l earning by propa gability was presented for both supervised and semi-supervised config-the standard deviations are represented by vertical bars. urations. Simultaneous feature extraction and label prop-agation brought great improvement in classification accu-racy compared with the state-of-the-art algorithm for semi-supervised learning tasks. We are planning to further ex-ploit learning by propagability in three aspects: 1) to study learning by propagation under unsupervised learning con-figuration, namely, clustering with feature extraction; 2) to accelerate the computational s peed by taking advantages of novel nonlinear optimization toolbox, since the current ver-sion of algorithm still suffers from the heavy computational cost; and 3) to study learni ng by propaga bility under the scenarios with uncertain labels.

This work was supported by MDA grant of R-705-000-018-279, Singapore.
 [1] P. Belhumeur, J. Hespanda, and D. Kiregeman. Eigen-[2] M. Belkin, I. Matveeva, and P. Niyogi. Regularization [3] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold reg-[4] D. Cai, X. He, and J. Han. Semi-Supervised Dis-[5] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhut-[6] J. Hull. A Database for Handwritten Text Recognition [7] I. Joliffe. Principal component analysis. Springer-[8] W. Rudin. Principles of Mathematical Analysis, 3nd [9] R. Salakhutdinov and G. Hinton. Learning a nonlinear [10] T. Sim, S. Baker, and M. Bsat. The CMU pose, il-[11] J. Yan, B. Zhang, N. Liu, S. Yan, Q. Cheng, W. Fan, [12] D. Zhou, O. Bousquet, T. Lal, J. Weston, and B. [13] X. Zhu, Z. Ghahramani and J. Lafferty. Semi-
