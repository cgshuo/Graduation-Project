 Mladen Kolar mladenk@cs.cmu.edu James Sharpnack jsharpna@cs.cmu.edu High-dimensional regression models have been stud-ied extensively in both machine learning and statisti-cal literature. Statistical inference in high-dimensions, where the sample size n is smaller than the ambient dimension p , is impossible without assumptions. As the concept of parsimony is important in many scien-tific domains, most of the research in the area of high-dimensional statistical inference is done under the as-sumption that the underlying model is sparse, in the sense that the number of relevant parameters is much smaller than p , or that it can be well approximated by a sparse model.
 Penalization of the empirical loss by the ` 1 norm has become a popular tool for obtaining sparse models and huge amount of literature exists on theoretical prop-erties of estimation procedures (see,e.g., Zhao &amp; Yu, 2006; Wainwright, 2009; Zhang, 2009; Zhang &amp; Huang, 2008, and references therein) and on efficient algo-rithms that numerically find estimates (see Bach et al., 2011, for an extensive literature review). Due to limi-tations of the ` 1 norm penalization, high-dimensional inference methods based on the class of concave penal-ties have been proposed that have better theoretical and numerical properties (see,e.g., Fan &amp; Li, 2001; Fan &amp; Lv, 2009; Lv &amp; Fan, 2009; Zhang &amp; Zhang, 2011). In all of the above cited work, the main focus is on model selection and mean parameter estimation. Only few papers deal with estimation of the variance in high-dimensions (Sun &amp; Zhang, 2011; Fan et al., 2012) al-though it is a fundamental problem in statistics. Vari-ance appears in the confidence bounds on estimated regression coefficients and is important for variable se-lection as it appears in Akaike X  X  information criterion (AIC) and the Bayesian information criterion (BIC). Furthermore, it provides confidence on the predictive performance of a forecaster.
 In applied regression it is often the case that the er-ror variance is non-constant. Although the assumption of a constant variance can sometimes be achieved by transforming the dependent variable, e.g., by using a Box-Cox transformation, in many cases transforma-tion does not produce a constant error variance (Car-roll &amp; Ruppert, 1988). Another approach is to ignore the heterogeneous variance and use standard estima-tion techniques, but such estimators are less efficient. Aside from its use in reweighting schemes, estimat-ing variance is important because the resulting pre-diction intervals become more accurate and it is often important to explore which input variables drive the variance. In this paper, we will model the variance directly as a parametric function of the explanatory variables.
 Heteroscedastic regression models are used in a vari-ety of fields ranging from biostatistics to economet-rics, finance and quality control in manufacturing. In this paper, we study penalized estimation in high-dimensional heteroscedastic linear regression models, where the mean and the log variance are modeled as a linear combination of explanatory variables. Mod-eling the log variance as a linear combination of the explanatory variables is a common choice as it guar-antees positivity and is also capable of capturing vari-ance that may vary over several orders of magnitudes (Carroll &amp; Ruppert, 1988; Harvey, 1976). Main contri-butions of this paper are as follows. First, we propose HIPPO (Heteroscedastic Iterative Penalized Pseudo-likelihood Optimizer) for estimation of both the mean and variance parameters. Second, we establish the or-acle property (in the sense of Fan &amp; Lv (2009)) for the estimated mean and variance parameters. Finally, we demonstrate numerical properties of the proposed procedure on a simulation study, where it is shown that HIPPO outperforms other methods, and analyze a real data set. 1.1. Problem Setup and Notation Consider the usual heteroscedastic linear model, where X = ( x 1 ,..., x n ) 0 = ( X 1 ,..., X p ) is an n  X  p matrix of predictors with i.i.d. rows x 1 ,..., x n , y = ( y 1 ,...,y n ) is an n -vector of responses, the vectors  X   X  R p and  X   X  R p are p -vectors of mean and vari-ance parameters, respectively, and = ( 1 ,... n ) is an n -vector of i.i.d. random noise with mean 0 and variance 1. We assume that the noise is indepen-dent of the predictors X . The function  X  ( x ,  X  ) has a known parametric form and, for simplicity of pre-sentation, we assume that it takes a particular form  X  ( x i ,  X  ) = exp( x 0 i  X  / 2).
 Throughout the paper we use [ n ] to denote the set { 1 ,...,n } . For any index set S  X  [ p ], we denote  X  S to be the subvector containing the components of the vector  X  indexed by the set S , and X S denotes the submatrix containing the columns of X indexed by S . For a vector a  X  R n , we denote supp( a ) = { j : a j 6 = 0 } the support set, || a || q , q  X  (0 ,  X  ), the ` we denote || X || = || X || 2 the ` 2 norm. For a matrix A  X  R n  X  p we denote ||| A ||| 2 the operator norm, || A || F the Frobenius norm, and  X  min ( A ) and  X  max ( A ) denote the smallest and largest eigenvalue respectively. Under the model in (1), we are interested in estimating both  X  and  X  . In high-dimensions, when p n , it is common to assume that the support  X  is small, that is, S = supp(  X  ) and | S | n . Similarly, we assume that the support T = supp(  X  ) is small. 1.2. Related Work Consider the model (1) with constant variance, i.e.,  X  ( x ,  X  )  X   X  0 . Most of the existing high-dimensional literature is focused on estimation of the mean param-eter  X  in this homoscedastic regression model. Under a variety of assumptions and regularity conditions, any penalized estimation procedure mentioned in introduc-tion can, in theory, select the correct sparse model with probability tending to 1. Literature on variance esti-mation is not as developed. Fan et al. (2012) proposed a two step procedure for estimation of the unknown variance  X  0 , while (Sun &amp; Zhang, 2011) proposed an estimation procedure that jointly estimates the model and the variance.
 Problem of estimation in the heteroscedastic linear re-gression models have been studied extensively in the classical setting with p fixed, however, the problem of estimation under the model (1) when p n has not been adequately studied. Jia et al. (2010) assume that  X  ( x ,  X  ) = | x 0  X  | and show that Lasso is sign consistent for the mean parameter  X  under certain conditions. Their study shows limitations of lasso, for which many highly scalable solvers exist. However, no new method-ology is developed, as the authors acknowledge that the log-likelihood function is highly non-convex. Dette &amp; Wagener (2011) study the adaptive lasso under the model in (1). Under certain regularity conditions, they show that the adaptive lasso is consistent, with sub-optimal asymptotic variance. However, the weighted adaptive lasso is both consistent and achieves optimal asymptotic variance, under the assumption that the variance function is consistently estimated. However, they do not discuss how to obtain an estimator of the variance function in a principled way and resort to an ad-hoc fitting of the residuals. Daye et al. (2011) develop HHR procedure that optimizes the penalized log-likelihood under (1) with the ` 1 -norm penalty on both the mean and variance parameters. As the ob-jective is not convex, HHR estimates  X  with  X  fixed and then estimates  X  with  X  fixed, until convergence. Since the objective is biconvex, HHR converges to a stationary point. However, no theory is provided for the final estimates. In this paper, we propose HIPPO (Heteroscedastic It-erative Penalized Pseudolikelihood Optimizer) for es-timating  X  and  X  under model (1).
 In the first step, HIPPO finds the penalized pseudo-likelihood maximizer of  X  by solving the following ob-jective where  X   X  S is the penalty function and the tuning pa-rameter  X  S controls the sparsity of the solution  X   X  . In the second step, HIPPO forms the penalized pseu-dolikelihood estimate for  X  by solving where  X   X  = y  X  X  X   X  is the vector of residuals. Finally, HIPPO computes the reweighted estimator of the mean by solving  X   X  w = arg min where  X   X  i = exp( x 0 i  X   X  / 2) are the weights. In classical literature, estimation under heteroscedas-tic models is achieved by employing a pseudolikelihood objective. The pseudolikelihood maximization princi-ple prescribes the scientist to maximize a surrogate likelihood, i.e. one that is believed to be similar to the likelihood with the true unknown fixed variances (or means alternatively). In classical theory, central limit theorems are derived for many pseudo-maximum like-lihood (PML) estimators using generalized estimating equations (Ziegler, 2011). HIPPO fits neatly into the pseudolikelihood framework because the first step is a regularized PML where only the mean structure needs to be correctly specified. The second step and third steps may be similarly cast as PML estimators. In-deed, all our theoretical results are due to the fact that in each step we are optimizing a pseudolikelihood that is similar to the true unknown likelihoods (with alter-nating free parameters). Moreover, it is known that if the surrogate variances in the mean PML are more similar to the true variances then the resulting esti-mates will be more asymptotically efficient. With this in mind, we recommend a third reweighting procedure with the variance estimates from the second step. Fan &amp; Li (2001) advocate usage of penalty functions that result in estimates satisfying three properties: un-biasedness, sparsity and continuity. A reasonable esti-mator should correctly identify the support of the true parameter with probability converging to one. Fur-thermore, on this support, the estimated coefficients should have the same asymptotic distribution as if an estimator that knew the true support was used. Such an estimator satisfies the oracle property . A number of concave penalties result in estimates that satisfy this property: the SCAD penalty (Fan &amp; Li, 2001), the MCP penalty (Zhang, 2010) and a class of folded con-cave penalties (Lv &amp; Fan, 2009). For concreteness, we choose to use the SCAD penalty, which is defined by its derivative where often a = 3 . 7 is used. Note that estimates pro-duced by the ` 1 -norm penalty are biased, and hence this penalty does not achieve oracle property. HIPPO is related to the iterative HHR algorithm of Daye et al. (2011). In particular, the first two itera-tions of HHR are equivalent to HIPPO with the SCAD penalty replaced with the ` 1 norm penalty. In prac-tice, one can continue iterating between solving (3) and (4), however, establishing theoretical properties for those iterates is a non-trivial task. From our nu-merical studies, we observe that HIPPO performs well when stopped after the first two iterations. 2.1. Tuning Parameter Selection As described in the previous section, HIPPO requires selection of the tuning parameters  X  S and  X  T , which balance the complexity of the estimated model and the fit to data. A common approach is to form a grid of candidate values for the tuning parameters  X  S and  X 
T and chose those that minimize the AIC or BIC criterion where, up to constants, is the negative log-likelihood and is the estimated degrees of freedom. In Section 4, we compare performance of the AIC and the BIC for HIPPO in a simulation study.
 2.2. Optimization Procedure In this section, we describe numerical procedures used to solve optimization problems in (2), (3) and (4). Our procedures are based on the local linear approximation for the SCAD penalty developed in (Zou &amp; Li, 2008), which gives: This approximation allows us to substitute the SCAD penalty P j  X  [ p ]  X   X  ( |  X  j | ) in (2), (3) and (4) with and iteratively solve each objective until convergence of {  X   X  ( k ) } k . We set the initial estimates  X   X  (0) to be the solutions of the ` 1 -norm penalized problems. The convergence of these iterative approximations follows from the convergence of the MM (minorize-maximize) algorithms (Zou &amp; Li, 2008).
 With the approximation of the SCAD penalty given in (8), we can solve (2) and (4) using standard lasso solvers, e.g., we use the proximal method of Beck &amp; Teboulle (2009). The objective in (3) is minimized using a coordinate descent algorithm, which is detailed in Daye et al. (2011). In this section, we present theoretical properties of HIPPO. In particular, we show that HIPPO achieves the oracle property for estimating the mean and vari-ance under the model (1). All the proofs are deferred to Appendix.
 We will analyze HIPPO under the following assump-tions, which are standard in the literature on high-dimensional statistical learning (see, e.g. Fan et al., 2012).
 Assumption 1. The matrix X = ( x 1 ,..., x n ) 0  X  R n  X  p has independent rows that satisfy x i =  X  1 / 2 z i where { z i } i are i.i.d. subgaussian random variables with E z i = 0 , E z i z 0 i = I and parameter K (see Appendix for more details on subgaussian random variables). Furthermore, there exist two constants C min ,C max &gt; 0 such that Assumption 2. The errors 1 ,..., n are i.i.d. subgaus-sian with zero mean and parameter 1.
 Assumption 3. There are two constants  X   X  and  X   X  such that ||  X  || X   X   X  &lt;  X  and ||  X  || X   X   X  &lt;  X  . Assumption 4. | S | = C S n  X  S and | T | = C T n  X  T for some  X  S  X  (0 , 1) and  X  T  X  (0 , 1 / 3) and constants C S ,C T &gt; 0.
 The following assumption will be needed for showing the consistency of the weighted estimator  X   X  w in (4). Assumption 5. Define There exist constants 0  X  D min ,D max  X  X  X  such that Furthermore, we have that With these assumption, we state our first result, re-garding the estimator  X   X  in (2).
 Theorem 1. Suppose that the assumptions (1)-(4) are satisfied. Furthermore, assume that  X  S  X  c q log( p ) exp( p c c q log( s ) exp( p c some  X  0  X  (0 , 1) . Then there is a strict local minimizer  X   X  = (  X   X  0 S , 0 0 S C ) 0 of (2) that satisfies for some positive constants c 1 ,c 2 , and c 3 and suffi-ciently large n .
 In addition, if we suppose that assumption (5) is sat-isfied, then for any fixed a  X  R s with || a || 2 = 1 the following weak convergence holds The first result stated in Theorem 1 established that  X   X  achieves the weak oracle property in the sense of (Lv &amp; Fan, 2009). The extra term exp( nomial in n and appears in the bound (9) due to the heteroscedastic nature of the errors. The second result establishes the strong oracle property of the estimator  X  tablish the asymptotic normality on the true support S . The asymptotic normality shows that  X   X  S has the same asymptotic variance as the ordinary least squares (OLS) estimator on the true support. However, in the case of a heteroscedastic model the OLS estimator is dominated by the generalized least squares estimator. Later in this section, we will demonstrate that  X   X  w has better asymptotic variance. Note that  X   X  correctly se-lects the mean model and estimates the parameters at the correct rate. From the upper and lower bounds on  X  S , we see how the rate at which p can grow and the minimum coefficient size are related. Larger the ambient dimension p gets, larger the size of  X  S , which lower bounds the size of the minimum coefficient. Our next result establishes correct model selection for the variance parameter  X  .
 Theorem 2. Suppose that assumptions (1)-(5) are satisfied. Suppose further that  X  T  X  n there is a strict local minimizer  X   X  = (  X   X  0 T , 0 the strong oracle property, Morover, for any fixed a  X  R t with || a || 2 = 1 the fol-lowing weak convergence holds With the convergence result of  X   X  we can prove consis-tency and asymptotic normality of the weighted esti-mator  X   X  in (4).
 Theorem 3. Suppose that the assumptions (1)-(5) are satisfied and that there exists an estimator  X   X  satisfy-ing ||  X   X   X   X  || 2 = O ( r n ) , for a sequence r n  X  0 and supp(  X   X  ) = supp(  X  ) . Furthermore, assume that  X  S c q log( p ) exp( p c c r n exp( p c 2 log( n )) log( n ) and log( p ) = O ( n  X  some  X  0  X  (0 , 1) . Then there is a strict local mini-mizer  X   X  w = (  X   X  0 w,S , 0 S C ) of (4) that satisfies for some positive constants c 1 ,c 2 , and c 3 and suffi-ciently large n .
 Furthermore, for any fixed a  X  R s with || a || 2 = 1 the following weak convergence holds where  X  2 w = a 0 ( E D SS )  X  1 a . Theorem 3 establishes convergence of the weighted es-timator  X   X  W in (4) and the model selection consistency. The rate of convergence depends on the rate of con-vergence of the variance estimator, r n . From Theo-rem 2, we show the parametric rate of convergence for  X   X  S . The second result of Theorem 3 states that the weighted estimator  X   X  w,S is asymptotically normal, with the same asymptotic variance as the generalized least squares estimator which knows the true model and variance function  X  ( x ,  X  ). In this section, we conduct two small scale simulation studies to demonstrate finite sample performance of HIPPO . We compare it to the HHR procedure (Daye et al., 2011).
 Convergence of the parameters is measured in the ` 2 norm, ||  X   X   X   X  || and ||  X   X   X   X  || . We measure the iden-tification of the support of  X  and  X  using precision and recall. Let  X  S denote the estimated set of non-zero coefficients of S , then the precision is calculated as Pre  X  := |  X  S  X  S | / |  X  S | and the recall as Rec  X  := | Similarly, we can define precision and recall for the variance coefficients. We report results averaged over 100 independent runs. 4.1. Example 1 Assume that the data is generated iid from the fol-lowing model Y =  X  ( X ) where follows a standard normal distribution and the logarithm of the variance is given by Rec  X  ||  X   X   X   X  || 2 Pre  X  Rec  X  The covariates associated with the variance are jointly normal with equal correlation  X  , and marginally N (0 , 1). The remaining covariates, X 4 ,...,X p are iid random variables following the standard Normal dis-tribution and are independent from ( X 1 ,X 2 ,X 3 ). We set ( n,p ) = (200 , 2000) and use  X  = 0 and  X  = 0 . 5. Estimation procedures know that  X  = 0 and we only estimate the variance parameter  X  . This example is provided to illustrate performance of the penalized pseudolikelihood estimators in an idealized situation. When the mean parameter needs to be estimated as well, we expect the performance of the procedures only to get worse. Since the mean is known, both HHR and HIPPO only solve the optimization procedure in (3), HHR with the ` 1 -norm penalty and HIPPO with the SCAD penalty, without iterating between (4) and (3). Table 1 summarizes the results. Under this toy model, we observe that HIPPO performs better than HHR when the correlation between the relevant predictors is  X  = 0. However, we do not observe the difference between the two procedures when  X  = 0 . 5. The dif-ference between the AIC and BIC is already visible in this example when  X  = 0. The AIC tends to pick more complex models, while the BIC is more conservative and selects a model with fewer variables. 4.2. Example 2 The following non-trivial model is borrowed from Daye et al. (2011). The response variable Y satisfies with p = 600,  X  0 = 2,  X  0 = 1,  X  and the remainder of the coefficients are 0. The co-and the error follows the standard Normal distribu-tion. This is a more realistic model than the one de-scribed in the previous example. We set p = 600 and the number of samples n = 200 and n = 400.
 Table 2 summarizes results of the simulation. We ob-serve that HIPPO consistently outperforms HHR in all scenarios. Again, a general observation is that the AIC selects more complex models although the differ-ence is less pronounced when the sample size n = 400. Furthermore, we note that the estimation error signif-icantly reduces after the first iteration, which demon-strates final sample benefits of estimating the variance. Recall that Theorem 1 proves that the estimate  X   X  con-sistently estimates the true parameter  X  . However, it is important to estimate the variance parameter  X  well, both in theory (see Theorem 3) and practice. Forecasting the gross domestic product (GDP) of a country based on macroeconomic indicators is of sig-nificant interest to the economic community. We ob-tain both the country GDP figures (specifically we use the GDP per capita using current prices in units of a  X  X ational currency X ) and macroeconomic variables from the International Monetary Fund X  X  World Eco-nomic Outlook (WEO) database. The WEO database contains records for macroeconomic variables from 1980 to 2016 (with forecasts).
 To form our response variable, y i,t , we form log-returns of the GDP for each country ( i ) for each time point ( t ) after records began and before the forecast-ing commenced (each country had a different year at which forecasting began). After removing missing val-ues, we obtained 31 variables that can be grouped into a few broad categories: balance of payments, govern-ment finance and debt, inflation, and demographics. We apply various transformations, including lagging and logarithms forming the vectors x i,t . We fit the heteroscedastic AR(1) model with HIPPO. In order to initially assess the heteroscedasticity of the data, we form the LASSO estimator with the LARS package in R selecting with BIC. It is common practice when diagnosing heteroscedasticity to plot the studen-tized residuals against the fitted values. We bin the bulk of the samples into three groups by fitted val-ues, and observe the box-plot of each bin by residuals (Figure 2). It is apparent that there is a difference of variances between these bins, which is corroborated by performing a F-test of equal variances across the sec-ond and third bins (p-value of 4  X  10  X  6 ). We further observe differences of variance between country GDP log returns. We analyzed the distribution of responses separated by countries: Canada, Finland, Greece and Italy. The p-value from the F-test for equality of variances between the countries Canada and Greece is 0 . 008, which is below even the pairwise Bonferroni correction of 0 . 0083 at 0 . 05 significance level. This demonstrates heteroscedasticity in the WEO dataset, and we are justified in fitting non-constant variance. We compare the results from HIPPO and HHR when applied to the WEO data set. The tuning parame-ters were selected with BIC over a grid for  X  S and  X 
T . The metrics used to compare the algorithms are mean square error (MSE) defined by 1 n P i ( y i,t  X   X  y i,t the partial prediction score defined as the average of the negative log likelihoods, and the number of se-lected mean parameters and variance parameters. We perform 10-fold cross validation to obtain unbiased es-timates of these metrics. In Table 3 we observe that HIPPO outperforms HHR in terms of MSE and partial prediction score. We have addressed the problem of statistical infer-ence in high-dimensional linear regression models with heteroscedastic errors. Heteroscedastic errors arise in many applications and industrial settings, including biostatistics, finance and quality control in manufac-turing. We have proposed HIPPO for model selection and estimation of both the mean and variance param-eters under a heteroscedastic model. HIPPO can be deployed naturally into an existing data analysis work-flow. Specifically, as a first step, a statistician performs penalized estimation of the mean parameters and then, as a second step, tests for heteroscedasticity by run-ning the second step of HIPPO. If heteroscedasticity is discovered, HIPPO can then be used to solve penal-ized generalized least squares objective. Furthermore, HIPPO is well motivated from the penalized pseudo-likelihood maximization perspective and achieves the oracle property in high-dimensional problems.
 Throughout the paper, we focus on a specific para-metric form of the variance function for simplicity of presentation. Our method can be extended to any parametric form, however, the assumptions will be-come more cumbersome and the particular numerical procedure would change. It is of interest to develop general unified framework for estimation of arbitrary parametric form of the variance function. Another open research direction includes non-parametric esti-mation of the variance function in high-dimensions, which could be achieved with sparse additive models (see Ravikumar et al., 2009).

