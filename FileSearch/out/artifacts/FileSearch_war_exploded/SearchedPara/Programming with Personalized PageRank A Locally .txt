 Many information-management tasks (including classifica-tion, retrieval, information extraction, and information in-tegration) can be formalized as inference in an appropriate probabilistic first-order logic. However, most probabilistic first-order logics are not efficient enough for realistically-sized instances of these tasks. One key problem is that queries are typically answered by  X  X rounding X  the query X  i.e., mapping it to a propositional representation, and then performing propositional inference X  X nd with a large database of facts, groundings can be very large, making inference and learning computationally expensive. Here we present a first-order probabilistic language which is well-suited to approxi-mate  X  X ocal X  grounding: in particular, every query Q can be approximately grounded with a small graph. The language is an extension of stochastic logic programs where inference is performed by a variant of personalized PageRank. Ex-perimentally, we show that the approach performs well on an entity resolution task, a classification task, and a joint inference task; that the cost of inference is independent of database size; and that speedup in learning is possible by multi-threading.
 [ Information Systems Applications ]: Miscellaneous Probabilistic Prolog, personalized PageRank
Many information-management tasks (including classifi-cation [18], retrieval [12], information extraction [23], and in-formation integration [24, 7]) can be formalized as inference in an appropriate probabilistic first-order logic. However, most probabilistic first-order logics are not efficient enough to be used for the large-scale versions of these tasks. One key Figure 1: A Markov logic network program and its grounding. (Dotted lines are clique potentials asso-ciated with rule R2, solid lines with rule R1.) problem is that queries are typically answered by  X  X round-ing X  the query X  X .e., mapping it to a propositional repre-sentation, and then performing propositional inference X  X nd for many logics, the size of the  X  X rounding X  can be extremely large for large databases. For instance, in probabilistic Data-log [12], a query is converted to a structure called an  X  X vent expression X , which summarizes all possible proofs for the query against a database; in ProbLog [10] and MarkoViews [14] similar structures are created, encoded more compactly with binary decision diagrams (BDDs); in probabilistic sim-ilarity logic (PSL) an intentional probabilistic program, to-gether with a database, is converted to constraints for a convex optimization problem; and in Markov Logic Net-works (MLNs) [25], queries are converted to a (proposi-tional) Markov network. As an illustration of the  X  X round-ing X  process, Figure 1 shows a very simple MLN and its grounding (which here is query-independent.).

The size of groundings is sometimes, but not always, diffi-cult to analyze. For instance, for MLNs, a naive grounding is linear in the number of possible facts in the database X  i.e., O ( n k ) where k is the maximal arity of a predicate and n the number of database constants. However, even a ground-ing of size linear in the number of facts in the database, | DB | , is impractically large for inference. Superficially, it would seem that groundings must inherently be o ( | DB | ) for some programs: in the example, for instance, the proba-bility of aboutSport(x) must depend to some extent on the entire hyperlink graph (if it is fully connected). However, it also seems intuitive that if we are interested in infer-ring information about a specific page X  X ay, the probabil-ity of aboutSport(d1)  X  X hen the parts of the network only distantly connected to d1 are likely to have a small influ-ence. This suggests that an approximate grounding strategy might be feasible, in which a query such as aboutSport(d1) would be grounded by constructing a small subgraph of the full network, followed by inference on this small  X  X ocally grounded X  subgraph. Likewise, consider learning (e.g., from a set of queries Q with their desired truth values). Learning might proceed by locally-grounding every query goal, allow-ing learning to also take less than O ( | DB | ) time.
In this paper, we present a first-order probabilistic lan-guage which is well-suited to approximate  X  X ocal X  grounding. We present an extension to stochastic logic programs (SLP) [9] that is biased towards short derivations, and show that this is related to personalized PageRank (PPR) [22, 6] on a linearized version of the proof space. Based on the connec-tion to PPR, we develop a proveably-correct approximate inference scheme, and an associated proveably-correct ap-proximate grounding scheme: specifically, we show that it is possible to prove a query, or to build a graph which con-tains the information necessary for weight-learning, in time O ( 1  X  ), where  X  is a reset parameter associated with the bias towards short derivations, and is the worst-case approxi-mation error across all intermediate stages of the proof. This means that both inference and learning can be approximated in time independent of the size of the underlying database  X  X  surprising and important result.

The ability to locally ground queries has another impor-tant consequence: it is possible to decompose the problem of weight-learning to a number of moderate-size subtasks (in fact, tasks of size O ( 1  X  ) or less) which are weakly coupled. Based on this we outline a parallelization scheme, which in our initial implementation provides an order-of-magnitude speedup in learning time.

Below, we will first introduce our formalism, and then de-scribe our weight-learning algorithm. We will then present experimental results on a prototypical inference task, and compare the scalability of our method to Markov logic net-works. We finally discuss related work and conclude.
We will now describe our  X  X ocally groundable X  first-order probabilistic language, which we call ProPPR. Inference for ProPPR is based on a personalized PageRank process over the proof constructed by Prolog X  X  Selective Linear Definite (SLD) resolution theorem-prover. To define the semantics we will use notation from logic programming [17]. Let LP be a program which contains a set of definite clauses c 1 ,...,c and consider a conjunctive query Q over the predicates ap-pearing in LP . A traditional Prolog interpreter can be viewed as having the following actions. First, construct a  X  X oot vertex X  v 0 , which is a pair ( Q,Q ) and add it to an otherwise-empty graph G 0 Q,LP . (For brevity, we drop the subscripts of G 0 where possible.) Then recursively add to G 0 new vertices and edges as follows: if u is a vertex of the form ( Q, ( R 1 ,...,R k )), and c is a clause in LP of the form R 0  X  S 0 1 ,...,S 0 ` , and R 1 and R 0 have a most general unifier  X  = mgu ( R 1 ,R 0 ), then add to G 0 a new edge u  X  v Table 1: A simple program in ProPPR. See text for explanation. about(X,Z) :-handLabeled(X,Z) # base. about(X,Z) :-sim(X,Y),about(Y,Z) # prop. sim(X,Y) :-links(X,Y) # sim,link. sim(X,Y) :-linkedBy(X,Y,W) :-true # by(W). the transformed query and ( S 0 1 ,...,S 0 ` ,R 2 ,...,R k sociated subgoal list . If a subgoal list is empty, we will de-note it by 2 . Here Q X  denotes the result of applying the substitution  X  to Q ; for instance, if Q = about ( a,Z ) and  X  = { Z = fashion } , then Q X  is about ( a,fashion ).
The graph G 0 is often large or infinite so it is not con-structed explicitly. Instead Prolog performs a depth-first search on G 0 to find the first solution vertex v  X  X .e., a ver-tex with an empty subgoal list X  X nd if one is found, returns the transformed query from v as an answer to Q . Table 1 and Figure 2 show a simple Prolog program and a proof graph for it. 1 Given the query Q = about(a,Z) , Prolog X  X  depth-first search would return Q = about(a,fashion) .
Note that in this proof formulation, the nodes are conjunc-tions of literals, and the structure is, in general, a digraph (rather than a tree). Also note that the proof is encoded as a graph, not a hypergraph, even if the predicates in the LP are not binary: the edges represent a step in the proof that reduces one conjunction to another, not a binary relation between entities.
In stochastic logic programs (SLPs) [9], one defines a ran-domized procedure for traversing the graph G 0 , which thus defines a probability distribution over vertices v , and hence (by selecting only solution vertices) a distribution over trans-formed queries (i.e. answers) Q X  . The randomized pro-cedure thus produces a distribution over possible answers, which can be tuned by learning to upweight desired (correct) answers and downweight others.

In past work, the randomized traversal of G 0 was defined by a probabilistic choice, at each node, of which clause to apply, based on a weight for each clause. We propose two extensions. First, we will introduce a new way of comput-ing clause weights, which allows for a potentially richer pa-rameterization of the traversal process. We will associate with each edge u  X  v in the graph a feature vector  X  u  X  v This edge is produced indirectly, by associating with every clause c  X  LP a function  X  c (  X  ), 2 which produces the vec-tor  X  associated with an application of c using mgu  X  . This feature vector 3 is computed during theorem-proving, and used to annotate the edge u  X  v in G 0 created by apply-
The annotations after the hashmarks and the edge labels in the proof graph will be described below. For conciseness, only R 1 ,...,R k is shown in each node u = ( Q, ( R 1 ,...,R
We use a set to denote a sparse vector with 0/1 weights.
An example of the feature vector would be: if the last clause of the program in Table 1 was applied to ( Q,linkedBy ( a,c,sprinter ) ,about ( c,Z )) with mgu  X  = { X = a,Y = c,W = sprinter } then  X  c (  X  ) would be { by ( sprinter ) } . ing c with mgu  X  . Finally, an edge u  X  v will be traversed with probability Pr( v | u )  X  f ( w , X  u  X  v ) where w is a param-eter vector and where f ( w , X  ) is a weighting function X  X .g., f ( w , X  ) = exp( w i  X   X  ). This weighting function now deter-mines the probability of a transition, in theorem-proving, from u to v : specifically, Pr w ( v | u )  X  f ( w , X  u  X  v in w default to 1.0, and learning consists of tuning these.
The second and more fundamental extension is to add edges in G 0 from every solution vertex to itself, and also add an edge from every vertex to the start vertex v 0 . We will call this augmented graph G Q,LP below (or just G if the subscripts are clear from context). These links make SLP X  X  graph traversal a personalized PageRank (PPR) pro-cedure, sometimes known as random-walk-with-restart [30]. These links are annotated by another feature vector function  X  restart ( R ), which is applied to the leftmost literal R of the subgoal list for u to annotate the edge u  X  v 0 .

These links back to the start vertex bias the traversal of the proof graph to upweight the results of short proofs . To see this, note that if the restart probability P ( v 0 | u ) =  X  for every node u , then the probability of reaching any node at depth d is bounded by (1  X   X  ) d .

To summarize, if u is a node of the search graph, u = ( Q X , ( R 1 ,...,R k )), then the transitions from u , and their respective probabilities, are defined as follows, where Z is an appropriate normalizing constant:
Finally we must specify the functions  X  c and  X  restart . For clauses in LP , the feature-vector producing function  X  c for a clause is specified by annotating c as follows: every clause c = ( R  X  S 1 ,...,S k ) can be annotated with an addi-tional conjunction of  X  X eature literals X  F 1 ,...,F ` , which are written at the end of the clause after the special marker  X # X . The function  X  c (  X  ) then returns a vector  X  = { F 1  X ,...,F where every F i  X  must be ground.

The requirement 4 that edge features F i  X  are ground is the reason for introducing the apparently unnecessary pred-icate linkedBy(X,Y,W) into the program of Table 1: adding the feature literal by(W) to the second clause for sim would result in a non-ground feature by(W) , since W is a free vari-able when  X  c is called. Notice also that the weight on the by(W) features are meaningful, even though there is only one clause in the definition of linkedBy , as the weight for applying this clause competes with the weight assigned to the restart edges.

It would be cumbersome to annotate every database fact, and difficult to learn weights for so many features. Thus, if c is the unit clause that corresponds to a database fact, then  X  (  X  ) returns a default value  X  = { db } , where db is a special feature indicating that a database predicate was used. 5
The function  X  restart ( R ) depends on the functor and ar-ity of R . If R is defined by clauses in LP , then  X  restart returns a unit vector  X  = { defRestart } . If R is a database predicate (e.g., hasWord(doc1,W) ) then we follow a slightly different procedure, which is designed to ensure that the restart link has a reasonably large weight even with unit fea-ture weights: we compute n , the number of possible bindings for R , and set  X  [ defRestart ] = n  X   X  1  X   X  , where  X  is a global parameter. This means that with unit weights, after nor-malization, the probability of following the restart link will be  X  .

Putting this all together with the standard iterative ap-proach to computing personalized PageRank over a graph
The requirement that the feature literals returned by  X  c must be ground in  X  is not strictly necessary for correctness. However, in developing ProPPR programs we noted than non-ground features were usually not what the programmer intended.
If a non-database clause c has no annotation, then the de-fault vector is  X  = { id(c) } , where c is an identifier for the clause c . [22], we arrive at the following inference algorithm for an-swering a query Q , using a weight vector w . Below, we let N 0 ( u ) denote the neighbors of u  X  X .e., the set of nodes v where Pr( v | u ) &gt; 0 (including the restart node v = v also let W be a matrix such that W [ u,v ] = Pr w ( v | u ), and in our discussion, we use ppr ( v 0 ) to denote the personalized PageRank vector for v 0 . 1. Let v 0 = ( Q,Q ) be the start node of the search graph. 2. For t = 1 ,...,T (i.e., until convergence): 3. At this point v T  X  ppr ( v 0 ). Let S be the set of nodes For example, given the query Q = about(a,Z) and the pro-gram of Table 1, this procedure would give assign a non-zero probability to the literals about(a,sport) and about(a,fashion) , concurrently building the graph of Figure 2.
Note that this procedure both performs inference (by com-puting a distribution over literals Q X  ) and  X  X rounds X  the query, by constructing a graph G . ProPPR inference for this query can be re-done efficiently, by running an ordinary PPR process on G . This is useful for faster weight learning. Unfortunately, the grounding G can be very large: it need not include the entire database, but if T is the number of iterations until convergence for the sample program of Ta-ble 1 on the query Q = about ( d,Y ), G will include a node for every page within T hyperlinks of d .

To construct a more compact local grounding graph G , we adapt an approximate personalized PageRank method called PageRank-Nibble [2]. This method has been used for the problem of local partitioning : in local partitioning, the goal is to find a small, low-conductance component  X  G of a large graph G that contains a given node v .

The PageRank-Nibble-Prove algorithm is shown in Ta-ble 2. It maintains two vectors: p , an approximation to the personalized PageRank vector associated with node v 0 and r , a vector of  X  X esidual errors X  in p . Initially, p =  X  and r = { v 0 } . The algorithm repeatedly picks a node u with a large residual error r [ u ], and reduces this error by distributing a fraction  X  0 of it to p [ u ], and the remaining fraction back to r [ u ] and r [ v 1 ] ,..., r [ v n ], where the v the neighbors of u . The order in which nodes u are picked does not matter for the analysis (in our implementation, we follow Prolog X  X  usual depth-first search as much as possible.) Relative to PageRank-Nibble, the main differences are the the use of a lower-bound on  X  rather than a fixed restart weight and the construction of the graph  X  G .

Following the proof technique of Andersen et al. [2], it can be shown that after each push, p + r = ppr ( v 0 ). It is also clear than when PageRank-Nibble terminates, then for any in any graph where N ( u ) is bounded, a good approximation can be obtained. It can also be shown [2] that the subgraph  X  G is in some sense a  X  X seful X  subset of the full proof space: for an appropriate setting of , if there is a low-conductance subgraph G  X  of the full graph that contains v 0 , then G be contained in  X  G : thus if there is a subgraph G  X  containing v that approximates the full graph well, PageRank-Nibble will find (a supergraph of) G  X  .
 Finally, we have the following efficiency bound:
Theorem 1 (Andersen,Chung,Lang). Let u i be the i -th node pushed by PageRank-Nibble-Prove. Then, P i | N ( u i ) | &lt;
This can be proved by noting that initially || r || 1 = 1, and also that || r || 1 decreases by at least  X  0 | N ( u i ) | on the i -th push. As a direct consequence we have the following:
Corollary 1. The number of edges in the graph  X  G pro-duced by PageRank-Nibble-Prove is no more than 1  X  0 .
Importantly, the bound holds independent of the size of the full database of facts . The bound also holds regardless of the size or loopiness of the full proof graph, so this inference procedure will work for recursive logic programs.

To summarize, we have outlined an efficient approximate proof procedure, which is closely related to personalized PageRank. As a side-effect of inference for a query Q , this procedure will create a ground graph  X  G Q on which person-alized PageRank can be run directly, without any (relatively expensive) manipulation of first-order theorem-proving con-structs such as clauses or logical variables. As we will see, this  X  X ocally grounded X  graph will be very useful in learning weights w to assign to the features of a ProPPR program.
As an illustration of the sorts of ProPPR programs that are possible, some small sample programs are shown in Fig-ure 3. Clauses c 1 and c 2 are, together, a bag-of-words classi-fier: each proof of predictedClass(D,Y) adds some evidence for D having class Y , with the weight of this evidence de-pending on the weight given to c 2  X  X  use in establishing re-lated(w,y) , where w and y are a specific word in D and y is a possible class label. In turn, c 2  X  X  weight depends on the weight assigned to the r ( w,y ) feature by w , relative to the weight of the restart link. 6 Adding c 3 and c 4 to this pro-gram implements label propagation, and adding c 5 and c 6 implements a sequential classifier.

These examples show that, in spite of its efficient infer-ence procedure, and its limitation to only definite clauses, ProPPR appears to have much of the expressive power of MLNs [11], in that many useful heuristics can apparently be encoded.
As noted above, inference for a query Q in ProPPR is based on a personalized PageRank process over the graph associated with the SLD proof of a query goal G . More specifically, the edges u  X  v of the graph G are annotated with feature vectors  X  u  X  v , and from these feature vectors, weights are computed using a parameter vector w , and fi-nally normalized to form a probability distribution over the
The existence of the restart link thus has another impor-tant role in this program, as it avoids a sort of  X  X abel bias problem X  in which local decisions are difficult to adjust. Table 2: The PageRank-Nibble-Prove algorithm for inference in ProPPR.  X  define push( u, X  0 ): end neighbors of u . The  X  X rounded X  version of inference is thus a personalized PageRank process over a graph with feature-vector annotated edges.

In prior work, Backstrom and Leskovec [3] outlined a fam-ily of supervised learning procedures for this sort of anno-tated graph. In the simpler case of their learning procedure, an example is a triple ( v 0 ,u,y ) where v 0 is a query node, u is a node in in the personalized PageRank vector p v a target value, and a loss ` ( v 0 ,u,y ) is incurred if p In the more complex case of  X  X earning to rank X , an example is a triple ( v 0 ,u + ,u  X  ) where v 0 is a query node, u are nodes in in the personalized PageRank vector p v and a loss is incurred unless p v Backstrom and Leskovic X  X  result is a method for computing the gradient of the loss on an example, given a differentiable feature-weighting function f ( w , X  ) and a differentiable loss function ` . The gradient computation is broadly similar to the power-iteration method for computation of the personal-ized PageRank vector for v 0 . Given the gradient, a number of optimization methods can be used to compute a local optimum.
 Instead of directly using the above learning approach for ProPPR, we decompose the pairwise ranking loss into a standard positive-negative log loss function. The training data D is a set of triples { ( Q 1 ,P 1 ,N 1 ) ,..., ( Q m where each Q k is a query, P k =  X  Q X  1 + ,...,Q X  I +  X  is a list of correct answers, and N k is a list  X  Q X  1  X  ,...,Q X  J  X   X  incorrect answers. We use a log loss with L 2 regularization of the pa-rameter weights. Hence the final function to be optimized is To optimize this loss, we use stochastic gradient descent (SGD), rather than the quasi-Newton method of Backstrom and Leskovic. Weights are initialized to 1 . 0 +  X  , where  X  is randomly drawn from [0 , 0 . 01]. We set the learning rate  X  of SGD to be  X  =  X  epoch 2 where epoch is the current epoch in SGD, and  X  , the initial learning rate, defaults to 1.0.
We implemented SGD because it is fast and has been adapted to parallel learning tasks [32, 21]. Local ground-ing means that learning for ProPPR is quite well-suited to parallelization. The step of locally grounding each Q  X  X mbarassingly X  parallel, as every grounding can be done in-dependently. To parallelize the weight-learning stage, we use multiple threads, each of which computes the gradient over a single grounding  X  G Q k , and all of which accesses a single shared parameter vector w . Although the shared parameter vector is a potential bottleneck [31], it is not a severe one, as the gradient computation dominates the learning cost. 7 To evaluate this method, we use data from several tasks. Because the semantics of ProPPR and other probabilistic logics are different, the tasks are evaluated by ranking the possible answers to a query, and scoring the ranking by a standard measure such as AUC; in other words, we are not attempting to evaluate the absolute probability scores pro-duced by ProPPR, only the relative scores for a query.
Our first sample task is an entity resolution task previ-ously studied as a test case for MLNs [27]. The program we use in the experiments is shown in Table 4: it is approxi-mately the same as the MLN(B+T) approach from Singla and Domingos. 8 To evaluate accuracy, we use the CORA dataset, a collection of 1295 bibliography citations that re-fer to 132 distinct papers. We set the regularization coeffi-cient  X  to 0 . 001, the number of epochs to 5, and the learning rate parameter  X  to 1. An L 2 -regularized standard log loss function was used in our objective function.

Our second task is a bag-of-words classification task, which was previously studied as a test case for both ProbLog [13] and MLNs [18]. In this experiment, we use the following ProPPR program: which is a bag-of-words classifier that is approximately the same as the ones used by Gutmann et al. [13], as well as
This is not the case when learning a linear classifier, where gradient computations are much cheaper.
The principle difference is that we do not include tests on the absence of words in a field in our clauses, and we drop the non-horn clauses from their program. Table 3: Some more sample ProPPR programs. LP = { c ,c 2 } is a bag-of-words classifier (see text). LP = { c a sequential classifier for document sequences. c : predictedClass(Doc,Y) :-c : similar(Doc1,Doc2) :-c : predictedClass(Doc,Y) :-c : transition(Y1,Y2) :-true, # transitionFeature(Y1,Y2) Lowd and Domingos 9 [18]. The dataset we use is the We-bKb dataset, which includes a set of web pages from four computer science departments (Cornell, Wisconsin, Wash-ington, and Texas). Each web page has one or multiple la-bels: course, department, faculty, person, research project, staff, and student . The task is to classify the given URL into the above categories. This dataset has a total of 4165 web pages. Using our ProPPR program, we learn a separate weight for each word for each label.

In addition to the entity resolution task and the bag-of-words classification task, we also investigate our approach for joint inference in a link (relation) prediction problem. In this experiment, our goal is to answer the following question: can we use ProPPR to perform joint inference to improve the performance of a link prediction task on a relational knowl-edge base? To do this, we use a subset of 19,527 beliefs from a knowledge base, which is extracted imperfectly from the web by NELL, a never-ending language learner [5]. The training set contains 12,331 queries, and the test set con-tains 1,185 queries. In contrast to a previous approach [16] for link prediction, we combine the top-ranked paths learned by PRA, another method for link prediction that we have ap-plied to the NELL X  X  KB [15], and transform these paths into ProPPR programs, then perform joint inference to predict the links between entities. The total number of translated rules is 797, and was set to 0 . 00001. One goal of this exper-iment is to evaluate the performance of ProPPR on larger logic programs, containing hundreds of rules. To do this, we build on a previous approach [16] called PRA for link predic-tion, which learns weighted sets of  X  X aths X  of relations. We compare two experimental settings: the rules (paths) that are non-recursive and recursive. A non-recursive rule only makes use of the information in the database, and therefore cannot be used for joint learning with other learned PRA rules. For example, the relation agentActsinLocation only has the following non-recursive rule:
Note that we do not use the negation rule and the link rule from Lowd and Domingos. and this fact agentActsinLocation predicate only uses the beliefs in the database, but not other PRA rules. Recursive rules, on the other hand, allow us to perform joint inference on all the learned PRA rules related to the given relation. Here is an excerpt of the recursive ProPPR program, trans-lated from the learned PRA rules: athletePlaySport(Athlete,Sport) :-athletePlaySport(Athlete,Sport) :-teamPlaysSport(Team,Sport) :-teamPlaysSport(Team,Sport) :-
We first consider the cost of the PageRank-Nibble-Prove inference/grounding technique. Table 5 shows the time re-quired for inference (with uniform weights) for a set of 52 randomly chosen entity-resolution tasks from the CORA dataset, using a Python implementation of the theorem-prover. We report the time in seconds for all 52 tasks, as well as the mean average precision (MAP) of the scoring for each query. It is clear that PageRank-Nibble-Prove offers a substantial speedup on these problems with little loss in accuracy: on these problems, the same level of accuracy is achieved in less than a tenth of the time.

While the speedup in inference time is desirable, the more important advantages of the local grounding approach are Table 5: Performance of the approximate PageRank-Nibble-Prove method on the Cora dataset, compared to the grounding by running personalized PageRank to convergence. In all cases  X  = 0 . 1 .
 Table 6: AUC results on CORA citation-matching.
 MLN(Fig 1) 0.513 0.532 0.602 0.544 MLN(S&amp;D) 0.520 0.573 0.627 0.629 ProPPR( w =1) 0.680 0.836 0.860 0.908
ProPPR 0.800 0.840 0.869 0.900 that (1) grounding time, and hence inference, need not grow with the database size and (2) learning can be performed in parallel, by using multiple threads for parallel computa-tions of gradients in SGD. Figure 3 illustrates the first of these points: the scalability of the PageRank-Nibble-Prove method as database size increases. For comparison, we also show the inference time for MLNs with three inference meth-ods: Gibbs refers to Gibbs sampling, Lifted BP is the lifted belief propagation method, and MAP is the maximum a posteriori inference approach. In each case the performance task is inference over 16 test queries.

Note that ProPPR X  X  runtime is constant, independent of the database size: it takes essentially the same time for 2 8 = 256 entities as for 2 4 = 16. In contrast, lifted be-lief propagation is around 1000 times slower on the larger database.

Figure 4 explores the speedup in learning (from grounded examples) due to multi-threading. The weight-learning is using a Java implementation of the algorithm which runs over ground graphs. The full CORA dataset was used in this experiment. As can be seen, the speedup that is obtained is nearly optimal, even with 16 threads running concurrently. Figure 3: Run-time for inference on the Cora dataset using ProPPR (with a single thread) as a function of the number of entities in the database. The base of the log is 2.
 We finally consider the effectiveness of weight learning. We train on the first four sections of the CORA dataset, and report results on the fifth. Following Singla and Domingos [27] we report performance as area under the ROC curve (AUC). Table 6 shows AUC on the test set used by Singla and Domingos for several methods. The line for MLN(Fig 1) shows results obtained by an MLN version of the program of Figure 1. The line MLN(S&amp;D) shows analogous results for the best-performing MLN from [27]. Compared to these methods, ProPPR does quite well even before training (with unit feature weights, w =1); the improvement here is likely due to the ProPPR X  X  bias towards short proofs, and the tendency of the PPR method to put more weight on shared words that are rare (and hence have lower fanout in the graph walk.) Training ProPPR improves performance on three of the four tasks, and gives the most improvement on citation-matching, the most complex task.

The results in Table 6 all use the same data and evaluation procedure, and the MLNs were trained with the state-of-the-art Alchemy system using the recommended commands Figure 4: Performance of the parallel SGD method on CORA dataset. The x axis is the number of threads on a multicore machine, and the y axis is the speedup factor over a single-threaded implementa-tion. for this data (which is distributed with Alchemy 10 ). How-ever, we should note that the MLN results reproduced here are not identical to previous-reported ones [27]. Singla and Domingos used a number of complex heuristics that are dif-ficult to reproduce X  X .g., one of these was combining MLNs with a heuristic, TFIDF-based matching procedure based on canopies [19]. While the trained ProPPR model outper-formed the reproduced MLN model in all prediction tasks, it outperforms the reported results from Singla and Domingos only on venue , and does less well than the reported results on citation and author 11 .
Similar to the evaluation of the entity resolution task, here we focus on three evaluations: the cost of inference as a func-tion of the database size, the accuracy in the classification task, and the speedup in the learning due to multi-threading.
We show the cost of inference as a function of database size on the WebKb dataset in the Figure 5. In this experi-ment, we fix the number of test queries, and vary the num-ber of entities in the database. We see that the run time for ProPPR is independent of the size of the database: it takes the same amount of time for ProPPR to perform inference for 2 10 = 1024 entities as for 2 4 = 16 entities. However, this is not the case for inference in the Markov logic network. We see that when the size of the database is small, all of the approaches have similar run time, but when there are 1024 entities in the database, the run time of each method diverges significantly. The result is consistent with those of the CORA experiments.

We also consider the accuracy of the ProPPR language on the Webkb dataset. We use exactly the same cross-validation experimental settings that previous work use [18, http://alchemy.cs.washington.edu
Performance on title matching is not reported by Singla and Domingos.
 Figure 5: Run-time for inference on the WebKb dataset using ProPPR (with a single thread) as a function of the number of entities in the database. The base of the log is 2. 13]: in each fold, for the four universities, we train on the three, and report result on the fourth. In Table 7, we show the detailed AUC results of each fold, as well as the av-eraged results 12 . First, we see that if we do not perform weight learning, the averaged result is equivalent to a ran-dom baseline. As reported by Gutmann et al. [13], the ProbLog approach obtains an AUC of 0.606 on the dataset. The voted perceptron algorithm (MLN VP, AUC  X  0 . 605) and the contrastive divergence algorithm (MLN CD, AUC  X  0 . 604) reported by Lowd and Domingos [18] are within the same range as ProbLog. When using the conjugate gra-dients approach, the MLN (CG) achieved an AUC of 0.730. When comparing to the trained version of ProPPR on the same dataset, we see that ProPPR obtains an AUC of 0.797, which outperforms all the results reported by ProbLog and MLN.

Finally, we consider the speedup in learning due to multi-threading on the WebKb dataset. Learning time averages about 950 seconds with a single thread, but this can be reduced to only two minutes if 16 threads are used. For comparison, Lowd and Domingos report that around 10,000 seconds were needed to obtain the best results were obtained for MLNs. The multi-threaded speed up performance on different sections of the WebKb dataset is shown in Table 8.
The accuracy results on the NELL link-prediction task are shown in Table 9. We observe that when not performing joint inference with the learned top-ranked paths, ProPPR obtains an AUC of 0.858. However, when using these rules for joint learning, we observe an AUC of 0.916 when using ProPPR. The total time for joint inference with 797 rules is 13 minutes.
Note that both [18, 13] do not show the detailed breakdown of the results on WebKb dataset. Table 7: AUC results on the WebKb classification task. Co.: Cornell. Wi.: Wisconsin. Wa.: Washing-ton. Te.: Texas.
 ProLog [13]  X   X   X   X  0.606 MLN (VP) [18]  X   X   X   X  0.605 MLN (CD) [18]  X   X   X   X  0.604 MLN (CG) [18]  X   X   X   X  0.730 ProPPR( w =1) 0.501 0.495 0.501 0.505 0.500 ProPPR 0.785 0.779 0.795 0.828 0.797 Table 8: Detailed Performance (seconds) of the parallel SGD method of ProPPR on the WebKb dataset. Co.: Cornell. Wi.: Wisconsin. Wa.: Wash-ington. Te.: Texas.
Although we have chosen here to compare mainly to MLNs [25, 27], ProPPR represents a rather different philosophy to-ward language design: rather than beginning with a highly-expressive but intractable logical core, we begin with a lim-ited logical inference scheme and add to it a minimal set of extensions that allow probabilistic reasoning, while main-taining stable, efficient inference and learning. While ProPPR is less expressive than MLNs (for instance, it is limited to definite clause theories) it is also much more efficient. This philosophy is similar to that illustrated by probabilistic simi-larity logic (PSL) [4]; however, unlike ProPPR, PSL does not include a  X  X ocal X  grounding procedure, which leads to small inference problems, even for large databases. Our work also aligns with the lifted personalized PageRank [1] algorithm, which can be easily incorporated as an alternative inference algorithm in our language.

Technically, ProPPR is most similar to stochastic logic programs (SLPs) [9]. The key innovation is the integration of a restart into the random-walk process, which, as we have seen, leads to very different computational properties.
There has been some prior work on reducing the cost of grounding probabilistic logics: notably, Shavlik et al [26] describe a preprocessing algorithm called FROG that uses various heuristics to greatly reduce grounding size and infer-ence cost, and Niu et al [20] describe a more efficient bottom-up grounding procedure that uses an RDBMS. Other meth-ods that reduce grounding cost and memory usage include  X  X ifted X  inference methods (e.g., [29]) and  X  X azy X  inference methods (e.g., [28]); in fact, the LazySAT inference scheme for Markov networks is broadly similar algorithmically to PageRank-Nibble-Prove, in that it incrementally extends a network in the course of theorem-proving. However, there is no theoretical analysis of the complexity of these meth-ods, and experiments with FROG and LazySAT suggest that they still lead to a groundings that grow with DB size, albeit more slowly.

ProPPR is also closely related to the PRA, learning al-gorithm for link prediction [15], like ProPPR, PRA uses Table 9: AUC results on the NELL link prediction task.
 random walk processes to define a distribution, rather than some other forms of logical inference, such as belief propaga-tion. In this respect PRA and ProPPR appear to be unique among probabilistic learning methods; however, this distinc-tion may not be as great as it first appears, as it is known there are close connections between personalized PageRank and traditional probabilistic inference schemes 13 . PRA, however, is much more limited than ProPPR: PRA uses random-walk methods to approximate logical inference. The set of  X  X nference rules X  learned by PRA corresponds roughly to a logic program in a particular form X  X amely, the form ProPPR allows much more general logic programs. How-ever, unlike PRA, we do not consider the task of searching for new logic program clauses.
We described a new probabilistic first-order language which is designed with the goal of highly efficient inference and rapid learning. ProPPR takes Prolog X  X  SLD theorem-proving, extends it with a probabilistic proof procedure, and then limits this procedure further, by including a  X  X estart X  step which biases the system to short proofs. This means that ProPPR has a simple polynomial-time proof procedure, based on the well-studied personalized PageRank (PPR) method.

Following prior work on PPR-like methods, we designed a local grounding procedure for ProPPR, based on local parti-tioning methods [2], which leads to an inference scheme that is an order of magnitude faster that the conventional power-iteration approach to computing PPR, takes time O ( 1  X  0 dependent of database size. This ability to  X  X ocally ground X  a query also makes it possible to partition the weight learning task into many separate gradient computations, one for each training example, leading to a weight-learning method that can be easily parallelized. In our current implementation, an additional order-of-magnitude speedup in learning is made possible by parallelization. Experimentally, we showed that ProPPR performs well on an entity resolution task, a clas-sification task, and a joint inference task. The cost of the inference is independent of the data size, and the speedup in learning is made possible due to multi-threading. We are grateful to anonymous reviewers for useful com-ments. This work was sponsored in part by DARPA grant FA87501220342 to CMU and a Google Research Award.
For instance, it is known that personalized PageRank can be used to approximate belief propagation on certain graphs [8]. [1] Babak Ahmadi, Kristian Kersting, and Scott Sanner. [2] Reid Andersen, Fan R. K. Chung, and Kevin J. Lang. [3] Lars Backstrom and Jure Leskovec. Supervised [4] Matthias Brocheler, Lilyana Mihalkova, and Lise [5] Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr [6] Soumen Chakrabarti. Dynamic personalized PageRank [7] William W. Cohen. Data integration using similarity [8] William W Cohen. Graph Walks and Graphical [9] James Cussens. Parameter estimation in stochastic [10] Luc De Raedt, Angelika Kimmig, and Hannu [11] Pedro Domingos and Daniel Lowd. Markov Logic: An [12] Norbert Fuhr. Probabilistic datalog X  X  logic for [13] Bernd Gutmann, Angelika Kimmig, Kristian Kersting, [14] Abhay Jha and Dan Suciu. Probabilistic databases [15] Ni Lao and William W. Cohen. Relational retrieval [16] Ni Lao, Tom M. Mitchell, and William W. Cohen. [17] J. W. Lloyd. Foundations of Logic Programming: [18] Daniel Lowd and Pedro Domingos. Efficient weight [19] Andrew McCallum, Kamal Nigam, and Lyle H. Ungar. [20] Feng Niu, Christopher R  X e, AnHai Doan, and Jude [21] Feng Niu, Benjamin Recht, Christopher R  X e, and [22] Larry Page, Sergey Brin, R. Motwani, and [23] Hoifung Poon and Pedro Domingos. Joint inference in [24] Hoifung Poon and Pedro Domingos. Joint [25] Matthew Richardson and Pedro Domingos. Markov [26] Jude Shavlik and Sriraam Natarajan. Speeding up [27] Parag Singla and Pedro Domingos. Entity resolution [28] Parag Singla and Pedro Domingos. Memory-efficient [29] Parag Singla and Pedro Domingos. Lifted first-order [30] Hanghang Tong, Christos Faloutsos, and Jia-Yu Pan. [31] Martin Zinkevich, Alex Smola, and John Langford. [32] Martin Zinkevich, Markus Weimer, Alex Smola, and
