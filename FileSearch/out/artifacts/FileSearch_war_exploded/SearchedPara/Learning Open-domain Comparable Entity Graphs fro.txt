 A frequent behavior of internet users is to compare among vari-ous comparable entities for decision making. As an instance, a user may compare among iPhone 5, Lumia 920 etc. products be-fore deciding which cellphone to buy. However, it is a challeng-ing problem to know what entities are generally comparable from the users X  viewpoints in the open domain Web. In this paper, we propose a novel solution, whic h is known as Comparable Entity Graph Mining (CEGM), to learn an open-domain comparable en-tity graph from the user search queries. CEGM firstly mine seed comparable entity pairs from user search queries automatically us-ing predefined query patterns. Next, it discovers more entity pairs with a confidence classifier in a bootstrapping fashion. Newly dis-covered entity pairs are organized into an open-domain comparable entity graph. Based on our empirical study over 1 billion queries of a commercial search engine, we build a comparable entity graph which covers 73.4% queries in the top 50 million unique queries of a commercial search engine. Through manual labeling in sampled sub-graphs, the average precision of comparable entities is 89.4%. As applications of the learned entity graph, the entity recommen-dation in Web search is empirically studied.
 H.3.3 [ Information Systems ]: Information Search and Retrieval Comparable Entity Mining, Open-domain, Log Mining Making decisions based on comparison among a set of candidate entities is a common user behavior. For example, with several but incomplete candidates (e.g., iPhone 5 and Lumia 920), we tend to look for better candidates by search engines before purchasing a  X  The work was done when the first author was visiting Microsoft Research Asia.
 cellphone. Thus it would be fundamentally beneficial to end users if search engines can recommend some comparable entities. This requires the search engines to obtain a large set of comparable enti-ties from all the web users. Actually it is still an open question that how many entity domains users may frequently compare entities in and what these entities are.

Collecting a large set of comparable entities is a challenging problem resulted from the following four reasons: (1) Open-domain problem. Currently, it is still an untouched problem that the entities being frequently compared are from what domains. This makes the problem an open-domain problem which is different from previous domain dependent efforts [6, 15]. (2) Unsupervised. To solve an open-domain problem should not rely on human labels and the opti-mal solution should be unsupervised instead of weakly-supervised algorithms [7]. Because it is almost impossible to know in advance all the possible domains in which users may compare something in. (3) User intents. Comparable e ntities should be r ecommended from user view. A high ratio of previous studies has attempted to extract comparable entities from free text [7, 13] documents. However, we argue that they cannot reflect the users X  intents in comparing en-tities and thus are generally not optimal for user decision making purpose. To learn what entities are most frequently compared by the crowd, i.e. the end users, we propose to use the search queries in query log for entity graph learning. (4) Mining comparable en-tities from queries makes the problem even harder since the search queries are generally short, which do not have sufficient contexts for model training in contrast to using the free text sentences.
To conquer the challenges, in this paper we propose to learn an open-domain comparable entity graph where vertexes are entities in any arbitrary domain and edges among them stand for the com-parability. To build the graph, we develop a novel algorithm called Comparable Entity Graph Mining (CEGM) in a fully automatic and parallelizable fashion. Specifically, the CEGM algorithm is com-posed of four components ( Figure 1 , more examples can be found in Figure 2 ). To start with, we use an unsupervised seed entity pair extraction component to generate open-domain comparable entity seeds automatically through some heuristics and statistical mea-sures. After this stage, an initial entity graph is constructed with vertexes (entities) and edges (degree of comparability). And then in order to roughly estimate the num ber of domains where entities are frequently being compared, a graph cut algorithm is utilized to group similar seeds into the same cluster. The other advantage of graph cut is that we can process sub-graphs at the same time which makes CEGM parallelizable. In the third step, seeds are sent to the query log from which patterns (or templates), will be learned. These patterns are then ranked based on a newly proposed strategy. Ranked patterns are sent to the query log to extract more candidate entity pairs for entity graph construction. Finally qualified pairs are further incorporated into the constructed graph.

In this paper, we aim to build an open-domain comparable enti-ty graph through learning from query log of a commercial search engine. We model it as an undirected graph, where each vertex is an entity such as  X  X oyota Camry " and each edge connecting two vertexes means that they are comparable. The weight on an edge demonstrates the comparability between the linked entity pairs.
Mathematically, we use Q to represent the set of all queries in the query log. Each query along with its appearance time is called a record denoted as r =( q,cnt ) ,where q is the query and cnt stands for the the frequency of q . An entity, denoted by e is an arbitrary named entity which has the same definition as in [8]. En-tities could belong to different domains, such as  X  X umia 920" in the Cellphone domain. All the domains are logically retained in a set denoted as D . An entity pair, denoted as ep , is a pair of entities. For better comprehension, we use s =( e 1 ,e 2 ,dc ) to represent a seed entity pair, where dc is the degree of comparability between the two entities. Note that the larger the dc is, the more comparable the two entities are. Seeds are used to learn query patterns with which new entity pairs can be extracted. A pattern is in the form of where string in the squared brackets can be empty and  X  ENT  X  serves as placeholder for entity in this paper. Unlike some clas-sical bootstrapping algorithms [1, 13], we only use lexical pat-terns instead of syntactic ones for efficiency purpose. This means that the part-of-speech tagging [ 17], dependency parser [12] are not used in this work. For example, suppose that we have s = ( Apple, Microsoft, 37) and the following records: A pattern p = ENT and ENT net worth can be derived from r 1 by replacing Apple and Microsoft with the placeholder  X  X NT" .Then p is applied back into r 1 and r 2 where we can learn one new compa-rable entity pair ep 1 =( IBM, apple, 23) . With these definitions, our problem is formulated as,  X  X iven a query log Q , our goal is to build an open-domain com-parable entity graph G =( V,E ) ,where V is the set of comparable entities in any arbitrary domain and E is the set of edges connect-ing them."
To build an open-domain comparable entity graph, we propose the CEGM (C omparable E ntity G raph M ining) algorithm, which is unsupervised and parallelizable. As shown in Figure 1 , our algo-rithm is composed of four components which will be detailed in the following subsections. We refer readers to Figure 2 for a better view of the data flow of CEGM.
Different from previous works where seeds are handcrafted at the very beginning, we use heuristics to automatically discover ini-tial seeds from queries. This is because targeting on open-domain makes it impossible to give seeds manually because we cannot know in advance all the possible domains.

Heuristics 1 (H1) - X  X s, v.s., vs. and versus" are used as the comparative pattern keywords to discover entity pairs po-sitioned at their ends.

For example, from the query  X  X Phone 4s vs Lumia 920", we can derive s = (iPhone 4s, Lumia 920, 37) . H1 is very indicative when describing comparative relations in queries according to our exper-iments. However, it has exceptions. As some instances, 1) repeated word ellipsis: r 1 = (iPhone 4s vs Lumia 920 screen, 21), 2) popular phrases: r 2 = (plants vs zombies, 49), 3) wrong comparison: r 3 = (iPhone vs Android, 18).

Ifweonlyrelyon H1 ,1) s = (iPhone 4s, Lumia 920 screen, 21) would be extracted; 2) plants and zombies would be treated as two entities instead of as a whole; 3) an edge would appear between en-tities from different domains, i.e. , cellphone and operating system. According to our experimental analysis, r 1 takes up to 95% of all the failure cases by using H1, leaving the remaining 5% occupied by r 2 and r 3 . These observations lead to the next heuristic used in CEGM which can well handle r 1 :
Heuristic 2 (H2) -e i,j exists if and only if e i vs e j e hold true simultaneously .

However, if a user forms the query in r 1 as "Lumia 920 screen vs iPhone 4s" (which is really the case), H2 becomes invalid. In order to handle this situation, we propose the third heuristic:
Heuristic 3 (H3) -if two queries, such as s i vs s j and s exist and s j is the substring of s k ,then s k is omitted.
This means that if iphone 4s vs lumia 920 is a more popular query, then the query iphone 4s vs lumia 920 screen should be omit-ted. Additionally, we collect all these tails (e.g., screen) and seeds containing such tails are further tailored. We count the appearance time of each entity pair and set a cut-off threshold (10 in this paper) to further obtain statistically significant pairs. With these seeds, we can build an initial entity graph (IEG, Figure 2(c) ).
As shown in Figure 2(c) , vertexes are linked together although they belong to different domains. In order to know how many do-mains users frequently compare something in, we utilize a graph cut algorithm to cut the IEG ( Figure 2(c) )into M sub-graphs. As shown in Figure 2(d) where different shapes represent different sub-graphs, some wrongly connected edges such as &lt;d 90 ,htc &gt; and &lt;t 2 i, htc &gt; in Figure 2(c) have been cut-off. In this paper, we use a state-of-the-art algorithm in finding link communities [2]. We then rank these M clusters according to their traffic which is the sum of the traffic of all the unique entities in the cluster. Note that when computing the traffic, those obvious navigational queries (e.g., facebook, youtube, etc.) have been eliminated by checking whether the query is contained in corresponding url. For instance, q = facebook  X  url = www.facebook.com. The reason is that these queries occupy large amount of traffic while contain little use-ful information. Finally, according to  X  X ighty-Twenty" rule, we re-main K (5768 in this paper) clusters which are in the top 20% and whose size (unique entity number) is bigger than 10. It should be noticed that we do not require that these K clusters are from K distinct domains. Here we made an assumption that K is the upper bound number of domains. Different segments are assigned with different class labels k automatically, where 1  X  k  X  K .
After graph cut, each cluster can be deemed as a sub-graph ( Figure 2(d) ) representing entity graph of different domains. Entities in the same cluster are of equally importance. To expand the graph, in this stage, we add a virtual link between each pair of entities in a sub-graph. Taking the examples in Figure 2(d) , three new virtual and &lt; samsung, ibm &gt; . Next, we put the entities belong to the same cluster into the query log to mine patterns as done in many other bootstrapping systems [1, 15].
All learned patterns in PSet k are ranked in a decreasing manner according to their appearance frequency. Patterns are then sent to the query log to discover candidate entity pairs. We loop for each query in the query log, and proceed to next query once the cur-rent one is successfully matched against a pattern in PSet reason is that patterns with higher rank should be more likely to ex-tract confident entity pairs. However, simply using the appearance frequency to weight patterns will lead to a severe problem called semantic drift [4]. We will discuss this in Section 3.3.4.
With millions of candidate entity pairs generated in the last step, this component is to select out high confident pairs from the candi-dates. We model the selection process as a classification problem whereaclassifier f : ep  X  y,y  X  X  0 , 1 } needs to be learned. Here the positive class "1" denotes  X  X omparable" and 0 otherwise.
Specifically, considering the specialty of our problem, we design several features including normalized pointwise mutual informa-tion of ep i , URL features as done in [8] etc., to classify entities in couple and logistic regression with L2 norm is used to solve the binary classification problem.
We examined patterns in all clusters and found that the top 3 are almost the same, i.e.,  X  ENT vs ENT ",  X  ENT and ENT ", and:  X  differences between ENT and ENT ". These patterns can be used for comparing entities in all the domains. Therefore, if we simply use patterns, ranked by their appearance frequency, to extract entity pairs, the performance will not as good as expected since there is a severe problem in bootstrapping systems called Semantic Drift [4].
To mitigate semantic drift, we argue that  X  X he more general a pattern is, the more likely it would cause semantic drift." Thus, if a pattern appears in many different pattern sets, the pattern X  X  weight should be small and if a pattern is unique to a specific pattern set, it should be ranked higher. Here we use term frequency-inverse document frequency (TFIDF) p as a part (the final weighting equation is Eqn. 3 where another term is incorporated) to reweight all the patterns, where | X | means the number of and p denotes the pattern.

Besides, once confident entity pairs are selected, we not only add them to the entity graph G , but also reweight the pattern for the next loop. Our motivation is that patterns which generate those confident en tity pairs should be w eighted higher .
 where  X  is the controlling parameter, L has the same size as P and the value of the i  X  th entry is the number of confident entity pairs that the pattern generates.

Combining the pattern reweighting strategy together, we form our final weighting equation for the t +1 step as an interpolation of two terms controlled by a parameter  X  :
Two datasets are used in our experiment, a sampled query log (0.2 billion English queries) and a full query log (1 billion English queries). About 0.2 million entity pairs are extracted by the heuris-tics introduced in Section 3.1 by using the sampled dataset. The larger query log is used to expand the initial entity graph. Click-through data satisfying the follo wing conditions ar e retained (400 million unique queries and 0.3 million uniquely generalized URLs) to generate URL features: 1) English queries only. 2) Query is longer than 3 and shorter than 64 characters. Too short 3) Let query + URL be the key which should appear at least twice.
Since it is hard to measure recall due to the lacking of ground truth, the performance is evaluated by the measure  X  X recision at the N th entity" for each class which is defined as [19]: where N c is number of entities which have been labeled as correct in the top N results and N pc is the number of entities which is  X  X artially correct".
In order to evaluate the algorithm performance, we report here the results of 18 uniformly sampled classes (e.g., Language, Car, Cellphone, Camera, City, Currency, Extensions, CPU, Video Card (VC), Tablet, Drug, Operating System (OS), Programming Lan-guage (PL), Browser, Football Club (FC), Company, Antivirus Soft-ware (AS), and Country ) from the constructed comparable entity graph according to the traffic distribution. Some rank higher (e.g., Cellphone ), some locate in the body (e.g., Drug ), while others dis-tribute at the tail positions (e.g., PL ). Seed entities which are auto-matically obtained after seed generation are shown in Table 1 .
Seeds quality is essential to bootstrapping al gorithms because error propagation is very fast in such systems. Even if handcraft-ed seeds may bring singularity (e.g., &lt; iphone 4s, iphone 5 &gt; may generate the pattern  X  X hich is better, ENT or ENT" which may pro-duce nearly every comparable things), needless to say the difficulty in unsupervised seeds collection algorithm proposed in Section 3.1. Hence, we first conduct the experiment to examine the seeds qual-ity and to see their impact on the later process.

Using the method introduced in Section 3.1 and 3.2, we construct a seed graph with 65,981 vertexes and 187,302 edges. We manually label each entity in the initial seed set of each class. The precision is calculated according to Eqn. 4. When labeling, users X  typo will not Table 1: Sampled entities in seeds and the precision of seed generation be taken as faults, e.g., kindal (kindle). The overall performance is shown in Table 1 , where wrong cases are shown in bold type.
It can be observed from Table 1 that using the proposed heuris-tics, we can easily generate abundant (21.4 entities i n average) and high quality (0.97 precision) seeds. By analyzing the wrong cas-es in all the classes, we found that there are two general situations which may lead to errors. 1) Singularity. Take the query  X  X uby vs budget rent a car" for example. A man whose name is Ethan Ruby once engaged in a lawsuit with a car rental company  X  X udget rent a car". Therefore, the pair &lt; ruby, budget rent a car &gt; would be extracted wrongly as a seed in Programming Language (PL) class since Ruby is often compared with other languages. 2) Users X  back-ground knowledge deficiency. For instance, iphone (cellphone) is always compared with android (operating system) which are enti-ties from different classes. Despite of these, we can still draw a conclusion that H1 , H2 and H3 are effective, and many popular entities in each domain can be found.
Open-domain Comparable Entity Graph (OCG) is composed of two components, i.e., vertexes and edges. Thus, the quality of OCG should also be tested from these two perspectives.
For vertex evaluation, we temporally cut off the edges among them while recording the weights as their frequency. For instance, in the cellphone class, there are two edges: &lt; iphone 4s, lumia 920, 39 &gt; and &lt; iphone 4s, iphone 5, 70 &gt; . Then the frequency for i-phone 4s, lumia 920 and iphone 5 are 109, 39 and 70. Entities from the same class are ranked from high to low according their frequen-cy and ask 3 volunteers to carefully label the top 50 entities in 18 testing classes. The overall performance is depicted in Figure 3 .
In all, the average precision of labeled entities is 89.4%, which is higher than that of some general-purpose knowledge base like KnowItAll [5] (64% on average), NELL [4] (74%), TextRunner [3] (80%) while slightly lower than that of Probase [18] (92.8%) and YAGO [16] (95%). We can observe from Figure 3 that the preci-sions of all the 18 classes except for Programming Language (PL) are around 85%. This is because seeds of PL are of low quality leading to a badly trained classifier which cannot distinguish be-tween positive and negative entity pairs. In an OCG, edges represent the comparability among entities. The greater the weights are, the more related the associated enti-ties would be. We expect that the constructed OCG can retrieve the most comparable entities once an entity is queried. We con-duct an experiment called comparative entity recommendation and compare our methods with two approaches: i Google Related Search (GRS) : We query Google with the en-ii Probase : Probase is reported to be the largest and the most
Table 2 list top 5 entities most related (in terms of comparabil-ity) to the target entity, such as nikon d3200 ( Camera ), new york ( City ), honda ( Car ), Xanax ( Drug ), intel i5 ( CPU ), and us dollar ( Currency ). It can be observed from Table 2 that our method can successfully recommend the most comparable entities. For exam-ple, for  X  X ikon d3200", results are products having similar perfor-mance and price such as  X  X annon 650d" or  X  X ikon d5100". For  X  X s dollar", the ranking results are currency of economic powers (e.g., Yuan in China, Yen in Japan, etc.) or regions (e.g., Euros in Eu-rozone). For the antianxiety drug  X  X anax", most of the retrieved drugs have the same effect except for  X  X meprazole" which is for diseases of digestive system, such as dyspepsia. An interesting ob-servation is that when we query OCG with a specific product (e.g.,  X  X ikon d3200" and  X  X anax"), a list of similar products will be re-trieved. However, if an abstract entity, such as a city name  X  X ew york", is taken as input, OCG will output their counterparts instead of specific entities. For example, for the famous car brand  X  X ord", the results are other noted automobile manufacturers like  X  X hevy", "Toyota" etc. rather than specific cars.
 Similar results of OCG and Probase can also be observed from Table 2 . However, since Probase is a general-purpose knowledge base, not as specific as ours, some classes are not well covered (e.g., only one entity is retrieved in CPU ) and some recommend-ed entities are incorrect which is highlighted in bold type, such as  X  X ecommended pentax".

The situation differs in the results of GRS. It generally suggests two kinds of queries: 1) Frequently searched queries starting with the given entity (e.g.,  X  X ntel i7 gaming" for  X  X ntel i5 vs". As a re-sult, results of GRS contain more noise than ours, such as  X  X ntel i7 gaming",  X  X hevy jokes", etc.; 2) Comparable entities (e.g.,  X  X hevy" for  X  X ord" and  X  X anadian" for  X  X s dollar"). These observations are similar with [13]. Underlined entities in Table 2 are overlapped re-sults both generated by OCG and GRS or Probase. The coincidence also proves that the constructed OCG can recommend comparable entities effectively. Figure 4: Comparison between the precision of entities using the proposed method (CEGM) and PBNER in the class of ex-tensions, cellphone, tablet, camera, city and an average over all the target classes respectively
At last, we evaluate the pattern reweighting strategy proposed in this paper. In all, 1,049,750 patterns are generated by our method. We compare our method with the most related work [7], as men-tioned in Introduction, denoted as PBNER. The core part of PBN-ER (i.e., entity extraction part) employs a state-of-the-art method proposed in [15] which weights a pattern according to its appear-ance time. However, since [7] is a 4 pages paper, many postprocess-ing procedures are missing. Therefore, we carefully implemented the core part of [7] (i.e., the algorithm presented in [15]) and tried our best to reproduce the postprocessing steps.

We draw the Precision/Rank curve on top 150 entities in 5 classes (e.g., extensions, cellphone, tablet, camera and city ) and an average precision curve for all the 18 testing classes in Figure 4 . It can be observed that the precision curve of CEGM (solid line) is always above PBNER (dashed line), though at some points they are very close. The last graph in Figure 4 is the overall performance where the proposed method outperforms PBNER with relative precision score boosts of 4% (90% vs. 86%) at rank 50, 6% (89% vs. 83%) at rank 100 and 6% (86% vs. 80%) at rank 150. This can prove the effectiveness of our reweighting strategy.
Due to the limit space, we list here some interesting observations which cannot be shown in the paper: 1) If seeds from a certain domain are not captured at the  X  X utomatic seed generation" step, we may not mine out entities from that domain. For example, entities from Movie is not covered by OCG since movies are seldom being compared; 2) using H1 would bring in words that do not conform the definition of entities. For instance, users often compare between two synonyms such as  X  X ffect vs effect",  X  X .g. vs i.e." etc.
In terms of discovering comparable entity pairs, our work is most related with comparable entity mining research [7, 9, 10, 11, 13]. Jindal and Liu (J&amp;L) [10, 11] focus on mining comparative rela-tions from English text documents . They are the first proposing a two-step framework in comparable entity mining which solves a classification problem first, i.e. whether a sentence is comparative, and then a label problem, i.e., which part of the sentence is the desideratum. Li et al. [13] present a weakly-supervised bootstrap-ping algorithm extending J&amp;L X  X  algorithm by merging the two-step framework into one step. However, their work mines comparable entities from questions which are required to be labeled as compar-ative, non-comparative, or unknown in the training phase. Jang et al. [9] also followed J&amp;L X  X  work but refined the classification prob-lem. Each sentence is classified into 7 classes defined by authors according to Korean linguistics, and then comparative elements are extracted from sentences classified as comparative ones. Jain et al. [7] try to identify comparable entities from both query and web text, where they used a weakly-supervised bootstrapping method.
Our work is also related to name d entity recognition (NER) [14], especially NER from query log (NERQ) [6, 19], which can be taken as our sub-problem. NERQ is emerged as a novel and im-portant topic recently because 1) as claimed in [6], 71% queries contain named entities; 2) better Named Entity Recognition (N-ER) can help search engines to understand users intents better and thus provide better search results. Different from traditional N-ER from texts, NERQ, in alternative, seeks to mine named enti-ties from users search log. Pa  X  sca [15] propose to use a template based bootstrapping method which orients NERQ to tuple (e.g., &lt;e 1 ,r,e 2 &gt; ,where e i , i =1 , 2 , stands for entity and r repre-sents the relation between them) extraction in Information Extrac-tion (IE). These pattern based methods are either semi-supervised or weakly-supervised . Thus, they cannot handle the open-domain problem (see Introduction for details). Additionally, Jain et al. [8] propose an unsupervised algorithm to solve open-domain NERQ which is based on the observation that oftentimes users construct their search query by copy-pasting phrases from existing texts, and they define a candidate entity as the maximal sequence of words in the query such that each word in the entity begins with an upper-case character. Except from these pattern based algorithms, Guo et al. [6] and Xu et al. [19] try to build probability models to capture the generative process of queries.

It is worth noting that this work is also different from relation specific Information Extraction (RIE) [1] and Open Information Extraction (OIE) proposed by Banko et al. [3]. The work in this paper can on one hand, get rid of seeds which is the same as OIE, and on the other hand, know the specific relation between entities which is the same as RIE.
We present an unsupervised open-domain comparable entity min-ing algorithm over query logs. The constructed graph is composed of 630,121 verified vertexes and 3 million edges. According to our estimation, entities in 560 domains are frequently being compared by users on the Internet. In the experimental part, we have com-prehensively examined the proposed algorithm and the constructed graph. The graph is currently the largest taxonomy which only fo-cuses on comparable relation. Starting with automatically obtained seeds can ensure handling the open-domain problem. The follow-ing bootstrapping process allows ex tracting candidate entity pairs which will be further filtered by the trained classifiers. In the fu-ture, we plan to: 1) solve the pr oblem of entity synonyms and 2) use the constructed graph to do more entity based applications, such as query suggestion, query intent analysis.
