 1. Introduction of three components: A database D of documents.
 A set T of topics (information needs, written in natural language).
 For each topic in T , a non-empty set of relevant documents from D .
 are relevance judged.

In the well known TREC environment, the proper subset is generated by the pooling technique. This tech-ally 100) top ranked documents is created. The union of the created sets forms the pool , say P to be relevance judged with respect to t ( Voorhees, 2004 ).

When a new system (a system that did not participate in the pooling process) is tested against a TREC col-ments that do not belong to P t , are retrieved within the 100 top ranking positions. Such documents are tory, since it may favour systems that participated in the pooling process.

These measures are compared, both theoretically and experimentally. In addition, we experimentally compare the two measures to a well-known evaluation measure, mean uninterpolated average precision (MAP).
The rest of the paper is organized as follows. Section 2 describes the test collections used in our experi-put forward conclusions. 2. Test collections
Our experiments used the same test collections as was used by Buckley and Voorhees (2004) , TREC-8 (ad hoc task), TREC-10 (Web track) and TREC-12 (robust track). Table 1 gives an overview of these three col-associated with several runs, and thereby with several systems. Also given in Table 2 is the mean number of retrieved documents (in TREC, maximum 1000 per topic), relevant and unjudged documents across the runs.
Since the measures we focus on in this work are dependent on the number of retrieved documents, each run that retrieved less than 95% of the maximal number of retrieved documents was excluded. For example, for
TREC-8 (with 50 topics) the maximal number of retrieved documents for a run is 50 1000  X  50 ; 000. There-fore, each TREC-8 run that retrieved less than 47,500 was excluded. Further, each run that did not retrieve any documents for one or more topics was excluded. These exclusion rules are the same as those employed in Buckley and Voorhees (2004) . We excluded 7 runs each from TREC-8 and TREC-12, while 21 runs were excluded from TREC-10. Table 2 shows that on average (over all runs and topics) there is a relatively large number of retrieved documents that have not been relevance judged, irrespective of which of the three TRECs documents ( Table 3 ). 3. Motivation for inventing alternative measures
We mentioned the issue of retrieved unjudged documents in the end of the preceding section. As can be seen as 80% at ranking position 1000, over all runs and all 100 topics. For TREC-12, the top 125 documents were used during the pooling process. Despite that, the share of unjudged documents at ranking position 125 is about 13%. Even at ranking position 1, there are documents that have not been judged.
One concern with unjudged documents is that runs, which did not contribute to the pools, may retrieve unjudged relevant documents, and thereby be treated unfairly during evaluation. The same problem also are guaranteed to be relevance judged. Although it has been argued on empirical grounds that the TREC col-lections are not biased against such runs ( Voorhees, 2002; Zobel, 1998 ), we believe that large shares of unjudged documents, especially at low ranking positions, is a sufficient motivation for the construction of measures that do not take unjudged documents into account.
 4. Two measures of retrieval effectiveness
Each measure is such that it relates all known relevant documents in a ranked list of documents to the same number of known irrelevant documents. 4.1. The Buckley and Voorhees measures
Buckley and Voorhees (2004) proposed a measure of retrieval effectiveness intended to be used when some retrieved documents have not been judged for relevance. To our knowledge, the measure is intended to be a measure of rank effectiveness. Such a measure favours IR methods that retrieve relevant documents early in but its performance in terms of rank effectiveness may be poor.

If we assume that the pooling technique has been used, R P ranked (with respect to the IR-method M ), known irrelevant documents for t . Further, let I of documents d such that d 2 I r and Rank  X  d ; t ; M  X  &lt; Rank  X  d ence ( bpref ) for the IR method M with respect to the topic t , is defined as: known irrelevant documents are ranked lower (higher ranks) than each known relevant document. Observe that the definition of this measure presupposes at least r known irrelevant documents for a given topic.
Since bpref is coarse when a topic has a small number (one or two) of known relevant documents, Buckley and Voorhees (2004) used a variant of the measure, bpref -10, in their experiments. Let I 10  X  r most highly ranked, known irrelevant documents for t . The variant is defined as where I 10+r ( d i ) is the number of documents d such that d 2 I d evant document is related, with respect to ranking position, to 10 + 1 irrelevant documents. 4.2. Rank effectiveness: a related measure intended to measure to what degree (known) irrelevant documents are ranked lower than (known) relevant documents. Let I be the set of all known irrelevant documents for topic t . The measure, rank effectiveness ( RankEff ) for the IR method M with respect to t , is then defined as: where I ( d i ) is the number of documents d such that d 2 I and Rank  X  d ; t ; M  X  &gt; Rank  X  d highest performance is obtained when all the n r known irrelevant documents are ranked lower than each known relevant document.

The RankEff measure gives the mean number of known irrelevant documents that are ranked lower than a known relevant document, in relation to the number of known irrelevant documents. That this statement is true is shown by the following derivation:
One potential problem with RankEff concerns known irrelevant documents that are not retrieved. Let the number of judged documents for a topic t be 6. Assume that two of the documents are judged relevant, four the two relevant ones in positions 1 and 2. Assume finally that IR-method M the two relevant and two of the irrelevant, with the two relevant ones in positions 1 and 2. Under these assumptions RankEff  X  M 1 ; t  X  X  1, while RankEff  X  M 2 ; t  X  X  0 : 5 : M known irrelevant documents, which is inappropriate. To avoid scenarios of this kind, we define, for d ; d 0 2 D , Rank  X  d ; t ; M  X  &gt; Rank  X  d 0 ; t ; M  X  as true if d is not retrieved and d
RankEff  X  M 1 ; t  X  X  RankEff  X  M 2 ; t  X  X  1. 4.3. Practical adjustments
In the TREC environment, retrieval effectiveness is evaluated at 1000 retrieved documents. It may be the case, and often is, that some known relevant documents are not among the top 1000 retrieved, for a given mer (and not to the coarser variant bpref ) in our experiments (Section 6 ).

Let R 0 be the set of retrieved known relevant documents for an IR method M and a topic t 2 T , and let r 6 r be the number of documents in R 0 . Let I 10  X  r  X  d 0 and Rank  X  d ; t ; M  X  &lt; Rank  X  d 0 i ; t ; M  X  , where d d 2 I and Rank  X  d ; t ; M  X  &gt; Rank  X  d 0 i ; t ; M  X  , where d bpref -10 and RankEff :
An IR method that retrieves only a small number, say k , of known relevant documents at the first k ranks on the l first ranks. This is a desirable property, which the measures have in common with MAP. MAP is ods ( Buckley &amp; Voorhees, 2000 ).

Note that the two measures, like MAP, give a non-optimal value when not all known relevant documents that r 0  X  r . 5. Theoretical comparison of the measures
Let I r  X  d i  X  be the number of documents d such that d 2 I
Consider the case where RankEff is modified in such a way that the measure is only applied to the r most highly ranked, known irrelevant documents. In that case we define RankEff as
Then we obtain the following: bpref and RankEff are thus identical when the latter measure is applied to the r most highly ranked, known irrelevant documents, or if r  X  n = 2.
 and RankEff are equivalent in the sense that they always agree with respect to the order of two compared IR methods. More formally, one may ask if the following statement holds: if and only if
However, the statement above is not true, which is shown by the following counterexample. Assume that n  X  30 and r  X  2, for a given topic t . Then n r  X  28  X j I j X  the number of known irrelevant documents in the lists of retrieved documents. Further, positions 16 X 29 are assumed to contain, for both M known irrelevant documents.

With the data in Table 4 , we obtain the following for bpref -10 and the two methods: For RankEff we obtain: and
Thus, RankEff  X  M 1 ; t  X  &gt; RankEff  X  M 2 ; t  X  . Given the data of the example, M
M RankEff has some appealing properties in relation to bpref -10: It uses more information, since each known irrelevant document is taken into consideration.
It can handle data sets with any number of relevant and irrelevant documents, except when the number of known relevant documents is 0, or the number of known irrelevant documents is 0 ( r  X  0or n  X  r , respectively).

It handles topics with a small number of relevant documents better than bpref -10 in the sense that unrea-sonably large differences in measurement values between IR methods are prevented.

Another difference between RankEff and bpref -10 concerns the case where a known relevant document swaps position with a higher ranked known irrelevant. A measure of rank effectiveness should increase as a consequence of such a change. RankEff and uninterpolated average precision (AP) increase under the change.
For bpref -10, this is not always the case. In Appendix , we give proofs of these statements. 6. Experimental comparison
TREC-8, TREC-10 and TREC-12. We are principally concerned with comparing RankEff and bpref -10. How-ever, we compare the two measures to MAP, heavily used in TREC. 6.1. Error rates of the runs performs better than the other for one of the two sets, but worse for the other set, a swap has occurred. When the error rates are computed, the difference in EM values between two runs (with respect system with the higher EM value actually is better. For more detailed information on the computing of error rates, see Voorhees and Buckley (2002) .

We computed error rates for bpref -10, RankEff and MAP, using the maximal topic set sizes (25 for TREC-8 and TREC-10, 50 for TREC-12). We counted the share of swaps for all pairs of runs from TREC-8, TREC-10 and TREC-12, and we utilized 1000 different randomly selected pairs of disjoint topic sets for each pair of runs.
 sion are given for each measure and for TREC-8, TREC-10 and TREC-12, respectively. A column labeled with  X % X  reports the share in percent of the best observed MAP (average bpref -10, average RankEff ) value obtained for a run and for a given TREC that d represents. A r column gives the standard deviation over the observed MAP (average bpref -10, average RankEff ) values for a given TREC. For TREC-8 and TREC-ations, which indicates that one often obtains a larger absolute difference between two systems compared to the differences for MAP and bpref -10. In this relative sense, RankEff outperforms the two other measures. 6.2. Full pools of relevance data  X  correlation between system rankings Let S be the set of the n runs executed in a given evaluation context, and let EM be an evaluation measure.
A system ranking with respect to S and EM is a list  X  r EM
Avg  X  r i  X  P EM Avg  X  r j  X  if i &lt; j , where EM Avg  X  r (For MAP, EM is AP, and EM Avg  X  r k  X  X  MAP  X  r k  X  .)
One way to empirically study if two evaluation measures measure the same thing is to compute the corre-lation between two system rankings, where one ranking is obtained from one of the measures, the other from the other measure. Table 8 gives, for each of the three TRECs, correlation data for bpref -10, RankEff and between bpref -10 and RankEff are fairly weak over all TRECs. bpref -10 has higher correlations with MAP than with RankEff . One of the correlation values between bpref -10 and MAP is (slightly) less than, while system rankings for the three measures. The x -axis in each figure represents systems, sorted by decreasing
MAP values. Both bpref -10 and RankEff exhibit a decreasing trend. However, the low Kendall correlation between RankEff and MAP ( Table 8 ) is clearly mirrored in the three graphs. RankEff disagrees with MAP ranked at position 42 by MAP is ranked as number 7 by RankEff . Similarly we can see a system in Fig. 4 (TREC-12) ranked number 17 by MAP and 49 by RankEff . 6.3. Effects of gradually reducing relevance data
For a given topic t 2 T and a given TREC, we started with the full pool, P the set of known irrelevant documents for t . Then the union of P selected and if the number of relevant documents was 40 and the number of irrelevant ones was 200, 0 : 95 40  X  38 relevant and 0 : 95 200  X  190 irrelevant documents were selected. If the number, say x ,of documents to be selected was a non-integer, we added 0.5 to x , and then the greatest integer less than x  X  0 : 5 was selected. This approach yields, for each a 2 V categories of documents. As the minimal number of relevant and irrelevant documents to include in a reduced less than 1 (10), we used the minimum value. 6.3.1. Change in absolute average values
The graph of Fig. 5 shows, for the three measures and for TREC-12, the effects of gradually reducing the for TREC-8 and TREC-10 are similar. The reduction has a clear impact on MAP and, to a lesser extent, on increases as the level of incompleteness increases. However, bpref -10 changes at a slower rate compared to
MAP, with regard to a  X  100 down to a  X  35. RankEff is the measure that exhibits the most consistent behav-iour: the average value stays approximately the same until 4% ( a  X  4) of the relevance data are left. to decrease (or increase) when the relevant documents are reduced, then the system is treated unfairly (or favoured). 6.3.2. Correlations between system rankings
It is highly desirable that an evaluation measure is stable in the sense that it ranks IR methods (at least might ask what would happen if more relevance data were added. It is possible that a hypothetical new rank-problem for the Cranfield evaluation model.

The graphs in Figs. 6 X 8 , which correspond to TREC-8, TREC-10 and TREC-12, plot the Kendall corre-lations between systems rankings obtained from MAP ( bpref -10, RankEff ) using the full relevance pools, and system rankings obtained from the same measure using reduced pools. The x -axis represents the level RankEff is flatter than the plots for the other two measures. As the relevance data are gradually reduced, reduced. For TREC-8 and TREC-12, we have to move to incompleteness levels of 90% ( a  X  10) and 91% ( a  X  9) before the value of s drops below 0.9. TREC-10 deviates from the other two TRECs in this respect, since its corresponding incompleteness level is 75% ( a  X  25). 7. Discussion and conclusion documents that have not been relevance judged. We compared the measures theoretically, and we experimen-tally compared them to each other and to the well-known evaluation measure MAP.

The experimental results indicate that RankEff may be a better alternative to MAP than bpref -10 when the tems that did not contribute to the pools.
 better performance than bpref -10. MAP has the worst performance, for all three considered TRECs.
The relative error rates of RankEff are lower than the corresponding rates for bpref -10, which in turn has ative) error rate is clearly a desirable property of an evaluation measure.

MAP and bpref -10 are in close agreement as to the ranking of systems when the full pools of judged doc-RankEff and MAP, is fairly small. This indicates that RankEff measures something else than bpref -10 and ferences between RankEff and bpref -10 concerns the case where a known relevant document swaps position with a higher ranked known irrelevant. RankEff and AP increase under the change, while bpref -10 does not lematic, it is questionable if a high degree of correlation is desirable.
 perhaps against other evaluation measures) using other test collections with incomplete relevance data.
Finally, if measures like RankEff and bpref -10 are used for evaluation in environments like TREC, one might consider instructing the participating groups to rank the full pool of judged documents for a topic.
In that way, there would be no need for the practical adjustments described in Section 4.3 ,and RankEff and bpref -10 could be used as they are intended to be used. Moreover, since RankEff correlates poorly with suring should be further addressed.
 Appendix. The three measures and the swap property
A desirable property of an evaluation mesaure EM is the following: (SWAP) If a known relevant document swaps position with a higher ranked known irrelevant, then the value on EM increases. Below, we prove that RankEff and AP satisfy SWAP, and that it is not always the case that bpref -10 satisfies SWAP. Proposition 1. RankEff satisfies SWAP.
 d and Rank  X  d  X  &gt; Rank  X  d r  X  , is not affected by the swap, for each known relevant d than d 0 or higher than d 00 . Now, since d 0 swaps position with d increases with at least 1. Moreover, for each known relevant document d
Rank  X  d 00  X  &lt; Rank  X  d r  X  &lt; Rank  X  d 0  X  , I ( d increases. h Proposition 2. AP satisfies SWAP.

Proof. Let d 0 be the known relevant document that swaps position with a higher ranked known irrelevant, d
The precision at each known relevant document d such that d is ranked lower than d affected by the swap. We consider two possible cases. (1) There is no known relevant document between d d evant documents up to d 0 is the same as it was before the swap, the new precision at d ment between d 0 and d 00 in the ranking. Let d 1 ; ... ; d and d 00 in the ranking, i.e., Rank  X  d 00  X  &lt; Rank  X  d
Rank  X  d j  X  if i &lt; j . Let P ( d ) be the original precision at d ,and P Then But then the value on AP increases. h Proposition 3. It is not always the case that bpref -10 satisfies SWAP.

Proof. Let d 0 be the known relevant document that swaps position with a higher ranked known irrelevant, d
Assume that Rank  X  d 00  X  &gt; Rank  X  d  X  , for each d 2 I vant documents (where r is the number of known relevant documents for the topic). In such a situation,
I 10+r ( d r ), the number of documents d such that d 2 I 10  X  r as it was before the swap, for each known relevant document d after the swap as it was before the swap, and thus the value does not increase. h References
