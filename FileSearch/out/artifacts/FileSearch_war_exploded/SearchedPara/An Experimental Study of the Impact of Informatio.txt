 Researchers have shown that various natural language pro-cessing techniques can be used in document analysis to im-pact search performance. For the most part, they exam-ined how an analysis system with certain performance char-acteristics can be leveraged to improve document and/or passage search results. We have previously shown that se-mantic queries which utilize named entity and relation in-formation extracted from the corpus can lead to significant improvement in search performance. In this paper, we ex-tend our previous efforts and examine how search perfor-mance degrades in the face of imperfect named entity and relation extraction. Our study was carried out by develop-ing gold standard annotated corpora and applying different error models to the gold standard annotations to simulate er-rors made by automatic recognizers. We identify automatic recognizer characteristics that make them more amenable to our search tasks, show that recognizer recall has more significant impact on semantic search performance than its precision, and demonstrate that significant improvement in both MAP and Exact Precision scores can be achieved by adopting automatic named entity and relation recognizers with near state-of-the-art performance.
 H.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Languages, Experimentation, Performance Semantic Search, Named Entity Recognition, Relationship Recognition Copyright 2007 ACM 978-1-59593-803-9/07/0011 ... $ 5.00.
The vast majority of search engines and their users to-day still rely on simple keywords for specifying information needs and for matching documents in the underlying col-lections to these information needs. However, there is a growing number of systems and applications in which richer information is encoded in the search indices to allow for more expressive runtime queries and to facilitate retrieval of more relevant documents. One such class of encoded in-formation can be extracted through automatic analysis of documents in the collection by utilizing natural language processing techniques, such as syntactic parsing [14, 8, 15], word sense disambiguation [17, 7, 9], named entity recogni-tion [11, 9, 15], and relation extraction [8, 15, 3].
We previously investigated the impact of named entity and relation extraction on document retrieval results [3]. We leveraged the XML Fragments query language [2] to make use of automatically extracted named entity and relation in-formation for addressing a number of query time semantic needs. We showed that in most of our use cases, signifi-cant improvement in search results was achieved using our semantic-enhanced queries compared with baseline queries without the semantic features. However, a couple of impor-tant open questions remained. First, although we showed that document search performance improvement when lever-aging our named entity and relation recognizers, it was un-clear what level of performance could potentially be achieved with perfect entity and relation recognition, nor how se-mantic search performance degrades in the face of imperfect recognition. Second, it was unknown what characteristics of automatic recognizers other than their precision and recall may affect their utility in semantic search tasks. We investi-gate these issues in this paper through systematic degrada-tion of named entity and relation annotations in gold stan-dard annotated corpora. We identify automatic recognizer characteristics that contribute to better semantic search per-formance, and demonstrate that although semantic search performance generally degrades as recognizer performance decreases, significant improvement in search results can be obtained by leveraging named entity and relation recogniz-ers with near state-of-the-art performance.

The rest of the paper is organized as follows. The next section discusses relevant previous work to situate our cur-rent efforts. Section 3 provides a brief overview of the XML Fragments query language and summarizes the results of our previous experiments. Section 4 discusses the reference corpora and gold standard annotations used in our current study. In Section 5, we discuss the annotation degradation process, including the error models we developed to simu-late error characteristics of different automatic recognizers. Section 6 discusses our experiments and results, and in Sec-tion 7 we conclude.
There is a fairly large body of work in which researchers leveraged natural language processing techniques to auto-matically analyze document collections and used the anal-ysis results to improve search performance. In a typical information retrieval system, words, word stems, or lemmas are extracted from document texts and stored in the search index. In these retrieval systems that make use of deeper NLP technology, information above the lexical level, such as part-of-speech tags, syntactic parses, word senses, named entity information, and relation information, are addition-ally stored in the index to be utilized at query time. These efforts have yielded various degrees of impact on search per-formance, depending on the approach and the data set, al-though most experiments reported higher impact on the pre-cision of search results than on recall.

Earlier work by Voorhees [17] and Sanderson [13] on per-forming word sense disambiguation to improve search per-formance showed the impact of WSD on this task to be quite minimal, whereas subsequent approaches evaluated on different data sets yielded moderate gains, in particular in search precision [7, 9]. There have also been attempts at utilizing information extracted from syntactic parse trees to enrich search indices, including extracting tree structures at the clause level [14], indexing phrases extracted from parse trees [19], extracting ternary expressions such as subject-relation-object triples from syntactic parses [8], construct-ing conceptual case frames [4], among many others. An-other body of work that involves encoding information ex-traction results into search indices include utilizing named entity information [11, 10] to provide high precision retrieval for question answering applications, as well as semantic-level relation information, such as OwnerOf(Person, Organiza-tion) and ProducesWeapon(Country, Weapon) [3]. Finally, Tiedemann [15] developed a multi-layer index that explic-itly encodes lexical information, part-of-speech tags, depen-dency parse information, and named entity information, and showed how the deeper analyses improved passage retrieval performance as well as the performance of a question an-swering system.

All of the previous efforts described above, with the excep-tion of [13] and [7], focused on developing methodologies to make use of certain types of linguistic information encoded in the search index and on evaluating the impact of such information on an end-to-end application such as document retrieval, passage retrieval, or question answering. In each study, the linguistic information was derived by one or more automatic systems employed by the authors, and the over-all performance of the end-to-end application was affected by the performance of these automatic extraction systems. However, since the performance of the automatic systems was not separately measured and analyzed, it is unknown, in cases of poor end-to-end performance, whether the lack of performance improvement is due to the methodology or the low performance of the automatic extraction systems, and in cases where performance improvement was observed, how the performance gain might increase with better extraction systems. &lt; book &gt; &lt; /book &gt;
In our work, we focus on the impact of named entity and relation information on document retrieval performance. We are particularly interested in how named entity and relation information can help improve search performance in the op-timal case (when named entity and relation recognition is perfect), and how search performance degrades as the en-tity and relation recognizers X  performance worsens. As a result, our work is most closely related to [13] and [7], in which the impact of word sense disambiguation accuracy was measured against document retrieval performance. In both cases, degradation of word sense disambiguation per-formance was introduced into gold standard corpora and search performance was measured against the degraded cor-pora. Our work differs from these two earlier efforts not only in the linguistic information addressed (named entities and relations vs. word senses), but also in our focus on de-veloping multiple error models to simulate the behavior of automatic recognizers and on identifying characteristics of automatic recognizers that make them more amenable to improving the performance of our search applications.
XML Fragments [2] were introduced as a document-centric means to specifying queries for searching XML documents. Traditionally, the XML document collections used for eval-uating XML search technologies contained structured doc-uments whose semantics are clearly encoded, such as the book example shown in Figure 1, taken from [2]. While XML Fragments have been shown to be sufficiently expressive and effective for addressing most information needs in several do-mains, those results were obtained using XML documents containing semantic markups that are near 100% accurate.
The XML Fragments query language extends typical key-word query languages by allowing XML tags to surround keywords. For example, the XML Fragment query &lt; book &gt; books written by Donald Knuth, including the example in Figure 1. In addition, XML Fragments support classical op-erators such as  X + X ,  X - X , and phrase . For additional features of the XML Fragments query language, see [2, 1].
Since most existing text collections do not contain docu-ments with proper semantic markup, such as that in Fig-ure 1, researchers have previously adopted semantic anal-ysis processes, including named entity and relation recog-nition, to automatically obtain the semantic annotations and utilized these annotations in the retrieval process [11, 9, 8, 15, 3]. In particular, we previously explored ways in which XML Fragments can be used to formulate queries that leverage automatic named entity annotations, such as Per-son , Organization , and Country , and relation annotations, such as Owner and ProducesWeapon , in the search index [3]. We demonstrated significant document retrieval per-formance improvement when leveraging XML Fragments to address three query-time semantic needs. In this paper we further explore two of these three use cases, namely tar-get information type specification and relation specification , which we explain in further detail here.

In the target information type specification use case, key-word queries are augmented with the semantic type of the information the user is seeking. For example, + X  X hite House X  + &lt; Zipcode &gt;&lt; /Zipcode &gt; effectively retrieves doc-uments containing the address of the White House: 1600 Pennsylvania Avenue NW, Washington, DC 20500, provided that  X 20500 X  was identified as a Zipcode during corpus anal-ysis. This strategy is effective when most query terms have low idf but do not frequently co-occur with the target in-formation type. In the relation specification use case, on the other hand, query terms are constrained by how they may relate to one another. For instance, &lt; Owner &gt; +Iraq + &lt; Weapon &gt;&lt; /Weapon &gt;&lt; /Owner &gt; retrieves documents in which an Owner relation annotation spans Iraq and a Weapon annotation. Because of the inherent difficulty in automatic relation extraction, this approach is suitable for applications in which the importance of hitlist precision out-weighs its recall.

In our earlier work [3], we reported a 4.1% and 37.5% improvement in MAP and Exact Precision, respectively, for target information type specification, and a 73% improve-ment in Exact Precision for relation specification using our automatic named entity and relation recognizers in corpus analysis. Although these are promising results, they do not tell us what the potential gain in search performance is with better automatic recognizers, nor how semantic search performance degrades as recognizer performance decreases. This paper investigates these issues by developing a gold standard annotated corpus, systematically introducing er-rors into the corpus to simulate recognizer errors, and mea-suring the impact that these errors have on semantic search performance.
The first step in our investigation involves manually devel-oping gold standard annotated corpora, which help estab-lish upperbound semantic search performance. We previ-ously used the TREC 2005 QA track document task dataset, which contains 50 questions and judgments against the one million document AQUAINT corpus, 1 in the target informa-tion type specification experiments, and a set of 25 questions and judgments, against a 37,000 document corpus from the Center for Nonproliferation Studies (CNS), in our relation specification experiments. In the current study, we use the same test queries, but evaluate them against smaller refer-Available from http://www.ldc.upenn.edu.
 AQUAINT 1,399 642 2,041 24 0
CNS 135 102 237 19 10 ence corpora derived from manually annotated gold stan-dard corpora. To minimize the labor-intensive manual an-notation effort, we developed two annotated subcorpora: a larger AQUAINT subcorpus, which includes only named entity annotations, for the target information type speci-fication task, and a smaller CNS subcorpus, which includes named entity and relation annotations, for the relation spec-ification task.

Since we expected both subcorpora to be very small com-pared with the original corpora, we constructed the subcor-pora with respect to the test sets to ensure that both rele-vant and irrelevant documents for each query are well repre-sented in the appropriate subcorpus. For each query, we in-cluded a set of relevant documents, which are all documents considered relevant in the judgment files, and a set of noise documents, which are up to ten most highly ranked irrele-vant documents retrieved by our system for that query in our earlier experiments. This criterion ensures that the irrele-vant documents chosen are highly competitive with the rele-vant documents from our search engine X  X  perspective. Using this mechanism, we constructed an AQUAINT subcorpus of 2,041 newswire articles and a CNS subcorpus containing 237 documents in the national intelligence domain.

As shown in Table 1, over half of all documents in both corpora were considered relevant for at least one query in the test set, while the others were not relevant for any query. The AQUAINT subcorpus was manually annotated with 24 named entity types to cover most answer types sought in TREC QA questions, while the CNS subcorpus was anno-tated with 10 relation types in the national intelligence do-main, such as ProducesWeapon and Owner , and 19 named entity types that represent arguments of these relations.
These two manually annotated gold standard subcorpora represent the outcome of corpus analysis produced by rec-ognizers with precision and recall of 1.0. These gold stan-dard annotations enable us to establish upperbound seman-tic search performance for our two tasks. The next section discusses various models we developed to simulate named entity and relation recognition errors, which we apply to our gold standard annotated corpora to model degradation of recognizer performance.
Our annotation degradation process modifies the gold stan-dard annotations based on certain principles in order to create new annotated corpora that satisfy given recognizer performance characteristics (in our experiments its precision and recall). The derived corpora are then used to simulate
Note, however, that documents relevant for one query are irrelevant for most other queries. Thus, for each query, there are significantly more noise documents than relevant ones, roughly matching the scenario for most search problems.
Figure 2: The Three-Step Degradation Process the outcome of automatic entity and relation recognizers with those characteristics to estimate their impact on search performance.

Clearly, different recognizers make different types of mis-takes, due to their algorithms and design principles. For instance, some recognizers may target higher recall through multiple annotations in case of ambiguity, while others select only the most confident annotation. As another example, re-lation recognizers may differ in terms of how strongly they rely on the semantic types of their potential arguments to be recognized correctly.

We developed several models to simulate recognizer er-rors. We will show later in the paper that at the same tar-get precision and recall level, the annotations produced by the different error models differ in their impact on semantic search performance. By leveraging multiple error models, our performance degradation analysis covers a wide range of recognizer design principles and thus a broad spectrum of their resulting characteristics. We therefore anticipate that a given automatic recognizer will likely produce annotations with characteristics similar to one of more of our simulated error models and yield semantic search performance analo-gous to those obtained in our experiments.
Our process for degrading the gold standard annotations is outlined in Figure 2. It takes as input a gold standard annotated corpus and the target precision and recall values. Its output is the same text corpus with errors introduced to the gold standard annotations to obtain the specified preci-sion and recall performance on the annotations. These de-rived corpora which satisfy certain recognizer performance characteristics are used in our experiments in Section 6 to determine the impact of recognizer accuracy on semantic search performance.

Figure 2 shows our degradation process with three sub-processes, all of which depend on the error model employed. The first step computes aggregate corpus level statistics on the gold standard annotations, including the frequency of each semantic type in the corpus and a ranked list of con-fusable semantic types for each annotated text span. The next two steps, which are independent in some error mod-els and intertwined in others, introduce errors into the gold standard annotations to reach the desired annotation preci-sion and recall levels. Based on the frequency of semantic types and the target precision and recall, we determine the number of false positive and false negative annotations that need to be introduced. Annotation precision is altered by introducing false positives, i.e., by adding incorrect anno-tations to the corpus, while annotation recall is changed by introducing false negatives, i.e., by removing correct annota-tions from the corpus. The next section describes in further detail our error models and the three degradation subpro-cesses associated with each model.
We developed three recognizer error models, which we call the micro model , the macro model , and the macro with re-placement model . Before going into detail on these error models, we first briefly discuss a mechanism that we lever-age in all error models to estimate the semantic ambiguity of annotated text spans. This mechanism utilizes a special-ized index which we call the Annotation Database (ADB) . The ADB aggregates, for each semantic type, all text spans covered by annotation instances of that type in the corpus. Its primary function is to support a query mechanism that, given a set of keywords and/or semantic types, returns a ranked list of semantic types that span all keywords/types in the corpus. 3 For example, queried against the ADB for the AQUAINT subcorpus,  X  X incoln X  returns four possible semantic types: Facility (as in Lincoln Tunnel ), City ( Lin-coln, Nebraska ) , Person ( Abraham Lincoln ), and Organi-zation ( Lincoln National Corporation ), indicating that in-stances of  X  X incoln X  in that subcorpus were annotated with one of those four semantic types depending on context. On the other hand,  X  X resident Abraham Lincoln X  was unam-biguously annotated as Person .

The ranked list of semantic types and their associated scores returned from the ADB indicate how ambiguous a text span is and are used to estimate how likely an automatic entity and/or relation recognizer is to make an erroneous an-notation on that text span. More specifically, text spans for which there is only one possible annotation in the corpus or those where the confidence score of the correct annotation is much higher than that of the top ranked incorrect annota-tion are considered more robust (and less ambiguous) than those where the confidence score differences for correct and incorrect annotations are small. In the above example, the Person annotation over  X  X resident Abraham Lincoln X  is less ambiguous than that over  X  X incoln X .
In the micro error model, target annotation precision and recall are achieved at the semantic type level, and the ambi-guity of an annotated text span is used in determining the placement of the false positive/negative annotations. This is a reference error model intended to capture the behav-ior of automatic recognizers whose errors are fairly evenly distributed among the semantic types it recognizes. In step one of the degradation process, we leverage the ADB to produce two sets of up to n ranked lists, one for each semantic type that appears in the annotated corpus. These sets of ranked lists represent confidence and confusability information at the semantic type level. For annotation con-fidence, a ranked list for semantic type CorrType contains all instances of annotations of CorrType in the gold stan-dard annotated corpus, ranked by the system X  X  estimate of
We focus on how we leverage functionalities provided by the ADB in our own work. Details on the ADB, including the mechanisms used to construct the index and to evalu-ate candidate semantic types against queries are beyond the scope of this paper. how confident an automatic recognizer may be in assigning CorrType to each text span. For annotation confusability, a ranked list for semantic type IncorrType contains a list of annotations which are not of IncorrType , ranked by the sys-tem X  X  estimate of their likelihood of being misrecognized as IncorrType by an automatic recognizer. Both sets of ranked lists are obtained by iterating through all annotations in the corpus and, for each annotation, querying the ADB with text extracted from the annotation and determining confi-dence and confusability of the annotation from the query results.

Step two of the degradation process uses annotation con-fusability information to introduce false positive annota-tions. For each semantic type, IncorrType , we compute fp , the number of false positive annotations needed to achieve the target precision level. We then select the top fp scoring annotations in the confusability ranked list for IncorrType and multiply annotate the same text span with an additional annotation of type IncorrType .

Step three of the process uses annotation confidence in-formation to introduce false negative annotations. For each type, CorrType , we compute fn , the number of false nega-tive annotations needed to achieve the target recall value, and then remove the bottom fn scoring annotations in the confidence ranked list for CorrType .

The above three-step process is adopted as is in the named entity annotation degradation process. In the relation anno-tation degradation process, we make one further distinction as follows. In step one of the degradation process, we com-puted confidence and confusability information by querying the ADB based on text extracted from each annotation. For entity annotations, the ADB query is the keywords spanned by the annotation. For relation annotations, however, we distinguish between the lexical confusability model in which the ADB query is the keywords in the arguments of the re-lation, and the named entity confusability model where the query is the semantic types of the relation arguments. The different ADB queries result in different annotation confi-dence and confusability information. As a result, different false positive and false negative annotations are selected for error introduction, thus producing different annotated cor-pora. We apply the lexical confusability vs. named entity confusability distinction to each of the three error models discussed above, resulting in six error models for relation recognizers.

Since target annotation precision and recall is achieved for each semantic type in the micro error model, it simu-lates automatic recognizers whose errors are fairly evenly distributed among the semantic types it covers. Further-more, for relation recognizers, the named entity confusabil-ity model simulates recognizers which emphasize the seman-tic types of the relation arguments in its recognition process, while the lexical confusability model simulates those that consider the lexical strings in the arguments more strongly than their semantic types.
The macro error model differs from the micro model in that target precision and recall are obtained only at the collection level; therefore, they can vary significantly from semantic type to semantic type. Conceptually, the degrada-
Recall that annotations in the ranked list for IncorrType are those whose correct type is anything but IncorrType . tion process closely follows that in the micro error model. However, where information was previously aggregated based on semantic type, the aggregation now occurs across types at the collection level.

In step one of the degradation process, the confidence and confusability ranked lists previously generated for each se-mantic type are merged to produce a total of two ranked lists that represent collection-level confidence and confusability information. The processes for introducing false positive and false negative annotations are again performed at the collection level. To achieve target precision, we compute to-tal fp , the total number of false positive annotations needed, and select the top total fp scoring annotations in the confus-ability ranked list to receive false positive annotations. To meet target recall, we compute total fn , the total number of false negative annotations needed, and remove the bottom total fn annotations in the confidence ranked list.
We believe that the macro error model is a more realistic simulation of the behavior of automatic recognizers than its micro counterpart in that most automatic recognizers have varying performance characteristics among the types they cover. This is mostly due to the fact that some seman-tic types are inherently more confusable with others, while certain semantic types (such as PhoneNumber and URL ) have very unique lexical features, and thus can be recog-nized with high precision and recall fairly easily regardless of approaches taken.
The key difference between the macro error model and the macro with replacement error model is that in the former, the processes for introducing false positives and false nega-tives are independent of one another while in the latter they are intertwined. More specifically, in the previous model, when introducing false positives, an erroneous annotation is made in addition to an existing correct annotation (i.e., the text span is annotated with multiple semantic types), while in this model, the incorrect annotation replaces the original correct annotation when possible. 5
The first step in the degradation process computes corpus-level confidence and confusability information as in the pre-vious model. In the second step where false positive an-notations are introduced, Each of the top total fp scoring annotations in the confusability ranked list is removed and an incorrect annotation of a likely confusable type is added. The removal of the correct annotation continues until the number of annotations removed exceeds the total number of false negatives ( total fn ) to be introduced in step three, after which the text spans will again be doubly annotated as in the previous model. In step three, we deduct the number of annotations already removed in step two from total fn and remove the remaining number of correct annotations from the bottom of the confidence ranked list.

In contrast to the macro error model, which simulates rec-ognizers that target higher recall by making multiple anno-
In our error models, we do not introduce false positive annotations on previously unannotated text spans for two reasons. First, computing confusability information on all unannotated text spans is computationally expensive, par-ticularly for multi-word spans. Second, our experience with analyzing the errors of automatic recognizer behavior on our semantic types, recognizers are more likely to erroneously assign the semantic type of a text span than to produce an annotation on an untyped text span. Where did Rocky Marciano live? How many customers does Telefonica of Spain have? Figure 3: Target Information Type Specification Ex-amples tations in cases of high ambiguity, the macro with replace-ment error model better simulates those recognizers that select their most confident semantic type for annotating a given text span.
As discussed earlier, we investigate two uses of XML Frag-ments for semantic search which showed significant perfor-mance improvement over baseline in our previous experi-ments [3], namely, target information type specification and relation specification. In this section, we explore the two uses separately, making use of annotated corpora automati-cally generated using the error models described above. Our experiments and analysis focus on 1) characterizing degrada-tion in semantic search performance in relation to degrada-tion in recognizer performance, 2) estimating the semantic search performance improvement achievable by recognizers performing near the current state of the art, 3) projecting how semantic search performance will improve as the per-formance of automatic recognizers improve, and 4) identify-ing characteristics of automatic recognizers that make them more amenable to obtaining greater improvement on these tasks.
This experiment evaluates the impact of entity recogni-tion performance on semantic search performance when us-ing XML Fragments to represent the semantic type of the target information sought by the user. Our test set is the 50 questions and relevant judgments in the TREC 2005 QA track document task [18] and the reference corpus is the AQUAINT subcorpus annotated with 24 entity types. We removed 9 questions from the test set where the desired an-swer type is not represented in the 24 entity types, resulting in a final test set of 41 questions. For each question in the test set, we manually generated a baseline query comprised of keywords in the question, and a semantic search query which is the baseline query plus the desired answer type. The outcome of this experiment enables us to draw con-clusions about how allowing users to explicitly specify the semantic type of the information they are seeking helps im-prove their retrieval results. Figure 3 shows two questions and their respective baseline and semantic search queries.
To evaluate these queries against corpora of varying de-grees of annotation quality, we applied the degradation pro-cesses associated with the three error models for entity rec-Figure 4: Target Information Type Evaluation Sum-mary ognizers discussed in Section 5.2 to the gold standard anno-tated AQUAINT subcorpus, decreasing precision and recall values independently from 1.0 to 0.5 at 0.1 intervals. This resulted in 36 annotated corpora for each of the three er-ror models. Each corpus was indexed and semantic search queries were issued against the index to retrieve the top 100 documents associated with each query in the annotated cor-pus. Additionally, baseline queries were issued against an indexed corpus to obtain baseline performance for compar-ison purposes. 6
We evaluated 7 all 108 semantic search runs and the base-line run against judgments released by NIST for the test set, using MAP and Exact Precision scores, which are the two measures under which significant improvements were obtained in our previous experiments. Note that the more relevant metric for our high precision semantic search ap-proach is Exact Precision, while we report the performance using MAP because it is a widely adopted metric for evalu-ating search results.

Figures 4(a) and 4(b) show summaries of our results for the two metrics, using as representative data points where precision equals recall for entity annotation performance. Overall, there is significant performance gain in MAP in
All indexed corpora yielded the same results for the base-line queries since the indices differed only in entity annota-tions which were not present in the baseline queries.
Using trec eval 8.1: http://trec.nist.gov/trec eval. Table 2: Target Information Type Evaluation Re-sults at P=0.8;R=0.8 MAP 0.6631 0 . 7087  X  0 . 7118  X  0.6491
Exact P 0.4195 0 . 4853  X  0 . 4738  X  0 . 4712  X   X  p&lt; 0 . 05;  X  p&lt; 0 . 005;  X  p&lt; 0 . 001;  X  p&lt; 0 . 0001 Figure 5: Precision vs. Recall for Target Informa-tion Type Specification; Baselines: MAP=0.6631; Exact P=0.4195 most cases, compared to a baseline of 0.6631, as well as sig-nificant gain in Exact Precision in all cases, compared to a baseline of 0.4195. We further examined the data points where precision and recall are both 0.8, a conservative lower-bound performance figure for state-of-the-art entity recog-nizers on standard test sets [16, 6]. Table 2 shows semantic search performance against the annotated corpus generated using our macro error model with precision and recall of 0.8, demonstrating a 6.9-7.3% improvement in MAP (with the exception of Macro Repl) and a 12.3-15.7% improvement in Exact Precision compared with baseline performance. All performance improvements reported in the table are statis-tically significant beyond the 95% confidence level. 8 9
Figures 4(a) and 4(b) further show that although at the same recognizer performance level, the errors introduced by some error models impacted semantic search performance more than others, all of them, as expected, showed degrada-tion in semantic search performance as the annotation pre-cision and recall decreased. We believe the two macro error models more realistically simulate the behavior of automatic recognizers than the micro error model. Furthermore, our figures clearly show that the macro error model consistently outperforms the macro with replacement error model. This suggests that, for purposes of corpus analysis for query time target information type specification, automatic recognizers that multiply annotate a text span in the face of confusable semantic types should be preferred over those that select the single most confident semantic type for annotation.
Using the Wilcoxon signed rank test in all experiments.
Figure 4(b) shows an Exact Precision gain of 19% (0.4195 to approximately 0.5), which is less than the 37% improve-ment reported in [3]. This is because our earlier results were obtained on a 1000-document hitlist, compared with the cur-rent 100-document hitlist. The larger hitlist size resulted in a lower baseline, hence a larger relative gain.
 Find documents about WMD weapons owned by Syria Find documents in which Reagan visits Brazil
So far in our analysis, we have selected data points where annotation precision equals recall as representatives. Fig-ure 5 shows the relative impact of precision and recall on semantic search performance using the macro error model. The two  X  X =1.0 X  lines show impact of recall degradation when precision is held at 1.0 and the two  X  X =1.0 X  lines show impact of precision degradation when recall is held at 1.0. The upper of the bottom two lines shows that with perfect precision, degradation in recall has almost no im-pact on search performance using the Exact Precision met-ric. This is hardly surprising as the metric disregards recall altogether. The lower line shows successive degradation in semantic search performance with perfect recall and grad-ual decrease in precision. It is worth noting that even at P=0.5 and R=1.0 (lowerright-most data point in Figure 5), the Exact Precision score of 0.4421 is still substantially bet-ter than the baseline of 0.4195. Examination of the top two lines in Figure 5 shows that when recall is 1.0 (upper line), precision degradation leads to graceful decrease in seman-tic search performance in MAP score; however, the lower line shows that with perfect precision, degradation in recall has far greater impact on search performance, particularly when recall falls below 0.7. In fact, while for perfect re-call, semantic search MAP score is still above the baseline of 0.6631 when precision is 0.5, for perfect precision, MAP drops below baseline between recall values of 0.6 and 0.7. This suggests that for the target information type specifica-tion task, automatic recognizers that emphasize recall over precision are likely to yield greater gain in MAP score.
This experiment evaluates the impact of relation recog-nition performance on semantic search performance when using XML Fragments to relate query terms. Our test set consists of 25 short descriptions of information needs and associated judgments that we developed for our previous experiments [3]. Our reference corpus is the CNS subcorpus annotated with 19 entity types and 10 relation types. Each description in our test set can be represented as one of the 10 relations with binary arguments of keywords and/or entity types. We manually generated, for each description, a base-line query which consists of keywords and/or entity types that address the information need, and a relation query in which query terms are associated with one another by a relation type. Figure 6 show two descriptions and their re-spective baseline and relation queries.
To evaluate these queries against corpora of varying de-grees of annotation quality, we applied the degradation pro-cesses associated with the six error models for relation rec-ognizers 10 discussed in Section 5.2 to the gold standard an-notated CNS subcorpus, decreasing relation annotation pre-cision and recall values independently from 1.0 to 0.5 at 0.1 intervals. This resulted in 36 annotated corpora for each of the six error models. Each corpus was indexed and rela-tion queries were issued against the index to retrieve the top 20 documents for each query in the annotated corpus. Ad-ditionally, baseline queries were issued against an indexed corpus to obtain baseline performance for comparison pur-poses. 11 These results represent the impact of relation an-notation degradation on relation search performance given perfect entity annotations. To examine how search per-formance degrades when coupling relation recognizers with state-of-the-art entity recognizers, we applied the macro er-ror model with entity precision and recall both at 0.8 to the gold standard CNS subcorpus to obtain an entity-degraded CNS subcorpus. The whole relation degradation experiment was then repeated on this entity-degraded subcorpus.
We evaluated all 216 relation search runs and the baseline run against our judgments for both the gold standard sub-corpus and the entity-degraded subcorpus experiments. We again evaluated the results using MAP and Exact Precision.
For the most part, the results from the two sets of ex-periments show similar trends. Because of space reasons, we focus only on the entity-degraded experiment as it more realistically represents expected performance when employ-ing automatic recognizers. Figures 7(a) and 7(b) show our results for MAP and Exact Precision, respectively, when re-lation annotation degradation was performed on the entity-degraded CNS subcorpus with entity precision and recall of 0.8. These figures show that there is significant performance improvement in both MAP and Exact Precision in nearly all cases compared to the MAP baseline of 0.5756 and the Exact Precision baseline of 0.3822.

Relation recognition is a much more difficult and much less studied task to date than entity recognition. Systems for relation extraction have generally reported F-score per-formance of between 0.5 and 0.7 depending on algorithms and test sets [12, 5, 20]. Based on these performance fig-ures, we further examined our data points where relation precision and recall are 0.6. Table 3 shows that with all six models, our relation search produced an improvement in MAP compared to baseline, with the best performing model achieving a 30.5% improvement. Three out of the six models produced statistically significant improvements above the 95% confidence level. Using the Exact Precision metric, relation search yielded very substantial performance increase over the baseline in all cases, ranging from 84.7% to 111.5%. These results are particularly significant in ap-plications where search results are presented for end user consumption as it increased the relevance ratio of returned documents from just over one in three to over 80%. Since automatic relation recognition is still very much in its in-
Lexical vs. named entity confusability for each of the mi-cro, macro, and macro with replacement error models.
Again, since the corpora only differ in relation annotations which are absent in baseline queries, all annotated corpora yielded the same results for the baseline. Figure 7: Relation Evaluation Summary Using De-graded Entity Annotations fancy, we took a more conservative approach and addition-ally examined data points where precision and recall are both 0.5 (results not shown in table). Our results in MAP showed that differences in performance compared with the baseline range from a slight 1.6% decrease to a 12.1% in-crease, although none of the differences is statistically sig-nificant. Results measured with Exact Precision, however, remained strong even with the lower relation annotation ac-curacy, with improvements ranging from 80.0% to 99.5%, all of which significant above the 99.9% confidence level. Our results show that even though current relation recognition performance is substantially lower than entity recognition performance, its accuracy is sufficient for producing signif-icant gain for semantic search applications. We expect its impact to further increase as the quality of automatic rela-tion recognizers continue to improve.

Figures 7(a) and 7(b) show that relation search perfor-mance decreases as relation annotation performance degrades, as expected. The clearest trend when comparing among the different error models is that the named entity con-fusability models (shown in dotted lines) uniformly outper-form their lexical confusability counterparts (shown in solid lines). This suggests that relation recognizers that empha-size semantic types of relation arguments produce relation annotations that are more amenable to the relation search task. Additionally, the gap in semantic search performance between the macro error model and the macro with replace-ment error model in our previous experiment did not surface in the current experiment. In fact, the results for the two  X  0 . 8085  X  0 . 7842  X  0 . 8085  X  Figure 8: Precision vs. Recall for Relation Specifi-cation; Baselines: MAP=0.5756; Exact P=0.3822 models when adopting the better performing named entity confusability variant were nearly identical, resulting in su-perimposed lines in the two graphs.

Finally, Figure 8 shows the relative impact of relation pre-cision and recall on relation search performance using the macro error model with named entity confusability. Here the results mostly mirror those shown in Figure 5. For MAP, annotation recall again showed stronger impact than preci-sion, as shown in the bottom two lines in the graph. For Ex-act Precision, precision again impacted search performance more than recall, as demonstrated by the top two lines. Overall, we observe that leveraging our relation specification capability in query formulation produces significantly better search results than baseline queries without relations. The performance difference is particularly significant using the Exact Precision metric, whose score approximately doubles when evaluating with simulated entity and relation recogniz-ers whose precision and recall are both 0.8 and 0.5, respec-tively. These results demonstrate the feasibility of adopting state-of-the-art automatic recognizers to improve the preci-sion of search results for appropriate applications.
In this paper, we investigated the impact of named en-tity and relation recognition accuracy on semantic search performance through automatic annotation degradation of gold standard annotated corpora. We focused on two previ-ously studied use cases for applying XML Fragments to se-mantic search, namely, target information type specification and relation specification. Through the application of mul-tiple error models that simulate errors made by automatic recognizers, we identified characteristics of automatic recog-nizers that make them more amenable to each of our two semantic search use cases, and showed that in both cases, recognizer recall has more significant impact on search per-formance than its precision when adopting the MAP metric. Our experimental results further demonstrated that signifi-cant document retrieval gain can be achieved when adopt-ing named entity and relation recognizers with near state-of-the-art performance, achieving a 15.7% improvement in Exact Precision in the target information type specification use case and a 111.5% improvement in Exact Precision in the relation specification use case. Our study substantiates the feasibility of adopting state-of-the-art entity and relation recognizers to improve search precision and projects further gain in semantic search performance as automatic recognizer performance improves beyond the current state of the art. We would like to thank Eric Brown, Dave Ferrucci, and David Gondek for helpful discussions and on the use of the Annotation Database. This work was supported in part by the Disruptive Technology Office (DTO) X  X  Advanced Ques-tion Answering for Intelligence (AQUAINT) Program under contract number H98230-04-C-1577. [1] A. Z. Broder, Y. S. Maarek, M. Mandelbrod, and [2] D. Carmel, Y. S. Maarek, M. Mandelbrod, Y. Mass, [3] J. Chu-Carroll, J. Prager, K. Czuba, D. Ferrucci, and [4] W. B. Croft and D. D. Lewis. An approach to natural [5] A. Culotta and J. Sorensen. Dependency tree kernels [6] R. Florian, H. Jing, N. Kambhatla, and I. Zitouni. [7] J. Gonzalo, F. Verdejo, I. Chugur, and J. Cigarran. [8] B. Katz and J. Lin. Selectively using relations to [9] R. Mihalcea and D. Moldovan. Semantic indexing [10] R. Mihalcea and D. Moldovan. Document indexing [11] J. Prager, E. Brown, A. Coden, and D. Radev. [12] D. Roth and W.-T. Yih. A linear programming [13] M. Sanderson. Word sense disambiguation and [14] A. F. Smeaton, R. O X  X onnell, and F. Kelledy. [15] J. Tiedemann. Integrating linguistic knowledge in [16] E. F. Tjong Kim Sang and F. De Meulder.
 [17] E. M. Voorhees. Using WordNet to disambiguate word [18] E. M. Voorhees and H. T. Dang. Overview of the [19] C. Zhai, X. Tong, N. Milic-Frayling, and D. Evans. [20] S. Zhao and R. Grishman. Extracting relations with
