 1. Introduction
Estimating query language model is an important task in language modeling (LM) approaches, since the query language and vice versa.

For example, suppose that there are two queries q 1 and q sure the retrieval performance of a query model estimation method. Assume that we have two estimation methods A and B, 0.01, and VAP for B is 0.0676. It turns out the VAP B is greater than VAP  X  over all estimated values.
 performance bias represents the gap between the actual mean performance and the mean performance target. For the exam-ple in Table 1 , assumes that the target AP (i.e., the upper-bound AP) can be 0.7 and 0.2 for queries q mance bias and variance, respectively. 1
In addition to the performance bias X  X ariance, we also formulate the estimation bias X  X ariance to measure the estimation quality of an estimated query model with respect to the true query model. Assume that the true information need can be represented by a set of truly relevant documents. Then, the true query model can be generated from truly relevant docu-mated model can be measured by the KL-divergence between the estimated model and the true model. The estimation bias estimated query model itself (i.e., its estimation quality).
 query model complexity, query model combination, document weight smoothness, non-relevant document removal). We iance tradeoff will occur and how the bias and variance can be reduced simultaneously. Based on the analysis, a set of hypotheses is formed. We then carry out extensive experiments based on TREC datasets to systematically evaluate the hypotheses based on the bias X  X ariance analysis. Experimental results on performance bias X  X ariance can generally verify formulation and the general principles of bias X  X ariance analysis. The experimental results on estimation bias X  X ariance can verify the hypotheses on the occurrence of bias X  X ariance tradeoff, but do not fully support hypotheses regarding the estimated query model may over-fit the relevant feedback documents, but may not fit the relevant documents that do not appear in the feedback document set. It can demonstrate that the improved retrieval performance cannot always guarantee the improvement of the estimation quality.

The proposed bias X  X ariance analysis is expected to form an analysis framework and potentially a novel evaluation strat-modeling factors (e.g., model complexity or model combination) and propose hypotheses on the bias X  X ariance tradeoff or bining retrieval effectiveness and stability.

The rest of the paper is organized as follows. In the next section, we present a literature review on the query language research directions. 2. Literature review
Zaragoza, 2009 ), while another way is from the query-generation perspective, leading to the language modeling approach in this paper is on the language modeling (LM) approach.
 mulation in early LM approaches, where the query representation is the original query language model estimated by the the query and its relevant documents are random samples from an underlying relevance model R . In practice, RM estimates an expanded query language model, which is generated from pseudo-relevant documents, rather than truly relevant docu-paper) can be generated from the truly relevant documents given a query.
 stable across different individual queries ( Collins-Thompson, 2009a ). The expanded query model may perform less effec-have been proposed to improve the robustness of query expansion. Tao and Zhai (2006) proposed a method to integrate the original query with feedback documents in a probabilistic mixture model and then regularize the parameter estimation. Li and Zhai (2009) proposed to adaptively combine the original query and the feedback information. Collins-Thompson and
Callan (2007) investigated the uncertainty of feedback-based query models and proposed to resample different feedback document models using Bootstrap sampling. In ( Collins-Thompson, 2009b; Dillon &amp; Collins-Thompson, 2010 ), the risk and reward tradeoff and optimization for query expansion were discussed. Lv, Zhai, and Chen (2011) proposed a Feedback-Boost method to improve the robustness of the expanded query model.
 trieval performance than to the estimation quality with respect to the true query model. comes from different per-topic AP values measured from different simulated document collections. They simulate a number off to investigate the retrieval effectiveness and stability across topics/queries.

The proposed bias X  X ariance analysis is different from the existing mean X  X ariance analysis in document ranking ( Wang, relevance score, while the bias and variance in our paper are associated to the retrieval performance and estimation quality. Moreover, in mean X  X ariance analysis, the relationships between mean and variance have not been explored, while in our paper, the relationship between mean and variance is studied by looking at the tradeoff between the bias and variance.
 model may over-fit the relevant feedback documents, but may not fit the other relevant documents which do not appear in the feedback document set. 3. Formulation of bias and variance 3.1. Introduction to bias X  X ariance analysis The bias X  X ariance analysis is a fundamental theory and has been extensively studied in parameter estimation ( Duda, bias X  X ariance decomposition for the squared loss of the estimation.
 be decomposed to bias and variance: where the expectation E is computed over all possible ^ y ; Bias different ( Bishop, 2006; Geman et al., 1992 ) and the estimated value can be considered as a random variable. has higher bias but lower variance, compared with a more complex method ( Geman et al., 1992 ). This means that the ex-pected estimation error of the simple method is often larger than that of the complex one, but the estimated values of the simple method over different samples are more stable than those of the complex one. To reduce the bias and variance above factors that can affect the bias and variance in Section 4.2 . 3.2. Bias and variance regarding retrieval performance sponding data (e.g., query terms, retrieved documents, or relevance judgements if available) as a sample to test the for different queries. 3.2.1. Bias X  X ariance based on actual performance b P Recall that we consider the actual performance (denoted b performance be b P i , and the corresponding performance target be P
P correspond to the estimated query model and the true query model, respectively (see Table 2 ).
Now, let P i b P i be the difference between b P i and P i where m is the number of all queries given a test collection.
 queries: expected performance. For instance, if the average precision (AP) is used as the performance metric, metrics can also be used in Eq. (3) .

E  X  b
P  X  . Let the difference between the actual mean performance and target mean performance can be defined as the perfor-mance bias:
The above Bias  X  b P  X  is equivalent to 1 m P i  X  P i b P mance b P i and the performance target P i over all queries. From Eq. (4) , it turns out that the higher E  X  the smaller performance bias would be, for the same set of queries and the same upper bound performance P . We now formulate the performance variance as mance. Again, in this paper, E  X  b P  X  denotes MAP and Var  X  performance. 3
Variance of AP (VAP) in fact computes the second central moment of AP, by considering the value of AP on different bias X  X ariance framework.
 Now, we can add the bias and variance together, yielding trieval effectiveness and stability, respectively, across all queries.
 be considered as a metric for the retrieval robustness. The bias X  X ariance decomposition of E  X  formulate the effectiveness-stability decomposition of retrieval robustness.

We do not argue that the overall quantity in Eq. (6) can cover every aspect of retrieval robustness in IR. However, it provides a decomposition perspective, which can help us understand and analyze the retrieval robustness. In addition, give us some clues on how to improve retrieval robustness. 3.2.2. Additional bias X  X ariance based on difference between
In the above bias X  X ariance formulation, the random variable is the actual performance b
P can be different for differen queries. Specifically, let and accordingly its target q i = P i P i . Obviously, q i ^ q . Next, we can define the bias of the random variable ^ q as: where E  X  ^ q  X  is an expectation value over all queries:
It shows that Bias  X  ^ q  X  is equivalent to 1 m P i  X  P i We now define the additional performance variance as
If P i is a constant for every query q i , then Var  X  ^ q  X  would be equivalent to Var  X  query. Then, performance targets in terms of hardness of different queries. In other words, for different queries q ven the existence of the system variance associated with P random variable is ^ q , rather than the actual performance impact of the aforementioned system variance on the bias and variance. We can first regularize the actual performance of each query q i . Specifically, we can let where b P 0 i is the regularized actual performance by considering the hardness of a query. Accordingly, the target of nated. We can then define the regularized ^ q i as: where ^ q 0 i represents the regularized difference between the actual performance query q i .
 and (10) , respectively: and
Next, we will present an example to discuss the relationships between performance bias X  X ariance and additional perfor-mance bias and variance. 3.2.3. Examples on different performance bias X  X ariance B is more effective than original model A , regardless of which variable is used.
 variance based on ^ q indicates that A is less stable.

In the further analysis and experiments in the later sections, we will pay more attention to the performance variance based on b P and ^ q 0 . The variance on b P directly computes the variance of the actual retrieval performance 3.3. Estimation bias and variance divergence between the estimated query model b h q and the true query model h process.

For each query q i , we denote the true query model as h q represented by the KL-divergence 4 between the b h q i and h Then, the mean estimation error over all concerned queries can be defined as the expected value of b g : expected estimation error in Eq. (17) represents the bias of the estimation.

More strictly, for each query q i , we can consider b g i corresponds to the case when the estimated query model b h for each query as D  X  h q i j h q i  X  X  0. Therefore, we can denote each g estimation error, implying the higher expected estimation quality.
 For the estimation variance, we can have which represents the variance of the estimation error for different individual queries (i.e., q represents estimation stability.
 By adding the squared bias and variance, we get which can represent the total estimation error. 4. Bias X  X ariance analysis of query language models
In Section 4.1 , we give a brief introduction to some background knowledge of the general language modeling (LM) and query language modeling approaches. In Section 4.2 , we first formulate the true query model, then present a systematic bias X  X ariance analysis of various estimated query models which reflect different key factors that can affect the model estimation. 4.1. Background of language modeling original query representation. It can be formulated as: where p ( q i  X  h d ) is the query-likelihood, q i ( q i ; 1 smoothed language model for a document d . The QL aims to estimate the probability that this document d generates the query q i .
 method, is used to estimate an expanded query language model based on relevance feedback: where b h  X  f  X  q i represents the feedback-based expanded query model, p ( h notes a set of feedback documents that generate the expanded query model, p ( q score, and the normalized QL score serves as the document weight:
In practice, the documents in D are pseudo-relevant feedback documents, i.e., top-ranked documents retrieved by the QL retrieval using the expanded query model.

For any estimated query model, the document retrieval can be based on the negative KL-Divergence ( Lafferty &amp; Zhai, 2001 ) between the estimated query language model b h q i where H  X  b h q i ; h d  X  is the cross entropy between b h model can be regarded as one estimation method for the query language model. 4.2. Analyzing query language models 4.2.1. True query model
We first define a form of the true query model. Assuming that the true information need can be reflected or represented also the motivation behind the true query model in the literature review) as follows: where h q i represents the true query model, D R denotes the set of all truly relevant documents, given the query q weights of all documents in the set D R in Eq. (25) are uniform since they have the same relevance judgements (i.e., 1) given binary judgement values. We think this is a reasonable way of deriving the true query model for the purpose this paper. 4.2.2. Factors affecting bias and variance query model or expanded query model would result in different kinds of estimated query models. Second, we consider dif-els in Eq. (22) .
 mentioned three factors. They are model complexity, model design, and training data size. Regarding query model estima-tion, the difference between original model and expanded model is related to the model complexity. The expanded query that the top-ranked documents are relevant and (2) it often involves more parameters, e.g., the number of expanded query terms or the number of feedback documents. The combination strategy and document weight issue are related to the model not incorporate any machine learning algorithms (e.g., regression or classification) in our current study.
We now briefly mention different estimated query models for which we will analyze. These models query model and expanded query model; (2) combined query model by original and expanded query models; (3) expanded query model with smoothed document weights for the feedback documents; (4) expanded query model with true relevance information, e.g., some known non-relevant documents; and (5) expanded query model with both true relevance information and smoothed document weights. 4.2.3. Original and expanded query models term representation. b h  X  f  X  q i in RM (see Eq. (22) ) represents a feedback-based expanded query model. some individual queries, the inclusion of non-relevant documents in pseudo-relevance feedback set can hurt the perfor-mance. Intuitively, a poor initial ranking (by original query) would include many non-relevant feedback documents that a poor initial performance would become even worse, while a better initial performance would become even better. This can result in the performance variance of the expanded query model being bigger than that of the original one.
For the estimation bias X  X ariance, recall that it is directly related to the divergence/similarity between the estimated query model and the true query model. The original query model original query terms. On the other hand, the true query model h (in Eq. (22) ) do not have such a sparsity problem since they are generated from a set of documents. Due to the range of
KL-divergence in [0,+ 1 ] and the sparsity of the original query model, the scale of D  X  divergence) of the original query model b h  X  o  X  q i will often be much bigger than the expanded model the aforementioned scale difference, the KL-divergence-based estimation variance of the original query model can also be bigger than that of the expanded model. To sum up, in KL-divergence-based estimation bias X  X ariance, the expanded query model often has smaller bias, and can also have smaller variance, compared with the original query model. The trend of estimation bias X  X ariance can be different when we use other divergence metrics, e.g., JS-divergence (JSD). can occur in the estimation bias X  X ariance using JS-divergence. 4.2.4. Combination between original and expanded query models
The combination between original and expanded query models was widely studied in the literature ( Abdul-Jaleel et al., 2004; Li, 2008; Lv &amp; Zhai, 2009; Tao &amp; Zhai, 2006 ). Basically, the combination can be formulated as where b h  X  c  X  q i is the combined query model, k is the combination coefficient of the original query Jaleel et al., 2004 ).

In Section 3.1 , we mentioned that the combination method may reduce the bias and variance simultaneously. Therefore, it kinds of bias X  X ariance formulation.

For performance bias X  X ariance, as discussed previously, one reason why the expanded query model has larger variance is that, for some queries, the performance can be hurt after query expansion when non-relevant terms are brought into query formance on average can be improved, given appropriate combination parameters. To sum up, the combined query model advantages of the original and expanded query models.

With regard to the estimation bias X  X ariance, when k is approaching 1, the combined query model is getting close to the increasing bias but also the increasing variance (along with the increasing k ), as we discussed in Section 4.2.3 . 4.2.5. Expanded query model with smoothed document weights
Recall that in the true query model (see Eq. (25) ), the document weights are kept uniform, leading to the most smooth with smoothed document weights.
 2011 ), which can be formulated as: where f S q i  X  d  X  is the smoothed document weight, S q i trols the smooth degree of document weights. When s = 1, the document weights are unchanged. The larger the s is, the greater degree of the smoothing would be. For example, assuming the original weights are 0.6250 and 0.3750 for d d , and the parameter s is 3, then the smoothed document weights are 0.5425 and 0.4575, meaning the document weight distribution becomes more smooth.

Using the smoothed document weights, the estimated query model can be formulated as: weights e S q i  X  d  X  (see Eq. (27) ).

The above smoothing method can improve the document weight smoothness among relevant documents in the pseudo-relevant feedback (PRF) document set. The improved smoothness can also broaden the topic coverage of the expanded query, in order to prevent too many weights on the topics represented in topmost documents which might be non-relevant. It has been shown that properly smoothing the document weights (with moderate smoothing parameters) can improve the effec-other hand, for some individual queries, smoothing may affect the discriminativity between the relevant documents and non-relevant document in the PRF document set. For instance, if too much smoothing is imposed and the weights of every ing the weights of feedback documents (with moderate smoothing parameters) can improve the retrieval effectiveness, but formance bias X  X ariance tradeoff.

The document weight smoothing can play a bigger role in reducing the estimation bias/variance, than in reducing the per-relevant documents are the same (i.e., very smooth). The smoothing method can improve the smoothness among relevant documents (in the feedback documents), which makes the estimated query model closer to the true one. 4.2.6. Expanded query model with available non-relevant data
One of the reasons for the stability problem of query expansion is that the expanded query model is often generated from a certain ratio (denoted as parameter r n ) of non-relevant documents is known. We then derive an expanded query model based on RM with part of known non-relevant documents (denoted as D where b h  X  n  X  q i is the estimated query model, D D N is the set of remaining documents, and S weight computed by the normalized QL score (see Eq. (23) ). Note that the non-relevant documents are selected in a top-down manner from the initial ranking of feedback documents, since the top non-relevant documents with bigger document weights have more influence on the query expansion.
 As the parameter r n increases, more non-relevant documents can be removed from the PRF document set, meaning the
PRF documents are purer to be truly relevant. It also means that we have more relevance judgements as r bias X  X ariance and estimation bias X  X ariance. 4.2.7. Expanded query model with document weight smoothing and non-relevant data
Now, let us consider the idea of combining the use of both relevance information (Section 4.2.6 ) and document weight smoothing (Section 4.2.5 ). We then come up with the estimated query model as follows.
If one removes all non-relevant documents (i.e., r n = 1) in Eq. (29) , the process to smooth the document weights (with increasing smoothing parameter s ) can be considered as an attempt to gradually approach the true query model in Eq. and variance can drop simultaneously along with the increasing smoothing parameter s . 5. Experiments
In this section, we are going to evaluate each estimated query model described in the previous section. We first summa-rize a number of hypotheses, drawing on the analysis in Section 4.2 . 5.1. Hypotheses h 1: For the original query model and the expanded model by RM, the performance bias X  X ariance tradeoff will occur. The estimation bias X  X ariance tradeoff may not occur when using KL-divergence. h 2: For the combined query model, the performance bias X  X ariance tradeoff will occur. A proper combination coefficient can reduce the performance bias and variance simultaneously. The KL-divergence-based estimation bias X  X ariance tradeoff may not occur. h 3: For the smoothed query model, the performance bias X  X ariance tradeoff will occur. Compared with the performance bias and variance, the estimation bias X  X ariance tradeoff is less likely to occur. h 4: For the expanded query model with available true relevance information (e.g., explicit relevance feedback formance bias and variance can be reduced simultaneously. The estimation bias and variance can be also reduced simultaneously. h 5: For the expanded query model with available true relevance information and document weight smoothing, there is a simultaneously.
 on different evaluation factors in the experiments. 5.2. Evaluation set-up
The evaluation involves four standard TREC collections, including WSJ (87 X 92, 173,252 documents), AP (88 X 89, 164,597 queries 151 X 200, while the ROBUST 2004 and WT10G collections are tested on queries 601 X 700 and 501 X 550, respectively. stemmed using the Porter stemmer and stop words are removed in the indexing process.

The first-round retrieval is carried out by a baseline language modeling (LM) approach, i.e., the query-likelihood (QL)
After the first-round retrieval, the top n ranked documents are selected as the pseudo-relevance feedback (PRF) docu-of expanded terms is fixed as 100. For any query model (including the original one), 1000 documents are retrieved by the negative KL-divergence measure. 5.3. Evaluation metrics
Average precision (AP) is used as the performance metric for each query q to measure the retrieval effectiveness over a set of queries. Then, in Eq. (3) , E  X 
Var  X  ^ q 0  X  in Section 3.2.2 ) based on ^ q 0 , i.e., the regularized ^ q .
The summed quantity of performance bias and variance (e.g., the E  X 
E  X  b P P  X  2 as bias 2 + var .
 model).

The estimation bias X  X ariance directly tests the estimation quality of each query model with respect to the true query gence based Bias  X  b g  X  and Var  X  b g  X  . 5.4. Bias X  X ariance results for different query models 5.4.1. Original and expanded query models
As we can see from Table 4 , on four collections, the expanded query models computed by RM are more effective than the icantly outperforms QL in MAP on every collection.
 trieval effectiveness and stability.
 tradeoff supports our hypothesis h 1 in Section 5.1 .

Now, we look at the results of bias 2 + var plotted in the first row of Fig. 1 . As mentioned in Section 5.3 , bias than the expanded query model on WSJ8792, AP8889 and ROBUST2004. On WT10G, the original query model is slightly less robust than the expanded query model.
 ranking, is not applicable to the original query model.
 hypothesis h 1.

Fig. 2 shows the results about the estimation bias and variance of the original and expanded query models, where k =1 corresponds to the original query model based on query likelihood (QL) and k = 0 corresponds to the expanded query model plotted the JS-divergence based Bias  X  b g 0  X  and Var  X  b g 0  X  , which has a clear tradeoff on all collections. model and the range of KL-divergence in [0,+ 1 ] as we discussed in Section 4.2.3 .
 5.4.2. Combined query models with different combination coefficient
Here, we evaluate the combined query model, 8 which is the combination (see Eq. (26) ) of the original query model (when the combination coefficient with respect to the original query model and k is chosen from the interval [0,1]. iance can be reduced. The above observation is consistent with our hypothesis h 2 in Section 5.1 .
The original query model only contains original query terms. Then, original query terms in the original query model have much bigger weights than those in the expanded query model. Therefore, a small k (e.g., 0.1) can adjust the weights of original query terms in the expanded model, while preventing the expanded model from being dominated by original query terms. Evaluation results regarding the robustness metric bias with a small k (e.g., 0.1) can be the most robust one among the models with different k values. shows similar trends with the bias X  X ariance results based on AP, and support the hypothesis h 2. (see the discussions in Section 4.2.3 ). 5.4.3. Expanded query model with smoothed document weights
Now, we evaluate the expanded query model by RM with smoothed document weights (described in Section 4.2.5 ). Recall that the bigger the smoothing parameter s is, the more smooth the document weights would be. For RM, we can consider its smoothing parameter s as 1, meaning the document weights remain unchanged. Therefore, RM corresponds the smoothed model when s =1in Fig. 4 .
 and ROBUST2004, and drops on WT10G. To sum up, we can observe a clear bias X  X ariance tradeoff on each collection. The above evidence supports our hypothesis h 3.
 We now explain why the observations on WSJ8792, AP8889 and ROBUST2004 are different from those on WT10G.
Smoothing can help improve the smoothness of relevant feedback documents in generating the estimated query model. Intu-ful. The initial ranking performance averaged over all queries on WSJ8792, AP8889 and ROBUST2004 is better than that on and reduce performance bias.

Even if the mean performance on WSJ8792, AP8889 and ROBUST2004 is improved, the performance of some individual expanded model in Fig. 4 . The variances of the original query model (see VAP of QL in Table 4 ) on WSJ8792, AP8889 and ROBUST2004 are all higher than that on the WT10G.

BUST2004 and WT10G, the bias X  X ariance tradeoff obviously occurs. On WSJ8792, compared with the RM-based expanded tradeoff.

We now report the estimation bias X  X ariance results plotted in Fig. 5 , where s = 1 corresponds to expanded query model imposed on the document weights.

We observe that document weight smoothing can help reduce the estimation bias. Regarding the variance, Fig. 5 shows
Section 4.2.5 , document weight smoothing can play an important role in the reduction of estimation bias and variance. 5.4.4. Expanded query model with available non-relevant data
In this subsection, we carry out experiments for the expanded query model by RM with part of non-relevant data avail-able. According to Section 4.2.6 . a certain percentage (denoted as r and we simply remove those non-relevant documents (see Eq. (29) ) in generating the query model. Thus, the expanded query model (by RM only) corresponds to the model when r n
Let us see the performance bias X  X ariance plotted in the first row of Fig. 6 , where parameter r increment 0.1. It clearly shows that on WSJ8792, AP8889, ROBUST2004, performance bias and variance (based on AP) can be reduced simultaneously. The above evidences support our analysis in Section 4.2.6 and the hypothesis h 4. documents in the feedback document set. For those queries, after removing some non-relevant ones, most remaining doc-uments could be still non-relevant and the room for performance improvement is very small. Meanwhile, there may exist some other queries for which the performance improvement can be bigger. As a result, the performance variance will be increased.
 non-relevant documents have a good combined effect of effectiveness and stability on each collection. h 4.

Now, we evaluate the estimation bias and variance of the expanded query model by RM with non-relevant documents available. The results are plotted in Fig. 7 . It is expected, that by increasing r in RM), the estimation quality of the estimated query model can be improved. Note that the expanded query model by RM
Var  X  b g  X  has a increasing trend, along with the increasing r the bias and variance can be reduced simultaneously.

Note that the true query model used for estimation bias X  X ariance evaluation is derived from all the truly relevant doc-(25) to generate the true query model, which shows that the estimation bias and variance can be often reduced simultaneously.
 based on relevant feedback documents, but may get far away from the true query model based on all relevant documents. In other words, the estimated query model can over-fit the relevant feedback documents, but may not fit the other relevant documents which do not occur in the feedback document set.
 5.4.5. Expanded query model with relevant feedback documents and document weight smoothing all the non-relevant documents and smoothing document weights.
 performance variance (based on AP) can increase a little bit on WSJ8792, ROBUST2004 and WT10G. This is because when s simultaneously reduced, which supports the hypothesis h 5. Fig. 9 also shows that as s increases, bias support the hypothesis h 5.

Bias parameter s increases. 6. Conclusions and future work 6.1. Conclusions the retrieval performance by directly measuring how closely an estimated query model can approach the true query model derived from the truly relevant documents. This leads to the estimation bias X  X ariance formulation, which is based on the divergence between the estimated query model and the true query model.

Based on four query modeling factors, i.e., query model complexity, query model combination, document weight smooth-ness and non-relevant documents removal, we analyze a number of representative query model estimation methods and of bias X  X ariance tradeoff. In the experiments, we have explained when the bias X  X ariance tradeoff can occur, and when the formance bias and variance simultaneously.

The experimental results of the estimation bias X  X ariance support the hypotheses h 1 h 3, but do not support the hypoth-when the bias and variance can be reduced simultaneously. The results in Section 5.4.4 show that the non-relevant docu-egy may over-fit the relevant feedback documents, but may not fit the other documents that do not appear in the feedback document set, thus can move away from the true query model defined in Eq. (25) . After we have removed all non-relevant documents in the feedback document set, we then use the document weight smoothing to improve the estimation quality. most remains unchanged. The total estimation error (summed over the estimation bias and variance) can be reduced by the document weight smoothing method.

The above observations show that improving retrieval performance do not guarantee the improvement of the estimation the retrieval performance only. 6.2. Potential impact and future work them, to reflect how the retrieval robustness should be decomposed differently in different scenarios. Moreover, we will investigate other performance metrics (e.g., logAP) in the performance bias X  X ariance analysis.
Based on the proposed bias X  X ariance analysis and evaluation methodology, we can study other query language model tions). The combination of two query models can be extended to the combination/ensemble of multiple (tens or hundreds) rankers in the web search scenario. In machine learning, ensemble learners can reduce both bias and variance simulta-neously. In principle, the ensemble learners correspond to the combined rankers. As another example, the document weight smoothness can be related to the diversity of topic coverage of feedback documents. Further, we may explore non-relevant documents removal in the implicit feedback or interactive feedback scenario.

Moreover, we can explicitly formulate the bias X  X ariance for the social and personalized search where the tradeoff be-for the above interesting research directions.
 Acknowledgement
National Program on Key Basic Research Project (973 Program, Grant Nos. 2013CB329304, 2014CB744604), the Natural Sci-ence Foundation of China (Grant Nos. 61272265, 61070044, 61105702), and EU X  X  FP7 QONTEXT project (Grant No. 247590). References
