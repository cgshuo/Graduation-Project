 bound quality, and empirical performance.
 function, and are expensive to update when the underlying di stance function changes. We compare our algorithm to related existing methods using a variety of standard data sets. We show that our method outperforms existing approaches, and e ven performs comparably to several 1.1 Related Work for other machine learning problems outside of metric learn ing, e.g. [10, 2, 12]. work has considered hash functions for L to locality-sensitive hash keys for online database mainte nance, as we propose in this work. implement it, and prove regret bounds. 2.1 Formulation and Algorithm them is defined as the matrix A = G T G , and distributing G into the ( u  X  v ) terms.
 is, we assume that at time step t , there exists a current distance function parameterized by A constraint is received, encoded by the triple ( u t , v t , y A , we first predict the distance  X  y loss  X  ( X  y the losses over all time steps, i.e. L ( to In that case, the corresponding loss function is  X  ( X  y A typical approach [10, 4, 13] for the above given online lear ning problem is to solve for A minimizing a regularized loss at each step: where D ( A, A we use the LogDet divergence D definite matrices and is given by D positive definiteness, and a maximum-likelihood interpret ation.
 Existing approaches solve for A  X   X  ( d A ( u t , v t ) , y t ) is approximated by  X   X  ( d A to maintain positive definiteness [4].
 In contrast, our algorithm proceeds by exactly solving for the updated parameters A Using straightforward algebra and the Sherman-Morrison in verse formula, we can show that the resulting solution to the minimization of (2.1) is: where z t = u t  X  v t and  X  y = d of noting that  X  y update (2.2) if the new constraint is violated.
 It is possible to show that the resulting matrix A  X  erty for online algorithms. In contrast, by minimizing the f unction f (Section 2.2) and empirical performance are notably strong er. We refer to our algorithm as LogDet Exact Gradient Online (LE GO), and use this name throughout 2.2 Analysis issues, we do not present the full proofs; please see the long er version for further details. (which has access to all constraints at once). Let  X  d points u t and v t with a fixed positive definite matrix A  X  , and let L suffered over all t time steps. Note that the loss L L optimal offline solution, i.e. it minimizes total loss incur red ( L the loss of the online algorithm L we now show that L In the result below, we assume that the length of the data poin ts is bounded: k u k 2 The following key lemma shows that we can bound the loss at eac h step of the algorithm: Lemma 2.1. At each step t , where 0  X   X  Proof. See longer version.
 Theorem 2.2.
 where L tion (2.3) , A Proof. The bound is obtained by summing the loss at each step using Le mma 2.1: The result follows by plugging in the appropriate  X  telescopes to D For the squared hinge loss  X  ( X  y same bound.
 The regularization parameter affects the tradeoff between L the coefficient of L R is small; for example, in the case when R = 2 and  X  = 1 , then the bound is L A  X  (4 + solution with zero error, i.e., L know that L timal solution A  X  , since the bound of POLA has a k A  X  k 2 if we scale the optimal solution by c , then the D rithm since, in the ITML-Online algorithm, the regularizat ion parameter  X  on the input data. An adversary can always provide an input ( u t , v t , y update can prevent ITML-Online from making progress toward s an optimal metric. In summary, we have proven a regret bound for the proposed LEG O algorithm, an online metric update to the online algorithm would require a costly update to their data structures. updating is required after substantial changes to the metri c are accumulated. 3.1 Background: Locality-Sensitive Hashing Locality-sensitive hashing (LSH) [6, 1] produces a binary h ash key H ( u ) = [ h for every data point. Each individual bit h function h h examples are, the more likely they are to collide in the hash t able. retrieve the (1 +  X  ) -nearest neighbors with high probability. 3.2 Online Hashing Updates The approach described thus far is not immediately amenable to online updates. We can imagine producing a series of LSH functions h Mahalanobis distance; when we update our matrix A parameterized by G O ( nd ) time, which may be prohibitive. In the following we propose a more efficient approach. Recall the update for A : A  X  G  X  update to (3.1) is to find the sign of Suppose that the hash functions have been updated in full at s ome time step t Now at time t , we want to determine which hash bits have flipped since t cisely, which examples X  product with some r T G versa. This amounts to determining all bits such that sign ( r T G alently, ( r T G r lent to finding all u such that ( r T G tor  X  u = [( r T G  X  q the bits have changed, we perform a full update to our hash fun ctions. space. We omit details due to lack of space. to guarantee positive definiteness). The final metric ( A learning results using [7, 15]). LEGO and ITML-Online have c omparable running times. However, with LEGO is 16.6 times faster, most likely due to the extra pr ojection step required by POLA. between POLA and LEGO. Note that LEGO beats or matches POLA X  X  test error in 33/45 (73.33%) our approach also fares well compared to other offline metric learners on this data set. We next consider a set of image patches from the Photo Tourism project [14], where user photos measures the distance between image patches better than L from the Notre Dame portion of the data set, and measure accur acy with precision and recall. The right plot of Figure 1 shows that LEGO and POLA are able to l earn a distance function that parameter, which prevents the method from improving over th e Euclidean baseline. In terms of training time, on this data LEGO is 1.42 times faster than POL A (on average over 10 runs). of the number of patches retrieved for four variations: LEGO with a linear scan, LEGO with our LSH updates, the L that the accuracy achieved by our LEGO+LSH algorithm is comp arable to the LEGO+linear scan (and similarly, L online hashing scheme. Moreover, LEGO+LSH needs to search o nly 10% of the database, which translates to an approximate speedup factor of 4.7 over the l inear scan for this data set. Next we show that LEGO+LSH performs accurate and efficient re trievals in the case where con-the static L to both the linear scan method (LEGO Linear) as well as the nai ve LSH method where the hash table is fully recomputed after every constraint update (LE GO Naive LSH). The curves stack up accuracy, LEGO Naive LSH with its exhaustive updates is slig htly behind that, followed by our Conclusions: We have developed an online metric learning algorithm toget her with a method to Acknowledgments: This research was supported in part by NSF grant CCF-0431257 , NSF-ITR award IIS-0325116, NSF grant IIS-0713142, NSF CAREER aw ard 0747356, Microsoft Research, and the Henry Luce Foundation.

