 Haim Avron haimav@us.ibm.com Satyen Kale sckale@us.ibm.com Shiva Prasad Kasiviswanathan spkasivi@us.ibm.com Vikas Sindhwani vsindhw@us.ibm.com IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 USA We consider the following convex optimization prob-lem over matrices: where f ( X ) is any convex function (not necessarily dif-ferentiable),  X  &gt; 0 is a regularization parameter, and k X k  X  denotes the nuclear (trace) norm of a matrix X , which is the sum, or equivalently the l 1 norm, of the singular values of X . We assume without loss of generality that m  X  n . This setup generalizes, from vectors to matrices, the widely successful idea of using l -regularization as a convex proxy for imposing spar-sity constraints. Sparsity in the spectrum of a matrix corresponds to low-rankness, a key modeling idea that is naturally justified in a variety of application con-texts, e.g., recommender systems, topic models, and multi-task learning.
 Two broad lines of research have emerged around the analysis and implementation of matrix estimation al-gorithms based on nuclear norm regularization. One concerns the theoretical characterization of the condi-tions under which an unknown low-rank matrix can be exactly recovered by solving a problem of the form (1), or variations thereof, given a set of partially observed entries with respect to which the function f provides a measure of prediction quality. A complementary line of work, which this paper contributes to, concerns the development of algorithmic frameworks to efficiently solve (1) for large-scale problems. Given the definition of the nuclear norm, the Singular Value Decomposition (SVD) tends to unsurprisingly play a critical computa-tional role in the design of nuclear norm solvers, e.g., the Singular Value Thresholding (SVT) (Cai et al., 2010), Soft-Impute (Mazumder et al., 2010), acceler-ated Proximal Gradient approach (Ji &amp; Ye, 2009) and related efforts, all involve applying a soft-thresholding operator on the singular values of an iterate, which re-quires repeated calls to an SVD solver. In particular, if the iterates need to pass through a region where the spectrum is dense, these techniques can potentially be-come prohibitively expensive. An alternative approach is proposed by Jaggi &amp; Sulovsk  X y (2010) who map (1) to the problem of optimizing a convex function over the set of positive semi-definite matrices with unit trace, for which the Sparse-SDP solver of Hazan (2008) is in-voked. This approach is appealing since each iteration involves the computation of only the largest singular value, and associated left and right singular vectors, of the gradient of f at the current iterate which involves relatively cheap sparse-matrix operations followed by quick rank-one updates. On the other hand, this ap-proach produces an -accurate solution with rank pos-sibly as large as  X ( 1 ) making it challenging to hold the factorization of the solution in memory and apply it in practice for generating predictions. Another class of methods, e.g. (Recht &amp; R  X e, 2011), works with a low-rank parameterization directly; in general, this leads to a non-convex formulation whose solutions may be highly sensitive to initialization.
 The contributions of this paper are as follows:  X  A new stochastic subgradient descent approach to solving (1). By utilizing a novel subgradient prob-ing technique that generates low-rank steps, com-bined with enforcing low-rankness in the iterates, our method is able to do cheap iterates and produce a low-rank solution. Furthermore, instead of using sparse SVD as the main computational kernel, our method uses highly scalable dense matrix operation (QR factorization of a tall-and-skinny matrix) as its basic kernel, so it is better poised to take advantage of the computational power of modern platforms.  X  While the theoretical worst-case running time of our basic algorithm is O mn 2  X  2 , an efficient variant that enforces low-rankness shows, empirically, linear complexity in m , n and r , where the rank of the solution is at most r .  X  We apply our method to matrix completion and show that it compares favorably to state-of-the-art techniques for solving (1) (Jaggi &amp; Sulovsk  X y, 2010; Mazumder et al., 2010; Shalev-Shwartz et al., 2011).
In addition, our approach is more general than these methods in that it applies, without any major mod-ifications, to the broader class of subdifferentiable elementwise loss functions. 1.1. Preliminaries We use i : j to denote the set { i,...,j } , and [ n ] = 1 : n . Vectors are always column vectors and are denoted by boldface letters. We use 0 m  X  n to denote an m  X  n matrix of all zeros. Given R  X  [ m ] and C  X  [ n ], we denote by X R , C the submatrix of X consisting of rows in R and columns in C . For a matrix X  X  R m  X  n with m  X  n ,  X  ( X ) = (  X  1 ( X ) ,..., X  n ( X )) is the vector of singular values of X , with entries in non-increasing order. The nuclear (trace) norm k X k  X  of a matrix X is the sum of the singular values of X . For a matrix X , let rank( X ) denote its rank. When the referenced matrix X is clear, we just use rank to denote rank( X ). Given the SVD of a matrix A  X  R m  X  n with m  X  n , viz. A = U  X  V &gt; , the reduced SVD factorization consists of discarding the last m  X  n columns of U and bottom m  X  n rows of  X . By removing singular triplets associated with zero singular values we get the compact SVD. Truncating a matrix to rank t (or truncated SVD), denoted by TSVD( A,t ) consists of removing the n  X  t smallest singular values from the reduced SVD. It is well known that TSVD( A,t ) is the best rank t approximation to A (spectral and Frobenius norm). A QR factorization of a matrix A  X  R m  X  n is decom-position of A into a product A = QR of an unitary matrix Q  X  R m  X  m and an upper triangular matrix R  X  R m  X  n . If m  X  n , then the reduced QR factoriza-tion consists of discarding the last m  X  n columns of Q and bottom m  X  n rows of R . The QR factorization can be computed efficiently and in a stable manner using O ( mn 2 ) operations.
 We assume the following properties of the function f , which are quite basic and easily satisfied in most ap-plications: 1. A subgradient of f at any point X ,  X  f ( X )  X   X  X f ( X ), is efficiently computable. 2. We know an upper bound  X  on k X opt k F . 3. We know an upper bound G on k X  f ( X ) k F for all
X such that k X k F  X   X . We now describe our stochastic subgradient descent (SSGD) algorithm for solving nuclear norm regularized problems of type (1). Let F ( X ) = f ( X ) +  X  k X k Instead of solving (1), we solve where K = { X  X  R m  X  n : k X k F  X   X  } . Note that X opt  X  X  , so this is problem is equivalent to (1). The reason we use this set K is to make sure our iterates are bounded. This is ensured by projecting on K if we ever step outside of K . The projection onto K is defined as follows: Definition 1 (Projection Operator for K ) . Define  X  K ( P ) = argmin Q  X  X  k P  X  Q k F = min { 1 ,  X  k P k F } P. Let X = U  X  V &gt; be a compact SVD of X . Let U rank ,V rank be U,V truncated to the first rank( X ) columns. It is well known that U rank V &gt; rank is a sub-gradient of k X k  X  (Watson, 1992), so we find that a subgradient of F ( X ) = f ( X ) +  X  k X k  X  is G ( F ( X )) def =  X  f ( X ) +  X   X  U rank V &gt; rank  X   X  This gives the following upper bound on kG ( F ( X )) k F for any X  X  X  : kG ( F ( X )) k F  X  k X  f ( X ) k F +  X  k U rank V &gt; rank The crucial ingredient for a SSGD algorithm is an un-biased estimator for a subgradient. Our technique for computing such an estimator is to probe G ( F ( X )) by multiplying it by a random matrix.
 Definition 2 (Probing Matrix) . A random n  X  k ma-trix Y is a probing matrix if E [ Y Y &gt; ] = I n  X  n where I n  X  n is the n  X  n identity matrix and the expectation is over the choice of Y .
 Here, k n is a parameter that is adjustable in our algorithm. The following lemma gives several fami-lies of distributions that generate probing matrices ef-ficiently. (All missing proofs appear in the full version of the paper (Avron et al., 2012).) Lemma 2.1. Let Y = Z/ trix drawn any one of the following distributions: 1. Independent entries taking values +1 and  X  1 with equal probability 1 / 2 . 2. Independent and identically distributed standard normal entries. 3. Each column of Z is drawn uniformly at random and independent of each other from { Then Y is a probing matrix.
 By linearity of expectation, for any matrix A we have E [ AY Y &gt; ] = A E [ Y Y &gt; ] = A . Thus, in our algo-rithms we use G ( F ( X )) Y Y &gt; as an unbiased estimator of G ( F ( X )). Now, using an unbiased estimator instead of an exact subgradient (that is, using SSGD instead of subgradient descent) is beneficial only if there is some computational advantage in doing so. Our probing technique has two potential advantages over the use of an exact subgradient. First, for any matrix A the ma-trix AY Y &gt; has rank at most k . So our estimator is, in fact, a low-rank unbiased estimator. We will utilize this low-rankness later on. Second, often it is more efficient to compute the product G ( F ( X )) Y than to actually compute G ( F ( X )). For example, if Y is com-posed of scaled identity vectors (case 3 in Lemma 2.1), then G ( F ( X )) Y is a matrix composed of k columns of G ( F ( X )), so we need to compute only a small portion of G ( F ( X )). Henceforth, we use the scaled identity vectors as the default probing matrix. 2.1. Basic SSGD Algorithm Using the above ingredients, we now describe a basic SSGD algorithm for solving (2). We use g ( t ) = G ( F ( X ( t ) )) Y Y &gt; as an unbiased estimator of G ( F ( X ( t ) )). Algorithm Basic-SSGD describes the complete procedure. Basic-SSGD is not efficient in terms of running time and memory requirements. Sub-sequent subsections give more efficient variants. Algorithm 1 Basic-SSGD Input: f ,  X  , T , step sizes  X  (1) ,..., X  ( T  X  1) , and k for t = 0 to T  X  1 do end for We now analyze Algorithm Basic-SSGD .
 Lemma 2.2. Let Y  X  R n  X  k . Then for any matrix In particular, if Y is a probing matrix from Defini-tion 2, then E [ k AY Y &gt; k 2 F ] = n k k A k 2 F . Substituting A = G ( F ( X ( t ) )) in Lemma 2.2 gives Finally, using this in a standard bound on the conver-gence rate of the SSGD algorithm , we get the follow-ing theorem about the convergence rate of the Basic-SSGD algorithm.
 Theorem 2.3. Suppose r is an upper bound on rank( X ( t ) ) for all t . Then if we set  X  ( t )  X  SSGD satisfies
E Thus Basic-SSGD converges to within  X k X opt k F of the optimal goal in O ( n/k 2 ) iterations. If we fix then convergence is in O ( n/k ) iterations. 2.2. Decreasing the Iteration Cost of In each iteration of Basic-SSGD , we have to compute g ( t ) . The straightforward way of computing g ( t ) quires computing the SVD of X ( t ) , which has O ( mn 2 ) time complexity and is computationally prohibitive for large matrices. Fortunately, when the rank of X ( t ) is small we can do much better. Let r ( t ) denote the rank of X ( t ) . Assume that we have a compact SVD de-X (0) = 0 m  X  n , we certainly have such a decomposition for X (0) . We now show how to generate the SVD of X ( t +1) , without explicitly forming X ( t +1) . To do so, we first define a class of loss functions.
 Definition 3 (Sum Loss Function) . A function f : R m  X  n  X  R is a sum loss function if for X  X  R m  X  n , f ( X ) = P ij f ij ( X ij ) where the f ij s are convex func-tions. We say that f is efficiently computable if f ij ( x ) and a subgradient f 0 ij ( x ) of f ij at x can be computed in O (1) time for every i , j , and x .
 Note that the above definition captures many com-monly used functions, including squared error, ab-solute error, hinge loss function (with or without smoothing), and can also apply to arbitrary domain-specific subdifferentiable elementwise loss functions. Lemma 2.4. Suppose that f is an efficiently com-putable sum loss function. Suppose that the probing matrix Y is composed of scaled identity vectors. Given the SVD of X ( t ) , there is an algorithm to compute the itly computing X ( t +1) itself ) in O ( m ( r ( t ) + k ) where r ( t ) is the rank of X ( t ) .
 Proof. We first write a formula for g ( t ) : g By first computing ( V ( t ) ) &gt; Y and then multiplying by U Since Y is composed of scaled identity vectors,  X  f ( X ( t ) ) Y is composed of scaled columns of  X  f ( X ( t ) ). The expansion f ( X ) = P ij f ij ( X plies that (  X  f ( X )) ij = f 0 ij ( X ij ), so to find  X  f ( X we simply have to compute the values  X  f ( X ( t ) ) at the corresponding columns. The entries of a single column of X ( t ) can be computed from the decompo-columns can be computed in O ( mkr ( t ) ). Therefore, S ( t ) can be computed in O ( mkr ( t ) ) time.  X  b U is already a low-rank representation of the matrix. We now show how to turn the low-rank representation b U a reduced QR decomposition of b U ( t +1) = Q U R U and b V we have The decomposition of the inner matrix is, in fact, a reduced SVD decomposition since Q U , M , Q V and N are orthonormal, and  X   X  ( t +1) is diagonal with positive values. Since it is a SVD we can efficiently do the K projection and remove zero (or numerically zero) sin-gular values to obtain the compact SVD decomposition X It is easy to verify that the dominant operation, in terms of running time, is the computation of the QR factorization of b U ( t +1) . Since this is a factoriza-tion of an m  X  ( r ( t ) + k ) matrix, the overall cost is O ( m ( r ( t ) + k ) 2 ), which is much better than O ( mn r ( t ) + k n . For a pseudo-code description, see full version of the paper (Avron et al., 2012).
 Unfortunately, in Basic-SSGD it is quite probable that rank of X ( t +1) is r ( t ) + k , and therefore, after n/k iterations the iterates could become full rank, and from there on updates will take O ( mn 2 ) time per iteration (same iteration cost as the trivial implementation of Basic-SSGD ). We will see how to avoid this situation in the next subsection.
 Scalability. The running time of each iteration is dominated by computing QR factorizations of tall-and-skinny dense matrices. We could have used SVD computations instead of QR computations. Since QR and SVD have equivalent asymptotic running this would not have changed the time complexity of the al-gorithm. However, we chose to use QR factorizations instead since it is a simpler operation that exhibits bet-ter running time in practice . The use of QR factoriza-tion also makes the update operation highly scalable on modern parallel architectures. By using a highly tuned package like ScaLAPACK (Choi et al., 1992) good speedups should be attainable with little effort. Recent research on QR factorization has shown how to implement it efficiently on communication-bound massive parallel machines (Demmel et al., 2008), MapReduce clusters (Constantine &amp; Gleich, 2011), and GPUs (Anderson et al., 2011). It is worth noting that our algorithm avoids the computation of singular values on large sparse matrices, which require more so-phisticated communication-avoiding methods (Hoem-men, 2010), which do not scale as well as dense linear algebra operations.
 2.3. Enforcing Low-Rank Iterates The update algorithm of Lemma 2.4 could be used in Algorithm Basic-SSGD to go from the SVD of X ( t ) sion in the previous section shows that if the iterates ( X ( t ) s) in Algorithm Basic-SSGD are low rank then the iterations are fast. This suggests the idea of explic-itly truncating the least singular values of the iterates to ensure that the iterates always remain low rank. In this section, we formalize this idea.
 Let M r = M m  X  n r denote the set of m -by-n ma-trices of rank at most r . Suppose we assume that rank( X opt )  X  r , i.e., X opt  X  M r , for some param-eter r . Then minimizing F ( X ) over K  X  M r yields the same optimum solution. We look at the prob-lem of solving (2) with the additional constraint that X  X  M r . However, the set M r is non-convex. Fur-thermore, rank constraints, like X  X  M r , typically result in NP-hard problems (see (Natarajan, 1995)). Our strategy is to again use a projected subgradient method. That is, in each iteration we start with a regular stochastic subgradient step. This step takes us out of M r . The following step is to project back to M r . It is well known that for any matrix X the best rank r approximation to X (measured in terms of Frobenius norm) can be computed by truncating the SVD to the top r singular values. Since our algorithm keeps a SVD representation of the iterates X ( t ) , we can compute the projection on M r efficiently. Despite not having a theoretical guarantee because of the non convexity of M r , our experiments, which we report in Section 4, suggest that O ( n/k ) iterations are still suf-ficient for this explicit rank-constrained nuclear norm regularized problem. Note that the projection opera-tor on K does not change the rank of the matrix since it is a simple scaling. For a pseudo-code description, see full version of the paper (Avron et al., 2012). Role of r . Since nuclear norm regularization is typi-cally used as a proxy for rank constraints, one might wonder why we impose an explicit rank constraint. Primarily, we use the rank constraint only for com-putational efficiency reasons though we consistently observed statistical benefits from additionally keeping the nuclear norm regularizer (i.e.,  X  &gt; 0). As long as r  X  rank( X opt ) the solution to the problem does not change, and the rank constraint is passive . So we need only an upper bound on the rank of the optimal so-lution. We can even select r = n , which will remove the rank constraint completely. However, the running time of the algorithm does depend on r , so it is best to set r as close as possible to rank( X opt ). Since we use nuclear norm regularization we expect the rank of Algorithm 2 SSGD-Matrix-Completion
Input: Z  X  R m  X  n and parameters r , s ,  X  , and  X   X   X   X  X   X  1 k Z k F ,  X   X   X  k Z k 2 F for t = 0 to s n r do end for X opt to be small.
 Output. The output of our algorithm matrix in com-pact SVD form. This is a much more succinct rep-resentation that requires only O ( mr ) memory words, instead of O ( mn ). A specific entry in the matrix can be computed in O ( r ) time, and the entire matrix can be computed in O ( mnr ) time.
 Choice of k . Since we are explicitly enforcing the rank of the iterates to be smaller than r we have r ( t )  X  r . This implies that each iteration is computed in O ( m ( r + k ) 2 ) time. There is a relation between k and the number of iterations: as k grows our gradients improve so we expect to converge in less iterations. In all our experiments we set k = r to avoid an additional tunable parameter In the low rank matrix completion problem, we are given a set of indices  X   X  [ m ]  X  [ n ] and associated val-ues ( Z ij ) ( i,j )  X   X  and we are required to complete the matrix with the lowest possible rank. Minimizing the rank is hard, so a popular approach for solving the matrix completion is as follows. Let P  X  be the projec-tion onto the index set  X . That is, ( P  X  ( X )) ij = X ij if ( i,j )  X   X  and 0 otherwise. Let Z be the matrix containing the known values at their correct position and 0 in all other positions. The problem to be solved is then For reasons that will be apparent later we chose to write the problem with two parameters (  X  and  X  ) in-stead of a single parameter  X  . Other variants are pos-sible, like using ` 1 -loss instead of ` 2 -loss, or any subd-ifferentiable loss function for that matter, but we will focus on (3).
 We now show how are approach can be used to solve (3). Set f ( X ) =  X  kP  X  ( X )  X  Z k 2 F . We have  X  f ( X ) = 2  X  ( P  X  ( X )  X  Z ). Note that f ( X ) is an effi-ciently computable sum loss function.
 We also need to bound the Frobenius norm of X opt in order to define the convex set K F . We observe that Finally, we need to set the step sizes  X  ( t ) . In our ex-periments, we found that using a fixed step size  X  gave the best results. To make the step size scale free we set  X  =  X  k Z k 2 F where  X  is a parameter.
 Heuristics for warm-starting the algorithm and setting the parameters. Our algorithm can start from any matrix with rank up to r as long as we have a compact SVD of that matrix. We warm-start our algorithm by a rank r truncated SVD of Z (i.e., X (0) = TSVD( Z,r )). The initial SVD is also useful for setting  X  and  X  in a scale free manner. The nuclear norm serves as a regularizer, so we expect  X  k X opt k  X  to be some magnitude smaller than  X f ( X opt ). We do not know the values of k X opt k  X  and f ( X opt ), so instead we use the values of k X (0) k  X  and f ( X (0) ). First, we set  X  = 1 / k Z k 2 F , which will make the value of  X f ( X ) range between 0 and 1. We then find  X  such that  X  k X (0) k  X  =  X   X   X f ( X (0) ), where  X  is a new parameter. That is we set  X  =  X   X  f ( X (0) ) / ( k Z k 2 F  X k X (0) k The complete pseudo-code listed in Algorithm SSGD-Matrix-Completion . Overall, Algorithm SSGD-Matrix-Completion has four parameters: (i) bound on the rank of the solution ( r ), (ii) number of super-iterations ( s ; we call every n r iterations a super-iteration), (iii) normalized regularization parameter (  X  ), and (iv) normalized step size (  X  ). We ran our matrix completion algorithm (Algo-rithm SSGD-Matrix-Completion ) on two stan-dard collaborative filtering datasets. The first dataset (MovieLens 10M, partition-rb) has about 10 7 ratings of 69878 users on 10677 movies. The second dataset (Netflix) has about 10 8 ratings of 480189 users on 17770 movies. The ratings are on an integer scale from 1 to 5. We partitioned each dataset into training and test sets as done by Jaggi &amp; Sulovsk  X y (2010). We pre-processed the datasets as follows. For every row and column of training matrix Z we computed the mean. Let  X  i denote the mean rating of row i , and  X   X  j denote the mean rating of column j . We subtract from each training and test rating, X ij , the value (  X  i +  X   X  j ) / 2. In the graphs we refer to our algorithm as  X  X SGD X . JSH and Soft-Impute We compared our results to two other matrix completion algorithms which are also based on solving nuclear norm regularized problems. The first algorithm (which we refer to as  X  X SH X ), sug-gested by the works of Hazan (2008) and Jaggi &amp; Sulovsk  X y (2010), is based on an extension of the Frank-Wolfe (Frank &amp; Wolfe, 1956) algorithm for optimizing a function over the bounded positive semidefinite cone (see also (Clarkson, 2008)). We used a simple MAT-LAB implementation of the Algorithm 2 from (Jaggi &amp; Sulovsk  X y, 2010). The function ApproxEV was im-plemented by calling MATLAB X  X  svds function with default parameters (fixed tolerance). The regulariza-tion parameter ( t ) was set according to the best val-ues reported (Jaggi &amp; Sulovsk  X y, 2010), i.e., t = 48333 for MovieLens and t = 99592 for Netflix. The sec-ond algorithm that we compared to is the soft singu-lar value thresholding algorithm of Mazumder et al. (2010). We refer to this algorithm as  X  X oft-Impute X . Here, we used the MATLAB code provided by the authors (see (Mazumder et al., 2010)). We used the path-following strategy suggested by the code, i.e., we set a path of values for the regularization parameter (  X  ) where the results on a value are used as a warm-start for the next value. The results where exam-ined (RMSE measured) at the different points along the regularization path, and the running time at any point  X  i is the sum of time needed to run the algo-rithm at  X  i and the running time of  X  i  X  1 (because of the warm start). The path used for MovieLens was (  X  0 / 2 , X  0 / 4 , X  0 / 8 , X  0 / 10), and the path used for Netflix was (  X  0 / 250 , X  0 / 300), where  X  0 is the spectral norm of the input matrix Z . Note that both Jaggi &amp; Sulovsk  X y (2010) and Mazumder et al. (2010) apply additional heuristics and/or post-processing to their basic algo-rithm. Additionally, Mazumder et al. (2010) measure only time spent on SVD computations.
 These differences in the experimental setup, together with our effort to bring all algorithms under ex-actly the same experimental protocol, might explain the discrepancy between the results we show here and the results reported in (Jaggi &amp; Sulovsk  X y, 2010) and (Mazumder et al., 2010). Nonetheless, for com-pleteness, we also report their published RMSEs. When all algorithms are compared in the same set-ting, and even relative to best reported RMSEs, we find that our SSGD approach compares favorably to the Frank-Wolfe and singular value thresholding based approaches for solving matrix completion problems. Experimental Setup. We used a 64-bit version of MATLAB 7 . 8. The experiments were done on a two quad-core Intel E5410 computer running at 2.33 GHz, with 32GB DDR2 800 MHz RAM, running Linux 2.6. None of the codes explicitly uses the eight cores, al-though some operations (like dense QR factorization) are automatically parallelized by MATLAB. The mea-sured running times are wall-clock times and were measured using the ftime Linux system call.
 Setting the parameters and sensitivity to them. In Figure 1, we examine SSGD-Matrix-Completion  X  X  sensitivity to the value of the param-eters r, X  , and  X  . The best choice of rank ( r ) is 11. However, increasing the rank from 11 to 15 only re-sults in a 0 . 75% increase in RMSE, and decreasing the rank to 7 only causes a 0 . 38% increase in RMSE. The best choice of  X  is around 0 . 01. Overestimating  X  af-fects the RMSE more adversely than underestimating. For example, setting  X  to 0 . 1 results in a 5% increase in the RMSE, whereas setting  X  to 0 . 001 only leads to a 0 . 48% increase. The best choice of step size (  X  ) is around 0 . 009, and the RMSE increases quite smoothly as we go away from this value. We set  X  = 0 . 015, and  X  = 0 . 005 based on preliminary observations on Movie-Lens 10M without attempting any exhaustive tuning. For Netflix, we simply used the same parameters with-out any additional experimentation.
 Results on the MovieLens 10M Dataset. In Fig-ure 2(a), we plot the RMSE on the test set as a func-tion of time. SSGD-Matrix-Completion decreases the error much faster than the other two algorithms, and maintains a better error throughout. SSGD-Matrix-Completion achieved a RMSE of 0 . 8721 af-ter 1 hour. After running for 180 super-iterations, which took 11 . 47 hours, the RMSE of SSGD-Matrix-Completion was 0 . 8555. Compared to this, the JSH obtained a RMSE 0 . 8640 after 11 . 66 hours and Soft-Impute obtained a RMSE of 0 . 8605 after 12 . 19 hours. Jaggi &amp; Sulovsk  X y (2010) report 0 . 8573 as the best RMSE obtained using their implementation.
 In Figure 2(b), we plot the RMSE as a function of rank of the iterates. Every iteration of JSH involves addition of a rank-one matrix, so the rank of iterates soon becomes large. For both JSH and Soft-Impute, we needed to go to a much larger rank to obtain a RMSE comparable to that of a rank-11 solution ob-tained by SSGD-Matrix-Completion . We stress that at a comparable RMSE a low rank solution is more useful than an high rank solution since it can be held in memory and can be queried much faster to produce a prediction.
 Results on the Netflix Dataset. Here too, SSGD-Matrix-Completion outperforms JSH and Soft-Impute. After running for 25 super-iterations, which took 23 . 97 hours, SSGD-Matrix-Completion ob-tained a RMSE of 0 . 9516. After 24 . 08 hours, JSH obtained a RMSE of 0 . 9583 while Soft-Impute achieved its best RMSE of 0 . 9603 after 8 . 55 hours (after 24 hours Soft-Impute X  X  RMSE was worse than this number). After 180 super-iterations, SSGD-Matrix-Completion obtained a RMSE of 0 . 9411. Jaggi &amp; Sulovsk  X y (2010) report 0 . 9478 as the best RMSE obtained using their optimized implementa-tion. Mazumder et al. (2010) report 0 . 9497 as the best RMSE obtained in their experiments.
 Comparison with GECO . The GECO algorithm proposed in Shalev-Shwartz et al. (2011) is not a solver for nuclear norm regularized problems, but uses a greedy method, with optimality guarantees, to opti-mize f under explicit low-rank constraints. We used the implementation provided to us by the authors with recommended parameters. On MovieLens 10M, GECO returned the best RMSE of 0 . 8771 at a rank of 17. Our approach yields a better RMSE at a lower rank though GECO X  X  results reaffirm the effectiveness of maintaining explicit low-rank constraints. On the other hand, the running time of the GECO implemen-tation was found to be significantly worse than other methods and we considered it impractical to run it on the Netflix dataset.
 Comparison to Non-Convex Methods. Methods that work directly with a low-rank parameterization are significantly different in flavor from convex meth-ods since the local minima they find may or may not be optimal for a given choice of rank parameter. In the-ory, they may be sensitive to initialization and require restarts as reported by Recht &amp; R  X e (2011). In prac-tice, on the two datasets that we tested after our draft was written, we found them to be quite robust and ef-ficient, e.g., on MovieLens 10M they get to an RMSE of 0 . 857 in 45 mins and on Netflix they attain RMSE of 0 . 9399 in a couple of hours. This is not surprising since these methods, and their variations, dominated the Netflix contest. We advocate that efforts to im-prove the performance of convex nuclear norm meth-ods, with an eye towards making them efficient and practical, should include such comparisons.

