 results with a fast implementation).
 vector w ` vector.
 feeding the algorithm a data sequence, ( X guaranteed bound on the regret: alization ability, i.e. we would like: to be small, where R ( w ) := E [ ` ( w ; ( X, Y ))] is the risk . In these conversions, we typically choose b w to be the average of the w algorithm.
 loss functions (i.e. ` square loss) or, indirectly, via strongly convex regulariz ers (e.g. L problem uses the hinge loss with L performance.
 which has some guaranteed cumulative regret bound Reg parameter b w from our online algorithm such that: the regret  X  if the regret is ln T then the additional penalty is O (  X  ln T penalty of O (1 /  X  T ) .
 algorithm) is concentrated rather sharply.
 can be applied to).
 more generally.
 loss, and to P EGASOS in Section 4. defined by k v k Z function satisfying the following assumption.
 f ( w ) = f ( w ; z ) is convex in w and satisfies: Denote the minimizer of F by w ? , w ? := arg min come available to us in that order. These have the property th at Now consider an algorithm that starts out with some w parameter w w Define the statistics, Define the sequence of random variables Since E concentration. of the increment in the regret f ( w concentration results.
 Lemma 1. Suppose assumption LIST holds and let  X  in (1) . Let be the conditional variance of  X  3.1 Proofs Proof of Lemma 1. We have, Taking expectation this gives, for any w , w 0  X  S , Now using this with w = w This implies that Combining (2) and (3) we get, found in the appendix.
 Lemma 3. Suppose X Let V = P T have, for any  X  &lt; 1 /e and T  X  3 , Proof of Theorem 2. By Lemma 1, we have  X  := 1  X  4 ln( T )  X  , we have By definition of Reg and therefore, with probability, 1  X  4 ln( T )  X  , we have Using Lemma 4 below to solve the above quadratic inequality f or Diff above theorem. Its proof can be found in the appendix. Lemma 4. Suppose s, r, d, b,  X   X  0 and we have Then, it follows that 4.1 Online to Batch Conversion for Learning with Bounded Los s Suppose ( X to correct prediction is y . The risk of w is defined by and let w ? := arg min such that w At the end, we output  X  w := ( P T Theorem 2. It bounds the excess risk R (  X  w )  X  R ( w ? ) . have, with probability at least 1  X  4 ln( T )  X  , Plugging it in the corollary above gives the following resul t. an online algorithm that generates w at least 1  X  4 ln( T )  X  , for any T  X  3 . 4.2 High Probability Bound for P EGASOS P ( x i , y i )  X  R d  X { X  1 } Let At time t , P EGASOS takes a (random) approximation w where Z is an example ( x to verify that For any z that is a subset of the data set, the function is Lipschitz on S with Lipschitz constant L = [0 , 3 / 2 + R/ tion LIST.
 Theorem 1 in Shalev-Shwartz et al. [2007] says, for any w , T  X  3 , where L = we easily get tion.
 Corollary 7. Let F be the SVM objective function defined in (4) and w SVM objective. Then, with probability 1  X  4  X  ln( T ) , we have
X 1  X   X  , Proof. Note that (5) implies that Reg Theorem 2 by plugging in  X  =  X  and B = 3 / 2 + R/  X   X  . Appendix Proof of Lemma 3. Note that a crude upper bound on Var discretization 0 =  X  specify the choice of  X  then  X   X  r l  X  b holds: In the second case, we have
