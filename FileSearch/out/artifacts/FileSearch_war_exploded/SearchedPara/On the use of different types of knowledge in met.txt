 1. Introduction
Many heuristic search methods for combinatorial optimization are based on one of the following two principles: neighborhood search or solution construction. In neighborhood search is given a neighborhood function that assigns to each candidate solution a so-called neighborhood, which is a subset of the search space. Heuristic methods based on neighborhood search follow a trajectory in the directed graph G whose node set is the search space. A node v is connected to a node w by an arc if and only if w is in the neighborhood of v . The trajectories in G may be deterministic as in the case of a simple tabu search method ( Glover and Laguna, 1997 ), or they might result from a random process as in the case of simulated annealing ( Kirkpatrick et al., 1983 ).

On the other side, optimization techniques based on solution construction explore the search space in form of a search tree which is defined by the solution construction mechanism. Following a path from the root node to a leaf corresponds to the process of constructing a candidate solution. Inner nodes of the tree can be seen as partial solutions. The process of moving from an inner node to one of its child nodes is called a construction step. A prominent example of deterministic constructive algorithms is greedy heur-istics. They make use of a weighting function that gives weights to the child nodes of each inner node of the search tree. At each construction step the child node with the highest weight is chosen. Metaheuristics such as ant colony optimization (ACO) ( Dorigo and
Stuetzle, 2004 ) or greedy randomized adaptive search procedures (GRASP) ( Feo and Resende, 1995 ) employ repeated probabilistic (or randomized) solution constructions. For each inner node of the tree and each child node is given the probability of performing the corresponding construction step. These probabilities, which may depend on weighting functions and/or the search history of the algorithm, define a probability distribution over the search space. In this work we refer to the resulting probability of generating a satisfying  X  for example, optimal  X  solution as the standard problem knowledge .

Recent variants of metaheuristics such as ACO and GRASP include two important features that are inspired by deterministic branch and bound derivatives such as beam search ( Ow and
Morton, 1988): 1. Lower bounds (respectively, upper bounds) are used for evaluating partial solutions; sometimes also for choosing among different partial solutions, or discarding partial solu-tions. Henceforth we will refer to this type of knowledge as the complementary problem knowledge . 2. The extension of partial solutions may be done in more than one way. The number of nodes which can be selected at each tree level is usually limited from above by a parametric constraint, resulting in parallel and non-independent solution constructions.

In particular, approximate and non-deterministic tree search (ANTS) procedures ( Maniezzo, 1999; Maniezzo and Carbonaro, 2000; Maniezzo and Milandri, 2002 ) and probabilistic beam search derivatives such as Beam-ACO and probabilistic beam search (see, for example, Blum, 2005, 2008; Blum et al., 2007;
Caldeira et al., 2007 ) are algorithms that exhibit the features described above. Several of the cited papers are current state-of-the-art algorithms for the problems to which they were applied.
Therefore, these works give clear empirical evidence for the usefulness of using (randomized) parallel and non-independent solution constructions.

The main goal of this paper is to provide a first theoretical explanation of why optimization algorithms that make use of (randomized) parallel and non-independent solution construc-tions work so well in practice. In particular, we show that these algorithms may have advantages over standard algorithm versions that use independent solution constructions. The organization of the paper is as follows. In Section 2 we introduce a tree search model (as a model for understanding the construc-tion of solutions). In Section 3 we deal with the notions of standard and complementary problem knowledge more in detail, showing by means of examples why the exploitation of the complementary problem knowledge may be beneficial. In Section 4 we theoretically analyze a simple algorithm for exploiting the complementary problem knowledge, while in Section 5 we confirm our theoretical findings by means of practical examples.
Finally, let us mention that this paper is a substantial extension of work that has been published in Blum and Mastrolilli (2007) . The extension concerns the theoretical analysis presented in Section 4 and its experimental confirmation presented in Section 5. 2. A tree search model
Many exact or approximate algorithms that are commonly used in practice for the solution of NP -hard combinatorial optimization problems build solutions step by step. We refer to these algorithms as constructive optimization algorithms and describe a model that captures the essential elements common to all of them.

In general, we are given an optimization problem P and an instance x of P . Typically, the set S x of possible solutions is exponentially large in the size of the input x . The goal is to find a solution to x belonging to some set Sat x D S x of satisfactory solutions. 1 Assume that each element y of Sat x can be viewed as a composition of l y ; x A N elements from a set S . From this point of view, Sat x can be seen as a set of strings over an alphabet S . Any element y of Sat x can be constructed by concatenating l y , x elements of S .

The following method for constructing elements of Sat x is instructive: A solution construction starts with the empty string
Y  X  e . The construction process consists of a sequence of construction steps. At each construction step, we select an element of S and append it to Y . The solution construction ends when either a leaf node of the tree is reached or when it becomes clear that Y cannot be extended into any element of Sat x no element of Sat x has prefix Y ). An algorithm of this kind can be described equivalently as a walk from the root v 0 of a tree to a node at level l y , x for any y A Sat x . The tree has nodes for all y and for all prefixes of elements of Sat x . The root of the tree is the empty string. There is a directed arc from node v to node w if w can be obtained by appending an element of S to v (i.e. if ( a
A S : w  X  va ). The set of nodes that can be reached from a node v via directed arcs are called the children of v . Note that the nodes at level i correspond to strings of length i .If v is a node corresponding to a string of length l 4 0 then the length l 1 prefix w of v is also a node. Thus, for every y A Sat x , there is a path of the tree may contain other nodes (which correspond to unsa-tisfactory solutions). It is not necessary to store the entire tree. An efficiently computable predicate P on the set of strings, such that P can be viewed as a pruning procedure. The children of any given node w can be computed by evaluating P ( wa ) for all a A sufficient for our purposes. We use the described tree search model as basis of our discussion.
 Assumptions. The analysis (but not the algorithms) provided in this paper assumes that there is a unique satisfactory leaf node v i.e. Sat x ={ v d }. Extensions to more general cases are left for future research. Without loss of generality, the target node v d maximum level d Z 1 of the search tree. Moreover, let c denote the maximum number of children of any given node. We assume that the values of c and d are bounded by a polynomial in the input size. A probabilistic constructive optimization algorithm is said to be successful if it can find the target node v d with high probability. 3. Standard and complementary problem knowledge
Metaheuristics such as ACO and GRASP utilize repeated probabilistic solution constructions where each construction step is performed probabilistically. Being at node v , the probability to move to a child w of v is denoted by Pr  X  w j v . Moreover, at each node may exist a certain probability Pr  X  stop j v for stopping (aborting) the solution construction. Note that these probabilities (sometimes called transition probabilities) define a probability distribution over the search space. Summarizing, a probabilistic solution construction works as follows: the process starts at the root v 0 of the search tree, and repeatedly chooses a child of the current node at random (i.e. with respect to the given probability distribution) until a leaf node is reached or the decision to stop has been made.

In the following let us examine the success probability of repeated applications (or runs) of such a probabilistic solution construction. Given any node v i at level i of the search tree, let Pr[ v i ] be the probability that node v i is visited during the solution construction. Note that there is a single path from v 0 , the root node, to v i : we denote the corresponding sequence of nodes by ( v , v 1 , v 2 , y , v i ). Clearly, Pr[ v 0 ]=1 and Pr  X  v i Success  X  r  X  denote the event of finding the target node v independent runs (that is, repetitions). 2 Note that the probability the following inequalities hold: 1 e r Pr  X  v d r 1  X  1 Pr  X  v d  X  r r r Pr  X  v d :  X  1  X  By (1), it immediately follows that the chance of finding node v large if and only if r Pr  X  v d is large , namely as soon as  X  O  X  1 = Pr  X  v d  X  :  X  2  X  In the following, we will not assume anything about the exact form of the given probability distribution. However, let us assume that the transition probabilities are heuristically related to the attractiveness of child nodes. In other words, we assume that in a case in which a node v has two children, say w and q , and w is known (or believed) to be more promising , then Pr  X  w j v 4 Pr  X  q j v . This can be achieved, for example, by defining the transition probabilities proportional to the weights assigned by greedy functions. Moreover, at any node v we can also have a non-zero probability Pr  X  stop j v that a solution construction is aborted at this partial solution represented by node v cannot be completed such that the result is a satisfiable solution; otherwise this probability may be related to the expectation that v can lead to a satisfiable solution. In general, the larger this expectation the smaller the probability to stop. The stopping probability, which may be obtained from bounding information, strongly influences the performance of many exact or approximate algorithms (see, for example, the pruning procedures in branch and bound algo-rithms). In the following we model the non-zero stopping probabilities by means of an additional virtual child node, that is, each node v with Pr  X  stop j v 4 0 has an additional virtual child node called stop . We call these nodes virtual nodes , because they are not really part of the search tree.

Clearly, the probability distribution reflects the available knowledge on the problem, and it is composed of two types of knowledge. If the probability Pr[ v d ] of reaching the target node v is  X  X  X igh X  X , then we have a  X  X  X ood X  X  problem knowledge. Let us call the knowledge that is responsible for the value of Pr[ v d standard problem knowledge (or just standard knowledge). From the dual point of view, we still have a  X  X  X ood X  X  knowledge of the problem if for  X  X  X ost X  X  of the wrong nodes (i.e. those that are not on the path from v 0 to v d ) the probability that they are reached is  X  X  X ow X  X . We call this knowledge the complementary problem knowledge (or just complementary knowledge). 3 The following example will show that these two types of knowledge are not the same. Consider the search tree of Fig. 1 , where the target node is v . Let us analyze two different probability distributions:
Case (a) For each v and a child w of v let P  X  w j v  X  0 : 5. Moreover, the stopping probabilities at all nodes are zero. This means that when probabilistically constructing a solution the probability of each child is the same at each construction step.

Case (b) The transition probabilities are defined as in case (a), but the stopping probabilities are set to 1 in the black nodes, i.e.
Pr  X  stop j blacknode  X  1. This means that each of the black nodes has a virtual child that has probability 1, and the white children of the black nodes have probability 0.

Note that in both cases the standard knowledge is  X  X  X carce X  X , since the probability that the target node v d is reached decreases exponentially with d , i.e. Pr  X  v d  X  2 d . However, in case (b) the complementary knowledge is  X  X  X xcellent X  X , since for most of the wrong nodes (i.e. the white nodes), the probability that any of them is reached is zero. Viceversa, in case (a) the complementary knowledge is still  X  X  X carce X  X , i.e. there is a  X  X  X igh X  X  probability that a white node is reached.

By using the intuition given by the provided example, let us try to better quantify the quality of the available problem knowledge.
Let V i be the set of nodes at level i , and let  X   X  i  X  X  for i =1, y , d , which is equal to the probability that the solution construction process reaches level i of the search tree. We observe that the presence of the non-zero stopping probabilities (namely,
Pr  X  stop j blacknode  X  1 in the example) can make the probabilities  X   X  i  X  smaller than one. Case (b) was obtained from case (a) by decreasing  X   X  i  X  (for i =1, y , d ) down to 2 i 1 (and without changing the probability Pr[ v i ] of reaching the ancestor v i of the target node at level i ), whereas in case (a) it holds that  X   X  i  X  X  1 (for i =1, general, good complementary knowledge is supposed to decrease  X   X  i  X  without decreasing the probability of reaching the ancestor v of the target node v d . This discussion may suggest that a characterization of the available problem knowledge can be given by the following knowledge ratio :
K  X  min
Observe that the larger the ratio the better the knowledge we have on the target node v d .In case (a) it is K v d  X  1 = 2 knowledge ratio of case (b) is K v d  X  1 2 , which is exponentially larger.

Finally, it is important to observe that the way of (repeatedly) constructing solutions in a probabilistic way does not exploit the complementary problem knowledge. For example in case (b), although the available knowledge is  X  X  X xcellent X  X , the target node v is found after an expected number of runs that is proportional to 1/Pr[ v d ]=2 d (see Eq. (2)), which is the same as in case (a). In other words, the number of necessary runs only depends on the standard knowledge. 4. Exploiting the complementary knowledge
Examples of constructive optimization algorithms that use and exploit the complementary knowledge are incomplete derivatives of branch and bound procedures ( Lawler and Wood, 1966 ) such as, for example, beam search ( Ow and Morton, 1988 ). Their central ideas are as follows. First, they allow the extension of partial solutions in more than one way. Second, bounding information  X  a lower bound in the case of minimization  X  is used to evaluate partial solutions. The bounding information is used for the definition of the complementary knowledge. For example, in case the lower bound of a partial solution is greater than the value of any satisfiable solution, the stopping probability at this partial solution is set to one. These algorithms generally bound the number of partial solutions per tree level from above. In fact, they might be characterized as constructive optimization algorithms that employ parallel and non-independent solution constructions .In recent years, versions of these probabilistic algorithms were proposed in which the parallel and non-independent solution constructions are performed probabilistically in an iterative (or repeated) way. Examples are, as mentioned before, approximate and non-deterministic tree search (ANTS) procedures ( Maniezzo, 1999; Maniezzo and Carbonaro, 2000; Maniezzo and Milandri, 2002) and probabilistic beam search derivatives such as Beam-
ACO and probabilistic beam search (see, for example, Blum, 2005, 2008; Blum et al., 2007; Caldeira et al., 2007 ). The usefulness of these algorithms has been shown empirically by the application to various combinatorial optimization problems such as set covering, the quadratic assignment problem, shop scheduling, supply chain management, assembly line balancing, and the shortest common supersequence problem.

With the aim of deriving a theoretical explanation of why many of these practical algorithms achieve state-of-the-art results, we now define a simple algorithm that is, in a sense, representative for the above mentioned techniques. This algor-ithm  X  henceforth denoted by PTS  X  a  X  (for probabilistic tree search  X  F is pseudo-coded in Algorithm 1. Hereby, a denotes the maximum number of allowed extensions of partial solutions at each construction step; in other words, a is the maximum number of solutions to be constructed in parallel. We use the following additional notation: For any given set S of tree nodes, let C  X  S  X  be the set of children of the nodes in S , including any virtual stopping nodes.
 Algorithm 1. PTS  X  a  X  Require a A Z  X 
Ensure R 0 :  X f v 0 g , i :  X  1 and R j :  X  | for j  X  1 ; 2 ; ... ; d ; while C  X  R i 1  X  is not empty and a target node is not reached do end while .

The algorithm works as follows. At each construction step i =1, y , d we have given a set of nodes R i 1 that have been reached in construction step i 1. As mentioned above, C  X  R i 1  X  denotes the set of nodes that can be reached from the nodes in R i 1 . Given
C  X  R i 1  X  , a probabilistic choices of nodes from C performed, resulting in the set R i of nodes. The probability
Pr  X  w j C  X  R i 1  X  of a node w A C  X  R i i  X  to be chosen is defined as follows:
Pr  X  w j C  X  R i 1  X  :  X  Pr  X  w P
Observe that for any w A C  X  R i 1  X  ,Pr  X  w j C  X  R i 1  X  is equal to the probability that w is reached with a single probabilistic solution construction starting from the root, when this construction is limited to the subtree defined by the nodes that were reached at levels j =1, y , i 1. 4.1. Theoretical analysis of PTS  X  a  X 
In this subsection we study the probability of PTS  X  a  X  for reaching the target node v d . We prove (see Theorem 1) that
PTS  X  a  X  is successful with  X  X  X igh X  X  probability as soon as a is of the order of 1 = K v d . 4 This means that the greater the knowledge ratio the smaller the required number of parallel solution constructions.

On the probability of reaching one single node : First, remember that there is a single path from v 0 , the root, to v d : as before we denote the corresponding sequence of nodes by ( v 0 , v 1
We associate to any node v the variable indicator I v defined as follows: I  X  For simplicity of notation we use Pr  X  I v (and Pr  X  I v ) to denote Pr  X  I v  X  1 (and Pr  X  I v  X  0 ).
 reaches v d is larger than 1/ e , namely Pr  X  I v d 4 1 = e . Proof. The probability of reaching v d can be recursively com-puted as follows: Probability Pr  X  I v i j I v i 1 can be put in the following form: where Pr  X  I v i 4 R i 1 j I v i 1 is the probability of reaching v nodes in R i 1 , given that v i 1 is reached and R i 1 is any subset of nodes at level i 1 such that v i 1 A R i 1 . By using standard theorems from probability theory, we obtain the following derivations: where Pr  X  v i j I v i 1 4 R i 1  X  Pr  X  v i = P s A C  X  R ability that the algorithm reaches v i given that v i 1 and the nodes from R i 1 are reached (see Eq. (5)). Since  X   X  i  X  X  0 o t o 1), we get Pr  X  I v d Z The claim follows by observing that exp  X  X  e a K
Discussion : By Theorem 1, choosing a Z  X  1 = K v d  X  ln  X  d  X  1  X  is a sufficient condition to ensure a  X  X  X igh X  X  success probability of reaching v d . Observe that the  X  X  X mportant X  X  value is given by 1 = K whereas ln  X  d  X  1  X  is just a  X  X  X mall X  X  factor (recall that we assume d to be polynomial in the input size), and therefore we omit it in the following discussion.

Recall that c denotes the maximum number of children of any node. The proposed algorithm PTS  X  a  X  , when a  X  O  X  1 = K forms an expected number of extensions of partial solutions that can be easily bounded from above by O  X  d c = K v d  X  . Viceversa, PTS  X  a  X  1  X  needs an expected number of extensions of partial solutions to reach v d that can be bounded by O  X  d c = Pr  X  v the  X  X  X mportant X  X  factors are, respectively, 1 = K K d Z Pr  X  v d , and actually K v d can be also exponentially larger than Pr[ v d ] in many settings (see, for example, Fig. 1 ). The reason of this potential advantage is due to the fact that when a  X  1 the algorithm only uses the standard knowledge, whereas when a 4 1 both the standard and the complementary knowledge are exploited by parallel non-independent solution constructions.
This gives evidence that a setting of a 4 1 may help to achieve substantial speedups when compared to PTS  X  a  X  1  X  , thanks to a better exploitation of the available problem knowledge.
At this point, the reader may object that the value of  X  1 = K d  X  ln  X  d  X  1  X  is not known a priori, even though the knowledge of that value is essential for  X  X  X  correct X  X  setting of a and therefore also for the success of PTS  X  a  X  (see Theorem 1). Therefore, we present in the following a kind of self-tuning strategy for setting a in order to deal with this problem. 4.2. Self-tuning: a strategy for setting a
In the following we assume that the expected running time t of PTS  X  a  X  (for any a ) can be approximated by an appropriate polynomial function of a , i.e. t a  X  O  X  a t  X  , for a suitably chosen constant t Z 0. Actually, the value of t a is at least O  X  a  X  and at most O  X  c d a  X  .

The general idea is to run PTS  X  a  X  repeatedly: run PTS  X  a  X  with  X  1 Z 1; if PTS  X  a 1  X  finds the desired target node we are done, otherwise restart PTS  X  a  X  from the beginning and run it with  X  2 Z 1, and so on. Such an experiment can be described by a from the set Z  X  [f1g . We henceforth denote this algorithm by
MS-PTS( S ), which stands for multi-start PTS using strategy S .Itis easy to see that any strategy S with a i Z 1 causes the algorithm, at some point, to find the target node. The running time, however, is a random variable, i.e. the resulting algorithm is a Las Vegas by choosing a Z d there is a  X  X  X igh X  X  probability that algorithm PTS  X  a  X  reaches the target node v d . If we knew d , then strategy
S  X  X  d ; d ; d ; ...  X  would be a (near-) optimal strategy that would be successful after only a few applications of PTS  X  d  X  .
Unfortunately, we do not know d a priori. The ideas of an alternative strategy presented in the following are taken from Luby et al. (1993) . Let T S * be the expected running time of MS-PTS( S *) with S * as described above. By Theorem 1 we have that T  X  O  X  t d  X  . Next, we explain that, with no knowledge about d , the expected time to reach the target node can be always bounded by
O  X  t log t d  X  . This performance is achieved by a pure strategy a very simple form that is easy to implement in practice. Formally, this strategy can be described as follows:  X  ( This strategy starts as follows:
S  X  X  1 ; 1 ; 2 ; 1 ; 1 ; 2 ; 4 ; 1 ; 1 ; 2 ; 1 ; 1 ; 2 ; 4 ; 8 ; 1 ; ...  X  :
Note that all values of a are powers of two, and each time a pair of runs of a given a has been completed, a run of twice that a is immediately executed.

By using the ideas in Luby et al. (1993), it can be promptly proven that the expected running time of MS-PTS( S u ) is at most
O  X  t log t d  X  , which is only a logarithmic factor slower than the expected time of MS-PTS( S *). The intuition for the bound (see
Luby et al., 1993, for more details) is that S u uses the following geometrically increasing values for a : 1,2,2 2 , y ,2 j , the first time PTS  X  a  X  succeeds. Clearly, there exists an integer n 4 0 such that 2 n 1 r d r 2 n . Once the strategy has performed a few runs with a  X  d (i.e. 2 n ), it will succeed with a fairly high probability. The total time spent on runs with a  X  d will be about
O  X  t
 X  . But the strategy is  X  X  X alanced X  X , in the sense that the total time spent on runs with different settings of a is roughly equal.
Since the number of different settings of a used up to this time is about log d , the total running time can be bounded from above by
O  X  t log t d  X  (by our assumption on the expected running time t
PTS  X  a  X  ). 5. Experimental evaluation
In the following we present empirical results in order to confirm our theoretical findings from the previous section. First, we present an experimental evaluation of MS-PTS( S u ) on artificial search trees. Then, we present the application of MS-PTS( ) using different strategies (including strategy S u ) to a real combinatorial optimization problem, the open shop scheduling (OSS) problem.
The choice of this problem was motivated by the fact that one of the current state-of-the-art methods for the OSS, Beam-ACO (Blum, 2005 ) is an algorithm that makes use of (randomized) parallel and independent solution constructions. Let us state clearly at this point that it is not our intention to produce a new state-of-the-art algorithm for the OSS problem. 5.1. Experimental results with artificial search trees We implemented a graphical simulation tool (AntSim) in Java.
This tools provides the possibility to load existing search trees and/or draw new search trees. Furthermore, it allows to follow the simulation of the proposed algorithm step by step. 6 The goal of the following experimental evaluation is to validate the claimed properties of MS-PTS( ). With this aim, we needed to have full control on the structure of the input trees. Therefore, we generated a set of artificial trees with a  X  X  X ow X  X  standard knowledge and a  X  X  X igh X  X  knowledge ratio. The outline of the input tree generation procedure is as follows.

The procedure for producing a tree depends on three input parameters, namely c , d and K min , where K min is the minimally required knowledge ratio for each level. The chosen value combinations for these three parameters are displayed in the second column of Table 1 (see Appendix A). The target node v at the maximum level d . The tree generation starts by producing the path from the root node to the target node, consisting of d +1 nodes. Then, the remaining tree is built level by level, node by node, starting from the root; we considered the nodes of each level in a random order, and for each node j we computed the maximum number c ( j ) of children that j can have without violating the requirement that the knowledge ratio must be larger or equal to K min ; a random number of children between zero and min f c ; c  X  j  X g is added to the tree, where c is the overall maximum number of children of any node (a fixed parameter).
The process is repeated for d levels. For each combination of parameters we generated a set of 10 instances, resulting in a total of 40 input trees. The probabilities for each solution extension are fixed such that each child node of a node has the same probability as the others. This means we have a low standard knowledge. The stopping probabilities are equal to one at any leaf node, and zero otherwise. Note that we simulate the  X  X  X ood X  X  complementary knowledge by allowing leaf nodes at each level of the search tree (not only at the maximum level). Each leaf node of a level l o d actually corresponds to a  X  X  X lack node X  X  in the example shown in Fig. 1 , having stopping probability 1.

For each input tree we computed the probability of reaching the target node, namely Pr[ v d ]; the inverse of this value is reported in the third column of Table 1 (see Appendix A). Observe that 1/Pr[ v d ] is equal to the expected number of iterations of MS-
PTS( S 1 ), where S 1 =(1,1,1, y ), and therefore it represents a lower bound on the expected number of extensions of partial solutions of MS-PTS( S 1 ). We also computed, for each input tree, the knowledge ratio K v d . Let us consider for a moment the case in which we set d  X d X  1 = K v d  X  ln  X  d  X  1  X e . Note that the expected number of extensions of partial solutions performed by PTS( a  X  d ) before the target node is reached is at least the number of extensions of partial solutions performed to reach level d 1, plus one further partial solution extension, namely at least
LB
In Table 1 (column headed by MS-PTS( S u )) we report the average number of partial solution extensions, out of five runs, required by
MS-PTS( S u ) to reach the target node. In our computational experiments, the average number of partial solution extensions required by MS-PTS( S 1 ) was always larger than 1/Pr[ v d decided therefore to omit the exact values. The advantage of MS-
PTS( S u ) over MS-PTS( S 1 ) is evident already by using the values of 1/Pr[ v d ] for comparison. Moreover, the reader can easily note that the number of partial solution extensions required by MS-PTS( S is at most only few times LB d . Therefore, without the knowledge of the exact value of K v d (as required by Theorem 1), and by using the proposed universal strategy of Section 4.2, it is still possible to obtain the performances claimed by Theorem 1. We can conclude that the presented experimental results for artificial trees are in accordance with our theoretical analysis. 5.2. Application to open shop scheduling
The OSS problem can be formalized as follows. Given is a finite set of operations O ={ o 1 , y , o n } which is partitioned into disjoint subsets M  X f M 1 ; ... ; M j M j g . The operations in M i A processed on the same machine. For the sake of simplicity we identify each set M i A M of operations with the machine they have to be processed on, and call M i a machine. Set O is additionally partitioned into disjoint subsets J  X f J 1 ; ... ; J j J j operations J j A J is called a job. Moreover, each operation o a fixed processing time p ( o ). We consider the case in which each machine can process at most one operation at a time. Operations must be processed without preemption (that is, once the processing of an operation has started it must be completed without interruption). Operations belonging to the same job must be processed sequentially.

A solution is given by permutations p M i of the operations in M 8 i
A f 1 ; ... ; j M jg , and permutations p J j of the operations in J 8 j
A f 1 ; ... ; j J jg . These permutations define processing orders on all the subsets M i and J j . Note that a permutation p of all the operations represents a solution to an OSS instance. This is because a permutation of all operations naturally defines permutations of the operations of each job and of each machine. There are several possibilities to measure the cost of a solution. Here we deal with makespan minimization. Each operation o has a well-defined earliest starting time t es ( o ) with respect to a (partial) solution. Note that all the operations that do not have any predecessor have an earliest starting time of 0. Accordingly, the earliest completion time of an operation o A O with respect to a (partial) solution is denoted by t ec ( o , s ) and defined as t p ( o ) (where p ( o ) is the processing time of o ). The objective function value f  X  p  X  of a solution p (also called the makespan) is given by the maximum of the earliest completion times of all the operations: f  X  p  X  :  X  max f t ec  X  o  X j o A O g :  X  7  X  We aim at minimizing f .

In order to define algorithm PTS  X  a  X  for the OSS problem we first have to outline the solution construction mechanism that defines the search tree. Solutions are constructed by filling the n positions of a permutation p one after the other (from left to right). Given a (partial) permutation p i 1 F with the first i 1 positions already filled  X  we henceforth denote by O + the set of operations that are not yet part of p i 1 . The solution construction starts with an empty permutation. At each construction step i (where i =1, y , n ) one of the operations from O + is put at position i of the partial permutation p i 1 . Generally, good solutions are obtained by choosing at each construction step an operation o
A O  X  that has a low earliest starting time t es ( o ). This is the motivation for restricting set O + at each construction step as follows. Let t min  X  min f t es  X  o  X j o A O  X  g and t min Then O  X  :  X f o A O  X  j t es  X  o  X  r t min  X  b  X  t max t min  X g with b Note that the setting of b determines the size of the resulting search tree. Generally, if b is too small, the optimal solutions might be excluded from the resulting search tree. As explained later, we chose a value for b F depending on the size of a problem instance  X  such that this does not happen.

Next we define the transition probabilities, that is, for each node v and each child w of v we define the probability Pr  X  w j v .In the case of the OSS problem, a node v is a partial permutation i 1 , and a child of p i 1 is obtained by putting one of the operations o A O  X  at position i of p i 1 . Therefore, we henceforth denote the probabilities Pr  X  w j v by Pr  X  o j p i 1 . Moreover, a child that is obtained by placing operation o A O  X  at position i of p characterized by the earliest starting time t es ( o )of o . Given all children of a partial permutation p i 1 , we order them with respect to these earliest starting times (in increasing order). Henceforth we call the position of a child in this order the rank of the child , denoted by r  X  o j p i 1  X  . For example, if o A O  X  has the smallest earliest starting time among all operations in O  X  , then r  X  o j p i 1  X  X  1. With these rank values we finally define the conditional probabilities for extending the partial permutation i 1 by placing o A O  X  at position i : Pr  X  o j p i 1 :  X  This definition is motivated by a popular greedy heuristic for the OSS problem that consists in choosing at each construction step the child that has rank 1, that is, the child obtained by choosing the operation o A O  X  with the smallest earliest starting time among all the operations in O  X  . The setting of the transition probabilities as outlined above is such that the solution obtained by the greedy heuristic has the highest probability to be constructed. In general, the closer a solution is to the solution obtained by the greedy heuristic, the higher is its probability to be constructed. The only exception to this setting of the transition probabilities occurs when the lower bound value LB  X  p i 1 given partial solution p i 1 is greater than the maximum value of any satisfiable solution. In our experiments we were only interested in finding optimal solutions. Therefore, the set of satisfiable solutions is the set of optimal solutions. In this case the additional virtual child of p i 1 (the virtual stopping node) has probability 1, and all the regular children have probability 0.
In the following we introduce the lower bound LB  X  X  that we used. We denote the operation of a job J j A J that was taken last into the partial solution p i 1 by o J j . Similarly, we denote the operation of a machine M i A M that was taken last into the partial solution p i 1 by o M i . Observe that the partition of the set of operations O with respect to a partial solution p i 1 into O (the operations that are already in p i 1 ) and O + (the operations that still have to be dealt with) induces a partition of the operations of every job J j A J into J j and J j + and of every machine M and M i + . Given these notations and a partial permutation p lower bound LB  X  p i 1  X  is computed as follows:
LB  X  p
In other words, lower bound LB  X  X  is computed by summing for every job and machine the processing times of the unscheduled operations, adding the earliest completion time of the operation of the respective job or machine that was scheduled last, and taking the maximum of all these numbers. As all the necessary numbers can be obtained and updated during the solution construction process, this lower bound can be very efficiently computed. With this information all elements of PTS( ) are well defined. 5.2.1. PTS  X   X  a  X  : a variation of PTS  X  a  X 
At each solution construction step i of algorithm PTS  X  a  X  , a elements are chosen probabilistically from C  X  R i 1  X  . This choice is realized as a choice with replacement . This means that any chosen element may be selected more than once, which is inefficient from an algorithmic point of view. Consider the tree in Fig. 2 : each time a node w has two children, assume that the transition probabilities are 0.5, respectively. The stopping probabilities are equal to one at any leaf node, and zero otherwise. Let the successful node v d be the unique node at the maximum level d .It can be easily checked that PTS  X  a  X  requires a setting of a  X  O  X  2 have a high success probability, although the number of visited nodes is clearly bounded by a polynomial in d .
 In fact, it is enough to reach any partial solution once. Therefore, we propose a version of PTS  X  a  X  F denoted by
PTS ( a )  X  which implements each construction step as a choice without replacement . This simple and natural diversification mechanism can increase the effectiveness and avoid some of the possible shortages of PTS  X  a  X  , as for example the one represented by Fig. 2 . Moreover, the reader can easily check that it inherits the  X  X  X ood X  X  properties of PTS  X  a  X  . 7 The analysis of PTS + for future research. However, we are going to apply its multi-start version MS-PTS + ( ) to the OSS problem. 5.2.2. Computational results
We chose the 60 instances provided by Taillard (1993) as test problems for the experimental evaluation. This set comprises six subsets of instances, each one consisting of 10 instances of the following strategies for our algorithms:
S : With this strategy PTS  X  a  X  is always executed with the setting  X  i , that is, S i =( i , i , i , y ). We used six different settings: i A f 1 ; 2 ; 5 ; 10 ; 50 ; 100 g , resulting in six different strategies.
S : The universal strategy as outlined in Section 4.2. For practical reasons we bounded the setting of a to 2 8 =256 from above. In other words, whenever a 4 256 with respect to the universal strategy, we set a  X  256.

We applied MS-PTS( ) as well as MS-PTS + ( ) to each of the 60 problem instances 100 times. For each application we allowed a maximum of j O j 10000 extensions of partial solutions, which results in a maximum of 10000 solution constructions. We were only interested in finding optimal solutions. 8 The performance of the algorithms is measured by two different values: 1. The success rate. For example, an algorithm that succeeds in finding an optimal solution in each of its 100 applications to a problem instance A has a success rate of 1.0 for the corresponding problem instance. 2. The average number of extensions of partial solutions needed to achieve success (or to terminate without success). In the following we will refer to this measure as the running time of an algorithm.
 We present only the results concerning the four bigger subsets of problem instances. They are shown in Figs. 3 and 4 . 9 subset of instances we have two graphics. The first one shows results concerning the success rate of the algorithms, and the second one concerning the running times. All the results are shown by means of boxplots that visualize the distribution of the results over the 10 instances of each instance subset. The boxes are drawn between the first and the third quartiles of the distribution, and the median is shown as a horizontal line in each box. The whiskers extend to data points that are no more than 1.5 times the interquartile range away from the box. Outliers are shown as points. For example, the application of algorithm MS-
PTS( S 1 ) 100 times to each problem instance of size j O j X  49 resulted in an average success rate and an average running time for each problem instance. As there are 10 instances of size j O j X  49 we have 10 average results concerning the success rate as well as the running times. The left most box of the left-hand-side graphic of Fig. 3 (a) shows the distribution of the 10 average success rates obtained by algorithm MS-PTS( S 1 ), whereas the left most box of the right-hand-side graphic of Fig. 3 (a) shows the distribution of the 10 average running times obtained by algorithm MS-PTS( S 1 ).
 The results allow us to make the following observations: 1. All the strategies S i with i 4 1 result in a higher success rate achieved in less running time as compared to strategy S 1
Remember that S 1 is the only strategy that does not exploit the complementary problem knowledge. Observe that even strat-egy S 2 allows in general to double the success rate while significantly reducing the running time as compared to strategy S 1 . 2. The best strategies are S 50 and S 100 . However, observe that the universal strategy S u is in general not much worse. This confirms the theoretical results regar ding the universal strategy. 3. When comparing the results of algorithm MS-PTS( ) with the results of the extended version MS-PTS + ( ), we can note that
MS-PTS + ( ) always results in a higher success rate and at the same time in a lower running time when compared to MS-
PTS( ). This confirms that  X  X  X hoosing without replacement X  X  is a better method for choosing among the possible extensions of partial solutions at each construction step.

Finally, let us remark that, despite its simplicity, MS-PTS( )is among the few algorithms from the literature that can find optimal solutions to all 60 problem instances by Taillard. To our knowledge, only the algorithms proposed in Blum (2005) , Sha and Hsu (2008) , and Tamura et al. (2009) are able to do that. This is a clear indication that the use of (randomized) parallel and independent solution constructions is one of the key points for the success of Beam-ACO ( Blum, 2005 ), which is one of the current state-of-the-art techniques for the OSS problem. 6. Conclusions and future work
In this paper we have dealt with probabilistic constructive optimization algorithms. In particular, we have studied  X  theoretically and experimentally  X  the potential of algorithms that exploit complementary problem knowledge by means of parallel and non-independent probabilistic solution construc-tions. The results have shown that exponential speed-ups may be achieved by algorithms such as MS-PTS( ), which exploit the complementary problem knowledge.

However, the success of MS-PTS( ) depends very much on the quality of the complementary problem knowledge. The algorithm does not work, for example, when the lower bound used for defining the complementary problem knowledge is not tight enough. We repeated the experiments for the OSS problem with lower bound values multiplied by a constant q o 1, which results in lower bound values that are less tight.
The results showed that with decreasing q the differences between the various strategies disappeared, and the performance of the algorithms using strategies other than S 1 was becoming closer and closer to the performance of the algorithms using strategy S 1 .

In the future, we plan to extend the theoretical analysis as well as the experimental work to more efficient versions of MS-PTS( ).
For example, the dependence on a tight lower bound may be reduced by allowing more than a extensions of partial solution at each step, with the subsequent restriction of the chosen child nodes to a subset containing the a best ones with respect to the lower bound values.
 Acknowledgments
We would like to thank Felipe Figueiredo and Michele Pedrazzi for the implementation of the Java tool. Monaldo Mastrolilli acknowledges support from the Swiss National Science Founda-tion Project 200021-104017/1,  X  X  X ower Aware Computing X  X , and by the Swiss National Science Foundation Project 200021-100539/1,  X  X  X pproximation Algorithms for Machine scheduling Through Theory and Experiments X  X .
 Christian Blum acknowledges support from the Spanish CICYT Project TIN2007-66523 (FORMALISM), and from the Ramo  X  n y Cajal program of the Spanish Ministry of Science and Technology of which he is a research fellow.
 Appendix A
In Table 1 we report the average number of partial solution extensions, out of five runs, required by MS-PTS( S u ) to reach the target node.
 References
