 Mapping one character sequence to another is a structured prediction problem that arises frequently in NLP and computational linguistics. Common applications include grapheme-to-phoneme (G2P), transliteration, vowelization, normalization, mor-phology, and phonology. The two sequences may have different lengths.

Traditionally, such settings have been modeled with weighted finite-state transducers (WFSTs) with parametric edge weights (Mohri, 1997; Eisner, 2002). This requires manual design of the transducer states and the features extracted from those states. Alternatively, deep learning has recently been tried for sequence-to-sequence transduction (Sutskever et al., 2014). While training these systems could dis-cover contextual features that a hand-crafted para-metric WFST might miss, they dispense with impor-tant structure in the problem, namely the monotonic input-output alignment . This paper describes a nat-ural hybrid approach that marries simple FSTs with features extracted by recurrent neural networks.
Our novel architecture allows efficient modeling of globally normalized probability distributions over string-valued output spaces, simultaneously with au-tomatic feature extraction. We evaluate on morpho-logical reinflection and lemmatization tasks, show-ing that our approach strongly outperforms a stan-dard WFST baseline as well as neural sequence-to-sequence models with attention. Our approach also compares reasonably with a state-of-the-art WFST approach that uses task-specific latent variables. Let  X  x be a discrete input alphabet and  X  y be a dis-crete output alphabet. Our goal is to define a con-ditional distribution p ( y | x ) where x  X   X   X  x and y  X   X   X  y and x and y may be of different lengths.
We use italics for characters and boldface for strings. x i denotes the i th character of x , and x i : j notes the substring x i +1 x i +2  X  X  X  x j of length j  X  i  X  0 . Note that x i : i =  X  , the empty string. Let n = | x | .
Our approach begins by hand-specifying an un-weighted finite-state transducer (FST), F , that non-deterministically maps any well-formed input x to all appropriate outputs y . An FST is a directed graph whose vertices are called states, and whose arcs are each labeled with some pair s : t , representing a pos-sible edit of a source substring s  X   X   X  x into a target substring t  X   X   X  y . A path  X  from the FST X  X  initial state to its final state represents an alignment of x to y , where x and y (respectively) are the concate-nations of the s and t labels of the arcs along  X  . In general, two strings x , y can be aligned through ex-ponentially many paths, via different edit sequences.
If we represent x as a straight-line finite-state au-tomaton (Figure 1), then composing x with F (Fig-ure 2) yields a new FST, G (Figure 3). The paths in G are in 1-1 correspondence with exactly the paths in F that have input x . G can have cycles, allowing outputs of unbounded length.

Each path in G represents an alignment of x to some string in  X   X  y . We say p ( y | x ) is the total prob-ability of all paths in G that align x to y (Figure 4).
But how to define the probability of a path? Tra-ditionally (Eisner, 2002), each arc in F would also be equipped with a weight. The weight of a path in F , or the corresponding path in G , is the sum of its arcs X  weights. We would then define the probability p (  X  ) of a path  X  in G as proportional to exp w (  X  ) , where w (  X  )  X  R denotes the weight of an object.
The weight of an arc h  X  s : t  X  X  X  h 0  X  in F is traditionally defined as a function of features of the edit s : t and the names ( h,h 0 ) of the source and target states. In effect, h summarizes the alignment between the pre-fixes of x and y that precede this edit, and h 0 sum-marizes the alignment of the suffixes that follow it.
Thus, while the weight of an edit s : t may depend on context, it traditionally does so only through h and h 0 . So if F has k states, then the edit weight can only distinguish among k different types of preced-ing or following context. That limitation is what we aim to correct in this paper, by augmenting our representation of con-text. Our contextual weighting approach will assign weights directly to G  X  X  arcs, instead of to F  X  X  arcs.
Each arc of G can be regarded as a  X  X oken X  of an edit arc in F : it  X  X pplies X  that edit to a particu-lar substring of x . It has the form i,h  X  s : t  X  X  X  j,h 0 represents the replacement of x i : j = s by t . The finite-state composition construction produced this arc of G by combining the arc h  X  s : t  X  X  X  h 0  X  in F with the path i  X  s j  X  in the straight-line automaton rep-resenting x . The latter automaton uses integers as
Our top-level idea is to make the weight of this arc in G depend also on ( x ,i,j ) , so that it can consider unbounded input context around the edit X  X  location. Arc weights can now consider arbitrary features of the input x and the position i,j  X  X xactly like the potential functions of a linear-chain conditional ran-dom field (CRF), which also defines p ( y | x ) .
Why not just use a CRF? That would only model a situation that enforced | y | = | x | with each character y aligned to x i , since the emissions of a CRF corre-spond to edits s : t with | s | = | t | = 1 . An FST can also allow edits with | s |6 = | t | , if desired, so it can be fit to ( x , y ) pairs of different lengths with unknown alignment, summing over their possible alignments.
A standard weighted FST F is similar to a dy-namic linear-chain CRF. Both are unrolled against the input x to get a dynamic programming lattice G . But they are not equivalent. By weighting G instead of F , we combine the FST X  X  advantage (aligning unequal-length strings x , y via a latent path) with the CRF X  X  advantage (arbitrary dependence on x ).
To accomplish this weighting in practice, sec-tions 4 X 5 present a trainable neural architecture for an arc weight function w = f ( s , t ,h,h 0 , x ,i,j ) . The goal is to extract continuous features from all of x . While our specific architecture is new, we are not the first to replace hand-crafted log-linear mod-
Note that as in a CRF, our arc weights cannot consider arbitrary features of y , only of x . Still, a weight X  X  dependence on states h,h 0 does let it de-pend on a finite amount of information about y (also possible in CRFs/HCRFs) and its alignment to x .
In short, our model p ( y | x ) makes the weight of an s : t edit, applied to substring x i : j , depend jointly on s : t and two summaries of the edit X  X  context:  X  a finite-state summary ( h,h 0 ) of its context in  X  a vector-valued summary of the context in x The neural vector is generally a richer summary of the context, but it considers only the input -side con-text. We are able to efficiently extract these rich fea-tures from the single input x , but not from each of the very many possible outputs y . The job of the FST F is to compute additional features that also depend on the output. 2 Thus our model of p ( y | x ) is defined by an FST together with a neural network. Our arc weight function f will make use of a vec-tor  X  i : j (computed from x ,i,j ) to characterize the substring x i : j that is being replaced, in context. We define  X  i : j as the concatenation of a left vector  X  j (describing the prefix x 0: j ) and a right vector  X  (describing the suffix x i : n ), which characterize x i : j jointly with its left or right context. We use  X  i to
To extract  X  j , we read the string x one character at a time with an LSTM (Hochreiter and Schmidhu-ber, 1997), a type of trainable recurrent neural net-work that is good at extracting relevant features from strings.  X  j is the LSTM X  X  output after j steps (which read x 0: j ). Appendix A reviews how  X  j  X  R q is computed for j = 1 ,...,n using the recursive, dif-ferentiable update rules of the LSTM architecture. We also read the string x in reverse with a second LSTM.  X  i  X  R q is the second LSTM X  X  output after n  X  i steps (which read reverse ( x i : n ) ).
We regard the two LSTMs together as a BiLSTM function (Graves and Schmidhuber, 2005) that reads x (Figure 5). For each bounded-length substring x i : j , the BiLSTM produces a characterization  X  i : j of that substring in context, in O ( n ) total time.
We now define a  X  X eep BiLSTM, X  which stacks up K BiLSTMs. This deepening is aimed at ex-tracting the kind of rich features that Sutskever et al. (2014) and Vinyals et al. (2015) found so effective in a different structured prediction architecture.
The k th -level BiLSTM (Figure 6) reads a se-quence of input vectors x ( k ) R d ( k ) , and produces a sequence of vectors  X  k = 1 , we define x (1) bedding of the character x i  X   X  x . For k &gt; 1 , we
After this deep generalization, we define  X  i : j to be the concatenation of all  X  ( k )
This novel deep BiLSTM architecture has more connections than a pair of deep LSTMs, since  X  ( k ) depends not only on  X  ( k  X  1) Thus, while we may informally regard  X  ( k ) ing a deep summary of the prefix x 0: i , it actually depends indirectly on all of x (except when k = 1 ). Given the vector  X  i : j , we can now compute the weight of the edit arc i,h  X  s : t  X  X  X  j,h 0  X  in G , namely w = f ( s , t ,h,h 0 , x ,i,j ) . Many reasonable functions are possible. Here we use one that is inspired by the log-bilinear language model (Mnih and Hinton, 2007):
The first argument to the inner product is an em-bedding e s  X  R d (1) of the source substring s , con-catenated to the edit X  X  neural context and also (for | s | = 1 , i.e. s is a single character, then we would use the embedding of that character as e s . Note that the embeddings e s for | s | = 1 are also used to encode the local context characters and the level-1 BiLSTM input. We learn these embeddings, and they form part of our model X  X  parameter vector  X  .
The second argument is a joint embedding of the other properties of the edit: the target substring t , the edit arc X  X  state labels from F , and the type of the edit ( INS , DEL , or SUB : see section 8). When re-placing s in a particular context, which fixes the first argument, we will prefer those replacements whose r embeddings yield a high inner product w . We will learn the r embeddings as well; note that their di-mensionality must match that of the first argument. The model X  X  parameter vector  X  includes the d ( K ) = O ( d (1) + Kq ) . It also O ( d (1) S ) parame-ters for the embeddings e s of the S different input substrings mentioned by F , and O ( d ( K ) T ) for the We train our model by maximizing the conditional log-likelihood objective, Recall that p ( y  X  | x ) sums over all alignments. As explained by Eisner (2002), it can be computed as the pathsum of the composition G  X  y  X  (Figure 4), divided by the pathsum of G (which gives the nor-malizing constant for the distribution p ( y | x ) ). The pathsum of a weighted FST is the total weight of all paths from the initial state to a final state, and can be
Eisner (2002) and Li and Eisner (2009) also ex-plain how to compute the partial derivatives of p ( y  X  | x ) with respect to the arc weights, essen-tially by using the forward-backward algorithm. We backpropagate further from these partials to find the gradient of (2) with respect to all our model param-eters. We describe our gradient-based maximization procedure in section 10.3, along with regularization.
Our model does not have to be trained with the conditional log likelihood objective. It could be trained with other objectives such as empirical risk or softmax-margin (Li and Eisner, 2009; Gimpel and Smith, 2010), or with error-driven updates such as in the structured perceptron (Collins, 2002). For a new input x at test time, we can now construct a weighted FST, G , that defines a probability distri-bution over all aligned output strings. This can be and its alignment.

In our present experiments, we find the most prob-able (highest-weighted) path in G (Dijkstra, 1959), and use its output string  X  y as our prediction. Note that Dijkstra X  X  algorithm is exact; no beam search is required as in some neural sequence models.

On the other hand,  X  y may not be the most prob-able string X  X xtracting that from a weighted FST is NP-hard (Casacuberta and de la Higuera, 1999). The issue is that the total probability of each y is split over many paths. Still, this is a well-studied prob-lem in NLP. Instead of the Viterbi approximation, we could have used a better approximation, such as crunching (May and Knight, 2006) or variational de-coding (Li et al., 2009). We actually did try crunch-ing the 10000-best outputs but got no significant im-provement, so we do not report those results. In our experiments, we choose F to be a simple con-textual edit FST as illustrated in Figure 2. Just as in Levenshtein distance (Levenshtein, 1966), it allows We consider the edit type to be INS if s =  X  , DEL if t =  X  , and SUB otherwise. Note that copy is a SUB edit with s = t .
 For a  X  X emoryless X  edit process (Ristad and Yianilos, 1996), the FST would require only a sin-gle state. By contrast, we use |  X  x | + 1 states, where each state records the most recent output character (initially, a special  X  X eginning-of-string X  symbol $ ). That is, the state label h is the  X  X istory X  output char-acter immediately before the edit s : t , so the state label h 0 is the history before the next edit, namely the final character of h t . For edits other than DEL , h t is a bigram of y , which can be evaluated (in context) by the arc weight function w = f ( s , t ,h,h 0 , x ,i,j ) .
Naturally, a weighted version of this FST F is far too simple to do well on real NLP tasks (as we show in our experiments). The magic comes from instead weighting G so that we can pay attention to the input
The above choice of F corresponds to the  X  (0 , 1 , 1) topology X  in the more general scheme of Cotterell et al. (2014). For practical reasons, we actually modify it to limit the number of consec-utive INS edits to 3. 5 This trick bounds | y | to be &lt; 4  X  ( | x | + 1) , ensuring that the pathsums in sec-tion 6 are finite regardless of the model parameters. This simplifies both the pathsum algorithm and the gradient-based training (Dreyer, 2011). Less im-portantly, since G becomes acyclic, Dijkstra X  X  algo-rithm in section 7 simplifies to the Viterbi algorithm. Our model adds to recent work on linguistic se-quence transduction using deep learning.
 Graves and Schmidhuber (2005) combined BiL-STMs with HMMs. Later,  X  X equence-to-sequence X  models were applied to machine translation by Sutskever et al. (2014) and to parsing by Vinyals et al. (2015). That framework did not model any alignment between x and y , but adding an  X  X tten-tion X  mechanism provides a kind of soft alignment that has improved performance on MT (Bahdanau et al., 2015). Faruqui et al. (2016) apply these meth-ods to morphological reinflection (the only other ap-plication to morphology we know of). Grefenstette et al. (2015) recently augmented the sequence-to-sequence framework with a continuous analog of stack and queue data structures, to better handle long-range dependencies often found in linguistic data.
 Some recent papers have used LSTMs or BiL-STMs, as we do, to define probability distributions over action sequences that operate directly on an in-put sequence. Such actions are aligned to the in-put. For example, Andor et al. (2016) score edit sequences using a globally normalized model, and Dyer et al. (2015) evaluate the local probability of a parsing action given past actions (and their result) and future words. These architectures are powerful because their LSTMs can examine the output struc-ture; but as a result they do not permit dynamic pro-gramming and must fall back on beam search.

Our use of dynamic programming for efficient ex-act inference has long been common in non-neural architectures for sequence transduction, including FST systems that allow  X  X hrasal X  replacements s : t where | s | , | t | &gt; 1 (Chen, 2003; Jiampojamarn et al., 2007; Bisani and Ney, 2008). Our work aug-ments these FSTs with neural networks, much as others have augmented CRFs. In this vein, Durrett and Klein (2015) augment a CRF parser (Finkel et al., 2008) to score constituents with a feedforward neural network. Likewise, FitzGerald et al. (2015) employ feedforward nets as a factor in a graphical model for semantic role labeling. Many CRFs have incorporated feedforward neural networks (Bridle, 1990; Peng et al., 2009; Do and Artieres, 2010; Vinel et al., 2011; Fujii et al., 2012; Chen et al., 2015, and others). Some work augments CRFs with BiLSTMs: Huang et al. (2015) report results on part-of-speech tagging and named entity recognition with a linear-chain CRF-BiLSTM, and Kong et al. (2015) on Chinese word segmentation and handwrit-ing recognition with a semi-CRF-BiLSTM. We evaluated our approach on two morphological generation tasks of reinflection (section 10.1) and lemmatization (section 10.2). In the reinflection task, the goal is to transduce verbs from one inflected form into another, whereas the lemmatization task requires the model to reduce an inflected verb to its root form.

We compare our WFST-LSTM against two stan-dard baselines, a WFST with hand-engineered fea-tures and the Moses phrase-based MT system (Koehn et al., 2007), as well as the more complex latent-variable model of Dreyer et al. (2008). The comparison with Dreyer et al. (2008) is of noted in-terest since their latent variables are structured par-ticularly for morphological transduction tasks X  X e are directly testing the ability of the LSTM to struc-ture its hidden layer as effectively as linguistically motivated latent-variables. Additionally, we provide detailed ablation studies and learning curves which show that our neural-WFSA hybrid model can gen-eralize even with very low amounts of training data. 10.1 Morphological Reinflection Following Dreyer (2011), we conducted our ex-periments on the following transduction tasks from the CELEX (Baayen et al., 1993) morphological database: 13SIA 7 X  13SKE, 2PIE 7 X  13PKE, 2PKE 7 X  z and rP 7 X  pA. 6 We refer to these tasks as 13SIA, 2PIE, 2PKE and rP, respectively.

Concretely, each task requires us to map a Ger-man inflection into another inflection. Consider the 13SIA task and the German verb abreiben ( X  X o rub off X ). We require the model to learn to map a past tense form abrieb to a present tense form abreibe  X  X his involves a combination of stem change and affix generation. Sticking with the same verb abreiben , task 2PIE requires the model to transduce abreibt to abreiben  X  this requires an insertion and a substitution at the end. The tasks 2PKE and rP are somewhat more challenging since performing well on these tasks requires the model to learn complex trans-duction: abreiben to abzureiben and abreibt to abgerieben , respectively. These are complex transductions with phenomenon like infixation in specific contexts ( ab zu rieben ) and circumfixation ( ab ge rieb en ) along with additional stem and af-fix changes. See Dreyer (2011) for more details and examples of these tasks.

We use the datasets of Dreyer (2011). Each exper-iment sampled a different dataset of 2500 examples from CELEX, dividing this into 500 training + 1000 validation + 1000 test examples. Like them, we re-port exact-match accuracy on the test examples, av-eraged over 5 distinct experiments of this form. We also report results when the training and validation data are swapped in each experiment, which doubles the training size. 10.2 Lemmatization Lemmatization is a special case of morphological re-inflection where we map an inflected form of a word to its lemma (canonical form), i.e., the target inflec-tion is fixed. This task is quite useful for NLP, as dictionaries typically list only the lemma for a given lexical entry, rather than all possible inflected forms. In the case of German verbs, the lemma is taken to be the infinitive form, e.g., we map the past partici-ple abgerieben to the infinitive abreiben .

Following Dreyer (2011), we use a subset of the lemmatization dataset created by Wicentowski (2002) and perform 10-fold experiments on four lan-guages: Basque (5843), English (4915), Irish (1376) and Tagalog (9545), where the numbers in paren-thesis indicate the total number of data pairs avail-able. For each experimental fold the total data was divided into train, development and test sets in the proportion of 80:10:10 and we report test accuracy averaged across folds. 10.3 Settings and Training Procedure We set the hyperparameters of our model to K = 4 (stacking depth), d (1) = 10 (character embedding dimension), and q = 15 (LSTM state dimension). The alphabets  X  x and  X  y are always equal; their size is language-dependent, typically  X  26 but larger in languages like Basque and Irish where our datasets include properly accented characters. With |  X  | = 26 and the above settings for the hyperparameters, the number of parameters in our models is 352 , 801 .
We optimize these parameters through stochas-tic gradient descent of the negative log-likelihood objective, and regularize the training procedure through dropout accompanied with gradient clip-ping and projection of parameters onto L2-balls with small radii, which is equivalent to adding a group-ridge regularization term to the training objective. The learning rate decay schedule, gradient clipping threshold, radii of L2-balls, and dropout frequency were tuned by hand on development data.

In our present experiments, we made one change to the architecture. Treating copy edits like other SUB edits led to poor performance: the system was unable to learn that all SUB edits with s = t were ex-tremely likely. In the experiments reported here, we addressed the problem by simply tying the weights of all copy edits regardless of context, bypassing (1) and instead setting w = c where c is a learned pa-rameter of the model. See section 11 for discussion. 10.4 Results Table 1 and 2 show our results. We can see that our proposed BiLSTM-WFST model always outper-forms all but the most complex latent-variable model of Dreyer (2011); it is competitive with that model, but only beats it once individually. All of Dreyer X  X  models include output trigram features, while we only use bigrams.

Figure 7 shows learning curves for the 13SIA and 2PKE tasks: test accuracy when we train on less data. Curiously, at 300 data points the performance of our model is tied to Dreyer (2011). We also note that our model always outperforms the Moses15 baseline on all training set sizes except on the 2PKE task with 50 training samples.

In general, the results from our experiments are a promising indicator that LSTMs are capable of extracting linguistically relevant features for mor-phology. Our model outperforms all baselines, and is competitive with and sometimes surpasses the latent-variable model of Dreyer et al. (2008) without any of the hand-engineered features or linguistically inspired latent variables.

On morphological reinflection, we outperform all of Dreyer et al X  X  models on 2PIE, but fall short of his latent-change region model on the other tasks (out-performing the other models). On lemmatization, we outperform all of Wicentowski X  X  models on all the languages and all of Dreyer et al. X  X  models on Irish and Tagalog, but, but not on English and Irish. This suggests that perhaps further gains are possible through using something like Dreyer X  X  FST as our F . Indeed, this would be compatible with much re-cent work that gets best results from a combination of automatically learned neural features and hand-engineered features. 10.5 Analysis of Results We analyzed our lemmatization errors for all the lan-guages on one fold of the datasets. On the English lemmatization task, 7 of our 27 errors simply copied the input word to the output: ate, kept, went, taught, torn, paid, strung . This suggests that our current aggressive parameter tying for copy edits may predict a high probability for a copy edit even in contexts that should not favor it.

Also we found that the FST sometimes produced non-words while lemmatizing the input verbs. For example it mapped picnicked 7 X  picnick, lining 7 X  lin . Since these strings would be rare in a corpus, many such errors could be avoided by a reranking approach that combined the FST X  X  path score with a string frequency feature.

In order to better understand our architecture and the importance of its various components, we per-formed an ablation study on the validation por-tions of the morphological induction datasets, shown in Table 3. We can see in particular that using a BiL-STM instead of an LSTM, increasing the depth of the network, and including local context all helped to improve the final accuracy.  X  X eep BiLSTM w/ Tying X  refers to our complete model. The other rows are ablation experiments X  architectures that are the same as the first row except in the specified way.  X  X eep BiLSTM (No Context) X  omits local context e x STMs w/o Copying X  does not concatenate a copy of x  X  X hallow BiLSTM X  reduces K from 4 to 1.  X  X i-Deep LSTM X  replaces our deep BiLSTM with two deep LSTMs that run in opposite directions but do not interact with each other.  X  X eep MonoLSTM X  redefines  X  i to be the empty vector,g i.e. it replaces the deep BiLSTM with a deep left-to-right LSTM.  X  X hallow MonoLSTM X  replaces the deep BiLSTM with a shallow left-to-right LSTM.  X  X o LSTM (Lo-cal Context) X  omits  X  i : j from the weight function al-together.  X  X eep BiLSTM w/o Tying X  does not use the parameter tying heuristic for copy edits.  X  X o LSTM (No Context) X  is the simplest model that we consider. It removes  X  i : j , e x it is precisely a weighting of the edits in our origi-nal FST F , without further considering the context in which an edit is applied.

Finally, to compare the performance of our method to baseline neural encoder-decoder models, we trained 1-layer and 4-layer neural sequence-to-sequence models with and without attention, us-ing the publicly available morph-trans toolkit (Faruqui et al., 2016). We show the performance of these models in the lower half of the table. The results consistently show that sequence-to-sequence transduction models that lack the constraints of monotonic alignment perform worse than our pro-posed models on morphological transduction tasks. As neither our FST-LSTM model or the latent-variable WFST model of Dreyer et al. (2008) uni-formly outperforms the other, a future direction is to improve the FST we use in our model X  X .g., by augmenting its states with explicit latent variables. Other improvements to the WFST would be to in-crease the amount of history h stored in the states, and to allow s , t to be longer than a single character, which would allow the model to segment x .

We are not committed to the arc weight function in (1), and we believe that further investigation here could improve performance. The goal is to define the weight of h  X  s : t  X  X  X  h 0  X  in the context summarized by  X  ,  X  j (the context around x i : j = s ) and/or  X  j ,  X  i (which incorporate s as well).

Any parametric function of the variables ( s , t ,h,h 0 ,  X  i ,  X  j ,  X  i ,  X  j ) could be used X  X or example, a neural network or a multilinear function. This function might depend on learned embeddings of the separate objects s , t ,h,h 0 , but also on learned joint embeddings of pairs of these objects (which adds finer-grained parameters), or hand-specified properties of the objects such as their phonological features (which adds backoff parameters).

A basic approach along the lines of (1) would use an inner product of some encoding of the arc ( s , t ,h,h 0 ) with some encoding of the context ( s ,h,h 0 ,  X  i ,  X  j ,  X  i ,  X  j ) . Note that this formulation lets the objects s ,h,h 0 play a dual role X  X hey may appear as part of the arc and/or as part of the context. This is because we must judge whether s = x i : j (with  X  X abel X  h  X   X  X  X  h 0  X  ) is an appropriate input seg-ment given the string context around x i : j , but if this segment is chosen, in turn it provides additional con-text to judge whether t is an appropriate output.
High-probability edits s : t typically have t  X  s : a perfect copy, or a modified copy that changes just one or two features such as phonological voicing or orthographic capitalization. Thus, we are interested in learning a shared set of embeddings for  X  x  X   X  y , and making the arc weight depend on features of the  X  X iscrepancy vector X  e t  X  e s , such as this vector X  X  would signal discrepancies of various sorts. We have presented a hybrid FST-LSTM architec-ture for string-to-string transduction tasks. This approach combines classical finite-state approaches to transduction and newer neural approaches. We weight the same FST arc differently in different con-texts, and use LSTMs to automatically extract fea-tures that determine these weights. This reduces the need to engineer a complex topology for the FST or to hand-engineer its weight features. We evalu-ated one such model on the tasks of morphological reinflection and lemmatization. Our approach out-performs several baselines and is competitive with (and sometimes surpasses) a latent-variable model hand-crafted for morphological transduction tasks.
