 Huazhong University of Science and Technology, Wuhan, Hubei, China 1. Introduction
Data mining focuses on the techniques for non-trivial extraction of implicit, previously unknown and potentially useful information from very large amounts of data [1 X 4]. In relation to this, a large amount of SE data is also produced in software development process such as requirement document, design specifications, source code files and information about bugs. These sources contain a wealth of valuable information.

In recent years, software practitioners and researchers have recognized that valuable information can be extracted from SE data to support software development practices and software engineering research. In this direction Mining Software Repositories (MSR) is specially promoted because of commonly avail-ability of software version control repositories, bug tracking repositories and archived communications for most software projects. Practitioners are increasingly applying data mining techniques on MSR to support software development practice and various SE tasks [5].

Software artifact such as source code is an important source of data that can be mined for interested patterns. It is typically structured and also contains semantically rich programming constructs such as variables, functions, data structures, and program structures which indicate patterns. We can uncover useful and important patterns and information by mining source code. Various data mining applications in software engineering have employed source code to aid software maintenance, program comprehen-sion and software components X  analysis.

The primary goal of software development is to deliver high quality software in the least amount of time. To achieve these goals software engineers are looking for tools which automatically detect different type of bugs to deliver high quality software and want to reuse existing frameworks or libraries for rapid software development. To accomplish these tasks practitioners increasingly applying data mining algorithms to various software engineering data [6] to improve software quality and productivity. To deliver high quality software automatic detection of bugs remains one of the most active areas in software engineering research. Practitioners desire tools that would automatically detect bugs and flag the location of bugs in their current code base so they can fix these bugs. In this direction much work has been done to develop tools and techniques which analyze large amount of source code data, to uncover the dominant behavior or patterns and to flag variations from that behavior as possible bugs. One major area in this direction is Rule Mining Techniques which induces set of rules from source code of existing projects and anomalies are uncover by looking for violation of specific rule. Most of the studies used static source code analysis to find programming rules and subsequent rule violation as bugs [7 X 11].

Another dominant work by mining source code is clone detection. Developers often reuse code frag-ments by copying and pasting (clone code) with or without minor adaptation to reduce programming efforts and shorten the development time. It also increase productivity since the code is previously tested and is less likely to have defects. However, clone code may cause potentially maintainability problem, for example when a cloned code fragment needs to be changed in case of change requirement or addi-tional features, all fragments similar to it should be checked for that change. Moreover, the handling of duplicated code can be very problematic such as an error in one component is reproduced in every copy. This problem has focused the attention of researcher towards development of clone detection tools which allow developers to automatically find the locations in code that must be changed when related code seg-ment changes. Several automated techniques for detecting code clones have been proposed differ by the level of comparison unit from single source lines to entire AST/PDG sub-trees/sub-graphs [12 X 17]. Here we focus only on techniques which are using data mining methods and few others leading techniques for clone detection.

The reuse of both software library and application framework is an important activity for rapid soft-ware development at the source level. In recent development setting programmer relies on frameworks and libraries, such as C ++ libraries, Java packages, Eclipse packages, and in-house libraries [18 X 21] that privilege the programmer to create high quality, full featured applications on-time. However, due to rapid change in software these libraries are not well documented and having complex APIs. By mining API usage patterns one can identify dominant and correct library usage patterns across many projects. Different mining techniques have been proposed in the literature which provide samples code [20,22 X  25]. The techniques are different in the way how the developer queries the target source repository to retrieve relevant code example.

Data mining can also be applied to source code change histories to predict bugs and locate possible changes. Many new bugs are introduced by developers who did not notice dependencies between entities, and failed to propagate changes correctly. Studies in [26 X 37] have been conducted to search for possible bugs and guide software changes by mining source code histories.

Source code also plays an important role in understanding large system because the documentation for these system rarely exists and if exists is not up-to-date. Practitioners also apply data mining tech-niques on large software system for program comprehension, architecture recovery or some other tasks mentioned in Section 3.

In this paper, we provide a comprehensive comparison and evaluation of the state of art static source code mining techniques. To date little effort has been spent to evaluate on this leading area of research. Apart from our own initial short survey [38], which examined three approaches, two other surveys have been presented. Kagdi et al. [39] provided survey of approaches those uses frequent-pattern mining to mine software repositories for various software evolution tasks. The surveyed approaches require ex-tensive history of software revisions in repository to be effective. In the other work, Halkidi et al. [40] surveyed approaches those applying data mining techniques on various sources of software engineer-ing data. In contrast, this work focus on a survey approaches those examine the relationship between source code entities, change relationship or reuse of component. This type of investigation is research on analysis methods to support testing, programming and maintenance task.

This work not only provides significant contributions to the source code mining research, but have also exposes how challenging to compare different tools due to the diverse nature of the techniques and target languages.

We aim to identify the essential strengths and weaknesses of individual tools and techniques to make an evaluation indicative of future potential e.g. when one aims to develop a new integrated or hybrid technique which address multiple challenges in one tool rather presenting another new tool. Moreover, by this survey we have made available prominent tools and techniques pertaining to source code mining to practitioners who are interested to improve software development process or its related challenges.
The rest of this paper is organized as follows. After introducing some background of software mining in Section 2, we provide a comprehensive overview of existing techniques on mining source code data in Section 3. Section 4 presents taxonomy of source code mining tools and techniques in term of general criteria in form of table. The organization of surveyed approaches in term of data mining approach is presented in Section 5. Comparison of source code mining tools in term of several evaluative criteria is presented in Section 6. Critiques on evaluated tools in term of strength and weaknesses are presented in Section 7. Finally, discussion and future trends are highlighted in Section 8 and the paper is concluded in Section 9. 2. Software mining
Software mining encompasses the use of data mining to improve software productivity and quality. We speak of software mining when Software Engineering (SE) data is used as input to data mining algorithm to extract latent valuable information from SE data. There are various sources of software engineering data such as code bases, execution traces, historical code changes, mailing lists, software metrics or bug databases on which data mining can be applied to support various SE tasks.

We specifically speak of source code mining when software engineering data pertaining to static source code is used as input to data mining technique. Static analysis of source code has several well-known benefits. Examining the source code without actually executing the code makes the quality of test suites, an active area of research [41,42]. Static analysis also allows code to be tested that is difficult to run in all environments, such as device drivers. 2.1. General code mining process
This section discusses an overall summary of the code mining process discussed in paper. The overall data mining process applied on code bases is shown in Fig. 1. The overall code mining process included more or less following steps:  X  Collecting target data. Collecting source code to local repository and determine what type of dataset  X  Preprocessing. It include extracting relevant data such as static methods, call sequences from source  X  Code transformation. Transforming data in a way adoptable to particular data mining algorithm.  X  Mining techniques. Choosing the appropriate mining algorithm to perform the desired function to  X  Post processing. It transforms mining results into appropriate format required to assist SE task. For  X  Ranking. Candidate results produced by mining algorithm are too many and often irrelevant. Rank-Software patterns mined by source code mining process can be used in following different ways.  X  They can be stored in a specification database so that programmers can refer in future project  X  Frequently appeared patterns can be used to discover implicit programming rules.  X  The mined patterns can be used to discover related defects.  X  Relevant patterns can be used for code optimization.  X  Pattern mining could help to determine code reusability. 3. Overview of static source code mining tools and techniques
Various approaches have been developed to benefit software engineering tasks by using data mining that deal with different types of SE data. The main contribution of this work is to investigate how data mining techni ques used on source code data to improve softw are quality and productivity. This survey is based upon the nature of information mined from target source code which subsequently used to improve software development. For example to reveal underlying correlation among the data set, one may go for mining such rules which show some association between data element. These rules subsequently can be used to detect software bugs as a possible rule violation. Clone mining techniques helps Software engineers to find duplicate code which can subsequently used in maintenance phases, code optimization etc. First we focus on those approaches that are relying on rule mining to detect software anomalies, data mining methods to find clone code and relevant API usage patterns as well as mining version histories to detect bugs and change prediction. Moreover, we also highlight on other areas that use data mining approaches on source code data. Hence, by reading this survey paper researchers are directed to specific source code activity they want to do in future. This list is by no means exhaustive but represents a number of different prominent investigations. 3.1. Mining rules to detect bugs
Rule mining techniques induce set of rules from existing projects which can be used to uncover poten-tial bugs as violation of specific program rule. Several methods were proposed to mine program source code and detect software anomalies as a possible rule violation.

Engler et al. [7] developed a static verification tool by using compiler extensions called checkers (written in the Metal language) to match rule template s, derived from knowledge of typical p rogramming errors, against a code base. Proposed tool extracts programming beliefs from acts at different location of source code by exploiting all possible paths between function call and cross check for violated beliefs e.g. a dereference of a pointer, p, implies a belief that p is non-null, a call to  X  X nlock (1) X  implies that 1 was locked etc. Rule template represent general programming rules such as such as  X  &lt;a&gt; must be paired with &lt;b&gt;  X  and Checkers, match rule templates to find the rules instance and discover code locations where it violates a rule that match an existing template. Two types of rules categories: MUST-rules (inferred from acts that imply beliefs code  X  X ust X  have) and MAY-rules (Inferred from acts that imply beliefs code  X  X ay X  have) are identified. For MUST rules internal consistency is checked and contradictions is directly flagged as bugs; for MAY-rules, a statistically based method is used to identify whether a possible rule must hold. Proposed approach applies statistical analysis, based on how many times the rule holds and how many it does not to rank deviations from programmer beliefs inferred from source code.

PR-Miner (Programming Rule Miner) [8] uses item-set mining to automatically extract general pro-gramming rules from software code written in an industrial programming language such as C and detect violations. It transforms a function definition into an item-set by hashing program elements to numbers. In this conversion process, similar program elements are mapped to the same number, which is accom-plished by treating identifiers with the same data types as identical elements, regardless of their actual names. By using the frequent item-set mining algorithm called FPclose, PR-Miner extracts rules from possible combination of multiple program elements of different types including functions, variables, data types, etc. that are frequently used together and find association among them. For efficiency PR-Miner generates only closed rules from a mined pattern. The rules extracted by PR-Miner are in general forms, including both simple pair-wise rules and complex ones with multiple elements of different types. By identifying which elements are used together frequently in the source code, such correlated elements can be considered a programming rule with relatively high confidence.

Chronicler [9] applies inter-procedural path-sensitive static analysis to automatically infer accu-rate function precedence protocols which specify ordering among function calls e.g. A call to pthread_mutex_init must always be present on program paths before a call to pthread_mutex_lock . Precedence relationship is computed using program X  X  control-flow structure and stored into a reposi-tory which analyses using sequence mining techniques to generate a collection of feasible precedence protocols. CHRONICLER first generates the control flow graph for each procedure and reverses the direction of all edges in control-flow graph to construct the precedence relation. The graphs obtained are fed into the relation builder and a cycle of relation, constraint, and constraint summary calculations is executed. The sequences obtained as a result of this process are then fed to a sequence mining tool. MAFIA [43] to generate the item sets that appear frequently based on a given confidence threshold. The protocols output by the sequence miner and the associated violations are ranked by processing them according to the confidence, length and frequency of occurrence of the protocol. The output of the entire process is a ranked order of function precedence protocols. Deviations from these protocols found in the program are tagged as violations and also represent potential sources of bugs.

Some complex rules may indicate variable correlations, i.e. these variables should be accessed together or modified in a consistent manner. In this direction Lu et al. [11] developed a tool called MUVI to mine variable pairing rules which applied the frequent itemset mining technique to automatically detect two types of bug i.e. (1) multi-variable inconsistent update bugs and (2) multi-variable related concurrency bugs, which may result due to inconsistent update of correlated variables, the variables that need to be accessed together. For example  X  thd-&gt;db_length  X  describes the length of the string  X  thd-&gt;db  X , so whenever  X  thd-&gt;db  X  is updated,  X  thd-&gt;db_length  X  should be updated consistently. The  X  X ccess together X  variables are those which appear in the same function with less than maximum distance statement apart, and collected by statically analysis of each function to form Acc-Set . MUVI X  X  applied FPclose algorithm to Acc_Set database, consisting of the Acc_Sets of all functions from the target program and output set of variable accessed more than minimum support number of functions. MUVI only focused on two kinds of variables: global variables and structure/class fields.

A new approach to mine implicit conditional rules is proposed by Chang et al. [10]. The proposed work detects neglected conditions by applying frequent sub graph mining on C code. Neglected con-ditions are missing conditions, cases or path which if not carefully revealed software open to many security vulnerabilities. Software open to security vulnerabilities often exploited by the attackers such as buffer overflow, SQL injection, Cross-site scripting, format string attacks. In proposed approach pro-gram units are represented as Program Dependency Graphs (PDGs) generated by CodeSurfer a static analysis tool. The PDGs are enhanced by adding directed edges, called shared data dependence edges path. The resulting graphs are called enhanced PDGs (EPDGs). Potential rules are represented by graph minors by contracting some paths edges. Because EPDG minors represent transitive (direct and indirect) intra-procedural dependences between program statements, they capture essential constraints between rules element. Rules are modeled as graph minors of enhanced procedure dependence graphs (EPDGs), in which control and data dependence edges are augmented by edges representing shared data depen-dences. The next step is to mine the resulting EP DGs to identify candidate rules. Rules are found by mining a database of near transitive closure of EPDGs, using frequent sub-graph mining algorithm, to find recurring graph minors on the assumption that the more a programming pattern is used, the more likely it is to be a valid rule. After valid rules are discovered and confirmed the graph database is searched again using a heuristic graph matching algorithm, to find rule violations corresponding to neglected con-ditions.

Kagdi et al. [44] presented two approaches for mining call usage patterns from source code. The first approach is based on the idea of itemset mining. It identifies frequent subsets of items that satisfy at least a user defined minimum support. As a result unordered patterns related to functions calls are generated. Sequential pattern mining applies on partial ordered list of function calls that produces more accurate results and less number of false positive. In general terms these approaches can assist with mining patterns of call usage and thus identifying potential bugs in a software system. 3.2. Mining code clones patterns
Reusing code fragments by copying and pasting with or without minor adaptation is a common activity in software development for quick performance gains during development and maintenance. Studies shows that 20 X 50% of large software systems contain so called clones  X  X imilar program structures X  repeated many times within or across programs in variant forms [12,45,46]. Independently of the reasons why they arise, such cloned structures hinder future maintenance. They complicate programs, make it difficult to trace the impact of change and increase the risk of update anomalies. Other problems triggered by clones include replication of unknown bugs, code bloat and dead code [12]. Many software engineering tasks such as program understanding (clones may carry domain knowledge), code quality analysis (fewer clones may mean better quality code), aspect mining (clones may indicate the presence of an aspect), plagiarism detection, copyright infringement investigation, software evolution analysis, code compaction (in mobile devices), virus detection, and bug detection may require the extraction of syntactically or semantically similar code fragments which makes clone detection an important and valuable part of software analysis [47]. There is a multitude of techniques for detecting code clones have been proposed in literature. Clone detectors [12,13,15 X 17,46,48 X 55]identify similar code automatically. Each technique is differ by the level of comparison unit of source code such as parameterized tokens strings [12,13] AST [14,15,50,54] and PGDs [16,17]. Here we focus on work which used data mining and few other basic tools in this area.

A string based approach to locate code duplication is proposed by Baker [12]. It uses sequence of lines as a representation of source code and detects line-by-line clones. A tool called Dup is developed which detects two type of matching code that is either exactly the same or name of parameters such as variable and constant are substituted. It performs the following sub processes: 1) Lines of source files are first divided into tokens by a lexical analyzer, 2) replacement of tokens (identifiers of functions, variables, and types) into a parameter identifier, 3) parameter tokens are encoded using a position index for their occurrence in the line. 4) All prefixes of the resulting sequence of symbols are then represented by a suffix tree, a tree where suffixes share the same set of edges if they have a common prefix. 5) Extraction of matches by a suffix-tree algorithm, if two suffixes have a common prefix, clearly the prefix occurs more than once and can be considered a clone.

CCFinder [13] is another token based clone detection technique with additional transformation rules to remove minor difference in source code. It transforms source code into tokens sequence through lexical analyzer to detect clone code portions that have different syntax but have similar meaning. It also ap-plies rule-based transformation such as regularization of identifiers, identification of structures, context information and parameter replacement of the sequence. Source normalizations is used to remove super-ficial differences such as changes in statement bracketing (e.g. if (a) b =2 ;vs.if(a){ b =2 ;}). Finally, clone pairs, i.e., equivalent substrings in the token sequence, are identified using suffix-tree matching algorithm.

CP-Miner [53] applies data mining to identify copy-paste defect in operating system code. By using frequent subsequence mining and tokenization technique it detects copy-paste-related wrong variable-name bugs. It transforms a basic block into number by tokenizing its component such as variable, oper-ators, constants, functions etc. Once all the components of a statement are tokenized, a hash value digest is computed using the hashpjw hash function. As a result program become a large sequence which is broken into small sequence by choosing basic block as a unit to get each basic block as a sequence. The CloSpan algorithm is applied to the resulting sequence database to find basic copy-pasted segments. By identifying abnormal mapping of identifiers among copy-paste segments, CP-Miner detects copy-paste related bugs, especially those bugs caused by the fact that the programmer forgot to modify identifiers consistently after copy-pasting.

Another clone detection tool is CloneDetection proposed by Wahler et al. [15]. This tool first converts source code into Abstract Syntax Tree (AST) which contains complete information about source code by using parser. AST is than transformed into XML representation. XML files are further configured to define metadata to show how statements are nested and considered as clones. Frequent itemset mining algorithm inputs XML configuration file and find frequent consecutive statements. Proposed technique only finds exact and parameterized clones at a more abstract level.

Qu et al. [17] Proposed a framework for pattern mining of clone code using spatial and graph base analysis of source code. Source code is first transformed into Program Dependency Graph (PDG), each line of code is encoded to a hash value by converting each node and edge of PDG into index string by applying hashing algorithm. Hence, original source code is converted into encoded graphic sequence. Spatial search is performed on encoded graphic sequence by using Winnowing algorithm to find pair wise matches which serves as input to graph based pattern mining. For graph base pattern mining PDG for each line pair is retrieved and graph based pattern matching VF algorithm applied to find matched sub graph inside the PDG pair. False positive pruning and pattern composition techniques are used to update the pattern database and discover more accurate and meaningful patterns of cloned codes. Finally the detected software patterns can be used for various applications such as pattern analysis, related defect discovery and code optimization.

Rysselberghe and Demeyer [55] present a technique to identify frequently applied changes by mining version histories based on clone detection. CVS deltas are examined and their corresponding source code changes are recorded in a text file. A clone-detection tool CCFinder using parameterized token matching is applied to this text file to find similar pairs of source code changes i.e. clones. The CVS deltas corresponding to these clones are considered as the Frequently Applied Changes (FACs). It is observed these FACs are typically caused by a  X  X ell-established X  solution at one place being replicated at other locations (later eliminated by a function) moving code (considered deleted and then added) and temporary addition of code that was later deleted. These changes are then studied to identify possible maintenance activities, such as refactoring. They also proposed matching frequently applied changes to bug reports helping to identify bugs in the code and solutions to these bugs. The technique is evaluated on the three-year version history of an open-source system, Tomcat. Both high and low threshold values on the number of matching tokens are experimented to detect FACs.
 Basit and Jarzabek [54] introduced the concept of structure clone and proposed a tool called Clone Miner . Proposed tool first extracts simple clones form the source code (similar code fragments) by using simple clone detector, as a default front-end tool [56]. Then by using frequent closed item sets mining to detect recurring groups of simple clones in different method and files. File clone sets and method clone sets are found by the process of clustering from the lower level structural clones respectively. Using this mechanism highly similar group of files and methods are found which consist of groups of cloned entities at successively higher levels of abstraction. Evaluation of proposed technique on several case studies shows the procedure of detecting structural clones can assist with understanding the design of the system for better maintenance and reengineering. 3.3. Mining API usage pattern
Another line of related research is how to write APIs code. A software system interacts with third-party libraries through various APIs. Using these library APIs often needs to follow certain usage pat-terns. These patterns aid developers in addressing commonly faced programming problems such as what checks should precede or follow API calls, how to use a given set of APIs for a given task or what API method sequence should be used to obtain one object from another. Much research has been conducted to extract API usage rules or patterns from source code by proposing tools and approaches that help developers to reuse existing frameworks and libraries more easily including [22 X 25,57,58].
In this direction Michail [58] developed a tool named CodeWeb which described how data mining can be used to discover library reuse patterns in existing applications. It mines association rules such as what application classes inheriting from a particular library class often instantiate another class or one of its descendants. Based on itemset and association-rule mining CodeWeb uncover entities such as components, classes and functions that occur frequently together in library usages. Michail explains by browsing generalized association rules, a developer can discover usage patterns in a way which takes into account inheritance relationship.

Holmes et al. [20] have developed Strathcona, an Eclipse plug-in, that enables localization of relevant code in an example repository. Their approach is based on six heuristics that match the structural context descriptions (parents, invocations and types) encapsulated in the developer code with that encapsulated in the example code. Each heuristic is used to query the code repository, returning a set of methods and classes where the result context matches the query X  X  context. The result is a set of examples (source code examples) that occur most frequently when applying all heuristics.

Mandelin et al. [23], developed a tool called Prospector for automatically synthesize the list of candi-date jungloid code based on simple query that described the required code in term of input and output. A Jungloids is a simple unary expression which helps to determine a possible call chain between a source type and a target type. A jungloid query is a pair ( T in , T out )where T in and T out are source and target ob-ject types respectively. The Jungloid graph is created using both API method signatures and a corpus of sample client programs, and consists of chains of objects connected via method calls. Prospector mines signature graphs generated from API specifications and jungloid graphs. The retrieval is accomplished by traversing a set of paths (API method call sequences) from T in to T out where each path represents a code fragment and a set of paths in turn composes all code fragments to form a code snippet. The code snippets returned by this traversal process are ranked using the length of the paths with the shortest path ranked first from T in to T out .
 Sahavechaphan and Claypool [22] developed a context-sensitive code assistant tool XSnippet, an Eclipse plug-in that allows developers to query for relevant code snippets from a sample code repos-itory to find code fragments relevant to the programming task at hand. XSnippet extends Prospector and adds additional queries, ranking heuristics and mining algorithms to query a code snippet from a sample code repository for code snippets relevant to the object instantiation at hand. XSnippets [22] transforms source classes into corresponding source code model instances using directed acyclic graph which captures class structure represented by inheritance hierarchy, fields, method and class behavior. Code relevance is defined by the context of the code, both in terms of the parents of the class under development as well as lexically visible types for a given method contained in class. A range of instan-tiation queries are invoked from java editor including generic query TQG that returns all possible code snippets for the instantiation of a type, to the specialized type-based TQT and parent based queries TQP, that return either type-relevant or parent-relevant results. User input the type of query, code context in which query is invoked and a specific code model instance to graph based XSnippet system. Mining algorithm BFSMINE, a breath first mining algorithm traverses a code model instance and produces as output, a set of paths that represent the final code snippets meet the requirement of the specified query. Paths can be either within the method scope or outside of the method boundaries, ensuring that relevant code snippets that are spread across methods are discovered. Ranking heuristics are applied to resultant code to remove duplicate, non compilable and non executable path and rank the code on the basis of con-text, frequency and length of snippet. The pruned mining paths are passed to the Snippet Formulation process that transforms each path to corresponding code snippet.
 MAPO [25] developed by Xie and Pei, mines frequent usage patterns of API through class inheritance. It uses API X  X  usage history to identify methods call in the form of frequent subsequences. The code search engine receives a query that describes a method, class, or package for an API and then searches open source repositories for source files that are relevant to the query. The code analyzer analyzes the relevant source files returned by the code search engine and produces a set of method call sequences, each of which is a callee sequence for a method defined in the source files. The sequence preprocessor inlines some call sequences into others based on caller-callee relationships and removes some irrelevant call sequences from the set of call sequences according to the given query. The frequent-sequence miner discovers frequent sequences from the preprocessed sequences. The frequent-sequence postprocessor reduces the set of frequent sequences in some ways.

PARSEWeb [24] developed by Thummalapenta and Xie uses Google code search for collecting rel-evant code snippets dynamically and mines the returned code snippets to find solution jungloids. The proposed technique is based upon the simple query which described the desired code in the form of  X  X ource Destination X  which search for relevant code sample of source and destination object usage and download to form a local source code repository. PARSEWeb analyzes the local source code repository and constructs a Directed Acyclic Graph (DAG). By searching the nodes in DAG PARSEWeb identifies nodes that contain the given Source and Destination object types and extracts a Method-Invocation Se-quences (MISs) that can transform an object of source type to object of destination type by calculating the shortest path between nodes. Similar MISs are clustered using a sequence postprocessor to form solution for given query. The final MISs are sorts using several ranking heuristic and serves as a solution for the given query. The suggested MIS contains all necessary information for the programmer to write code for getting the Destination object from the given Source object. Parseweb also suggest the relevant code sample as well as MISs and uses an additional heuristic called query splitting that helps to address the problem where code samples for the given query are split among different source files. 3.4. Mining co-changes and bug fix changes patterns
As stated in 1 st law of software evolution by Lehman and Belady [59], a system has to undergo continuous change in order to remain satisfactory for its stakeholders. Source code is one of the important artifacts that can be accessed from source code repository at any stage (i.e. version) in the history of the software evolution. Source code version histories contain wealth of information that how source code evolve during development. Information regarding changes made to fix a problem, accommodating new changes, adding new feature are recorded. When a programmer is changing a piece of code, they want to determine which related files or routines are updated to be consistent with these changes. To help identifying the relevant parts of the code for a given task, there is need of tools and techniques that statically or dynamically analyzes dependencies between parts of the source (e.g. [60 X 62]).
Approaches in [26 X 35], applies data mining techniques on source code control change histories, such as CVS to identify and predict software change. These studies shows that suggestions based on historical co-changes are useful to correctly propose the entities which must co-change.

Zimmermann et al. [28,32], developed a tool called Rose which can guide programmers to locate possible changes by mining historical changes such as source code and related files. The proposed tool uses the association rules extraction technique to identify co-occurring changes by exposing relationship between the modifications of software entities. It aim to answer the question, when a particular source code entity (e.g. a function A ) is modified, what other entities are also modified (e.g. the functions with names B and C )? The proposed tool parses the source code and maps the line numbers to the syntactic or physical level entities. These ent ities are represented as a triple ( filename, type, id ). The subsequent entity changes in the repository are grouped as a transaction. An association rule mining techniques is then applied to determine rules of the form B,C  X  A . This information prevents errors due to incomplete changes and finds couplings undetectable by program analysis.
Ying et al. [26], also proposed a technique that also uses association rule mining on CVS version archives. It identifies the change patterns from the source code change history of a system and predict source code change prediction at file level. Each change pattern consists of sets of the names of source files that have been changed together frequently in the past. To provide a recommendation of files rel-evant to a particular modification task at hand, the developer needs to provide the name of at least one the patterns to find those that include the identified starting file(s). The usefulness of recommended files f
Hassan and Holt [29] proposed a method for tracking c hanges of entities. A variety of heuristics (de-veloper based, history based, code layout based, file based and process based) are proposed which are used to predict the entities that are candidates for a change on account of a given entity being changed. CVS annotations are lexically analyzed to derive the set of changed entities from the source code repos-itories.

There is the rich literature regarding bug detection and prediction by mining historical data [36,37,63 X  68]. These studies mine history data to find pattern in bug fix changes. The tool developed by Williams and Hollingsworth [36], DynaMine by Livshits and Zimmermann [63] mines simple rules from soft-ware revision histories. These rules involve mostly method pairs. Williams and Hollingsworth [36,37] proposed method automatically mine bug-fix information from source code repository to improve bug finding/fixing tools. The type of bug considered was a function-return-value check. It is a two step ap-proach. The first step in the process is to identify the types of bugs that are being fixed many times in source code. The second step is to build a bug detector driven by these findings. The idea is to develop a function return value checker based on the knowledge that a specific type of bug has been fixed many times in the past. Briefly, this checker looks for instances where the return value from a function is used in the source code before being tested. The checker does a data flow analysis on the variable holding the returned value only to the point of determining if the value is used before being tested. It simply identi-fies the original variable and determines the next use of that variable. Code checker is used to determine when a potential bug has been fixed by a source code change. It runs over both versions of the source code. If for a particular function called in the changed file the number of calls remains the same and the number of warnings produced by tool decreases, the change is said to fix a bug.

Livshits and Zimmermann [63] use source code versions to mine call usage patterns by using itemset mining. A tool DYNAMINE is proposed which determine useful usage patterns e.g. call pairs. They classified the mined patterns into valid patterns, likely error patterns, and unlikely patterns with addi-tional dynamic analysis. A candidate pattern mined from the version archive considered to be a valid pattern if it is executed a specified number of times and an unlikely pattern otherwise. Likewise, if a valid pattern is also violated (i.e., only a proper subset of the calls are executed) a large number of times, it is considered as an error pattern. Their approach is more specific in finding violation patterns on method usage pairs. For example, blockSignal() and unblockSingal() should always be paired in the source code. In addition to the standard ranking methods they also presented a corrective ranking (i.e. based on past changes that fixed bugs) to order the mined patterns. The approach is validated on Eclipse and jEdit systems. The results indicate that their approach along with the corrective ranking is effective in reporting error patterns.

Williams et al. [64] proposed a method that automatically mine function usage patterns and detect software bugs via static analysis of a single version and evolutionary changes. The patterns specifically considered are the patterns called after (i.e. a function B is called after function A ) and conditionally called after (i.e. a function B is called after function A, but guarded by a condition). Mining the source code repository identifies the instances of such usage patterns. The goal was to find new instances in the current version. Function calls are identified by using C parser. A function usage pattern is the pair of function call found within a distance specified by the number of lines of code. When a function returns a value, using the value without checking it may be a bug.

Kim et al. [69] built BugMem a project-specific bug finding tool which detects potential bugs by analyzing the history of bug fixes and suggests corresponding fixes. It mines bug fixes from software repositories to reconstruct pairs of bug and fix patterns. To construct patterns of defects and their fixes it checks all kind of component in changed region and suggest correct code to repair detected buggy code. 3.5. Mining source code for other purpose
This section provides an overview of mining approaches used to assist with various SE tasks by using any kind of software engineering data.

An approach is proposed in [70] that exploits association rules extraction techniques to analyze defect data. Software defects include bugs, specification and design changes. The collected defect data under analysis are nominal scale variables such as description of defect, priority to fix a defect and its status as well as interval and ratio scale variable regarding defect correction effort and duration. An extended association rule mining method is applied to extract useful information and reveal rules associated with defect correction effort.

Tjortjis et al. [71] employ association rule mining o n source code for grouping t ogether similar entities within a software system. The item set used by them consists of variables, data types and calls to blocks of code (modules), where modules may be functions, procedures or classes. The transaction set thus consists of variables, types accessed and calls made by modules. In their algorithm large item sets are first generated by finding item sets that have a higher support than a user-defined threshold. From this item set association rules with confidence greater than a user-defined threshold are generated. Finally groups of modules are created based on the number of common association rules. A variety of techniques are proposed by applying data mining on source code entities for program comprehension and architecture recovery to support software maintenance [72 X 80].

Kanellopoulos et al. [73] Proposed a framework for knowledge acquisition fro m source code in order to comprehend an object oriented system and evaluate its maintainability. Clustering techniques are used to understand the structure of source code and assessing its maintainability. The proposed framework works by extracting entities and attribute from source code and constructs input model. Another part of the framework is an extraction process which aim to extract elements and metrics from source code. Extracted information is stored in a relational database to apply data mining techniques. Clustering techniques are applied to analyze the input data and provide a rough grasp of the software system for maintenance engineer. Clustering produces overviews of systems by creating mutually exclusive groups of classes, member data and methods based on their similarities.

Pinzger and Gall [72] uses code patterns to recover software architecture. In their approach user spec-ify the code patterns (text and structural information of source code) by describing their association. Based on specified pattern definition input patterns are matched with source files to reconstruct higher-level patterns that describe the software architecture.

Mancoridis et al. [80] Proposed an automatic technique that creates a hierarchical view of the system organization based solely on the components and relationships that exist in the source code. The tech-nique extracts the Modular Dependency Graph (MDG) from source code in order to identify significant connection among the system modules and stores in database. A textual representation of MDG is ob-tained by querying the database. Clustering is pe rformed on MDG that aims to partition the components of a system into compact and well separated clusters.
Sartipi et al. [81] uses both association rule mining and clustering to identify structurally related fragments in the architecture recovery process of legacy system. The source code of a legacy system is analyzed and a set of frequent itemsets is extracted by using clustering and pattern matching techniques. The proposed algorithm defined the components of the legacy system and the best matching component of the system is selected upon user query. Also scores are associated with each possible answer (match) to the user query and thus a ranking of design alternatives can be presented to the user for further evaluation. 4. Analysis of surveyed tools and techniques
The approaches surveyed in Section 3 have a number of common characteristics. They all are working on source code data at some level of software granularity e.g. functions, procedures, classes, variables, data types, structure and files. All extract pertinent information from source code analyzes this informa-tion and derive conclusions within the context of particular SE tasks. We have organized the surveyed approaches in term of four main facets: mining approach, input, results and SE task benefited. Table 1 organized the survey approaches on following four common facets.  X  Mining approach entails the algorithm used by proposed technique. Different algorithm used in  X  Input criterion shows which elements of source code are used as input by data mining tool such as  X  The criterion Results reflects which type of mining information are extracted by each approach e.g.  X  We also included the specific SE task that each approach addressed. This gives a general context to 5. Data mining techniques used to mine source code
This section organizes the surveyed approaches in term of data mining technique used. To apply mining algorithm source code is first transformed into format suitable for particular mining algorithm. This is done by extracting relevant data from the raw source code data for example, static method call sequences or call graphs. This data is further processed by cleaning and properly formatting it for the mining algorithm. For example, the input format for transaction data can be a transaction database where each transaction contains sets of items. In general, mining algorithms fall into following main categories:  X  Frequent pattern mining. Finding frequently occurring patterns.  X  Association rules mining. Find association among frequently occurring patterns  X  Sequential pattern mining. Finding commonly occurring sequential patterns.  X  Pattern matching. Finding data instances for given patterns.  X  Clustering. Grouping data into clusters  X  Graph mining. Graph mining algorithms includes: frequent sub-graph mining, graph matching,  X  Classification. Predicting labels of data based on the already labeled data.
 Table 2 organizes the surveyed approaches in term of data mining techniques they used.
 6. Comparison of source code mining tools
An overall analysis of the tools and techniques with respect to several general characteristics is shown in Table 1. This section compares those approaches which developed a supporting tool as a plug-in for the programming environment. Source code is provided as input to tool and it applies data mining technique to detect frequently co-occurring patterns. Such a tool can predict and suggest probable changes to the source code.
 Tool Availability indicates whether there is documented IDE support for the method/tool. Only a few methods provide direct IDE support. Most of the tools are not freely available hence hampers the quan-titative analysis of tools.

The External Dependencies indicates whether the tool requires other language, environment or tools to work, for example PR-Miner [8] and MUVI require parser to convert source code into item-set database. CCFinder requires language-dependent transformation rules and likewise other tools also have some external dependency.

Language support indicates the languages supported by tools. We can observe that there are very few tools that are aimed at OO-languages (e.g., C ++ ).

Algorithm/technique , Identifies the different algorithms used in source code mining research from other domains. For example, the suffix-tree algorithm computes all of the same subsequences in a se-quence composed of a fixed alphabet (e.g. characters, tokens and hash values of lines) in linear time and space. It can only handle exact sequences. On the other hand data mining algorithms are well suited to handle arbitrary gaps in the subsequences. Apart from data mining techniques other analysis techniques are also observed such as Engler et al. work uses two techniques Internal Consistency which finds errors where programmers have violated beliefs that must hold and Statistical Analysis extracts beliefs from a much noisier sample where the extracted beliefs can be either valid or coincidental.

The Empirical Validation criterion shows the kind of validation that has been reported for each tool and Availability of Empirical Results indicates whether the results of the validations are available. The last criterion, Evaluation , indicates the common systems that have been used as experiment to run the tool. Table 3 compares the surveyed approach in term of tool developed. 7. Critique on source code mining approaches
Table 1 shows most of approaches used frequent pattern mining techniques to identify patterns from source code. By making comparison between these techniques we can identify strength and limitations of these techniques. 7.1. Mining rules to detect bugs
Engler et al. [7] approach relies on developers to supply rule templates such as function A must be paired with function B and corresponding checkers. Since such template-based methods only cover the given or explicit rules known in advance, it may miss many violations due to the existence of implicit rules. Moreover, It only performs one type of pattern analysis such as:  X  X unction A should be paired with function B  X  and does not consider other semantic dependencies.

PR-Miner [8] find implicit programming rules and rule violations that is based on frequent item-set mining and does not require specification of rule templates. It can detect simple function pair-wise rules, complex rules as well as variable correlation rules. However, PR-Miner does not consider relevant con-straints between rule elements and so apparently will identify a set of elements that frequently appear together in functions as a possible rule without other evidence that the elements are semantically related. It computes the association in entire program elements by just counting the together occurrences of any two elements and not considering data flow or control flow which leads to increase number of false neg-ative of violations in control path. Also numbers of false positives are increased as no inter-procedural analysis is used. Both Engler et al. work and PR-Miner discover patterns involving pairs of methods calls and functions, variables, data types that frequently appear in same methods and do not contain control structures or conditions among them, also the order of method calls is not considered. However, compared with Engler et al. work that extracts only function-pair based rules, PR-Miner extracts sub-stantially more rules by extracting rules about variable correlations. Moreover, PR-Miner requires full parser to replace to work with other programming languages.
 These limitations are addressed by inter-procedural path-sensitive static analysis tool CHRONI-CLER [9] which is fundamentally different from PR-Miner as it ensures path-sensitivity hence generate less number of false negative as compared to PR miner. Since CHRONICLER computes association of specific function rather than entire program hence reduces the number of protocol generated by elimi-nating false positive as reported by PR-Miner. It differ from Engler et al. [7] approach as it computes the precedence relationship based on program X  X  control flow structure whereas, Engler et al. work de-tects relations between pairs of functions by exploiting all possible paths. However, CHRONICLER does not take data flow or data dependency into account. A new approach to discovering implicit con-ditional rules [10] addresses this limitation by transforming program units into program dependency graph which captures data and control flow dependencies as well as other essential constraint among program elements. The approach requires the user to indicate minimal constraints on the context of the rules to be sought, rather than specific rule templates. However, frequent sub-graph mining algorithm does not handle directed graphs, multi-graphs (multiple edge between given pair of node) and require the modification of graphs. Modifications such as ignoring edge directions or replacing a call site graph with a single node may cause information loss so that precision is sacrificed in rule discovery. Moreover the approach considered only a small set of nodes in PDGs, and the patterns are only control points in a program.

All of the approaches mentioned above focused on procedures and component interfaces instead of variable correlations where as MUVI [11] mines variable correlations and generate variable-pairing rules. Engler et al. [7] also detect variable inconsistency through logical reasoning for example, some statement indicates that a pointer might be NULL but a subsequent statement assumes that pointer must not be NULL so a conflict arise where as MUVI [11] detect inconsistencies using pattern analysis on multi-variab le access correlations. 7.2. Mining code clones patterns
Dup [12] uses an order-sensitive indexing scheme to normalize for detection of consistently renamed syntactically identical clones whereas, CCFinder [13] applies additional transformations of source code that actually change the structure of the code so that minor variations of the same syntactic form treated as similar. However, token-by-token matching is more expensive than line-by-line matching in terms of computational complexity since a single line is usually composed of several tokens. Token based methods have intrinsic limitation for pattern mining of cloned codes due to using only spatial space analysis such as reordered or inserted statements can break a token sequence which may otherwise be regarded as a duplicate to another sequence. CloneDetection search clones in general tree structure and works on abstract level as compared to others. Dup, CCFinder and CloneDetection identify clone code that can be helpful in software amenability to identify section of code that should be replaced by procedure but do not detect copy paste related bugs. On the other hand CP miner [53] applies data mining technique to identify similar sequence of tokenized statements rather than token comparison and detect copy paste related bugs. Compared to CCFinder, CP-Miner is able to find 17.52% more copy-pasted segments because CP-Miner can tolerate statement insertions and modifications. whereas, Graph based analysis [17] can capture more complicated changes such as statement reordering, insertion and control replacement, compared with the common token-based approaches by capturing software X  X  inherit logic relationship through PDG. However, graph-based techniques are limited in scalability. All the mentioned clone detection techniques detects simple clones i.e. fragment of duplicated code and not looking at the big picture where these fragments of duplicated code are possibly part of a bigger replicated program structure. In contrast Clone Miner [54] performs further analysis on simple clones that co-exists and relates to each other in certain way.
 Simple clone detectors usually detect clones larger than a certain threshold (e.g., clones longer than 5 LOC). Higher thresholds risk false negatives, while lower thresholds detect too many false positives. In comparison, Clone Miner can afford to have a lower threshold for simple clones, than a stand-alone sim-ple clone detector, without returning too many false positives. This is because it can use the grouping as a secondary filter criterion to filter out small clones that do not contribute to structural clones. However, like other approaches it also detects clone based on physical location of clones and not detect semantic associations among clones. 7.3. Mining API usage patterns
CodeWeb demonstrate how the library classes have been reused in existing applications. To get this information, a developer must populate CodeWeb with applications that are similar to the one which they are developing. To use CodeWeb developer must find similar applications of interest in advance. It also uses the structural attributes to compare complete projects against one another instead of enabling the use of fragments of projects. The need to find applications in advance suggests that a developer would be more likely to engage in the use of CodeWeb at the beginning of the development process as it is based on browsing rather than querying.

Given an API sample, Strathcona, Prospector, XSnippet, MAPO and Parseweb provide example code of that API. Strathcona suggest similar code examples stored in an example repository by matching the context of the code under development with the samples stored in the example repository. Strathcona generates relevant solutions when the exact API is included in the search context but mostly programmer has no knowledge of which API has to be used for solving the query. It is based on heuristics which are generic and generate many irrelevant examples.

Prospector tries to solve the queries related to a specific set of frameworks or libraries by using API signatures. As API signatures are used for addressing the query, Prospector returns many irrelevant examples. Strathcona, for example, does not specialize the heuristics it employs based on the developer X  X  context and its results straddle the extremities  X  in some cases providing too many irrelevant results while in others over-constraining the context to provide too few or no results. Prospector, while performing better than Strathcona in general has its own limitations. First, its over-reliance on API information can result in too many irrelevant results. For example, two unrelated paths can be connected by generic classes such as Object and ArrayList discounting the diversity in their semantics. Second, the context description is limited to only visible input types of fields declared in the boundary of method and class while context information such as the  X  X arent X  is ignored thereby missing a set of potentially qualified hits. Prospector can generate compilable code for its suggested solutions.

XSnippet simply returns the set of all code samples contained in the sample repository that instantiate the given destination object type, irrespective of the source object type. Moreover, XSnippet is also limited to the queries of a specific set of frameworks or libraries. Strathcona and XSnippet use the code relevance to define the code context which best fit the required code. However Strathcona only use the lexically visible types to define the code relevance where as XSnippet uses parents of the class under development as well as lexically visible types for a given method contained in class to define code relevance. The major problem with both of approaches is the availability of limited code samples stored in the repository.

MAPO defines a query that describes a method, class, or package for an API, the tool can gather relevant code samples from open source repositories and conduct data mining. It can extract common patterns among the list of relevant code examples returned by a code search engine. It does not synthe-sized code that can be directly inserted into developers X  code. For using MAPO Programmers need to know the API to be used to identify usage patterns of that API.

PARSEWeb like MAPO takes queries of the form  X  X ource object type to destination object type X  as an input and suggests what API method sequence should be used to obtain one object from another potential solution. PARSEWeb search web dynamically for relevant solution and not limited to the queries of any specific set of frameworks or libraries like Prospector and XSnippet. Parseweb uses code sample for solving given query hence identifying more relevant code sample. Prospector which solves the queries through API signatures and has no knowledge of which MISs is often used compared to other MISs that can also serve as a solution for the given query. PARSEWeb performs better in this scenario because it tries to suggest solutions from reusable code samples and is able to identify MISs that are often used for solving a given query. However, PARSEWeb suggests only the frequent MISs and code samples, but cannot directly generate compilable code. Neither PARSEWeb nor Prospector considers the code context. 7.4. Mining co-changes and bug fix changes patterns
Zimmermann et al. [28] and Ying et al. [26] both uses association rule mining on CVS data to mine co-change patterns i.e. is potentially relevant piece of code to a given fragment of source code. However, in Zimmermann et al., approach resultant rules must satisfy some support and confidence. In this way it can give misleading association rules in cases where some files have changed significantly more often than others. Whereas, Ying et al. only uses the support value and additional correlation rule mining, which takes into account how often both files are changing together as well as separately. Both approaches pro-duce similar quantitative results. The qualitative analyses differ. Zimmermann et al. present some change associations that were generated from their approach and argue that these associations are of interest. In contrast, Ying et al. especially evaluated the usefulness of the results by analyzing the recommendations provided in the context of completed modification tasks. Moreover, Zimmermann et al., approach sug-gests the fine grained entities (method and classes) that changed together provide better results because smaller units of source code suggest a similar intention behind the separated code. In contrast Ying et al. approach change patterns describes files that change together repeatedly. In comparison Hassan and Holt [29] proposed tracking changes of more fine grained entities, namely, function, variable, or data type, to determine how changes propagate from one entity to another.

The work by Williams and Hollingsworth [36,37,64] and Dynamine [63] combined revision history mining and program analysis to discover common error patterns. Williams and Hollingsworth [36,37] claims if function return value without first checking its validity is used it may lead to a latent bug. In practice, this approach leads to many false positives, as typical code has many locations where return values are used without checks. Moreover, they focus on prioritizing or improving only existing error patterns. Instead Dynamine [63] concentrate on discovering new patterns and dynamic analysis of de-tected patterns leads to less number of false positive. Additionally, Williams and Hollingsworth [64] also mines version histories for detecting bugs in method usage pair by focusing only on pair of function used together and their violation, in contrast Dynamine uses usage patterns of functions to detect violation.
All the previous bug detection techniques searches for predefined common bug patterns such call usage or method pair. In contrast BugMem [69] learned from previous bug fix change in specific project so the bug patterns are project specific, and project-specific bugs can be detected. However, it only considered bug fixing patterns and does not consider the changes which may introduce new bugs. 8. Discussion and future trends
A wide range of research has been done in the area of checking and enforcing specific coding rules, the violation of which leads to well-known types of bugs. Since data mining algorithms are traditionally meant for large dataset stored in database or warehouse and not directly applicable on source code. A great deal of time and effort has been spent by researcher to find worthwhile rules due to the complexity of data extraction and preprocessing methods. Most of bug detection techniques are application specific. As a result lesser known types of bugs and applications remain virtually unexplored in error detection research. A better approach is needed if we want to test new or unfamiliar applications with error de-tection tools. Furthermore, the rule mining approaches detects general programming rules from source code. The performance could be improved if domain specific information is combined and some knowl-edge about specific rules is provided to rule mining technique to extract only of them from source code. It could notably increase the accuracy and efficiency. Moreover, bug finding techniques solely relies on source code data or historical changes. In this way bug reported by the tester become isolated from development team. Based on our observation a system of bug finding techniques which correlates bug reports and the corresponding source code changes might be helpful.

There is lot of studies related to find bug fixes from history data. All are focus on one aspect of change e.g. bug fixes change [69]. All types of change pattern could provide useful information to the developers when they are changing their code. The developers need to use multiple techniques on one project to find impact of change. However finding all existing program file change patterns like bug fixing, bug introducing and bug fix introducing might be helpful. We concludes there is a strong need of light weight approach to use prior bug finding techniques together to maximize bug detection capability.
Also a variety of architecture recovery techniques are available. A common idea is to integrate several tools in architecture workbenches. In this way a variety of techniques will available in one umbrella to examine a system and extract static and dynamic views to reconstruct a system.

We have analyzed the major applications of clone detection. It signals weak points in the program and encourages the restructuring and refactoring . A fully automatic replacement of clones by higher order structures is certainly not the best choice. But in this aspect integration with an interactive program development environment would be very helpful. 9. Conclusion
In this paper we have provided concise but comprehensive survey of state of art source code mining tools and techniques. So far this is first survey which includes combination of different techniques. Comparison of techniques and tools shows, there is a no single technique which is superior to all other in every aspects because all techniques have strength and weaknesses and intended for different task and context. The comparison also helps how to employ a set of different tools to achieve better results.
The results of this survey show all the previous studies mines a specific pattern types to accomplish a certain SE task. Thus, programmers need to employ multiple methods to mine different kind of useful information which increase computational cost and time. However, SE tasks increasingly demand the mining of multiple correlated patterns together to achieve the most effective result. Based on our ob-servation a hybrid light weight tool is required. The tool should extract multiple patterns from source code and applies data mining techniques at different layers to assists in multiple software engineering tasks over different phases of development life cycle e.g. assisting programming in writing code, bug detection and software maintenance. In this direction we are working on development of light weight tool to extracts a variety of patterns from source code as presented in [84]. The work is in its initial stage and in future further research would enrich in context of pattern findings and their violation. References
