 The inclusion of document length factors has been a major topic in the development of retrieval models. We believe that current models can be further improved by more re-fined estimations of the document X  X  scope. In this poster we present a new document length prior that uses the size of the compressed document. This new prior is introduced in the context of Language Modeling with Dirichlet smooth-ing. The evaluation performed on several collections shows significant improvements in effectiveness.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Retrieval models General Terms: Performance, Experimentation.
 Keywords: Document Length, Document Priors, Language Models, Compression.
Document length is recognized as an important compo-nent to achieve state of the art performance in Informa-tion Retrieval (IR). For instance, popular IR models, such as BM25 [4], pivoted vector space models [5] or Language Models with Dirichlet smoothing [6], incorporate some form of document length correction.

However, these document length corrections have been often based on very rough estimations of the document X  X  contents, such as the size of the document in bytes or the number of terms in the document. Here, we argue that the scope of a document should be captured using more elabo-rated measures. In this poster, we propose to apply a docu-ment length prior obtained from the size of the compressed document. Our research hypothesis is that, to estimate a document X  X  scope, this compression-based measure is more reliable than either the original size of the document or the number of terms in the document. If two documents have equal size but the compressed size of one of them is much smaller than the other document X  X  compressed size then this seems to indicate that the former document is more verbose than the first one. To the best of our knowledge, this simple idea, which can be easily implemented and evaluated, has not been explored for ad-hoc document retrieval. In con-trast, the use of compression-based techniques has attracted a great deal of attention in areas such as classification and clustering [3, 1].
 to improve LM estimations [2] but they were only benefi-cial to Jelinek-Mercer (JM) smoothing, and not to Dirichlet smoothing. This is because JM smoothing does not provide any length normalization, and so a length-based prior pro-vides some form of correction. In this poster we demonstrate that our novel prior is actually able to produce significant improvements in effectiveness over the standard Dirichlet model. We compare the following non-uniform priors 1 : where com ( d ) is the size (bytes) of the compressed doc-ument (zipped) divided by the original size (bytes) of the document. These priors will be referred to as terms prior andzippedprior,respectively.
The priors described above were evaluated with three ad-hoc collections (TRECs 5, 6 &amp; 8, 50 queries each) and a web collection (WT10g, 100 queries). We applied the Porter stemmer, removed common words using an standard sto-plist, and tested the following values for  X  : 10, 500, 1000, 2000, 3000, 4000, 5000, 10000, 50000. Although we ran ex-periments with short queries (title subfield) and long queries (all subfields), we report here only the results for short queries. With long queries, there was no significant differ-ence between priors.

The best MAP and P@10 results for each prior are shown in Table 1 and can be summarized as follows. First, the terms prior is worse than the standard Dirichlet configura-tion (i.e. uniform prior). This is not surprising as the same outcome was reported in [2]. Dirichlet with an uniform prior already incorporates length correction ( | d | in eq. 3) and it does not get further benefits from adding length normaliza-tion through a document prior. Second, the zipped prior shows the best performance and, in most of the cases, the improvement over the uniform prior is statistically signif-icant. Furthermore, we studied how performance evolves across all parameter settings and found that the improve-ment obtained with the zipped prior is very stable. In Fig-ure 1 we show how the performance of the methods evolves against  X  . The figure represents only the WT10g collec-tion but the same trends occurred in all the collections with both evaluation measures. The se results are very remark-able as they demonstrate that the zipped prior beats the other priors not only in the best case scenario but also in any non-optimal case. For a clearer view, the graph shows only  X  values up to 5000 but the trends remain the same for higher  X  values.

With long queries, the difference between distinct priors was negligible. No advantage or disadvantage was found with compression. This makes sense because standard Dirich-let tends to retrieve too many long documents when queries are short but this excessive promotion of long documents does not happen with long queries [2] (as n grows the penal-ization of long documents increases). Therefore, the zipped prior, which penalizes verbose documents, is less useful with long queries.
We also tested a prior based on the number of unique terms and another based on the size in bytes but there were no major differences between these priors and the terms prior.
