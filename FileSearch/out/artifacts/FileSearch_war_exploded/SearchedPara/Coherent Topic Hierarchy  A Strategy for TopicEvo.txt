 Jiahui Zhu 1 , Xuhui Li 2 , 3 , Min Peng 2 , 4( The prosperity of the microblogging services at all levels of the social life has been witnessed in the past few years. This kind of services, such as Twitter and Sina Weibo, are becoming the preferred online platforms for people to express their experiences, opinions, thoughts, etc. Therefore, microblog feeds, i.e., the textual aspects of user generated contents, gradually act as the carriers of topics which somehow reflect public concerns at that time. With time elapsing, these topics often evolve in both word presentation and occurrence intensity, leading to complex relations between every two sequential time points. Modeling the topic evolution can be of great significance to track user preferences or event stages, thus providing helpful guidance in personalized recommendation, opinion summarization and emergency detection.
 tion or transition patterns, are usually considered as the mainstream technolo-gies of solving the topic evolution due to their advantages of high robustness and fitness. These variants of LDA [2, 3] not only strive to extract the topics over the word distribution, but also reconcile the topics to their evolutionary threads. Apart from the topic models, classical data mining methods, including clustering [ 4 ], frequent patterns mining [ 5 ] and automatic summarization [ 6 ]are also quite suitable for the topic evolutionary analysis. These non-parametric or less parametric methods are overall more efficient and motivated than the topic models which are always involved in intractable training and inferring. all of the above methods. The first one is how to effectively extract a set of semantically coherent topics from the microblog feeds which have the charac-teristics of short text, erratic form and voluminous amount. The topics released by this kind of LDA variants [2, 3] are named as latent topics , with a paucity of concentration and interpretation. These incoherent topics are not appropri-ate for the evolutionary analysis as they may entangle users in the semantic confusion. The second challenge is how to efficiently construct the evolutionary structure. The results of the methods based on classical data mining are not always intuitive enough since they pay little attention to the topology of the topics. Recent works [7, 8] attempt to clarify that the relations of different top-ics can be modeled into the structure of the hierarchy, namely topic hierarchy . This type of hierarchical structure sketches the topic evolutionary patterns both horizontally and vertically, resulting in a more succinct presentation compared with a welter of topic threads as in [ 6 ].
 we design a coherent topic hierarchy (CTH) to improve the coherence of topics as well as the presentation of evolution. First, the biterm topic model (BTM) [ 9 ] is incorporated to discover some coherent topics from the microblog feeds at each time point. Then the simBRT, which is a topic similarity enhanced version of the basic Bayesian rose tree (BRT) [ 10 ], is developed to figure out the relations of the topics at different time points. Finally, a cross-tree random walk with restart (CT-RWR) model is established to effectively generate the whole topic hierarchy along the timeline. Our contributions are listed as follows: analysis. In this hierarchical strategy, both the topic coherence and the relation structure are carefully considered. world topics topology of the microblog feeds. Therefore, the topic similarity is of great utilization during the tree construction. (3) We formulate a CT-RWR model to measure the relations between each pair of sequential trees, so as to maintain a hierarchical structure along the timeline. Topic Model based Methods : In order to analyze the evolution of topics, researchers were assiduous to devise probabilistic topic models with temporal information. Some of the fundamental works include DTM [ 2 ], TOT [ 11 ], and OLDA [ 3 ]. To further discover some essential evolutionary patterns, the evolu-tionary hierarchical Dirichlet processes (EvoHDP) [ 12 ] was proposed. However, the EvoHDP requires a cascaded Gibbs sampling scheme to infer the model parameters, which may prohibit the explicit comprehension. More efficient mod-els, such as TM-LDA [ 13 ], focus on formulating the topic transition through matrix decomposition, while neglecting the interpretation of its topics. All the topic models above are incoherent topic models, which fail to present meaningful topics.
 Coherent Topic Models : To enhance the interpretation of the topics, coherent topic models [ 14 ] are developed to refine the LDA X  X  topics in a more meaning-ful way. Usually, the coherent topic models are linked to the knowledge bases [ 15 ], and the domain information is helpful to improve their semantic coherence. Coherent topic models are also known as focused topic models, which concen-trate on the posterior sparsity of the document-topic and topic-word distribu-tions. Presenting the document-topic and topic-word distributions sparsely, like the DsparseTM in [ 16 ], can sufficiently enhance the distinctiveness of the topics. The biterm topic model [ 9 ] is also a coherent topic model that is designed to better fit the short text cases. In this paper, we employ the BTM to extract some coherent topics.
 Topic Hierarchy : Hierarchical structure can vividly reveal the relations between topics and their traces along timeline [ 17 ]. Topic hierarchy is not easily constructed when encountered with multi-source texts [ 18 ]. Recently, Bayesian rose tree [ 10 ] has attracted great attention in text mining [ 7 ] due to its high fitness and smoothness. It can organize the relations of documents or topics into a multi-branch tree through likelihood maximization. Though the interpretable characteristics of coherent topic models and the fitness of Bayesian rose tree are quite appropriate for modeling the topic concepts, previous works still concern little about their cooperation. The main objective of our work is to construct a coherent topic hierarchy along the timeline for topic evolutionary analysis. Generally, the construction of our CTH can be divided into three parts: (1) Coherent topics extraction : Including the coherent topic model BTM and sparsification of topic distributions.
 on the combination of BRT and topic distribution similarity. topic hierarchy along the timeline by our CT-RWR.
 3.1 Coherent Topics Extraction In our cases, the whole microblog feeds are regarded as a stream S = D point t , d t M is the M th feed in D t . After text modeling with vector space of vocabularies, the BTM can be employed to extract the coherent topics. first extracts K topics from the feeds batch D t by using the document-level word co-occurrence patterns, i.e., biterms. These K topics are the topic-word distributions, denoted as  X  t , each of which is a multinational distribution over the vocabulary with Dirichlet prior  X  . Then for each biterm in the feeds batch, topics are assigned by collapsed Gibbs sampling [ 9 ] to form the topic assignment sequence of all the feeds. Finally, the feed-topic distribution  X  prior  X  can be inferred by counting the proportion of the biterms X  topic assign-ments indirectly. By performing the BTM, we can derive the feed-topic distri-bution  X  t  X  R M  X  K and the topic-word distribution  X  t  X  R K  X  V where M is the number of the feeds and V is the size of the vocabulary. bution should be ensured. In real-world microblog stream, it is also reasonable to assume that each topic is only related with a limited number of words, rather than the whole vocabulary [ 16 ]. Hence we define the sparsity of each topic as sp ( sp V ), that means each topic only cares about no more than sp words in this vocabulary. Therefore, we improve the BTM via considering the sparsity of word distributions of its topics to make it more appropriate in generating coherent topics from microblog stream. The procedures of the topics sparsifi-cation are: 1) For each topic in  X  t , select the sp most relevant words with the highest probabilities as its topic descriptive words; 2) sum up the probabilities over these sp words, denoted as p sum ; 3) normalize each of these sp probabilities with p sum , while set the rest irrelevant probabilities as zeroes. We denote these sparse topic-word distributions as  X  t , namely, the coherent topics. 3.2 Topic Tree Construction In this section, we organize the coherent topics at each time point to a topic tree. As the merge or split patterns of topics in real-world applications are not always one-on-two , so the traditional topic hierarchy based on binary tree may fail to figure out the intrinsic topology. To overcome this problem, we take the advantage of the Bayesian rose tree [10], a kind of multi-branch tree which allows one-to-many relations between topics. To further reveal the real topic topology, we take the topic similarity into account. We name our tree as simBRT. Basic BRT. Usually, the BRT is constructed in a greedy aggregation. For a BRT at time point t , all the coherent topics  X  t = { z t the main problem is how to organize them into a multi-branch tree to better fit the text data. At first, each topic z t i is regarded as an individual tree on its own, namely, T t i = { z t i } . Then in each iteration, two trees are selected to greedily consist a new tree T m by one of the three following basic operations: (1) Join: T t m = { T t i ,T t j } ,so T m has two branches. (2) Absorb: T t m = { ch ( T t i ) ,T t j } ,so T m has | (3) Collapse: T t m = { ch ( T t i ) ,ch ( T t j ) } ,so T Here ch (  X  ) denotes the children set of a tree. The join is the traditional oper-ation as in binary tree, while the absorb and collapse operations cater to the multi-branch tree. These three operations are shown in Figure 2. In each itera-tion, the combining objective is to maximize the following ratio of probability: where  X  t m =  X  t i  X   X  t j are the coherent topics under tree structure T is the likelihood of topics  X  t m under T t m . Previous works [7, 10] have addressed that p (  X  t m | T t m ) can be calculated through a dynamic programming paradigm: where f (  X  t m ) is the marginal probability of  X  t m , which can be modeled by the DCM distribution [ 7 ], ch ( T t m ) is the children set of T probability that all the topics in T t m are kept in the same partition,  X  defined as: where n T t m = | ch ( T t m ) | , and 0  X   X   X  1 is the partition granularity. simBRT. Eq.1 has provided high fitness to the tree construction, but in our scenario, the tree nodes are the topic distributions, rather than the document vectors in previous works. To simultaneously achieve high smoothness, we must also take the topic similarity into consideration. To this end, we refactor Eq.1 by adding the similarities of the topic distributions into join, absorb, or collapse operations.
 Leibler divergence can be used to measure the similarity between every two topics. For topic z i and z j , their similarity is defined as: two topics. To obtain the topic similarity in tree construction, the most impor-tant step is to define the weighted topic distribution in each operation. by T t i and T t j . Hence, their weighted topic distribution is defined as: all the children of T t i . Then the weighted topic distribution is defined as: children of T t i and T t j . So the weighted topic distribution is defined as: Specifically, the final merged topic distribution under T the topic similarity between the weighted topic WT and the final merged topic avg (  X  m ) is added into the primitive objective function in Eq.1. Thus, Eq.1 can be rewritten as: Therefore, in our scenario, to construct the most reasonable tree to interpret the real topology of the topics is equivalent to maximize Eq.8 with join, absorb, or collapse operation at each step. In this way, we can derive a similarity enhanced Bayesian rose tree of the coherent topics at time point t . 3.3 Timeline Topic Hierarchy Construction In Section 3.2, we have structured a tree of all the coherent topics at each time point. To further analyze the evolution of the topics along the timeline, we need to bond each pair of sequential trees with topic transition metrics. Sequential Trees Modeling with CT-RWR. A traditional way of measuring the relations of topics from two sequential time points is to directly compute the KL divergence of each pair of topics, as in [ 5 ]. However, in this case, the tree structure of the topics is neglected. In order to make full use of the tree structure, we not only take the KL divergence, but also take the likelihood of the First Common Ancestor (lFCA) as the similarity metrics.
 For two sequential simBRTs at time t  X  1and t , we model the topic evolution based on the restart version of random walk (RWR) [ 19 ]. As our RWR model is settled between two sequential trees, we name it cross-tree RWR. In traditional RWR, topic z i evolves to other topics with probabilities p and evolves to itself with probability p ii . Usually, the evolution yields to a steady state after several times of walking. Here, for all the topics in  X  topics in  X  t , we have two types of similarities: inner-tree similarity and cross-tree similarity . The inner-tree similarity within either  X  the lFCA of each pair of topics: For the cross-tree similarity between  X  t  X  1 and  X  t , we resort to KL divergence Then the transition probability matrices are as follows: where D t  X  1 , D t and D  X  are the degree matrices respectively. According to the RWR model, the inner-tree state probability matrices R t  X  mulated by the iterative ways as in [ 19 ]. Consequently, both of these two state probability matrices will converge after several steps. The converged matrices can be calculated as: where  X  and  X  are prior probabilities that the topic will not evolve to itself. Cross-tree Relations Organization. Note that the key in our timeline topic hierarchy construction is the cross-tree transition probability matrix P seems not so easy to be deduced (like Eq.11) because of its dynamic characteris-tic. In our cases, the inner-tree transition probability matrices P stable, as we have constructed the simBRT to reveal their intrinsic relations. As for the cross-tree transition probability matrix P  X  , its stability is not guaran-teed. For the topic  X  t  X  1 i in time point t  X  1 and the topic  X  transition may be influenced not only by the similarity of this pair of topics, but also some other topics that are implicitly related to  X  t  X  cross-tree transition patterns. After taking into full account of the transition rules, we focus on two types of cross-tree transition patterns: Before-&gt; Before-&gt; After (BBA) and Before-&gt; After-&gt; After (BAA), as shown in Figure 3. Then the stable transition probability, denoted as SP  X  , can be calculated as: where  X  is the prior probability of the transition pattern BBA. R all the relations that come from the topics in time point t involves all the relations that come from the topics in time point t . relevance) of the topics between two sequential batches both in topic-word dis-tributions and tree structures. Additionally, we give out two thresholds  X  and  X  to burn out the weak relations. The relations are selected by two rules: 1) All the relations with strength above  X  will be reserved, while all the relations with strength below  X  will be discarded. 2) In other cases, for each topic in  X  the topic with the highest strength from  X  t  X  1 will be related.
 capture the whole topic hierarchy along the timeline. To evaluate the effectiveness of our CTH, we carry out our experiments on both Sina Weibo feeds and Twitter feeds. The Sina Weibo feeds are gathered by our web crawler 2 when given the search keywords, e.g.,  X  Two Sessions  X ,  X  MH 370  X . We set 65 keywords, each of which represents a global topic. The dataset spans from Jan. 1 to Sep. 15 in 2014, with totally about 6.6 million feeds labelled by these 65 keywords. All the data is divided into 257 batches, each of which contains all the feeds produced in one day. The Twitter feeds dataset is provided by [ 8 ], including 12 global topics and approximately 3 million feeds from Oct. 1, 2012 to Jan. 4, 2014. The Twitter feeds are divided into 231 batches, two days per batch.
 Preprocessing work involves feeds batch filtering, word segmentation, stop words removal, vector space modeling, etc. Here we use Jieba segmentation. All the algorithms are implemented in Python except the BTM 4.1 Evaluation of Topic Coherence To evaluate the coherence of topics is difficult since there is no such thing as standard  X  X oherence X . In [ 16 ], the point-wise mutual information (PMI) has been adopted as a metric to measure the semantic coherence of topics. Therefore, we also take the PMI as the major criterion. Here, we compute the PMI of the topics extracted by the sparse BTM (in our CTH) against the LDA (as a baseline) in both the Sina Weibo and Twitter dataset. Additionally, the topic-word distribution sparsity sp is set to 5. The results are shown in Figure 4. From Figure 4(a) and Figure 4(b), we can see that the PMI scores of the BTM are higher than the LDA in both datasets, indicating the better coherence of the topics produced by the BTM. Meanwhile, the PMI scores of the Sina Weibo topics are expressly higher than those of the Twitter topics. This is partly because that the Twitter feeds contains more noisy words or phrases and the word feature space of their topics is larger than that of the Sina Weibo topics. Compared with the smaller word feature space, the larger one is usually not so workable to reveal topics with semantic coherence. 4.2 Evaluation of simBRT To directly evaluate the effectiveness of our simBRT is also an open ended ques-tion. However, the tree structure somehow reflects a clustering result of the topics. Hence we conduct our experiments on the normalized mutual informa-tion (NMI) and cluster number error (CNE) as used in [ 7 ]. The NMI reveals the similarity of topics within the same cluster, while the CNE measures the devia-tion of the cluster number between the simBRT and the ground-truth. A higher NMI and a lower CNE indicate a better performance, so we use the NMI/CNE to present the soundness of our simBRT. The results of 10 sequential batches (10 trees) from the Sina Weibo dataset are also displayed in Figure 4. We employ the basic BRT without topic similarity enhancement as a baseline. Here, the topics number K is set to 20 to provide an over-complete topics set.
 is higher than that of the BRT. This can be sure that the simBRT integrates the topic distribution similarity while the basic BRT only depends on the tree struc-ture. Figure 4(d) displays the NMI of each topic tree, where the NMI is highly consistent with the NMI/CNE. Figure 4(e) describes the number of clusters of each topic tree. In our scenario, the clusters number is judged by the number of branches in the second level (root is the first level). Also, we can see that the clusters number of our simBRT is more approximate to the real topology, nearly half of the total cases is strictly equal to the ground-truth, while the BRT is not so well-behaved.
 4.3 Timeline Topic Hierarchy Presentation We manipulate each pair of sequential trees with the CT-RWR from Mar. 4, 2014 to Mar. 10, 2014 (totally a week) in Sina Weibo dataset. Here, the topic number K is set to 10 as to give a more succinct structure. The prior probabilities  X  and  X  in CT-RWR are all set as 0.8. Besides, we set the prior probability  X  of the cross-tree transition pattern BBA to 0.5, as to share equal probability with BAA. As for the burning thresholds  X  and  X  for screening out the weak relations, we set them as 0.9 and 0.4 (approximate values after normalized by the range of the relation scores) respectively according to their count distribution, as displayed in Figure 4(f). The coherent topic hierarchy is shown in Figure 5.
 In Figure 5, each simBRT contains ten coherent topics and each pair of sequential simBRTs is bonded with colorful edges. Note that each color is related to a certain global topic, e.g., all the green nodes are the coherent topics of the global topic  X  Kunming Attack  X . Focusing on the topic tree at Mar. 4, as we can see, all the light blue nodes are put together, so are the green ones. This means that the topics linked to the same global topic can be accurately aggregated together by the simBRT. But this is not always true when the topic tree is too broad (in Mar. 5). Besides, all the simBRTs during this week contain lots of branches at the second level. This is partly because that the topic distinctive-ness of the BTM is favorable. And this also suggests that the simBRT can still aggregate the similar topics in such a strictly discriminative environment. According to the whole topic hierarchy, we can explicitly pick out a thread of a global topic. For example, we can see that during this week, the global topic  X  Two Sessions  X  is popular, thus a complete thread (the light blue thread in Figure 5)  X 1-1-4-10-6-10-8 X  runs through all the timeline. Besides, we can also grasp the keywords evolution of this topic, for example, the keywords mentioned in the first day of  X  Two Sessions  X  X re X  CPPCC+committee+reporter  X , but in the next day, the keywords are  X  committee+deputy+NPC  X , as listed in the middle column of Table 1 (topic keywords of thread  X 1-1-4-10-6-10-8 X ). What X  X  more, we can also discover some emergent topics in this topic hierarchy. For example, in the simBRT at Mar. 8, we can see a red node labelled with  X 9 X  that is not related to a previous node and the red color is the first come. In this situation, a new global topic occurs. This global topic is the  X  MH 370  X . In general, the coherent topic hierarchy is rather beneficial for evolutionary analysis of topics. In this paper, we present a hierarchical strategy by constructing a timeline coher-ent topic hierarchy to perform the topic evolutionary analysis on microblog feeds. In our CTH, the semantic coherence is highly ensured by the sparse BTM, and the topic relation is soundly enhanced by the topic distribution similarity based on the Bayesian rose tree. Particularly, the cross-tree relation is well modeled by the CT-RWR, which creates an efficient and precise hierarchical connection along timeline. Our experimental results show that the coherence of topics in our CTH is higher than those of the LDA. Meanwhile, the simBRT outperforms the basic BRT in topics clustering. Besides, the timeline coherent topic hierarchy is quite reasonable for topic evolutionary analysis. In the future, we will refine the CTH to generate more coherent topics and elaborate the CT-RWR to fit for big data cases, so as to consolidate its scalability in real-world applications.
