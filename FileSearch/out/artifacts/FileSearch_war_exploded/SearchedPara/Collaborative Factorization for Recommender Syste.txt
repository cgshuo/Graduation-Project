 Recommender system has become an effective tool for in-formation filtering, which usually provides the most useful items to users by a top-k ranking list. Traditional recom-mendation techniques such as Nearest Neighbors (NN) and Matrix Factorization (MF) have been widely used in real rec-ommender systems. However, neither approaches can well accomplish recommendation task since that: (1) most N-N methods leverage the neighbors X  behaviors for prediction, which may suffer the severe data sparsity problem; (2) MF methods are less sensitive to sparsity, but neighbors X  influ-ences on latent factors are not fully explored, since the latent factors are often used independently. To overcome the above problems, we propose a new framework for recommender systems, called collaborative factorization. It expresses the user as the combination of his own factors and those of the neighbors X , called collaborative latent factors, and a ranking loss is then utilized for optimization. The advantage of our approach is that it can both enjoy the merits of NN and MF methods. In this paper, we take the logistic loss in RankNet and the likelihood loss in ListMLE as examples, and the corresponding collaborative factorization methods are called CoF-Net and CoF-MLE. Our experimental results on three benchmark datasets show that they are more effective than several state-of-the-art recommendation methods.
 H.3.3 [ Information Search and Retrieval ]: Information filtering Recommender System, Nearest Neighbors, Matrix Factor-ization, Learning to Rank  X 
The work was performed when the first author was a visit-ing student at Institute of Computing Technology, Chinese Academy of Sciences.

Recently, recommender system has become an effective tool for information filtering, and has played an importan-t role in many popular web services, such as Amazon, Y-ouTube, Netflix, Yahoo! etc. In most applications, the core task of a recommender system is to well predict the utili-ties of different items, and then return to users top-k useful items by a ranking list.

In literature, the main recommendation techniques can be divided into two categories, i.e. Nearest Neighbors (NN) and Matrix Factorization (MF). Nearest neighbors methods [2], leverage user X  X  neighbors behavior for recommendation. However, this approach may fail as the severe data sparsity in recommendation makes the computation of of neighbors not accurate any more. Compared with NN, MF methods, such as SVD [9] and SVD++ [4], adopt the latent factors model to uncover the interests of a user, thus it is less sensi-tive to data sparsity. However, the latent factors of different users are usually used independently, and the neighbors X  in-fluences are rarely explored.

In order to take the advantages of both techniques, we propose a new framework called collaborative factorization (CoF for short). The central idea is to introduce the col-laborative philosophy into matrix factorization by defining the latent factors of a user as the combination of his own factors and those of his neighbors X  in traditional MF, called collaborative latent factors. This is also in accordance with the intuition that when making a decision, the user X  X  own opinion and his/her friends X  suggestions may both play an important role.

In this framework, we treat recommendation as a ranking problem, which is more consistent with the typical output of recommender systems (i.e. a ranking list of most useful items). A ranking loss defined on the basis of the collab-orative latent factors is then utilized for optimization. To avoid overfitting, regularizations are further taken in CoF. In this paper, we use both pairwise and listwise losses s-ince they are two dominant ranking losses in the literature of learning to rank. Specifically, we use the logistic loss in RankNet [5] and the likelihood loss in ListMLE [6], and the corresponding methods are called CoF-Net and CoF-MLE, respectively.

Finally, we conduct extensive experiments on the dataset-s of Movielens and Yahoo!Rand. The experimental results show that our proposed CoF-Net and CoF-MLE can signif-icantly outperform several state-of-the-art recommendation methods, including NN, MF and the Hybrid methods.
To sum up, the contribution of this paper lies in that: (1) The introduction of collaborative latent factors, which can be viewed as a novel approach to leverage collaborative philosophy in matrix factorization; (2) The proposal of a unified framework to solve the rec-ommendation problem, which is convenient to accommodate any form of ranking losses.

The rest of the paper is organized as follows. In Section 2 we briefly discuss related work. The collaborative factoriza-tion framework is presented in Section 3. The experimental results are discussed in Section 4. We summarize our work in the last section.
Nearest Neighbors [2] is a classical recommendation ap-proach. It leverages users X  neighbors who have similar rating behavior for prediction. However, the rating data is often highly sparse, which makes the chosen neighbors unreliable.
Matrix factorization [1, 4, 8, 9, 10, 11] is another popular recommendation technique. It explains ratings as the inner product of the latent factors of items and users. The usage of low rank latent factors makes it less sensitive to sparsity, but the neighbors X  influences are highly ignored.
There are also some work trying to combine the two tech-niques to enhance recommendation, such as re-ranking mod-el and hybrid model [4, 12]. For example, Koren et al. [4] proposed to directly combine matrix factorization mod-el and item-based neighborhood model from algorithm-level to make more accurate model. Our work is different from their methods: (1) they aim at minimizing rating predic-tion error, while we use ranking based loss; (2) they do not fully explore neighbors X  influences, while we introduce the collaborative idea in expressing user latent factors.
Please note that Ma etc. [7] conduct similar user repre-sentation, however, the objective function in their work is regression-based loss, and the social connections are explic-itly given. Compared with their approach, our setting is more general and the motivation is different.
Our approach uses latent factors model (i.e. matrix factor-ization) as its centric form. A latent factors model for rec-ommendation can be formalized in the following way. Given a set of m users U , a set of n items I , and an observed rating matrix R , one aims to learn two low rank matrixes P and Q for users and items respectively by optimizing the following objective function: where L ( R; P; Q ) is the loss function, P is a k  X  m matrix, Q is a k  X  n matrix, and k is the dimension of latent fac-tors.  X  P  X  2 F and  X  Q  X  2 F are regularization items to overcome overfitting and P and Q are parameters. The idea behind latent factors model is that ratings can be explained as the inner product of the latent factors of items and users.
In traditional matrix factorization [9], recommendation task is viewed as a regression problem where one aims to ap-proximate the observed rating matrix R as the inner product of matrix P and Q . Therefore, the loss function takes the form of mean square error as following where column vector p u in P stands for the factors of user u , and column vector q i stands for the factors of item i .
From the above process, we can see that the latent fac-tors are used independently, and the neighbors X  influences is ignored in MF methods.

In our approach, we show how to introduce collaborative philosophy into MF and propose a novel framework for rec-ommendation, namely collaborative factorization. Here we first introduce the collaborative latent factors.
Inspired by the intuition that when making a decision, the user X  X  own opinion and his friends X  suggestions are both taken into consideration, we propose to model the latent factors of a user as the combination of his/her own factors and those of his/her neighbors X , as defined as follows, called collaborative latent factors. The formalization is as follows wh ere is the parameter to show the trade-off between the user X  X  own interest and his neighbors X  influences, F ( u ) stand-s for the set of neighbors of user u , and S uv ( S uw ) stands for the similarity between user u and v ( w ). Any user-user similarity metric can be used in the above definition.
With the collaborative latent factors, ratings R ui can be represented as the inner product of the corresponding fac-tors, formulated as follows.

R ui =  X  p T u q i =( p T u + (1  X  )
With the collaborative latent factors defined above, we now formalize our collaborative factorization framework. Since the typical output of a recommender system is usual-ly a top-k ranking list, we naturally treat recommendation as a ranking problem. Therefore, the goal of collaborative factorization is to minimize the following subject: where L ( R u 1 ;  X   X   X  ; R un ) is a ranking loss of user u  X  X  ratings on item set I , and the other notations are the same as men-tioned before.

We can see that the proposed collaborative factorization is a general framework which can accommodate any kind of ranking losses. In this paper, we adopt the pairwise and listwise losses, which are two dominant ranking losses in the literature of learning to rank. We call the corresponding approach as pairwise collaborative factorization and listwise collaborative factorization.
In pairwise collaborative factorization, a pairwise loss L is used, and the ranking loss on the item sets are defined as the sum of losses on all the pairs: where D u stands for the pairs constructed from user X  X  rating profile. For example, if a user has rated item i higher than j , then the pair ( i; j ) is included in D u .
Any pairwise ranking loss in learning to rank can be used in Eq. (6), such as the hinge loss in RankSVM, the expo-nential loss in RankBoost, and the logistic loss in RankNet. In this paper, we take the logistic loss in RankNet as an ex-ample, and obtain the following method, named CoF-Net.
In listwise collaborative factorization, a listwise loss L used, and the ranking loss on the item sets are defined as the loss on the generated list: where u stands for the permutation generated from user X  X  rating profile. For example, if a user has rated item i higher than j , and j higher than k , then the generated permutation is ( i; j; k ).
 Any listwise ranking loss in learning to rank can be used in Eq. (8), such as the likelihood loss in ListMLE, the entropy loss in ListNet, and the cosine loss in RankCosine. In this paper, we take the likelihood loss in ListMLE as an example, and obtain the following method, named CoF-MLE. wh ere 1 ( i ) represents the item ranked in position i of , and | R u | is the length of user u  X  X  rating profile.
For optimization, we just use stochastic gradient descent in this paper. We omit the details of algorithm iteration framework due to the lack of space and the reader can refer to [8, 10] for similar description.
In this section, we conduct extensive experiments on bench-mark datasets to show the superiority of our proposed col-laborative factorization methods.
Datasets : We use three benchmark datasets for experi-ments: Movielens100K, Movielens1M 1 and Yahoo!Rand 2 .
The Movielens100K dataset consists of about 100 K rat-ings made by 943 users on 1628 movies. The Movielens1M dataset consists of about 1 M ratings made by 6040 users on 3706 movies. Following the experimental strategies used in [10, 13], for each user, we sample N ratings for training, and sample 10 ratings from training set to tune hyper parame-ters (validation set). After that, we fix the hyper parameters and the validation set are added to the original training set, then we retrain the model. In our experiments, we set N as 20 and 50, and users with less than 30 and 60 ratings are removed to ensure that NDCG@10 can be obtained.

The Yahoo!Rand dataset is a much newer dataset. It con-tains ratings for songs collected from two different sources. The first one consists of 300,000 ratings supplied by 15,400 users with at least 10 existing ratings during normal inter-action with Yahoo! Music services between 2002 and 2006. The second one consists of ratings for randomly selected ht tp://www.grouplens.org/node/73 http://webscope.sandbox.yahoo.com/ songs collected during an on-line survey conducted by Ya-hoo! Research and hosted by Yahoo! Music between August 22, 2006 and September 7, 2006. 5400 Participants were asked to rate 10 songs selected at random from a fixed set of 1000 songs. All the ratings from the first source are used for training, and ratings from the second source is used for test-ing. The experiments on this dataset are more convincing, since the test data are collected on random sampled songs.
Baseline Methods : Besides the typical collaborative filtering methods such as user-based K-Nearest-Neighbors (KNN) [2] and matrix factorization methods aim to mini-mize RMSE (Root Mean Square Error), e.g. Probabilistic Matrix Factorization (PMF) [9], we also use state-of-the-art ranking based methods including BPR [8] and ListMLE-MF [11, 10] as baselines. Furthermore, we also conduct the experiments on a hybrid method (HYBRID) (Equation 16 in [4]). Here we modify the HYBRID method by learning user-user weight (similarity) other than the original item-item weight, since we focus on investigating the user-wise neighborhood effect throught this paper.

We find that further precision improvements can be achi-eved by extending global average rating, user rating bias, item rating bias, item-based implicit feedbacks and item-based neighbors information to our framework (we verify this in separated experiments not reported here). But we omit these terms like most of the collaborative ranking meth-ods [1, 8, 9, 10, 11, 13, 14] to keep fairly comparison with our baselines.

Hyper Parameters : Latent factors dimension is fixed as 5 in all methods. Learning rate and regularization values are chosen from { 10 1 ; 10 2 ; 10 3 ; 10 4 ; 10 5 } for every model. The size of nearest neighbors (chosen by Pearson correlation coefficient) is set to 100 in HYBRID. For CoF-Net and CoF-MLE, we choose neighbors by cosine similarity and set the nearest neighbors size as 50 which makes them work very well. We don X  X  investigate the effect of other neighborhood size and other similarity in detail due to the page limitation. The best value of combination coefficient is 0.15 for CoF-Net, and 0.65 for CoF-MLE.

Evaluation Measures : The traditional evaluation mea-sure RMSE places equal emphasis on high rating and low rating. Therefore, it is not suitable for the top-k recom-mendation scenario. In this paper, we choose Normalized Discount Cumulative Gain (NDCG) [3] as the evaluation measure, which is a popular evaluation metric in the com-munity of information retrieval. NDCG can leverage the relevance judgment in terms of multiple ordered categories, and has an explicit position discount factors in its definition as follows: where : { 1 ; 2 ; :::; N }  X  { 1 ; 2 ; :::; N } is a permutation gen-erated in the descending order of the predicted ratings, and 1 ( p ) stands for the item that is ranked in position p of the ranking list . N K ( u ) is the normalization factors. We set K to 10 like in [1, 10, 14], and we report the average NDCG@10 over all users in our experiment.
The experiment results are listed in Table 1. From the results, we can see that: (1) Models combining MF and NN will obtain enhanced performances. For example, on Yahoo!Rand, the NDCG@10 of KNN, PMF and HYBRID is 0.802, 0.823, and 0.830, respectively. KNN obtains the lowest performance due to the unreliable neighbors computation on sparse data. PM-F works better since it employ dimensionality reduction to relieve sparsity. By combining MF and NN, HYBRID ob-tains even better performance. Similar results can be found in the ranking based methods. By introducing the collab-orative latent factors, CoF-MLE consistently outperforms ListMLE-MF (about 1%) and CoF-Net consistently outper-forms BPR (about 2%). (2) RMSE minimization is not optimal for top-k recom-mendations. From the results in Table 1, we can see that models which aim at minimizing rating prediction error per-form worse than the ranking based models. For example, PMF obtains lower NDCG@10 scores than all of our rank-ing based CoF methods (i.e. CoF-MLE, CoF-Net, BPR, LISTMLE-MF). (3) As the methods combining MF and NN, the proposed collaborative factorization (CoF) outperforms the tradition-al hybrid methods (HYBRID). From Table 1, CoF-Net is the best model and it significantly outperform HYBRID (about 2%). We can see that by introducing the collaborative idea to the representation of user latent factors, the neighbors X  effect are better explored in CoF than HYBRID, which di-rectly combines MF and NN from algorithm-level.
In this paper, we propose a new recommendation frame-work to enjoy both the merits of nearest neighbors and ma-trix factorization, called collaborative factorization. Firstly, collaborative latent factors are defined as the combination of the user X  X  own factors and those of his neighbors X . Sec-ondly, a ranking loss is utilized as the objective for top-k recommendation. Our experiments on several benchmark datasets show that our approach can significantly outper-form the traditional methods.

As for future work, we can further study whether there exists other ways to introduce collaborative idea to matrix factorization.
 This research work was funded by the National Natural Science Foundation of China under Grant No. 61232010, No. 61203298, No. 61003166, No. 60973003, 863 Program of China under Grants No. 2012AA011003, and National Key Technology R&amp;D Program under Grant No. 2011BAH11B02, No. 2012BAH39B02, No. 2012BAH39B04. We also wish to express our thanks to Yahoo!Research for providing the Ya-hoo!Rand dataset to us. [1] S. Balakrishnan and S. Chopra. Collaborative ranking. [2] M. Deshpande and G. Karypis. Item-based top-n [3] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [4] Y. Koren. Factor in the neighbors: Scalable and [5] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. [6] F. Xia, Tie-Yan Liu, J. Wang, W. S. Zhang and H. Li, [7] H. Ma, I. King, and M. R. Lyu. Learning to [8] S. Rendle, C. Freudenthaler, Z. Gantner, and [9] R. Salakhutdinov and A. Mnih. Probabilistic matrix [10] Y. Shi, M. Larson, and A. Hanjalic. List-wise learning [11] T. Tran, D. Q. Phung, and S. Venkatesh. Learning [12] Gideon Dror, Noam Koenigstein, Yehuda Koren, [13] M. N. Volkovs and R. S. Zemel. Collaborative ranking [14] M. Weimer, A. Karatzoglou, and M. Bruch. Maximum
