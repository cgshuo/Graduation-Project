 With an increasingly amount of information in web forums, quick comprehension of threads in web forums has become a challeng-ing research problem. To handle this issue, this paper investigates the task of Web Forum Thread Summarization (WFTS), aiming to give a brief statement of each thread that involving multiple dynamic topics. When applied to the task of WFTS, traditional summarization methods are cramped by topic dependencies, topic drifting and text sparseness. Consequently, we explore an unsuper-vised topic propagation model in this paper, the Post Propagation Model (PPM), to burst through these problems by simultaneously modeling the semantics and the reply relationship existing in each thread. Each post in PPM is considered as a mixture of topics, and a product of Dirichlet distributions in previous posts is employed to model each topic dependencies during the asynchronous discus-sion. Based on this model, the task of WFTS is accomplished by extracting most significant sentences in a thread. The experimental results on two different forum data sets show that WFTS based on the PPM outperforms several state-of-the-art summarization meth-ods in terms of ROUGE metrics.
 H.3.1 [ Content Analysis and Indexing ]: abstracting methods; H.3.3 [ Information Search and Retrieval ]: Information filtering; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation Web Forums, Thread Comprehension, Summarization, Topic Mod-eling
Web forum is an online portal for open threads on specific is-sues. For each thread, a user makes an initial post and others ex-press their opinions by replying to some previous ones. In web forums, people participate in threaded discussions to deliver knowl-edge and ask questions. Unfortunately, they have to face ascending amount of redundant thread information when browsing web fo-rums. Generally, people often read only few posts ranked ahead in a thread with large posts quantities, whereas this scanning style only reflects incomplete views of the conversation. Hence with an increase of the thread volume, the requirement for summarizing each thread to help human comprehension is becoming more and more urgent. Intuitively, a smooth transition from multi-document summarization to web forums seems to be reasonable. However, as complicated and dynamic topics exist in each ongoing thread, traditional multi-document summarization methods fail to capture the following three characteristics that are crucial to summarize the content of one thread, especially in those ones with large quantities of posts:
Topic Dependencies . Thread is a kind of asynchronous conver-sation based on temporal topic dependencies among posts. When one thread participant A replies to another post X  X  author sider a topic dependency has been built from user B to user among posts dominant the topic dependencies [8].

Topic Drifting . A conversation always results in various sub-topics. As the post conversation progresses, the semantic diver-gence among these subtopics will be widened.

Text Sparseness . Most posts are composed of short and el-liptical messages. As short texts do not provide sufficient term co-occurrence information, traditional text representation methods, such as  X  X f-idf X , have several limitations when directly applied to the mining tasks [5].

In this paper, we investigate the task of Web Forum Thread Sum-marization (WFTS), aiming to generate a compressed version of a given thread delivering the majority of topics adequately. To handle the three challenges, tracking those dynamic topics existing in each thread has become a significant work to generate the summariza-tion. In recent years, topic models has attracted much attention to the topic discovery and tracking in complicated structural data [6, 9, 13, 1, 14]. This paper explores a novel topic model, called Posts Propagation Model (PPM) [11], that takes account of topic depen-dencies , topic drifting and text sparseness during the thread model-ing process.

Illuminated by the Dirichlet-tree distribution [2], PPM connects topic dependencies based on reply-relations so that topic distribu-tions depend on the product of parameters of its modified struc-tural ancestors. Based on probabilistic distributions derived from the PPM, we accomplish WFTS task by extracting significant sen-tences to generate the summarization. Topic-sensitive markov ran-dom walks(MRW) and mutual reinforcement between posts and sentences is employed for sentence scoring process.
The rest of this paper is organized as follows: Our problem for-mulation is demonstrated in section 2. The Model (PPM) is demon-strated in section 3. In section 4 PPM-based WFTS is detailed. Section 5 presents and discusses the experiments and results, and section 6 concludes the paper.
In this section, we discuss the summarization problem in detail and then present the definition of the WFTS task. First, we intro-duce some basic concepts in web forums:
D EFINITION 1(P OST ). A post is a user-submitted message containing the content and the time it was submitted. A group of related posts constitute into one thread, where all posts appear as boxes one after another. Except for the root post in one thread, each post is submitted to "reply" one of its previous post in the thread.
D EFINITION 2(T HREAD ). A thread is a collection of posts, usually displayed from oldest to latest. One thread begins with a root post that may contain questions or news, and is followed by a series of non-root posts, each of which replies to one of its previous posts in the same thread.

As mentioned in previous section, ascending amount of redun-dant thread in web forums make the quick comprehension become more and more difficult for participants. Based on this structure in web forums, we investigate the problem of quick comprehen-sion of one forum X  X  thread with multiple posts. An intuitive way to help people understand the thread rapidly is to generate a brief and understandable summary for all posts in the ongoing thread. Therefore, this paper handles the Web Forum Thread Summariza-tion (WFTS) task that extracts sentences from the thread to generate a summary. To formally define our problem, we give the definition of the task of Web Forum Thread Summarization(WFTS): For one thread with | S | sentences Web Forum Thread Summarization(WFTS) aims to generate a brief statement of the whole thread by extracting Lim significant sen-tences from all posts, where Lim is a threshold of the summary size.

An intuitive way to solve WFTS task is just transfer traditional document-summarization methods to the WFTS task. However, ex-istence of multiple dynamic topics and the short-text nature in each post obstruct this transformation process. As we have mentioned previously, topic dependencies , topic drifting and text sparseness are three main challenges in WFTS process. To handle the three challenges, using a dynamic way to track all topics seems to be a reasonable solution. Thus, in this paper we employ a dynamic topic model, efficient in track topics among complicated documents, to model each thread in web forums.
Based on the reply relations among posts, we extract the reply-relation tree (RRT) structure to represent each thread, intuitively.
D EFINITION 4(R EPLY -R ELATION T REE ). The built-in struc-ture of a thread can be represented by a tree ( r, V, E )  X  u, v  X  V , u, v  X  E iff post v replies post u .

For our modeling task, thread in web forums is a kind of on-going document. If we handle all posts as a whole collection,
Figure 1: Graph model representation for Posts Propagation every time to generate the summary would have to re-train the whole model. Thus a trade-off agglomeration process [7] is em-ployed here to merge those short posts submitted close in time. At last, RRT =( r, V, E ) is transformed into a graph structure as RRG(reply-relation graph) =( s, b V, b E ) , which is defined below. b E ) is a directed network derived from a RRT =( r, V, E ) ,where is a source node with in-degree is 0 and r  X  s . b V and b E node set and edge set of RRG respectively.  X  t  X  b V , note ( t ) notes a nodes subset of V , such that, all nodes of note ( t ) reduced into the node in b V and labeled by t .Edge t i ,t  X  u  X  t i ,  X  v  X  t j , u, v  X  E in RRT =( r, V, E ) .
Given the RRG =( s, b V, b E ) after the agglomeration, we primar-ily define the following notations for each thread.
As we all know, the topic dependencies and the topic drifting both come with reply-relations among nodes in RRG. Thus for node t (not the source node) in RRG =( s, b V, b E ) , a reasonable as-sumption is derived that there are some semantic dependencies be-tween t and all its predecessors in paths from t to s . Illuminated by the Dirichlet-tree distribution [2], we calculate the topic dependen-cies in node t using a product of Dirichlet distributions placed over the probabilities of each internal node on each path from source node to t . We denote this calculation as PoD process.
Given RRG =( s, b V, b E ) ,for note ( t ) ,t  X  b V , dependencies is generated from a product of Dirichlet distribution over decessors. Let  X  t,t be the edge weight from node t to one of its successors t , which characterizes the contact between t and is the set of edges included in any path from the source node t . To generate  X  t  X  PoD ( {  X  c,d , X  c , X  d } ( c,d )  X  E the posterior PoD values for t  X  X  predecessors; then  X  t is derived from the product of posterior parameters of each edge.
To reduce another influence of the text sparseness , background semantic similarities is added to the PPM modeling process. As mentioned earlier, participants usually use different terms to de-scribe the same topic. Intuitively, it will be helpful to integrate several semantic similarities into the calculation of topic depen-dencies. For the vocabulary in a thread, we establish a W  X   X  similarity matrix W sim where each row corresponds to a similarity vector including semantic similarities from  X  most similar words. We obtain the semantic similarity sim ( w, w ) between word w , from the WordNet:Similarity tool 1 . Thus for post collection we rewrite the parameter  X  z,t over topic z :
Thus the calculation of PoD with {  X  c,d , X  c , X  d } ( c,d been introduced in Equation 1. Given RRG =( s, b V, b E ) posts agglomeration, the generative process for each post collection note ( t ) in the PPM is as follows: 1. For the vocabulary, establish the similarity matrix W sim 2. For each topic z, 1  X  z  X  K :Draw 3. For each post p, p  X  note ( t ) :
Figure 2 shows a graph model representation of the PPM, where shaded and unshaded nodes indicate observed and latent variables, respectively. For each note ( t ) in RRG =( s, b V, b E ) distribution is intractable because of the unknown relation between  X  and  X  t . To find an approximate inference method, we use the Gibbs EM algorithm [12] for inference that optimizes the edge parameters {  X  c,d } ( c,d )  X  E t and the Dirichlet prior sampling iteration.
For each post collection note ( t ) in RRG =( s, b V, b E ) two parametric matrix  X  t and  X  t after Gibbs EM sampling, which reflect the topic-post distribution and the word-topic distribution, respectively. i.e P ( z | p )=  X  z,p ; P ( w | z t )=  X  z,t,w distributions, a naive summarization method is proposed as the ba-sic algorithm. Then we introduce the markov random walk(MRW) strategy and the mutual reinforcement effect between posts and sen-tences to our WFTS process. http://www.cogs.susx.ac.uk/users/drh21
We can calculate the probability of each topic by summing and normalizing mixture components over topics for all of posts in the thread. By assuming that each post X  X  generation is assigned the same probability, we have: where | V | is the amount of all posts. Equation (3) actually indi-cates the synthetic salience of topic z all over the dynamic conver-sation. Meanwhile, we assume that each sentence is generated by a mixture of topics. Thus given sentence s j ,wehave: P ( s j )= where P ( z ) is derived from Equation (3), and n ( w ) reflects the term-frequency of w in sentence s j . Based on Equation (4), each sentence X  X  probability in the thread is assigned. We rank the sen-tences by the value of probabilities from Equation (4).
The basic algorithm is a little unrealistic that each post X  X  gener-ation is assigned the same probability 1/ | V | . Illuminated by the idea of the Topic-sensitive PageRank [4], we utilize a new WFTS method with the markov random walks(MRW), where each sen-tence is scored by the "votes" from other sentences based on an complete weighted undirected graph.

Formally, let G S =( S, E S ) be the complete undirected graph with S nodes and E S edges, where there are | S | sentences in the thread and each edge ( s i ,s j )  X  E S has an affinity weight that reflects the similarity between sentence s i and s j ,i = j ter topic modeling, each sentence s has the topical feature vector D s,z = { P ( w | z, s ) } W w =1 ,that: where we utilize the W sim to reduce the sparsity when sim ( w, w )  X  W D j ,z , the topical divergence is given:
Di JS ( s i ,s j ,z )= 1 where M is the average of the two probability vectors, KL ( D M ) is the Kullback-Leibler Divergence.The divergence is transformed into similarity measure by:
So we have the affinity matrix SIM for all S sentences, SIM sim ( s i ,s j ) . After the row-normalization for SIM , transition prob-ability matrix SIM is built that each row smoothing vector 1/ | S | , salience score Sco ( s j ) for each sentence s is deduced by: where  X  is the damping factor usually set to 0 . 85 as in PageRank algorithm. The salience score vector Sco for all sentences are set to 1 at the iteration beginning. Usually the convergence of the iter-ation algorithm is achieved when the difference between the scores computed at two successive iterations for any sentences falls below a given threshold. Sentences are ranked according to their salience scores converged in Sco .
All of the naive scoring and the MRW-based scoring ignore the effect of the posts to sentences X  salience. Intuitively, a post is im-portant if (1) it includes the important sentences; (2) it associates to other important posts. On the other hand, a sentence is important if (1) it appears in an important post; (2) it associates to the other important sentences. Thereby, mutual reinforcement(MR) [16, 15] between sentences and posts is employed to calculate the salience of sentences based on the MRW-based scoring procedure.
Let Pco denote salience scores for all posts in the thread, we construct the salience score vector Rco =[ Pco,Sco ] T for the MR framework. Each post p can be represented in the form of: (1) a vector of post-topic distributions D p = { P ( z | p ) } K matrix KW where item KW i,j denotes the topic-word distribution The first representation is used for the post to post affinity calcula-tion whereas the second is used for the affinities between posts and sentences. In the MR framework, the key problem is to construct the blocking matrix PSM including the affinities among objects (sentences and posts).
 where PIM denotes the post-post affinity matrix. Item PIM reflects the similarity measure between post p i and post
Meanwhile, both affinity matrices PS and its transposed matrix of
SP reflect the similarities between posts and sentences. For each item ( i, j ) in PS ,wehave:
After the row-normalization for the block matrix PSM , the rank-ing of posts and sentences can be iteratively derived from the MR framework by:
The ranking process of the Equation (12) is the same way with the Equation (8) in section 4.2. After achieving the convergence of the iteration, both posts and sentences in the thread have been ranked by the salience scores Rco .

For summary extraction, we establish a greedy algorithm to de-tect the semantic orthogonality among summary sentences. Given selected sentences set S s and the current candidate sentence one step of extraction, s will not be selected unless the maximal semantic similarity between s and S s , calculated by Equation (7), is below one threshold.
A new data set is opt to be created because there is no exist-ing benchmark data set for evaluating the WFTS task. In this pa-per, Apple Discussion 2 and Slashdot 3 are used as our data sources. We obtained threads from our two data sources with the limitation that each thread has no less than 5 posts. To evaluate our sum-mary method performances with different thread volume, we clas-sify threads into 4 different thread volume intervals. Since constitute one web page in Apple Discussion, we set intervals as amount of threads in each interval is equal. 100 threads (50 from Apple and 50 from Slashdot) on each interval are crawled respec-tively.

To generate the evaluation references, 4 human assessors were asked to summarize the content individually. The final evalua-tion score for a WFTS strategy is on average of all scores using each referenced summary. We use ROUGE toolkit 4 to measure our proposed WFTS methods. ROUGE-1 (Recall against unigram), ROUGE-2 (Recall against bigram), and ROUGE-L (Recall against longest common subsequence) are chosen for the WFTS perfor-mance measure. In the ROUGE settings we use Porter Stemming algorithm to stem the words to their root form.
In section 3, we agglomerate posts from RRT =( r, V, E ) struc-ture into RRG =( s, b V, b E ) structure. To identify how this strat-egy enhance the summarization performance by reducing the text sparseness during the PPM process, we compare the results of the PPM-based WFTS using posts agglomeration with the one that ignores the posts agglomeration. Based on the PPM, we denote PPM-S as the basic algorithm in section 4.1, PPM-ST as the topic-sensitive MRW-based method in section 4.2, and PPM-STP as the MR-based ranking method using mutual reinforcement in section 4.3. These methods are measured in terms of ROUGE metrics for 200 summary length when stop-words are kept.

Table 2 presents us performances with and without the posts ag-glomeration. We can find for each WFTS algorithm the results after merging posts outperform results without posts agglomera-tion. The difference is obvious so that it is easy to conclude the posts agglomeration is worthwhile even though this may gener-ate a little information loss for reply-relations. As shown in Table 2, both PPM-STP and PPM-ST are able to produce better results than PPM-S that seems a little naive for WFTS calculation. How-ever, the tradeoff between the low complexity cost and competitive ROUGE performances make PPM-S still valuable in practice.
Several baselines, including both methods using other topic mod-els and some effective algorithms in the field of document sum-marization [3, 17, 10], are introduced for comparison. Among the topic models, we choose Latent Dirichlet Allocation (LDA) and Dynamic Mixture Models (DMMs) as comparisons. All we proposed WFTS algorithms in section 4 are based on topic mod-els, thus we compare different performances between the PPM and these models using the MR-based WFTS algorithm in section 4.3. http://discussions.info.apple.com http://slashdot.org version 1.5.5 is used here
For all 400 threads, we calculate results of all baselines in terms of ROUGE-1, ROUGE-2 and ROUGE-L using 2 methods, keeping stop-words and removing stop-words. For each method, we evalu-ated performances for 200 and 400 length summary. As shown in Table 1, PPM-STP results in obvious improvements over the oth-ers: For 200 length summary, PPM-STP achieves an increase of 1.9%, 2.4% and 1.8% over LDA in terms of ROUGE-1, ROUGE-2 and ROUGE-L respectively. For 400 length summary, PPM-STP gives an increase of 4.6%, 11.8% and 5.1% over LDA when stop-words are kept. We further compare PPM with the WFTS using DMMs. By and large, for 200 length summary PPM-STP offers relative performance improvements of 1.4%, 1.8% and 0.4%, re-spectively, in the ROUGE-1, ROUGE-2 and ROUGE-L measures as compared to the DMMs; while the relative improvements are 2.4%, 5.1% and 3.0% in the same measurements for 400 length summary case. Thus although only slight improvement happens when the summarization length is relatively small, dissimilarities between PPM and DMMs rises with the increase of the summary length. A natural explanation to the fact is Dynamic Mixture Mod-els (DMMs) cannot capture reply-relations existing in each thread.
To precisely illustrate the performances for various thread vol-ume, the impact of the thread volume is evaluated in Figure 7 by comparisons of WFTS performances on each interval respectively. In Figure 7, (a) and (b) illustrate the ROUGE-1 and ROUGE-2 scores for 200 length summary whereas (c) and (d) reflect the ROUGE metrics for 400 length summary. In Figure 7(a) and (c), PPM, DMMs and LDA have similar performances in terms of ROUGE-1 when posts number  X  15. However, with the increase of thread vol-ume, ROUGE-1 from the LDA and DMMs decreases rapidly while the PPM keep relatively stable. Shown by Figure 7(b) and (d), PPM has obviously better performance in terms of ROUGE-2 than others for each interval. All these improvements reflect the effectiveness for the thread summarization task to capture the reply-relations in each thread. In section 3, given each thread X  X  vacabulary W we establish a W  X   X  matrix including the background semantic similarities be-tween each word and its top  X  most similar words. The larger the more dependencies for one word propagated from other simi-lar words. Figure 8 (a) to (c) shows the ROUGE-1, ROUGE-2 and ROUGE-L curves for different WFTS strategies under the PPM, respectively. As shown in Figure 8, when  X  is greater than 1, bet-ter performances are observed for both PPM-based strategies. This can suggest that semantic similarities among words improve the performance of each PPM-based WFTS strategy. Meanwhile, we can observe that each metrics (ROUGE-1, ROUGE-2 and ROUGE-L) keeps on increasing till the  X  comes to 6 , after that value curves for the PPM-based methods begin to decline.
In this paper, we propose the task of Web Forum Thread Sum-marization (WFTS) to help people understand the thread in web fo-rums rapidly. Web Forum Thread Summarization works by extract-ing a group of sentences from all posts in one thread to constitute the summary. Based on a hierarchical Bayesian model that track all dynamic topics through the threaded discussion, we employ the markov random walk strategy and the mutual reinforcement to the sentence scoring progressively. Final summary is generated from a greedy sentence extraction process that keep the semantics orthog-onality. In experiments, we establish our data set from popular web forums and verify the effectiveness of our WFTS strategies, which give us remarkable performances comparing with other effective baselines. This work is supported by the Natural Science Foundation of China (60970047, 60903108), IIFSDU (2009TB016) and the Nat-ural Science Foundation of Shandong Province (Y2008G19). (c) ROUGE-1, Length=400 [1] D. Blei and J. Lafferty. Dynamic topic models. In ICML , [2] S. Dennis. On the hyper-Dirichlet type 1 and hyper-Liouville [3] A. Haghighi and L. Vanderwende. Exploring content models [4] T. Haveliwala. Topic-sensitive pagerank: A context-sensitive [5] X. Hu, N. Sun, C. Zhang, and T. Chua. Exploiting internal [6] T. Iwata, S. Watanabe, T. Yamada, and N. Ueda. Topic [7] A. Jain, M. Murty, and P. Flynn. Data clustering: a review. [8] C. Lin, J. Yang, R. Cai, X. Wang, and W. Wang.
 [9] Y. Liu, A. Niculescu-Mizil, and W. Gryc. Topic-link LDA: [10] D. Radev, H. Jing, M. Sty, and D. Tam. Centroid-based [11] Z. Ren, J. Ma, G. Wang, C. Cui, and X. Han. Dynamically [12] H. Wallach. Topic modeling: beyond bag-of-words. In [13] C. Wang, D. Blei, and D. Heckerman. Continuous time [14] X. Wang and A. McCallum. Topics over time: a non-markov [15] F. Wei, W. Li, Q. Lu, and Y. He. Query-sensitive mutual [16] H. Zha. Generic summarization and keyphrase extraction [17] L. Zhou and E. Hovy. Digesting virtual geek culture: The
