 Given a boolean who-watched-what matrix, with rows representing users and columns representing movies, how can we find an interpretation of the data with low error? How can we find its underlying structure, helpful for compression, prediction and denoising? Boolean matrices appear naturally in many domains (e.g. user-reviews [ 3 ] or user-item purchases [ 16 ], graphs, word-document co-occurrences [ 4 ] or gene-expression datasets [ 17 ]) and describing the underlying structure of these datasets is the fundamental problem of community detection [ 6 ] and co-clustering [ 15 ] techniques.
 boolean matrix M , with small reconstruction error while easily describing M  X  X  latent structure. We propose FastStep , a method for finding a non-negative fac-torization that, unlike commonly used decomposition methods, yields the best interpretability by combining a boolean reconstruction with non-negative factors . This combination allows FastStep to find structures that go beyond blocks, providing more realistic representations. Figure 1 a showcases three com-munities (representing 3 venues) in the DBLP dataset that illustrate the impor-tant hyperbolic structures found in real data; compare them to the community found by FastStep in Fig. 1 b representing the American community in the Airports dataset.
 Using our scalable method, we analyze two datasets of movie ratings and clear and surprising decompositions. As an additional example, Fig. 2 illustrates an application of FastStep to the task of community detection. Using route information alone, the world airports are decomposed in 10 factors that clearly illustrate geographical proximity. As we explain in more detail in Sect. 4.3 ,the communities we find have an arbitrary marginal and do not need to follow a block shape.
 Real and Non-negative Matrix Decompositions. In the Singular Value Decomposition (SVD) [ 7 ], a real matrix M is decomposed into U X V and V are real orthogonal matrices and  X  is a k  X  k non-negative diagonal matrix. While the Eckart-Young theorem [ 5 ] proves this to be the best approximation using regular matrix multiplication and real entries, negative values in the factor matrices make it hard to interpret. What does it mean for an element to have a negative score in a component? For non-negative M , Non-Negative Matrix Factorization (NNMF) [ 9 ] methods were developed to overcome this problem. reconstructed matrix is not boolean. One simple idea is rounding or thresholding the reconstructed matrix, but no guarantee can be given on the reconstruction error. Another possibility is thresholding the factor matrices and using boolean algebra in order to obtain a boolean reconstruction, but selecting the appropriate threshold is a difficult problem as a clear cut-off might not exist. Decomposition of Boolean Matrices. Tao Li [ 11 ]proposedanextension of the K-means algorithm to the two-sided case where M is decomposed into AXB T with A and B binary, and an alternating least squares method when X is the identity matrix. Pauli Miettinen showed that Boolean Matrix Factor-izations (BMF) methods could achieve lower reconstruction error than SVD in boolean data and proposed an algorithm using association rules ( exploits the correlations between columns, but unfortunately it X  X  time complex-ity is O ( nm 2 ). Zhang et al. [ 17 ] proposed two approaches for BMF, one using a penalty in the objective function ( BMF-penalty ) which achieved good results for dense datasets, and an alternative thresholding method ( which by thresholding factor matrices is better suited for sparse datasets. None of these methods is scalable and they have the problem of forcing a tiling of the data matrix, as each factor is effectively treated as a block. In particular, the notion of  X  X mportance X  inside a cluster, which previously existed in NNMF, is now lost and the analysis of the resulting factors is limited. In Logistic PCA (L-PCA), Schein et al. [ 12 ] replace PCA X  X  Gaussian assumption with a Bernoulli distribution and fit their new model using an alternating least squares algorithm that maximizes the log-likelihood. Their alternating algorithm has running time O ( nm ) when applied to a n  X  m matrix and therefore does not scale. It is also hard to interpret due to the possibility of negative values in the factors. Related Techniques. There is a strong relationship between boolean matrices and graph data, where matrix decompositions are linked to community detection and graph partitioning, but we would like to refer the reader to a review on as the Hyperbolic Community Model [ 2 ] has shown the non-uniform nature of real-world communities and has highlighted the need for boolean decomposition methods which do not discard node importance.
 Leskovec et al. [ 10 ] approximated the log-likelihood of the fit by exploiting the Kronecker nature of their generator. In the Compact Matrix Decomposition [ 14 ], Jimeng Sun et al. approximated the sum-square-error (SSE) by sampling a set of rows and columns and scaling the error in the submatrix accordingly. Table 1 provides a quick comparison of some of the methods discussed in this section. We characterize as Beyond blocks methods who do not force a rec-tangular tiling of the data. Arbitrary Marginals refers to a method X  X  ability to represent any marginal in the data (e.g. rectangles, but also triangles or hyper-of elements representing a factor. Given our focus on efficient decompositions, we will limit our comparison in Sect. 4 to scalable methods.
 ity of a boolean matrix decomposition: boolean reconstruction allows clear pre-factors establishes the importance of elements and enable the representation of beyond-block structures. In this section, we introduce a new formulation using a step operator that achieves both goals. 3.1 Formal Objective Let M be a n  X  m boolean matrix. Our goal is to find a n  X  matrix A and a m  X  r non-negative matrix B , so that the product AB good approximation of M after thresholding: function to each element X ij : where  X  is a threshold parameter. Note that the choice of  X  does not affect the decomposition, as matrices A and B can always be scaled accordingly. 3.2 Step Matrix Decomposition The thresholding operator renders the objective function non-differentiable and akin to a binary programming problem. In order to solve it, we will approximate the objective function of Eq. 1 by a function with similar objective: where M was transformed so that it has values in { X  1 , 1 with  X  1.
 when x is negative; the intuition is that this error function will be approximately is in place whenever their signs differ.
 and one possibility is using gradient descent. The gradient is given by Lemma 1. Let S ij = 3 is given by: Proof. Omitted for brevity.
 after each iteration -this projection is made to a small value instead of to 0, as A = B = 0 is a stationary point of the objective function and the algorithm wouldn X  X  improve.
 Our experiments indicate that stochastic gradient descent with batches corre-sponding to factors provides the quickest convergence, as factors quickly converge to different submatrices. Our results also indicate that initializing A and B to small random numbers provides the best results. Comparing alternative gradient descent methods is out of the scope of this paper.
 error that it should be chosen to be the highest possible value in order to improve convergence and to get a sharper decomposition, as long as numerical stability is not compromised. Our implementation uses  X  = 20.
 Complexity. A straightforward implementation would take O ( TNMR where T is the number of iterations, N and M are the dimensions of the matrix and R is the rank of the decomposition. However, by using additional O ( NM ) memory, caching and updating S in each iteration, it can be reduced to O ( TNMR ). 3.3 FastStep Matrix Decomposition Unfortunately, the previous algorithm is not adequate for many datasets given its quadratic nature; it grows linearly in O ( NM ). In many scenarios such as community detection and recommender systems, M is extremely sparse and algorithms must be linear (or quasilinear) in the number of non-zeros ( E ). In the following, we describe how to quickly approximate F ( A , B ) and the respective gradients of the sparse matrix.
 Fast Gradient Calculation. AsshowninEq. 4 , calculating the gradient exactly requires O ( NM ) operations per factor because each A mation over all elements B jk . Furthermore, there is a A means that this loop cannot be easily unrolled or reused between elements of A . The goal of this subsection is to approximate the gradient of the factor using a number of operations in the order of O ( E ), the number of non-zeros in the matrix.
 Careful analysis of the structure of this summation in the gradient allows us to quickly approximate it. The impact of position ( i, j ) in factor k is a sigmoid function, scaled by B jk and with parameter S ij . This means that only positions with simultaneously high S ij and B jk have a significant impact on the gradient, which implies that we should first consider pairs ( i, j ) with high A correlates well with both metrics.
 In other words, Eq. 4 can be approximated as where P ( t )isthesetofelementsof M that the decomposition  X  X elieves X  should P ( t ) that each factor k would like to reconstruct and P ( t )= P intuition is that, initially, only non-zeros contribute to the gradient so we can quickly calculate it with no error using the second summand of Eq. 5 .Aswe iterate, the error will gradually move from the non-zeros of M to some of the zeros. However, given M  X  X  sparsity and the symmetry of the error function  X  the error of misrepresenting a one is the same as misrepresenting a zero  X  be kept small and in the order of O ( rE ); Fig. 3 shows the error as the size of P ( t ) increases.
 t pairs ( i, j ) with highest A ik B jk ,let a k and b k be columns k of matrices A and B , respectively. After sorting a k and b k , the biggest A ik B jk not currently in P k can be selected from a very small set of elements along one sort of  X  X iagonal X  in the matrix.
 In particular, it can be shown that element ( x, y ) should not be added to P k before both ( x  X  1 ,y )and ( x, y  X  1) are added, as they would necessarily be at least as big. There-fore, one can keep a priority queue approximate the gradient of all elements in factor k in O ( t + n log n + m log m ) operations.
 Fast Function Evaluation. Given the method currently used to quickly calcu-E + P ( t ). Although fast, some positions of the matrix would never be considered and the algorithm would over-fit, thus it cannot be used to detect convergence. dient descent (i.e. after all the batches are completed), we calculate an estimate the zeros of the matrix and then scaling the error accordingly. Additionally, in order to decrease the probability of underestimating F ( A , B ) and compromising future iterations of the gradient descent, we take the median of 9 simulations. Complexity. Using the same notation as before, the time complexity is now bounded by the number of non-zeros and P = | P ( t ) | , which as we showed can be O ( rE ), and the number of samples S to check for convergence. The complexity is now O ( TR ( E + P log(min( N, M )) + N log N + M log M + S )). Obtaining Clusters from A and B. When a binary answer on whether a given element  X  X elongs X  to a factor is desired (e.g. community detection), a clear interpretation exists solely based on the principles of the decomposition: Definition 1 PartofaFactor . A row element i belongs to a factor is non-zero in the reconstructed matrix in row i and if this factor contributed section. FastStep was tested on 2 fairly different real-world datasets, see Table 2 for details. MovieLens100k and MovieLens10m are user-movie ratings datasets made available by MovieLens and the Airports dataset is a graph made available by OpenFlights . Unless otherwise specified, FastStep was run using the default parameters defined in Sect. 3 and 1000000 samples.
 We answer the following questions: Q1. How scalable is the fast version of FastStep ? Q2. How does the reconstruction error compare to other methods? Q3. How effective and interpretable is the FastStep decomposition? 4.1 Scalability The fast approximation of the gra-dient has a runtime proportional to the number of non-zeros of the matrix. For the runtime to be repro-ducible, we took different subsets of the MovieLens10m dataset by remov-ing all the ratings of movies produced after a given decade. Please note that the matrix was not resized, resulting in columns (and possibly rows) full of zeros.
 Figure 4 shows the execution time of the decomposition for these different matrices. Notice the sub-quadratic run-ning time. 4.2 Low Reconstruction Error When considering the same number of factors, a lower reconstruction error implies better compression and potentially enables lower-rank representations of the data. Given the boolean nature of M , the error function is intuitively easy to represent. Let M represent the original dataset and R represent the reconstructed matrix, then the error E is given by E = || We compared FastStep to other methods that were quasilinear in the num-ber of non-zeros. Table 3 compares the squared error of FastStep and HyCoM in the MovieLens100k and Airports datasets when using 10 fac-tors. For SVD and NNMF, as arbitrary values such as 0.5 do not guarantee the lowest error, we tried all thresholds and considered the optimal. For we selected the lowest error from Fig. 3 and its equivalent in the MovieLens100k data (which converged after considering only 2 rE non-zeros). For HyCoM, we considered as error the sum of the edges not represented and the mistakes made inside each community. Among the state of the art methods, we did not compare with non scalable algorithms (L-PCA, ASSO, BMF-Threshold).
 be appropriate given the same number of parameters, their expressiveness is not the same given their different characteristics. In this regard, by allowing negative numbers, SVD is at an advantage when compared to the rest of the methods. Please note that common techniques such as the Bayesian Information Criterion (BIC) [ 13 ] or the Akaike Information Criterion (AIC) [ 1 ] would not provide a fairer comparison because, as the number of parameters is the same, all methods would keep the same relative rank . Techniques such as Minimum Description Length (MDL) [ 8 ] measure the number of bits required to encode both the error and the model, but it is not clear which method should be used to represent real numbers, especially given that the importance of the bits is not the same -as a result, methods such as HyCoM that uses integer values would greatly benefit.
 struction error while maintaining higher interpretability. 4.3 Discoveries MovieLens. The MovieLens100k user-movie dataset was decomposed using a rank-10 decomposition and the factors were clustered as described.
 Table 4 illustrates the top-5 movies (ranked by score) in three of the fac-tors and shows a grouping by movie theme.
 correspond to a given genre (movies might have more than one tag, so genres do not sum to 1). We labeled group A as teenagers due to the clear prevalence of Action and Adventure movies. In group B, most of the movies rated were in categories of Comedy, Children X  X , Animation and Adventure; we hypothesize that users rating these movies are parents and labeled the group accordingly. Finally, we labeled group C as females due to the Drama, Comedy and Romance movie genres.
 Airports. The Airports dataset is a symmetric matrix representing an undi-rected unipartite graph, which implies that B = A as we are looking for com-munities. The minimization problem is similar min gradient is omitted for brevity.
 Figure 2 shows a geographical plot of the airports in the different communi-ties; some big hubs, such as Frankfurt and Heathrow, appear in multiple commu-nities and were coded with a single color to simplify visualization. Even though no geographical information was used to perform this task, there is a very clear distinction between north American airports, Brazilian airports, European air-ports, previous French colonies in Africa, Russian airports, Middle-Eastern air-ports and south-east Asia airports. Additionally, in order to illustrate one of the surprising findings, Fig. 6 highlights the two European communities (in blue and yellow) along with the overlapping airports (in green). While it would initially seem that all these airports should be considered the same community, a quick overview makes us realize that they are in fact divided by  X  X ajor airports X  and  X  X econdary airports X , usually operated by low-cost companies. The airports with the highest score in the  X  X ajor airports X  community are Barcelona, Munich and Amsterdam, while the airports with the highest score in the  X  X ow-cost X  group are Girona (85 km from Barcelona), Weeze (70 km from Dusseldorf) and Frankfurt-Hahn (120 km from Frankfurt). We consider these and other surprising findings to be very strong empirical evidence on FastStep  X  X  usefulness for these tasks. Another important improvement of the FastStep decomposition is its abil-cency matrix of the American community found in the previous decomposition. As we have non-negative factors, lets explore the additional information avail-able in matrix A . The airports with the highest score correspond to central airports in continental United States with hubs from big airlines: Minneapolis, Denver, Chicago, Dallas, Detroit, Houston, etc. Therefore, using this decompo-sition alone, measures of centrality can be directly obtained.
 descending order, closely follow a power-law. This characteristic has been previ-ously observed in ground-truth communities using significantly different ground-truth definitions [ 2 ]. Given that no bias was introduced in FastStep carefully combines a non-negative decomposition and a boolean reconstruction for the best interpretability of the data. We have shown that it achieves lower reconstruction error than similar methods and have provided strong empirical evidence of its ability to find structural patterns in the data. The main contributions of this work are the following: 1. New formulation and tractable approximation: We introduce a novel 2. Scalable: A very efficient approximation enables a runtime linear in the 3. Low reconstruction error when compared to standard methods. 4. Realistic representation which relates to nodes in clusters or degree inside 5. Meaningful and interesting discoveries in real-world datasets. Reproducibility. Available at http://cs.cmu.edu/  X  maraujo/faststep/ .
