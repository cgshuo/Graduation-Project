 k M k 1 is P i,j | M ( i,j ) | and k M k  X  is max ij | M ( i,j ) | . e.g. P . The norm kPk of an operator is defined as where the supremum is over matrices M .
 M , and matrices each column of which is in the column space of M . M , Note that for any matrix X , space of X .
 M 0 have complementary supports). Note that P presented at the end of the proof.
 We will make use of the following: positive number satisfying: 1. k Q k &lt; 1 2. kP T ( Q ) k  X   X  2 min { c 1 ,c 2 } 3.  X   X   X  D : 4.  X   X   X  D : 5. P  X  P [ ( UU &gt; + Q ) = c 1 P [  X  B if  X  6 = 0.
 Using these three subgradients, the difference in the objective value can be bounded as follows: The last six terms of the last RHS satisfy: It follows that  X  X  T Q,  X   X  . We bound these three separately.
 which is strictly greater than zero for  X  6 = 0. and use the weights c 1 and c 2 given in Theorem 1. We specify P ] Q and P [ Q separately. Note that these matrices have zero-mean entries.
 where assumption of the theorem and p  X  q  X  p (1  X  q ), we have p (1  X  q ) &amp; n log 4 n ` 2 these facts frequently in the proof.
 bounded by 1 4 . sum of a deterministic matrix Q  X  ,d with all non-zero entries equal to b 1  X  Q `  X  have i.i.d zero-mean entries which are bounded almost surely by B  X  := max n 1 ` the assumption of Theorem 1. 6 max Now observe that ( UU &gt; P ] Q m )( i,j ) = P l  X  V ables with bounded magnitude and variance. Using Lemma 7, we obtain that for i  X  V ] , where in the last inequality we use t  X  p 4 &amp; log n ` with the same quantity (cf. [1]). Property 3b) can be verified similarly.
 Consider the two terms in the parenthesis in the last RHS. For the first term, we have For the second term, we have the following which implies (1 + ) c 2 1  X  p p  X  (1  X  2 ) c 1 . We conclude that proving property 4a).
 Consider the factor before the norm in the last RHS. Similarly as before, we have This implies (1 + ) c 1 q 1  X  q  X  (1  X  ) c 2 . We conclude that proving property 4b).
 Let n 0 =  X  2 n and assume n 0 is an integer. Let A 0  X  R n 0  X  n 0 be such a matrix that Consider the following padded program that the unique optimal solution (  X  K 0 ,  X  B 0 = A 0  X   X  K 0 ) to (CP1 X ) has the form We claim that  X  K = P ] K  X  is the unique optimal solution to (CP1). have Define K 0 0 = contradicting the fact that (  X  K 0 ,  X  B 0 = A 0  X   X  K 0 ) is the unique optimal to (CP1 X ). Let M =  X  min ( K ).
 y  X  z .
 For each such choice, the probability that (3 . 2) does not hold is U i and the other to U i \ U of  X  is exactly the sign of  X  d Y,Z  X  c 2 c maybe for at most one set of size at least | U i | X  m . If we now also assume that We now claim that this function is an injection. We will need the following assumption: ( Z  X  Z 0 )  X  [ n ] \ V i , max {| Z | , | Z 0 |} X  m , min {| Y | , | Y 0 |} X  M  X  m : m probability that (3.6) is violated is at most for some C &gt; 0. Using (3.4), this is at most bounded above by exp { C 000 m Y log n } , for some global C 000 &gt; 0. Assuming for some combination with probability at most which is at most exp { X  20 log n } if Y,Y 0 ,Z,Z 0 with probability at least 1  X  n  X  4 .
 than that of ( K,B ), contradicting optimality of the latter. (Note that k K k  X  = k K 0 k  X  .) contained in V \{ U 1  X  X  X  X  X  U r } . We need the following assumption. for some j  X  [ k ] , | Z | X  m , Z  X  V j =  X  : by f ( X,Y,Z )  X  g ( X,Y,Z ) for any such option is at most respectively satisfies f ( X,Y,Z )  X  g ( X,Y,Z ) is at most which is at most exp { X  10 log n } assuming uniformly with probability at least 1  X  n  X  4 .
 cost of ( K 0 ,B 0 ) is strictly lower than that of ( K,B ). Note that the expression | X | c accounts for the trace norm difference k K 0 k  X   X  X  K k  X  = | X | . set T i = U i  X  V  X  ( i ) satisfies i U strictly smaller than that of ( K,B ). Fix the value of the collection the cost of ( K,B ) is lower than that of ( K 0 ,B 0 ) is at most difference k K k  X   X  X  K 0 k  X  = P r i =1 m i -this is similar to what we did above .) As long as over all possibilities for  X  ( Y ), of which there are at most n 2 . Taking C 1 ,C 2 large enough to satisfy the requirements above concludes the proof. Proof. We remind the user that g = b 3 b proof.
 C  X  in the proof of the main theorem.
 at most  X  2 and is bounded in absolute value by B . Then with probability at least 1  X  2 n  X  2 commutative Bernstein Inequality (Theorem 1.6 in [2]) with t = 6 max  X  desired bound.
 ance bounded by  X  2 and is bounded in absolute value by B a.s.. Then we have that The following well known consequence of the above lemma will also be of use. is bounded in absolute value by B a.s. Then we have n .
 Nir Ailon nailon@cs.technion.ac.il Yudong Chen ydchen@utexas.edu Huan Xu mpexuh@nus.edu.sg This paper considers a classic problem in machine learning and theoretical computer science, namely graph clustering, i.e., given an undirected unweighted graph, partition the nodes into disjoint clusters, so that the density of edges within one cluster is higher than those across clusters. Graph clustering arises nat-urally in many applications across science and engi-neering, such as community detection in social net-work, submarket identification in E-commerce and sponsored search and co-authorship analysis in ana-lyzing document database. From a purely binary clas-sification theoretical point of view, the edges of the graph are (noisy) labels of similarity or affinity be-tween pairs of objects, and the concept class consists of clusterings of the objects (encoded graphically by identifying clusters with cliques).
 Many theoretical results in graph clustering (e.g., Condon &amp; Karp, 2001; McSherry, 2001) consider the planted partition model, in which the edges are gener-ated randomly; see Section 1.1 for more details. While numerous different methods have been proposed, their performance guarantees all share the following man-ner  X  under certain condition of the density of edges (within clusters and across clusters), the proposed method succeeds to recover the correct clusters exactly if all clusters are larger than a threshold size , typically  X   X (  X  the reason for this requirement is simple: The random noise gives rise to eigenvalues of order  X   X ( graph adjacency matrix, dominating spectral informa-tion corresponding to the clusters if they are all small. In this paper, we aim to break this small cluster barrier of graph clustering for a certain family of al-gorithms based on convex relaxation. When all the clusters are very small, identifying them seems inher-ently hard 1 , and is not the focus of this paper. Instead, in this paper we investigate the following: Can we still recover large clusters in the presence of small clusters? Intuitively, this should be doable. To illustrate, con-sider the example where the given graph G consists two disjoint subgraphs G 1 and G 2 , where G 1 by it-self is a graph that can be correctly clustered using some existing method, G 2 is a very small clique, and there are only relative few edges connecting G 1 and G 2 . G certainly violates the minimum cluster size re-quirement of previous results, but why should G 2 spoil our ability to recover G 1 ? Our main result confirms this intuition. We show that the cluster size barrier arising in previous work (e.g., Chaudhuri et al., 2012; Bollob  X as &amp; Scott, 2004; Chen et al., 2012; McSherry, 2001) is not really a restric-tion, but rather an artifact of the attempt to solve the problem in a single shot. Using a more careful analy-sis, we prove that the mixed trace-norm and ` 1 based convex formulation, initially proposed in Jalali et al. (2011) and Chen et al. (2012), can recover clusters of size  X   X ( The main implication of this result is that one can ap-ply an iterative  X  X eeling X  strategy, recovering smaller and smaller clusters. The intuition is simple  X  sup-pose the number of clusters is limited, then either all clusters are large, or the sizes of the clusters vary sig-nificantly. The first case is already covered by exist-ing results. The second one is equally easy: use the aforementioned convex formulation, the larger clusters can be correctly identified. If we remove all nodes from these larger clusters, the remaining subgraph con-tains significantly fewer nodes than the original graph, which leads to a much lower threshold on the size of the cluster for correct recovery, making it possible for correctly clustering some remaining smaller clusters. By repeating this procedure, indeed, we can recover the cluster structure for almost all nodes with no lower bound on the minimal cluster size . We summarize our main contributions and techniques: (1) We provide a refined analysis of the mixed trace norm and ` 1 convex relaxation approach for exact re-covery of clusters proposed in Jalali et al. (2011) and Chen et al. (2012), focusing on the case where small clusters exist. We show that if there is a number x in  X   X (  X  smaller than x/ log 2 n , and at least one cluster is large, then with high probability, the convex relaxation leads to a unique solution correctly identifying all big clus-ters while  X  X gnoring X  the small ones. We call such a solusion admissible . Notice that the multiplicative gap between the two thresholds is logarithmic w.r.t. n . In addition, it is possible to arbitrarily increase x , thus turning a  X  X nob X  in quest of an interval ( x/ log 2 n,x ) that is disjoint from the set of cluster sizes. The anal-ysis is done by identifying a certain feasible solution to the convex program and proving its almost sure optimality using a careful construction of a dual cer-tificate . This method has been performed before only in the case where all clusters are large. (2) We provide a converse of the result just described. More precisely, we show that if for some value of the knob x an optimal solution is admissible, then the so-lution is useful (in the sense that it correctly identifies big clusters), even if there exist clusters in the interval ( x/ log 2 n,x ). (3) The last two points imply that if some interval of the form ( x/ log 2 n,x ) is free of cluster sizes, then an exhaustive search of this interval will construc-tively find big clusters. This gives rise to an itera-tive algorithm, using a  X  X eeling strategy X , to recover smaller and smaller clusters that were not recoverable in a one shot convex relaxation step. We then prove that as long as the number of clusters is bounded by  X (log n/ log log n ), regardless of the cluster sizes, we can correctly recover the cluster structure for an over-whelming fraction of nodes. (4) We extend the result to the partial observation case, where only a fraction of similarity labels (i.e., edge/no edge) is known. As expected, smaller observa-tion rates allow identification of larger clusters. Hence, the observation rate serves as the  X  X nob X . This gives rise to an active learning algorithm for graph cluster-ing based on adaptively increasing the rate of sampling in order to hit a corresponding interval free of cluster sizes, and concentrating on smaller inputs as we iden-tify big clusters and peel them off.
 Beside these technical contributions, this paper pro-vides novel insights into low-rank matrix recovery and more generally high-dimensional statistics, where data are typically assumed to obey certain low-dimensional structure. Numerous methods have been developed to exploit this a priori information so that a consistent estimator is possible even when the dimensionality of data is larger than the number of samples. Our re-sult shows that one may combine these methods with a  X  X eeling strategy X  to further push the envelope of learning structured data  X  By iteratively recovering the easier structure and then reducing the problem size, it is possible to learn structures that are other-wise difficult using previous approaches. 1.1. Previous work The literature of graph clustering is too vast for a de-tailed survey here; we concentrate on the most related work, and in specific those provide theoretical guaran-tees on cluster recovery.
 Planted partition model: The setup we study is the classical planted partition model (Condon &amp; Karp, 2001), also known as the stochastic block model (Hol-land et al., 1983). Here, n nodes are partitioned into subsets, referred as the  X  X rue clusters X , and a graph is randomly generated as follows: for each pair of nodes, depending on whether they belong to a same subset, an edge connecting them is generated with a probabil-ity p or q respectively. The goal is to correctly recover the clusters given the random graph. Earlier work on the planted partition model focused on the 2-partition or more generally l -partition case with l = O (1), i.e., the minimal cluster size is  X ( n ) (Condon &amp; Karp, 2001; Carson &amp; Impagliazzo, 2001; Bollob  X as &amp; Scott, 2004). Recently, several works have proposed methods to handle sublinear cluster sizes. These works can be roughly classified into three approaches: randomized algorithms (e.g., Shamir &amp; Tsur, 2007), spectral clus-tering (e.g., McSherry, 2001; Giesen &amp; Mitsche, 2005; Chaudhuri et al., 2012; Rohe et al., 2011)), and al-gorithms based on convex optimization (Jalali et al., 2011; Chen et al., 2012; Ames &amp; Vavasis, 2011; Oy-mak &amp; Hassibi, 2011; Mathieu &amp; Schudy, 2010). While these work differs in the methodology, they all impose constraints on the size of the minimum true cluster  X  the best result up-to-date requires it to be  X   X ( Correlation Clustering This problem, originally de-fined by Bansal et al. (2004), also considers graph clus-tering but in an adversarial noise setting. The prob-lem is NP-Hard to approximate to within some con-stant factor. Prominent work includes Demaine et al. (2006); Ailon et al. (2008); Charikar et al. (2005). A PTAS is known in case the number of clusters is fixed (Giotis &amp; Guruswami, 2006).
 Low rank matrix decomposition via trace norm: Motivated from robust PCA, it has recently been shown (Chandrasekaran et al., 2011; Cand`es et al., 2011), that it is possible to recover a low-rank ma-trix from sparse errors of arbitrary magnitude, where the key ingredient is using trace norm (a.k.a. nuclear norm) as a convex surrogate of the rank. A similar result is also obtained when the low rank matrix is corrupted by other types of noise (Xu et al., 2012). Of particular relevance to this paper is Jalali et al. (2011), Chen et al. (2012) and Jalali &amp; Srebro (2012), where the authors apply this approach to graph clus-tering, and specifically to the planted partition model. Indeed, Chen et al. (2012) achieve state-of-art per-formance guarantees for the planted partition prob-lem. However, they don X  X  overcome the  X   X ( mal cluster size lower bound.
 Active learning/Active clustering Another line of work that motivates this paper is study of active learn-ing algorithms (a settings in which labeled instances are chosen by the learner, rather than by nature), and in particular active learning for clustering. The most related work is Ailon et al. (2012), who investigated active learning for correlation clustering. The authors obtain a (1 +  X  )-approximate solution with respect to the optimal, while (actively) querying no more than O ( n poly(log n,k, X   X  1 )) edges. The result imposed no restriction on cluster sizes and hence inspired this work, but differs in at least two major ways. First, Ailon et al. (2012) did not consider exact recovery as we do. Second, their guarantees fall in the ERM (Em-pirical Risk Minimization) framework, with no run-ning time guarantees. Our work recovers true clus-ter exactly using a convex relaxation algorithm, and is hence computationally efficient. The problem of active learning has also been investigated in other cluster-ing setups including clustering based on distance ma-trix (Voevodski et al., 2012; Shamir &amp; Tishby, 2011), and hierarchical clustering (Eriksson et al., 2011; Kr-ishnamurthy et al., 2012). These setups differ from ours and cannot be easily compared. Throughout, V denotes a ground set of elements, which we identify with the set [ n ] = { 1 ,...,n } . We assume a true ground truth clustering of V given by a pairwise disjoint covering V 1 ,...,V k , where k is the number of clusters. We say i  X  j if i,j  X  V a for some a  X  [ k ], otherwise i 6 X  j . We let n i = | V i all i  X  [ k ]. For any i  X  [ n ],  X  i  X  is the unique index satisfying i  X  V  X  i  X  .
 For a matrix X  X  R n  X  n and a subset S  X  [ n ] of size m , the matrix X [ S ]  X  R m  X  m is the principal minor of X corresponding to the set of indexes S . For a matrix M ,  X ( M ) denotes the support of M , namely, the set of index pairs ( i,j ) such that M ( i,j ) 6 = 0. The ground truth clustering matrix, denoted K  X  , is defined so that K  X  ( i,j ) = 1 is i  X  j , otherwise 0. This is a block diagonal matrix, each block consisting of 1 X  X  only. Its rank is k . The input is a symmetric matrix A , a noisy version of K  X  . It is generated using the well known planted clustering model, as follows. There are two fixed edge probabilities, p &gt; q . We think of A as the adjacency matrix of an undirected random graph, where edge ( i,j ) is in the graph for i &gt; j with probability p if i  X  j , otherwise with probability q , independent of other choices. The error matrix is denoted by B  X  := A  X  K  X  . We let  X  :=  X ( B  X  ) denote the noise locations .
 Note that our results apply to the more practical case in which the edge probability of ( i,j ) is p ij for each i  X  j and q ij for i 6 X  j , as long as (min p ij ) =: p &gt; q := (max q ij ). We remind the reader that the trace norm of a ma-trix is the sum of its singular values, and we define the ` 1 norm of a matrix M to be k M k 1 = P ij | M ( ij ) | . For a set  X   X  [ n ]  X  [ n ], P  X  ( M ) denotes the matrix ob-tained from M by setting M ( i,j ) = 0 for all ( i,j )  X   X  . Consider the following convex program, combining the trace norm of a matrix variable K with the ` 1 norm of another matrix variable B using two parameters c 1 ,c 2 that will be determined later: (CP1) min k K k  X  + c 1 P  X ( A ) B Theorem 1. There exist constants b 1 ,b 3 ,b 4 &gt; 0 such that the following holds with probability at least 1  X  n  X  3 For any parameter  X   X  1 and t  X  [ 1 4 p + 3 4 q, 3 4 p + define ` = b 3 If for all i  X  [ k ] , either n i  X  ` ] or n i  X  ` [ and if ( is an optimal solution to (CP1), with c = then (  X  K,  X  B ) = ( P ] K  X  ,A  X   X  K ) , where for a matrix M , P
M is the matrix defined by (Note that by the theorem X  X  premise,  X  K is the matrix obtained from K  X  after zeroing out blocks correspond-ing to clusters of size at most ` [ .) The proof is based on Chen et al. (2012) and is deferred to the supplemen-tal material due to lack of space. The main novelty in this work compared to previous work is the treatment of small clusters of size at most ` [ , whereas in pre-vious work only large clusters were treated, and the existence of small clusters did not allow recovery of the big clusters.
 Definition 2. An n  X  n matrix K is a partial clus-tering matrix if there exists a collection of pairwise disjoint sets U 1 ,...,U r  X  [ n ] (the induced clusters ) such that K ( i,j ) = 1 if and only if i,j  X  U s for some s  X  [ r ], otherwise 0. If K is a partial clustering matrix then  X  min ( K ) is defined as min r i =1 | U i | . The definition is depicted in Figure 1. Theorem 1 tells us that by choosing  X  (and hence c 1 , c 2 ) prop-erly such that no cluster size falls in the range ( ` [ ,` the unique optimal solution (  X  K,  X  B ) to convex program (CP1) is such that  X  K is a partial clustering induced by big ground truth clusters.
 In order for this fact to be useful algorithmically, we also need a type of converse: there exists an event with high probability (in the random process generating the input), such that for all values of  X  , if an optimal solu-tion to the corresponding (CP1) looks like the solution (  X 
K,  X  B ) defined in Theorem 1, then the blocks of  X  K cor-respond to actual clusters.
 Theorem 3. There exists constants C 1 ,C 2 &gt; 0 such that with probability at least 1  X  n  X  2 , the following holds. For all  X   X  1 and t  X  [ 3 4 q + 1 4 p, 1 4 q + 3 ( K,B ) is an optimal solution to (CP1) with c 1 ,c defined in Theorem 1, and additionally K is a partial clustering induced by U 1 ,...,U r  X  V , and also  X  min ( K )  X  max then U 1 ,...,U r are actual ground truth clusters, namely, there exists an injection  X  : [ r ] 7 X  [ k ] such that U i = V  X  ( i ) for all i  X  [ r ] . (Note: Our proof of Theorem 3 uses Hoeffding tail bounds for simplicity, which are tight for p,q bounded away from 0 and 1. Bernstein tail bounds can be used to strengthen the result for other classes of p,q . We elaborate on this in Section 3.1.) The combination of Theorems 1 and 3 implies that, as long as there exists a relatively small interval which is disjoint from the set of cluster sizes, and such that at least one cluster size is larger than this interval (and large enough), we can recover at least one (large) clus-ter using (CP1). This is made clear in the following. Corollary 4. Assume we have a guarantee that there exists a number  X   X  b 4 such that no cluster size falls in the interval size at least s := max { b 3 b q ) 2 ,C 2 p p (1  X  q ) n log n/ ( p  X  q ) } . Then with proba-bility at least 1  X  n  X  2 , we can recover at least one cluster of size at least s efficiently by solving (CP1) with  X  =  X / b 4 Of course we do not know what  X  (and hence  X  ) is. We could exhaustively search for a  X   X  1 and hope to recover at least one large cluster. A more interesting question is, when is such a  X  guaranteed to exist? Let g = b 3 b (multiplicative) gap size, equaling the ratio between ` and ` [ (for any  X  ). If the number of clusters k is a priori bounded by some k 0 , we both ensure that there is at least one cluster of size n/k 0 , and by the pigeon-hole principle, that one of the intervals in the sequence ( n/gk 0 ,n/k 0 ) , ( n/g 2 k 0 ,n/gk 0 ) ,..., ( n/g k 0 ). is disjoint of cluster sizes. If, in addition, the smallest interval in the sequence is not too small and n/k is not too small so that Corollary 4 holds, then we are guaranteed to recover at least one cluster using Algorithm 1. We find this condition difficult to work with. An elegant, useful version of the idea is obtained if we assume p,q are some fixed constants. 2 As the following lemma shows, in this regime, k 0 can be assumed to be almost logarithmic in n to ensure recovery of at least one cluster. 3 In what follows, notation such as C ( p,q ) ,C 3 ( p,q ) ,... denotes universal positive functions depending on p,q only. Lemma 5. There exists C 3 ( p,q ) ,C 4 ( p,q ) ,C 5 &gt; 0 such that the following holds. Assume that n &gt; C 4 ( p,q ) , and that we are guaranteed that k  X  k 0 , where k 0 log log n . Then with probability at least 1  X  n gorithm 1 will recover at least one cluster in at most C k 0 iterations.
 The proof is deferred to the supplemental material sec-tion. Lemma 5 ensures that by trying at most a loga-rithmic number of values of  X  , we can recover at least one large cluster, assuming the number of clusters is roughly logarithmic in n . The next proposition tells us that as long as this step recovers the clusters covering at most all but a vanishing fraction of elements, the step can be repeated.
 Proposition 6. A pair of numbers ( n 0 ,k 0 ) is called is good, then ( n 00 ,k 00 ) is good for all n 00 ,k n The proof is trivial. The proposition implies an induc-tive process in which at least one big (with respect to the current unrecovered size) cluster can be efficiently removed as long as the previous step recovered at most ing, we proved the following: Theorem 7. Assume n,k satisfy the requirements of Lemma 5. Then with probability at least 1  X  2 n  X  1 Al-gorithm 2 recovers clusters covering all but at most observation case, without any restriction of the min-imal cluster size. Moreover, if we assume that k is bounded by a constant k 0 , then the algorithm will re-cover clusters covering all but a constant number of input elements. 3.1. Partial Observations We now consider the case where the input matrix A is not given to us in entirety, but rather that we have ora-cle access to A ( i,j ) for ( i,j ) of our choice. Unobserved Algorithm 1 RecoverBigFullObs( V,A,p,q ) require: ground set V , A  X  R V  X  V , probs p,q n  X  X  V | t  X  1 4 p + 3 4 q (or anything in [ 1 4 p + 3 4 q, 3 4 p + // (If have prior bound k 0 on num clusters, // take ` ]  X  n/k 0 ) end while return  X  Algorithm 2 RecoverFullObs( V,A,p,q ) require: ground set V , matrix A  X  R V  X  V , probs p,q { U 1 ,...,U r } X  RecoverBigFullObs( V,A,p,q )
V 0  X  [ n ] \ ( U 1  X  X  X  X  X  U r ) if r = 0 then else end if values are formally marked with A ( i,j ) =?.
 Consider a more particular setting in which the edge probabilities defining A are p 0 (for i  X  j ) and q 0 (for i 6 X  j ), and we observe A ( i,j ) with probability  X  , for each i,j , independently. More precisely: For i  X  j we have A ( i,j ) = 1 with probability  X p 0 , 0 with probability  X  (1  X  p 0 ) and ? with remaining probability. For i 6 X  j we have A ( i,j ) = 1 with probability  X q 0 , 0 with probability  X  (1  X  q 0 ) and ? with remaining probability. Clearly, by pretending that the values ? in A are 0, we emulate the full observation case with p =  X p 0 , q =  X q 0 Of particular interest is the case in which p 0 ,q 0 are held fixed and  X  tends to zero as n grows. In this regime, we have the following result, which follows directly from Theorem 1 by setting  X  = 1, p =  X p 0 and q =  X q 0 (note that Theorem 1 allows p and q to be o (1)).
 Corollary 8. There exist constants b ( p 0 ,q 0 ) ,b 3 ( p 0 ,q 0 ) ,b 4 ( p 0 ,q 0 ) ,b 5 ( p that for any sampling rate parameter  X  the following holds with probability at least 1  X  n  X  3 . define ` If for all i  X  [ k ] , either n i  X  ` ] or n i  X  ` [ and if ( is an optimal solution to (CP1), with (Note: We X  X e abused notation by reusing previously defined global constants (e.g. b 1 ) with global func-observation probability  X  can be used as a knob for controlling the cluster sizes we are trying to recover, instead of  X  . We would also like to obtain a version of Theorem 3. In particular, we would like to understand its asymptotics as  X  tends to 0.
 Theorem 9. There exist constants C vation rate parameters  X   X  1 , the following holds with probability at least 1  X  n  X  2 . If ( K,B ) is an optimal solution to (CP1) with c 1 ,c 2 as defined in Theorem 8, and additionally K is a partial clustering induced by U ,...,U r  X  V , and also  X  min ( K )  X  max then U 1 ,...,U r are actual ground truth clusters, namely, there exists an injection  X  : [ r ] 7 X  [ k ] such that U i = V  X  ( i ) for all i  X  [ r ] .
 The proof is given in the supplemental material. Using the same reasoning as before, we derive the following: Theorem 10. Let g = b 3 ( p 0 ,q 0 ) /b 4 ( p 0 ,q 0 ) log b ists a constant C 4 ( p 0 ,q 0 ) such that the following holds. Assume the number of clusters k is bounded by some  X  {  X  0 , X  0 g,..., X  0 g k 0 } for which, if A is obtained with observation rate  X  (zeroing ?  X  X ), then with probability at least 1  X  n  X  2 , any optimal solution ( K,B ) to (CP1) with c 1 ,c 2 from Corollary 8 satisfies (4). (Note that the upper bound on k 0 ensures that  X g k 0 is a probability.) The theorem is proven using the pigeonhole principle, noting that one of the intervals ( ` sizes, and there is at least one cluster of size at least n/k 0 . The theorem, together with Corollary 8 and Theorem 9 ensures the following. On one end of the spectrum, if k 0 is constant (and n is large), then with high probability we can recover at least one large clus-ter (of size at least n/k 0 ) after querying no more than values of A ( i,j ). On the other end of the spectrum, if k 0  X   X  (log n ) / (log log n ) and n is large enough (expo-nential in 1 / X  ), then we can recover at least one large cluster after querying no more than n 1+ O (  X  ) values of A ( i,j ). (We omit the details of the last fact from this version.) This is summarized in the following: Theorem 11. Assume an upper bound k 0 on the num-ber of clusters k . As long as n is larger than some func-tion of k 0 ,p 0 ,q 0 , Algorithm 4 will recover, with proba-bility at least 1  X  n  X  1 , at least one cluster of size at least n/k 0 , regardless of the size of other (small) clusters. Moreover, if k 0 is a constant, then clusters covering all but a constant number of elements will be recovered with probability at least 1  X  n  X  1 , and the total number of observation queries is (5), hence almost linear. Unlike previous results, our recovery guarantee im-poses no lower bounds on the size of the smallest clus-ter. Note that the underlying algorithm is an active learning one, as more observations fall in smaller clus-ters which survive deeper in the recursion of Alg. 4. Algorithm 3 RecoverBigPartialObs( V,k 0 ) (Assume p 0 ,q 0 known, fixed) require: ground set V , oracle access to A  X  R V  X  V , upper bound k 0 on number of clusters n  X  X  V | g  X  b 3 ( p 0 ,q 0 ) /b 4 ( p 0 ,q 0 ) log 2 n for s  X  X  0 ,...,k 0 } do end for return  X  Algorithm 4 RecoverPartialObs( V,k 0 ) (Assume p 0 ,q 0 known, fixed) require: ground set V , oracle access to A  X  R V  X  V , upper bound k 0 on number of clusters { U 1 ,...,U r } X  RecoverBigPartialObs( V,k 0 )
V 0  X  [ n ] \ ( U 1  X  X  X  X  X  U r ) if r = 0 then else end if We experimented with simplified versions of our al-gorithms. Here we did not make an effort to com-pute the precise values of various constants defining the algorithms in this work, creating a difficulty in ex-act implementation. Instead, we assume p and q is known in (CP1), and set c 1 ,c 2 according to (2) with t = 1 4 p + 3 4 q and b 1 = 2. For Algorithm 1, we start with  X  = 1 and multiply it by 1 . 1 in each iteration until a partial clustering matrix is found. In Algorithm 3,  X  is increased by an additive factor of 0 . 025. Still, it is obvious that our experiments support our theoretical findings. A practical  X  X ser X  X  guide X  for this method with actual constants is subject to future work. We use the Augmented Lagrangian Multiplier (ALM) method described in (Chen et al., 2012) to solve (CP1). In the sequel, whenever we say that  X  X lusters { V i 1 ,V i 2 ,... } were recovered X , we mean that (CP1) resulted in an optimal solution (  X  K,  X  B ) with  X  K being a partial clustering matrix induced by { V i 1 ,V i 2 ,... } . Experiment 1 (Full Observation) Consider n = 1100 nodes partitioned into 4 clusters V 1 ,...,V 4 , of sizes 800 , 200 , 80 , 20, respectively. The graph is gen-erated according to the planted partition model with p = 0 . 7, q = 0 . 3, and we assume full observation. We apply our algorithm and check if it successfully recov-ers all the clusters. We repeat for 20 times and observe 90% success. Table 1 shows one of the 20 execution; the algorithm terminates in 2 iterations and the recov-ered clusters at each iteration are shown.
 Experiment 2 (Partial Observation -Fixed Sampling Rate) We have n = 1100 with clusters V ,...,V 4 of sizes 800 , 200 , 50 , 50. The graph is gen-erated with p 0 = 0 . 7, q 0 = 0 . 1, and observation rate  X  = 0 . 3. Out of 20 instances, our algorithm succeeds for 70% of the time. One such instance is shown in Table 1. In the other instances, only V 1 and V 2 are re-covered, probably because the remaining graph is too small for exact recovery under random noise.
 Experiment 3 (Partial Observation -Incre-mental Sampling Rate) We test Algorithm 4. We have n = 1100 with clusters V 1 ,...,V 4 of sizes 800 , 200 , 50 , 50. The observed graph is generated with p 0 = 0 . 7, q 0 = 0 . 3, and an observation rate  X  which we now specify. We start with  X  = 0 and increase it by 0 . 025 incrementally until we recover (and then remove) at least one cluster, then repeat. In all 20 instances, our algorithm recovers all the clusters when it terminates. Table 1 show one typical instance. Experiment 3A We repeat the last experiment with a larger graph: n = 4500 with clusters V 1 ,...,V 6 of sizes 3200 , 800 , 200 , 200 , 50 , 50, and p 0 = 0 . 8 ,q 0 . 2. One execution is shown in Table 1. Note that we recover the smallest clusters, whose size is below Experiment 4 (Mid-Size Clusters) Our current theoretical results do not say anything about the mid-size clusters  X  those with sizes between ` [ and ` . It is interesting to study the behavior of (CP1) in the presence of mid-size clusters. We generated an instance with n = 750, {| V 1 | , | V 2 | , | V 3 | , | V then solved (CP1) with a fixed  X  = 1. The low-rank part  X  K of the solution is shown in Fig. 2. The large cluster V 1 is completely recovered in  X  K , while the small clusters V 3 and V 4 are entirely ignored. The mid-size cluster V 2 , however, exhibits a pattern we find diffi-cult to characterize. This shows that the gap between ` ] and ` [ in our theorems is a real phenomenon and not an artifact of our proof technique. Nevertheless, the large cluster appears clean, and might allow recov-ery using simple procedures. If this is true in general, it might not be necessary to search for a gap free of cluster sizes. Perhaps for any  X  , (CP1) identifies all large clusters above ` ] after a possible simple mid-size cleanup procedure. Understanding this phenomenon and its algorithmic implications is of much interest. An immediate goal is to better understand the  X  X id-size crisis X . Our current results say nothing about clusters that fall in the interval ( ` [ ,` ] ). Our numerical experiments confirm that the mid-size phenomenon is real: they are neither completely recovered nor entirely ignored by the optimal  X  K . obvious pattern.
 Our study was mainly theoretical, focusing on the planted partition model. Our experiments focused on confirming the theoretical findings with data generated exactly according to the distribution we could provide provable guarantees for. It is interesting to apply our methods to real applications, particularly big datasets merged from web application and social networks. Another interesting direction is extending the  X  X eel-ing strategy X  to other high-dimensional learning prob-lems. One intuitive explanation of the small cluster barrier encountered in previous work is ambiguity  X  when viewing from the whole graph, a small cluster is both a low-rank matrix and a sparse one. Only when  X  X ooming in X  (after removing big clusters), small clus-ters patterns emerge. There are other formulations with a similar property. For example, in Xu et al. (2012), the authors propose to decompose a matrix into the sum of a low rank one and a column sparse one to solve an outlier-resistant PCA task. Notice that a column sparse matrix is also low rank. We hope the  X  X eeling strategy X  may also help with that problem. The authors would like to thank the anonymous re-viewers for helpful comments. N. Ailon acknowledges the support of a Marie Curie International Reinte-gration Grant PIRG07-GA-2010-268403, and a grant from Technion-Cornell Innovation Institute (TCII). Y. Chen is supported by NSF Grant EECS-1056028 and DTRA grant HDTRA 1-08-0029. H. Xu is sup-ported by the Ministry of Education of Singapore through AcRF Tier-two grant R-265-000-443-112 and NUS startup grant R-265-000-384-133.
 Ailon, N., Charikar, M., and Newman, A. Aggregating inconsistent information: Ranking and clustering. J. ACM , 55(5):23:1 X 23:27, 2008.
 Ailon, N., Begleiter, R., and Ezra, E. Active learning using smooth relative regret approximations with applications. In COLT , 2012.
 Ames, B. and Vavasis, S. Nuclear norm minimization for the planted clique and biclique problems. Math-ematical Programming , 129(1):69 X 89, 2011.
 Bansal, N., Blum, A., and Chawla, S. Correlation clustering. Machine Learning , 56:89 X 113, 2004. Bollob  X as, B. and Scott, AD. Max cut for random graphs with a planted partition. Combinatorics, Prob. and Comp. , 13(4-5):451 X 474, 2004.
 Cand`es, E., Li, X., Ma, Y., and Wright, J. Robust principal component analysis? J. ACM , 58:1 X 37, 2011.
 Carson, T. and Impagliazzo, R. Hill-climbing finds random planted bisections. In SODA , 2001.
 Chandrasekaran, V., Sanghavi, S., Parrilo, S., and
Willsky, A. Rank-sparsity incoherence for matrix decomposition. SIAM J. on Optimization , 21(2): 572 X 596, 2011.
 Charikar, Moses, Guruswami, Venkatesan, and Wirth, Anthony. Clustering with qualitative information. J. Comput. Syst. Sci. , 71(3):360 X 383, 2005.
 Chaudhuri, K., Chung, F., and Tsiatas, A. Spectral clustering of graphs with general degrees in the ex-tended planted partition model. COLT , 2012.
 Chen, Y., Sanghavi, S., and Xu, H. Clustering sparse graphs. In NIPS. Available on arXiv:1210.3335 , 2012.
 Condon, A. and Karp, R.M. Algorithms for graph par-titioning on the planted partition model. Random Structures and Algorithms , 2001.
 Demaine, E., Emanuel, D., Fiat, A., and Immorlica, N. Correlation clustering in general weighted graphs. Theoretical Comp. Sci. , 2006.
 Eriksson, B., Dasarathy, G., Singh, A., and Nowak,
R. Active clustering: Robust and efficient hierarchi-cal clustering using adaptively selected similarities. arXiv:1102.3887 , 2011.
 Giesen, J. and Mitsche, D. Reconstructing many par-titions using spectral techniques. In Fundamentals of Computation Theory , pp. 433 X 444, 2005.
 Giotis, Ioannis and Guruswami, Venkatesan. Correla-tion clustering with a fixed number of clusters. The-ory of Computing , 2(1):249 X 266, 2006.
 Holland, P. W., Laskey, K. B., and Leinhardt, S.
Stochastic blockmodels: Some first steps. Social net-works , 5(2):109 X 137, 1983.
 Jalali, A., Chen, Y., Sanghavi, S., and Xu, H. Cluster-ing partially observed graphs via convex optimiza-tion. In ICML. Available on arXiv:1104.4803 , 2011. Jalali, Ali and Srebro, Nathan. Clustering using max-norm constrained optimization. In ICML. Available on arXiv:1202.5598 , 2012.
 Krishnamurthy, A., Balakrishnan, S., Xu, M., and
Singh, A. Efficient active algorithms for hierarchical clustering. arXiv:1206.4672 , 2012.
 Mathieu, C. and Schudy, W. Correlation clustering with noisy input. In Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms , pp. 712 X 728. SIAM, 2010.
 McSherry, F. Spectral partitioning of random graphs. In FOCS , pp. 529 X 537, 2001.
 Oymak, S. and Hassibi, B. Finding dense clusters via low rank + sparse decomposition. arXiv:1104.5186v1, 2011.
 Rohe, K., Chatterjee, S., and Yu, B. Spectral clus-tering and the high-dimensional stochastic block model. Ann. of Stat. , 39:1878 X 1915, 2011.
 Shamir, O. and Tishby, N. Spectral Clustering on a Budget. In AISTATS , 2011.
 Shamir, R. and Tsur, D. Improved algorithms for the random cluster graph model. Random Struct. &amp; Alg. , 31(4):418 X 449, 2007.
 Voevodski, K., Balcan, M., R  X oglin, H., Teng, S., and Xia, Y. Active clustering of biological sequences. JMLR , 13:203 X 225, 2012.
 Xu, H., Caramanis, C., and Sanghavi, S. Robust PCA via outlier pursuit. IEEE Transactions on Informa-
