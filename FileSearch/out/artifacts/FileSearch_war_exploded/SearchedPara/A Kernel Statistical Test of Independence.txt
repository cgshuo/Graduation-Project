 MPI for Biological Cybernetics Kernel independence measures have been widely applied in re cent machine learning literature, most dependence measures indicate a statistically significant dependence, however. In other words, we dismiss with high probability the hypothesis that the under lying variables are independent. guaranteed to detect all modes of dependence between the ran dom variables. Contingency table-[19], although to our knowledge they have been used only to co mpare univariate random variables. sample size m ) and accurate means of obtaining a threshold which HSIC will only exceed with independence test for structured data.
 We begin our presentation in Section 2, with a short overview of cross-covariance operators be-tween RKHSs and their Hilbert-Schmidt norms: the latter are used to define the Hilbert Schmidt Independence Criterion (HSIC). In Section 3, we describe ho w to determine whether the depen-operator norms and norms of mean elements of the random varia bles in feature space. Finally, in Section 4, we give our experimental results, both for testin g dependence between random vectors (which could be used for instance to verify convergence in in dependent subspace analysis [25]), downloaded from http : // www . kyb . mpg . de / bs / people / arthur / indep . htm Our problem setting is as follows: Problem 1 Let P P P from each x  X  X , such that the inner product between the features is given by the kernel function map  X  ( y ) . Following [7], the cross-covariance operator C f  X  F and g  X  G , The cross-covariance operator itself can then be written where  X  of the cross-covariance matrix between random vectors. Whe n F and G are universal reproducing compact domains X and Y , then the largest singular value of this operator, k C if x  X  X  X  y [11, Theorem 6]: the operator therefore induces an independ ence criterion, and can be used than the maximum singular value, we may use the squared Hilbe rt-Schmidt norm (the sum of the squared singular values), which has a population expressio n (assuming the expectations exist), where x  X  denotes an independent copy of x [9, Lemma 1]: we call this the Hilbert-Schmidt independence criterion (HSI C).
 We now address the problem of estimating HSIC( P unbiased estimator of (2) is a sum of three U-statistics [21, 22], where ( m ) the set { 1 ,...,m } , k where the summation indices now denote all r -tuples drawn with replacement from { 1 ,...,m } ( r being the number of indices below the sum), K is the m  X  m matrix with entries k kernel k statistic is equivalent to the characteristic function-ba sed statistic [6, Eq. 4.11] and the T 2 kernels, however, such as kernels on strings (as in our exper iments in Section 4) and graphs (see resembles those in [2]. The approach in [4] differs with both the present work and [2], however, B-spline kernels) when computing the empirical test statis tic. statistic HSIC the alternative hypothesis H our case HSIC the null hypothesis (bearing in mind that a zero population H SIC indicates P is defined as the probability of rejecting H independent. Conversely, the Type II error is the probabili ty of accepting P and a Type II error of zero, in the large sample limit.
 How, then, do we set the threshold of the test given  X  ? The approach we adopt here is to derive the asymptotic distribution of the empirical estimate HSIC is therefore divided into two parts. First, we obtain the dis tribution of HSIC H Asymptotic distribution of HSIC The first theorem holds under H Theorem 1 Let to a Gaussian according to The variance is  X  2 Proof We first rewrite (4) as a single V-statistic, where we note that h ated U-statistic HSIC see [22]. Since the difference between HSIC 3 below), HSIC The second theorem applies under H Theorem 2 Under H erate, meaning E Section 5.5.2] where z where the integral is over the distribution of variables z Proof This follows from the discussion of [21, Section 5.5.2], mak ing appropriate allowance for case of a U-statistic, the sum would be over terms  X  Approximating the 1  X   X  quantile of the null distribution A hypothesis test using HSIC guaranteed by the decay as m  X  1 of the variance of HSIC is complex, however: the question then becomes how to accura tely approximate its quantiles. One approach, taken by [6], is to use a Monte Carlo resampling technique: the ordering of the Y sample is permuted repeatedly while that of X is kept fixed, and the 1  X   X  quantile is obtained from the resulting distribution of HSIC expensive to compute). Specifically, we make the approximat ion m HSIC b ( Z )  X  An illustration of the cumulative distribution function (CDF) obtained via the Gamma approximation is given in Figure 1, along with an empirical CDF obtained by repeated draws of HSIC mation is quite accurate, especially in areas of high prob-ability (which we use to compute the test quantile). The accuracy of this approximation will be further evaluated experimentally in Section 4.
 To obtain the Gamma distribution from our observa-tions, we need empirical estimates for E (HSIC var(HSIC b ( Z )) under the null hypothesis. Expressions for these quantities are given in [13, pp. 26-27], however these are in terms of the joint and marginal characteris-tic functions, and not in our more general kernel setting (see also [14, p. 313]). In the following two theorems, we provide much simpler expressions for both quantities, in terms of norms of mean elements  X  covariance operators and C entirely in terms of kernels, which makes possible the appli cation of the test to any domains on which kernels can be defined, and not only R d .
 Theorem 3 Under H where the second equality assumes k by replacing the norms above with \ k  X  in a (generally negligible) bias of O( m  X  1 ) in the estimate of k  X  Theorem 4 Under H Denoting by  X  the entrywise matrix product, A 2 the entrywise matrix power, and B = (( HKH )  X  ( HLH )) 2 , an empirical estimate with negligible bias may be found by r eplacing the taking the product of the empirical operator norms (althoug h the scaling with m is unchanged). in (4), may be computed in O( m 2 ) . test vectors in R d which have a dependence relation but no correlation, as occu rs in independent Independence of subspaces One area where independence tests have been applied is in det er-mining the convergence of algorithms for independent compo nent analysis (ICA), which involves separating random variables that have been linearly mixed, using only their mutual independence. ICA generally entails optimisation over a non-convex funct ion (including when HSIC is itself the for classical approaches to ICA, the global minimum of the optimisation might not correspond to in this context, while the test of [13] has been used in [14] fo r verifying ICA outcomes when the data are stationary random processes (through using a subse t of samples with a sufficiently large delay between them). Contingency table-based tests may be l ess useful in the case of independent independence measures might work better for this problem.
 In our experiments, we tested the independence of random vec tors, as a way of verifying the so-lutions of independent subspace analysis. We assumed for ea se of presentation that our subspaces have respective dimension d follows. First, we generated m samples of two univariate random variables, each drawn at ra ndom from the ICA benchmark densities in [11, Table 3]: these incl ude super-Gaussian, sub-Gaussian, multimodal, and unimodal distributions. Second, we mixed t hese random variables using a rota-tion matrix parameterised by an angle  X  , varying from 0 to  X / 4 (a zero angle means the data are plots in Figure 2, top left). Third, we appended d  X  1 dimensional Gaussian noise of zero mean by an independent random d -dimensional orthogonal matrix, to obtain vectors depende nt across all observed dimensions. We emphasise that classical approach es (such as Spearman X  X   X  or Kendall X  X   X  ) are completely unable to find this dependence, since the var iables are uncorrelated; nor can we recover the subspace in which the variables are dependent us ing PCA, since this subspace has the and d = 1 , 2 , 4 .
 We compared two different methods for computing the 1  X   X  quantile of the HSIC null distribution: repeated random permutation of the Y sample ordering as in [6] ( HSICp ), where we used 200 per-mutations; and Gamma approximation ( HSICg ) as in [13], based on (9). We used a Gaussian kernel, with kernel size set to the median distance between points in input space. We also compared with in each partition, and comparing this with the number of samp les that would be expected under the approximates the RKHSs F and G by finite sets of basis functions. Parameter settings were as in tions). The y -intercept on these plots corresponds to the acceptance rat e of H plots indicate acceptance of H As expected, we observe that dependence becomes easier to de tect as  X  increases from 0 to  X / 4 , when m increases, and when d decreases. The PD and fCorr tests perform poorly at m = 128 , but approach the performance of HSIC-based tests for increa sing m (although PD remains slightly worse than HSIC at m = 512 and d = 1 , while fCorr becomes slightly worse again than PD ). PD Although HSIC-based tests are unreliable for small  X  , they generally do well as  X  approaches  X / 4 (besides m = 128 , d = 2 ). We also emphasise that HSICp and HSICg perform identically, although HSICp is far more costly (by a factor of around 100, given the number of permutations used). Dependence and independence between text In this section, we demonstrate inde-( English text and its French translation. Our dependent data consisted of a set of paragraph-long graphs were matched to random French paragraphs on the same t opic: for instance, an English paragraph on fisheries would always be matched with a French p aragraph on fisheries. This was designed to prevent a simple vocabulary check from being use d to tell when text was mismatched. identification of the person speaking). We used the k -spectrum kernel of [16], computed according on an SVM classifier for Fisheries vs National Defense, separ ately for each language (performance a simple kernel between bags of words [3, pp. 186 X 189]. Resul ts are in Table 1. Our results demonstrate the excellent performance of the HSICp test on this task: even for small sample sizes, HSICp with a spectral kernel always achieves zero Type II error, an d a Type I error sizes, the advantage vanishes. The HSICg test does less well on this data, always accepting H for HSICg at m = 50 still fell between the dependent and independent values of HSIC extracts, which yielded similar results. ple size m . In our experiments, HSIC-based tests always outperformed the contingency table [17] and functional correlation [4] approaches, for both univar iate random variables and higher dimen-sional vectors which were dependent but uncorrelated. We wo uld therefore recommend HSIC-based tests for checking the convergence of independent componen t analysis and independent subspace dependence between data of completely different types, suc h as images and captions. Acknowledgements: NICTA is funded through the Australian Government X  X  Backing Australia X  X  Ability initiative, in part through the ARC. This work was supported in part by the IST Programme of the European Community, under the PASCAL Network of Excel lence, IST-2002-506778.
