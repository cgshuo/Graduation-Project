 In this paper, we are concerned with the problem of au-tomatically extracting web data records that contain user-generated content (UGC). In previous work, web data records are usually assumed to be well-formed with a limited amount of UGC, and thus can be extracted by testing repetitive structure similarity. However, when a web data record in-cludes a large portion of free-format UGC, the similarity test between records may fail, which in turn results in lower performance. In our work, we find that certain domain con-straints (e.g., post-date ) can be used to design better similar-ity measures capable of circumventing the influence of UGC. In addition, we also use anchor points provided by the do-main constraints to improve the extraction process, which ends in an algorithm called MiBAT (Mining data records Based on Anchor Trees). We conduct extensive experiments on a dataset consisting of forum thread pages which are col-lected from 307 sites that cover 219 different forum software packages. Our approach achieves a precision of 98.9% and a recall of 97.3% with respect to post record extraction. On page level, it perfectly handles 91.7% of pages without ex-tracting any wrong posts or missing any golden posts. We also apply our approach to comment extraction and achieve good results as well.
 H.3.m [ Information Storage and Retrieval ]: Miscella-neous -Data Extraction; Web Algorithms, Performance, Experimentation User-generated content, information extraction, structured data  X 
This work was done when Xinying Song and Jing Liu were visiting students at Microsoft Research Asia.

Web 2.0, web applications that encourage user participa-tion, is a well known concept nowadays and is becoming more and more popular. Along with its popularity, enor-mous valuable knowledge and information, which we call user-generated content (UGC), has been accumulated over years and still keeps growing. Extracting this valuable web data in an automatic and scalable manner can benefit a lot of applications like question answering [22], blog or review mining [10], and expert search on web communities.
Typically, web pages generated by Web 2.0 applications contain a large amount of UGC, such as forum posts, blogs, reviews, comments, etc. According to Wikipedia, UGC refers to  X  X arious kinds of media content, publicly available, that are produced by end-users X , and thus has high diversity in both content and format. In this paper we focus on tackling the complexity of extracting web data records containing UGC. Hereafter, for the ease of presentation, we will use as our primary example the application of extracting posts from web forums (as shown in Fig. 1) although our approach can be applied to other types of applications as well. Figure 1: A typical web forum thread page, showing two posts and one embedded advertisement bar
Web data extraction has been a hot research topic [4] in recent years. Recent work mainly follows two categories of approaches: semi-automatic and fully automatic. Semi-automatic approaches require manually labeled data for ei-ther learning extraction rules [11], inducing wrappers based on a tree-structured template [6, 8, 25, 26], or training su-pervised statistical models on a specific domain [20, 27]. Due to the laborious nature of labeling, such semi-automatic ap-proaches are not scalable for web scale data extraction.
In contrast, fully automatic approaches do not require any labeled data. Such approaches mainly study two sub-categories of problems: (1) extracting a list of data objects (records) from a single page and (2) learning a template from multiple pages of the same type [2, 7]. The prob-lem we study in this paper falls into the first sub-category. One of the representative approaches is MDR (Mining Data Records in Web pages) [12, 13] (including its extension work [14, 17, 23]). On the basis of MDR we are to develop our own approach. MDR identifies a list of records by conduct-ing a similarity test against a pre-defined threshold for two sub-trees in the DOM tree of a web page. Such a method is referred to as the similarity-based approach [15], because the underlying assumption is that data records belonging to the same list usually have similar DOM tree structures.
Web data records containing UGC usually consist of two parts: well-formatted structured data (e.g., author, publica-tion date, etc.), referred to as the template part, and free-format unstructured UGC. Due to the existence of UGC, the values of similarity between data records may vary greatly, which makes it less practical to set a good and robust simi-larity threshold and thus results in failure of the similarity-based approach. Fig. 2 shows the tree alignment for the two posts in Fig. 1. We can see that the two records look dissimilar due to the existence of the large portion of UGC.
Intuitively the problem can be solved if we are able to dif-ferentiate the structured template from unstructured UGC on DOM trees and use the template part to perform the similarity test. However, it is not easy to make such differ-entiation in an accurate and robust way.
 Figure 2: Tree match of two posts (gray triangles denote UGC while gray rectangles denote post-date )
Inspired by domain dependent work [20, 27], we find that some domain dependent constraints help detect the appro-priate part of the tree for the similarity test. For example, for extracting posts from web forums, a good and intuitive constraint will be the post-date (publication date of a post) because it is a part of the structured data of posts occur-ring in every post and also can be easily identified (Fig. 2). Motivated by this intuition, we propose two similarity mea-sures to solve the difficulty caused by UGC. Note that, in addition to forums, almost all types of web data records con-taining UGC have post-date , such as blogs, user comments (e.g. Twitter, Flickr, YouTube, Digg) or reviews (e.g. Ama-zon), etc. Therefore, our proposal is not restricted to forum sites.

Domain constraints also provide strong anchor point infor-mation for data record detection. For example, each forum post must contain exactly one sub-tree containing post-date . We proposed a novel data record extraction algorithm in-spired by this intuition.

In summary, in this paper we aim to solve the problem of extracting from a single page a list of web data records that contain UGC in a fully automatic way. Previous work in this topic usually focuses on data objects containing no UGC, for example product lists [13, 23], search engine results [5, 17, 24] or DBLP literature reference records [15]. None of them explicitly claim to take care of the UGC part. Yang et al. [20] work on forum data extraction but in a semi-automatic way. Our contributions are as follows:
Web data extraction has been an extensively studied re-search topic in recent years, resulting in a rich variety of approaches. We discuss highly relevant work here and refer the readers to a survey [4] for further study.

Early work on automatically extracting data records from a single page employs a set of heuristic rules to identify data record boundaries, including [9] and OMINI [3]. Later work is based on repetitive pattern mining from HTML tag sequences, such as IEPAD [5] and Dela [19]. Recent work is based on similar sub-tree mining on the DOM tree of the web page, represented by MDR [13]. It is reported in [13] that MDR outperforms both OMINI and IEPAD.

Due to its simplicity and effectiveness, MDR has attracted wide research interests and been extended in many studies. One improvement direction is incorporating visual layout information [17, 23, 24]. However, visual features usually require proper rendering with additional resources (such as CSS files), thus not being always available and generally helpful. Our work in this paper is purely based on the DOM tree structure without incorporating any visual features. We will show by experimental results that such a pure tag-tree based approach achieves satisfactory performance as well.
Web data can be a relation of k -tuple (where each record has k attributes), or a complex object with a hierarchical structure like nested lists [4]. The former is called flat and the latter nested . In this paper we mainly focus on flat data, for web data records containing UGC are usually displayed in a flat fashion. Nested objects can also be naturally han-dled by an extention of using a post-order traversal along the DOM tree as shown in [14].

Though working with different motivations, Miao et al. [15] also address similar issues of MDR, i.e. the similarity test issue and the greedy manner (Sec. 3.1), but in a dif-ferent way. They transform the input page from a DOM tree into tag path occurrence patterns (called visual signals) and identify the set of tag paths that represent a list of ob-jects by applying spectral clustering. However, visual signals are easily affected by not only the number of data records present on the page but also the noisy tree nodes sharing the same tag paths. Therefore it is hard to guarantee the performance of the clustering on the visual signal set.
Our key idea is employing domain constraints to give strong clues to extract data records accurately. The pro-posed post-date is just one kind of domain constraint, which we will explain in Sec. 4.2. Interestingly we have noticed some related work that utilizes this idea either implicitly or in different scenarios. Embley et al. [9] utilize an ontology heuristic that one or more fields appear once and only once in each record; this resembles the concept of domain con-straints in our work. However, their motivation is to get an estimation of the number of records on the page, which is combined with other heuristics like identifiable  X  X eparator X  tags to discover the record boundary. In contrast, in our work we leverage domain constraints to inspect the structure of the template part of records, based on which the similar-ity test can be conducted and records can be determined. Zheng et al. [25] detect record boundary by the annotation evidence available from labeled data, however, such labeling evidence is not generally available in a large scale.
Zhu et al. [27] and Yang et al. [20], though employing different statistical models, both resort to the same idea: integrating data record extraction, attribute extraction and labeling into one phase, so that the data record extraction can benefit from the availability of the semantics of attribute labeling. This can be seen as utilizing more domain con-straints in our view. However, their work follows essen-tially a semi-automatic approach. To fulfill the need of the statistical model, their work requires heavy domain-specific knowledge, including various pre-defined (attribute) labeling spaces [27], manually-crafted rich features for identifying at-tribute content and capturing the sophisticated relationships between attributes and records [20]. The performance will be highly dependent on the quality of the labeled data as well as the richness of the feature set designed. In the case of up-dating the attribute sets or applying such models from one domain (e.g. forums) to other types of data (e.g. blogs and reviews), we probably would have to re-design the feature set and re-train the model. Therefore we refer to such work as highly domain dependent. In our case, as will be shown in Sec. 6, by incorporating only a small amount of general domain constraints in a fully automatic fashion, our work is accurate across a wide variety of domains/applications.
To summarize the relation with previous work, our ap-proach (1) utilizes the domain constraints not incorporated by previous fully-automatic approaches [13], which results in a more robust performance; (2) resorts to only a bit of general domain knowledge compared to the highly domain dependent approach [20, 27] (that relies on rich domain spe-cific knowledge and features), which makes it applicable to varieties of domains/applications.
In this section, we introduce our proposal for extracting web data records containing UGC by reviewing MDR [13] and discussing its limitations.
To extract data records, MDR is based on two basic ob-servations [13]: 1. A group of similar objects, which forms a data region, 2. Every record in a data region is formed by the same
Based on these assumptions, MDR employs a fairly straight-forward greedy approach to identify data records, by enu-merating the start offset and length of every combination of sub-trees under each parent and checking if the pair of two adjacent sub-tree combinations is similar in HTML tags or structure against a predefined threshold.

The main issue that we find regarding MDR is the similar-ity measure, which judges whether two sub-trees are similar or not. In the original work [13], MDR compares only the tag strings of the roots of two trees. We refer to this kind of similarity measure as Top Level (TL) .Wecanexpect that, due to the lack of inspection on lower level structure, this similarity measure would bring errors. In Liu X  X  later work [12], they mention the use of the tree similarity by comparing two trees, as illustrated in Fig. 3. We refer to it as Tree Similarity (TS) . However, as discussed in Sec. 1, two data records of the same type may still be quite dissim-ilar when they contain a large portion of UGC. Sec. 4.1 will formally define similarity measures.

Another issue, as denoted in [15], is that MDR works in a greedy manner: any mistakes made in earlier steps, by noise, will affect the mining procedure and thus corrupt the final result. In addition, MDR cannot identify non-consecutive data records.
Our main proposal for enhancing MDR is to refine the similarity test between two (data) records to alleviate the effects caused by the irregular UGC part. More specifically, we propose better similarity measures by focusing the cal-culation of similarity on appropriate tree fragments, after conducting the tree matching procedure on the two trees. The key challenge is how to select the appropriate tree frag-ments that the similarity calculation should focus on.
As discussed in Sec. 1, ideally the best way is to differ-entiate the regular template part from the irregular UGC part and compute the similarity on the template part to serve as the similarity measure. We refer to this measure as Template Tree (TT) . For example in Fig. 3, we treat all sub-trees beneath Node D as UGC, and use the remaining trees to measure similarity.

However, it is not easy to differentiate UGC from the tem-plate due to the flexible and complicated usage of HTML tags. We have to resort to finding an approximate tree frag-ment that works as close to the template part as possible. One simple way to do the approximation might be comput-ing similarity using the top N levels of nodes only. We refer to it as Top N Level (TnL) . However, TnL may still work poorly due to a lack of deeper structural information.
Inspired by domain dependent work [20, 27], we find that domain constraints help detect the appropriate tree frag-ments for the similarity test. For example, for the applica-tion of extracting posts from web forums, a good constraint will be the post-date (publication date of a post) because it is a part of structured data occurring in every post and also can be easily identified (Fig. 2). We will discuss the do-main constraints in more detail and show how to use those to design better similarity measures in Sec. 4.2.
Domain constraints also provide strong anchor point in-formation for data record detection. For example, for ex-tracting forum posts, each post (data record) must contain exactly one sub-tree containing post-date . Inspired by this intuition, we propose a novel algorithm called MiBAT (Min-ing data records Based on Anchor Trees) that detects the position and boundary of each record by utilizing such an-chor points. The proposed MiBAT does not have the greedy deficiency of MDR and it can also identify and extract non-consecutive data records in a natural and uniform manner. We will introduce more details about MiBAT in Sec. 5.
In this section we will formulate similarity measures and propose measures guided by domain constraints.
The structure of a web page can be described by the DOM tree [1]. A tree is an ordered pair T =( V, E ) comprising a set V of nodes together with a set E of edges. Hereafter, for ease of presentation, we denote the node sets of two trees T and T 2 to be V 1 and V 2 respectively.

Two trees can be compared and matched by finding an optimal mapping between them, based on which the simi-larity score can be computed. The concept of tree mapping [18] is formally defined as follows:
Definition 1. A mapping M from tree T 1 to T 2 is a set of ordered pairs of nodes ( u, v ), u  X  V 1 ,v  X  V 2 , satisfying the following conditions that for all ( u 1 ,v 1 ) , ( u 2 ,v
In our work we adopt a restricted version of tree mapping called top-down mapping [16] defined below, which has been successfully applied to many web related applications [8]:
Definition 2. A mapping M from tree T 1 to T 2 is top-down if it satisfies the condition: for all non-root nodes u V , v  X  V 2 ,if( u, v )  X  M ,then( parent ( u ) , parent ( v )) where parent ( v ) is the parent of v .
 The tree matching shown in Fig. 3 is actually a top-down mapping. As in [12, 23], we will use a top-down mapping algorithm given by [21] in O ( n 2 )time.
 Generally, the similarity of two trees is computed as [12]:
Definition 3. Given the tree mapping result M ,thesim-ilarity score of two trees T 1 and T 2 is computed as where | M | is the number of match pairs in M ,whichalso equals to the number of matched nodes in T 1 or in T 2 . We can see that Equ. (1) takes all tree nodes into account. As discussed before, given two trees and the mapping result, multiple similarity scores, referred to as similarity measures , can be calculated based on different sub-sets of tree nodes. We refer to sub-sets of tree nodes as tree fragments 1 .
Definition 4. A tree fragment selection function is map-ping f : V X  X  , which maps the node set V of T to a sub-set of nodes f ( V ), i.e. f ( V )  X  V .

Definition 5. Given two trees T 1 , T 2 and their mapping result M ,a similarity measure associated with a tree frag-ment selection function f is computed as where f ( V 1 )  X  f ( V 2 )= { ( u, v ) | u  X  f ( V 1 ) ,v filters out irrelevant match pairs from M with respect to the relevant tree fragments f ( V 1 )and f ( V 2 ).

Each similarity measure is uniquely determined by the associated tree fragment selection function f .Particularly, the four similarity measures discussed in Sec. 3 are formally defined as: Top Level (TL) is by f TL ( V )= { root ( T ) } ,where root ( T ) Tree Similarity (TS) is by f TS ( V )= V .
 Template Tree (TT) is by f TT ( V )= template ( T ), where Top N Level (TnL) is by f TnL ( V )= { v | v  X  V, depth ( v )
We next show how to use domain constraints to derive similarity measures. Particularly, for the ease of presenta-tion, we will use post-date (publication date of a post) as the example constraint although other constraints can be utilized in a similar manner.

A good domain constraint like post-date usually has two properties: (1) being easily identified (e.g., a method based on regular expression matching is sufficient in our experi-ments) and (2) always occurring as key structured data in every data record even across different types of media (e.g., post-date can be found in reviews and blogs as well).
Here a tree fragment is actually a set of nodes, instead of a sub-tree with sets of nodes and edges. We can see that, given the tree mapping result, a similarity score can be computed using the sets of nodes only, as shown in Equ. (1).
We refer to the lowest tree node (usually a leaf node) that contains post-date as the pivot . Our intuition is that the pivot usually belongs to the structured part of a data record, based on which we can sketch (at least part of) the template part for the similarity test, and thus obtain approximation similarity measures that work close to the ideal TT.
The simplest sketch of the template will be the pivot it-self, which corresponds to judging two sub-trees of records as similar if they match at the pivot. We define the corre-sponding similarity measure as follows: Pivot Match (PM) is by f PM ( V )= { p } where p is the
One may find that all ancestors of a pivot, which forms a tree path from root to the pivot, should also belong to the template part and thus be matched. Actually this has already been captured in PM, due to the top-down tree map-ping procedure in use which requires that parent nodes must be matched before child nodes (Sec. 4.1).

Next we try to further improve this similarity measure by enlarging the involved tree fragment. Our intuition is that, since the pivot usually belongs to the template part of the record, its sibling nodes are also likely to belong to the template part. Therefore we have the following measure: Pivot and Siblings (PS) is by f PS ( V )= { v | v  X  V, Taking the two trees T 1 and T 2 in Fig. 3 as an example, if F is the pivot, then f PS ( V 1 )= { E, F, G } ,f PS ( V 2
It is very likely that, compared to PM, PS explores a large portion of the template, and thus has more discriminative power. We will show in Sec. 6.1 that PS achieves signifi-cantly better results than PM.

We conclude this section by summarizing in Fig. 4 the generalization relationship between similarity measures dis-cussed so far. Edge linkage means that the more general measure (the higher) can turn into the more special one (the lower) by enlarging the tree fragments in similarity compu-tation. We can see that TT is the ideal similarity measure, but in general it is hard to obtain. Compared to the ideal measure, TS is strict while other measures are lenient. Figure 4: Relationships between similarity measures
We have introduced two domain-constraint guided simi-larity measures, i.e. PM and PS. In this section, we propose a data record mining algorithm using either PM or PS.
Our intuition is very simple: each record consists of one or several sub-trees, only one of which contains the pivot (Sec. 4.2). We call such sub-trees that contain pivots as anchor trees, since they provide anchor point information about where data records are located. We simply look for possible records around those anchor trees.

For example, in Fig. 5, having identified those anchor trees (triangle nodes in the figure), we can find and extract each data record composed by a set of adjacent sibling sub-trees around every anchor tree (gray nodes in the figure). Figure 5: Mining data records based on anchor trees (triangle nodes represent anchor trees while every four consecutive gray nodes shows a data record)
In our context, we slightly extend the basic assumptions made by MDR [13] and assume that data records have the following assumptions: Same parent A list of data records are formed by child Same length Each data record consists of the same num-Non-contiguity The data record list does not have to be Similarity Data records must be structurally similar with
The first two assumptions are due to MDR. MDR also as-sumes that data record lists are consecutive, which is relaxed in our work by the third assumption. The forth assumption is due to our motivation and proposal in previous sections. One may suspect that these assumptions may be too strong. However, based on our observations, they indeed reasonably capture the truth. For example consider the same length assumption. As pointed out by MDR, this assumption only requires that records have the same number of nodes at the top level; it allows records to have fairly diverse structure at lower levels. Web data records generated from a com-mon template/schema usually share a similar structure but may differ in specific data values or fields. Data fields are usually shown at lower levels while the structure skeleton is primarily determined by upper level nodes. For this reason it is very likely that data records, though perhaps having missing data values in deeper structure, still have the same number of nodes at the top level. Therefore, this assumption not only enables the mining procedure but also captures a wide variety of regularly structured web data objects.
Given these assumptions, our algorithm can be stated as follows: along a traversal on the DOM tree, for each parent node we (1) find the anchor trees and then (2) determine the record boundary, i.e. start offset and length, and extract data records around each anchor tree. In Fig. 5, having identified the anchor trees (denoted by gray triangle nodes), we know that data records start from the position -2 relative to each anchor tree and have a length of 4.

Alg. 1 shows the overall algorithm of MiBAT. Note that under a parent node there may be multiple sets of data ob-jects, each corresponding to a different set of anchor trees. MiBAT will find all sets of anchor trees (Line 4), and process each for record extraction (Lines 5  X  7). Line 6 determines the record boundary and returns data records.
 Algorithm 1 Mining based on anchor trees MiBAT ( T ) 1:  X   X  X } 2: for parent tree node p in T 3: t 1 ...t n  X  the child sub-tree list of p 4:  X   X  FindAnchorTrees ( t 1 ...t n ) 5: for anchor tree list ( a 1 ...a m )in X  6: R  X  DetermineBoundary ( t 1 ...t n ,a 1 ...a m ) 7:  X   X   X   X  X  R } a list of data records found 8: return  X  return all record lists
In the next two sub-sections we will discuss how to find anchor trees from a child sub-tree list of a parent and how to determine record boundary, respectively.
Anchor trees are a set of sibling sub-trees under the same parent, no matter if they are consecutive or non-consecutive, as long as they all satisfy the domain-constraint guided sim-ilarity measure, i.e. matching at or around the pivot.
As discussed in Sec. 4.2, the text of a pivot can be easily identified by a pivot format classifier. For example, a date-time format classifier can be employed in the case of using post-date as the domain constraint. However, not all the nodes containing text in pivot format are real pivots. For example, in forum posts, UGC may also contain strings in date format. We refer to the nodes containing text in pivot format as candidate pivots. From the definition we know that candidate pivots are real pivots only if they can match between all data records. In the example shown in Fig. 6, given gray triangles being anchor trees, we can see that only Node A is the real pivot.
 Figure 6: Finding anchor trees (each triangle de-notes a tree; each circle denotes a candidate pivot; the gray node is the real pivot; gray triangles are anchor trees)
We employ the algorithm shown in Alg. 2 to find anchor trees and also to obtain the real pivot set on-the-fly as a byproduct. The basic idea is very simple: once we obtain candidate pivots, we can use them to identify new anchor trees (Lines 9  X  11); once a new anchor tree is added, we use it to update the candidate pivot set (Line 12). The covered [ i ] ( i =1 ...n ) is used to ensure that a tree belongs to at most one anchor tree set. It also helps avoid returning redundant sub-sets of the anchor trees. Function DomainCompare () compares two trees using one of the domain-guided simi-larity measures, i.e. either PM or PS. It also returns the candidate pivots matched in the tree matching.
 Algorithm 2 Finding anchor trees FindAnchorTrees ( t 1 ...t n ) 1:  X   X  X } 2: covered [ i ]  X  0for i =1 ...n 3: for i  X  1 to n 4: if covered [ i ]=1 then continue 5: a 1  X  i , m  X  1 anchor tree list with counter of m 6: CPSet  X  candidate pivots in t i by classifier 7: for j  X  i +1 to n 8: if covered [ j ]=1 then continue 9: matchedCP  X  DomainCompare ( t i ,t j ,CPSet ) 10: if matchedCP =  X  similarity test succeeds 11: m  X  m +1, a m  X  j expand the list 12: CPSet  X  CPSet  X  matchedCP update 13: covered [ j ]  X  1 14: if m  X  2 m =1means t i is not an anchor tree 15:  X   X   X   X  X  ( a 1 ...a m ) } 16: return  X  return all anchor tree lists DomainCompare ( t i ,t j ,CPSet ) 1: M  X  TreeMatching ( t i ,t j ) 2: matchedCP  X  X } 3: for u in CPSet check each in CPSet 4: if exists candidate pivot v in t j that 5: matchedCP  X  matchedCP  X  X  u } 6: return matchedCP
Using Fig. 6 as an example, we start from Tree 1 and the candidate pivot set (denoted as CPSet ) being { A, B, C } Tree 2 is not an anchor tree because it cannot match Tree 1 at any candidate pivot, thus not being similar by domain constraints. Tree 3 is identified as an anchor tree and CPSet is updated to be { A, B } . Tree 4 is skipped since it does not contain any candidate pivot. Lastly, Tree 5 is added to the anchor tree list and CPSet is finally updated to be { A } such a procedure we successfully find anchor trees 1, 3 and 5, and the real pivot node A.

Note that since Tree 2 also contains candidate pivots, it may belong to another anchor tree list and thus a new set of data records, together with other trees. Alg. 2 will continue this procedure from Tree 2 (Lines 3  X  5), but would not find any matches and will stop.
When anchor trees have been identified, we are ready to determine the boundary of records, i.e. (1) the start offset of a record relative to each anchor tree, and (2) the record length. Once the record boundary is obtained, it will be straightforward to extract data records.
In this section we first use the three cases in Fig. 7 as an example to show the basic idea, then present the general algorithm and its pseudo-code in Alg. 3.
 Figure 7: Boundary determination (triangles denote anchor trees; dashed boxes denote expansions) Case 1. Two or more anchor trees are adjacent. In this case, it is trivial to see that every single anchor tree forms a data record, as shown in Fig. 7(a).

If it is not Case 1, then the minimal distance between two anchor trees is greater than 1, and each data record may con-sist of multiple adjacent sub-trees around the anchor tree, instead of the single anchor tree only. The intuition here is that: each data record should consist of as many consis-tent sub-trees around each anchor tree as possible, as long as the similarity assumption in Sec. 5.1 is satisfied .Take Fig. 7(b) as an example. Compared to treating each data record as consisting of a single anchor tree (the triangular DIV ) only, it would make more sense to treat a record as con-sisting of a sub-tree triple of TR TR DIV (in dashed boxes), because all such triples naturally form a data record set in the figure. Therefore record boundary should result in the largest sections of adjacent sub-trees that consistently form a list of data records around all anchor trees.

Starting from the anchor tree, we try to expand the data record in two directions before (1) we encounter the left or right boundary of the child list or another anchor tree (Line 2 and 9 in Alg. 3), or (2) the newly expanded tree violates the similarity assumption in Sec. 5.1 (Line 4 and 10 in Alg. 3). We say that all valid expanded sub-trees around each anchor tree form an expansion (illustrated in dashed boxes in Fig. 7). Then we face either Case 2 or Case 3 below.
 Case 2. The length of each expansion is less than or equal to the minimal distance between two anchor trees. Take Fig. 7(b) as an example, where the expansion is TR TR DIV . In such a case, no two expansion regions around different anchor trees overlap with each other and it is natural that the sub-trees within each expansion form a data record. Case 3. If the length of each expansion is greater than the minimal distance between two anchor trees, there must be two expansion regions overlapping on a few sub-trees. Take Fig. 7(c) as an example, where the expansion around each anchor tree contains exactly five sub-trees of TR TR DIV TR TR and two consecutive expansion regions overlap on two sub-trees of TR TR . In this case, the largest record length will be determined by the minimal distance of two anchor trees, i.e. 3 in Fig. 7(c), and there will be ambiguity about the start offset of the data record. For example in Fig. 7(c) there are three possible start offsets, i.e. -2, -1 and 0 respec-tively. In this case, we just need to find the start offset lead-ing to the maximum similarity among all possible choices. Note that setting a similarity threshold would not resolve the ambiguity.

All three cases can be integrated into one procedure, as illustrated in Alg. 3. The input is the child sub-tree list t ...t n and indices of anchor trees a 1 ...a m . As discussed above, we first obtain the minimal distance between two an-chor trees ( anchorGap , Line 1), and the expansion (Lines 2 For Case 1 the length of expansion expanLen is 1. Then the record length k will be the smaller one between anchorGap and expanLen (Line 16). Lines 17  X  20 enumerate all possi-ble start offsets and select the best record list that has the largest similarity score. Given the length of k , for each start offset of x , the record list is R ( x ) = R ( x ) 1 ...R list of the i th record (Line 18). The similarity score of R is defined as that is, the sum of the tree similarity scores of the cor-responding sub-trees between every two consecutive data records.

Note that for Cases 1 and 2, where the record length is equal to the expansion length, i.e. k = expanLen ,there will be only one offset candidate x = 0 (Line 17), which is consistent with what we have discussed above. Therefore all three cases are integrated into Alg. 3.
Both MDR and MiBAT output every data record list iden-tified. Every data record list is called a data region [13]. In our scenario we have to determine which region contains the list of concerned records, i.e. forum posts. We refer to it as the main region .

Ideally the main region can be identified by building a classifier using rich domain features (e.g. size or position of the region). In our work, we instead use two simple heuris-tics: 1. The list of posts must have post-date in each record 2. The list of posts should occupy a majority of the page Therefore, to select the main region we first filter out irrel-evant data regions by Heuristic 1 and then select one that has the largest score defined by Heuristic 2. We found in experiments that this selection procedure is highly effective.
Note that Heuristic 1 is built-in by MiBAT, due to the similarity test by domain constraints. But for MDR, we have to explicitly apply Heuristic 1 by filtering out regions that do not contain a matched node containing text in date-time format. Algorithm 3 Determining record boundary DetermineBoundary ( t 1 ...t n ,a 1 ...a m ) 2: left  X  0 left boundary of expansion 3: for k  X  1 to min { anchorGap, a 1 } X  1 4: if exists 1  X  i, j  X  m that DiffTag ( t a i  X  k ,t a j 5: break 6: else 7: left  X  left  X  1 8: right  X  0 right boundary of expansion 9: for k  X  1 to min { anchorGap  X  1 ,n  X  a m } 10: if exists 1  X  i, j  X  m that DiffTag ( t a i + k ,t a j 11: break 12: else 13: right  X  right +1 14: expanLen  X  right  X  left +1 length of expansion 15: R  X  =[] intialize the result 16: k  X  min { anchorGap, expanLen } length of record 17: for x  X  k  X  expanLen to 0 enumerate start offset 20: R  X  =argmax { Score ( R  X  ) ,Score ( R ( x ) ) } Equ. (3) 21: return R  X  return the best record list
In this section, we evaluate our approach empirically. Par-ticularly, we (1) test the scalability of our approach by ap-plying it to forum pages from various types of forum sites (Sec. 6.1) and review/blog pages (Sec. 6.2) and (2) demon-strate its effectiveness by comparing it with one state-of-the-art semi-automatic approach [20] (Sec. 6.3).
In this experiment, we are to evaluate the performance of our method when applying it to forum post extraction. There are many forum software types on the web for forum content generation and management. To verify the scalabil-ity of our approach, we require that the dataset covers as many software types (and therefore forum sites) as possible.
In order to construct the dataset, we first built a list of forum software by checking a few well-known web sites that list, review, or compare forum software packages, i.e. Big Boards, Forum Matrix, Forum Software Reviews, Hot Scripts, and Wikipedia. Then, for each software package within the list, we tried to find at least one sample site by checking the official site of the software or issuing queries like  X  X orums powered by XYZ X  to search engines like Bing and Google. For some software packages that have multiple ver-sions, e.g. vBulletin, we kept one sample site for each version but treated them as belonging to the same software package, due to a lack of clear measurements of the structural differ-ence between multiple versions. We also added a few forum sites powered by customized software. Finally, we obtained a dataset consisting of 307 forum sites that cover 171 known forum software packages and 48 customized types.

The forum software list shows a great variety, covering most popular software packages (like vBulletin, phpBB, In-vision, etc) and various programming languages (like PHP, ASP.NET, etc). For the 307 forum sites, we didn X  X  restrict their topics. Thus, they cover many categories of topics (including travel, computer, science and mathematics, pho-tography and also general topics).

From each forum site that we collected, we randomly crawled a few thread pages. After removing those non-thread pages and thread pages containing zero or only one post, we kept 1,200 thread pages (452 pages were discarded during the process). After manually labeling the dataset (by identifying the golden posts as demonstrated in Fig. 1), we used 200 of them as the development set and the remaining 1,000 pages as the test set. We are also careful to make sure that the test set contains at least one and at most six thread pages for each forum site. In the test set, the total number of posts is 11,139. 64.8% of pages contain no more than 10 posts while 91% of pages contain no more than 20 posts. 12.3% of pages contain only 2 posts while 0.4% of pages contain more than 100 posts.
As our approach is fully-automatic (without the use of labeled data), we implemented MDR [12, 13] as one of the baseline methods. Besides, as discussed in Sec. 3.1, MDR can only extract a consecutive record list. It may miss those posts from the non-consecutive points, either to the end, thus missing one or two records, or in the middle, thus resulting in two separated record lists. As a work-around, we proposed a two-pass (2Pass) method to help MDR extract as many data records as possible from a non-consecutive data region. It is quite straightforward: in the first pass we employ the basic MDR to explore a set of con-secutive records, then in the second pass we check those non-consecutive siblings and put them back to the existing region if they also meet the comparison criterion, regard-less of whether they are adjacent to the existing records or not. We used the 2Pass method as another baseline and denoted it as MDR2Pass. To make the baselines stronger, we also extended both baselines with all types of similarity measures.

We utilized the development set to tune the similarity threshold for all similarity measures. For TnL, we use n =3 and denote it as T3L. We evaluate the result on both post level and page level. On post level we use the standard precision and recall as evaluation metrics. On page level, we examine how many pages a method can handle perfectly without extracting any wrong, or missing any golden, posts. We also exam-ine how many pages a method extracts/misses zero or  X  2 wrong/golden posts.

A post record is regarded as correctly extracted if it con-tains exactly the same set of DOM tree nodes with the golden record after removing blank tag nodes, e.g. &lt;hr /&gt; .
Tables 1 and 2 give the results on both post and page level. We refer to MiBAT using Pivot and Siblings (PS) as the best method. We conducted the significant test (sign test) and the result is that the best method outperforms other methods significantly (p-value=0.01).

For Table 1, we have the following observations: Table 1: Post extraction (post level, prec./rec.)
On page level, as shown in Table 2, besides similar obser-vations as those with post level, we discuss new results in three aspects:
Since the best method perfectly processed 91.7% of pages without errors, we checked all remaining 83 pages and exam-ined the reasons. We found that two major errors are due to (1) forums having the first post under a different parent (22 pages, 26.5%) or in a different structure (28 pages, 33.7%) from the remainings posts, and (2) pivot format classifier er-ror (10 pages, 12%). The former is beyond the scope of our method while the latter can be alleviated by using a more accurate date-time classifier. In addition, we also notice that 6 page errors (7.2%) are due to main region selection error, which can be solved using more domain features.
In this section, we are to test the performance of our method and two baselines when applied to other domains, i.e. extracting comments from blogs and reviews pages.
We constructed two datasets. One consists of randomly sampled blog pages that potentially contain comments from the index repository of Bing. The other consists of ran-domly crawled pages from a list of manually collected 15 well-known UGC sites, which contain user reviews and com-ments on products, news, books and pictures, e.g., Amazon, Flickr, Epions.com. After manually labeling each page and discarding those with zero or only one comment, the final datasets consist of 221 pages from 163 blog sites and 246 pages from the 15 UGC sites.

In this experiment, we didn X  X  tune the threshold, but used that obtained in Sec. 6.1 directly. However, we introduce one more heuristic in the main region selection: we filter out those data regions in which the HTML text is a purely date-time string. This heuristic is very useful when processing blog pages since there is usually a long list of links pointing to older blog archives with text in date-time format.
Table 3 shows the experimental results, where B1 refers to baseline MDR2Pass+TS, B2 refers to MDR2Pass+PS and M is our method MiBAT+PS. For blog comments, our method significantly outperforms the baselines. For review comments, though having similar a recall to baselines (near 81%), our method achieves a higher precision (94.1%).
Note that the result in Table 3 is not as good as that in Table 1 and Table 2, especially for the recall of review comment extraction (81.8%). One main reason is that in 29 pages (out of 246) of the dataset of review pages, not all comment records are located under the same parents, which voilates the same parent assumption (Sec. 5.1). Due to space limitations, we leave the investigation of this issue to the journal version of this paper.

In this section, we are to show the effectiveness of our approach by comparing it with one state-of-the-art semi-automatic approach (Yang et al., [20]).

We use the same dataset as in Yang et al. Among the 20 forum sites in this dataset, there turn out to be 4 known forum software packages and 5 customized types (since 12 out of 20 forum sites use vBulletin). Our dataset stated in Sec. 6.1 contains all the 4 known forum software packages and overlaps on 8 forum sites with their dataset. In addition, we do not exclude those single-post pages from their test corpus, although handling single-post pages is not within the scope of this paper.

Yang et al. X  X  work extracts posts from web forum sites by employing a supervised statistical model equipped with both page-level and site-level knowledge as features. However, our method relies only on the single post page and does not require any labeled data. Again in this experiment we did not tune the threshold parameter, but used the parameter obtained in Sec. 6.1 directly. Pivot and Siblings (PS) is used as the similarity measure.

Table 4 compares the results on post level. The first three rows show the performance of Yang et al. X  X  method incor-porating features of different levels, i.e. single page features (denoted as SP), site-level features (SL) and multiple page features (MP). The next two rows report MiBAT X  X  result, corresponding to taking or not taking single-post pages into account in evaluation respectively.
 Table 4: Comparison with Yang et al. (post level)
We have the following observations, which confirm that our method is very effective:
So far we have focused on the settings that MiBAT works in the online mode, that is, it takes one single page as input, without any additional site-level information. However, it is fairly straightforward to generate a tree-structured template (wrapper) from the extracted post records, as demonstrated by existing research [23], and this template can be used to extract records from pages of the same type on the site. In this way, single-post pages can be also handled naturally. Therefore, we conducted a simple experiment to test Mi-BAT X  X  performance when it works in the batch mode. For each site, we use randomly sampled 20 thread pages to learn the template, by (1) employing MiBAT to process each page, (2) selecting the page for which the extracted data region has the largest score defined by Heuristic 2 in Sec. 5.4 and (3) generating the template from this page as in [23]. No human efforts are involved in the process. The last row (Mi-BAT+MP) in Table 4 shows the performance of this method, which achieves competitive precision and much better recall compared with Yang et al.
In this paper we studied the problem of automatically extracting from a single page a list of web data records that contain UGC. We proposed to utilize domain constraints to design better similarity measures as well as to acquire anchor point information for data record extraction, resulting in our novel mining algorithm MiBAT. Extensive experiments prove the effectiveness of our work.

Future work follows two paths: (1) continue to improve our approach by relaxing the assumptions and applying to more domains, and (2) move to extract data attributes [23].
We would like to thank Jiang-Ming Yang for sharing the dataset in the referred paper, and Matt Callcut for his proof-readingofthispaper.
