 &gt; to gorithms is caused by the instability of split selection meth-ods. Split selection methods evaluate split candidates ac-cording to some split evaluation function and select the best candidate by which to partition the data. If at some node there is no dominant split (i.e., a tie or almost equally good split candidates), a split candidate is selected as the split. The resultant tree structure may be sensitive to small changes, because a minor change in the training sample may cause a split that was slightly inferior to the selected split to become slightly superior. Once a different split is selected, the subtree evolving from that node may be very different from the original one. 
Knowing what might be the cause of the instability prob-lem, how do we justify that one split is slightly inferior to, or almost as good as another? In next section, we analyze the relationship between a data change and the resulting split change. 
To study the relationship between a data change and the resulting split change, we analyze the goodness measures of the two splits before and after the change. Assume that the objects in a training sample belong to either the class '+' or the class '-', each node has only two child nodes, and changes are made to class labels. Assume that S and 
S' are two slightly different training samples and the trees grown on the two samples differ in the subtrees T and T' as shown in Figure 2. Denote the data associated with the root nodes of T and T' by D and D', respectively, and denote the differences between the two data by the change parameter of the objects in D that are changed. Figure 2: The trees grown on S and S' differ in the subtree T and T' 
We use the split selection method from [2], and the gini index to measure the node impurity. The goodness of a split s is defined as the decrease in impurity between s and its two child nodes, i.e., goodn,D(S) = I(n) --pL " I(nz ) --pn . I(nn ), where s divides the data D associated with a node n into two subsets such that a proportion pL of the objects in D go to the left child node nz and the proportion pn go to the right child node ha. The impurity of a node n measured by the gini index is defined as I(n) = 2  X  f+  X  f_, where f X  is the proportion of the objects belonging to class =1:. 
To measure the goodness of the split to of the root node of T, assume that to partitions the data D into two subsets 
Dto,L and Dto,R and a proportion p of the objects in D go to Dto,L. Let f, g, and h be the fractions of the objects in 
D, Dto,L, and Dto,n, respectively, that belong to the class '+'. To measure the goodness of the split candidate t at the root node of T, assume that t partitions the data D into two subsets Dt,L and Dt,n and a proportion q of the objects in D go to Dt,z. Let u and v be the fractions of the objects in Dt,L and Dr,R, respectively, that belong to the class '+'. The other two sets of the parameters measuring the goodness of to and t at the root node of T' can be defined similarly. Table 1 summarizes all the parameters. The following two lemmas establish the properties for Theorem 1. class labels only, D and D' contain equal number of objects. 
If D and D' are divided by the same split, then p' = p and q' =q. satisfy the same split conditions, and S and S' differ only in the class labels of some objects, D and D' contain the same number of the objects. Therefore, p' = p, and q' = q. [] 
To compute the new class frequencies in D~o,z and D~o,n , change e is refined as follows. Note that the parameters are proportional to the number of objects in D. x0 : the fraction of the objects in Dto,L that change class from '+' to '-'. 
Xl : the fraction of the objects in Dto,L that change class from '--' to '+'. x2 : the fraction of the objects in Dto,n that change class from '+' to '-'. x3 : the fraction of the objects in Dto,n that change class from '--' to '+'. 
LEMMA 2. The proportions of the objects in D~o,n and (b} h' = h-~ where co = xo -xl, and cI = x2 -x3, I--p' respectively. 
PROOF. (a) The number of the objects in Dt0,L that be-long to the class '+' is g .p. IDI. Since the change has made x0  X  IDI objects in Dt0,L change class from '+' to '-', and xl  X  IDI objects change classes from '-' to '+', the num-ber of the objects that belong to the class '+' in D~o,L is g .p. [D[ -(xo' [D[ -xl" [D[). From lemma 1, [D'[ = [D[ and p' = p. The frequency of the class '+' in D~o,L is (b) Proof of (b) is similar to the proof of (a). [] 
The new class frequencies u' and v' can be obtained sim-ilarly. 
THEOREM 1. Assume that D' is obtained by changing the classes of a fraction lel of the objects in D. The effect of the change c to the class frequencies in Dto,z and Dto,R are described by co and ct , and the effect of the change e to the class frequencies in Dt,L and Dt,R are described by c2 and co. Then the split to, which is the split selected to partition the data D, will be inferior to a split t in D' if and only if where e(c) = e(co, el, e2) = 2 [ 2gco + 2hCl -2uc2 -2v(co + e~ -e~) --4 -_EL + 1 + ~oo+o,-~)~ ]. p l--p q l--q of the split candidates to and t in D and D' using the gini index are good.,D(to) = 2f(1 --f) --2pg(1 --g) --2(1 --p)h(1 -h), good.,D(t) = 2f(1 --f) --2qu(1 --u) --2(1 --q)v(1 --v), good.,,o,(to) = 2f'(1-f')-2p' g'(1-g')-2(1-p')h'(1-h'), good.,,o,(t) = 2f'(1-f')-2q'u'(1-u')-2(1-q')v'(1-v'). can be expressed as good., ,D, (to)-good.,,o, ( t ) = good.,D (to)-good.,D(t) --2(Ko -Kx), where Ko = pgg + (1 -p)hh -[p' g'g' + (1-p')h'h'], and K1 = quu+ (1-q)vv-[q'u' u' + (1-cco+cx -e2. From Lemma 2, e(c) = ~(co, cl, c2) = 2(Ko-K1) ( X =) Conversely, suppose that goodn,D(tO) --good.,D(t) &lt;_ e(c). goodn,D(to) --goodmD(t) --e(e) = good.,,D,(to) --goodn,,D, (t) _&lt; O. Thus, goodn,,D, (to) &lt; goodn,,D, (t). This proves that to is inferior to t in the changed data D'. [] splits can be defined. Assume that to, t:, ... , t~ are the split candidates in D, to is the best split, and c i* is the smallest change that satisfies the inequality good.,D(to) --
Define a change x to be smaller than another change y if the number of the objects changed by x is less than the number sensitivity and almost equally good splits are given below. of the smallest change that satisfies the inequality (1); that is, the smallest fractional change in D that may cause ti to become superior to to. to with respect to a predefined fraction c if [ci*[ &lt; c. fraction c are those that may be superior to to if the fraction c of D is changed. As an example, consider four split can-didates to, tl, t2, and t3. Assume that to is the best split, and the relative sensitivities between to and the other splits are 1%, 2%, and 5%, respectively. If 3% of data is changed, then to may be replaced by tl and t2. If a change is less than 1%, no split change will occur. the split of a node and t is a split that is almost as good as to with respect to a fraction c. Then t may be the split selected to partition the changed data, if a change is made to a fraction ~:lff_l ISl of the objects in the training sample S, where D is the data associated with the node. respect to a fraction c, t may become superior to to if the fraction c of D is changed. The fraction c change in D is equivalent to the fraction ~1 change in S. Therefore, t S is changed. [] assumed that changes are made to class labels only. This restriction can be relaxed. The theorems for general case are the same except that a different function e is derived [8]. Since the function e is used to identify almost equally good splits, e in Theorem 1 can be used in general case to measure the relative sensitivity between splits. most equally good splits may result in rules sensitive to small changes, and Theorem 1 provides a measurement for iden-tifying almost equally good splits. To lessen the instability problem, the standard split selection algorithm is enhanced by considering splits of the same degree of importance when forming the splitting predicate at a node. The concept of the improvement is described as follows: StableSplit(Data D, Fraction c) 
I. Generate split candidates, and find the best split to. 572 Candidate Measure A1 to Ai A1 A A2 to Ai 
AlO ---1 All= 1 0.000880 A12 = 1 0.000515 Alz = 1 0.000314 
A14 = 1 0.000000 A16 = 1 0.000219 A2o A17 --1 0.000000 Als = 1 0.000013 
Alo = 1 0.000073 Figure 4: Goodness measures and relative sensitiv-ities for the original sample either an insignificant pattern change or a noise, and selects "A1 = 1 A A2 --= 1" as the split of the root node. 
We continued to test the stability of the two algorithms by adding another 2.3% change to the changed training sample. The new trees built by CART and the proposed algorithm are shown in Figures 5(c) and (c'). Though the rule still holds for almost 78% of the objects in the sample, the tree built by CART fails to discover it and only two of the factors in the rule are discovered. The proposed algorithm, on the other hand, is insensitive to noise because the combination of almost equally good splits filtered out noise. 
Figures 5(d) and (d') show the trees built by the two al-gorithms after adding another 2.5% change to the sample. CART fails to discover any of the factors in the rule even though there are still about 76% of the objects classified by the rule. On the other hand, the proposed algorithm identified "A2 = 1", "AT = 1", and "A10 = 1" as almost equally good splits, and chose "A2 = 1 A A7 = 1" as the splitting predicate of the root. Because "A2 = 1" is part of the splitting predicate of the root node, the splits based on A1, A3, and A4 are discovered at lower levels. "At = 1" is included in the splitting predicate of the root node be-cause the accumulated changes which had been in favor of A7 become significant enough to indicate a possible pattern change. The accuracy comparison of the two algorithms is shown in Figure 6. root node. We use synthetic data which contains 1000 ob-jects and has 20 attributes. (Note that sample size and the number of attributes are not the reasons for instability.) The training sample is formed by randomly generating the ob-jects and then assigning the class labels based on the rule, "i/(A1 = 1)A(A2 = 1)A(Aa = 1)A(A4 = 1), then class = '+', otherwise class = '-' ". A 20% noise level is added to the training sample; that is, 20% of the sample objects are misclassifled. We conducted the experiment by adding small changes to the training sample and studied the result-ing tree structure changes. 
Figures 5(a) and (a') show the trees grown on the origi-nal sample by CART and the proposed algorithm, respec-tively. Note that the tree built by the proposed algorithm is more concise and expressive. Figure 4 shows the com-puted relative sensitivities of the splits at the root nodes. The proposed algorithm selects "A1 = 1" and "A2 = 1" to form the splitting predicate because "A2 = 1" is almost as good as "A1 = 1" with respect to 1% change. The com-puted sensitivity shows that "A1 = 1" is less stable than "A1 = i A A2 = I", because "A1 = 1" is insensitive to changes up to 0.7% compared to 2% for "A1 = 1 A Aa = 1". 
We added a 1.5% change to the sample. From the sen-sitivity measures in Figure 4 we already know that "A1 = 1 A A2 = 1" is insensitive to the change, and "A1 = 1" may be replaced. The constructed trees shown in Figures 5(b) and (b') confirm that. Even though the attribute Ar has nothing to do with the classification, it is considered by CART as the most important factor. The proposed algo-rithm, on the other hand, is able to discover the true rule because of the consideration of almost equally good splits. At the root node, "A1 = 1" and "A2 = 1" are identified almost as good as "AT = 1". The evaluation of the com-binations of the three splits filters out "At = 1", which is Figure 6: The accuracy comparison of the two algo-rithms 
The results show that the proposed algorithm is more sta: ble, more tolerant to noise, and can reflect possible pattern changes. Unlike the series of abrupt changes in the tree structures built by CART, the proposed algorithm holds the tree structure until the changes are significant enough to be-come a new pattern. At the beginning, both algorithms are able to discover the rules. As data changes, CART starts to pick the noise or insignificant pattern change to parti-tion data. On the contrary, the proposed algorithm is able to discover the rule. Thus the proposed algorithm is more accurate. 
We have presented fundamental theorems for the instabil-ity problem, ~md evaluated tree classifiers by their stability. Theorem I gives the relationship between a data change and the resulting split change. Based on Theorem 1, split sensi-tivity and splits that are almost equally good can be defined. 
