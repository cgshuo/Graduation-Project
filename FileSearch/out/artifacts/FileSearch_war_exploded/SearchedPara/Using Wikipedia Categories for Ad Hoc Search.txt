 In this paper we explore the use of category information for ad hoc retrieval in Wikipedia. We show that techniques for entity ranking exploiting this category information can also beappliedtoadhoctopicsandleadtosignificantimprove-ments. Automatically assigned target categories are good surrogates for manually assigned categories, which perform only slightly better.
 H.3.3 [ Information Search and Retrieval ]: Retrieval Models Experimentation, Measurement, Performance Ad hoc retrieval, Category information, Wikipedia
When retrieving information from Wikipedia, we can take advantage of the specific structure of this resource. The document structure, links and categories all provide addi-tional information that can be exploited. In this paper we will focus on category information. Category information has proved to be of great value for entity ranking in Wi-kipedia [5]. Improvements occur not only when manually assigned target categories are used, but also when target categories are deducted from example entities. The main difference with ad hoc retrieval is that, in entity ranking, pages can only be relevant if they are of the right entity type, whereas ad hoc retrieval places no categorical restrictions on relevance. However, if a Wikipedia category is relevant to the query topic, the pages that fall under this category may be more likely to be relevant. Our first research question is therefore: Since usually ad hoc topics have no target categories as-signed to them, and providing target categories for entity ranking is an extra burden for users, we also examine ways to assign target categories to queries. Our second research question is: In the rest of this paper we first describe our models for using category information in section 2, then our experiments in section 3 and finally our conclusion in section 4.
To use Wikipedia X  X  category information, we associate query topics to categories, and experiment with both manual and automatic assignment of categories. Manual assignment was done by one of the authors. The advantage of automat-ically assigning target categories is that it requires no user effort. There are many ways to automatically categorize top-ics, for example by using text categorization techniques [3]. For this paper we keep it simple and exploit the existing Wi-kipedia categorization of documents. From our baseline run we take the top N results, and look at the T most frequently occurring categories belonging to these documents, while re-quiring categories to occur at least twice. These categories are assigned as target categories to the query topic.
We use the Wikipedia categories by defining similarity functions between the categories of retrieved pages and the target categories. Pages with categories similar or equal to the target categories get a high category score. Unlike the approach in [5], where scores are estimated using lexi-cal similarity of category names, we use similarity of pages associated with the category.

We use a language modeling approach [1] to calculate dis-tances between categories. First of all we make a maximum likelihood estimation of the probability of a term occurring in the concatenated text of all pages belonging to that cate-gory. To account for data sparsity, we smooth the probabil-ities of a term occurring in a category with the background collection, which is the entire Wikipedia. The final P ( t is estimated with a parsimonious model [2] that uses an it-erative EM algorithm as follows:
E-step: e t = tf t,C  X   X P ( t
M-step: P ( t | C )= e t P Table 1: Overlap with categories of relevant docu-ments The maximum likelihood estimation of P ( t | C )isusedas initial probability. Now we can calculate distances between categories. We do this using KL-divergence to calculate a category score that is high when the distance between the categories is small. We sum the scores of each target cat-egory, using only the minimal distance from the document categories to a target category. So if one of the categories of the document is exactly the target category, the distance and also the category score for that target category are 0, no matter what other categories are assigned to the document.
Besides the category score, we also need a content score for each document. This score is calculated using a language model with Jelinek-Mercer smoothing without length prior. Finally, we combine the content and category scores through a linear combination. Both scores are calculated in the log space, and then a weighted addition is made.

S ( d | QT )=  X   X  log ( P ( q | d )) + (1  X   X  )  X  log ( S
In this section we first describe our experimental setup, then examine the effects of using category information for retrieval, and compare the effects of manual and automatic category assignment.
To create our baseline runs incorporating only the content score, we use Indri [4]. Our baseline is a language model using Jelinek-Mercer smoothing with  X  =0 . 1. We also ap-ply pseudo-relevance feedback, using the top 50 terms from the top 10 documents. The category score is only calcu-lated for the top 1000 documents of the baseline run. These documents are reranked to produce the run that combines content and category score.

Our topic set consists of the INEX Ad hoc topics from 2007, to which we manually and automatically assigned cat-egories. For the automatically assigned categories, we have two parameters, N the number of top results to use, and T the number of target categories that is assigned for each topic. For the parameter  X  we tried values from 0 to 1, with steps of 0.1. The best values of  X  turned out to be on the high end of this spectrum, therefore we added two additional values of  X  : 0.95 and 0.98.
To find the best values for parameters N and T ,wecom-pare the top categories in our run to the top categories of the relevant documents. Table 1 shows the overlap between the automatically assigned categories and the relevant cat-egories.

The results of our experiments expressed in MAP are sum-marized in Table 2. This table gives the content score, which we use as our baseline, the category score, the combined score using  X  =0 . 9 and the best score of their combination with the corresponding value of  X  . The best value for  X  dif-fers per topic set, but for all sets  X  is quite close to 1. The reason for the high  X  values is that the category scores are an order of magnitude larger, because instead of scoring a few query terms, all the terms occurring in the language model of the category are scored. So even with small weights, the category score contributes significantly to the total score.
When we use the category information for the ad hoc top-ics with manually assigned categories MAP improves signif-icantly with an increase of 11.3%. Using the automatically assigned topics, almost the same results are achieved. The best automatic run uses the top 10 documents and takes the top 2 categories, reaching a MAP of 0.3438, a significant im-provement of 9.1%.
In this paper we have investigated the use of categories to find information in Wikipedia. Using category information leads to significant improvements over the baseline, so we find a positive answer to our first research question. Consid-ering our second research question, automatically assigned categories prove to be good substitutions for manually as-signed target categories. Similar to the runs using manually assigned categories, using the automatically assigned cate-gories leads to significant improvements over the baseline. This research was supported by the Netherlands Organiza-tion for Scientific Research (NWO, under project # 612.-066.513, 639.072.601, and 640.001.501).

