 The analysis of the leading social video sharing platform YouTube reveals a high amount of redundancy, in the form of videos with overlapping or duplicated content. In this pa -per, we show that this redundancy can provide useful infor-mation about connections between videos. We reveal these links using robust content-based video analysis technique s and exploit them for generating new tag assignments. To this end, we propose different tag propagation methods for automatically obtaining richer video annotations. Our tec h-niques provide the user with additional information about videos, and lead to enhanced feature representations for ap -plications such as automatic data organization and search. Experiments on video clustering and classification as well a s a user evaluation demonstrate the viability of our approach . H.4 [ Information Systems Applications ]: Miscellaneous; H.3.5 [ Information Systems ]: INFORMATION STOR-AGE AND RETRIEVAL X  On-line Information Services Algorithms video duplicates, content-based links, automatic tagging , neighbor-based tagging, tag propagation, data organizati on
The rapidly increasing popularity and data volume of mod-ern Web 2.0 content sharing applications originate in their ease of operation for even unexperienced users, suitable me ch-anisms for supporting collaboration, and attractiveness o f shared annotated material (images in Flickr, bookmarks in del.icio.us, etc.). One of the primary sites for video shari ng is YouTube 1 . Recent studies have shown that traffic to/from this site accounts for over 20% of the web total and 10% of the whole internet [5], comprising 60% of the videos watched on-line [9].

The growing size of folksonomies has motivated a real ne-cessity to provide effective knowledge mining and retrieval tools. Current solutions are primarily focused on the analy -sis of user generated text; folksonomies have shown to pro-vide relevant results at a relatively low cost, as they are contributed by the community and text-based tools have been used for topic detection and tracking [1], information filtering [30], or document ranking [29].
 However, manually annotating content such as videos in YouTube, is an intellectually expensive and time consuming process, and, as a consequence, annotations such as tags are often very sparse. Furthermore, keywords and community-provided tags lack consistency and present numerous irreg-ularities (e.g. abbreviations and mistypes). This can lead to a decreased quality of the information presented to users of the Web 2.0 environment; moreover, it makes the utilization of techniques for automatic data organization using clas-sification or clustering, retrieval and knowledge extracti on relying on textual features a difficult task. Content-based techniques, however, are not yet mature to outperform text-based methods for these purposes [11].

Recent studies [3, 27] show evidence of a significant amount of visually redundant footage (i.e. near-duplicate videos ) in video sharing websites, with a reported average of over 25% near-duplicate videos detected in search results. Many wor ks have already started to consider this redundancy a problem for content retrieval, and propose ways to remove it from the system. In this paper, we look at duplication from a differ-ent perspective and demonstrate that content redundancy in Web 2.0 environments such as Youtube can be seen as a feature rather than a problem. We automatically analyze the dataset to find duplicates and exploit them to obtain richer video annotations as well as additional features for automatic data organization.

Previous work has focused on metadata as the way to get to the content. In our work we proceed differently, using content to improve the overall quality of annotations. We present a novel hybrid approach combining video content analysis and network algorithms to improve the quality of annotations associated to videos in community websites. In contrast to existing approaches, we adapt robust video du-plicate detection methods to the Web 2.0 context, enabling us to effectively use content features to link video assets an d http://www.youtube.com plementary information which can help to create a richer des cription. build a new graph structure connecting videos. In what is a novel approach to folksonomy analysis, we use these new content-based links to propagate tags to adjacent videos, utilizing visual affinity to spread community knowledge in the network.

Figure 1 shows an example of automatically generated tags using our proposed methods. Uploaders of overlap-ping sequences of the same video provide their personal per-spective on its content, in the form of different keywords. Individually, each description lacks part of the semantics conveyed by the asset; when considering them together, we achieve a more comprehensive description. By uploading videos containing redundant sequences, different users hav e contributed to complete the information about the original source video, including characters (e.g. Bush), dates (e.g . 2008), and other more general concepts (e.g. protest, right s).
The rest of this paper is organized as follows: In Sec-tion 2 we discuss related work on folksonomies and sharing environments, tagging, and duplicate detection. We pro-vide a short overview of duplicate and overlap detection of videos and a graph formalization of these relationships in Section 3. Based on this formalization, we describe several techniques for automatic tagging in Section 4, both neighbo r and context-based (TagRank). In Section 5 we provide the results of the evaluation of our automatic tagging methods for YouTube video clustering and classification as well as a user study. We conclude and show directions of our future work in Section 6.
Mining links between resources has received growing re-search interest, especially on the web, the most popular and referenced example being Google X  X  PageRank [21], where the graph of web document nodes is formed by the hyperlinks contained in them. We can also find examples of algorithms taking advantage of  X  X mplicit X  links [29], where links are i n-ferred from user actions (e.g. access patterns). In this lat er category we can include near-duplicate document detection techniques [28]. Tools, such as Charikar X  X  fingerprint [4], have shown success in achieving near-duplicate identifica-tion in massive collections of web pages [20]. The graph of inter-document links is used, in this case, to perform many common procedures in web processes optimization, such as web crawling filtering [20], enhanced ranking schemes [29], first story detection [24], or plagiarism detection [8]. The se techniques rely on the textual nature of documents. In this paper, we focus on exploiting visual relationships available in richer multimedia scenarios.

Visual content analysis enhances knowledge about video collections. Many previous works use this information to im -prove retrieval results. Re-ranking is perhaps the most com -mon application. Hauptmann et al. [27] use a Content-based copy retrieval (CBCR) approach to detect near-duplicate videos in order to promote diversity on search results by re-moving redundant entries. Liu et al. [19] proceed similarly, but consider also text to establish links between videos, to enable covering different interpretations of queries at the top of the results list. In contrast to previous approaches, we e x-ploit visual links between videos to improve the quality and homogeneity of annotations .

Various approaches have been exploiting graph structures in Folksonomies. A node ranking procedure for folksonomies , the FolkRank algorithm, has been proposed in [12]. FolkRank operates on a tripartite graph of users, resources and items , and generates a ranking of tags for a given user. Another procedure is the Markov Clustering algorithm (MCL) in which a renormalization-like scheme is used in order to de-tect communities of nodes in weighted networks [26]. A PageRank-like algorithm based on visual links between im-ages is used to improve the ranking function for photo search in [14]; in contrast we are using weight propagation for au-tomatic video tagging . In another paper on image search Craswell and Szummer [6] introduce a random walk algo-rithm operating on a bipartite click graph with edges con-necting queries and clicked images. The term X  X agRank X  X c-curs in another context in the preliminary work [25] where a general ranking of tags for whole folksonomies is generated , rather than the relevancy of tags for individual objects as described in our work for the specific case of videos.
To the best of our knowledge, our paper is the first to pro-pose a hybrid approach using content-based copy retrieval techniques in combination with novel tag propagation meth-ods to automatically annotate videos in folksonomies.
In this section, we provide a short overview on automatic video duplicate and overlap detection. We then formalize the output of overlap detection methods as graph structures . In the next section, we describe our methods for automatic tag assignment and propagation based on this formalization .
The identification of near-duplicate video content has re-ceived significant attention by the multimedia research com -munity for a number of applications, e.g. copyright in-frigement detection. These techniques are commonly re-ferred as Content-based Copy Retrieval (CBCR). Current works feature recall and precission values close to 100%, an d support the detection of noisy and degraded copies, which may include compression artifacts, analog conversion dist or-tions, overlaid content such as subtitles or chromatic chan ges among others [15]. Many of the principles used by text-based duplicate detection techniques can be adapted to the video domain [13]. Fingerprints are the most commonly used de-tection tool; they are generated using specific features of moving visual content, such as temporal video structure or time-sampled local or global image invariants.

We built a video copy detection system to detect redun-dancy in the YouTube scenario based on the work origi-nally presented in [23]. This fingerprint-based method re-lies on robust hash functions, which take an input message (video frames in our case) and generate a compact output hash value, with the condition that similar input messages generate similar output values. All videos involved in the detection are converted into hash values, and detection is performed as a search problem in the hash space.

The detailed description of the CBCR system used is be-yond the scope of this paper and can be consulted in the original publication. However, it is necessary to introduc e a global overview of how the system works as well as some terminology which we will use in the following sections. We consider a video collection C = { V i : 1  X  i  X  N } of N elements. Each video V i = { f i j : 1  X  j  X | V i |} of the collection is composed of a number, | V i | , of frames, f . We also consider a set of video queries, denoted by Q k = { f k j : 1  X  j  X | Q k |} . The system uses the robust hash function and search procedure described in [23] to identify in the incoming stream Q k any occurrences of a complete video V i , or a fragment V ( m,n ) i = { f i j : m  X  j  X  n } of it. The ability to detect video fragments enables us to ob-tain a more comprehensive visual affinity graph, compris-ing not just near-duplicates, but other forms of redundancy which can also be exploited in the context of this paper. The computational complexity of this process is linear with the database size, but can be reduced using Locality Sensi-tive Hashing (LSH [16]) techniques to improve performance in more demanding scenarios. We conducted a pilot ex-periment to validate the viability of this detector using 15 0 hours obtained by querying for popular music video clips in YouTube, including numerous duplicates and common dis-tortions (different frame rates, resolutions, subtitles, e tc). We used this as the query video collection, Q k , and com-pared them to a database C of DVD quality versions of the queried songs. The precision-recall break-even point was approximately 0.8.
If we consider our content database C = { V i : 1  X  i  X  N } to be a set of YouTube search results, every item V i can potentially include content from every other. To perform a comprehensive identification, we need to consider the set C  X  = C  X  V i and the input query Q i = V i for i  X  [1 , N ], where N denotes the size of the database.

Visual connections between each pair of videos in the ana-lyzed set are expressed in the form V ( m,n ) i  X  V ( p,q ) sequent processing stage is performed, in which connection s found for each pair of videos ( V i , V j ) are closely analyzed to classify their relationship. We consider the duration of ea ch pair of videos, | V i | and | V j | . We also consider the visual overlap between them, O ( V i , V j ), i.e. the video resulting of the frames present in both V i and V j . According to these parameters we classify videos as: These relationships can be formalized in a visual affinity graph G O = ( V O , E O ) with undirected weighted edges de-noting overlaps. In this abstraction, videos can be consid-ered as single elements instead of frame sets; we will refer t o them as v i in the rest of the paper.

The set of nodes V O  X  C is the subset of videos in the collection having one or more relationships of any kind to others. The set of edges E O links together visually related videos:
In this section, we describe two classes of novel methods for automatic tag assignment using content overlap. These are 1) neighbor-based methods, which take just immediately overlapping videos, i.e. direct neighbors in the overlap gr aph into account, and 2) the TagRank method based on propa-gation of tag weights within the visual overlap graph.
For neighbor-based tagging we consider relationships in the overlap graph G O = ( V O , E O ) in order to transfer tags between adjacent videos.
For simple neighbor-based tagging, we transform the undi-rected overlap graph into a directed and weighted graph O = ( V O , E  X  O ), with ( v i , v j ) and ( v j , v i )  X  E  X  E . The weight w ( v i , v j ) assigned to an edge ( v i , v the influence of video v i on video v j for tag assignment. In this paper we are using the heuristic weighting function where | v j | is the (temporal) length of video v j , and | v denotes the length of the intersection between v i and v j This weighting function describes to what degree video v j is covered by video v i . Note that in case where v i and v are duplicates, if v i is a parent of v j (meaning that v more general video, and v j can be considered as a specific Figure 2: Overlap regions R 1 , . . . , R 5 of a video v cov-ered by four other videos scene from this video) then the weighting function w assigns the maximum value 1 to ( v i , v j ).

How can we use the described relationships and weights for automatic tagging? Let T = { t 1 , . . . , t n } be the set of tags originally (manually) assigned to the videos in V O and let I ( t, v i ) be an indicator function for original tags t  X  T , with I ( t, v i ) = 1 iff v i was manually tagged by a user with tag t , I ( t, v i ) = 0 otherwise. We compute the relevance rel ( t, v i ) of a tag t from adjacent videos as follows: i.e., we compute a weighted sum of influences of the overlap-ping videos containing tag t . In this way, we obtain relevance values for all tags from the overlapping videos, and can gen-erate autotags ( v i ) of automatically assigned new tags for each video v i  X  V using a threshold  X  for tag relevancy: In order to compute feature vectors (e.g. for clustering or classification) for videos v i , we use the relevance values rel ( v i , t ) of tags t as features weights. Enhanced feature vectors can be constructed as a combination of the origi-nal tag weights ( I ( t, v i ) normalized by the number of tags) and the relevance weights for new, automatically added tags (normalized by the number of tags).
For a set of overlapping videos, i.e. neighbors in the graph, situations with multiple redundant overlaps as shown in Fig -ure 2 can occur. In order to avoid a too high increase of the relevance values for automatically generated tags in com-parison to original tags, we propose a relaxation method for regions with redundant overlap.

Let N ( v ) = { v i | ( v i , v )  X  E  X  O } be the set of overlapping videos for video v . An overlap region R i of v can be defined as a video subsequence s  X  v, | s | &gt; 0, of maximum length contained in a maximum subset Y  X  N ( v ), i.e. with none of the remaining videos N ( v )  X  Y overlapping with s . Figure 2 shows an example of a video with 5 overlap regions. For the k = P y  X  Y I ( t, y ) videos from Y containing tag t , the contribution of t to an overlap region s is computed by i.e., for each additional video contributing the same tag t , this contribution is reduced by a relaxation parameter 0 &lt;  X   X  1. In order to obtain all contributions to the relevance of tag t to video v , we sum up the contributions for the Figure 3: Although there is no overlap between videos v 1 and v 3 , a context relationship (dotted line) can be established via the overlap graph. (disjoint) overlap regions. Putting all pieces together we obtain the following equation for the relevance of tag t for video v : rel ( t, v ) = X where is the number of videos in subset X containing tag t . Thresh-olds can be applied and feature vectors constructed as de-scribed above for the simple case.
Up to now, we have just taken the direct neighbors of videos in the overlap graph into account, neglecting contex t relationships as shown in Figure 3. In this subsection we describe a tag weight propagation method which allows for the iterative transfer of tags along paths of arbitrary leng th. Due to its similarity to PageRank-like weight propagation for web pages, we call the method TagRank . However, the objective of TagRank is not to assign relevance values for the videos in the overlap graph; instead, TagRank is an al-ternative method for computing relevance values rel ( t, v ) of a tag t for a given video v .

Let w ( v i , v j ) be the edge weight corresponding to the in-fluence of video v i to an overlapping video v j . Then we define the TagRank TR( t, v i ) for a video v by the following recursive relationship: rel ( t, v i ) = TR( v i , t ) = X
For all videos v i this computation can be expressed in matrix form as:
This Eigenvector equation can be solved using power it-eration. Similar to Kleinbergs HITS [17] the rows are not guaranteed to sum up to 1, and re-normalizations of the rank vector are required. In contrast to the Random Surfer Model for PageRank, we don X  X  consider the possibility of random jumps within the video graph. For the TagRank method to converge, this is not necessary because sinks as in the Web graph are impossible due to the reflexivity of the overlap relationship. Furthermore, this enables us to perform the TagRank computation separately, and thus more efficiently, for each connected component, which is crucial because of the high number of tags.

Another aspect is that we want to take the original (man-ually generated) tag assignments into account. If we would simply consider the solution for Equation 8 we would lose this information because for a given node v in a connected component the solution would eventually converge to the same value TR( v, t ) for each tag t . This is not intuitive and instead we perform a limited number  X  of iterations (where  X  is a tuning parameter) using the original tag assignment in the form as start vector for the TagRank iterations. Limiting the number of iterations results in higher weights for tags from videos in the closer neighborhood, and is similar to the stra t-egy deployed in [6] for random walks in click graphs.
In this section we present the results of our evaluation for automatic tagging. We first describe our strategy for gathering a video collection from YouTube, and elaborate on the characteristics of our data set. Then, we present the outcome of our two-fold evaluation methodology: 1) we examine the influence of the enhanced tag annotations on automatic video classification and clustering; 2) we provid e the results of a user evaluation by direct relevance assess-ment of the automatically generated tags for a set of videos.
We created our test collection by formulating queries and subsequent searches for  X  X elated videos X , analogously to t he typical user interaction with the YouTube system. Given that an archive of most common queries does not exist for YouTube, we selected our set of queries from Google X  X  Zeit-geist archive from 2001 to 2007. These are generic queries, used to search for web pages and not videos. In order to re-move queries not appropriate to video search (e.g.  X  X indows update X ) we removed those for which YouTube returned less than 100 results. In total, 579 queries were accepted, for which the top 50 results were retrieved. Altogether, we col-lected 28 , 216 videos using those queries (some of the results were not accessible during the crawling because they were removed by the system or by the owner). A random sam-ple of these videos was used to extend the test collection by gathering related videos, as supported by the YouTube API. In total, 267 queries for related videos were performed , generating 10 , 067 additional elements.

The complete collection, C , used for the evaluation had a final size of 38 , 283 videos, 390 GB of information com-prising over 2900 hours of video with an average duration of 4:15 min per video. We used the  X  X obs project X  [22] as the programming interface to access content and characteristi cs of downloaded videos.
 Table 1: Results of CBCR analysis for collection C .
We analyzed the collected set C to quantify the presence of visual connections in them, and studied the distribution of the different relationships. We considered the study of G
O = ( V O , E O ), the graph of related videos, as well as the relevance of each kind of relationship, computing the subse t of nodes having each of the defined visual links.
Table 1 summarizes the results. A significant proportion of videos in the test collections, over 35%, feature one or more connections to other elements. These videos, denoted by the set V O , represent the subset of the collection that we used to evaluate our tag propagation methods in Section 5.2. They are organized in 3779 connected components, with an average size of 3 . 61 videos per component. Note that any video can be linked by more than one relationship, contribut -ing to the different categories that we analyze below.
Duplication is the most common form of visual relation-ship, accounting for over 15% of the videos. Note that du-plicated videos share the exact same visual content, and their tags should be intuitively valid in any element of the clique they belong to. The high presence of this relation-ship suggests the occurrence of many useful links which can be exploited to propagate relevant tags. Part-of relation-ships have also noticeable presence in the set, though their amount is considerably lower than duplicates, with an aver-age of around 9%.
In Section 4, we have presented different methods for au-tomatically generating tags, resulting in richer feature r ep-resentations of videos. Machine learning algorithms make use of this feature information to generate models, and to automatically organize the data. In this section, we will show results for classification (supervised learning) as we ll as clustering (unsupervised learning) of YouTube videos us -ing feature vectors obtained by automatic tagging.
Classifying data into thematic categories usually follows a supervised learning paradigm and is based on training items that need to be provided for each topic. We used linear sup-port vector machines (SVMs) in our experiments, as they have been shown to perform very well for text-based classi-fication tasks (see, e.g.,[7]).
 As classes for our classification experiments, we chose the YouTube categories containing at least 900 videos in our data set. These were the 7 categories  X  X omedy X ,  X  X ntertain-ment X ,  X  X ilm &amp; Animation X ,  X  X ews &amp; Politics X ,  X  X ports X ,  X  X eople &amp; Blogs X , and  X  X usic X . We did this in order to obtain equal numbers of training/test videos per category, omitting videos without category label as well as categorie s containing fewer videos. We performed binary classificatio n experiments for all ` 7 2  X  = 21 combinations of these class pairs (e.g.  X  X usic X  vs.  X  X ports X ). Settings with more than two classes can be reduced to multiple binary classification pro b-lems that can be solved separately [2]. For each category, we randomly selected 400 videos for training the classificatio n model and a disjoint set of 500 videos for testing the model. We trained different models based on T=10,25,50,100, 200, and all 400 training videos per class.

We compared the following methods for constructing fea-ture vectors from video tags: 1. BaseOrig : Vectors based on the original tags of the 2. NTag : Vectors constructed based on the tags and 3. RedNTag : Vectors using tags generated by overlap 4. TagRank  X  (with  X  = 2,4,8 iteration steps): Vectors
Our quality measure is the fraction of correctly classified videos ( accuracy ). Finally, we computed micro-averaged re-sults for all topic pairs. The results of the comparison are shown in Table 2. The main observations are:
Clustering algorithms partition a set of objects, YouTube videos in our case, into groups called clusters . For our exper-iments we choose k-Means [10], a simple but still very popu-lar and efficient clustering method. Let k be the number of classes and clusters, N i the total number of clustered doc-uments in class i , N ij the number of documents contained in class i and having cluster label j . Unlike classification results, the clusters do not have explicit topic labels. We define the clustering accuracy as follows:
Accuracy for clustering can be seen as a measure of how good the partitioning produced by a clustering method re-flects the actual class structure. Note that, similar to some other measures of cluster validity known in the literature, the minimum value for clustering accuracy is larger than 0 (1 /k for the case of equal number of items in each of the k classes).

For a number of clusters k = 2 , 3 , 4 , 5 we considered all possible ` 7 k  X  combinations of tuples of classes (e.g. the triple  X  X usic X  vs.  X  X ports X  vs.  X  X ilm &amp; Animation X ) for the 7 YouTube categories mentioned in Section 5.2.1. For each tuple, we randomly selected 500 videos per class, performed the k-means algorithm and computed the macro-averaged accuracy for the k -tuples.

We constructed feature vectors based on the original tags ( BaseOrig ), simple neighbor-based tagging ( NTag ), over-lap redundancy aware neighbor-based tagging ( RedNTag ), and TagRank ( TagRank  X  with number of iterations  X  = 2,4,8) in the same way as for the classification experiments described in the previous Section 5.2.1. The results of the comparison are shown in Table 3. The main observations are very similar to the supervised scenario:
To support the results obtained for automatic data orga-nization, we conducted an additional user-based experimen t. Three assessors provided relevance judgments for the auto-matically generated tags. For this experiment we chose the neighbor based methods ( NTag and RedNTag ) as they provided the best results for both classification and cluste r-ing.

A web service was implemented to help the assessors to provide their judgments. The interface included a playable version of the video, automatically extracted key-frames, the title and the description to help them understand the con-tent, so they could provide more accurate judgments. The videos presented to the evaluators were randomly selected and the tags were displayed in random order. The evalua-tors were asked to rate each new tag using a five-level Likert scale [18]. This interface is depicted in Figure 4.
A total of 3578 tags, for 300 different videos, were manu-ally assessed using the described interface. (Manual evalu a-tion of video annotations can require a substantial amount of time including the inquiry of background information.) We studied the average relevance for the set of generated tags, autotags ( v i ), for different values of threshold  X  (see Figure 4: Web interface for the collection of rele-vance judgments equation 3). For this purpose, we sorted the list of tags in decreasing order of rel ( t, v i ) value, and selected  X  values at different levels of that list, in increments of 10% of the full autotags ( v i ) size. The results are shown in Figure 5.
The figure reveals an expected decreasing relevance pat-tern for growing values of autotags ( v i ) set sizes. By raising the threshold  X  , we can obtain increasingly higher average relevance values for the sets of new tags. When consider-ing the 10% best rated autotags ( v i ) for each method, Red-NTag achieves higher relevance in comparison with NTag . On average, the difference between both methods is small, in consonance to results obtained for automatic data orga-nization.
In this paper, we have shown that content redundancy in social sharing systems can be used to obtain richer anno-tations for shared objects. More specifically, in what is a Figure 5: Average relevance judged manually by as-sessors for increasing sizes of autotags ( v i ) considered novel hybrid approach, we have used content overlap in the video sharing environment YouTube to establish new con-nections between videos forming a basis for our automatic tagging methods. Classification and clustering experiment s show that the additional information obtained by automatic tagging can largely improve automatic structuring and orga -nization of content; our preliminary user evaluation indic ates an information gain for viewers of the videos.

We plan to take confidence values for visual overlap de-tection into account in order to generate a smoothing func-tion to reduce the contribution of weak edeges to the overall graph. We also plan to extend and generalize this work to consider links between various kinds of resources such as videos, pictures (e.g. in Flickr) or text (e.g. in del.icio. us), and use them to extract additional information (e.g. in form of tags or other metadata) to improve knowledge manage-ment and information retrieval processes. We also plan to consider the analysis and generation of deep tags (i.e. tags linked to a small part of a larger media resource) which are starting to be supported by the main Web 2.0 sites. Further-more, we aim to refine the defined propagation methods by introducing smoothing methods to improve context-based tag propagation.
We think that the proposed techniques have direct appli-cations to search improvement, where augmented tag sets can reveal resources previously concealed. In this connec-tion, integration and user evaluation within a system con-text and encompassing additional complementary retrieval and mining methods is of high practical importance. This research was partially funded by the EU Marie Curie ToK Grant Memoir (MTKD-CT-2005-030008), and the Large-Scale Integrating EU Project LivingKnowledge . [1] J. Allan, R. Papka, and V. Lavrenko. On-line new [2] E. L. Allwein, R. E. Schapire, and Y. Singer. Reducing [3] M. Cha, H. Kwak, P. Rodriguez, Y.-Y. Ahn, and [4] M. S. Charikar. Similarity estimation techniques from [5] X. Cheng, C. Dale, and J. Liu. Understanding the [6] N. Craswell and M. Szummer. Random walks on the [7] S. Dumais, J. Platt, D. Heckerman, and M. Sahami. [8] N. Shivakumar and H. Garcia-Molina. Scam: A copy [9] P. Gill, M. Arlitt, Z. Li, and A. Mahanti. Youtube [10] J. Han and M. Kamber. Data Mining: Concepts and [11] J. S. Hare, P. H. Lewis, P. G. B. Enser, and C. J. [12] A. Hotho, R. J  X  aschke, C. Schmitz, and G. Stumme. [13] S. Huffman, A. Lehman, A. Stolboushkin, [14] Y. Jing and S. Baluja. Pagerank for product image [15] A. Joly, O. Buisson, and C. Frelicot. Content-based [16] Y. Ke, R. Sukthankar, and L. Huston. An efficient [17] J. M. Kleinberg. Authoritative sources in a [18] R. Likert. A technique for the measurement of [19] L. Liu, W. Lai, X.-S. Hua, and S.-Q. Yang. Video [20] G. S. Manku, A. Jain, and A. D. Sarma. Detecting [21] L. Page, S. Brin, R. Motwani, and T. Winograd. The [22] J. San Pedro. Fobs: an open source object-oriented [23] J. San Pedro and S. Dominguez. Network-aware [24] N. Stokes and J. Carthy. Combining semantic and [25] B. Szekely and E. Torres. Ranking bookmarks and [26] S. van Dongen. A cluster algorithm for graphs. [27] X. Wu, A. G. Hauptmann, and C.-W. Ngo. Practical [28] H. Yang and J. Callan. Near-duplicate detection by [29] B. Zhang, H. Li, Y. Liu, L. Ji, W. Xi, W. Fan, [30] Y. Zhang, J. Callan, and T. Minka. Novelty and
