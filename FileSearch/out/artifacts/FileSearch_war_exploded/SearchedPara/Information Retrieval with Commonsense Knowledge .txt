 This paper employs ConceptNet, which covers a rich set of commonsense concepts, to retrieve images with text descriptions by focusing on spatial relationships. Evaluation on test data of the 2005 ImageCLEF shows that integrating commonsense knowledge in information retrieval is feasible. H.3.3 [ Information Search and Retrieval ]: Retrieval Models Experimentation, Performance Commonsense Knowledge, ConceptNet, Concept Expansion External resources like WordNet have been introduced to deal with the issue of divergence in vocabulary between queries and documents [2,5]. However, a query sometimes diverges from its relevant documents not only in vocabulary, but also in  X  X oncept X . For example, a query  X  groom and bride  X  may be relevant to a document containing the word  X  wedding  X . As  X  groom and bride  X  and  X  wedding  X  are related concepts in common sense, using common sense in IR may make progress for the state-of-the-art IR systems. using Cyc [3] and ConceptNet [6]. Liu and Lieberman [4] used commonsense knowledge to expand the query with the related concepts. However, the above works did not make formal evaluation, so that we were not sure if the effects of introducing common sense are positive or negative. This paper investigates the usefulness of commonsense knowledge for image retrieval. The basic idea is to find concepts of objects that have spatial relationship (i.e., occur nearly in space) with each other, based on text descriptions of images. ConceptNet [6] is presently the largest commonsense knowledgebase. Its framework is a semantic network. Nodes in ConceptNet are semi-structured natural language fragments, e.g.,  X  food  X ,  X  grocery store  X ,  X  buy food  X ,  X  at home  X , etc. Each node represents one concept in real world. An edge between two nodes represents one relationship between two concepts. Nowadays ConceptNet covers relationships of twenty types such as causal, spatial, functional, etc. Here we only adopt project_spatial ( concepts ) to consider spatial relationship between concepts. This function carries out spreading activation [7] through spatial relationships in ConceptNet. It helps IR systems find those concepts which usually coexist with the concepts of parameters in the space of real world. For example, the result of project_spatial ( X  groom  X ,  X  bride  X ) is { X  tuxedo (15%) X ,  X  bouquet (15%) X ,  X  veil (15%) X ,  X  wedding dress (14%) X ,  X  gown (13%) X ,  X  wedding cake (12%) X , ...}, which is a set of concepts and their strengths of relationships to  X  groom  X  and  X  bride  X . For image collections with text annotations, text descriptions can be viewed as  X  X ocuments X . We use ConceptNet to expand concepts of objects in a text description T as follows. I. Lemmatizing and POS-tagging T by using MontyLingua [6]. II. Choosing concepts in T to be expanded. These concepts are III. Utilizing spatial relationships in ConceptNet to find concepts IV. POS-tagging the projection-concepts found at stage III and chosen, and nouns not part of a noun phrase chosen are selected too. For example, given a text description  X  studio portrait of woman in patterned blouse  X , the concepts chosen for expansion, are  X  studio portrait  X ,  X  woman  X , and  X  blouse  X . At stage III, each of the projection-concepts returned by ConceptNet has its relationship related to the expanded-concept with some strength. A projection-concept with strength lower than a threshold will not be used later. We also limit the number of projection-concepts no more than half of the number of expanded-concepts. The original text descriptions and the projection-concepts of images are indexed separately. At query time, the IR system searches the two document collections and linearly combines the results as follows. where S 1 ( i ) and S 2 ( i ) are the scores of image i in the collection of projection-concepts, respectively, and c 1  X  c 2  X  1. The ImageCLEF test collection in CLEF 2005 [1], i.e., the St. Andrews image collection and 28 ad hoc topics in English, is adopted. There are 28,133 images each of which is a nnotated with a text description including three fields: headline (H), caption (P), and category (T). While we explore different fields for concept expansion, the corresponding baseline performance is obtained using the same fields. Okapi with BM25 is employed. Three evaluation criteria are considered: mean average precision (MAP), precision at top 20 documents (P@20), and R-Precision. obtained with c 1 =1 and c 2 =0, i.e., concept expansion is not used. It shows that IR with commonsense knowledge (OURS) is better than the baseline (BL). When using headline and caption, the improvement is verified as significant by a t-test with a confidence level of 95% (marked by an asterisk in Table 1). It indicates that our approach is more suitable for precision-oriented tasks. The following examples explain how common sense improves P@20. Several relevant images of topic 18  X  woman in white dress  X  are ranked low in the baseline because their captions describe occasions about wedding, but do not contain the critical words such as  X  X hite dress X . These images are expanded with projection-concepts like  X  X hite dress X , so their ranks are moved to the top 20 in our approach. Another example is topic 24  X  close-up picture of bird  X . While captions of some relevant images mention bird-related concepts, such as  X  X agle X ,  X  X est X , etc, our approach can enhance the concept of  X  X ird X  for those images highly related to bird, so that the performance gets improved. Another interesting example is topic 9  X  Horse pulling cart or carriage  X . Some of the relevant images describe the royal in a carriage. In the descriptions of these images, the concept  X  X arriage X  doesn X  X  occur but  X  X oyal X  does. Our approach expands these images with  X  X arriage X  since  X  X arriage X  and  X  X oyal X  have spatial relationship with each other in ConceptNet. This kind of common sense is culture-specific. Field(s) used ( c 1 =.8, c 2 =.2) .0799 ( c 1 =.7, c 2 =.3) .2693 ( c 1 =.95, c 2 =.05) observe how the effects of commonsense knowledge vary when different fields are used. The improvement of using headline only is 1.92% and not significant. In contrast, when both headline and caption are used, the improvement in P@20 (i.e., 2.83%) is significant. It confirms that commonsense knowledge is deeply context-sensitive. When we use headline only for concept expansion, the context information is so little that introducing commonsense knowledge has comparatively less benefit. When both headline and caption are used, the context information is rich enough to introduce useful commonsense knowledge. However, when all the three fields are used, it shows that the context information increases but the improvement of our approach (0.4%) is much smaller than that of using headline and caption. The reason is that the category field has covered much commonsense knowledge needed for this task. Note that this field consists of tags of multiple categories annotated by librarians working at St. Andrews Library. It also reflects that humans always employ their commonsense knowledge when dealing with document (image) classification. Even we can ascertain commonsense knowledge is useful in IR because the baseline performance of using the category field is obviously superior to that without using the category field. criterion is P@20, five topics get improvements without lowering the performance of the other topics. Table 2 shows the five topic numbers and their performances. We find that four of the five topics have baseline performance lower than the average (0.3821). This phenomenon indicates that our approach mainly takes effect on  X  X ifficult X  topics. In other words, it illustrates that the use of commonsense knowledge is necessary for the tasks that are  X  X ifficult X  for current IR systems. Topic No. &lt;5&gt; &lt;9&gt; &lt;11&gt; &lt;18&gt; &lt;24&gt; We introduce commonsense knowledge into IR by expanding concepts in image descriptions with spatially related concepts. The experiment results show that our approach is more suitable for precision-oriented tasks and for  X  X ifficult X  topics. In future work, we will investigate a dynamic weighting scheme to combine scores in a way of query by query. We will also investigate how to apply other kinds of relationships in ConceptNet to other IR tasks. Research of this paper was partially supported by National Science Council, Taiwan, under the contracts NSC94-2752-E001-001-PAE and NSC95-2752-E001-001-PAE. [1] Clough, P., M X ller, H., Deselaers, T., Grubinger, M., [2] Kim, S.B., Seo, H.C., and Rim, H.C. Information retrieval [3] Lenat, D.B. Cyc: A large-scale investment in knowledge [4] Liu, H. and Lieberman, H. Robust photo retrieval using world [5] Liu, S., Liu, F., Yu, C.T., and Meng, W. An effective [6] Liu, H. and Singh, P. ConceptNet: a practical commonsense [7] Salton, G. and Buckley, C. On the use of spreading activation 
