 Currently, the most prevalent way to music information retrieval is based on the so-called metadata search, which operates by manually annotating music data according music in the same way as they retrieve the text information. However, since the concrete descriptions, such as title or lyrics, usually cannot reflect the abstract content of music directly, it is often the case that users know what the song they want sounds like, but just cannot recall or totally have no idea about its title or lyrics. As a result, formulating a text query explicitly sometimes could be a difficulty for users. To overcome this handicap, a new promising solution is the so-called query-by-humming or query-by-singing [1-9], which allows users to retrieve a song by simply humming or singing a fragment of that song. Since no textual input is needed, query-by-humming or query-by-singing could not only increase the usability of a music retrieval system, but also allow the access to the system with no keyboard supported, e.g., to retrieve music via mobile devices. symbolic representation based on musical scores. It specifies some manner of instructions about what, when, and how long a note should be played with which instruments. Examples of this category include MIDI (Musical Instrument Digital Interface) and Humdrum. Since no real acoustic signal is included, a MIDI or Humdrum file would have different sounds when it is played by different devices. The second category of digital music is concerned with the data containing the acoustic signals recorded from real performances. The most widespread formats are CD (.wav) and MP3 (MPEG-I Layer 3). This type of music is often polyphonic , in which many notes may be played simultaneously, in contrast to monophonic music, in which at most one note is played at any give time. From the perspective of music retrieval, searching for a MIDI object from a database is much easier than searching for an MP3 object, because extracting score information is easy from a symbolic file, query-by-humming or query-by-singing [1-6] has almost focused on MIDI music. However, methods, specifically designed to retrieve CD or MP3 music [7-9] are still very scarce and needed to be explored. comparison between the symbolic sequences. Ghias et al. [1] proposed an approximate pattern matching approach, which converts a query or each of the MIDI preceding one). The similarity between a query X  X  sequence and each of the MIDI documents X  sequences are then computed by string matching. Since most users are not professional singers, a query X  X  sequence inevitably contains transposition errors (e.g., UUSDD  X  UDSDD), dropout errors (e.g., UUSDD  X  USDD), and duplication errors (e.g., UUSDD  X  UUUSDD). To tolerate the above errors, several methods have been further proposed, with the dynamic time warping (DTW) [4][5] being the most popular. Moreover, it is obvious that the three symbols  X  X  X ,  X  X  X , and  X  X  X  are not sufficient to represent all kinds of melody patterns precisely. Thus, more sophisticated representations, such as MIDI note number representation and broken-edge graph representation [4], have been studied subsequently. In addition, related work in [2][3] further considered the tone distribution in a song, tone transition between two adjacent notes, and the difference with respect to the first note. investigation on retrieving polyphonic objects of popular music. To permit the comparison between monophonic queries and polyphonic documents, methods of main melody extraction and error correction are proposed, with the statistical analysis accommodate users X  unprofessional or personal singing styles, methods are proposed existing in queries. popular music are discussed in Section 2. The configuration of our music retrieval system is introduced in Section 3. Our approaches for melody extraction and melody comparison are presented in Sections 4 and 5, respectively. Finally, the experimental results are discussed in Section 6 and conclusions are drawn in Section 7. reliable information retrieval system. It is known that popular music is simple by the characteristics of popular music, which could be exploited to benefit the realization of a popular music retrieval system.
 instrumental statement of the subsequent sections; 2. verse , which typically comprises the main content of story represented in a song's lyrics; 3. chorus , which is often the heart of a song where the most recognizable melody is present and repeated; 4. bridge , which comes roughly two-thirds into a song, where a key change, tempo change or new lyric is usually introduced to create a sensation of something new coming next; 5. outro , which is often a fading version of chorus or an instrumental restatement of some earlier sections to bring the song to a conclusion. Except for intro and outro, each of the sections may repeat several times with varying lyrics, melodies, etc. The most common stru cture of a popular song consists of  X  X ntro-verse-chorus-verse-chorus-bridge-outro X  or  X  X ntro-verse-verse-chorus-chorus-bridge-outro X . In essence, verse and chorus contain the vocals sung by the lead singer, while intro, bridge, and outro are often largely accompaniments. This makes it natural that verse and chorus are the favorites that people go away humming when they hear a retrieval system. Hz) to B5 (987.8Hz), corresponding to a varying range of 43 semitones 1 . However, it range, and the varying range of the sung notes within a verse or chorus section can be even narrower. Fig. 1 shows an example of a segment of a song performed with that of the chorus, because the sung notes within a section do not spread over all the possible notes, but only distribute in their own narrower range. An informal survey using 50 pop songs shows that the range of sung notes within a whole song and within a verse or chorus section are around 25 and 22 semitones, respectively. Fig. 2 details sung notes, by discarding the virtually impossible notes. accompaniment during most or all vocal passages. Various signals from different sources are mixed together into a single track in a CD. Even in stereo, signals in each accompaniment only. This makes it more difficult to design a system for retrieving CD music than to design a system for retrieving MIDI music, since the desired background signals. In addition, the background accompaniments often play notes several octaves above or below the singing, in order that the mix of music can sound accompaniments further make the vocal melody notoriously difficult to extract. An example of song performed with MIDI is shown in Fig. 3. We can see from the notes indicated by arrows that a large proportion of sung notes are accompanied by the notes one or two octaves above them. Nevertheless, the harmonicity, viewed from another angle, may be exploited as a constraint in the determination of sung notes. A method based on this idea to improve the main melody extraction is discussed in a greater detail in Section 4. similar melody to the sung query. Fig. 4 shows a block diagram of the retrieval system. It operates in two phases: indexing and searching. system. Further, a user X  X  singing tends to begin with the initial of a sentence of lyrics. For instance, a user may query the system by singing a piece of The Beatle X  X   X  X esterday X  like this,  X  X uddenly, I'm not half to man I used to be. There's a shadow hanging over me. X  By contrast, a sung query like  X  X  used to be. There's a shadow X  or  X  X alf to man I used to be. X  is believed almost impossible. Therefore, segmenting a song into semantically-meaningful phrases could not only match users X  queries better, but also improve the efficiency of the system in the searching phase. Next, the second step of the indexing proceeds with the main melody extraction for each of the phrases. It converts an audio signal from the waveform samples into a sequence of musical note symbols. Accordingly, the database is composed of note-based sequences of stage of this system, the phrase segmentation is performed manually. complete phrase or an incomplete phrase but always starts from the beginning of a phrase. The system commences with the end-point detection that records the singing voice and marks the salient pauses within the singing waveform. Next, the singing waveform is converted into a sequence of note symbols by using the main melody extraction modular as in the indexing phase. Then, the retrieval task is narrowed down to a problem of comparing the similarity between the query X  X  note sequence and each of the documents X  note sequences. The song associated with the note sequence most similar to the query X  X  note sequence is regarded as relevant and presented to the user. Given a music recording, the aim of main melody extraction is to find the sequence inventory of possible notes performed by a singer. The task, therefore, is to determine music signal is first divided into frames by using a fixed-length sliding window. Every frame is then convolved with a Hamming window and undergoes a fast Fourier transform (FFT) with size J . Since musical notes differ from each other by the fundamental frequencies (F0s) they present, we may determine if a certain note is sung in each frame by analyzing the spectral intensity in the frequency region where the F0 of the note is located.  X  j  X  J . If we use the MIDI note number to represent e 1 , e 2 ,..., e N , and map the FFT indices into MIDI note numbers according to the F0 of each note, the signal X  X  energy on note e n in frame t can be estimated by and where  X  X  is a floor operator, F ( j ) is the corresponding frequency of FFT index j , and U (  X  ) represents a conversion between the FFT indices and the MIDI note numbers. large proportion of the signal X  X  energy. Sometimes the energy on a harmonic note number can be even larger than the energy on the true sung note number; hence, the note number receiving the largest energy is not necessarily what is sung. To determine the sung note more reliably, this study adapts Sub-Harmonic Summation (SHS) [10] to this problem. possible note by summing up the signal X  X  energy on a note and its harmonic note numbers. Specifically, the strength of note e n in frame t is computed using value less than 1 to discount the contribution of higher harmonics. The result of this summation is that the note number corresponding to the signal X  X  F0 will receive the largest amount of energy from its harmonic notes. Thus, the sung note in frame t could be determined by choosing the note number associated with the largest value of the strength, i.e., during the vocal passages, the note number associated with the largest value of strength may not be produced by a singer, but the concurrent instruments instead. As a consequence, whenever the strength of the sung note is not the maximum, an error estimation of the sung note would happen. This problem may be alleviated by using Gb, G, Ab, A, Bb, and B) by ignoring the difference between octaves. As mentioned in Section 2, since the background accompaniments often play notes several octaves tone class. However, because of using 12 classes only, tone chroma cannot express a melody pattern with sufficient precision to distinguish from one another. Recognizing this, we focus on investigating the method to correct the error estimation of the sung notes, instead of using the tone chroma representation. of rectification, which identifies the abnormal individuals in a note sequence and forces them back to the normal. The abnormality in a note sequence roughly arises from two types of errors: short-term error and long-term error. The short-term error is error could be amended by using the median filtering, which replaces each note of frame with the local median of its neighboring frames. One the other hand, the long-singer. These successive wrong notes are very likely several octaves above or below sequence being wider than that of the true sung note sequence. As mentioned in 22 semitones. Therefore, we may adjust the suspect notes by shifting them several obtained by where R is the normal varying range of the sung notes in a sequence, say 22, and o is considered as a wrong note and needs to be adjusted, if it is too far away from o , i.e., | o  X  o | &gt; R /2. The adjustment is done by shifting the wrong note  X  ( o  X  ( o t  X  o  X  R /2)/12  X  octaves. Given a user X  X  query and a set of music documents, each of which is represented by a note sequence, our task here is to find a music document whose note sequence is most similar to the query X  X  note sequence. Since users X  singing may be significantly different from what they want to retrieve in terms of key, tempo, ornamentation, etc., it is impossible to find a document X  X  sequence exactly match the query X  X  sequence. Moreover, the main melody extraction is known to be frequently imperfect, which sequences. To perform a reliable melody similarity comparison, an approximate matching method tolerable to occasional note errors, is therefore needed. from a user X  X  query and a particular music document to be compared, respectively. similarity. For this reason, we apply Dynamic Time Warping (DTW) to find the q ,..., q t } and { u 1 , u 2 ,..., u " }, computed using: and where  X  is a small constant that favors the mapping between note q t and u " , given the conditions for the above recursion are defined by where we have assumed that a sung query always starts from the beginning of a document. After the distance matrix D is constructed, the similarity between q and u can be evaluated by frame between T /2 and min(2 T , L ) of the document X  X  sequence, and assume that a document whose length of sequence less than T /2 would not be a relevant document to the query. and the document could be rather different. To deal with this problem, the dynamic compared. This could be done by shifting the query X  X  note sequence up or down that of the document to be compared. Briefly, a query X  X  note sequence is adjusted by where q and u are the means of the query X  X  note sequence and the document X  X  note sequence, respectively. However, our experiments find that the above adjustment can not fully overcome the transposition problem, since the value of ( q  X  u ) can only reflect a global difference of key between a query and document, but cannot handle this problem better, we further modify the DTW similarity comparison by considering the key shifts of a query X  X  note sequence. Specifically, a query sequence q most similar to u , i.e., where q (0) = q . documents, another problem needed to be addressed is the existence of voiceless regions in a sung query. The voiceless regions, which may arise from the rest, pause, etc., result in some notes being tagged with  X 0 X  in a query X  X  note sequence. However, the corresponding non-vocal regions in the document are usually not tagged with  X 0 X , because there are accompaniments in those regions. This discrepancy may severely discount the similarity S ( q , u ) for any q and u having the same tune. Fig. 5 shows an detected by simply using the energy information, the accurate detection of non-vocal regions in a music document remains a very difficult problem. Therefore, to sidestep this problem, we further modify the computation of d ( t , " ) in Eq. (7) by voiceless regions of a query. Fig. 5. (a) a phrase document, (b) a query sung according to this phrase, (c) the log-energy profile of this sung query 6.1 Music Database The music database used in this study consisted of 100 tracks 3 from Mandarin pop music CDs. Each of the tracks was segmented manually into several phrases, which gives a total of 2,613 phrase documents. The waveform signal of each phrase document was down-sampled from the CD sampling rate of 44.1 kHz to 22.05 kHz, to exclude the high frequency components that usually contain sparse vocal information. In addition, we collected 253 queries sung by 5 male and 2 female users. Each query is sung according to one of the 2,613 phrase documents, but can be an incomplete phrase. and song accuracy. The phrase accuracy is defined as the percentage of the queries that can receive their corresponding phrase documents, i.e., In addition, considering a more user-friendly scenario that a list of phrase documents ranked according to the query-document similarity can be provided for users X  choices, we also computed the Top-N phrase accuracy defined as the percentage of the querieswhose corresponding phrase documents are among Top-N. the same song, and what a user would like to retrieve is a song instead of a phrase. It is computed by We also computed the Top-N song accuracy defined as the percentage of the queries whose corresponding songs are among Top-N. 6.2 Experimental Results Our first experiment was conducted to evaluate the performance of song retrieval with respect to the potential enhancement of the main melody extraction. Specifically, we compared the three methods to main melody extraction, namely, the note sequence generation by Eq. (4) along with the six-frame median filtering, the conversion of note sequences to tone chroma sequences, and the note sequence rectification by Eq. (5). The inventory of possible sung notes consisted of the MIDI numbers from 41 to 83, which corresponds to the frequency range of 87 to 987 Hz. The melody similarity comparison in this experiment was performed on the basis of Eqs. (9) and (10). Table 1 shows the retrieval results. We can see from Table 1 that the retrieval performance obtained with the method of using Eq. (4) and median filtering was the worst among the three methods compared, mainly because this method determines the sung notes background accompaniments. It is also shown in Table 1 that a slightly better performance can be achieved by converting note sequences into tone chroma sequences, which avoids the risk of mis-estimating a sung note as its octaves. However, due to the limited precision in melody representation, the tone chroma performance. By contrast, the note sequence rectification by Eq. (5) keeps the fine precision of using note numbers in melody representation and tries to correct the errors in a note sequence. We can see from Table 1 that the note sequence rectification noticeably improves the retrieval performance, and proves superior to the tone chroma method. Main melody extraction method 
Note sequence generation by Eq. (4) and six-frame median filtering 32.0 / 37.9 41.1 / 49.8 50.6 / 62.9 
Conversion of note sequences to tone chroma sequences 36.8 / 45.1 45.9 / 55.7 54.2 / 68.4 
Note sequence rectification by Eq. (5) addressing the transposition problem. Specifically, we used the method of shifting a query X  X  note sequence upward or downward several semitones together with Eq. (11) to perform the similarity comparison with each of the documents X  sequences. Table 2 shows the experimental results. Here, K = 0 means that no shifting is performed, and shown in Table 1. We can see from Table 2 that the retrieval performance improves as the value of K increases, which indicates that the more the possible changes of key is taken into account, the greater the chance that a query X  X  sequence matches the correct document X  X  sequence. However, increasing the value of K heavily increases the computational cost, because the similarity comparison requires two extra DTW operations whenever the value of K is increased by one. An economic value of K = 1 was thus chosen in our subsequent experiments. can benefit greatly by detecting and excluding the non-singing segments of a query during the DTW similarity comparison. This indicates that the proposed system is capable of handling the inadequate pause, key-shifting, or tempo of a sung query. In summary, our experimental results show that whenever a user sings a query to search for one of the one hundred songs, the probability that the desired song can be found in a Top-10 list is around 0.8, in a Top-3 list is around 0.7, and in a Top-1 list is around 0.6. Although there is much room to further improve, our system shows the feasibility of retrieving polyphonic pop songs in a query-by-singing framework. various concurrent accompaniments are mixed together into a single track, the melody extraction process can be seriously interfered by the accompaniments, leading to the inevitable errors. Drawn from the observations that the varying range of the sung proportion of sung notes are accompanied by the notes several octaves above or below them, we have developed a feasible approach to melody extraction and error correction. Meanwhile, we have also devised a similarity comparison method based on DTW to handle the discrepancy of tempo variation, pause, transposition between queries and documents. handle a wider variety of queries and songs. Specifically, the current system assumes that a query can be either a complete phrase or an incomplete phrase of a song, and a query must start from the beginning of a phrase. It is necessary to further address the case when a query contains multiple phrases of a song or when a query does not start from the beginning of a phrase. In addition, methods for automatic segmentation of songs into phrases are needed in order to automate the whole indexing process. Furthermore, our future work will incorporate some sophisticated methods in the general document-retrieval field, such as relevance feedback, to improve the current system. This work was supported in part by the Nation Science Council, Taiwan, under Grants NSC92-2422-H-001-093 and NSC93-2422-H-001-0004. 
