 This paper presents Yagada , an algorithm to search labelled graphs for anomalies using both structural data and numeric attributes. Yagada is explained using several security-related examples and validated with experiments on a physical Ac-cess Control database. Quantitative analysis shows that in the upper range of anomaly thresholds, Yagada detects twice as many anomalies as the best-performing numeric discretization algorithm. Qualitative evaluation shows that the detected anomalies are meaningful, representing a com-bination of structural irregularities and numerical outliers. Categories and Subject Descriptors: G.2.2 [Graph The-ory]: Graph algorithms General Terms: Algorithms Keywords: graph mining, transactions, anomaly detection
Graph representations are useful in many domains where data has a natural structure which cannot be easily repre-sented as a set of attributes. Graphs are commonly used to represent complex structures such as social networks, bio-logical or ecological processes and computer networks. Al-though graphs are often used to visualise networks for analy-sis by humans [9], machine learning techniques can improve on this by exposing knowledge hidden within the network structure. This paper presents a novel algorithm, Yagada (Yet Another Graph-based Anomaly Detection Algorithm), to detect anomalies in labelled graphs.

Our research field is the security applications of network analysis, which include fraud detection and intrusion detec-tion [5, 9]. In this paper, we analyse data from a physical Access Control System, of the type used to secure doors in buildings such as airports, power stations, hospitals and offices. The current generation of such systems can detect suspicious events  X  X uch as the use of an ID Card which has been reported stolen, or an attempt to physically force a door X  X ut cannot detect suspicious patterns  X  X ransactions which are innocent in themselves but anomalous when con-sidered in context: an airport technician who regularly hangs around in the baggage handling area; or a clerk who is spend-ing an unusually long time on their own in the cash room.
There is little published research on mining physical Ac-cess Control databases. [2] presents four algorithms to de-tect suspicious patterns: temporal patterns ( X  X one overstay X ); repetitive access patterns; displacement patterns; and out-of-sequence patterns. The patterns are detected by com-paring timing values to a threshold. This produces a bi-nary result ( X  X ormal X  or  X  X uspicious X ) but does not indicate the degree of suspiciousness. If the  X  X ormal X  duration of stay varies ( e.g. , at different times of day), a single param-eter cannot represent this. Furthermore, the algorithms are based on domain knowledge. In contrast, our approach rep-resents the transactions as a graph, using a single algorithm to detect both structural anomalies (unusual paths through the building) and numeric anomalies (unusual timing data).
Existing graph-based anomaly detection algorithms [5, 14] define  X  X ormal X  patterns in terms of frequent substructures. Subdue [4] uses a greedy beam search to discover frequent substructures and evaluates them using the Minimum De-scription Length (MDL) principle. After compressing the graph, substructures which are more frequent have a shorter Description Length (DL): by implication, substructures with a high DL are more anomalous. [10] uses this principle as the basis of a heuristic to detect anomalies. Each substructure is evaluated on the basis of how much and how soon it is com-pressed by Subdue. GBAD [5] is a suite of three anomaly detection algorithms, also based on Subdue: GBAD-MDL detects anomalous modifications; GBAD-P (Probability) de-tects anomalous insertions; GBAD-MPS (Maximum Partial Substructure) detects anomalous deletions. The above al-gorithms operate on unweighted graphs with discrete vertex and edge labels. If the graph contains continuous (numeric) attributes, these cannot be incorporated and are therefore ignored. We can improve the accuracy of anomaly detection by incorporating the numeric attribute values.

A few algorithms take numeric edge weights into consid-eration. OddBall [1] operates on egonets but ignores vertex and edge labels. OddBall can therefore detect anomalous vertices with respect to their immediate neighbourhood, but cannot detect anomalous substructures. [8] applies three edge-weighting approaches to gSpan for transaction graph mining. The search space for frequent subgraph discovery is reduced, by assuming that high-weighted edges are more significant than low-weighted edges. with two Denial of Service (DoS) attacks taking place. continuous labels, by including the numeric values into the probability that a particular edge should exist. It is assumed that the values are distributed normally. Yagada can detect anomalies without making such assumptions about the prior distribution of numeric values.

Two recent papers incorporate numeric labels in a more general way: [11] includes a discretization step before at-tempting frequent subgraph matching, using one of several unsupervised discretization methods. Seven methods are proposed to estimate the number of bins k . [13] is a brief outline of an extension to Subdue, which incorporates nu-meric values using frequency histograms. We evaluate these approaches in Sect. 4 and demonstrate that Yagada out-performs these algorithms for anomaly detection.

The rest of this paper is organised as follows: Sect. 2 provides an overview of our approach to detecting anoma-lies in structural and numeric data. Sect. 3 presents the details of Yagada . In Sect. 4, we evaluate Yagada on a real-world Access Control database and compare Yagada  X  X  performance against the algorithms discussed above. Our conclusions are presented in Sect. 5.
The example graphs in Fig. 1 contain both structural and numeric anomalies. Fig. 1a is a single large graph with dis-crete labels and numeric edge weights. Fig. 1b is a database of graph transactions with discrete and numeric attributes. Fig. 1a shows a fragment of traffic in a TCP/IP network. Two types of interchange between the hosts in the network are shown: ping is used to test that another host is alive, and SYN is used to synchronise the order of TCP packets. SYN is a three-part exchange: after the TCP client sends the SYN request, the server acknowledges (SYNACK) and waits for the client acknowledgement (ACK).

The anomalies in the graph represent two common Denial of Service (DoS) attacks. The first is a SYN flooding at-tack, where a client starts many SYN exchanges but fails to send the final ACK, forcing the server to hold all its ports open. The second is a ping flooding attack, where the client consumes the server X  X  bandwidth.

To illustrate how Subdue searches for anomalous substruc-tures, we first define some formal properties of graphs:
Definition 1. A graph G = ( V, E ) consists of a set of ver-tices (or nodes) V and edges E . E  X  { ( v, w ) : v, w  X  V  X  V } . Two vertices v and w are adjacent iff ( v, w )  X  E . If the tuple ( v, w ) is ordered, the graph is said to be directed , otherwise it is undirected .

Definition 2. A substructure S = ( V S , E S ) is said to be isomorphic to G iff V S  X  V, E S  X  E . If  X  ( v, w )  X  E \ E
S : v /  X  V S , w /  X  V S , the substructure is unconnected . An unconnected substructure is also known as a subgraph .
Definition 3. The weight of each edge e  X  E is denoted by W ( e )  X  R . If  X  e : W ( e ) = 1, the graph is unweighted , otherwise it is weighted .

Definition 4. For a graph G containing substructure S , the most frequent substructure is the one that minimises: where DL ( G | S ) is the Description Length of G after com-pressing it with S and DL ( S ) is the Description Length of S [4]. This implies that substructures with a high value of M ( S, G ) are infrequent and therefore anomalous.
To illustrate structural anomaly detection, let G be the graph in Fig. 1a. Using Subdue X  X  greedy beam search and MDL evaluation (Equation 1), we ascertain that the best frequent substructure S 1 is: Note that Subdue operates on unweighted graphs, so W ( e ) wa s not taken into consideration. If we compress G with S we obtain a new graph G 1 : In the next iteration, we discover the best frequent substruc-t ure S 2 = { . . . , (ACK , S 1 ) } . We compress G 1 with S evaluate the DL of the remaining substructures. The sub-structure with the highest DL is highlighted with a box in Fig. 1a. Subdue has discovered the SYN flooding attack, but not the ping flooding attack. Incorporating the numeric edge weights W ( e ) into Subdue does not help, as 3, 5 and 9473 will be considered as distinct discrete values, resulting in substructures which are all equally anomalous.
Our approach solves this problem by making a distinction between numeric values which are  X  X ormal X  and those which are anomalous, and then incorporating that information into the graph. Stated more formally, we would like to alter G such that W ( e ) evaluates to a constant value q 0 for all { e  X  E : e is normal } , but evaluates to a set of values q for { e  X  E : e is anomalous } . We will defer our discussion of how q i is to be calculated until Sect. 3. For the purposes of this discussion, let us assume that we can create G a = G , where W ( e ) = 227 is replaced with q 1 , W ( e ) = 9473 is replaced with q 2 and all other edge weights are replaced with q 0 . Now we can apply Subdue to the weighted graph G , treating the values q i as discrete. We discover that the two most frequent substructures S a 1 and S a 2 are: After compressing G a wi th S a 1 and S a 2 , we obtain G All of the normal substructures have been compressed away an d only the anomalous substructures are left. As the DL of uncompressed substructures is always higher than the DL of compressed substructures, we can use M ( S a i , G a ) (Eq. 1) to measure the anomalousness of each substructure S a i .
We have demonstrated that replacing numeric edge weights with anomaly scores allows both structural and numeric anomalies to be detected in a weighted graph. We now ex-tend this approach to the more general problem of labelled graphs. We formally define a labelled graph as follows:
Definition 5. Let L V be the set of vertex labels and L E be the set of edge labels. If L V  X  L E =  X  , the graph is unlabelled , otherwise it is labelled .
 Definition 6. Let L D be the set of discrete labels and L be the set of numeric labels, such that L = L V  X  L E = L
D  X  L N . Let A D be the set of discrete attribute values and A N  X  R be the set of numeric attribute values, such that A = A D  X  A N . The label-to-value mapping function for vertices is denoted as: The label-to-value mapping function for edges L E is defined in a similar manner. Edge weights can be considered as a special case of edge attributes: W ( e ) = L E ( e,  X  weight  X ).
Fig. 1b shows an example of a labelled graph which con-tains four graph transactions (subgraphs), G ( i )  X  G ( iv ) subgraph represents a cash withdrawal from a bank account. The  X  X ub X  vertex for each transaction is labelled with the account number (discrete). The hub is connected to four vertices labelled with the transaction type (discrete; W is a withdrawal), the physical location where the transaction took place (discrete; A or B ), the time that the transac-tion took place (numeric) and the amount (numeric). This graph contains two anomalous subgraphs. Transaction (ii) took place at an unusual time of day and was for an unusual amount. Transaction (iii) took place in an unusual location.
Let G be the graph in Fig. 1b. We can use Subdue and calculate an anomaly score a s for each subgraph G s as: a s = 1  X  where n is the number of iterations and DL i ( G s ) is the description length of G s after i iterations [10]. Ignoring nu-meric labels A N , we find the best frequent substructures S and S 2 :
We compress G wi th S 1 and S 2 and evaluate a s for all four substructures. As G ( i ) , G ( ii ) and G ( iv ) are completely compressed, they have a lower anomaly score. a ( iii ) is the highest score, so we identify subgraph G ( iii ) as the most anomalous (highlighted with a box in Fig. 1b).

Once again, Subdue has correctly identified the structural anomaly but does not discover the numeric anomalies. Ap-plying our approach, we map all numeric labels l  X  L N to a constant value q 0 if they are  X  X ormal X  and with a number q indicating the degree of anomalousness otherwise.
Thus we create a new graph G b where L V ( G b ( ii ) ,  X  amount  X ) = q 1 , L V ( G b ( ii ) ,  X  time  X ) = q 2 and all other numeric at-tributes take value q 0 . Applying Subdue to G b , we discover the best frequent substructures S b 1 and S b 2 are:
We compress G b wi th S b 1 and S b 2 and evaluate a s as be-fore. Now, only G b ( i ) and G b ( iv ) are completely compressed and have the lowest anomaly scores a ( i ) and a ( iv ) . The val-ues for a ( ii ) and a ( iii ) are higher, correctly identifying both anomalous transactions.
In Sect. 2, we described the spirit of our approach and showed how anomaly scores for numeric attributes can be integrated into the graph structure. Now we consider how to calculate the anomaly scores q i and present our algorithm for graph-based anomaly detection, Yagada .

There are a number of well-known approaches for detect-ing outliers in numeric data sets. The simplest is to have a threshold t : if a numeric value x &gt; t , it is considered anoma-lous [2]. This approach has several weaknesses: it requires some method of determining t ; it cannot detect anomalies in the middle of the data range, only at the extremes; and the result is a binary X  X  X ormal X  or  X  X nomalous X  X  X ith no measure of the degree of anomalousness.

Fig. 2a shows a dataset T = { t 0 , t 1 , . . . , t n } , where each t , 0  X  i  X  n represents the elapsed time for a staff member to walk between a specific pair of physical Access Control sensors in an office building (see Sect. 4 for details of the dataset). It is obvious from visual inspection that there is more than one pattern of behaviour. These distinct patterns can be modelled as a mixture of K Gaussian distributions. The probability of observing t i is: where K is the number of distributions;  X  j,i ,  X  j,i and  X  the weight, mean and standard deviation of the j th Gaus-sian in the mixture at time t i ; and  X  is a Gaussian pdf. The resulting Gaussian Mixture Model (GMM) is shown in Fig. 2b (with K = 3). In devising Yagada , we compared four approaches to computing the anomaly score q i : Statistical: If we model the data using a GMM as shown above, P ( t i is an outlier) = 1  X  P ( t i ). The distribution of outlier scores for T is shown in Fig. 2c. Using this approach, we can set an anomaly threshold 0  X  q a  X  1 and define GMMs can be rapidly computed using Expectation Max-imisation. The main difficulty is selecting appropriate pa-rameters: K and q a must be calculated separately for each numeric label on every vertex and edge.
 Nearest Neighbour-based: The above problem can be avoided by using a non-parametric approach, such as k-distances [12]. The distance is calculated from each t i to its k th nearest neighbour. k is a global smoothing parameter and the selection of k is independent of the statistical dis-tribution of T . Reasonable results are obtained with k = 1. Higher values of k reduce the sensitivity of detection. Fig. 2d shows the k-distances for T with k = 10. Samples in dense parts of the dataset have very small k-distances (close to 0). Outliers have k-distances which are orders of magnitude larger. Therefore we can define The general k-distances algorithm has O ( N 2 ) complexity, but various heuristics have been proposed to reduce this [12]. If T is one-dimensional and has been sorted (as in Fig. 2d), the complexity can be reduced to O ( N ).
 Density-based: Using GMMs, we detect outliers with re-spect to the global distribution of T . With k-distances, we detect outliers with respect to the global density of T . If the density of the clusters is not uniform, both approaches will detect sparse clusters as anomalies. Local Outlier Factors (LOF) [3] calculates an anomaly score based on both the distance from and the relative density of the local neigh-bourhood. Samples belonging to a dense cluster or deep within a sparse cluster have LOF( t i )  X  1. Outliers have LOF values several times larger. Using LOF, we define LOF( T ) is shown in Fig. 2e. Note that LOF has detected anomalies at the low end of the data range, which were not detected using k-distances. While LOF can detect anomalies very accurately, it suffers from O ( N 2 ) complexity. Cluster-based: CBLOF (Cluster-Based Local Outlier Fac-tors) [7] is an efficient algorithm which detects outlierness relative to local clusters. Each cluster in the dataset is clas-sified as  X  X arge X  or  X  X mall X  using two parameters,  X  and  X  . The outlier score CBLOF( t i ) is the distance to the closest large cluster centroid. So samples in small or sparse clusters are more anomalous than those in big or dense clusters. To use CBLOF, we set an anomaly threshold q a and define CBLOF( T ) is shown in Fig. 2f, using k-means clustering with k = 3 ,  X  = 0 . 95 ,  X  = 2. The distribution of anomaly values is similar to Fig. 2c: in fact, CBLOF has similar ad-va ntages and disadvantages to GMM. The complexity is gen-erally low (depending on the chosen clustering algorithm), but the optimal values of k ,  X  and  X  must be determined separately for each numeric label on every vertex and edge.
Yagada must assign a constant value q 0 to normal values of t i and an anomaly score q i to numeric outliers. From our evaluation above, we see that a non-parametric approach is more suitable for unsupervised learning. Although LOF returned the most accurate results, its O ( N 2 ) complexity made it unsuitable for use with large datasets. We therefore settled on K-Nearest Neighbours (k-distances) as the best trade-off between accuracy and efficiency. k-NN also has the property that it scales easily to multi-dimensional data. The choice of outlier detection algorithm is considered further in Sect. 5. Having chosen the outlier detection method, we can now present the complete algorithm for Yagada : Algorithm 1 Ya gada : detect anomalies in labelled graphs Require: G = ( V , E, L V , L E , L V , L E ) Return: Set of subgraphs S = { s 0 , . . . , s n } ; corresponding 1: Partition V into subsets V i : V = V 0  X  V 1  X  . . .  X  V 2: for all i : 0  X  i &lt; N do 3: for all v  X  V i do 4: let L V ( v,  X  X nomaly score X ) = Discretize( v, V i , L 5: end for 6: end for 7: let L V = L V \ L N 8: Partition E into subsets E j : E = E 0  X  E 1  X  . . .  X  E 9: for all j : 0  X  j &lt; M do 10: for all e  X  E j do 11: let L E ( e,  X  X nomaly score X ) = Discretize( e, E i , L 12: end for 13: end for 14: let L E = L E \ L N 15: Execute Subdue( G ) for Limit iterations 16: for all subgraphs s  X  G do 17: Calculate anomaly score a s by Eq. 2 18: end for Algorithm 2 Di scretize : calculate an anomaly score on the numeric attributes of a vertex or edge using k-distances Require: v, V, L , k Return: Outlier score q i 1: let q 0 = 0 2: let F V = L ( V, L N ) 3: let fv v = L ( v, L N ) 4: let fv w = k th nearest neighbour( fv, F V ) 5: let q = distance( fv v , fv w ) 6: if q  X  0 then 7: return q 0 8: else 9: return q 10: end if Algorithm 1 labels vertices with their anomaly score (lines 1 X  7). Vertices are partitioned so the attributes of vertices of the same type are considered together. The vertex type may be defined by its label (Fig. 1a) or by the type of edge it is connected to (Fig. 1b). The procedure is repeated for edges (lines 8 X 14). We then use Subdue to detect frequent sub-structures (line 15) and calculate the anomaly scores for the compressed graph (line 17). Algorithm 2 details the step of calculating the anomaly score from the numeric attributes. F V is a set of feature vectors created from the attributes of the vertices (or edges) under consideration (line 2). The dimensionality of F V is | L V  X  L N | for vertices and | L for edges. fv v , the feature vector for the current vertex, is compared to F V to find the k-distance to its k th nearest neighbour (lines 4 X 5). If the k-distance is normal, we re-turn the constant q 0 ; otherwise, we use the k-distance as the measure of outlierness (lines 6 X 10).

Although a formal analysis of complexity is beyond the scope of this paper, we will comment briefly: we optimised the algorithm by pre-sorting T (average O ( N log N ); worst case O ( N 2 )), which allowed us to calculate k-distances in linear time. We also maintained a cache of k-distances for all attribute-value pairs.
In this section, we compare the performance of Yagada to standard Subdue (ignoring the numeric attributes) and to three of the discretization approaches proposed for fre-quent subgraph discovery [11, 13]. Subdue discovers struc-tural anomalies only and gives a baseline for comparison. The discretization approaches consider numeric attributes by sorting into ten bins of (a) Equal Frequency and (b) Equal Width, and (c) k-means clustering with k = 10. We compared to Yagada using k-distances with k = 10.
The dataset for our experiments is from an Access Con-trol system in a typical office building. There are 256 staff members and 155,100 Access Card transactions, represent-ing all movements within the building over the course of 15 months. We reorganised the data into approximately 40,000 graph transactions (subgraphs), where each transaction rep-resents the path taken by a specific user on a specific day. Each vertex represents a door sensor, and directed edges rep-resent movements. We added a numeric attribute to each edge ( v, w ) for the elapsed time in seconds between present-ing an Access Card at v and presenting it at w .

We applied each of the above approaches and used Eq. 2 to calculate an anomaly score a = [0 , 1] for every subgraph. Fig. 3 shows the results as a cumulative probability distri-bution. Most of the data X  X he  X  X ormal X  part X  X as a . 0 . 33 (not shown): this represents the boundary between anoma-lous and normal transactions. Fig. 3 shows the relative per-formance of the five anomaly detection approaches: Sub-due detects very few anomalies; KMeans performs some-what better; EqualFreq and EqualWidth perform similarly, and Yagada detects the most anomalies. At the upper part of the range ( a &gt; 0 . 8), Yagada detects almost four times as many anomalies as the next best algorithm. In the mid-part of the range ( a &gt; 0 . 65), Yagada detects approximately twice as many anomalies. This difference decreases with de-creasing values of a , until at the boundary between normal and anomalous values ( a  X  0 . 33), the algorithms agree. This behaviour shows that while all algorithms may be discov-ering the same anomalous patterns, Yagada detects them with much higher confidence, and provides much better dis-crimination between slightly anomalous and very anomalous patterns. Table 1 bears this out: of the top 10 anomalies discovered by any of the other four approaches, Yagada de-tects all of them and in most cases assigns an anomaly score Figure 3: Cumulative Frequency Distribution show-in g transactions discovered as anomalous ( a  X  0 . 4 ) Table 1: Comparison of Top 10 Anomalies discov-er ed by EqualFreq, EqualWidth and KMeans which is greater than or similar to that produced by the best-performing of the alternative algorithms.

We also performed a qualitative analysis to confirm that the anomalies discovered by Yagada are meaningful and useful. We examined a random sampling of anomalous pat-terns and checked that they were meaningful, such as failing to swipe at one sensor along a path. The patterns that Ya-gada discovered with highest confidence were those which combined unusual paths with unusual timings.

We conclude that Yagada is able to detect all of the anomalies which can be discovered using any of the alterna-tive approaches, usually with greater confidence. Yagada outperforms the other approaches as it uncovers a larger number of meaningful anomalies at the higher values of a .
In this paper, we presented Yagada , an algorithm for de-tecting anomalies in graphs that consist of both structural data and numeric attributes. The novelty of our method is to replace numeric values in the graph with a constant q 0 if the value is normal, and an anomaly score q i otherwise. When we subsequently search the graph for frequent sub-structures, q 0 will be incorporated into frequent patterns. The values q i are infrequent and therefore substructures which contain q i 6 = q 0 are more anomalous. Our experiments demonstrate that calculating an outlier value for numeric at-tributes outperforms other numeric discretization methods for anomalous substructure discovery.

We evaluated several approaches to numeric anomaly de-tection and concluded that LOF and k-Nearest Neighbour (k-distances) were most suitable. These algorithms return an outlierness score close to some constant (0 or 1) for nor-mal values, and much larger scores for anomalous values. k-distances was chosen over LOF due to its lower computa-tional complexity. If we can make some assumptions about the data, it may be possible to devise a hybrid version of LOF. This would be an application-specific version of Ya-gada , which trades off generality for improved performance and accuracy. As discretization is performed by a separate function, Yagada lends itself to this kind of customization.
Yagada combines the values for all numeric labels on a vertex or edge into a single feature vector. In our exper-iments, we had only one feature, elapsed time. Yagada allows that we extend this into a 2-dimensional vector ( e.g. , elapsed time and time of day) or multi-dimensional. As the number of dimensions is increased, so is the complexity of computing the k-distance between samples. The most effi-cient variant of k-distances is O ( c  X  + N ), i.e. linear com-plexity with regard to the data size but exponential with regard to the number of dimensions  X  . In our future work, we will evaluate whether k-distances is the most appropriate outlierness measure for multi-dimensional numeric data.
The algorithm demonstrated here is for use on static graphs, so it could be used for forensic analysis of graph transaction databases. We plan to extend the approach for on-line de-tection of anomalies using dynamic graphs.
