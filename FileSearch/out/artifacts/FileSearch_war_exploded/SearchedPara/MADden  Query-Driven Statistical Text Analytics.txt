 In many domains, structured data and unstructured text are both important natural resources to fuel data analysis. Sta-tistical text analysis needs to be performed over text data to extract structured information for further query processing. Typically, developers will need to connect multiple tools to build off-line batch processes to perform text analytic tasks. MADden is an integrated system developed for relational database systems such as PostgreSQL and Greenplum for real-time ad hoc query processing over structured and un-structured data. MADden implements four important text analytic functions that we have contributed to the MADlib open source library for textual analytics. In this demon-stration, we will show the capability of the MADden text analytic library using computational journalism as the driv-ing application. We show real-time declarative query pro-cessing over multiple data sources with both structured and text information.
 H.2.4 [ Systems ]: Relational databases, Textual databases; I.2.7 [ Natural Language Processing ]: Text analysis Design, Algorithms Databases, Text Analytics, Query-Driven
For many applications, unstructured text and structured data are both important natural resources to fuel data anal-ysis. For example, a sports journalist covering NFL (Na-tional Football League 1 -32 American football teams with more than 1700 players) games would need a system that can analyze both the structured statistics (e.g., scores, bi-ographic data) of teams and players and the unstructured tweets, blogs, and news about the games.

In such applications, analytics are performed over text data from many sources. Text analysis uses statistical ma-chine learning (SML) methods to extract structured infor-mation, such as part-of-speech tags, entities, relations, senti-ments, and topics from text. The result of the text analysis can be joined with other structured data sources for more advanced analysis. For example, a sports journalist may want to correlate fan sentiment from tweets with statistics describing the player and team performance of the Miami Dolphins 2 .

To answer such queries, a software developer is needed to understand and connect multiple tools, including Lucene for text search, Weka or R for sentiment analysis, and a database to join the structured data with the sentiment re-sults. Using such a complex offline batch process to an-swer a single query makes it difficult to ask ad hoc queries over ever-evolving text data. These queries are essential for applications, such as computational journalism, e-discovery, and political campaign management, where queries are ex-ploratory in nature, and follow-up queries need to be asked based on the result of previous queries.

MADden implements four important text analysis func-tions, namely part-of-speech tagging, entity extraction, clas-sification (e.g., sentiment analysis), and entity resolution. Text analysis functions are developed on PostgreSQL, a single-threaded database, and Greenplum, a massive parallel pro-cessing (MPP) framework. Two SML models and their inference algorithms are adapted: linear-chain Conditional Random Fields (CRF) 3 and Naive Bayes[5]. In-database and parallel SML algorithms are implemented in Greenplum MPP framework. MADden text analytic library is inte-grated into the MADLib open-source project 4 .Thedeclar-ative SQL query interface with MADden text analysis func-tions provides a higher-level abstraction. Such an abstrac-tion shields users from detailed text analytic algorithms and enables users to focus more on the application specific data explorations.

In the demonstration of MADden, we will show the fol-lowing points using journalism on the NFL as our driving example: used to encode arbitrary relationships for statistical process-ing. database analytics http://madlib.net match ( object 1 ,object 2) Entity Resolution sentiment ( text ) Sentiment Analysis entity find ( text ) Detects Named Entities viterbi Part of speech tags using CRF
Table 1: Listing of current MADden functions
In this section we first discuss the general architecture and the basic techniques used in the implementation of the text analytics algorithms in MADden. Then we give an example of POS tagging implementation.
MADden is a four layered system, as can be seen in Fig-ure 1. The user interface is where both naive and advanced users can construct queries over text, structured data, and models. From the user interface, queries are then passed to the DBMS, where both MADLib and MADden libraries sit on top of the query processor to add statistical and text processing functionality. It is important to emphasize that MADlib and MADden perform functions at the same logi-cal layer. To enable text analytics, MADden works alongside statistical functions found in the MADlib library [1]. These queries are processed using PostgreSQL and Greenplum X  X  Parallel DB architecture to further optimize on replicated storage and parallel query optimization. In this section we describe various text analysis algorithms. Many approaches exist for in-database information extrac-tion. We build on our previous work using Conditional Ran-dom Fields (CRFs) for query-time information extraction [4]. We perform the extraction and the inference inside of the database. We rely on information provided in the query to make decisions on the type of algorithm used for extrac-tion. Table 1 describes a list of statistical text analysis tasks.
Entity resolution or co-reference resolution is the problem where given any two mentions of a name, they are clustered only if they refer to the same real world entity. Certain entities may be misrepresented by the presence of different names, misspellings in the text, or aliases. It is important to resolve these entities appropriately to better understand the data. Increasingly informal text, such as blog posts and tweets requires entity resolution. To handle misspellings and nicknames we use trigram indices to perform approxi-mate matches of searches for names as database queries [2]. This method allows us to use indices to perform queries on only the relevant portions of the data set; this way we do no extra processing.

We implemented functions to perform classification tasks such as POS tagging and sentiment analysis. These func-tions work at both a document and sentence level. In senti-ment analysis we classify text by polarities, where positive sentiment refers to the positive nature of the expressed opin-ion., and negative nature for negative sentiment. Much work has already been done in this area for document-level and entity-level sentiment [3, 6]. We can join with other tables and functions within a SQL query, allowing more complex queries to be declaratively realized.
 With a parallel database architecture such as Greenplum, we can parallelize to further optimize queries written with MADden. Each node within the parallel DB could run some query over a subset of the data (data parallel). This includes the statistical methods in MADLib, which were all built to be data parallel. Greenplum has a parallel shared nothing architecture. Data is loaded onto segment servers. When a query is issued, a parallel query optimizer creates a global query plan which is pushed to each of the segment servers. Query-driven algorithms can then be executed in parallel over several data servers.
Core to many natural language processing tasks, POS in-volves the labeling of terms within text based on their func-tion in a particular sentence. We implemented POS tagging in PostgreSQL and Greenplum. Our code is a apart of the MADLib open source system.

MADden uses first-order chain CRF to model the labeling of a sequence of tokens. The factor graph has observed nodes on each sentence token, with latent label variables attached to each token. Factors are functions that connect two nodes or signify the ends of the chain. We generate the features using a function generatemrtbl . This function produces a table rfactor for single state features and a table mfactor for two state features.

Training the CRF model is a one time task that is per-formed outside the DBMS 5 . We use a python script to parse and import the trained model into tables in the DBMS.
Inference is performed over the stored models in order to find the highest assignment of labels in the model. We cal-culate the top 1 most probable label assignment. This is cal-culated using the Viterbi dynamic programming algorithm over the label space.

We use the PL/Python language to manage the work flow of all the calculations. The computationally expensive func-tion viterbi is implemented as database user-defined func-tions in the C language. The feature generation and execu-at http://crf.sourceforge.net tion of inference over a table of sentences is implemented in SQL. When executed in Greenplum the query is performed in parallel.

Implementing POS tagging inside the DBMS allows us to perform inference over a subset of tokens in response to a query instead of performing batch tagging over all tokens. We also get the benefit of using the query engine to paral-lelize our queries without losing the ability to drive the work flow using PL/Python.

Example Q0 performs POS tagging for all the sentences that contain the word  X  X aguar X . This query interface allows the user to perform functions on a subset of the data. The segmenttbl holds a list of tokens and their position for each document ( doc id ). We assume a document is a sequence of tokens.

We describe various data sources from the NFL domain for computational journalism. Finally, we describe the two query-driven user interfaces for exploratory text analysis applications.
Our sample demonstration for MADden involves a vari-ety of NFL based data sources. The data is represented in Table 2 as an abbreviated schema 6 .

The NFLCorpus table holds semistructured data. Textual data from blogs, news articles, and fan tweets, with docu-ment metadata such as timestamp, tags, type, among oth-ers. The tweets were extracted using the Twitter Streaming API 7 with a series of NFL related keywords, and the news articles and blogs were extracted from various sports media websites. These documents vary in size and quality. We have around 25 million tweets from the 2011 NFL season, including plays and recaps from every game in the season.
The statistical data was extracted from the NFL.com player database. Each table contains the player X  X  name , position , number , and a series of stats corresponding to the stat type (Some players show up in multiple tables, others in only one). The Player table holds information about a player in the NFL, including col lege , birthday , height , weight ,as well as years in NFL .The Team table holds some basic in-formation about the 32 NFL teams, including location , con-ference , division ,and stadium . TeamStats_2011 holds the team rankings and stats in a vareity of categories (Offense, Defense, Special Teams, Points, etc.). Extracted_Entities stores the extracted entities found in the NFLCorpus docu-ments.
We plan to give an interactive demonstration of the MAD-den X  X  capabilities. The demonstration will be based around over an API using a foreign data wrapper (fdw). N F LCorpus doc id, type, text, tstmp, tags P layerStats 2011 pid, type specific stats Player pid, f name, l name, college, etc Team Stats 2011 team, points, pass yds, various stats Team team, city, state, stadium Extracted Entities doc id, entity Figure 2: Example MADden UI query template MADden UI, a web interface that allows users to perform analytic tasks on our dataset. MADden UI has two forms of interaction: raw SQL queries, and a Mad Lib 8 style in-terface, with fill-in-the-blank query templates for quick in-teraction as seen in Figure 2.

For the demonstration, in order for users to interpret re-sults more easily, a series of visualizations will be made available in the MADden UI. We will also provide different datasets to combine and query upon using MADden.
We would like to thank Morgan Bauer for his work in building this system. Christan Grant is funded a National Science Foundation Graduate Research Fellowship under Grant No. DGE-0802270. This work was also supported by a gen-erous grant from Greenplum/EMC.
