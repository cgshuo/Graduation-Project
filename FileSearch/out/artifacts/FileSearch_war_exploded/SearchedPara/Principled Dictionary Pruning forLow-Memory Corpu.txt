 Compression of collections, such as text databases, can both reduce space consumption and increase retrieval efficiency, through better caching and better exploitation of the mem-ory hierarchy. A promising technique is relative Lempel-Ziv coding, in which a sample of material from the collection serves as a static dictionary; in previous work, this method demonstrated extremely fast decoding and good compres-sion ratios, while allowing random access to individual items. However, there is a trade-off between dictionary size and compression ratio, motivating the search for a compact, yet similarly effective, dictionary.

In previous work it was observed that, since the dictionary is generated by sampling, some of it (selected substrings) may be discarded with little loss in compression. Unfortu-nately, simple dictionary pruning approaches are ineffective. We develop a formal model of our approach, based on gen-erating an optimal dictionary for a given collection within a memory bound. We generate measures for identification of low-value substrings in the dictionary, and show on a vari-ety of sizes of text collection that halving the dictionary size leads to only marginal loss in compression ratio. This is a dramatic improvement on previous approaches.
 E.4 [ Coding and Information Theory ]: [Data compaction and compression]; H.3.3 [ Information Storage and Re-trieval ]: Information Search and Retrieval X  Search process Corpus compression; string algorithms; retrieval efficiency; optimization
Compression plays a key role in the efficiency of large-scale information retrieval systems such as web search engines [1, 2, 7, 19, 28, 31]. In particular, compression of stored data can enable both reduced storage demands and improved re-trieval speed, through lower data transfer costs and better caching. For web-scale collections, a repository compres-sion scheme must meet several constraints: that documents can be retrieved and decompressed in any order; that mem-ory requirements are reasonable, regardless of the size of the collection; and that new material can be added to the repos-itory. Underlying this, good compression effectiveness must be achieved and decompression speed must be high.

An approach to compression that meets these goals is rel-ative Lempel-Ziv factorization (RLZ) [9, 15, 30]. In RLZ, the collection text is parsed into a contiguous sequence of fragments, where each fragment is sourced from an external static dictionary. RLZ dramatically outperforms repository adaptations of general-purpose compression approaches, for example based on the Lempel-Ziv (LZ) family [29], as it can exploit global properties of the collection.

RLZ uses a portion of the to-be-compressed data as the external dictionary [9, 15]. In the dictionary-generation method proposed by Hoobin et al. [9], fixed-size blocks of data (say 1 KB) are sampled from the repository and then concatenated to form the dictionary. In this way, a dictio-nary of any given size can be generated by simply varying the number and size of samples. With a sufficient number of samples  X  say, a million  X  there is an extremely high likeli-hood that all common strings are present in the dictionary, and (as we confirm in our experiments reported here) excel-lent compression can be achieved, easily outperforming, for example, Huffman-based methods [28].

However, it is also the case that some strings are sampled many times (as would be expected, statistically), meaning that there is extensive redundancy in the dictionary and it is larger than required. Hoobin et al. [10] observed that some parts of the dictionary were rarely, or even never, used. As an illustration, with the first 1 GB of documents in GOV2 [6] as the test collection, and 5% of the collection sampled (with a sample block size of 1 KB) as the test dictionary, we com-press the test collection relative to the test dictionary. The reference frequency of each byte in the dictionary is the num-ber of times that byte is referred to by an LZ factor in the compressed collection. Figure 1 visualizes the reference fre-quency for 32 randomly chosen blocks, and shows that some of the dictionary is indeed little used.
 performance for a given dictionary size. As shown in previ-ous work [9], and as we show here, larger dictionaries give better compression. But to achieve fast decoding and ran-Figure 1: Reference dictionary frequency heat-map for 32 1-KB blocks randomly extracted from a  X  50 MB RLZ dic-tionary. The darker the point, the higher the frequency of use in the factorization. dom access, RLZ also requires the dictionary to be small enough to reside in memory. In a mobile environment, the limits on transmission speed and storage space make it valu-able to keep the dictionary as small as possible, while still maintaining a good compression ratio. Excessive dictionary size may also be a disadvantage when data is compressed and mirrored in two remote servers.

However, optimal dictionary pruning is a difficult prob-lem. In a dictionary D , there are  X ( |D| 2 ) candidate seg-ments to remove. Removal of a single segment has an un-predictable effect on the remainder of the dictionary. A substring of length ` may be identical in the first `  X  1 bytes to some other substring, or may be entirely different to any other material; several low-usage substrings may be iden-tical but for one byte; deletion of a substring creates new substrings by concatenation of the material to the left and right; and so on. We have found in our experiments that na  X   X ve approaches to pruning do not give good results.
In this paper, we formulate dictionary pruning as an op-timization problem. Pruning is similar to known NP-hard compression problems, so we propose a heuristic measure to measure the  X  X alue X  (in terms of contribution to compression performance) of each byte in the dictionary. This measure guides our iterative scheme for pruning the dictionary in a principled way.

As we do not alter the decompression-time costs, there is no impact on the impressive retrieval speed that was orig-inally reported. The results for compression show that we can substantially reduce the dictionary size with only a small fraction of the compression degradation of other methods. For example, on the 426 GB GOV2 collection and a dictio-nary of 1000 MB, the data is reduced to 10.271% of its orig-inal size; with the existing method [10], halving the dictio-nary to 500 MB increased compressed size by 0.276% (in ab-solute terms), whereas with our method it increases by only 0.005%. Halving again to 250 MB gives increases of 3.181% (previous method) and 0.636% (our method), respectively, compared to the 1000 MB dictionary. Our method shows how to halve dictionary size with virtually no impact on compression, and thus has the potential to yield substantial gains in practice for data retrieval, storage, and transmis-sion.
Compression has been extensively employed and studied in the area of text retrieval systems. In this paper, we fo-cus on compression of the text collection [8], a very different problem to inverted index compression [32] and full-text in-dex compression [20].

The general-purpose LZ family [29] can be viewed as an on-line dictionary-based compression scheme. A sliding win-dow captures repetition in the data; these previously ob-served strings act as an internal dynamic dictionary. The compression observed on a single document tends to be poor, since insufficient material is available to build a representa-tive dictionary; to adapt these approaches to repositories, typically one concatenates and compresses blocks of docu-ments together. While this can provide good compression, it means that a whole block must be transmitted and de-compressed to access a single document, greatly reducing the value and applicability of the method.

There are several approaches based on off-line or static dictionaries. Word-based methods using Huffman codes have attracted considerable interest in information retrieval re-search [28], but have shortcomings. In particular, they are limited to cases where the characteristics of the data (for ex-ample, that it consists of ASCII text that can be parsed into words) are known in advance; and compression performance is relatively poor. Another family is based on dictionary in-ference [3, 4, 18, 24]. These methods use the data (or a large part of it) to infer a dictionary represented as a simple hier-archical grammar, and then replace the bytes or words with references to tokens in the dictionary. They have the general strong disadvantage that the data must be held in memory during dictionary inference. An alternative, proposed by Kuruppu et al. [14] for genomic data, is to construct the grammar iteratively in an offline manner, which can yield reasonable compression performance, but is extremely slow and the resulting dictionary tends to be large.

A further class of methods is based on delta compression, which is primarily designed for sharing of versions of ma-terial and requires that the whole repository be used as a dictionary of long strings [11, 13, 21]. While it has super-ficial similarities to our problem, such methods do not by themselves constitute a solution for repository compression; moreover, these methods require an underlying compression method of the kind being explored here.
 a bound on the dictionary size, the compression efficiency of RLZ [9]. This off-line dictionary compression algorithm has been applied to genomes [15, 16, 17] as well as text. In RLZ, a fixed number, say k , of blocks (that is, substrings) of fixed length, say 1 KB, are sampled from the original byte sequences of the collection and concatenated, in their origi-nal order, to form the dictionary (Figure 2). This dictionary remains unchanged during compression and decompression. The data is then factorized with respect to the dictionary using a greedy strategy inspired by the LZ77 factorization algorithm of Chen et al. [5], with the help of a suffix array. Figure 2: Overview of RLZ. (a) The collection is sampled: blocks are concatenated (in collection order) to form a dic-tionary. (b) Each document is factorized relative to the dic-tionary. These factors are then encoded and concatenated (in document order) to constitute the compressed represen-tation of the collection.

Each factor is represented as a pair ( p,l ): p is the position in dictionary D where the factor starts, and l is the length of the factor. To represent a character c that does not appear in the dictionary, ( p,l ) = ( c, 0). These factors are then encoded with standard simple methods such as byte-oriented codes.
A particular attraction of RLZ is the speed of decompres-sion. Hoobin et al. [9] report experiments where in the best case RLZ decompresses at around 18,000 GOV2 [6] docu-ments per second (or around 120 per second, with random access), compared to around 2600 documents per second (or around 70 X 100 per second, with random access) with previ-ous methods. They show that RLZ is consistently superior in decompression speed to previous methods, including re-trieval of uncompressed text.

The work of Hoobin et al. [10] is the only previous ex-amination of the dictionary pruning problem. In their ap-proach, they removed blocks from the original dictionary based on the statistics derived from the initial compression and then reconstructed a smaller dictionary with less redun-dancy. This strategy serves as our major baseline and is de-scribed in detail in Section 3.2. We show that this approach to redundancy elimination does not provide satisfactory re-sults when the pruned amount is large.
In this section, we first present a formulation of the dictio-nary pruning problem. We describe and discuss straightfor-ward redundancy elimination approaches, as baselines. We then explain our method, which like the other approaches is heuristic but is based on observations that arise from formal analysis of the problem.

Notation. We let [ x..y ] be a representation of the sequence { x,x + 1 ,...,y  X  1 } , and A [ x..y ] stand for the array, or substring, A [ x ] ,A [ x + 1] ,...,A [ y  X  1].
Let C represent the collection of files, indeed a concate-nation of the files. Samples are generated from the text, that is, we choose blocks (substrings) from C , and concate-nate these, in C order, to form a dictionary D of (initial) specified size. The aim is to prune the dictionary D , while maximizing the compression effectiveness.

In general, a factorization F of C with respect to a dic-tionary D (however obtained) is a sequence of M factors by requirement (1) below. Let L i be the length of the text collection represented by the first i factors. That is L and L i = L i  X  1 + max( l i , 1), acknowledging that l  X  X ode X  for a single character. The requirement is that L
M = |C| and C [ L i  X  1 ..L i ] = (In our implementations, we do not allow factors to span document boundaries. For simplicity, here, we consider the collection to be one contiguous string.)
The encoding of the factors varies in size due to the prop-erties of variable-byte representations of the lengths { l For the purpose of the optimization, the variation is un-likely to be important because, for each of the great ma-jority of factors, a single byte suffices to encode its length. We therefore make the simplifying, and established [25], as-sumption that all non-zero-length factors are encoded with the same number of bytes, f , and that each zero-length fac-tor consumes one byte. Hence the compression effectiveness (which we call  X  X ost X ) of F , with m non-zero-length factors, is fm + ( M  X  m ), leading to this characterization of the pruning problem: Unless otherwise specified, all pruning algorithms in this pa-per leave the remaining parts of the dictionary in the same order as prior to the pruning.

This problem is similar to the dictionary selection problem of Storer and Szymanski [25]. In their model, however, the dictionary can itself be factorized and the aim is to minimize the total compressed representation of both the dictionary and the collection. This is very similar to one of the com-pression effectiveness measures we describe in Section 4, the archived ratio. Although there are differences X  X ur model has a bound on the uncompressed dictionary size, and is generated from samples X  X e expect that their NP-hardness results can be applied to our formulation.

Given this formulation, we could annotate each byte in the dictionary by the number of times it is referenced, as calculated in Algorithm 1.

Intuitively, the dictionary can be pruned by removing bytes, or strings of bytes, that have low numbers of ref-erences; and indeed this is the method we pursue below. However, we note that in general this may not be optimal. Algorithm 1 Calculating byte reference frequencies. Input: Factorization F , Dictionary size d Output: Vector of byte reference frequencies r [0 ..d ] 1: procedure Freq ( F ,d ) 2: r [0 ..d ]  X  ~ 0 3: for ( p,l )  X  X  do 4: for j  X  [ p..p + l ] do 5: r [ j ]  X  r [ j ] + 1 6: return r For example, if we remove the unreferenced substring abc but also remove b from abcd elsewhere in the dictionary, the removal of abc can imply increased costs. That is, where abc could previously be encoded with a single reference, it might now require three. Given that we are beginning with an ef-fective dictionary, our new statement of the problem implies that we need to reduce the dictionary without destroying useful factors or unnecessarily increasing the number of fac-tors. Heuristics for identifying the material to delete from a dictionary should attempt to minimize fragmentation of factors, as well as to remove material that has a relatively low reference count.

Both our in-principle analysis of the problem and our ex-periments highlight the degree to which deletions from a dictionary have unpredictable effects on the factorization. Removal of a single character from a factor used early in a document can cause the entire remaining factorization to change. In our investigation, we discovered no simple mea-sure that reliably quantified the impact of deletions from a dictionary. Hoobin et al. [10] proposed an elimination method based on block removal, which we show as Algorithm 2 and re-fer to as REM . A frequency counter is maintained for each block in the dictionary. Each time a factor is generated, the frequency counter of the corresponding block in which the factor occurs is incremented. The blocks with the lowest counters are then removed, until the size bound on the dic-tionary is achieved. In this algorithm, Factorize ( C , D ) in line 2 corresponds to the RLZ algorithm [9, 15].
 Algorithm 2 Pruning the dictionary by removing least fre-quently referred-to blocks [10].
 Input: Text Collection C , Original Dictionary D , Block size Output: Pruned dictionary D 0 1: R [0 .. |D| / X  ]  X  ~ 0 . Block reference counts 2: F  X  Factorize ( C , D ) 3: for each ( p,l ) in F do 4: for i  X  [ p/ X .. ( p + l  X  1) / X  + 1] do . blocks involved 5: R [ i ]  X  R [ i ] + 1 6: Based on R [] counts, discard d  X  / X  e least-frequently 7: return D 0  X  remaining blocks
Algorithm REM of Hoobin et al. [10] compresses better, and is faster, than LZMA when the dictionary size is halved, while it outperforms ZLIB even with a ten-fold reduction in dictionary size. However, this strategy is far from optimal. Figure 1 shows that there are striking differences between the reference frequency of various parts of a sample block. As REM treats the block as the unit of elimination, it may remove a highly useful substring from the dictionary, should it happen to reside in an otherwise useless block. Elimina-tion at a finer granularity is needed, prompting the following techniques.
 print technology has been widely used for the tasks of dupli-cate identification and redundancy elimination [12, 13, 21, 22, 23, 26]. Algorithm 3 identifies redundant chunks using the Rabin-Karp rolling hash function [12].
 Algorithm 3 Pruning the dictionary by removing redun-dant fixed-length substrings.
 Input: Original dictionary D , Chunk size c Output: Pruned dictionary D 0 1: Using a rolling hash, calculate hash of each c -gram in D 2: Identify the identical length-c substrings 3: Discard the redundant substrings 4: return D 0  X  remainder of dictionary
If c is large, then Algorithm 3 may not reduce the dictio-nary much, as only repeated strings of length c or greater are pruned. There are more chunk candidates to remove for small c , but then long, useful strings tend to be broken up in unpredictable ways. In preliminary experiments, we found that this chunk-level reduction does lead to slightly better compression than REM. It is, however, considerably less ef-fective than our subsequent techniques; space constraints compel us to omit the details.
 of the dictionary at a chunk level, individual redundant bytes could be removed. Based on the factorization of the collec-tion, the least-frequently used bytes are removed from the dictionary, until the dictionary is sufficiently small. The remaining bytes are kept in original order. Preliminary re-sults show that such filtering does outperform chunk-level elimination (Algorithm 3). However, performance degrades drastically when too much material is removed, because such filtering makes the pruned dictionary too fragmented, which dramatically increases the number of factors. Again, we omit the details of these preliminary experiments.
A major challenge in dictionary pruning is how to choose the segments to remove. In other words, it is essential to estimate as precisely as possible the consequences of remov-ing a particular substring. In principle, one method is to compress the text collection against a version of the dictio-nary from which a particular candidate segment has been ex-cluded. Then, based on the resulting compression ratio, we can tell which segments have the least effect on the compres-sion. However, in practice, re-evaluation of the compression for each candidate segment is out of the question.

Instead, we propose a measure to estimate a segment X  X   X  X ontribution X  to the compression if it is kept in the pruned dictionary. To calculate this measure, only the dictionary it-self is required, not the collection. The segment is factorized against the  X  X runed X  dictionary, that is, against a notional version of the dictionary in which the candidate segment is absent (Figure 3). With the constraint that the factors of a candidate segment should not overlap the segment itself, the standard Factorize routine of RLZ suffices.
 Figure 3: Estimating the value of a candidate segment by factorizing it against the rest of the dictionary.

In Figure 3, the candidate segment s (in a dotted bubble) can be described by three factors that appear in the rest of the dictionary. Thus each reference to this segment of the dictionary will produce three factors when being compressed using the pruned dictionary.
 of removing a segment s from dictionary D . Suppose s is exactly the target of some string t in the factorization of C against D . If the dictionary D  X  s were used instead of D , then t would be factorized the same way that s is factorized against D X  s . To estimate this effect, we calcu-late nfac( s, D ), which is the number of factors that s gener-ates when factorized against D X  s . For the second candidate segment in Figure 3, this value is three. If a notional t  X  X  tar-get were s , then it would now require nfac( s, D ) factors.
More generally, (part of) s may be (part of) the target of some string t . Now, when the collection is factorized against D X  s , that string t might instead be factorized differently. However, it is possible that the part of t whose target is part of s is factorized in the same way as the common subsegment of s is against D  X  s . On average, if t targets only the subsegment s 0 in s , then it would incur nfac( s, D )  X | s new factors when factorized against D  X  s . Counting this from the point of view of s , we consider the average number of times each byte of s is a target, denoted by Fre( s,r ). This by nfac( s, D ), this is a rough estimate of the number of new factors appearing in the factorization of C when s is removed from D . We refer to this measure as FF (frequency &amp; factor), and propose the removal of segments that have the lowest FF values.

Were it included in a dictionary, segment s would consume | s | space. Therefore we also introduce the per-byte measure FFL( s, D ), which is FF( s, D ) / | s | .

Though these two measures are only an approximation, this  X  X actorizing and counting X  strategy provides us with an estimate of the effect of removing a segment. Importantly, it is relatively cheap to calculate.
 CARE algorithm. Our c ontribution-a ware re duction algo-rithm (CARE) may be applied as a one-off procedure, or it-eratively. We start by describing the core of the process, in Algorithm 4. It removes from the dictionary those segments that have low FFL (or FF) values. Importantly, to control the number of candidate segments, we consider only seg-ments of length at least  X  , containing no byte with reference frequency greater than  X  . These candidates, S , are, in prac-tice, found via a greedy heuristic, Candidates ( D ,r, X , X  ), based on the reference-frequency vector r = Freq ( F , |D| ). Given a starting point in the string, a segment of bytes with frequency at most  X  and of maximal length is found. Should this segment X  X  length be at least  X  , it is added to S , other-wise it is ignored. The search for candidates resumes with the next byte whose frequency is at most  X  . By design, the candidate segments do not overlap.
 Algorithm 4 Pruning the dictionary using CARE.
 Input: Text collection C , Original dictionary D , Byte-Output: Pruned dictionary D 0 1: procedure O-Pruning (  X , X ,  X ) . One-off pruning 2: F  X  Factorize ( C , D ) 3: r  X  Freq ( F , |D| ) 4: S  X  Candidates ( D ,r, X , X  ) 5: if P 6: return D 7: for each s in S do 8: Calculate Fre( s,r ) and 9: Execute Factorize ( s, D ) and calculate nfac( s, D ) 10: FFL( s, D )  X  Fre( s,r )  X  nfac( s, D ) / | s | 11: Based on the FFL-values, discard segments in S 12: return D 0  X  remainder of dictionary Algorithm 4 is called O-Pruning as it is a one-off process. However, it can be applied iteratively: at each step the dic-tionary size is reduced by a specified amount. Our results show that iterating this procedure with a  X  X mall X  amount removed from the dictionary each time results in different outcomes to those of pruning the dictionary in a single step. In most of our experiments we use subsets of GOV2 [6]. The collections small , medium , large , and full correspond to the first 1 GB, 10 GB, 100 GB, and all (426 GB) of the documents, respectively. We first study in detail the ef-fectiveness of our method by carrying out experiments with various settings on both the small and medium datasets, then we repeat the experiments on the large and full datasets to demonstrate the scalability of our method. For each exper-iment, an original dictionary is generated by the sampling technology described in [9, 10]; we then prune each dictio-nary to a variety of fixed sizes.

The baselines we use are the plain sampling strategy [9], which we call ORI , and the previous redundancy elimination (or pruning) method REM [10]. Unless indicated otherwise, 1 KB is the default for both the sample block size used dur-ing original RLZ dictionaries generation and reduced unit size in REM. The coding schemes used to compress the position and length of factors are Z ( ZLIB , zlib.net ) and V ( VBYTE [27]), respectively. This combination achieves the fastest compression time and is only marginally worse than the best, but much slower, combination (ZZ) reported in previous work [9] in terms of compression ratio.
 Since the superiority of RLZ over the compression libraries ZLIB and LZMA has already been established [9], we do not examine these latter two further. CARE is only concerned with the construction of the dictionary, so it does not affect the decompression process or the compressed data layout, and thus does not affect the retrieval time of RLZ. Therefore our evaluations do not include retrieval speed.

As shown by Hoobin et al. [9], we can achieve a compres-sion ratio of less than 10% for a large collection (GOV2, 426 GB), where compression ratio is the final compressed size as a percentage of the original size. This is achieved with a dictionary ratio of only 0.5% or less (the ratio of dictionary size to the collection size). However, to maintain the same compression ratio as an unpruned dictionary, for a small col-lection (say, 4 GB), we have to increase the dictionary ratio to 20% X 30%. Since the dictionary ratio and the compression ratio are both relative to the uncompressed data size, we can introduce a new performance measure (AR, for archived ra-tio ) that covers them both. The AR is the compressed size of the dictionary and collection together, as a percentage of the uncompressed collection size. We use a standard tool ( 7zip ) to compress the dictionary, to represent the size it would occupy during archiving. In other words, AR is the size required for storage or transmission of a repository that has been compressed with RLZ.
Our experiments involve parameters that trade against each other in complex ways. For example, as can be observed in these experiments, the dictionary size is not a function of the collection size, for a given compression ratio. As an-other example, a given dictionary size can be achieved by sampling; or can be achieved by pruning from a larger dic-tionary. The pruning can be achieved directly, or iteratively. We thus need to report IDS , the initial dictionary size; the PDS , or pruned dictionary size; the ICR , or initial compres-sion ratio; and the step , which is the amount the dictionary size is reduced in each pruning iteration. Ultimately, we wish to discover the best compression ratio available for a given dictionary size which, as we show, is given by our new method CARE.

For a fair comparison, we first generate equal-sized dic-tionaries with each method. That is, given an IDS and a sequences of pruned sizes, we compare directly sampling (ORI) to achieve the pruned size to sampling to the original size then applying REM and CARE to achieve the pruned size. In this first experiment, the CARE algorithm uses FFL as the measure for the removal of candidate segments from the dictionary, and the pruning strategy is one-off (non-iterative). Only the segments with maximum byte frequency at most  X  and length at least  X  may be removed. Sensitivity to these parameters is discussed later.

Figure 4(a) shows that, with the small dataset, CARE is consistently better than ORI in terms of compression ra-tio for same-sized dictionaries. The ORI line represents the result of using a range of initial dictionary sizes; the REM and CARE lines are the result of using a specified initial dictionary size (250 MB) and then pruning. As the pruning continues, CARE outperforms REM when the dictionaries are reduced by a a third or more. Meanwhile REM gets dramatically worse, and after a two-fold reduction in dictio-Compression Ratio (%) Compression Ratio (%) Figure 4: Compression ratio achieved by different construc-tion strategies. The initial dictionary size is 250 MB and 2000 MB for the small and medium datasets, respectively. Pruning in CARE is applied one-off; FFL is used as the measure. nary size becomes poorer than the commensurate directly-sampled dictionary. The same patterns are also observed in Figure 4(b) for the medium dataset.

We also investigate the impact of pruning on AR for each method. As depicted in Figures 5(a) and 5(b), in contrast to the compression ratio, AR does not change monotoni-cally as the dictionary is pruned. At first AR slightly de-creases, because the saving in dictionary size is greater than the loss in compression ratio. For example, when the dictio-nary for medium dataset is pruned from 2000 MB to 1500 MB (310 MB to 268 MB, in terms of compressed size), we save 0.4% in compressed dictionary size while losing around 0.1% in compression ratio. However, with the dictionary size fur-ther reduced, AR increases instead. The 7zip utility com-presses the dictionary so well (around 15%-16% of original) that the gap between the sizes of the compressed dictio-naries is dramatically narrowed. Therefore, the saving in dictionary size is eventually overwhelmed by the increase in compression ratio.

Though pruning the dictionary will eventually lead to a poorer archived ratio and compression ratio than was avail-Figure 5: Archived ratio achieved by different construc-tion strategies. The initial dictionary size is 250 MB and 2000 MB for the small and medium datasets, respectively. Pruning in CARE is applied one-off; FFL is used as the measure. able with the original dictionary size, for a given (uncom-pressed) dictionary size, it can give a much better AR. For example, on medium , compare ORI at 1000 MB (AR = 12.4%) to CARE at 1000 MB (AR = 10.9%), having pruned a 2000 MB dictionary to 1000 MB. That is, it is better to build a large dictionary and prune than to directly create a small dictionary.
 We next compare the two CARE measures proposed in Section 3.3, with regard to evaluation of candidate segments. Table 1 shows that FFL is superior to FF in every setting. Note that the data in the ORI, REM, and FFL (CARE) columns is identical to that illustrated in Figure 4. The results here also reveal that each setting of CARE is consis-tently better than REM when there is significant pruning. Table 1 shows that the loss of compression ratio caused by CARE pruning (compare to the IDS) is less than 1% even when the dictionary size is reduced by half.
 only one-off pruning. We investigate the choice of the two Table 1: One-off pruning using CARE with different mea-sures (FF &amp; FFL). The values of  X  and  X  are the same as those in Figure 4.
 small (1GB) 150 13.09 12.95 12.53 11.39 (IDS=250MB) 100 14.65 16.81 14.12 12.74 (ICR=10.68%) 50 17.09 22.22 16.43 15.28 medium (10GB) 1500 11.07 10.12 10.46 10.17 (IDS=2000MB) 1000 12.39 12.65 11.79 10.86 (ICR=10.03%) 750 13.23 15.47 12.67 11.75 Table 2: Iterative pruning using CARE with different com-binations of (  X , X  ) for the FF &amp; FFL measures on small dataset (1 GB), IDS=250 MB, ICR=10.68%.
 PDS Compression Ratio (%) (MB) FF FFL FF FFL FF FFL 200 10.78 11.57 11.10 11.13 10.83 11.26 10.83 150 12.95 12.71 11.79 12.13 11.32 12.40 11.33 100 16.81 14.23 12.98 13.41 12.48 13.85 12.51 arguments X  X he upper limit of frequency  X  and the lower limit of length  X   X  X n the context of iterative pruning. Ta-ble 2 presents the results of three different combinations of these two arguments on the small dataset. The differences in compression ratio among the combinations are small. Op-timizing the parameter selection is a research question we leave for the future, but these results suggest that, no mat-ter which combinations we use, CARE is consistently better than REM. The results on the medium dataset are much the same. In the following evaluations, the combinations (  X , X  ) = (10 , 20) and (10 , 50), are set as the default values in experiments for the small and medium dataset, respectively. These were chosen based on initial experiments.

As described in Section 3.3, there are two ways to progress the pruning. One is to prune the dictionary to a fixed vol-ume in an one-off manner, while the other is to iteratively reduce the dictionary multiple times by a fixed step. Fig-ures 6(a) and 6(b) show that the iterative strategy is consis-tently better than the one-off method. And as the reduction continues, the advantage of the iterative strategy increases. The results also demonstrate that FFL remains superior to FF as a measurement of the value of segments. The iter-ative FF CARE algorithm (I-FF) in Figure 6(b) runs out of candidates after being pruned to 750 MB, so there is no corresponding result for a dictionary of 500 MB.
 the step size  X  on the effectiveness of different strategies. The results in Table 3 show that we can achieve better com-pression by choosing smaller step sizes, though dictionary construction is slower. However, the improvement achieved Figure 6: Compression ratio of fixed-size dictionaries gener-ated by CARE with one-off (O) and iterative (I) strategies. FFL is used as the measure. by fine granularity is very small. Thus, we continue to use the previous settings in our remaining experiments. For the purpose of this table, we ignored settings where the gap be-tween the IDS and PDS was not a multiple of the step size.
Table 4 shows the results of CARE dictionary reduction with different initial dictionary sizes (IDS). The results sug-gest that, by starting from a larger IDS, we end up with a better dictionary for each specified final size. The reason is that the larger dictionary represents the collection better, while the multiple rounds of pruning reduce the dictionary to the most valuable substrings. The results on the small dataset (not shown) support this observation.

We also present results for ORI and REM on the medium dataset in Table 5. By comparing Tables 4 and 5, we ob-serve that, for REM, a smaller IDS leads to a better pruned dictionary. When REM starts with a larger dictionary, more blocks must be removed, which makes the pruned dictionary less representative of the collection. More significantly, even the worst case in CARE is better than the best case in REM (except when the pruned volume is small). The results on the small dataset (not shown) are similar.
 Table 3: Iterative pruning using CARE with various pro-gressive pruning differences.
 medium (10GB) (MB) 500 MB 250 MB 100 MB Table 4: Iterative pruning using CARE with different IDSs on medium dataset (10 GB), step  X  = 250 MB.

PDS Compression Ratio (%) IDS=2000MB IDS=1500MB IDS=1000MB (MB) ICR=10.03% ICR=11.07% ICR=12.39% 1250 10.36 11.02  X  1000 10.76 11.26  X  Table 5: Performance of dictionary construction methods (ORI and REM) on medium dataset (10 GB).
 Constructed Compression Ratio (%) Dictionary ORI REM with IDS (MB)
Size (MB) IDS=2000 IDS=1500 IDS=1000
The impact of the block size on the effectiveness of RLZ is not studied in previous work [9, 10]. Table 6 shows that the sample size has little effect on CARE, and only very limited impact on the other methods.
 on both the small and medium datasets, we repeat the ex-periments on the large dataset as well as the full GOV2 collection. Table 7 demonstrates that CARE significantly outperforms REM as expected. Halving the dictionary size using CARE causes less than 0.3% loss in compression ratio, while the compression ratio is 1% better than that of the commensurate size of the dictionary constructed by ORI. When reducing the dictionary to only a quarter of its orig-inal size, compression loss is only around 1.4%, while REM suffers around 6.5% X 7%.

In Table 7 we also report the compressibility of different pruned dictionaries ( 7zip is used here). The results show that the dictionaries are less compressible after pruning, which means they now contain less redundancy. For com-mensurable size of pruned dictionaries, the number of factors generated by REM is far larger than that by CARE, explain-ing why CARE outperforms REM in compression ratio. For example, with a 250 MB dictionary (IDS=1000 MB), REM produces 5.64 billion factors, CARE 3.55 billion factors. CARE uses FFL with  X  = 10,  X  = 20, iteratively .
 Table 7: Performance of dictionary construction methods on large dataset (100 GB). CARE uses FFL,  X  = 100,  X  = 20, iteratively .

PDS IDS = 2000 MB IDS = 1000 MB ICR = 11.71% ICR = 12.96% (MB) ORI REM CARE ORI REM CARE 1500 12.23 11.76 11.73  X   X   X  1000 12.98 13.43 11.98  X   X   X  1500 215 256 256  X   X   X  1000 146 206 210  X   X   X 
Table 8 shows that all the conclusions drawn above also hold on the whole GOV2 corpus (426 GB). For example, a CARE-based dictionary of 500 MB can give compression as good as that originally available with 1000 MB. Table 9 shows results for the Wikipedia dataset, which is very differ-ent from GOV2. The Wikipedia data is highly structured, with many common elements repeated from page to page, whereas GOV2 contains highly diverse material from every branch of the US government. However, the compression results are very similar, as is the relative behavior of the different algorithms.
 In both tables, reducing the dictionary by a quarter with REM or CARE has almost no impact on compression ratio; the tiny changes (improvements in a couple of cases!) are due to the effect of different but nearly equivalent factors being chosen. For greater reductions, however, the CARE method again exhibits much better performance, with very slow degradation in compression ratio compared to the al-ternatives. These results show that our CARE method has proved much the most effective way of reducing dictionary size, and also show the benefit of starting with a large dic-tionary which is then progressively reduced.
Relative Lempel-Ziv factorization is an efficient compres-sion algorithm that provides both good compression ratio Table 8: Performance of dictionary construction methods on GOV2 (426 GB). CARE uses FFL,  X  = 200,  X  = 20, iteratively .

PDS IDS = 2000 MB IDS = 1000 MB ICR = 9.419%  X  ICR = 10.271% (MB) ORI REM CARE ORI REM CARE 1500 9.789 9.414 9.425  X   X   X  1000 10.271 9.588 9.437  X   X   X  750  X   X   X  10.645 10.262 10.272 500 11.083 12.371 10.036 11.083 10.547 10.276 250  X   X   X  11.987 13.452 10.907  X  Table 9: Performance of different dictionary construction methods on Wikipedia dataset (251 GB). CARE uses FFL  X  = 200,  X  = 20, iteratively .

PDS IDS = 2000 MB IDS = 1000 MB ICR = 8.688% ICR = 9.900% (MB) ORI REM CARE ORI REM CARE 1500 9.202 8.708 8.738  X   X   X  1000 9.900 9.342 8.898  X   X   X  750  X   X   X  10.383 9.909 9.901 500 11.096 11.369 9.787 11.096 10.517 10.052 250  X   X   X  12.226 12.561 11.066 and fast retrieval. Though it only requires a relatively small dictionary, compared with the size of the collection to be compressed, the dictionary size is still an essential concern as it must be maintained in memory.

We first formulate the dictionary pruning problem as an optimization problem and then propose heuristic strategies for pruning the dictionary while maintaining compression ef-fectiveness. Our main heuristic can be calculated efficiently by factoring segments of the dictionary against the dictio-nary itself. By identifying and eliminating low-value seg-ments, we can markedly reduce the volume of the dictionary without significant loss of compression performance.
RLZ may be deployed on mobile devices, where a fixed dictionary can be used to reduce download requirements. In such a context, dictionary size must be kept small, and the value of these kinds of savings is accentuated.

In our view, we should next refine our understanding of the dictionary optimization problem. The consequent prun-ing algorithms could start with much larger dictionaries, which are then progressively reduced, and we hypothesize that compression will be even more effective. However, the existing results are already significantly superior to any cur-rent alternative, and provide a practical method for large-scale corpus compression.
We thank Christopher Hoobin for providing the source code of RLZ. This work is partially supported by The Aus-tralian Research Council, NSF of China (61373018, 11301288), Program for New Century Excellent Talents in University (NCET-13-0301) and Fundamental Research Funds for the Central Universities(65141021). Jiancong would also like to thank the China Scholarship Council (CSC) for the State Scholarship Fund. [1] R. A. Baeza-Yates and B. A. Ribeiro-Neto. Modern [2] S. B  X  uttcher, C. L. A. Clarke, and G. V. Cormack. [3] A. Cannane and H. E. Williams. General-purpose [4] A. Cannane and H. E. Williams. A general-purpose [5] G. Chen, S. J. Puglisi, and W. F. Smyth. Lempel-Ziv [6] C. Clarke, N. Craswell, and I. Soboroff. Overview of [7] W. B. Croft, D. Metzler, and T. Strohman. Search [8] P. Ferragina and G. Manzini. On compressing the [9] C. Hoobin, S. J. Puglisi, and J. Zobel. Relative [10] C. Hoobin, S. J. Puglisi, and J. Zobel. Sample [11] J. J. Hunt, K.-P. Vo, and W. F. Tichy. Delta [12] R. M. Karp and M. O. Rabin. Efficient randomized [13] P. Kulkarni, F. Douglis, J. D. LaVoie, and J. M. [14] S. Kuruppu, B. Beresford-Smith, T. C. Conway, and [15] S. Kuruppu, S. J. Puglisi, and J. Zobel. Relative [16] S. Kuruppu, S. J. Puglisi, and J. Zobel. Optimized [17] S. Kuruppu, S. J. Puglisi, and J. Zobel. Reference [18] N. J. Larsson and A. Moffat. Offline dictionary-based [19] C. D. Manning, P. Raghavan, and H. Sch  X  utze. [20] G. Navarro and V. M  X  akinen. Compressed full-text [21] Z. Ouyang, N. D. Memon, T. Suel, and [22] A. Peel, A. Wirth, and J. Zobel. Collection-based [23] P. Shilane, M. Huang, G. Wallace, and W. Hsu. [24] P. Skibinski, S. Grabowski, and S. Deorowicz. [25] J. A. Storer and T. G. Szymanski. Data compression [26] T. Suel, P. Noel, and D. Trendafilov. Improved file [27] H. E. Williams and J. Zobel. Compressing integers for [28] I. H. Witten, A. Moffat, and T. C. Bell. Managing [29] J. Ziv and A. Lempel. A universal algorithm for [30] J. Ziv and N. Merhav. A measure of relative entropy [31] N. Ziviani, E. Silva de Moura, G. Navarro, and R. A. [32] J. Zobel and A. Moffat. Inverted files for text search
