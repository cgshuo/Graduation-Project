 Recent computer, internet and hardware advances produce massive data which are accumulated rapidly. Applications include genomics, remote sensing, network security and web analysis. Undoubtedly, knowledge acquisition and discovery from such data become an important issue. One common technique to analyze data is clustering which aims at grouping entities with similar characteristics together so that main trends or unusual patterns may be discovered.
 special interest for their connections to statistical models which are very useful in many applications. Density-based clustering has the advantages for (i) allowing arbitrary shape of cluster and (ii) not requiring the number of clusters as input, which is usually difficult to determine. Examples of density-based algorithms can be found in [1, 2, 3].
 mon approach is so-called bump-hunting: first find the density peaks or  X  X ot spots X  and then expand the cluster boundaries outward until they meet some-where, presumably in the valley regions (local minimums) of density contours. The CLIQUE algorithm [3] adopted this methodology. (A2) Another direction is to start from valley regions and gradually work uphill to connect data points in low-density regions to clusters defined by density peaks. This approach has been used in Valley Seeking [4] and DENCLUE [2]. (A3) A recent approach, DBSCAN [1], is to compute reachability from some seed data and then connect those  X  X eachable X  points to their corresponding seed. Here, a point p is reachable from a point q with respect to MinPts and Eps and there is a chain of points p 1 = q, p 2 ,...,p n = p such that, for each i ,the Eps -neighborhood of p i contains at least MinPts points and contains p i +1 .
 the peak and valley regions are well-defined and easy to detect. When clusters touch each other, which is often the case in real situations, both the cluster cen-ters and cluster boundaries (as the peaks and valleys of the density distribution) become fuzzy and difficult to determine. In higher dimension, the boundaries become wiggly and over-fitting often occurs.
 new ingredients adopted to overcome problems that many density-based algo-rithms share. The major steps of our method are as follows: (i) obtain a density function by Kernel Density Estimation (KDE); (ii) identify peak regions of the density function using a surface evolution equation implemented by the Level Set Methods (LSM); (iii) construct a distance-based function called Cluster Intensity Function (CIF) based on which valley seeking is applied. In the followings, we describe each of the above three notions. An efficient graph-based implementa-tion of the valley seeking algorithm can be found in [4].
 Kernel Density Estimation (KDE). In density-based approaches, a gen-eral philosophy is that clusters are high density regions separated by low den-sity regions. We particularly consider the use of KDE, a non-parametric tech-nique to estimate the underlying probability density from samples. More pre-cisely, given a set of data { x i } N i =1  X  R p , the KDE is defined to be f ( x ):= 1 / ( Nh p N ) N i =1 K (( x  X  x i ) /h N ) where K is a positive kernel and h N is a scale parameter. Clusters may then be obtained according to the partition defined by the valleys of f .
 tifying high density regions is independent of the shape of the regions. Smoothing effects of kernels make density estimations robust to noise. Kernels are localized in space so that outliers do not affect the majority of the data. The number of clusters is automatically determined from the estimated density function, but one needs to adjust the scale parameter h N to obtain a good estimate. fundamental drawbacks which deteriorate the quality of the resulting clusterings. KDEs are very often oscillatory (uneven) since they are constructed by adding many kernels together. Such oscillatory nature may lead to the problem of over-fitting, for instance, when clusters touch each other, a smooth cluster boundary between the clusters are usually preferred than an oscillatory one. Last but not least, valleys and peaks of KDEs are often very vague especially when clusters touch each other.
 ponents and the KDE f . We observe that the valleys and peaks correspond to the two smaller large clusters of the KDE are very vague or may even not exist. Thus, the performance of KDE-based bump-hunting and/or valley seeking could be poor when the clusters are overlapped.
 Level Set Methods (LSM). We recognize that the key issue in density-based approach is how to advance the boundary either from peak regions outward to-wards valley regions, or the other way around. In this paper, we employ LSM, which are effective tools for computing boundaries in motion, to resolve the boundary advancing problem. LSM have well-established mathematical founda-tions and have been successfully applied to solve a variety of problems in image processing, computer vision, computational fluid dynamics and optimal design. LSM use implicit functions to represent complicated boundaries conveniently. While implicit representation of static surfaces has been widely used in com-puter graphics, LSM move one step further allowing the surfaces to dynamically evolve in an elegant and highly customizable way, see [5] for details. conveniently and smoothness can be easily controlled by a parameter that characterizes surface tension; (ii) merging and splitting of boundaries can be eas-ily done in a systematical way. Property (ii) is very important in data clustering as clusters can be merged or split in an automatic fashion. Furthermore, the advancing of boundaries is achieved naturally within the framework of partial differential equation (PDE) which governs the dynamics of the boundaries. Cluster Intensity Functions (CIF). We may use LSM strictly as an effective mechanism for advancing boundaries. For example, in the above approach (A1), once the density peaks are detected, we may advance cluster boundaries towards low-density regions using LSM. However, it turns out that utilizing LSM we can further develop a new and useful concept of cluster intensity function . A suitably modified version of LSM becomes an effective mechanism to formulate CIFs in a dynamic fashion. Therefore our approach goes beyond the approaches (A1) X (A3) described earlier.
 ters are well-separated, CIFs become similar to density functions. But as clusters touch each other, CIFs still clearly describe the cluster structure whereas density functions and hence cluster structure become blurred. In this sense, CIFs are a better representation of clusters than density functions.
 Although CIFs are also built on the top of KDEs, they are cluster-oriented so that only information contained in KDEs that is useful for clustering is kept while other irrelevant information is filtered out. We have shown that such a filtering process is very important in clustering especially when the clusters touch each other. On the other hand, it is well-known that when the clusters are well-separated, then valley seeking on KDEs results in very good clusterings. Since the valleys of CIFs and KDEs are very similar, if not identical, when the clusters are well-separated, clustering based on CIFs is as good as that based on KDEs. However, advantages of CIFs over KDEs become very significant when the clusters touch each other.
 signed by applying the valley seeking algorithm [4] but with the density function replaced the distance-based CIF. In this section, we describe our methodology to construct clusters using LSM. We start by introducing some terminologies. A cluster core contour (CCC) is a closed surface surrounding the core part/density peak of a cluster at which density is relatively high. A cluster boundary refers to the interface between two clusters, i.e., a surface separating two clusters. A CCC is usually located near a density peak while a cluster boundary is located at the valley regions of a density distribution. Here, a point x is said to belong to a valley region of f if there exists a direction along which f is a local minimum. The gradient and the Laplacian of a function g are denoted by  X  g and  X  X  respectively. in details in the next subsections: (i) initialize CCCs to surround high density regions; (ii) advance the CCCs using LSM to find density peaks; (iii) apply valley seeking algorithm on the CIF constructed from the final CCCs to obtain clusters. 2.1 Initialization of Cluster Core Contours (CCC) We now describe how to construct an initial cluster core contours  X  0 effectively. The basic idea is to locate the contours at which f has a relatively large (norm of) gradient. In this way, regions inside  X  0 would contain most of the data points  X  we refer these regions as cluster regions . Similarly, regions outside  X  0 would contain no data point at all and we refer them as non-cluster regions . Such an interface  X  0 is constructed as follows.
 Definition 1. An initial set of CCCs  X  0 is the set of zero crossings of  X  X  , the Laplacian of f . Here, a point x isazerocrossingif  X  X  ( x )=0 and within any arbitrarily small neighborhood of x , there exist x + and x  X  such that  X  X  ( x + ) &gt; 0 and  X  X  ( x  X  ) &lt; 0 .
 The idea of using zero crossings of  X  X  is that it outlines the shape of datasets very well and that for many commonly used kernels (e.g. Gaussian and cubic B-spline) the sign of  X  X  ( x ) indicates whether x is inside or outside  X  0 . datasets are several folds: (a) the solution is a set of surfaces at which  X  f is relatively large; (b) the resulting  X  0 is a set of closed surfaces; (c)  X  0 well cap-tures the shape of clusters; (d) the Laplacian operator is an isotropic operator which does not bias towards certain directions; (e) the equation is simple and easy to solve; (f) it coincides with the definition of edge in the case of image pro-cessing. In fact, a particular application of zero crossings of Laplacian in image processing is to detect edges to outline objects;(g) the sign of  X  X  ( x ) indicate whether x is inside (negative) or outside (positive) of a cluster region. Fig. 1(a). We observe that the CCCs capture the shape of the dataset very well. 2.2 Advancing Cluster Core Contours Next, we discuss how to advance the initial CCCs to obtain peak regions through hill climbing in a smooth way. We found that this is a key issue in density-based approaches and is also how ideas from LSM come into play. More precisely, we employ PDE techniques to advance contours in an elegant way.
 parameterize such a family of CCCs by a time variable t , i.e., the i -th CCC at time t is denoted by  X  i ( t ). Moreover,  X  (0)  X   X  0 .
  X  =  X  ( x ,t ), i.e.,  X  ( t )= { x :  X  ( x ,t )=0 } . The value of  X  at non-zero level sets can be arbitrary, but a common practice is to choose  X  to be the signed distance function  X   X  ( t ) ( x ) for numerical accuracy reasons [5]. In general, the signed distance function with respect to a set of surfaces  X  is defined by To evolve  X  ( t ) (where  X  (0) is the initial data) with speed  X  =  X  ( x ,t ), the equation is given by  X  X / X  X  =  X   X   X  which is known as the level set equation [5]. Our PDE also takes this form.
  X  ( t )at x is given by  X  ( x ,t )=  X  X  (  X   X  ( x ,t ) /  X   X  ( x ,t ) ). is chosen to be  X   X  (0) ( x )), the time dependent PDE that we employ for hill climbing on density functions is given by This equation is solved independently for each cluster region defined according to  X  ( t ). During evolution, each contour and hence each cluster region may split or merge. Evolution is stopped when no further splitting occurs.
 density peaks. Moreover, the factor also adjusts the speed of each point on the CCCs in such a way that the speed is lower if  X  f is larger. Thus the CCCs stay in steep regions of f where peak regions are defined better. In the limiting case where f has a sharp jump (  X  f  X  X  X  ), the CCCs actually stop moving at the jump. We remark that in traditional steepest descent methods for solving minimization problems, the speed (step size) is usually higher if  X  f if larger, which is opposite to what we do. This is because our goal is to locate steep regions of f rather than local minimums. without exerting surface tension, the CCCs could become wiggly which may lead to the common problem of over-fitting of KDEs. Therefore, we employ the term  X  to resolve such a problem. In fact, if  X  is kept to be a signed distance function for all t , i.e.,  X   X   X  1, then  X  =  X  X  so that  X  is smoothed out by Gaussian filtering. In the variational point of view, the curvature term exactly corresponds to minimization of the length (surface area in general) of the CCCs. will be adjusted dynamically during the course of evolution. At the beginning of evolution of each  X  i (0), we set  X  = 0 in order to prevent smoothing out of important features. After a CCC is split into pieces, tension is added and is gradually decreased to 0. In this way, spurious oscillations can be removed without destroying other useful features. Such a mechanism is similar to cooling in simulated annealing.
 locate peak regions; (ii) adjusts the speed according to the slope of the KDE; (iii) removes small oscillations of the CCCs by adding tension so that hill climb-ing is more robust to the unevenness of the KDE (c.f. Examples 1 and 2 in  X  3). In addition to these, the use of LSM allows the CCCs to be split and merged easily.
 by Eq. (2). The two CCCs correspond to outliers are freezed quickly. We observe that the contours are attracted to density peaks. When a contour is split into several contours, the pieces are not very smooth near the splitting points. Since tension is added in such cases, the contours are straighten out quickly. 2.3 Cluster Intensity Functions In non-parametric modelling, one may obtain clusters by employing valley seek-ing on KDEs. However, as mentioned above, such methods perform well only when the clusters are well-separated and of approximately the same density in which case peaks and valleys of the KDE are clearly defined. On the other hand, even though we use the density peaks identified by our PDE Eq. (2) as a starting point, if we expand the CCCs outward according to the KDE, we still have to face the problems of the KDE; we may still get stuck in local optimum due to its oscillatory nature.
 better representation of clusters than that by KDEs. Due to the advantages of CIFs, we propose to perform valley seeking on CIFs to construct clusters, rather than on KDEs. Here, CIFs are constructed based on the final CCCs obtained by solving the PDE Eq. (2).
 while information irrelevant to clustering contained in KDEs is filtered out. Moreover, peaks and valleys of CIFs stand out clearly which is not the case for KDEs. The principle behind is that clustering should not be done solely based on density, X  rather, it is better done based on density and distance. For example, it is well-known that the density-based algorithm DBSCAN [1] cannot separate clusters that are closed together even though their densities are different. (which are constructed based on density). Thus, CIFs combine both density and distance information about the dataset. This is a form of regularization to avoid over-specification of density peaks.
 (zero crossings of  X  X  or its refined version), the CIF  X  with respect to  X  is defined to be the signed distance function (1),  X  =  X   X  .
 sign being positive if x lies inside  X  and negative if x lies outside  X  . Roughly speaking, a large positive (respectively negative) value indicates that the point is deep inside (respectively outside)  X  while a small absolute value indicates that the point lies close to the interface  X  .
 peaks correspond to the three large clusters can be clearly seen which shows that our PDE is able to find cluster cores effectively. Based on the CIF, valley seeking (c.f.  X  1) can be easily done in a very robust way. In Fig. 2(d), we show the valleys of the CIF juxtaposed with the dataset and the final CCCs. means algorithm. Thus, our method generalizes the k -means algorithm in the sense that a  X  X luster center X  may be of arbitrary shape instead of just a point. just the singularities of the level set function (i.e. CIF) having negative values. On the other hand, the singularities of the level set function having positive values are the peaks or ridges of the CIF (also known as skeleton). In addition to the examples shown in Figs. 1 X 2, we give more examples to fur-ther illustrate the usefulness of the concepts introduced. Comparisons with val-ley seeking [4] and DBSCAN [1] algorithms (c.f.  X  1) are given in Examples 1 and 2. Clustering results of two real datasets are also presented. For visualiza-tion of CIFs which is one dimension higher than the datasets, two dimensional datasets are used while the theories presented above apply to any number of dimensions.
 by the authors in [1].
 Example 1. We illustrate how the problem of over-fitting (or under-fitting) of KDEs is resolved using our method. In Fig. 3, we compare the clustering results of valley seeking using the scale parameter h =0 . 6 , 0 . 7 and the DBSCAN algorithm using Eps =0 . 28 , 0 . 29. The best result is observed in Fig. 3(a) but it still contains several small clusters due to the spurious oscillations of the KDE. For other cases, a mixture many small clusters and some over-sized clusters are present. In contrast, our method (shown in Fig. 2) resolves these problems by (i) outlining the shape of the dataset well by keeping the CCCs smooth; (ii) using curvature motion to smooth out oscillations due to unevenness of KDEs. Example 2. A dataset with 4000 uniformly distributed points lying in two touching circles is considered. The dataset together with the zero crossings of  X  X  are shown in Fig. 4(a). The result of our method is in Fig. 4(b). We observe that the final CCCs adapt to the size of the clusters suitably. The results of valley seeking on KDEs ( h =0 . 05 , 0 . 06) are shown in Fig. 4(c) and (d) where the unevenness of the KDEs result in either 2 or 4 large clusters. The results of DBSCAN with Eps =0 . 010 , 0 . 011 are in Fig. 4(e) and (f) which contain many small clusters. In addition, this example also makes it clear that density functions must be regularized which is done implicitly by adding surface tension in our method.
 Example 3. This example uses a dataset constructed from the co-expression patterns of the genes in yeast during cell cycle. Clusters of the data are expected to reflect functional modules. The results are shown in Fig. 5. We observe that the valleys of the CIF are right on the low density regions and thus a reasonable clustering is obtained.
 Example 4. Our next example uses a real dataset from text documents in three newsgroups. For the ease of visualization, the dataset is first projected to a 2-D space using principle component analysis. The results in Fig. 6 show that the clustering results agree with the true clustering very well. In the paper, we introduced level set methods to identify density peaks and val-leys in density landscape for data clustering. The method relies on advancing contours to form cluster cores. One key point is that during contour advance-ment, smoothness is enforced via LSM. Another point is that important features of clusters are captured by cluster intensity functions which serve as a form of regularization. The usual problem of roughness of density functions is overcome. The method is shown to be much more robust and reliable than traditional methods that perform bump hunting or valley seeking on density functions. core contours are constructed, outliers are clearly revealed and can be easily identified. In this method, different contours evolve independently. Thus outliers do not affect normal cluster formation via contour advancing; This nice property does not hold for clustering algorithms such as the k -means where several outliers could skew the clustering.
 interface propagation in LSM. A more elegant approach is to recast the cluster core formation as a minimization problem where the boundary advancement can be derived from first principles which will be presented in a later paper.
