 The Web has become one of the most important information sources in people X  X  daily life, however, much untruthful information spreads on the web. Untruthful information may mislead users or affect user experiences, t herefore, it is urgent to determine whether a piece of information is trustful or not. Information is mainly loaded by sentences. Sentences that state facts , rather than opinions , are called fact statements[8]. Generally speaking, fact statements are in two categor ies: positive fact statements and negative fact statements. In this paper, we mainly f ocus on positive fact statements, since for any negative fact statement, there is a positive one corresponding to it, and truthfulness determination of a negative fact statement can be inferred through determination of the positive one. In this paper, when we say a fact statement, we mean a positive one.
A fact statement is either trustful or untruthful, depending on wether the contents it carries is a objective fact or not. When a user determine the trustfulness of a fact statement, he/she will specify some part(s) of the fact statement he/she is not sure about. The part(s) are called the doubt unit(s) [8]. After the doubt unit(s) is/are specified, the fact statement can be regarded as an answer to a question. Based on the number of correct answers of the corresponding questions, fact statements can be divided into two categories: unique-answer fact statement and multi-answer fact statement. For example, Obama is the American President is a trustful fact statement. If the doubt unit is Obama , it is an answer of whoisAmericanPresident . Since the question has only one correct answer, the fact statement is an unique-answer fact statement.

Most of the existing works utilize a popular search engine to collect the search re-sults of the target fact statement, and the tr uthfulness of the target fact statement is determined by analyzing the search results, e.g., the amount and the sentiments of the search results are considered as the dominan t factors in some works. However, this is not desirable for the following reasons: (i). The amount of the search results is not always reliable in trustfulness determination. Since not all search results support the target fact statement. (ii).T he sentiments of the search results do not always represent their sentiments on the targe t fact statement, since not every search results X  theme hap-pens to be the fact statement. Fig.1 shows a search result about the fact statement Wa r-ren Moon was born in 1956 . Obviously, the theme is not the fact statement. In other works, the trustful fact statement is identifi ed from the alternative fact statements of a target fact statement. Here, the alternativ e statements are the fact statements which are answers to the question corresponding to the target fact statement. There are three lim-itations in these works: (i). The doubt unit must be specified, otherwise the alternative fact statements can X  X  be found. (ii). If the doubt unit includes much information, it is difficult, sometimes even impossible to find the proper alternative fact statements. (iii). This method is not fit for the trustfulness determination of multi-answer fact statements, since only one fact statement is picked out as the trustful fact statement.

In order to solve these problems, we propos e a method, called Multi-verifier, to de-termine the truthfulness of a fact statement, which is suitable for the truthfulness deter-mination of multi-answer fact statements. In summary, the contributions of this paper are summarized as follows.  X  We propose a novel method, called Multi-veri fier, to determine the truthfulness of  X  We measure the support score of a search result to the target fact statement. We  X  We rank the search results on the target fact statement based on credibility. We  X  We run a set of experiments and evaluate the method in terms of reasonability and For the rest of this paper, we first summarize the related works. Then, we describe the method in details. Experiments and analysis are shown in section 4. At last, we conclude this paper. We conduct a brief review of existing works on the following aspects: web page credi-bility and statement trustfulness.

Generally speaking, credible sources are likely to present trustful information. Some researchers study credibility of web pages, and they utilize web page features to deter-mine whether a web page is credible or not, e.g., page keywords, page tittle, page style, etc[1][2][3]. However, the trustfulness of web page content hasn X  X  been considered in these works. Some web pages which meet th ese features may present incorrect facts. Other studies focus on spam web pages detection based on link analysis and content for filtering out low quality pages[4][5]. However spam web pages are not equal to un-truthful web pages and non-spam web pages may present incorrect facts. These studies can X  X  be used in fact statement determination directly.

Some researchers have started discussing the truthfulness of individual statements in very recent years. [6] introduces system Honto?search1.0 , which doesn X  X  deter-mine an uncertain fact directly, but provides users with additional data on which users Honto?search2.0 [7] is the improved version of Honto?search1.0 . This system has two function: comparative fact finding and asp ect extraction. Given an uncertain fact, the system collects comparative facts and estimates the validity of each fact. Just for checking the credibility of these facts in details, necessary aspects about the uncertain fact are extracted. Scoring the facts and th e aspects, users can get more information and determine the uncertain fact. Verify [8] can determine the truthfulness of the fact state-ment directly. Given a target fact statement, Verify finds the alternative fact statements of the fact statement, ranks these fact statements, and chooses the fact statement at the highest position as the trustful fact statement. In [7][8], doubt unit(s) of the fact state-ment should be specified. In addition, when the doubt unit include s much information, it is difficult, sometimes even impossible to find the proper alternative(comparative) fact statements. Especially, for multi-answer fact statements, they can X  X  work well. The goal of this paper is to determine whether a fact statement is trustful or not. Firstly, we submit the target fact statement to a popular search engine, and get the top-n search results; secondly, the support scores of the search results of the fact statement are mea-sured; thirdly, the credibility ranking of the search results is captured; at last, based on the support scores and credib ility ranking, the truthfulnes s of the fact statement is determined.
 3.1 Notation We u s e fs to denote the target fact statement. After deleting stop words in fs ,therest are the keywords of fs , which is represented by K . R denotes the collection of the search And s i is the snippet related to fs in p i . K r i is the collection of words which belong to both K and r i . c i is used to denotes the shortest consecutive sentences including K r i in r . su p ( r i , fs )  X  [ 0 , 1 ] is used to describe the support score of r i to fs . 3.2 Support Score Measurement Not all words and sentences in a search result make sense for the trustfulness deter-mination of the target fact statement. Th e words and sentences in the search result, which make sense for the truthfulness determ ination of the fact statement, are called necessary words and necessary sentences respectively. Necessary words are extracted from necessary sentences, but not every word in necessary sentences is necessary. Fig.1 shows a search result on Moon was born in 1956 . Obviously, Born Harold War-ren Moon, November 18, 1956... is necessary. In the sentence, Los Angeles, CA are not necessary words.

In order to measure the support score of a search result more accurately, we extract the necessary words from the search result, and compute the corresponding support score based on the necessary words. The process to measure the support score is com-posed of two stages: necessary words detection and support score computation. Necessary Words Detection. Given a search result r i , the sentences in c i are necessary sentences, since c i is the shortest consecutive sentences including K r i in r i .Weanalyze the relationships of words in c i and find the necessary words of r i . In this paper, we use Stanford Parser to find the grammatical relationships of words in c i .

Stanford Parser 1 is used to find the grammatical relationships of words in a sentence[10]. The grammatical relationship format is d . name ( d . dependent , d . governor ) . For example, amod ( great , president ) is a grammatical relationship. Here, great is a de-pendent and president play a role of governor. amod is the name of the relation. The relation means great is the modifier of president . There are 52 grammatical relationships in Stanford typed dependencies. We divide the grammatical relationships provided by Stanford Parser into two categories based on their importance to the skeleton of a sen-tence: essential grammatical relationships, represented by R e and unessential grammat-ical relationship, represented by R o .
 m ) denotes the collection of the grammatical relationships between words in c i .Wetake the following steps to find N i .  X  Capture the shortest consecutive sentence c  X  Get the collection of the grammatical relationships between the words in c  X  Use heuristic rules on the collection of grammatical relationships to find N The heuristic rules are as follows: (i). If d ij ( 1  X  j  X  m ) is essential, d ij . dependent and d . governor are put into N i .(ii).If d ij ( 1  X  j  X  m ) isn X  X  essential and d ij . dependent  X  N , d ij . governor are put into N i .

Algorithm 1 shows the procedure of finding the necessary word collection of a search the keyword set of fs and the search result, represented by K r i , the essential grammat-ical relationship collection R e and the unessential grammatical relationship collection R . While the output is the necessary word collection N i . Firstly, c i is captured. If t i containing K r i in s i (Line1-7). Secondly, the collection of grammatical relationships between words in c i , called D i , is found out(Line 8). Thirdly, N i is set to K r i (Line 9). Fourthly, retrieving D i and applying the heuristic rules, new necessary words are put into N i (Line 12-17). The algorithm repeat the fourth step, until no new necessary word is found(Line 10-18).
 Algorithm 1. Finding necessary words Support Score Computation. We compute su p ( r i , fs ) based on the distance between r and fs , represented by dis ( r i , fs ) . dis ( r i , fs ) can be computed based on the distance of N i and K . Inspired by edit distance, we define the distance between two sets. Definition 1 (Set distance). The distance of two sets is the number of insertion or deletion operations required to transform a set into another.
 is inserted in S 1 , S 1 = S 2 . So the set distance of S 1 and S 2 is 2.

We u s e Sdis ( N i , K ) to denote the set distance of N i and K . Here, K is the collection of keywords of fs . In order to get accurate Sdis ( N i , K ) ,thewordsin K or N i are stemmed directly. The reason is that fs can be express in a different way. Given a sentence fs 1 , which is different from fs but express the meaning of fs . fs can be transformed into fs 1 by word change. We suppose there are some words in fs can X  X  be changed from fs to fs 1. And if these words changes, the meaning fs can X  X  be kept. Here we use Du to denote the collection of these unchangeable words in fs . Obviously, Du  X  K .Since word matching is a factor considered in returning search results, Du is composed of the nouns and numbers in fs . Based on K and Du , we give out an equation to compute dis ( r i , fs ) . Inspired by inverse hyperbolic function, we use the following equation to compute su p ( r i , fs ) .Thevalueof  X  is determined in our experiments. In support score computation, we do not consider whether the search result oppose the fact statement or not. Since the focus is th e positive fact statements in this paper and there are few search results explicitly opposin g the target positive fact statement. In addition, for the search results implicitly opposing the fact statement, corresponding support scores may be smaller based on the equations or the search results will be considered neutral. 3.3 Credibility Ranking Capture If a fact statement is supported by more credible search results, the fact statement is more likely to be trustful. Thus, the credibility of the search results is an important factor in fact statement determination. Usua lly, credible search results are likely to be presented by credible web sites/pages. The credibility of search results can be measured by the credibility of the web sites/pages presenting them. Moreover, reputable web sites/pages are likely to be credible[1][2][9]. If a web site/page is popular and important, it could be reputable. The credibility of the web site/page can be measured based on its importance and popularity.
 Importance Ranking. The search results of a given fact statement display in a certain order. We consider the order as the importance ranking of the search results. The order is brought by the search engine which is used to find the search results. Besides, the importance of the sources of the search results are considered by the search engine. For a fact statement fs and a search result r i on fs , Srank is used to denote the importance ranking of the search results on fs ,and Srank i is the position of r i in Srank . Given an-we do not adopt the popular Pagerank for the following reasons: (i). The Pagerank val-ues of web sites/pages can X  X  be derived. (ii). Although the Pagerank levels are available, the range of Pagerank level is too smal l ([0, 10]) to satisfy our requirements. Popularity Ranking. We u s e Arank to denote the popularity ranking, Arank i represents the Arank value of r i . We capture the popularity ranking of search results based on Alexa 2 ranking. Alexa ranking is a traffic ra nking of each site based on reaches and page views. It can be used to measure popularity of web sites. Given a fact statement fs and a collection of search results R on fs . Alexa i is used to denote Alexa ranking value of the site from which r i is derived. Based on Alexa i ( 1  X  i  X  n ) , we can get the popularity ranking of the search results in R . Based on Alexa ranking, Arank i can be derived by the following equations.
 Since Alexa ranking is for all web sites, given Alexa i and Alexa j , the gap between them may be very large. We use the above four equations to map Alexa i into range [1, n]. We first introduce the median of Alexa i represented by Alexa m and get the interval ranking Amid by Equation.3 and Equation.4, just for keeping the distribution of Alexa i ( 1  X  i  X  n ) unchanged. Then, we linearly map Amid i into [1, n] and get Arank i by Equation.5 and Equation.6.
 Credibility Ranking. We u s e Crank to denote the credibility ranking of the search results on the same fact statement. Crank i is the position of r i in Crank . Based on Srank i and Arank i , Crank i can be computed. The follow equation describes how to compute Crank i based on Srank i and Arank i .
 Here, Srank and Arank play the same important roles in getting Crank .Weset w i and w 2 to 0 . 5. 3.4 Fact Statement Determination The search results on a given fact statement are in two categories: positive search results and neutral search results. S pos denotes the collection of the positive search results and S neu is the collection of the neutral search results. Given a search result r i ,if K r i ( Du is the unchangeable word collection of fs ), r i  X  S pos ;otherwise, r i  X  S neu .We of contributions of the search results in S pos . Similarly, * Neu is the sum of contributions of the search results in S Neu .

We believe if a fact statement is trustful, * Po s should be larger than * Neu . Here, *  X  .If * Po s / * Neu is larger than or equal to  X  , fs is trustful; otherwise, fs is untrustful. Algorithm 2. Fact statement determination
Algorithm 2 shows the procedure of determining a fact statement. The input of the algorithm is the collection of search re sults on the target fact statement R , the credibility ranking of the search results Crank , the support scores of the search results to the fact statement su p ( r i , fs )( 1  X  i  X  n ) , the set of words which belong to the keyword set of fs and the search result, represented by K r i , the unchangeable word collection of the fact statement Du , and the threshold  X  . While the output is the result of determination ( trustful or untruthful ). First, the contributions form positive search results and neutral search results are worked out respectively (Line 1-6). Secondly, the ratio between pos-itive contributions and neutral contributions are computed. At last, the algorithm can decide whether the fact statement is truthful or not (Line 7-10). We run a set of experiments to evaluate Multi-verifier in terms of precision and avail-ability. First, we detect the distribution of the search results including the target fact statements; secondly, we evaluate the ava ilability of the method to capture credibility ranking; thirdly, we evaluate the availab ility of the method to compute support score; at last, we measure precision of Multi-verifier while varying threshold  X  and the number of adopted search results n .

We make use of a synthetic dataset, which can be generated by the method intro-duced in [8]. The dataset is composed of 50 tr ustful fact statements and 50 untruthful fact statements. These fact statements are fetched from Trec2007 3 . In the trustful fact statements, 30 are unique-answer ones and the rest are multi-answer ones. Yahoo boss 2.0 4 is used to collect search results. Here, fo r each fact statement, top-150 search re-sults are collected. 11 experienced users, who are graduate students and experienced Internet users, help to mark the search results . If the search result includes the meaning of the corresponding fact statement, it is marked as 0, otherwise, it is marked as 1. The experiments run on an Intel Core 2 Quad 2.66Hz, windows 7 machine with 2GB main memory. 4.1 Distribution of Search Results In this experiment, we detect the average percentage of the search results which include the meanings of the target fact statements. F or untruthful fact statements, there are few search results including them. Thus, this experiment is about trustful fact statements. We u s e P f to represent the average percentage of the search results including the target fact statements. fs u denotes the trustful unique-answer fact statements, fs m represents the trustful multi-answer fact statements. Fig.2 shows P f values when n changes. Here, n is the number of the search results adopted for a fact statement. It can be seen from the figure, both P f for fs u and P f for fs m decrease with the increase of n . For a multi-answer fact statement, there are more than one correct answers to the corresponding question. Thus P f for fs u is always larger than the P f for fs m . In addition, when n increases, more search results which don X  X  include the target fact statements come out. 4.2 Evaluation of Credibility Ranking By computing the spearman correlation value between importance ranking( Srank )and popularity ranking( Arank ), we discuss whether the two rankings can replace each other or not. Fig.3 shows the average correlative value of Srank and Arank ,when n changes. It can be seen that, the average correlative value is always smaller than 0 . 63; when n = 40, the average correlative value is at the peak; when n 40, the average correlation value decreases with the increase of n . Therefore, Srank and Arank can not replace each other.
In the experiments, distributions of credibility ranking ( Crank ) and popularity rank-ing ( Arank ) are detected. Fig.4 shows distribu tions of credibility ranking and popular-ity ranking. The horizontal axis represents the positions of search results in importance ranking( Srank ), and the vertical axis represents the average credibility rank values or popularity rank values of the search results at corresponding positions . It can be seen in Fig.4, the average Arank values increase with the increase of Srank . However, changes of average Crank values are not so sharply. It means that the Arank or Srank can not replace Crank , and some search results at higher positions in Srank or Arank may be at lower positions in Crank . It is consistent with our observation. 4.3 Evaluation of Support Score We evaluate the availability of the way to measure the distance of a search result and the target fact statement and as certain the optimal values of  X  by experiments.
Fig.5(a) shows the average distance for trustful unique-answer fact statements, when n changes. rd su represents the average distance for these search results which do not included the meaning of the corresponding fact statement. rd si denotes the average dis-tance for these search results including the m eaning of the target fact statements. It can be seen from the figure, rd si is smooth. With n increases, rd su increases. rd su is always greater than rd si . The difference between rd su and rd si is larger than 0 . 3. Fig.5(b) shows the average distance for trustful multi-answer fact statements, when n changes. rd su and rd si represent the same as them in Fig.5(a). In this figure, rd su is always above rd si and the smallest difference is larger than 0 . 4. Fig.5(c) shows the average distance for un-truthful fact statements, when n varies. In Fig.5(c), rd su is always above rd si and the smallest difference is greater than 0 . 8. From the experiments, we can know whether a search result includes the target fact statement can be reflected by the distance.
In computation of support scores,  X  plays an important role (Equation.2). We believe if the value of  X  makes the difference between the average support scores of the search results including target fact statements an d that of the other search results reach a max-imum, the  X  value is optimal. Fig.6 shows the difference on n = 150, when  X  changes. From this figure, when  X  = 1 . 3, the difference reach the peak (0.2). It means 1.3 is the optimal value of  X  . 4.4 Evaluation of Truthfulness Determination During the truthfulness determination of f act statements, the number of the search re-sults adopted for a fact statement n and the threshold  X  influence the precision of the truthfulness determination. In the experiments, we measure the precision of Multi-verifier, based on various n and  X  values. Since  X  is more than one, the range of  X  is from 1 to 5.

Fig.7 shows the precision on n when  X  = 1 . 5. From this figure, when the n = 30, the precision reaches the peak(0.9). When n 30, the precision is enhanced with the increase of n . The reason is that more search results are positive in top 30 search results, and more positive search results are considered with the increase of n .When n 30, the precision decreases with the increase of n . The reason is that more neutral search results are considered with increase of n when n 30. Fig.8 shows the precision on  X  when n = 30. From this figure, When  X  = 1 . 5, the precision reaches the greatest value(0 . 9). When  X  1 . 5, the precision is enhanced with the increase of  X  . Since, when  X  is smaller, some untruthful fact statements ar e regarded as trustful ones. However, when  X  is greater, some trustful fact statements ar e regarded as untruthful ones. Thus, when  X  1 . 5, the precision decreases with the increase of  X  . Fig.9 shows the precision on  X  and n . From this figure, the overall trend is that the precision increases and then decreases as the increase of n and  X  . Especially, when  X  = 1 . 5and n = 30, the precision reach the peak(0.9).

The experiments show the method is availa ble and accurate. Since the dataset in-cludes many multi-answer fact statements, we can see that the method is also available for multi-answer fact statements determination.

Since Honto?search can not determine a fact statement directly, it is not necessary to run experiments to compare it with our method. Although Verify can directly de-termine a fact statement, it is based on alte rnative fact statements and not suitable for multi-answer fact statements. Thus, it is not necessary to compare it with our method. In this paper, we propose a method called Multi  X  veri f ier to determine the truthfulness of a fact statement. In Multi  X  veri f ier , the search results on the fact statement,whose truthfulness is needed to be determined, are found by a popular search engine. And the support scores and the credib ility ranking are considered in determining the truthfulness of the fact statement. The experiments shows the method is powerful. However, our focus is domain-independent fact statements, and the domain knowledge isn X  X  used. In the future, we will focus on the domain-dependent fact statements, we think that usage of domain knowledge can bring higher precision. In addition, the quality of related information is important for the truthfulness determination. We will try to find a method to enhance the quality of related information on a fact statement.
 Acknowledgments. This work is partly supported by the Important National Science &amp; Technology Specific Projects of China (Grant No.2010ZX01042-001-002), the Na-tional Natural Science Foundation of China (Grant No.61070053), the Graduates Sci-ence Foundation of Renmin University of China(Grant No.12XNH177) and the Key Lab Found (07dz2230) of High Trusted Computing in Shanghai.

