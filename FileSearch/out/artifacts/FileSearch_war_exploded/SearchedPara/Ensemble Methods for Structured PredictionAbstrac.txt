 Google Research, 111 8th Avenue, New York, NY 10011 Vitaly Kuznetsov VITALY @ CIMS . NYU . EDU Ensemble methods are general techniques in machine learning for combining several hypotheses to create a more accurate predictor ( Breiman , 1996 ; Freund &amp; Schapire , 1997 ; Smyth &amp; Wolpert , 1999 ; MacKay , 1991 ; Freund et al. , 2004 ). These methods often significantly improve the performance in practice and additionally benefit from favorable learning guarantees. However, ensemble meth-ods and their theory have been developed primarily for the binary classification problem or regression tasks. These techniques do not readily apply to structured prediction problems. While it is straightforward to combine scalar outputs for a classification or regression problem, it is less clear how to combine structured predictions such as phone-mic pronunciation hypotheses, speech recognition lattices, parse trees, or outputs of several machine translation sys-tems.
 Consider for example the problem of devising an ensemble method for pronunciation, a critical component of modern speech recognition ( Ghoshal et al. , 2009 ). Often, several pronunciation models or experts are available for transcrib-ing words into sequences of phonemes. These models may have been derived using other machine learning algorithms or they may be based on carefully hand-crafted rules. In general, none of these pronunciation experts is fully ac-curate and each expert may be making mistakes at differ-ent positions in the output sequence. One can hope that a model that patches together the pronunciation of different experts could achieve a superior performance.
 Similar ensemble structured prediction problems arise in other tasks, including machine translation, part-of-speech tagging, optical character recognition and computer vision, with structures or substructures varying with each task. We seek to tackle all of these problems simultaneously and consider the general setting where the label or output as-sociated to an input x  X  X  is a structure y  X  X  that can be decomposed and represented by l substructures y 1 , . . . , y For the pronunciation example just discussed, x is a spe-cific word or word sequence and y its phonemic transcrip-tion. A natural choice for the substructures y k is then the individual phonemes forming y . Other possible choices in-clude n -grams of consecutive phonemes or more general subsequences.
 We will assume that the loss function considered admits an additive decomposition over the substructures, as is com-mon in structured prediction. We also assume access to a set of structured prediction experts h 1 , . . . , h p that we treat as black boxes. Given an input x  X  X , each of these ex-perts predicts l substructures h j ( x ) = ( h 1 j ( x ) , . . . , h The hypotheses h j may be the output of other structured prediction algorithms such as Conditional Random Fields ( Lafferty et al. , 2001 ), Averaged Perceptron ( Collins , 2002 ), StructSVM ( Tsochantaridis et al. , 2005 ), Max Mar-gin Markov Networks ( Taskar et al. , 2004 ), the Regression Technique for Learning Transductions ( Cortes et al. , 2005 ), or some other algorithmic or human expert. Given a labeled combine the predictions of these experts to form an accu-rate ensemble.
 Variants of the ensemble problem just formulated have been studied in the past in the natural language processing and machine learning literature. One of the most recent, and possibly most relevant studies for sequence data is that of Nguyen &amp; Guo ( 2007 ), which is based on the forward stepwise selection procedure introduced by Caruana et al. ( 2004 ). Starting with a possibly empty collection of ex-perts, E 0 , that algorithm performs T iterations. To make predictions using a collection of models, E t , a variant of a majority-vote scheme per position is proposed, and at each the collection E t  X  1 in such a way that E t = E t  X  1  X  X  h has the best performance on the training set among all sets E t  X  1  X  X  h j } , j = 1 , . . . , p . This algorithm always per-forms at least as well as the best expert among h 1 , . . . , h on the training set. If the initial collection E 0 of experts is empty, then E 1 simply contains the expert with the smallest error on the training set. Further additions to E t only de-crease that error, hence the performance of this algorithm on the training set cannot be worse than the performance of the best expert.
 One disadvantage of this greedy approach is that it may fail to select an optimal ensemble of experts in cases where experts specialize in local predictions. Consider the case where expert h k is a strong predictor for the k th substruc-ture but does not perform well on other substructures. As-sume further that expert h 0 is a jack-of-all-trades and per-forms better than any of h 1 , . . . , h p on average, but each h k beats h 0 at position k . Then, one can show that the stepwise selection routine may end up with an ensemble consisting of only h 0 , while an optimal solution would use expert h k to predict the k th substructure. We provide an explicit construction of such an example in Appendix I and report similar empirical observations in Section 5 . Ensemble methods for structured prediction based on bag-ging, random forests and random subspaces have been pro-posed in ( Kocev et al. , 2013 ). One of the limitations of this work is that it is applicable only to a very specific class of tree-based experts introduced in that paper. Similarly, a boosting approach was developed in ( Wang et al. , 2007 ) but it applies only to local experts. In the context of nat-ural language processing, a variety of different re-ranking techniques have been proposed for somewhat related prob-Sagae &amp; Lavie , 2006 ; Zhang et al. , 2009 ). But, re-ranking methods do not combine predictions at the level of sub-structures, thus the final prediction of the ensemble coin-cides with the prediction made by one of the experts, which can be shown to be suboptimal in many cases. Further-more, these methods typically assume the use of probabilis-tic models, which is not a requirement in our learning sce-nario. Other ensembles of probabilistic models have also been considered in text and speech processing by forming a product of probabilistic models via the intersection of lat-tices ( Mohri et al. , 2008 ), or a straightforward combination of the posteriors from probabilistic grammars trained using EM with different starting points ( Petrov , 2010 ), or some other rather intricate techniques in speech recognition ( Fis-cus , 1997 ). See Appendix J for a brief discussion of other related work.
 Most of the references mentioned do not give a rigorous theoretical justification for the techniques proposed. We are not aware of any prior theoretical analysis for the en-semble structured prediction problem we consider. Here, we aim to bridge this gap and develop ensemble methods that both perform well in practice and enjoy strong theo-retical guarantees. Two families of algorithms are intro-duced. In Section 3 we develop ensemble methods based on on-line algorithms. To do so, we extend existing on-line-to-batch conversions to our more general setting. A boosting-type algorithm is also presented and analyzed in Section 4 . Section 5 reports the results of our extensive experiments. As in standard supervised learning problems, we as-sume that the learner receives a training sample S = (( x 1 , y 1 ) , . . . , ( x m , y m ))  X  X  X  Y of m labeled points drawn i.i.d. according to some distribution D used both for training and testing. We also assume that the learner has access to a set of p predictors h 1 , . . . , h p mapping X to Y to devise an accurate ensemble prediction. Thus, for any input x  X  X , he can use the prediction of the p able to the learner about these p experts, in particular the way they have been trained or derived is not known to the learner. But, we will assume that the training sam-ple S available to learn the ensemble is distinct from what may been used for training the algorithms that generated h ( x ) , . . . , h p ( x ) .
 To simplify our analysis, we assume that the number of substructures l  X  1 is fixed. This does not cause any loss of generality so long as the maximum number of substruc-tures is bounded, which is the case in all the applications we consider. The quality of the predictions is measured by a loss function L : Y X Y  X  R + that can be decomposed as a sum of loss functions ` k : Y k  X  R + over the substructure sets Y k , that is, for all y = ( y 1 , . . . , y l )  X  X  with y and y 0 = ( y 0 1 , . . . , y 0 l )  X  X  with y 0 k  X  X  k , We will assume in all that follows that the loss function L is bounded by some M &gt; 0 : L ( y , y 0 )  X  M for all ( y , y A prototypical example of such loss functions is the nor-malized Hamming loss, L Ham , which is the fraction of sub-structures for which two labels y and y 0 disagree. In this section, we present an on-line learning solution to the ensemble structured prediction problem just discussed. We first formulate the problem as that of on-line learning with expert advice, where the experts correspond to the paths of a directed graph. The on-line algorithm generates at each iteration a distribution over the path-experts. A crit-ical component of our approach consists of using the dis-tributions to define a prediction algorithm with good gener-alization guarantees. This requires an extension of the ex-isting on-line-to-batch conversion techniques to the more general case of combining distributions over path-experts (instead of combining intermediate hypotheses). 3.1. Path experts Each expert h j induces a set of substructure hypotheses h , . . . , h l j . As already discussed, one particular expert may be better at predicting the k th substructure while some other expert may be more accurate at predicting another substructure. Therefore, it is desirable to combine the sub-structure predictions of all experts to derive a more accurate prediction. This leads us to considering a directed graph G such as that of Figure 1 which admits l + 1 vertices 0 , 1 , . . . , l and an edge from vertex k to vertex k + 1 la-beled with each of the p hypotheses h k 1 , . . . , h k p the experts h 1 , . . . , h p for the k th substructure. Graph G compactly represents a set of path experts : each path from the initial vertex 0 to the final vertex l is labeled with a sequence of substructure hypotheses h 1 j fines a hypothesis which associates to input x the output h path experts. We also denote by h each path expert defined its k th substructure hypothesis h k j ture prediction problem can then be formulated as that of selecting the best path expert (or collection of path experts) in the graph G . Note that, in general, the path expert se-lected does not coincide with any of the original experts h , . . . , h p .
 More generally, our paths experts can be selected from a directed acyclic graph of experts G 0 distinct from G , as il-lustrated by Figure 2 . This can be motivated by scenarios where some prior knowledge is available about the expert predictions for different substructures (see Appendix A ), which could be related to phonotactic constraints, as in the example of pronunciation sequences, or any other prior constraint on illegal n -grams or other subsequences that would result in ruling out certain paths of graph G . For convenience, in what follows, we will discuss our al-gorithms and solutions in the specific case of the graph G . However, the on-line learning algorithms we use apply in the same way to an arbitrary directed acyclic graph G 0 . The randomized algorithm we describe can also be used in a similar way and our batch learning guarantees for our ran-domized algorithm can be straightforwardly extended to an arbitrary graph G 0 . In fact, those guarantees are then some-what more favorable since the number of path experts in G will be smaller than in G . 3.2. On-line algorithm Using G , the size of the pool of experts H we consider is p , and thus is exponentially large with respect to p . But, since learning guarantees in on-line learning admit only a logarithmic dependence on that size, they remain infor-mative in this context. However, the computational com-plexity of most on-line algorithms also directly depends on that size, which would make them impractical in this context. But, there exist several on-line solutions precisely designed to address this issue by exploiting the structure of the experts as in the case of our path experts. These include the algorithm of Takimoto &amp; Warmuth ( 2003 ) de-noted by WMWP, which is an extension of the (random-ized) weighted-majority (WM) algorithm of Littlestone &amp; Warmuth ( 1994 ) (see also ( Vovk , 1990 )) to more general bounded loss functions 1 combined with the directed graph Weight Pushing (WP) algorithm of Mohri ( 1997 ), and the Follow the Perturbed Leader (FPL) algorithm of Kalai &amp; Vempala ( 2005 ).
 The basis for the design of our batch algorithms is the WMWP algorithm since it admits a more favorable regret guarantee than the FPL algorithm in our context. However, we have also carried out a full analysis based on the FPL Algorithm 1 WMWP algorithm.
 { h 1 , . . . , h p } ; parameter  X   X  (0 , 1) . for j = 1 to p and k = 1 to l do end for for t = 1 to T and j = 1 to p and k = 1 to l do end for
Return matrices { W 1 , . . . , W T } algorithm which can be found in Appendix D .
 As in the standard WM algorithm ( Littlestone &amp; Warmuth , 1994 ), WMWP maintains a distribution p t , t  X  [1 , T ] , over the set of all experts, which in this context are the path experts h  X  H . At each round t  X  [1 , T ] , the al-gorithm receives an input sequence, x t , incurs the loss E h  X  p t [ L ( h ( x t ) , y t )] = P h p t ( h ) L ( h ( x tiplicatively updates the distribution weight per expert: where  X   X  (0 , 1) is some fixed parameter. The number of paths is exponentially large in p and the cost of updat-ing all paths is therefore prohibitive. However, since the loss function is additive in the substructures, the updates are multiplicative, and p t can be compactly represented and updated by maintaining a potential value stored at each ver-tex ( Takimoto &amp; Warmuth , 2003 ). The cost of the update is then linear in the size of the graph.
 The graph G we consider has a specific structure, thus, our description of the algorithm can be further simplified by maintaining at any round t  X  [1 , T ] , an edge weight w t,kj for the j th edge, j  X  [1 , p ] , between vertices k  X  1 and k . This defines a matrix W t = ( w t,kj ) kj  X  R l  X  p with the following properties: 1. for any path expert h defined by h j 1 1 , . . . , h j 2. the weights of outgoing edges sum to one at any vertex This clearly ensures that P h  X  H p t ( h ) = 1 with the update rule ( 2 ) replaced by the following equivalent and more ef-ficient edge weight update: Algorithm 1 gives the pseudocode of WMWP. 3.3. On-line-to-batch conversion The WMWP algorithm does not produce a sequence of path experts, rather, it produces a sequence of distributions p , . . . , p T over path experts, or equivalently a sequence of matrices W 1 , . . . , W T . Thus, the on-line-to-batch con-version techniques described in ( Littlestone , 1989 ; Cesa-Bianchi et al. , 2004 ; Dekel &amp; Singer , 2005 ) do not readily apply. Instead, we propose a generalization of the tech-niques of Dekel &amp; Singer ( 2005 ). The conversion consists of two steps: first extract a good collection of distributions P  X  { p 1 , . . . , p T } ; next use P to define an accurate hy-pothesis for prediction. For a subset P  X  X  p 1 , . . . , p define
 X ( P )= where  X  &gt; 0 is a fixed parameter. The second equality in ( 4 ) is a straightforward consequence of the identity p t Q k =1 w t,kj k and the additive decomposition of L in terms of l k s (see Lemma 6 in the appendix). With this definition, we choose P  X  as a minimizer of  X ( P ) over some collection P of subsets of { p 1 , . . . , p T } : P  X   X  argmin P  X  X  choice of P is restricted by computational considerations. One natural option is to let P be the union of the suffix follows that P includes the set { p 1 , . . . , p T } . Next we define a randomized algorithm based on P  X  . Given an input x , the algorithm consists of randomly selecting a path h according to and returning the prediction h ( x ) . Note that computing and storing p directly is not efficient. To sample from p , we first choose p t  X  P  X  uniformly at random and then sample a path h according to that p t . Observe that for any fixed k  X  [1 , l ] , P l j =1 w t,kj = 1 , thus the non-negative weights w t,kj define a distribution over the edges leaving vertex k that we denote by w t,k  X  . Thus, to sample h from p t we can simply draw an edge from each of the l distributions w t,k  X  (the probability mass of a path is the product of the prob-ability masses of its edges). Note that once an input x is received, the distribution p over the path experts h induces a probability distribution p x over the output space Y . It is not hard to see that sampling a prediction y according to p x is statistically equivalent to first sampling h according to p and then predicting h ( x ) . We will denote by H Rand randomized hypothesis thereby generated.
 An inherent drawback of randomized solutions such as the one just described is that for the same input x the user can receive different predictions over time. Randomized solu-tions are also typically more costly to store. A collection of distributions P can, however, also be used to define a deterministic prediction rule based on the scoring function approach. The majority vote scoring function is defined by e h The majority vote algorithm denoted by H MVote is then de-fined by H MVote ( x ) = argmax y  X  X  e h MVote ( x , y ) , x  X  X  . In the case of the graph G , the maximizer of e h MVote is found efficiently by choosing y such that y k has the maximum weight in position k .
 In the next section, we present learning guarantees for H diction rules in Appendix E . 3.4. Batch learning guarantees We first present learning bounds for the randomized predic-tion rule H Rand . Next, we upper bound the generalization error of H MVote in terms of that of H Rand .
 Proposition 1. For any  X  &gt; 0 , with probability at least 1  X   X  over the choice of the sample (( x 1 , y 1 ) , . . . , ( x drawn i.i.d. according to D , the following inequality holds: where L t = E h  X  p t [ L ( h ( x t ) , y t )] .
 Proof. Let P = { p t 1 , . . . , p t | P | } . Observe that We denote the inner summand by A s and observe that A s forms a martingale difference with respect to the filtration G s = F t s associated with the process ( x t , y t ) , i.e. F  X  -algebra generated by ( x 1 , y 1 ) , . . . , ( x t , y Since p t is determined by F t  X  1 and ( x t , y t ) is independent of F t  X  1 , we can write where E 1: q indicates that the expectation is taken with E [ A s |G s  X  1 ] = 0 , which implies that A s is a martingale difference sequence. Since | A s |  X  M/ | P | , it follows from Azuma X  X  inequality that the probability of the event is at most  X  . Since P  X  is a minimizer of 1 | P | P | P | M conclusion follows.
 The next step consists of relating the expected loss of H to the regret R T of the WMWP algorithm: Theorem 2. For any  X  &gt; 0 , with probability at least 1  X   X  over the choice of the sample (( x 1 , y 1 ) , . . . , ( x drawn i.i.d. according to D , the following inequalities hold: E [ L ( H Rand ( x ) , y )]  X  inf E [ L ( H Rand ( x ) , y )]  X  inf See Appendix B for a proof of this result. We now up-per bound the generalization error of the majority-vote al-gorithm H MVote in terms of that of the randomized algo-rithm H Rand , which, combined with Theorem 2 , immedi-ately yields generalization bounds for the majority-vote al-gorithm H MVote . The first proposition, which admits a sim-ple proof, relates the expected loss of the majority vote al-gorithm to that of a randomized algorithm in the case of the normalized Hamming loss.
 Proposition 3. The following inequality relates the gener-alization error of the majority-vote algorithm to that of the randomized one: where the expectations are taken over ( x , y )  X  X  and h  X  p . Proof. By definition of the majority vote, if H MVote makes an error at position k on example ( x , y ) , then, the total weight of incorrect labels at that position must be at least half of the total weight of labels in that position. In other words, the following inequality holds for any k : Summing up these inequalities over k and taking expecta-tions yields the desired bound.
 Proposition 3 suggests that the price to pay for derandom-ization is a factor of 2 . However, this may be too pes-simistic. A more refined result presented in the following proposition shows that often this price is lower. Proposition 4. The following bound holds for any distri-bution D over X  X Y :
E [ L Ham ( H MVote ( x ) , y )]  X  2 E [ L Ham ( H Rand ( x ) , y )] where  X  ( x , y ) = P l k =1  X  k ( x , y ) with  X  k ( x , y ) = max 0 , 1 | P The proof is a refinement of the proof of Proposition 3 and can be found in Appendix B . Each  X  k in Proposition 4 can be interpreted as the edge of incorrect labels and this result implies that any additional edge of an incorrect hypothesis (beyond 1 2 ) should not be included in the bound. Our methods generalize the results of Dekel &amp; Singer ( 2005 ) where l = 1 and each p t is a probability point mass at a hypothesis h t produced by an on-line algorithm on the t th iteration. It is also possible to extend the cross-validation approach of Cesa-Bianchi et al. ( 2004 ) to our set-ting, but the learning guarantees for this algorithm end up being less favorable than those just given (see Appendix C for a full description and analysis). Our results and algo-rithms can be extended to the case of other directed acyclic graphs of path experts and other derandomization methods (see Appendix E for a more detailed discussion). In this section, we devise a boosting-style algorithm for our ensemble structured prediction problem. The variants of AdaBoost for multi-class classification such as Ada-Boost.MH or AdaBoost.MR ( Freund &amp; Schapire , 1997 ; Schapire &amp; Singer , 1999 ; 2000 ) cannot be readily applied in this context. First, the number of classes to consider here is quite large, as in all structured prediction problems, since it is exponential in the number of substructures l . For example, in the case of the pronunciation problem where the number of phonemes for English is in the order of 50 , the number of classes is 50 l . But, the objective function for AdaBoost.MH or AdaBoost.MR as well as the main steps of the algorithms include a sum over all possible la-bels, whose computational cost in this context would be prohibitive. Second, the loss function we consider is the normalized Hamming loss over the substructures predic-tions, which does not match the multi-class losses for the variants of AdaBoost. 2 Finally, the natural base hypotheses for this problem admit a structure that can be exploited to devise a more efficient solution, which of course was not part of the original considerations for the design of these variants of AdaBoost. 4.1. Hypothesis sets and loss function The predictor H ESPBoost returned by our boosting algorithm is based on a scoring function e h : X  X  Y  X  R , which, as for standard ensemble algorithms such as AdaBoost, is a convex combination of base scoring functions e h t : e h = P t =1  X  t e h t , with  X  t  X  0 . The base scoring functions we consider for our problem are derived from the path experts in H . For each path expert h t  X  H , we define a scoring function e h t as follows: Thus, the score assigned to y by the base scoring function e h is the number of positions at which y matches the pre-diction of path expert h t given input x . H ESPBoost is defined as follows in terms of e h or h t s:  X  x  X  X  , H ESPBoost ( x ) = argmax 4.2. ESPBoost algorithm For any i  X  [1 , m ] and k  X  [1 , l ] , we define the margin of e h k for point ( x i , y i ) by  X  ( e h k , x i , y i ) = e Lemma 5. The following upper bound holds for the empir-ical normalized Hamming loss of the hypothesis H ESPBoost Algorithm 2 ESPBoost Algorithm.

Inputs: S = (( x 1 , y 1 ) , . . . , ( x m , y m )) ; set of experts { h 1 , . . . , h p } . for i = 1 to m and k = 1 to l do end for for t = 1 to T do end for Return e h = P T t =1  X  t e h t In view of this upper bound, we consider the objective func-tion F : R N  X  R defined for all  X  = (  X  1 , . . . ,  X  N )  X  by
F (  X  ) = where h 1 , . . . , h N denote the set of all path experts in H . F is a convex and differentiable function of  X  . Our algo-rithm, ESPBoost (Ensemble Structured Prediction Boost-ing), is defined by the application of coordinate descent to the objective F . Algorithm 2 shows the pseudocode of the ESPBoost (see Appendix G.2 for the details of the deriva-tion of the coordinate descent algorithm).
 Our weak learning assumption in this context is that there exists  X  &gt; 0 such that at each round, t verifies t &lt; For the graph G , at each round, the path expert h t with the smallest error t can be determined easily and efficiently by first finding for each substructure k , the h k t that is the best with respect to the distribution weights D t ( i, k ) . Observe that, while the steps of our algorithm are syntac-tically close to those of AdaBoost and its multi-class vari-ants, our algorithm is distinct and does not require sums over the exponential number of all possible labelings of the substructures and is quite efficient. We have derived margin-based learning guarantees for ESPBoost which are presented in detail and proven in Appendix G.3 . We used a number of artificial and real-life data sets for our experiments. For each data set, we performed 10-fold cross-validation with disjoint training sets. 3 We report the average test error for each task. In addition to the H MVote H two algorithms discussed in more detail in the appendix: a cross-validation on-line-to-batch conversion of the WMWP algorithm, H CV , and a majority-vote on-line-to-batch con-version with FPL, H FPL , and a cross-validation on-line-to-batch conversion with FPL, H FPL-CV . Finally, we compare with the H SLE algorithm of Nguyen &amp; Guo ( 2007 ). 5.1. Artificial data sets Our artificial data set, ADS1 and ADS2 simulate the sce-narios described in Section 1 . In ADS1 the k th expert has a high accuracy on the k th position, in ADS2 an expert has low accuracy in a fixed set of positions. More details on the data set and the experimental parameters can be found in Appendix H.1 .
 Table 1 reports the results of our experiments. In both cases H
MVote , our majority-vote algorithm based on our on-line-to-batch conversion using the WMWP algorithm (together with most of the other on-line based algorithms), yields a significant improvement over the best expert. It also out-performs H SLE , which in the case of ADS1 even fails to outperform the best h j . After 100 iterations on ADS1, the ensemble learned by H SLE consists of a single expert, which is why it leads to such a poor performance. It is also worth pointing out that H FPL-CV and H Rand fail to outperform the best model on ADS2 set. This is in total agreement with our theoretical analysis since, in this case, any path expert has exactly the same performance and the error of the best path expert is an asymptotic upper bound on the errors of these algorithms.
 5.2. Pronunciation data sets We had access to two proprietary pronunciation data sets, PDS1 and PDS2. In both sets each example is an English word, typically a proper name. For each word, 20 pos-sible phonemic sequences are available, ranked by some pronunciation model. Since the true pronunciation was not available, we set the top sequence to be the target label and used the remaining as the predictions made by the experts. The only difference between PDS1 and PDS2 is their size: 1 , 313 words for PDS1 and 6 , 354 for PDS2.
 In both cases on-line based algorithms, specifically H MVote significantly outperformed the best model as well as H SLE see Table 2 . The poor performance of H ESPBoost is due to the fact that the weak learning assumption is violated after 5 -8 iterations and hence the algorithm terminates. 5.3. OCR data set Rob Kassel X  X  OCR data set is available for download from http://ai.stanford.edu/  X btaskar/ocr/ . It con-tains 6 , 877 word instances with a total of 52 , 152 charac-ters. Each character is represented by 16  X  8 = 128 binary pixels. The task is to predict a word given its sequence of pixel vectors. To generate experts we used several soft-ware packages: CRFsuite ( Okazaki , 2007 ) and SVM struct SVM multiclass ( Joachims , 2008 ), and the Stanford Classi-fier ( Rafferty et al. , 2014 ). We trained these algorithms on each of the predefined folds of the data set and used the resulting models to generate expert predictions.
 The results reported in Table 7 in Appendix H show that ensemble methods lead only to a small improvement in per-formance over the best h j . This is because the best model h j dominates all other experts and ensemble methods can-not benefit from patching together different outputs. 5.4. Penn Treebank data set The part-of-speech task (POS) consists of labeling each word of a sentence with its correct part-of-speech tag. The Penn Treebank 2 data set is available through LDC license at http://www.cis.upenn.edu/  X treebank/ and con-tains 251 , 854 sentences with a total of 6 , 080 , 493 tokens and 45 different parts of speech.
 For the first experiment (TR1), we used 4 disjoint train-ing sets to produce 4 SVM multiclass models and 4 maxi-mum entropy models using the Stanford Classifier. We also used the union of these training sets to devise one CRF-suite model. For the second experiment (TR2) we trained 5 SVM struct models. The same features were used for both experiments. For the SVM algorithms, we generated 267 , 214 bag-of-word binary features. The Stanford Classi-fier and CRFsuite packages use internal routines to gener-ate their features. For more detail, see Appendix H . The results of the experiments are summarized in Table 3 . For TR1, our on-line ensemble methods improve over the best model. Note that H SLE has the best average loss over 10 runs for this experiment. This comes at a price of much higher standard deviation which does not allow us to con-clude that the difference in performance between our meth-ods and H SLE is statistically significant. In fact, on two runs H
SLE chooses an ensemble consisting of a single expert and fails to outperform the best model. We presented a broad analysis of the problem of ensem-ble structured prediction, including a series of algorithms with learning guarantees and extensive experiments. Our results show that our algorithms, most notably H MVote , can result in significant benefits in several tasks, which can be of a critical practical importance. In Appendix H , we also report very favorable results for H MVote when used with the edit-distance, which is the natural measure in many applications. A natural extension of this work consists of devising new algorithms and providing learning guar-antees specific to other loss functions such as the edit-distance.
 The extension of our algorithms and solutions to other di-rected graphs, as discussed in Appendix E , can further in-crease the applicability of our methods and enhance perfor-mance. While we aimed for an exhaustive study including multiple on-learning algorithms, different conversions to batch and derandomizations, we are aware that the problem we studied is very rich and admits many more facets and scenarios that we plan to investigate in the future. We warmly thank our colleagues Francoise Beaufays and Fuchun Peng for kindly extracting and making available to us the pronunciation data sets used for our experiments and Richard Sproat and Brian Roark for help with other data sets. This work was partly funded by the NSF award IIS-1117591 and the NSERC PGS D3 award.

