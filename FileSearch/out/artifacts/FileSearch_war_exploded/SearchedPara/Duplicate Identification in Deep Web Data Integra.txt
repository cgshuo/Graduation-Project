 Deep web data integration is becoming a hot area for both research and industry. There is often high redundancy among web databases, so it is a necessary step to match duplicates for data cleaning or further applications, such as price comparison services and the information merging. 
Duplicate identification(a.k.a. de-duplication, record linkage, etc.) is the process of identifying different representations of one entity, which is always a challenging task in heterogeneous data sources integration. To the best of our knowledge, lots of solutions have been proposed to address this issue[7]. However, most of them focus on the problem only between two sources. Due to the large scale of deep web, lots of web databases are integrated in practice. As a result,  X   X   X  matchers have to be built for n web databases if traditional approaches were applied. 
Fig.1 shows three movies records from web databases W A , W B and W C . M( W A , W B ) is the customized duplicate matcher for W A and W B , and an example rule requires title similarity of two duplicate records is larger than 0.85. The threshold is high due to the matched in fact. More importantly, the new shared attribute, genre , cannot be handled works[5][ 13 ] have realized this fact, and they try to select the best matching technique or combine multiple matching algorithms to improve matching performance. But it is a challenging problem to build and maintain the library of matching algorithm and select the optimal one. 
In this paper, we study the problem of duplicate identification over multiple web databases. There are a large number of web databases in one domain and usually lots of web databases are integrated, and it is not practical to build and maintenance lots of matchers. The proposed approach is based on two interesting observations. First, the presentation variations of an attribute are finite, which means an instance of it can be transformed into other forms according to some variation rules(e.g. the person name  X  X im Gray X  has limited variations, such as  X  X . Gray X ,  X  X ray J. X .). Second, there exists similarity dependency among attributes, i.e. the similarity of one attribute can be improved by the similarities of other attributes. For example, if having known the actors and the publication dates of two movie records are same, we are more certain the titles are same too. However, previous works only calculate the attribute similarity independently without considering their dependency. 
In summary, the contributions of this paper are present as follows. First, we believe interesting issue in deep Web data integration. Second, we identify the similarity dependencies among attributes, and proposed an inference-based method to improve the record similarity by exploiting the similarity dependencies. Third, an efficient approach is proposed to building the universal matcher, which can greatly reduce the cost of manual labeling. 
The rest of this paper is organized as follows. Section 2 introduces the variation rules to handle various representations of attribute values, and further give the uniform representation of record similarity. An inference-based method to improve building the universal matcher. Section 5 presents the experiments. Section 6 contains the related works. Section 7 is the conclusion. This section first presents the variation rules of attributes, and then gives the uniform representation of record similarity based on the variation rules. 2.1 Variation Rules Different representations would be found for the same object from different web databases. A number of methods have been developed to calculate the attribute similarity by assigning a 0-1 real number. Such real-value based approaches are not presentations of an attribute follow finite variation rules. We summarize and classify all the observed variation rules in Table 1, and the examples are also given. 
For each attribute, one or several rules are assigned to it by domain experts. For assigned to the person name related attributes(e.g.  X  X ctor X  and  X  X irector X ) in movie domain. Though being a manual task, the assignments are universal and stable on domain level. In practice, assigning rules to about 25 popular attributes in one domain is enough because the  X  X are X  attrib utes have little contribution. 2.2 Representations of Attribute Similarity and R ecord Similarity An important characteristic of our approach is using three logic values (Yes, Maybe probabilistic-based approaches, the three logic values are based on the variation rules defined below (suppose v 1 and v 2 are two compared values): (1) YES ( Y ). If v 1 and v 2 are absolutely same, their similarity is "YES". the assigned variation rules. For example,  X  X ohn Smith X  can be transformed into "S. John" with  X  Rearrangement  X  and  X  Prefix  X  rules. In this situation, it is uncertain whether v 1 and v 2 are the same semantic, and  X  X AYBE X  is labeled. (3) NO ( N ). If v 1 cannot be transformed into v 2 by applying the assigned rules,  X  X O X  is labeled, such as  X  X ohn Smith X  and  X  X . Jensen X . 
Based on the three logic values, attribute similarity can be represented as  X  X  X ,  X  X  X  or s example to illustrate the representation of record similarity.  X  X  X  is the obstacle to duplicate identification, and it must be  X  X  X  or  X  X  X  in fact. Hence, if  X  X  X  is transformed into  X  X  X  or  X  X  X  with more evidences, duplicates can be identified more accurately. Existing approaches do not catch sight of the inherent similarity similarity dependency and then present a novel method to improve record similarity by using Markov Logic Networks to exploit the attribute similarity dependency. 3.1 Attribute Similarity Dependency Actually, there exists potential similarity dependency among attributes. For example, it is difficult to determine the titles of the two records in Table 2 are same no matter which current similarity function is applied. But intuitively, we will be sure the titles are same if having known both actors and publication dates are same.  X   X   X   X  X   X   X   X   X   X   X   X  X   X   X  X   X   X   X   X   X  X  X  X   X   X  X   X   X  Where s i  X  X  X  X  X  X  denotes the 
In fact, this is an interactive and propagating process. Table 3 shows three Formula 2 illustrate the interactive process, while Formula 1 and Formula 2 illustrate the propagating process. To discover attribute similarity dependency, we employ Markov Logic Networks(MLNs)[11] to model the graph-like dependency. The basic similarity dependency using this model is presented. 3.2 Markov Logic Networks MLNs are a combination of probabilistic graph model and first-order logic to handle the uncertainty in the real world. In a first-order logic, if a world violates one constraint it will have probability zero. MLNs soften the constraints of the first-order MLNs is a more sound framework since the real world is full of uncertainty and violation. In MLNs, each formula has an associated weight to show its importance: higher the weight is, the greater the difference in log probability between a world that satisfies the formula. 
MLNs is a template for constructing Markov Network [ 10 ]. With a set of formulas and constants, MLNs define a Markov network with one node per ground atom and one feature per ground formula. The probability of a state x in such a network is: where Z is a normalization constant, n i ( x ) is the number of true groundings of formula F probability of all the predicates. More details of MLNs is discussed in [11]. 3.3 Automatic Formulas Generation The formulas of first-order logic are constructed using four types of symbols: constants, variables, functions, and predi cates. The predications are the three logic values {Y, M, N}. The constants are the attributes of one domain. In existing works(e.g. [ 19 ]), the formulas of MLNs are written manually by experts. It is time-consuming and difficult to find the important formulas. Alternatively, we generate formulas automatically based on the following formula templates. following form: where a i is an attribute, m is the number of attributes are involved in this formula. The example formulas shown in Table 2 are generated with the formula templates, and their weights are trained with MLNs. A large number of formulas will be produced by Eq. (2) 
The dependencies of two attributes are found first, then the dependencies of i attributes are found by extending the dependencies of i -1 attributes. The whole process of the formulas generation includes two stages. In the first stage, all formulas involving two attributes, such as  X  X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X  X  X  X  X  X  , are exhausted as the initial formula set, and further, weights are assigned by MLNs through training. In the second stage, high-weight(&gt;1) formulas are selected, and the two formulas whose query predicates are same are merged into one two-evidence-predicate formula. And further three-evidence-predicate formulas are generated by merging two-evidence-predicate formulas an d one-evidence-predicate formulas. In the traditional way, the training set for n web databases is times the training set for two web databases. It is not practical to label such large a training set. To reduce the labeling cost, this section proposes an efficient approach for building the universal matcher for n web databases. 
The basic idea of our approach is that, given n web databases { W 1 , W 2 ,..., W n }, the initial matcher is built for W 1 and W 2 , and then it is evolved into a new one when W 3 is incorporated. The evolvement process stops until all web databases have been incorporated. The flowchart of building the universal matcher is shown in Fig. 2, which consists of three stages. The rest of this section introduces them in turn. 4.1 Training Set Generation Two types of training sets are generated: labeled training set for Stage II and unlabeled labeled as matched or not. For each record pair, one record is from W 1 and the other is different web databases except W 1 and W 2 . All the record pairs have been processed by the uniform representation component and the record similarity improvement component and transformed into the form of the example shown in Table 1. 4.2 Initial Matcher Building MLNs to build the initial matcher. The evidence predicates are the shared attributes of the two web databases, and the only query predicate is the match decision for the record pairs. The formulas can be represented as follows: where a i is an attribute of the two records shared, S i  X  X  X  X  X  X  . The record pair ( r 1 , r 2 ) is matched if Match( r 1 , r 2 ) is true, otherwise not matched. Considering the efficiency, the low-weight formulas (say, less than 0.3) can be pruned at this stage because they have little contributions to the performance. 4.3 Matcher Evolving propose an evolving strategy to reduce the labeling cost. As shown in Fig. 3, the evolving strategy is a cycle process. In each round, some informative samples are selected automatically by the current matcher and are labeled. And then the matcher is improved with the labeled record pairs. The process stops until the matcher cannot be improved anymore. from the unlabeled training set for labeling. The informative samples can be classified into two types. The first type is the record pairs that contain new attributes, and the second type matcher is in the range [  X  X  X  X  X  ](  X  X  X  X  X  X  X  X  ) , which means this sample cannot be determined by the current matcher with high probabilities. To avoid too many samples are round is: where  X   X  are  X  at the t th round, TS lab is the current labeled training set (because some labeled samples are appended to TS lab in each round), and are the positive samples and the negative samples of TS lab respectively, P t and P t-1 are the probabilities round respectively. When  X   X  X  X   X  X   X  , the evolving process stops, which means the current matcher be improved when more samples are labeled. 5.1 Experiment Setup Our data set includes three popular domains: movie, book, and research paper. For each domain, we select five popular we b databases. Table 4 shows the details about the date set. The records in the da ta set are extracted by the customized wrappers for the web databases. The whole data set is divided into two parts averagely. One is as the training set, and the other is as the test set. record pairs and the total number of matched record pairs, and recall is defined as the total number of correctly matched record pairs and the total number of actual matched record pairs. F-measure is the harmonic mean of precision and recall . 5.2 Performance The performance of our approach is evaluated on the three domains respectively, and the order of web databases coincides with the order shown in Table 2. We also that in Fig. 1 for each domain is trained. For the attributes of person name and attributes, the traditional edit distance is adopted. We use the popular tool Weka to learn the thresholds of the attributes. The experimental results are shown in Fig. 5. We explain the experimental results on two aspects. (1) The performance of our approach is much better than the rule-based approach on all the three measures. This proves our approach two domains. The main reason is that spelling errors in research paper domain are much less than the other two domains. Since once a spelling error occurs, the attribute similarity is  X  X  X , and the duplicates this record will have little chance to be matched. 5.3 Evaluation of Record Similarity Improvement The goal of this experiment is to evalua te the contribution of record similarity improvement to the performance of the matcher. We turned off the record similarity improvement component is and then carry out the experiment again. As it can be seen improvement component in all the three domains(especially book domain). 5.4 Labeling Cost the labeling costs on the three domains. y axis refers to the number of labeled samples. From the experimental results indicate that the number of labeled samples drops significantly as the evolving rounds increasing. When the evolving rounds are more than 9, no samples need labeling for research paper domain (the evolving process stops), while only about 100 samples are required to label for other two domains. In addition, the curve of book domain is not as regular as those of other two domains. This phenomenon is caused by two reasons. The first reason is spelling spelling-error record pair is labeled,. Second, the involved attributes of unlabeled training set contain some new ones compared to those of labeled training set, which makes more samples be labeled at the first time of the evolving process. 5.5 Scalability Our ultimate goal is  X  X ne matcher, one domain X , i.e. only one duplicate identification matcher is competent for one domain. In this way, no training is needed when new web databases are incorporated. To verify this hypothesis, we use the record pairs experimental results in Fig. 5 are very close to those in Table 5. This indicates that the matcher can still achieve a good performance without training when a new web database( W 5 ) is incorporated. Duplication identification has been a thriving area of data integration surveyed in [9] and [7]. Previous researches mainly focused on similarity functions. They can be classified into two kinds according to the tasks they engaged in. Attribute similarity . The most common reason of mismatches is the presentation variations of shared attributes. Therefore, duplicate detection typically relies on string comparison techniques to deal with presentation variations. Multiple methods have been developed for this task. Edit distance and its variations [9], jaccard similarity, tf-idf based cosine similarity [10] are the most popular similarity functions. Some of them are semantic-specific, such as the Ja ro distance [6] and Jaro Winkler distance [7] for person names, and functions used by tools such as Trillium [8] for addresses. Duplicate iden tification. The records usually consist of multiple attributes, making the duplicate detection problem more complicated. There are multiple techniques for duplicate detection, such as Bayesian-inf erence based techniques[16], active-learning based techniques[2], distance based techniques[4], rule based approaches[8], etc. A recent trend is to investigate algorithms that compute the similarity join exactly. Recent advances include inverted index-based methods[14], prefix filtering-based techniques[3,18], and signature-based methods[1]. Most of them focused on designing an appropriate similarity function. However, no similarity function can conciliate all presentation variations of an entity(record). Bayesian-inference-based techniques are the most similar approach with ours. They also represent record approach. But in the context of deep web integration, many web databases will incur more presentation variations, which makes this task very challenging and further impacts on the accuracy of inference. Ou r approach supplements  X  X aybe X  to accept the uncertainty and eliminate the uncertainty as far as possible by exploring the similarity dependency among attributes. 
Overall, our solution includes both the two tasks above, and the closely coupled solutions are proposed for them respectively. The main differences of our solution and with predefined variation rules instead of traditional similarity functions. Second, we first discover and exploiting the similarity dependency among attributes, while previous works loose insight of this. web data integration. The proposed solution can build one universal matcher for multiple web databases in one domain instead of ones. We believe this is the first try to address the duplicate identification problem by building one universal matcher. Our solution shows better performance when web databases have little spell errors and have rich schemas. This work was supported in part by the China Postdoctoral Science Foundation funded project under grant 20080440256 and 200902014, NSFC (60833005 and 60875033), National High-tech R&amp;D Program (2009AA011904 and 2008AA01Z421), the Doctoral Fund of Ministry of Education of China (200800020002), and National Development and Reform Commission High-tech Program of China (2008-2441). The authors would also like to express their gratitude to the anonymous reviewers for providing some very helpful suggestions. 
