
School of Computing, College of Arts and Science, Universiti Utara Malaysia, Sintok, Kedah, Malaysia Center for Arti fi cial Intelligence Technology, Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia, Bangi, Selangor, Malaysia Graduate School of Business, Universiti Kebangsaan Malaysia, Bangi, Selangor, Malaysia 1. Introduction interesting rare patterns hidden in the large volume of textual documents. Text deviations have often knowledge that distinctively deviate from the general information contained in textual documents.
Retrieving and mining relevant information from vast amount of text is a daunting task due to the lack of formal structure in the documents. A great challenge in this area is to represent text with a document and the query as vectors of term weights. One limitation in the vector-space model is that the term weights are determined heuristically. Attempts are also made to represent text using N-grams as reported in [3]. However, both the vector space and N-grams represent words in isolation without documents using graphical text representation such as Conceptual Graphs (CG) [4 X 7], Formal Concept Analysis (FCA) [8,9], Concept Frame Graphs (CFC) [10] and Ontology [11].

Among these methods, CG has gained considerable attention due to various reasons: i.e. fi rstly, it represent accurate and highly structured information beyond the keyword approach [12] and fourthly, both semantic and episodic association between words can be represented using CGs [13]. Considering its potential, CG is employed in this work to successfully capture the structure and semantics of the extracted information. One distinguishable difference of our work is we model complete sentences to conquer the meaning. Each sentence in a document expresses a unique concept through a particular arrangement of terms. The meaning of sentences is essential in detecting sentence deviations.
Representing the extracted text using CG effectively captures the structure and semantics of the sentences but it brings out an important issue; the NP-complete problem of graph matching. Most to detect deviating knowledge from text represented as conceptual graph interchange format (CGIF). CGIF is a standard for CG notation in linear form and it is intended for easy transfer of CGs between systems. We opt to use CGIF for the reason of easy storage and transfer of CGIF knowledge base for future usage.

The transformation of sentences into CGIF starts with sentence parsing which is implemented using the Link Grammar Parser (LGP) [14]. Next, the general English grammar rule is referred to develop CG CGIF. Next, a deviation based method, which implements a new error-tolerance dissimilarity algorithm is proposed to identify the deviating CGIF. The proposed method embeds synonyms into the CGIFs and uses a standard CGIF in the comparison. Hence, it is far less computationally demanding compared to namely; text representation schemes that capture semantics and the complexity of current graph mining methods.

Experimental evaluation using real world textual datasets reveals that the proposed method accurately detects the deviating knowledge. As a comparison, two other concept similarity methods that employ results show that the proposed method outperforms the others with an improved accuracy comparable to level that the produced results are statistically signi fi cant.

The rest of this paper is arranged in accordance to the following sections. Section 2 details out the motivation and contribution of the work. Section 3 presents a brief overview of conceptual graph evaluation and results are presented in Section 7 followed by some concluding remarks in Section 8. 2. Motivation and contribution
In general, the problem of deviation detection demands distinctively dissimilar approachesengagingon documents. Further review on graph based representation reveals that the most prevailing problem in representing text as graph is the problem of graph comparison that can become NP-complete. Deviation portray large differences between deviating and normal data. Further review on deviation based method that is able to cater real world noises.

As a solution to the above problems, this research is focused on developing a deviation detection of complete sentences to conquer the meaning that each sentence represents as opposed to modeling documents, phrases or words. A computational linguistics-based method speci fi cally deep parsing is this article since they have been discussed in our previous published paper [15,16]. The meaning of sentences is essential in detecting sentence deviations. Therefore, synonym is embedded in the CGIF. To alleviate the complexity of graph matching, standard CGIF is introduced and a matching function is performed on the CGIF to effectively detect deviations.

Our previous work in [17] presents the dissimilarity algorithms for deviation detection without noise The work presented here is an extension of the work reported in [17] where in the previous work the comprehensive and includes the big picture of our proposed deviation detection method. As a summary, the combination of rule-based information extraction, deep parsing, graph based text representation and a deviation based method that proposes a synonym embedded standard CGIF with error tolerance dissimilarity function collectively uncover interesting contributions of this research. 3. Fundamentals of conceptual graphs connected, bipartite (involving two elements: concepts and relations) graphs. A graph is comprised of concept nodes and relation nodes. The concept nodes r epresent concepts such as entities, attributes, The arcs are used to link the concept nodes to the relation nodes.

An example of CG to represent the sentence  X  X he directors submit their report with the audited accounts of the company X  is shown in Fig. 1. As shown in Fig. 1, the concept nodes are drawn as a box circle.

CG can also be represented in linear form for ease of reference and storage as shown below: [submit]  X  edges. E = { e 1 ,e 2 ,e 3 ... e in V . Therefore for the given example, the formal de fi nition of CG is as follows:
G = ( V,E ) where: V = { director, submit, report, account, audited, company }
New graphs can be created by either generalizing or specializing from existing graphs. A number of can be performed on the produced CG. Additional information such as descriptions and the organization analysis. CG is proven to be competitive and more expressive than the logic-based method [18]. 4. Related works 4.1. Deviation detection in text
Conventional deviation detection methods developedfor structured categorical data as reviewed in [19] sparseness and temporal aspects of textual data. The methods developed for this problem can be classi fi ed into two broad approaches which have statistical and machine learning basis [20 X 22]. Two main paradigms exist in the statistical approaches; parametric and non parametric methods [20]. hidden markov model [29 X 31] assume that the data are distributed according to a certain distribution any distribution. Mixture models like the Gaussian mixture models and the EM algorithm require too much training data in order to perform well. Bayesian network performs well for text categorization of modelling more than one state and needs segmented training data which are too expensive.
Non-parametric methods do not rely on probabilis tic distribution of the data. One example of non-parametric method is statistics which are based on ranks [32 X 45]. Other examples include histogram are one of the most basic non-parametric approaches. In such a method a similarity measure commonly (IR) models (e.g. variants of term frequency  X  inverse document frequency models). Although previous when the document is decomposed into sentences [37]. Other similarity measures which are based on scheme.

Although non-parametric approaches are appealing s ince the correct probab ility distribution is not and ranking, however this method depends closely on the text representation scheme used. Statistical sparse data whereas k-means method requires optimal value of k which is not easy to derive.
Both parametric and non-parametric methods apply statistical inference test to a given data which dimensional distributions.

The machine learning approaches try to automatically acquire knowledge from training data or anal-ysis of empirical data. Machine learning approaches can generally be classi fi ed into supervised and unsupervised learning [48]. Supervised learning involves learning a model from given examples. Super-Neural Network [49] and Support Vector Machine [24,28,50] are the most common supervised machine learning methods used to solve the text deviation detection problem.

Unsupervised learning methods are clustering based method [24,51 X 53], deviation based method [54] and Self Organizing Neural Networks [55]. Self Organizing Neural Network is able to adapt to new clusters. Apparently, these approaches are slow since we do not know how the data are clustered and Furthermore, most cluster-based algorithm relies on some distance computation between data items They perform mining tasks on conceptualgraphs through various comparisons, conceptualclustering and the development of conceptual hierarchies. The limitation of this method is in the comparison process which becomes polynomial as the size of data increases.
 function to identify deviations by examining the main characteristics of objects in a group. Objects 4.2. Measuring graph similarity NP-complete problem of graph mat ching. The initial comparison m ethod for CG as introduced in [13] CG and the execution time is at the best NP-complete [59]. Due to the above reasons, most researchers have a tendency to apply a simpler method to measure CG similarity by focusing on graph uni fi cation and intersection operations.

The graph matching approach employed in [60] where CG is used to represent source code is divided into various measures including associating weights, similarity between concepts, expanding concept nodes and measuring similarity of the extended concepts. Furthermore, they also calculate the type of becomes polynomial and involves large number of parameters. In [61], the authors proposed a CG matching algorithm that detects the semantic similarity between concepts and relations. This method hierarchy respectively. Even though their method combines syntactic and semantic context information, the computational complexity of their algorithm is polynomial
According to van Rijsbergen [62] similarity is a measure of the association or relatedness between objects characterized by discrete-state attributes. Some popular similarity measures are the dice and proportion of (weighted) words two texts have in common versus the words they do not have in common. comparison in the evaluation process of our method. For easy reference we named this method CG-dice. In [8,63], the Tversky X  X  model are used as the basis of developing a model to measure the similarity between graphs. Tversky X  X  model is based on set theory and enables the measurement of similarity of concepts on the large contexts using uni fi cation of sets. We refer to this method as FCA-RS and is explained in Section 4.2.2. This method is also used as a comparative method to evaluate our proposed method. In the remainder of this section, we brie fl y review CG-dice and FCA-RS 4.2.1. CG-dice
In this method, the overlap between two conceptual graphs is measured by considering both concept nodes and relation nodes. The similarity between two conceptual graphs G 1 and G 2 is measured by the
Conceptual similarity s
And relational similarity s where G 1 is conceptual graph 1, G 2 is conceptual graph 2, G concept nodes of graph G , m arcs in the immediate neighbourhood of the graph G calculated using Eq. (3). This is done because, the relational similarity s a zero value, but s should not be zero when s degree of connection of the elements of G calculated using Eq. (4).
The coef fi cient b =1  X  a . The result from using this method is the cumulative similarity s (where 0 &lt;s 1) for each comparison. The higher values indicate similarity; hence if we use the scores to identify deviations, the deviations are marked by smaller values. 4.2.2. FCA-RS A study on a similarity measure for Formal Concept Analysis (FCA) is based on Tversky X  X  model and of this method to our proposed method is on the similarity function which is developed for FCA and considers concepts and attributes using a variation of Tversky X  X  model. The model does not include an In our work,  X  is set to 0.5 to give equal emphasize to concepts and relations. 5. Proposed CGIF representation
The proposed CGIF representation introduces the concept of standard CGIF and synonym embedding in the graphs. 5.1. Standard CGIF for graph matching
Graph matching is a complex task when adopting graph based representation because the execution time is at the best NP-complete [59]. To resolve this problem the proposed CGIF representation aprede fi ned standard produced by the Malaysian Government Authority, Bank Negara Malaysia (BNM) licensed Islamic banks. The GP8i is analysed and standards related to the performance indicators are extracted and parsed. The standard CGIFs are created from the extracted standard sentences contained in GP8i . When standard CGIF is used to identify deviating graphs, the number of comparison increases linearly with the number of CGIFs. If there were no standard CGIF, the graphs need to be compared among each other. Hence, without the use of standards, the number of comparison becomes exponential. 5.2. Synonym embedding in CGIF
Synonyms are different words with similar meanings. For example, the word  X  amount  X  can also calculate the semantic similarity of two words. The embedding of synonyms in this work is considered CGIF implies that the different terms used to convey the same meaning in textual documents can be CGIF representation proposed in this work is embedded with synonyms.
 do not include synonym embedding. The effort and cost of constructing synonym list for all generated CGs are extremely high. To resolve this problem the embedding of synonyms in this work is performed only on the standard CGIFs. Using standard CGIF which is equipped with synonym lists, synonym standard CGIF.
 5.3. CGIF de fi nition and notation
The CGIF is proposed in [64] as a standard representation of conceptual graphs. It is intended for The syntax for CGIF is de fi ned using Extended Backus Normal Form (EBNF) rules and meta level conventions [13,64]. In this work, the CG represents relationships between words. The vertices represent either concepts or conceptual relations and the edges are connections between them. This original notion as proposed in [13,64]:
The concepts are represented by square brackets, and the conceptual relations are represented by concept that the character string de fi nes.
 are regarded as important indicators to measure company performance. 5.3.1. Original notions
The following notations are based on original notations proposed in [64] with index tailored to the problem domain and the type of data it represents.
 De fi nition 1. &lt; concept list &gt;:: = { [concept CGIF where: concept concept in the &lt;concept list&gt;.
 ferred*d: X   X  X  } respective identi fi ers; a, b, c and d .
 given CGIF where: the identi fi er of the second concept the relation relates to. Example 2. &lt;relation list&gt; = { ( agt ?d ?c )( were ?d ?b )( agt ?d ?a ) } connects concept d to a .
 De fi nition 3. G where: performance indicator i .

ACGIF, G performance indicators on various years. Each G indicator on the given year, each subsequent sentence is numbered with x Example 3. G 161 = { [total assets*a: X   X  X  [amount*b: X   X  X  [total liabilities*c:  X   X  X  [transferred*d: X   X  X  } ( agt ?d ?c )( were ?d ?b )( agt ?d ?a ) } The index given to each graph G depends on the problem domain and can be changed according to the information that each graph represents. 5.3.2. Proposed notions: With synonym list
The following notations are the proposed notations for the proposed standard CGIF. The original notations are enhanced with additional embedding of concept synonyms in the standard concept list set of the notation.
 De fi nition 4. &lt; standard concept list &gt;:: = { [concept of all concepts in the standard CGIF where: concept and may include lemmatized words of the concept.
 ry over reassign shift]] }
Example 4 presents the four concepts shown in Example 1 with their respective synonymlist. Note that some concepts do not have any synonym therefore their respective synonym list is empty. For example; there are no synonyms for the concept of total assets as shown in this example.
 De fi nition 5. SG where: i is the performance indicator identi fi er.

A standard CGIF, SG relations which relates concepts within each sentence. Each SG consists of all possible synonyms of the concept and may include lemmatized words of the concept. Example 5. SG 1 = { [total assets*a: X   X  X  ]] [amount*b: X   X  X add up quantity sum total]] [total liabili-( agt ?d ?c )( were ?d ?b )( agt ?d ?a ) }
Example 5 represents standard CGIF for performance indicator 1, i.e. total assets. This standard CGIF consists of four concepts with its respective synonym lists and three relations. 6. The proposed error tolerance dissimilarity function ( CG-etf )
As has pointed out in Section 2, many related works on graph based deviation detection are computa-our proposed method uses the basic construct of a deviation based method which is the dissimilarity function. Compared to other similarity or dissimilarity measures the proposed dissimilarity function measure with a proposed error tolerance factor ( etf a major advantage of the proposed method compared to other works in the area that use graph based as important in [65].
 symmetric difference of sets and maximum degree of graphs. Here, the concepts of, error tolerance factor, symmetric difference between graphs, maximum degree relations and dissimilarity function are introduced. The section continues to discuss the algorithms to implement the introduced concepts and ends with a step by step example on how the Cg-etf compares two given CGIF. 6.1. Error tolerance factor
The error tolerance factor ( etf represents the degree of acceptable error to smooth out the dissimilarity of D ( G it indicates how much the dissimilarity between the CGIFs can be reduced by removing one unmatched concepts or relations from the comparison. n is the number of possible unmatched concept allowed for of the compared conceptual graphs. 6.1.1. Symmetric difference but not in both. Therefore the de fi nition of the symmetric difference between a given CGIF, G corresponding standard CGIF, SG G are not elements of SG SG or SG 6.1.2. Maximum degree In graph theory, a degree is a measure of immediate adjacency [66]. The degree, d v in a graph G is the number of edges incident to v . The maximum degree  X ( G ) of a graph G is the in the symmetric difference of G given in Eq. (7)
Let E be the relation edges in G
Therefore the degree of each concept vertex v in the set of edges E as given in Eq. (8) and is denoted by dG of the concept vertices in G over all concept vertexes in G decreasing order (e.g. dG
With the de fi nition of Symmetric difference and Maximum degree, the value of etf can be calculated with Eq. (9). where: n = { 1,2, ... K } . K is the number of concept vertex that has the maximum degree of relation edges in the symmetric difference graph G difference graph G removed from the union graph of G 6.2. Dissimilarity function with embedded etf
Basedonthede fi nition of etf a given standard conceptual graph SG SG index n .

The dissimilarity function presented above indicates that the dissimilarity between any two CGIFs is between 0 and 1 indicates the degree of dissimilarity between CGs. 6.3. Algorithms the steps to compute the dissimilarity score between compared CGIF with its corresponding standard to accomplish the task of deviation detection between CGIFs.

The algorithm begins by i nitializing the CGIFs to represent each sentence. It indexes the CGIF for corresponding SGi is retrieved. In the next step a generalization function is performed on the CGIF. This is done by matching the concepts from the G iyx with the concepts and synonyms of the SG andalsointhe &lt;relation list&gt; .

Next step in this algorithm is to perform a matching process of the G iyx and SG factor, Algorithm 2 is devised to calculate etf
Algorithm 2 begins by fi nding the symmetric difference between the compared CGIF, G iyx and the standard CGIF, SG difference between G each vertex nodes in the symmetric difference of G iyx and SG etf example of how to calculate the dissimilarity score and the error tolerance factor. 6.4. Example
This section gives a simple example in order to understand the implementation of the proposed total assets: The standard sentence extracted from GP8i for performance indicator 1: The example sentence is transformed into CGIFs: obtained: The vertex set of the symmetric difference of G 151 adSG 1 is denoted by: The edge set of the symmetric difference of G 151 adSG 1 is denoted by:
For each concept vertex V ( G 151  X  SG 1 ) , fi nd the degree of vertex denoted by d ( v
The degree sequence, ds maximum degree of graph G 151  X  SG1 Therefore,  X  ( G 151  X  SG 1 )= 4isfor ds 1 and ds 2 Since the |  X  ( G 151  X  SG 1 ) | =2= K and n K ,then n = { 1,2 } , etf can now be calculated using Eq. (9):
With the value of etf as such: 0.9 is de fi ned this sentence will be regarded as deviation.
 To smooth out the dissimilarity score even further, the etf 2 can be used be regarded as deviations.

From the above example, the dissimilarity between two CGIFs can be directly calculated using the This method is convenient enough to calculate the dissimilarity between two complex sentences in the large context. 7. Evaluation
This section presents the evaluation of the proposed method. Here, we compare the results of CG-etf to domain expert X  X  judgment. In addition, we also compare CG-etf with CG-dice and FCA-RS which explanation of the dataset and the process of transforming text into CGIF. 7.1. Dataset and transforming text into CGIFs 909 pages with approximately 163,000 words. Given the above document collection, the set of CGIF into CGIF are explained in [16]. Figure 2 illustrates an example of these processes. 7.1.1. Extracting relevant sentences is performed on the documents with an inte grated development environment named VisualText .The coding is done with NLP++ programming language. The whole process can be seen as a step by step documents.
 dates. Next, the documents are zoned into paragraphs, headers, sentences, and table zones. Zoning fa-This further improves the process of fi nding the required information.
 to be searched and extracted. The results are extracted sentences that contain relevant performance to [15]. 7.1.2. Transforming extracted sentence into CGIF
The extracted sentences are parsed in order to reveal the underlying structure. We employ LGP [14] to to conceptual graphs [67]. Suchanek et al. [68] report that the LGP provides a much deeper semantic which represents sentences; NP represents Noun Phrases; VP represents Verb Phrases and PP represents Preposition Phrases.
 The produced sentence structure is traversed from its roots to generate the CGIF. The Standard following the CGIF notation as explained in Section 5. The constructed CGIF can be manipulated directly to perform deviation detection using our proposed method described in Section 6. For the creation of standard CGIF, the same process is performed on the standard sentence extracted from the BNM guideline; GP8i . 7.2. Experimental settings
This section explains the setting up of experiments and the choice of evaluation measures that are benchmark to compare the performance of our method and the other compared methods. To assess the scores produced using CG-dice and FCA-RS. A comparison graph is plotted to show the results.
The precision, recall and F-measure are calculated to compare each method against the actual devi-and retrieved deviations is the number of deviations retrieved by the method. The F-measure combines precision and recall where F-measure = 2  X  precision  X  recall precision
Another way to evaluate the performance of certain method as opposed to a baseline method is to judgments. For every dissimilarity scores produced by method A coef fi cient, r to an expert dissimilarity score B where A is the mean score of method A and B in a tabular form.

In addition, we analyse the business performance of the company by calculating the fi nancial ratios dissimilarity scores produced by Cg-etf.
 results have occurred by chance. These results are shown in a tabular form. 7.3. Experimental results
The baseline results are obtained by giving the same set of sentences to human experts. One important statements. Table 1 presents the deviating sentences picked up by the experts and the accompanying reason why these sentences are picked as deviations.

The dissimilarity scores produced by Cg-Etf, Cg-dice and FCA-RS are compared to the baseline expert scores. The graph in Fig. 3 shows the dissimilarity scores for all compared methods.
The above graph clearly shows that our method, Cg-etf is strongly correlated to the human evaluation since it fails to identify any sentence as deviation.
 ment for the comparison. The correlation coef fi cient is calculated to measure how well the compared method correlates with expert judgements. Table 2 shows the calculated scores.

Typically the most important measure of performance is recall , which is a measure of completeness As such Cg-etf records a recall of 100% while CG-dice is 16%. FCA-RS has a recall of 0% which means none of the deviations are correctly identi fi ed using this method.

For the precision scores, Cg-dice has 100% precision. Precision is a measure of accuracy. That is, if a method has extracted the correct deviations, then it has high precision. However, the method is 75%. F-measure combines the precision and recall scores and provides a clear-cut measurement. The highest F-measure is 86% which is of Cg-etf. This is followed by 28% for Cg-dice and 0% for FCA-RS. Each method yields highly different F-measure scores. One may question the reliability of the scores for Cg-etf, Cg-dice and FCA-RS are 98%, 55% and 47% respectively. Now, there is not much difference between Cg-dice and FCA-RS, however a higher score is recorded for the proposed method Cg-etf. This further strengthens the advantage of the proposed error tolerance dissimilarity function correlate with the expert judgement with scores of 96 X 99%.

To further evaluate our results we have calculated the annual fi nancial ratios of ROA, ROE and EM using the extracted numerical values. These ratios are used to plot a 2-y axis line graph to compare for each fi nancial ratio. ROA and ROE are the indicators measuring managerial ef fi ciency. ROA is of these indicators shows higher managerial performance. Lowest ROA and ROE values are recorded compared to the less popular EM ratio. EM measures the amount of assets per equity capital. A higher indicates greater risk for a bank. The highest EM value is recorded for the year 2006. Similarly, our dissimilarity scores are higher for that year.

The results show that our method can be used to detect the performance trend by discovering deviating is low for a speci fi c year.

The simple difference in the dissimilarity scores is not reliable enough to determine the degree of samples t-test. Table 3 shows the result of the t-test for the method comparison.

The calculated t-value for all the comparison is greater than the critical value in the 0.001 (99.9%) dissimilarity function used caused the differences observed (with 99.9 + %con fi dences). 8. Conclusion
The experimental results are very encouraging. The proposed deviation mining method outperforms problem of processing textual documents by restricting the search space. To capture the semantics of sentences, we prove that exploiting whole sentences and representing them with CGIFs renders signi fi cant improvement. Although our implementation of CG does not exploit its full potential, the experimental results show that our method performs substantially better than compared methods for deviation mining task. One reason for our success is in the fact that too much contextual knowledge reduces the effectiveness of similarity measures.

To reduce the complexity of graph matching, we propose the use of standard CGIF to identify deviating graphs. Hence, the number of comparison in our method is n times, where n is the number of CGIFs. If there were no standard CGIF, the graphs need to be compared among each other. Hence, the number of becomes exponential as the number of CGIFs increases. This exponential computation is solved with the use of standard sentences in Cg-etf where each CGIF is compared with one standard; thus the number of comparison becomes linear as the number of CGIF increases. Therefore, the proposed method is considered extremely suitable for large datasets.

Besides that, our proposed method includes a specialized dissimilarity measure that considers both concepts and relations equally. With respect to other similarity measures for CGs, this method has depicted a higher correlation with human experts. One important advantage of this method is we have explicitly embedded concept s ynonyms into the conceptual gra phs. This enables the semantic degree of dissimilarity between CGIF are smoothed with the introduction of an error tolerance factor. It smoothes the dissimilarity function by removing a number of unmatched concepts and relations from the comparison. This enables the comparison to be lenient enough to support real world subjective sentences.
 The practical implication of the proposed method is the low computational costs of graph matching. performance of the companies. In fi nancial statements, the tabular formatting of text and numeric together with heavy usage of fonts, colours and graphics have presented an extra challenge. Although application for other domains.

Despite the fact that the experimental results are favourable, there are some areas on which further CG encapsulates all forms of knowledge but for practical reasons, we have only captured the concepts encountered is when processing dataset that does not have any standards. A possible solution for this problem would be to devise a machine learning method which can scan the dataset and create standard data.
 be more meaningful if the performance of the proposed method is evaluated on other similar documents with some variations or entirely different domain. This would further strengthen the fi ndings.
Finally, the evaluation of the method is done using a relatively small number of performance indi-cators which results a small number of constructed CGIFs. The reason for processing such amount of to extend the evaluation on larger data. Besides the aforementioned suggestions, there are much more work can improve the method signi fi cantly.
 References
