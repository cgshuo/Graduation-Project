 Longitudinal analysis is important in many disciplines, such as the study of behavioral transitions in social science. Only very recently, feature selection has drawn adequate atten-tion in the context of longitudinal modeling. Standard tech-niques, such as generalized estimating equations, have been modified to select features by imposing sparsity-inducing regularizers. However, they do not explicitly model how a dependent variable relies on features measured at prox-imal time points. Recent graphical Granger modeling can select features in lagged time points but ignores the temporal correlations within an individual X  X  repeated measurements. We propose an approach to automatically and simultane-ously determine both the relevant features and the relevant temporal points that impact the current outcome of the de-pendent variable. Meanwhile, the proposed model takes into account the non-i.i.d nature of the data by estimating the within-individual correlations. This approach decomposes model parameters into a summation of two components and imposes separate block-wise LASSO penalties to each com-ponent when building a linear model in terms of the past  X  measurements of features. One component is used to select features whereas the other is used to select temporal con-tingent points. An accelerated gradient descent algorithm is developed to efficiently solve the related optimization prob-lem with detailed convergence analysis and asymptotic anal-ysis. Computational results on both synthetic and real world problems demonstrate the superior performance of the pro-posed approach over existing techniques.
 G.1.6 [ Numerical Analysis ]: Optimization X  Gradient meth-ods ; H.2.8 [ Database management ]: Database Applica-tion X  Data mining  X  Correpsondence should be adressed to Jinbo Bi.
 c  X  Algorithms, Performance, Experimentation Longitudinal modeling; regularization methods; sparse pre-dictive modeling; regression
A longitudinal study collects and analyzes repeated mea-surements of a set of features for a group of subjects through time. Longitudinal analyses are important in many areas, example, to predict binge drinking of college students, a longitudinal study may be designed to monitor them weekly or even daily in terms of multiple covariates, such as, the level of stress, status of negative affects and social behav-iors [4, 1]. The fluctuation of these covariates is used to analyze and predict binge drinking (the dependent or out-come variable) of a student at the current observation time point. Changes of the covariates in the proximal time points are anticipated to alter the likelihood that a student binge drinks at the current observation point. To precisely under-stand how covariates affect the outcome, the analysis has to model not only the current values of the covariates but also their proximal values as well as take into account the correlation structure in the repeated measurements.
Typically, longitudinal data are analyzed by extending generalized linear models (GLM) with different assumptions, such as marginal models, random effects models, and tran-sition models [6]. For example, a marginal model regresses the outcome on the current observation of features but fac-tors in a within-subject correlation matrix that is estimated for a few proximal time points. In contrast, a random ef-fects model reflects the variability among individuals rather than the population average comparing with marginal mod-els. For marginal modeling, generalized estimating equa-tions (GEE) are the most widely used methods which esti-mate a predictive model to predict the current outcome to-gether with correlations among different outcomes observed temporally. The resultant predictive models are generally more accurate than those of classic regression analysis that assumes independently and identically distributed ( i.i.d. ) observations [12]. Research on feature selection in longi-tudinal data leads to a new family of methods based on the penalized GEE (PGEE)[8]. For random effects models, gen-eralized linear mixture model(GLMM)[11, 15] is the major method. It explores natural heterogeneity across individuals in the regression coefficients and represents this heterogene-ity by a probability distribution.

None of those extensions of GLM aim to detect causal re-lationships from temporal changes of covariates to the out-comes of the current effect. In many studies, it is however necessary and insightful to model simultaneously the corre-lation among outcome records and the lagged causal effects of covariates [1]. For example, psychologists have identified that there is lagged effect in the alcohol use behavior. An individual X  X  drinking today may be a response to an elevated level of stress two days back rather than the current day. It is actually an important question for psychologists to find out both which temporal points and which covariates influ-ence the current outcome the most. This lagged effect is not used by temporal marginal modeling to make predictions.
On the other hand, researchers have developed machine learning approaches for longitudinal analysis that predict an outcome using feature values at multiple time points [2, 13]. For example, graphical Granger modeling [2], and grouped graphical Granger modeling[13] are insightful to explore the influences from past temporal information present in time series data in the modeling and understanding of the causal relationships. These methods assume that past values of cer-tain time series features causally affect an outcome variable, and hence construct a model based on these values to predict future outcomes. Often, they estimate causality relationship (causal graph) among all features. However, these methods assume i.i.d. samples which are clearly violated in longitu-dinal data, and moreover they are incapable of selecting the most influential time points.

All existing methods either assume i.i.d. samples in Granger causality modeling or assume correlated samples but do not model temporal causal effects. Therefore, we propose a new learning formulation that constructs predictive models as functions of covariants not only from the current observation but also from multiple previous consecutive observations, and simultaneously determine the temporal contingency and the most influential features. The proposed method has the following advantages: 1. The proposed method makes predictions based on lagged 2. The proposed method also learns simultaneously a struc-3. We develop a family of methods where the outcome 4. We provide the convergence analysis in Section 3.1 and We have empirically compared the proposed method against the state of the art on both synthetic and real world datasets. The computational results demonstrate the effectiveness and the capability of our approach.
 Figure 1: The outcome y t at time t can be relevant to multiple covariates x 1 ,x 2 ,  X  X  X  ,x d observed at current and several previous time points t  X  1 ,t  X  2 ,  X  X  X  ,t  X   X  , which forms a data matrix X (left). If we associate with each entry of this matrix a weight in our ad-ditive prediction model, then our model coefficients form a matrix W (right). If the coefficient matrix is sparse, then the resultant model will be selective in terms of covariates and time points.
In our approach, the predictive model takes the form of the trace of the product of the lagged data X and the model coefficient matrix W as shown in Figure 1. The model coef-ficients are organized into a matrix rather than a vector used in traditional analysis because this way reflects the structure in the lagged data. Note that the lagged observations of y can also be included in the data matrix X to be used in the predictive model. For notational convenience, we just use X to represent the data that are used to form the model.
We first briefly review two most relevant sets of longitu-dinal analytics in Section 2.1 which will help elucidate the advantages of our proposed formulation.
We introduce the notation that is used through out the paper. A bold lower case letter denotes a vector, such as v . The k v k p refers to the ` p norm of a vector v , which is formed of v and d is the length of v . A bold upper case letter de-notes a matrix such as M . Similarly, m ( i, ) , m ( ,j ) represent the i -th row, j -th column and ( i,j )-th component of M , respectively. The Frobenius norm and ` p,q norm of a matrix M refer, respectively, to k M k F , which is equal to ( tr ( M &gt; M )) 1 / 2 , and k M k p,q , defined by P n where n is the number of rows in M , and tr ( M ) indicates the trace of M . We assume that vect( M ) is the column-major vectorization of M , which is defined as vect( M ) =  X  M 1 , M 2  X  is the inner product of two matrices M M 2 that is computed as the inner product of vect( M 1 ) and vect( M 2 ). The operator reshape( v ) re-shapes v into a ma-trix of a proper size determined by the specific context.
Assume that we are given data of m number of individu-als on d number of features (independent variables) that are repeatedly measured at n i time points for each individual i . The data of each individual i is represented by a matrix X of individual i at time point t . Without loss of generality, we assume that all individuals have data at the same con-secutive time points ( n i = n ) to simplify the notation and the subsequent analysis. Data on the dependent variable (outcome) is also given in y ( i ) of length n that contains the observations at the n time points for individual i . Typically, a longitudinal study aims to estimate the effect of covariates on the dependent variable. The notion of Granger Causality was introduced by the Nobel prize winning economist, Clive Granger, and has proven useful in time series analysis [10]. It is based on the intu-ition that if a time series variable causally affects another, the past observations of the former should be useful in pre-dicting the future outcome of the latter.

Specifically, a time series observation x is said to Granger cause another time series outcome, y , if the regressing for y in terms of past y and x is significantly better than the regressing just with past values of y . The so-called Granger test first performs two regressions: the past observations, and then uses a hypothesis test such as an F-test to determine if the outcome y t can be pre-dicted significantly better from the past covariate x . Recent graphical Granger models [2, 13] extend it from a single time series covariate x to multiple covariates X . They learn the coefficients a and w  X  X  with LASSO type of regularizers and evaluate if coefficients are non-zero for Granger causality.
GEE estimates the parameters of a GLM while taking into account the correlations in the training examples. Similar to GLM, it assumes that the dependent variable comes from a class of distributions known as the exponential family. For each member in this family, there exists a link function that can be used to translate the nonlinear model into a linear model. The expectation of the outcome y ( i ) t for subject i at time t is computed as: where  X  ( i ) t represents the mean model, g  X  1 is the inverse of a link function g in a GLM [14], and  X  ( i ) t = x ( i ) t variance of y ( i ) t is computed as var( y ( i ) t ) = var(  X   X  is a scaling parameter that may be known or estimated.
GEE presumes a so-called working correlation structure, typically denoted by R (  X  ), where  X  is a parameter to be de-termined from data. The common choices of R (  X  ) include exchangeable, tri-diagonal and the first-order autoregressive (AR(1)) formula [12]. The exchangeable correlation struc-ture, also called equi-correlation , assumes that corr ( y  X  for all t 6 = t 0 . The tri-diagonal structure uses a tridiagonal erwise. The AR(1) formula assumes a correlation structure along continuous time, and uses corr ( y it ,y it 0 ) =  X 
To estimate the regression coefficients w , GEE uses the the estimating equations that are formulated, in general, by setting the derivative of an appropriate loss function to 0. Although a loss function may not be explicitly written out, the estimating equations always can be computed by where the n  X  d matrix D ( i ) =  X   X  ( i ) / X  w where  X  ( i ) all  X  ( i ) t ,  X  t = 1 ,  X  X  X  ,n into a vector, s ( i ) = y n  X  n matrix  X  ( i ) is the estimated covariance structure as: where A ( i ) is an n  X  n diagonal matrix with var(  X  the t -th diagonal element. Algorithms are given in [12] to compute w and  X  for the different choices of R (  X  ).
In our approach, each training example consists of the cur-rent and  X  previous records of the repeated measurements. Let be a d  X  (  X  + 1) data matrix for subject i . Given T total measurements for each subject, the index t of X ( i ; t ) from  X  + 1 in order to have enough previous observations in the first training example. Hence, there are totally n = T  X   X  training examples for each subject. If X ( i ; t ) previous  X  + 1 values of y ( i ) as a feature, then the model y t = tr X gives the same model like Eq.(1) in the graphical Granger models.

The Granger models would assume that the training ex-amples are i.i.d. . However, the consecutive examples are not mutually independent because they contain overlapping x t  X   X  +1 ). GEE provides a mechanism to estimate the sam-ple correlation simultaneously while constructing predictive models, and to extend the linear models to generalized lin-ear models. To apply GEE to our model, we replace  X  used in GEE by the following formula Substituting Eq.(5) for  X  in Eq.(2) yields a formulation sim-ilar to GEE. The regression coefficients W can be estimated through the well-developed GEE estimators. In particu-lar, the quasi-likelihood methods of GEE estimate W by minimizing a loss function that is defined via the model deviance. The model deviance measures the difference be-tween the log-likelihood of the estimated mean model  X  ( i ) and that of the observed values y ( i ) . For instance, the model deviance for a linearly regressive response is written contains the observed responses for subject i , and  X  ( i ) estimated expectations of y for subject i . If the response follows an arbitrary distribution, the model deviance may not correspond to an explicit function. For the exponential family, it takes a special form as discussed in Theorem 1 be-low, which is still complicated. We denote by Dev ( i ) ( W ,  X  ) the deviance occurred on subject i . GEE minimizes a loss function of P m i =1 Dev ( i ) ( W ,  X  ) for the optimal W by solv-ing the estimating equations , i.e., taking the derivatives of the loss function and setting them to 0.

Now, to select among features and discover the most in-fluential time points in predicting y over time, (and also to control the model capacity,) we apply regularizers to the model parameters. We first decompose W into a summa-tion of two components as W = U + V and apply different regularizers to U and V . The block-wise LASSO, such as the ` 1 , 2 matrix norm, is widely-used in multi-task learning or feature selection with group structures, but has not been explored within the GEE setting. To the best of our knowl-edge, it has not been studied in longitudinal analytics how to produce shrinkage effects simultaneously on both features and contingent temporal records through proper regulariza-tion. The general ` 1 ,p matrix norm [23] calculates the sum of the ` p norms of the rows in a matrix. Regularizers based on the ` 1 ,p norms encourage row sparsity by shrinking the entire rows to have zero entries.

In our parameter matrix W , rows correspond to features and columns correspond to the observation time points. If we apply the ` 1 , 2 norm to U (row-wisely), the optimal so-lution of U will contain rows with all zero entries. Thus, a selected subset of features in the  X  + 1 observations will be used in the predictive model to predict the current outcome. The ` 1 , 2 norm of V &gt; (column-wisely) encourages to select among columns of V . If the k -th column of V contains the largest values in the selected columns, the current outcome is most contingent on the previous ( k  X  1)-th record, thus having the ( k  X  1)  X  X agged X  effect. Overall, we solve the fol-lowing optimization problem for the best model parameters W which is computed as U + V : min where W in the deviance is simply replaced by U + V .
The optimization of Eq.(6) is challenging. In general, even solving the GEE formulation is not easy as it estimates not only the model expectation but also the variance term  X  ( i ) . The algorithm that solves the GEE (i.e., the esti-mating equations) applies the Newton-Raphson method in the iterative reweighted least squares (IRLS) procedure [8] to estimate w and  X  ( i ) . However, this method does not solve any formula that uses regularizers. By modifying the Newton-Raphson method or shooting algorithm [8], it can be extended only to the regularizers that are decomposable into individual parameters w j . For instance, the ` 1 vector norm of w can be decomposed into the summation of individual | w j | , j = 1 ,  X  X  X  ,d . The ` 1 , 2 matrix norm, unfortunately, can not be decomposed in such a way. Therefore, we have devel-oped an accelerated gradient descent method based on the fast iterative shrinkage-thresholding algorithm (FISTA) [3]. Further, the following theorem shows that Eq.(6) is a convex optimization problem in terms of W . Our algorithm can be proved to find the global optimal solution W of Eq.(6) when  X  is fixed (to a consistent estimate given by GEE).
Theorem 1. The first term of Eq.(6) is convex and con-tinuously differentiable with respect to U and V if the dis-link function is continuous.

Proof. First, let us recall that the probability density function of a distribution in the exponential family takes the following form: specified for each member of the exponential family, and  X  is a parameter in the mean as defined in Eq.(2). Typically, a t (  X  ) =  X  . Then, the deviance of the exponential family can be computed as where  X   X  ( i ) t denotes the true value under a saturated model,  X   X  t denotes the fitted values of the model. Thus,  X   X  that b ( X   X  ( i ) t ) is a convex function on the natural parameter space H = {  X   X  | b (  X   X  ) &lt;  X  X  [19]. Thus, the deviance contains either linear terms or a convex term with respect to  X   X  . In our model (5),  X   X  is linear with respect to W . Hence, the deviance term in Eq.(6) is convex with respect to U and V .
Moreover, it is true that b 0 ( X   X  ( i ) t ) =  X   X  ( i ) t is the inverse of a continuous link function [19]. The first term of Eq.(6) is continuously differentiable with respect to U and V . Thus, theorem 1 holds.
To solve Eq.(6), we design an alternating optimization algorithm that alternates between optimizing two working sets of variables: one set consisting of U and V and the other consisting of  X  . (a) Find U and V when  X  is fixed
When  X  is fixed, the objective function of Eq.(6), denoted by f ( U , V ), is convex with a continuously differentiable part ` ( U , V ) that is the deviance and a nonsmooth part R ( U , V ) that constitutes the two regularizers. We hence have We develop a FISTA algorithm in the following iterative procedure to find optimal U and V .

Denote the iterates at the k -th iteration by U k and V k with respect to U and V , respectively, For any given point (  X  mal map for the non-smooth R
Q L,  X  U ,  X  V ( U , V ) = ` (  X  U ,  X  V ) + R ( U , V ) If ` ( U , V ) has Lipschitz continuous gradient with Lipschitz modulis L . Then, according to the Lemma 2.1 in [3], the inequality holds indicating that Q L,  X  U ,  X  V ( U , V ) is the upper bound of f ( U , V ).

Starting from an initial point ( U 0 , V 0 ), we iteratively search for the optimal solution. At each iteration k , we first (at the first iteration, (  X  U 1 ,  X  V 1 ) = ( U 0 , V 0 where t k is a scalar and updated at each iteration as: Then, we solve the following problem for a solution ( U k , V k ), where  X  U ` k and  X  V ` k are respec-tively the partial derivatives of ` computed at (  X  U k , L acts as a learning step size.
 Since there is no interacting term between U and V in Eq.(9), the problem can be decomposed into two separate subproblems as follows: min min The two subproblems share the same structure and thus can be solved following the same procedure. Hence, we only show how to solve (10) for the best U .

Eq.(10) is equivalent to the following problem after omitting constants, and this problem has a closed-form solution where each row of U k , U k ( i, ) is: and P ( k ) =  X  U k  X  1 L  X  U ` k . The gradient vector  X  the gradient of the deviance) can be computed by Eq.(3) with the fixed  X  , i.e.
In the above discussion, the Lipschitz modulus L is com-puted and given. However, the calculation of L can be com-putational expensive. We therefore follow the similar ar-gument in [9] to find a proper approximation L k at each iteration k starting from L 0 &gt; 0. Recall that the Lipschits constant L is defined: where  X  max (  X  ) indicates the maximum singular value of the Hessian of ` . Decompose the Hessian matrix  X  X  X  ` W | W  X  0 into M &gt; M where M  X  R d (  X  +1)  X  q and q is the rank of the Hessian matrix. We have an upper bound of L as follows:
We use the upper bound  X  L in Eq.(13) as L in our itera-tions. Using this upper bound may increase the number of iterative steps for convergence. Algorithm 1 summarizes the steps for finding optimal U and V with fixed  X  .
 Algorithm 1 Search for optimal U and V with fixed  X  Input: X , y ,  X  ,  X  1 ,  X  2
Output: U , V 1. k = 1, compute  X  L and initialize t 1 = 1, U 0 =  X  U and V 0 =  X  V 1 = 0 ; 2. Solve Eq.(9) to obtain U k and V k . 3. Compute t k +1 by Eq.(8). 4. Compute  X  U k +1 and  X  V k +1 by Eq.(7). 5. k = k + 1.

Repeat 2  X  5 until convergence. (b) Find  X  when U and V are fixed
When U and V are fixed, the regularizers no longer ap-pear in the objective of Eq.(6). Eq.(6) is degenerated into just the GEE formula with  X  as the variables. Hence,  X  can be estimated via the standard GEE procedure, i.e., from the current Pearson residuals defined by: where  X  ( i ) t,t is the t -th diagonal entry in the matrix  X  The specific estimator of  X  depends on the choices of R (  X  ). This GEE-based procedure has been shown to find a consis-tent estimate of  X  [12].

Let N = mn be the total number of training examples, and p = d (  X  + 1) be the practical number of parameters in W . A general approach to estimating R is given by: for j = 1 ,  X  X  X  ,n , and k = 1 ,  X  X  X  ,n . In addition, the scaler parameter  X  in Eq.(4) can be estimated as follows: Algorithm 2 depicts the overall procedure for solving Eq.(6). Algorithm 2 Main algorithm -Jointly select features and temporal points Input: X , y ,  X  1 ,  X  2
Output: U , V 1. Set R (  X  ) = I ; 2. Solve for U and V using Algorithm 1. 3. Estimate  X  using a proper estimator in [12] and com-pute R (  X  ) by Eq.(14) and  X  by Eq.(15).

Repeat 2  X  3 until convergence.
We provide a convergence analysis for Algorithm 1 and an asymptotic analysis for the proposed formulation.
We show that Algorithm 1 converges to the optimal solu-tion with a convergence rate of O (1 /k 2 ). The proof follows largely the arguments in [3]. We only provide a sketch here.
Theorem 2. Let U k and V k be the pair of the matrix generated by Algorithm 1. Then for any k  X  1 f ( U k , V k )  X  f (  X  U ,  X  V )  X  where (  X  U ,  X  V ) is a globally optimal solution of Eq.(6).
Proof. We start with defining the following quantities where  X  U 1 = U 0 ,  X  V 1 = V 0 , and subsequent  X  U k defined by Eq.(7). Following the proof of Theorem 4.4 in [3], in the first iteration, given t 1 = 1, we have a 1 = 2 L b = || U 1  X   X  U || 2 F  X  X | V 1  X   X  V || 2 F . We show that a by applying Lemma 2.3 in [3], which yields Reorganizing the above inequality yields Thus, a 1 + b 1  X  c holds.

Then, according to Lemma 4.1 in [3], we have for every k  X  1, a k  X  a k +1  X  b k +1  X  b k , together with a 1 + b which derives into the following inequality, Therefore, we obtain that Given t k is updated according to Eq.(8), it is easy to show that t k  X  ( k + 1) yields By the Remark 3.2 in [3] and the inequality (13), we also know that an upper bound of L k is  X  L . Hence, f ( U k , V k )  X  f (  X  U ,  X  V )  X  In our algorithm, we set L k =  X  L,  X  k .

Remark 1. The loss function, ` ( U , V ) , of an exponen-tial distribution has Lipschitz continuous gradient within the range {|| U || 1 , 2  X   X  1 , || V &gt; || 1 , 2  X   X  2 } where  X  stant values in terms of  X  1 , X  2 , respectively to guarantee the non-trivial step size  X  L . Otherwise, it may lead to a sub-optimal solution.
To facilitate the asymptotic analysis, we re-write the no-tation as follows: let and where one block X i ; t corresponds to U and the other to V . Then, correspondingly, we have  X  ( i ) t = ( h ( i ) f ( U , V ) can be re-written as f (  X  ) = ` (  X  ) + R (  X  ;  X 
Solve Eq.(6) yields a solution to the penalized estimating equations: assuming  X  1 =  X  2 =  X  for notational convenience which will not change the property. Given our model definition timating functions in GEE [12] whereas the second term corresponds to the regularizers. The asymptotic property of Eq.(6) can be naturally derived from the results in [12] which have proved that the estimating equations L (  X  ) = P i ( D of  X  . We extend the same argument to our formulation Eq.(6) in Theorem 3 under the following regularity condi-tions: H ( i ) is bounded, and lim m  X  X  X  ( P i H ( i ) ) /m = H also not singular Moreover, L (  X  ) is twice continuously differentiable with re-spect to  X  , and  X  X / X   X  is positive definite.

Theorem 3. Assume that: (1)  X   X  is a consistent estima-tor given  X  ; (2)  X   X  is a consistent estimator given  X  ; and (3) the tuning parameter  X  m = o ( larity conditions listed above, optimizing Eq.(6) yields an asymptotically consistent and normally distributed estima-tor  X   X  , that is: where  X   X  is the true model coefficients in a model of E ( y g matrix (see [12] for details of  X  ).

Proof. Multiplying 1 /m to both sides of Eq.(17) yields an estimate of  X   X  that is asymptotically consistent with  X  Since our regularizer R (based on the ` 1 , 2 matrix norm) is Lipschitz continuous, its partial derivative  X  X  (  X  ) / X   X  is bounded. The second term of Eq.(18) vanishes when m  X   X  , and thus the conclusion holds.
 Recall how  X   X  and  X   X  are estimated in the proposed method. Those estimates from the Pearson residuals are consistent. Thus, the estimate  X   X  in the proposed method is asymp-totically consistent and normally distributed according to Theorem 3.
The purposed algorithm is suitable to optimize any loss function that has Lipschitz continuous gradient. In this sec-tion, we discuss that three exemplar exponential families: Gaussian, Bernoulli, and Poisson, satisfy the Lipschitz con-dition. We specify how to compute the gradient of the loss function for these distributions. The gradients will instanti-ate (and replace) Eq.(12) used in our algorithm.
If the outcome follows a Gaussian distribution, then the outcome y is linearly regressive in terms of the covariates in the observations. The mean and the conditional covariance of y with a working correlation structure R (  X  ) are calcu-lated as: so the gradient  X  U ` k in Eq.(12) at the k -th iteration can be computed as be similarly computed. Hence, the gradient is linear in terms of  X  , and thus Lipschitz continuous.
If the generalized variables  X  follow a Bernoulli distribu-tion and the outcomes are binary variables. The relationship between the outcome and covariates can be learned by a lo-gistic regression which is a special case of the GLM with the Bernoulli assumption. Hence, the mean and the conditional covariance of y with the working correlation structure R (  X  ) are formulated as
The gradient  X  U ` k in Eq.(12) can be written as:  X  ( i ) (  X  U k ). The gradient  X  V ` k can be similarly computed.
If the generalized variables  X  follow a Poisson distribution and the outcomes contain count values. The relationship of the outcome and covariates is learned by a Poisson regres-sion. The mean and the conditional covariance of y with the working correlation structure R (  X  ) are formulated as ent  X  U ` k can be computed using the general formula Eq.(12). The loss function of Poisson regression does not have glob-ally Lipschitz continuous gradient. But the regularized loss function is equivalent to requiring the constraints, || U ||  X  and || V &gt; || 1 , 2  X   X  2 [17] for appropriate values of  X   X  2 that are determined according to  X  1 and  X  2 . The loss function of Poisson regression does have Lipschitz continu-ous gradient within the confined region.
We validated the proposed approach by comparing it to several most relevant and recent methods. Three GLM-based [16] methods: GEE [12], GLMM [11, 15], and RE-EM tree 1 [18] were compared. The recent graphical Granger modeling 2 [13] and a support vector machine based method called CSVM were also used. RE-EM tree and graphical Granger modeling could only be applied to regression prob-lems (linearly regressive data from Gaussian distributions), and CSVM was only suitable to classification tasks (logis-tically regressive data from Bernoulli distributions). We named our approach by LGL (longitudinal group lasso). The normalized mean squared error (nMSE), which is the MSE divided by the variance of y [22, 9], was used to measure regression performance. The area under the ROC curve (AUC) [5] was used to measure classification performance. An R package is available in the Comprehensive R Archive Network (CRAN) downloaded from the author X  X  website http://www-bcf.usc.edu/  X  liu32/code.html Figure 2: The model constructed by our approach LGL on a synthetic dataset.
 Figure 3: Comparison between the constructed models by LGL and Granger.
We generated a data matrix X  X  R d  X  Tm from the normal distribution N (0 , 16), where d = 200, T = 30, and m = 400. and  X  = 4 were formed from the matrix X . Then, U and V were generated from the normal distribution N (0 , 49). We set the rows corresponding to features from 1 to 150 in U to zero and the columns 2 and 5 of V to zero, and com-puted W = U + V . The residuals s ( i ) of every subject were generated from a multivariate normal distribution of differ-matrix of the residual followed different working correlation structures R (  X  ) with the parameter  X  = 0 . 64. We generated 9 sets of regression residuals by choosing different combina-tions of the variances and the working correlation structures. Finally, the outcome variables y ( i ) were computed as y The above procedure produced regression data. Using the same data X , the outcome y ( i ) t of a classification problem was generated from the Bernoulli Distribution with B(1 , X  where we used Eq.(19) with the regression y ( i ) to obtain  X  ( i ) . We hence obtained totally 18 synthesized data with 9 datasets for each distribution. We used the 25 early records of each subject to compose the training data and the rest 5 records to form test data.

Table 1 shows the results where we can see that LGL out-performed all other methods on all the simulated datasets. The proposed method with correct correlation assumptions always performed the best. The graphical Granger model-ing performed reasonably well but lacked of consideration of temporal correlation in the consecutive records. When the simulated noise increased, the performance of all methods had dropped as expected. We further demonstrate the se-lected features and temporal contingency. Figure 2 shows the constructed U , V , and W by the LGL on the regression data with the AR(1) covariance structure and N (0 , 3 2 ) resid-ual where darker colors indicate larger values (and white means 0). Most of the features from 150 to 200 were selected in U and the correct columns (i.e., 1 , 3 , 4) were selected in V . We compared our approach with the Granger model that also learned W in Figure 3. Obviously, the Granger model excluded too many variables in the model. These results demonstrate the capability of LGL in terms of simultane-ously capturing the important features and lagged effects.
We tested our approach on two real-world datasets: the college alcohol use dataset; and the national longitudinal survey of youth (NLSY) dataset 3 . All comparison methods were used except GLMM due to its prohibitive computa-tional costs. The college alcohol use dataset consisted of data from 504 college students on 52 variables in a period of continuous 30 days. The 52 variables measured each subject on daily stress, moods, emotion and substance use behav-ior. One of the variables measured the number of night-time drinks, which was our outcome variable, forming a regres-sion problem. We also predicted the binge drinking behav-ior which is defined as having 5 or more night-time drinks, which formed a classification problem. The NLSY dataset consisted of 11 yearly data for 3,376 subjects on 27 variables. The outcome variable measured the number of days that a subject had binge drinking in past 30 days, forming a re-gression problem. The other 26 variables measured features, such as smoking, drug use, family support and education.
For the college alcohol use data, we experimented with and the rest for training. We found  X  = 3 was feasible. Larger  X  would not change the results because the extra time points would be excluded by our model. However, it practically would cut down the sample size of each subject. The parameters  X  1 and  X  2 in our approach and any tuning parameters in other methods were tuned in a three-fold cross validation within the training data. Table 2 shows the re-sults where our approach LGL outperformed other methods in most settings. Among the four different correlation as-sumptions, LGL with AR(1) obtained the best performance on three of the four settings. The results also confirmed that modeling the correlation among repeated observations im-proved prediction performance [12]. We also observed that for instance, 16 out of 51 variables were selected when we used the last 5 days to test binge drinking prediction. Fea-tures related to exited mood, under stress and interacting with friends during night time were the risk factors for binge drinking. The past 3 days were all included in the model, showing there was  X  X agged X  effects in alcohol use. The effect of past days was reduced with prolonged time lag.

For the NLSY dataset, we experimented respectively with using the last one, two and three years from each subject for test and the rest in training. We also considered  X  = 3, which means we used 3 year lagged data to predict the cur-rent year X  X  behavior. All tuning parameters were tuned us-http://www.bls.gov/nls/nlsy97.htm Figure 4: The model constructed by our approach on the NLSY dataset. ing a within-training two-fold cross validation. The results are reported in Table 3. For any assumption of the work-ing correlation structure, LGL had comparative performance with RE-EM tree and consistently outperformed GEE in all of the three experiments. LGL with tri-diagonal correlation performed the best on this dataset. The results here again show that taking care of the correlation among repeated ob-servations improves the performance (given we see that LGL with the independent correlation assumption had the worst performance among all LGL variants).

The gray map of U , V and W constructed by LGL is shown in Figure 4 to illustrate an example for the tri-diagonal working correlation assumption. Out of the 26 features, 12 were selected by LGL and we list them below.
 F2: # days of smoking a cigarette in the past 30 days F3: Received a training certificate or vocational license F7: The grade began during the academic year F8: # months that respondent did not attend school during the academic year F12: The college degree working toward or attained F13: The highest grade completed as of the survey year F15: The highest grade attended as of the survey day F16: The highest grade completed as of the survey day F17: # days of using marijuana in the past 30 days F19: # times of using some drug or other substance right before school or during school or work hours F25: As the victim of a violent crime in the survey year F26: Divorced parents.
 This list shows that a subject X  X  smoking, drug use, educa-tion background and family support influenced his or her drinking behavior. Figure 4 demonstrates that the data in the third prior year might be obsolete to predict this year X  X  behavior as LGL only selected the past two years for use in the model as seen in the plot of V .
We have proposed a new learning formulation for longi-tudinal analytics. Unlike existing methods, the proposed approach can simultaneously determine the temporal con-tingency and the influential features in predicting an out-come over time. The model parameter matrix is computed by the summation of two component matrices: one matrix reflects the selection among covariates; and the other char-acterizes the dependency along the temporal line. More-over, our approach simultaneously models the sample cor-relations in the longitudinal data while constructing a pre-dictive model. The related optimization problem can be efficiently solved by a new accelerated gradient descent al-gorithm. Convergence analysis shows that the algorithm can find the global optimal solution for the model with a quadratic convergence rate. An asymptotic analysis shows that the solution of our formulation is a consistent estimate of the model parameters. Hence, the proposed approach solves an underdeveloped problem -jointly learning the rel-evant features and determining how current outcome relies on past observations. Empirical studies on both synthetic and real-world problems demonstrate the superior perfor-mance of the proposed approach over the state of the art. This work was supported by NSF grants IIS-1320586, DBI-1356655 and NIH grant R01DA037349. Jinbo Bi was also supported by NSF grants IIS-1407205 and IIS-1447711. [1] S. Armeli, T. S. Conner, J. Cullum, and H. Tennen. A [2] A. Arnold, Y. Liu, and N. Abe. Temporal causal [3] A. Beck and M. Teboulle. A fast iterative [4] J. Bi, J. Sun, Y. Wu, H. Tennen, and S. Armeli. A [5] C. D. Brown and H. T. Davis. Receiver operating [6] P. Diggle, P. Heagerty, K.-Y. Liang, and S. Zeger. [7] J. H. Fowler and N. A. Christakis. Dynamic spread of [8] W. J. Fu. Penalized estimating equations. Biometrics , [9] P. Gong, J. Ye, and C. Zhang. Robust multi-task [10] C. W. Granger. Testing for causality: a personal [11] N. M. Laird and J. H. Ware. Random-effects models [12] K. Y. Liang and S. L. Zeger. Longitudinal [13] A. Lozano, N. Abe, Y. Liu, and S. Rosset. Grouped [14] P. McCullagh and J. A. Nelder. Generalized linear [15] C. McCulloch and S. Searle. Generalized, Linear, and [16] U. Olsson. Generalized linear models , volume 18. 2002. [17] M. R. Osborne, B. Presnell, and B. A. Turlach. On [18] R. J. Sela and J. S. Simonoff. Re-em trees: a data [19] T. A. Severini. Elements of Distribution Theory , [20] C. A. Stappenbeck and K. Fromme. A longitudinal [21] L. Wang, J. H. Zhou, and A. N. Qu. Penalized [22] Y. Zhang and D.-Y. Yeung. Multi-task learning using [23] Y. Zhang, D.-Y. Yeung, and Q. Xu. Probabilistic
