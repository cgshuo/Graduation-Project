 A. Zandifar, R. Duraiswami, L.S. Davis Abstract. Detection and recognition of textual infor-mation in an image or video sequence is important for many applications. The increased resolution and capa-bilities of digital cameras and faster mobile processing allow for the development of interesting systems. We present an application based on the capture of infor-mation presented at a slide-show presentation or at a poster session. We describe the development of a sys-tem to process the textual and graphical information in such presentations. The application integrates video and image processing, document layout understanding, op-tical character recognition (OCR), and pattern recogni-tion. The digital imaging device captures slides/poster images, and the computing module preprocesses and an-notates the content. Various problems related to met-ric rectification, key-frame extraction, text detection, en-hancement, and system integration are addressed. The results are promising for applications such as a mobile text reader for the visually impaired. By using powerful text-processing algorithms, we can extend this frame-work to other applications, e.g., document and confer-ence archiving, camera-based semantics extraction, and ontology creation. 1 Introduction One of the goals of computer vision is to develop systems that function in the world by understanding the objects in it and performing tasks such as navigating through it. State-of-the-art computer vision systems today can function somewhat below the capabilities of a lower life form in achieving such computer vision goals. A visually impaired person can often accomplish these functions by using other senses and simple aids. On the other hand, there are some high-level functions that a human being uses vision for that a visually impaired person might have difficulty with. Chief among these are identifying and processing text. Text in vision scenes provides an extremely rich source of already processed information that is often highly relevant to the understanding of the information a literate human or a future information-processing appliance might use to understand the world. This information is widespread in human environments such as merchandise labels, printed instructions, room numbers, street signs, newspapers, articles, and others. scientific and business communication is the use of slide shows and posters. Often, organizations or individuals record these presentations but have no means to index or retrieve these digital images by subject. In both these problems we need to be able to detect and recognize the layout of text in images and make sense of the images. ment of a vision system for the processing of scene text in a relatively restricted context: the processing of im-ages captured in a presentation or a poster session. Our system aims at mapping the layout of a slide or a poster into text and image blocks, performing appropriate recti-fication and image processing of the text blocks, followed by optical character recognition.
 person or for meeting archiving. Text-processing algo-rithms that extract latent semantics [25] have become very powerful. The availability of the text in the pre-sentations (without having access to the digital source slides) can allow these slides to be indexed and retrieved. 2 Scenario and problems In this paper the goal is to change information from one medium (lecture presentation/slide/poster) to another (text and graph bounding boxes followed by OCR). Here we consider that images of slides/posters are taken by a digital camera. These images are composed of text and graphic blocks and background. After image blocks are stored, the rectified text blocks are binarized and passed to OCR software. Finally, we store the detected text and images in a searchable format. Moreover, for recognized text blocks, we include the content and font size informa-tion. Prior knowledge consists of expected image layout since slides/posters consist of text/graph blocks. acter recognition is reliable only if the text blocks are provided in the frontoparallel view. In practice, the im-ages are deformed when the optical axis of a camera is not perpendicular to the presentation/poster surface. Therefore, the challenge is to extract the frontoparal-lel view of the deformed image. This is called  X  X etric rectification. X  In a frontoparallel view, right angles are projected to right angles and parallel lines are projected to parallel lines. For metric rectification, features must be found. Such features can be parallel lines and right angles in the image. Other features can be the text lines. Hence, in Sect. 4.3 we will introduce an automatic and precise line-segment-detection algorithm to detect these features. Then, text and image regions are segmented from rectified images. Before providing text boxes to OCR, we preprocess them to improve OCR output qual-ity. Note that all these problems stated are for one im-age, not an image sequence. In practice, a digital cam-era takes a video of text printed on a surface. A video contains a lot of redundant frames with the same in-formation. Thus the problem is how to extract changes in a video, changes in slide/poster content and not illu-mination change or camera jitter. The schematic of the video-based slide/poster recording framework is shown in Fig. 1. 3 Related work Camera-based document image analysis is addressed in a recent review article [7]. The following papers touch on problems of video analysis of scene text.
 Camera-based acquisition : Mirmehdi et al. [19] ad-dress a simple scheme for auto zoom of a camera. This method is useful if the background around an object has low variance compared to the object. Then, in an observation window, variance is used as an indicator of best zoom. In [31], a video-based interface to access tex-tual information for the visually impaired is discussed, and auto-focusing and auto-zooming algorithms are pre-sented. The best focus is achieved when the edges are strongest in the image. The best zoom is set when the readable font size of a text region is more than the OCR readable font size constraint. We consider this method for preprocessing real-time recorded video content by controlling the zoom and focus of a camera.
 Key-frame extraction : Since in video of lectures tex-tual information does not vary rapidly, we need to detect the changes in video and remove redundant frames. In [29], a simple difference operation is introduced. This algorithm is very accurate for still camera pose and constant illumination conditions. We used the phase-correlation method from the image-registration litera-ture to detect the changes in slide or poster video con-tent. This algorithm is stable under global illumination changes and slight camera jitter [10, 14].
 Metric rectification : The common method in the lit-erature is to extract vanishing lines and right angles in an image [1, 3, 16]. Extraction of vanishing lines is achieved by different methods, such as the projection profile method [3] and the illusory and nonillusory lines in textual layouts [18]. We employ an automatic line seg-ment algorithm for line detection. We cluster the line segments in feature space (edge angle and edge distance as features) using a mean shift algorithm [2]. We im-plement the algorithm of [16, 17], which is suitable for our problem scenario since the image of a poster/slide includes rectangular boxes and lines.
 Text segmentation : There are various text segmen-tation algorithms in the computer vision and document understanding literature that address the following three basic problems: feature extraction, clustering, and vali-dation. For feature extraction, there are different filter-ing methods: steerable pyramid, Laplacian pyramids [8], Gabor filters, etc. Our system uses Gabor filters for the feature-extraction part. For clustering, we employ a K -means algorithm. More generally, a mean shift filter does not require prior knowledge of cluster numbers [2]. In [4], different features from local moments of pixel intensity are used. We use the text segmentation module in [9, 30, 13]. In this paper, we consider the clustering method, al-though for more complex problems [15], learning would be the choice.
 Enhancement : A global thresholding scheme is not ideal for camera-captured images due to lighting vari-ation and complex background [8]. The survey [26] com-pares eleven different adaptive thresholding methods and concludes that Niblack [24] is the best. In this paper, we apply Niblack X  X  method for binarization of text boxes before sending them to OCR.
 Contribution : While many of the individual compo-nents have been described previously, our contribution is the development of a video-based interface, a uni-fied framework to analyze text and graphs printed in video lectures and storing them in a searchable for-mat. Here, our video-based framework provides a more general approach to poster/presentation analysis com-pared to the work of [20, 28]. Furthermore, we provide the qualitative results of the video-based framework in the poster/presentation scenario, which shows the per-formance of the framework. 4 Preprocessing 4.1 Key-frame extraction Since, in a video of lectures, textual information does not vary rapidly, many frames will have the same infor-mation. Therefore, we do not want to waste processing resources on the redundant frames. In [29], a simple dif-ference operation is used on three consecutive frames. The difference between two consecutive frames in time is: where m and n are the pixel dimensions of a frame. Here, we set a frame as a key frame, if: The input to the key-frame-extraction module is a video and the output is a set of sorted frames in time. This algorithm works extremely well if the camera is still and the same illumination condition holds. It often happens that the illumination varies during the lecture presenta-tion and, moreover, there is a slight camera movement while capturing the content. In this case the simple dif-ference algorithm fails. Our solution is to use phase corre-lation [14] for key-frame extraction. This method, which is well known in the image-registration literature, uses the discrete Fourier transform (DFT) of two consecu-tive frames to compute the overlap percentage. Consider two consecutive frames denoted as f 1 = I ( x, y ; t ) and f 2 = I ( x, y ; t + 1). Denote the DFT of these frames as F ( w 1 ,w 2 ) and F 2 ( w 1 ,w 2 ). Then the cross power spec-trum is: If f 2 is a translated version of f 1 , then: where  X  is a constant illumination factor. So the CPM is: Therefore, the inverse of the CPM gives an impulse at ( x, y ) and the impulse height is the amount of nor-malized similarity overlap between f 1 and f 2 (0 corre-sponds to no overlap and 1 to the maximum area over-lap). This method is fast enough for real-time applica-tions and is invariant to constant illumination changes. To suppress the repeating nature of the frequency spec-trum and to give less weight to the boundary pixels, we use a raised cosine function, as a window that smoothly reaches 0 at the boundaries. This spatial filter gives more weight to pixels close to the center of an image than to the boundaries. This spatial filter (Hamming cosine win-dow) is formulated in 1D as: Summarizing the key-frame-extraction method for some threshold tol (we experimentally choose 0 . 2): 1. The first frame is a key frame. 2. While receiving the video sequence do: bust for all types of translations and constant illumina-tion changes. At each time step, we keep only two frames in memory and the process is very fast using the FFT (fast Fourier transform). 4.2 Metric rectification An image of a presentation that is not frontoparallel to the image plane of a camera is deformed due to perspec-tive projection. This distortion is called the keystone ef-fect . This means parallel lines and right angles are not projected as parallel lines and right angles in the image plane (Fig. 2). For planar surfaces the deformation can be modeled by a 3  X  3 matrix, a  X  X omographic transfor-mation, X  that maps the pixels of the unwarped image to the warped image [21]: where H is the homographic mapping, ( u, v ) is the spa-tial location of a pixel in the image of frontoparallel view, and ( x, y ) is the corresponding pixel in the image cap-tured by the camera. Knowledge of at least four corners in the image is enough to estimate the eight unknown parameters of the mapping by least-squares-estimation algorithms (up to scale) [12]. Often we do not have the exact correspondences, and also the corners may not be visible. However, we can use the linear features (lines and right angles) in the image for the rectification process. In presentations, lines and boxes in the image provide such linear clues. In this paper, keystone correction is addressed by estimating vanishing lines and right an-gles. Before describing the algorithm, we review a few definitions from projective geometry.
 Points and lines in homogenous representation : Let l be a line in a 2D plane denoted by ax + by + c =0. A line is represented by ( a, b, c ) T , and if a point ( x, y ) on a plane is represented in homogenous coordinates as x =( x, y, 1) T , then the line equation in the homogenous coordinates is l T x = 0. The intersection of two lines l and l is the point x ; x = l  X  l . The line joining two points x and x is l = x  X  x . Therefore, lines and points are dual in projective geometry.
 Intersection of two parallel lines : Consider two par-allel lines l and l with coordinates of ( a, b, c ) T ( a, b, c ) T . The intersection of two parallel lines is x = l  X  l =( c  X  c )( b,  X  a, 0). Ignoring the scale factor ( c the intersection would be ( b,  X  a, 0) T , which does not be-long to R 2 . In general, the intersection of two parallel lines, an ideal point , is of the form ( x 1 ,x 2 , 0) T . The line at infinity that passes through an ideal point (from the equation l T x = 0) is represented as l  X  =(0 , 0 , 1) T . Transformation of lines :Ifapoint x is mapped by a matrix H to a point x , then we can show x = H x , where H isa3  X  3 homographic matrix, as: where all entries are scaled by h 9 . Therefore, if a point x belongs to a line l , then: and consequently line l is mapped to l by a matrix H  X  T : Vanishing points and vanishing line : In a perspec-tive image of a plane, an ideal point is mapped by a homographic transformation H to a vanishing point. A vanishing line is an image of the line at infinity in the image plane. Figure 3 demonstrates two vanishing points and the vanishing line of a perspectively skewed im-age. Here, we denote the two spaces: affine skewed space and perspectively skewed space E and F , respectively. Therefore, as Eq. 10 shows, we can find a transforma-tion that maps the line at infinity in E to the vanishing line in the image ( F ).
 Decomposition of a projective transformation :It is known that H can be decomposed into S , A , and P (similarity, affine, and projection) matrices [16]. There-fore: H = SAP, (11) where ( l 1 ,l 2 ,l 3 ) is a vanishing line vector in the image plane ( F ), R 2  X  2 is the rotation matrix around the im-age axis of a camera, and s is the isotropic scaling. The interpretation of  X  and  X  is that they specify the image of circular points (see below). Therefore, for the metric rectification, we compute (  X ,  X , l 1 ,l 2 ,l 3 ) T . Circular points : These are points on the line at in-finity that are fixed under any similarity transforma-tion. These points are often called absolute points I and J :(1 ,  X  i, 0) T denoted in the homogenous coordinates (where i 2 =  X  1). These points are the intersection of any circle with the line at infinity and are mapped to (  X   X   X i, 1 , 0) T on the affine plane ( E ) by a matrix A and ( F ) by a matrix ( A  X  P ). Unfortunately, we cannot com-pute circular points directly because they are complex numbers. Instead, we calculate them indirectly through their dual conic representation.
 Absolute conic : It is known that the absolute conic is dual to the circular points as C  X   X  = IJ T + JI T , where C  X  is an absolute dual conic.
 Rectification algorithm : We can solve for the metric rectification in two ways. In the first method, we extract vanishing lines and then at least two right angles for the metric rectification. We then compute the matrix P from the vanishing line and then A from two right angles. In the second method, we extract five right angles (five pairs of orthogonal lines) and solve for the image of absolute dual conics D in the projective plane. D is denoted as: where M and N are images of circular points in the pro-jective plane. Each pair of orthogonal lines places a lin-ear constraint on D . From D entries, the five known un-known parameters (  X ,  X , l 1 ,l 2 ,l 3 ) T are extracted. Based on the angle between lines in projective geometry, we can show that orthogonal lines are conjugate with respect to D . Each pair of orthogonal lines adds a linear constraint on D : for orthogonal lines l a and l b . In Sect. 4.3 we describe the precise line-detection algorithm we use. For more information on the details of the rectification algorithm, we refer the reader to [16].
 faces. These surfaces, applicable surfaces, have special differential geometric properties of vanishing Gaussian curvature at any point and isometry with flat surfaces. We address and develop the 3D structure recovery and unwarping of applicable surfaces using differential geom-etry in [11]. 4.3 Line detection We compute the edge map of the input image using the Robert operator [8], which is thinned by nonmaxima sup-pression [6]. Then, we make a feature vector with com-ponents of edge angles and edge distances. The distance used is that of an edge line segment to the center of the image and the angle is the angle of an edge line segment with respect to the horizontal axis. In feature space, we find the center of clusters using the mean shift algorithm with a large mean shift radius of the kernel [2]. Then, for each set of pixels with a specific label, we relabel each connected component. Now, the angle map and distance map of the edges are recomputed and pixels are reclus-tered with the small kernel radius. At the final stage, we determine endpoints of pixels with the same labels. Af-ter lines are segmented precisely, the dominant direction of the segmented lines is chosen using the histogram of the segmented line angles. Since lines of different domi-nant directions are assumed to be orthogonal, so we rela-bel a pair of orthogonal lines for the metric rectification method either method I or method II. 4.4 Text segmentation and enhancement Unlike scanner-based systems, in camera-based OCR systems the image is low quality and blurred, so the out-put of OCR is poor. The quality of the image is a func-tion of the presentation quality, the camera capturing parameters, camera motion, and so on. Here, we assume that the camera is fixed while capturing a video of lec-tures. Therefore, the challenge is to enhance the image before sending it to OCR. The steps are text segmenta-tion and adaptive binarization.
 filter banks associated with an edge map for text segmen-tation. The Gabor filter method gives both the benefits of Fourier methods and local spatial distribution meth-ods. The feature responses of the filters at each pixel are designed to identify text-bearing regions. Although none of the filters can individually identify text and nontext regions, a concatenation of the filters provides text detec-tion. This method is robust and precise for text segmen-tation in natural scenes, text in different sizes and ori-entations, and complex background. To improve the seg-mentation results, we will later introduce postprocessing algorithms on the output of the text-segmentation mod-ule. A two-dimensional Gabor function g ( x, y ) in polar coordinates can be written as: g ( x, y ;  X  x , X  y ,w, X  )= 1 where  X  x and  X  y are the standard deviations of the Gaus-sian mask in the x and y directions, w x and w y are the center frequencies of the filter,  X  = tan  X  1 ( w y w entation, and w = w 2 x + w 2 y is the radial frequency. Gabor functions with different scales and orientations form a complete but nonorthogonal basis set. Expand-ing an image using this basis provides a localized fre-quency description. In Fig. 4, the filter outcuts in two scales and four directions are shown. One of the Gabor filter X  X  characteristics is its orientation selectivity. As-sume the orientation  X  =  X   X  ; the Gaussian mask filters the image in the  X   X  orientation only and blocks other orientations. For the feature-extraction part, we choose two scales with four orientations (0  X  , 45  X  , 90  X  , 135 each scale. We implement the Gabor filter bank from [27]. To increase the precision of the feature extraction part, we choose the magnitude of the responses for each pixel filtered by a nonlinear soft thresholding function of: where  X  =0 . 2 (experimentally). We associate further a partially redundant feature, a local edge density mea-sure. This feature improves the accuracy and robustness of the method while reducing false detections. Before the clustering step, features are normalized to have zero mean and unit variance [9].
 clustering algorithm to cluster feature vectors. Empiri-cally, the number of clusters (value K ) was set to 3. This value works well with all test images. The cluster whose center is closest to the origin of the feature vector space is labeled as background (there is no significant edge in any orientation and scale if the background is an almost uni-form pattern), while the furthest one is labeled as text. If the background is not stationary or highly textured (as often happens in lecture presentations), we could learn the background and subtract it from the key-frame slide. We do not discuss this here.
 filter to remove small noise due to the nonuniformity of the background. Using a morphological operator (closing with disk), we increase the area of text region candidates. Then we use connected component analysis to label all the text box candidates for future processing. The fi-nal stage of the text-detection module is a validation module that confirms text boxes. To increase the text-segmentation module X  X  precision and efficiency, there are a few heuristics that are helpful in removing the outlier detected text boxes. We can remove the box if: 1. The OCR output is null. 2. The text box area is less than some threshold value. binarization. It is shown in the literature that the global thresholding scheme is not ideal for camera-captured images due to lighting variation and complex back-ground [8]. In the histogram space, the foreground and background density functions are intermixed, so a re-liable decision boundary (global threshold) cannot be achieved. With a wrong threshold, we either lose impor-tant textual information or add more unwanted edges to the OCR. We implemented the Niblack adaptive thresh-olding scheme to binarize each text box extracted by the text-segmentation module [24]. In this algorithm, we compute the local threshold value in a local window as: where M ( x, y ) and V ( x, y ) are mean and variance at each local window size w centered at pixel ( x, y ). The Niblack parameter k is an input parameter to the binarization module. For our system, we set k to  X  0 . 2. 4.5 Structured output In a camera-based presentation analysis framework, we seek an annotating scheme to extract important and compressed information about slides/posters. For the text data embedded in a slide/poster we can recover the font size of each text box (like the algorithm in [31]) and its spatial location. Therefore, we can sort them in a structured format like a PowerPoint presentation, e.g., title, text box, and graph captions, for each box whose coordinate, textual content read by OCR, and font size we record. To find the font size, we calculate the hori-zontal projection profile of a binarized text box. Such a horizontal profile includes pulses (Fig. 5). The average font size is defined as a median over all pulse widths. 5 Implementation issues and results We developed a video-based framework for analysis of presentations. Our integrated system consists of a Sony DFL-VL500 digital camera, Pentium III 866-MHz com-puter. This interface captures a video of lectures or a poster/slide and converts the content to the structured format. Our software is written in MATLAB and uses IPL and the OpenCV library for image processing [22] and Scansoft2000 for OCR [23].
 ulated and real video sequence. In the simulated version, we initially considered all slides of a presentation. Then, we randomly added in between the frames by a random generator, e.g., after the initial frame 1 we add 14 frames with different random uniform illumination of frame 1 after frame 1. The output of this forward simulation was 164 frames. So we applied the key-frame-extraction method and calculated the exact number of key frames, which initially was 10. We applied the same method to the video of lectures and posters with a value of 0.2 for the overlapping percentage factor, and the results were precise and robust.
 poster (one frame extracted from the key-frame extrac-tion) module. We applied the automatic metric rectifi-cation algorithm described in Sect. 4.2. The OCR out-put by Scansoft2000 of the rectified image is shown in Fig. 6. The smaller figures are intermediate steps of the automatic metric rectification. We applied the algorithm to different images under projection and the algorithm worked extremely well if the slide/poster layout con-sisted of text/graph boxes.
 ferent examples; original color images are on the left and the output of the text-segmentation module after the morphological operation on the right. We gathered differ-ent text sizes and orientations on different backgrounds. The first example is for complex backgrounds and highly textured graphs. The second image is of simple graphs and texts. The third image is of the different text orienta-tions on simple background (Scansoft2000 can process up to 30  X  rotation). We converted color images to grayscale images. In all the cases we had the ground truth, and the missing rate was negligible. The text box font sizes in such cases were more than OCR readable font size. In the first example of Fig. 7 the rotated text was not readable by the algorithm. The rotated text in gray color space was not clear from the background pattern. These are the main results of our video-based interface: 1. Automatic metric rectification is possible because in 2. Automatic orthogonal pair detection fails if there is 3. The two methods of metric rectification give the same 4. Key-frame extraction is robust and precise under uni-5. The text-segmentation module works well for differ-6. Our system is capable of reading the textual infor-module. The test data are a collection of 50 posters taken by a Powershot S200 digital camera (image size is 2 megapixels) and 25 presentation videos taken by a Sony DFW-VL500 digital camera (frame size 480  X  640). The hit rate is the correct detection percentage, the false rate is the false detection percentage, and the miss rate is the missing percentage. These are the numbers for Ta-ble 1: for the key-frame-extraction module on the presen-tation videos, the total number of frames was 571 with 8 errors and 12 false frames. In the text-segmentation module, the detected number of words was 2026 with 36 errors and 46 false detected words. Furthermore, in the text-segmentation module, we used the word-based hit/false/missed rate for the evaluation. On the poster database, there were 693 lines with 27 errors and 44 false detected lines. The number of all orthogonal pairs was 138 with 19 error and 23 false detected orthogonal line pairs. We considered the quantitative results only on submodules, because with their failure the metric rec-tification result is no longer valid. For correct answers, the average angle error after rectification compared to the ground truth was very small. 6 Conclusions and future work In this paper, we presented a system to extract textual and graphical information in lecture presentations or posters/slides using video and image processing, optical character recognition (OCR), and pattern recognition. The related computer vision problems were introduced and solved. The results were promising and efficient for the video-based interface. The indexed output is rep-resented in structured format: text, graph, and impor-tance of text in the content. Thus a video of a lecture or slide/poster can be compressed without losing any key information and still be small enough to be retrieved in online environments. The ability to capture and process textual information by a camera-based scanning system has many applications, e.g., mobile text reader for the visually impaired, sign detection and translation, docu-ment and conference archiving, semantic extraction, and so on. In the future, we will add more functionality to the integrated system, e.g., super-resolution enhancement [5] of key frames using temporal data, a robust algorithm to text detection, and graph segmentation algorithms. References
