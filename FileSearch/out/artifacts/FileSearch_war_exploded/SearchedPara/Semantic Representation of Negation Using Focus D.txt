 Understanding the meaning of text is a long term goal in the natural language processing commu-nity. Whereas philosophers and linguists have pro-posed several theories, along with models to rep-resent the meaning of text, the field of computa-tional linguistics is still far from doing this automati-cally. The ambiguity of language, the need to detect implicit knowledge, and the demand for common-sense knowledge and reasoning are a few of the dif-ficulties to overcome. Substantial progress has been made, though, especially on detection of semantic relations, ontologies and reasoning methods.
Negation is present in all languages and it is al-ways the case that statements are affirmative by default. Negation is marked and it typically sig-nals something unusual or an exception. It may be present in all units of language, e.g., words ( in credible ), clauses ( He doesn X  X  have friends ). Negation and its correlates (truth values, lying, irony, false or contradictory statements) are exclu-sive characteristics of humans (Horn, 1989; Horn and Kato, 2000).

Negation is fairly well-understood in grammars; the valid ways to express a negation are documented. However, there has not been extensive research on detecting it, and more importantly, on representing the semantics of negation. Negation has been largely ignored within the area of semantic relations.
At first glance, one would think that interpreting negation could be reduced to finding negative key-words, detect their scope using syntactic analysis and reverse its polarity. Actually, it is more com-plex. Negation plays a remarkable role in text un-derstanding and it poses considerable challenges.
Detecting the scope of negation in itself is chal-lenging: All vegetarians do not eat meat means that vegetarians do not eat meat and yet All that glitters is not gold means that it is not the case that all that glitters is gold (so out of all things that glitter, some are gold and some are not). In the former example, the universal quantifier all has scope over the nega-tion; in the latter, the negation has scope over all .
In logic, two negatives always cancel each other out. On the other hand, in language this is only theo-retically the case: she is not unhappy does not mean that she is happy ; it means that she is not fully un-happy, but she is not happy either .

Some negated statements carry a positive implicit meaning. For example, cows do not eat meat implies that cows eat something other than meat . Otherwise, the speaker would have stated cows do not eat . A clearer example is the correct and yet puzzling state-ment tables do not eat meat . This sentence sounds unnatural because of the underlying positive state-ment (i.e., tables eat something other than meat ).
Negation can express less than or in between when used in a scalar context. For example, John does not have three children probably means that he has either one or two children. Contrasts may use negation to disagree about a statement and not to negate it, e.g., That place is not big, it is massive defines the place as massive , and therefore, big . Negation has been widely studied outside of com-putational linguistics. In logic, negation is usu-ally the simplest unary operator and it reverses the truth value. The seminal work by Horn (1989) presents the main thoughts in philosophy and psy-chology. Linguists have found negation a complex phenomenon; Huddleston and Pullum (2002) ded-icate over 60 pages to it. Negation interacts with quantifiers and anaphora (Hintikka, 2002), and in-fluences reasoning (Dowty, 1994; S  X anchez Valencia, 1991). Zeijlstra (2007) analyzes the position and form of negative elements and negative concords.
Rooth (1985) presented a theory of focus in his dissertation and posterior publications (e.g., Rooth (1992)). In this paper, we follow the insights on scope and focus of negation by Huddleston and Pul-lum (2002) rather than Rooth X  X  (1985).

Within natural language processing, negation has drawn attention mainly in sentiment analysis (Wilson et al., 2009; Wiegand et al., 2010) and the biomedical domain. Recently, the Negation and Speculation in NLP Workshop (Morante and Sporleder, 2010) and the CoNLL-2010 Shared Task (Farkas et al., 2010) targeted negation mostly on those subfields. Morante and Daelemans (2009) and  X  Ozg  X ur and Radev (2009) propose scope detectors using the BioScope corpus. Councill et al. (2010) present a supervised scope detector using their own annotation. Some NLP applications deal indirectly with negation, e.g., machine translation (van Mun-ster, 1988), text classification (Rose et al., 2003) and recognizing entailments (Bos and Markert, 2005).
Regarding corpora, the BioScope corpus anno-tates negation marks and linguistic scopes exclu-sively on biomedical texts. It does not annotate fo-cus and it purposely ignores negations such as (talk-ing about the reaction of certain elements) in NK3.3 cells is not always identical (Vincze et al., 2008), which carry the kind of positive meaning this work aims at extracting (in NK3.3 cells is often identi-cal). PropBank (Palmer et al., 2005) only indicates the verb to which a negation mark attaches; it does not provide any information about the scope or fo-cus. FrameNet (Baker et al., 1998) does not con-sider negation and FactBank (Saur  X  X  and Pustejovsky, 2009) only annotates degrees of factuality for events.
None of the above references aim at detecting or annotating the focus of negation in natural language. Neither do they aim at carefully representing the meaning of negated statements nor extracting im-plicit positive meaning from them. Simply put, negation is a process that turns a state-ment into its opposite. Unlike affirmative state-ments, negation is marked by words (e.g., not , no , never ) or affixes (e.g., -n X  X  , un-). Negation can inter-act with other words in special ways. For example, negated clauses use different connective adjuncts that positive clauses do: neither , nor instead of ei-ther , or . The so-called negatively-oriented polarity-sensitive items (Huddleston and Pullum, 2002) in-clude, among many others, words starting with any-( anybody , anyone , anywhere , etc.), the modal aux-iliaries dare and need and the grammatical units at all , much and till . Negation in verbs usually requires an auxiliary; if none is present, the auxiliary do is in-serted ( I read the paper vs. I didn X  X  read the paper ). 3.1 Meaning of Negated Statements State-of-the-art semantic role labelers (e.g., the ones trained over PropBank) do not completely repre-sent the meaning of negated statements. Given John didn X  X  build a house to impress Mary , they en-code AGENT ( John , build ), THEME ( a house , build ), PURPOSE ( to impress Mary , build ), NEGATION ( n X  X  , build ). This representation corresponds to the inter-pretation it is not the case that John built a house to impress Mary , ignoring that it is implicitly stated that John did build a house .

Several examples are shown Table 1. For all state-ments s , current role labelers would only encode it is not the case that s. However, examples (1 X 7) carry positive meaning underneath the direct mean-ing. Regarding (4), encoding that the UFO files were released in 2008 is crucial to fully interpret the statement. (6 X 8) show that different verb argu-ments modify the interpretation and even signal the existence of positive meaning. Examples (5, 9) fur-ther illustrate the difficulty of the task; they are very similar (both have AGENT , THEME and MANNER ) and their interpretation is altogether different. Note that (8, 9) do not carry any positive meaning; even though their interpretations do not contain a verbal negation, the meaning remains negative. Some ex-amples could be interpreted differently depending on the context (Section 4.2.1).

This paper aims at thoroughly representing the se-mantics of negation by revealing implicit positive meaning. The main contributions are: (1) interpre-tation of negation using focus detection; (2) focus of negation annotation over all PropBank negated sen-tion; and (4) model to semantically represent nega-tion and reveal its underlying positive meaning. 3.2 Negation Types Huddleston and Pullum (2002) distinguish four con-trasts for negation:  X  Verbal if the marker of negation is grammati- X  Analytic if the sole function of the negated  X  Clausal if the negation yields a negative clause  X  Ordinary if it indicates that something is not the
In this paper, we focus on verbal, analytic, clausal, and both metalinguistic and ordinary negation. 3.3 Scope and Focus Negation has both scope and focus and they are ex-tremely important to capture its semantics. Scope is the part of the meaning that is negated. Focus is that part of the scope that is most prominently or explic-itly negated (Huddleston and Pullum, 2002).

Both concepts are tightly connected. Scope corre-sponds to all elements any of whose individual fal-sity would make the negated statement true. Focus is the element of the scope that is intended to be in-terpreted as false to make the overall negative true.
Consider (1) Cows don X  X  eat meat and its positive counterpart (2) Cows eat meat . The truth conditions of (2) are: (a) somebody eats something; (b) cows are the ones who eat; and (c) meat is what is eaten. In order for (2) to be true, (a X  X ) have to be true. And the falsity of any of them is sufficient to make (1) true. In other words, (1) would be true if nobody eats , cows don X  X  eat or meat is not eaten . Therefore, all three statements (a X  X ) are inside the scope of (1).
The focus is more difficult to identify, especially without knowing stress or intonation. Text under-standing is needed and context plays an important role. The most probable focus for (1) is meat , which corresponds to the interpretation cows eat something else than meat . Another possible focus is cows , which yields someone eats meat, but not cows .
Both scope and focus are primarily semantic, highly ambiguous and context-dependent. More ex-amples can be found in Tables 1 and 3 and (Huddle-ston and Pullum, 2002, Chap. 9). Negation does not stand on its own. To be useful, it should be added as part of another existing knowl-edge representation. In this Section, we outline how to incorporate negation into semantic relations. 4.1 Semantic Relations Semantic relations capture connections between concepts and label them according to their nature. It is out of the scope of this paper to define them in depth, establish a set to consider or discuss their detection. Instead, we use generic semantic roles.
Given s: The cow didn X  X  eat grass with a fork , typical semantic roles encode AGENT ( the cow , eat ), THEME ( grass , eat ), INSTRUMENT ( with a fork , eat ) and NEGATION ( n X  X  , eat ). This representation only differs on the last relation from the positive counter-part. Its interpretation is it is not the case that s. Several options arise to thoroughly represent s . First, we find it useful to consider the seman-tic representation of the affirmative counterpart: AGENT ( the cow , ate ), THEME ( grass , ate ), and IN -STRUMENT ( with a fork , ate ). Second, we believe detecting the focus of negation is useful. Even though it is open to discussion, the focus corre-sponds to INSTRUMENT ( with a fork , ate ) Thus, the negated statement should be interpreted as the cow ate grass, but it did not do so using a fork .
Table 2 depicts five different possible semantic representations. Option (1) does not incorporate any explicit representation of negation. It attaches the negated mark and auxiliary to eat ; the negation is part of the relation arguments. This option fails to detect any underlying positive meaning and cor-responds to the interpretation the cow did not eat , grass was not eaten and a fork was not used to eat .
Options (2 X 5) embody negation into the represen-tation with the pseudo-relation NOT . NOT takes as its argument an instantiated relation or set of relations and indicates that they do not hold.

Option (2) includes all the scope as the argument of
NOT and corresponds to the interpretation it is not the case that the cow ate grass with a fork . Like typi-cal semantic roles, option (2) does not reveal the im-plicit positive meaning carried by statement s . Op-tions (3 X 5) encode different interpretations:  X  (3) negates the AGENT ; it corresponds to the cow  X  (4) applies NOT to the THEME ; it corresponds to  X  (5) denies the INSTRUMENT , encoding the mean-
Option (5) is preferred since it captures the best implicit positive meaning. It corresponds to the se-mantic representation of the affirmative counterpart after applying the pseudo-relation NOT over the fo-cus of the negation. This fact justifies and motivates the detection of the focus of negation. 4.2 Annotating the Focus of Negation Due to the lack of corpora containing annotation for focus of negation, new annotation is needed. An ob-vious option is to add it to any text collection. How-ever, building on top of publicly available resources is a better approach: they are known by the commu-nity, they contain useful information for detecting the focus of negation and tools have already been developed to predict their annotation.
We decided to work over PropBank. Unlike other resources (e.g., FrameNet), gold syntactic trees are available. Compared to the BioScope corpus, Prop-Bank provides semantic annotation and is not lim-ited to the biomedical domain. On top of that, there has been active research on predicting PropBank roles for years. The additional annotation can be readily used by any system trained with PropBank, quickly incorporating interpretation of negation. 4.2.1 Annotation Guidelines The focus of a negation involving verb v is resolved as:  X  If it cannot be inferred that an action v oc- X  Otherwise, focus is the role that is most promi-
All decisions are made considering as context the previous and next sentence. The mark -NOT is used to indicate the focus. Consider the following state-ment (file wsj 2282, sentence 16).
The previous sentence is Applied, then a closely held company, was stagnating under the manage-ment of its controlling family . Regarding the first verb ( growing ), one cannot infer that anything was growing, so focus is MNEG . For the second verb ( providing ), it is implicitly stated that the company was providing a not satisfactory return on invest-ment , therefore, focus is A 1 .

The guidelines assume that the focus corresponds to a single role or the verb. In cases where more than one role could be selected, the most likely focus is chosen; context and text understanding are key. We define the most likely focus as the one that yields the most meaningful implicit information.

For example, in (Table 3, example 2) [He] A0 could be chosen as focus, yielding someone can stomach the taste of Heinz, but not him . However, given the previous sentence ( [. . . ] her husband is adamant about eating only Hunt X  X  ketchup ), it is clear that the best option is A 1 . Example (5) has a similar ambiguity between A 0 and A 2 , example (9) between MTMP and A 4 , etc. The role that yields the most useful positive implicit information given the context is always chosen as focus.

Table 3 provides several examples having as their focus different roles. Example (1) does not carry any positive meaning, the focus is V . In (2 X 10) the verb must be interpreted as affirmative, as well as all roles except the one marked with  X   X   X  (i.e., the focus). For each example, we provide PropBank an-notation (top), the new annotation (i.e., the focus, bottom right) and its interpretation (bottom left). 4.2.2 Interpretation of -NOT The mark -NOT is interpreted as follows:  X  If MNEG -NOT ( x , y ), then verb y must be  X  If any other role is marked with -NOT , ROLE -
Unmarked roles are interpreted positive; they cor-respond to implicit positive meaning. Role labels ( A 0 , MTMP , etc.) maintain the same meaning from PropBank (Palmer et al., 2005). MNEG can be ig-nored since it is overwritten by -NOT .

The new annotation for the example (Figure 1) must be interpreted as: While profitable, it (the com-pany) was not growing and was providing a not sat-isfactory return on investment . Paraphrasing, While profitable, it was shrinking or idle and was providing an unsatisfactory return on investment . We discover an entailment and an implicature respectively. 4.3 Annotation Process We annotated the 3,993 verbal negations signaled with MNEG in PropBank. Before annotation began, all semantic information was removed by mapping all role labels to ARG . This step is necessary to en-sure that focus selection is not biased by the seman-tic labels provided by PropBank.

As annotation tool, we use Jubilee (Choi et al., 2010). For each instance, annotators decide the fo-cus given the full syntactic tree, as well as the previ-ous and next sentence. A post-processing step incor-porates focus annotation to the original PropBank by adding -NOT to the corresponding role.

In a first round, 50% of instances were annotated twice. Inter-annotator agreement was 0.72. After careful examination of the disagreements, they were resolved and annotators were given clearer instruc-tions. The main point of conflict was selecting a fo-cus that yields valid implicit meaning, but not the most valuable (Section 4.2.1). Due to space con-straints, we cannot elaborate more on this issue. The remaining instances were annotated once. Table 4 depicts counts for each role. We propose a supervised learning approach. Each sentence from PropBank containing a verbal nega-tion becomes an instance. The decision to be made is to choose the role that corresponds to the focus.
The 3,993 annotated instances are divided into training (70%), held-out (10%) and test (20%). The held-out portion is used to tune the feature set and results are reported for the test split only, i.e., us-ing unseen instances. Because PropBank adds se-mantic role annotation on top of the Penn TreeBank, we have available syntactic annotation and semantic role labels for all instances. 5.1 Baselines We implemented four baselines to measure the diffi-culty of the task:  X  A1 : select A 1 , if not present then MNEG .  X  FIRST : select first role.  X  LAST : select last role.  X  BASIC : same than FOC-DET but only using fea-5.2 Selecting Features The BASIC baseline obtains a respectable accuracy of 61.38 (Table 6). Most errors correspond to in-stances having as focus the two most likely foci: A 1 and MNEG (Table 4). We improve BASIC with an extended feature set which targets especially A 1 and the verb (Table 5).

Features (1 X 5) are extracted for each role and capture their presence, first POS tag and word, length and position within the roles present for that instance. Features (6 X 8) further characterize . A1-postag is extracted for the following POS tags: DT, JJ, PRP, CD, RB, VB and WP; A1-keyword for the following words: any , any-body , anymore , anyone , anything , anytime , any-where , certain , enough , full , many , much , other , some , specifics , too and until . These lists of POS tags and keywords were extracted after manual ex-amination of training examples and aim at signaling whether this role correspond to the focus. Examples of A 1 corresponding to the focus and including one of the POS tags or keywords are:  X  [Apparently] M ADV , [the respondents] A0 do n X  X   X  [The oil company] A0 does n X  X  anticipate  X  [Money managers and other bond buyers] A0  X  He concedes H&amp;R Block is well-entrenched  X  [We] A0 don X  X  [see] V [ Features (11 X 16) correspond to the main verb. VP-words ( VP-postag ) captures the full se-quence of words (POS tags) from the beginning of the VP until the main verb. Features (15 X 16) check for POS tags as the presence of certain tags usually signal that the verb is not the focus of negation (e.g., [Thus] [react] v [
Features (17 X 22) tackle the predicate, which in-cludes the main verb and may include other words (typically prepositions). We consider the words in the predicate, as well as the specific thematic roles for each numbered argument. This is useful since PropBank uses different numbered arguments for the same thematic role depending on the frame (e.g., A 3 is used as PURPOSE in authorize.01 and as IN -STRUMENT in avert.01 ). As a learning algorithm, we use bagging with C4.5 decision trees. This combination is fast to train and test, and typically provides good performance. More features than the ones depicted were tried, but we only report the final set. For example, the parent node for all roles was considered and discarded. We name the model considering all features and trained using bagging with C4.5 trees FOC-DET .
 Results over the test split are depicted in Table 6. Simply choosing A 1 as the focus yields an accuracy of 42.11. A better baseline is to always pick the last role (58.39 accuracy). Feeding the learning algo-rithm exclusively the label corresponding to the last role and flags indicating the presence of roles yields 61.38 accuracy ( BASIC baseline).

Having an agreement of 0.72, there is still room for improvement. The full set of features yields 65.50 accuracy. The difference in accuracy between BASIC and FOC-DET (4.12) is statistically significant (Z-value = 1 . 71 ). We test the significance of the dif-ference in performance between two systems i and j on a set of ins instances with the Z-score test, where and  X  In this paper, we present a novel way to semantically represent negation using focus detection. Implicit positive meaning is identified, giving a thorough in-terpretation of negated statements.

Due to the lack of corpora annotating the focus of negation, we have added this information to all the negations marked with MNEG in PropBank. A set of features is depicted and a supervised model pro-posed. The task is highly ambiguous and semantic features have proven helpful.

A verbal negation is interpreted by considering all roles positive except the one corresponding to the focus. This has proven useful as shown in several examples. In some cases, though, it is not easy to obtain the meaning of a negated role.

Consider (Table 3, example 5) P&amp;G hasn X  X  sold coffee codes P&amp;G has sold coffee, but not to airlines . How-ever, it is not said that the buyers are likely to have been other kinds of companies. Even without fully identifying the buyer, we believe it is of utmost im-portance to detect that P&amp;G has sold coffee . Empir-ical data (Table 4) shows that over 65% of negations in PropBank carry implicit positive meaning.
