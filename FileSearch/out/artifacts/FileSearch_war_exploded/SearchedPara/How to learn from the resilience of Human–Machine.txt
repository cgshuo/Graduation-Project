 1. Introduction
Resilience is related to the ability of a material to recover from a shock or disturbance. Resilience is a relatively new field of research, although the concept has been used in physics for Charpy impact tests throughout nearly all of the XXth century.
The concept of resilience has also been developed in the field of ecology and is used to characterise natural systems that tend to maintain their integrity when subjected to disturbances ( Ludwig communities and has been applied to psychology, psychiatry ( Gousse  X  , 2005 ), sociology, economy, biology ( Orwin and Wardle, et al., 2007 ; Nakayama et al., 2007 ; Luo and Yang, 2002 ), and automation ( Tianfield and Unland, 2004 ; Neema et al., 2004 ; Numanoglu et al., 2006 ).

Psychological resilience is linked to the invulnerability theory ( i.e. , the positive capacity of people to cope with trauma and to bounce back). Biological or ecological resilience is based on the theory of viability ( i.e. , the ability for an organism to survive after disruption). The resilience of industrial systems is linked to on-line safety management, faced with known or unknown situations. It differs from the traditional off-line safety analysis process, which aims at foreseeing undesirable situations and proposing schemes to avoid their occurrence or protect the system from their consequences. From the organisational and safety management viewpoints, resilience is the capacity of a system to survive, adapt and face unforeseen changes, even catastrophic incidents. There are many formal definitions of resilience, but most of them suppose the existence of functional capacities in order to make a system resilient: the capacity to recognise, adapt to, and absorb changes.

When a Human X  X achine System (HMS) does not have suffi-cient resources or competences to control such functions, it cannot be resilient, or its resilience may be managed by another
HMS. Another strategy can be applied: learning to face new or unknown situations. HMS decision-makers have to make sense of these kinds of situations and identify alternatives to control them.
When the management of these situations is successful, the HMS is resilient.

This paper focuses on the positive control of new, unknown, unexpected or surprising situations and on the possibility of learning from resilience. It proposes a functional architecture for learning from resilience indicators and their evolution. An exam-ple applied to a cockpit and its four-person flight crew illustrates the feasibility of such learning.

The rest of this paper is organised as follows. Section 2 focuses on the concept of resilience applied to HMS and the indicators for assessing it. Section 3 presents an original method to learn from other resilience indicators. Section 4 provides an example of learning from resilience. Section 5 offers our conclusions and prospects for future research.
 2. Pending issues about resilient HMS
One of the first substantive publications on resilience as applied to engineering was  X  X  X esilience Engineering: Concepts and Precepts X  X  ( Hollnagel et al., 2006 ). The basic concepts behind resilience engineering are developed, but at the present stage, resilience engineering has several fundamental problems: (1) there is no appropriate definition of resilience, and (2) the differences between resilience and other similar concepts ( e.g. , robustness, reliability) are not clarified. These problems need to be addressed in order to advance resilience engineering and transform a theoretical concept into an applied science by defin-ing a quantitative method that can measure system resilience. 2.1. Definition of HMS resilience
Wreathall (2006) defined resilience as  X  X  X he ability of an organisation (system) to keep, or recover quickly to, a stable state, allowing it to continue operations during and after a major mishap or in the presence of continuous significant stresses X  X . As a resilience definition, Wreathall X  X  definition lacks a distinction of resilience from robustness ( Zhang and Lin, 2010 ; Wang et al., 2010 ). Both terms are related to the ability of a system to keep functioning faced with disturbances.

Zhang and Lin (2010) further defined resilience as a system property about how the system can still function to the desired level when it suffers from partial damage. This definition was able to distinguish resilience from robustness: for a robust system, the physical structure of the system is still intact, whereas for resilient system, the physical structure is damaged ( Gao, 2010 ).
This definition sees resilience as a system X  X  post-damage property ( i.e. , the system X  X  ability to recover its functions faced with damage). In essence, a resilient system contains characteristics of a robust system in that it is the magnitude of the disturbance that differentiates resilient system from robust one.
We aim to apply the resilience concept to HMS, with human operators as an unpredictable source of both reliability and errors.
We distinguish a robust system from resilient one, based on the nature or typology of the threa ts/disturbances, as defined by
Westrum (2006) : robust systems deal with regular and, at a certain level, irregular threats, whereas re silient systems manage unknown situations ( e.g ., unexpected or unprecedented disturbances).
Assuming that optimal performance level exists ( i.e. ,aninitial nominal HMS state or a baseline), after any disturbance, either internal disturbance ( e.g. , human errors or technical failures) or external disturbances ( e.g. , environmental events), the HMS perfor-mance may be degraded. Several scenarios can be envisioned:
If the HMS is capable of returning to the initial nominal performance ( i.e. , known disturbance situations), the system can be defined as resistant.

If the HMS is capable of recovering from a disturbance and stabilizing at another  X  X  X cceptable X  X  performance level, which is an unoptimal performance due to controlling unknown situa-tions ( e.g. , unexpected or unprecedented disturbances), the system can be defined as resilient.

If the HMS is not capable of recovering from a disturbance ( i.e. , not an acceptable performance) or stabilizing itself, the system is neither resistant nor resilient.

Human operators and machines in the HMS cooperate to ensure an optimal operation, and they are potentially available resources to make the HMS resilient. These resources need particular capacities, and some methods exist to make a system resilient. One of these methods is related to the learning process. 2.2. Resilience and methods
In order to be resilient, a system or an organisation required the following four qualities ( Steen and Aven, 2011 ): the ability to anticipate risk events; the ability to monitor what is going on, including its own performance; the ability to respond to unplanned events (regular, irregular or unprecedented) in a robust or flexible manner; and the ability to learn from experience.

Resilient systems are supposed to adapt to unplanned events with their ability to anticipate failures, to control disturbances, to react and to recover from these events ( Fig. 1 ). The system also has the possibility of learning from its reactions to unplanned events ( i.e. , successes and failures). Thus, the design of resilient systems can be based on some principles, such as the five principles defined by Zhang and Lin (2010) , which mainly high-light the need of a certain degree of functional redundancy, a controller for redundancy and learning management, a sensor for monitoring the whole system X  X  performance, a predictor for predicting potential threats or analysing poten-tial vulnerabilities of the system, and an  X  X  X ctuator X  X  for implementing changes or training.
A non-resilient system cannot continue to operate correctly after a major mishap or in presence of continuous stress. Many methods or mechanisms can be used to recover from such situations, such as
Using non-affected elements to compensate and accomplish the functions of the affected or degraded parts ( Chen et al., 2007 ; Nakayama et al., 2007 ; Numanoglu et al., 2006 ), with what works compensating for what does not work.

Maintaining the system between the minimum and maximum thresholds of acceptability or perturbation management instead of a stable point or value ( Martin, 2005 ).

Putting critical elements in redundancy ( Luo and Yang, 2002 ), with the affected elements no longer being solicited and being replaced by redundant components.

Optimising the mitigation of the perturbations when the redun-dancy strategy is too expensive ( Neema et al., 2004 ; Tianfield and Unland, 2004 ), thus developing fault resilient systems.
Applying the principles of cooperation ( Vanderhaegen, 1997 , 1999a ; Hsieh, 2009 ; Zieba et al., 2009 ), with cooperation between humans and/or artificial agents facilitating the pro-blem-solving for new situations.

Developing systems or databases based on feedback in order to copy successful practices and learn from failed practices ( Vanderhaegen, 2010a ).

Activating the required resources by managing performance evolution and decision-maker autonomy ( Zieba et al., 2010 , 2011 ), which can lead to increasing the global system capacity using learning.

The affected elements learn or re-learn how to work correctly or to work better ( Cheveau and Wybo, 2007 ), with what does not work being reset or rebooted and prepared for future operations.

Learning vital functions for humans ( i.e. , the ability to develop attitudes or behaviours ensuring the survival of the human organism) ( Marcantoni, 2009 ).

The last four items on the list concerns the learning processes to make a HMS resilient. The 5 principles mentioned above for resilient system design ( Zhang and Lin, 2010 ) also concern learning management and training. Therefore, the system X  X  learn-ing capacities are thus important features and have to be devel-oped. This is a long-term, variable and dynamic process. It emphasises the need of HMS to continuously improve their learning capacities. 2.3. Resilience and learning
Human X  X achine Systems regularly try to anticipate and resist disturbances but may be vulnerable to critical or unexpected disturbances. Therefore, HMSs have to manage their knowledge dynamically in order to overcome problems and improve their learning capacities. Vanderhaegen (2010a) has proposed a beha-vioural model to react and learn from the successful or failed control of known and unknown situations. The model is based on three main activities: prognosis, diagnosis and trial-and-error reasoning ( Fig. 2 ).

The prognosis function leads to the identification of the possible evolutions of the system state, with or without actions. The diagnosis function relates to the explanation of the current system state in terms of the previous state s. When this identification is not possible due to unknown system stat e, trial-and-error reasoning is needed in order to apply an action and to wait and see its consequences. This trial-and-err or process tries to understand the current system state and to propose a new adapted action plan.
The management of known or unknown situations or threats requires different  X  X  X emands X  X  on a resilient system ( Westrum, 2006 ; Hale and Heijer, 2006 ). For instance, the system can develop a standard response to known situations. However, the unknown situations are more challenging: unprecedented threats that cannot be anticipated may push the HMS outside of its experience. Thus, HMS resilience relates to the system ability to adapt its responses X  capacities in order to circumvent threats.
Human operators are able to gain experience and to learn by repeating their control tasks, thus improving their behaviour.
They can adjust themselves according to the dynamic changes of the HMS they control. This requires adaptive and proactive behaviour ( i.e. , resilient behaviour) to control the system perfor-mances, especially faced with unexpected situations. Resilience is a dynamic process. It is not a static system state, and continuous verifications are necessary in order to qualify the HMS as resilient ( Hale and Heijer, 2006 ). Assessing HMS resilience is required in order to: categorise and compare systems in terms of their resilience characteristics, evaluate the impact of upcoming learning processes in terms of their resilience, and evaluate the possible evolution of HMS resilience.
 2.4. Resilience and measurements
The Charpy impact test gives a first measurement of resilience, related to the ability of a system to recover from a shock.
Hollnagel and Woods (2006) argue that resilience itself cannot be measured, but the potential for resilience can be measured.
There is still some debate regarding the definition of resilience and its difference with other similar concepts ( e.g ., robustness, reliability). Since the definition of resilience in the literature is vague or conceptual, its quantification may still be under development.

In order to  X  X  X ngineer X  X  resilience, some objective and quantitative indicators are needed. Objective i ndicators are based on normative interpretation of the data, and quantitative indicators are measur-able metrics that are used to identify when a system X  X  performance changes ( Wreathall, 2011 ). Thus, these indicators can be used to judge whether the system X  X  resilience levels are acceptable.
There is currently no measurement for assessing a HMS X  X  resilience. Therefore, this paper proposes a quantitative resilience indicator. In other domains, several resilience indicators based on the evolution of performance have been proposed in the litera-ture. In order to present these indicators, Fig. 3 presents an example of changes in system safety when faced with a distur-bance. The baseline in this figure represents totally safe condi-tions, and the minimum acceptable threshold indicates an acceptable safety level for designers. E max is the maximum amplitude of the disturbance X  X  effect on safety, and E j is the amplitude of the disturbance X  X  effect on safety at time T
Resilience has been evaluated by estimating the maximum intensity of an absorbable force ( E max ) without perturbing the system X  X  functions. In the ecological domain, Orwin and Wardle (2004) have linked resilience with the measurement of the instantaneous and maximal disturbance: resilience T j  X  2
The instantaneous resilience varied between 0 and  X  1, where the value  X  1 corresponds to the maximal resilience when the disturbance X  X  effects were recovered ( E j  X  0). This indicator does not consider the recovering time: two systems with different recovering time D t 1 and D t 2 cannot have the same resilience. For
Luo and Yang (2002) , the measurement of the resilience is linked with the speed of recovery from a disturbance: resilience  X  T r T p  X  2  X 
This computer communications indicator does not consider the disturbance effect on the system. However, if a system can handle a large number of disturbances, this system may be more resilient.

Pe  X  rez-Espan  X  aandSa  X  nchez (2001) have calculated the resilience of ecological system by the opposite of the tangent of the ratio between the resistance and the recovery time of a disturbance: resilience  X  tan 1  X  1 = E max  X  T
The instantaneous resilience varied between 0 1 and 90 1 , where the value 90 1 corresponds to the maximal resilience level. Their resilience indicator is performed within an interval ( i.e. , during the disturbance effect on the system) and does not take the general evolution of the system into account. For an application to the Human X  X achine Systems, there is a lack of such instanta-neous resilience indicators. All the proposed indicators mentioned above do not consider simultaneously disturbance period, effect, and recovery speed. All propose an instantaneous value without considering their possible evolutions.

Wang et al. (2010) proposed an indicator for a company information system X  X  resilience based on the maximum recovery ability of the system. For a partially damaged company informa-tion system, the resilience is defined as resilience  X  max where d i represents the demand time for the recovery of function i ( i  X  1, y , m ), m is the number of function of the system, and c completion time. z i is the weight of the function i ,whichrepresents the importance of this function base d on the particular features of the system among all functions and is su bject to the following constraint:
When all functions can be recovered within the demand time, the resilience value will be larger than 1. The larger the resilience value, the more resilient a given system.

The indicator of Wang et al. (2010) is limited in that it considers the resource reallocation for different recovery solutions, which supposes the number of functions and the number of recovery solutions are known. However, HMSs are supposed to be dealing with unknown situations, so their indicator is not appropriate. Other indicators are thus required for studying HMS resilience. 3. How to learn from resilience
For studying HMS, several other authors have proposed resi-lience indicators. They assess resilience not only for the safety impact but also for other relevant system criteria. They are used as inputs or outputs for a functional architecture of a system able to learn from such indicators. 3.1. Resilience indicators for the learning process
Assessing HMS resilience requires evaluating two classes of indicators: the performance stability indicator on a given time interval ( i.e. , the time period during which the performance improves or stays the same), and the HMS performance indicators related to the consequences of human actions in order to compare performance levels between two dates ( i.e. , the current one and a past one from, for example, a sampling period).

In order to assess the system X  X  resilience, the system safety factor S ( t ) is determined by the cumulative effects of the possible awareness) ( Gu et al., 2009 ). For instance, resilience can be assessed between times t b and t e ( Fig. 4 ). The time t beginning of disturbance effect ( i.e. , the safety indicator is below a minimum acceptable threshold), and the time t e is the result of the recovering process ( i.e. , the end of the unacceptable performance).
Based on this safety indicator, Enjalbert et al. (2011) have proposed a local resilience evaluation: local resilience  X  dS  X  t  X  dt  X  6  X 
The local resilience is an instantaneous measurement of resili-ence. Its value depends on the effect of disturbance on the system: it can be negative if the performance decreases or positive if the system recovers from the disturbance. The global resilience is the integral of local resilience over a period of time ( Enjalbert et al., 2011 ): global resilience  X 
Table 1 presents the possible evaluations that can be obtained for the times t i (during safety performance decrease), t imum effect of disturbance) and t j (during safety performance recovery). The time t p is the beginning of disturbance effect ( i.e., the safety indicator is below a minimum acceptable threshold), and the time t r is the result of the recovering process ( i.e., the end of the unacceptable performance), as shown in Fig. 3 .
This evaluation (Eqs. (6) and (7)) is from a mono-criterion ( i.e. , safety criterion) viewpoint. It is used to produce a multi-criteria viewpoint. These new criteria of system evaluation concern particular human or machine behaviours or their effects, or the occurrence or the consequences of external perturbations ( e.g. ,the instantaneous or anticipated human workload ( Vanderhaegen, 1999b , 1999c ), the number of human errors ( Vanderhaegen et al., 2004 ), and the quality or production of services ( Polet et al., 2009 )). Thus, several performance criteria, related to the system safety, the human workload and the team mission are defined and described in detail in Section 4.2 .

Based on these criteria, by applying Eqs. (6) and (7), we can assess a multi-criteria resilience ( i.e. , local and global) of a HMS: local_resilience_on_safety is the local resilience evaluation based on the safety indicator, global_resilience_on_safety is the global resilience evaluation based on the safety indicator, and so on for all considered criteria.

The evolution of the resilience can then be assessed recur-sively. If a perturbation is not recovered, the system is not resilient. Therefore, two recovery levels can be identified: prevention recovery from the perturbation X  X  occurrence, and protection recovery from the consequences of this perturbation.
All these multi-criteria resilience indicators are the inputs or the outputs of our learning system. This system is then able to anticipate the evolution of system resilience in terms of several performance criteria or to propose alternatives to recover from perturbations: prevention and protection recovery processes. 3.2. The system architecture to learn from resilience
Our learning system, shown in Fig. 5 , uses a reinforced iterative formalism for sequential learning for HMS resilience:
Iterative learning  X  learning built on prior knowledge to predict the future evolution of indicators of resilience.

Reinforced learning  X  for a given iteration i , the correct predic-tion assessment consists of comparing the real resilience is integrated and synthesised in a reduced number of cases, allowing the correction or reinforcement of the global knowl-edge database.

Sequential learning  X  learning built on a chronological sequence of events.

This system has a feedforward X  X eedback architecture and is able to learn from resilience indicators. These indicators are the possible evolutions of the system resilience, the possible alter-natives for human operators to control resilience).

The controllers or decision-makers (either automated or human) need feedback information about the actual state of the controlled process to satisfy their safety management objectives.
Thus, Leveson (2004) has proposed an accident model STAMP (Systems-Theoretic Accident Model and Processes), where the system is a dynamic process that is continually adapting, based on feedback loops of information and control, to accomplish its goals and to react to changes in the system and its environment.
In fact, an inadequate or missing feedback can lead the system into hazards and accidents.

In our structure, the feedback process recovers possible erro-neous knowledge, refines knowledge or creates new knowledge ( Vanderhaegen et al., 2011 ). The feedforward process assesses the possible future decisions in terms of the current system states and the management of the previous states. Thus, the feedforward X  feedback mechanism uses current knowledge related to pre-vious activities in order to calculate the future ones ( Ouedraogo et al., 2010a , 2010b ). Therefore, at iteration i , the input vectors O O
I ( I [1, y , i ] ) can contain a chronological sequence of resilience values ( e.g. ,
I [1, y , i ]  X  {local _ resilience _ on_safety ( t 1, y , global _ resilience_on_safety ( t 1, y , t i ), local _ resilience_on_workload ( t 1, y , t i ), global_resilience_on_workload ( t 1, y , t i ), local _ resilience_on_mission ( t 1, y , t i ), global _ resilience_on_mission ( t 1, y , t i )}).

This input vectors ( I [1, y , i ] ) dimension is related to i , the number of iterations; so in every iteration, the input dimension is increased.

Then, the system can complete the sequence by predicting the other resilience values ( e.g. , for n 4 i
O n [ i  X  1, y , n ]  X  { local _ resilien ce_ on_safety ( t global _ resilience_on_safety ( t i  X  1, y , t n ), local _ resilience_on_workload ( t i  X  1, y , t n ), gl obal _ resilience_on_workload ( t i  X  1, y , t n ) , local _ resilience_on_mission ( t i  X  1, y , t n ), global _ resilience_on_mission ( t i  X  1, y , t n )}). in every iteration, the output dimension is decreased.
As the criteria are related to the interactions between humans and the system, and to the system structure and its operation, our future research has the objective of predicting human operator actions in order to determine the appropriate alternatives of human operations, for example, in the decision-making system. ological sequence of resilience values and the system may predict the human operators X  possible alternative actions plan as outputs The prediction application is developed in C  X  X  based on the Kohonen self-organisation map. The interested reader can consult
Zhang et al. (2004) and Vanderhaegen et al. (2009) for a more detailed discussion of the implementation. This system requires a euclidean distance ( D ) in Eq. (8) that identifies in the knowledge
D  X  I The feedforward part of the architecture may predict [ i  X  1, y , n ] , which is based on the minimum of the euclidean
Through the feedback part of the architecture, the database ( W ) is then incremented by values of w [ i ] at the i th iteration.
Based on the current error ( e [ i ] ), which is the difference between the observed resilience values O [ i ] at the i th iteration and iteration i 1, database ( W ) content is reinforced thanks to the following equation: w and parameter Z includes the learning rate and a neighbourhood threshold function. 4. Application of our architecture to a military air transportation system
Our system architecture was validated with a military air transportation system, involving a simulated cockpit with a four-person flight crew. 4.1. Experimental protocol The experiments were performed with the In-flight Refuelling Group of the Istres air base (France). Military teams, working in small 4-person groups, are trained together and are brought to make many decisions in uncertain situations. Their activities are reproducible through a flight simulator: a BC-135 Boeing during an in-flight refuelling.
 The experimental scenario was inspired by a real incident. Initially, smoke accompanied by a burning smell appears in the cabin. Then, a series of failures without apparent links occurs ( e.g. , frost on the windows, loss of fuel indications, overheating transformer, smoke). The aircraft is over the ocean and cannot land. The problem is an electrical failure and is located on the specific area of a generator. Its fuse, which is less visible, has blown. In fact, all the failed components have the same origin, but expert opinion remains divided between two possible causes. Thus, the team has to face an ambiguous or uncertain situation.
Facing these repetitive failures, the team has to make sense of the situation in order to apply the correct procedures. They are not supposed to know the recovery rules, but they have all the manuals to identify the recovery rules. Several criteria are identified to assess such behaviours in terms of resilience. 4.2. Performance criteria for resilience assessment Several criteria were defined in order to evaluate the general HMS evolution: criteria related to system safety, human work-load, and the team mission. These criteria are the main factors that concern system performance under major mishap. The safety criterion relates to system performance level faced with failures. Initially, the system is supposed to be almost 100% safe. Multiple sequential disturbances ( e.g. , frost on the windows, overheated transformer with smoke, loss of fuel indications) occur at time ( T may decrease this system safety level ( S s ( t )), depending on the effect of these disturbances. E i , j  X  a , b and so on, where E the effect E of a disturbance event i at time T j , and a and b are the disturbance effect values (given in percentages).
 Initially, the maximum safety level is given by S  X  t  X  X  S s _ init  X  t  X  X  100 %  X  10  X 
For the occurrence of a single event, the maximum safety level is expressed as S  X  t  X  X  S s  X  t  X 
For the occurrence of multiple events, the maximum safety level is assessed as follows: S  X  t  X  X  S s  X  t  X  a b ...  X  12  X  If the system safety level decreases below the minimum accep-the disturbance. As a result, the safety level increases, but a performance loss still remains. This recovery process from an event occurrence is assessed as follows:
S  X  t  X  X  S s  X  t  X  X  where D a or D b y ( e.g. , D  X  0.1) are the performance losses after a recovery from disturbance effect E  X  a , b , and so on.
The human workload criterion is linked to the number of interactions between the staff members ( i.e. , communication frequency) and between the staff and the technical system ( e.g ., standard procedures, applied actions). Initially, the team is free to do or not do anything. Their available workload capacity is maximal and given by:
S  X  t  X  X  S w _ init  X  t  X  X  100 %  X  14  X 
In order to accomplish their mission, the team may have more, or less, work to do because of the disturbance occurrences and the team members may increase their workload. In fact, they are supposed to apply the standard procedures and actions, to increase their communication frequency in order to find the appropriate actions and to make and validate checklists, for example. These behaviours aim at overcoming the initial or current problems.

The available workload capacity level decreases according to the frequency of team communications, the standard procedures performed and the frequency of the applied actions. This work-load criterion is then based on the number and demand of the communications, procedures and actions:
S  X  t  X  X  S w  X  t  X  x  X  X  number of communications  X  where x is a scaled factor, with x  X  0.1% based on the maximal available workload capacity (Eq. (14)) and the number of inter-actions: when the number of interactions (communications, procedures, actions) reach 1000, the available workload criterion downs to 0%; i.e., S w  X  S w_init x n (1000)  X  100% 0.1% 1000  X  0%.
The mission criterion is the respect of the landing time and the aircraft X  X  landing location. Without any disturbance, the team is supposed to respect the mission. The mission success can be evaluated in terms of the landing time and the landing location.
Due to the occurrence of unexpected events, the team can make some changes in landing time or location. Some penalties may be applied depending on disturbance effects, in terms of landing delay and/or landing location:
S  X  t  X  X  S m  X  t  X  2  X  disturbance _ ef f ect _ landing _ time  X 
Therefore, the mission criterion can be seen as the correlation between system safety ( e.g ., change of the landing location to improve system safety) and its workload demand ( i.e. , procedures and actions applied to overcome a problem). The evolution of these criteria and the associated local and global resilience can then be evaluated during the entire mission. 4.3. Evolution of the performance criteria
Initially, the system is supposed to be almost 100% safe. The team is free to do or not do anything. Their available workload capacity is maximal ( i.e. , 100%), and the team is supposed to respect the given mission ( i.e. , 100%). Faced with repetitive fail-ures, the team has to make sense of the situation in order to overcome the problems. To evaluate the evolution of the given criteria, we consider a set of actions performed by the flight crew in response to each abnormal situation, thus contributing, at least theoretically, to the system X  X  resilience.

The safety criteria depend on the occurrence of disturbances (Eqs. (10) X (13)): its amplitude, its occurrence time, the crew X  X  recognition and the crew X  X  recovery time. Table 2 presents the disturbance amplitudes defined based on the disturbance X  X  criti-city: from  X  X  overheating transformer with smoke  X  X  (30%) to  X  X  frost on the windows  X  X  ( 10 %).

The available workload (Eqs. (14) and (15)) depends on the standard procedures performed, and in cases of a disturbance occurrences, the number of interactions ( e.g ., communications, procedures, actions) between operators and the system. The mission criteria (Eq. (16)) depend on the actions and decisions affecting the landing time or location, such as securing the flight by re-routing to a location X, help from air traffic controller (ATC), queuing analysis, and/or changing flight plan. The disturbances listed in Table 2 also affected the mission criteria at certain level.
All these  X  X  X isturbances X  X  are classified according to their effects on mission criteria, from  X  X  X e-routing to a location X X  X  (30%) to  X  X  X apid test X  X  (5%). 4.4. Evolution of the local and global resilience
If the situation is not understood and not recovered during the landing, the situation can turn into a disaster ( i.e. , a crash). In cases when this disaster occurs, all the values of the criteria (safety, mission and workload) would be equal to 0. Based on the definitions of the criteria in the previous section, Fig. 6 gives an example of the evolution of the safety, mission and workload criteria for a team. Initially, the system is supposed to be almost 100% safe, the team are supposed to respect the given mission (100%), and there is no workload demand.

Due to the perturbations, the system safety level decreases so the team has to communicate in order to perform actions and/or procedures to keep the criteria higher than the minimum accep-table threshold. Therefore, the available workload level decreases too, and the team may not be able to respect the initial mission in terms of landing time. With more and more perturbations, the main criteria, and thus the safety level, decrease below the minimum acceptable threshold, defined by team in terms of the safety level: 90%. The team changes the landing location to improve the system X  X  safety level, thus increasing the team members X  workload, to overcome the problem. By the end of the mission, the team manages to have a safe landing, thus increasing the safety level and available workload to 100%, but they changed the landing location so the mission cannot attain 100%. 14 specific times, corresponding to the measurable conse-quences of perturbation on the Human X  X achine System, have been selected. Then, based on the local and the global resilience equations (Eqs. (6) and (7)), it can be observed that 13 iterations are needed to complete the evolution of the resilience values. criterion (safety, workload or mission) at specific time ( t global resilience is the integral of local resilience over a period of time or interval, as defined in Section 3.1 . Thus, we can have a look at the effect of the perturbation when it occurs: the local resilience is negative if the system performance decreases or positive if the system recovers from the disturbance. The capacity of the system to handle perturbation over the time can also be considered: the lower the system X  X  global resilience, the more this system is resilient ( Fig. 8 ). 4.5. Results of the learning from resilience indicators
In this simulation, the input vectors ( I [1, y ,i ] ) contain a partial chronological sequence of resilience values:
I [1, y ,i ]  X  { local_resilience_on _ safety ( t 1 , y , t global_resilience_on _ safety ( t 1 , y , t i ), local _ resilience_on_workload ( t 1 , y , t i ), global _ resilience_on_workload ( t 1 , y , t i ), local _ resilience_on_mission ( t 1 , y , t i ), global _ resilience_on_mission ( t 1 , y , t i )}.

For initialisation, database ha s been constituted with the first two iterations, and so 11 remaining iterations are needed in order to complete the n  X  13 iterations of studied experiment. The system complete the sequence by predicting the other resilience values, the local resilience prediction : the global resilience prediction :
Fig. 9 gives the results related to the prediction quality of our architecture. The prediction rate is a comparison between the real resilience indicator values and the predicted ones.

Both local and global resilience prediction rates converge toward almost 100% after 11 iterations because database stores information and increases in every iteration. Indeed, correct pre-diction is easier to determine as the number of iteration to predict decreases. However, through all 11 iterations, the local resilience prediction rate stay very good around 90% because values from the calculated indicator do not change very much whereas the global resilience prediction rate may be more challenging because varia-tion of the indicator based on the experiment time are important. Authors should consider a way to integrate periodic measure between iterations in further development of the applied archi-tecture to avoid such a disparity in results. 5. Conclusion
The concept of resilience was defined and applied to HMS in terms of managing the system X  X  safety during perturbations. Some series of possible learning behaviours in order to improve the system resilience were proposed. The existing indicators to assess this resilience were also studied. Some valuable indicators to assess the resilience of HMS were defined, and a sequential, iterative and reinforced system able to learn from the temporal evolution of such indicators was proposed. Our system architec-ture includes a feedforward process to predict the evolution of the resilience indicators and a feedback process to refine the system knowledge, taking into account the inputs and the outputs of the previous iterations. Its implementation is based on the Kohonen model.

A practical example for military air transportation is detailed to illustrate the feasibility of such a system. For resilience assessment, a multi-criteria ( i.e. , safety, mission and workload) resilience approach was developed in order to monitor the local and the global resilience evolution of the HMS. For predicting the resilience evolution, our feedforward X  X eedback architecture is evaluated in term of prediction quality.

Future research will refine the definition of the criteria, for example, the workload criterion could take into account the duration of the procedures, actions or communications. Another possible development is to use the possible evolution of the resilience indicators to predict or define the most appropriate alternatives for human operator actions. This can provide an online tool for decision-making or monitoring.
 Acknowledgements The present research was supported by the International Campus on Safety and Intermodality in Transportation, the Nord-Pas-de-Calais Region, the European Community, the
Regional Delegation for Research and Technology, the Ministry of Higher Education and Research, and the National Center for
Scientific Research. The authors would like to thank the European project, Information Technology for Error Remediation And Trapping Emergencies (ITERATE), in the seventh Framework Program, for the analysis of transport driver behaviours, and the
REACT project, financed by DGA (French army), for helping heterogeneous military units to learn and react faced with unexpected events. The authors gratefully acknowledge the sup-port of these institutions.
 References
