 Data uncertainty is inherent in applications such as sen-sor monitoring systems, location-based services, and biolog-ical databases. To manage this vast amount of imprecise information, probabilistic databases have been recently de-veloped. In this paper, we study the discovery of frequent patterns and association rules from probabilistic data under the Possible World Semantics . This is technically challeng-ing, since a probabilistic database can have an exponential number of possible worlds. We propose two efficient algo-rithms, which discover frequent patterns in bottom-up and top-down manners. Both algorithms can be easily extended to discover maximal frequent patterns. We also explain how to use these patterns to generate association rules. Exten-sive experiments, using real and synthetic datasets, were conducted to validate the performance of our methods. Source codes and data are available at : http://www.cs.hku.hk/ ~ lwsun/codes/kdd10/ H.2.8 [ Information Systems ]: Database applications X  Data Mining Algorithms, Experimentation, Performance, Theory uncertain data, frequent pattern, association rule
The data managed in many emerging applications is often uncertain. Integration and record linkage tools, for example, associate confidence values to the output tuples according to the quality of matching [11]. In structured information ex-tractors, confidence values are appended to rules for extract-ing patterns from unstructured data [28]. In habitat moni-toring systems, data collected from sensors like temperature and humidity are noisy [1]. The locations of users obtained through RFID and GPS systems are also imprecise [25, 16]. To handle these problems, probabilistic databases have been recently proposed, where uncertainty is treated as a  X  X irst-class citizen X  [8, 11, 21, 10, 15].

Due to its simplicity in database design and query seman-tics, the tuple-uncertainty model is commonly used in prob-abilistic databases. Conceptually, each tuple carries an ex-istential probability attribute, which denotes the confidence that the tuple exists. Figure 1 illustrates this model, which records traffic violation events due to red-light running. The details of each event (e.g., location, and traffic volume) are captured by a red-light camera system, which contains sen-sors and cameras mounted in road intersections. Each tuple is annotated by a probability that a true violation happens. The probability that a violation occurs is determined by sen-sor measurement errors, as well as the uncertainty caused by automatic information extraction of the photographs taken by the system [30].

To interpret tuple uncertainty, the Possible World Se-mantics (or PWS in short) is often used [11]. Concep-tually, a database is viewed as a set of deterministic in-stances (called possible worlds ), each of which contains a set of zero or more tuples. A possible world for Figure 1 con-sists of the tuples { t 2 ,t 3 ,t 5 } , existing with a probability of (1  X  0 . 1)  X  1 . 0  X  0 . 5  X  (1  X  0 . 2)  X  1 . 0 = 0 . 036. Any query evaluation algorithm for a probabilistic database has to be correct under PWS. That is, the results produced by the algorithm should be the same as if the query is evaluated on every possible world [11].

Although PWS is intuitive and useful, evaluating queries under this notion is costly. This is because a probabilistic database has an exponential number of possible worlds. For example, the table in Figure 1 has 2 3 = 8 possible worlds. Performing query evaluation or data mining under PWS can thus be technically challenging. In fact, the mining of un-certain or probabilistic data has recently attracted research attention [3]. In [18], efficient clustering algorithms were de-veloped to group uncertain objects that are close to each
Figure 2: Sample p-FPs derived from Figure 1 other. Recently, a Na  X   X ve Bayes classifier has been devel-oped [26]. The goals of this paper are: (1) propose a defini-tion of frequent patterns and association rules for the tuple uncertainty model; and (2) develop efficient algorithms for mining patterns and rules that are correct under PWS. Figure 3: Sample p-ARs derived from Figure 1
The frequent patterns discovered from probabilistic data are also probabilistic, to reflect the confidence placed on the mining results. Figure 1 shows two probabilistic frequent patterns (or p-FP ) extracted from the database in Figure 1. A p-FP is a set of attribute values that occur frequently with sufficiently-high probabilities. In Figure 1, the support probability mass function (or support pmf ) for each p-FP is shown. This is the pmf of the number of tuples (or support count ) that contains a pattern. Under PWS, a database is a set of possible worlds, each of which records a (differ-ent) support of the same pattern. Hence, the support of a frequent pattern is a pmf. In Figure 1, if we consider all pos-sible worlds where pattern { location= x } occurs three times, the pmf of { location= x } with a support of 3 is 0.49. For the p-FP s shown, Figure 3 displays their related probabilistic as-sociation rules (or p-ARs). Here, rule r 2 suggests that with a 0.49 probability, 1) red-light violations occur frequently at location x and 2) when this happens, the involved vehicle is likely driving at a high speed amid low traffic. We will later explain more about the semantics of p-FP and p-AR.

A simple way of finding p-FPs is to extract frequent pat-terns from every possible world. This is practically infea-sible, since the number of possible worlds is exponentially large. We present simple and effective methods to prune infrequent patterns. We also adopt dynamic programming (DP) to compute the support pmf of a pattern. This method, also used by [31, 27] to derive p-FPs for other probabilis-tic models, has a complexity of O ( n 2 ). We further de-velop a divide-and-conquer (DC) approach, which achieves O ( n log 2 n ) time. Based on these methods, we propose the p-Apriori algorithm to retrieve p-FPs in a bottom-up man-ner. We further observe that, given patterns X and Y , if X is a subset of Y , the support pmf of X can be efficiently derived from that of Y . We realize this by proposing the TODIS algorithm, which extracts p-FPs in a top-down fash-ion. We extend p-Apriori and TODIS to retrieve maximal p-FPs, i.e., those that are not the subsets of other p-FPs. Our experiments on real and synthetic datasets showed that DC is more scalable than DP, and TODIS is generally faster than p-Apriori. When TODIS is used with DC together, it can outperform p-Apriori by an order of magnitude.

In exact databases, deriving association rules from fre-quent patterns is not difficult. Given two frequent patterns X and XY , the confidence of X  X  Y can be calculated with an arithmetic division on their supports. This is no longer true for probabilistic data. Here, the support of X and XY become correlated random variables. It is not clear how to define and compute the  X  X onfidence X  of X  X  Y . We propose the concept of p-AR, which naturally extends the association rule semantics. We also study an efficient method, which uses deconvolution operations, to evaluate the probability of p-AR based on the support pmfs of p-FPs. We further develop a new algorithm for generating p-ARs, which ex-ploits the anti-monotonicity property of p-ARs, and attain a 80% performance gain in our experiments.

Prior work. [7] studied approximate frequent patterns on noisy data, while [19] examined association rules on fuzzy sets. The notion of a  X  X ague association rule X  was developed in [20]. These solutions were not developed on probabilis-tic data models. For probabilistic databases, [9, 2] derived patterns based on their expected support counts. [31, 27] found that the use of expected support may render impor-tant patterns missing. They discussed the computation of the probability that a pattern is frequent. While [31] han-dled the mining of single items, our solution can discover patterns with more than one item. The data model used in [27] assumes that for each tuple, each attribute value has a probability of being correct. This is different from the tuple-uncertainty model, which describes the joint probabil-ity of attribute values within a tuple. Moreover, our DC algorithm is asymptotically faster than the DP algorithms used in [31, 27], and is thus more scalable for large and dense datasets. To our best knowledge, none of the above works considered the important problem of generating association rules on probabilistic databases.

This paper is organized as follows. Section 2 introduces the notions of p-FPs and p-ARs. Sections 3 and 4 present two algorithms for mining p-FPs. In Section 5, we develop an algorithm for generating p-ARs. Section 6 presents our experimental results. We conclude in Section 7. We first review frequent patterns and association rules in Sections 2.1. Then, we discuss the uncertain data model in Section 2.2. We present the problems of mining p-FPs and p-ARs, in Sections 2.3 and 2.4.
A transaction is a set of items (e.g., goods bought by a customer in a supermarket). A set of items is also called an itemset or a pattern . Given a transaction database of size n and a pattern X , we use sup ( X ) to denote the support of X , i.e., the number of times that X appears in the database. A pattern X is frequent if: where minsup  X  X  X  [1 ,n ] is the support threshold [4]. Given patterns X and Y (with X  X  Y =  X  ), if pattern XY is fre-quent, then X is also frequent (called the anti-monotonicity property). Also, X  X  Y is an association rule if the follow-ing conditions hold: where sup ( XY ) sup ( X ) , denoted by conf ( X  X  Y ), is the confi-dence of X  X  Y , and minconf  X  X  X  (0 , 1] is the confidence threshold . To verify Equation 3, the values of sup ( XY ) and sup ( X ) have to be found first.

We remark that a transaction database is essentially a re-lational table with asymmetric binary attribute values. For example, the existence of item  X  X pple X  in a transaction is equivalent to a binary attribute of a tuple with a value of 1. This kind of attributes, assumed in this paper, is also considered by some mining algorithms (e.g., [4, 5]). To han-dle other attribute types (e.g., continuous and categorical), discretization and binarization techniques can be used to convert them to binary attributes [29].
We assume that each transaction has an existential proba-bility , which specifies the chance that the transaction exists. Figure 4(a) illustrates this database, in which each trans-
To understand how probabilistic frequent patterns can ( X ) can be obtained by counting the number of times existential probability of T i . If I = { e 1 , e 2 , . . . , e of all m distinct items in PDB , then T i .S  X  I for any T der the PWS, PDB is a set of possible worlds W . Table ?? (Appendix ?? ) lists all the possible worlds for the database in Figure 2(a). Each possible world W i  X  W is a possible world instance of PDB , with a probability of P r ( W i ). For example, P r ( W ) = T 1 .p  X  (1  X  T 2 .p )  X  (1  X  T 3 .p )  X  T 4 .p , or 0 . 09. Notice that the sum of possible world probabilities is one. Also, the number of possible worlds is exponentially large, i.e., |W| = O (2 n ). Our goal is to discover patterns and rules without expanding PDB into possible worlds. For easy reference, Table ?? (Appendix ?? ) summarizes the symbols used in our paper. To understand how probabilistic frequent patterns can be defined, let us first explain the notion of  X  X upport X  in a probabilistic database. Given an itemset X , we denote its support in each possible world W i as sup i ( X ). Note that sup ( X ) can be obtained by counting the number of times that X appear in W i . Since each W i exists with a proba-bility, the support of X in PDB , i.e., sup ( X ), is a random variable. More specifically, sup ( X ) is a probability mass function (pmf) f X ( k ), where k is a non-negative integer.

In fact, f X ( k ) can be interpreted as the probability that sup ( X ) equals to k . Also, if k &gt; n , sup ( X ) = 0. In this paper, we represent f X as an array, where f X [ k ] denotes the probability P ( sup ( X ) = k ), for k = [0 , . . . , n ]. Figure 2(b) depicts the support pmf of pattern { a } for Table ?? . The probability that sup ( { a } ) = 1 is 0.29.
 Figure 2: support pmf f { a } .
 Now, let P ( fp: X ) the frequentness probability of X , i.e., the probability that X is frequent ( fp: X denotes that X is a frequent pattern). Then, we have: P ( fp: X ) = P ( sup ( X )  X  minsup ) (4) Let minprob  X  R X  (0 , 1] be the probability threshold of a frequent pattern. Then, the following extends the defi-nition of  X  X requent patterns X  (Equation 1) for probabilistic databases.
 Definition 1 (Probabilistic Frequent Pattern).
 A pattern X is called a probabilistic frequent pattern (or p-FP) in PDB if
Now, let P ( fp: X ) the frequentness probability of X , i.e.,
Let minprob  X  R X  (0 , 1] be the probability threshold of Definition 1 (Probabilistic Frequent Pattern).

We can now define the problem of probabilistic frequent
Problem 1 (p-FP Mining). Given a database PDB , action is a set of items represented by letters. This model has been used to capture uncertainty in many applications, including data streams [10] and geographical services [22]. Now, let P ( E ) be the probability that an event E occurs, and PDB be a probabilistic database of size n . Also, let T (where i = 1 ,...,n ) be the ID of each tuple in PDB . Sup-pose T i .S is the set of items contained in T i , and T i existential probability of T i .

Under PWS, PDB is a set of possible worlds W . Figure 5 lists all possible worlds for Figure 4(a). Each world W i W exists with probability P ( W i ). For example, P ( W 2 T .p  X  (1  X  T 2 .p )  X  (1  X  T 3 .p )  X  T 4 .p , or 0 . 09. The sum of possible world probabilities is one. Also, the number of possible worlds is exponentially large, i.e., |W| = O (2 Our goal is to discover patterns and rules without expanding PDB into possible worlds. Table 1 summarizes the symbols used.

We first explain the concept of  X  X upport X  for probabilistic data. Given a pattern X , we denote its support in each world W i as sup i ( X ). Note that sup i ( X ) is obtained by counting the number of times X appears in W i . Since each W i exists with a probability, the support of X in PDB , i.e., sup ( X ), is a random variable. We denote by f X the probability mass function (pmf) of sup ( X ), where k is a non-negative integer that sup ( X ) can take. Specifically, f ( k ) is the probability that sup ( X ) = k , and f X ( k ) = 0 for any k /  X  [0 ,n ]. We use an array to store the non-zero values of f X , where f X [ k ] = P ( sup ( X ) = k ). Figure 4(b) depicts the support pmf of { a } for Figure 5. The probability that sup ( { a } ) = 1 is 0.29.

Definition 1. A pattern X is a probabilistic frequent pat-tern (or p-FP) in PDB if where minprob  X  X  X  (0 , 1] is the probability threshold .
Problem 1 (p-FP Mining). Given PDB , minsup and minprob , return a set of { X,f X ( k ) } , where X is a p-FP.
As we will discuss, the pmfs obtained with p-FP s are es-sential to generating probabilistic association rules. There are methods to approximating and compressing pmfs (e.g., see [12]). Here we assume that the pmf is exact, but our solutions can be extended to support these schemes. Next, we present a useful lemma.
 Lemma 1 (Anti-monotonicity). If pattern X is a p-FP, then any pattern X 0  X  X is also a p-FP.

Proof. Let X 0 be a sub-pattern of X . Suppose that W X and W X 0 are the sets of possible worlds where X and X 0 frequent respectively. For each possible world W i  X  W X X is frequent in W i , then X 0 is also frequent. Hence, we have W X  X  W X 0 . Then P ( sup ( X )  X  minsup ) = P and P ( sup ( X 0 )  X  minsup ) = P W is a p-FP, we have: P ( sup ( X 0 )  X  minsup )  X  P ( sup ( X )  X  minsup )  X  minprob . Therefore, X 0 is also a p-FP.
The anti-monotonicity property is true for frequent pat-terns in exact data [4]. Lemma 1 allows us to stop examining a pattern, if any of its sub-pattern is not a p-FP. A p-FP X is said to be maximal if we cannot find another p-FP Y such that X  X  Y . A maximal p-FP can succinctly represent a set of p-FPs when their supports are not concerned. Since the mining of maximal frequent patterns is an important problem [5] for exact data, we also study maximal p-FPs: Problem 2 (Maximal p-FP Mining). Given a database PDB , minsup and minprob , return all maximal p-FPs.
In a probabilistic database, the support counts of patterns are random variables. Let P ( X  X  Y ) be the probability that X  X  Y is an association rule. By Equations 2 and 3, we have:
Definition 2. X  X  Y is a probabilistic association rule (p-AR in short) if The problem of p-AR mining is defined as follows.

Problem 3 (p-AR Mining). Given minsup , minprob , minconf , and the p-FPs and their support pmfs obtained from Problem 1, derive all p-ARs and their probabilities.
A simple way of solving Problems 1, 2 and 3 is to ex-pand PDB into all possible worlds, compute patterns and rules from each world, and then combine the results. If minsup = 2, minconf = 0 . 5, and minprob = 0 . 2, for Fig-ure 4(a), { a }  X  { c } is an association rule only in worlds W 5 and W 8 (Figure 5), with P ( { a }  X  { c } ) = Pr ( W Pr ( W 8 ) = 0 . 09 + 0 . 21 = 0 . 3. Since this is larger than 0 . 2, { a }  X  { c } is a p-AR . This method is not practical, due to the large number of possible worlds. To tackle Problems 1 and 2, we propose two efficient algorithms, namely p-Apriori and TODIS , in respectively Sections 3 and 4. Then we ad-dress Problem 3 in Section 5.
To solve Problem 1, we propose the p-Apriori algorithm, which is an adaptation of the Apriori algorithm [4] for proba-bilistic databases. Specifically, p-Apriori uses the bottom-up framework [4]: each item is tested to see whether it is a p-FP. All probabilistic frequent singletons then have their support pmfs computed, and are used to generate size-2 patterns (called candidate patterns). These patterns are examined to see which are frequent patterns. The size-2 p-FPs again have their support pmfs evaluated, and are used to create size-3 candidate patterns. The process is repeated until no more frequent patterns are found. In general, to create a size-( m +1) candidate patterns from size-m p-FPs, we use the anti-monotonicity property (Lemma 1), which implies that a size-( m +1) pattern can be a candidate only if all its size-m sub-patterns are frequent. Finally, the p-FPs and their support pmfs are returned.

The p-Apriori can also solve Problem 2, by returning only the maximal p-FPs that appear in p-Apriori X  X  mining result.
For p-Apriori to work well, we have to efficiently check whether a given pattern X is frequent. When the database is exact, we can scan the database once, find the support count of X , and test it with Equation 1. For p-Apriori, we have to (1) find X  X  X  support pmf; (2) derive P ( sup ( X )  X  minsup ) and test against Equation 4. To find X  X  X  support pmf, we may consider its support from all possible worlds; however, this is practically infeasible. In Sections 3.1, we discuss how to prune infrequent patterns without deriving support pmfs. For patterns that cannot be pruned, we discuss two efficient methods for generating support pmfs, in Sections 3.2 and 3.3. We present a data structure to improve our algorithms, in Section 3.4. Let cnt ( X ) be the number of tuples that contain pattern X regardless of the tuple probabilities (i.e., T i .p ). Also, let esup ( X ) be the expected support of X , which can be found by summing up all T i .p  X  X  in PDB [9]. The lemmas below describe how to prune X without knowing its support pmf. Lemma 2. If cnt ( X ) &lt; minsup , then X is not a p-FP.
Proof. The maximum possible value of sup ( X ) is cnt ( X ), which happens when all tuples that contain X exist. Hence, P [ sup ( X ) &gt; cnt ( X )] = 0. Since cnt ( X ) &lt; minsup , we have P [ sup ( X )  X  minsup ]  X  Pr [ sup ( X ) &gt; cnt ( X )] = 0. There-fore, for any minprob &gt; 0, X cannot be a p-FP according to Equation 4.
 Lemma 3. Let  X  = esup ( X ) , and  X  = minsup  X   X   X  1  X  . Then X is not a p-FP if:
Proof. Notice that the probability P [ sup ( X )  X  minsup ] is equal to Pr [ sup ( X ) &gt; minsup  X  1], or Pr [ sup ( X ) &gt; (1 +  X  )  X  ]. Using the Chernoff Bound [23], we have: Pr [ sup ( X ) &gt; (1 +  X  )  X  ] &lt; which is less than minprob . Hence, according to Equation 4, X is not a p-FP.
 To prune X , we first scan the database once and obtain the values of cnt ( X ) and esup ( X ), in O ( n ) time. Then, the lemmas are used to prune away X , in O (1) time. For patterns that cannot be pruned, we next present two efficient techniques to derive their support pmfs.
Algorithm 1 ( DP ) finds support pmf by dynamic program-ming. The pmf f X of pattern X is initialized to { 1 , 0 ,..., 0 } in Step 2 (i.e., sup ( X ) is zero before PDB is visited). Then, each f X [ k ] is updated by the information of every tuple T (Steps 3-7). Here, p X i is the probability that X occurs in tuple T i , where p X i = T i .p if X  X  T i .S , or zero otherwise. The array f 0 X is a temporary buffer. Step 6 is a recursive equation. It means that sup ( X ) can be i when either 1) X occurs in the current tuple and had ( i  X  1) occurrences in visited tuples, or 2) X does not occur in the current tuple Algorithm 1 : DP but occurred in visited tuples for exactly i times. This step is repeated until all tuples have been processed. The time and space complexity of DP are O ( n 2 ) and O ( n ) respectively. Algorithm 2 : DC
Algorithm 2 ( DC ) is another way of evaluating support pmf. Given a pattern X , Steps 2-4 compute the pmf for the case where PDB has only one tuple. Otherwise, PDB is horizontally partitioned into two databases ( D 1 and D in Step 5. Then, DC is recursively invoked on D 1 and D 2 obtain X  X  X  pmf for each database (Steps 6-7). The two pmfs are used to generate the support pmf of X (Step 8).
To understand Step 8, let sup D 1 ( X ) and sup D 2 ( X ) be the support of X in D 1 and D 2 respectively. Since sup D 1 and sup D 2 ( X ) are independent random variables, sup ( X ) = sup D 1 ( X ) + sup D 2 ( X ). Let f 1 X and f 2 X be the pmfs of sup D 1 ( X ) and sup D 2 ( X ) respectively. Then, In fact, f X is the convolution of f 1 X and f 2 X , denoted by f
X = f 1 X  X  f 2 X [14]. While a na  X   X ve way of evaluating Equa-tion 7 requires O ( n 2 ) time, this can be improved by applying Discrete Fourier Transform (DFT) on f X 1 and f X 2 , comput-ing the pairwise products of the two transformed sequences, and performing an inverse DFT on the product. With the use of the Fast Fourier Transform (FFT) algorithm, Equa-tion 7 can be evaluated in O ( n log n ) time [24].
Complexity. Let c ( n ) be the time cost of Algorithm 2 with database size n . Steps 6 and 7 both need c ( n/ 2) time, and Step 8 takes O ( n log n ) time with the use of the FFT algorithm. Then, c ( n ) = 2 c ( n/ 2) + O ( n log n ), which yields O ( n log 2 n ). Thus, DC is more efficient and scalable than DP for large datasets. The space complexity is O ( n ).
So far, all tuples in PDB are used to prune a pattern X , or to find X  X  X  support pmf. This is not always necessary. Consider a tuple T i , where X does not appear in T i .S . Since T does not contribute to the values of cnt ( X ) and esup ( X ), the absence of T i will not affect the correctness of Lemmas 2 and 3. Since p X i = 0, Algorithms 1 and 2 also work without T . In fact, our pruning lemmas and support pmf algorithms can be used on only the list of tuples which contain X . We call this the inverted probability list (or ip-list ) of X , denoted by L X . Figure 6(a) shows L { a } for pattern { a } . Now let l = | L X | . If (1) PDB is sparse, and (2) X is long, then l can be much smaller than n . In this case, the algorithms executed on L X can be faster than PDB . From now on, we assume that L X is used in our algorithms.

Let us summarize the process of handling a pattern X in p-Apriori. We first scan the database to obtain L X , cnt ( X ) and esup ( X ). Then, Lemmas 2 and 3 are used to check whether X can be pruned. If not, either Algorithm 1 or 2 is used to find the support pmf, which is passed to Equation 4 for checking the frequentness of X . Finally, L X is discarded, so that the memory can be reused to store other lists.
Complexity. Let l = | L X | . We can generate L X by scan-ning the database in O ( n ) time. Notice that cnt ( X ) = l , and esup ( X ) can be found by summing up all the tuple probabil-ities on L X . Afterwards, Lemmas 2 and 3 can be evaluated in O ( l ) time. If X cannot be pruned, and Algorithm 1 ( DP ) is used on L X , then O ( l 2 ) operations are needed. If Algo-rithm 2 ( DC ) is used instead, a O ( l log 2 l ) cost is required. Finally, we discard L X . Hence, the overall cost of evaluat-ing a pattern in p-Apriori is O ( n + l 2 ) when Algorithm DP is used, or O ( n + l log 2 l ) when Algorithm DC is used. The space complexity is O ( n ).

While Lemmas 2 and 3 can prune infrequent patterns ef-fectively, our experiments revealed that p-Apriori spends a lot of time to compute pmfs for the remaining patterns. Moreover, to extract maximal p-FPs, p-Apriori cannot avoid deriving the pmfs of all their sub-patterns. We next study a novel algorithm that alleviates these problems.
We call the second p-FP mining algorithm TODIS , which stands for  X  X Op-Down Inheritance of Support pmf X . It in-volves the execution of two phases: (1) extract patterns that are supersets of p-FPs; and (2) derive p-FPs in a top-down manner (i.e., in descending order of pattern size).
We first explain the intuition behind TODIS. Let X 0 be a sub-pattern of X . Then, any tuple that consists of X must also contain X 0 . Hence, the ip-list of X , i.e., L is a subset of L X 0 . In Figure 6(b), for instance, L { aeg } L { ae }  X  L { a } . Notice that these ip-lists, with their common entries shadowed, are quite similar. Since ip-lists are used to generate support pmfs, these pmfs should be similar too. We exploit this observation by incrementally deriving support pmfs. For example, L { aeg } is first used to generate f We next use f { aeg } to derive f { ae } , which then produces f { a } . This new approach, as we will explain, is faster than generating each pmf from scratch. In Section 4.1, we discuss how TODIS makes use of this technique. We explain how to adapt TODIS to solve Problem 2 (i.e., find maximal p-FPs) in Section 4.2.
Phase 1: Generate candidate patterns. We first swiftly identify a set of patterns which contains all p-FPs. This is done by slightly modifying p-Apriori. Specifically, for patterns that cannot be pruned by lemmas 2 and 3, we assume that they are candidate p-FPs, without evaluating their exact support pmfs. All these candidate p-FPs gener-ated are then passed to the next phase for verification.
Phase 2: Top-down support inheritance. To illus-trate this step, we suppose that the patterns in Figure 7 are those generated by Phase 1. Since TODIS examines patterns in descending order of their lengths, the longest pattern, i.e., { aeg } , is considered first. Specifically, the support pmf of { aeg } , or f { aeg } , is created  X  X rom scratch X , by Algorithms DP or DC . Then f { aeg } is inherited by its three sub-patterns: { ae } , { ag } and { eg } . That is to say, we use f { aeg } the pmf for these sub-patterns. For other size-2 patterns, { ac } and { fh } , we also generate their pmfs from scratch. After all the support pmfs for size-2 patterns are generated, the pmfs are inherited by size-1 patterns that are subsets of them. For example, { a } inherits the pmf of { ae } . All patterns that are true p-FPs (Equation 4) are returned. In this example, all patterns except { ac } and { c } are p-FPs.
How to use the  X  X nherited support pmf X  to generate a pmf of a given pattern X ? We define the exclusive item of X , de-noted by X. exItem , to be the set difference between X and the pattern from which X inherits the pmf. In Figure 7, for example, { ae } is a sub-pattern of { aeg } and inherits f Then, { ae } . exItem is { aeg } X  X  ae } , or { g } . To derive f given f { aeg } and { g } , we first evaluate the pmf f { ae g } algorithms DP or DC . Since the sets of tuples that contain { aeg } and { ae g } are disjoint, sup ( { aeg } ) and sup ( { ae g } ) are independent. As discussed in Section 3.3, f { ae } convolution f { aeg } and f { ae g } . We call the above proce-dure the update of support pmf. Observe that list L is empty, and is shorter than L { ae } that contains T T 3 (Figure 6(b)). Hence, with the efficient convolution of f computing it from scratch.

Inheriting from the  X  X est X  pmf. A pattern may be able to inherit pmf from two or more patterns. In Figure 7, for instance, { a } can inherit the support pmf from either { ae } and { ag } , since { a } is a sub-pattern of both of them. Which pmf should { a } inherit then? We make the decision based on the lengths of the ip-lists. To understand, let X a parent of pattern X . Then, X 00 = X  X  X. exItem . Also, let L be the ip-list where X  X  T i .S but X. exItem /  X  T i .S . Then, L is used by the support pmf update operation to compute X . For example, L = L { ae g } in the previous paragraph. Notice that | L | = | L X | X  X  L X 00 | . If L X 00 is longer, then L is shorter and the pmf update is more efficient. For example, suppose | L { ag } | &gt; | L { ae } | , then { a } inherits f f { ae } . Hence, a pattern should choose to inherit from the parent whose ip-list is the longest , in order to minimize the cost of updating its pmf.
 Algorithm 3 : TODIS Algorithms. Based on the above discussions, we propose Algorithm 3. Let F k ( k = 1 ,...,m ) be the set of p-FPs of length k , where m is the size of the largest p-FP. Also, let F be the set of all F k  X  X . For each pattern X  X  F k , let X. cnt be the length of L X . In Step 2, we execute Phase 1 and store all candidate p-FPs in F . Then, Steps 6-15 execute Phase 2, by examining each pattern in F k in descending order of k . Step 8 updates X  X  X  support pmf, based on X. exItem and the pmf inherited from its parent. Steps 11-13 select the best support pmf for X to inherit, where a pattern X  X  inherited pmf is replaced by another one if there exists a longer ip-list. In Steps 14-15, X is removed from F k if it is not a p-FP (Equation 4). The final result, F , is returned in Step 16. Algorithm 4 shows the routine for updating support pmf. Steps 3-5 generate L , the ip-list used by the support pmf update operation. If X has no exclusive item, X is not a subset of any candidate p-FP. Then L = L X , and we evalu-ate f X with the DP or DC algorithm (Steps 6-7). Otherwise, we compute the support pmf on L , perform convolution with the inherited pmf of X (i.e., f 0 ), in order to generate f Step 11 updates the length of L X . Finally, L is discarded and X is returned (Steps 12-13).
 Complexity. In Algorithm 3, evaluating a maximal p-FP X  X  pmf requires executing DP / DC alone, and so the cost is the same as that of DP / DC . For a non-maximal p-FP X , its pmf is obtained by updating the inherited pmf, with a cost of O ( n + l 0 2 ) (for DP ) or O ( n + l 0 log Here, l 0 = | L X | X  max {| L X 00 | | X  X  X 00 } . When l Algorithm 4 : UpdateSupPMF TODIS computes pmfs much faster than p-Apriori. To facil-itate searching of sub-patterns for inheritance (Step 9) and deletion of patterns (Step 15), we build a trie [17] of depth k for each F k , in which each non-leaf node uses a hash map to store the addresses of its child nodes. The cost of a pattern retrieval and deletion in F k is O ( k ).

Since we only need the maximal candidates for top-down generation in Phase 2, we remark that it is possible to adapt existing algorithms of maximal FP mining (e.g., [6, 13]) to achieve a faster candidate generation in Phase 1.
To discover only maximal p-FPs, TODIS can be modi-fied as follows. First, the same Phase 1 is used to find the candidate patterns. In Phase 2, once we have identified a p-FP, we do not compute the support pmf of its sub-patterns. Finding maximal p-FPs with TODIS is generally faster than p-Apriori, since (1) long patterns that are potentially max-imal p-FPs can be quickly yielded in Phase 1; and (2) to obtain a maximal pattern, there is no need to generate the support pmf for any of its sub-patterns. Next, we examine how association rules can be generated from p-FPs. We now discuss efficient techniques for tackling Problem 3. In Section 5.1 we present an algorithm to compute the prob-ability of an association rule. Then, Section 5.2 explains how p-ARs can be obtained.
Let X and Y be disjoint patterns, i.e., X  X  Y =  X  . To check whether X  X  Y is a p-AR, we have to compute the probability P ( X  X  Y ), and compare it against minprob (Equation 6). As discussed, computing this probability by expanding possible worlds is not practical. Given that X and XY are p-FPs, and their support pmfs are f X and f XY we now explain how to efficiently evaluate P ( X  X  Y ).
Step 1. Find f X Y . Notice that the sets of tuples that contain XY and X Y are disjoint. Hence, sup ( XY ) and sup ( X Y ) are independent random variables. Since sup ( X ) = sup ( XY )+ sup ( X Y ), f X is the convolution of f XY and f Conversely, f X Y is the deconvolution of f X and f XY . By us-ing Fast Fourier Transform methods, deconvolution can also be implemented in O ( n log n ) time. The details can be found in [24].

Step 2. Compute P ( X  X  Y ) by using the values of f XY and f X Y , based on the following lemma: Lemma 4. The probability of X  X  Y is: Proof.

Since XY and X Y are disjoint, the above becomes: Hence, Lemma 4 is correct.
 Algorithm 5 : Computing prob. of a p-AR
Algorithm 5 implements the above procedures. We first evaluate the deconvolution f X Y (Step 2). Then, Steps 7-14 compute P ( X  X  Y ) with Equation 8. While a basic implementation of Equation 8 requires O ( n 2 ) time, we use a variable, prCum , to accumulate the temporary sum of f X Y [ j ]. Then Equation 8 can be evaluated in O ( n ) time. Hence, Algorithm 5 has a O ( n log n ) cost.
We now explain how to obtain the p-ARs from the p-FPs generated by p-FP mining algorithms. We first present the following lemma.
Lemma 5 (Anti-monotonicity). Let X be a p-FP, X 0 and X 00 be non-empty patterns where X 00  X  X 0  X  X . If X  X  X 0  X  X 0 is a p-AR, then X  X  X 00  X  X 00 is a p-AR.
Proof. Let X be a p-FP, and X 0 and X 00 be non-empty sub-patterns of X , where X 00  X  X 0 . Suppose that W 0 and W 00 are the set of possible worlds where X  X  X 0  X  X and X  X  X 00  X  X 00 are association rules respectively. For each possible world W i  X  W , if X  X  X 0  X  X 0 is an asso-ciation rule, then so is X  X  X 00  X  X 00 , based on the anti-monotonicity property of association rules for exact data [4]. We know that P ( X  X  X 0  X  X 0 ) = P W P ( X  X  X 00  X  X 00 ) = P W is an association rule, we have: P ( X  X  X 00  X  X 00 )  X  P ( X  X  X 0  X  X 0 )  X  minprob . By Equation 6, X  X  X 00  X  X 00 is therefore also a p-AR.

This lemma implies that if XY Z is a p-FP and X  X  Y Z is a p-AR, then so is XY  X  Z and XZ  X  Y .

To generate p-ARs, we adopt the framework of the Apriori algorithm [4], which was developed for exact data. Specif-ically, let X i be a size-i sub-pattern of X . For a p-FP X , we enumerate all X 1  X  X  as the consequent of the rule. Then we check whether X  X  X 1  X  X 1 is a p-AR by computing its probability with Algorithm 5. If X  X  X 1  X  X 1 is not a p-AR, then for any X i where X 1  X  X i , X  X  X i  X  X i cannot be a p-AR (by Lemma 5). Hence, we can stop examining the rules that contain the superset of X 1 as the consequent. Otherwise, we check the validity of X  X  X 2  X  X 2 , for every X 2 such that X 1  X  X 2 . The checking goes on with larger X s, until we find that X  X  X i  X  X i is not a p-AR, or X i = X . We repeat this for every p-FP X .
We evaluated the performance of our solutions on two datasets. The first one, called T 25 I 10 D 500, is provided by the IBM data generator 1 . The average length of a trans-action is 25, the average length of a frequent pattern is 10, and the dataset size n is 500 k . The existential probability of each tuple is randomly drawn between [0 , 1]. Since no item has support larger than 10% of n , this dataset is sparse, and we set minsup to be 0 . 65%  X  n .

The second dataset, called Accident , comes from the Fre-quent Itemset Mining (FIMI) Dataset Repository 2 . It con-tains 340 k transactions, which record traffic accidents in the Flanders-Belgium region during 1991-2000. Each transac-tion stores the attributes (e.g., time, place and car type) of an accident. The average length of a frequent pattern is 45, and the number of items is 572. The existential probabil-ity of each transaction is a random variable, drawn from a Gaussian distribution with mean 0.5 and variance 0.02. The default value of minsup is 35% of n .

For both datasets, the default values of minprob and min-conf are 0.5 and 0.9 respectively. All the experiments were carried out on the Windows XP operating system, on a ma-chine with a 2.66 GHz Intel Core Duo processor and 2GB memory. The programs were written in C++ and compiled on Microsoft Visual Studio 2005. We next present the re-sults for the two datasets in Sections 6.1 and 6.2. http://www.almaden.ibm.com/cs/disciplines/iis/ http://fimi.cs.helsinki.fi/data/ (a)Basic solution. We compared the running time of p-Apriori and the  X  X asic X  solution, which finds p-FP by ex-panding possible worlds. For p-Apriori, we used DP to com-pute support pmf. Figure 8(a) shows that the performance of the basic solution degrades sharply with a slight increase of n , due to the exponential growth of the number of possi-ble worlds. p-Apriori does not expand possible worlds, and so its performance is much better. In fact, all our solutions (i.e., p-Apriori and TODIS) significantly outperform the ba-sic solution. Next, we focus on our solutions. (b)ip-list. We then studied the benefit of using the ip-list in p-Apriori. We used DP-n and DC-n to denote the versions of DP and DC that do not use the ip-list. Figure 8(b) shows that DP ( DC ) perform better than DP-n (correspondingly DC-n ). Hence, it is worthwhile to build and maintain the ip-list. For example, when n = 8 k , DC is two order of magnitudes faster than DC-n . We next assume that the ip-list is used. (c)Scalability. We compared p-Apriori and TODIS over a wide range of n in Figure 8(c). For both algorithms, we examined DP and DC . The four variants scale well with n . Also, DC is always better than DP . When TODIS and DC are used together, the best result is yielded. At n = 1000 k , TODIS-DC is more than one order of magnitude faster than pApriori-DP . (d)Effect of minsup . Figure 8(d) compared p-Apriori and TODIS under different minsup values. When minsup decreases, the performance gap between pApriori-DP and TODIS-DP increases. With a smaller minsup , longer patterns have to be tested, and so computing their pmf from scratch is more expensive. TODIS , which updates pmfs instead, ben-efits more in handling long patterns. The same can be said for pApriori-DC and TODIS-DC . Again TODIS-DC is the best. (e)Analysis of TODIS. Recall that TODIS spends some effort (in Phase 1) to generate candidate patterns, so that they can be used to identify p-FPs in Phase 2. Over differ-ent values of minsup , the time spent on Phase 1 is a small fraction of that of Phase 2 (Figure 8(e)). The effort of find-ing candidate patterns in Phase 1 is thus worthwhile, since this facilitates the top-down pmf update in Phase 2. Hence TODIS outperforms p-Apriori. We also investigate the ef-fectiveness of pruning lemmas used in Phase 1. Let r be the number of true p-FPs. The number of candidates generated in Phase 1 is less than 2 r under different minsup settings. (f )Effect of minprob . Figure 8(f) showed that the run-ning times of p-Apriori and TODIS increase with a lower minprob . A lower minprob implies that more patterns can become p-FPs. Hence both algorithms spend more time to compute their pmfs. Again, TODIS outperforms p-Apriori. (g)Maximal p-FPs. Figure 8(g) studied algorithms for finding maximal p-FPs. Since the results produced by TODIS and p-Apriori contain the maximal p-FPs, these p-FPs can be obtained from their results. We also see that TODIS-MAX (Section 4.2) is faster than TODIS , since not all p-FPs have their pmfs evaluated. At minsup = 0 . 7%  X  n , TODIS-MAX is 20% faster than TODIS . (h)Finding p-ARs. In Figure 8(h), we examined the use of the anti-monotonicity property (Lemma 5) for generating p-ARs. The running time increases with smaller minprob , since more rules become p-ARs. Under a wide range of minprob , Lemma 5 prunes a lot of rules without computing their probabilities. Hence, the speed of generating p-ARs increases significantly.
Since most experiments on the real dataset show a sim-ilar trend as the synthetic data, we only present the most representative ones. (i)Effect of minsup . Figure 8(i) compared p-Apriori and TODIS using DC . We do not show the results for DP , since they take more than three hours to complete. TODIS substantially outperforms p-Apriori; at minsup = 35%  X  n , TODIS is 75% faster than p-Apriori. This big difference is due to the fact that the real dataset is dense  X  regard-less of the tuple probability, 11 singletons have a support of more than 80%. This results in long ip-lists, which benefits TODIS more than p-Apriori. (j)Maximal p-FPs. As shown in Figure 8(j), TODIS-MAX is much better than TODIS . When minsup is 35%  X  n , TODIS-MAX is 70% faster than TODIS . The dense dataset implies that the maximal p-FPs can be large. A maximal p-FP contains many subpatterns. While TODIS has to compute pmfs for all subpatterns, TODIS-MAX does not do so. Hence TODIS-MAX is better than TODIS . (k)Finding p-ARs. From Figure 8(k), we observed that Lemma 5 effectively reduces its running time under differ-ent minprob values. Finally, Figure 8(l) showed that the p-AR algorithm needs more time to complete with a lower minconf . When minconf decreases, more rules can become p-ARs, and so more time is spent on computing their prob-abilities. The running time flattens when minconf  X  0 . 75. We found that most rules generated have confidence higher than 0.75, and so a decrease of minconf does not have much effect on the performance.
We studied efficient algorithms for extracting frequent patterns from probabilistic databases. The TODIS algo-rithm, when used together with DC, yields the best perfor-mance. We examined the efficient mining of p-ARs, which, to our knowledge, has not been studied before. We plan to study the discovery of association rules for other uncertain data models. We will also consider to extend existing mining methods on exact databases, to handle probabilistic data.
This work was supported by the Research Grants Coun-cil of Hong Kong (GRF Projects 513508 and 711309E). We would like to thank the anonymous reviewers for their in-sightful comments. [1] A. Deshpande et al. Model-driven data acquisition in [2] C. Aggarwal, Y. Li, J. Wang, and J. Wang. Frequent [3] C. Aggarwal and P. Yu. A survey of uncertain data [4] R. Agrawal and R. Srikant. Fast algorithms for mining [5] R. Bayardo, Jr. Efficiently mining long patterns from [6] D. Burdick, M. Calimlim, J. Flannick, J. Gehrke, and [7] H. Cheng, P. Yu, and J. Han. Approximate frequent [8] R. Cheng, D. Kalashnikov, and S. Prabhakar.
 [9] C. K. Chui, B. Kao, and E. Hung. Mining frequent [10] G. Cormode and M. Garofalakis. Sketching [11] N. Dalvi and D. Suciu. Efficient query evaluation on [12] M. Garofalakis and A. Kumar. Wavelet synopses for [13] K. Gouda and M. J. Zaki. GenMax: An efficient [14] R. Hogg, A. Craig, and J. Mckean. Introduction to [15] J. Huang et al. MayBMS: A Probabilistic Database [16] N. Khoussainova, M. Balazinska, and D. Suciu. [17] D. Knuth. The art of computer programming, vol. 3 . [18] H. Kriegel and M. Pfeifle. Density-based clustering of [19] C. Kuok, A. Fu, and M. Wong. Mining fuzzy [20] A. Lu, Y. Ke, J. Cheng, and W. Ng. Mining vague [21] M. Mutsuzaki et al. Trio-one: Layering uncertainty [22] M. Yiu et al. Efficient evaluation of probabilistic [23] R. Motwani and P. Raghavan. Randomized algorithms . [24] A. Oppenheim, R. Schafer, and J. Buck. Discrete-time [25] P. Sistla et al. Querying the uncertain position of [26] J. Ren, S. Lee, X. Chen, B. Kao, R. Cheng, and [27] T. Bernecker et al. Probabilistic frequent itemset [28] T. Jayram et al. Avatar information extraction [29] P. Tan, M. Steinbach, and V. Kumar. Introduction to [30] C. Yang and W. Najm. Examining driver behavior [31] Q. Zhang, F. Li, and K. Yi. Finding frequent items in
