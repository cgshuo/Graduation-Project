 In this paper, a problem, called "the density divergence problem" is explored. This problem is related to the phe-nomenon that the densities of the clusters vary in di erent subspace cardinalities. We take the densities into consid-eration in subspace clustering and explore an algorithm to adaptively determine di erent density thresholds to discover clusters in di erent subspace cardinalities.
 Categories and Subject Descriptors: H.2.8 [Database Applications]: Data mining General Terms: Algorithms, Management Keywords: Subspace clustering, Density divergence prob-lem
Among recent studies on high-dimensional data cluster-ing, subspace clustering is the task of automatically de-tecting clusters in subspaces of the original feature space. Most of previous works [1][2][4][5] adopt the density-based approach, where clusters are regarded as regions of high den-sity in a subspace that are separated by regions of lower density.

However, a critical problem, called " the density divergence problem " is ignored in mining subspace clusters such that it is infeasible for previous subspace clustering algorithms to simultaneously achieve high precision and recall 1 for clusters in di erent subspace cardinalities. "The density divergence problem" refers to the phenomenon that the cluster densi-ties vary in di erent subspace cardinalities. Note that as the number of dimensions increases, data points are spread out in a larger dimensional space such that they will be more sparsely populated in nature. This phenomenon implies that finding clusters in higher subspaces should be with a lower density requirement (otherwise we may lose true clusters in such situations), thus showing the existence of the den-sity divergence problem. Due to the requirement of varying density thresholds for clusters in di erent dimensions, it is challenging in subspace clustering to simultaneously achieve high precision and recall for clusters in di erent subspace
For a cluster, recall is defined as the percentage of the data points in a true cluster, that are identified in this cluster. Precision is defined as the percentage of the data points in this cluster, that really belong to the true cluster. cardinalities. More explicitly, since previous subspace clus-tering algorithms [1][2][4][5] identify the high-density regions (clusters) in all subspaces with a s ingle density threshold, the trade-o between recall and precision will be inevitably faced. The density threshold should be set low enough for the higher dimensional clusters being discovered with high recall. However, for the clusters in lower subspace, the low threshold may lead the low-density regions around the clus-ters to be discovered as dense ones, thus resulting in de-creased precision of the lower dimensional clusters.
Clearly, the trade-o between precision and recall in pre-vious subspace clustering, which is incurred by the "density divergence problem," solely depends on the determination of the density threshold. However, it is quite subtle to set an appropriate density threshold, and the parameter deter-mination is fully left unsolved to users, thus degrading the applicability of subspace clustering. A reasonable consid-eration is to take densities into account, since clusters are of di ering densities in di erent subspace cardinalities. To achieve this, we devise an innovative algorithm to adaptively determine the density thresholds for di erent cardinalities.
To extract clusters with di erent density thresholds in dif-ferent cardinalities is useful but is quite challenging. Note that previous algorithms are infeasible in such an environ-ment due to the lack of monotonicity property. That is, if a n -dimensional unit is dense, any ( n 3 1) -dimensional projec-tion of this unit may not be dense. Without the monotonic-ity property, the Apriori-like generate-and-test scheme adopted in almost previous works cannot be adopted for discovering all dense units in our model. A direct extension of previous methods is to execute a subspace algorithm once for each subspace cardinality n by setting the corresponding density threshold to find all n -dimensional dense units. However, it is very time consuming due to repeated execution of the targeted algorithm and repeated scans of database. To ef-ficiently discover dense units, a practicable way would be to store the complete information of the dense units in all subspace cardinalities into a compact structure such that the mining process can be directly performed in memory without repeated database scans.

Motivated by this idea, we propose to construct a com-pact structure which is extended from the FP-tree [3] to store the crucial information of the dataset. The base idea is by transforming the problem of identifying "dense units" in subspace clustering into a similar problem of discovering "frequent itemsets" in association rule mining. Thus, we construct the compact structure by storing the complete in-formation of the dense units satisfying di erent thresholds in di erent subspace cardinalities such that dense units can be discovered from this structure e ! ciently.
We adopt the grid-based approach to discover subspace clusters, where the data space is partitioned into a number of non-overlapping rectangular units by dividing each attribute into equal-length intervals. Consider the projection of the data set in a n -dimensional subspace. A " n -dimensional unit" u is defined as the intersection of one interval from each of the n attributes. Let count(u) denote the number of data points contained in unit x . Note that the units in the same subspace cardinalities have the same size, and there-fore we can use the count values to approximate the densities of the units in the same subspace cardinalities. Thus, the n -dimensional clusters can be discovered by first identifying the n -dimensional dense units, and then grouping the con-nected ones into clusters. Two n -dimensional units x 1 &gt;x are connected if they have a common face or if there exists another n -dimensional unit x 3 such that both x 1 and x 2 connected to x 3 .

For identifying dense units, we propose to use di erent density thresholds for di erent subspace cardinalities. Let n denote the density threshold for the subspace cardinality n ,andlet Q be the total number of data points. In ad-dition, an user input parameter ,calledthe unit strength factor , is introduced for specifying how dense a unit would be identified as a dense one. Then, we define the density threshold n as: When the data are uniformly distributed in a n -dimensional subspace, the number of data points in each of the n n -dimensional units in this subspace will be Q@ n ,i.e. the average unit density. In this scenario, there are no clusters discovered because everywhere in this space is almost of the same density. As the data are more compacted into clusters, the units within clusters will be much denser and would have a larger count value than the average density. Thus, the input parameter is introduced such that a n -dimensional unit will be identified as a dense one if its count value exceeds times of the average unit density, i.e., Q@ n .

In addition, a user parameter, n max , is introduced for spec-ifying the maximal subspace cardinality in such a way that clusters in cardinality up to n max are discovered. Problem Definition: Given the unit strength factor and the maximal cardinality n max , for the subspaces with cardinality n from 1 to n max , find the clusters in which each is a maximal set of connected dense n -dimensional units whose count values exceed the density threshold n =
We explore a method to discover the dense units in sub-space cardinality from 1 to n max .Afterthedenseunits are mined, we can follow the procedure proposed in [1] to discover the clusters, where the connected dense units are grouped into clusters. Therefore, we focuses on discovering the dense units in all subspaces.

The challenge of discovering dense units satisfying dif-ferent density thresholds in di erent subspace cardinalities is that the monotonicity property no longer exists. That is, if a n -dimensional unit satisfies the threshold n ,any ( n 3 1) -dimensional projection of this unit may not satisfy the threshold n 3 1 . Without the monotonicity property, the Apriori-like candidate generate-and-test scheme adopted in almost previous works cannot be adopted for discovering the dense units in our model.

To remedy this, we propose to first transform the dataset and then condense it with a compact structure for e ! ciently discovering dense units. The base idea is by transforming the problem of identifying "dense units" in subspace clus-tering into a similar problem of discovering "frequent item-sets" in association rule mining. Note that a n -dimensional unit can be represented by the set of n 1 -dimensional units, corresponding to the n intervals. By transforming each g -dimensional data record into g 1-dimensional units it resides in, the count value of a n -dimensional unit can be calcu-lated by directly counting the occurrences of the set of n dimensional units in the transformed dataset. Thus, finding n -dimensional dense units whose counts exceed the thresh-old n is similar to mining frequent n -itemsets satisfying the minimum support. After the dataset is transformed, we can extend the FP-tree [3] for subspace clustering to store the complete information of the dense units satisfying di erent thresholds. Thus, dense units can be discovered e ! ciently from the tree, and then clusters are formed by grouping the connected dense units.
We in this paper studied the density conscious subspace clustering to take consideration of the densities into sub-space clustering. An innovative algorithm was proposed to discover clusters in di erent cardinalities with di erent den-sity thresholds such that clusters in di erent cardinalities can be discovered with both high precision and recall. Acknowledgements The work was supported in part by the National Science Council of Taiwan, R.O.C., under Contracts NSC93-2752-E-002-006-PAE. [1] R. Agrawal, J. Gehrke, D. Gunopulos, and [2] C.H.Cheng,A.W.Fu,andY.Zhang.Entropy-based [3] J.Han,J.Pei,andY.Yin.MiningFrequentPatterns [4] K. Kailing, H.-P. Kriegel, and P. Kroger.
 [5] H.S.Nagesh,S.Goil,andA.Choudhary.Adaptive
