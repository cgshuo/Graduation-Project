 A typical mixture model is composed of a number of separately parameterized density models each of which has two important properties: it is typically fitted by using the EM algorithm that alternat es between two steps. The E-step uses property 1 to compute the posterior probability that each da tapoint came from each of the component models. The posterior is also called the  X  X esponsibility X  o f each model for a datapoint. The M-step uses property 2 to update the parameters of each model to rais e the responsibility-weighted sum of the log probabilities it assigns to the datapoints. The M-st ep also changes the mixing proportions of the component models to match the proportion of the training data that they are responsible for. Restricted Boltzmann Machines [5] model binary data-vecto rs using binary latent variables. They are considerably more powerful than mixture of multivariat e Bernoulli models 1 because they allow many of the latent variables to be on simultaneously so the nu mber of alternative latent state vectors is exponential in the number of latent variables rather than being linear in this number as it is with a mixture of Bernoullis. An RBM with N hidden units can be viewed as a mixture of 2 N Bernoulli models and with the 2 N mixing proportions being implicitly determined by the same parameters. Figure 1: (a) Schematic representation of an RBM, (b) an impl icit mixture of RBMs as a third-order Boltzmann machine, (c) schematic representation of an impl icit mixture.
 It can also be viewed as a product of N  X  X ni-Bernoulli X  models (plus one Bernoulli model that is implemented by the visible biases). A uni-Bernoulli model i s a mixture of a uniform and a Bernoulli. The weights of a hidden unit define the i th probability in its Bernoulli model as p where  X  ( x ) = (1 + exp(  X  x ))  X  1 .
 The modeling power of an RBM can always be increased by increa sing the number of hidden units [10] or by adding extra hidden layers [12], but for datasets t hat contain several distinctly differ-ent types of data, such as images of different object classes , it would be more appropriate to use a mixture of RBM X  X . The mixture could be used to model the raw da ta or some preprocessed rep-resentation that has already extracted features that are sh ared by different classes. Unfortunately, RBM X  X  cannot easily be used as the components of mixture mode ls because they lack property 1: It is easy to compute the unnormalized density that an RBM assigns to a datapoint, but the normal-ization term is exponentially expensive to compute exactly and even approximating it is extremely time-consuming [11]. There is also no efficient way to modify the parameters of an RBM so that the log probability of the data is guaranteed to increase, bu t there are good approximate methods [5] so this is not the main problem. This paper describes a way of fi tting a mixture of RBM X  X  without explicitly computing the partition function of each RBM. We start with the energy function for a Restricted Boltzmann Machine (RBM) and then modify it to define the implicit mixture of RBMs. To simplify the descri ption, we assume that the visible and hidden variables of the RBM are binary. The formulation belo w can be easily adapted to other types of variables (e.g., see [13]).
 The energy function for a Restricted Boltzmann Machine (RBM ) is where v is a vector of visible (observed) variables, h is a vector of hidden variables, and W R is a matrix of parameters that capture pairwise interactions b etween the visible and hidden variables. Now consider extending this model by including a discrete va riable z with K possible states, rep-resented as a K -dimensional binary vector with 1-of-K activation. Defining the energy function in terms of three-way interactions among the components of v , h , and z gives where W I is a 3D tensor of parameters. Each slice of this tensor along the z -dimension is a matrix that corresponds to the parameters of each of the K component RBMs. The joint distribution for the mixture model is where is the partition function of the implicit mixture model. Re-writing the joint distribution in the usual mixture model form gives Equation 5 defines the implicit mixture of RBMs. P ( v , h | z distribution, with W R being the k th slice of W I . Unlike in a typical mixture model, the mixing proportion P ( z via the energy function in equation 2. Changing the bias of th e k th unit in z changes the mixing proportion of the k th RBM, but all of the weights of all the RBM X  X  also influence it. F igure 1 gives a visual description of the implicit mixture model X  X  struct ure. Given a set of N training cases { v 1 , ..., v N } , we want to learn the parameters of the implicit mix-ture model by maximizing the log likelihood L = P N gradient-based optimization to do this. The expression for the gradient is where hi equation 6 can be estimated by sample means if unbiased sampl es can be generated from the corre-get around this problem by using the contrastive divergence (CD) learning algorithm [5], which has been found to be effective for training a variety of energy-b ased models (e.g. [8],[9],[13],[4]). Sampling the conditional distributions: We now describe how to sample the conditional distri-second case is easy: given z then sample from its conditional distribution P this distribution factorial. So the i th visible unit is drawn independently of the other units from t he Bernoulli distribution (see below) and sampled. Then, given z its conditional distribution P drawn from the Bernoulli distribution To compute P ( z | v ) we first note that where the free energy F ( v , z If the number of possible states of z is small enough, then it is practical to compute the quantity F ( v , z k = 1) for every k by brute-force. So we can compute Equation 11 defines the responsibility of the k th component RBM for the data vector v . Contrastive divergence learning: Below is a summary of the steps in the CD learning for the implicit mixture model. Repeating the above steps for a mini-batch of N for each component k in the mixture model: S + the approximate likelihood gradient (averaged over the min i-batch) for the k th component RBM is Note that to compute the outer products D + and D  X  for a given training vector, the component RBMs are selected through two separate stochastic picks . Therefore the sets S + be of the same size because the choice of the mixture componen t can be different for v Scaling free energies with a temperature parameter: In practice, the above learning algorithm causes all the training cases to be captured by a single compo nent RBM, and the other components to be left unused. This is because free energy is an unnormalize d quantity that can have very different numerical scales across the RBMs. One RBM may happen to produ ce much smaller free energies than the rest because of random differences in the initial pa rameter values, and thus end up with high responsibilities for most training cases. Even if all t he component RBMs are initialized to the exact same initial parameter values, the problem can still a rise after a few noisy weight updates. The solution is to use a temperature parameter T when computing the responsibilities: By choosing a large enough T , we can make sure that random scale differences in the free en ergies do not lead to the above collapse problem. One possibility is to start with a large T and then gradually anneal it as learning progresses. In our experiments we foun d that using a constant T works just as well as annealing, so we keep it fixed. We apply the implicit mixture of RBMs to two datasets, MNIST [ 1] and NORB [7]. MNIST is a set of handwritten digit images belonging to ten different c lasses (the digits 0 to 9). NORB contains stereo-pair images of 3D toy objects taken under different l ighting conditions and viewpoints. There are five classes of objects in this set ( human , car , plane , truck and animal ). We use MNIST mainly as a sanity check, and most of our results are for the much more difficult NORB dataset. Evaluation method: Since computing the exact partition function of an RBM is int ractable, it is not possible to directly evaluate the quality of our mixture model X  X  fit to the data, e.g., by computing Figure 2: Features of the mixture model with five component RB Ms trained on all ten classes of MNIST images. the log probability of a test set under the model. Recently it was shown that Annealed Importance Sampling can be used to tractably approximate the partition function of an RBM [11]. While this is an attractive option to consider in future work, for this p aper we use the computationally cheaper approach of evaluating the model by using it in a classificati on task. Classification accuracy is then used as an indirect quantitative measure of how good the mode l is.
 A reasonable evaluation criterion for a mixture modelling a lgorithm is that it should be able to find clusters that are mostly  X  X ure X  with respect to class labels . That is, the set of data vectors that a particular mixture component has high responsibilities fo r should have the same class label. So it should be possible to accurately predict the class label of a given data vector from the responsibilities evaluate it by training a classifier that takes as input the re sponsibilities of the mixture components for a data vector and predicts its class label. The goodness o f the mixture model is measured by the test set prediction accuracy of this classifier. 4.1 Results for MNIST Before attempting to learn a good mixture model of the whole M NIST dataset, we tried two simpler modeling tasks. First, we fitted an implicit mixture of two RB M X  X  with 100 hidden units each to an unlabelled dataset consisting of 4,000 twos and 4,000 thr ees. As we hoped, almost all of the two X  X  were modelled by one RBM and almost all of the threes by t he other. On 2042 held-out test cases, there were only 24 errors when an image was assign ed the label of the most probable RBM. This compares very favorably with logistic regression which needs 8000 labels in addition to the images and gives 36 errors on the test set even when usin g a penalty on the squared weights whose magnitude is set using a validation set. Logistic regr ession also gives a good indication of the performance that could be expected from fitting a mixture of t wo Gaussians with a shared covariance matrix, because logistic regression is equivalent to fittin g such a mixture discriminatively. We then tried fitting an implicit mixture model with only five c omponent RBMs, each with 25 hidden units, to the entire training set. We purposely make the mode l very small so that it is possible to visually inspect the features and the responsibilities of t he component RBMs and understand what each component is modelling. This is meant to qualitatively confirm that the algorithm can learn a sensible clustering of the MNIST data. (Of course, the model will have poor classification accuracy as there are more classes than clusters, so it will merge mult iple classes into a single cluster.) The features of the component RBMs are shown in figure 2 (top row). The plots in the bottom row show the fraction of training images for each of the ten classes th at are hard-assigned to each component. The learning algorithm has produced a sensible mixture mode l in that visually similar digit classes are combined under the same mixture component. For example, ones and eights require many similar features, so they are captured with a single RBM (lef tmost in fig. 2). Similarly, images of fours, sevens, and nines are all visually similar, and they a re modelled together by one RBM (middle of fig. 2). We have also trained larger models with many more mixture com ponents. As the number of com-ponents increase, we expect the model to partition the image space more finely, with the different components specializing on various sub-classes of digits. If they specialize in a way that respects the class boundaries, then their responsibilities for a dat a vector will become a better predictor of its class label.
 The component RBMs use binary units both in the visible and hi dden layers. The image dimension-ality is 784 (28  X  28 pixels). We have tried various settings for the number of m ixture components (from 20 to 120 in steps of 20) and a component X  X  hidden layer s ize (50, 100, 200, 500). Classifica-tion accuracy increases with more components, until 80 comp onents. Additional components give slightly worse results. The hidden layer size is set to 100, b ut 200 and 500 also produce similar accuracies. Out of the 60,000 training images in MNIST, we us e 50,000 to train the mixture model and the classifier, and the remaining 10,000 as a validation s et for early stopping. The final models are then tested on a separate test set of 10,000 images.
 Once the mixture model is trained, we train a logistic regres sion classifier to predict the class label from the responsibilities 2 . It has as many inputs as there are mixture components, and a t en-way softmax over the class labels at the output. With 80 componen ts, there are only 80 10 + 10 = 810 parameters in the classifier (including the 10 output biases ). In our experiments, classification accuracy is consistently and significantly higher when unnormalized responsibilities are used as the classifier input, instead of the actual posterior probabili ties of the mixture components given a data vector. These unnormalized values have no proper probabili stic interpretation, but nevertheless they allow for better classification, so we use them in all our expe riments.
 which indicates that the implicit mixture model has learned clusters that mostly agree with the class boundaries, even though it is not given any class informatio n during training. 4.2 Results for NORB NORB is a much more difficult dataset than MNIST because the im ages are of very different classes of 3D objects (instead of 2D patterns) shown from different v iewpoints and under various lighting conditions. The pixels are also no longer binary-valued, bu t instead span the grayscale range [0 , 255] . So binary units are no longer appropriate for the visible lay er of the component RBMs. Gaussian visible units have previously been shown to be effective for modelling grayscale images [6], and therefore we use them here. See [6] for details about Gaussia n units. As in that paper, the variance of the units is fixed to 1, and only their means are learned.
 Learning an RBM with Gaussian visible units can be slow, as it may require a much greater number of weight updates than an equivalent RBM with binary visible units. This problem becomes even worse in our case since a large number of RBMs have to be traine d simultaneously. We avoid it by first training a single RBM with Gaussian visible units and binary hidden units on the raw pixel data, and then treating the activities of its hidden layer as pre-processed data to which the implicit mixture model is applied. Since the hidden layer activities of the pre-processing RBM are binary, the mixture model can now be trained efficiently with binary unit s in the visible layer 3 . Once trained, the low-level RBM acts as a fixed pre-processing step that con verts the raw grayscale images into binary vectors. Its parameters are not modified further when training the mixture model. Figure 3 shows the components of the complete model.
 A difficulty with training the implicit mixture model (or any other mixture model) on NORB is that the  X  X atural X  clusters in the dataset correspond to the six lighting conditions instead of the five object classes. The objects themselves are small (in terms o f area) relative to the background, while lighting affects the entire image. Any clustering signal pr ovided by the object classes will be weak compared to the effect of large lighting changes. So we simpl ify the dataset slightly by normalizing the lighting variations across images. Each image is multip lied by a scalar such that all images have the same average pixel value. This significantly reduce s the interference of the lighting on the mixture learning 4 . Finally, to speed up experiments, we subsample the images f rom 96  X  96 to 32  X  32 and use only one image of the stereo pair. We refer to this data set as  X  X odified NORB X  or  X  X NORB X . It contains 24,300 training images and an equal n umber of test images. From the training set, 4,300 are set aside as a validation set for earl y stopping.
 We use 2000 binary hidden units for the preprocessing RBM, so the input dimensionality of the implicit mixture model is 2000. We have tried many different settings for the number of mixture components and the hidden layer size of the components. The b est classification results are given by 100 components, each with 500 hidden units. This model has about 100 500 2000 = 10 8 parameters, and takes about 10 days to train on an Intel Xeon 3 Ghz processor.
 representations. Mixture of Factor Analyzers (MFA) [3] is s imilar to the implicit mixture of RBMs in that it also learns a clustering while simultaneously lea rning a latent representation per cluster component. But it is a directed model based on linear-Gaussi an representations, and it can be learned tractably by maximizing likelihood with EM. We train MFA on t he raw pixel data of MNORB. The MFA model that gives the best classification accuracy (shown in table 2) has 100 component Factor Analyzers with 100 factors each. (Note that simply making th e number of learnable parameters equal is not enough to match the capacities of the different m odels because RBMs use binary latent representations, while FAs use continuous representation s. So we cannot strictly control for capacity when comparing these models.) implicit mixture model whose component RBMs have no hidden u nits and only visible biases as trainable parameters. The differences are that a Bernoulli mixture is a directed model, it has explic-itly parameterized mixing proportions, and maximum likeli hood learning with EM is tractable. We train this model with 100 components on the activation proba bilities of the preprocessing RBM X  X  hidden units. The classification error rate for this model is shown in table 2. Table 2: MNORB Test set error rates for a logistic regression classifier with different types of input representations.
 structure in the data. By the classification accuracy criter ion, the implicit mixture is also better than MFA. The results also confirm that the lack of explicitly para meterized mixing proportions does not prevent the implicit mixture model from discovering intere sting cluster structure in the data. We have presented a tractable formulation of a mixture of RBM s. That such a formulation is even possible is a surprising discovery. The key insight here is t hat the mixture model can be cast as a third-order Boltzmann machine, provided we are willing to a bandon explicitly parameterized mixing proportions. Then it can be learned tractably using contras tive divergence. As future work, it would be interesting to explore whether these ideas can be extende d to modelling time-series data.
