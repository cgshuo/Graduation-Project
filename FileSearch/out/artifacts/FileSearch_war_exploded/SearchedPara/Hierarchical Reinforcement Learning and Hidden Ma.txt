 Surface realisation decisions in a Natural Language Generation (NLG) system are often made accord-ing to a language model of the domain (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; White, 2004; Belz, 2008). However, there are other linguistic phenomena, such as alignment (Pickering and Garrod, 2004), consis-tency (Halliday and Hasan, 1976), and variation, which influence people X  X  assessment of discourse (Levelt and Kelter, 1982) and generated output (Belz and Reiter, 2006; Foster and Oberlander, 2006). Also, in dialogue the most likely surface form may not always be appropriate, because it does not cor-respond to the user X  X  information need, the user is confused, or the most likely sequence is infelicitous with respect to the dialogue history. In such cases, it is important to optimise surface realisation in a uni-fied fashion with content selection. We suggest to use Hierarchical Reinforcement Learning (HRL) to achieve this. Reinforcement Learning (RL) is an at-tractive framework for optimising a sequence of de-cisions given incomplete knowledge of the environ-ment or best strategy to follow (Rieser et al., 2010; Janarthanam and Lemon, 2010). HRL has the ad-ditional advantage of scaling to large and complex problems (Dethlefs and Cuay  X ahuitl, 2010). Since an HRL agent will ultimately learn the behaviour it is rewarded for, the reward function is arguably the agent X  X  most crucial component. Previous work has therefore suggested to learn a reward function from human data as in the PARADISE framework (Walker et al., 1997). Since PARADISE-based re-ward functions typically rely on objective metrics, they are not ideally suited for surface realisation, which is more dependend on linguistic phenomena, e.g. frequency, consistency, and variation. However, linguistic and psychological studies (cited above) show that such phenomena are indeed worth mod-elling in an NLG system. The contribution of this paper is therefore to induce a reward function from human data, specifically suited for surface genera-tion. To this end, we train HMMs (Rabiner, 1989) on a corpus of grammatical word sequences and use them to inform the agent X  X  learning process. In addi-tion, we suggest to optimise surface realisation and content selection decisions in a joint, rather than iso-lated, fashion. Results show that our combined ap-proach generates more successful and human-like utterances than a greedy or random baseline. This is related to Angeli et al. (2010), who also address in-terdependent decision making, but do not use an opt-misation framework. Since language models in our approach can be obtained for any domain for which corpus data is available, it generalises to new do-mains with limited effort and reduced development time. For related work on using graphical models for language generation, see e.g., Barzilay and Lee (2002), who use lattices, or Mairesse et al. (2010), who use dynamic Bayesian networks. We are concerned with the generation of navigation instructions in a virtual 3D world as in the GIVE scenario (Koller et al., 2010). In this task, two peo-ple engage in a  X  X reasure hunt X , where one partici-pant navigates the other through the world, pressing a sequence of buttons and completing the task by obtaining a trophy. The GIVE-2 corpus (Gargett et al., 2010) provides transcripts of such dialogues in English and German. For this paper, we comple-mented the English dialogues of the corpus with a is given in Figure 1. This example also exempli-fies the type of utterances we generate. The input to the system consists of semantic variables compara-ble to the annotated values, the output corresponds to strings of words. We use HRL to optimise deci-sions of content selection ( X  X hat to say X ) and HMMs for decisions of surface realisation ( X  X ow to say it X ). Content selection involves whether to use a low-, or high-level navigation strategy. The former consists of a sequence of primitive instructions ( X  X o straight X ,  X  X urn left X ), the latter represents contractions of se-quences of low-level instructions ( X  X ead to the next room X ). Content selection also involves choosing a level of detail for the instruction corresponding to the user X  X  information need. We evaluate the learnt content selection decisions in terms of task success. For surface realisation , we use HMMs to inform the HRL agent X  X  learning process. Here we address the one-to-many relationship arising between a se-mantic form (from the content selection stage) and its possible realisations. Semantic forms of instruc-tions have an average of 650 surface realisations, including syntactic and lexical variation, and deci-sions of granularity. We refer to the set of alterna-tive realisations of a semantic form as its  X  X eneration space X . In surface realisation, we aim to optimise the tradeoff between alignment and consistency (Picker-ing and Garrod, 2004; Halliday and Hasan, 1976) on the one hand, and variation (to improve text quality and readability) on the other hand (Belz and Reiter, 2006; Foster and Oberlander, 2006) in a 50 / 50 dis-tribution. We evaluate the learnt surface realisation decisions in terms of similarity with human data.
Note that while we treat content selection and surface realisation as separate NLG tasks, their op-timisation is achieved jointly. This is due to a tradeoff arising between the two tasks. For exam-ple, while surface realisation decisions that are opti-mised solely with respect to a language model tend to favour frequent and short sequences, these can be inappropriate according to the user X  X  information need (because they are unfamiliar with the naviga-tion task, or are confused or lost). In such situa-tions, it is important to treat content selection and surface realisation as a unified whole. Decisions of both tasks are inextricably linked and we will show in Section 5.2 that their joint optimisation leads to better results than an isolated optimisation as in, for example, a two-stage model. 3.1 Hierarchical Reinforcement Learning The idea of language generation as an optimisa-tion problem is as follows: given a set of genera-tion states, a set of actions, and an objective reward function, an optimal generation strategy maximises the objective function by choosing the actions lead-ing to the highest reward for every reached state. Such states describe the system X  X  knowledge about the generation task (e.g. content selection, naviga-tion strategy, surface realisation). The action set describes the system X  X  capabilities (e.g.  X  X se high level navigation strategy X  ,  X  X se imperative mood X  , etc.). The reward function assigns a numeric value for each action taken. In this way, language gen-eration can be seen as a finite sequence of states, actions and rewards { s where the goal is to find an optimal strategy auto-matically. To do this we use RL with a divide-and-conquer approach to optimise a hierarchy of genera-tion policies rather than a single policy. The hierar-chy of RL agents consists of L levels and N models per level, denoted as M i and i  X  X  0 , ..., L  X  1 } . Each agent of the hierar-chy is defined as a Semi-Markov Decision Process (SMDP) consisting of a 4-tuple &lt; S i S a transition function that determines the next state s  X  from the current state s and the performed ac-tion a , and R i the reward that an agent receives for taking an ac-tion a in state s lasting  X  time steps. The random variable  X  represents the number of time steps the agent takes to complete a subtask. Actions can be either primitive or composite. The former yield sin-gle rewards, the latter correspond to SMDPs and yield cumulative discounted rewards. The goal of each SMDP is to find an optimal policy that max-imises the reward for each visited state, according to specifies the expected cumulative reward for execut-ing action a in state s and then following policy  X   X  i We use HSMQ-Learning (Dietterich, 1999) to learn a hierarchy of generation policies. 3.2 Hidden Markov Models for NLG The idea of representing the generation space of a surface realiser as an HMM can be roughly de-fined as the converse of POS tagging, where an in-put string of words is mapped onto a hidden se-quence of POS tags. Our scenario is as follows: given a set of (specialised) semantic symbols (e.g., likely sequence of words corresponding to the sym-bols? Figure 2 provides a graphic illustration of this idea. We treat states as representing words, and se-quences of states i sentences. An observation sequence o of a finite set of semantic symbols specific to the in-struction type (i.e.,  X  X estination X ,  X  X irection X ,  X  X rien -tation X ,  X  X ath X ,  X  X traight X ). Each symbol has an ob-servation likelihood b bility of observing o in state i at time t . The tran-sition and emission probabilities are learnt during training using the Baum-Welch algorithm. To de-sign an HMM from the corpus data, we used the ABL algorithm (van Zaanen, 2000), which aligns strings based on Minimum Edit Distance, and in-duces a context-free grammar from the aligned ex-amples. Subsequently, we constructed the HMMs from the CFGs, one for each instruction type, and trained them on the annotated data. 3.3 An HMM-based Reward Function Induced Due to its unique function in an RL framework, we suggest to induce a reward function for surface re-alisation from human data. To this end, we create and train HMMs to represent the generation space of a particular surface realisation task. We then use the forward probability, derived from the Forward algorithm, of an observation sequence to inform the agent X  X  learning process. Whenever the agent has generated a word sequence w to the likelihood of observing the sequence in the data. In addition, the agent is rewarded for short content selection (cf. Section 2). Note that while re-ward P ( w agents M 3 of the hierarchy. We test our approach using the (hand-crafted) hierar-chy of generation subtasks in Figure 2. It consists of a root agent ( M 0 and high-level ( M 2 for instruction types  X  X rientation X  ( M 3 ( tion X  ( M 3 face generation. They will be trained using HRL with an HMM-based reward function induced from human data. All other agents use hand-crafted re-wards. Finally, subtask M 1 system utterance. The states of the agent contain all situational and linguistic information relevant to its decision making, e.g., the spatial environment, space constraints, please see Dethlefs et al. (2011) for the full state-action space. We distinguish prim-itive actions (corresponding to single generation de-cisions) and composite actions (corresponding to generation subtasks (Fig. 2)). 5.1 The Simulated Environment The simulated environment contains two kinds of uncertainties: (1) uncertainty regarding the state of the environment, and (2) uncertainty concerning the user X  X  reaction to a system utterance. The first as-pect is represented by a set of contextual variables Altogether, this leads to 115 thousand different con-textual configurations, which are estimated from data (cf. Section 2). The uncertainty regarding the user X  X  reaction to an utterance is represented by a Naive Bayes classifier, which is passed a set of contextual features describing the situation, mapped with a set of semantic features describing the utter-most likely user reaction (after each system act) of and request help . 8 The classifier was trained on the annotated data and reached an accuracy of 82% in a cross-corpus validation using a 60% -40% split. 5.2 Comparison of Generation Policies We trained three different generation policies. The learnt policy optimises content selection and sur-face realisation decisions in a unified fashion, and is informed by an HMM-based generation space reward function. The greedy policy is informed only by the HMM and always chooses the most likely sequence independent of content selection. The valid sequence policy generates any grammat-ical sequence. All policies were trained for 20000 wards of all three policies (averaged over ten runs), shows that the  X  X earnt X  policy performs best in terms of task success by reaching the highest overall re-wards over time. An absolute comparison of the av-erage rewards (rescaled from 0 to 1 ) of the last 1000 training episodes of each policy shows that greedy improves  X  X ny valid sequence X  by 71% , and learnt improves greedy by 29% (these differences are sig-nificant at p &lt; 0 . 01 ). This is due to the learnt policy showing more adaptation to contextual features than the greedy or  X  X alid sequence X  policies. To evaluate human-likeness, we compare instructions (i.e. word sequences) using Precision-Recall based on the F-Measure score, and dialogue similarity based on the Kulback-Leibler (KL) divergence (Cuay  X ahuitl et al., 2005). The former shows how the texts generated by each of our generation policies compare to human-authored texts in terms of precision and recall. The latter shows how similar they are to human-authored texts. Table 1 shows results of the comparison of two human data sets  X  X eal1 X  vs  X  X eal2 X  and both of them together against our different policies. While the greedy policy receives higher F-Measure scores, the learnt policy is most similar to the human data. This is due to variation: in contrast to greedy be-haviour, which always exploits the most likely vari-ant, the learnt policy varies surface forms. This leads to lower F-Measure scores, but achieves higher sim-ilarity with human authors. This ultimately is a de-sirable property, since it enhances the quality and naturalness of our instructions. We have presented a novel approach to optimising surface realisation using HRL. We suggested to inform an HRL agent X  X  learning process by an HMM-based reward function, which was induced from data and in which the HMM represents the generation space of a surface realiser. We also proposed to jointly optimise surface realisation and content selection to balance the tradeoffs of (a) frequency in terms of a language model, (b) alignment/consistency vs variation, (c) properties of the user and environment. Results showed that our hybrid approach outperforms two baselines in terms of task success and human-likeness: a greedy baseline acting independent of content selection, and a random  X  X alid sequence X  baseline. Future work can transfer our approach to different domains to confirm its benefits. Also, a detailed human evaluation study is needed to assess the effects of different surface form variants. Finally, other graphical models besides HMMs, such as Bayesian Networks, can be explored for informing the surface realisation process of a generation system.
 Acknowledgments Thanks to the German Research Foundation DFG and the Transregional Collaborative Research Cen-tre SFB/TR8  X  X patial Cognition X  and the EU-FP7 project ALIZ-E (ICT-248116) for partial support of this work.
