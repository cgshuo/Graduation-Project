 attention since the birth of the field. The most common modern approaches scan the image for candidate objects and score each one. This is typified by the s liding-window object detection ap-proach [22, 20, 4], but is also true of most other detection sc hemes (such as centroid-based meth-e.g., [9, 20, 6]. Recent works are adopting a more holistic ap proach by combining the output of mul-tiple vision tasks [10, 11] and are reminiscent of some of the earliest work in computer vision [1]. However, these recent works use a different representation for each subtask, forcing information sharing to be done through awkward feature mappings. Anothe r difficulty with these approaches is that the subtask representations can be inconsistent. Fo r example, a bounding-box based object detector includes many pixels within each candidate detect ion window that are not part of the ob-ject itself. Furthermore, multiple overlapping candidate detections contain many pixels in common. How these pixels should be treated is ambiguous in such appro aches. A model that uniquely iden-encodes a bias of the true world (i.e., a visible pixel belong s to only one object).
In this work, we propose a more integrated region-based appr oach that combines multi-class im-age segmentation with object detection. Specifically, we pr opose a hierarchical model that reasons simultaneously about pixels, regions and objects in the ima ge, rather than scanning arbitrary win-dows. At the region level we label pixels as belonging to one o f a number of background classes (currently car and pedestrian ) or unknown .
Our model builds on the scene decomposition model of Gould et al. [7] which aims to decompose an image into coherent regions by dynamically moving pixel b etween regions and evaluating these moves relative to a global energy objective. These bottom-u p pixel moves result in regions with co-herent appearance. Unfortunately, complex objects such as people or cars are composed of several dissimilar regions which will not be combined by this bottom -up approach. Our new hierarchi-cal approach facilitates both bottom-up and top-down reaso ning about the scene. For example, we can propose an entire object comprised of multiple regions a nd evaluate this joint move against our global objective. Thus, our hierarchical model enjoys the b est of two worlds: Like multi-class image segmentation, our model uniquely explains every pixel in th e image and groups these into seman-tically coherent regions. Like object detection, our model uses sophisticated shape and appearance features computed over candidate object locations with pre cise boundaries. Furthermore, our joint model over regions and objects allows context to be encoded t hrough direct semantic relationships (e.g.,  X  X ar X  is usually found on  X  X oad X ). Our method inherits features from the sliding-window objec t detector works, such as Torralba et al. [19] and Dalal and Triggs [4], and the multi-class image segm entation work of Shotton et al. [16]. We further incorporate into our model many novel ideas for im proving object detection via scene context. The innovative works that inspire ours include pre dicting camera viewpoint for estimat-ing the real world size of object candidates [12], relating  X  things X  (objects) to nearby  X  X tuff X  (re-gions) [9], co-occurrence of object classes [15], and gener al scene  X  X ist X  [18].

Recent works go beyond simple appearance-based context and show that holistic scene under-standing (both geometric [11] and more general [10]) can sig nificantly improve performance by combining related tasks. These works use the output of one ta sk (e.g., object detection) to provide features for other related tasks (e.g., depth perception). While they are appealing in their simplic-ity, current models are not tightly coupled and may result in incoherent outputs (e.g., the pixels in a bounding box identified as  X  X ar X  by the object detector, may be labeled as  X  X ky X  by an image segmentation task). In our method, all tasks use the same reg ion-based representation which forces consistency between variables. Intuitively this leads to m ore robust predictions.

The decomposition of a scene into regions to provide the basi s for vision tasks exists in some scene parsing works. Notably, Tu et al. [21] describe an appr oach for identifying regions in the scene. Their approach has only be shown to be effective on tex t and faces, leaving much of the framework, but do not provide an exact segmentation of the im age. Gould et al. [7] provides a com-plete description of the scene using dynamically evolving d ecompositions that explain every pixel (both semantically and geometrically). However, the metho d cannot distinguish between between foreground objects and often leaves them segmented into mul tiple dissimilar pieces. Our work builds on this approach with the aim of classifying objects.

Other works attempt to integrate tasks such as object detect ion and multi-class image segmenta-tion into a single CRF model. However, these models either us e a different representation for object and non-object regions [23] or rely on a pixel-level represe ntation [16]. The former does not enforce label consistency between object bounding boxes and the und erlying pixels while the latter does not distinguish between adjacent objects of the same class.
 window approach. However, unlike our method, they use a sing le over-segmentation of the image and make the strong assumption that each segment represents a (probabilistically) recognizable ob-ject part. Our method, on the other hand, assembles objects ( and background regions) using seg-ments from multiple different over-segmentations. The mul tiple over-segmentations avoids errors made by any one segmentation. Furthermore, we incorporate b ackground regions which allows us to eliminate large portions of the image thereby reducing the n umber of component regions that need to be considered for each object.

Liu et al. [14] use a non-parametric approach to image labeli ng by warping a given image onto a scales easily to a large number of classes. However, the meth od does not attempt to understand the row of cars will be parsed as a single region) and cannot captu re combinations of classes not present in the training set. As a result, the approach performs poorl y on most foreground object classes. We now present an overview of our joint object detection and s cene segmentation model. This model combines scene structure and semantics in a coherent energy function. 3.1 Energy Function Our model builds on the work of Gould et al. [7] which aims to de compose a scene into a number ( K ) of semantically consistent regions. In that work, each pixe l p in the image I belongs to exactly one region, identified by its region-correspondence variable R simply the set of pixels P objects. Double indices indicate pairwise terms between ad jacent entities (e.g., pq or rs ).
Regions, while visually coherent, may not encompass entire objects. Indeed, in the work of Gould ciency by allowing an object to be composed of many regions (r ather than trying to force dissimilar regions to merge). The object to which a region belongs is den oted by its object-correspondence variable O which we denote by O noted by P multiple disconnected components.
 model. Each pixel has a local appearance feature vector  X  appearance variable A label S object, in turn, has an associated object class label C component in our model is the horizon which captures global g eometry information. We assume that the image was taken by a camera with horizontal axis para llel to the ground and model the horizon v hz  X  [0 , 1] as the normalized row in the image corresponding to its locat ion. We quantize v
We combine the variables in our model into a single coherent e nergy function that captures the structure and semantics of the scene. The energy function in cludes terms for modeling the location of the horizon, region label preferences, region boundary q uality, object labels, and contextual re-lationships between objects and regions. These terms are de scribed in detail below. The combined energy function E ( R , S , O , C ,v hz |I ,  X  ) has the form:
E =  X  hz ( v hz ) + X (appearance and shape) belonging to the regions, i.e.,  X  reg that all terms are conditioned on the observed image I and model parameters  X  . The summation over context terms includes all ordered pairs of adjacent ob jects and regions, while the summation function is shown in Figure 1.
 The first three energy terms are adapted from the model of [7]. We briefly review them here:
Horizon term. The  X  hz term captures the a priori location of the horizon in the scen e and, in our learned from labeled training images.

Knowing the location of the horizon allows us to compute the w orld height of an object in the scene. Using the derivation from Hoiem et al. [12], it can be s hown that the height y (or region) in the scene can be approximated as y origin above the ground, and v object/region, respectively. In our current work, we assum e that all images were taken from the foreground ). For convenience we include the v hz variable in this term to provide rough geometry information. If a region is associated with an object, then w e constrain the assignment of its class label to foreground (e.g., a  X  X ky X  region cannot be part of a  X  X ar X  object).
More formally, let N  X  r : P r ,v where  X  ( ) is the multi-class logit  X  ( y | x ;  X  ) = exp {  X  T y x } region term versus the other terms in the model.

Boundary term. The term  X  bdry penalizes two adjacent regions with similar appearance or l ack of boundary contrast. This helps to merge coherent pixels in to a single region. We combine two denote the Mahalanobis distance between vectors x and y , and E boundary. Then the boundary term is where the  X  pixels and neighboring pixels, respectively. In our experi ments we restrict  X   X   X  =  X  the trade-off between the region similarity and boundary co ntrast terms and weight them against the other terms in the energy function (Equation 1).

Note that the boundary term does not include semantic class o r object information. The term purely captures segmentation coherence in terms of appeara nce.

Object term. Going beyond the model in [7], we include object terms  X  obj in our energy function that score the likelihood of a group of regions being assigne d a given object label. We currently classify objects as either car , pedestrian or unknown . The unknown class includes objects like trash is defined by a logistic function that maps object features  X  each object class. However, since our region layer already i dentifies foreground regions, we would like our energy to improve only when we recognize known objec t classes. We therefore bias the object term to give zero contribution to the energy for the cl ass unknown . 1 Formally we have where N
Context term. Intuitively, contextual information which relates object s to their local background can improve object detection. For example, Heitz and Koller [9] showed that detection rates im-prove by relating  X  X hings X  (objects) to  X  X tuff X  (backgroun d). Our model has a very natural way of encoding such relationships through pairwise energy terms between objects C do not encode contextual relationships between region clas ses (i.e., S help. 2 Contextual relationships between foreground objects (i.e ., C (e.g., people found on bicycles), but are not considered in t his work. Formally, the context term is where  X  energy function. Since the pairwise context term is between objects and (background) regions it grows linearly with the number of object classes. This has a d istinct advantage over approaches which include a pairwise term between all classes resulting in quadratic growth. 3.2 Object Detectors Performing well at object detection requires more than simp le region appearance features. Indeed, to our object feature vector  X  methods for adapting state-of-the-art object detector tec hnologies for this purpose.
In the first approach, we treat the object detector as a black-box that returns a score per (rectan-gular) candidate window. However, recall that an object in o ur model is defined by a contiguous set of pixels P make classification more robust we search candidate windows in a small neighborhood (defined over scale and position) around this bounding box, and take as our feature the output of highest scoring window. In our experiments we test this approach using the HO G detector of Dalal and Triggs [4] which learns a linear SVM classifier over feature vectors con structed by computing histograms of gradient orientations in fixed-size overlapping cells with in the candidate window.

Note that in the above black-box approach many of the pixels w ithin the bounding box are not actually part of the object (consider, for example, an L-sha ped region). A better approach is to mask out all pixels not belonging to the object. In our implementa tion, we use a soft mask that attenuates errors. The masked window is used at both training and test ti me. In our experiments we test this more integrated approach using the patch-based features of Torralba et al. [19, 20]. Here features are extracted by matching small rectangular patches at vari ous locations within the masked window and combining these weak responses using boosting. Object a ppearance and shape are captured by operating on both the original (intensity) image and the edg e-filtered image.

For both approaches, we append the score (for each object) fr om the object detection classifiers X  linear SVM or boosted decision trees X  X o the object feature ve ctor  X 
An important parameter for sliding-window detectors is the base scale at which features are ex-tracted. Scale-invariance is achieved by successively dow n-sampling the image. Below the base-scale, feature matching becomes inaccurate, so most detect ors will only find objects above some minimum size. Clearly there exists a trade-off between the d esire to detect small objects, feature high-resolution images while still being able to identify s mall objects, we employ a multi-scale ap-proach. Here we run our scene decomposition algorithm on a lo w-resolution ( 320  X  240 ) version object-detector features we map the object pixels P at the higher resolution. We now describe how we perform inference and learn the parame ters of our energy function. 4.1 Inference We use a modified version of the hill-climbing inference algo rithm described in Gould et al. [7], which uses multiple over-segmentations to propose large mo ves in the energy space. An overview image using an off-the-shelf unsupervised segmentation al gorithm (in our experiments we use mean-shift [3]). We then run inference using a two-phased approac h.
 objects. Thus we remove the object variables O and C from the model and artificially increase the boundary term weights (  X  bdry exactly as in [7] by iteratively proposing re-assignments o f pixels to regions (variables R ) and re-new configuration is lower, the move is accepted, otherwise t he previous configuration is restored and the algorithm proposes a different move. The algorithm p roceeds until no further reduction in energy can be found after exhausting all proposal moves from a pre-defined set (see Section 4.2).
In the second phase, we anneal the boundary term weights and i ntroduce object variables over all foreground regions. We then iteratively propose merges and splits of objects (variables O ) as well as high-level proposals (see Section 4.2 below) of new r egions generated from sliding-window object candidates (affecting both R and O ). After a move is proposed, we recompute the optimal cannot be reduced by any of the proposal moves.
 features and energy terms for the regions affected by a move. However, inference is still slow given To improve running time, we leave the context terms  X  ctxt out of the model until the last iteration through the proposal moves. This allows us to maximize each r egion term independently during each proposal step X  X e use an iterated conditional modes (ICM ) update to optimize v hz after the region labels have been inferred. After introducing the con text term, we use max-product belief propagation to infer the optimal joint assignment to S and C . Using this approach we can process an image in under five minutes. 4.2 Proposal Moves We now describe the set of pixel and region proposal moves con sidered by our algorithm. These moves are relative to the current best scene decomposition a nd are designed to take large steps in the energy space to avoid local minima. As discussed above, e ach move is accepted if it results in a lower overall energy after inferring the optimal assignmen t for the remaining variables. The most basic move is to merge two adjacent regions. More sop histicated moves involve local re-assignment of pixels to neighboring regions. These move s are proposed from a pre-computed dictionary of image segments  X  . The dictionary is generated by varying the parameters of an un-supervised over-segmentation algorithm (in our case mean-shift [3]) and adding each segment  X  to the dictionary. During inference, these segments are used t o propose a re-assignment of all pixels in the segment to a neighboring region or creation of new regi on. These bottom-up proposal moves work well for background classes, but tend to result in over-segmented foreground classes which have heterogeneous appearance, for example, one would not e xpect the wheels and body of a car to be grouped together by a bottom-up approach.

An analogous set of moves can be used for merging two adjacent objects or assigning regions to objects. However, if an object is decomposed into multipl e regions, this bottom-up approach is problematic as multiple such moves may be required to produc e a complete object. When performed independently, these moves are unlikely to improve the ener gy. We get around this difficulty by introducing a new set of powerful top-down proposal moves ba sed on object detection candidates. Here we use pre-computed candidates from a sliding-window d etector to propose new foreground regions with corresponding object variable. Instead of pro posing the entire bounding-box from the detector, we propose the set of intersecting segments (from our segmentation dictionary  X  ) that are fully contained within the bounding-box in a single move. 4.3 Learning We learn the parameters of our model from labeled training da ta in a piecewise fashion. First, the validation on a subset of the training data. Boosted pixel ap pearance features (see [7]) and object detectors are learned separately and their output provided as input features to the combined model.
For both the base object detectors and the parameters of the r egion and object terms, we use a closed-loop learning technique where we first learn an initi al set of parameters from training data. We then run inference on our training set and record mistakes made by the algorithm (false-positives for object detection and incorrect moves for the full algori thm). We augment the training data with these mistakes and re-train. This process gives a significan t improvement to the final results. We conduct experiments on the challenging Street Scene data set [2]. This is a dataset consisting of 3547 high-resolution images of urban environments. We resc aled the images to 320  X  240 before running our algorithm. The dataset comes with hand-annotat ed region labels and object boundaries. However, the annotations use rough overlapping polygons, s o we used Amazon X  X  Mechanical Turk to improve the labeling of the background classes only. We ke pt the original object polygons to be consistent with other results on this dataset.

We divided the dataset into five folds X  X he first fold (710 image s) was used for testing and the remaining four used for training. The multi-class image seg mentation component of our model achieves an overall pixel-level accuracy of 84.2% across th e eight semantic classes compared to 83.0% for the pixel-based baseline method described in [7]. More interesting was our object detec-tion performance. The test set contained 1183 cars and 293 pe destrians with average size of 86  X  48 and 22  X  49 pixels, respectively. Many objects are occluded making thi s a very difficult dataset.
Since our algorithm produces MAP estimation for the scene we cannot simply generate a tion results. Instead we take the max-marginals for each C approach is that our method does not have overlapping candid ates and hence does not require arbi-trary post-processing such as non-maximal suppression of s liding-window detections.
Our results are shown in Figure 3. We also include a compariso n to two baseline sliding-window approaches. Our method significantly improves over the base lines for car detection. For pedestrian detection, our method shows comparable performance to the H OG baseline which has been specif-ically engineered for this task. Notice that our method does not achieve 100% recall (even at low precision) due to the curves being generated from the MAP ass ignment in which pixels have already been grouped into regions. Unlike the baselines, this force s only one candidate object per region. However, by trading-off the strength (and hence operating p oint) of the energy terms in our model we can increase the maximum recall for a given object class (e .g., by increasing the weight of the object term by a factor of 30 we were able to increase pedestri an recall from 0.556 to 0.673).
Removing the pairwise context term does not have a significan t affect on our results. This is due to the encoding of semantic context through the region te rm and the fact that all images were of urban scenes. However, we believe that on a dataset with mo re varied backgrounds (e.g., rural scenes) context would play a more important role.

We show some example output from our algorithm in Figure 4. Th e first row shows the original image (left) together with annotated regions and objects (m iddle-left), regions (middle-right) and predicted horizon (right). Notice how multiple regions get grouped together into a single object. The remaining rows show a selection of results (image and ann otated output) from our method. In this paper we have presented a hierarchical model for join t object detection and image segmenta-tion. Our novel approach overcomes many of the problems asso ciated with trying to combine related vision tasks. Importantly, our method explains every pixel in the image and enforces consistency be-tween random variables from different tasks. Furthermore, our model is encapsulated in a modular energy function which can be easily analyzed and improved as new computer vision technologies become available.

One of the difficulties in our model is learning the trade-off between energy terms X  X oo strong a boundary penalty and all regions will be merged together, wh ile too weak a penalty and the scene will be split into too many segments. We found that a closed-l oop learning regime where mistakes from running inference on the training set are used to increa se the diversity of training examples made a big difference to performance.

Our work suggests a number of interesting directions for fut ure work. First, our greedy inference procedure can be replaced with a more sophisticated approac h that makes more global steps. More importantly, our region-based model has the potential for p roviding holistic unified understanding of an entire scene. This has the benefit of eliminating many of the implausible hypotheses that plague current computer vision algorithms. Furthermore, b y clearly delineating what is recognized, potential for increasing our library of characterized obje cts using a combination of supervised and unsupervised techniques.

