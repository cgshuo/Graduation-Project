 This paper describes TIPPPS (Time Interleaved Product Purchase Prediction System), which analyses billing data of corporate customers in a large telecommunications com-pany in order to predict high value upsell opportunities. The challenges presented by this prediction problem are signif-icant. Firstly, the diversity of products used by corporate telecommunications customers is huge. This, coupled with low product take-up rates, makes this a problem of learn-ing from a very high dimensional feature space with very few minority examples. Further, it is important to give priority specifically to the identification of those new cus-tomers who are of high value. These challenges are overcome by introducing a number of modifications to standard data pre-processing and machine learning algorithms, the most important of which are time-interleaving of data and value weighting . Time interleaving is the concatenation of exam-ples from multiple time periods, thus increasing the num-ber of training examples, and hence the number of minority examples. Value weighting assigns importance to minority examples in proportion to the dollar value of take-up, thus biasing the system to identify high value customers. These modifications create a novel algorithm that makes the pre-diction system practical and usable.

Comparison with other techniques designed for similar problems shows that the expected average improvement in ranking accuracy achieved using these modifications is 3.7%. TIPPPS has been in operation for several months and has been successful in identifying many upsell opportunities that were not identified by using the previous manual system. Categories and Subject Descriptors: H.2.8 [Database Application]: Data mining General Terms: Design Keywords: Upsell, Learning from Few Positive Examples, Area under ROC, SVM Applications Copyright 2005 ACM 1-59593-135-X/05/0008 ... $ 5.00.
Many data mining techniques and systems have been de-veloped to predict churn and upsell in the retail telecommu-nications industry [1, 4, 5, 9, 13, 14]. The corporate market segment in any service provision industry, however, is very different from the retail consumer segment. Firstly, in the corporate segment, the range and diversity of the products and packages available is far wider than that available in the consumer segment. For instance, in the telecommunica-tions industry, a whole range of audio and video conferencing products that are regularly used by businesses are often not even offered in the consumer segment. In addition, while the consumer customers may be segmented based largely on the size of their budget for a particular service, the pat-tern of product spend in the corporate segment is driven by a number of factors such as the industry that the com-pany operates in, and the markets and regions it services. Thus, a large bank is likely to have a very different pattern of product spend compared to a large supermarket chain, even if their total spend on a service such as telecommuni-cation or transport is the same. At the same time, two large banks may have very different product usage depending on the niche markets they serve. A third difference between the corporate and the consumer segments is in the size of the total customer base. The number of customers in the corpo-rate segment is much smaller than that in the consumer seg-ment, usually numbering in the thousands compared to the hundreds of thousands in the consumer segment. Further, in the corporate segment, the number of new customers for any single product in a particular time period may be just a handful. Thus, for the corporate segment, data mining techniques need to cope with high dimensional input spaces with very few minority class examples.
 This paper describes TIPPPS (Time Interleaved Product Purchase Prediction System), a novel algorithm that pre-dicts upsell opportunities in the corporate segment of a large full service telecommunications company ( telco ). TIPPPS uses the billing data for each product for each customer in order to generate a prioritised listing of customers, where the prioritisation is based on the likelihoods that the customers will, in the next time period, take up a particular product that they are not currently using. TIPPPS overcomes the problem of having very few examples of take-up per time period, i.e., small minority class, by using concatenation of examples from multiple time periods. The high dimension-ality of the input space is addressed using support vector machines for learning. In addition, the minority examples are weighted in proportion to the dollar value of take-up so that customers with high value take-up are given more importance.

TIPPPS has been compared with a number of existing al-gorithms both in cross validation tests and in predictions as would occur in a production environment. In the production tests, it achieved an average expected improvement in rank-ing accuracy of 3.7% over other standard techniques suitable for this problem. TIPPPS is currently in use on a trial ba-sis in a number of sales regions within a large telco, and has been successful in identifying many new upsell opportunities that were not identified by using the current manual system.
This paper is organised as follows. Section 2 describes the business need and the dimensionality of the data mining problem that it entails. Section 3 discusses the data pre-processing, including the time interleaved concatenation of data that ensures that there are sufficient minority class ex-amples for learning. Section 4 introduces the basic classifier that was used and the extensions to that classifier that make the solution feasible. Section 5 compares the results of our solution against other techniques. Section 6 discusses the production system and initial feedback from the business.
The business need articulated by the company was to in-crease the revenue from existing corporate customers by sell-ing new products to current customers, i.e., upsell to exist-ing customers to increase the share of their wallet. Further, while the identification of any potential buyer of a prod-uct was important, the requirement was to identify poten-tial high value customers ahead of others. In data mining terms, the goal then was to sort existing customers of a ser-vice provider who are non-users of a particular product, in the order of likelihood of taking up that product in the next time period multiplied by the expected dollar amount of the take-up. This goal is very similar to that presented in the 1998 KDD task, charity , which required the identification of former donors to a charity fund who are likely to respond to a future mailing with a generous donation. While the goals of the two problems, upsell and charity are very similar, there are some important differences.

Multiple prediction Problems . In upsell , it is neces-sary to identify high value customers for every one of the hundred or so products. In charity , there is only the single problem of donor identification.

Range of predicted value . The ranges of the target variable in the two domains are very different. The dona-tion amount in charity ranges from 0 to 500 dollars with a breakeven amount of 68 cents which covers the mailing cost. In upsell , the take-up amount may range from 0 to over a million dollars. The cost of following the wrong prospects is one of wasted resources and is difficult to estimate. On the basis of advice from the business, this cost is set as a fixed amount, TU MIN , the minimum take-up. Customers are considered to be positive examples of take-up of a par-ticular product, if and only if their spend on that product in the previous time period is $0 and the spend on the same product in the current time period is greater than TU MIN
Imbalance in class sizes . In upsell , the size of the mi-nority class, i.e., the number of new customers for a prod-uct, varies from a high of 10% to a low of less than 0.1%. Figure 1 shows the distribution of minority class percent-Figure 1: Distribution of minority class percentage over the product set age for the product set for two different corporate customer segments, namely, the large corporates (first row) and the medium businesses (second row). The x-axis is the minority class percentage, averaged over all available time periods. The y-axis is the number of products with that minority class percentage. Thus, the first bar in the graphs on the left shows the number of products that have a minority class percentage between 0% and 0.5%. The graphs on the right zoom in on the first two intervals (0%-1%).

We note that while the imbalance is pronounced in both segments, it is much worse in the medium businesses. For instance, in the large corporates segment, there are 51 prod-ucts with less than 0.5% minority class examples (top left graph) and 25 of these have less than 0.1% minority class examples (top right graph), while in the medium businesses segment, the numbers of products at less than 0.5% and 0.1% imbalance are 74 and 54, respectively (bottom graphs). In charity , on the other hand, the imbalance in classes is not so pronounced, with donors forming 5% of the dataset. Hence, for upsell , it is necessary to use techniques that can cope with extreme imbalance.

Number of minority class examples . Since the total number of corporate customers is much smaller than that of consumer customers, the actual number of minority exam-ples, (i.e., the number of customers who took up a product in a time period) may be just a handful. Figure 2 shows the distribution of minority class size for the product set for the two corporate customer segments in the same format as that in Figure 1. The x-axis is the number of new customers for a product averaged over all available time periods. The y-axis is the number of products with that many new customers. The graphs on the right hand side zoom in on the products with fewer than 50 customers per time period.

In the large corporates segment, there are over 45 prod-ucts with fewer than 10 minority class examples on average (the first bar in the top right graph). In the medium busi-nesses, 60 products have fewer than 10 examples of take-up (the first bar in the bottom right graph). Hence, for many products, techniques to increase the number of minority ex-amples may be necessary in order to be able to have suffi-cient minority examples to learn from. This is in contrast Figure 2: Distribution of minority class set size over the product set to charity where the total number of minority examples is in the thousands.

Predictors available for learning . In charity , the data available for learning include the donors X  donation his-tories and demographic information. The predictors in up-sell consist of the billing history of the customer where this history is available as revenue per billing period for each product. Clearly, predictors that describe the company de-mographics, such as the total turn-over and the number of employees, could be useful in the prediction task. Similarly, the involvement of the sales force as well as any competitor interest could be additional predictors that may increase accuracy. However, these data are not available for all cor-porate customers for all billing periods, so modelling is per-formed with the billing data only.

Periodicity in predictions . Another important differ-ence is that, unlike the 1998 KDD task, there is a time element associated with the upsell application. Predictions for take-up in the next billing period(s) are required at the end of every billing period, while the prediction of potential donors in charity was for a single promotion. This tempo-ral element has implications on how the training and test data may be combined. Firstly, since the billing history es-sentially consists of snap shots in time, time series derived features such as projected revenue in the next time period may be added as additional predictors to enhance the mod-els. Secondly, if the derived features in the training data use multiple billing periods, the test data also need to use the same number of previous billing periods.

The following sections discuss the techniques used to ad-dress these issues and develop a system for learning from data snapshots in time, using very few minority examples.
The input data for learning are available as snapshots in time with periodicity L , where L is the length of a billing period. Denote the i th billing period by B i and let B i represent the current billing period. Let r i,P represent the spend 1 of a customer on product P in the time period i . Then the training set for modelling the take-up of product The terms spend of a customer and the revenue obtained P includes all customers whose spend on P in the previous positive examples of take-up are all those customers with a spend on P that is greater than the minimum threshold TU MIN in the current billing period, i.e., r i c ,P &gt; TU The predictors for learning come from the pattern of prod-uct spend on all products other than P in the billing period B c  X  1 , i.e., r i c  X  1 ,Q ,  X  Q 6 = P . The test data is analogously defined with the periods moved forward by one billing pe-riod. Thus, to be included in the test data, customers have r ,P = 0. The predictors are the product revenues in the current period, i.e., r i c ,Q ,  X  Q 6 = P . Note that the test set will include many of the customers who were part of the majority class examples in training.

This section discusses techniques for processing this type of data that represents snapshots in time. In particular, the focus is on data combination (Section 3.1) and feature extraction techniques (Section 3.3) for dealing with data in time snapshots, as well as techniques for increasing the num-ber of minority class examples (Section 3.2).
The availability of input data as snapshots in time without any information regarding the precise timing of an event, such as take-up of a product, within that billing period may may give rise to two types of mislabelling as follows.
One simple technique to mitigate the effect of this misla-belling is to consider a longer time period for product usage for labelling examples. We will refer to this as data aggre-gation . Data aggregation involves using multiple time pe-riods for determining which examples are to be included in training and test sets. With aggregation to k billing periods, the training set includes all those customers for whom r i c  X  j,P = 0 ,  X  j = 1 ..k . The positive examples in the training set are those customers for whom ( k  X  TU MIN ). The test set consists of all those customers from a customer are used interchangeably. Both terms stand for the billed revenue stored in the database.
The terms positive and negative are used to denote minor-ity and majority instances, respectively. Figure 3: Distribution of minority class set size over the product set with aggregation for whom r i c + j,P = 0 ,  X  j = 0 ..k  X  1. The predictors for the training set come from B i c  X  j , j = 1 ..k , and those for the test set come from B i c + j , j = 0 ..k  X  1. As an example, if the original time period was a quarter, then aggregation to two time periods means that a positive example of take-up in training is when the spend on the product in the previous half year was zero and the total spend on the same product in the current half year is &gt; 2  X  TU MIN . The prediction for the test set is for the next half year. Thus, the time period for train and test span 6 billing periods (= 3  X  k ), 4 for training (= 2  X  k ) and 4 for test (= 2  X  k ) with an overlap of 2 (= k ), where k is the number of periods aggregated.
While data aggregation is primarily for improving the quality of labels in training, it also has an impact on the number of minority examples available for learning. Fig-ure 3 shows how data aggregation affects the average num-ber of new customers per time period per product. Results are presented in the same format as that in Figure 2. How-ever, the graphs on the right hand side now zoom in on products with fewer than 80 customers. The three bars in each group represent no aggregation (single), aggregation to two billing periods (double) and aggregation to three billing periods (triple). Looking at the first bar group in the top right graph, the number of products with fewer than 20 ex-amples decreases substantially (from 70 to 48) when data from two billing periods are aggregated. However, when the time period is increased to three, the number of products with fewer than 20 examples increases from 48 to 51. A similar trend, although not as substantial, is observed in the medium businesses with the number of products with fewer than 20 examples being 71, 66, and 71, with single, double and triple aggregation, respectively.

The increase in the number of minority class examples with double aggregation is primarily due to the fact that the reduction in the false negative mislabellings is larger than the increase due to the false positive mislabelling. With triple aggregation, however, the effect of the two misla-bellings are roughly equal, and there is not much change in the number of positive examples. Thus, double aggrega-tion, in addition to removing the effects of mislabelling, also has the advantage of increasing the number of minority class examples in the training and test sets.

Figure 4: Time Interleaving, Double Aggregation
As discussed in Section 2, the number of examples of take-up of a particular product P in any given billing period is small, and the paucity of minority examples in training is one of the major stumbling blocks in learning to predict customers who are likely to take up that product in the next time period.

Double aggregation increases the number of minority class examples so that a few more products have more than 20 examples of take-up, while triple aggregation has no posi-tive effect in the number of minority class examples. With aggregation to k billing periods, the training set requires 2 k billing periods of data. If six billing periods of data are available for training, one could implement triple aggrega-tion. However, another way to utilise the available data and increase the training data substantially is to interleave the data as follows (Figure 4). 1. Aggregate data from two billing periods so that four 2. Generate three sets of data, each offset by a single 3. Concatenate the three sets S i , i = 1 .. 3 to yield T , a Thus, time interleaving with double aggregation, uses data from six billing periods to train models for predicting prod-uct take-up in the subsequent two billing periods (Figure 4). The test data, S 5 , is not interleaved and spans the four pe-riods B 5 ..B 8 . Membership of the test set, as well as the predictors are determined by revenues in B 5 and B 6 . Pre-dictions are generated for periods B 7 and B 8 , on the basis of the models created using T .

Figure 5 shows the effect of this interleaving on the mi-nority class size and compares it to that obtained by double aggregation without interleaving. The format of the figure is the same as that for Figure 3 with the difference that the last bar in a bar group shows results for data obtained with time interleaving. Results for single and double aggrega-tions are presented as the first two bars of the bar group as before. As is evident from Figure 5, the improvement in mi-nority class size using time interleaving is dramatic, with the Figure 5: Distribution of minority class size aggre-gation and time interleaving number of products with fewer than 50 examples dropping from 85 to 46 in the corporate segment and from 83 to 65 in the medium businesses segment (left graphs). Thus, time interleaving with aggregation to two billing periods enables modelling of a much larger set of products than before.
This repeated use of data from some time intervals for time interleaving means that all training examples for mod-elling a product P are not completely independent, and cus-tomers may be included in the training set multiple times. For instance, customers with zero spend on P in all billing periods B 1 to B 4 are present as negative examples in all three sets S 1 ..S 3 . On the other hand, customers whose spend on P is zero in B 1 to B 3 and is &gt; 2  X  TU MIN in B , are included as negative examples in S 1 and positive examples in S 2 , and then excluded from S 3 . The former, namely, the repeated inclusion of some customers from mul-tiple periods as negative examples, could introduce a bias in modelling. The latter, namely, the inclusion of the same cus-tomer as a negative example and a positive example from different time periods, is likely to assist in understanding the relationship between the pattern of spend on products and the likelihood of take-up of product P . If there are changes in this relationship over time, then the datasets us-ing later billing periods, e.g., S 3 , are more likely to model them correctly than those using the earlier ones. Hence, one may consider incorporating a weighting scheme that assigns greater weight to the datasets that arise from later billing periods.
As stated in the previous section, the data that are used for prediction of take-up of product P are the revenues per product Q ,  X  Q 6 = P , for the first two billing periods of the four periods used to generate any S i . The revenue for B i.e., r i +1 ,Q is used as is. In addition, the following continuous predictors are derived: (1) the change in revenue from B i B i +1 , i.e., r i +1 ,Q  X  r i,Q , and (2) the projected revenue for the third billing period computed by linear extrapolation as twice the revenue in B i +1 minus the revenue in B i , i.e., 2 r
Each of these three predictors is computed both for the raw revenues as well as the revenue by the total spend of the customer in that billing period. This scaling allows per-centage spend and percentage changes in spend to be used as predictors in addition to the absolute values. This yields six continuous predictors for each product. In addition, bi-nary predictors indicating churn and take-up are computed for each product. Churn from a product Q is true for a cus-tomer for whom r i,Q &gt; 0 and r i +1 ,Q = 0. Take-up is true in the reverse case, namely, r i,Q = 0 and r i +1 ,Q &gt; 0.
Each continuous predictor is converted to 10 binary pre-dictors by binning into 10 equisize bins, where the bins are created using the training samples only. The total number of predictors after all derivations and binning is 62 (= 6  X  10+2) per product, and with around 100 products, the input fea-ture space for modelling contains  X  6200 binary predictors.
The principal advantage of binning is that it allows the development of non-linear models while still using linear modelling techniques. Hence, more complex models may be generated using a simpler algorithm. In addition, since bins capture the percentile into which the continuous predictor falls without regard to the actual value itself, it can over-come the negative impact of the variation in predictor ranges on learning algorithms. This impact could be considerable, given the range and diversity of products and customers in the corporate market. Predictor scaling instead of binning can also achieve the same effect. In addition, scaling retains ordinal information and captures any linear relationships in the data. Hence, future work will explore the use of scaled continuous predictors in addition to binned predictors.
We have used Support Vector Machines (SVM) as our principal classifier. This is due to the the ability of SVMs to handle high dimensional feature spaces [7] and extreme imbalance [12]. We recall the basic SVM formulation in a form suitable for our adaptations.

Consider a training set T of iid observations drawn from a population, with T =  X  ( ~x i , y i )  X  i =1 ,...,m , where ~x are the predictors, and y i  X  X  X  1 , +1 } are the bipolar labels for the m observations. The case of interest here is when the minority class, labelled +1, is much smaller than the majority class, labelled  X  1, consisting of a minute fraction, say &lt;&lt; 1% of the data.
 Our aim is to find a  X  X ood X  discriminating function f : R n  X  R that scores the minority class instances higher than the background class instances. If the functions considered are linear, then f ( ~x ) may be written as  X  ~w, ~x  X  + b , where  X  ~w, ~x  X  represents the dot product between the ~w and ~x . The values for ~w and b in the case of the standard linear two class SVM is obtained by minimising the following functional: 1 2 where p can be set to 1 to yield SVM-L1, a linear penalty machine [6, 15] or 2 to yield SVM-L2, a quadratic penalty machine, [8]. C i  X  0 is the regularisation constant of the i th instance and controls the trade-off between complexity of the solution and error penalty. Depending on the values assigned to C i , we get three different SVMs:
Two sets of experiments were performed. The first set is used to determine which of the three SVMs discussed in Section 4 is best and should be used for the second set of experiments. This determination is based on performance comparisons with 10-fold cross-validation tests on a number of different products from the large corporates and medium businesses segments. For this evaluation, training and test sets are not partitioned time-wise. Rather they are ran-dom samples from one large set of iid observations (Sec-tion 5.2). The second set of experiments evaluates the use-fulness of time interleaving with tests in the production sce-nario, namely, predictions for two billing periods based on the data from earlier billing periods as discussed in Sec-tion 3.2. Data from nine billing periods was available for both sets of experiments.

The performance measures and the results of the experi-ments are discussed below.
We have used the AUC , the Area Under the Receiver Op-erating Characteristic curve (ROC) as our primary perfor-mance measure. The ROC curve is a plot of the true positive rate against the false positive rate . The true positive rate is defined as the fraction of target (or positive) cases in the set of instances which has been classified as target. The false positive rate is the fraction of the negative (not in target class) instances which the system erroneously identifies as belonging to the target class. The ROC is generated from the list of instances sorted by the confidence that they be-long to the target class. The larger the AUC, the more accurate the ranker. A perfect classifier has an AUC of 1 while the uniform random classifier has an AUC of 0.5. The reasons for choosing this measure are the following. In the upsell application, which is our focus, it is not nec-essary to classify instances into two groups, but rather to sort the instances in order of relevance. For applications that require ranking, the AUC naturally provides a mea-sure of classifier goodness that is independent of the cutoff threshold [2, 3, 16].
 Table 1: Cross-Validation performance on Large Corporates Segment Since the focus is on identifying high value customers, the Value AUC (VAUC) is used in addition to the AUC. The VAUC measure is very similar to the AUC in that it is also the area under a curve. However, the y-axis of the curve for VAUC is the number true positives weighted by value, instead of the number of true positives used in the AUC. The value used for weighting in upsell is the dollar amount of take-up and hence provides an indication of the accuracy weighted by potential value.
Using revenue data from the nine available billing periods, six data sets, S i , i = 1 .. 6 may be constructed each using the data from four billing periods B i to B i +3 . As discussed in Section 3, for modelling take-up of a particular product P , the customers to be included in the set S i are determined by the revenues for P in B i and B i +1 . The predictors also come from the revenue information in B i and B i +1 , while the labelling is determined by the revenues for P in B i +2 and B i +3 . All six datasets S 1 to S 6 are pooled together to form the dataset for the cross-validation tests.

The dataset for each product was split into 10 mutually exclusive folds, where each fold was created using propor-tional sampling without replacement. Each classification method learnt 10 models of the data, each using one of the 10 folds for training and the remaining 9 for testing. This non-standard use of the larger partition for testing ensures that there are sufficient minority class examples in test despite the small minority class sizes, and the test AUCs are not influenced by the chance positioning of a few positive exam-Table 2: Cross-Validation performance on Medium Businesses Segment ples. Furthermore, for every fold, the training sets are mu-tually exclusive and the test sets are nearly the same. Thus, variations in performance are almost exclusively due to ei-ther the learning algorithm or variation in the training set. Hence, it is possible to compare the performance of differ-ent algorithms by computing the differences in AUC/VAUC obtained by them for the same fold. All performance mea-sures quoted are averages computed on the test set over the 10 cross-validation tests.
 Tables 1 and 2 present the results of these experiments. Each row represents the performance on a single product. Results are shown for 29 products in the large corporates segment and 30 products in the medium businesses segment. The total size of the test set and the number of minority ex-amples in it are shown in column 2 and 3, respectively. The training set size is one ninth of that of the test set, and contains the same proportion of minority examples (  X  sam-pling variations). Columns 4 to 6 show the average AUCs as percentages, i.e., the true AUC multiplied by 100, ob-tained using the three different SVMs: SV M std , SV M bal and SV M val . Columns 7 to 9 show the average VAUCs for the same three SVMs. The highest AUC and VAUC in each row are italicised to show the best choice (even if the improvement is only marginal). In addition, any highest AUCs and VAUCs that are significantly better than both the other two methods are highlighted in boldface. Signifi-cance is measured at the 95% level assuming normality.
Effect on large corporates. As can be seen from Ta-ble 1, for the large corporates segment, SV M val has a mean AUC that is the largest for 17 of the 29 products explored, and in 9 of the 17 cases the improvement over the other SVMs is significant. The results for VAUC are similar with SV M val having the best VAUC for 19 of the 29 products, and in 10 of the 19 cases, the improvement in VAUC is significant. More importantly, there is only one product that has a significantly better VAUC using either SV M std or SV M bal . It is worth noting that although SV M val is focussed on classifying high value minority cases correctly, it has a positive effect on the ranking of all minority cases. This is reflected in the high AUCs obtained using SV M val
Effect on medium businesses. The results for the medium businesses are even more favourable for SV M val with 25 out of the 30 products having the largest mean AUC using SV M val , and 13 of these 25 AUC improvements being significant. Measuring the performance with respect to the high value customers, SV M val has the largest VAUC for 23 products, and 15 of these are significant. Further, there is no product that has a significantly better AUC or VAUC using either SV M std or SV M bal .

Based on the above results, SV M val was chosen for use in the production setting.
As seen from Section 3.2, time interleaving allows larger numbers of products to be modelled by increasing the amount of training data and hence the number of minority class ex-amples. This section evaluates the impact of time interleav-ing on the performance of the system. The evaluation is performed by generating ranked lists in the production set-ting, i.e., predictions for two billing periods based on train-ing data from earlier billing periods. For the method with no interleaving, data from four previous billing periods are required for training, while the time interleaving method re-quires data from six previous billing periods. Hence, with data from nine billing periods, results can be generated for two test sets spanning two overlapping time periods, namely, take-up in B 7 or B 8 and take-up in B 8 or B 9 .
Tables 3 and 4 present the results of the experiments in the production setting for the large corporates and medium businesses respectively. Column 1 is the product number from the corresponding Table 1 or 2. The total size of the test set and the number of minority examples in it are shown in column 2 and 3 respectively. Since AUCs are meaningful only if there are some minority examples, AUC and VAUC results are presented only for those products from Tables 1 and 2 with at least one minority example in the test set in each of the time periods. Columns 4 to 9 pertain to the non interleaved method, while columns 10 to 15 refer to the time interleaved method. Columns 4 and 5 (and 10-11) show the total training set size and the number of minority class examples. Columns 6 and 7 (and 12-13) present the mean % AUC and mean % VAUC, respectively. The method with the largest value for each product is highlighted regardless of whether that difference is statistically significant.
Effect on large corporates. The training set size in the time interleaved method is, predictably, roughly three times as large as that for non-interleaved method for each product (comparing column 4 to column 10 in Tables 3 and 4). This increase in size is particularly important in the case of the large corporates segment, where the number of customers who take up a product in a time period is often very small. For this segment, time interleaving has a better AUC than non-interleaving for 26 of the 29 products (the number of highlighted entries in column 12 in Table 3). The impact on VAUC is also considerable with 23 of the 29 products having a better VAUC.

Effect on medium businesses. Use of time interleav-ing is beneficial in the medium businesses segment as well. Around two third of the products, i.e., 17 out of the 27 products for which there was some take-up in the periods considered, have a higher AUC when using the time inter-leaved method (highlighted entries in column 12 in Table 4).
Stability of models. Columns 8 and 14 show the ab-solute difference in the AUC obtained for the two different test sets without and with time interleaving respectively. Columns 9 and 15 show the same for the VAUC metric. The larger the values in these columns, the less stable the model over time. Hence, the method with the smaller difference is highlighted. As indicated by the number of highlighted en-tries in these columns in both tables, time interleaving does indeed produce more stable models. However, even with in-terleaving, there is still large differences in the AUCs and VAUCs obtained for the two different test sets due to the very small number of minority examples in test sets.
Significance. There are important differences between the results of these and the earlier experiments. Firstly, the training and test sets in these experiments are not iid since they are from different time periods. Further, the test sets in the second series of experiments are much smaller, with many products having fewer than 5 minority examples, and this is likely to lead to high variance in AUC due to chance positioning of the few positive examples. In addition, given the number of billing periods of data available, test results could be generated only for two overlapping periods.
Due to these differences, the significance of the impact of time interleaving is computed by calculating the differ-ence in the performance metric obtained with and without interleaving for each product and time period for both seg-ments. This yields 112 differences for 2 time periods and 56 products (29 in large corporates and 27 in medium busi-nesses). The average improvement in % AUC using time interleaving has an expected value of 3.7%, and will be at least 2.3% with 95% confidence. The corresponding figures for % VAUC are 4.0% expected average improvement and at least 2.6% improvement with 95% confidence.

Use of data from overlapping periods. As discussed in Section 3.2, the use of time interleaving means that some customers are included repeatedly as negative examples. This has the effect of increasing the weight of these negative ex-amples, and may have contributed to the poorer perfor-mance with time interleaving for some of the products in the two segments. One way to mitigate this effect is to use all examples from S 3 , the dataset using the latest time pe-riods, but use only the positive examples from S 1 and S 2 This has two advantages. Firstly, positive examples from S j are typically excluded from S k , k &gt; j due to their non-zero spend on the product being modelled, and hence the same customers are unlikely to be included multiple times as negative examples. In addition, while minority class size is increased using examples from S 1 and S 2 , the already large majority class is not swamped with instances from earlier periods. This may increase the accuracy of the model [12] while at the same time lead to shorter training times owing to smaller training set sizes.
TIPPPS is currently implemented on a trial basis within the business in a number of sales regions. The trial system accepts billing data for eight quarters and generates priori-tised customer lists for the products that the business is interested in promoting. Lists are generated by using the last six quarters of billing information for training, and are sorted by the likelihood of a customer taking up that product in the next two quarters. The output of TIPPPS includes the rank of the customer which is used for sorting and a score between 0 and 10 indicating the propensity for take-up in the next two quarters. This score is calculated by calibrat-ing the score returned by the SVM model [11]. In addition, TIPPPS also generates scores to indicate how the propensity of take-up is changing. These scores are calculated on the basis of models created using the billing data from quarters 1-6 and 2-7 and provide information on whether the cus-tomer is moving towards or away from the product. The score for the next two quarters which is created using the latest data, i.e., quarters 3-8, is used by the business to limit the number of sales opportunities that will be inves-tigated. The business may use different cut-off thresholds for different products and regions. Thus, TIPPPS is used in an advisory capacity, generating potential opportunities for sales executives.

Evaluating the impact of an advisory computational sys-tem is difficult given the fact that it is not only the advice but also the action following the advice that is critical in converting opportunities to sales. One methodology that is statistically valid is to divide the sales regions into two roughly similar populations, provide TIPPPS advice to one and not the the other and then compare the outcomes from the two. However, such a scenario is difficult to implement in the business since the sales regions are very diverse both in terms of performance as well as customer base. Also, since the lead time for conversion of opportunities to sales is quite long with corporate customers, it may be a year or two before the results of such an evaluation are available. Hence, initial feedback from business has been in the form of additional sales opportunities generated in the regions where TIPPPS has been operational for a few months, and the potential revenue from these opportunities.

Based on business feedback, TIPPPS identifies three times as many upsell opportunities as the current manual system in the large corporates segment, while in the medium busi-nesses segment the number of new opportunities is around five times that identified manually. This difference is not surprising given that there are fewer corporate customers per sales team in the large corporates segment, and needs of those customers are closely monitored and anticipated. Hence, new opportunities for sales are more likely to be iden-tified by the sales team. The medium businesses segment, on the other hand, has a much larger number of customers per sales team, and hence many of the potential sales op-portunities are missed due to lack of resources.

The impact of TIPPPS in different sales regions has been markedly different. In one region in the medium businesses segment, all the opportunities suggested by TIPPPS were already being pursued by the sales team. In another region in the same segment, on the other hand, TIPPPS sugges-tions resulted in a number of new sales opportunities that had not been identified by the sales team. This difference in outcomes from a prediction system which is not region-specific indicates that statistically valid evaluation of this system is indeed difficult.
Future work includes extending the application of TIPPPS to more regions, products and business problems. This re-quires the development of a turn-key prediction system that takes in the required data every billing period, creates mod-els and lists, and publishes the generated lists via a graphical user interface.

An example of a different business application is one of predicting product churn in the corporate market. TIPPPS may be very simply tailored to predict product churn by modifying the minority class definition to include those cus-tomers with prior product usage greater than some thresh-old, and zero current product usage. Such a churn prediction system is useful if the business has methods to prevent churn that do not impact on the revenue substantially.

Yet another area of future work is to improve the accu-racy and usability of TIPPPS. Accuracy may be increased by learning industry specific models, particularly for the medium businesses segment where the pattern of telecom-munications product usage is largely driven by the industry segment. AUC may also be increased by developing SVMs that incorporate the AUC statistic directly into the opti-misation problem [10]. In addition, usability of the system may be enhanced by predicting the potential dollar value of take-up for each product and customer.
We have presented TIPPPS, a system for predicting upsell opportunities in the corporate segment of a telco. TIPPPS overcomes the problem of learning from few minority ex-amples by using time interleaved concatenation of data. It gives priority to customers with potential high value take-up by using SV M val , an SVM that weights the minority class examples based on the dollar value of take-up.

Comparison with other SVMs using cross-validation tests shows that SV M val obtains significantly better VAUC in 25 of the 59 products tested, and a significantly worse VAUC in only one of the 59 cases. Tests in a production setting show that time interleaving produces an expected average improvement of 3.7% in % AUC and 4.0% in % VAUC. Thus, value weighting and time interleaving combine to make the system practical and accurate.

TIPPPS has been in operation for several months in a number of sales regions within the company. Preliminary feedback from the business indicates that TIPPPS has been successful in identifying many new upsell opportunities. The permission of the Managing Director, Telstra Research Labs, to publish this paper is gratefully acknowledged. [1] W.-H. Au, K. C. C. Chan, and X. Yao. A novel [2] A. P. Bradley. The use of the area under the roc curve [3] M. Craven. The Genomics of a Signaling Pathway: A [4] P. Datta, B. Masand, D. R. Mani, and B. Li.
 [5] A. Herschtal and B. Raskutti. Optimising Area Under [6] T. Joachims. Text Categorization with Support Vector [7] T. Joachims. Estimating the Generalization [8] A. Kowalczyk. Maximal margin perceptron. In [9] G. Piatetsky-Shapiro and B. Masand. Estimating [10] A. Rakotomamonjy. Optimizing AUC with SVMs. In [11] B. Raskutti, H. Ferr  X a, and A. Kowalczyk. Exploring [12] B. Raskutti and A. Kowalczyk. Extreme re-balancing [13] S. Rosset, U. Murad, E. Neumann, Y. Idan, and [14] S. Rosset, E. Neumann, U. Eick, N. Vatnik, and [15] V. Vapnik. Statistical Learning Theory . Wiley, New [16] G. M. Weiss and F. Provost. The effect of class
