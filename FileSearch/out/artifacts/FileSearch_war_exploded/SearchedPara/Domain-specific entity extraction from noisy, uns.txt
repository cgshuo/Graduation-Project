 ORIGINAL PAPER Sergey Bratus  X  Anna Rumshisky  X  Alexy Khrabrov  X  Rajenda Magar  X  Paul Thompson Abstract Domain-specific knowledge is often recorded by experts in the form of unstructured text. For example, in the medical domain, clinical notes from electronic health records contain a wealth of information. Similar practices are found in other domains. The challenge we discuss in this paper is how to identify and extract part names from technicians repair notes, a noisy unstructured text data source from Gen-eral Motors X  archives of solved vehicle repair problems, with the goal to develop a robust and dynamic reasoning system to be used as a repair adviser by service technicians. In the present work, we discuss two approaches to this problem. We present an algorithm for ontology-guided entity disambigu-ation that uses existing knowledge sources, such as domain-specific taxonomies and other structured data. We illustrate its use in the automotive domain, using GM parts ontology and the unit structure of repair manuals text to build context models, which are then used to disambiguate mentions of part-related entities in the text. We also describe extraction of part names with a small amount of annotated data using hidden Markov models (HMM) with shrinkage, achieving an f -score of approximately 80%. Next, we used linear-chain conditional random fields (CRF) in order to model observa-tion dependencies present in the repair notes. Using CRF did not lead to improved performance, but a slight improvement over the HMM results was obtained by using a weighted combination of the HMM and CRF models.
 Keywords Text analysis  X  Language models  X  Information extraction  X  Ontology-guided search 1 Introduction In many specialized areas, domain knowledge is often cod-ified in the form of specialized lexicons and ontologies. At the same time, working domain experts often keep records of their actions in the form of unstructured notes. Using domain knowledge recorded in such form presents a seri-ous problem, since annotating large amounts of unstructured text for machine learning algorithms with domain concepts and semantic types requires extensive human labor.
For example, such is the situation with clinical notes from the electronic patient records. Despite some recent efforts [ 14 ], there is a distinct lack of annotated corpora for unstructured medical text. At the same time, a number of lex-icons and ontologies are available in the medical domain. For example, the National Library of Medicine maintains a Uni-fied Medical Language System (UMLS) that provides both taxonomy in the form of a network of semantic types, and metathesaurus defining and linking relevant concept struc-tures; a SNOMED nomenclature of clinical terms is available from the College of American Pathologists.

A similar situation exists in many manufacturing compa-nies that often maintain relevant ontologies and thesauri for their products. The challenge we discuss in this paper is how to identify and extract part names from technicians repair notes, a noisy unstructured text data source from General Motors X  archives of solved vehicle repair problems, with the goal to develop a robust and dynamic reasoning system to be used as a repair adviser by service technicians.

This extraction problem is alleviated by the presence of a comprehensive and actively maintained ontology, which contains many semantic and lexical clues to disambiguat-ing the said notes. We proceed from the assumption that the writer abbreviates ( and the reader therefore interprets ) the notes based on a shared conceptual context and that the same context is expressed in the ontology. This allows us to build and compare context models derived from both text units and the ontology, and base semantic disambiguation on an information-theoretic basis, as described in Sect. 3 .
We use the term ontology loosely, to refer to what is used as a de facto tool for knowledge organization and commu-nication structuring by a large body of engineers and other specialists. Strictly speaking,  X  X erminology X  or  X  X ontrolled vocabulary X  might be more fitting. However, in our choice of the term, we want to stress the fact that the data set is closely linked with (and in practice may be inseparable from) the actual organization of knowledge regarding the unit struc-ture and composition of the objects. In other words, the role played by it in reality is closest to that of a comprehensive taxonomy encompassing a de facto ontology. The structured organizational knowledge expressed in this data set should not be considered trivial because the very existence of this data is due to real communication needs of large bodies of domain experts and represents a systematic effort to facilitate their collaboration.

General Motors has numerous structured knowledge sources that are maintained for various purposes, in particular a taxonomy of part names organized by functional subsump-tion, which we use as the ontology. The unstructured text data we use come from GM X  X  massive archives of service tech-nicians X  notes on solved repair problems. The overall goal of the project described in this paper is to use these archives and resources to develop robust, dynamic textual case-based reasoning (TCBR) systems [ 1 , 10 ] that can be used as repair advisers by service technicians and engineers. A core chal-lenge is in the taxonomic indexing of repair text archives, so that smart search, e.g., ontology-guided search (OGS), can be used to match the description of the symptoms of a new problem with those of solved problems in the archive. Since classifying and disambiguating text are key elements of the indexing process, it includes a strong element of natural language processing (NLP) and information extraction. For example,  X  X AS CAP X  and  X  X UEL CAP X  are synonyms and should be classified together, but  X  X AS X  by itself requires disambiguation, because it has three distinct meanings in the context of three distinct subsystems: the powertrain (as fuel for the engine), the heating and cooling system (as refriger-ant for the air conditioning), and the fuel cell power system , where hydrogen gas is used to generate electricity. Clearly, two kinds of methods must be developed: (1) methods to classify text by locating key phrases on lexical taxonomies and (2) methods to disambiguate the text by using context to determine which regions of the taxonomies are more likely to be most relevant.

The types of solved-problem archives we have in mind consist of a few structured attributes along with technicians X  notes in free-form text. Existing collections of text anno-tated by parts of speech, such as the Penn TreeBank, are of little use in analyzing this kind of text. It has a specialized vocabulary and abbreviations. The text blocks are brief, and commonly, they do not obey the rules of standard English spelling or grammar. Therefore, certain standard NLP tech-niques have not been fruitful, and domain-specific alterna-tives seem necessary. It needs to be taken into account that we do not have the resources to hand-annotate such text in the domain-specific ways required. Since the text consists of ungrammatical fragments with no sentence boundaries for each fragment, it is doubtful whether annotating parts of speech right away would be the right approach. For example, consider the following actual sample from our notes: CUST STATES THE RIGHT SIDE OF THE HOOD IS SITTING HIGHER THENA FOUND RT SIDE OF HOOD LOOSE TIGHTEN HOOD BOLT For a domain expert, this segment yields the following sentences: 1. CUST STATES THE RIGHT SIDE OF THE HOOD 2. THENA FOUND RT SIDE OF HOOD LOOSE 3. TIGHTEN HOOD BOLT
There is a number of anomalies in this segment, in addition to the absence of sentence boundaries. Notice that unstruc-tured notes in different domains carry their own peculiarities. Consider the following excerpt from a free-text note taken from an electronic medical record:
While sentensification may be less difficult, syntactic anomalies remain a problem, as does extensive use of domain-specific acronyms and abbreviations. It is clear, how-ever, that the same kind of semantic abbreviation takes place, which depends on a shared concept system. By considering such examples, we have been led to consider approaches that do not depend on standard grammar or spelling, but do exploit the domain-specific structures.

The challenge is to use the existing (and maintained) knowledge resources, in lieu of annotated data that needs to be created. For example, there are several structured lex-icons of part names. (They are actually names of categories of parts, but we will refer to them as part names.) Indexing with part names may be sufficient for many smart search applications. In other words, if we can find solved problems involving the parts referenced in the new problem, then we have reduced the number of relevant  X  X ases X  significantly.
These various domain-specific knowledge resources have good coverage, but they lack the structure and precision needed for TCBR and OGS. Therefore, we have looked at techniques for refining these knowledge resources to make them more consistent, less redundant, and more powerfully structured. We have also developed robust indexing (and search) techniques that are not destabilized by noisy data.
We note that both our ontology and reference text samples are mission-critical and actively maintained for the specific domain. Hence, whereas one can rely on these properties being true in the automotive and medical domains, in other domains where manuals and procedures might not be as mis-sion critical, our methods may not yield the results as good as can be obtained in the domains at hand. 2 Overview Knowledge resources created for day-to-day purposes in a company can often be adapted for decision support. Uschold noted that a company X  X  glossary or thesaurus could be adapted to become a semantic net [ 17 ]. Any manufactur-ing company, such as General Motors, has numerous lists and lexicons related to the names associated with the design, engineering, manufacturing, and servicing of its products. Our key example is the lists of automotive parts that are referenced in servicing vehicles. We might have focused on lexicons for design, engineering, or manufacturing, in devel-oping decision-support tools for those areas. An example for manufacturing is the GM Variation-Reduction Adviser [ 11 , 12 ].

For knowledge to function as an asset in a modern corpo-ration, it must be explicit and machine-readable.  X  X xplicit X  means that it is  X  X ritten down, X  as opposed to carried in peo-ple X  X  heads or derivable via data analysis. Knowledge must be machine-readable because the sheer volume of data makes human processing impossible. It is commonly said that 80% of corporate data exists as text: program notebooks, problem summaries in warranty records, technical assistance center logs, customer surveys, and various other archives of records, logs, and diaries. Most of this text is machine-readable only in a limited sense. For example, keyword search can find items by exact matching, but the slightest differences in expres-sion can cause an item to be missed. Ideally, one would like to ask questions, such as  X  X as this issue come up before? X  or  X  X ow many times has this issue come up before? X  and get the answer by searching, cross-referencing, and comparing these various archives. Current tools and methods cannot do this.

Repair records are usually partly structured with important information captured as free text. We want a TCBR system that accepts a  X  X uery X  consisting of the symptoms of a prob-lem and responds with a prioritized list of repair records that have similar symptoms. We want to use the knowledge struc-tures that are available and maintained by the manufacturer. This is a very rich source of already-available knowledge. We would like to avoid a significant knowledge acquisition effort specifically for the TCBR application, e.g., annotating text for training ML methods.

An archive of text records of solved repair problems poses major challenges when matching it to the codified contex-tual knowledge tree. One can easily imagine using such an archive to avoid repeating time-consuming diagnostic and repair experiences, but in practice, it is difficult to use. This is because of (1) assumed contextual information that is not written into (2) the paraphrase problem, and (3) ambiguous language.

Let us consider our model TCBR application in more detail: a technician with an unsolved repair problem is search-ing the Warranty Data Archive to determine whether his unsolved problem or a similar problem has arisen before. The technician enters the symptoms and wants the records returned in the  X  X ost relevant symptom X  order. After some analysis of such repair records, we have determined that most symptoms have the form:  X  X ART NAME is broken. X 
We have focused our efforts on identifying and classify-ing the part names references in a repair record. These part names have become our surrogate for symptoms. Part name extraction is indicative of much broader NLP problems posed by the text outlined above, such as the material context omit-ted from the text, paraphrases missing from the grammar, ambiguous language, and so on. We focus on this particular problem because it appears to deliver a highest payoff in the industrial scenarios.

In fact, extracting entity names, such as part names, is the first step toward understanding of natural text as it immedi-ately enables business analytics and other statistical applica-tions even at its simplest. It also significantly improves search of document collections and, therefore, location of relevant documents. For example, in the automotive domain, much of the business activity revolves around making, ordering, or replacing parts. In the medical domain, symptoms and diagnoses play a similar role.

The approach we imagine is to use information extraction to create a structured index of the blocks of text included with the repair records. We want to identify and classify part names in text. It is through the classification of part names that both indexing and similarity are defined. The indexing process would include cleaning the text, extracting the part names, and mapping the part names into existing taxono-mies of part names. Thus, the values for the slots in the index would be taken from one or more existing parts taxonomies. Similarity would be defined from the taxonomic structures. A characteristic of the domain is that these part names and their structures are constantly changing. Thus, the system would require a certain robustness.

An important consideration for TCBR is that the objects of interest be structured into is a and part-of taxonomies (and perhaps other relations). This is because indexing text leads to a desire to generalize and specialize to solve the paraphrase problem and the disambiguation problem. One natural con-text for an object in a taxonomy is the path from the node where the object is named to the root, as well as the descen-dants of the node. If the same name is used in several nodes (e.g.,  X  X AS X ), then the choice of node is the disambiguation of the name, and matching the textual context with the taxo-nomic context is one way to choose the most relevant node.
The kinds of lexicons and lists we used were as follows:  X  T, a taxonomy of part name categories. The relations  X  a list of standard abbreviations,  X  an engineering glossary,  X  L, a list of labor code descriptions, and  X  a mapping of the elements of L to the elements of T.
We note that the ontology provided to us represents the summary of a large organization X  X  efforts in structuring com-munications between its units and, as such, resembles a coding tree (in the information theory sense) rather than a codification of synonymy, hyper/hyponymy, substitutability and other semantic relations captured by a lexical KBs such as WordNet [ 6 ]. Specifically, our ontology is meant to cap-ture context relationships between terms and concepts rep-resented by its nodes in a way that a domain expert would rely on when communicating with a colleague. Thus, it is obvious to us that our ontology and WordNet are products of two different processes, the former deriving from an orga-nizational process designed specifically to produce the cor-pus at hand as a remedy for concrete communication issues, and the latter being a general and broad attempt to structure language norms not tailored to any specific project needs. Even though we cannot quantify the difference in the prod-uct of these two processes exactly, we observe much more focused and uniform grouping of our domain-specific ontol-ogies, with a much more fixed set of relationships, mostly containment, reflected in the tree structure, versus a more general network-like structure of the synsets .

While many authors, e.g., [ 2 , 3 , 5 ], have used natural lan-guage processing or machine learning techniques for vari-ous language processing tasks with biomedical texts, we are unaware of any approaches to lexical disambiguation based on information-theoretic mappings to taxonomies such as presented here. The following describes several examples of this research. Demner-Fushaman et al. [ 5 ] describe how to improve the accuracy of text categorization based on a thesaurus by eliminating highly ambiguous terms from the thesaurus. Chapman et al. [ 3 ] use a na ive Bayes classifier to categorize chief complaints from free text fields of medical records into seven early presentations of disease categories, or syndromes, for use in an electronic syndromic surveil-lance system. Bundschus et al. [ 2 ] compare the use of sev-eral topic modeling techniques for use in (semi-) automated generation of metadata annotation for PubMed abstracts. By contrast, our approach uses both new data structures and new algorithms to organize the domain knowledge at hand.
In Sect. 3 , we describe the algorithm for ontology-guided entity disambiguation. In Sect. 4 , we describe the overall OGS-based TCBR application pipeline. In Sect. 5 , we discuss part name extraction, which supports the TCBR application, and present experimental results. Section. 6 provides a dis-cussion of these results, and Sect. 7 concludes. 3 Ontology-guided entity disambiguation In unstructured expert-generated text, domain-specific enti-ties (e.g., parts in automotive domain, diagnoses, test results, therapy protocols in patient records) are referred to by inherently ambiguous short noun phrase mentions that are disambiguated by human readers based both on textual con-text and on their extra-textual domain knowledge. Automatic indexing and efficient searching of unstructured noisy text corpora require that this disambiguation be performed auto-matically so that part mentions are extracted and annotated with the respective part identifiers.

In each domain, this knowledge is partly expressed in domain-specific taxonomies and concept systems. For exam-ple, medical domain ontologies may classify entities as body parts, clinical findings, procedures and treatments, and so on. In the GM automotive domain, structural ontologies of auto-mobile parts classify parts by functionality, as well as by sys-tems, subsystems and assemblies. Our method uses lexical features derived from these ontologies, together with lexical features from the context surrounding the automotive text mentions, to perform disambiguation.
The main idea behind the algorithm we propose is the following. We need to disambiguate each part mention to a particular node in the ontology tree. The elements of the noun phrase NP comprising the part mention may occur in the names and descriptions associated with a given set of nodes in the ontology tree. We make the simplifying assumption that the target part mention will disambiguate to one of these candidate nodes . We term the set of candidate nodes, along with the paths from each node to the root, a tree cut induced by the part mention. We associate a probability distribution withagiven tree cut and assign a probability to each leave by assuming equiprobable branching at each node. 1 Entropy of a tree cut is then computed as the entropy of the associated probability distribution.

For each candidate node, we obtain its tree context by assembling the lexical items from the names and descrip-tions of the nodes on its path to the root. We then look at the context in which the target part mention occurs and compute a set-theoretic intersect of this context with the tree context of the candidate node. Each term in the intersect will occur only in some candidate nodes, thereby inducing a further cut on the tree cut corresponding to the target part mention. We compute an association score between the candidate node and the target part mention by weighting each term in the intersect by the overall reduction in the entropy it induces on the original tree cut. Thus, for example, the term  X  X ssem-bly X  may not be very informative, but it may reduce the set of candidate nodes for a particular part mention to a single node. Each term is also weighted according to its distance from the target entity in text and the tree path distance from the candidate node. The part mention is then disambiguated to the node that maximizes this association score.
More formally, for an entity E (part mention) associated with a noun phrase NP , we define its context C as a col-lection of features derived from the unit(s) of text contain-ing the NP (the sentence, the paragraph, or other structural units if defined, for instance, the headings of the manual sec-tion and chapter for the NP s in a manual). For each node N of the ontology tree T , we likewise define a collection of features C , derived from the lexical contents of N and its ancestors on the path to the root of the tree, as well as of its siblings and additional lexical units attached to the nodes of the ontology tree T .
 For all candidate nodes N in T , we compute the score Q ( E , C , N , C ) and select the node N with the highest score. The function Q is based on information-theoretic measures associated with the lexical units in the tree T , based in their occurrence in the nodes throughout T . Essentially, the mea-sure associated with a single unit (word or stem) expresses the uncertainty about the identity of the node N containing that unit once the unit is known. For example, a word or stem that occurs in the names or descriptions of many nodes through-out T has a higher measure of uncertainty than a word or stem that occurs only in a few leaves or branches of T . These uncertainty scores of participating units are then combined to form Q . The definition of function Q is discussed below.
In our initial experiments, we also considered adding sib-ling child and associated metadata nodes in the tree to the nodes X  tree context. While anecdotally such inclusion with an appropriate weight coefficient appears helpful X  X n partic-ular in the situations where the creators of the ontology appar-ently counted on the human reader deriving the context from the words previously used in siblings as a form of abbrevia-tion X  X e do not currently possess convincing measurements in favor of this hypothesis. We note this omission of lexical units that would be helpful for automated processing is a general human tendency X  X he same tendency to eliminate redundancy that our entire model aims to capture. Generally, we found the context gathered on the upward path toward the root of the tree discriminating enough. 3.1 Information-theoretic context association score Let tokenization of a paragraph P produce a sequence of tokens { t i } . If sentence boundaries are available, we further subdivide this sequence into groups by their respective sen-tences. Let { u i } be the sequence of lexical units derived from { t } after tokenization. The lexical units are derived by a trans-formation according to a dictionary D of abbreviations and multi-token terms; some tokens are expanded to several lex-ical units, some are stemmed, the stem replacing the token in its place in the sequence, some multi-token groups col-lapsed to a single unit. This transformation from sequences of tokens to sequences of standardized lexical units will be referred to as U :{ t i } X  X  u i } .

For an entity E that spans lexical units u k ,..., u k + l define the primary context C E as the collection of units u ,..., u as a manual, we also add to this primary context the units of text structurally related to P , such as headings under which P occurs. As an extension of this approach, we assign to each lexical unit u in C E a weight depending on u  X  X  position relative to E (e.g., decreasing with the distance from u to E , counted within P in the number of lexical units separating them, and outside P in the number of structural elements separating the current element from P ). Denote this weight d ( u , E ) .
 For a candidate node N of T , define the ontology context C
N of N as consisting of lexical units derived according to the rules of the transformation U from the names and descrip-tions attached with the ancestors and siblings of node N . Each unit u coming from a node N u is associated with a weight based on the distance between N and N u , counted in the number of nodes separating N and N u on the path to the root of T , with a special distance value fixed for siblings of N . Denote this weight d T ( N u , N ) .
 We define the score Q ( E , C E , N , C N ) as follows: Q ( E , C where C ( E , N ) is the set of terms in the set-theoretic context intersect C ( E , N ) = ( U ( E )  X  C E )  X  C N , T W is the tree cut induced on T by the set of lexical units derived from the W , and H ( T W ) is the entropy [ 4 ] measure associated with it as described above.

We considered using other parts of speech and their rela-tions, such as verb phrases, but our texts are noun-rich and verb-poor. Examining other syntactic constructs in addition to NP s is an interesting further extension of this work. 4 Matching noun phrases to XML ontology In this section, we describe the application of the above algorithm in a test setting. The ontology used for the task was derived from Vehicle Partitioning and Product Structure (VPPS) taxonomy maintained within GM. VPPS is a tax-onomy of part names organized by functional subsumption, which contains roughly 5,000 nodes. It is supplemented by a lexicon of part name categories (UPC-FNA) that contains 55,000 entries. The text data we used come from the low-noise technical text from the GM car user manual, as well as the noisy repair notes. 4.1 Data formatting We received the original VPPS and UPC-FNA data as large spreadsheets in the MS Excel format, with the tree struc-ture being represented by way of cell indentation. We found this representation hard to validate and maintain, especially because our planned approach was to continually enhance the ontology tree based on the merging of these sources with added annotation. Moreover, while validating the original spreadsheets, we found some apparent format discrepancies and errors (e.g., mis-attached nodes), most probably due to past maintenance edits, an outcome we wished to avoid.
The parts lists and ontologies being  X  X iving documents X  actively maintained and exchanged by different groups of specialists, we also anticipated providing the data owners with tools for future validation and management that would not conflict with our added annotations. We therefore decided to represent, process, and maintain the underlying main data set combining VPPS and UPC-FNA (which serves as input to all our algorithms) as a single XML document, validated both through the standard XML parsing mechanisms and by our own scripts, and processed according to the DOM model.
The obtained matches of part mentions against the ontol-ogy must be output for human consumption and verification. When developing the pipeline, we output such matches in a simple ASCII markup, which also makes it easy to use in further scripting. We describe our results and markup below. We also output supporting data as YAML, a data interchange format similar to XML but less verbose. 4.2 System description In the experiments with part mentions from the GM car man-uals, ontology-guided search is currently performed for any text on the paragraph level. Noun phrases are extracted from the sentences comprising each paragraph, filtered against the unigrams encountered in the tree, their entropies computed and sorted, and the corresponding tree nodes are shown for each paragraph along with the phrases.

As a part of preprocessing the ontology, lexical units for abbreviation expansions were added as attributes to their respective nodes, so abbreviation expansion is taken care of by the design of the ontology. The paragraph boundaries are obtained via title headings in the original SGML documents. The titles for each text unit are preserved, parsed, and used as a part of text context for each part mention. Each paragraph is sentensified, and each sentence is then passed through an English parser X  X urrently Charniak X  X  parser X  X nd the NP s are found with their NN | JJ modifiers. Measuring parsing performance was not a part of this phase of the project, but checking them by hand found virtually no errors in the tech-nical text of the manuals, likely due to the simple structure of the technical text at hand. The NP s are filtered against the unigrams list of all the words encountered in the ontol-ogy, thus forming the set of interesting, or sensible, phrases, which we will refer to as mentions .

Top scoring ontology nodes are then identified for each mention found in a paragraph, and a list of mentions (rele-vant and/or identifiable NP s), with the corresponding top dis-ambiguation candidates for each NP , is associated with each paragraph. The resulting paragraph summaries are stored in a machine-oriented, human-readable YAML format, available for immediate inspection and verification after each auto-matic pipeline run. The produced document is then repre-sented as the list of paragraph and sentence summaries. The pipeline steps are described below in the order of their actual execution.  X  Text Extraction from SGML . Text and headings are  X  Sentence Boundary Detection and Parsing . Sentences  X  Noun Phrase Extraction . We read parse trees from  X  Entropy Computation and Sorting . For each extracted NP,  X  Text Markup and Output . Finally, we mark up the original The final markup of the text looks as shown in Fig. 1 . The original sentences are printed one per line, with Sn: prefix, where n is the sentence number. The para-graphs are delimited with the lines of the form: @Par Pn:
Only the paragraphs containing the mentions for which a match was found contain the appropriate markup. Each men-tion block starts with its number, @Mention 1 , @Mention 2 , etc., and consists of a mention header line, containing the mention phrase and stats, for example: where the interpretations of each markup element are as follows: @Mention 1 this is the first matched [regulator voltage] the actual mention (NN NN) POS tags corresponding :S1 originating sentence =2.585 entropy for this men-(6) number of leaf nodes
We then show the top matched nodes containing the men-tion , including their path names, showing for each ancestor starting at the root, the slash-separated node name (if avail-able) or node type:
Repair notes are processed in the same manner, using sim-ilar filtering mechanisms. In the absence of the hierarchical title heading structure, text-only context before and after the target NP is used in disambiguation. As NPs are extracted from each sentence, we retain only the content words when querying the ontology (skipping determiners, conjunctions, possessive markers, etc.,). A sample markup of disambigua-ted part mention from a repair note is shown in Fig. 2 .Note that in this example, the target NP  X  X iring harness X  is highly ambiguous and occurs in 1539 nodes in the ontology tree. 5 HMM/CRF approach to part name extraction from noisy text First, let us reiterate what we mean by part names. We are interested in categories of parts, such as  X  X IL PAN. X  There are many kinds of  X  X ctual X  oil pans, and each is identified by a unique name (or rather a combination of data, includ-ing a part number, supplier, date of manufacture, and other identifying information). We find it convenient in this paper to refer to OIL PAN and other part name categories as  X  X art names, X  but we are always referring to categories. We want to be able to identify part names in text. We want to classify the discovered part names by mapping them to T. In order to accomplish this, we need to consider which words and two-word phrases of the name are semantically  X  X nformative X  and which are not. In this paper, we focus on identifying part names. In the case that we have grammatical text, we would expect to be able to extract noun phrases (NPs) from the text using NLP techniques as a starting point for identifying part names. Then, we could determine by attempting to map each NP to T whether the NP is a part name or some other object (e.g., CUSTOMER).

However, the structure of part names can help us identify them, even when the text is fairly ungrammatical. For con-creteness, imagine that a technician is having trouble with the  X  X eft outside rear view mirror. X  Naturally, he might look for this exact string, but a verbatim in a warranty record might describe the  X  X EFT OUTSIDE REAR VIEW MIRROR X  in different ways. The most obvious variants are created sim-ply by omitting some of the qualifying adjectives, which is often done when the context is assumed. Further, vehicles have a bilateral symmetry, so that many parts come in a left (driver X  X  side, etc.,) version and a right (passenger X  X  side, etc.) version. Sometimes, the difference will be important to the problem-solving potential of a warranty record, but often it is not.

The semantic head of the above phrase is  X  X IRROR, X  which in this case is the syntactic head. Mirrors in vehicles are either  X  X ear view X  or  X  X anity, X  but vanity mirrors can be only inside. Thus, technicians might neglect to add the qualifying phrase  X  X ear view X  if they have already notes  X  X utside. X  It might not even make sense for the search to favor these kinds of qualifying adjectives, because such qualifiers are often omitted in technician X  X  notes. Thus, while if they are present, they add to the information gain of the phrase, if they are not present, there is no information loss. In other words, it means nothing; it is a consequence of the  X  X ssumed context, X  which is so common in such notes. A hurried technician might sim-ply have written  X  X UTSIDE MIRROR X  knowing that vanity mirrors are never outside, and the left-right distinction is not important for the particular write up. The essential structure of the NP is  X  X IRROR, X  which is essentially a concept class, along with enough qualifiers to distinguish it from all other mirrors. There is (at least one) taxonomic tree implicit in this analysis.

The training data for part name identification consist of repair notes where part names have been labeled. For the evaluation, we hand-labeled 1,000 randomly sampled repair notes. We divided the repair notes into two sets of 500 each, one for evaluation during development and the other for final testing. We also used approximately 30,000 part name phrases and a lexicon of 1,600 part name words collected from these phrases during training. For testing, we used five-fold cross-validation.

The structure of our HMM consists of  X  X arget X  and  X  X on-target X  states, s i . Part names correspond to the  X  X arget X  state of the HMM. The non-target states are Start, Prefix, Suf-fix, Background, and End. The Prefix state corresponds to a fixed-length sequence of words before the target words. Sim-ilarly, the Suffix state corresponds to a fixed-length sequence of words following the target words. The remaining words are thought of as being emitted by the Background state. The probabilities are estimated as ratios of counts. The transi-tion probability P ( s j | s i ) is calculated as the total number of ( s , s the training data. The emission probability P (w | s i ) is calcu-lated as the number of w labeled as s i divided by the total number of s i labels. One of the issues we have had to face is getting sufficient labeled data for training. This has had implications for the effectiveness of HMM structure used to model our data [ 7 , 8 ]. Our HMM is complex with as many states as possible and is able to capture the intricate structure of the data in use; however, it results in poor (high variance) parameter estimation because of the sparseness of training data. In contrast, simpler models with fewer states, while giv-ing robust parameter estimates, are not expressive enough for data modeling. In order to strike a balance, we used a statis-tical smoothing technique called  X  X hrinkage X  to combine the estimates from these models of differing complexity. Freitag and McCallum [ 8 ] report positive results using  X  X hrinkage X  to perform information extraction using HMMs. Some states from a complex model are shrunk to a common state to form a new HMM structure X  X ence the term shrinkage. To further improve parameter estimates, states from the new HMM can be further shrunk to form another HMM with even fewer states, thus forming a shrinkage hierarchy. In our case, we shrunk the Prefix and Suffix states to a common Context state. We then employed another level of shrinkage, in which all the states were shrunk to a single state.

The recall and precision scores from the fivefold cross-validation along with the F -score are presented below. Table. 1 shows results for the fully expressive model alone, as well as for the optimal shrinkage mixture of three HMM models.

The fully expressive model has poor recall though preci-sion is good. This indicates that the model by itself is not sufficient to cover all part names. The F -score of the fully expressive model is low. On the other hand, using shrink-age with optimal mixture weights improves recall values substantially while maintaining high precision Table 2 .The substantial improvement in recall indicates that the shrink-age mixture helps to smooth parameter estimates to expand coverage of part names not handled by the fully expressive model alone.

We have investigated the effectiveness of using HMMs with shrinkage for part name extraction and found that HMMs do well modeling the repair notes as shown by an F -score that hovers around 80%. Next, we sought to improve performance on the remaining 20% of the part names that were missed or incorrectly labeled by HMMs by introducing a more flexible model called linear chain conditional random field [ 15 , 16 ]. We hypothesized that the errors not handled by HMMs could be handled using the observation dependen-cies found in repair notes. It is desirable to integrate these dependencies into the models to improve overall classifica-tion accuracy.

We used unigram and bigram features. Unigram features are obtained using the identity of the current word. For each word seen in the training data, the unigram feature value is assigned to 1. Bigram features use previous and current word. Contrary to our original hypothesis, extraction using a CRF did not outperform HMM with shrinkage, although CRF does perform substantially better than the HMM without shrinkage.
 Analyzing the misclassifications of the HMM and the CRF, we noticed that there was a fair amount of differ-ence in which items were misclassified. This suggested that a weighted combination of the two techniques might improve performance. To combine the two models, we merged Viterbi search [ 13 ] in both the HMM and the CRF using a weighted combination.

Our results from the weighted combination of the HMM and the CRF are shown in Table. 3 . There is some improve-ment in overall score, but there is little improvement over either model alone. 6 Discussion When we used fully expressive HMMs, performance was poor, because of an insufficient amount of training data. With shrinkage, performance dramatically improved. We thought that we could make further improvement using CRFs, since, unlike HMMs, CRFs would allow us to model observation dependencies. However, CRF did not outperform HMM with shrinkage. Since the two approaches misclassified different part names, it seemed that it might be possible to find an optimal way to combine the two approaches to obtain better performance than with either approach by itself.

For example, the HMM and the HMM + CRF miss  X  X n X  in  X  X n star antenna, X  reporting only  X  X tar antenna X  as a part name, whereas the CRF correctly reports the entire part name. It turns out that  X  X n X  occurs in the training data mostly as a non-target word, so the HMM does not catch it. On the other hand, the CRF catches  X  X n X  because of the bigram  X  X n star X , which is unambiguously labeled as target-target in training. The HMM + CRF is not able to catch it, which means that on this particular observation the HMM label dominates the CRF label.

On the other hand,  X  X ads X  in  X  X tep pads X  is missed by the CRF, but not by the HMM and the HMM + CRF. Here,  X  X tep X  is caught by all three methods since it is unambigu-ously labeled as target in training. But  X  X ads X  is not observed in training, so the CRF misses it. However, in the case of the HMM, the target label of  X  X tep X  must have influenced the tar-get X  X arget transition between  X  X tep X  and  X  X ads X , thus caus-ing the HMM to not miss  X  X ads X  (the target-target transition probability is considerable). The fact that the HMM + CRF does not miss  X  X ads X  must mean that in the combination the HMM label is stronger than the CRF label at this particular observation.

We produced our HMM + CRF combination by merging the Viterbi search used by both HMMs and CRFs. We did this by using the Expectation Maximization algorithm to esti-mate the optimal weights. We achieved some improvement in overall score, but did not make a substantial improvement over either model alone. The reason is that we use the same pair of weights for each token position during the Viterbi search. Ideally, during the Viterbi search, it would be desir-able to provide a weight of 1 to the model that correctly labeled the token in question and a weight of 0 to the model that did not provide the correct label. If we had an algorithm that could correctly select the appropriate model at each token position, the best f -score we would get is 86.62%. (We found this score by tallying against the labeled version of the test set). Finally, it is likely that all of the approaches that we tried would have done better had more training data been available.

With improved coverage of training data, both the HMM and the CRF would be able to perform better. In the case of the CRF, we can expect more training data to help improve bigram coverage for use in bigram features. A comprehen-sive bigram coverage adds robustness even in the presence of the ambiguity problem posed by unigrams because bigrams are unlikely to be ambiguously labeled. 7 Conclusions Textual case-based reasoning is a promising technology for organizations with large amounts of textual information and taxonomies, such as the part name taxonomy described here, which can be used to aid the case-based reasoning applica-tion. When, as is the case here, that extraction is from noisy data, effective techniques must be found to extract informa-tion, here part names, to support TCBR.

While corpora linguistic techniques can perform well with large amounts of labeled training data, many organizations with textual data are not in a position to develop these large training sets. HMMs with shrinkage and CRF approaches, as described in this paper, show how extraction can be success-ful with much smaller training sets.

The benchmarks obtained with TCBR using relatively small amounts of annotated data should be used in a compar-ison against algorithms that use exclusively domain ontology and structured data resources to guide part name extraction and identification.

Many organizations represent institutional knowledge in the form of structured data which either exists as or is easily converted into a hierarchical ontology. Such an ontology as we have shown can play an important role in disambiguation of noun phrase spans in short-hand professional notes. Our experience shows that such ontologies underline the language of professional communication and can therefore be effi-ciently used in their automatic understanding and augmenta-tion. We presented a method for exploiting explicit ontologies to this effect. Automatic ontology construction techniques may find a new use in concert with our method to further improve the accuracy of automatic context understanding. References
