 Measuring the semantic relatedness of two pieces of text is a fundamental problem in language processing tasks like plagiarism detection, query ranking, and question answering. In this paper, we address the sentence similarity measurement prob-lem: given a query sentence S 1 and a comparison sentence S 2 , the task is to compute their similar-ity in terms of a score sim ( S 1 ,S 2 ) . This simi-larity score can be used within a system that de-termines whether two sentences are paraphrases, e.g., by comparing it to a threshold.

Measuring sentence similarity is challenging because of the variability of linguistic expression and the limited amount of annotated training data. This makes it difficult to use sparse, hand-crafted features as in conventional approaches in NLP. Re-cent successes in sentence similarity have been ob-tained by using neural networks (Tai et al., 2015; Yin and Sch  X  utze, 2015). Our approach is also based on neural networks: we propose a modular functional architecture with two components, sen-tence modeling and similarity measurement.

For sentence modeling, we use a convolutional neural network featuring convolution filters with multiple granularities and window sizes, followed by multiple types of pooling. We experiment with two types of word embeddings as well as part-of-speech tag embeddings (Sec. 4). For similar-ity measurement, we compare pairs of local re-gions of the sentence representations, using multi-ple distance functions: cosine distance, Euclidean distance, and element-wise difference (Sec. 5).
We demonstrate state-of-the-art performance on two SemEval semantic relatedness tasks (Agirre et al., 2012; Marelli et al., 2014), and highly com-petitive performance on the Microsoft Research paraphrase (MSRP) identification task (Dolan et al., 2004). On the SemEval-2014 task, we match the state-of-the-art dependency tree Long Short-Term Memory (LSTM) neural networks of Tai et al. (2015) without using parsers or part-of-speech taggers. On the MSRP task, we outper-form the recently-proposed convolutional neural network model of Yin and Sch  X  utze (2015) with-out any pretraining. In addition, we perform ab-lation experiments to show the contribution of our modeling decisions for all three datasets, demon-strating clear benefits from our use of multiple per-spectives both in sentence modeling and structured similarity measurement. Most previous work on modeling sentence simi-larity has focused on feature engineering. Sev-eral types of sparse features have been found use-ful, including: (1) string-based, including n -gram overlap features on both the word and character levels (Wan et al., 2006) and features based on machine translation evaluation metrics (Madnani et al., 2012); (2) knowledge-based, using exter-nal lexical resources such as WordNet (Fellbaum, 1998; Fern and Stevenson, 2008); (3) syntax-based, e.g., modeling divergence of dependency syntax between the two sentences (Das and Smith, 2009); (4) corpus-based, using distributional mod-els such as latent semantic analysis to obtain fea-tures (Hassan, 2011; Guo and Diab, 2012).

Several strongly-performing approaches used system combination (Das and Smith, 2009; Mad-nani et al., 2012) or multi-task learning. Xu et al. (2014) developed a feature-rich multi-instance learning model that jointly learns paraphrase rela-tions between word and sentence pairs.

Recent work has moved away from hand-crafted features and towards modeling with dis-tributed representations and neural network archi-tectures. Collobert and Weston (2008) used con-volutional neural networks in a multitask setting, where their model is trained jointly for multiple NLP tasks with shared weights. Kalchbrenner et al. (2014) introduced a convolutional neural net-work for sentence modeling that uses dynamic k -max pooling to better model inputs of varying sizes. Kim (2014) proposed several modifications to the convolutional neural network architecture of Collobert and Weston (2008), including the use of both fixed and learned word vectors and varying window sizes of the convolution filters.

For the MSRP task, Socher et al. (2011) used a recursive neural network to model each sen-tence, recursively computing the representation for the sentence from the representations of its constituents in a binarized constituent parse. Ji and Eisenstein (2013) used matrix factorization techniques to obtain sentence representations, and combined them with fine-tuned sparse features us-ing an SVM classifier for similarity prediction. Both Socher et al. and Ji and Eisenstein incor-porated sparse features to improve performance, which we do not use in this work.

Hu et al. (2014) used convolutional neural net-works that combine hierarchical sentence mod-eling with layer-by-layer composition and pool-ing. While they performed comparisons directly over entire sentence representations, we instead develop a structured similarity measurement layer to compare local regions. A variety of other neural network models have been proposed for similarity tasks (Weston et al., 2011; Huang et al., 2013; An-drew et al., 2013; Bromley et al., 1993).

Most recently, Tai et al. (2015) and Zhu et al. (2015) concurrently proposed a tree-based LSTM neural network architecture for sentence model-ing. Unlike them, we do not use syntactic parsers, yet our performance matches Tai et al. (2015) on the similarity task. This result is appealing because high-quality parsers are difficult to ob-tain for low-resource languages or specialized do-mains. Yin and Sch  X  utze (2015) concurrently de-veloped a convolutional neural network architec-ture for paraphrase identification, which we com-pare to in our experiments. Their best results rely on an unsupervised pretraining step, which we do not need to match their performance.

Our model architecture differs from previous work in several ways. We exploit multiple per-spectives of input sentences in order to maxi-mize information utilization and perform struc-tured comparisons over particular regions of the sentence representations. We now proceed to de-scribe our model in detail, and we compare to the above related work in our experimental evaluation. Modeling textual similarity is complicated by the ambiguity and variability of linguistic expression. We designed a model with these phenomena in mind, exploiting multiple types of input which are processed by multiple types of convolution and pooling. Our similarity architecture likewise uses multiple similarity functions.

To summarize, our model (shown in Figure 1) consists of two main components: 1. A sentence model for converting a sentence into a representation for similarity measure-ment; we use a convolutional neural network architecture with multiple types of convolution and pooling in order to capture different granu-larities of information in the inputs. 2. A similarity measurement layer using multi-ple similarity measurements, which compare lo-cal regions of the sentence representations from the sentence model.

Our model has a  X  X iamese X  structure (Bromley et al., 1993) with two subnetworks each process-ing a sentence in parallel. The subnetworks share all of their weights, and are joined by the simi-larity measurement layer, then followed by a fully connected layer for similarity score output. Figure 1: Model overview. Two input sentences (on the bottom) are processed in parallel by iden-tical neural networks, outputting sentence repre-sentations. The sentence representations are com-pared by the structured similarity measurement layer. The similarity features are then passed to a fully-connected layer for computing the similarity score (top).
 Importantly, we do not require resources like WordNet or syntactic parsers for the language of interest; we only use optional part-of-speech tags and pretrained word embeddings. The main dif-ference from prior work lies in our use of multiple types of convolution, pooling, and structured sim-ilarity measurement over local regions. We show later in our experiments that the bulk of our perfor-mance comes from this use of multiple  X  X erspec-tives X  of the input sentences.

We describe our sentence model in Section 4 and our similarity measurement layer in Section 5. In this section we describe our convolutional neu-ral network for modeling each sentence. We use two types of convolution filters defined on differ-ent perspectives of the input (Sec. 4.1), and also use multiple types of pooling (Sec. 4.2).

Our inputs are streams of tokens, which can be interpreted as a temporal sequence where nearby words are likely to be correlated. Let sent  X  resented by Dim -dimensional word embeddings, where sent i  X  R Dim is the embedding of the i -th word in the sequence and sent i : j represents the concatenation of embeddings from word i up to and including word j . We denote the k -th dimen-sion of the i -th word vector by sent [ k ] note the vector containing the k -th dimension of words i to j by sent [ k ] Figure 2: Left: a holistic filter matches entire word vectors (here, ws = 2 ). Right: per-dimension fil-ters match against each dimension of the word em-beddings independently. 4.1 Convolution on Multiple Perspectives We define a convolution filter F as a tuple  X  ws ,w F ,b F ,h F  X  , where ws is the sliding window the filter, b F  X  R is the bias, and h F is the activa-tion function (a nonlinear function such as tanh ). When filter F is applied to sequence sent , the inner product is computed between w F and each possible window of word embeddings of length ws in sent , then the bias is added and the activa-tion function is applied. This results in an output where i  X  [1 , 1 + len  X  ws ] . This filter can be viewed as performing  X  X emporal X  convolution, as it matches against regions of the word sequence. Since these filters consider the entirety of each word embedding at each position, we call them holistic filters; see the left half of Figure 2.
In addition, we target information at a finer granularity by constructing per-dimension filters dings, where w of Figure 2. The per-dimension filters are simi-lar to  X  X patial convolution X  filters except that we limit each to a single, predefined dimension. We include separate per-dimension filters for each di-mension of the input word embeddings.
  X  ws ,w sults in an output vector out where entry i (for i  X  [1 , 1 + len  X  ws ] ) equals out Our use of word embeddings in both ways allows more information to be extracted for richer sen-tence modeling. While we typically do not expect individual dimensions of neural word embeddings Figure 3: Each building block consists of multiple independent pooling layers and convolution layers with width ws 1 . Left: block A operates on entire vectors of word embeddings. Right: block B oper-ates on individual dimensions of word vectors to capture information of a finer granularity. to be interpretable to humans, there may still be distinct information captured by the different di-mensions that our model could exploit. Further-more, if we update the word embeddings during learning, different dimensions could be encour-aged further to capture distinct information.
We define a convolution layer as a set of con-volution filters that share the same type (holistic or per-dimension), activation function, and width ws . The type, width, activation function, and num-ber of filters numFilter in the layer are chosen by the modeler and the weights of each filter ( w F and b ) are learned. 4.2 Multiple Pooling Types The output vector out F of a convolution filter F is typically converted to a scalar for subsequent use by the model using some method of pooling. For example,  X  X ax-pooling X  applies a max operation across the entries of out F and returns the max-imum value. In this paper, we experiment with two additional types of pooling:  X  X in-pooling X  and  X  X ean-pooling X .

A group, denoted group ( ws , pooling , sent ) , is an object that contains a convolution layer with width ws , uses pooling function pooling , and op-erates on sentence sent . We define a building block to be a set of groups. We use two types of building blocks, block A and block B , as shown in Figure 3. We define block A as { group A ( ws a ,p, sent ) : p  X  X  max , min , mean }} . That is, an instance of block A has three convolu-tion layers, one corresponding to each of the three pooling functions; all have the same window size ws a . An alternative choice would be to use the multiple types of pooling on the same filters (Ren-nie et al., 2014); we instead use independent sets blocks of type A for all holistic convolution layers.
We define block B as That is, block B contains two groups of convolu-tion layers of width ws b , one with max-pooling and one with min-pooling. Each group B (  X  ) con-tains a convolution layer with Dim per-dimension convolution filters. That is, we use blocks of type B for convolution layers that operate on individual dimensions of word vectors.

We use these multiple types of pooling to ex-tract different types of information from each type of filter. The design of each group (  X  ) allows a pooling function to interact with its own underly-ing convolution layers independently, so each con-volution layer can learn to recognize distinct phe-nomena of the input for richer sentence modeling.
For a group A ( ws a , pooling a , sent ) with a con-volution layer with numFilter A filters, we define the output oG A as a vector of length numFilter A where entry j is where filters are indexed as F j . That is, the output of group A (  X  ) is a numFilter A -length vector con-taining the output of applying the pooling function A component group B (  X  ) of block B contains Dim filters, each operating on a particular di-mension of the word embeddings. We define the output oG B of group B ( ws b , pooling b , sent ) as a Dim  X  numFilter B matrix where entry [ k ][ j ] is where filter F [ k ] 4.3 Multiple Window Sizes Similar to traditional n -gram-based models, we use multiple window sizes ws in our building blocks in order to learn features of different lengths. For example, in Figure 4 we use four building blocks, each with one window size ws = Figure 4: Example neural network architecture for a single sentence, containing 3 instances of block A (with 3 types of pooling) and 2 instances of block B (with 2 types) on varying window sizes ws = 1 , 2 and ws =  X  ; block A operates on entire word vec-tors while block B contains filters that operate on individual dimensions independently. 1 or 2 for its own convolution layers. In order to retain the original information in the sentences, we also include the entire matrix of word embeddings in the sentence, which essentially corresponds to ws =  X  .

The width ws represents how many words are matched by a filter, so using larger values of ws corresponds to matching longer n -grams in the input sentences. The ranges of ws values and the numbers of filters numFilter of block A and block B are empirical choices tuned based on vali-dation data. In this section we describe the second part of our model, the similarity measurement layer.

Given two input sentences, the first part of our model computes sentence representations for each of them in parallel. One straightforward way to compare them is to flatten the sentence represen-tations into two vectors, then use standard met-rics like cosine similarity. However, this may not be optimal because different regions of the flattened sentence representations are from differ-ent underlying sources (e.g., groups of different widths, types of pooling, dimensions of word vec-tors, etc.). Flattening might discard useful com-positional information for computing similarity. We therefore perform structured comparisons over particular regions of the sentence representations.
One important consideration is how to iden-tify suitable local regions for comparison so that we can best utilize the compositional information in the sentence representations. There are many possible ways to group local comparison regions. In doing so, we consider the following four as-pects: 1) whether from the same building block; 2) whether from convolutional layers with the same window size; 3) whether from the same pooling layer; 4) whether from the same filter of the under-regions that share at least two of these conditions.
To concretize this, we provide two algorithms below to identify meaningful local regions. While there exist other sets of comparable regions that share the above conditions, we do not explore them all due to concerns about learning efficiency; we find that the subset we consider performs strongly in practice. 5.1 Similarity Comparison Units We define two comparison units for comparing two local regions in the sentence representations:
Cosine distance ( cos ) measures the distance of two vectors according to the angle between them, while L 2 Euclidean distance ( L 2 Euclid ) and element-wise absolute difference measure magni-tude differences. 5.2 Comparison over Local Regions Algorithms 1 and 2 show how the two sentence representations are compared in our model. Algo-rithm 1 works on the output of block A only, while Algorithm 2 deals with both block A and block B , focusing on regions from the output of the same pooling type and same block type, but with differ-ent filters and window sizes of convolution layers.
Given two sentences S 1 and S 2 , we set the max-imum window size ws of block A and block B to be n , let regM  X  represent a numFilter A by n +1 ma-trix, and assume that each group  X  outputs its cor-responding oG  X  . The output features are accumu-lated in a final vector fea . 5.3 One Simplified Example We provide a simplified working example to show how the two algorithms compare outputs of block A only. If we arrange the sentence representations into the shape of sentence matrices as in Figure 5, Algorithm 1 Horizontal Comparison Algorithm 2 Vertical Comparison then in Algorithms 1 and 2 we are essentially com-paring local regions of the two matrices in two di-rections: along rows and columns.

In Figure 5, each column of the max / min / mean groups is compared with all columns of the same pooling group for the other sentence. This is shown in red dotted lines in the Figure and listed in lines 2 to 9 in Algorithm 2. Note that both ws 1 and ws 2 columns within each pooling group should be compared using red dotted lines, but we omit this from the figure for clarity.

In the horizontal direction, each equal-sized max / min / mean group is extracted as a vector and is compared to the corresponding one for the other sentence. This process is repeated for all rows and comparisons are shown in green solid lines, as per-formed by Algorithm 1. 5.4 Other Model Details Output Fully-Connected Layer. On top of the similarity measurement layer (which outputs a vector containing all fea  X  ), we stack two linear layers with an activation layer in between, fol-lowed by a log-softmax layer as the final output layer, which outputs the similarity score.
 Activation Layers. We used element-wise tanh Figure 5: Simplified example of local region com-parisons over two sentence representations that use block A only. The  X  X orizontal comparison X  (Algorithm 1) is shown with green solid lines and  X  X ertical comparison X  (Algorithm 2) with red dot-ted lines. Each sentence representation uses win-dow sizes ws 1 and ws 2 with max / min / mean pool-ing and numFilter A = 3 filters. as the activation function for all convolution filters and for the activation layer placed between the fi-nal two layers. Everything necessary to replicate our experimen-tal results can be found in our open-source code 6.1 Tasks and Datasets We consider three sentence pair similarity tasks: 1. Microsoft Research Paraphrase Corpus 2. Sentences Involving Compositional Knowl-3. Microsoft Video Paraphrase Corpus 6.2 Training We use a hinge loss for the MSRP paraphrase identification task. This is simpler than log loss since it only penalizes misclassified cases. The training objective is to minimize the following loss (summed over examples  X  x,y gold  X  ): loss (  X ,x,y gold ) = where y gold is the ground truth label, input x is the pair of sentences x = { S 1 ,S 2 } ,  X  is the model weight vector to be trained, and the func-tion f  X  ( x,y ) is the output of our model.
We use regularized KL-divergence loss for the semantic relatedness tasks (SICK and MSRVID), since the goal is to predict the similarity of the two sentences. The training objective is to minimize the KL-divergence loss plus an L 2 regularizer: loss (  X  ) = where b f  X  is the predicted distribution with model weight vector  X  , f is the ground truth, m is the number of training examples, and  X  is the regu-larization parameter. Note that we use the same KL-loss function and same sparse target distribu-tion technique as Tai et al. (2015). 6.3 Experiment Settings We conduct experiments with ws values in the range [1 , 3] as well as ws =  X  (no convolution).
We use multiple kinds of embeddings to rep-resent each sentence, both on words and part-of-speech (POS) tags. We use the Dim g = 300 -dimensional GloVe word embeddings (Pennington et al., 2014) trained on 840 billion tokens. We use Dim k = 25 -dimensional PARAGRAM vec-tors (Wieting et al., 2015) only on the MSRP task since they were developed for paraphrase tasks, having been trained on word pairs from the Para-phrase Database (Ganitkevitch et al., 2013). For POS embeddings, we run the Stanford POS tag-ger (Manning et al., 2014) on the English side of the Xinhua machine translation parallel cor-pus, which consists of Xinhua news articles with approximately 25 million words. We then train Dim p = 200 -dimensional POS embeddings us-ing the word2vec toolkit (Mikolov et al., 2013). Adding POS embeddings is expected to retain syn-tactic information which is reported to be effec-tive for paraphrase identification (Das and Smith, 2009). We use POS embeddings only for the MSRP task.

Therefore for MSRP, we concatenate all word and POS embeddings and obtain Dim = Dim g + Dim p + Dim k = 525 -dimension vectors for each input word; for SICK and MSRVID we only use Dim = 300 -dimension GloVe embeddings.

We use 5-fold cross validation on the MSRP training data for tuning, then largely re-use the same hyperparameters for the other two datasets. However, there are two changes: 1) for the MSRP task we update word embeddings during train-ing but not so on SICK and MSRVID tasks; 2) we set the fully connected layer to contain 250 hidden units for MSRP, and 150 for SICK and MSRVID. These changes were done to speed up our experimental cycle on SICK and MSRVID; on SICK data they are the same experimental settings as used by Tai et al. (2015), which makes for a cleaner empirical comparison.

We set the number of holistic filters in block A to be the same as the input word embeddings, therefore numFilter A = 525 for MSRP and numFilter A = 300 for SICK and MSRVID. We set the number of per-dimension filters in block B to be numFilter B = 20 per dimension for all three datasets, which corresponds to 20  X  Dim fil-ters in total.

We perform optimization using stochastic gra-dient descent (Bottou, 1998). The backpropaga-tion algorithm is used to compute gradients for all parameters during training (Goller and Kuch-ler, 1996). We fix the learning rate to 0.01 and 6.4 Results on Three Datasets Results on MSRP Data. We report F1 scores and accuracies from prior work in Table 1. Ap-Table 1: Test set results on MSRP for paraphrase identification. Rows in grey are neural network-based approaches. proaches shown in gray rows of the table are based on neural networks. The recent approach by Yin and Sch  X  utze (2015) includes a pretraining technique which significantly improves results, as shown in the table. We do not use any pretrain-ing but still slightly outperform their best results which use both pretraining and additional sparse features from Madnani et al. (2012).

When comparing to their model without pre-training, we outperform them by 6% absolute in accuracy and 3% in F1. Our model is also supe-rior to other recent neural network models (Hu et al., 2014; Socher et al., 2011) without requiring sparse features or unlabeled data as in (Yin and Sch  X  utze, 2015; Socher et al., 2011). The best re-sult on MSRP is from Ji and Eisenstein (2013) which uses unsupervised learning on the MSRP test set and rich sparse features.
 Results on SICK Data. Our results on the SICK task are summarized in Table 2, showing Pearson X  X  r , Spearman X  X   X  , and mean squared error (MSE). We include results from the literature as reported by Tai et al. (2015), including prior work using re-current neural networks (RNNs), the best submis-sions in the SemEval-2014 competition, and vari-ants of LSTMs. When measured by Pearson X  X  r , the previous state-of-the-art approach uses a tree-structured LSTM (Tai et al., 2015); note that their best results require a dependency parser.

On the contrary, our approach does not rely on parse trees, nor do we use POS/ PARAGRAM em-beddings for this task. The word embeddings, Table 2: Test set results on SICK, as reported by Tai et al. (2015), grouped as: (1) RNN vari-ants; (2) SemEval 2014 systems; (3) sequential LSTM variants; (4) dependency and constituency tree LSTMs (Tai et al., 2015). Evaluation metrics are Pearson X  X  r , Spearman X  X   X  , and mean squared error (MSE).
 Table 3: Test set results on MSRVID data. The B  X  ar et al. (2012) and  X  Sari  X  c et al. (2012) results were the top two submissions in the Semantic Textual Similarity task at the SemEval-2012 competition. sparse distribution targets, and KL loss function are exactly the same as used by Tai et al. (2015), therefore representing comparable conditions. Results on MSRVID Data. Our results on the MSRVID data are summarized in Table 3, which includes the top 2 submissions in the Seman-tic Textual Similarity (STS) task from SemEval-2012. We find that we outperform the top system from the task by nearly 3 points in Pearson X  X  r . 6.5 Model Ablation Study We report the results of an ablation study in Ta-ble 4. We identify nine major components of our approach, remove one at a time (if applicable), and perform re-training and re-testing for all three tasks. We use the same experimental settings in Sec. 6.3 and report differences (in accuracy for MSRP, Pearson X  X  r for SICK/MSRVID) compared to our results in Tables 1 X 3. We remove components one at a time and show differences.
The nine components can be divided into four groups: (1) input embeddings (components 1 X 2); (2) sentence modeling (components 3 X 5); (3) sim-ilarity measurement metrics (component 6); (4) similarity measurement layer (components 7 X 9). For MSRP, we use all nine components. For SICK and MSRVID, we use components 3 X 9 (as de-scribed in Sec. 6.3).

From Table 4 we find drops in performance for all components, with the largest differences ap-pearing when removing components of the simi-larity measurement layer. For example, conduct-ing comparisons over flattened sentence represen-tations (removing component 9) leads to large drops across tasks, because this ignores struc-tured information within sentence representations. Groups (1) and (2) are also useful, particularly for the MSRP task, demonstrating the extra benefit obtained from our multi-perspective approach in sentence modeling.

We see consistent drops when ablating the Ver-tical/Horizontal algorithms that target particular regions for comparison. Also, removing group (3) hinders both the Horizontal and Vertical al-gorithms (as described in Section 5.1), so its removal similarly causes large drops in perfor-mance. Though convolutional neural networks al-ready perform strongly when followed by flattened vector comparison, we are able to leverage the full richness of the sentence models by performing structured similarity modeling on their outputs. On the SICK dataset, the dependency tree LSTM (Tai et al., 2015) and our model achieve comparable performance despite taking very dif-ferent approaches. Tai et al. use syntactic parse trees and gating mechanisms to convert each sen-tence into a vector, while we use large sets of flex-ible feature extractors in the form of convolution filters, then compare particular subsets of features in our similarity measurement layer.

Our model architecture, with its many paths of information flow, is admittedly complex. Though we have removed hand engineering of features, we have added a substantial amount of functional architecture engineering. This may be necessary when using the small training sets provided for the tasks we consider here. We conjecture that a sim-pler, deeper neural network architecture may out-perform our model when given large amounts of training data, but we leave an investigation of this direction to future work.

In summary, we developed a novel model for sentence similarity based on convolutional neural networks. We improved both sentence modeling and similarity measurement. Our model achieves highly competitive performance on three datasets. Ablation experiments show that the performance improvement comes from our use of multiple per-spectives in both sentence modeling and structured similarity measurement over local regions of sen-tence representations. Future work could extend this model to related tasks including question an-swering and information retrieval.
 This work was supported by the U.S. National Sci-ence Foundation under awards IIS-1218043 and CNS-1405688. Any opinions, findings, conclu-sions, or recommendations expressed are those of the authors and do not necessarily reflect the views of the sponsor. We would like to thank the anony-mous reviewers for their feedback and CLIP lab-mates for their support.
