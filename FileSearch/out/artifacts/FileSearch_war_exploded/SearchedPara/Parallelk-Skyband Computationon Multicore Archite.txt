 In recent decades, more manufacturers are pr oducing multicore architecture proces-sors due to the frequency wall. As a consequence, even personal computer has 4 cores, 8 cores or more. It is meaningful to design algorithms to leverage this feature, espe-cially for problems which are computationally expensive. k -skyband query is a typical problem of such kind that requires extensive computation. This paper investigates the problem, and designs a parallel solution using multicore processors.

Given a data set D of elements in a d -dimensional numerical space, we say an ele-ment p dominates another element q ,if p is not worse than q in every dimension and is better than q in at least one dimension. Without lo se of generality, in this paper, we assume lower value is preferred. Given a data set D ,a k -skyband query retrieves the set of elements which are dominated by at most k elements. The concept of k -skyband is closely related to another two concepts, skyline and top-k ranking queries. Given a data set D , skyline query retrieves the elements of a data set that are not worse than others. Clearly, skyline query is a special case of k -skyband where k =0 . Given a data set D and a ranking function f , top-k ranking query retrieves the k elements with highest score according to f .Note k -skyband query keeps a minimum candidate set for top-k query. Thus, keeping k -skyband of a data set is sufficient to answer all top-k queries. Below, we give an example of k -skyband.
 Example 1. Consider the 13 2 -dimensional elements in a data set as shown in Fig-ure 1. k -skyband query with k =0 (or skyline) returns the set { a, i, k } . 1 -skyband consists { a, i, k, b, h, m } which are worse than at most 1 other element. 2 -skyband con-sists { a, i, k, b, h, m, c, g } which are worse than at most 2 other elements.
To the best of our knowledge, this paper is among the first to study the problem of parallel k -skyband computation on multicore architecture. Our main contributions are summarized as follows.  X  We design a parallel algorithm to efficiently compute k -skyband query and analyse  X  We conduct extensive experimental evaluation to verify the effectiveness and effi-The rest of the paper is organized as follows. In Section 2, we formally define the k -skyband problem and provide some necessary background information about our parallel techniques. In Section 3, we present the BBSkyband algorithm for k -skyband computation, which is used as the baseline algorithm. Section 4 presents the parallel version of BBSkyband algorithm (Parallel BBSkyband). In Section 5, we examine the efficiency of our algorithm. Related work is summarized in Section 6. This is followed by conclusions. We present the problem definition and intr oduce a parallel library, OpenMP, in this section. 2.1 Problem Definition Definition 1 (Dominate). Consider two distinct d -dimensional elements p and q , and for 1  X  i  X  d , and there exists j , such that p [ j ] &lt;q [ j ] .
 Definition 2 ( k -skyband query). Given a data set D of elements in a d -dimensional numerical space, the k -skyband query retrieves the subset of D which are dominated by at most k elements in D .
 Problem Statement. In this paper, we study the problem of efficiently retrieving k -skyband elements from a data set using parallel techniques. 2.2 OpenMP In this paper, we use OpenMP [5] to parallelize the k -skyband computation. OpenMP (Open Multiprocessing) is an API that suppor ts multi-platform shared memory multi-processing programming in C, C++, and Fortran, on most processor architectures and operating systems. It consists of a set of compiler directives, library routines, and envi-ronment variables that in fluence run-time behavior.

OpenMP is an implementation of multithreading, a method of parallelizing whereby a master thread (a series of instructions executed consecutively) forks a specified num-ber of slave threads and a task is divided among them. The threads then run concur-rently, with the runtime environment allocating threads to different processors.
There are many details about the usage of OpenMP. Here, we only present the two examples we used in our implementation. More information can be found on their website 1 .
 Example 2. for ( i =0; i&lt;size ; i ++) The first line of Example 2 instructs the compiler to distribute loop iterations within the team of threads that encounters this work-sharing construct. Besides,  X  X efault (shared) private (i) X  means all the variables in the loop region be shared across all threads except i . In the following sections, for a concise presentation, we use  X  X arallel X  instead of  X  pragma omp parallel for default (shared) private (i) X .
 Example 3. omp set num threads ( NUMBER OF PROCESSORS ); The API in Example 3 instructs the compiler to set the number of threads in subsequent parallel regions to be NUMBER OF PROCESSORS. This can be useful to see how the running time changes against varying number of threads. Papadias et al. develop a branch-and-bound algorithm (BBS) to get the skyline with the minimal I/O cost [11]. They also point that their algorithms can be used to compute k -skyband with some modifications.

First, we demonstrate the main idea behind BBS. Theorem 1. An element p cannot dominate another element q ,ifthesumof q  X  X  all dimension is smaller than that of p . That is, d i =1 p [ i ] &lt; d i =1 q [ i ]  X  q  X  p . This theorem is intuitive and can be proved using Definition 1.

Then, we modify the BBS algorithm to compute k -skyband. Algorithm 1 shows the details of an algorithm adapted from BBS algorithm. H is a minheap. S is a buffer used to collect all the elements of k -skyband. k denotes the k -skyband width. R is the root of the data set X  X  R-tree index. All elements that cannot be in S are discarded in Line 6 (dominated by more than k elements). All elements that should be in S are inserted into S in Line 12 . All entries that may contribute to final result are inserted into the heap in Line 10 .
 Algorithm 1. BBSkyband Correctness and Co mplexity Analysis. We prove the correctness in two steps. First, all the elements in S must be in k -skyband. This is guaranteed by Theorem 1. Second, all the elements that are discarded cannot be in k -skyband. This is intuitive because these elements are already dominated by k +1 elements. We analyse the worst case complexity of Algorithm 1; i.e ., when all the elements are k -skyband elements. Let N denote the size of the data set. In this case, the most time-consuming parts are Line 5 and Line 9 of Algorithm 1; and thus, the time complexity is given by O( N 2 ).
Although the worst case time complexity is O( N 2 ), this algorithm is still efficient because it discards the non-k -skyband elements at a level as high as possible. Below is an example for Algorithm 1.
 Example 4. Consider the running example in Section 1. Suppose we are going to find 2 -skyband and the R-tree is depicted in Figure 2. Applying the BBSkyband (Algorithm 1), we have the following steps in Table 2. We omit some steps in which the top entry is an element. In the initial step, we insert all the entries of root R , e 6 and e 7 ,into H .Then we remove top entry, e 7 , from H . Note the distance of e 7 is 0+3=3 which is smaller than that of e 6 , 5+1=6 . Because no element in S dominates e 7 , we expand e 7 .After expanding e 7 ,wehave { e 3 ,e 6 ,e 5 ,e 4 } in H . Then we remove and expand e 3 .After that, we have { i, e 6 ,h,e 5 ,e 4 ,g } in H . Then, we remove i . Because i is at leaf level, we insert i into S instead of expanding. This process goes until there is no element or entry left in H . Finally, S contains the 2 -skyband, that is, { i, h, m, k, a, g, c, b } . In this section, we are going to parallelize the baseline algorithm in Section 3.
By analysing the algorithm, we find the most computation consuming parts are Line 5 and Line 9 in Algorithm 1 because the element is compared with all the elements in S .
A naive solution is to parallelize these two lines by using the parallel command in Example 2. However, for ex ample, when executing Line 5 , a thread has found more than k dominating elements and break out, how could other threads be notified? To solve that, we either have to let a thread waits for all other comparing threads terminate, or use communication method between threads. Both of the two ways decrease the ben-efit we get from parallelization.
 Algorithm 2. Parallel BBSkyband
Algorithm 2 shows the details about our algorithm to parallelize BBSkyband. S is used to store all the k -skyband elements we have found. S is a buffer to store candidate k -skyband elements. k is the width of the k -skyband. e k , e k and c jk are used to record the number of elements dominating e k , e k and c jk respectively. Our solution is to use abuffer S to store all candidate entries which are not dominated by more than k other elements in this buffer (Lines 4  X  9 ). Then, we compare these candidate entries with elements that are already in k -skyband, i.e. elements in S (Lines 11  X  15 ). This operation can be run concurrently. After that, if the entry is an intermediate node, we compare each child node of the entry with k -skyband elements in S concurrently (Lines 19  X  23 ); If the element is a leaf node, we insert it into S (Line 28 ). By doing this, we parallelize the most time-consuming parts, Line 5 and Line 9 in Algorithm 1.
 Correctness and Co mplexity Analysis. The proof of correctness is quite similar to that of Algorithm 1. All the elements in S must be in k -skyband and all the elements that are discarded cannot be in k -skyband. Next, We analyse the worst case complexity of Algorithm 2; Similarly, we assume all the elements are k -skyband elements. Let N denote the size of the data set. In this case, the most time-consuming parts are Line 12 and Line 20 of Algorithm 2. And thus, the time complexity is given by O( N 2 /c )where c is the number of cores.
 Example 5. Consider the running example in Section 3. Suppose we are going to find 2 -skyband using parallel BBSkyband (Algorithm 2). Table 3 shows the steps of the algorithm. We omit some steps where there is no intermediate entry in S . In the initial step, we insert all the entries of root R , e 6 and e 7 , into minheap H . Then we remove top entry e 7 . Because no element in S dominates e 7 , e 7 is stored in S . In the next loop, because there is an intermediate entry ( e 7 )in S , we compare e 7 with every element in S and record the number of elements that dominate e 7 (Here is 0 ). Note that, if there are other elements in S , we can compare them with elements in S concurrently. Then, we expand e 7 (like Lines 19  X  23 in Algorithm 2). Note that, this process can be done concurrently. After expanding e 7 , we add entries that are not dominated by more than k elements to H (like Lines 24  X  26 in Algorithm 2). Here, we have { e 3 ,e 6 ,e 5 ,e 4 } in H . Then we remove and expand e 3 . After that, we have { i, e 6 ,h,e 5 ,e 4 ,g } in H .Note that, i will be moved to S . Similar process goes on until there is no element or entry left in H and S . Finally, S contains the 2 -skyband, that is, { i, h, m, a, k, g, b, c } . This section reports the experimental studies. 5.1 Setup We ran all experiments on a DELL OPTIPLEX 790 with Windows 7, two quad-Intel Core CPU and 2 G 1333 MHz RAM. We evaluated the efficiency of our algorithm against data set distribution, dimensionality, data set size, number of cores and k -skyband width, respectively. Our experime nts are conducted on synthetic data sets.
The synthetic data sets are generated as follows. We use the methodologies in [12] to generate 1 million data elements with dimensionality from 2 to 5 and the spatial location of data elements follow two kinds of distributions, independent and anti-correlated .
The default values of the par ameters are listed in Table 4. Data set distribution and number of cores do not have default values because we vary all the values of them in all experiments.

Parameters are varied as follows:  X  data set distribution: Independent, Anti-correlated.  X  dimensionality: 2 , 3 , 4 , 5 .  X  data set size: 64 K, 128 K, 256 K, 512 K, 1024 K.  X  number of cores: 1 , 2 , 4 , 8 .  X  k -skyband width: 2 , 4 , 6 , 8 . 5.2 Evaluation We evaluate our algorithm on both Independent and Anti-correlated data set. Besides, as demonstrated in Example 3, we use the command in Example 3 to set different number of threads which can also be regarded as different cores. Particularly, when c =1 ,we use BBSkyband as baseline algorithm rather than Parallel BBSkyband.

The first set of experiments is reported in Figure 3 where dimensionality varies. The performance of our algorithm decreases as the dimensionality increases. This is because when dimensionality increases, the cost of comparing two elements increases (Note the comparing cost is O ( d ) where d is the dimensionality). Besides, there are usually more elements in a k -skyband due to higher dimensionality. Because with higher dimension-ality, an element has to be better than another one in more dimensions. Note having more elements in k -skyband also increases the cost for operations such as Line 12 and Line 20 in Algorithm 2.

Figure 4 evaluates the system scalability towards the data set size. The performance of our algorithm decreases as the data set size increases. When data set size increases, there is a larger R-tree as index. The opera tion on R-tree takes more time. What is more, larger data set usually results in more elements in a k -skyband.

Figure 5 evaluates the impact of k -skyband width. As expected, Figure 5 shows that processing cost increases when k increases. k is the width of k -skyband. With larger k , we have to do more dominating test and we usually have more elements in a k -skyband.
In all figures, there is an obvious improvement when the number of cores rises. How-ever, it is hard to get a speedup that strictly equals the number of cores. This is because there are other costs that are not parallelized in our algorithm (I/O operation, collecting candidate elements, insertion into heap, etc.). Spatial database has draw many a ttention in recent decades. Beckmann et al. [2] design the R-tree as an index for efficient computation. Roussopoulos et al. [12] study Nearest Neighbor Queries and propose three efficient pruning rules. They also extend Nearest Neighbor Queries to K Nearest Neighbor Queries which return top-K preferred ob-jects. B  X  orzs  X  onyi et al. [3] first study the skyline operator and propose an SQL syntax for the skyline query. After that, the skyline query is widely studied by a lot of papers, like Chomicki et al. [4], Godfrey et al. [7] and Tan et al. [13]. Papadias et al. develop a branch-and-bound algorithm (BBS) to get the skyline with the minimal I/O cost [11].
The concept of k -skyband is first proposed in [11]. [10] shows that it suffices to keep the k -Skyband of the data set to provide the top-k ranked queries for the monotone preference function f .Reverse k -skyband query is recently studied in [9].
With the development of multi-core processors, a recent trend is to study the parallel skyline computation. Most of them are divide and conquer algorithms. Some papers focus on distributed skyline computation. That is, their algorithm runs on server cluster with several servers, such as [1]. Other papers focus on parallize skyline computation on different cores instead of servers. This is different since all the threads share the same memory and there is much less communication cost. Park et al. design a paral-lel algorithm on multicore architecture with an easy idea. But it distributes the most computation-consuming part to every core. Our work is based on their algorithm. Vla-chou et al. [14] employ the technique which partitions the data set according to the weight of each axis. Moreover, K  X  ohler et al. [8]useasimilarideabutafasterway to partition the data set. Besides, Gao et al. [6] design a parallel algorithm for skyline queries in multi-disk environment. In this paper, we investigate the problem of k -skyband query. We give a modified Al-gorithm (BBSkyband) from the traditional algorithm and parallelize it, namely, Parallel BBSkyband algorithm. The experimental results demonstrate that a simple design of the parallel algorithm provides a satisfactory runtime performance. We believe that more efficient algorithm can be further developed based on our presented method. Acknowledgements.. This work was supported in part by NSFC 61003049, the Nat-ural Science Foundation of Zhejiang Province of China under Grant LY12F02047, the Fundamental Research Funds for the Ce ntral Universities under Grant 2012QNA5018, the Key Project of Zhejiang University Ex cellent Young Teacher Fund (Zijin Plan), and the Bureau of Jiaxing City Science and Technology Project under Grant 2011AY1005.
