 Reputation management experts have to monitor X  X mong others X  X witter constantly and decide, at any given time, what is being said about the entity of interest (a company, organization, personality. . . ). Solving this reputation mon-itoring problem automatically as a topic detection task is both essential X  X anual processing of data is either costly or prohibitive X  X nd challenging X  X opics of interest for rep-utation monitoring are usually fine-grained and suffer from data sparsity.

We focus on a solution for the problem that (i) learns a pairwise tweet similarity function from previously anno-tated data, using all kinds of content-based and Twitter-based features; (ii) applies a clustering algorithm on the previously learned similarity function. Our experiments in-dicate that (i) Twitter signals can be used to improve the topic detection process with respect to using content sig-nals only; (ii) learning a similarity function is a flexible and efficient way of introducing supervision in the topic detec-tion clustering process. The performance of our best sys-tem is substantially better than state-of-the-art approaches and gets close to the inter-annotator agreement rate. A de-tailed qualitative inspection of the data further reveals two types of topics detected by reputation experts: reputation alerts / issues (which usually spike in time) and organiza-tional topics (which are usually stable across time). H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Experimentation, Measurement Online Reputation Monitoring, Similarity Functions, Topic Detection, Twitter
What are people saying about a given entity (company, brand, organization, personality, etc.) right now? Is there any issue that may damage the reputation of the entity? If so, what actions should be taken about it?
In order to answer such questions for a given client (the entity of interest), reputation experts have to daily moni-tor Twitter (among others) and discover, at any given time, what is being said about the client. Solving this reputation monitoring problem automatically as an (entity-specific) topic detection task is both essential and challenging [15, 19]. Es-sential, because real-time online opinions and comments are now key to understand the reputation of organizations and individuals and manage their public relations, and because manual processing of entity-related Twitter streams is very costly and sometimes simply unfeasible. And challenging, because topics of interest for reputation monitoring are usu-ally fine-grained and suffer from data sparsity X  X nless the client is Apple, Barack Obama or similar.
 The largest evaluation effort on topic detection for Online Reputation Monitoring on Twitter to date has been Rep-Lab 2013 [2], where the test collection provided manual an-notations by reputation experts on 142,527 tweets referring to 61 different entities (companies in the banking and cars domains, universities, and music bands). From the results of the participant systems it is not clear whether the topic detection process could benefit from training data (which is available in the dataset), and there is no clear evidence on whether Twitter-specific data (such as tweet metadata, hashtags, timestamps, etc.) could be effectively used to im-prove the results of term-based clustering.

Therefore, in this paper we focus on two related research questions: 1. Can Twitter signals be used to improve entity-specific 2. Can previously annotated material be used to learn bet-
In order to answer these two questions, we have modeled the topic detection problem as a combination of two tasks: 1. The first is learning tweet similarity: we use all types 2. The second is applying a clustering algorithm that uses
We detail our approach in Section 2, describe and discuss the result of our experimentation in Section 3, review re-latedworkinSection4, and summarize our main results in Section 5.
Givenanentity(e.g., Yamaha ) and a set of tweets rel-evant to the entity in a certain time span, the task con-sists of identifying tweet clusters, where each cluster rep-resents a topic/event/issue/conversation being discussed in the tweets, as it would be identified by reputation manage-ment experts.

Note that this is not a classification task, since topics dis-cussed in a given stream of tweets are not known a priori. Furthermore, this is not a standard Topic Detection setting, because in our scenario each of the tweets must be assigned to a topic. From the perspective of reputation management, reputation alerts X  X ssues that may affect the reputation of the client X  X ust be detected early, preferably before they explode, and therefore the number of tweets involved may be small at the time of detection. That makes the task harder than standard topic detection, mainly due to sparsity issues: topics about a given entity in a short time frame are part of the  X  X ong tail X  of Twitter topics, and some of them are small even in comparison with the size of the entity-specific Twitter stream.

Table 1 illustrates some examples of tweets belonging to the same topics, extracted from the RepLab 2013 dataset (described in detail in Section 3.1) and corresponding to entities Maroon 5 , Yamaha , Ferrari , Bank of America and Coldplay .
Probabilistic generative approaches are a popular strategy to handle topic detection tasks, but might be less appro-priate to solve this problem because of data sparsity [27]. Instead, we focus on learning similarity measures between tweets that predict whether two given tweets are about the same topic or not. We explore a wide range of similarity signals between tweets (terms, concepts, hashtags, author, timestamp, etc.) and use them as classification features to learn similarity measures. Similarity measures are, in turn, fed into a competitive clustering algorithm in order to detect topics.

Following the methodology proposed in [ 5] for a different clustering problem, we model the problem as a binary clas-sification task: given a pair of tweets d 1 ,d 2 , the system must decide whether the tweets belong to the same topic ( true )ornot( false ). Each pair of tweets is represented as a set of features (for instance, term overlapping between both tweets), which are used to feed a machine learning al-gorithm that learns a similarity function. Once we have learned to classify tweet pairs, we take the positive classifi-cation confidence as a similarity measure, which is used by a Hierarchical Agglomerative Clustering (HAC) algorithm to identify the topics.

We now detail the learning similarity step and the clus-tering step. Finally, in Section 2.3 we describe the features used to learn the similarity function.
Our first goal is to find a classification function that takes two tweets as input and decides if the tweets belong to the same topic or not. Once the pairwise binary classification model is built, its confidence is used as pairwise similarity measure. Formally, let d, d be two tweets in a set T .We want to learn a boolean function that says if both tweets belong to the same topic or not. We define a list of features F d,d =( f 1 ( d, d ) ,f 2 ( d, d ) ...f whereeachofthefeaturesisanestimationoftheoverlapbe-tween d, d according to different signals. Then we estimate the similarity between d, d as the probability that they be-long to the same topic given F d,d :
For each entity, we compute the confidence score for all the possible pairs of tweets related to it. The resulting similarity matrix is used by the Hierarchical Agglomerative Clustering (HAC) algorithm [24], with single linkage, that has been proven to perform competitively in clustering tasks such as Web People Search [ 13, 28]. In HAC there is no need to specify the number of clusters a priori: the first step is to create one cluster for each tweet in the similarity matrix, and then compute for each cluster the similarity to all other clusters. If the highest similarity computed is above a pre-defined threshold, the two clusters are merged together (ag-glomerated). A similarity threshold is then used as a stop criterion to get a flat clustering solution. As for  X  X ingle link-age X , it refers to the way in which clusters are compared during the clustering process: in single-link clustering, the similarity between two clusters is computed as the similar-ity of their most similar members (i.e. it focuses on the area where both clusters are closets to each other). A drawback of this method is that clusters may be merged due to single noisy elements being close to each other, but in practice it seems to be the best choice in problems related to ours [ 23, 28, 5].
In our study we consider a total of 13 features that cap-ture many types of Twitter signals. Features can be divided in four families: term features , that take into account sim-ilarity between the terms in the tweets; semantic features , that model tweet similarity by mapping tweets to concepts in a knowledge base, and then measuring concept overlap be-tween tweets; metadata , which indicate whether the tweets have authors, named users (i.e. twitter users mentioned in the tweets), URLs and hashtags in common; and time-aware features , which say how close are the creation timestamps for the tweets being compared.

The most obvious signal to take into account is word sim-ilarity. Tweets sharing a high percentage of vocabulary are likely to talk about the same topic and hence, to belong to the same cluster. We experiment with three term features that differ in how the terms are weighted:
Intuitively, representing tweets with semantics extracted from a knowledge base can be useful to group tweets that do not have words in common. For instance, the tweets d 1 and d 2 about Maroon 5 in Table 1 can be clustered together because the phrases mexicanas and Mexico both link to the concept Mexico . In some cases this relation could also be captured with stemming, but at the cost of additional false matches. In addition, it might be useful to detect salient terms when word similarity is low. For instance, the Jaccard similarity for tweets d 5 and d 6 is not high, but mapping into Wikipedia matches Alonso , Ferrari and estrategia in both tweets, which lead to a high concept match between them.
In our experiments we adopt an entity linking approach to gather Wikipedia entries that are semantically related to a tweet: the commonness probability [ 26] X  X asedontheintra-Wikipedia hyperlinks X  X hich computes the probability of a concept/entity c being the target of a link with anchor text q in Wikipedia by: where L q,c denotes the set of all links with anchor text q and target c .
As the dataset contains tweets in two languages, we use both (Spanish and English) Wikipedia dumps. Spanish Wi-kipedia articles are then translated to the corresponding En-glish Wikipedia article by following the inter-lingual links, using the Wikimedia API. 1
Tweets are then represented as the bag-of-entities derived from linking each n-gram in the content of the tweet to the most probable Wikipedia entity. In case of n-gram overlap, only the longest is considered.

Analogously to term features, we compute the semantic features semantic_jaccard , semantic_lin_cf and seman-tic_lin_tfidf over the bag-of-entities tweet representation. The feature semantic_jaccard is similarly defined by the Best RepLab system [ 34], detailed in  X  3.5.

Frequently, topics reflect an ongoing event (such as a live performance of a music group) or conversation. For this rea-son, close timestamps increase the probability of two tweets being related. For instance, tweets d 15 and d 16 were both published in the hour preceding a concert by Coldplay.
We define the features to estimate temporal relation be-tween tweets, given the timestamps t and t ,as: which takes values between 0 and 1. We turn this equation into three different features, depending on how we represent time: in milliseconds ( time_millis ), hours ( time_hours )or days ( time_days ).

Note that the author and the timestamp of the tweets are also considered by the Temporal Twitter-LDA system [ 34], described in  X  3.5. http://www.mediawiki.org/wiki/API:Properties
We first describe the dataset used for our experiments and then we analyze the results that help to answer our research questions: first, we study the impact of the different signals in the process of learning a similarity function in  X  3.2; then, we study the effect of embedding the similarity functions in the Clustering process to solve the Topic Detection task: in  X  3.3, we investigate the benefits of Twitter-related signals; in  X  3.4, whether the learning process is effective, and in  X  we compare our results with state-of-the-art results on the same corpus, i.e., the best RepLab 2013 systems. Finally, we report the results of a failure analysis that gives some insights into how reputation experts annotate and which are the main challenges for automatic systems. 2
To address our research questions we use the largest Twit-ter collection for reputation monitoring known to us, the RepLab2013 [ 2] dataset. The dataset comprises a total of 142,527 manually annotated tweets in two languages: En-glish and Spanish. This set is divided into 61 subsets cor-responding to tweets mentioning one of 61 entities belong-ing to four domains: automotive, banking, universities and music. For every entity, 750 (1,500) tweets were used as training (test) set on average, with a difference of up to six months between tweets in the training test and tweets in the test set. 3 Crawling was performed from June 1, 2012 to December 31, 2012 using each entity X  X  canonical name as query (e.g.,  X  X tanford X  for Stanford University). Since en-tity names are often ambiguous, tweets were first annotated with relevance ( Is the tweet about the entity of interest? )and only relevant tweets were then manually grouped in topics. In our experiments we use the subset of relevant tweets.
In order to better understand the real impact of similarity functions, we have removed from the collection those tweets annotated in the collection as  X  X ear-duplicates X  (i.e., shar-ing most terms), which represent 5% of the collection. In Twitter, near duplicates are usually retweets (copies of the original tweet, possibly with some minor addition or change) or the result of posting some online content on Twitter (the user clicks the  X  X ost in Twitter X  button that most online media offer). Virtually, every topic detection strategy will cluster those near-duplicates together, and that makes more difficult to estimate the real differences between systems.
Our final dataset comprises a total of 100,869 tweets an-notated with 8,765 different topics. In average, this corre-sponds to 544 (1,109) tweets and 57 (87) topics for training (testing) per entity.

Before computing the features, tweets were normalized by removing punctuation, lowercasing, tokenizing by whites-paces and removing stopwords and words with less than three characters.
Before tackling the topic detection task, we analyze the effectiveness of different signals to learn a similarity func-tion. Given the small size of a tweet, our hypothesis is that Code and proposed system outputs for the RepLab 2013 Topic Detection Task are pub-licly available at http://damiano.github.io/ learning-similarity-functions-ORM/
Note that training and test are different and disjoint sets for every entity in the collection. Twitter-specific signals should help building better similar-ity functions.

We start by building a pairwise classification model using linear kernel SVM 4 [20] We randomly sample 80,000 pairs of tweets from the RepLab 2013 training dataset, keeping the true and false classes balanced. We run a 10-fold cross-validation on this sample. Table 2 reports results in terms of averaged accuracy (which is a suitable measure as classes are balanced) for different feature combinations.
We use the Student X  X  t-test to evaluate the significance of observed differences. We denote significant improvements with  X  and  X  X  X  ( p&lt; 0 . 05 and p&lt; 0 . 01, respectively).
The relative differences seen on SVM cannot be directly extrapolated to any Machine Learning algorithm. Therefore, we also compute Maximal Pairwise Accuracy (maxPWA) [ 5], which is a theoretical upper bound of the effectiveness of dif-ferent feature combinations, and computes the performance of an ideal Machine Learning algorithm that, for each clas-sification instance, only listens to the features that give the right information 5 .

Remarkably, the Pearson correlation between the accu-racy of the linear SVM and the theoretical upper bound maxPWA is 0.93. In other words, whenever a set of features gives useful additional information (as reflected in the the-oretical upper bound for any learning algorithm), SVM is able to profit in direct proportion to the amount of new use-ful signal available. Therefore, differences seen with SVM can be generalized to other algorithms.

An inspection of the results in Table 2 shows that: We tested other machine learning algorithms like Na  X   X ve Bayes and Decision Trees, obtaining lower absolute results but similar relative improvements; hence we report results for SVM only.
Given the quadratic cost of computing maxPWA X 0( n 2 ) for n pairs X  X e use a balanced sample of 8,000 pairs and report the averaged scores over 10 runs.

In summary, most signals in our study are able to improve the classification process with statistical significance over the use of term-based features only, and their combination gives the best performance. Although the absolute performance of the best learned function seems low (0.63 accuracy), we will see in the following sections that, once the classification confidence is used as similarity measure, it leads to the best topic detection performance reported on the RepLab dataset so far.
 We now turn to the experiments on the Topic Detection Task. We first compare the effect of considering different Twitter signals in our similarity function (  X  3.3), then we study the effect of the learning process (  X  3.4)withrespect to an unsupervised alternative, and finally we compare our results with the state-of-the-art (  X  3.5).
We have seen that a classification model that combines all the features is the most accurate. We now use the positive classification confidence score for a pair of tweets as estima-tion of the similarity between them, and feed the single-link HAC clustering algorithm with this similarity score to detect the topics in the test set, for each of the 61 entities included in the dataset.
 In order to answer one of our initial research questions, Can Twitter signals be used to improve entity-specific topic detection? , we compare the results of HAC using two learned similarity functions: a baseline using terms_jaccard as sig-nal, and our best function, which uses all features. 6 We report results using the official evaluation measures at the RepLab 2013 Topic Detection Task: Reliability &amp; Sensitiv-ity (R&amp;S) [4] and its balanced F-Measure (harmonic mean), F ( R, S ). Note that, in clustering tasks, R&amp;S are equiva-lenttothewell-knownBCubedPrecisionandRecallmea-sures [3 ].
 Figure 1 shows results as macro-averaged R&amp;S in the Rep-Lab 2013 test dataset. Reliability (y-axis), Sensitivity (x-axis) and F 1 ( R, S ) (dot size and numbers) are plotted. Note that F 1 ( R, S ) is not the harmonic mean of the average R&amp;S, but the average of the harmonic mean for each test case (the 61 entities in the test collection). Each dot in a curve repre-sents the output of the HAC algorithm at different similar-ity thresholds (in percentiles). A lower similarity threshold gives larger clusters, increasing Sensitivity (BCubed Recall) at the expense of Reliability (BCubed Precision).
If we compare using all features with term similarity only (SVM(all)+HAC versus SVM(term jaccard)+HAC), Figure 1 shows that they have the same maximal value ( F 1 ( R, S )=0 . 47), but using all features gives more Reli-abilityathighSensitivityscores. Inordertobetterquantify the differences between the systems, we report two mea-sures that summarize the difference of both curves in a single score: the Area Under the R&amp;S Curve (AUC) and the Mean Average Reliability (MAR), which is the counterpart of the standard IR evaluation measure MAP (Mean Average Pre-cision) for our curves. Table 3 reports both measures for the two systems. As previously, we denote significant improve-ments with  X  and  X  X  X  ( p&lt; 0 . 05 and p&lt; 0 . 01, respectively).
Note that we use the expression  X  X witter signals X  in a broad sense (signals that go beyond terms in the tweet), and therefore we also consider semantic features which are not, strictly-speaking, Twitter-specific signals. bound (maxPWA) for different signal combinations. Table 3: Topic Detection: Using all signals versus term co-occurrence, comparison of R&amp;S curves with Area Under the Curve and Mean Average Reliabil-ity.

In terms of Mean Average Reliability, using all features improves over term co-occurrence with statistical signifi-cance (3% relative improvement). In terms of AUC, there is a 2% relative improvement but the difference is not statis-tically significant. Overall, our results suggest that the use of Twitter signals can improve the topic detection process, although the difference is not dramatic.
Our second research question was: Can previously anno-tated material be used to learn better topic detection mod-els? . Although many clustering problems are unsupervised in nature, supervision in reputation monitoring makes sense: clients are monitored daily, and what has been seen before is annotated and has an effect on how fresh information is processed. Can we profit from such annotations? The case of the RepLab dataset is challenging, because tweets in the training and test sets are separated by up to six months X  depending on the entity X  X nd the issues about an entity can change dramatically in Twitter in a period of six months.
We investigate this question by comparing two approaches that use the same signal (term co-occurrence as measured by the Jaccard formula): an unsupervised system, which uses directly the Jaccard measure between two tweets as similarity measure; and a supervised system, that uses our learned similarity function using the Jaccard measure as the only feature for the classifier. In both cases, we feed the HAC algorithm with each of the similarity measures. Figure 1 includes both curves (terms jaccard+HAC and SVM(terms jaccard)+HAC), and shows that there is a sub-stantial difference between them. The supervised system consistently improves the performance of the unsupervised version regardless of how we set the similarity threshold. Table 4: Supervised versus Unsupervised Topic De-tection.

Table 4 compares the supervised and unsupervised ap-proaches in terms of AUC and MAR. The supervised system outperforms its unsupervised counterpart with a 2% relative improvement in terms of MAR, which is statistically signif-icant. The difference in terms of of AUC is larger (5%), but is not statistically significant.

Overall, our results indicate that previous annotations can be used to learn better topic models, although differences are not large in our experimental setting. Probably if the time gap between tweets in the training and test sets were smaller (for instance, days instead of months), the effect of learning would be higher.
The differences we have detected could be irrelevant or misleading if both our baseline and contrastive systems were below state-of-the-art results. Therefore, we compare our approach with two competitive systems from RepLab 2013: system to best RepLab system (p &lt; 0 . 01).
Figure 1 compares all the systems. Numbers with  X  X  X  in-dicate statistical improvements ( p&lt; 0 . 01) of our best system (SVM(all features)+HAC), at different similarity thresholds, with respect to the best RepLab system. 7
Note that our approach significantly outperforms both the best RepLab system and the T.Twitter-LDA approach, for any reasonable threshold. Note also that a direct application of the HAC algorithm using Jaccard as similarity metric also performs better than the two RepLab systems, which seems to confirm that a standard clustering algorithm may be more robust when there is data sparsity, as is the case of reputation monitoring.

If we compare with inter-annotator agreement, our best system (with F 1 ( R, S )=0 . 47) gets very close to the re-ported annotator agreement on the dataset, which is 0.48, measured as the F 1 score of one annotator vs other [2]. Inter-annotator agreement is low, but this is not surprising for a clustering task, even if annotators are reputation experts. But it may be unrealistic to look for improvements in F 1 beyond what we have reached with our learned similarity measures. It is probably more practical to do failure analy-sis and study where the challenges of the task lie and what
Note that R , S and F 1 ( R, S ) for the two RepLab systems reported are different than the official scores [ 2], because we are excluding unrelated tweets from our evaluation, and we are excluding also near-duplicates (as described in 3.1). Nevertheless, all systems benefit similarly from the normal-ization and it does not produce any change in the official ranking is the performance of our systems on a case-per-case basis. This is what we do in the next section.
So far, we have only investigated average results of our systems across the 61 entities in the RepLab dataset. Here we perform a more detailed analysis of results.

Surprisingly X  X iven the substantial differences between the entities in the dataset X  X he standard deviation of our best system is low in terms of both R , S and F 1 ( R, S )(lessthan 0.09 in all cases). In particular, the F 1 values of our system trained with all features have a standard deviation of 0.06, whichcompareswellwithrespecttothebestRepLab2013 system (which has a standard deviation of 0.1). Apparently, our system not only performs better in average, but is also more robust across test cases.

In terms of the effect of combining signals, we have seen than taking into account all signals has a slight X  X ut statisti-cally significant X  X mprovement with respect to term match-ing. If we look case by case, there are only five entities (8% of the whole set) where the average Reliability of using all signals is lower X  X y a difference of at most 0.02 X  X han using term co-occurrence only: Capital One , Shakira , PSY , Banco Santander and BBVA . In most cases, for these entities there are large topics that are easy to identify by co-occurrence. For instance, BBVA has a topic Sports sponsor in which the annotator has grouped all mentions to BBVA sports spon-soring activities. The topic covers 52% of the target tweets, and can be identified with a few keywords that have high precision and high recall and refers to the name of the Span-ish Soccer League. Likewise, the entity Shakira contains a topic Charity , with 92 tweets, that refers to the Barefoot Foundation and can be detected by the keyword support or the hashtag #BuyABrick .

Finally, we have manually inspected hard topics  X  X hose where our system either fails to cluster, leaving most tweets in single clusters, or creates just a few big noisy clusters X  and easy topics  X  X hose that are accurately solved by any of the similarity combinations tested in our experiments.
Remarkably, we found that hard topics seem to be gen-eral, organizational topics that are used by the reputation manager to organize the information in an abstract man-ner. Some examples are  X  X oncern of Customers X  ,  X  X ad Ser-vice X  ,and  X  X ate -Opinions X  for the banking domain,  X  X ans Tweeting X  in the music domain or  X  X ooking Forward to Own aCar X  ,  X  X egative Opinion of an Owner X  in the automotive domain. In these cases, the content overlap between tweets can be low; for instance, customers complain about the ser-vice of their bank in many different ways.

On the other hand, topics easy to find are fine-grained and either refers to specific events X   X  X an Arrested for Racial Abuse during Capital One Cup Game X  ,  X  X isco Hires Bar-clays to Sell Linksys X  ,  X  X arclays Fires or Disciples Staff for LS X  ,  X  X alls to Condemn Uganda X  X  Politics X  ,  X  X atar Selling Warrants X  ,  X  X ave Matthews Band at Wells Fargo Center X   X  or talk about a singular dimension of the entity X   X  X exus Owners Club X  ,  X  X tock Analysis X  ,  X  X xchange Rates X  .Ingen-eral, the vocabulary used in event-like topics tends to be more specific than in organizational topics such as  X  X ronic Comments of Costumers X  , reducing the difficulty to identify topical relations.

The nature of hard and easy topics is, therefore, quite different. From the point of view of reputation monitoring, the second type of topics is probably more relevant, as it is where reputation alerts tend to be. Hard topics, on the other hand, seem more like a way of categorizing tweets that do not belong to any significant trending topic, and they are more likely to be used differently by different annotators; perhaps the inter-annotator agreement in the dataset would be higher if we only look at event-like topics. In any case, it is probably useful to make this distinction explicit both when creating test collections and when reporting results for the task.
We first overview the related work on topic and event de-tection in Twitter; then we summarize the application of topic models to this task, and we finish discussing the state-of-the-art of topic detection for Online Reputation Monitor-ing.
 Topic Detection and Tracking (TDT) in texts has been widely studied as event-based organization of newswire stories [1]. In the last years, topic and event detection in Twitter has also become a very active research area [32, 29]. Co-occurrence bursts and temporal signals have been adopted for detecting topics in both the blogosphere [30 , 16, 37] and the Twit-tersphere [25, 8, 37]. Platakis et al.[30] apply Kleinberg X  X  probabilistic automata method [21] to blogs burst model-ing and extracting structure from a text stream. Math-ioudakis &amp; Koudas [ 25] group bursty keywords into related groups based on their co-occurrences and Benhardus and Kalita [8] outlines methodologies for using streaming data, e.g., analysis of tf.idf term weighting and normalized term frequency to identify trending topics. Weng et al. [ 37]and Chen &amp; Roy [ 11] detect events by grouping a set of signals (words) with similar patterns of bursts using a modularity-based graph partitioning method. Becker et al.[ 7] analyzed the effectiveness of combining meta-data information (tags, time, location, etc.) to textual data for clustering of social media documents (e.g., Flickr images) according to previ-ously unknown real-life events. As in our case, their results indicated that meta-data was helpful in their Flickr event detection scenario. Unlike the scenarios tackled in previous work, in our setting (i) we are looking at entity-specific top-ics, which causes data sparsity and (ii) we need to assign eachdocument/tweettoatopic.
 Topic Models. Recently,topicsmodelssuchasLDA[ 10] and PLSA [17] have been adapted to the context of Twitter. The general assumption is that each author has a certain distribution of topics, while each tweet is associated only to one topic [38, 31]. Hong &amp; Davison [18 ] conducted experi-ments on the quality of topics derived from different aggre-gation strategies. They concluded that topic models learned from messages posted by the same user may lead to supe-rior performance in classification problems. The common characteristic of all previous work is that there are contexts where there is enough information and redundancy to detect temporal bursts of term frequencies. However, in the ORM scenario, the user is interested in a particular entity, that is, only in a tiny subset of the Twitter stream. Detecting topics of a given entity of interest in Twitter roughly corresponds to the long tail in Web search scenarios. This lead to data sparsity, which is a bottle-neck for topic models [ 27]. our knowledge, RepLab 2013 [ 2] is the largest Twitter col-lection for reputation monitoring, and provides the most reli-able test collection for the Topic Detection task. Besides the two systems described in detail in  X  3.5, RepLab participa-tion included both supervised and unsupervised techniques. On one hand, different clustering techniques such as HAC, VOS clustering [9] X  X  community detection algorithm X  X nd K-star [33] were used by the participants. The most common similarity functions are cosine [9] and Jaccard similarity [ 33] over terms. Similar to ours, the term clustering approached presented by UNED [ 34] uses a supervised learned similarity over Twitter signals (authors, URLs, timestamps and hash-tags). However, it computes similarities between terms X  X n order to detect keywords associated to clusters X  X ather than between tweets.

On the other hand, LIA [ 14] and UAMCLYR [33]tack-led the Topic Detection task as a multi-class classification problem. LIA [14] used Maximum A Posteriori probability of the most pure headwords of the topics in the training set to assign the tweets in the test set. UAMCLYR [33]used standard multi-class classification techniques, such as Naive Bayes and Sequential Minimal Optimization Support Vector Machines (SMO SVM).

Overall, the results of official RepLab systems were the first set of experiments on the RepLab 2013 dataset. As we have seen in our experiments, a HAC algorithm over term similarity outperforms all the RepLab systems: this is another evidence that corroborates the issue of data sparsity in our Online Reputation Monitoring problem.
Apart from the RepLab Topic Detection Task, Chen et al. [12] have recently studied the problem of discovering hot topics about an organization in Twitter. The problem tack-led here is slightly different to our scenario: instead of clus-tering all the tweets related to an entity of interest, they are only interested in detecting the hot emerging topics from an initial clustering generated by cosine similarity. Their ground truth does not include clustering relationships be-tween tweets. Instead of this, they align topics with online news and they manually evaluate the aggregated output of different hot topic detection methods to create the ground truth deciding whether a topic is emerging or not.
Online Reputation Management can be seen as the  X  X ong tail X  of topic detection in Twitter: except for a few excep-tions, the volume of information related to a specific orga-nization/company at a given time is orders of magnitude smaller than Twitter trending topics, and this data sparsity makes the problem much more challenging than analyzing Twitter trends.
 In this context, our experimental results indicate that (i) Twitter information (authors, timestamps, etc.) can indeed be used to improve topic detection with respect to the use of textual content only. (ii) It is possible to learn effec-tively from manually annotated topics, using them to im-prove the estimation of pairwise tweet similarity. (iii) A conventional clustering algorithm (HAC) using our learned similarity functions performs substantially better than state-of-the-art approaches X  X ncluding Temporal Twitter-LDA X  on the same test collection, and gets close to the inter-annotator agreement rate. Our results seem to confirm that, when data is sparse as in our reputation monitoring scenario, conventional clustering X  X oupled with an effective similarity function X  X an be more effective than using generative mod-els such as Temporal Twitter-LDA.

A detailed qualitative analysis of our results has revealed that there is a special type of topics in the manual data which are harder to detect automatically. These are orga-nizational topics which, rather than grouping tweets about a specific issue or event, have a more taxonomical or struc-tural nature: for instance, a reputation expert may group together all tweets which are hate opinions about a bank. Organizational topics tend to be stable across time, and have a wider vocabulary entropy. In contrast, reputation alerts, which are the key issues from a monitoring perspec-tive (e.g., director of the bank accused of evading taxes )tend to be spikes in a certain period of time. Organizational topics are not only the main challenge for topic detection systems, but they may also explain the low inter-annotator agreement rates even when, as in the case of the dataset used in our experiments, manual annotations are performed by trained experts. It would be useful, in future test collec-tions, to make this distinction explicit both when creating test collections and when reporting results for the task. This research was partially supported by the European Community X  X  FP7 Programme under grant agreement nr. 288024 (LiMoSINe), the Spanish Ministry of Education (FPU grant nr. AP2009-0507), the Spanish Ministry of Science and Innovation (Holopedia Project, TIN2010-21128-C02), the UNED (project nr. 2012V/PUNED/0004), the Regional Government of Madrid and the ESF under MA2VICMR (S2009/TIC-1542) and a Google Faculty Research Award (Axiometrics Project). [1] J. Allan. Introduction to Topic Detection and [2] E. Amig  X o, J. Carrillo de Albornoz, I. Chugur, [3] E. Amig  X o, J. Gonzalo, J. Artiles, and F. Verdejo. A [4] E. Amig  X o, J. Gonzalo, and F. Verdejo. A General [5] J. Artiles, E. Amig  X  o, and J. Gonzalo. The role of [6] J. Artiles, J. Gonzalo, and S. Sekine. WePS 2 [7] H. Becker, M. Naaman, and L. Gravano. Learning [8] J. Benhardus and J. Kalita. Streaming Trend [9] J. L. A. Berrocal, C. G. Figuerola, and [10] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [11] L. Chen and A. Roy. Event Detection from Flickr [12] Y. Chen, H. Amiri, Z. Li, and T.-S. Chua. Emerging [13] Y. Chen, S. Y. M. Lee, and C.-R. Huang. PolyUHK: [14] J.-V. Cossu, B. Bigot, L. Bonnefoy, M. Morchid, [15] N. Glance, M. Hurst, K. Nigam, M. Siegler, [16] D. Gruhl, R. Guha, D. Liben-Nowell, and A. Tomkins. [17] T. Hofmann. Probabilistic latent semantic indexing. In [18] L. Hong and B. Davison. Empirical study of topic [19] B. Jansen, M. Zhang, K. Sobel, and A. Chowdury. [20] T. Joachims. Text categorization with support vector [21] J. Kleinberg. Bursty and Hierarchical Structure in [22] D. Lin. An information-theoretic definition of [23] G. S. Mann. Multi-document Statistical Fact [24] C. D. Manning, P. Raghavan, and H. Sch  X  utze. [25] M. Mathioudakis and N. Koudas. TwitterMonitor: [26] E. Meij, W. Weerkamp, and M. de Rijke. Adding [27] S. Moghaddam and M. Ester. On the Design of LDA [28] R. Nuray-Turan, Z. Chen, D. V. Kalashnikov, and [29] S. Petrovi  X c, M. Osborne, and V. Lavrenko. Streaming [30] M. Platakis, D. Kotsakos, and D. Gunopulos.
 [31] D. Ramage, S. Dumais, and D. Liebling.
 [32] T. Sakaki, M. Okazaki, and Y. Matsuo. Earthquake [33] C. S  X  anchez-S  X  anchez,H.Jim  X  enez-Salazar, and W. A. [34] D. Spina, J. Carrillo de Albornoz, T. Mart  X   X n, [35] D. Spina, E. Meij, M. de Rijke, A. Oghina, M. T. Bui, [36] X. Wang and A. McCallum. Topics over Time: A [37] J. Weng, Y. Yao, E. Leonardi, and F. Lee. Event [38] W. X. Zhao, J. Jiang, J. He, Y. Song, [39] W. X. Zhao, J. Jiang, J. Weng, J. He, E.-P. Lim,
