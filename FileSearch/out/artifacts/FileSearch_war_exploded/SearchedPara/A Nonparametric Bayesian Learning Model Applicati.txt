 Categorization is an important problem in the case of natural language texts and images processing due to the increased generation of digital documents (images or text). Learning techniques are frequently used for this task by building and training classifiers from a set of preclassified examples. Finite mixture models are one of the most used learning techniques and have several features that make them attractive for categorization and clustering problems [1]. One of the most challenging aspects, when using finite mixture models, is usually to estimate the number of clusters which best describes the data without over-or under-fitting it. For this purpose, many approaches have been suggested [2].

In this paper, we are interested in Bayesian nonparametric approaches for modeling and selection using mixture of Dirichlet processes [3] which has been shown to be a powerful alternative to select the number of clusters [4,5,6]. In con-trast with classic Bayesian approaches which suppose an unknown finite number of mixture components, nonparametric Bayesian approaches assume infinitely complex models (i.e an infinite number of components). Indeed, nonparametric Bayesian approaches allow the increasing of the number of mixture components to infinity, which removes the problems underlying the selection of the number of clusters which can increase or decr ease as new data arrive. Because of their simplicity and thanks to the development of MCMC techniques, infinite mixture models based on Dirichlet processes are now widely used in different domains and variety of applications. The majority of the work with infinite mixture mod-els makes the Gaussian assumption. How ever, we have shown in previous works that other distributions such as the Dirichlet [7] and the generalized Dirichlet [8,9,10] can give better results in some applications and are more appropriate especially when modeling proportion vect ors. We propose then a Bayesian hier-archical infinite mixture model based on generalized Dirichlet distributions for text and image categorization.

The remainder of this paper is structur ed as follows. In section II, we present the formulation of the infinite generalized Dirichlet mixture by specifying the priors and the conditional posterior distributions. Section III gives the complete Gibbs sampling algorithm. Experimental results are illustrated in Section IV. Finally, Section V concludes the paper. 2.1 The Infinite Generalized Dirichlet Mixture Model Let X =( X 1 ,..., X N ) be a set of independent vectors representing N texts or images. By considering a Dirichlet process X can be modeled using a set of latent parameters {  X  1 ,..., X  N } where each X i has distribution F (  X  i )andeach  X  is drawn independently and identically from a mixing distribution G on which a Dirichlet process prior is placed: where G 0 and  X  define a baseline distribution for the Dirichlet process prior and the concentration parameter , respectively. The Dirichl et process mixture can be viewed as the limit of a finite mixture model with a uniform Dirichlet prior (i.e a Dirichlet with hyperparameters  X /M ,where M is the number of clusters) [11]. Let Z i be an integer denoting the unknown component from which X i is drawn. The unobserved (or missing) vector Z =( Z 1 ,...,Z N ) is generally called the  X  X embership vector X  of the mixture model and the joint model over X , Z is given by p ( X ,Z )= p ( X| Z ) p ( Z )where and  X  is the set of all the parameters defining the mixture model. In addition, by taking the Dirichlet prior, we can easily show that [11] where n j is the number of vectors previously affected to cluster j .Bytakingthe limit of the previous equation, as M  X  X  X  , we can show that Where R and U are the sets of represented and unrepresented clusters, respec-tively. Note that despite the fact that the number of clusters is supposed to be infinite, the number of represented (i.e nonempty) clusters is finite and should be between 1 and N . From Eq. 5, we can note also that if a cluster is repre-sented, its conditional prior will depend on the number of observations assigned to it. Conditional prior for unrepresented clusters depends on  X  and N . Indeed, according to Eq. 5 a new cluster may appear with a probability  X  N  X  1+  X  .Thus, the average number of clusters M is given by [12,13] N i =1  X   X  + i  X  1  X  X  (  X  log N ). which shows that the number of clusters increases only in a logarithmic manner in the number of observations.

The Dirichlet process mixture approach for clustering is based on the MCMC technique of Gibbs sampling [6] by generating the assignments of vectors accord-ing to the posterior distribution where Z  X  i represents all the vectors assignments except Z i and X  X  i represents all the vectors except X i . An important problem now is the choice of the distri-butions p ( X i |  X  j ) representing the different components in our mixture model. The majority of the work with infinite mixture models makes the Gaussian as-sumption. In this paper, however, we introduce an infinite mixture model based on generalized Dirichlet distributions which have been shown to be able to give better results than the Gaussian [8,9,10]. 2.2 Generalized Dirichlet Distribution If the random vector X i =( X i 1 ,...,X id ) follows a generalized Dirichlet distri-bution, the joint density function is given by [8]: l =1 ,...,d  X  1and  X  l =  X  l  X  1. Note that the generalized Dirichlet distribution is reduced to a Dirichlet distribution [7] when  X  l =  X  l +1 +  X  l +1 . Let us introduce an interesting property of the generalized Dirichlet distribution previously used in [8] and that we shall exploit in this paper. Indeed, if a vector X i has a generalized Dirichlet distribution with parameters (  X  1 , X  1 ,..., X  d , X  d ), then we can construct a vector Y i =( Y i 1 ,...,Y id ) using the following geometric transformation Y il = T ( X il ): In this vector Y i ,each Y il , l =1 ,...,d has a Beta distribution with parameters  X  l and  X  l and then a given generalized Dirich let distribution associated with a given cluster j canbedefinedasfollows respectively. The symbol  X  j =( s j , m j ) refers to the entire set of parameters to be estimated, where s j =( s j 1 ,...,s jd )and m j =( m j 1 ,...,m jd ). In the following subsections, we focus on the development of the priors and posteriors of our infinite generalized Dirichlet mixture model. 2.3 Priors and Conditional Posteriors We know that each location m jl is defined in the compact support [0,1], then an appealing flexible choice as a prior is the Beta distribution, with location  X  and scale  X  common to all components, which was found flexible in real applications. Thus, m j for each cluster is given the following prior: where m j =( m j 1 ,...,m jd ). The conditional posterior distribution for m j is obtained by multiplying the prior p ( m j |  X , X  ) by the likelihood conditioned on Z : where n j = N i =1 I Z i = j and represents the number of vectors belonging to cluster j . The hyperparameters,  X  and  X  , associated with the m jl are given uniform (we have started by testing a Beta prior for  X  , and the best experimental results were obtained with location equal to 1 and scale fixed to 2, which corresponds actually to a uniform distribution) and inverse Gamma priors, respectively: Note that although inverse Gamma priors have been used in many applications as priors, they can cause serious problems if the scale is too close to zero [14]. However, we did not observe this problem in our case. Besides, we made this specific choice because as a lighter-taile d distribution, the inverse Gamma pre-vents to place too much mass on large scale values [15]. For the hyperparameters  X  and  X  , Eq. 10 is considered as the likelihood which gives with Eqs. 12 and 13 the following: Since the scale s jl control the dispersion of the distributions, a common choice as a prior is an inverse gamma with shape  X  and scale common to all components [15], then Having this prior, the conditional posterior for s j is The hyperparameters,  X  and , associated with the s jl are given inverse Gamma and exponential priors, respectively: For the hyperparameters  X  and , Eq. 16 is considered as the likelihood which gives with Eqs. 18 and 19 the following: The choice of  X  is crucial in our model. In fact, the number of clusters is directly related to  X  which controls the generation frequency of new clusters. Then, we have chosen an inverse gamma prior for the concentration parameter  X  which gives with Eq. 5 the following posterior (for more details, see [11]) p (  X  | M, N ;  X ,  X  )  X  Having all the posteriors in hand, we can employ a Gibbs sampler and each iteration will be based on the following steps:  X  Generate Z i from Eq. 6, i =1 ,...,N .  X  Update the number of represented components M .  X  X pdate n j and P j = n j N +  X  , j =1 ,...,M .  X  Update the mixing parameters of unrepresented components P U =  X   X  + N .  X  Generate m j from Eq. 11 and s j from Eq. 17, j =1 ,...,M .  X  Update the hyperparameters: Generate  X  from Eq. 14,  X  from Eq. 15,  X  from Note that in the initialization step, the algorithm started by assuming that all the vectors are in the same cluster and the initial parameters are generated as random samples from their prior distribution. The distributions given by Eqs. 14, 15, 20, 21 and 23 are not of standard forms. However, it is possible to show that they are log-concave (i.e it is straightforward to show that the second derivatives of the logarithms of these functions are negative), then the samples generation is based on the adaptive rejection sampling (ARS) [16] to obtain the values of  X  ,  X  ,  X  , and  X  . The sampling of the vectors Z i requires the evaluation of the integral in Eq. 6 which is not analytically tractable. Thus, we have used an approach, originally proposed by Ne al [6] and used with success by Rasmussen [11] in the case of infinite Gaussian mixtures, which consists on approximating this integral by generating a Monte Carlo estimate by sampling from the priors of m j and s . More details about this sampling method are given in [6]. The sampling of m j and s j is more complex, since the posteriors given by Eqs. 11 and 17 do not have known forms. Thus, we have used the Metropolis-Hastings algorithm (M-H) [17].
 In this section, we describe some experiments to compare our infinite mix-ture model to the finite model that we previously proposed. In particular, we compare the proposed approach in this paper, for estimation and selection, with the approach in [8] and the Bayesian approach in [10]. Besides, we com-pare the performances of infinite generalized Dirichlet and infinite Gaussian mixtures. In these applications our specific choice for the hyperparameters is 4.1 Text Categorization In the first experiment, we test our mod el for the classification of three well-known data sets consisting of a set of documents and which were used, for instance, in [18]: the industry sector, t he Mod Apte split of the Reuters-21570 document collection, and the 20 newsgroups data sets. The industry sector data set 1 has a vocabulary of 55,055 words and is composed of 104 classes containing 9555 documents having an average of 606 words. The first half of this data set is used for training and the second one for testing. The Mod Apte split data set [19] is a subset of the well known corpus Reuters-21578 2 , has a vocabulary of 15,996 words and is composed of 90 classes containing 7,770 training documents and 3,019 test documents. The documents in this data set have an average of 70 words and are multi-labeled. The 20 newsgroups data set 3 contains 18,828 documents grouped in 20 classes (80% of the documents is used for training and the rest for testing) with a vocabulary of 61,298 words and average document length of 116 words.

The first step in our experiment was removing all stop and rare words from the vocabularies associated with each data set. After this first pre-processing, each document was represented by a vect or of counts. The dimensionality of these vectors were then reduced using L DA [20]. As a result, each document was represented by a vector of proportions. The proportions vectors in the different training sets were then modeled by infinite generalized Dirichlet mixtures, us-ing the algorithm in the previous section, and each test vector was affected to the class that gives the highest likelihood. Following [18], the performance of our model for the industry sector and newsgroups data sets was based on the precision measure given by where TP and FP are the numbers of true positives and false positives, respec-tively. Table 1 shows the classification results, averaged over 20 random selection of the training and test sets, for the industry sector and 20 newsgroups data sets. The infinite model produces better results than the finite one estimated either using the HSEM or the Bayesian methods. For the Mod Apte split data set, however, the precision alone is not a suffi cient measure, since the data is multi-labeled, then we have also consider ed the recall measure given by [18] where FN is the number of false negatives. The precision and the recall are then combined using the break-even point d efined by the micro or macro averaging given by [18,21] where M is the number of document classes, N is the total number of documents and n j the number of documents in class j . Table 2 shows the classification results for the Mod Apte split data set 4 . 4.2 Image Databases Categorization Using Bag-of-Visual Words Images categorization can be based on global and/or local features and is an important step for different problems such as image recommendation [22]. Re-cently, local feature-based approaches h ave shown excellent results. One of these methods, that we will follow in our experiments, is the bag of keypoints approach [23] originated from an analogy to learning methods in the case of text catego-rization. After detecting keypoints (or local interest points) in the images using one the various existing detectors [24], an important step in this approach is the extraction of local descriptors that should be invariant to images transfor-mations, occlusions and lighting variations [23]. keypoints are then grouped into a number of homogenous clusters V , using a clustering o r vector quantization algorithm such as K-means, according to the similarity of their descriptors. Each cluster center is then treated as a visual word, and we obtain a vocabulary of V visual words describing all possible local image patterns. Having this vocabulary in hand, each image can be represented as a V -dimensional vector containing the proportion of each visual word in that image. The resulted vector can be used then for the categorization task.

The performance of our infinite mixture model was evaluated on a database containing 13 categories of natural scenes [25,26]: highway (260 images), inside of cities (308 images), tall buildings (356 images), streets (292 images), sub-urb residence (241 images), forest (328 images), coast (360 images), mountain (374 images), open country (410 images), bedroom (174 images), kitchen (151 images), livingroom (289 images), and office (216 images). Figure 1 shows ex-amples of these images which have an average size of approximately 250  X  300 pixels. The keypoints were detected us ing the Harris affine detector [24]. Then, we have used Scale Invariant Feature Transform (SIFT), which performs bet-ter than the majority of the existing d escriptors [27], c omputed on detected keypoints of all images and giving 128-dimensional vector for each keypoint. Moreover, SIFT vectors, extracted from 650 training images from all categories (50 images from each category), were clustered using the K-Means algorithm providing 150 visual-words. Each image in the database was then represented by a 150-dimensional vector of proportions.

Our categorization approach is based on a classifier. The inputs to the classifier are the 150-dimensional vectors extracted from the different database classes. These vectors are separated into the unknown or test set of vectors, whose class is unknown, and the training set of vectors (we take randomly 100 vectors for training from each class), whose class is known. The training set is necessary to adapt the classifier to each possible class before the unknown set is submitted to the classifier. Then, we apply our algorithm, presented in Section 3, to the training vectors in each class. After this stage, each class in the database is represented by a generalized Dirichlet mixture. Finally, in the classification stage each unknown image is assigned to the class increasing more its loglikelihood.
Table 3 shows the loglikelihoods of the training data in the different classes when using the model in this paper, the finite mixture model proposed in [8] and the Bayesian approach described in [10]. In the reported results, the val-ues of the loglikelihoods are divided by the number of vectors in each training class. The table shows the clear dominance of the infinite model. The results can be explained by the fact that the infinite model outperforms by its ability to incorporate uncertainties related to the selection of the correct number of clus-ters. We determined the average confusio n matrices reported by the generalized Dirichlet and the Gaussian mixtures by running the estimation algorithms 10 times with varying random selection of the training and test sets. The average classification accuracies were 74.21% (634 misclassified images) and 63.68% (893 misclassified images) when we used generalized Dirichlet and Gaussian mixtures, respectively. The fact that the generali zed Dirichlet outperforms the Gaussian is actually an expected result, since it is well known that the Dirichlet distribu-tion is an excellent choice to model normalized histograms [2,7]. According to the confusion matrices, an important part of the misclassification errors occurs among the categories: bedroom, livingroom, kitchen and office which is the same conclusion reached in [25]. We have described and illustrated a Bayesian nonparametric approach based on infinite generalized Dirichlet mixtures. We have shown that the problem of determining the number of clusters can be avoided by using infinite mixtures which model the structure of the data well. Indeed, the resulting optimal clus-tering is obtained by averaging over all n umber of clusters of different possible models. The inference of the infinite gene ralized Dirichlet mixture was imple-mented through Markov chain Monte Car lo methods. The experimental results have shown that our infinite model offer excellent modeling capabilities. The completion of this research was made possible thanks to the Natural Sci-ences and Engineering Research Counc il of Canada (NSERC), and a NATEQ Nouveaux Chercheurs Grant.

