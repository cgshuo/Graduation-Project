 Alternating least squares (ALS) is a powerful matrix factor-ization (MF) algorithm for both explicit and implicit feed-back based recommender systems. As shown in many arti-cles, increasing the number of latent factors (denoted by K ) boosts the prediction accuracy of MF based recommender systems, including ALS as well. The price of the better ac-curacy is paid by the increased running time: the running time of the original version of ALS is proportional to K 3 . Yet, the running time of model building can be important in recommendation systems; if the model cannot keep up with the changing item portfolio and/or user profile, the predic-tion accuracy can be degraded.

In this paper we present novel and fast ALS variants both for the implicit and explicit feedback datasets, which offers better trade-off between running time and accuracy. Due to the significantly lower computational complexity of the algorithm X  X inear in terms of K  X  X he model being gener-ated under the same amount of time is more accurate, since the faster training enables to build model with more latent factors. We demonstrate the efficiency of our ALS variants on two datasets using two performance measures, RMSE and average relative position (ARP), and show that either a significantly more accurate model can be generated under the same amount of time or a model with similar predic-tion accuracy can be created faster; for explicit feedback the speed-up factor can be even 5 X 10.
 I.2.6 [ Artificial Intelligence ]: Learning X  parameter learn-ing D. Tikk is currently on leave from Dept. of Telecom. and Media Informatics, Budapest University of Technology and Economics, Magyar Tud  X osok krt. 2., Budapest, Hungary those ones that are simple, accurate enough, and can scale up well to very large dataset 1 .

The faster training time of an algorithm, allowing for var-ious running time X  X ccuracy trade-off scenarios, can be ex-ploited in commercial applications in several ways. First, services with typically short item lifetime X  X uch as news feeds, auction portals, non-repetitive cultural events X  X equire frequent periodical retraining of the recommendation model otherwise the model becomes outdated and inaccurate. The incremental update of the recommendation model with in-teractions on new items offers only a temporal solution, be-cause this technique does not handle item-item relations. Similarly, but in much less extent, the user profiles can also be eroded with time. A faster algorithm enables more frequent retraining and constantly higher recommendation quality at the same hardware cost. Second, faster training time enables to build a better recommendation model un-der the same amount of time. This can be either achieved by building a more accurate recommendation model with a larger parameter set (for example the number of latent factors can be increased at latent factor models), or run-ning the same training setting for more epochs. Third, the saved computational overhead obtained by replacing a slower method can be devoted for other purposes.

This paper is organized as follows. Section 2 introduces alternating least squares as a matrix factorization based recommender algorithm for both explicit and implicit feed-back datasets, describes the computational complexity of the naive implementation and the one using the Sherman X  Morrison formula (SMF). Section 3 presents our fast ALS variants: an efficient approximate ridge regression imple-mentation and its application for ALS based recommender algorithms. We report on and analyze the results obtained with our proposed algorithms in Section 4. A brief review of related work is presented in Section 5. We conclude the paper and give hints on future work in 6.
We use the following notation in the paper. N and M denote the number of users and items, resp. We use u  X  dices for items. The rating of user u on item i is r ui , and its prediction is  X  r ui . All r ui ratings are arranged in R ; R denotes the set of ( u, i ) indexes of R where (a) a rating is provided (for explicit feedback), (b) a positive feedback is provided (for implicit feedback). R  X  denotes each ( u, i ) pair of R , i.e. |R  X  | = N  X  M .

Matrix factorization approaches have been applied suc-cessfully for both rating-based and implicit feedback-based collaborative filtering (CF) problems [2, 3, 7, 9, 11, 12]. MF methods perform a so-called low rank matrix approximation: the matrix R is approximated as a product of two lower rank matrices: R  X  PQ T , where P  X  R N  X  K is the user feature matrix, Q  X  R M  X  K is the item feature matrix, K is the number of features that is a predefined constant, and the approximation is only performed at ( u, i )  X  X  positions.
The r ui element of R is approximated by http://www.newscientist.com/article/ dn17823-photo-finish-for-the-1m-movie-prediction-prize. html user: for the u -th user it takes the feature vector ( q i ) of items rated by the user as input variables, and the value of the ratings ( r ui ) as output variable, and finds the optimal p u by RR. Though, in the original paper a non-negative RR is performed, in this paper we relax this restriction and we allow for negative values in the solution.

Formally, let the matrix Q [ u ]  X  R n u  X  K denote the re-striction of Q to the items rated by user u , where n u = |{ i : ( u, i )  X  X }| , the vector r u  X  R n u  X  1 denote the ratings given by the u -th user to the corresponding items, and let where A u is the covariance matrix of input (considering user u in context) and d u is the input-output covariance vector. Then RR recomputes p u as
In the P -step a ridge regression is solved for each user, which is O ( NK 3 ) as noted in [7, 12]. Similarly, the Q -step requires O ( K 2 |R| + MK 3 ). According to [2, 7], the number of re-computations needed ranges between 10 and a  X  X ew tens X . It has been pointed out that larger K yields more accurate predictions [7, 12].
In the case of the implicit feedback, each user rates each item either positively (user u viewed item i ) or negatively (user u did not view item i ). This is in contrast with the ex-plicit case, where only a small subset of items are rated (but usually on a finer scale). Consequently, at implicit feedback the recommendation algorithm should handle a full rating matrix, in contrast to the explicit feedback case. Given that the sparsity of the explicit rating matrix is usually less than 1%, the difference in the size of the data is usually several orders of magnitude.

Hu et al. [7] proposed an elegant ALS variant for implicit feedback datasets, referred to as IALS. The key idea behind their approach is to assign a preference value r ui and a con-fidence level c ui to each element of R  X  . Preference typically takes values 0 and 1, indicating whether a user watched a particular item or not. The confidence level is related to how confident we are about user preferences. If she watched the item for long, or visited it two times, we can be more confident. Then the optimal model is defined as: ( P  X  , Q  X  ) = arg min where e ui = r ui  X  p T u q i is the prediction error. Note that R  X  contains all ( u, i ) pairs regardless of whether user u has watched item i or not. The authors use one restriction, namely if u has not watched i , then r ui = r 0 = 0 and c ui = c = 1 , where r 0 and c 0 are predefined constants, typically set to r 0 = 0 and c 0 = 1 . Similarly to the explicit feedback case, the authors used ALS to solve the problem, but with the following two differences: a somewhat larger K can easily catch up with the accuracy of the original ALS under smaller amount of time.
In this section we propose an approximate algorithm for optimizing (4). Let e i = y i  X  w T x i denote the error on the i -th example. The proposed algorithm works as follows: Let denote the k -th element of w by w k ( k = 1 , . . . , K ). Initialize w with 0 . Find the optimal solution only for w 1 , assuming the other elements of w are fixed. Then optimize w , assuming other coordinates ( w 1 , w 3 , w 4 , . . . , w K ) are fixed, and so on, in each step optimize only one element of w . The loop of the consecutive separate optimization of w -s are repeated until a termination condition is met.
The recomputation of w k is a univariate ridge regression, while the recomputation of the entire w is a multivariate ridge regression. To optimize only w k , we need to solve the following ridge regression problem: Here x il is the ( i, l )-th element of X . We refer to k as the active feature.

At implementation, we can set w k to 0 just before we optimize it. With this modification y i  X  becomes equal to e ui = y i  X  w T x i . However, when we change w k to w 0 k , we can easily update e i to e 0 i : In other words: after the optimization of w k  X  1 , but before the optimization of w k , y i  X  recomputed for each i in total in O ( n ) time.

Algorithm 1 describes formally our RR1 algorithm for the weighted ridge regression case.

A few notes to the algorithm: Line 1 initializes the in-ner variables. The outermost for -cycle repeatedly optimizes each element of w . Lines 5 X 6 sets w k to 0 and adjusts the e i user with all negative ratings. In this way, from the view-point of RR or RR1, rating these K items will be equivalent to rating all the M items negatively.

Since A 0 is a symmetric real matrix (and also positive semi-definite), its eigenvalue decomposition exists, where S  X  R K  X  K is the orthogonal matrix of the eigenvectors ( S T S = SS T = I ), and  X   X  R K  X  K is a diagonal matrix, containing the non-negative eigenvalues.

Let G = denote the j -th column of G T (here j = 1 , . . . , K ), that is the feature vector of the j -th aforementioned synthetic example. Note that if a user rates the g j examples with confidence level c j = 1 , the resulting covariance matrix, A 0 : =
Next we specify the ratings r j on these g j examples, such that d 0 is exactly reproduced. Let r  X  R K  X  1 denote the vector of r j values. Then, by definition, we have To make the left hand side equal to d 0 , we have to solve this system of linear equations for r as ( G T )  X  1 d 0 . In some rare cases G may be non-invertible. It can be shown that even in this case d 0 is in the image of matrix G (the proof is omitted here due to the lack of space). Summarizing the above construction in a nutshell: rating all items with c 0 confidence and r 0 as rating value is equivalent to rating the and r j as rating value.
 Now we describe how IALS1 uses the synthetic examples. To recompute p u , we assume that the user u rated the fol-lowing items: We apply RR1 for 1 cycle with these examples, p u is both the input and the output weight vector for RR1.

Regularization (usually) means that the diagonal of A is increased by a regularization factor  X  , which can be also user-dependent:  X  u . In the context of IALS1, regulariza-tion can be handled easily: assume that the regularization factor for user u is  X  u , i.e. we try to minimize the following cost function: When optimizing p u , we can create K synthetic regulariza-tion examples with rating value 0 and confidence level 1. Let the k -th such synthetic example be the k -th row of the user rates all these K examples with confidence level 1 Train. In our experiments we used our Probe10 set 2 that contains only a 1 / 10 subset of the Probe ratings ( 140 840 ratings), and put the rest of Probe into Train. Probe10 ap-proximates very well the withheld Quiz set [11], in the sense that predictors evaluated on both produce very similar re-sults; the difference is almost always less than 2 bps; 3 due to this Probe10 has been also used by others in the NP com-munity. Before experimentation, we subtracted the global mean, 3 . 6043 , from all ratings.
We applied a de-identified repetitive implicit feedback (RIF) dataset for implicit feedback experiments. 4 The database consists of 2 parts; the first one contains 9 617 414 implicit feedbacks from 215 630 users on 73 863 items created in a pe-riod of 182 days (the very same user can have several implicit feedbacks on the same item), the second one provides time information about availability of items. We partitioned the set of implicit feedbacks into a train set (181 days, 9 439 863 events) and a test set (1 day, 55 711 events) for the evalua-tion of IALS and IALS1. All experiments were performed on an Intel Core2 Quad Q9300 cpu on 2.5GHz, using only 1 core. We evaluated the ALS and the ALS1 algorithms on the Netflix Prize dataset, optimizing the following cost function: column of P and the second column of Q to be 1.
 We varied the number of features, K , and measured the RMSE on the test set after each epoch. Table 1 summarizes the results after 10 and 25 epochs. We also report the time needed to train the model. The RMSE of ALS and ALS1 are more similar after 25 than after 10 epochs: the largest difference is only 4 bps. However, ALS1 requires much less time for training: as K grows, the gap becomes larger. For the typical choice of K = 50 , the training with ALS1 for 25 epochs requires less than an hour, while ALS needs more than 7 hours. When K is 100, ALS1 is already 17 times faster. We were further able to run ALS1 with K = 1000 faster than ALS with K = 100 .
 From these tables we can conclude, that on the Netflix Prize dataset, ALS1 is about an order of magnitude faster than ALS at the usual K values, while offering almost the same RMSE. On the other hand if that small difference in RMSE matters, we can run ALS1 with somewhat more fac-tors to catch up with the accuracy of ALS.

Figure 1 shows the results measured in each epoch. We de-picted only the best RMSE results achieved with both ALS and ALS1 under the same amount of time, independently
A Perl script is available at our homepage, gravityrd.com , which selects the Probe10 from the original Netflix Probe set to ensure repeatability. 1 bps = 0 . 0001 RMSE
The terms of the agreement allows us to publish the de-identified dataset with the proprietary information removed, but the owner wanted to remain anonym.
 rithms on the RIF dataset. We optimized the following cost function: where c ui = 1 +  X   X  r ui [7],  X  = 200 ,  X  u =  X  i = 1000 .
We the used average relative position (ARP) as the eval-uation metric for this implicit feedback dataset, which is defined as follows.

For a given user u in the test set, the recommendation algorithm has to order all the recommendable items. This ordering can be based on the  X  r ui  X  R prediction values: items with highest  X  r ui are ranked first. For user u , the set of items are divided into two parts: relevant and non-relevant ones for the user. Assume that the recommendable items are indexed by i ranging from 1 to M . Let r ui  X  { 0 , 1 } denote whether an item is relevant to the user or not, e.g. user will view that item during the test period or not. Then the position for item i relevant to user u is defined as: The relative position is defined as: For example, if we have 103 items, only 3 of them (indexed by 1,2 and 3) are relevant for user u , and the ranking al-gorithm puts them at the 1st, 20th and 54th position, then the relative positions are rpos u 1 = 1 / 100, rpos u 2 = 20 / 100 and rpos u 3 = 54 / 100. This is a normalized measure. The average relative position is defined as: where the summation is taken over each positive implicit feedback of the test period. We remark that during training, each positive implicit feedback of the test set (i.e. r ui = 1 ) must be considered as it would be negative implicit feedback (i.e. r ui = 0 ) to avoid information leakage. We remark that a very similar measure was proposed in [7, 8], indeed ARP can be seen as the formal definition of the rank measure proposed in these works.

We experimented on the RIF dataset with various K val-ues to compare the performance of IALS and IALS1 algo-rithms.

Table 2 contains comparative results of the evaluation af-ter 10 and 20 epochs. With increasing K the difference between the training time of IALS and IALS1 gets larger. When K is 100, IALS1 is 10 times faster than IALS. When we compare a results of IALS to the results of IALS1 on the next K values (at 20 epochs9, we can see that IALS1 is both faster and more accurate in these comparisons, espe-cially when K is large. For example, IALS1 with K = 500 can reach better performance in a shorter time than IALS with K = 250 .

Figure 2 compares the time X  X ccuracy trade-off of the two algorithms. We used the same consideration to depict the curves as for Figure 1. The curve of IALS1 is always located under IALS, so we can draw similar conclusions as before, i.e. IALS1 offers superior trade-off possibility between running time and performance compared to IALS.

The curves of IALS1 and IALS have similar characteris-tics: steep start, flat in the middle, and gets steep again at eters [2, 11, 13]. For example in [13], the authors report on the use of a parallel Matlab on a Linux cluster with 30 machines to run an ALS variant with K = 1000 and achieve Quiz RMSE = 0 . 8985 . Note that we run ALS1 with K = 1000 on a single machine.

Training one feature at a time has also been used for MF algorithms. In the seminal work of Simon Funk [5], an MF variant is proposed using such a feature training; each fea-ture is trained consecutively until overlearning is detected. There are three important differences between the meth-ods: (1) Funk X  X  algorithm does not optimize old features, just introduce new ones one-by-one; (2) his algorithm trains one feature per epoch, thus it requires many epochs (many re-iteration through the ratings); (3) his algorithm applies gradient descent.

In [1], another ALS variant is proposed, which shares many characteristics with Funk X  X  one: (1) it does not op-timize old features; (2) trains one feature per epoch; (3) there is no explicit cost function, the computation of the residual of old features is rather complicated.
In this paper we presented novel and fast ALS variants for recommendation problems. The article has three main contributions:
Since ALS is a general method for matrix factorization, as future work we intend to experiment with the proposed ALS variants on other application domains different from collaborative filtering. We are also willing to investigate the idea of IALS1 when the preference/confidence matrices of negative implicit feedback are of rank K 2 . The proposed ALS1 and IALS1 store only the diagonal of the covariance matrix. We may relax this restriction and store data also in the box-diagonal. This leads to multivariate regression problems but with small number of variables. At IALS1 gradient descent method can replace RR1, offering the same time complexity. Here, in order to avoid negative learning rate caused by negative confidence values, one has to use merged single positive difference examples, as proposed in our last remark at IALS1.
 D. Tikk was supported by the Alexander-von-Humboldt Foun-dation.
