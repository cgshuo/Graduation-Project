 Metric learning plays a crucial role in the computer vision and pattern recognition community. Many tasks, such as image classification, clustering, content-based image annotation and retrieval [Wu et al. 2011] depend critically on the choice of an appro-priate distance metric. In the literature, various metric-learning methods have been proposed in the past few years [Bar-Hillel et al. 2003; Chang and Yeung 2007; Davis et al. 2007; Jin et al. 2009; Liu et al. 2010; Weinberger et al. 2006; Xing et al. 2003; Yeung et al. 2008; Zhan et al. 2009]. Nevertheless, these algorithms all deal with data lying in a single-view observation space.

In many real-world applications, the same object (e.g., face, pose) may have differ-ent observations (or descriptions) from multiple views which are highly related but sometimes look different from each other, for example, facial expression recognition for different people, pose estimation with image sequences from different objects 1 ,ob-ject recognition with picture s from different camera angles , and identity recognition with video and audio streams. Recently, some new applications have emerged, includ-ing face matching with near infrared (NIR) images and visual (VIS) ones [Lei and Li 2009], and the alignment of face images with different resolutions [Li et al. 2009]. All these applications naturally bring about a new problem: how to compute the distance metric or measure the relationship between multiview observations, which is referred to as multiview metric learning . To the best of our knowledge, Zheng et al. [2010] is the only previous work on multiview metric learning.

Multiview metric learning is a challenging task, since different observations may locate in the same or different dimensional space(s). As illustrated in Figure 1(a), two image sequences with similar pose variations are located in the same space (more gen-erally, they can be in different dimensional spaces). Considering that the appearance of each view is disparate, it is difficult to produce optimal comparison by regarding multi-ple observations as one view and measure their relationship using single-view metric-learning algorithms. In the application of low-high resolution face image matching, as shown in Figure1(b), low-resolution images and high-resolution ones form two-view observation spaces with different dimensions. In such situations, even single-view metric-learning algorithms cannot work, since the dimensions of multiview observa-tions are different.

In the literature, some algorithms have been proposed to measure the relationship between two related datasets. Canonical Correlation Analysis (CCA) [Hotelling 1936] is one of the most popular statistical methods correlating linear relationships between two variables. It has been applied to a wide range of real-world applications with great success [Hardoon et al. 2004]. However, CCA is a global and linear methodology and fails to deal with data involving complex nonlinear correlation. Kernel CCA (KCCA) [Akaho 2001] extends the nonlinear processing ability by using the so-called  X  X ernel trick X . Non-Consolidating Correlation Analysis (NCCA) [Ek et al. 2008] also extends CCA by learning additional non-shared transformations for each view. NCCA com-prises two steps: applying CCA to find shared embedded data and applying NCCA to find non-shared embedded data. In spirit, the shared embedding space learning in the first step is in the same as that in CCA. CCA, KCCA, and NCCA all deal with correlation in a global way. The advantage of these methods is that metric learning could be formulated as convex optimization problems with no local optima and could be solved using efficient algorithms. More recently, the work in Zheng et al. [2010] learns global Mahalanobis distance metrics, which extends Neighborhood Components Anal-ysis (NCA) [Goldberger et al. 2004] to a multiview setting. Since the cost function in Zheng et al. [2010] is not convex, there is no guarantee of obtaining the global optimal solutions with gradient computation. In a nutshell, these algorithms all ignore the details of the local structures.

Actually, as pointed out by Bottou and Vapnik [1992], it is usually not easy to find a unique function which holds good predictability in the entire data space. Vapnik [1995] further demonstrates that the local learning algorithms usually achieve lower empirical errors than global ones. This is b ecause nearby instances are more likely generated by the same data-generation model, while far away instances tend to differ in it. Accordingly, neighboring instances may have the same or similar distance metric, while for instances lying away in different neighboring spaces, distance metrics change heavily. In addition, it is proposed in Wu et al. [2007] that learning in a local manner can sufficiently boost capacity powers.

Some studies have been devoted to distance metric learning considering local in-formation. Local Linear Embedding (LLE) [Saul et al. 2003] and Locality Preserving Projections (LPP) [He and Niyogi 2003] are two dimensionality reduction methods, which can be seen as learning distance metrics by preserving local geometric struc-tures. Yang et al. [2006] propose another efficient algorithm for local distance metric learning in a probabilistic framework. However, LLE does not give explicit metrics for unseen data, while LPP and Yang X  X  method are still global in the sense that the same transformation is applied to all instances during metric learning. Frome et al. [2006] propose a method to learn distinctive distance functions for different instances as a combination of elementary distances between patch-based visual features; they then extend it to enable the comparison between them [Frome et al. 2007]. Since la-bel information is necessary in the learnin g process, the method could only compute local distance for labeled data. Chang and Yeung [2007] also propose a method called locally smooth metric learning, in which the learned local metrics vary smoothly and could preserve the intraclass topological structure of the data. However, a heuristic initialization stage is needed for setting the target locations of all labeled instances. Zhan et al. [2009] address the local metric learning problem using metric propagation under a transductive setting, which cannot generalize to new test data.

These local methods are all proposed for metric learning in a single-view observation space. Although the idea of local learning has been applied on multiview scenarios, current work mainly focuses on trans ductive classification [Wu and Sch  X  olkopf 2007], clustering [Wang et al. 2007; Wu and Sch  X  olkopf 2006], and dimensionality reduction [Wu et al. 2007]. As far as we know, there is little work dedicating to the topic of metric learning with local methodology on multiview observation spaces in the literature.
Inspired by the idea of  X  X hinking globally and fitting locally X  [Saul et al. 2003], in this article, we propose a novel method called Multiview Metric Learning with Global consistency and Local smoothness (MVML-GL) under a semi-supervised setting. Sim-ilar ideas have been adopted elsewhere, such as large margin classifiers [Huang et al. 2004], in which the model learns the decision boundary by considering the data in both a local and a global fashion. The basic idea of our method is to reveal the shared la-tent space of the multiview observations by embodying global consistency constraint and preserving local geometric structure. Our method decomposes the multiview met-ric learning as a two-step approach: graph-based embedding for multiview observa-tions and regression-based mapping functions learning for each instance. Specially, in the first step, our method seeks a shared latent feature space to establish the re-lationship between data from multiview observation spaces according to the labeled instances pairs. In the second step, the explicit mapping functions between the input spaces and the shared latent space are learned via regularized locally linear regres-sion, which allows different lo cal metrics to be learned at d ifferent locations of each input space. Furthermore, the graph-Laplacian regularization term is incorporated to keep the learned metric varying smoothly. It should be noted that the first step is globally consistent, which simultaneously considers geometric structures contained in each view and connections between data from different views, and the second step is locally smooth, which enables each instance to have its own specific distance metric instead of applying a uniform one for all instances. In addition, the two steps can both be formulated as convex optimization problems with closed-form solutions.

The contribution of the article is highlighted in the following.  X  This article proposes a robust and flexible multiview metric-learning method by jointly considering global consistency and local smoothness.  X  The proposed method formulates global and local metric learning as two convex optimization problems which could be efficiently solved with closed-form solutions.  X  The essential connection between multiview metric learning and manifold align-ment is discussed.

The rest of this article is organized as follows. In Section 2, we present the proposed multiview metric-learning method in details. In Section 3, we discuss the relation-ship between multiview metric learning and manifold alignment. Section 4 shows the experimental results with application to manifold alignment for pose estimation and facial expression recognition. Finally, S ection 5 gives some concluding remarks and discussions about future work. The target of the multiview metric-learning method is to compute distance metrics between samples from multiple observation datasets. Without loss of generality, we take two-view observation spaces for example, and our algorithm could be naturally extended to metric learning for multiview observations, as depicted in Section 2.4.
Since two-view datasets are from different observations of the same object, they are highly related to each other. It is reasonable to assume that some common features across both spaces can be represented in a s hared latent feature space where the in-trinsic relationship of the two datasets is revealed. As a consequence, we resort to a shared latent feature space to establish the relationship among different observations. Although sometimes the two views may seem to be quite different in nature, the in-trinsic connection between them still exists, since they are from the same object. Some well known works, such as CCA [Hotelling 1936] and KCCA [Akaho 2001], also exploit the assumption on the intrinsic relationship of data from different views, though it may be not suitable for all real-world cases. These methods have been successfully applied in many challenging applications. When the two views are quiet different, the performance of alignment from the low-dimension embeddings may also degrade heavily. In this scenario, we need more labeled corresponding pairs and better feature representation for each view in order to improve the accuracy of alignment.
Similar to spectral regression [Cai et al. 2007], we would accomplish multiview met-ric learning by two steps: (1) get the common low-dimensional embeddings for all la-beled correspondence pairs, and (2) learn the relationships between the input space of each observation and the shared latent space for unlabeled and test data.

In the following sections, we first claim some notations and present our method in two steps: the globally consistent shared latent feature space learning and locally smooth multiview metric learning. Then, the overall algorithm is summarized, fol-lowed by a short discussion. In the end, we perform a time complexity analysis and provide several alternative ways to improve for real applications. Let us represent two datasets G , P with their matrix forms as X g =[ x g 1 , x g 2 ,  X  X  X  x g N and X p =[ x p 1 , x p 2 ,  X  X  X  x p N data point in the D g ( D p ) dimensional input space. Suppose K correspondence pairs the training sets can be separated into labeled and unlabeled subsets, denoted as G = { G l , G u } and P = { P l , P u } .Let x i ( x p either from set P or set G , S l ( x i ), and let S u ( x i ) indicate the labeled and unlabeled datasets which are derived from the same input space as x i ,thatis,if x i  X  P ,then S ( x i )= P l , S u ( x i )= P u . In addition, we denote the unknown common embeddings of X g and X p by Y g =[ Y l low-dimensional embeddings for all labeled data. In order to derive the common low-dimensional embeddings for all labeled correspon-dence pairs, two important issues should be taken into consideration: (1) the embedded correspondence pairs should be as close a s possible and (2) the common embeddings should preserve the local geometric structures in all original input spaces. In this way, we also consider local smoothness for labeled data.
 We achieve the overall objective by minimizing the following energy function: where s ij ( s ij ) defines the edge weight of the corresponding graph for each dataset. With the prior knowledge hidden in available labe led correspondence pairs, the two graphs can be connected by adding some edges between related vertices. Let S be the weight matrix of the connected graph, and each entry in S is defined as where N k ( x i ) denotes the set of k -nearest-neighbors of x i ,  X  i (  X  j )isthedistancefrom x ( x j )toits m th nearest neighbor, and m is a local scale factor. This way of defining the adjacency matrix is called local scaling [Zelnik-Manor and Perona 2005], and it controls the decreasing speed of s ij with the distance between x i and x j . Note that the local parameters k and m control the values of s ij and s ij and further influence the rela-tive contribution of local structure preser ving in each observation space. Accordingly, no additional turning parameters are introduced to balance the three terms in Eq. (1). With these notations, the objective function can be simplified and rewritten as where L = D  X  S is called the Laplacian matrix, and D is a diagonal matrix with the entries D ii = j s ij . In the literature a similar idea of using graph Laplacian for multiple-view data can be found in Sindhwani and Niyogi [2005]. However, our method aims to find a shared latent subspace for local multiview metric learning, which is different from that work.

The objective function gives a high penalty when the labeled correspondence pairs from different spaces or neighboring data points in the same space are mapped far apart. Therefore, it tries to find a shared latent feature space which reflects the in-trinsic relationship between multiview observations and preserves the local geometric structures of data in each input space. For the preceding optimization problem, we set aconstraint,( Y l ) T DY l = I , to avoid trivial solutions. Then, the minimization problem is then formulated as Finally, the common low-dimensional embeddings for all labeled data can be obtained by solving a generalized eigenvector problem, which is expressed as 2.3.1. Multiview Metric Definition. The embedding results obtained in the preceding sec-tion are only the solutions for labeled training data. To map the unlabeled and new test data into the shared latent feature space, we turn to learn the mapping function f between y i and x i ,thatis, y i = f i ( x i ). Specially, we consider a linear transformation and b i is a translation vector. One often deals with the bias term b i by appending each instance with an additional dimension x i  X  [ x i ;1], W i  X  [ W i ; b T i ]. Then the linear transformation function becomes Note that the transformation function f is defined for each individual data point but not shared by all data points in each input space globally.
 the distance metric between them is defined as their Euclidean distance in the shared latent feature space, which can be formulated as It X  X  worth noting that, in most cases, W g i is not equal to W p j for the multiview metric-learning problem, since using the same projection for the two types of data would not produce optimal comparison. Furthermore, the dimensions of the two projec-tions would not be equal in the case that the data x g i and x p j are in different di-mension spaces. When W g i = W p j = W , the multiview distance metric defined previ-ously will degrade to a traditional Mahalanobis distance metric defined as d ( x g i , x p j )= ( x 2.3.2. Locally Smooth Metric Learning. It is desirable that two adjacent data points in each observation space should be mapped to close locations in the shared latent feature space. In other words, the mapping functions should vary smoothly in the original input spaces so that the datasets after metric learning can preserve the original local geometry structure. Motivated by this expectation, we learn the mapping function via regularized locally linear regression.

Without loss of generalization, let x i ( x p i or x g i ) be an unlabeled or a new test data point either from set P or set G , then the optimal local affine transformation f i for x i is computed by minimizing the following objective function: The first term in the objective function is the well known moving least square [Levin 1998]; the second term is the shrinkage constraint, also known as the Tikhonov regu-larizer [Hastie et al. 2001], which helps to improve the generalization of the solutions. The parameter  X  is used to balance the two terms. For moving the least square term,  X  ij is called the moving weight, which reflects the similarity between x i and x j and is defined as  X  ij =1 / ( x i  X  x j 2 +1 x the value 1 if a = b and 0 otherwise, and  X  is a small adjustment value to prevent the denominator from degenerating to 0. y j is the low-dimensional embedding for labeled data x j , which has already been obtained in Section 2.2.

The optimal transformation function f i is found by projecting itself onto the la-beled data samples and minimizing the weighted reconstructed errors, as formulated in Eq. (8). During this procedure, moving weights are incorporated in order to express the relative importance of labeled data. It works well when there are enough labeled correspondence pairs in the training set. However, the performance degrades when the labeled pairs are scarce or not evenly distributed in original input spaces. Inspired by manifold regularization [Belkin et al. 2004; Shao et al. 2011], in the proposed method, an additional regularization term is imposed onto the objective function, which aims to utilize the large amount of unlabeled data to achieve better generalization ability.
Considering the smoothness of the transformation functions, an additional regu-larization term is imposed onto the objective function. Ultimately, we formulate the objective function which uses both labeled and unlabeled data as follows: where the last term imposes the penalty that preserves the smoothness of local map-ping functions between point x i and the unlabeled data in the shared latent space. Besides, s ip is the weight defined in Eq. (2), and is a parameter that controls the balance between the fitting accuracy and regularization performance.
 2.3.3. Optimization Solution. The optimal transformation matrix W i can be obtained with a closed-form solution by solving a convex optimization problem. Substituting Eq. (6) into the loss function J , defined in Eq. (9), we can obtain Then, take the derivative of J with respect to W T i and set it to zero: After expanding the equation, we have and the optimal W T i can be finally represented as where I is the identity matrix, and  X  W p is the transformation function for computed unlabeled data point x p .

Note the objective function J ( f i )inEq.(9)isquadraticin f i ; therefore, it is theo-retically possible to obtain a closed-fo rm solution for the parameters of all u unlabeled transformations by solving a set of u equations. Nevertheless, this approach is un-desirable as it requires inverting a possibly large u  X  u matrix. We propose here a more efficient alternative approach for obtaining an approximate solution. For the lo-cal transformations of unlabeled data, we first order them based on the distances to labeled data, and then the transformations for the data points closer to the labeled points are estimated before those that are farther away. This alternative approach may be regarded as a process of propagating the changes from the labeled points to the unlabeled points.

Consequently, if x i is an unlabeled data sample, S u ( x i ) denotes the i  X  1 local trans-formation functions estimated before; while if x i is a new test data sample, S u ( x i ) represents all unlabeled data since their transformation functions have already been computed during the training stage. Since the final solution W i in Eq. (13) is linear with respect to all known values, we can express it in closed-form and compute effi-ciently without any iterative process. The algorithm flow of the proposed method is summarized in Table I. In our method, two optimization problems are defined (in Step 3, Step 4 and 5) and both take into con-sideration preserving of local structure. In Step 3, the low-dimensional embeddings are learned for all labeled data through local-scaling-based graph Laplacian matrix. In Steps 4, and 5, different local metrics are l earned at different locations of the input spaces via regularized locally linear regression. Consequently, the overall transforma-tions of all points in input spaces are locally linear but globally nonlinear. With such a property, our method not only can keep the advantage of easy computation due to the local processing manner, but also has a strong nonlinear processing ability for complex status.

The proposed MVML-GL method can be easily generalized to metric learning in more than two observation spaces. The generalized algorithm has two differences compared with that of Table I: multiple adjacency graphs are constructed for different observation datasets in Step 1; they are th en connected into one graph according to the labeled correspondence pairs among multiple datasets in Step 2. In particular, when there is only one observation space, our method will degrade to the conventional single-space-based metric-learning method.

In addition, our method is not restricted to semi-supervised learning (SSL) set-tings and can also be used in supervised learning (which can also be considered as the extreme case of SSL). For our method, SSL can make use of unlabeled data to achieve better performance, especially when the labeled data is sparse and unevenly distributed. In the offline phase, the optimal solutions for all labeled samples of both views are obtained by solving a generalized eigenvector problem in Eq. 5, of which the computa-tional cost is O ((2 l ) 3 ), and l is the number of labeled data in each view.
In the online phase, for each local smooth transformation, the main computation burden comes from the computation of t he matrix inverse in Eq. (13), where d is the dimension of the input space. Let T ( d ) be the time complexity of computing the inverse with the method of Coppersmith and Winogard. Thus the algorithm is efficient as long as d is not exceptionally large.

When d is large, we can reduce the computational complexity by several alterative ways. One straightforward way is to perf orm principal component analysis (PCA) [Jolliffe 2002] first to reduce the feature dimensions. Another way is to use the prin-ciple of the Woodbury matrix identity [Petersen and Pedersen 2008]. The inverse part denoted as M in Eq. (13) can be rewritten as where X l is the labeled data matrix in one view with size of d  X  l ; D  X  is a l  X  l diagonal matrix with each diagonal entry as  X  ij , I is the identity matrix, and a =  X  + According to the Woodbury matrix identity, the inverse computation in Eq. (14) has an equivalent form expressed as Since D  X  is a diagonal matrix, the inverse compuation is very efficient. In this case, the inverse operation is conducted on an l  X  l matrix with the complexity of O ( l 3 ). As a consequence, the overall computation complexity in the online phase is O ( l 3 + ld 2 ), where O ( ld 2 ) is the complexity for matrix multiplication. When the labeled data number l is smaller than input dimension d , we could utilize the Woodbury matrix identity instead to facilitate the computation.
 The average running time of the proposed method on a typical computer (2.53GHz CPU, 4G memory) is shown in Table II. We can see that the running time decreases dramatically by using the Woodbury matrix identity, especially when the dimension of features is high.

In addition, to get the trade-off between a ccuracy and efficiency, we can also gener-alize the proposed method by assuming that the samples in a local region share the same projection rather than that each sample has its own one. From the perspective of manifold learning, the entire set of data in each view could be regarded as a manifold. Thus, multiview metric learning could be interpreted as a dis-tance measurement among the data points of different manifolds. In spirit, multiview metric learning has a deep connection with a typical manifold learning application called manifold alignment, which aims to learn the correspondences between samples from different manifolds.

Broadly speaking, manifold alignment algorithms mainly fall into two cate-gories: implicit-mapping-based solution and explicit-mapping-based. On one hand, the implicit-mapping-based methods, with no explicit projective mapping to be learnt, gen-erate nonlinear low-dimensional embedding for each manifold. The nonlinear low-dimensional embedding can be interpreted as the data representation y g i and y p j ,as defined in Eq. (7). Since no mapping functions could be directly derived, this kind of methods could only do distance computation for data in the training set. Some repre-sentative work [Gong et al. 2005; Ham et al. 2005; Shon et al. 2006; Xiong et al. 2007], tend to directly align multiple data manifolds into a shared latent space or predefined target coordinates, and then match instances in correspondence. The main drawback of implicit-mapping-based methods is that they cannot process new test data without retraining. On the other hand, explicit-mapping-based methods, which learn linear transformations for each manifold, can be interpreted as learning two projective ma-trices W g i and W p j , as defined in Eq. (7), and essentially solving the similar problem as multiview distance metric learning. Manifold alignment using Procrustes analy-sis [Wang and Mahadevan 2008] is a representative explicit-mapping-based method, which can be generalized to new data points. However, Procrustes analysis, which only learns a single affine transformation between one manifold and the other, fails to work well when the relationship between the two manifolds is beyond the affine transformation.

Conclusively, explicit-mapping-based manifold alignment methods are solving a similar problem as multiview metric learning; and implicit-mapping-based ones also have its essential connections to multiview metric learning. Taking the relationship into consideration, we could evaluate the proposed multiview metric-learning algo-rithm by comparing with some state-of-the-art manifold alignment algorithms. To demonstrate the superiority of the proposed MVML-GL algorithm, we conduct var-ious experiments on two typical computer vision applications: pose estimation and fa-cial expression recognition. For comprehensive comparison, the MVML-GL algorithm is compared with some state-of-the-art methods on real-world datasets. It should be noted that in the following experiments, we do not compare with the method of Zheng et al. [2010]. This is because that we aim to align the instances based on the consis-tency of multiple views, while the method of Zheng et al. [2010], additional labeled data in each view is needed in order to achieve a better discriminative ability. Our method is under the same setting as CCA where only labeled correspondence pairs are needed during the training stage, which is more general compared with that in Zheng et al. [2010].

In experiments, the distance between one point x p j in the probe and each point x g i in the gallery set is calculated according to Table I; then the nearest one is aligned as the counterpart of x p j .

There are a few parameters involved in the MVML-GL algorithm. The dimension of the common embeddings is fixed to five, and the regularization coefficients  X  and are set to some values in (0 , 1] which can be finally determined by cross-validation. In practical implementation, we use the source code provided by Magnus Borga and M.B. Blaschko et al. for CCA 2 and KCCA 3 , respectively. The LPA algorithm mainly takes two steps: manifold dimensionality reduction and alignment using Procrustes Analysis. Regarding the second step in LPA, we utilize the source code provided by Miguel A. Carreira-Perpinn to do Procrustes analysis. 4 The NMA algorithm is imple-mented by us. In the comparative studies, the parameters are tuned to achieve the best performance. COIL-20 [Nene et al. 1996] is a dataset which contains 1,440 images of 20 objects. As shown in Figure 2, the pose coordinates for each object specify the camera movements around it, which contain 72 different sites at intervals of 5  X  . In experiments, we at-tempt to align the images with various poses from different objects for pose estimation. For each observation, 32 images are selected evenly for the training set, and the rest of the 40 images for testing. Each image is resized to 16  X  16 pixels, and image intensity is used as the feature.

To illustrate the alignment results in an intuitve way, we inlay two aligned se-quences on two concentric circles. As indi cated in Figure 3, the blue-bold lines con-nect the labeled correspondence pairs, while the black-dashed and red lines denote the connections for aligned unlabeled and test points, respectively. In perfect align-ment, the connections should be along the direction of radius (of the rays emitting from the center of the concentric circles). Due to the space limitation, the Euclidean results are omitted in Figure 3. Under this criterion, it can be seen that our method has no line cross and the lines connecting unlabeled and test data are more regular compared with other methods. It demonstrates that our method is locally smooth and has a strong out-of-sample ability to process new test data. Moreover, the quantitative comparison of the pose estimation is given in Table III in the case that the labeled correspondence number is 8 and 16.

From the comparative results shown in Figure 3 and Table III, we can see that the baseline method, which simply adopts Euclidean distance metric in input spaces, yields poor performance. The Locality Preserving Projections (LPP) method [He and Niyogi 2003], which can uncover the essential manifold structure in a single observa-tion space, fails to handle the scenario of multiview observation spaces well. The CCA method, which is linear and global, cannot provide satisfactory results either, since it only considers the relationship between the correspondence pairs without preserv-ing local structure for each input space. The Nonlinear Manifold Alignment (NMA) method [Ham et al. 2005], which performs better than CCA, takes into account local structure preserving, but it cannot proce ss the new test data since no explicit mapping is learned. As for Linear Procrustes Analysis (LPA) [Wang and Mahadevan 2008], it cannot give satisfactory results, since the relationship between the two manifolds is beyond the affine transformation. Among all the presented methods, the proposed MVML-GL achieves the best results. The metrics learned in the shared latent feature space via regularized locally linear regression vary smoothly and leads to significant performance benefits.

In the experiments, there are two important parameters, the local scalar m and the number of nearest neighbors k . We first select m with different labeled data numbers and different k . Then, the parameter k is selected after determining the optimal m .As depicted in Figure 4, the pose estimation e rrors of the overall trend increase with the number of local scalar m , for different labeled data number and k -nearest-neighbors X  settings. This may be due to the fact that the differences in appearance are large at an interval of five degrees of movement in the COIL-20 dataset. Accordingly, the width in the Gaussian kernel should be small enough to make the peak sharp. As depicted in Figure 5, the pose estimation errors decreas e and finally level off with the increasing number of the k -nearest-neighbors. In addition, it is better to set even numbers for k than odd ones, since the pose variations are symmetric. As a result, we set m =1and k = 2 for the COIL-20 dataset.

In the experiments just presented, the labeled data is evenly sampled. However, in real-world computer vision problems, the labeled data is scarce and not homoge-neously sampled in general. In this situation, most of the existing methods suffer from serious performance drops. As shown in Table IV, we give two examples when eight labeled points are unevenly distributed, while the unlabeled data is evenly dis-tributed. From the results, we can see that the proposed MVML-GL method achieves relatively low estimation errors by utilizing a large amount of unlabeled data for better generalization.
 In this section, we will illustrate our method on a real-world face pose matching problem. The database we used is a private multi-pose database that we created. It consists of 1,011 images of ten people taken under normal indoor lighting conditions and fixed background with a Sony EVI-D31 camera. The poses are almost continuous (from  X  90  X  to +90  X  , as shown in Figure 6(b)), and two example sequences are shown in Figure 6(a). In this experiment, the settings are almost the same as that for the COIL-20 dataset except that two additional comparative experiments are con-ducted and more quantitative results are given in order to achieve a more intensive evaluation.

Some qualitative results are illustrated in Figure 7, where the labeled data is evenly distributed, and the number of correspondce pairs is 13. Under the same visual cri-terion for the COIL-20 dataset, the proposed MVML-GL achieves the best alignment results among all comparative methods.

Furthermore, the quantity results are summarized in Table V. As shown in this ta-ble, Euclidean and LPP, both of which are single-view observation-space-based meth-ods, exhibit high errors. The KCCA method, which has nonlinear processing ability, gets better results than CCA in most cases. NCCA, which introduces additional pri-vate latent spaces, also decreases the errors compared to CCA. NMA gets relatively good results for unlabeled data points but fails to handle new test data. LPA still can-not give satisfactory results in this situation. Among all these methods, the proposed MVML-GL method achieves the lowest errors.

Moreover, the changes of estimation errors with the number of labeled data are shown in Figure 8. For a more clear illustration, the unsupervised methods Euclidean and LPP results are omitted, since the results do not change with the number of la-beled data. Overall, the estimation errors decrease with the number of labeled data. When the labeled data number is relatively small, the changes are obvious.
In addition, some visual results when the labeled data is unevenly distributed are illustrated in Figure 9. In this scenario, the Euclidean, LPP, CCA, and KCCA all have line-crosses of large angles. In contrast, our method keeps relatively good alignment results. It further demonstrates that the proposed method provides more accurate and stable results for unlabeled and test data and is more suitable for practical pose-alignment tasks compared with other methods.

Note that the first step, that is, the learning of the shared latent subspace (Phase I in Table I), can also be implemented by other methods, such as CCA and KCCA. We evaluate the contributions of the first step by replacing the proposed shared la-tent subspace learning method with CCA and KCCA, respectively, to get the common low-dimensions for all labeled data. Then the same metric-learning method is per-formed in the second step (Phase II and Test Stage in Table I) for unlabeled and test data. As depicted in Figure 10, the estimat ion errors of our proposed method decrease significantly compared with other comparative studies, which is mainly because the proposed shared latent subspace learning method introduces two local geometric struc-ture preserving constraints for each view. In contrast, CCA and KCCA only preserve the sample correspondences but no topological structures when establishing the rela-tionship between two views. Since in the second step, the mapping function for unla-beled or test data is a locally smooth learnin g procedure in the shared latent space, the performance will seriously decrease if the lo cal structure cannot be preserved. There-fore, the shared latent subspace learning is a basis for the second step, and it also plays a very important role for the ultimate alignment performance.

The impacts of parameters k and m for the multi-pose face dataset are illustrated in Figure 11 and 12. From the result, we can see that the pose estimation errors first decrease and then increase with the number of m . The changes of estimation errors with the number of k are similar with that in COIL-20. Since the poses are almost continuous in the multi-pose face dataset, exploiting a relatively large number of nearest neighbor points will lead to a better approximation for the data manifold structure. Thus, the optimal parameter is selected as m =2 , k = 6 to benefit from the local structure preserving for each view. Experiments are also conducted on a pub lic facial expression dataset. The JAFFE dataset [Lyons et al. 1998] contains 213 images of seven facial expressions posed by ten Japanese female models. There are three or four images for each expression of each person. We randomly select one image of each expression as labeled data and the rest as unlabeled data. In our experiment, one person is labeled with known expressions, and we recognize the expressions of the other nine persons just by aligning their facial images to the standard image set.

Since sadness and disgust are omitted as hard-to-evoke expressions, we illustrate our method with five expressions including anger, fear, happiness, neutral, and sur-prise. Table VI shows the overall recognition results of the proposed MVML-GL method in the form of classification matrix. In our experiments, there is only one recognition failure for the expressions anger, neutral, and surprise, respectively. For happiness expressions, the success is 100%.

We further compare our method with other related work. The recognition results of the comparative studies are summarized in Table VII. From the results, we can see that the Euclidean distance and LPP metric yield poor performances. CCA and KCCA are better than LPP as two general multiview metric-learning methods. NMA and LPA achieve even higher recognition rates, benefiting from the local preserving in each view. Our method gets comparable results as NMA and LPA and slightly better than KCCA, concerning the average recognition rates.

In addition, our method could be extended to distance metric learning in more than two observation spaces. In Figure 13(a), we show the the alignment results among three observations of all expressions. Each dot represents a face image, and the color specifies its expression. In perfect alignment, the dots with the same color should be connected. It can be seen that our method achieves good results except for some expres-sions, which are similar and hard to distinguish. Figure 13(b) shows some examples of the aligned expression pairs; the red box highlights the wrong alignment.
 The impacts of parameters k and m for the JAFFE dataset are shown in Figure 14. Similar to COIL-20, the pose estimation errors increase with the number of local scalar m . And the best recognition rate is achieved when k equals two. Note the changes of k in the JAFFE dataset are different with that of k in the pose dataset. This may be due to the fact that expression recognition is a multiclass problem, and the expression images do not lie on a continuous manifold in general, which makes the change of parameters k more complicated. In this article, we have proposed a new multiview metric-learning algorithm to estab-lish the relationship between multiview observations. Our method first reveals the globally consistent shared latent space of the multiview data by considering the geo-metric structure of each view and the connections between data from different views. Then regularized locally linear regression is performed to learn the explicit mapping functions between the input spaces and the shared latent space, in which the graph-Laplacian regularization term is incorporated to keep the learned metric functions various smoothly. These two procedures both can be formulated as convex optimiza-tion problems which could be solved efficiently with closed-form solutions. Experimen-tal results on pose and facial expression matching provide empirical evidence for the effectiveness of our approach.

Despite its promising performance, there is still room for us to further improve our method. To trade accuracy for efficiency , the proposed multiview metric-learning method can be generalized to piece-wise linear, in which the samples in a local region share the same projection instead of each sample having its own one, which could bring some computation advantages for large-scale practical applications. Another interesting extension is to introduce discriminative information into the regularization framework for some classification problems.

