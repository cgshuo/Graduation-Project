 Re-finding what we have accessed before is a common be-havior in real life. Psychological studies show that context under which information was accessed can serve as a pow-erful cue for information recall.  X  Finding the sweet recipe that I read at the hotel on the trip to Africa last year  X  X sa context-based re-finding request example. Inspired by users X  recall characteristics and human memory, we present a con-text memory model, where each context unit links to the data created/accessed before. Context units are organized in a clustering and associative manner, and evolve dynami-cally in life cycles. Based on the context memory, we build a recall-by-context query model. Two methods are devised to evaluate context-based recall queries. Our experiments with synthetic and real data show that evaluation exploring the use of context associations can get the best response time. H.3.3 [ Information Search and Retrieval ]: Search pro-cess Algorithms, Design human memory, context, information re-finding
Motivation. To re-access information one has come across occasionally or intentionally before is very common in daily  X  lives. Teevan et al. [17] once analyzed a one-year Web log of 114 users and found that 40% of queries are re-finding requests. A similar finding was obtained by [15] that over 58% of Web pages accessed are re-visits to pages previously seen, based on the analysis of 6-week usage data of 23 users.
Information re-finding is different from finding. There is uncertainty in the latter because users don X  X  know enough information, while re-finding is a more directed process as users have seen the information before [3]. A general way to support re-finding is to maintain query logs [2]. However, because of users X  dim memories of the past (as evidenced in [16], original queries were remembered wrongly 28% due to miss-remembering or forgetting), sometimes it is a difficult and time-consuming task to re-find what they want by sim-ply entering keywords of the previous information content.
Inspiration by Human Memory. Psychological stud-ies show that context under which information was accessed before can serve as a powerful cue for information recall, as it is easier to remember than detailed content itself [9]. For example, it may be hard to recall a recipe  X  X  detail accessed one year ago, but the associated contexts last year, at a ho-tel, traveling to Africa , etc. may leave a deeper impression, and can thus serve as cues to re-find target information.
Life scientists discovered that there is a kind of memory mechanism called episodic memory in a human brain, which enables people to be consciously aware of earlier experiences under context [19]. It is the episodic memory that receives and stores information about temporally dated episodes or events, together with their temporal-spatial relations [18]. The memory trace, which is the central representation of the to-be-remembered event, is a multi-dimensional collec-tion of elements, features, or attributes [20]. The retrieval of episodic memories involving an interaction between a re-trieval cue and a memory trace, is a cue dependent process that reflects the temporal contiguity and the semantic rela-tionship of the cue and the target entities [14].
Our Work. Inspired by the human memory retrieval mechanism, we build a link between the information and its previous accessing context instance, represented as a multi-dimensional vector. A context memory contains a large vol-ume of associated context instances organized in clusters. To mimic the amnesia characteristic of human memory, we bind each context instance with a dynamic life-cycle degra-dation policy. Memory reinforcement is also incorporated by adjusting the degradation speeds of context instances.
Based on the context memory, we build a recall-by-context model to support users X  re-finding queries, where a query is indeed a context instance, and a ranked list of the matched context instances is an intermediate result that links to the information accessed before as the final result. We explore the use of context clusters and associations for efficient pro-cessing of context-based re-finding queries. Our experiments on synthetic and real data show that the association link en-ables the best response time and scalability due to its index-like role. We also examine three similarity ranking functions and find out their little influence upon results ranking. The contributions of the paper are summarized as follows.  X  We present a brain memory inspired, context-based in-formation re-finding framework, which enables users to re-find results accessed before by relevant contexts.  X  We build a context memory model, which specifies clus-tering and associative structures of context instances, as well as their dynamic life-cycle evolution strategies.  X  We build a recall-based querying-by-context model based on the context memory model, and evaluate several querying methods and ranking methods.

The rest of the paper is organized as follows. We review some closely related work in Section 2. In Section 3, we present the context memory model, followed by a context-based re-finding model in Section 4 describing several query evaluation methods, and report corresponding performance in Section 5. We conclude the paper in Section 6.
Web Search. Mayer [11] made a good survey on the existing approaches and tools for web re-visitation. Typi-cal techniques to assist Web re-finding include bookmarks, history lists, search engines, etc. Google X  X  Web History [1] keeps users X  Web activity data such as search requests and clicked pages and classifies them into different topics such as Web, images, news, etc., and allows users to browse or search their historically accessed pages. Teevan [16] built a search engine which can support not only finding of new information but also re-finding of old information. Morris et al. [12] developed a SearchBar to organize a user X  X  Web access history in a hierarchy, including user X  X  recent search topics, queries, results visited and user X  X  notes. By browsing the hierarchy, the user can re-acquire previous search infor-mation (such as queries, results visited, and their notes).
Personal Information Management. Dumais et al. [6] developed a system Stuff I X  X e Seen to facilitate personal in-formation re-use. It builds an index for what a person has seen, and uses file-type, access date, and author for filtering and sorting results. Context and memory cues are also taken into account to enhance content-based personal information searching [8, 5]. Chen et al. [4] built a desktop search sys-tem iMecho exploiting semantic associations among files to enhance full-text search, where the associations are mined from contents such as similar-to relation and users X  such operations as jump-to, copy-from, same-task, etc. Li and Meng proposed a query method to re-find referenced files, given some file items as input. They identified three types of reference relations based on users X  sequence operations, such as adjacent, inclusive, and lineage relations, based on which three adjacency matrixes of file items are produced. Query results can thus be computed by multiplying the en-try vector with the matrixes [10].

While great efforts have been made to explore contex-tual cues for re-finding, this study draws inspirations from human memory and aims at a more general context-based re-finding framework, and consequently the query model against a well-organized and evolving context memory.
To support context-based information re-finding, the first thing to do is to build a context memory model.
We mimic human memory and organize our context mem-ory into a short-term context memory unit (SCM) and a long-term context memory unit (LCM), as shown in Figure 1. There are two types of memory units in LCM: perma-nent and general . The former records life-long experiences and is immutable, while the latter will decay.

In the study, the circumstance under which a user X  X  infor-mation access occurs is called an access context . Contexts can be either internal user -related (like user name, activity, etc.) or external environment -related (like place, surround-ing people, etc.) [7]. If the accessed information (like an article) is of interest to the user, a linkage between the ac-cess context instance and the information is created and to-gether with the detail of information content, is stored into the contextually accessed entity repository .

Context memory is dynamic and information transits across the two memory units as showed in Figure 1. This paper fo-cuses on the long-term context memory unit (abbreviated as context memory for short) where contexts will decay grad-ually in life-cycles. We describe its static structure and dy-namic evolution in detail in the following subsections.
Access context is comprised of n contextual attributes ( A 1 ,A 2 ,...,A n ). The domain of each attribute forms a hier-archy of levels of abstraction that can be viewed as a lattice ( H,  X  h ), where H =( h 1 ,h 2 ,...,h s  X  1 ,ALL )of s levels with the levelId (1 , 2 ,...,s -1 ,s ), and  X  h is a partial order among the levels of H , such that ( h 1  X  h h i  X  h ALL ), 1 &lt;i&lt;s . Figure 2 depicts two hierarchy examples for contextual at-tribute Time and Location . A weight in [0, 1] expresses the hierarchical similarity factor between two consecutive hierarchical levels h i and h i +1 in H , denoted as s i,i +1
Definition 1. Let c and c be two contextual attribute val-ues of A . Assume function h ( A, v ) returns the hierarchical level id of v . c is called an ancestor of c , denoted as c &lt;h ( A, c ) and there exists an upward path from c to c . Definition 2. Given two contextual attribute values c, c  X  Dom ( A ), let k = h ( A, c ), k = h ( A, c ). The similarity be-tween c and c denoted as sim ( A, c, c ) is defined as follows: (1) if c = c , sim ( A, c, c )=1; (4) if c and c have a common ancestor p ,let m = h ( A, p ),
The above definition is quite intuitive. The similarity be-tween two contextual attributes is subject to their distances at hierarchical levels. It is easy to show that if c 1  X  a c  X  a c 3 ,then sim ( A, c 1 ,c 3 ) &lt;sim ( A, c 2 ,c 3 ).
A context instance is an instantiation of its n contex-tual attributes, represented as a tuple C =( c 1 ,c 2 ,...,c where c i  X  Dom ( A i ) for every 1  X  i  X  n . Based on the similarities of their respective contextual attribute val-ues, we can measure the similarity of two context instances C =( c 1 ,c 2 ,...,c n )and C =( c 1 ,c 2 ,...,c n ) as follows.
Definition 3. Assume context is an n -dimensional vector ( A 1 ,A 2 ,...,A n ), let C =( c 1 ,c 2 ,...,c n )and C =( c ...,c n ) be two context instances. (1) C is equal to C , denoted as C = C , if and only if  X  i  X  X  1 , 2 ,...,n } ( c i = c i ). (2) C is more general than C , denoted as C  X  C ,ifand only if  X  i  X  X  1 , 2 ,...,n } (( c i  X  a c i )  X  ( c i = c (3) C is associated with C on contextual attribute A i , denoted as C A i  X  X  X  C , if and only if sim ( A i ,c i ,c i  X  is a threshold value set beforehand.

Definition 4. Centered around a representative con-textual attribute value r  X  Dom ( A ), a set of context instances form a cluster , denoted as CC ( A, r ), where for  X 
C =( c 1 ,c 2 ,...,c n )  X  CC ( A, r ) sim ( A, c i ,r )  X  Dom ( A ))  X  (( c i = r )  X  ( c i  X  a r )), where  X  is a clustering threshold value in [0,1].
 Definition 5. A context memory snapshot is a graph CM =( V CC ,E CC ), where V CC is a set of vertices (repre-senting context instance clusters) and E CC is a set of edges on vertices (representing association relationships of context instances). The graph varies with time goes by, that is, it X  X  a function of time to some extent.
To measure the degradation of contextual attribute value c  X  Dom ( A ), we employ a modified exponential-power func-tion [13], denoted as R ( A, c, t ) to express c  X  X  retention strength after elapsing time t .Arealnumber r  X  [0 , 1] is used to represent the retention strength for each value, which is clearly remembered if r approaches 1 and tends to be forgot when close to 0.
 where r 0 is the initial value of retention strength,  X  is the decay rate coefficient,  X  max and  X  min are two thresholds.  X  If ( r 0 &gt; X  max ): the retention degree of the value remains unchanged, corresponding to the permanent LCM.  X  If ( r 0 &lt; X  min ): the retention degree is set to 0, corre-sponding to the SCM.  X  If (  X  min  X  r 0  X   X  max ): the retention degree is weakened as time passes, corresponding to the general LCM.
For a contextual attribute A , different retention ranges are allocated to different hierarchical levels as shown in Figure 2 and particularly, the union of all the exclusive retention ranges in a hierarchy is [  X  min , X  max ]. if R ( A, c, t ) at t = t 0 ,and R ( A, c, t )  X  (  X  j  X  1 , X  j ]at t = t 0 i&lt;j  X  s ), then c will degrade from h i to h j .Whenavalue c of attribute A is recalled by a user, r 0 is increased by a percentage of  X  r and  X  is decreased by a percentage of  X  thus slowing down the degradation speed.
Context-based re-finding differs from database querying in three aspects. Firstly, request formulation is based on contextual attributes rather than database contents. Sec-ondly, query target is context memory snapshot rather than database. Thirdly, an intermediate query result is a ranked list of context instances, with their linked information ac-cessed before as the final query result.
A context-based re-finding query can be denoted as a function RF ( Q, CM )= C 1 ,C 2 ,...,C m ,where Q is the query request formulated as a context instance, CM is the query target that is the context memory snapshot, and the intermediate query result is a ranked list of context instances in CM , C 1 ,C 2 ,...,C m , whose ranking is determined by a ranking function. For the final result generation via the accessing context instances is quite straightforward, we fo-cus on the evaluation of the intermediate context instances result in the following discussion.

In the study, we consider three ranking methods based on simple similarity, weighted similarity, and negative dis-similarity between Q and C .Let Q =( q 1 ,q 2 ,...,q n ), and C =( c 1 ,c 2 ,...,c n ) without loss of generality.
Ranking by simple similarity. Use the similarity func-tion defined above to rank context instances in the memory snapshot against Q .

Rank ( Q, C )= Sim ( Q, C )= 1
Ranking by weighted similarity. A weight vector ( w 1 ,w 2 ,...,w n ) is used to state the precise degrees of Q  X  X  contextual attribute values, where w i  X  [0 , 1], n i =1 w
Ranking by negative dissimilarity. The similarity be-tween Q and C can also be calculated via their dissimilarity: Dissim ( Q, C )=1  X  min n i =1 sim ( A i ,q i ,c i ).
Rank ( Q, C )=1  X  Dissim ( Q, C )=
A query may or may not exactly match to a context in-stance in CM . Three kinds of matching between Q and C (denoted as C Q ) are considered and included in the re-finding result: 1) exactly matching ( Q = C ); 2) specifically matching ( C  X  Q ); and 3) generally matching ( Q  X  C ).
A straightforward way to re-find information by Q is to scan CM , and get those three kinds of matching context instances, and return them after ranking. Apparently, this naive solution takes O ( n  X | CM | ) time cost in the dominate matching part, and can X  X  scale-up well with a large volume of existing and consistently incoming context instances in the memory. Efficient re-finding strategies are needed.
For each contextual attribute A i , we can get a set of con-text instance clusters as follows: 1) Identifying a representative attribute value r for a new cluster CC ( A i ,r ). Find the un-clustered context instance whose contextual attribute value of A i situates at the highest hierarchical level, and take the value as r . 2) For each un-clustered context instance C in CM ,ifits attribute value equals to r or is descendant of r ,andits similarity to r is no less than a cluster threshold  X  ,then put C into CC ( A i ,r ), where CC ( A i ,r )= { C | ( C  X  un-clustered)  X  (( c i  X  a r )  X  ( c i = r ))  X  sim ( A i 3) Repeat 1) and 2) until all instances are clustered.
Nowweobtain n cluster sets CL ( A 1 ) ,CL ( A 2 ) ,...,CL ( A where CL ( A i )= { CC ( A i ,r 1 ) ,CC ( A i ,r 2 ) ,...,CC ( A (1  X  i  X  n ), m is the total number of clusters in CL ( A i )and CC ( A i ,r j ) is a context instance cluster for every 1 The obtained clusters in this way are exclusive.

After clustering, we can then check the matching of Q on a cluster base. We call CC ( A i ,r )a candidate cluster of Q ,if it contains instances that may match Q , i.e., satisfying one of the following conditions: (1) r = q i ;(2) r  X  a q i ;and(3)
Figure 3: Association chains of context instances q  X  a r .As Q may indicate n attribute values for matching, we choose the attribute value leading to the least number of candidate clusters to start matching. After filtering out candidate clusters based on the selected attribute value, we further examine whether each of their instances, C ,matches the rest attribute values of Q .Ifyes,weput C into the result list; and discard it, otherwise. The pseudocode of cluster-based re-finding is shown in Algorithm 1. Algorithm 1 Cluster- X  -based Re-finding 1: L  X  X  X  ; 7: Add C to L ; 8: L  X  Rank ( L , Q ); 9: return L ;
Correctness. The exclusive candidate clusters contain all possible context instances that match Q . This is obvious, as for any context instance C , C Q implies that ( r s = q  X  ( r s  X  a q i )  X  ( q i  X  a r s ), which is the filtering condition.
Time Complexity Analysis. Selection of a starting contextual attribute A i (line 2) takes O( n ). Filtering out matching contextual instances (line 3-7) takes O( n  X | CL ( A |
CC ( A i ,r s ) | ). Therefore, the time complexity of the algo-on the number of clusters and the size of clusters, which are sensitive to the clustering threshold  X  .
We also explore context associations to re-find informa-tion. For each contextual attribute A i , we build for every value v an association chain Chain ( A i ,v ), which consists of all the context instances with the same attribute value of A i . Figure 3 shows four context instances in the mem-ory snapshot, and a few chains are illustrated in the left of it. To facilitate exactly, specifically, and generally matching between the query and context instances, we extend asso-ciation chains to include all the ancestors and descendants according to the contextual attribute hierarchies, and obtain EChain ( A i ,v ), so that for  X  C  X  EChain ( A i ,v ), ( c or ( v  X  a c i )or( c i = v ), as shown in the right of Figure 3.
Given a query Q , we start from the extended association chain of the shortest length, and then check the matching of each chained context instance against other contextual at-tribute values requested in Q . Algorithm 2 gives the pseu-docode of association-based re-finding approach.
 Algorithm 2 Association-based Re-finding 2: L  X  X  X  ; 6: Add C to L ; 7: L  X  Rank ( L , Q ); 8: return L ;
Correctness. As all the context instances that possibly match Q are included in the selected chain EChain ( A i ,v ) which is of the shortest length, the result list contains all matching context instances.

Time Complexity Analysis. Selection of the shortest extended association chain takes O( n ). Checking whether a context instance in the selected association chain matches Q takes O( n  X  Length ( EChain ( A i ,q i ))). So the total time cost is O( n  X  Length ( EChain ( A i ,q i ))).
We examine the performance of the two re-finding algo-rithms on both synthetic and real data sets. We evaluate their scalability in terms of query response time on large-scaled synthetic data, and their applicability on a small-scaled real data set. We study the influence of context degra-dation on query quality through the two widely adopted pre-cision and recall measurements:
We first build a complete contextual attribute hierarchies, each of which contains approximately 1000 nodes and totally 7 hierarchical levels. To generate a context memory snap-shot that is a set of context instances, we randomly pick up an attribute value from each hierarchy, and combine them to form an n -dimensional context instance. For each context memory snapshot, we generate 100 re-finding queries, each of which is an n -dimensional context instance. We bind a weight with each attribute in the query, and some attribute values may be missing. For each query, we indicate a true context instance that the user is looking for.

Context Degradation and Reinforcement. Each at-tribute of a context instance has its own initial retention factor r 0 and decay rate  X  , and decays independently based on the function R ( A, c, t ), where  X  max =0 . 98,  X  min The attribute value will be brought into the permanent unit if r&gt; X  max while deleted if r&lt; X  min . When all the at-tributes are deleted, the context instance will die out. Ev-ery time a value degrades, there is a probability (0 . 05 in the experiments) the context instance gets reinforced, i.e. ,  X  is decreased 10% and r 0 is increased 5%. Figure 4 shows the Figure 4: Change of memory retention strengths under different ( r 0 , X  )s (a) dimension number = 4 Figure 5: Query response time versus clustering threshold  X  change of retention strengths of attribute values under dif-ferent ( r 0 , X  )s as time elapses, where A 1 (0.87, 0.11), A 0.25), A 3 (0.38, 0.32), A 4 (0.73, 0.54), A 5 (0.53, 0.82).
Response Time. We run 100 queries on different-scaled context memory snapshots, and calculate the average re-sponse time of the cluster -based and association -based ap-proaches. The clustering threshold  X  is set to 0.9 (Figure 5showsthe cluster -based approach gets the best response time when  X  =0.9). We examine the behaviors of these two algorithms by varying the number of context instances in a 4-dimensional memory snapshot from 100K to 600K (Fig-ure 6(a)), and varying the dimensional number from 2 to 7 (Figure 6(b)) under size = 400K, respectively.

The association-based method performs consistently bet-ter than the cluster-based one does, thanks to its effective index-like association chains constructed in advance. Both methods take more time to re-find context instances along with the growth of the memory size. However, increasing the contextual dimension number dim hardly affects the re-sponse time of both. This might be due to the fact that both methods have already appropriately organized the context instances beforehand and the increase of dim just incurs a little bit more time to match the involved attribute values. (a) dimension number = 4 Figure 6: Re-finding response time on synthetic data
Query Quality. We evaluate the query quality through recall and precision measurements. Three ranking methods described in Section 4.1 are tested. It is interesting to note their very similar behaviors. We execute 100 queries over the synthetic data set of memory size = 400K, and compute the average precision and recall rates. Along with the in-crease of dimension number from 2 to 7, both precision and recall get improved, as shown in Figure 7(a) and 7(b). This is contributed to two factors: 1) with more contextual di-mensions, more query conditions are imposed, leading to a better query quality; and 2) more dimensional attribute val-ues of a context instance participate the degradation makes the chance that the context instance gets lost out of the memory become smaller, leading to a better query quality.
We collect from the IE browser 1000 typical URLs that a user visited during the past 4 weeks. Since pieces of ex-tremely aged URL information is too obscure for the user to remember and his/her attempts to re-find become less, we just extract a sample covering not a too long time span. Linked to each URL is the access context instance that con-tains 4 attribute values visit time, Web site, topic, and visit purpose . We raise three type of queries: 1) 2 effective dimen-sions ( ALL, Web site, topic, ALL ); 2) 3 effective dimensions ( ALL, Web site, topic, visit purpose ); and 3) 4 effective di-mensions ( visit time, Web site, topic, visit purpose ). For each type, 20 queries are put forward and tested. Due to the small-scaled real data set, we only evaluate the re-finding query quality. Figure 7(c) shows that memory degradation may not throw away wanted instances, but may bring more irrelevant context instances into the query result.
In this paper, we present a context-based information re-finding framework to enable users to re-find information by accessing context. Inspired by human memory we build a context memory model, based on which we build a context-based re-finding model and present two algorithms to pro-cess a context-based re-finding request. Our experiments show that the association-based strategy has the faster re-sponse time than cluster-based strategy has. We also exam-ine the influence of memory degradation on query quality.
There are some interesting topics to be explored in the fu-ture. While appropriate degradation of context memory can reduce storage demands and improve query performance, too fast or too slow degradation must be avoided. Feedbacks from users could help make proper decisions. Dynamically tuning of the degradation is also worthwhile to explored.
