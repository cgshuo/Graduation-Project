 Gaussian processes are a flexible and popular approach to non -parametric modelling. Their con-predictive power rival state-of-the-art discriminative m ethods such as the support vector machine, for training scales as N 3 for N data points, and the cost of prediction is O ( N 2 ) per test case. where M  X  N is the size of an auxiliary set, often a subset of the training data, termed variously terms interchangeably. Qui  X nonero-Candela and Rasmussen [8] demonstrated how many of these In this paper we consider the  X  X ully independent training co nditional X  or FITC approximation, which appeared originally in Snelson and Ghahramani [1] as the spa rse pseudo-input GP (SPGP). Restricted to a Gaussian noise model, the FITC approximatio n is entirely tractable; however, for many problems, the Gaussian assumption is inappropriate. I n this paper, we describe an extension is not only a common problem, but our results bear out the intu ition that sparse methods are well-often allowing large regions to be summarized with very few i nducing inputs. Contrast this with regression problems, where higher frequency components in the latent signal demand the pseudo-inputs appear in much higher density.
 The informative vector machine (IVM) of Lawrence et al. [2] i s another sparse GP method that has been extended to non-Gaussian noise models. It is a subset of data method in which the active set stage the optimal inclusion. When a threshold number of poin ts have been added, the algorithm influence the model only in the weak sense of guiding previous steps of the algorithm. Our method is for the same active set a closer approximation to the posteri or distribution. Secondly, unlike the likelihood estimates, and derivatives thereof, to allow mo re reliable model selection. Finally, we training data, as compared with the greedy approach that dri ves the IVM, can be a great advantage section 6.
 section 4 provides a brief account of the procedure for model selection; experimental results appear in section 5, which we discuss in section 6; our concluding re marks are in section 7. Given a domain X and covariance function K ( , )  X  X  X X X  R , a Gaussian process (GP) over the space of real-valued functions of X specifies the joint distribution at any finite set X  X  X  : where the f = { f matrix , the evaluation of the covariance function at all pairs ( x the posterior distribution over the f , given the observed X and y , which with the assumption of i.i.d. Gaussian corrupted observations is also normally di stributed. Predictions at X marginalizing over f in the (Gaussian) joint p ( f , f In order to derive the FITC approximation, we follow [8] and i ntroduce a set of M inducing inputs  X  where p ( u |  X  X ) = N ( u ; 0 , K communication between them must pass through the bottlenec k of the inducing inputs. The FITC approximation follows by letting where Q Using (1) and marginalizing over the exact prior on u we obtain the approximate prior on f In the original paper, Snelson and Ghahramani placed the pse udo-inputs randomly and learned their locations by non-linear optimization of the marginal likel ihood. We have adopted the idea in this algorithm.
 In the case of classification, a sigmoidal function assigns c lass labels y that increases monotonically with the latent f cally a Gaussian approximation. Of the latter methods, expe ctation propagation is possibly the most accurate (at least for GP classification; see [10]), and it is the approach we follow below. 12]. Suppose we have an intractable distribution over f whose unnormalized form factorizes into a product of terms, such as a dense Gaussian prior t { t parameters  X  , since in this case their product retains the same functiona l form as its components. The Gaussian (  X  ,  X  ) has a natural parameterization ( b ,  X  ) = (  X   X  1  X  ,  X  1 this form, its site function is exact: where Z is the marginal likelihood and z by the current marginal with the omission of that site, and th e true likelihood term t so-called tilted distribution q n ( f then fits only the parameters  X  with scale z remaining sites are liable to change, and several iteration s may be required before convergence. In the discussion below we omit the moment calculations for t he probit model, since they correspond mean and covariance structure of the approximate posterior is preserved. Examining the form of the prior (3), we see the covariance consists of a diagonal compo nent D where P updated during the course of the EP iterations). Since the ob servations y can expect this decomposition to persist in the posterior.
 EP requires efficient operations for marginalization to obt ain p ( f precision. Decomposing M = R T R into its Cholesky factor, 2 we represent the posterior covariance A and mean h by where D is diagonal,  X  is N  X  1 and  X  is M  X  1 . Writing p T Now consider a change in the precision at site n by  X  e n = 1 of the old precision matrix and the change in precision. If we let E = D  X  1 +  X  and incorporating the update to site n ,
A where we expand the inversion to obtain a rank-1 downdate to t he Cholesky factor R ; 3 in summary If the second site parameter, corresponding to precision ti mes mean, is changed by b where R new = rot180 chol rot180 I + R 0 P T 0  X  ( I + D 0  X  )  X  1 P 0 R T 0 Finally, the mean is refreshed using where we have assumed h Reviewing the algorithm above, we see that EP costs are domin ated by the O ( M 2 ) Cholesky down-which is O ( N M 2 ) , together leading to asymptotic complexity of O ( N M 2 ) . 3.1 Predictions To make predictions, we marginalize out u from (2). Initially, Bayes X  theorem is used to find the posterior distribution over u from the inferred posterior over f : Let our posterior approximation be q ( f | y ) = N ( f ; h , A ) . Hence after precomputations,  X  In the classification domain, we will usually be interested i n EP provides an estimate of the log evidence by matching the 0t h-order moments z When our posterior approximation is exponential family, Se eger [12] shows the estimate to be where  X ( ) denotes the log partition function and  X  are again the natural parameters, with super-pseudo-input locations, and noise model parameters. When t he EP fixed point conditions hold (that is, the moments of the tilted distributions match the margin als up to second order for all sites), derivatives  X  of the covariance structure, it is again possible to limit th e complexity to O ( N M 2 ) . IVM, in which sites excluded from the active set have paramet ers clamped to zero, and where those gained from the previous step. These complications are all s idestepped in our SPGP implementation. We conducted tests on a variety of data, including two small s ets from [14] 4 and the benchmark already divided the data into training and test partitions. Comparisons are made with the full GP classifier, and the SVM, a widely-used discriminative model which in practice is found to yield vector machine, the informative vector machine, and the spa rse pseudo-input GP classifier. synth 250 : 1000 2 0.097 0.227 0.098 98 0.096 0.235 150 0.087 0.234 4 crabs 80 : 120 5 0.039 0.096 0.168 67 0.066 0.134 60 0.043 0.105 10 banana 400 : 4900 2 0.105 0.237 0.106 151 0.105 0.242 200 0.107 0.261 20 breast-cancer 200 : 77 9 0.288 0.558 0.277 122 0.307 0.691 120 0.281 0.557 2 diabetes 468 : 300 8 0.231 0.475 0.226 271 0.230 0.486 400 0.230 0.485 2 flare-solar 666 : 400 9 0.346 0.570 0.331 556 0.340 0.628 550 0.338 0.569 3 german 700 : 300 20 0.230 0.482 0.247 461 0.290 0.658 450 0.236 0.491 4 heart 170 : 100 13 0.178 0.423 0.166 92 0.203 0.455 120 0.172 0.414 2 image 1300 : 1010 18 0.027 0.078 0.040 462 0.028 0.082 400 0.031 0.087 200 ringnorm 400 : 7000 20 0.016 0.071 0.016 157 0.016 0.101 100 0.014 0.089 2 splice 1000 : 2175 60 0.115 0.281 0.102 698 0.225 0.403 700 0.126 0.306 200 thyroid 140 : 75 5 0.043 0.093 0.056 61 0.041 0.120 40 0.037 0.128 6 titanic 150 : 2051 3 0.221 0.514 0.223 118 0.242 0.578 100 0.231 0.520 2 twonorm 400 : 7000 20 0.031 0.085 0.027 220 0.031 0.085 300 0.026 0.086 2 waveform 400 : 4600 21 0.100 0.229 0.107 148 0.100 0.232 250 0.099 0.228 10 anisotropic version primarily to allow comparison with the SVM: lacking a probabilistic foundation, other GP models, we fit hyperparameters by gradient ascent on the estimated marginal likelihood, to five randomly initialized models which the evidence most f avoured. Results on the R  X atsch data the site given in footnote 5. In comparison with that model, S PGP tends to give sparser and more accurate results (with the benefit of a sound Bayesian framew ork).
 Identical tests were run for a range of active set sizes on the IVM and SPGP classifier, and we have attempted to present the large body of results in its most com prehensible form: we list only the sparsest competitive solution obtained. This means that using M smaller than shown tends to cause as
M  X  N we expect error rates to match those of the full model (at leas t for the IVM, which model, the essential question is: what is the greatest spars ity we can achieve without compromising Small values of M for the FITC approximation were found to give remarkably low error rates, and incremented singly would often give an improved approximat ion. In contrast, the IVM predictions were no better than random guesses for even moderate M  X  X t usually failed if the active set was smaller than a threshold around N/ 3 , where it was simply discarding too much information X  X nd greater step sizes were required for noticeable improvemen ts in performance. With a few exceptions then, for FITC we explored small M , while for the IVM we used larger values, more widely spread. More challenging is the task of discriminating 4s from non-4 s in the USPS digit database: the data are 256-dimensional, and there are 7291 training and 2007 te st points. With 200 pseudo-inputs (and 51,200 parameters for optimization), error rates for SPGPC are 1.94%, with an average negative log to 1.79% and 0.048 nats. When provided with only 200 points, t he IVM figures are 9.97% and 0.421 as  X  X ot 4 X  X  X ut given an active set of 400 it reaches error rate s of 1.54% and NLP of 0.085 nats. A sparse approximation closely related to FITC is the  X  X eter ministic training conditional X  (DTC), whose covariance consists solely of the low-rank term LML T ; it has appeared elsewhere under mean of this process. FITC is similar, but the draws are noisy samples from the posterior process itself X  X ence, while the noise component for DTC is a constan t corruption  X  2 , for FITC it grows away from the inducing inputs to K in FITC) that the optimization of pseudo-inputs by gradient ascent on the marginal likelihood can succeed: without the noise reduction afforded locally by re locating pseudo-inputs, DTC does not the same mechanism operates in general for non-Gaussian noi se.
 were effective. We hypothesize however that the most inform ative vectors in the greedy sense of the IVM tend to be those which lie close to the decision bounda ry. Such points will have a rela-squared. A preferable solution may be that empirically foun d to occur with Tipping X  X  relevance vector machine (RVM) [15], a degenerate GP where a particula r prior on weights means only a few basis functions survive an evidence maximization procedur e to form the model; 7 there, the classi-fier was often parameterized by points distant from the decis ion boundary, suggested to be more  X  X epresentative X  of the data.
 may more easily be found if the inducing inputs can be positio ned independently of the data. This number of training points. We drew samples from a two-dimens ional  X  X or X  problem, consisting of rate of around 13% and in loose terms a complexity which requi res an active set of size four. By increasing the size of the training set N in increments from 40 to 400, we obtained the learning curves of figure 1 for the IVM and FITC models: plotted against N is the size of active set required for the error rate to fall below 15%. Whereas the FITC model re quires a constant four points to explain the data, the demands of the IVM appear to increase al most linearly with N . approximation run using the IVM active set and, generously, optimal kernel parameters. With a relatively simple and low-dimensional problem, the benefit of the adaptable active set that FITC offers is clearly less significant than that of the improved a pproximation itself X  X lthough there is data. However, a sensible compromise where optimization of all pseudo-inputs is computationally explored by Snelson and Ghaharamani [17] for this model in th e case of regression, is to learn a low dimensional projection of the data X  X dvantageous, sinc e in this setting the pseudo-inputs only operate under projection and can be treated as low-dimensio nal, potentially reducing significantly the scale of the optimization problem. We report results of t his extension in future work. We have presented an efficient and numerically stable way of i mplementing the sparse FITC model in Gaussian processes. By way of example we considered binar y classification in which extra data points are introduced to form a continuously adaptable acti ve set. We have demonstrated that the locations of these pseudo-inputs can be fit synchronously wi th parameters of the kernel, and that dimensionality, are not amenable to this approach since the number of hyperparameters is unfeasibly large for non-linear optimization. In this case, we suggest resorting to a greedy approach, using a An alternative which deserves investigation is to attempt a n initial round of k-means clustering. References
