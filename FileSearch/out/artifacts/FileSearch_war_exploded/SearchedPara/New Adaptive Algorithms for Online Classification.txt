 Linear discriminative online algorithms have been shown to perform very well on binary and mul-ticlass labeling problems [10, 6, 14, 3]. These algorithms work in rounds, where at each round a new instance is given and the algorithm makes a prediction. After the true class of the instance is revealed, the learning algorithm updates its internal hypothesis. Often, such update is taking place only on rounds where the online algorithm makes a prediction mistake or when the confidence in due to its prediction, such as the total number of mistakes.
 Until few years ago, most of these algorithms were using only first-order information of the in-put features. Recently [1, 8, 4, 12, 5, 9], researchers proposed to improve online learning algo-rithms by incorporating second order information. Specifically, the Second-Order-Perceptron (SOP) proposed by Cesa-Bianchi et al. [1] builds on the famous Perceptron algorithm with an additional data-dependent time-varying  X  X hitening X  step. Confidence weighted learning (CW) [8, 4] and the adaptive regularization of weights algorithm (AROW) [5] are motivated from an alternative view: maintaining confidence in the weights of the linear models maintained by the algorithm. Both CW and AROW use the input data to modify the weights as well and the confidence in them. CW and AROW are motivated from the specific properties of natural-language-precessing (NLP) data and indeed were shown to perform very well in practice, and on NLP problems in particular. However, the theoretical foundations of this empirical success were not known, especially when using only the diagonal elements of the second order information matrix. Filling this gap is one contribution of this paper.
 In this paper we extend and generalizes the framework for deriving algorithms and analyzing them through a potential function [2]. Our framework contains as a special case the second order Percep-tron and a (variant of) AROW. While it can also be used to derive new algorithms based on other loss functions.
 For carefully designed algorithms, it is possible to bound the cumulative loss on any sequence of samples, even adversarially chosen [2]. In particular, many of the recent analyses are based on the online convex optimization framework, that focuses on minimizing the sum of convex functions. Two common view-points for online convex optimization are of regularization [15] or primal-dual progress [16, 17, 13]. Recently new bounds have been proposed for time-varying regularizations in [18, 9], focusing on the general case of regression problems. The proof technique derived from our framework extends the work of Kakade et al. [13] to support time varying potential functions. We also show how the use of widely used classification losses, as the hinge loss, allows us to derive new powerful mistake bounds superior to existing bounds. Moreover the framework introduced supports the design of aggressive algorithms, i.e. algorithms that update their hypothesis not only when they make a prediction mistake.
 Finally, current second order algorithms suffer from a common problem. All these algorithms main-tain the cumulative second-moment of the input features, and its inverse, qualitatively speaking, is rithm will take more time to update its value. When the instances are ordered such that the value of corresponding to this feature to a wrong value and will decrease its associated learning rate to a low value. This combination makes it hard to recover from the wrong value set to the weight associated with this feature. Our final contribution is a new algorithm that adapts the way the second order information is used. We call this algorithm Narrow Adaptive Regularization Of Weights (NAROW). Intuitively, it interpolates its update rule from adaptive-second-order-information to fixed-second-order-information, to have a narrower decrease of the learning rate for common appearing features. We derive a bound for this algorithm and illustrate its properties using synthetic data simulations. At each round t , an instance x t  X  R d is presented to the algorithm, which then predicts a label  X  y  X  X  X  1 , +1 } . Then, the correct label y t is revealed, and the algorithm may modify its hypothesis. The aim of the online learning algorithm is to make as few mistakes as possible (on any sequence  X  y We strive to design online learning algorithms for which it is possible to prove a relative mistakes bound or a loss bound. Typical such analysis bounds the cumulative loss the algorithm suffers, P in relative mistakes bound, where we bound the number of mistakes of the learner with R ( u ) + P that can be found in hindsight given all the samples. Often R (  X  ) depends on a function measuring the complexity of u and the number of samples T , and ` is a non-negative loss function. Usually ` In the following we denote by M to be the set of round indexes for which the algorithm performed a mistake. We assume that the algorithm always update if it rules in such events. Similarly, we denote by U the set of the margin error rounds, that is, rounds in which the algorithm updates its hypothesis mistake rounds is called conservative (e.g. [3]). Following previous naming convention [3], we call zero, even if its prediction was correct.
 We define now few basic concepts from convex analysis that will be used in the paper. Given a A differentiable function f : X  X  R is  X  -strongly convex w.r.t. a norm k X k if for any u , v  X  S and turns out to be a key property to design online learning algorithms. We now introduce a general framework to design online learning algorithms and a general lemma which serves as a general tool to prove their relative regret bounds. Our algorithm builds on previous algorithms for online convex programming with a one significant difference. Instead of using a fixed summarized in Fig. 1.
 The following lemma is a generalization of Corollary 7 in [13] and Corollary 3 in [9], for online learning. All the proofs can be found in the Appendix.
 Lemma 1. Let f t ,t = 1 ,...,T be  X  t -strongly convex functions with respect to the norms k  X  k f and x 1 ,..., x T be an arbitrary sequence of vectors in R d . Assume that algorithm in Fig. 1 is run on this sequence with the functions f i . Then, for any u  X  S , and any  X  &gt; 0 we have can state the following Corollary that holds for any convex loss ` that upper bounds the 0/1 loss. Figure 1: Prediction algorithm choose the optimal tuning of  X  we should know quantities that are unknown to the learner. We could use adaptive regularization methods, as the one proposed in [16, 18], but in this way we would lose the possibility to prove mistake bounds for second order algorithms, like the ones in [1, 5]. In the next Section we show how to obtain bounds with an automatic tuning, using additional assumption-ion on the loss function. 3.1 Better bounds for linear losses fication. It has been used, for example, in Support Vector Machines [7] as well as in many online learning algorithms [3]. It has also been extended to the multiclass case [3]. Often mistake bounds satisfies the following condition Thanks to this condition we can state the following Corollary for any loss satisfying (2). any u  X  S , and any  X  &gt; 0 we have optimal  X  , we obtain The intuition and motivation behind this Corollary is that a classification algorithm should be inde-exactly the same predictions, because only the sign of the prediction matters. Exactly this indepen-dence in a scale factor allows us to improve the mistake bound (1) to the bound of (3). Hence, when (2) holds, the update of the algorithm becomes somehow independent from the scale factor, and we an aggressive version of the Perceptron algorithm, with a possible variable learning rate. We now show the versatility of our framework, proving better bounds for some known first order and second order algorithms. 4.1 An Aggressive p-norm Algorithm We can use the algorithm in Fig. 1 to obtain an aggressive version of the p-norm algorithm [11]. Set f k X k p , where 1 /p + 1 /q = 1 . Moreover set  X  t = 1 in mistake error rounds, so using the second bound of Corollary 2, and defining R such that k x t k 2 p  X  R 2 , we have Solving for M we have freedom to set  X  t in margin error rounds. If we set  X  t = 0 , the algorithm of Fig. 1 becomes the bound, thanks to last term that is subtracted to the bound.
 In the particular case of p = q = 2 we recover the Perceptron algorithm. In particular the minimum to  X  supporting the aggressive updates versus the conservative ones. 4.2 Second Order Algorithms Figure 2: NLP Data: the number of words vs. the word-rank on two sen-timent data sets.
 identity we have f  X  t (  X  t )  X  f  X  t  X  1 (  X  t ) =  X  ( x in Corollary 2, and setting  X  t = 1 we have
M + U  X  L + p u &gt; A T u This bound recovers the SOP X  X  one in the conservative case, and improves slightly the one of AROW for the aggressive case. It would be possible to improve the AROW bound even more, setting  X  t to a 4.3 Diagonal updates for AROW Both CW and AROW has an efficient version that use diagonal matrices instead of full ones. In this case the complexity of the algorithm becomes linear in dimension. Here we prove a mistake bound for the diagonal version of AROW, using Corollary 2. We denote D t = diag { A t } , where A t is in Corollary 2 and Lemma 12 in [9], we have 1
M + U  X  X The presence of a mistake bound allows us to theoretically analyze the cases where this algorithm could be advantageous respect to a simple Perceptron. In particular, for NLP data the features are these  X  X are X  features are usually the most informative ones (e.g. [8]). Fig. 2 shows the number of times each feature (word) appears in two sentiment datasets vs the word rank. Clearly there are few very frequent words and many rate words. These exact properties were used to originally derive the CW algorithm. Our analysis justifies this derivation. Concretely, the above considerations leads us to think that the optimal hyperplane u will be such that where I is the set of the informative and rare features and s is the maximum number of times these s small enough, it is possible to show that, with an optimal tuning of r , this bound is better of the this algorithm, it is enough to have s &lt; MR 2 2 d , and to set r = sMR 2 MR 2  X  2 sd . We now introduce a new algorithm with an update rule that interpolates from adaptive-second-order-information to fixed-second-order-information. We start from the first bound in Corollary 2. We set f and set  X  t = 1 . With this choices, we obtain the bound
M + U  X  X minimize the bound, in particular to have a small value of the sum  X  ( u &gt; x t ) 2 r when  X  t  X  1 b and r t = +  X  otherwise. Here b is a parameter. With this choice we have that where in the last inequality we used an extension of Lemma 4 in [5] to varying values of r t . Tuning  X  we have M + U  X  X This algorithm interpolates between a second order algorithm with adaptive second order informa-tion, like AROW, and one with a fixed second order information. Even the bound is in between from growing too much, as in AROW/SOP. We thus call this algorithm NAROW, since its is a new adaptive algorithm, which narrows the range of possible eigenvalues of the matrix A t . We illustrate empirically its properties in the next section. similar manner of previous work [4]. We repeat its properties for completeness. We generated 5 , 000 points in R 20 where the first two coordinates were drawn from a 45  X  rotated Gaussian distribution with standard deviation 1 and 10 . The remaining 18 coordinates were drawn from independent Gaussian distributions N (0 , 8 . 5) . Each point X  X  label depended on the first two coordinates using ordered the training set in four different ways: from easy examples to hard examples (measured by the signed distance to the separating-hyperplane), from hard examples to easy examples, ordered by x  X  y for i = 1 and i = 3 -respectively. An illustration of these ordering appears in the top row of Fig. 3, the colors code the ordering of points from blue via yellow to red (last points). We evaluated four algorithms: version I of the passive-aggressive (PA-I) algorithm [3], AROW [5], AdaGrad [9] and NAROW. All algorithms, except AdaGrad, have one parameter to be tuned, while AdaGrad has two. These parameters were chosen on a single random set, and the plots summarizes the results averaged over 100 repetitions.
 The second row of Fig. 3 summarizes the cumulative number of mistakes averaged over 100 repe-titions and the third row shows the cumulative number of mistakes where 10% artificial label noise was used. (Mistakes are counted using the unnoisy labels.) Focusing on the left plot, we observe that all the second order algorithms outperform the single first order algorithm -PA-I. All algorithms make few mistakes when receiving the first half of the data -the easy examples. Then all algorithms start to make more mistakes -PA-I the most, then AdaGrad and closely following NAROW, and AROW the least. In other words, AROW was able to converge faster to the target separating hyperplane just using  X  X asy X  examples which are far from the separating hyperplane, then NAROW and AdaGrad, with PA-I being the worst in this aspect. The second plot from the left, showing the results for ordering the examples from hard to easy. All algorithms follow a general trend of making mistakes in a linear rate and then stop making mistakes when the data is easy and there are many possible classifiers that can predict correctly. Clearly, AROW and NAROW stop making mistakes first, then AdaGrad and PA-I last. A similar trend can be found in the noisy dataset, with each algorithm making relatively more mistakes.
 The third and fourth columns tell a similar story, although the plots in the third column summarize results when the instances are ordered using the first feature (which is informative with the second) and the plots in the fourth column summarize when the instances are ordered using the third unin-formative feature. In both cases, all algorithms do not make many mistakes in the beginning, then at some point, close to the middle of the input sequence, they start making many mistakes for a while, and then they converge. In terms of total performance: PA-I makes more mistakes, then AdaGrad, AROW and NAROW. However, NAROW starts to make many mistakes before the other algorithms and takes more  X  X xamples X  to converge until it stopped making mistakes. This phenomena is further shown in the bottom plots where label noise is injected.
 We hypothesize that this relation is due to the fact that NAROW does not let the eigenvalues of the matrix A to grow unbounded. Since its inverse is proportional to the effective learning rate, it means that it does not allow the learning rate to drop too low as opposed to AROW and even to some extent AdaGrad. hinge loss. This general tool allows to design theoretical motivated online classification algorithms Our framework also provided a missing bound for AROW for diagonal matrices. We have shown its utility proving better bounds for known online algorithms, and proposing a new algorithm, called NAROW. This is a hybrid between adaptive second order algorithms, like AROW and SOP, and a static second order one. We have validated it using synthetic datasets, showing its robustness to the malicious orderings of the sample, comparing it with other state-of-art algorithms. Future work will focus on exploring the new possibilities offered by our framework and on testing NAROW on real world data.
 P f u  X  where we used the definition of w t in Algorithm 1.
 in Lemma 1 we have the stated bound. For the additional statement, using Lemma 12 in [16] and f f t +1 ( x ) , so we have that B  X  0 .
 Proof of Corollary 2. Lemma 1, the condition on the loss (2), and the hypothesis on f T gives us Note that  X  is free, so choosing its optimal value we get the second bound. [1] N. Cesa-Bianchi, A. Conconi, and C. Gentile. A second-order Perceptron algorithm. SIAM [2] N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games . Cambridge University Press, [3] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. Online passive-aggressive [4] K. Crammer, M. Dredze, and F. Pereira. Exact Convex Confidence-Weighted learning. Ad-[5] K. Crammer, A. Kulesza, and M. Dredze. Adaptive regularization of weight vectors. Advances [6] K. Crammer and Y. Singer. Ultraconservative online algorithms for multiclass problems. Jour-[7] N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines and Other [8] M. Dredze, K. Crammer, and F. Pereira. Online Confidence-Weighted learning. Proceedings [10] Y. Freund and R. E. Schapire. Large margin classification using the Perceptron algorithm. [11] C. Gentile. The robustness of the p-norm algorithms. Machine Learning , 53(3):265 X 299, 2003. [12] E. Hazan and S. Kale. Extracting certainty from uncertainty: Regret bounded by variation in [13] S. Kakade, S. Shalev-Shwartz, and A. Tewari. On the duality of strong convexity and strong [14] J. Kivinen, A. Smola, and R. Williamson. Online learning with kernels. IEEE Trans. on Signal [15] A. Rakhlin and A. Tewari. Lecture notes on online learning. Technical report, 2008. Avail-[16] S. Shalev-Shwartz. Online learning: Theory, algorithms, and applications. Technical report, [17] S. Shalev-Shwartz and Y. Singer. A primal-dual perspective of online learning algorithms. [18] L. Xiao. Dual averaging method for regularized stochastic learning and online optimization.
