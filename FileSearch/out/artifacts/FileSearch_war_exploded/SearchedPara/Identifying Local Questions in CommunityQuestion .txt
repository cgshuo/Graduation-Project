 Community Question Answering (CQA) services, such as Yahoo! Answers 1 , Baidu Zhidao 2 ,Quora 3 , Facebook Questions 4 , have been booming recently. Such platforms enable users to ask and answer questions in natural language, and thus satisfy their information need with pertinent user-generated content tailored to their complex intent.

In Broder X  X  [1] seminal work, user X  X  intent is defined as  X  X he need behind the query X . Unfortunately, most CQA portal s do not consider user X  X  locality intent, therefore user X  X  information need is not satisfied geographically. For example, at the time of writing this paper, the question  X  X hats the best restaurant to watch fireworks from in Hongkong? X  attracts only one response from Yahoo!Answers, leaving a large margin space for the system to attract more Hongkong based users to answer it.

To shed light on user X  X  locality intent, we propose to classify questions into two categories according to the locality intent: local and global. By considering the question, for instance,  X  X hat X  X  the best restaurant to watch fireworks from in Hongkong? X  as a local one, a CQA system can route the question directly to some specific local answerers by identifying the corresponding spatial scope. On the other hand, by identifying  X  X here is a good place I can chat to people about money making ideas? X  as a global question, we can highlight the question on the home page to attract more people to answer it, regardless of their locality background. After performing the classification over local and global, we further pinpointed spatial scope of the question by analyzing its thematic features so as to enable the search radius to vary dep ending on user X  X  information need. For example, users querying for a coffee shop are probably looking for one within walking distance. If they are consulting the local tax rate, they will expect a dis-tance of vicinity council. If they want to buy cheap ticket for travelling, however, distance may not be important as tickets can be bought from the Internet. CQA systems can then automatically pinpoint the specific locality scope by combining one X  X  GPS and the spatial scope of the topic. This is a tempting scenario for mo-bile environment: one can ask local question without explicitly mentioning their current location and intended search rad ius, producing a significantly enhanced user-experience in terms of simplicity and flexibility.

In this paper, we build a predictive model through machine learning based on both text and metadata features to identify the locality intent. Our investigation reveals that the Probability Estimation model achieves a superior performance than S-EM and Biased-SVM  X  two state-of-the-arts PU learning models. In addition to revealing the general locality intent, the spatial scope of the question is also further targeted and exploited. Our experiment shows that the F 1 score of 0.738 and 0.754 can be achieved on Yahoo!Answers and WikiAnswers datasets respectively.

The rest of this paper is organised as follows. In Section 2, we review the related work. In Section 3, we define our taxonomy of user X  X  locality intent in CQA. In Section 4, we introduce the PU approach to question classification with only positive and unlabelled examples. In Section 5, we describe the exper-imental setup and present our findings. In Section 6, we conclude our work and contributions. The problem of understanding user X  X  locality intent is first proposed in the con-text of Web search engines. Luis et al. [4] classify the locality intent of Web search queries into two categories: global and local. However, this taxonomy is not that suitable for CQA, because web search engines aim to retrieve the most relevant web pages while CQA services strive to find the most appropriate people with the matching knowledge.

Tom et al. [12] proposed a classification-based approach for question routing, which directs questions to answerers who are most likely to provide answers. They propose to use local and global features to enhance classifier X  X  performance. Baichuan et al. [5] provide a question routing frame work, which comprehensively considers user X  X  expertise, availability and answerer rank by having these features integrated into a single language model. These works are somewhat similar to our motivations, but none of them leverage user X  X  geographical information to make inference.

With regard to the task of semi-supervised learning in CQA, our previous work [2] has already revealed that unlabelled questions can be made useful to improve the performance of question classification. In that work, we employ a Co-training framework to identify subj ective and social questions in CQA. But different from the Co-training framework in which both positive and negative labelled examples are compulsory for training, in this task we take advantage of the PU learning framework that only requires positive ones to use. To the best of our knowledge, this is the first CQA work that integrates PU learning framework in the designing of the system. Taking into account the special locality c haracteristics of CQA, we propose the following taxonomy that classifies questions into two categories in terms of their underlying geographical locality: local and global.
 Local Questions. The intent of such questions is to get information regarding Global Questions. The intent of such questions is to get information irrespec-Notice that our taxonomy is a two-level hierarchy, in which local category are further broken down into subcategories to pinpoint question X  X  spatial scope. We inherit the administrative place types of Yahoo! Placemaker namely, Country, State, County, Town, and Local Administrative Area to further break down local questions into the second level of spacial scope. More-detailed information re-garding different Places vs. Place Names can be found at the Yahoo! Placemaker Key Concepts page 5 . In the locality classification task, dozens of local questions can be automatically detected from location-based categories. For example, in the Dining Out category of Yahoo! Answers, questions have been br oken down into cities subcategories scattered around the world. On the other hand, however, it X  X  impractical to label large amounts of global examples manually. Traditional supervised learning models are thus not helpful to the building of an automated training model  X  they require training in both local and global examples. Naturally we think PU learning framework can fit in to this context quite well.

Basically, PU learning is a semi-supervised learning framework, which builds a classifier with only positive and unlabelled training examples, to predict both positive and negative examples in unlabelled or test dataset. A short introduc-tion, which describes PU learning models, is given in the following sub-sections. 4.1 S-EM S-EM model is first proposed in [7] and can break down into two steps. The first step is to identify reliable negative examples from the unlabelled set U ,which works by sending some spy examples from the positive set P to the U .The reliable negative examples is found through multiple iterations by running the first step couple of times. The second step is to use EM algorithm to build the final classifier. But EM algorithm makes some mixture model assumptions [9] of the datasets which can not be guaranteed to take hold, and thus it is often suffered from the problem of mismatch. 4.2 Biased-SVM The Biased-SVM [6] approach modifies the SVM formulation to make it fit in to the setting of PU learning, which can be described in the following SVM reformulations.
 In the Equation 1, x i is the input vector of the training example and y i is its class label, y i  X  X  1 ,  X  1 } . The first k  X  1 examples are positive examples labelled 1, while the rest are unlabelled examples, which are treated as negative labelled -1. C + and C  X  are parameters to weight positive errors and negative errors differently. We give a bigger value for C + and a smaller value for C  X  because unlabelled examples, which assumed as ne gative, contains positive examples. The C + and C  X  values are chosen by using a separate validation set to verify the performance of the resulting classifier. 4.3 Probability Estimation Probability Estimation approach is famous for its prominent accuracy and dis-tinctive computational simplicity. This approach is first proposed in [3] and utilizes some probabilistic formulas.

Denote x an example and y the binary label (local and global); let local questions be positive examples and global questions be negative ones. Let s =1 if the example x is labelled, and s = 0 if otherwise. Thus, the condition that only positive examples are labelled can be described as: The formula informs us that when y =  X  1, the probability of x being labelled is zero. So the objective now is to learn the classification function f ( x )= Pr ( y = 1 | x ). To start with, selected completely at random assumption has to be satisfied: the labelled positive examples are chosen completely at random from all the positive examples, and thus, The training set consists of two parts: the labelled dataset P (when s =1)and the unlabelled dataset U (when s =0).Let g ( x )= Pr ( s =1 | x ) be the function that estimates the probability of an example being labelled, f ( x )= Pr ( y =1 | x ) be the function that estimates the probability of an example belonging to positive category. Then the following lemma shows how to derive f ( x )from g ( x ) . Lemma 1: suppose the  X  X elected completely at random X  assumption holds. Consequently, The above equation suggests that we can attain a positive-negative classifier (this is exactly what we need) by having a positive-unlabelled classifier divided by c  X  the probability that a random positive example being labelled. Notice that in Equation (4), c = Pr ( s =1 | y = 1) is a constant that represents the probability of positive examples being labelled. So the problem now lies in how to estimate the constant c by using a trained classifier g and a validation dataset. Three estima-and e 3 = max x  X  V g ( x ). In the above formulas, V is the validation datasets, P consists of all the labelled examples of V , n is the cardinality of P . 5.1 Dataset The Yahoo! Answers dataset is derived from Yahoo! Answers Comprehensive Questions and Answers (v1.0), a dataset kindly provided by Yahoo Research Group 6 . It consists of 4483032 questions and respect answers from 2005/01/01 to 2006/01/01. An unlabelled set and test set are randomly selected across all 26 Yahoo main categories. Note that as we leverage a PU learning framework in our task, the training set will only involve local questions. The training set is automatically extracted from Dining Out, Travel, and Local Business categories with questions of a city name being assi gned as the subcategory, whereas test set is manually labelled for both local and global examples.

The WikiAnswers dataset is collected from WikiAnswer 7 dating from 2012/01/01 to 2012/05/01 contains a total of 824320 questions (note that this is only a subset and cannot cover all the questions during that period of time). All the local questions are derived fr om WikiAnswers Local category as we find this is the only category in WikiAnswers that completely devoted to locality intent. We present the detailed statistics regarding the test and training sets, and validation set in Table 1. Acronym YA and WA represent Yahoo! Answers and WikiAnswers respectively.

With respect to the second-level classification, we use the same dataset by se-lecting all the questions containing at least one location reference(which is tagged by using Yahoo! Placemaker). There are 324537 and 12401 such questions avail-able in Yahoo! Answers and WikiAnswres d atasets, respectiv ely, which directly serves as the second-level datasets for classification. What X  X  more, all the location references in the training set are hidden to emulate the scenario when mobile users forget to type in the specific localities. 5.2 Performance Measure Since the class sizes are imbalanced in this problem, we use the F 1 score [8] instead of accuracy to measure the performance of question classification. The F 1 score is the harmonic mean of precision P and recall R : F 1 = 2 PR P + R ,where micro-averaged F 1 (mi F 1 ) and macro-averaged F 1 (ma F 1 ) [11] will be reported in the next section. The former carries out averaging over all test questions while the latter over all question categories, therefore the former is dominated by performance on major question categories while the latter treats all question categories equally. 5.3 Experimental Results A number of machine learning algorithms implemented in Weka 8 , including C4.5, Random Forest, Naive Bayes, k-Nearest-Neighbours, and Linear Support Vector Machine (SVM), have been tested for semi-supervised learning (PU learning). What we find is that SVM can constantly outperform other schemes so we use it as the basic learning scheme in the following subsections.
 Semi-Supervised Learning. We exploit several locality features that can help detecting the locality intent within the qu estion, namely locat ion frequency and location level, in addition to the textual features. But the original question datasets are not geographically annotated and contain no locality information. In order to extract location references and assign geographical scope to each question, Yahoo! PlaceMaker is employed to equipped original datasets with the location-specific explanation. There are two versions of scopes available in Place-maker, namely the geographical scope and the administrative scope. Geographic Scope is the place that best describes the document and may be of any place type. Administrative Scope is the place that best describes the document and has an administrative place type (which refers to Country, State, County, Local Administrative Area and Town). We use the geographical scope in this paper because we find this version provide more details than administrative scope 9 .
Figure 1 is the learning curve of PU lea rning schemes given a varying number of positive labelled examples (the unlabelled examples are fixed at 5000 ). We employ the S-EM scheme to serve as baseline and the Biased-SVM as the state-of-the-art. As far as we can tell in the mi F 1 figures, two datasets have a similar result, in which Probability Estimation and Biased-SVM perform significantly better than S-EM given sufficient amounts of labelled examples, but the gap starts to decrease when we shrink the lab elled size. All three approaches give a comparable performance when providing only 500 labelled examples or less.
As for ma F 1 figure over the YA dataset, Probability Estimation can consis-tently outperform the other two schemes  X  an approximately 23% error reduction on the basis of Biased SVM  X  irrespective of the labelled data size; At the same time, Biased-SVM is slightly better than S-EM approach with an average 2% improvement. The result gener ated on the WA dataset for ma F 1 is quite similar and we do not mention it here. We propose that the probability approach can overwhelm the other two due to the maldistributional nature of the test set: 20% positive examples vs. 80% negative ones. In Probability Estimation model, having the non-traditional classifier divided by a constant c enables the clas-sifier to be more tolerant towards the positive classifying errors by sacrificing some negative examples  X  we believe that is why Probability Estimation, in some cases, is even slightly worse than Biased SVM under mi F 1  X  producing a superior result for ma F 1 by picking up the minority class in general. 5.4 Predicting Spatial Scope We use the SVM implemented by Platt et al. [10] with a probabilistic output and adopt a linear kernel in this task  X  we find that linear kernel generally outperform non-linear ones. We use 5-fold cross-validation to get a good value of C and  X  for predicting the spatial scope. What we found is that choosing C with the range of 1 &lt;C&lt; 2 is good for all configurations across different datasets. Similarly,  X  between 1 e  X  8to1 e  X  12 can be regarded as a good value. The classification finally keep the default values of Weka with C =1and  X  =1 e  X  12 , as we find this combination can keep producing stable results.

Table 2 gives the result of the ma F 1 and mi F 1 comparison over each scope level. Under the evaluation of ma F 1 , the prediction on country, town and state scopes have a superior performance than the rest, this suggest that these three scopes are relatively easier to identify by inferring the question X  X  topic (for both Yahoo! Answers and WikiAnswers). But the system only display a mediocre per-formance regarding county and local administrative area scopes, which leads to our speculation that the questions in a higher scope level may have more dis-criminative power than questions in lower scope level. This is quite explainable: questions with a larger scope tend to hav e generalization b ehaviour whereas questions with smaller scope are liable to have uniqueness behaviour. Under the evaluation of mi F 1 , the performance over Yahoo! Answers and WikiAnswers are 0.738 and 0.754 respectively, which suggests that majority of the local questions X  scope can be accurately predicted even if user does not mention the place names. The main contribution of this paper is threefold. First, we identify several meta-data features which can be used together with standard text features by machine learning algorithms to classify questions according to their geographical locality. Second, we prove that Probability Estimation approach can consistently outper-form the S-EM and Biased-SVM on the evaluation of ma F 1 and mi F 1 .Third,we prove that the spatial scope of a local qu estion can be inferred accurately even if it does not mention any place name.
 Acknowledgements. We are grateful to anonymous reviewers for their scrupu-lous review and constructive criticism, improving the quality of this work. We would like to thank Martyn Harris and Tom Ue for their meticulous proofreading and beneficial discussions.

