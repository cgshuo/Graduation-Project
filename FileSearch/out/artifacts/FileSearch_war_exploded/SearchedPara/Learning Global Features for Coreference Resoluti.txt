 While structured, non-local coreference models would seem to hold promise for avoiding many com-mon coreference errors (as discussed further in Sec-tion 3), the results of employing such models in practice are decidedly mixed, and state-of-the-art results can be obtained using a completely local, mention-ranking system.

In this work, we posit that global context is indeed necessary for further improvements in coreference resolution, but argue that informative cluster , rather than mention, level features are very difficult to de-vise, limiting their effectiveness. Accordingly, we instead propose to learn representations of mention clusters by embedding them sequentially using a re-current neural network (shown in Section 4). Our model has no manually defined cluster features, but instead learns a global representation from the indi-vidual mentions present in each cluster. We incor-porate these representations into a mention-ranking style coreference system.

The entire model, including the recurrent neu-ral network and the mention-ranking sub-system, is trained end-to-end on the coreference task. We train the model as a local classifier with fixed context (that is, as a history-based model). As such, unlike several recent approaches, which may require complicated inference during training, we are able to train our model in much the same way as a vanilla mention-ranking model.

Experiments compare the use of learned global features to several strong baseline systems for coref-erence resolution. We demonstrate that the learned global representations capture important underlying information that can help resolve difficult pronom-inal mentions, which remain a persistent source of errors for modern coreference systems (Durrett and Klein, 2013; Kummerfeld and Klein, 2013; Wise-man et al., 2015; Martschat and Strube, 2015). Our final system improves over 0.8 points in CoNLL score over the current state of the art, and the im-provement is statistically significant on all three CoNLL metrics. Coreference resolution is fundamentally a clustering task. Given a sequence ( x n ) N n =1 of (intra-document) mentions  X  that is, syntactic units that can refer or be referred to  X  coreference resolution involves parti-such that all the mentions in any particular cluster mentions within a particular cluster may be ordered will use the notation X ( m ) tion in the m  X  X h cluster.

A valid clustering places each mention in exactly one cluster, and so we may represent a clustering with a vector z  X  X  1 ,...,M } N , where z n = m iff x tempt to find the best clustering z  X   X  X  under some scoring function, with Z the set of valid clusterings.
One strategy to avoid the computational in-tractability associated with predicting an entire clus-tering z is to instead predict a single antecedent for each mention x n ; because x n may not be anaphoric (and therefore have no antecedents), a  X  X ummy X  an-tecedent may also be predicted. The aforemen-tioned strategy is adopted by  X  X ention-ranking X  sys-tems (Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013), which, formally, predict an antecedent  X  y  X  X  ( x n ) for each mention x , where Y ( x n ) = { 1 ,...,n  X  1 , } . Through tran-sitivity, these decisions induce a clustering over the document.

Mention-ranking systems make their antecedent predictions with a local scoring function f ( x n ,y ) defined for any mention x n and any antecedent y  X  X  ( x n ) . While such a scoring function clearly ignores much structural information, the mention-ranking approach has been attractive for at least two reasons. First, inference is relatively simple and ef-ficient, requiring only a left-to-right pass through a document X  X  mentions during which a mention X  X  an-tecedents (as well as ) are scored and the highest scoring antecedent is predicted. Second, from a lin-guistic modeling perspective, mention-ranking mod-els learn a scoring function that requires a mention x n to be compatible with only one of its coreferent antecedents. This contrasts with mention-pair mod-els (e.g., Bengtson and Roth (2008)), which score all pairs of mentions in a cluster, as well as with cer-tain cluster-based models (see discussion in Culotta et al. (2007)). Modeling each mention as having a single antecedent is particularly advantageous for pronominal mentions, which we might like to model as linking to a single nominal or proper antecedent, for example, but not necessarily to all other corefer-ent mentions.

Accordingly, in this paper we attempt to maintain the inferential simplicity and modeling benefits of mention ranking, while allowing the model to utilize global, structural information relating to z in mak-ing its predictions. We therefore investigate objec-tive functions of the form where g is a global function that, in making pre-dictions for x n , may examine (features of) the clus-tering z 1: n  X  1 induced by the antecedent predictions made through y n  X  1 . Here we motivate the use of global features for coreference resolution by focusing on the issues that may arise when resolving pronominal mentions in a purely local way. See Clark and Manning (2015) and Stoyanov and Eisner (2012) for more general motivation for using global models. 3.1 Pronoun Problems Recent empirical work has shown that the resolu-tion of pronominal mentions accounts for a substan-tial percentage of the total errors made by modern mention-ranking systems. Wiseman et al. (2015) show that on the CoNLL 2012 English development set, almost 59% of mention-ranking precision errors and almost 24% of recall errors involve pronominal mentions. Martschat and Strube (2015) found a sim-ilar pattern in their comparison of mention-ranking, mention-pair, and latent-tree models.

To see why pronouns can be so problematic, con-sider the following passage from the  X  X roadcast Conversation X  portion of the CoNLL development set (bc/msnbc/0000/018); below, we enclose men-tions in brackets and give the same subscript to co-clustered mentions. (This example is also shown in Figure 2.) This example is typical of Broadcast Conversation, and it is difficult because local systems learn to my-opically link pronouns such as [you] 5 to other in-stances of the same pronoun that are close by, such as [you] 1 . While this is often a reasonable strategy, in this case predicting [you] 1 to be an antecedent of [you] 5 would result in the prediction of an incoher-ent cluster, since [you] 1 is coreferent with the singu-lar [I] 1 , and [you] 5 , as part of the phrase  X  X ll of you, X  is evidently plural. Thus, while there is enough in-formation in the text to correctly predict [you] 5 , do-ing so crucially depends on having access to the his-tory of predictions made so far, and it is precisely this access to history that local models lack.
More empirically, there are non-local statistical regularities involving pronouns we might hope mod-els could exploit. For instance, in the CoNLL train-ing data over 70% of pleonastic  X  X t X  instances and over 74% of pleonastic  X  X ou X  instances follow (re-spectively) previous pleonastic  X  X t X  and  X  X ou X  in-stances. Similarly, over 78% of referential  X  X  X  in-stances and over 68% of referential  X  X e X  instances corefer with previous  X  X  X  and  X  X e X  instances, respec-tively.

Accordingly, we might expect non-local models with access to global features to perform signifi-cantly better. However, models incorporating non-local features have a rather mixed track record. For instance, Bj  X  orkelund and Kuhn (2014) found that cluster-level features improved their results, whereas Martschat and Strube (2015) found that they did not. Clark and Manning (2015) found that incorporating cluster-level features beyond those involving the pre-computed mention-pair and mention-ranking prob-abilities that form the basis of their agglomerative clustering coreference system did not improve per-formance. Furthermore, among recent, state-of-the-art systems, mention-ranking systems (which are completely local) perform at least as well as their more structured counterparts (Durrett and Klein, 2014; Clark and Manning, 2015; Wiseman et al., 2015; Peng et al., 2015). 3.2 Issues with Global Features We believe a major reason for the relative inef-fectiveness of global features in coreference prob-lems is that, as noted by Clark and Manning (2015), cluster-level features can be hard to define. Specif-ically, it is difficult to define discrete, fixed-length features on clusters, which can be of variable size (or shape). As a result, global coreference features tend to be either too coarse or too sparse. Thus, early attempts at defining cluster-level features simply ap-plied the coarse quantifier predicates all , none , most to the mention-level features defined on the men-tions (or pairs of mentions) in a cluster (Culotta et al., 2007; Rahman and Ng, 2011). For example, a cluster would have the feature  X  X ost-female=true X  if more than half the mentions (or pairs of mentions) in the cluster have a  X  X emale=true X  feature.
On the other extreme, Bj  X  orkelund and Kuhn (2014) define certain cluster-level features by con-catenating the mention-level features of a cluster X  X  constituent mentions in order of the mentions X  ap-pearance in the document. For example, if a clus-ter consists, in order, of the mentions ( the president , he , he ), they would define a cluster-level  X  X ype X  fea-ture  X  X -P-P=true X , which indicates that the cluster is composed, in order, of a common noun, a pronoun, and a pronoun. While very expressive, these con-catenated features are often quite sparse, since clus-ters encountered during training can be of any size. To circumvent the aforementioned issues with defin-ing global features, we propose to learn cluster-level feature representations implicitly, by identifying the state of a (partial) cluster with the hidden state of an RNN that has consumed the sequence of men-tions composing the (partial) cluster. Before pro-viding technical details, we provide some prelimi-nary evidence that such learned representations cap-ture important contextual information by display-ing in Figure 1 the learned final states of all clus-ters in the CoNLL development set, projected using T-SNE (van der Maaten and Hinton, 2012). Each point in the visualization represents the learned fea-tures for an entity cluster and the head words of mentions are shown for representative points. Note that the model learns to roughly separate clusters by simple distinctions such as predominant type (nom-inal, proper, pronominal) and number (it, they, etc), but also captures more subtle relationships such as grouping geographic terms and long strings of pro-nouns. 4.1 Recurrent Neural Networks A recurrent neural network is a parameterized non-linear function RNN that recursively maps an in-put sequence of vectors to a sequence of hidden states. Let ( m j ) J j =1 be a sequence of J input vec-tors m j  X  R D , and let h 0 = 0 . Applying an RNN to any such sequence yields where  X  is the set of parameters for the model, which are shared over time.

There are several varieties of RNN, but by far the most commonly used in natural-language pro-cessing is the Long Short-Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), par-ticularly for language modeling (e.g., Zaremba et al. (2014)) and machine translation (e.g., Sutskever et al. (2014)), and we use LSTMs in all experiments. 4.2 RNNs for Cluster Features Our main contribution will be to utilize RNNs to produce feature representations of entity clusters which will provide the basis of the global term g . mentions ( X ( m ) der). We therefore propose to embed the state(s) of
In order to run an RNN over the mentions we need an embedding function h c to map a mention to a real vector. First, following Wiseman et al. (2015) define  X  a ( x n ) : X  X  X  0 , 1 } F as a standard set of local in-dicator features on a mention, such as its head word, its gender, and so on. (We elaborate on features be-low.) We then use a non-linear feature embedding h c to map a mention x n to a vector-space represen-tation. In particular, we define where W c and b c are parameters of the embedding.
We will refer to the j  X  X h hidden state of the RNN cording to the following formula again assuming that h ( m ) fectively run an RNN over each (sequence of men-ment, and thereby generate a hidden state h ( m ) responding to each step of each cluster in the docu-ment. Concretely, this can be implemented by main-taining M RNNs  X  one for each cluster  X  that all share the parameters  X  . The process is illustrated in the top portion of Figure 2. We now describe how the RNN defined above is used within an end-to-end coreference system. 5.1 Full Model and Training Recall that our inference objective is to maximize the score of both a local mention ranking term as well as a global term based on the current clusters: We begin by defining the local model f ( x n ,y ) with the two layer neural network of Wiseman et al. (2015), which has a specialization for the non-anaphoric case, as follows: Above, u and v are the parameters of the model, and h a and h p are learned feature embeddings of the local mention context and the pairwise affinity between a mention and an antecedent, respectively. These feature embeddings are defined similarly to h , as where  X  a (mentioned above) and  X  p are  X  X aw X  (that is, unconjoined) features on the context of x n and on the pairwise affinity between mentions x n and antecedent y , respectively (Wiseman et al., 2015). Note that h a and h c use the same raw features; only their weights differ.

We now specify our global scoring function g based on the history of previous decisions. Define h &lt;n as the hidden state of cluster m before a de-cision is made for x n  X  that is, h ( m ) &lt;n is the state of cluster m  X  X  RNN after it has consumed all mentions in the cluster preceding x n . We define g as where NA gives a score for assigning based on a non-linear function of all of the current hidden states: See Figure 2 for a diagram. The intuition behind the first case in g is that in considering whether y is a good antecedent for x n , we add a term to the score that examines how well x n matches with the predicting that x n is non-anaphoric, we add the NA term to the score, which examines the (sum of) the current states h ( m ) &lt;n of all clusters. This information is useful both because it allows the non-anaphoric score to incorporate information about potential an-tecedents, and because the occurrence of certain singleton-clusters often predicts the occurrence of future singleton-clusters, as noted in Section 3.
The whole system is trained end-to-end on coref-erence using backpropagation. For a given training tion to cluster, which induces an oracle clustering. While at training time we do have oracle clusters, we do not have oracle antecedents ( y ) N n =1 , so following past work we treat the oracle antecedent as latent (Yu and Joachims, 2009; Fernandes et al., 2012; Chang et al., 2013; Durrett and Klein, 2013). We train with the following slack-rescaled, margin objective: where the latent antecedent y ` n is defined as if x n is anaphoric, and is otherwise. The term  X ( x n ,  X  y ) gives different weight to different er-ror types. We use a  X  with 3 different weights (  X  1 , X  2 , X  3 ) for  X  X alse link X  ( FL ),  X  X alse new X  ( FN ), and  X  X rong link X  ( WL ) mistakes (Durrett and Klein, 2013), which correspond to predicting an antecedent when non-anaphoric, when anaphoric, and the wrong antecedent, respectively.

Note that in training we use the oracle clusters compute all the hidden states h ( m ) which makes training quite simple and efficient. This approach contrasts in particular with the work of Bj  X  orkelund and Kuhn (2014)  X  who also incor-porate global information in mention-ranking  X  in that they train against latent trees , which are not an-notated and must be searched for during training. On the other hand, training on oracle clusters leads to a mismatch between training and test, which can hurt performance. 5.2 Search When moving from a strictly local objective to one with global features, the test-time search problem becomes intractable. The local objective requires O ( n 2 ) time, whereas the full clustering problem is NP-Hard. Past work with global features has used integer linear programming solvers for exact search (Chang et al., 2013; Peng et al., 2015), or beam search with (delayed) early update training for an approximate solution (Bj  X  orkelund and Kuhn, 2014). In contrast, we simply use greedy search at test time, which also requires O ( n 2 ) time. 3 The full algorithm Algorithm 1 Greedy search with global RNNs is shown in Algorithm 1. The greedy search algo-rithm is identical to a simple mention-ranking sys-tem, with the exception of line 11, which updates the current RNN representation based on the previ-ous decision that was made, and line 4, which then uses this cluster representation as part of scoring. 6.1 Methods We run experiments on the CoNLL 2012 English shared task (Pradhan et al., 2012). The task uses the OntoNotes corpus (Hovy et al., 2006), consist-ing of 3,493 documents in various domains and for-mats. We use the experimental split provided in the shared task. For all experiments, we use the Berke-ley Coreference System (Durrett and Klein, 2013) for mention extraction and to compute features  X  a and  X  p .
 Features We use the raw B ASIC + feature sets de-scribed by Wiseman et al. (2015), with the following modifications:  X  We remove all features from  X  p that concate- X  We add true-cased head features, a current  X  We add features indicating if a mention has a  X  We add a single centered, rescaled document These modifications result in there being approx-imately 14K distinct features in  X  a and approxi-mately 28K distinct features in  X  p , which is far fewer features than has been typical in past work.
For training, we use document-size minibatches, which allows for efficient pre-computation of RNN states, and we minimize the loss described in Sec-tion 5 with AdaGrad (Duchi et al., 2011) (after clipping LSTM gradients to lie (elementwise) in (  X  10 , 10) ). We find that the initial learning rate cho-sen for AdaGrad has a significant impact on results, and we choose learning rates for each layer out of { 0 . 1 , 0 . 02 , 0 . 01 , 0 . 002 , 0 . 001 } .
In experiments, we set h a ( x n ) , h c ( x n ) , and h ( m ) to be  X  R 200 , and h p ( x n ,y )  X  R 700 . We use a single-layer LSTM (without  X  X eep-hole X  connec-tions), as implemented in the element-rnn li-brary (L  X  eonard et al., 2015). For regularization, we apply Dropout (Srivastava et al., 2014) with a rate of 0.4 before applying the linear weights u , and we also apply Dropout with a rate of 0.3 to the LSTM states before forming the dot-product scores. Following Wiseman et al. (2015) we use the cost-weights  X  =  X  0 . 5 , 1 . 2 , 1  X  in defining  X  , and we use their pre-training scheme as well. For final re-sults, we train on both training and development por-tions of the CoNLL data. Scoring uses the official CoNLL 2012 script (Pradhan et al., 2014; Luo et al., 2014). Code for our system is available at https: //github.com/swiseman/nn_coref . The system makes use of a GPU for training, and trains in about two hours. 6.2 Results In Table 1 we present our main results on the CoNLL English test set, and compare with other recent state-of-the-art systems. We see a statistically significant improvement of over 0.8 CoNLL points over the pre-vious state of the art, and the highest F 1 scores to date on all three CoNLL metrics.

We now consider in more detail the impact of global features and RNNs on performance. For these scores in Table 2 as well as errors broken down by mention type and by whether the mention is anaphoric or not in Table 3. Table 3 further parti-tions errors into FL , FN , and WL categories, which are defined in Section 5.1. We typically think of FL and WL as representing precision errors, and FN as representing recall errors.

Our experiments consider several different set-tings. First, we consider an oracle setting ( X  X NN, OH X  in tables), in which the model receives z preceding x n in the document, and is therefore not forced to rely on its own past predictions when pre-dicting x n . This provides us with an upper bound on the performance achievable with our model. Next, we consider the performance of the model under a greedy inference strategy (RNN, GH), as in Al-gorithm 1. Finally, for baselines we consider the mention-ranking system (MR) of Wiseman et al. (2015) using our updated feature-set, as well as a non-local baseline with oracle history (Avg, OH), which averages the representations h c ( x j ) for all x errors are still backpropagated through the h c repre-sentations during learning.

In Table 3 we see that the RNN improves per-formance overall, with the most dramatic improve-ments on non-anaphoric pronouns, though errors are also decreased significantly for non-anaphoric nom-inal and proper mentions that follow at least one mention with the same head. While WL errors also decrease for both these mention-categories under the RNN model, FN errors increase. Importantly, the RNN performance is significantly better than that of the Avg baseline, which barely improves over mention-ranking, even with oracle history. This sug-gests that modeling the sequence of mentions in a cluster is advantageous. We also note that while RNN performance degrades in both precision and recall when moving from the oracle history upper-bound to a greedy setting, we are still able to recover a significant portion of the possible performance im-provement. 6.3 Qualitative Analysis In this section we consider in detail the impact of the g term in the RNN scoring function on the two error categories that improve most under the RNN model (as shown in Table 3), namely, pronominal WL errors and pronominal FL errors. We consider an example from the CoNLL development set in each category on which the baseline MR model makes an error but the greedy RNN model does not.

The example in Figure 3 involves the resolution of the ambiguous pronoun  X  X is, X  which is brack-eted and in bold in the figure. Whereas the baseline MR model incorrectly predicts  X  X is X  to corefer with the closest gender-consistent antecedent  X  X ustin X   X  thus making a WL error  X  the greedy RNN model correctly predicts  X  X is X  to corefer with  X  X r. Kaye X  in the previous sentence. (Note that  X  X he official X  also refers to Mr. Kaye). To get a sense of the greedy RNN model X  X  decision-making on this example, we color the mentions the greedy RNN model has pre-dicted to corefer with  X  X r. Kaye X  in green, and the mentions it has predicted to corefer with  X  X ustin X  in blue. (Note that the model incorrectly predicts the initial  X  X  X  mentions to corefer with  X  X ustin. X ) Let-ting X (1) refer to the blue cluster, X (2) refer to the green cluster, and x n refer to the ambiguous mention  X  X is, X  we further shade each mention x j in X (1) so that its intensity corresponds to h c ( x n ) T h (1) k = j + 1 ; mentions in X (2) are shaded analogously. Thus, the shading shows how highly g scores the the initial  X  X ustin X  mentions are added to X (1) the g -score is relatively high. However, after  X  X he com-pany X  is correctly predicted to corefer with  X  X ustin, X  the score of X (1) drops, since companies are gener-ally not coreferent with pronouns like  X  X is. X 
Figure 4 shows an example (consisting of a tele-phone conversation between  X  X  X  and  X  X  X ) in which the bracketed pronoun  X  X t X  X  X  is being used pleonas-tically. Whereas the baseline MR model predicts  X  X t X  X  X  to corefer with a previous  X  X t X   X  thus mak-ing a FL error  X  the greedy RNN model does not. In Figure 4 the final mention in three preceding clusters is shaded so its intensity corresponds to the magni-tude of the gradient of the NA term in g with re-spect to that mention. This visualization resembles the  X  X aliency X  technique of Li et al. (2016), and it at-tempts to gives a sense of the contribution of a (pre-ceding) cluster in the calculation of the NA score.
We see that the potential antecedent  X  X -Bahn X  has a large gradient, but also that the initial, obvi-ously pleonastic use of  X  X t X  X  X  has a large gradient, which may suggest that earlier, easier predictions of pleonasm can inform subsequent predictions. In addition to the related work noted throughout, we add supplementary references here. Unstruc-tured approaches to coreference typically divide into mention-pair models, which classify (nearly) every pair of mentions in a document as coreferent or not (Soon et al., 2001; Ng and Cardie, 2002; Bengt-son and Roth, 2008), and mention-ranking models, which select a single antecedent for each anaphoric mention (Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013; Wiseman et al., 2015). Structured approaches typically divide between those that induce a clus-tering of mentions (McCallum and Wellner, 2003; Culotta et al., 2007; Poon and Domingos, 2008; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012; Cai and Strube, 2010), and, more recently, those that learn a latent tree of mentions (Fernandes et al., 2012; Bj  X  orkelund and Kuhn, 2014; Martschat and Strube, 2015).

There have also been structured approaches that merge the mention-ranking and mention-pair ideas in some way. For instance, Rahman and Ng (2011) rank clusters rather than mentions; Clark and Man-ning (2015) use the output of both mention-ranking and mention pair systems to learn a clustering.
The application of RNNs to modeling (the trajec-tory of) the state of a cluster is apparently novel, though it bears some similarity to the recent work of Dyer et al. (2015), who use LSTMs to embed the state of a transition based parser X  X  stack. We have presented a simple, state of the art approach to incorporating global information in an end-to-end coreference system, which obviates the need to de-fine global features, and moreover allows for simple (greedy) inference. Future work will examine im-proving recall, and more sophisticated approaches to global training.
 We gratefully acknowledge the support of a Google Research Award.
