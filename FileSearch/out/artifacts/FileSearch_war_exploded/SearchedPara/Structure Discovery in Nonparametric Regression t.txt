 David Duvenaud  X  X  dkd23@cam.ac.uk James Robert Lloyd  X  X  jrl44@cam.ac.uk Roger Grosse  X  rgrosse@mit.edu Joshua B. Tenenbaum  X  jbt@mit.edu Zoubin Ghahramani  X  zoubin@eng.cam.ac.uk Kernel-based nonparametric models, such as support vector machines and Gaussian processes ( gp s), have been one of the dominant paradigms for supervised machine learning over the last 20 years. These meth-ods depend on defining a kernel function, k ( x,x 0 which specifies how similar or correlated outputs y and y are expected to be at two inputs x and x 0 . By defin-ing the measure of similarity between inputs, the ker-nel determines the pattern of inductive generalization. Most existing techniques pose kernel learning as a (possibly high-dimensional) parameter estimation problem. Examples include learning hyperparameters (Rasmussen &amp; Williams, 2006), linear combinations of fixed kernels (Bach, 2009), and mappings from the in-put space to an embedding space (Salakhutdinov &amp; Hinton, 2008). However, to apply existing kernel learning algorithms, the user must specify the parametric form of the ker-nel, and this can require considerable expertise, as well as trial and error.
 To make kernel learning more generally applicable, we reframe the kernel learning problem as one of structure discovery, and automate the choice of kernel form. In particular, we formulate a space of kernel structures defined compositionally in terms of sums and prod-ucts of a small number of base kernel structures. This provides an expressive modeling language which con-cisely captures many widely used techniques for con-structing kernels. We focus on Gaussian process re-gression, where the kernel specifies a covariance func-tion, because the Bayesian framework is a convenient way to formalize structure discovery. Borrowing dis-crete search techniques which have proved successful in equation discovery (Todorovski &amp; Dzeroski, 1997) and unsupervised learning (Grosse et al., 2012), we auto-matically search over this space of kernel structures using marginal likelihood as the search criterion. We found that our structure discovery algorithm is able to automatically recover known structures from synthetic data as well as plausible structures for a va-riety of real-world datasets. On a variety of time series datasets, the learned kernels yield decompositions of the unknown function into interpretable components that enable accurate extrapolation beyond the range of the observations. Furthermore, the automatically discovered kernels outperform a variety of widely used kernel classes and kernel combination methods on su-pervised prediction tasks.
 While we focus on Gaussian process regression, we be-lieve our kernel search method can be extended to other supervised learning frameworks such as classi-fication or ordinal regression, or to other kinds of ker-nel architectures such as kernel SVMs. We hope that the algorithm developed in this paper will help replace the current and often opaque art of kernel engineering with a more transparent science of automated kernel construction. Gaussian process models use a kernel to define the covariance between any two function values: Cov( y,y 0 ) = k ( x,x 0 ). The kernel specifies which struc-tures are likely under the gp prior, which in turn de-termines the generalization properties of the model. In this section, we review the ways in which kernel fam-ilies 1 can be composed to express diverse priors over functions.
 There has been significant work on constructing gp kernels and analyzing their properties, summarized in Chapter 4 of (Rasmussen &amp; Williams, 2006). Com-monly used kernels families include the squared expo-nential ( SE ), periodic ( Per ), linear ( Lin ), and ratio-nal quadratic ( RQ ) (see Figure 1 and the appendix). Composing Kernels Positive semidefinite kernels (i.e. those which define valid covariance functions) are closed under addition and multiplication. This allows one to create richly structured and interpretable ker-nels from well understood base components.
 All of the base kernels we use are one-dimensional; ker-nels over multidimensional inputs are constructed by adding and multiplying kernels over individual dimen-sions. These dimensions are represented using sub-scripts, e.g. SE 2 represents an SE kernel over the second dimension of x . Summation By summing kernels, we can model the data as a superposition of independent func-tions, possibly representing different structures. Sup-pose functions f 1 ,f 2 are draw from independent gp priors, f 1  X  GP (  X  1 ,k 1 ), f 2  X  GP (  X  2 ,k 2 ). Then f := f 1 + f 2  X  GP (  X  1 +  X  2 ,k 1 + k 2 ). In time series models, sums of kernels can express su-perposition of different processes, possibly operating at different scales. In multiple dimensions, summing kernels gives additive structure over different dimen-sions, similar to generalized additive models (Hastie &amp; Tibshirani, 1990). These two kinds of structure are demonstrated in rows 2 and 4 of figure 2, respectively. Multiplication Multiplying kernels allows us to ac-count for interactions between different input dimen-sions or different notions of similarity. For instance, in multidimensional data, the multiplicative kernel SE 1  X  SE 3 represents a smoothly varying function of dimensions 1 and 3 which is not constrained to be additive. In univariate data, multiplying a kernel by SE gives a way of converting global structure to local structure. For example, Per corresponds to globally periodic structure, whereas Per  X  SE corresponds to locally periodic structure, as shown in row 1 of figure 2. Many architectures for learning complex functions, such as convolutional networks (LeCun et al., 1989) and sum-product networks (Poon &amp; Domingos, 2011), include units which compute AND-like and OR-like operations. Composite kernels can be viewed in this way too. A sum of kernels can be understood as an OR-like operation: two points are considered similar if either kernel has a high value. Similarly, multiply-ing kernels is an AND-like operation, since two points are considered similar only if both kernels have high values. Since we are applying these operations to the similarity functions rather than the regression func-tions themselves, compositions of even a few base ker-nels are able to capture complex relationships in data which do not have a simple parametric form.
 Example expressions In addition to the examples given in Figure 2, many common motifs of supervised learning can be captured using sums and products of one-dimensional base kernels: Bayesian linear regression Lin Bayesian polynomial regression Lin  X  Lin  X  ... Generalized Fourier decomposition Per + Per + ... Generalized additive models P D d =1 SE d Automatic relevance determination Q D d =1 SE d Linear trend with local deviations Lin + SE Linearly growing amplitude Lin  X  SE We use the term  X  X eneralized Fourier decomposition X  to express that the periodic functions expressible by a gp with a periodic kernel are not limited to sinusoids. As discussed above, we can construct a wide variety of kernel structures compositionally by adding and mul-tiplying a small number of base kernels. In particular, we consider the four base kernel families discussed in Section 2: SE , Per , Lin , and RQ . Any algebraic ex-pression combining these kernels using the operations + and  X  defines a kernel family, whose parameters are the concatenation of the parameters for the base kernel families.
 Our search procedure begins by proposing all base ker-nel families applied to all input dimensions. We allow the following search operators over our set of expres-sions: (1) Any subexpression S can be replaced with S + B , (2) Any subexpression S can be replaced with S X B , (3) Any base kernel B may be replaced with any other These operators can generate all possible algebraic ex-pressions. To see this, observe that if we restricted the + and  X  rules only to apply to base kernel fam-ilies, we would obtain a context-free grammar (CFG) which generates the set of algebraic expressions. How-ever, the more general versions of these rules allow more flexibility in the search procedure, which is use-ful because the CFG derivation may not be the most straightforward way to arrive at a kernel family. Our algorithm searches over this space using a greedy search: at each stage, we choose the highest scoring kernel and expand it by applying all possible operators. Our search operators are motivated by strategies re-searchers often use to construct kernels. In particular,  X  One can look for structure, e.g. periodicity, in the  X  One can start with structure, e.g. linearity, which  X  One can add features incrementally, analogous to Scoring kernel families Choosing kernel struc-tures requires a criterion for evaluating structures. We choose marginal likelihood as our criterion, since it bal-ances the fit and complexity of a model (Rasmussen &amp; Ghahramani, 2001). Conditioned on kernel parame-ters, the marginal likelihood of a gp can be computed analytically. However, to evaluate a kernel family we must integrate over kernel parameters. We approxi-mate this intractable integral with the Bayesian infor-mation criterion (Schwarz, 1978) after first optimizing to find the maximum-likelihood kernel parameters. Unfortunately, optimizing over parameters is not a convex optimization problem, and the space can have many local optima. For example, in data with pe-riodic structure, integer multiples of the true period (i.e. harmonics) are often local optima. To alleviate this difficulty, we take advantage of our search proce-dure to provide reasonable initializations: all of the parameters which were part of the previous kernel are initialized to their previous values. All parameters are then optimized using conjugate gradients, randomly restarting the newly introduced parameters. This pro-cedure is not guaranteed to find the global optimum, but it implements the commonly used heuristic of it-eratively modeling residuals. Nonparametric regression in high dimensions Nonparametric regression methods such as splines, lo-cally weighted regression, and gp regression are pop-ular because they are capable of learning arbitrary smooth functions of the data. Unfortunately, they suf-fer from the curse of dimensionality: it is very difficult for the basic versions of these methods to generalize well in more than a few dimensions. Applying non-parametric methods in high-dimensional spaces can require imposing additional structure on the model. One such structure is additivity. Generalized addi-tive models (GAM) assume the regression function is a transformed sum of functions defined on the individual els have a limited compositional form, but one which is interpretable and often generalizes well. In our gram-mar, we can capture analogous structure through sums of base kernels along different dimensions.
 It is possible to add more flexibility to additive mod-els by considering higher-order interactions between different dimensions. Additive Gaussian processes (Duvenaud et al., 2011) are a gp model whose ker-nel implicitly sums over all possible products of one-dimensional base kernels. Plate (1999) constructs a gp with a composite kernel, summing an SE kernel along each dimension, with an SE-ARD kernel (i.e. a prod-uct of SE over all dimensions). Both of these models can be expressed in our grammar.
 A closely related procedure is smoothing-splines ANOVA (Wahba, 1990; Gu, 2002). This model is a lin-ear combinations of splines along each dimension, all pairs of dimensions, and possibly higher-order com-binations. Because the number of terms to consider grows exponentially in the order, in practice, only terms of first and second order are usually considered. Semiparametric regression (e.g. Ruppert et al., 2003) attempts to combine interpretability with flexibility by building a composite model out of an interpretable, parametric part (such as linear regression) and a  X  X atch-all X  nonparametric part (such as a gp with an SE kernel). In our approach, this can be represented as a sum of SE and Lin .
 Kernel learning There is a large body of work at-tempting to construct a rich kernel through a weighted sum of base kernels (e.g. Christoudias et al., 2009; Bach, 2009). While these approaches find the optimal solution in polynomial time, speed comes at a cost: the component kernels, as well as their hyperparameters, must be specified in advance.
 Another approach to kernel learning is to learn an em-bedding of the data points. Lawrence (2005) learns an embedding of the data into a low-dimensional space, and constructs a fixed kernel structure over that space. This model is typically used in unsupervised tasks and requires an expensive integration or optimisation over potential embeddings when generalizing to test points. Salakhutdinov &amp; Hinton (2008) use a deep neural net-work to learn an embedding; this is a flexible approach to kernel learning but relies upon finding structure in the input density, p(x). Instead we focus on domains where most of the interesting structure is in f(x). Wilson &amp; Adams (2013) derive kernels of the form SE  X  cos( x  X  x 0 ), forming a basis for stationary ker-nels. These kernels share similarities with SE  X  Per but can express negative prior correlation, and could usefully be included in our grammar.
 Diosan et al. (2007) and Bing et al. (2010) learn com-posite kernels for support vector machines and rel-evance vector machines, using genetic search algo-rithms. Our work employs a Bayesian search criterion, and goes beyond this prior work by demonstrating the interpretability of the structure implied by composite kernels, and how such structure allows for extrapola-tion.
 Structure discovery There have been several at-tempts to uncover the structural form of a dataset by searching over a grammar of structures. For example, (Schmidt &amp; Lipson, 2009), (Todorovski &amp; Dzeroski, 1997) and (Washio et al., 1999) attempt to learn para-metric forms of equations to describe time series, or relations between quantities. Because we learn expres-sions describing the covariance structure rather than the functions themselves, we are able to capture struc-ture which does not have a simple parametric form. Kemp &amp; Tenenbaum (2008) learned the structural form of a graph used to model human similarity judg-ments. Examples of graphs included planes, trees, and cylinders. Some of their discrete graph structures have continous analogues in our own space; e.g. SE 1  X  SE 2 and SE 1  X  Per 2 can be seen as mapping the data to a plane and a cylinder, respectively.
 Grosse et al. (2012) performed a greedy search over a compositional model class for unsupervised learning, using a grammar and a search procedure which parallel our own. This model class contained a large number of existing unsupervised models as special cases and was able to discover such structure automatically from data. Our work is tackling a similar problem, but in a supervised setting. To investigate our method X  X  ability to discover struc-ture, we ran the kernel search on several time-series. As discussed in section 2, a gp whose kernel is a sum of kernels can be viewed as a sum of functions drawn from component gp s. This provides another method of visualizing the learned structures. In particular, all kernels in our search space can be equivalently writ-ten as sums of products of base kernels by applying distributivity. For example, We visualize the decompositions into sums of compo-nents using the formulae given in the appendix. The search was run to depth 10, using the base kernels from Section 2.
 Mauna Loa atmospheric CO 2 Using our method, we analyzed records of carbon dioxide levels recorded at the Mauna Loa observatory. Since this dataset was analyzed in detail by Rasmussen &amp; Williams (2006), we can compare the kernel chosen by our method to a kernel constructed by human experts.
 Figure 3 shows the posterior mean and variance on this dataset as the search depth increases. While the data can be smoothly interpolated by a single base kernel model, the extrapolations improve dramatically as the increased search depth allows more structure to be included.
 Figure 4 shows the final model chosen by our method, together with its decomposition into additive compo-nents. The final model exhibits both plausible ex-trapolation and interpretable components: a long-term trend, annual periodicity and medium-term devi-ations; the same components chosen by Rasmussen &amp; Williams (2006). We also plot the residuals, observing that there is little obvious structure left in the data. Airline passenger data Figure 6 shows the decom-position produced by applying our method to monthly totals of international airline passengers (Box et al., 1976). We observe similar components to the pre-vious dataset: a long term trend, annual periodicity and medium-term deviations. In addition, the com-posite kernel captures the near-linearity of the long-term trend, and the linearly growing amplitude of the annual oscillations.
 Solar irradiance Data Finally, we analyzed annual solar irradiation data from 1610 to 2011 (Lean et al., 1995). The posterior and residuals of the learned ker-nel are shown in figure 5.
 None of the models in our search space are capable of parsimoniously representing the lack of variation from 1645 to 1715. Despite this, our approach fails gracefully: the learned kernel still captures the peri-odic structure, and the quickly growing posterior vari-ance demonstrates that the model is uncertain about long term structure. We validated our method X  X  ability to recover known structure on a set of synthetic datasets. For several composite kernel expressions, we constructed synthetic data by first sampling 300 points uniformly at random, then sampling function values at those points from a gp prior. We then added i.i.d. Gaussian noise to the functions, at various signal-to-noise ratios (SNR). Table 1 lists the true kernels we used to generate the data. Subscripts indicate which dimension each kernel was applied to. Subsequent columns show the dimen-sionality D of the input space, and the kernels chosen by our search for different SNRs. Dashes -indicate that no kernel had a higher marginal likelihood than modeling the data as i.i.d. Gaussian noise.
 For the highest SNR, the method finds all relevant structure in all but one test. The reported additional linear structure is explainable by the fact that func-tions sampled from SE kernels with long length scales occasionally have near-linear trends. As the noise increases, our method generally backs off to simpler structures. In addition to the qualitative evaluation in section 5, we investigated quantitatively how our method per-forms on both extrapolation and interpolation tasks. 7.1. Extrapolation We compared the extrapolation capabilities of our model against standard baselines 2 . Dividing the air-line dataset into contiguous training and test sets, we computed the predictive mean-squared-error (MSE) of each method. We varied the size of the training set from the first 10% to the first 90% of the data. Figure 7 shows the learning curves of linear regres-sion, a variety of fixed kernel family gp models, and our method. gp models with only SE and Per ker-nels did not capture the long-term trends, since the best parameter values in terms of gp marginal like-lihood only capture short term structure. Linear re-gression approximately captured the long-term trend, but quickly plateaued in predictive performance. The more richly structured gp models ( SE + Per and SE  X  Per ) eventually captured more structure and performed better, but the full structures discovered by our search outperformed the other approaches in terms of predictive performance for all data amounts. 7.2. High-dimensional prediction To evaluate the predictive accuracy of our method in a high-dimensional setting, we extended the compari-son of (Duvenaud et al., 2011) to include our method. We performed 10 fold cross validation on 5 datasets 3 comparing 5 methods in terms of MSE and predictive likelihood. Our structure search was run up to depth 10, using the SE and RQ base kernel families. The comparison included three methods with fixed kernel families: Additive gp s, Generalized Additive Models (GAM), and a gp with a standard SE kernel using Automatic Relevance Determination ( gp SE -ARD). Also included was the related kernel-search method of Hierarchical Kernel Learning (HKL). Results are presented in table 2. Our method outper-formed the next-best method in each test, although not substantially.
 All gp hyperparameter tuning was performed by au-tomated calls to the GPML toolbox 4 ; Python code to perform all experiments is available on github 5 . Towards the goal of automating the choice of kernel family, we introduced a space of composite kernels de-fined compositionally as sums and products of a small number of base kernels. The set of models included in this space includes many standard regression models. We proposed a search procedure for this space of ker-nels which parallels the process of scientific discovery. We found that the learned structures are often capa-ble of accurate extrapolation in complex time-series datasets, and are competitive with widely used kernel classes and kernel combination methods on a variety of prediction tasks. The learned kernels often yield de-compositions of a signal into diverse and interpretable components, enabling model-checking by humans. We believe that a data-driven approach to choosing kernel structures automatically can help make nonparamet-ric regression and classification methods accessible to non-experts.
 Acknowledgements We thank Carl Rasmussen and Andrew G. Wilson for helpful discussions. This work was funded in part by NSERC, EPSRC grant EP/I036575/1, and Google.
 Kernel definitions For scalar-valued inputs, the squared exponential ( SE ), periodic ( Per ), linear (
Lin ), and rational quadratic ( RQ ) kernels are defined as follows: Posterior decomposition We can analytically de-compose a gp posterior distribution over additive com-ponents using the following identity: The conditional distribution of a Gaussian vector f 1 conditioned on its sum with another Gaussian vector f = f 1 + f 2 where f  X  X  (  X  1 , K 1 ) and f 2  X  X  (  X  2 , K 2 ) is given by Bach, F. Exploring large feature spaces with hierarchi-cal multiple kernel learning. In Advances in Neural Information Processing Systems , pp. 105 X 112. 2009. Bing, W., Wen-qiong, Z., Ling, C., and Jia-hong, L.
A GP-based kernel construction and optimization method for RVM. In International Conference on
Computer and Automation Engineering (ICCAE) , volume 4, pp. 419 X 423, 2010.
 Box, G.E.P., Jenkins, G.M., and Reinsel, G.C. Time series analysis: forecasting and control . 1976. Christoudias, M., Urtasun, R., and Darrell, T.
Bayesian localized multiple kernel learning. Tech-nical report, EECS Department, University of Cali-fornia, Berkeley , 2009.
 Diosan, L., Rogozan, A., and Pecuchet, J.P. Evolving kernel functions for SVMs by genetic programming.
In Machine Learning and Applications, 2007 , pp. 19 X 24. IEEE, 2007.
 Duvenaud, D., Nickisch, H., and Rasmussen, C.E. Ad-ditive Gaussian processes. In Advances in Neural Information Processing Systems , 2011.
 Grosse, R.B., Salakhutdinov, R., Freeman, W.T., and
Tenenbaum, J.B. Exploiting compositionality to ex-plore a large space of model structures. In Uncer-tainty in Artificial Intelligence , 2012.
 Gu, C. Smoothing spline ANOVA models . Springer Verlag, 2002. ISBN 0387953531.
 Hastie, T.J. and Tibshirani, R.J. Generalized additive models . Chapman &amp; Hall/CRC, 1990.
 Jaynes, E. T. Highly informative priors. In Proceedings of the Second International Meeting on Bayesian Statistics , 1985.
 Kemp, C. and Tenenbaum, J.B. The discovery of structural form. Proceedings of the National Academy of Sciences , 105(31):10687 X 10692, 2008. Lawrence, N. Probabilistic non-linear principal com-ponent analysis with gaussian process latent variable models. The Journal of Machine Learning Research , 6:1783 X 1816, 2005.
 Lean, J., Beer, J., and Bradley, R. Reconstruction of solar irradiance since 1610: Implications for climate change. Geophysical Research Letters , 22(23):3195 X  3198, 1995.
 LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D.
Backpropagation applied to handwritten zip code recognition. Neural Computation , 1:541 X 551, 1989. Plate, T.A. Accuracy versus interpretability in flexible modeling: Implementing a tradeoff using Gaussian process models. Behaviormetrika , 26:29 X 50, 1999. ISSN 0385-7417.
 Poon, H. and Domingos, P. Sum-product networks: a new deep architecture. In Conference on Uncer-tainty in AI , 2011.
 Rasmussen, C.E. and Ghahramani, Z. Occam X  X  razor.
In Advances in Neural Information Processing Sys-tems , 2001.
 Rasmussen, C.E. and Williams, C.K.I. Gaussian Pro-cesses for Machine Learning . The MIT Press, Cam-bridge, MA, USA, 2006.
 Ruppert, D., Wand, M.P., and Carroll, R.J. Semipara-metric regression , volume 12. Cambridge University Press, 2003.
 Salakhutdinov, R. and Hinton, G. Using deep belief nets to learn covariance kernels for Gaussian pro-cesses. Advances in Neural information processing systems , 20:1249 X 1256, 2008.
 Schmidt, M. and Lipson, H. Distilling free-form natu-ral laws from experimental data. Science , 324(5923): 81 X 85, 2009.
 Schwarz, G. Estimating the dimension of a model. The Annals of Statistics , 6(2):461 X 464, 1978.
 Todorovski, L. and Dzeroski, S. Declarative bias in equation discovery. In International Conference on Machine Learning , pp. 376 X 384, 1997.
 Wahba, G. Spline models for observational data .
Society for Industrial Mathematics, 1990. ISBN 0898712440.
 Washio, T., Motoda, H., Niwa, Y., et al. Discover-ing admissible model equations from observed data based on scale-types and identity constraints. In
International Joint Conference On Artifical Intelli-gence , volume 16, pp. 772 X 779, 1999.
 Wilson, Andrew Gordon and Adams, Ryan Prescott.
Gaussian process covariance kernels for pattern discovery and extrapolation. Technical Report
