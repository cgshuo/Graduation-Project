 The World Wide Web (WWW) provides much information about persons, and in recent years WWW search engines h ave been commonly used for learning about persons. However, ambiguity in person names (i. e. , many persons having the same name), typically causes the sea rch results of one person name to result in Web pages about several different persons.
 In this paper, the ambiguity of person name in Web pages is defined as follows. Each string appearing as a name on a Web page is a reference to a certain entity in the real world, i. e. , each name refers to an entity. The ambiguity of person name in Web pages is that person names that have the same string in many Web pages refers to different entities.

For example, if you want to know about a  X  X eorge Bush X  who is not the pres-ident but an ordinary person, many pages about the president that are returned as the search result may be a problem yo u. According to the circumstances, we may have to look once more to find Web pages about the target person among the many search results, which may be hard and time consuming work.
Hereinafter, we use a term  X  X erson name X  to mean a string indicating the name of a person.

In this paper, we propose a novel framework for person name disambiguation (i. e. , the problem of clustering Web pages about persons with the same name according to the true entities. )
Our framework is based on the following three intuitions: 1. Each person has his/her own social network. 2. There are specific compound key words that characterize him/her. 3. Each person is related to some specific topics.
 These intuitions led to our framework, which comprises the following steps.
First, we extract social networks by finding co-occurrences of person names with Named Entity extraction tools (NE taggers). Second, we measure document similarities based on occurrences of ke y compound words that are extracted by using statistics of compound nouns and their components. Third, we infer topic information from documents based on a basic topic model Unigram mixture, which is a probabilistic generative model of a document. In particular, we use Dirichlet Process Unigram Mixture (DPUM), which is an extension of unigram mixture that uses Dirichlet process. Finally, we cluster Web pages by using the above three types of features (i.e., social networks, document similarities, and documents topics. ) Among these three st eps, the first step is the one proposed in our previous work [14].

The remaining part of this paper is organized as follows. Section 2 and 3 explain the task definition and related works Sect ion 4 explains our framework. Section 5 evaluates our framework with an actua l Web document dataset. Section 6 sum-marizes our work. Our task, the disambiguation of person names appearing on Web pages, is for-malized as follows. The query (targ et person name) is referred to as q .Theset of Web pages obtained by inputting query q to a search engine is denoted by P appearance of string q on Web page d i is assumed to be s ij .Each s ij indicates the name q .Now,thesetof s ij is assumed to be S . We define function  X  : S X  X  . Function  X  is a mapping from the name appearing in the document to entities in the real world. In other words,  X  maps from a string to an entity. Our purpose is to find function  X   X  that will approximate function  X  .

The modeling above permits the same string q appearing in the same doc-ument to refer to different entities. Web pages with such properties are quite rare and dealing with them makes the system more complicated, so we decided to ignore such pages by assuming that all instances of the same string q on a such that  X  j,  X  ( S ij )= e m . This assumption means that the same name that appears multiple times on one page only refers to one entity. This results in a simpler model i.e.,  X  : P X  X  . In this research, our aim was to estimate  X  .The problem here is n (that appears in the definition of E )isnotknowninadvance. In other words, we do not know how many distinct entities have the string q . We actually estimated  X  by clustering Web pages.

Our system works as follows. Given query q , the system retrieves Web pages that have string q using a search engine and then disambiguates the reference. Finally, the system outputs a set of page clusters, each of which refers to a single entity. Several important works have tried to solve the task described in the previous section. Bagga and Baldwin [4] applied the vector space model to calculating similarity between names using only co -occurring words. Based on this, Niu et al. [13] presented an algorithm that uses information extraction results in addition to co-occurring words. Howeve r, these methods had only been tested on artificial small test data, leaving doubt concerning their suitability for practical use. Mann and Yarowsky [9] employed a clustering algorithm to generate person clusters based on extracted biographic data. However, this method was also only tested on artificial test data. Wan et al. [16] proposed a system that rebuilt search results for person names. Their system, called WebHawk, was aimed at practical use like our systems, but their task was somewhat different. Their system was designed for actual frequent queries. The algorithm of their system was specialized for English person nam e queries that consist of three words: family name, first name, and middle name. They mainly assumed queries such into consideration, which may have impr oved accuracy. However, it would not be suitable for other types of names such as those in Japanese (consisting only of a family name and given name).

As another approach to this task, Bekkerman and McCallum [5] proposed two methods of finding Web pages that refer to a particular person. Their work consists of two distinct mechanisms: the first is based on link structure and the second uses agglomerative/conglomerative double clustering. However, they focused on disambiguating an existing social network of people, which is not the case when searching for people in real situations. In addition, our experience is that the number of direct links between pages that contain the same name are fewer than expected, so information on link structures would be difficult to use to resolve our task. Although there may be indirect links (i. e. , one page can be found from another page via other pages), it is too time consuming to find them. In this section, we explain three types o f features of the proposed framework: social networks, document similarities and documents topics.
 4.1 Preprocessing We eliminate noise tokens such as HTML tags and stop words. We extract local texts that appear within 100 words before and after the target person name (query). In this study, our analysis is limited to the local texts. 4.2 Extraction of Social Networks This section explains how to extract social networks and to cluster pages by using social networks. This method was used in our previous work[14].
We use graph representation of relations between documents. Let G be an undirected graph with vertex set V and edge set E .Eachvertex v i  X  V corre-sponds to page d i . Then, edge e ij represents that d i and d j refer to the same entity.

On the other hand, social networks can be seen as another graph structure in which each node represents an entity, each edge represents that the fact that two entities have a relation, and each co nnected component represents one social network. We assume that every pair of entities that appears in the same page have a relation. We also assume that the same name in the same social network refers to the same entity.

In graph G ,wemakeedge e ij if the same person name m (other than q ) appears in both of d i and d j because, roughly speaking, this means that both of d i and d j are related to m (i.e., both are in the same social network which m belongs to. ) 1 Moreover, we utilize the place names and organization names that appear near the position of the target person name to extract more information of social networks. the place names and organization names can be discriminating as well as person names around the target parson name. To identify person, place, and organization names, we used CaboCha 2 as an NE tagger which tags each proper noun according to context , such as person name, place name, or organization name.
 The clustering algorithm by Soci al Networks is presented below.

This is where  X  and  X  are parameters for weighting and  X  SN is a threshold. In this study,  X  and  X  are constrained as  X &gt;&gt; X  . The constraint says that person names are more important than other names.

 X  ( d x )=  X  ( d y ) means two pages, d x and d y , are to be in the same cluster and clustering is done as follows. Let G be an undirected graph with vertex set V and edge set E .Eachvertex v i  X  V corresponds to page d i .Theresultof the above procedure gives edge set E .Eachedge e ij  X  E exists if and only if constraint  X  ( d i )=  X  ( d j ) was added in Step 3 of the above algorithm. Then, graph G = V, E has some connected components . Each connected components means one cluster of Web pages all of which refer to the same entity.
In Fig. 1, the dotted-lines show occurrences of the same person name, place name or organization name. In Fig. 2, the solid lines show the connection among documents whose SN similarities are over the threshold. 4.3 Document Similarities Based on Compound Key Words This section explains how to measure document similarities based on key com-pound words and to cluster documents by similarity.

First, we calculate an importance score of compound words in a document with the method proposed by Nakagawa et al. [11]. Next, we construct a compound the indices of the compound words in documents and s v is the score of compound word v . Then, we measure the document similarity by using the scalar product of the compound word vectors. Finally, we cluster the documents by the similarity and a threshold.
 The importance score for the compound words is calculated as follows: Let CW (= W 1 W 2  X  X  X  W L ) be a compound word, where W i ( i =1 , 2 ,  X  X  X  ,L )isa simple noun. f ( CW ) is the number of independent occurrences of compound word CW in a document where  X  X ndependent X  occurrence of CW means that CW is not a part of any longer compound nouns. The importance score of compound word CW is LR ( CW )isdefinedasfollows: LN ( W i )and RN ( W i ) are the frequencies of nouns that directly precede or suc-ceed simple noun W i .
 This system can be obtained as  X  X erm Extraction System 3  X .
 The clustering algorithm by key compound words is presented below.
 Having constrains  X  ( d x )=  X  ( d y ), clustering is done in the same way as Social Networks. 4.4 Estimate Latent Topic of Document In this paper, we assume that pages referring to the same entity have the same latent topic that indicates a word distribution. Therefore, inferring the latent topic of a page allows the pages that have the same topic to be categorized into the same cluster.
As a clustering algorithm that can treat latent topics, we adopt unigram mix-ture that is a basic topic model [12]. Moreover, we use unigram mixture expanded by Dirichlet process [7] : Dirichlet pro cess unigram mixture(DPUM). DPUM can estimate the number of latent topics corresponding to a set of pages. In the per-son name disambiguation, the number of true entities(topics) is unknown at first, so DPUM is suitable to our purpose.

Unigram mixture is a probabilistic generative model of a document based on unigram model, which assumes that the words of every document are drawn inde-pendently from a single multinomial distribution. In unigram mixture, each doc-ument is generated by the topic-conditional multinomial distribution p ( w | z,  X  ). z  X  X  1 , 2 ,  X  X  X  ,T } is a latent topic and T is the number of latent topics.  X  = {  X  and  X  tw is the probability that word w is generated from topic t .Itisaproblem that the number of latent topics is unknown in advance. To solve this problem, a nonparametric Bayes model using Dirichlet process was proposed [7,1,6]. This model can change the model structure (the number of latent topics, etc...) in correspondence with the data. A mixture model expanded by Dirichlet process is called Dirichlet process mixture(DPM) [1].

Sethuraman provides a constructive rep resentation of Dirichlet process as stick-breaking process [15]. By using Sti ck-breaking process, the effective learn-ing algorithm of DPM can be proposed [6].

The stick-breaking process is based on countably infinite random variables {  X   X  0 is a concentrate parameter and G 0 is a base measure of Dirichlet process. In DPUM, G 0 is Dirichlet distribution p (  X  |  X  )where  X  is a parameter of Dirichlet distribution. Beta is a beta distribution.
 We write  X  (= {  X  }  X  t =1 )  X  SB (  X  ;  X  0 )if  X  is constructed by Eq. (3).
The process of generating a document in DPUM by using the stick-breaking process is as follows: 1. Draw  X  (= {  X  }  X  t =1 )  X  SB (  X  ;  X  0 ) 2. Draw  X  t  X  G 0 ( t =1 , 2 ,  X  X  X  ,  X  ) 3. For each document d : Note that Multi is a multinomial distribution and p ( w = v | z = t,  X  )=  X  tv .
Therefore, DPUM can be formulated in the joint probability distribution as follows. M is the number of documents. N d is the number of words in a document d . w n th word in the sequence. p (  X  |  X  0 )is SB (  X  |  X  0 ).

For inference of latent topics in DPUM, w e adopt Variational Bayes inference, which provides a deterministic method [3]. Blei et al. proposed a framework of Variational Bayes inference for DPM that was restricted to an exponential family mixture and was formulated by Sti ck-breaking process [6]. This inference scheme does not need to set the number of latent topics, but it does need to set a maximum number of latent topics due to computational cost. 5.1 Data Set As we mentioned, the English corpus for the Name Disambiguation task is de-veloped in WePS [2]. Because our system targets Japanese Web pages, however, we developed an original Japanese Web page test set for this task as follows.
We first input Japanese person name queries into a search engine. Some of the person queries were chosen from among ambiguous popular names. For example,  X  X aro Kimura X  is a very common name in Japan, and we found there were many people called  X  X aro Kimura X , including a famous commentator, a member of the Diet, a translator, and a schoolma ster. Some other queries were selected from persons in our laboratory, and other person name queries were generated automatically.

Second, we tried to extract Web pages containing these names. We retrieved these pages with a search engine. If the query hit many pages, we collected the top 100-200 Web pages.

Finally, these pages were manually annotated 4 . Annotators removed pages that violated our assumption that one page refers to only one entity. As a result, we collected 5015 Web pages on 38 person names, and all page references were clarified. 5.2 Evaluation Precision (P), recall (R), and F-measure (F) were used as the evaluation met-rics in our experiments. All metrics were calculated as follows [8]. Assume C = { a set for the result of clustering, where C i and D j are sets of pages. For each all clusters D j (1  X  j  X  m )as
The F-measure of C i (F i ) was calculated by F i =max j F ij .Using j = argmax j F ij ,P i and R i were calculated as P i =P ij , R i =R ij .
The entire evaluation was conducted by calculating the weighted average where weights were proportional to the number of elements in the clusters, cal-culated as where |C| = n i =1 | C i | . The weighted average precision and recall were also calculated in the same way for the F-measure. 5.3 Baseline Baseline is a clustering method that uses well-known document similarities by word frequency.

First, we construct a word frequency vector wfv j =( f 1 ,f 2 ,  X  X  X  ,f W )foreach document where { 1 , 2 ,  X  X  X  ,W } are the indices of the vocabulary in documents and f w is the frequency of word w in a document d j . Then, we measure the document similarity by using the scalar product of the word frequency vectors: sim Base ( d x ,d y )= wfv x  X  wfv y Finally, we cluster the documents by the simi-larity sim Base and a threshold  X  Base . The clustering is done in the same way as Compound Key Words.

Moreover, we tested the No Cluster algorithm in which all documents are categorized into different clusters, that is, there are not any documents that are categorized into the same cluster. 5.4 Experimentation We investigated which of the Social Network ( SN ), Compound Key Words ( CKW ), Dirichlet Process Unigram Mixture ( DP ) or their combinations were the best. Combinations of two or three methods means different methods to-gether used together.

More precisely, the result of the combination of SN and CKW is given by considering graph G = V, E SN  X  E CKW and G = V, E SN  X  E CKW where G
SN = V, E SN is the result for SN and G CKW = V, E CKW is the result for CKW. DP needs to initialize the latent topic z d of a document and the max-imum number of latent topics. Sin ce we had to determine thresholds  X  SN and  X  CKW , we used 5-fold cross validation for the evaluation of SN/CKW methods or their combinations.  X  SN and  X  CKW were estimated to maximize training set F-measure, and then test set F-measure was calculated using these estimated parameters.

When DP was applied in a stand-alone way, the latent topic was initialized randomly and the maximum number of topics was set to 100. When DP was combined with SN/CKW methods, SN/CKW methods were applied first, and DP was then initialized with the SN/CKW results. That is, we regarded the clusters constructed by SN/CKW methods as the initial latent topics of DP and applied DP. In this case, the maximum number of latent topics was set to the number of the cluster constr ucted by SN/CKW methods.

Table 1 lists the results of an average of 38 queries. Fig. 3-6 shows F-measure of SN, CKW, SN  X  CKW, SN+DP, CKW+DP and (SN  X  CKW)+DP with re-spect to each person name. According to the results, Either SN or CKW showed the great improvement from the baseline. In addition, they seem to employ dis-tinct type of information to a certain extent because SN  X  CKW shows four to five points improvement from SN or CKW alone. The fact that DP also improves SN or CKW on F-measure means that DP introduces another aspect of the information, i.e., documents topics. As expected from these results, proposed methods (SN  X  CKW)+DP showed the highest performance on F-measure among others. We propose a novel framework for person name disambiguation that has the following three components processes: social networks, document similarities by compound key words and documents topi cs. Experiments using an actual Web document dataset show that the result of our framework is promising because our framework uses distinct type of information potentially being within documents. Acknowledgments. This research was funded in part by Category  X  X  X  of  X  X cientific Research X  Grants in Japan.

