 1. Introduction
Clustering is an important issue in the analysis and exploration of data. There is a wide area of clustering applications including information retrieval, image segmentation, character recognition, VLSI design, computer graphics and gene anal-rithms include: monitoring computer networks for administration purposes, visualizing knowledge bases to support ysis. In this context, graph-clustering is also known as community structure (Girvan &amp; Newman, 2002; Newman, 2004 ).
Community structure algorithms have been applied in the study of several networks, including networks of email messages literature.

Similarity (or equivalently distance) based, hierarchical agglomerative clustering algorithms (including the conventional, (2004), Zahn (1971), Kannan, Vempala, and Vetta (2004), and Zhao and Karypis (2004) .In Stein and Niggemann (1999) , tivity. An extension to the graph domain for the k-medoids algorithm (Kaufman &amp; Rousseeuw, 1990) has been given by criterion function. Two algorithms, a divisive (Newman &amp; Girvan, 2004 ) and an agglomerative one (Newman, 2004 ), have been proposed for the optimization of modularity.

Most of the above mentioned graph-clustering algorithms are based on the intuitive notion of intra-cluster (within clus-tering algorithms. According to Girvan and Newman (2002), Newman (2004), and Newman and Girvan (2004), clusters are groups with a high density of within-group edges and a lower density of between-group edges. In Brandes et al. (2008) , identify a cluster that contains vertices more strongly connected with its graph complement. For example, in the graph of {13,14,15,16} is not a conventional one. The number of links connecting vertex 13 to cluster {13,14,15,16} is lower than the number of links connecting vertex 13 to the remaining graph {1,2,3,4,5,6,7,8,9,10,11,12}. to any other cluster. Correspondingly, we call refined, a clustering consisting of refined clusters. Note that clustering nected with cluster {13,14,15,16} than with cluster {1,2,3,4} or cluster {5,6,7,8} or cluster {9,10,11,12}. Note that refined clusters may be more densely interconnected than conventional ones. For example, consider graphs in be added then vertex 7 will be equally connected with its cluster and the remaining graph; hence cluster {7,8,9} will not clusters.

Densely interconnected refined clusters may be natural in many real word applications. Clustering the pages of a web site constitutes a prominent example of such an application. Most information exchange tasks between users and the site require and finally, a page to provide the newcomer with an appropriate identification number. We call these clusters, functional we show in Section 3, most of the existing graph-clustering algorithms tend to merge densely interconnected refined clus-result on few large-scale clusters, which contain disparate pages.

In this context, the main contribution of our work is summarized as follows: We clarify and formalize the notions of refined and conventional cluster and present their basic properties.
We show that many well-known graph-clustering algorithms typically fail to extract densely interconnected refined clus-ters. We highlight limitations of each examined algorithm that result to such a failure.

We propose a novel graph-clustering algorithm which overcomes the abovementioned limitations and is efficient in the exploration of densely interconnected clusters.

Experimentally, we evaluate our approach with respect to existing hierarchical agglomerative graph-clustering approaches (modularity (Newman, 2004 ) and distance ( Kaburlasos et al., 2007, 2009 ) clustering). Earlier experimentation
Newman, 2004; Rattigan et al., 2007) use benchmark graphs for the evaluation of clustering algorithms. Each benchmark graph encapsulates a pre-specified clustering. Clustering solutions derived by algorithms under evaluation are compared with the pre-specified clustering. However, the pre-specified clustering used by the abovementioned approaches is assumed to be a conventional one. Hence, the evaluation is limited in the exploration of clustering solutions consisting of relatively sparsely interconnected clusters. Our experimentation involves benchmark graphs ( Moussiades &amp; Vakali, 2009) that overcome this limitation as well as two real datasets, each originating from the pages of a web site. Although applications mentioned in this introduction. Here, we demonstrate an application on supporting plagiarism detection in students programming assignments. Experimental results show that our algorithm performs favourably in comparison to both, the distance and the modularity agglomerative graph-clustering algorithms.
 a case study regarding the exploration of densely interconnected refined clusters for six well-known graph-clustering ap-connected clusters. Comparative experimental results are presented and discussed in Section 5. Finally, Section 6 reports conclusions and highlights further research topics. 2. Basic definitions and notations
Moreover, G is assumed to be a simple graph as no two of its links join the same pair of vertices and  X  thermore, G does not include disconnected vertices as each disconnected vertex forms a separate cluster and need not be included in the clustering process. Therefore, d  X  v  X  &gt; 0 8 of a vertex is equal to the number of links connected to the vertex. We denote by C  X f c into k clusters with [ c i 2 C  X  V and c i \ c j ; i  X  j  X  0. The cluster at which vertex called a clustered graph. Graph g  X  c  X  denotes a subgraph of G induced on cluster c 2 C , i.e., g  X  c  X  X f c ; f nected if there is a link between any two of its vertices.
 is stated otherwise.
 one endpoint in cluster c and the other in cluster s .
 For example, for graph in Fig. 2 a holds d  X f 3 ; 4 ; 5 g ; f 6 ; 7 ; 8 g X  X  2.

The connection degree between a cluster, c 2 C , and a vertex between c and singleton (single member cluster) { v }; i.e., d ( c , For example, for graph in Fig. 2 a holds d  X f 3 ; 4 ; 5 g ; 0  X  X  1.
 (internal links).
 For example, for graph in Fig. 2 a holds I f 0 ; 1 ; 2 g  X  3.
 cluster c (external links).
 For example, for graph in Fig. 2 a holds X f 0 ; 1 ; 2 g  X  2.

Note that from Definition 3 follows that the external degree of cluster c 2 C equals to the sum of connection degree between each vertex of c and V c .

Proposition 1. The internal degree of cluster c 2 C equals to the sum of connection degree between each vertex by 2.

Proof. Each internal link of cluster c counts twice in P v 2 c 2 I We remark that value of P v 2 c d  X  c ; v  X  is always even.
 degree of cluster s plus the connection degree between clusters c and s.
 tex that belongs to s . Therefore, I c [ s  X  I c  X  I s  X  d  X  c ; s  X  . h degree of cluster s minus twice the connection degree between c and s.

Proof. The number of external links of c that do not connect to s is X that do not connect to c is X s d  X  s ; c  X  . Therefore, the number of external links of c [ s is X d  X  c ; s  X  X  X c  X  X s 2 d  X  c ; s  X  . h 2.1. Conventional and refined cluster Here, we formalize the notions of conventional and refined cluster and we discuss some of their properties.
Definition 4. A cluster c 2 C is called conventional if g  X  c  X  is connected and the following condition is satisfied (S1) d  X  c ; v  X  &gt; d  X  V c ; v  X  ; 8 v 2 c
Condition S1 requires that all vertices of a conventional cluster are more strongly connected with their cluster than with the remaining graph.

Definition 5. A cluster c 2 C is called refined if g  X  c  X  is connected and the following condition is satisfied (S2) d  X  c ; v  X  &gt; d  X  s ; v  X  ; 8 v 2 c ; 8 s 2 C c
Condition S2 requires that all vertices of a refined cluster are more strongly connected with their cluster than with any other cluster.

Note that both conventional and refined clusters are required to be connected. Clusters, which are not connected, violate contain conventional and non conventional refined clusters.

We remark that for graph G there may be more than one refined clustering solutions, either having the same or a different clustering {{0,1,2,3,4,5,6,7,8},{9,10,11}} and clustering {{0,1,2},{3,4,5,6,7,8,9,10,11}}. We next give two more properties of a refined clustering.
 Proposition 4. A refined clustering does not include singletons.

Proof. If vertex u forms a singleton, f u g2 C then d  X f u g ; u  X  X  0 because  X  isfied for singleton {u}. h chical agglomerative methods, is obviously refined.
 Proposition 5. A completely connected graph cannot be partitioned into a refined clustering.
Proof. Assume C is refined. Let c ; s 2 C ; c  X  s and v 2 c ; u 2 s .If G is completely connected, vertex other vertices in cluster c and to all vertices in cluster s . Therefore
Similarly,
Since C is refined
By replacing (2.2) and (2.3) in (2.6)
By replacing (2.4) and (2.5) in (2.7)
Inequality (2.8) contradicts the initial assumption that C is refined. h 3. Existing graph-clustering approaches and densely interconnected clusters mum cut tree clustering algorithm ( Flake et al., 2004 ), or CCA for short. Moreover, we study the behaviour of each presented algorithm in the exploration of densely interconnected clusters. Using a simple clustering task, we demonstrate fined clustering {{0,1,2}, {3,4,5}, {6,7,8}, {9,10,11}} of graph in Fig. 5 . 3.1. Distance based clustering
In each step of iteration, distance based clustering algorithms merge these two clusters that are nearest to each other, operation  X  X  ^ S  X  returns the lowest value of set S and d to vertex v j . The following three functions are used as distance functions plete link and single link distance measures, respectively.
 tering from the derived clustering hierarchy. Modularity maximum value is assumed to correspond to the  X  X  X est X  clustering mum number of clusters. Distance clustering, in each step of iteration, computes the distance between clusters but it does applications.

Next, we discuss another shortcoming of distance clustering. The quality of clustering solutions derived by distance clus-distance between two clusters is degenerated to the length of the shortest path that connects the corresponding vertices, independently of the specific distance function, single link, complete link or average similarity that is employed. {4,5,6) and {7,8,9) have been formulated then vertex 0 is merged together with cluster {4,5,6}. However, distance based
Clusters that contain vertices with a high degree have an increased number of neighbours in comparison with clusters that do not contain vertices with high degree. Hence, they have an increased probability of being merged together with other clusters. Therefore, distance methods tend to create large-scale clusters around vertices with high degree. Obviously, the processing order effect is augmented for datasets consisting of densely interconnected clusters. result, distance clustering typically fails to decompose the graph in Fig. 5 in its refined clustering. 3.2. Modularity based clustering Modularity clustering is based on the maximization of the so-called modularity criterion function Q given by
For convenience, we call the i th term in the modularity formula, modularity of cluster c a cluster with relatively higher internal and lower external density of links.
 The agglomerative optimization of modularity is based on the variation in modularity, D 2), the variation in modularity upon merging two clusters c , s is given by
As we show in Proposition 6 , the computation of D Q should be evaluated in a different manner. Proposition 6. The variation in modularity value upon merging clusters c ; s 2 C ; D
Proof. Assume clusters c and s are merged together to form cluster k . Then e
By substituting from Proposition 2 , it follows
In addition,
With substitution from Proposition 3 , it follows
By substituting (3.5) to (3.4) follows
By substituting e kk from (3.2) to (3.6), it follows
The variation in modularity is given by D Q  X  c ; s  X  X  X  e (3.7) and making the operations, it follows D Q  X  c ; s  X  X  e
For an example consider the graph in Fig. 7 . It contains the refined clustering {0,1,2}, {3,4,5}, {6,7,8}, {9,10,11}, {12,13,14}. The optimization of modularity as it is given by Newman (2004) leads to the creation of clusters {0,1}, {2}, D
Q computation that we propose merges clusters {0,1} and {2}, hence in the derived hierarchy, the refined clustering is in-cluded. In our experimentation, we use the computation of D
Note that in each step of agglomeration, D Q value may be the same for two or more pairs of clusters. In this case, one of ority to vertices with lower degree. For two singletons c  X f and X c  X  d  X  v  X  and X s  X  d  X  u  X  . With substitution to Eq. (3.1), it follows that D equation implies that among singletons processing priority is given to a pair with greatest d  X  in general, processing priority is given to a pair with lowest product of degrees. However, modularity clustering fails to neighbours and the modularity value of the cluster. For cluster c 2 C , the following equality holds:
According to equality (3.8), the modularity of cluster c decreases when the number of cluster neighbours, X that densely interconnected clusters have negative modularity value. Modularity clustering tends to merge these clusters together with their neighbours towards modularity maximization. Regarding the graph in Fig. 5 , note that modularity value {6,7,8}, {9,10,11})} = 0.13. However, a clustering, C , consisting of only one cluster has modularity value equal to 0, gle cluster solution than to the refined clustering. 3.3. Structure identification with MajorClust
Stein and Niggemann (1999) propose the so-called weighted partial connectivity as a measure for the structure of a graph; hence, suitable for decomposing a graph into its structural components (clusters). Moreover, they present algorithm
MajorClust, which approximates a graph X  X  optimum value of weighted partial connectivity. MajorClust initially assigns each The iteration ends if a complete pass through the set of vertices has been performed and no vertex changed its cluster. MajorClust returns a single clustering solution for an input graph.
 mulates a separate cluster. Therefore, one of them is chosen randomly and is assigned to the same cluster with vertex 7. step. Vertex 0 has four neighbours: 6,2,11,1. Assume that is assigned to the same cluster with vertex 2. Thus, cluster consequences of such behaviour are augmented for densely interconnected clusters. 3.4. Graph-clustering based on minimum cut trees tex with each graph vertex with a link weighted according to the value of the input parameter a . For the derived graph, a the minimum cut tree, artificial vertex is removed and the resulting connected components form the clusters of the input at its center (for a &gt; 0 : 45  X  , or the artificial sink is a leaf (for a only of singletons or in the single-cluster solution. 4. Graph-clustering based on ICR function that we call intra connection ratio or ICR for short, and an agglomerative method for ICR maximization; hence, we call the proposed algorithm AICR (agglomerative maximization of ICR). Similarly to AMod, AICR implies a best clustering at maximum ICR value. 4.1. Criterion function ICR Below, we first define the intra connection ratio of a cluster and then we define function ICR. and each of its vertices divided by the sum of degree of all vertices in c . Note, that d  X  v  X  &gt; 0 8 v 2 V implies P v 2 c d  X  v  X  &gt; 0 8 c 2 C . Moreover,
P of links.
 of all clusters in C
We remark that the maximum value of ICR depends both on the density and the size of G . Moreover, the value of ICR for a cluster solution or a solution consisting only of singletons.

Next, we first present the computation of ICR variation upon merging two clusters and then we discuss ICR in relation to densely interconnected clusters.
 Proposition 7. The variation in ICR value upon merging two clusters c ; s 2 C ; D Proof. If k  X  c [ s , the variation in ICR upon merging c and s is By replacing from Eq. (2.1) By replacing from Proposition 1 By replacing from Propositions 2 and 3, it follows 4.1.1. Densely interconnected clusters and ICR
Note that for two singletons c  X f v g ; s  X f u g with f v
X  X  d  X  u  X  . With substitution to Eq. (4.1), it follows that
Similarly, it can be shown that if c  X f v g ; s  X f u g and f
As we have already mentioned ICR reduces the processing order effect and avoids merging refined clusters even if they are densely interconnected. More specifically:
Most singletons are assigned to clusters prior ICR maximization. As it is shown by Eq. (4.2), merging two neighbouring singletons always results to an increment of ICR . In addition, as it is also shown by Eq. (4.2), among singletons, pro-cessing priority is given to a pair with lowest sum of vertices degree. Thus, AICR reduces the processing order effect.
For example consider the graph in Fig. 5 . In the first step of agglomeration, only pairs {0,1}, {4,5}, {6,8} and {10,11} are candidates for merging, whereas for distance clustering and MajorClust all pairs of adjacent vertices are candidates for merging.

Typically, clusters that are merged together prior ICR maximization are not refined. Both sparsely and densely intercon-e.g., for graph in Fig. 8 holds D Q  X f 0 ; 1 ; 2 g ; f 3 ; 4 ; 5 g X  X  0 : 22, whereas D maximization merges cluster {0,1,2} with cluster {3,4,5}, whereas ICR does not. For an additional example note that for graph in Fig. 5 , it holds that D ICR  X f 0 ; 1 ; 2 g ; f 6 ; 7 ; 8 g X  X  1 refined ones, e.g., for graph in Fig. 5 , it holds that D avoids merging refined clusters with their neighbours. 4.2. Hierarchical agglomerative optimization of ICR increase (or smallest decrease) of ICR .

Both the worst-case and the average-case complexity of the generic hierarchical agglomerative algorithm is O  X j V j (Manning, Raghavan, &amp; Schutze, 2008 ). In AICR , we take advantage of the specific nature of D reduce the average-case complexity to O  X j V j 2 log j V j X  . Next, we first present algorithm AICR and then we study its complexity.
 c  X f v i g implies I ci  X  0. In line 4, we initialize the number of each singleton external links and the connection degree between two singletons. In line 5, we take advantage of the knowledge that D nected components; the algorithm terminates in line 6b. Otherwise, cluster c form the new cluster c k , in line 6c. In line 6d, clusters c ber of external links of cluster c k is updated in line 6f ( Proposition 3 ) and finally, in line 6g, we recalculate D new cluster ( Proposition 7 ).

The iteration controlled by line 6 continues until all vertices have been merged together into one cluster or clustering C consists of disconnected components, producing a clustering hierarchy. One may think that the iteration should terminate when ICR starts to decrease. However, we do not know if ICR has only one maximum. Moreover, there may be more than one try to maximize ICR for each clustering in the derived hierarchy.
 Although the processing order effect is reduced, it is still possible that the value of D pairs of clusters (step 6a). In this case, one of these pairs is chosen randomly. Hence, each execution of the algorithm does not produce the same clustering hierarchy. Therefore, the experimental evaluation of the algorithm requires that the algorithm be executed several times on each dataset. This requirement holds for the remaining algorithms that are examined in this paper; for distance clustering, AMod and MajorClust, it is due to the processing order effect, whereas for CCA, it is due to the fact that the min cut tree for a given graph is not unique (Gomory &amp; Hu, 1961 ). gletons with lowest sum of degrees are merged together. Candidate pairs are {0,1}, {4,5}, {6,8}, {10,11}. Since it holds D ICR  X f 0 g ; f 1 g X  X  D ICR  X f 4 g ; f 5 g X  X  D ICR  X f 6 g ; f 8 g X  X  D relatively low. Thus, the algorithm proceeds by merging two singletons, those with lowest product of degrees. Continuing sult to an ICR decrement. Note that random choices performed in this clustering task do not affect the result at ICR maximization. 4.2.1. Complexity
Proposition 8. The worst-case complexity of AICR is O  X j V j O  X j V j X  . Moreover, lines 6a ... 6g are repeated j V j 1 times. Hence, the complexity of AICR is Therefore, the worst-case complexity of AICR is O  X j V j 3 average-case complexity is O  X j V j 2 log j V j X  . h Practically, the algorithm clusters a graph consisting of 1000 vertices in about 2 min when running on an Intel Core
Duo CPU. The algorithm need not be executed during user X  X omputer interaction; hence, there is no doubt about its applicability. 4.3. Extension for weighted graphs
Note that all definitions and propositions from Section 2 are based on the connection degree. Therefore, AICR is directly extended to weighted graphs by simply assuming a link-weight function w : L ! R w  X f v ; u g X  &gt; 0if f v ; u g2 L , and redefining the connection degree between two clusters as follows. links having one endpoint in cluster c and the other in cluster s .
 have weight equal to 1. Then the connection degree between clusters {0,1,2} and {3,4,5} equals to 4, i.e., to 5. The external degree of cluster {0,1,2}, given by Eq. (2.1), equals to 4, etc. 5. Experimentation
In this section, we present comparative experimental results detection in students programming assignments. We also include a discussion of the results. 5.1. Experimentation with artificial datasets
For these experiments, we have used benchmark graphs proposed by Moussiades and Vakali (2009), which can be clus-sion and description of benchmark graphs, then we describe our artificial datasets, and finally, we present experimental results. 5.1.1. Benchmark graphs tered graph that has a conventional or a refined reference clustering. The base graph of a benchmark is the benchmark with has common reference clustering with its base graph. A base graph ( G , C ) can be described by the size of C , denoted by j C j , the size of a minimum cluster in C , denoted by j c internal density, which is a measure of the density of intra-cluster links.

Definition 11. Given a clustered graph, (( V , L ), C ), the internal density of vertex degree between v and c(v ) divided by the size of c(v ) minus 1. over all vertices of a graph clustered into completely connected subgraphs.

Definition 12. Given a clustered graph, (( V , L ), C ), the external linkage of vertex between v and V c  X  v  X  divided by the connection degree of
The maximum value of average external linkage for a clustered graph with a conventional reference clustering is limited by description of its base graph and its average external linkage.

Fig. 10 , presents a base graph with j C j X  5 ; j c min j X  3 ; j c a corresponding benchmark graph with average external linkage equal to 0.38. 5.1.2. Description of artificial datasets
We use 90 benchmarks, organised in 9 datasets (ds 1 ... ds and increases for each successive dataset, up to 2.99 for ds external linkage is increasing (links between clusters becomes denser). Furthermore, we include an additional dataset (ds containing 10 benchmarks. All 90 benchmarks have common base graph with # C = 10, j c internal density equal to 1. Hence, they have common reference clustering. However, this clustering is a refined one for graphs in datasets ds 1 ... ds 9 , whereas it is a conventional clustering for graphs in ds and density of inter-cluster links, the same clustering may be refined for a graph and conventional for another. In Table 1, we give the average external linkage, and the corresponding standard deviation, for each dataset. 5.1.3. Results 1. In general, the larger the value of purity, the better the clustering solution is.
 Purity values in Figs. 12 and 13 are averages of 10 clustering solutions, one for each benchmark in a dataset.
In Fig. 12 , we show the purity of clustering solutions that have been given by the compared algorithms for 10 clusters, which is the number of clusters of the reference clustering.

In Fig. 13 , we show purity values given by AICR and AMod at the maximization of ICR and modularity criterion function, over the whole clustering hierarchy.

In Fig. 14 , we show modularity values that correspond to the clustering hierarchy derived by AMod, for a dataset with low average external linkage (ds 1 ) and a dataset with high average external linkage (ds of space.

ICR values that correspond to the clustering hierarchy derived by AICR, for a  X  X  X parse X  (ds shown in Fig. 15 . 5.2. Experimentation with web sites
In this section, we show results concerning the application of the compared algorithms on two web sites, the web site of a vertex corresponds to a web page and an edge corresponds to the set of hyperlinks that links two pages. Pages, which are uted in 17 disconnected components. The graph representing SingularLogic consists of 604 vertices and 15,595 links. We re-mark that for these applications, a pre-known clustering is not available. Hence, purity cannot be used.
In Fig. 16 , we show the modularity values that correspond to the clustering hierarchy derived by the application of AMod on csd site graph.
In Fig. 17 , we show the modularity values that correspond to the clustering hierarchy derived by the application of AMod on singular site graph.

In Fig. 18 , we show the ICR values that correspond to the clustering hierarchy derived by the application of AICR on csd site graph.

In Fig. 19 , we show the ICR values that correspond to the clustering hierarchy derived by the application of AICR on sin-gular site graph.
 the csd site graph. From the hierarchy produced by AICR, we have selected the solution at maximization of ICR. Correspond-ingly, from the hierarchy given by AMod, we have selected the solution at maximization of modularity. From the hierarchy produced by AAS, we have selected two solutions, the one that corresponds to the highest ICR value and the solution that of clusters, column  X  X  X in X  indicates the size of minimum cluster, column  X  X  X ax X  indicates the size of maximum cluster, column  X  X  X ean X  indicates the average cluster size, column  X  X  X tDev X  indicates the standard deviation of the cluster sizes 10 executions of each algorithm on each web site graph.

Similar information regarding the singular site graph is given in Table 3 . 5.3. Supporting plagiarism detection in students programming assignments
In this section, we demonstrate the application of AICR in supporting plagiarism detection in students programming assignments. Plagiarism detection is based on measuring the similarity between two program files. Several appropriate sim-then used by the tutor in order to set learning groups and promote cooperation between students ( Moussiades &amp; Vakali, 2005). Suspicious for plagiarism programs can be represented using a weighted graph such that a vertex represents a pro-gram and a weighted link represents the similarity between two programs. The dataset used for this experiment consists of 51 source code files of a real student programming assignment. The similarity measure that we have used is the one proposed by Moussiades and Vakali (2005) . The highest value of this similarity measure is obtained for programs that are identical to each other and is equal to 1. The cut-off value used is 0.6. Fig. the corresponding criterion function. 5.4. Discussion of the results 5.4.1. AMod
As it is shown by Figs. 12 and 13 , purity values achieved by AMod are relatively high for datasets ds notably lower for densely interconnected datasets ds 7 ,ds plains the modularity curve for dataset ds 9 in Fig. 14 . Note that in Fig. 13 , purity value is lower for dataset ds values for datasets ds 1  X  X s 6 and ds 10 , but higher than purity values for datasets ds negative modularity value; q ({0,1,2}) = 0.197, whereas each of the remaining clusters has modularity value equal to 0.09.
As a result, cluster {0,1,2} is merged together with clusters {3,4,5} and {12,13,14}, towards modularity maximization, whereas clusters {6,7,8} and {9,10,11} retain their autonomy. Therefore, in this example modularity is maximized for clus-tering { { 0,1,2,3,4,5,12,13,14}, {6,7,8}, {9,10,11}}. The modularity value of the reference clustering for dataset ds are merged together with some of their neighbours whereas some of the clusters with positive modularity retain their autonomy, which explains the purity value obtained for dataset ds
Web sites often contain pages with too many neighbours. In csd site graph, there are 11 vertices with degrees that vary neighbours. The lowest vertex degree is 1 for csd graph and 8 for singular graph. Therefore, while the agglomeration pro-ceeds, some clusters contain vertices with high degree, hence they possess a high number of neighbours and a low modu-number of clusters of the best clustering produced by AICR (Table 2)) includes 62 clusters with negative modularity value and 33 clusters with positive modularity value.
 ters with negative modularity have been merged together with some of their neighbours, which results in clustering solu-hence, they contain pages belonging to various functional clusters. This conclusion has also been confirmed by an examina-help system, since a selector containing hundreds of page addresses is not especially helpful to the user. 5.4.2. Distance clustering
Solutions given by AAS and ACL contain either large-scale clusters or too many singletons (Tables 2 and 3 ). We remark 4). Solutions with many singletons have been derived at a relatively early stage of agglomeration whereas solutions with order effect, which is present in both sparsely and densely interconnected datasets. 5.4.3. AICR As it has already been mentioned in Section 4, AICR overcomes both limitations of distance and modularity clustering. ficial datasets and the web sites. 5.4.4. Supporting plagiarism detection
Regarding the application on supporting plagiarism detection in students programming assignments ( Figs. 20 and 21 ); {1,4,16,17,25} and cluster {3,9} with cluster {0,10,12,15,26}. Here, we note that programs 6, 7 and 8 are very similar to the same cluster of plagiarism. On the other hand, program 7 is not related with any program in cluster {1,4,16,17,25}. wise to check manually if programs 3 and 9 form a separate cluster of plagiarism. Independently of his/her final judgment, sion, we consider AICR performance on supporting plagiarism detection, i.e., AICR performance on a conventional real data-set, encouraging for most application scenarios mentioned in the introduction. Furthermore, this conclusion is also confirmed by results on conventional artificial datasets (dataset ds 6. Conclusions
In this paper, we clarify the notion of conventional and refined cluster. We show that refined clusters may be more den-sely interconnected than conventional ones. In addition, we show that many graph-clustering approaches do not take into consideration refined clusters. Moreover, they result in unbalanced clustering solutions if densely interconnected refined solutions even for datasets consisting of densely interconnected clusters. Experimental results show that our approach is exceptionally efficient for densely interconnected datasets and encouraging for conventional datasets. Further research on the topic includes experimentation in more application areas and improvement of the ICR optimization method. References
