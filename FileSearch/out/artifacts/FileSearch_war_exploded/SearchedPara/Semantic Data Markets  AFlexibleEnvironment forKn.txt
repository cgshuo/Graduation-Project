 We present Nyaya , a system for the management of Semantic-Web data which couples a general-purpose and extensible storage mechanism with efficient ontology rea-soning and querying capabilities. Nyaya processes large Semantic-Web datasets, expressed in multiple formalisms, by transforming them into a collection of Semantic Data Kiosks . Nyaya uniformly exposes the native meta-data of each kiosk using the Datalog  X  language, a powerful rule-based modelling language for ontological databases. The kiosks form a Semantic Data Market where the data in each kiosk can be uniformly accessed using conjunctive queries and where users can specify user-defined constraints over the data. Nyaya is easily extensible and robust to updates of both data and meta-data in the kiosk and can readily adapt to different logical organization of the persistent stor-age. The approach has been experimented using well-known benchmarks, and compared to state-of-the-art research pro-totypes and commercial systems.
 H.2.4 [ Database Management ]: Query Processing, Rule-based databases; H.2.3 [ Database Management ]: Query languages Management, Performance, Experimentation ontological databases, Datalog  X  , query rewriting, query an-swering, semantic data management, semantic web  X 
This research has been partially funded by the Eu-ropean Commission, Programmes IDEAS-ERC, Projects no.227977-SMScom and no. 246858-DIADEM and from the Oxford Martin School X  X  grant no. LC0910-019-Extreme and Ubiquitous Computing.

Ever since Tim Berners Lee presented the design princi-ples for Linked Data 1 , the public availability of Semantic Web data has grown rapidly. Today, many organizations and practitioners are all contributing to the  X  X eb of Data X , building RDF repositories either from scratch or by publish-ing, in RDF, data stored in traditional formats. The adop-tion of ontology languages such as RDFS and OWL supports this trend by providing the means to semantically annotate Web data with meta-data, enabling ontological querying and reasoning. Despite the fact that storing, reasoning over, and querying large datasets of semantically annotated data in a flexible and efficient way represents a challenging area of re-search and a profitable opportunity for industry [16], seman-tic applications using RDF and linked-data repositories set performance and flexibility requirements that, in our opin-ion, have not yet been satisfied by the state of the art of data-management solutions. In particular, current semantic data-management systems present some common shortcomings: (i) they usually operate within a language-dependent frame-work, implementing the reasoning and query-processing al-gorithms for a specific ontology language; (ii) they hardly adapt to requirements of changing the underlying physical organization (e.g., for optimization purposes); (iii) to bring the expressiveness and performance desiderata to terms, most systems unnecessarily restrict the allowed combination of ontology-modeling constructs, although in many cases de-cidability and tractability of reasoning and query answering are syntactically identifiable on a per-ontology basis.
We discuss possible solutions to these problems by pre-senting Nyaya 2 , an environment for Semantic-Web data management which provides  X  in the same order as above  X  (i) flexible and uniform ontology-reasoning and querying ca-pabilities over semantic data sets expressed in different for-malisms, (ii) an efficient and, most importantly, general and extensible storage policy for Semantic-Web datasets, which can adapt to different query engines, exploiting their opti-mization strategies, (iii) the possibility to syntactically check whether the meta-data in a given repository can be queried efficiently and to use the subset of the language that keeps the complexity of the process at the required level. The last aspect is particularly significant. Suppose for instance that the combined use of two language constructs could poten-tially lead to undecidability or to a complexity increment; normally, with a Semantic Web language the two constructs would not appear in the language together, thus severely re-ducing expressiveness. Conversely, Nyaya does not restrict a-priori the language but checks each set of meta-data for decidability and tractability.
 Reasoning and querying in Nyaya is based on Datalog  X  [5, 6], a family of rule-based languages that ex-tends Datalog [8] to allow a controlled class of unsafe rules, i.e., rules with existentially quantified variables in their heads. Datalog  X  captures the most common ontology lan-guages for which query answering is tractable, and pro-vides efficiently-checkable, syntactic conditions for decidabil-ity and tractability. To our knowledge, this is the first full and generic implementation of a Datalog  X  engine.
Nyaya allows the construction of persistent repositories of Semantic-Web data that we call Semantic Data Kiosks . As shown at the bottom of Figure 1, a kiosk is populated by importing an available RDF dataset, possibly coupled with constraints defined on its content expressed in some Semantic-Web language such as RDF(S) and OWL (or vari-ants thereof). In order to allow inferencing, native vocabu-laries and meta-data are extracted and represented as (first-order) Datalog  X  constraints. In addition, the entities of a kiosk are mapped to their actual representation in the stor-age by means of a set of non-recursive Datalog rules called storage program . This enables the separation of concerns be-tween, on the one hand, the reasoning and query-processing algorithms and, on the other hand, the logical organiza-tion of the storage, which can be changed without affecting querying. A collection of kiosks constitutes what we call a Semantic Data Market , a place that exposes the content of all the kiosks in a uniform way and where users can issue queries and collect results, possibly by specifying additional constraints over the available data using Datalog  X  .Anim-portantaspectof Nyaya is its flexibility. First, whenever a fresh Semantic-Web source is made available, a new kiosk can be easily built from it by extracting its meta-data and importing its data; in this way, its content is promptly avail-able to the users of the semantic data market. Second, if a user wants to query the same kiosk by adopting a different set of constraints, the query is automatically reformulated accordingly and issued to the kiosk.

Summarizing, our main contributions are the following:  X  the definition of the Semantic Data Kiosk, an exten-sion of the standard Knowledge Base where the ontological constraints and the logical organization of persistent infor-mation are uniformly represented in the same language;  X  the associated efficient and model-independent storage mechanism for Semantic-Web data, which guarantees the separation of concerns between the reasoning capabilities and the underlying organization of data;  X  a powerful rule-based inference and query-answering en-gine over massive data sets based on Datalog  X  ,whoseeffi-ciency is confirmed by our experimental results over widely accepted benchmarks.

The rest of the paper is organized as follows. Section 2 presents the notion of semantic data kiosk and the adopted storage and querying techniques of Nyaya .Section3dis-cusses related literature, while Section 4 illustrates the im-plementation of Nyaya and the experiments that allowed us to evaluate Nyaya  X  X  performance. Finally, Section 5 draws some conclusions and delineates our future work. Semantic Data Kiosk. The basic building block of Nyaya has three components: 1. an ontological program  X  O ,madeofasetof Datalog  X  [5, 6] rules over a collection of first-order predi-cates O , called ontology predicates , that represent the onto-logical entities of the current application; 2. a storage program  X  S ,madeofasetoffullTGDshav-ing, as head, a (single) atom with predicate in O and, as body, a conjunction of atoms over a set of first-order predi-cates S , called storage predicates , that represent constructs of the metamodel M ; 3. a database D over S .
 A Semantic Data Market is just a collection of Semantic Data Kiosks.

Storage. Following an approach for the uniform manage-ment of heterogeneous data models [2], in Nyaya data and meta-data are extracted from the sources and represented using a meta-model M made of a generic set of constructs , each of which represents a primitive used in a concrete Semantic-Web language (such as RDF, RDFS and OWL). This has two main advantages: (i) it provides a framework in which different Semantic-Web languages can be handled in a uniform way and (ii) it allows the definition of language-independent reasoning capabilities. The constructs of M are chosen by factoring out common modeling primitives of different models. For instance, M includes the construct Class that is used in both RDFS and OWL to represent a set of resources having common characteristics. Each con-struct is associated with an identifier, a name, a set of fea-tures , and a set of references to other constructs. Nyaya lies on a relational implementation of the meta-model where each construct corresponds to a relational table. The ap-proach is easily extensible; for instance, generalization hi-erarchies can be added to M by specifying constructs that capture the notions of subclass and subproperty [2].
As an example, the RDF/RDFS stock-exchange scenario of Figure 2 is represented using M as illustrated in Figure 3 (where some fields are omitted for the sake of readability). The example involves three classes: Stock , Company ,and Figure 3: Representation of the running example in the meta-model used for data storage.
 Idx , which models financial indexes. Each class is repre-sented by a tuple in the Class table. The instances of these classes, namely ftse (the financial index FTSE 100), bayl (the British Airways PLC stock) and ba (the British Air-ways company), are represented by tuples in the I-Class table. The relationships comp and refBus , modeling the stock composition of a financial index and the company re-lated to each stock respectively, are represented by tuples in the ObjectProperty table. Similarly, the attribute name of a financial index and the attribute value of a stock are represented in the DataProperty table. Instances of these relationships are represented in the i-ObjectProperty and i-DataProperty tables, respectively. In particular, the bayl stock, which is a component of the financial index ftse , refers to the ba company and has value 294.30 pounds. Note that data and meta-data are managed in a uniform way. In Section 4 we illustrate the indexing and partitioning tech-niques adopted to guarantee good performance when the above tables become very large.
 Ontological and storage programs. The top half of Table 1 shows a possible terminological knowledge over the stock-exchange scenario illustrated above. The first con-straint specifies that each stock index must have a name, while the second one specifies that a stock has a value, a reference business ( refBus ), and it is part of a financial in-dex. The third constraint specifies that a stock is a particu-lar form of financial instrument ( FinIns ), and the following one defines a new class representing investments. The last Table 1: Semantic Data Kiosk: Stock Exchange two constraints are: (i) a negative constraint specifying that loans and stocks are disjoint classes, and (ii) a functional constraint on the relationship comp . The bottom half of Ta-ble 1 provides three examples of storage-program rules for the class idx , the object property comp and the data prop-erty value , respectively. These rules describe how concepts, roles and attributes of the ontology are represented in the meta-model used for storage and are automatically derived when a new semantic data kiosk is built.
 Querying. Let us now consider a semantic data kiosk K =( X  O ,  X  S ,D ). Querying K requires to consider both the constraints in  X  O andin X  S .

In Nyaya querying and reasoning are reduced to con-junctive query answering under first-order reducible con-straints [17], i.e., given a query q , it is possible to construct a first-order query q rew such that for every database D the answers to q over D given the constraints in  X  O  X   X  S are answers of q rew over D (i.e., the query q rew  X  X mbeds X  the con-straints in  X  O  X   X  S ). This class of constraints corresponds to the class of all the non-recursive Datalog programs for which data-complexity of query answering with respect to a fixed query and a fixed set of rules is in uniform AC 0 [15]. This allows Nyaya to delegate both reasoning and querying to the underlying relational database engine thus obtain-ing the same efficiency as for traditional database queries. Notice that the rules of  X  S always constitute a safe and non-recursive Datalog program. Therefore, if  X  O is FO-reducible then conjunctive-query answering over D under  X  O  X   X  S is also FO-reducible [10], implying that the organization of the data in the persistent storage does not affect the complexity of query answering. Based on this result, the construction of a new Semantic Data Kiosk from a fresh data source re-quires only to check for FO-reducibility of its meta-data. This check needs also to be applied whenever a user intro-duces a new user-defined Datalog  X  program to be used on topofanexistingstoreddataset.

Nyaya provides an implementation of two FO-reducible members of the Datalog  X  family: Linear Datalog  X  [5], and Sticky Datalog  X  [6]. Both languages are strictly more expressive than DL-Lite [7], the best-known language for tractable ontology-based data access and thus together offer a powerful formalism for expressing ontological constraints. Let Q be a user conjunctive query expressed in Datalog-like syntax over a semantic data kiosk. Following Figure 1, the processing of a query Q against a semantic data kiosk K =( X  O ,  X  S ,D )proceedsasfollows. 1. The CQ Q is reformulated using the rules in  X  O by applying TGD-Rewrite [11], a backward-chaining resolution algorithm that produces a perfect rewriting Q O of Q with respect to  X  O . By virtue of the FO-reducibility of Sticky and Linear Datalog  X  , Q O is a union of conjunctive queries. 2. Every Q i O  X  Q O is rewritten in terms of the storage tables using the rules in  X  S .Foreach Q i O , the result is another CQ Q i S that is perfect rewriting of Q i O with respect of  X  S . The union of all the Q i S forms a perfect rewriting of
Q with respect to  X  O  X   X  S . 3. The query  X  Q is rewritten in SQL and executed over the underlying DBMS.

Consider the Semantic Data Kiosk in Table 1 and the query Q : Q ( A )  X  name ( C , A ) , comp ( B , C ) , stock ( B )re-trieving the names of the financial indexes with at least one quoted stock (i.e., a sanity check query). After the applica-tion of the steps above, the final query is easily translated into the following SQL statement to be executed on the un-derlying relational database.

This dramatic simplification is due to the fact that the knowledge of the meta-model allows us to eliminate a high number of automatically generated subqueries, reducing the number of joins to be executed, with a noticeable improve-ment in performance.
Existing systems for the management of Semantic-Web data can be discussed according to two major issues: storage and querying .

Considering the storage, two main approaches can be iden-tified: the first focuses on developing native storage systems (such as AllegroGraph 3 or OWLIM 4 ) to exploit ad-hoc op-timisations, while the second (e.g., Sesame 5 ,TAP 6 ,Jena Virtuoso 8 and the semantic extensions implemented in Or-acle Database 11g R2 [9]) make use of traditional DBMSs, e.g. relational and object-oriented. Generally speaking, na-tive storage systems are more efficient in terms of load and update time, whereas the adoption of mature data man-agement systems has the advantage of relying on consoli-date and effective optimisations. Indeed, a drawback of na-tive approaches consists in the need for re-thinking query-optimization and transaction-processing techniques. Differ-ently from the other approaches, Nyaya provides a general-purpose storage policy for Semantic-Web data sets that al-lows the uniform management of different ontological for-malisms and language-independent reasoning capabilities. A rule-based mapping between the ontological entities and their representation in the storage separates concerns be-tween the query-processing algorithms and the physical or-ganization of data, which can be changed without affecting querying. Our current relational implementation takes ad-vantage of RDBMS optimization techniques based on index-ing and partitioning.

On the querying side, if we assume the absence of ontolog-ical constraints, the efficiency of query processing depends only on the logical and physical organization of the data and on the query language complexity. However reasoning is a fundamental requirement of Semantic-Web applications, and efficient reasoning algorithms are now available for sev-eral ontology languages based on description logics. A first family of systems [4, 19, 18] materialises all the inferences; this dramatically improves query-processing performance, but loading time is inflated because all the complexity of reasoning is deferred to the loading time and could poten-tially generate an exponential blow-up of space occupation. Moreover, update management is also critical since the in-ferred data must be updated accordingly. Full materialisa-tion is therefore suitable for stable data sets, especially when the query patterns are known in advance. A second class of systems [1, 14, 3, 12] including Nyaya executes inferencing on-the-fly through intensional query reformulation ,thusthe complexity of query processing depends on both the logi-cal organisation of the data and the expressiveness of the adopted data language and of the queries. This approach has many advantages: in the first place, space occupation is reduced to the minimum necessary to store the data set and it is possible to define a suitable trade-off for inference ma-terialisation depending on which queries are executed most frequently. Another advantage is the robustness to updates: since inferences are computed every time, there is no need for incremental updates. The major drawback of on-the-fly inferencing is the impact of reasoning time on query pro-cessing. In Nyaya , the separation between the ontological entities and the organization of data allow us to push the rewriting up to the level of the persistent storage, increasing the efficiency of query answering. In addition, the Datalog language adopted in Nyaya allows to test decidability and tractability of query answering for a given ontology in poly-nomial time.
One of the most interesting contributions of Nyaya is its overall efficiency that makes it appealing for general-purpose applications. In this section we provide a detailed experi-mental evaluation that supports this claim.

Nyaya has been implemented in Java and its inference component has been built by extending the IRIS Datalog engine 9 . Oracle 11g R2 has been used for the persistent storage, exploiting the native referential partitioning tech-nique of the DBMS. In particular, tables storing instance-level predicates are horizontally partitioned with respect to the foreign keys that refer to the tables storing schema-level predicates. A prototype implementation of Nyaya is avail-able on the Web 10 .Wecompared Nyaya with two well-known representatives of semantic-data management sys-tems that rely on full materialization: BigOWLim 11 and IODT 12 .Inparticular,weused BigOWLim v.3.3 over the File System, and IODT v.1.5 equipped with the Scalable Ontology Repository (SOR). In addition, we compared the rewriting technique of Nyaya with the on-the-fly inferenc-ing mechanisms of REQUIEM [14] over the Nyaya storage meta-model, since REQUIEM does not rely on a specific back-end. All the experiments have been performed on a dual-quad core 2.66GHz Intel Xeon, running Linux RedHat, with8GBofmemory,6MBcache,anda2-disk1Tbyte striped RAID array. In our experiments we used two widely-accepted benchmarks: LUBM 13 and UOBM [13], for which we considered an instance of 12.8 million triples. Since the expressiveness of the meta-data in the above data sets ex-ceeds the one of Datalog  X  (i.e., OWL-Lite for LUBM and OWL-DL for UOBM), in our experiments we manually pro-duced the FO-reducible approximation of the constraints for all the data sets. These approximations are available on the Nyaya Web site for the reproducibility of the experiments. We have also used Wikipedia3, a conversion of the English Wikipedia 14 into RDF. This is a monthly updated data set containing around 47 million triples. The expressiveness of its meta-data completely falls within that of Datalog  X  so no adaptations were made. The performance of the systems has been measured with respect to: (i) data loading, (ii) querying and reasoning, and (iii) maintenance.

Data loading. As shown in Table 2, Nyaya behaves much better than IODT and BigOWLim . Actually, the main advantage of our approach is that we execute pure data loading while the other systems have to pre-compile the knowledge-base to materialize it, producing a very large (and possibly exponential) number of tuples to be stored. The loading time for BigOWLim degrades dramatically from LUBM to UOBM (135 times slower) due to the in-creasing complexity of the meta-data, and from LUBM to Wikipedia3 (532 times slower), due to the large set of facts. Moreover our framework exploits the storage meta-model presented in Section 2 which allows a parallel execution of data import into the DBMS. BigOWLim has to process all the triples in memory while IODT needs to maintain com-plex block indexes. Since for REQUIEM we used our stor-age model, a comparison with it is not meaningful here.
Querying and reasoning. We p erformed cold-cache ex-periments (by dropping all file-system caches before restart-ing the systems and running the queries) and warm-cache experiments (without dropping the caches). We repeated all the tests three times and measured the mean execution times. Three phases of the query process have been con-sidered: preprocessing , execution and traversal . In the pre-processing phase BigOWLim loads statistics and indexes in main memory, while Nyaya and REQUIEM compute the rewriting of queries. IODT does not require preprocessing. We executed the sets of queries included in the LUBM and UOBM benchmarks (they are not reported here due to space limitation). A significant result is the speed-up between our approach and the others. We computed the speed-up for all data sets as the ratio between the execution time of each competitor approach P , and that of our approach Nyaya , or briefly as S = t P / t Nyaya . The speed-up for UOBM is re-ported in Figure 4. The vertical axis is the speed-up S com-puted for each competitor approach while on the horizontal axis we have the 13 queries. We also report the geomet-ric mean (GM) of speed-up values for both cold-cache and warm-cache experiments. In general BigOWLim performs better than IODT ,and Nyaya performs very well with re-spect to both of them: for cold-cache experiments, the mean Nyaya is 1,6 times and 1,9 times faster than BigOWLim and IODT respectively. For some queries BigOWLim and IODT are faster than our system since the query rewriting produces a large number of CQs to be executed. In these cases we have S&lt; 1. Actually, we are currently investigating further optimization techniques to significantly reduce this number. Nyaya is 25 times faster than REQUIEM (for the sake of readability in Figure 4 the maximum value on the vertical axis is 10). This is due to the query rewriting phase of
REQUIEM , that is the most expensive operation of the overall process. In [10] we reported in detail performance evaluation of each query. We omit the results for LUBM because the systems present a similar behavior.
 In the experiments done on the Wikipedia3 data set, Nyaya behaved in general better than the other systems, while IODT performed better than BigOWLim . This is due to the significant amount of data to process, although the set of ontological constraints are simpler in this case. For space limitations, the details have been presented in [10].
Maintenance. In these last experiments we tested main-tenance operations in terms of insertion (deletion) of new (old) terms or assertions, and update of existing terms or as-sertions. Figure 5 shows the performance of each system for update operations of data and meta-data over the LUBM, UOBM and Wikipedia data sets. As for data loading, the comparison with REQUIEM is meaningless. We consid-ered the mean response times (in msec) to insert, delete and update a meta-data predicate (i.e., a concept, a role or an attribute) or a fact in the database. The results show how Nyaya outperforms the other systems when dealing with in-sertions and deletions of both data and meta-data. In fact, in both BigOWLim and IODT the insertion a new predi-cate or a fact entails re-compiling a (possibly exponential) number of tuples referring to the new predicate while in Nyaya this simply requires an insertion or a deletion of a single-tuple. On the other hand, the update of an existing predicate needs to be propagated in order to satisfy the con-straints. As a consequence, the complexity of this task de-pends on the amount of data stored in the database. In this respect, Figure 5 shows the behavior of Nyaya in the case of meta-data updates when pure-SQL operations are used (i.e., independently of the DBMS) and when optimized partition-maintenance functions provided by the specific DBMS are adopted.
In this paper we presented Nyaya , a system for the uni-form management of different repositories of Semantic Web data. This work opens a number of interesting and chal-lenging directions of further research. A natural step is to further extend Nyaya to deal with non-FO-reducible lan-guages like Guarded Datalog  X  [5]. Moreover, on a different front, it would be interesting to exploit Nyaya for support-ing also the reconciliation and the integration of the data of the Semantic Data Market. [1] A.Acciarri,D.Calvanese,G.deGiacomo,D.Lembo, [2] P. Atzeni, P. Cappellari, R. Torlone, P. Bernstein, and [3] C. Beeri, A. Levy, and M. Rousset. Rewriting queries [4] B. Bishop, A. Kiryakov, D. Ognyanoff, I. Peikov, [5] A. Cal`  X , G. Gottlob, and T. Lukasiewicz. A general [6] A. Cal`  X , G. Gottlob, and A. Pieris. Advanced [7] D. Calvanese, G. de Giacomo, D. Lembo, [8] S. Ceri, G. Gottlob, and L. Tanca. What you always [9] E. Chong, S. Das, G. Eadon, and J. Srinivasan. An [10] R. De Virgilio, G. Orsi, L. Tanca, and R. Torlone. [11] G. Gottlob, G. Orsi, and A. Pieris. Ontological [12] C. Lutz, D. Toman, and F. Wolter. Conjunctive query [13] L. Ma, Y. Yang, Z. Qiu, G. T. Xie, Y. Pan, and [14] H. P  X  erez-Urbina, B. Motik, and I. Horrocks. Tractable [15] M. Vardi. On the complexity of bounded-variable [16] R. D. Virgilio, F. Giunchiglia, and L. Tanca, editors. [17] K. Wang and L. Yuan. First-order logic [18] Z. Wu, G. Eadon, S. Das, E. I. Chong, V. Kolovski, [19] J. Zhou, L. Ma, Q. Liu, L. Zhang, Y. Yu, and Y. Pan.
