 Support vector data description (SVDD) is very useful for one-class classification. However, it incurs high time complexity in handling large scale data. In this paper, we propose a novel and effi-cient method, named K-Farthest-Neighbors-based Concept Bound-ary Detection (KFN-CBD for short), to improve the SVDD learn-ing efficiency on large datasets. This work is motivated by the observation that SVDD classifier is determined by support vec-tors (SVs), and removing the non-support vectors (non-SVs) will not change the classifier but will reduce computational costs. Our approach consists of two steps. In the first step, we propose the K-farthest-neighbors method to identify the samples around the hyper-sphere surface, which are more likely to be SVs. At the same time, a new tree search strategy of M-tree is presented to speed up the K-farthest neighbor query. In the second step, the non-SVs are eliminated from the training set, and only the identified boundary samples are used to train the SVDD classifier. By removing the non-SVs, the training time of SVDD can be substantially reduced. Extensive experiments have shown that KFN-CBD achieves around 6 times speedup compared to the standard SVDD, and obtains the comparable classification quality as the entire dataset used. I.2.6 [ Artificial Intelligence ]: Learning X  Induction ; I.5.4 [ Pattern Recognition ]: Application Theory, Algorithm, Performance one-class classification
Support Vector Machine (SVM) [1, 2, 3] is a powerful technique in machine learning and data mining. It is firmly grounded in the framework of statistical learning theory developed by Vapnik. Sup-port Vector Data Description (SVDD) [4, 5] is an extension of SVM for one-class classification problems where only samples from one class ( target class ) are involved in the training phase. In SVDD, the training data is transformed from original data space ( input space ) into a higher dimensional feature space via a mapping function, making the data more compact and distinguishable in this feature space. A hyper-sphere is then determined to classify a test sample into the target class or the non-target class. To date, SVDD has been used in a wide variety of real-world applications [6, 7].
SVDD incurs a critical computational efficiency issue when it is applied on large scale datasets. This issue prevents a wide use of SVDD in handling real-life applications such as credit card transac-tional fraud detection, since it usually involves millions of records in several days. It would take months to learn the SVDD classifier on such large volume of data. Therefore, it is essential to explore new approaches to improve SVDD efficiency.

This paper proposes a novel and efficient approach, called K-farthest-Neighbors-based Concept Boundary Detection (KFN-CBD) , for a scalable and reliable SVDD classification on large scale data. The main idea of speeding up SVDD is to identify those samples that are more likely to be SVs, so that the training set size n is largely reduced and leads to a consequent high efficient SVDD. The main contributions of our work are summarized as follows.
Firstly, we present a framework for speeding up SVDD on large size datasets. It is known that the SVDD classifier is decided by SVs which lie on or outside the hyper-sphere [5]. However, the ex-act SVs can not be known until the SVDD classifier is constructed. From a new angle, our framework is proposed to detect the sam-ples around the hyper-sphere surface (called boundary samples ), which are more likely to be SVs. Then, the identified boundary samples, rather than the whole dataset, are used to train the SVDD classifier. In this way, a large amount of samples, which are more likely to be non-SVs, are eliminated from the training procedure. Consequently, the training cost can be substantially reduced, while maintaining comparable performance.

Secondly, we present a novel KFN method for boundary samples detection. In SVDD, after the data is mapped from the input space into a feature space via a proper kernel function, the target class tends to aggregate compactly and spherically in this feature space. A hyper-sphere is then constructed by enclosing the target class with high performance. Based on this geometric characteristic of SVDD, the KFN method is proposed to determine the boundary samples by considering the distances between the sample and its KFN. Distinguished from the related approaches, our method is proposed based on the geometric characteristic of SVDD classifier, which enables a more efficient and reliable identification of SVs.
Thirdly, we extend M-tree [8] to accelerate the KFN query. Al-Symbol Definition K the number of farthest neighbors considered Q query point
T ( O i ) covering tree of object O i r Q query radius of Q d min ( T ( O i )) distance lower bound between Q and T ( d max ( T ( O i )) distance upper bound between Q and T ( d k min { d min ( T ( O i )) } of K objects in FN array
PR a queue of pending requests which have not
FN array an array storing the KFN candidates ptr ( T ( O r )) pointer to the root of T ( O r ) oid ( O r ) object identifier
KFN (  X  ( x i )) top K-farthest neighbors list of  X  ( x i Algorithm 1 Framework for speeding up SVDD 1: Step 1 (Boundary Concept Preprocessing): For a given training 2: Step 2 (Boundary Concept Learning): Each sample  X  ( x though some optimal methods have been proposed for the K-nearest neighbors (KNN) query, little attention is pai d on KFN search. M-tree, which is originally designed to speed up K nearest neighbors search, is extended to KFN query by performing a novel tree search strategy. To the best of our knowledge, it is the first method for fast K-farthest neighbors query.

Experiment has been done to investigate the performance of our method on large size datasets. The results have shown that KFN-CBD obtains around 6 times speedup compared to the standard SVDD. At the same time, KFN-CBD acquires comparable clas-sification quality as the entire dataset used, in terms of accuracy, Tabl e 1.
The SVDD classifier is decided by only the SVs which lie out-side or on the hyper-sphere surface. In other words, removing the non-SVs does not change the SVDD classifier [5]. we propose to determine the SVs by identifying the samples lying around the hyper-sphere boundary (boundary samples). This is because, the SVs of SVDD are always located outside or on the hyper-sphere surface, and the samples near the hyper-sphere boundary are most likely to be SVs. Motivated by this, the framework to speed up SVDD on large size datasets is presented in Algorithm 1.
Based on the framework, the details of the boundary concept pre-processing and boundary concept learning are presented as follows:
The first preprocessing step is performed only once for the whole dataset. This stage determines a set of farthest neighbors for each sample. Compared to K-nearest neighbors, K-farthest neighbors method aims at identifying K samples which has the farthest dis-tances to the target object. At the end of this stage, we obtain a data structure which consists of the indexes and distances of the K-farthest neighbors for each sample in the whole dataset.
Similar to nearest neighbor determination, computing the K-farthest neighbors of each sample can be an expensive operation for large datasets. M-tree [8, 9], originally designed for fast K-nearest neigh-bors search, is extended here for speeding up K-farthest neighbors determination. M-tree for K-farthest neighbors query consists of two steps: tree building and tree query .Inthe tree building step, a hierarchical tree is constructed. The tree building process for K-farthest neighbors query is the same as that for K-nearest neighbors. In the tree query step, unlike K-nearest neighbors method, we pro-pose a new tree search strategy for efficient K-farthest neighbors search. Due to space limitation, we omit an introduction of M-tree and refer the reader to [8] about this topic. The details of tree query are given as follows.

At the beginning of query, r Q =0 and the search starts from the root node. During the query process, the entries in PR with max { d max ( T ( O i )) } will be searched first. This is because K-farthest neighbors are more likely to be acquired from the farthest subtree and d max ( T ( O i )) is the largest possible distance between query point Q and the objects in subtree T ( O i ) . Meanwhile, tree T (
O i ) with d min ( T ( O i )) &gt;d k will be inserted in FN array; when there is no entry in FN arry, d k =0 . Hence, FN array always contains K max { d min ( T ( O i )) } of objects having been searched, which are considered as the K-farthest neighbors candidates.
After insertion into FN array, let r Q = d k and the dynamic re-gion enlarges. Consequently, entries with d max ( T ( O i removed from PR, i.e. excluded from the search. This is because if the maximum distance d max ( T ( O i )) of Q and T ( O i than the minimum distance of those stored in FN array, i.e. d is unlikely to contain K farthest neighbors and hence can be safely pruned. When there is no object to be filtered, the query stops and K-farthest neighbors are contained in FN.

Take Figure 1 as an example to further illustrate the query pro-cess of K-farthest neighbors 1 . Here, K =2 . At the beginning of search, the query starts at the root level with [ root, 0] shown in Figure 1 (a). Thereafter, object II and I are put into PR and FN array; r Q stays 0 , because r Q is equal to the smallest d min ( O ) in FN array, i.e. r Q = d min ( I ) , while d shown in Figure 1 (b). Then object II is queried since d max d max ( I ) , and object D, I and C are thereafter placed into PR; con-sidering that d min ( D ) &gt;d min ( II ) and d min ( II d min ( D ) and d min ( II ) are put in FN array; r Q increases from to d min ( II ) , as shown in Figure 1 (c). Within the entries stored in PR , object D has the maximum d max ( T ( O )) , and hence it is the next object to be searched. After object D is filtered, object have the smallest distance lower bound d min ( O 7 ) and d among all the objects filtered; r Q expands to d ( Q, O 7 is enlarged, object C and I are fully covered by r Q , therefore, they can be pruned and removed from PR, as illustrated in Figure 1 (d). The query stops since PR is empty and the 2 farthest neighbors are contained in FN array, i.e. O 8 and O 7 .
The second step is the boundary concept learning, in which a subset of samples from the training data is selected to train the classifier. To do this, a scoring function is adopted to weight the samples according to their proximity to the hyper-sphere boundary.
To simplify the presentation, here we use O i to represent T The objective of the scoring function is to assign higher scores to samples closer to the hyper-sphere boundary.

Assume that the training data is { x 1 , x 2 , ... , x ping image of training data in feature space is {  X  ( x 1  X  ( x l ) } ,where  X  (  X  ) is a mapping function. Let KFN (  X  the top K-farthest neighbors list of  X  ( x i ) . The scoring function is given based on the K-farthest neighbors as follows:
From the scoring function in Equation (1), it can be seen that the value of f (  X  ( x i )) falls into the range of [0 , 1] from sample  X  ( x i ) to its K-farthest neighbors are large, the score is high. Likewise, if sample  X  ( x i ) is close to its K-farthest neighbors, a small score is obtained.

As we know, after the training target data is mapped from input space into a feature space via a proper kernel function, the target data tends to aggregate compactly and spherically in the feature space, so that a hyper-sphere classifier is obtained by enclosing the target data with satisfactory accuracy, as shown in Figure 2 (a). Considering this geometrical characteristic of SVDD, the scoring function can be effective in determining the boundary samples. Let us take Figure 2 (b) as an example to illustrate how the scoring function contributes to boundary sample determination.

In Figure 2 (b),  X  ( x 1 ) is a boundary sample which lies near the hyper-sphere boundary, while  X  ( x 5 ) is an internal sample which is located deeply inside the sphere. Samples  X  ( x 2 ) ,  X  ( are the top 3 farthest neighbors of sample  X  ( x 1 ) , while the thest neighbors of sample  X  ( x 5 ) are  X  ( x 6 ) ,  X  ( x order to make the illustration clear, the other samples have not been drawn out in Figure 2 (b). Let us consider the scoring function of  X  ( x 1 ) and  X  ( x 5 ) as follows:
From Figure 2 (b), it is observed that by tuning the parameter K , we can make the distances between the boundary sample  X  ( its K-farthest neighbors generally larger than those of the internal sample  X  ( x 5 ) . Hence, we have f (  X  ( x 1 )) &gt;f (  X 
From this example, it can be seen that by selecting an appro-priate parameter K , a large score will be assigned to the sample near the hyper-sphere boundary, while the sample lying inside the hyper-sphere usually has a small score. Considering that the SVDD classifier is a tight sphere enclosing the data, the sample near the sphere surface is usually at a greater distance from its K-farthest neighbors than the sample which deeply perches inside the sphere. Therefore, the scoring function can be considered as an effective Figure 2: Illustration of the scoring function with 3-farthest neighbors way to determine the samples which are more likely to lie around the hyper-sphere boundary.

Selection of subset size . After K-farthest neighbors have been determined by the extended M-tree, each sample in the dataset is as-signed a score according to Equation (1), which indicates its prox-imity to the hyper-sphere surface. The sample with a large score tends to lie near the hyper-sphere boundary and that with a small value, always locates deeply inside the hyper-sphere. To prune out the samples, we firstly sort the samples according to their scores. The sum H of all sorted scores is obtained. Starting from the sam-ple with the highest score, we keep picking up the samples until the sum of chosen samples reaches a pre-specified percentage of H . In this way, the samples that most significantly contribute to the overall score can be selected out for training.

Suppose that a subset of samples with largest scores are selected out and are contained in S sub . We re-index the samples in S  X  ( x 1 ) ,  X  ( x 2 ) , ... ,  X  ( x | S sub | ) ,sothat The training problem of SVDD is then transformed as follows:
Compared to the standard SVDD which is trained on the entire dataset, our proposed approach learns the SVDD classifier using only a subset of samples which are located near the hyper-sphere boundary. As a result, the computation complexity of SVDD train-ing can be largely reduced and the accuracy is as good as if the entire dataset had been used.

In all, the kernel function is usually used to map the training data from the input space into a higher dimensional feature space. By adopting a proper kernel parameter, the training data in the fea-ture space can become more compactly and spherically. Based on the transformed data, the scoring function is presented to weight the samples based on the distances to their K-farthest neighbors. By tuning the parameter K , we can make the scoring function dis-criminative enough to identify the boundary samples and internal samples. At the same time, we can adjust the subset size to make it 50%).
 for score evaluation, T l -time taken by subset learning) (K = 100, P = 50%). include as many as SVs, so that the subset can be utilized to train a more accurate classifier.
We investigate the accuracy, recall, precision and efficiency of our method on large size datasets. For comparison, the standard SVDD and two representative data processing methods, i.e., ran-dom sampling method [10] and selective sampling method [11], are used as baselines.

All the experiments run on a Linux machine with 2.66GHz pro-cessor and 4GB DRAM. The training of SVDD is performed based on LibSVM, and RBF kernel is adopted.
The large Shuttle dataset 2 consists of 58000 samples with 7 classes and 9 features. The sample sizes from class 1 to class 7 are 45586 , 50 , 171 , 8903 , 3267 , 10 and 13 , respectively.
Since this dataset is designed for multi-class classification, we follow the operations in [5] to obtain sub-dataset for one-class clas-sification purposes. Since the first class of each occupies over samples respectively, we only consider it as the target class.
In this section, we will investigate the performance of our method when K = 100 and P = 50% . Although larger values for K and P can improve performance, it is found that our method has reached nearly the same classification quality as the whole dataset used under K = 100 and P = 50% . The performance variation of our method under different K and P will be discussed later.
Table 2 presents the results of standard SVDD and our approach in terms of accuracy, precision and recall. It is clearly shown that our approach can obtain nearly the same results as SVDD. In Table 2, the accuracies of random sampling and selective sampling on seven sub-datasets are also given. It can easily be seen that our proposed approach obtains significantly better accuracies than the comparable random sampling and selective sampling method.
The training time of our approach consists of the boundary con-cept preprocessing time T p and the subset learning time T mainly refers to the score evaluation, while T l is the time taken by learning the classifier on the selected subset. For all the results re-lated to the speedup, we have already taken both T p and T account. Table 3 specifies the training time and the number of SVs of our method. The speedup comparisons of SVDD, random sam-pling, selective sampling and our method are also given. It can be seen that our approach is around 6 times faster than SVDD, and is on average 3 times faster than the comparable selective sampling method.

From the above analysis, it is clear that our approach saves a large amount of training time while guaranteeing classification qual-
Download at http://archive.ics.uci.edu ity, compared to the standard SVDD. In addition, our approach turns out to be much faster than both the random sampling method and selective sampling method, while demonstrating an even better classification performance.
SVDD faces critical computational complexity in handling large scale data. This paper has proposed an efficient strategy to detect samples around the hyper-sphere surface, providing a reliable and scalable solution for speeding up SVDD on large datasets. Our proposed KFN-CBD holds the following characteristics: (1) a K-farthest neighbors based scoring function is proposed to weight the samples according to their proximity to the hyper-sphere surface; (2) by adopting a new tree search strategy, M-tree is extended for speeding up K-farthest neighbors determination. To the best of our knowledge, this is the first method for fast k-farthest neighbors query.
This work is sponsored in part by QCIS, CMCRC. [1] V. N. Vapnik. Statistical learning theory. John Wiley and [2] D.Z. Liu and K.A. Hua. Incremental query evaluation for [3] G.J. Shanahan and N. Roma. Boosting support vector [4] D.M.J. Tax and R.P.W. Duin. Support vector data description. [5] D.M.J. Tax and R.P.W. Duin. Support vector data [6] D. Tax and R. Duin. Support vector data description applied [7] S.-W. Lee and J. Park. Low resolution face recognition based [8] P. Ciaccia, M. Patella, and P. Zezula. M-tree: An efficient [9] P. Ciaccia, M. Patella, and P. Zezula. Bulk loading the [10] L. Breiman. Bagging predictors. Machine Learning , [11] P.J. Kim, H.J. Chang, D.S. Song, and J.Y. Choi. Fast support
