 competition is just a mouse click away, it is crucial to segment the customers intelligently in order to offer more targeted and personalized products and services to them. Traditionally, customer segmentation is achieved using statistics-based methods that compute a set of statistics from the customer data and group customers into segments by applying distance-based clustering algorithms in the space of these statistics. In this paper, we present a direct grouping based approach to computing customer segments that groups customers not based on computed statistics, but in terms of optimally combining transactional data of several customers to build a data mining model of customer behavior for each group. Then building customer segments becomes a combinatorial optimization problem of finding the best partitioning of the customer base into disjoint groups. The paper shows that finding an optimal customer partition is NP-hard, proposes a suboptimal direct grouping segmentation method and empirically compares it against traditional statistics-based segmentation and 1-to-1 methods across multiple experimental conditions. We show that the direct grouping method significantly dominates the statistics-based and 1-to-1 approaches across all the experimental conditions, while still being computationally tractable. We also show that there are very few size-one customer segments generated by the best direct grouping method and that micro-segmentation provides the best approach to personalization. Index Terms  X  customer segmentation, marketing application, personalization, 1-to-1 marketing, customer profiles 
Customer segmentation, such as customer grouping by the level of family income, education, or any other demographic variable, is considered as one of the standard techniques used by marketers for a long time [25]. Its popularity comes from the fact that segmented models usually outperform aggregated models of customer behavior [26]. More recently, there has been much interest in the marketing and data mining communities in learning individual models of customer behavior within the context of 1-to-1 marketing [23] and personalization [5], when models of customer behavior are learned from the data pertaining only to a particular customer. These learned individualized models of customer behavior are stored as parts of customer profiles and are subsequently used for recommending and delivering personalized products and services to the customers [1]. 
As was shown in [14], it is a non-trivial problem to compare segmented and individual customer models because of the tradeoff between the sparsity of data for individual customer models and customer heterogeneity in aggregate models: individual models may suffer from sparse data, while aggregate models suffer from high levels of customer heterogeneity. Depending on which effect dominates the other, it is possible that models of individual customers dominate the segmented or aggregated models, and vice versa. 
A typical approach to customer segmentation is based on the statistics-based approach that computes the set of statistics from customer X  X  demographic and transactional data [3, 14, 28], such as the average time it takes the customer to browse the Web page describing a product, maximal and minimal times taken to buy an online product, RFM statistics [21], etc. After such statistics are computed for each customer, the customer base is partitioned into customer segments by using various clustering methods on the space of the computed statistics [14]. It was shown in [14] that the best statistics-based approaches can be effective in some situations and can even outperform the 1-to-1 case under certain conditions. However, it was also shown in [14] that this approach can also be highly ineffective in other cases. This is primarily because computing different customer statistics would results in different n -dimensional spaces, and various distance metrics or clustering algorithms would yield different clusters. Depending on particular customer statistics, distance functions and clustering algorithms, significantly different customer segments can be generated. 
In this paper, we propose the direct grouping segmentation approach that partitions the customers not based on computed statistics and particular clustering algorithms, but in terms of directly combining transactional data of several customers, such as Web browsing and purchasing activities, and building a single model of customer behavior on this combined data. This approach avoids the pitfalls of the statistics based-approach in that it does not require selection of arbitrary statistics and grouping customers based on these statistics. Instead, it provides a more direct approach to customer segmentation by combining customers X  data collectively resulting in better model for this group of customers. In this paper we try to partition the customer base into an optimal set of segments using the direct grouping approach, where optimality is defined in terms of a fitness function of a model learned from the customer segment X  X  data. We formulate this optimal partitioning as a combinatorial optimization problem and show that it is NP-hard. Then we propose a suboptimal polynomial-time direct grouping method, called Iterative Merge ( IM ), and compare it to the standard statistics-based and 1-to-1 approaches. We show that IM significantly dominates the statistics-based and 1-to-1 methods across all the experimental conditions examined in this paper, thus demonstrating the applicability of the direct grouping methods to building personalized models of customers. Therefore, we demonstrate empirically that it is better to segment customer bases by first directly partitioning customer data and then building predictive models from the partitioned data rather than first computing some arbitrary statistics, clustering the resulting n -dimensional data points into segments, and then building predictive models on these segments. We also examine the nature of the segments generated by the IM method and observe that there are very few size-one segments, that the distribution of segment sizes reaches its maximum at a very small segment size, and that the rate of decline in the number of segments after this maximum follows a Zipf X  X  distribution. This observation, along with the dominance of IM over the 1-to-1 method, provides support for the micro-segmentation approach to personalization [16], where the customer base is partitioned into a large number of small segments, such as undergraduate students at University of XYZ majoring in computer science and living in the dorms. 
In summary, we make the following contributions in this paper:  X  Propose the direct grouping method for segmenting customer bases, formulate the optimal segmentation problem, and show that it is NP-hard.  X  Propose a suboptimal direct grouping method, IM, and compare IM against the statistics-based segmentation and the 1-to-1 approaches and demonstrate that IM significantly dominates them.  X  Show that the tail of the cluster size distribution generated by IM follows a Zipf X  X  distribution and that there are very few size-one clusters. This provides support for the micro-segmentation approach to personalization. 
The problem of optimal segmentation of customer base can be formulated as follows. Let C be the customer base consisting of N customers, each customer C i is defined by the set of m demographic attributes A = {A 1 , A 2 , ..., A transactions Trans(C i ) = {TR i1 , TR i2 , ..., TR by customer C i , and h summary statistics S i S }, computed from the transactional data Trans(C Moreover, each transaction TR ij is defined by a set of transactional attributes T = {T 1 , T 2 , ..., T p }. The number of transactions k i per customer C i varies. Finally, we combine the demographic attributes {A i1 , A i2 , ..., A im ,} of customer C and his/her set of transactions {TR i1 , TR i2 , ..., TR complete set of customers X  data TA(C i ) = {A i1 , A TR our work. As an example, assume that customer C i can be defined by attributes A = {Name, Age, Income, and other demographic attributes}, and by the set of purchasing transactions Trans(C i ) she made at a Web site, where each transaction defined by such transactional attributes T as an item being purchased, when it was purchased, and the price of an item. Finally, a summary statistics vector S computed for all of C i  X  X  purchasing sessions and can include such statistics as the average amount of purchase per a Web session, the average number of items bought, and the average time spent per online purchase session. 
Given the set of n customers C 1 , ..., C n , and their respective customer data p i = { TA(C 1 ),...,TA(C to build a single model M i of this group of customers p measure its performance using some fitness function f mapping the set of customer data p i into reals, i.e., f(p  X  . For example, model M i can be a decision tree built on its predictive accuracy on the out-of-sample data or obtained using k-fold cross-validation. 
The function f can be very complex in general, as it represents the predictive power of an arbitrary predictive model M i trained on all the customer data contained in p For example, f could the relative absolute error of a neural network model trained and tested on p i via ten-fold cross validation. Another example could be the R generated from a logistic regression of all the transactional and demographic variables on one dependent purchase variable using all data contained in p i . This means that, in general, function f does not have  X  X ice X  properties, such as additivity or monotonicity. For example, f({TA(C i )}) can be greater than, less than, or equal to f({TA(C i ),TA(C any i, j. This lack of nice properties of fitness functions will be a defining issue when we formulate an optimal customer segmentation problem later in this section and will make this problem computationally complex. 
Partitioning the customer base C into a mutually exclusive collectively exhaustive set of segments P = {p 1 ,...,p k }, by building models M i for each segment p described above, is called direct grouping segmentation. Note that in this approach we group customers into segments based on some performance criteria for the segment rather than clustering customers based on intra or inter cluster distance measures. We next formulate an optimal segmentation problem that does this partitioning in the  X  X est possible X  manner. 
Optimal Customer Segmentation problem. Given the customer base C of N customers, we want to partition it into the disjoint groups P = {p 1 ,...,p k }, such that the models M i built on each group p i would collectively produce the best performance for the fitness function f(p i ) taken over p ,...,p k . Formally, this problem can be formulated as follows. Let  X  i be a weighting measure specifying  X  X mportance X  of segment i . Some examples of  X  i simple average 1/k and proportional weights |TA(p i )|/|TA(C)| . Then we want to find partition of the customer base C into the set of mutually exclusive collectively exhaustive segments P = {p 1 ,...,p segment p i is defined by its customer data p {TA(C j ),...,TA(C m )}, such that the following fitness score is maximized (or minimized if we used error rates as our fitness score) over all possible partitions P . 
Note that there can be combinations of customer transactions groupings, known as the Bell numbers, which are the number of ways that N distinguishable objects can be grouped into non-empty sets. Since B N are very large numbers, even for small N  X  X , finding an optimal partition constitutes a complex optimization problem. Due to the arbitrary nature of the fitness function f , the optimal customer segmentation is a combinatorial partition problem [13] with very little constraints. 
Proposition. Optimal Customer Segmentation (OCS) problem is NP-hard. 
This result can be obtained by reducing the clustering problem, that is NP-hard [4], to the OCS problem. 
Since the OCS problem is NP-hard, we propose the following suboptimal polynomial customer segmentation methods providing reasonable fitness scoring results:
Statistics based  X  These methods group customers by first computing some statistics from customers X  demographic and transactional data, consider these statistics as points in an n -dimensional space, and then group customers into segments by applying various clustering algorithms to these n -dimensional points. 1-to-1  X  This approach builds customer segments of size 1 (individual models of customers) by learning them only from the data pertaining to individual customers. 
Direct Grouping  X  We described direct grouping in this section. Instead of looking for an optimal grouping of customers, which is NP-hard, we present a polynomial-time suboptimal direct grouping method IM in Section 4. work. 
The problem of finding the global optimal partition of customers is related to the work on a) combinatorial optimization problems in operations research, b) customer segmentation and clustering in marketing, and c) data mining research on customer segmentation. We examine the relationship of our work to these three areas of research in this section. 
Combinatorial optimization models are used across a wide range of applications. The common feature among these problems is that in many practical problems, activities and resources, such as tasks and people are indivisible [13]. Combinatorial optimization problems are in general considered NP-hard, however, depending on the mathematical formulation of a particular problem, exact solutions or close to exact approximations can be achieved. While we cannot use any existing problem formulations in solving our research question due to the lack of additivity, monotonicity and other  X  X ice X  characteristics of our fitness function, we could take cues from various approaches used in solving combinatorial optimization problems. These approaches include  X  X ranch &amp; bound X  enumerative techniques [17], Lagrangian relaxation and decomposition methods [10], and cutting plane algorithms based on polyhedral combinatorics[9]. 
Despite recent advances in finding solutions to various combinatorial optimizations problems, there are still a large set of problems considered too complex to derive optimal solutions [13]. Therefore, various heuristics were explored in obtaining good solutions that have no guarantees as to their  X  X loseness X  to the optimal solution. Our research problem falls in this category. Heuristics used by operation researchers in solving combinatorial optimizations problems include greedy hill-climbing [18], simulated annealing [11], evolutionary algorithms [20], and neural networks [2]. In our work, we deployed the greedy hill-climbing approach in conjunction with the branch and bound enumerative techniques in developing our fitness function based methods. 
Our work is also related to the work on clustering that partitions the customer base and their transactional histories into homogeneous clusters for the purpose of building better models of customer behavior using these clusters [3, 14, 28]. However, any objective measure of intra-cluster similarity and inter-cluster dissimilarity is hard to come by and is shown to be rather erratic [14]. Instead, in this paper we group customer X  X  transactions together and measure performance not in some sense of  X  X ntra-cluster similarity X , but rather based purely on some performance fitness function having direct implication on the effective utility of any subsequent customer groupings. 
Building on top of previous data mining research on customer segmentation [14, 28], this research aims to formulate automated customer segmentation methods that are not influenced by arbitrary selection of summary statistics or population specific factors such as customer heterogeneity and data volume. 
In this section, we describe the details of the statistics-based, 1-to-1 and direct grouping approaches and propose their representative implementations before empirically comparing them across various experimental settings. 
In terms of the statistics-based segmentation, we consider the following two variants of the hierarchical approach that are described in [7, 12] and deployed in [14, 22]: 
Hierarchical Clustering (HC) : Using the same hierarchical clustering techniques as in [14], we learn predictive models of customer behavior of the form where X 1 , X 2 , ..., X p are some of the demographic attributes from A and some of the transactional attributes from T (see Section 2), and function f  X  is a model that predicts certain characteristics of customer behavior, such as prediction of the product category or the time spent on a Web site purchasing the product. The correctness measure of this prediction is our fitness function f (defined in Section 2). These models f  X  , defined by expression (1), are built for the groups of customers that are obtained as follows. 
Starting from a single aggregated grouping of all customers, we use hierarchical clustering methods on the set of summary statistics {S 1 , ..., S h } and partition the set of m customers by iteratively applying Euclidean distance-based clustering algorithms in the n -dimensional customer summary statistics space. The Hierarchical Clustering ( HC) method generates new levels of segment hierarchy via progressively smaller groupings of customers X  transactions until the single customer (1-to-1) level is reached and each segment contains transactions from a single customer. The decision to group certain customers together is done by clustering via FarthestFirst [12], a greedy k-center unsupervised clustering algorithm that is found to perform well in [14] on customer summary statistics and demographics attributes {A 1 , A 2 , ..., A ..., S h }. We compute these segments for each level of the segmentation hierarchy (containing progressively smaller segments), and for each level L, compute the weighted sum of fitness scores . Then the segmentation level with the highest overall fitness score (besides the 1-to-1 level) is selected as the best possible segmentation of the customer base. groupings of customer transactions from unsupervised clustering algorithms, as HC does, EC forms customer groupings by building a C4.5 decision tree  X  on customer summary statistics and demographics {A 1 , A 2 , ..., A m ..., S h }, where the class label is the model X  X  dependent variable Y in (1). Unlike HC , this approach is a supervised clustering algorithm, where  X  X imilar X  customers are grouped in terms of summary statistics and demographics to reduce the entropy of the class label. Once the C4.5 forms the groupings based on the principle of class label entropy minimization, we compute the weighted sum of fitness scores generated by f  X  in (1) across these different groupings of customer transaction data. Intuitively, this should be a better approach to clustering customers than HC because by making grouping decisions based on class label purity, we are in effect measuring similarity in the output space, which reduces the variance of the dependent variable Y classified by our predictive models. In addition, there is no fixed splitting factor as in the case of HC, as each tree split is based on the number of different values an independent attribute may have. Thus, each split could result in a different number of sub-clusters, which could provide extra flexibility for building more homogeneous and better performing customer segments. However, the formation of customer groups is still based on customer summary statistics which, depending on the types of statistics used, can yield very different decision trees. 
As explained in Section 2, the 1-to-1 approach builds predictive models of customer behavior only from individual customer X  X  transactional data. In other words, we build a predictive model (1) for each customer C i , i = 1, ..., N, using only the demographic and the transactional data of that customer, and we do not have to deal with customer grouping at all in this case. For each model of customer C we compute fitness function f(C i ) (e.g., using 10-fold cross-validation) and obtain the whole distribution of these fitness scores for i = 1, ..., N.
The direct grouping approach makes decision on how to group customers into segments by directly combining different customers into groups and measuring the overall fitness score as a linear combination of fitness scores of individual segments, as described in Section 2. Since the optimal segmentation problem is NP-hard (see Section 2), we propose the following suboptimal method IM . 
Iterative Merge (IM): IM is an iterative segmentation reduction approach which starts from a set of single customer segments and iteratively merges together two segments that result in better performance combined. More specifically, starting with segments containing individual customers, IM seeks to iteratively merge two existing segments Seg A , and Seg B at a time when 1) the predictive model based on the combined data performs better and 2) combining Seg A with any other existing segments would have resulted in a worse performance than the combination of both Seg A and Seg B . IM is greedy because it attempts to find the best pair of customers groups and merge them together resulting in the best merging combination. The specifics of the IM algorithm are presented in Figure 1. 
IM runs in O(n 3 ) in the worst case because a single merge of two groups takes O(n 2 ) time in the worst case, and there can be up to n of such merges. However, in practice, the search space of IM is not very large because it merges groups, not individual customers, at a time, and the empirical results reported in Section 6 confirm this observation. customer segments of comparable sizes, where each customer segment under merging consideration can significantly affect the performance of the combined segmentation, thus lessen the chance of building large and poorly performing customer segments. 
To compare the relative performance of direct grouping, statistics-based, and 1-to-1 approaches, we conduct pair-wise performance comparisons using a variant of the non-parametric Mann-Whitney rank test [19] to test whether the fitness score distributions of two different methods are statistically different from each other. To ensure robustness of our findings, we set up the pair-wise comparisons across the following four dimensions: 
Types of datasets. In our study we worked with the following datasets: (a) Two  X  X eal-world X  marketing datasets containing panel data 1 of on-line browsing and purchasing activities of Web site visitors and panel data on beverage purchasing activities of  X  X rick-and-mortar X  stores. The first dataset contains ComScore data from Media Metrix on Internet browsing and buying behaviors of one hundred thousand users across United States for a period of 6 months (available via Wharton Research Data Services -http://wrds.wharton.upenn.edu/ ). The second dataset contains Nielsen panelist data on beverage shopping behaviors of 1,566 families for a period of one year.
 The ComScore and Nielsen marketing datasets are very different in terms of the type of purchase transactions (Internet vs. physical purchases), variety of product purchases, number of individual families covered, and the variety of demographics. Compared to Nielsen X  X  beverage purchases in local supermarkets, ComScore dataset covers a much wider range of products and demographics and is more representative of today X  X  large marketing datasets. We further split these two real world datasets into four datasets of ComScore high-and low-volume customers, which represents the top and bottom 1,000 customers in terms of transaction frequencies respectively. Similarly, Nielsen high-and low-volume customer datasets were generated using the top and bottom 500 customers in terms of transaction frequencies respectively. (b) Two simulated datasets representing high-volume customers ( Syn-High ) and low-volume customers ( Syn-Low ) respectively, where within each dataset, customer differences are defined by generating different customer summary statistic vector S i for each customer i . All subsequent customer purchase data are generated from the set of summary statistic vectors S i . 
The Syn-Low and Syn-High datasets were generated as follows. 2,048 unique customer summary statistics were generated by sampling from ComScore customer summary statistics distributions, which is then used to generate the purchase transactions with four transactional variables. The number of transactions per customer is also determined from ComScore customer transaction distributions. This dataset is used to better simulate real world transactional datasets. 
Since for the ComScore and Nielsen we consider two datasets (each having high-and low-volume customers), this means that we use six datasets in total in our studies. Some of the main characteristics of these six datasets are presented in Table 1. In particular, CustomerType column specifies the transaction frequency of these datasets, High meaning that customers perform many transactions on average, while Low means only few transactions per customer. The columns  X % of Total Population X ,  X  X amilies X , and  X  X otalTransactions X  specify the percentage of total data population, the number of families, and the sample family transactions contained in the sample datasets. 
Types of predictive models. Due to computational expenses of the model-based methods, we build predictive models using two different types of classifiers via Weka 3.4 system [27]: C4.5 decision tree [24] and Na X ve Bayes [15]. These are chosen because they represent popular and fast-to-generate classifiers. 
Dependent variables. We built various models to make predictions on transactional variables, TR ij , and compare discussed approaches across different experimental settings. Examples of some of the dependent variables are day of the week, product price, category of website in ComScore datasets, and category of drinks bought, total price, and day of the week in the Neilson datasets. The data we used to train any one model are customer C independent variables X 1 , X 2 ,..., X p , except TR ij performance measures: percentage of correctly classified instances (CCI), root mean squared error (RME), and relative absolute error (RAE) [27]. 
Given models  X  and  X  ,  X  is considered  X  X etter X  than only when it provides better classification results and fewer is the fitness function which we use in IM to pick the best possible merge during every iteration. To pick the best segment level in HC , the CCI, RME, and RAE distributions of different segment levels are compared separately in choosing the best performing segment level that has the most right skewed CCI distribution and left skewed RME and RAE distributions. 
In terms of data pre-processing, we discretized our datasets to improve classification speed and performance [6]. Nominal transaction attributes, such as product categories, were discretized to roughly equal representation in sample data to avoid overly optimistic classification due to highly skewed class priors. We also discretized continuous valued attributes such as price and Internet browsing durations based on entropy measures via our implementation of Fayyad X  X  [8] recursive minimal entropy partitioning algorithm. and EC across all these dependent variables, classifiers, and six datasets to select the best one. The results of these comparisons are reported in the next section. 
In this section, we present our empirical findings. As mentioned in Section 5, we compare the distribution of performance measures generated by considered predictive models across various experimental conditions. Since we make no assumptions about the shape of the generated performance measure distributions, we use a variant of the non-parametric Mann-Whitney rank test [19] to test whether the distribution of performance measures of the one method is statistically different from another method. For example, to compare HC against the 1-to-1 method for the CCI measure, consider the distribution of the CCI measure generated for the best segmentation level of the HC hierarchical clustering, and compare it against the distribution of the CCI measure obtained for each individual customer. Then we apply the Mann-Whitney rank test to compare the two distributions. 
The null hypothesis for comparing distributions generated by methods A and B for a performance measure is: (I) H 0 : The distribution of a performance measure generated by method A is not different from the distribution of the performance measure generated by method B. H 1 +: The distribution of a performance measure generated by method A is different from the distribution of the performance measure generated by method B in the positive direction. generated by method A is different from the distribution of the performance measure generated by method B in the negative direction. 
To test these null hypotheses across distributions of performance measures generated by different methods, we proceeded as follows. For each dataset, classifier and dependent variable we generate 3 sets of customer groups, CG 1 , CG 2 and CG 3 , using our three segmentation methods IM, HC and EC . Let cg ij denote a particular group j of customers belonging to customer group set CG i generated by method i ( IM, HC or EC ). For each cg ij , , we generate a separate model, m ij , that predicts the dependent variable of the model via ten-fold cross validation and computes three performance measures CCI ij , RME ij , and RAE ij . 
Let M i denote the set of models generated from evaluating all customer groups in CG i for method i , and let CCI i , RME i , and RAE i be three sets of performance measures evaluated on model set M i for all customer groups in CG i . To compare segmentation method i  X  X  performance against method h , we would compare whether the distribution of performance measures of CCI i , RME RAE i is statistically different from that of CCI RAE h respectively via the Mann-Whitney rank tests using hypotheses H 0 , H 1 +, H 1 -specified above. 
For example, for the comparisons involving HC and 1-to-1 , the above scenario of comparing three measures is repeated across six datasets, three dependent variables per dataset, and two classifiers, resulting in 108 statistical significance tests per method to method comparison pair. 
We next compare HC against EC and the direct grouping method IM against the statistics-based and the 1-to-1 approaches to determine the best segmentation approach. We compare the two statistics-based methods HC and EC across six datasets, three dependent variables per dataset, two classifiers and three performance measures per model to determine which method is better. This resulted in the total of 108 Mann-Whitney tests for this pair-wise comparison. Table 2 lists the number of statistical tests rejecting the null hypothesis (I) at 95% significance level. As Table 2 shows, only 2 out of 108 produced statistically significant differences between the HC and EC methods, in which HC dominated EC . 
From this comparison we can conclude that HC and EC methods provide similar performance results with HC  X  X lightly X  dominating EC , i.e., EC  X  HC . Since there is a small difference between the two statistics-based segmentation methods, we could have chosen any of the two methods. We decided to choose HC as a representative statistics-based segmentation method to be compared against IM and 1-to-1 approaches in the following section. 
In this section, we compare the best methods out of the three different modeling approaches to predicting customer behavior. As stated in Sections 6.1, we selected the HC method to represent statistics-based grouping methods because it outperformed EC . Therefore, we compared HC , IM and 1-to-1 methods across the six datasets, three dependent variables per dataset, two classifiers, and three performance measures. This resulted in the total of 108 Mann-Whitney tests per pair-wise comparison. 
Table 3 summarizes the three pair-wise comparisons by listing the number of statistical tests rejecting the null hypothesis (I) at 95% significance level. As evident from the number of statistically significant test counts, IM clearly dominates 1-to-1 , which in turn dominates HC . 
As demonstrated in [14], HC dominates 1-to-1 for the low-volume, highly idiosyncratic customers assuming that a good clustering method is used for HC . Therefore, we decided to do the same type of comparison, as reported in Table 3 but just for the High-Volume customers (Table 4) and the Low-Volume customers (Table 5), where the high-volume customers constitute customers with at least hundred transactions per household, and the low-volume customers constitute customers with just ten transactions per household for ComScore and simulated datasets and about forty transactions per household for Nielsen datasets (See Table 1). 
As Tables 4 and 5 show, 1-to-1 still dominates HC for both high-and low-volume customers, suggesting that the statistics-based clustering is inferior to 1-to-1 for both the high-and the low-volume customers. We also note that 1-to-1 performs somewhat better against IM among high-volume datasets relative to low-volume datasets, which does make intuitive sense, as the high-volume customers would be more probable to have enough transaction data to effectively model individual customer behavior. However, IM still shows significant performance dominance against both 1-to-1 and HC across all the experimental conditions, including high-and low-volume customers. To get a sense of the magnitude of the dominance that IM has over 1-to-1 , we computed the difference between the medians of each distribution. For a particular dataset, dependent variable, classifier and performance measure, we took the two distributions of the performance measures across all the segments for the IM and all the individual customers for the 1-to-1 methods. Then we determined the medians of the two distributions 2 (one for IM and one for 1-to-1 ), and computed the differences between them. We repeated this process for all the 108 comparisons across the six datasets, 3 dependent variables per dataset, two classifiers, and 3 performance measures, and plotted out the histograms of the median differences for the CCI, RME, and RAE measures in Figure 2-4 respectively. Note that to plot out histograms across real values, we grouped the median differences across the distribution comparisons into bins along the X-axis, while the Y-axis represent the number of tests that falls within the median difference bin. 
The negative values for the CCI measure and positive values for the RME and RAE measures in Figures 2  X  4 show that IM significantly outperforms the 1-to-1 method across most of the experimental conditions, thus providing additional visual evidence and the quantitative extent of the dominance of IM over 1-to-1 that was already statistically demonstrated with the Mann-Whitney tests. 
We also did the same type of comparison for the HC and the IM methods. We show in Figure 5-7 the left skewed median difference distribution for the CCI measure and the right skewed median difference distributions for the RME and RAE measures. As in the case of IM vs. 1-to-1 , Figures 5  X  7 clearly demonstrate IM  X  X  dominance over HC . 
Lastly, we did the same type of comparison for the HC and the 1-to-1 methods, and the results are reported in Figures 8  X  10. As Figure 8 shows, 1-to-1 clearly dominates HC in terms of median CCI difference distributions. However, the small difference in RME error and the relatively evenly distributed RAE median difference distribution indicate that 1-to-1 produces approximately the same amount of errors as HC . Thus, unlike the case of IM  X  X  dominance over HC , the 1-to-1 approach does not clearly dominate HC across all the experimental conditions. 
We can gain further insight into the issue of performance dominance by plotting percent histograms of CCI distributions across different methods and different experimental conditions. Because of the space limitation, we present only three representative examples of these 108 performance measure histograms in Figures 11  X  13. 
Figure 11 shows the histogram of the CCI performance measure distribution of the Na X ve Bayes models generated by 1-to-1 approach across 1,000 unique customer X  X  data from the High-volume ComScore dataset. The x-axis indicated the actual CCI score from a specific NaiveBayes model trained and tested on a specific segment of customers, while the y-axis indicates the percentage of all the models having the corresponding CCI performance measure. Note how the CCI score varies from 10% to 100% correct, and the mean of the distribution is slightly above 50%. 
Figure 12 displays the histogram of the CCI performance measure distribution of the best performing segment-level (as explained in Section 4.1) within the set of models trained on segments generated by HC . Note how the CCI scores now have a tighter range, from close to 20% to 60% correct if we discount some outliers. However, the mean of the CCI distribution is significantly lower, at a little less than 30%. This illustrates our findings in the previous section, where compared to 1-to-1 , HC has reduced variance and error, but also has a lower CCI measure. This finding is consistent with the results of the Mann-Whitney distribution comparison tests for 1-to-1 vs. HC , as reported in Table 4. 
Figure 13 shows CCI distribution generated by the IM methods. Note that the distribution is slightly wider than that of HC , ranging from 30% to 90%. However, IM has tighter variance than 1-to-1 and does not drop in CCI mean relative to 1-to-1 and definitely has a higher mean compared to HC . This CCI distribution generated by IM clearly shows improved performance over the HC and 1-to-1 methods for the reasons demonstrated above, which is consistent with the results of the Mann-Whitney comparison tests as also reported in Table 4. 
Again, Figures 11  X  13 provide only three examples of distributions of the CCI measure out of the total of 108 histograms. However, these examples are very typical and clearly delineate the differences between the IM, HC and 1-to-1 methods. Therefore, these selected CCI histograms provide additional insights into the nature of the IM dominance over the 1-to-1 and HC methods, as demonstrated in Tables 3  X  5 and Figures 2-7. 
In summary, our empirical analysis clearly shows that, contrary to the popular belief [23], the 1-to-1 approach is definitely not the best solution for predicting customer behaviors. On the other hand, IM , which is essentially a micro-segmentation approach to segmentation, shows clear dominance over all methods tried in our experimental settings. size-one segments that were present in the distribution of 1-to-1 CCI in Figure 11, which did not get picked by the IM method as presented in Figure 13. This shows that, while the IM method is statistically dominant over 1-to-1 and HC, IM is still not the optimal segmentation solution described in Section 2. Nevertheless, IM  X  X  dominance over other popular segmentation methods across all the experimental settings indicates that it constitutes a sound initial approach towards reaching the final goal of generating best computationally tractable approximate solutions of the intractable optimal segmentation problem. 
In this section we make a closer examination of the segments created by the IM method. Specifically, we want to study the distribution of segment sizes generated by IM and investigate ways to improve IM . 
Figure 14 shows the distribution of segment sizes generated by IM for the high-volume customer datasets aggregated over all the experimental conditions. We note that the overall counts of segments peaks at segments of size two and then decrease steadily as the segment sizes increase. We also observe small counts among segments of odd sizes, which is an expected artifact of the IM algorithm where segment groups of roughly equal sizes were iteratively merged to improve performance. However, IM does not inherently discriminate against segments of size one X  X . Rather, segments will remain as size one if there are no other segment, once combined, that could improve the new combinations X  overall fitness. Thus, these observations provide evidence against the 1-to-1 approach to personalization, as most of size-one segments do find at least one other size-one segment to merge and improve the overall performance, as evident from the spike in size-two segments in Figure 14. 
Figure 15 shows the distribution of segment sizes generated by IM across low-volume customer datasets. Interestingly, the spike of the segment size distribution occurs at segments of size four. This does make intuitive sense, as low-volume customers need to form bigger groups in order to reach the  X  X ritical mass X  in terms of the data necessary for building good predictive models. Taken together, both Figure 14 and 15 suggest that IM  X  X  dominance over 1-to-1 and HC is largely due to the formation of large numbers of small customer segments, thus adding support to the use of micro-segmentations in forming robust and effective customer behavior models. 
The distribution of segment sizes generated by IM (as shown in Figures 14 and 15), clearly indicates that IM was able to find better performing groupings than just simple size-one segments. The peak at segment size of two and four implies that segments of small sizes, but sizes greater than one, are better performing segments for IM . And by the definition of IM , these small multi-customer segments, when modeled together, significantly outperform their respective individual segments of size one, as demonstrated from IM  X  X  dominance over 1-to-1 . 
While the IM direct grouping approach does not constitute an optimal grouping, the lack of size-one segments after many rounds of attempted segment merges implies that there will not be many size-one segments in the optimal solution. In addition, the optimal solution will definitely dominate the 1-to-1 solution, and we conjecture that it will contain predominantly small sized segments, resulting in a micro-segmented solution, as in the case of IM . We emphasize this lack of size-one segments in the optimal solution is only a conjecture and need to be proven, as we investigate better methods to approach the optimal partition solution. 
As one additional step in the analysis, we characterize the rate of decline in segment counts as segment sizes increase past the initial peaks in the distribution of segment sizes. From Figures 14 and 15, we observe that the rate of decline in segment counts follows the Zipf X  X  distribution [29], and we formally tested and proven this conjecture as follows. Zipf X  X  law states that where P n is the frequency of occurrence of a segment of size n . 
We fitted the regression model (2) against the high-volume and low-volume data to test the Zipf X  X  law hypothesis, and it turned out that the regression model indeed fitted the data. In particular, the coefficient a in Equation (2) for the high-volume customers, the segmentation size distribution starting from segment size two has a value of a =0.828, with p-value less than 0.001. As for low-volume customers, segmentation size distributions starting from segment size four has a =1.67, with p-value less than 0.01. As with many natural phenomena that has a Zipf X  X  distribution, our result suggest that the decline rate in terms of segment counts per segment size, starting from the peak of the segment size distribution, would also follow a Zipf X  X  distribution in the optimal solution. However, formal analysis is required to prove this conjecture. 
In this paper, we examined the problem of optimal partitioning of customer bases into homogeneous segments for building better customer profiles and proposed the direct grouping approach as a solution. This approach partitions the customers not based on computed statistics and particular clustering algorithms, but in terms of directly combining transactional data of several customers and building a single model of customer behavior on this combined data. We formulated the optimal partitioning problem as a combinatorial optimization problem and showed that it is NP-hard. Then we proposed a suboptimal polynomial-time direct grouping method, IM , and compared IM against the traditional statistics-based and 1-to-1 clustering approaches. We showed that IM significantly dominates the statistics-based approaches deploying standard clustering methods across all the experimental conditions examined in this paper. We also showed that, contrary to the popular beliefs, 1-to-1 turned out to be significantly inferior to IM across all the experimental conditions. We then examined the nature of the segments generated by IM and observed that there were very few size-one segments, that the distribution of segment sizes reached a maximum at the very small segment sizes, and that the rate of decline in the number of segments after this maximum followed a Zipf X  X  distribution. This observation, along with the dominance of IM over 1-to-1 , provides strong support for the micro-segmentation approach to personalization, where the customer base is partitioned into a large number of small segments. insights into the optimal customer partitioning problem, including the distribution of the segment sizes for this optimal partitioning (e.g., does it form the Zipf distribution?). We would also like to develop additional polynomial-time direct grouping methods that approach this optimal solution within some bounding limits and thus outperform IM and, hence, the 1-to-1 method. Finally, we would like to test the effectiveness of our segmentation strategies not only in terms of predictive performance but also in terms of the standard marketing oriented performance measures such as customer value, profitability and other economics based performance measures. 
