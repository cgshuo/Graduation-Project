 1. Introduction
The central problem in information retrieval is ranking documents according to their relevance to a query. In cross-language retrieval, the documents are in one language and the queries are in another. Pro-babilistic models for information retrieval rank documents based on probabilities, or scores related to pro-babilities, in many different ways. Probabilistic models in wide use today fall into two major classes X  X  traditional retrieval models which attempt to estimate probability of relevance of each document given a query, and language models , which attempt to model the generation of a query given a document.
Each of these approaches has its strengths and weaknesses. Both approaches can use multiple translations for query terms. Traditional retrieval models have been extended to handle multiple evidence and structured queries. Within systems based on these models, researchers have improved retrieval performance success-fully by techniques such as query expansion and structured query translation, which are heuristic and ad hoc, rather than by extensions of the formal models. Language models provide a formal framework which gives more guidance for handling translation, query (and document) expansion, and other enhancements within the model. However, important elements like structured queries have not yet been incorporated.
One goal of this research is a careful comparison of these two different approaches to cross-language information retrieval. We compare two widely used approaches based on the two probabilistic retrieval models, the query net model with structured query translation, as implemented in INQUERY, and the cross-lingual language model. The goal is not simply to make claims that one model or the other is better, since our experience shows that we can get comparable performance with both models. Rather, we explore the strength and weakness of each method, by examining how each performs with different kinds of re-sources. In addition, we compare two different approaches to query expansion and explore the role of trans-lation probabilities in language modeling.

In what follows, we first review retrieval models and language modeling, and highlight the differences between these two approaches that are of interest in the present research. Next, we review two approaches to query expansion, one widely used and one new. Finally, we present the experiments on English/Arabic and English/Spanish cross-language retrieval and discuss the conclusions that can be drawn about the two approaches, about query expansion, and about translation probabilities. 1.1. Retrieval models and structured query translation
The first probabilistic retrieval model was published by Maron and Kuhns (1960) . Their goal was to measure for each document the probability that the document will satisfy a given request for information. Other well known probabilistic models are those of Robertson and Jones (1976) , Croft and Harper (1979) ,
Fuhr (1989) and Turtle and Croft (1991) . These all share certain properties. The probability that a docu-ment is relevant to a query is a function of the distributions of query terms in relevant and nonrelevant documents. The models differ in what is assumed about these distributions and how these probabilities are estimated.

INQUERY ( Turtle &amp; Croft, 1991 ), is unique in incorporating an inference net model, allowing struc-tured queries. Like many other approaches, INQUERY computes a belief score that a document D is rel-versions of INQUERY: and avg len is the mean length of documents in the collection. The TF component of this weight is the Oka-pi tf ( Robertson, Walker, Jones, Hancock-Beaulieu, &amp; Gatford, 1995 ). For a typical weighted sum query, the score for a document is simply a weighted average of term scores over the term occurrences in the query:
However, the query net approach of Turtle and Croft (1991) allows the use of many operators. The #wsum operator computes the average shown in Eq. (4) . Other operators, such as Boolean AND and OR, and syn-onyms, combine term weights in more complex ways. Of particular interest in the present work is the syn-onym operator. To treat a set of terms as synonyms, tf  X  idf weights are derived for the set of synonyms based on the statistics of the terms that make up the set, in effect treating the occurrences of all the syno-synonyms, where tf T , D is the number of occurrences of all of the terms in the set T in the document D , and { d set of documents containing any of the terms in T . This tf to yield the tf  X  idf score for a synonym set.

Researchers have found the synonym operator useful for cross-language retrieval. A successful approach translation . A simple example for Spanish can be seen in Fig. 1 . 1.2. Language models
Language models (LM) have been used for a long time in speech recognition ( Bahl, Jelinek, &amp; Mercer, machine translation domain ( Brown et al., 1990 ; Brown, Della Pietra, Della Pietra, &amp; Mercer, 1993 ), and
Song &amp; Croft, 1999 ).
A language model is a probability distribution over terms. In the simplest version of these models, the unigram model, the terms are single words or stemmed words. The particular form of language model we describe here is due to Miller et al. (1999) . A query is a bag or a sequence of single terms, generated from independent random samples of a term from one of two distributions X  X  X he distribution of words in a model of a document, and the distribution of words in a background model such as General English. For each pick, one samples the document model with probability k , and the background model with probability 1 k . Thus, the probability of generating a query Q from a document D and a background model GE is: where e is an English word in query Q , P ( e j D ) is the probability of drawing word e from the document model, and P ( e j GE) is the probability of drawing word e from the background model of general English. based on a widely used extension of the above monolingual model ( Xu et al., 2001 ). To generate a query from a document in a different language, say Arabic, one samples either the Arabic document, or the Eng-lish background model. When the sample is taken from the Arabic document model, an Arabic word is first chosen at random from the document model, and then an English translation for that Arabic word is cho-sen at random from a bilingual lexicon. Thus the probability of generating an English query Q Arabic document model D a is word e in some large sample of English documents. P ( a j D an Arabic document model D a , is estimated as the relative frequency of Arabic word a in the document D depends upon what kind of cross-language resources are available. If one has a sentence-aligned parallel corpus, one can estimate these probabilities by (appropriately smoothed) relative frequencies of alignments tional to estimate P ( e j a )as1/ n , where n is the number of English translations for the Arabic word a .
Researchers using language modeling have been able to attain performance comparable to that of tf  X  idf models like INQUERY, but without the extensive tuning of parameters. Language modeling has become popular because it is well grounded in statistical theory and is easily extendable to handle enhancements such as query and document expansion, n -gram units, stemming alternatives, etc. 1.3. Translation probabilities
The translation probability P ( e j a ) above is an important component of these language models. Good esti-mates of translation probabilities can be obtained by aligning parallel corpora, and counting the occur-rences of alignments of word pairs. It is widely believed that the excellent cross-language results obtained in recent TREC experiments stem from the accurate estimation of translation probabilities obtain-able from parallel corpora, and to the model s ability to use these translation probabilities. In contrast, structured query translation does not use translation probabilities. It is clear that some translations are more likely than others, and that structured query translation performance can be diminished by the inclu-sion of too many alternative translations. It is common practice, when starting with a dictionary made from a parallel corpus, to discard low probability translations.
If the translation probabilities are the determining factor in the effectiveness of language modeling, then retrieval should be less effective when only poor translation probability estimates are available. However, several researchers in our lab have mentioned paradoxical findings to us (Personal communication) in which large changes to translation probabilities made little difference in language model retrieval effective-ness. Furthermore, we have attained good results with conventional dictionaries in language modeling, using the relatively poor estimates of 1/ n mentioned above. This paints a confusing picture, which we felt needed closer scrutiny. 1.4. Query expansion and relevance modeling
An almost universal finding in IR, and CLIR, is that regardless of the model used, expansion techniques improve retrieval, as measured by an increase in mean average precision. Early work in information retrieval showed that queries could be improved by relevance feedback , that is, adding and reweighting terms from retrieved documents relevant to a query ( Rocchio, 1971 ). Later work showed that the same general approach could be applied without relevance information, using a small number of the top-ranked retrieved docu-ments ( Xu &amp; Croft, 1996 ). This pseudo-relevance feedback approach has been so successful that most of zalo, &amp; Kluck, 2002 ) and NTCIR ( NTCIR Workshop, 2001 ) cross-language evaluations use it in some form.
Recently, researchers have proposed query expansion methods within the monolingual language mode-however, the researchers attaining the best cross-lingual performance are still using the same local context analysis or pseudo-relevance feedback techniques used with traditional retrieval models. These techniques do not fit into the formal framework of language modeling.

Lavrenko and Croft (2001) have recently developed a new kind of language model called a relevance model which brings the concept of relevance back into language modeling but which can also be viewed as a query expansion technique. In this framework we assume that for every information need there exists an underlying relevance model R which assigns the probabilities P ( w j R ) X  X  X he probability of observing a word w in the set of documents relevant to that information need. The innovation in this approach is in assuming that both queries and relevant documents are random samples from the distribution P ( w j R ). This is in contrast to other language model formulations which assume that only queries are generated by sam-approximated by the probability of co-occurrence between the word w and the query Q = e
Lavrenko and Croft present two ways of estimating the joint probability P ( w , Q ). In Method 1, used in the present research: where M is a set of unigram distributions we are sampling, P ( D ) is the probability of choosing distribution
D , P ( w j D ) is the probability of choosing word w from the model D , and P ( e ing query word e i from model D . P ( D ) is taken to be uniform, and both P ( w j D )and P ( e relative frequencies:
Here, k is a smoothing parameter as in Eq. (7) , P ( w j GE) is the probability of word w in the background model as in Eq. (7) , and tf w , D is the frequency of word w in document D .

Note that this form of relevance modeling can be viewed as query expansion. The set of unigram distri-butions are document models, and in practice the set of documents is obtained by taking the top-ranked documents from an LM retrieval pass.

Relevance models were extended to handle cross-language IR by Lavrenko, Choquette, and Croft (2002) . Consider English queries and Arabic documents. The cross-language relevance model is given in
Eq. (12) , which is the same as Eq. (10) , above, but now subscripts indicate that the documents D w a are in Arabic, and the query terms e i are English: P ( w a j D a ) can still be estimated as in Eq. (11) , with the document and background models in Arabic. P ( e i j D a ) is estimated as in Berger and Lafferty (1999) and Xu et al. (2001) :
Note that this is the same as the expression inside the product in Eq. (8) . Once the relevance model is esti-mated, documents are ranked according to their cross-entropy with the relevance model: 2. Overview of present and previous research
This research is intended to address the three issues discussed above. The first is a direct, well controlled comparison of the two dominant approaches to cross-language retrieval, holding constant the kinds of pre-processing performed on the collections and queries. In CLIR today, two of the probabilistic approaches reviewed above X  X  X tructured query translation and language modeling are used widely ( Oard &amp; Gey, 2003 ).
However, research groups tend to choose one approach or the other, and so direct comparisons have been rare.

One other study has compared structured query translation with LM for cross-language retrieval ( Xu et al., 2001 ). They found that language modeling performed better than structured query translation when used with a dictionary derived from a parallel corpus, or a combined dictionary derived from parallel and nonparallel sources. However, they also found that language modeling and structured query translation gave comparable results when used with dictionaries that did not have translation probabilities obtained from parallel corpora. But the story was not completely told. Their experiments covered only Chinese/Eng-lish retrieval, and they presented results only with unexpanded queries. In order to make this comparison general, we use English queries with two very different languages, Arabic and Spanish. For each language, we use two different dictionaries, one probabilistic dictionary derived from a parallel corpus, and one con-ventional dictionary. We also test both unexpanded and expanded queries.

Second, we compare query expansion via pseudo-relevance feedback with relevance modeling. Lavrenko et al. (2002) compared relevance modeling with cross-lingual language modeling based on unexpanded que-ries, on the same Chinese data set used in the Xu et al. (2001) experiments, but given that relevance mode-technique.
The third issue addressed by the present research is the importance of translation probabilities in the language modeling approach. The comparison of the two kinds of dictionaries has some bearing on this issue, but is confounded with differences in coverage. For this reason we include an experimental condition of the n English translations for each Arabic or Spanish word in the parallel dictionary. If accurate trans-lation probabilities are important, then this condition should show degraded performance relative to the condition using probabilities based on parallel data. 3. Experimental methods We present cross-language retrieval experiments using two different retrieval systems, LM and INQU-
ERY, performing identical tokenization, stemming, and stop word removal for both. The experiments are carried out with English queries and collections in two languages X  X  X rabic and Spanish. For each lan-guage, we have two sets of resources. The first is a probabilistic dictionary built from a parallel corpus, which has good translation probabilities. The second is a more conventional dictionary, which may or may not have good coverage, but does not have good probabilities. Monolingual retrieval conditions are included as baselines for cross-lingual conditions. 3.1. Test data The corpus for the English X  X rabic experiments, consisting of 383,872 Arabic documents from Agence France Presse from the years 1994 X 2000, was used for the TREC cross-language track in 2001 ( Gey &amp;
Oard, 2002 ) and in 2002 ( Oard &amp; Gey, 2003 ). Title and description fields from two query sets were used X  X 25 queries from TREC 2001, and 50 queries from TREC 2002.

Two corpora and query sets were used for the English X  X panish experiments. From the TREC-4 multi-lingual track ( Harman, 1996 ) we have El Norte newspaper articles from Mexico, and 25 TREC-4 topics provided both in Spanish and English. Queries for these experiments were title and description fields from the topics. The second English/Spanish data set was from TREC-5, for which the corpus was a set of
Agence France Presse articles in Spanish from 1994 ( Smeaton &amp; Wilkinson, 1997 ). There were 25 topics (51 X 75) for this collection. Queries were made from the description fields because this set of topics had no Spanish titles.

A summary of the statistics of these data sets can be seen in Table 1 . Note that the mean query lengths and document lengths do not include stop words.
 3.2. Processing of text
All of the English text (in queries, in parallel corpora, in dictionaries, and in the English collection used for English query expansion) was normalized to lower case but not stemmed. Stop words were removed, using INQUERY s stop list of 418 words. Numbers were also removed.

All the Arabic text was converted to Windows CP1256 encoding. Arabic text was then normalized and stemmed as described in Larkey, Allan, Connell, Bolivar, and Wade (2003) , removing punctuation, diacrit-certain Arabic characters: replacing , ,and with bare alif , replacing final with , and replacing final with . Then Arabic text was stemmed using the UMass light10 stemmer, which first strips ( and ) from the beginnings of words, then removes definite articles ( ) from word beginnings, and 10 suffixes from word ends ( ) in a specific order allowing more than
Spanish was normalized to lower case, stop words were removed, and words were stemmed using a Porter-like stemmer for Spanish ( Broglio, Croft, Callan, &amp; Nachbar, 1995 ). 3.3. Bilingual resources
The Arabic parallel or probabilistic dictionary was derived from the UN Arabic/English parallel corpus distributed by the Linguistic Data Consortium. It consists of 675 MB in 3,270,000 aligned sentences. Eng-lish and Arabic sentences were processed as described above. The statistical translation training program
GIZA++ ( Och &amp; Ney, 2000 ) was used to align the Arabic and English sentences, and to build a translation model using IBM model 4 ( Brown et al., 1993 ). Translations which had a probability of less than 0.0001 were removed. A smaller version of each parallel dictionary was made for structured query translation, which contained only translation pairs with probabilities of 0.15 or higher, a choice based on our TREC 2001 research. The sizes of parallel corpora and dictionaries are summarized in Table 2 .
The nonparallel Arabic dictionary is the UMass dictionary, built for our TREC 2001 and 2002 work, from many different sources. It is described more thoroughly in Larkey et al. (2003) .

The Spanish parallel dictionary was built from a parallel corpus of European Parliament proceedings ( Koehn, 2002 ) which are available in 11 languages. The Spanish and English part of the corpus consists of 240 MB of data in 746,000 aligned sentences, taken from proceedings between April, 1996 through
December, 2001. We noticed some French files included both on the English and Spanish side, but we made no attempt to clean up the data. The Spanish parallel dictionary was built like the Arabic dictionary, using GIZA++.

The nonparallel Spanish dictionary was made from an electronic version of the Collins Spanish/English dictionary.

For both Arabic and Spanish, probability estimates of 1/ n were assigned to translation pairs in the non-parallel UMass and Collins dictionaries, as described previously. For both Arabic and Spanish, a compos-ite dictionary was built by combining the parallel and nonparallel dictionaries. Each translation pair received the mean of its probability estimates from the component dictionaries. 3.4. Retrieval implementation
In this section we fill in implementation and parameter details. The experiments were carried out using our own search engine which can simulate INQUERY, and which can perform monolingual and cross-lingual language modeling, all with the same pre-processing. 3.4.1. INQUERY and structured query translation
For cross-language retrieval, English queries were translated to Arabic or Spanish using structured query translation, according to the following procedure: for each English query word, if the word is found in the dictionary, take all the translations and place them inside a #syn (synonym) operator. If the English word is not found in the dictionary, stem the word with the kstem ( Krovetz, 1993 ) stemmer, and try again. If any of the translations consist of a phrase rather than a single word, the phrase is enclosed in a #filreq operator.
This operator is essentially a Boolean AND operator, which captures the requirement that if we are looking then a #wsum (weighted sum) of all the synonym sets as in Fig. 1 . 3.4.2. LM
LM retrieval was carried out using Eq. (7) for monolingual, and Eq. (8) for cross-lingual retrieval. A value of k = 0.5 was used in all monolingual runs, and k = 0.7 was used for cross-lingual runs. These values for k , and the parameters for pseudo-relevance feedback (number of documents used for query expansion, number of words to add to expanded queries) were fixed at values that have worked well in past research.
We did not tune these parameters for the present research. In cross-lingual runs, if an English query word was not present in the dictionary, it was replaced with its stem and looked up again. 3.4.3. Query expansion and relevance modeling English queries were expanded using AP news articles from 1994 through 1998 from the Linguistic Data
Consortium s North American News Supplement ( LDC, 1998 ). Arabic and Spanish queries were expanded using the document collections being searched. To expand queries in INQUERY conditions, the top-ranked 10 documents were taken from an INQUERY retrieval run, and all the words in the retrieved documents were ranked by the sum over the 10 documents, of their tf  X  idf scores. For English queries (pre-translation), the top five new words were added to the queries. In expanding monolingual Arabic or Spanish queries, the top 50 new words were added to the queries. Final term weights were set to 2 w term weight, and w e = 1. Our pre-translation expansion is unusual in adding so few words. We have seen mixed success with pre-translation expansion when large numbers of words are added. Adding few words does not always aid retrieval as much as adding more words, but it rarely hurts performance.
In an expanded INQUERY cross-lingual run, the first pass was English query expansion. After the query was translated into a structured query with synonyms as in Fig. 1 , a post-translation expansion pass took the top 10 retrieved documents, and made a new structured query consisting of the old structured query in which each synonym set got twice its original weight, plus the 50 new terms were added under the weighted sum operator, each with a weight of 1. This expanded, translated, and further expanded query was used to retrieve the final ranked list of documents.

To expand queries in LM conditions with pseudo-relevance feedback, the top-ranked 10 documents were taken from an LM retrieval run. The words from the retrieved documents were ranked, as in the INQU-
ERY conditions, using tf  X  idf scores. For monolingual Arabic or Spanish, retrieval, the top 50 new terms were added to the original query, weighted 2 w o + w e as above.

In an expanded LM cross-lingual run using pseudo-relevance feedback, Arabic and Spanish query expansion was carried out by retrieving documents using the unexpanded English query. The top scoring 100 terms from the top-ranked 10 documents were taken and used as a new Arabic or Spanish query (with-out the original terms), which was then run as a monolingual LM retrieval run. The final ranked list of doc-uments for this LM run was combined with the ranked list from a cross-lingual run using the expanded
English queries, and the final score for each document was the mean of its scores on the two ranked lists. Before combining the two ranked lists, scores were normalized according to the formula score ( score min )/( max min ). Then scores were summed across the two lists.
 In selecting values for the relevance modeling parameters, we were guided by experiments reported in
Lavrenko et al. (2002) and Liu and Croft (2002) . We did not tune the parameters on the present data. In a monolingual relevance model run, the first pass was an LM run with Dirichlet smoothing, k = doclength /( doclength + 1000 ). For cross-lingual relevance modeling, the first pass is a cross-lingual
LM run. For both monolingual and cross-lingual relevance modeling, a new  X  X  X uery X  X  (relevance model) was made with 500 terms from the top 20 documents in the monolingual case, and the top 50 documents in the cross-lingual case, weighted as described previously in Eq. (12) . Five hundred terms would be a large number for a query, but they allow a good estimate of the language model of relevant documents. Many of the added terms receive very low probabilities, so they do not have the detrimental effect on precision that the terms would have if added to a conventional query. 4. Comparison of retrieval approaches
The results of the experiments comparing retrieval approaches, and comparing expansion methods can be seen in Tables 3 X 6 . Monolingual results are included as baselines.

The Wilcoxon matched pairs test ( Siegel, 1956 ) was used for all significance tests reported here. Results for the two Arabic query sets were combined and results for the two Spanish query sets were combined to give the statistical tests more power. A p -value of 0.05 was considered the cutoff for significance. 4.1. Monolingual results
Monolingual retrieval results on individual query sets can be seen in each of the tables above. On unexpanded queries, INQUERY performs significantly better than LM (Arabic p = 0.0002; Spanish p &lt; 0.05). After query expansion, there are no significant differences among the monolingual conditions: expanded INQUERY, LM with pseudo-relevance feedback, and relevance modeling all perform equivalently.

The monolingual retrieval results also confirm a pattern that is well known in the literature: query expan-sion generally improves retrieval performance independent of the retrieval model used. 4.2. Cross-language retrieval results
The cross-language retrieval results can be seen in Tables 3 X 6 . The pattern of results on cross-lingual retrieval is more complicated than on monolingual retrieval. For this reason, pairwise comparisons with significance levels are shown in Table 7 . The parallel dictionaries show a different pattern of results than the probabilistic and combined dictionaries. The important points can be summarized as follows:  X  Nonparallel dictionaries show a pattern of results similar to monolingual retrieval:  X  INQUERY (structured query translation) performs significantly better than LM on unexpanded  X  On expanded queries, INQUERY with pseudo-relevance feedback, LM with pseudo-relevance feed- X  Probabilistic and combination dictionaries:
We note other patterns in these data. Query expansion improves cross-lingual performance greatly. We also note that on Arabic, cross-lingual retrieval is better than monolingual retrieval, even when we correctly use expanded monolingual as the baseline for expanded cross-lingual, which many researches have not done when making such comparisons. Cross-lingual performance as a percentage of monolingual can be seen in the last row of Tables 3 X 6 . For Arabic these percentages refer to the combined dictionary. For Span-ish the percentages refer to the parallel dictionary. The nonparallel dictionary for Spanish performed so badly, particularly on TREC 5, that the performance on the combination dictionary was worse than on the parallel dictionary alone. This poor performance appears to be due primarily to the poor coverage of names in the Collins dictionary.

Getting better performance on cross-lingual retrieval than on monolingual retrieval may seem paradox-ical. A closer examination of one query sheds some light on how this can occur. TREC 2002 query 32 pro-vides a good example. The English query is: 32. Caspian Beluga Conservation: What Beluga conservation projects are present in the Caspian region?  X  X  X eluga X  X  is obviously an important term in this query. In the Arabic query provided by NIST,  X  X  X eluga X  X  is translated as (blwga), essentially a transliteration of the word  X  X  X eluga. X  X  This Arabic word does not occur in the corpus. In the 34 documents that were judged relevant to this query, the Arabic word the dictionary as a translation for sturgeon, and (bilwga), a slightly different transliteration than
NIST s. Our English query expansion phase added the word  X  X  X turgeon X  X  to the query, so the Arabic word cross-lingual performance using the combined dictionary, we find 0.0715 monolingual average precision for query 32, and 0.8529 cross-lingual LM average precision. Interestingly, in both monolingual and cross-lingual cases, all 34 of the documents judged relevant were retrieved in the top 1000, but monolingual que-ries did a far poorer job at ranking them.

This example illustrates that dictionary translation can overcome a vocabulary mismatch problem. In general, using multiple translations is what allows dictionary-based CLIR methods to perform better than monolingual retrieval, particularly when coupled with expansion techniques that tend to emphasize words that co-occur with query terms. In monolingual search, a query may have apparently good search terms, but they may not happen to match the terms that occur in the corpus. This kind of mismatch is particularly likely with language pairs like English and Arabic, in which each has a high degree of variability in spelling foreign words.

In the Spanish experiments, and in Xu et al. s Chinese experiments, cross-language retrieval did not ex-ceed monolingual. The difference may be due to the quality of the dictionaries, and whether NIST judged documents that were returned by queries in both languages, or just one language.

In this section we have presented the comparison of structured query translation and language modeling, and we have compared the two methods of query expansion. In the following section we focus on transla-tion probabilities. 5. Role of translation probabilities
The cross-lingual results above showed that when we have a parallel dictionary, language modeling can yield higher average precision in an IR task, presumably because LM is based on a model that incorporates translation probabilities, whereas INQUERY does not use translation probabilities. In this section we take a closer look at the importance of translation probabilities.

A superficial look at the results on the four data sets above shows that on the Spanish data sets, the par-ference between the performance of the two dictionaries. However, the parallel vs nonparallel difference is confounded with differences in coverage. A more direct assessment of the importance of translation prob-abilities can be carried out by replacing the probabilities obtained from parallel corpus alignment with flat probability distributions.

The effect of probabilities is also confounded with the effect of thresholding. A dictionary made with a low threshold includes many low probability translation pairs. The language model uses the probabilities to limit the influence of these low probability translations on the final probability. A dictionary made with a higher threshold does not include these low probability translations. This leads to the question of whether including these words in the dictionary provides some benefit. In the INQUERY conditions, they have been thresholded out.

In the next experiment we examine LM performance using parallel dictionaries made with different thresholds, and we compare the results with performance on the dictionaries containing the same transla-tion pairs, but with flattened probability distributions. For reference, we include INQUERY performance.
Four new dictionaries were made from each of the parallel dictionaries, by removing translation pairs set of dictionaries was made by replacing the probabilities in each of these dictionaries by 1/ n , where n is the number of translations of each word into English. The sizes of these dictionaries can be seen in Tables 8 and 9 .

The results of retrieval experiments using these dictionaries can be seen in Table 10 . The thresholding results are similar to those found by Xu et al. (2001) , in that LM degrades as the threshold is raised, and that INQUERY with structured query translation improves as the threshold is raised (within the range tested). When we compare the best performance of LM (at the lowest threshold) with the best performance of INQUERY (at high thresholds), the picture is mixed. As we already noted in Section 4.2, unexpanded queries show no consistent difference between LM and INQUERY. For expanded queries, LM is better than INQUERY.
 Turning to the flat probabilities, the CLIR results can be seen in the columns labeled LM1/ n and tistically significant. The pattern of results is consistent. With low thresholds, in which many low probabil-ity translations are included in the dictionary, higher average precision is obtained with full probabilities than with flat probabilities. At higher thresholds, the probabilities do not matter.

Overall, the most effect retrieval is attained using expanded queries with language modeling and trans-lation probabilities derived by aligning a parallel corpus. One may note that the tf  X  idf and language models are conceptually similar. Both find a ranking score for a document in relation to a query, and that score is a function of relative term frequencies, smoothed by collection statistics. that one might be able to increase the effectiveness of structural query translation by incorporating trans-lation probabilities into synonym processing. 6. Conclusions
In the comparison of structured query processing with language modeling, we found that structured query processing gave slightly better results than language modeling when queries were not expanded.
On the other hand, when queries were expanded, language modeling gave better results, but only when using a probabilistic dictionary derived from a parallel corpus.

In comparing two methods of query expansion, we have shown that relevance modeling performed as well as pseudo-relevance feedback. On Spanish but not Arabic data, relevance modeling was significantly better than pseudo-relevance feedback. However, the processing of relevance modeling was substantially slower than pseudo-relevance feedback, so it is unclear whether it gives a practical advantage.
In examining the role of translation probabilities in a dictionary derived from a parallel corpus, we found that replacing the translation probabilities with flat probabilities results in a small but significant degradation in retrieval performance leading to the conclusion that accurate translation probabilities do contribute to higher precision.
 References
