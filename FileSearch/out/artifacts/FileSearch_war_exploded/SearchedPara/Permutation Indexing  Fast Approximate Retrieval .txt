 Inverted indexing is a ubiquitous technique used in retrieval sys-tems including web search. Despite its popularity, it has a drawback -query retrieval time is highly variable and grows with the corpus size. In this work we propose an alternative technique, permutation indexing , where retrieval cost is strictly bounded and has only log-arithmic dependence on the corpus size. Our approach is based on two novel techniques: (a) partitioning of the term space into over-lapping clusters of terms that frequently co-occur in queries, and (b) a data structure for compactly encoding results of all queries com-posed of terms in a cluster as continuous sequences of document ids. Then, query results are retrieved by fetching few small chunks of these sequences. There is a price though: our encoding is lossy and thus returns approximate result sets. The fraction of the true results returned, recall, is controlled by the level of redundancy. The more space is allocated for the permutation index the higher is the recall. We analyze permutation indexing both theoretically under simplified document and query models, and empirically on a realistic document and query collections. We show that although permutation indexing can not replace traditional retrieval methods, since high recall cannot be guaranteed on all queries, it covers up to 77% of tail queries and can be used to speed up retrieval for these queries.
 H.3.3 [ Information Search and Retrieval ]: Selection process Inverted index, precomputation, embedding
With proliferation of the web, the volume of the potentially search-able content has been growing rapidly, some of the main sources of growth being the  X  X urated X  content such as news and Wikipedia ar-ticles, user generated content such as comments, blog posts, and  X  Part of this work done while the authors were at Yahoo! Research. social network updates. Search engines strive to index all the con-tent on the web and to serve the best matches on queries in fractions of a second. Moreover, search is used in the back-end for other ap-plications beyond web search such as computational advertising [4, 20], and recommender systems [1].

Due to the power law nature of the query traffic, web search being one example, tail queries constitute about 50% of the query volume [3]. Since these queries were never seen before, caching is ineffective and they are processed by the back-end retrieval system.
The basic principle behind inverted index algorithms that power most retrieval systems relies on creating, for every term, a posting list that contains ids of all documents where the term occurs. At retrieval time, posting lists of query terms are combined to com-pute query results. Retrieval cost is proportional to the lengths of these posting lists -these grow linearly with the corpus size and are highly variable for different terms. Much work has been done on improving scalability of retrieval algorithms including caching [14], precomputation [10], static index pruning [5, 6], multi-stage rank-ing [21], and distributed query processing in large clusters [9].
In this work we depart from the traditional approaches of caching and precomputation, which, while giving significant performance benefits, do not eliminate the linear dependence of the retrieval cost on the corpus size. We consider a less explored point in the accuracy-cost design space  X  while traditional retrieval systems have predictable retrieval accuracy but variable cost, our goal is to bound retrieval cost (independently of the query and of the corpus size) while relaxing accuracy guarantees. Although this may be un-acceptable for some applications where accuracy is critical, such as legal search, for other applications where content is abundant and ranking is imperfect, such as web or user generated content search, our approach can be a viable alternative. A related approach was considered by Anagnostopoulos et al. [2] who proposed to select a small subset of documents that can (approximately) answer a large fraction of real user queries.

Achieving the above-described goal of bounding retrieval com-plexity might seem infeasible, since in a sense it would require high level of precomputation covering a query space exponential in the query length. For example, with a lexicon of 100K terms, the num-ber of only 4-term queries is 100 , 000 4  X  10 20 . Naively precom-puting and storing such volume of data is infeasible. Moreover, for a related problem it was shown that building a feasibly small data structure allowing constant-time retrieval is impossible for general document and query collections [12].

The central observation we rely on in this work is that majority of real user queries span only a small subspace of the whole query space. The first technical challenge we tackle is identifying this subspace. We show that there is a scalable algorithm that identifies a collection of even smaller subspaces, whose union covers more than half of all user queries. Each of these smaller subspaces spans 100 -s to 10 , 000 -s of lexicon terms and thus covers order of 100 to 10 , 000 i of i -term queries for all i (in practice we assume that i  X  10 ). The second challenge is to compactly encode results of all queries in each subspace into a data structure of reasonable size. Although orders of magnitude smaller than the full query space, it is still infeasible even to enumerate P 10 i =1 10 , 000 alone precompute and store all their result sets. This leads us to the idea of lossy encoding (compression), where the decoded result sets are only approximations of the true results.

Permutation index is a collection of document sequences (or-dered lists of document ids) where each sequence encodes result sets of all the queries in a particular query subspace. For each sub-space of, say, 1 , 000 terms we construct a document sequence by first projecting all documents onto the 1 , 000 terms, and then pro-jecting these 1 , 000 -element vectors onto the line induced by a par-ticular permutation of the 1 , 000 terms. Such a sequence contains approximate result sets of all queries in the subspace as continuous (overlapping) subsequences. The objective in selecting the terms that go into a subspace and their order in permutation, is to maxi-mize result accuracy.

Our indexing algorithm works as follows: we select a subset of terms that tend to co-occur in queries, and construct a permutation of them. We then create, for every document, a sparse binary vector where we put 1 in element i if and only if the document contains the i -th term in the permutation (all-zero vectors are discarded). Sorting the vectors in the reverse order of their lexicographical val-ues gives the document sequence. Figure 1 shows an example of a term-document matrix induced by permutation ( c,e,a,b,d ) and the corresponding document sequence d 2 ,d 3 ,d 1 ,d 4 contains a document id and the corresponding document vector where empty cells are zeros.
 Figure 1: Example term-quence length. Suppose we wish to answer the 2-term query [c e] using the permutation/sequence depicted in Figure 1. The subse-quence d 2 ,d 3 , whose vectors starts with "1 1" is exactly the result of query [c e].

Clearly, all the documents in the returned subsequence contain all the query terms, i.e., the precision is 1. However, if the retrieval vector contains one or more zeros, some results may be lost. Con-sider query [e b] and its retrieval vector "0 1 0 1". The retrieved subsequence contains d 1 , but another result d 2 is masked by "c". Reduced recall is indeed the main price we pay for the ability to return (a subset of) query results with a single fetch of a continuous subsequence of document ids. To achieve perfect recall we could enumerate all the binary vectors of length 4 that contain 1 in the second and fourth elements, but for longer permutations the num-ber of such vectors becomes large and retrieval cost increases pro-hibitively. We thus opt to access only one subsequence prefixed by the original retrieval vector, and use several complementary tech-niques to minimize the effect of masking and improve recall: (1) place frequently queried terms towards permutation head, and the terms appearing frequently in documents towards permutation tail; (2) generate a large number (thousands) of diverse permutations to increase the likelihood of typical term combinations to appear early in some permutation; (3) generate document  X  X ubvectors X  that con-tain subsets of the most important document terms (e.g., accord-ing to tf-idf weighting); and (4) for every query, access several se-quences (tens) and return the union of the results.

In this work we establish general viability of the approach and propose algorithmic solutions for efficiently constructing and query-ing a permutation index for three types of retrieval: conjunctive boolean, conjunctive ranked, and disjunctive ranked. As boolean conjunctive retrieval naturally maps to permutation indexing, we start from theoretically analyzing its properties both to obtain in-sights and to facilitate the development of effective algorithm for constructing permutations. We show that the retrieval cost depends only logarithmically on the corpus size, and that while the recall does depend on document properties and on query distribution, it does not depend on the corpus size. As constructing an optimal permutation given a corpus and a sample of queries is NP-hard, we derive a greedy approximation algorithm. We propose a reduction of ranked (top-k) retrieval to permutation indexing, and develop several heuristics that improve recall and reduce index size. Per-mutation index does not directly support positional retrieval, but it naturally supports a fixed window-based proximity retrieval when window size is known at indexing time. In such a case permutation index is smaller and retrieval is less expensive than non-proximity retrieval, in contrast to inverted indexing where proximity retrieval is more expensive and requires larger index. Due to lack of space we omit proofs and detailed algorithm implementations.

Permutation retrieval is naturally parallelizable and scales easily, as its most expensive steps are independent of each other and can be implemented using the scatter-gather approach. The storage over-head is variable and depends on the desired accuracy-cost tradeoff. We found typical overhead to be from 4x up to 100x of the inverted index size. Index size increase is offset by the reduction of random accesses compared conventional inverted indices, making it more friendly to cheaper disk storage.

While bounds on the retrieval cost and the linear dependence of storage cost on the corpus size follow directly by algorithms X  con-struction, we lack tight analytic results on retrieval quality. To fill this gap, and to determine a set of parameters that lead to a rea-sonable tradeoff between storage cost, retrieval cost, and retrieval quality, we conduct experimental evaluation on 1.6M web docu-ments and a sample from user query log. We construct a permuta-tion index consisting of 4 , 000 permutations, and containing 3 . 5 to 13 time more posting than the inverted index of the same corpus. Retrieval achieves precision of 100% and recall of at least 90% on 18% to 77% of user queries depending on the retrieval type, even after removing all cacheable queries. Retrieval is up to two orders of magnitude cheaper than standard inverted index retrieval, and guarantees much lower cost variability from query to query. Preliminaries. Let D be the corpus and T be the term lexicon. Let n = |D| denote the number of documents. In an inverted index each term t  X  X  is associated with a posting list that contains doc-ument ids of all documents that contain t . Query results are com-puted by intersecting or merging posting lists corresponding to the query terms, for conjunctive and disjunctive queries respectively.
In boolean conjunctive retrieval queries and documents are bags of words from T , and our goal is, given a query, to return all the documents from D that contain all the query terms, see, e.g., the Max Successor [7] algorithm. In ranked retrieval we are inter-ested only in the top-k matching documents that maximize some scoring function, where k is a small constant. In the vector space model that we use in this work, queries and documents are real weight of term t in document d (resp. query q ). Score is defined by score ( d,q ) = P t  X  X  q [ t ] d [ t ] .
 Permutation retrieval. A term permutation p is a permutation of all terms in T . Given p, a document (resp. query) vector is a bi-nary vector of length |T| where the bits are ordered according to p , and the bits corresponding to the terms occurring in the document (resp. query) are set to 1. A permutation induces a unique docu-ment sequence which is a list of document ids in the descending lexicographical order of their document vectors. Permutation in-dex consists of a large set of permutations and corresponding doc-ument sequences. Given the set of permutations P = { p 1 , p and the corresponding sequences S = { s 1 , s 2 ,... } , and query q , the retrieval Algorithm 1 proceeds as follows: It first finds the top- X  sequences on which the query is likely to have high recall. For every p from the top permutations it constructs retrieval vector q that contains all the 1-s in the full query vector with trailing zeros truncated (we omit subscript p when clear from the context). Re-trieval vector q induces a continuous subsequence where all docu-ment vectors have prefix q . Using a search tree of depth O (log n ) we find the subsequence in O (log n ) steps. Finally, the algorithm reads all the document ids in the subsequence and adds them to the result set.

We assume that the term permutations selected by the index build-ing algorithm reside in RAM, since their size is O ( |T| ) and is neg-ligible compared to the size of document sequences. The costs of EncodeVector and of GetTopPermutations are negligible as these can be efficiently implemented using compact lookup data structures. Thus, the main cost of the algorithm is two search oper-ations and one sequential read per each document sequence. Algorithm 1 Boolean Conjunctive Retrieval Algorithm 1: Input: P , S ,q, X  . 2: Output: Set of document ids. 3: W  X  GetTopPermutations ( P ,q, X  ) 4: R  X  X  X  5: for p i  X  W do 6: q 0  X  EncodeVector ( p i ,q ) 7: q  X  Truncate trailing zeros in q 0 8: Search for the subsequence of s i prefixed by q 9: R i  X  Read all document ids in the subsequence 10: R  X  R S R i 11: return R
The implementation of EncodeVector is straightforward: we start with an empty vector, for each query term look-up its position in the permutation, and add 1 to the vector at this position. Clearly, the documents in the subsequence prefixed by such a vector con-tain all query terms, and thus the algorithm X  X  precision is 1. Recall can be lower than 1 though. We thus formally define the recall of boolean retrieval from single permutation: Let T ( q ) be the true result set of query q obtained, e.g., through some standard list in-tersection algorithm, and let A ( q, p ) be the results retrieved from permutation p and the corresponding document sequence. Then, the recall of algorithm A is
As we want to avoid retrieval in GetTopPermutations , we estimate recall using coarse-grained corpus statistics, namely doc-ument frequencies. We compute the probability h ( t ) of term t ap-pearing in a random document as the document frequency of that term divided by the number of documents. Then, the probability of a random document to not contain term t is 1  X  h ( t ) . Assuming independence among the term occurrences, we estimate recall as where the indicator function I checks whether query q can be re-trieved using p, q is the retrieval vector of q for permutation p, and q [ t ] is the value of term t in q . We precompute the prod-uct for every term at memory cost proportional to the permuta-tion length. Then, we only look-up the precomputed values for the terms in the query, reducing the complexity from O ( |T| ) to O ( | q | ) . GetTopPermutations ( P ,q, X  ) computes recEst ( q, p ) for every p  X  P , and return the  X  ones with the highest estimated recall.

L EMMA 1. The expected recall on a query distribution remains constant as we add to the index documents generated by selecting L d terms independently at random from h .
We assume access to a large sample Q from the desired query distribution  X  Q obtained, e.g., by sampling from a past query log. We use Q as training data to build the index that maximizes recall on  X  Q .

L EMMA 2. In a simplistic model where queries contain single terms distributed according h q , and documents contain pairs of terms selected independently at random from h , the optimal per-mutation is obtained by sorting terms in the descending order of Intuitively, the above result means that we want the terms most frequent in queries and least frequent in documents to be at the permutation head, thus minimizing recall loss due to masking. Hardness of Constructing an Optimal Permutation. We say that a permutation (resp. a set of permutations) covers a query if the query has recall above threshold r cov on the permutation (resp. the union of results of the permutations in the set). The following lemma shows that the problem of finding optimal permutation is NP-hard in the general case.

L EMMA 3. For any recall threshold 0 &lt; r cov  X  1 it is NP-hard to find a single permutation p for which the number of queries not covered by p is at most 1 . 36 times the number of queries missed by the optimal permutation p  X  .
 Building Multiple Permutations. Algorithm 2 constructs the set of permutations P . The algorithm iteratively constructs one per-mutation at a time. In each round it assigns an importance weight to each query according to how well existing permutations cover it (line 6). Terms are scored (line 9) by the ratio of the benefit to retrieval (total weight of the queries containing the term) and the estimated index size increase from adding the term to the permuta-tion. The inner loop greedily eliminates lowest scoring terms (line 11), and the prefix of the inverse elimination order defines the per-mutation (line 12). Algorithm 2 Permutation Building Algorithm 1: Input: Q ,h,N . 2: Output: Set of permutations P 3: P 0  X  X  X  4: for i = 1 to N do 5: Initialize hypergraph H ( V = T ,E = Q ) 6: For each q  X  E , set w q  X  1  X  recEst ( q, P i  X  1 ) 7:  X  p  X  () 8: while H is not empty do 10: Append t to  X  p 11: Remove t from V , and all edges adjacent to t from E 12: p i  X  reverse  X  p and truncate it starting from i s.t. 14: return P N Justification and derivation of the algorithm. The problem we wish to solve is finding a small set of permutations P that maxi-mizes the expected coverage of queries from  X  Q : where seqSize ( p ) is the number of documents in p X  X  document sequence, z is the size budget (sequences take most of the space in permutation index), and I (  X  ) is the indicator function. rec ( q, P ) generalizes Eq. (1) for multiple permutations in P :
As the sharp coverage threshold in Eq. (3) leads to an optimiza-tion problem that is hard to solve we instead relax the objective to maximizing E q  X   X  Q rec ( q, P ) . Let E q,d,i be the event that result d is retrieved by permutation p i for query q , where q  X  d . To esti-mate rec ( q, P ) we assume that for each query q and document d the events E q,d,i are independent. Hence we estimate rec ( q, P ) by 1  X  Q p  X  X  (1  X  rec ( q, p )) . Furthermore, assuming independence of terms in documents, we replace rec ( q, p ) by recEst ( q, p ) defined in in Eq. (2). Now let us write recEst ( q, P ) = 1  X  Q p  X  X  (1  X  recEst ( q, p )) , and f ( P ) = E q  X   X  Q recEst ( q, P ) . We now replace the objective in (3) by
Since f is submodular and monotone non-decreasing over the set of permutations and f (  X  ) = 0 , solving (4) under the constraint P p  X  X  seqSize ( p )  X  z is a special case of maximizing a mono-tone submodular set function subject to a modular constraint [11]. Consider the greedy algorithm that starts with P 0 =  X  and itera-tively sets P i +1 = P i  X  X  p i } choosing a near optimal improvement p in each step such that holds for some 0 &lt;  X   X  1 . By the result of Nemhauser et al. and Goundan and Schulz [11] this greedy algorithm achieves an ap-proximation ratio of (1  X  1 /e  X  ) . Observe that the closer  X  is to 1 , the better the algorithm performs.

Now set w q = 1  X  recEst ( q, P i ) and observe that the goal in choosing the next permutation is maximizing
We make several further simplifications to admit optimizing the resulting objective. First, we allow p to be an incomplete permu-tation, since at some point the benefit of adding more terms to p becomes too low due to low recall on these terms. We control the number of terms by the recall threshold r min , and do not add terms beyond position i s.t. Q 1  X  j  X  i (1  X  h ( p [ j ])) &lt; r the above guarantees recall of at least r min on all terms in the (par-tial) permutation, we replace recEst ( q,p ) by a constant 1 . Third, we approximate seqSize ( p ) by P t  X  p h ( t ) . Although this some-what overestimates the size compared to a more accurate estimator 1  X  Q t  X  p (1  X  h ( t )) , the overestimation is not very high since the permutation is relatively short. Finally, replacing expectation by sum we obtain the following objective:
Optimizing the above ratio is a generalization of the well-known densest subgraph problem [13] to weighted hypergraphs, where nodes are terms and hyperedges are queries. The weight of hy-peredge q is w q , and the cost of node t is h ( t ) . An approximate solution for the densest subgraph problem is found by the reverse greedy algorithm that iteratively removes the  X  X ightest X  node, ac-cording to the ratio of the node X  X  degree, the sum of weights of hy-peredges adjacent to it, and the node X  X  cost. A prefix of the reversed removal sequence is an approximate solution to our problem. The complete algorithm for building N permutations is shown in Algorithm 2. The complexity of the algorithm is O ( N P q  X  X  up to a logarithmic factor due to maintaining a min-heap of hyper-graph nodes. Note that the algorithm constructs the optimal per-mutation for the simplistic query and document model described in Lemma 2.
Here we describe two modifications of our algorithms to go be-yond boolean retrieval. For lack of space we omit the details. Ranked Retrieval. We use the fraction of the retrieved score in-spired by the  X  X elative aggregated goodness X  metric for measuring the quality of top-k approximation [16] and as the objective we want to maximize. Score recall is the fraction of the sum of scores of the true results retrieved by our algorithm: where A k ( q ) are the (approximate) top-k results retrieved by the algorithm we are going to develop. Directly optimizing for top-k retrieval is notoriously hard, due to its strong non-linearity, thus we take a heuristic approach and reduce ranked retrieval to conjunctive retrieval through transforming the real-valued query and document vectors to a set of binary vectors each. Then, we implement ranked retrieval by modifying boolean retrieval Algorithm 1 as follows: (1) we do not require the permutation to contain all query terms, i.e., for every permutation p  X  X  we consider the partial boolean query q T p; (2) we read at most m document ids from a subsequence, since partial queries can map to long subsequences; and (3) we score retrieved result using the full vector-space score function and return the top-k.
We build permutations using the algorithm described in Section 3. Then, for every permutation p, we convert every real-valued document vector d into a set of binary vectors B ( p ,d ) . The ob-jective is maximizing the probability that for q  X  Q , there exist vector  X  d  X  B ( p ,d ) such that the binary subquery  X  q = q T p is a prefix of  X  d . As we are not aware of efficient algorithm max-imizing this objective, we resort to heuristics. Apart of d itself, B ( p ,d ) contains binary variants of  X  X ubvectors X  of d with highest include log 2 ( || d || 0 ) such subvectors, each containing half of the 1-s of the previous vector. To reduce the effect of masking, we also include in B ( p ,d ) suffixes of the vectors described above. In our 5 of the leftmost 1-s in  X  d . While both these heuristics increase the size of the document sequences, they increase it by a constant factor. To somewhat offset this size increase, we prune documents that are less likely to be retrieved by our algorithm. We linearly scan document sequences and look for subsequences with the same document vectors. Since we are interested only in the top-k results, for every unique document vector we keep at most k postings that have the highest score.
 Proximity Retrieval. Permutation indexing naturally supports proximity retrieval, where a document matches a query only if all query terms appear within some x -term window in the document. Proximity has been shown to have positive effect on retrieval qual-ity [18, 8, 17]. When proximity window size x is known at the in-dexing time, we remove from B all document (sub)vectors where not all terms ( 1 -s in the vector) appear within x -term window in d . Depending on x this can significantly reduce the index size.
In this section we evaluate permutation indexing on a web search dataset. The main goal is to provide a proof of the concept for appli-cations like web search, and to give additional qualitative insights not available through analysis. In particular we do not compare to the numerous optimizations of retrieval algorithms, caching, and precomputation approaches for two reasons: First, it is tricky to make such a comparison objective due to dependence of retrieval systems X  performance on parameters, datasets, and evaluation set-tings. Second, our approach is conceptually different and we expect it to complement existing approaches rather than replace them. We leave to future work exploration of the parameter space, and tuning as part of particular applications and retrieval systems. Data. To make our experiments reproducible we use public data: the TREC WT10g corpus that consists of 1.6M documents and the AOL query log [15] from which we removed stopwords (the 200 most frequent terms in WT10g) and dropped all single term queries since these can be precomputed/cached exhaustively. We split the queries into the training (90%) and the testing (10%) sets by times-tamps. To analyze post-cache performance we deduplicated each set, and furthermore, removed from the testing set all queries that appear in the training set. Result cache (even of infinite capacity) would have zero hit rate on such a query stream. The final training set contains 5M queries, while for testing we used 2K queries sam-pled uniformly from the testing set. The number of distinct terms in the training set  X  the lexicon size  X  is 350K terms.
 Retrieval algorithms. We evaluated three retrieval tasks: (1) C
ONJ -B OOL  X  boolean conjunctive retrieval that returns all doc-uments matching a conjunctive query, (2) C ONJ -R ANK conjunctive retrieval that returns up to k most highly ranked docu-ments matching a conjunctive query, and (3) D ISJ -R ANK  X  ranked disjunctive retrieval returning up to k most highly ranked docu-ments matching a disjunctive query. For ranked retrieval we use k = 10 and the standard BM25 scoring function.

The ground truth result sets were constructed using the MaxSuc-cessor algorithm [7] for C ONJ -B OOL and C ONJ -R ANK , and MaxS-core [19] for D ISJ -R ANK . Except for MaxScore which scores re-sults during retrieval, we use forward index in a post-processing step to score and rank retrieved results. For C ONJ -B OOL R
ANK we excluded from the evaluation sets about 50% of queries that contain rare terms appearing in less than 1 , 000 documents. This allows more objective evaluation since on such queries both traditional retrieval as well as permutation retrieval achieve 100% recall at bounded cost.
 Metrics. Our objective is approximating standard retrieval at lower cost, rather than improving relevance, thus our evaluation is against results of the baseline inverted index algorithms. As precision is always 1 by construction, our main quality metric is recall. For a given recall threshold X , we say that a query is covered if a re-trieval algorithm reaches recall (resp. score recall for ranked re-trieval) X % or above on that query. We then measure the coverage  X  the fraction of covered test queries, and denote the coverage at X by Cov X . For instance, Table 1 shows that Cov 90 of C ONJ is 0 . 52 .

For cost metrics we consider two main cost components of re-trieval algorithms: locating a particular position in a posing list  X  skipping, and document scoring. We define skipCost of an al-gorithm that performs M skipping operations in the lists of terms ( t ,t 2 ,...,t M ) to be P M i =1 log | t i | , where | t i | denotes the length of the posting list of t i . scoreCost is the number of invocations of the scoring function.
 C ONJ -B OOL 0 . 18 0 . 19 0 . 14 0 . 18 C ONJ -R ANK 0 . 52 0 . 45 0 . 28 0 . 29 D ISJ -R ANK 0 . 71 0 . 77 0 . 29 0 . 32 Table 1: Coverage of regular and proximity retrieval from per-mutation index.
 C ONJ -B OOL 400 17 , 000  X  18 , 000 C ONJ -R ANK 400 55 , 000  X  72 , 000 D ISJ -R ANK 400 197 , 000  X  274 , 000 C ONJ -B OOL 516  X  487 346  X  418 0 C ONJ -R ANK 490  X  440 339  X  407 261  X  446 D ISJ -R ANK 626  X  380 358  X  552 43 , 000  X  40 , 000 Constructing the Permutation Index. We ran Algorithm 2 de-scribed in Section 3 to build N = 4 , 000 permutations of 23K most frequent terms that appear in at least 1,000 documents, us-ing r min = 0 . 4 . Permutations contain between few tens to few thousands of terms. We built two sets of sequences: one for regular and one for proximity retrieval with window size 100. Figure 2: Cov 90 as a function of number of subsequences accessed
A posting in the permutation index is an instance of a document id in a document sequence. In the resulting permutation indices we have 2 , 800 postings per document in all the sequences (resp. 755 in proximity sequences). This is about 13 times (resp. 3 . 5 times for proximity sequences) more than the average 216 postings per document in the inverted index of the same corpus.
 Coverage and Costs. Tables 1, 2, and 3 summarize the coverage and the costs of the algorithms with  X  = 20 and m = 100 . We did not measure the baseline cost of proximity retrieval and consider the cost of regular retrieval to be a lower bound, since proximity retrieval is generally more expensive.

Table 1 shows the fraction of the test queries on which our algo-rithms achieve recall (resp. score recall) of 90% , and 100% (i.e., re-trieve the exact results). The lower coverage of C ONJ -B pared to the ranked retrieval is explained by three factors: (1) we excluded from evaluation 50% of queries on which both the base-lines and our algorithms achieve 100% recall at bounded cost, i.e., we only evaluate on  X  X arder X  queries; (2) in boolean retrieval recall is measured relatively to all document matching the query, while in ranked retrieval relative to the top-k ones; (3) The index is opti-mized for ranked retrieval by preferring document subvectors with higher scores (see Section 4).

Figure 2 shows Cov 90 of regular retrieval as a function of the number of sequences  X  . Even when our algorithm accesses a sin-gle subsequence it covers 4%  X  33% of queries, and by 20 subse-quences the coverage nearly saturates. This explains our choice of  X  = 20 .

In Table 2, the skipCost of our algorithms is constant since it is the product of 20 and the logarithm of sequence length (  X  20 ). In addition to being constant, the cost of our approach is orders of magnitude lower that that of the baselines. In Table 3, the vari-ability of the scoring cost of our algorithms is due to the different number of candidates retrieved from document sequences; yet, it is bounded by  X m = 2 , 000 . The results confirm that our algorithms have significantly lower average cost and that their worst case cost is bounded.
In this work we presented a novel approach to boolean and ranked retrieval where the retrieval cost is bounded and is independent of the corpus size. The approach is based on lossy precomputation of query results and trades off recall and space overhead.

Many open questions remain in this new area. From deeper the-oretical analysis, through improving the underlying algorithms and optimizing them for particular retrieval tasks and designing effec-tive triggering mechanism, to larger scale evaluation and explo-ration of the parameter space. The relative simplicity of supporting proximity gives hope that more general positional retrieval can be efficiently supported as well.
