 Improving verbose (or long) queries poses a new challenge for search systems. Previous techniques mainly focused on two aspects, weighting the important words or phrases and selecting the best subset query. The former does not con-sider how words and phrases are used in actual subset queries, while the latter ignores alternative subset queries. Recently, a novel reformulation framework has been proposed to trans-form the original query as a distribution of reformulated queries, which overcomes the disadvantages of previous tech-niques. In this paper, we apply this framework to verbose queries, where a reformulated query is specified as a sub-set query. Experiments on TREC collections show that the query distribution based framework outperforms the state-of-the-art techniques.
 Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval General Terms Algorithms, Experimentation, Performance Keywords Verbose Query, Query Distribution, Query Reformulation
Verbose (or long) queries have attracted much attention recently, since they allow users to express their information need using natural languages. However, current search sys-tems can not deal with the verbose query well due to its complexity. Thus, improving verbose queries poses a new challenge for the development of search systems.
Previous techniques on improving verbose queries can be roughly divided into two categories. The first category em-phasizes important words and phrases of the original query [6][3][2]. The methods from this category do not consider how those important words and phrases are used together to form actual subset queries therefore missing important relationships between words and phrases. The second cat-egory attempts to select the best subset query [5][1]. The methods of this category indeed consider a subset query as a whole, but they mainly focus on picking the best subset query and ignore alternative subset queries.

Recently, a general reformulation framework has been pro-posed to transform the original query into a distribution of reformulated queries [9]. This framework addresses the disadvantages of previous techniques mentioned above. On one hand, a reformulated query is explicitly modeled in this framework, which helps capture the dependencies between words and phrases that are imposed by actual queries. On the other hand, this framework considers not only the best reformulated query but also other alternatives. In this pa-per, we apply this general framework to improve verbose queries, where the original verbose query is reformulated as a distribution of subset queries.

Modeling the subset distribution for a verbose query has been first studied by Xue et al [10]. However, they used a fixed parameter to combine the original query with the generated subset distribution. Thus, a principled method that can effectively incorporate the original query and its subset queries within the same distribution is still missing.
In this section, we describe how to model the subset dis-tribution for a verbose query. Formally, given the original verbose query Q , we first construct a set of subset queries V Q s = { Q s } ,where Q s is a subset query extracted from Note that Q also belongs to V Q s . Then, Q is reformulated as a distribution over V Q s , i.e. P Q s = { ( P ( Q s | Q ) P (
Q s | Q ) is the probability assigned to Q s in the distribu-tion P Q s . We will describe the generation of V Q s and the estimation of P ( Q s | Q ), respectively.

Following Kumaran and Carvalho [5], only subset queries with the length between three to six words are considered. In addition, we also consider two special subset queries, one is the original query and the other is the key concept discovered in previous work [2]. Furthermore, since the subset queries generated will be finally used for retrieval, it is necessary to indicate the retrieval model used. In this paper, we consider two types of retrieval models, the Query Likelihood Model (QL) [8] and the Sequential Dependency Model (DM) [7].
In order to estimate the probability for each Q s ,weas-sume P ( Q s | Q ) is a linear combination of a variety of query features. These features characterize a subset query as a whole therefore capturing the relationships between words and phrases. Examples of features include various query quality predictors, the number of passages where a subset query has been observed in target corpus and the language model probability returned by Microsoft Web NGram Ser-vice. In order to learn the combination parameter for each query feature, we generate the corresponding retrieval fea-ture by calculating the sum of the retrieval scores of using all subset queries weighted by their query feature values. Then, a learning to rank method is used to learn the parameters Table 2: Results of different models. q,d,s,k denotes significantly different with QL ( q ), DM ( d ), SRank ( ) and KeyConcept ( k ), respectively.
 of these generated retrieval features and these parameters fi-nally serve as the combination parameters of query features. In this paper, a ListNet [4] is used as the learning method.
Two TREC collections (Gov2 and Robust04) are used for experiments. For each collection, the index is built using Indri with Porter Stemmer. For each topic, the description part is used as the query after stopword removal. Mean average precision (MAP) and precision at 10 (P@10) are used to measure the retrieval performance. The two tailed t-test measures significance. The query set is split into a training set and a test set. Ten-fold cross validation is con-ducted. Two types of subset query distributions are imple-mented, one uses QL as the retrieval model (QDist-QL) and the other uses DM (QDist-DM). Table 1 shows an exam-ple of the subset distribution learned for QDist-QL, which significantly outperforms the original query.

The baselines used include QL, DM, SRank [5], KeyCon-cept [2] and two methods from Xue et al [10] (QL+SubQL and DM+SubQL). The retrieval performance is displayed in Table 2. The best performance is bolded.
 Table 2 shows that QDist-QL is comparable with DM, SRank and KeyConcept, i.e., the state-of-the-art techniques on improving verbose queries. Moreover, QDist-DM sig-nificantly outperforms most of the baseline methods and achieves the best performance on both collections. Then, QL+SubQL and DM+SubQL are compared with QDist-QL and QDist-DM, respectively. The difference is that the for-mer uses a fixed parameter to combine the original query with the generated subset distribution, while the latter learns a unified distribution including both the original query and the subset queries. The latter method outperforms the for-mer one on Gov2 and they are comparable on Robust04.
The current subset distribution consists of queries with the mixed length from three to six. It is interesting to ex-plore the fixed-length subset distribution. Table 3 shows the performance of using the fixed-length subset distribu-tion from three to six.  X  X ix X  X enotes the distribution mixing queries with different lengths.

Table 3 shows that the performance of the fixed-length subset distributions is close to and sometimes even better Table 3: Retrieval performance of fixed-length sub-set query distribution.
 than the mixed-length distribution and the length itself does not have much influence. Thus, we can replace the mixed-length subset distribution with the fixed-length distribution, which significantly reduces the number of subset queries in the distribution therefore improving efficiency.
Modeling the subset distribution for a verbose query helps overcome the disadvantages of previous techniques. A re-cently proposed framework is used in this paper to learn a unified subset distribution. Experiments on TREC collec-tions show the effectiveness of this method.
 This work was supported in part by the Center for Intelli-gent Information Retrieval and in part by ARRA NSF IIS-9014442. Any opinions, findings and conclu sions or recom-mendations expressed in this material are the author X  X  and do not necessarily reflect those of the sponsor.
