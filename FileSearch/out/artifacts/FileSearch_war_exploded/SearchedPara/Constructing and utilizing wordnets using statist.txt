 Abstract Lexical databases following the wordnet paradigm capture information about words, word senses, and their relationships. A large number of existing tools and datasets are based on the original WordNet, so extending the landscape of resources aligned with WordNet leads to great potential for interoperability and to substantial synergies. Wordnets are being compiled for a considerable number of languages, however most have yet to reach a comparable level of coverage. We propose a method for automatically producing such resources for new languages based on WordNet, and analyse the implications of this approach both from a linguistic perspective as well as by considering natural language processing tasks. Our approach takes advantage of the original WordNet in conjunction with trans-lation dictionaries. A small set of training associations is used to learn a statistical model for predicting associations between terms and senses. The associations are represented using a variety of scores that take into account structural properties as well as semantic relatedness and corpus frequency information. Although the resulting wordnets are imperfect in terms of their quality and coverage of language-specific phenomena, we show that they constitute a cheap and suitable alternative for many applications, both for monolingual tasks as well as for cross-lingual interoperability. Apart from analysing the resources directly, we conducted tests on semantic relatedness assessment and cross-lingual text classification with very promising results.
 Keywords Lexical resources WordNet Machine learning 1 Introduction Lexical databases are indispensable for many natural language processing tasks. WordNet (Fellbaum 1998 ) is the most well-known and most widely used lexical database for English language processing, and is the fruit of over 20 years of manual work carried out at Princeton University. A large number of existing tools and datasets are based on WordNet, so extending the landscape of resources aligned with WordNet leads to great potential for interoperability and to substantial synergies. The original WordNet for the English language inspired endeavours to create similarly structured resources ( X  X  X ordnets X  X ) for other languages, e.g. in the context of the EuroWordNet EU project (Vossen 1998 ), the BalkaNet project (Tufis  X  et al. 2004 ), as well as under the auspices of the Global WordNet Association. Nevertheless, we contend that despite several decades of work on such resources, there is still a great need for additional research into more efficient means of producing them. Consider, for instance, that there are about 7,000 living languages, but only around 50 for which wordnet versions have been created, many indeed still in a preliminary stage with very low coverage, and only about a handful of languages with wordnet versions that are freely downloadable from the Internet. Furthermore, several existing wordnets unfortunately form completely independent networks that are not connected to and hence not interoperable with other wordnets.
The main bottleneck is the laborious compilation process, which requires skilled experts to work on such a resource for several years. In order to complement the existing manually compiled wordnets, we thus propose a new approach to constructing wordnets that trades off accuracy for a much faster compilation process, and hence frequently leads to more terms being covered than in existing wordnets. Our approach is based on learning classifications, and therefore is completely automatic once an initial set of training associations is provided. The fact that the wordnets are aligned with the original Princeton WordNet greatly facilitates interoperability with existing wordnets (e.g. English-language glosses are available) as well as many additional resources such as ontologies and mappings, as detailed in Sect. 2 .

Certainly, the resulting wordnets will not have the same level of accuracy as resources carefully constructed by skilled lexicographers, however they can (1) serve as a valuable starting point for creating more accurate ones, and (2) be used immediately in many natural language processing tasks where coverage is more important than perfect accuracy, as will later be demonstrated in Sect. 6 .
The remainder of this article is organized as follows. Section 2 begins with a brief introduction to wordnets and their role for interoperability. After a brief summary of alternative compilation techniques in Sect. 3 , the main focus of this article will be a thorough description of an automatic statistical approach to constructing wordnets in Sect. 4 . The implications of using such an approach as well as evaluation results are studied in great detail in Sect. 5 . Section 6 considers possible applications of automatically built wordnets, discussing human use as well as experimental results on natural language processing tasks such as semantic relatedness and cross-lingual text classification. Finally, concluding remarks are provided in Sect. 7 . 2 Wordnets and their role for interoperability We will begin by introducing Princeton WordNet, the original wordnet that inspired all successors, as well as by discussing the role of wordnets for interoperability. 2.1 Princeton WordNet Princeton WordNet (Fellbaum 1998 ) is a lexical database for the English language that captures information about how words and word senses in the English language are related. It lists the senses that a word can assume and identifies senses that are synonymous in meaning as semantic units called synsets . Terms and synsets are organized as a network of nodes linked by various lexico-semantic relations.
The hyponymy relation can be defined as one that  X  X  X olds between a more specific, or subordinate, lexeme and a more general, or superordinate, lexeme, as exemplified by such pairs as  X  X ow X : X  X nimal X ,  X  X ose X : X  X lower X  X  X  (Lyons 1977 ). Hypernymy is the respective inverse relation. In WordNet, these are captured as relations between word senses. The antonymy relation represents semantic opposition between terms. Other relations include instance relationships and several kinds of meronymic relations. 2.2 Wordnets and interoperability There is a significant amount of ongoing work on standards that will facilitate interoperability for language resources and natural processing applications. Apart from agreeing on common data formats, an important challenge is the establishment of shared identifiers that allow us to unambiguously refer to linguistic phenomena. Examples include the ISO 639 standards for language codes and the development of the ISO Data Category Registry to provide labels for parts of speech, syntactic constituency, etc. (Francopoulo et al. 2008 ).

At the same time, there is also an increasing need to refer to word senses in an unambiguous way, e.g. in translation resources. We believe that WordNet qualifies as a suitable starting point for developing a multilingual sense inventory. Wordnets in several languages are already connected to the original one. Geographical information (Buscaldi and Rosso 2008 ) and pictures (Deng et al. 2009 ) are available for many sense identifiers listed in WordNet. Other resources linked to WordNet include topical domain labels (Bentivogli et al. 2004 ), verb lexicons such as VerbNet (Kipper et al. 2000 ) and FrameNet (Baker and Fellbaum 2008 ), and ontologies like SUMO (Niles and Pease 2003 ), YAGO (Suchanek et al. 2007 ), DOLCE (Gangemi et al. 2003 ), and OpenCyc (Cycorp Inc. 2008). Via YAGO, WordNet is also connected to Wikipedia and many other datasets in the Linked Data Web (Bizer et al. 2009 ).

By building new wordnets that are aligned with the English WordNet, we can not only contribute to this infrastructure and increase its value, but also benefit from it when deploying the new wordnets for natural language processing.
 3 Previous work on building wordnets automatically Prior to introducing our statistical approach to constructing wordnets, we will summarize some of the previous means of creating wordnets.

One general strategy is the so-called merge model , where an existing thesaurus is converted to a wordnet-like format and then semi-automatically linked to other wordnets or to an interlingual synset index. The downside of this strategy is that it cannot be applied to a large range of languages, unless some pre-existing wordnet-like thesaurus for each of these languages is found or established.

An alternative general strategy is the expand model , which requires much fewer pre-existing resources. The general approach is as follows: (1) Take an existing wordnet for some language L 0 , usually Princeton WordNet for English. (2) For each sense s listed by the wordnet, translate the terms associated with s from L 0 to a new language L N using a translation dictionary. (3) Additionally retain all appropriate semantic relations between senses from the existing wordnet in order to arrive at a new wordnet for L N .

The main challenge lies in determining which translations are appropriate for which senses. A dictionary translating an L 0 -term e to an L N -term t does not imply that t applies to all senses of e . For example, with regard to the translation from the English word  X  X  bank  X  X  to the German  X  X  Bank  X  X , we observe that the English term can also be used for riverbanks, while the German  X  X  Bank  X  X  cannot (and likewise, German  X  X  Bank  X  X  can also refer to a park bench, which does not hold for the English term).

In order to address these problems, several different heuristics have been proposed. Knight ( 1993 ) created an ontology for machine translation by linking entries in Longman X  X  Dictionary of Contemporary English to WordNet, taking into account gloss definitions as well as the semantic hierarchy information present in the dictionary, though unfortunately not available in the settings we consider (cf. Sect. 4.2 ). Okumura and Hovy ( 1994 ) used a Japanese-English dictionary to link a Japanese lexicon to this ontology, based on several heuristics, most importantly monosemy, i.e. considering when the ontology lists only one candidate concept for an English translation, and equivalent word matches, i.e. accepting the concepts shared by multiple translations of a word.

Another important line of research starting with Rigau and Agirre ( 1995 ), and extended by Atserias et al. ( 1997 ) resulted in automatic techniques for creating preliminary noun-only versions of the Spanish WordNet and later also the Catalan WordNet (Benitez et al. 1998 ). Several heuristic decision criteria were used in order to identify suitable translations, e.g. monosemy/polysemy heuristics, checking for senses with multiple terms having the same L N -translation, as well as heuristics based on conceptual distance measures. Later, these were combined with additional Hungarian-specific heuristics to create a Hungarian nominal WordNet (Miha  X  ltz and Pro  X  sze  X  ky 2004 ).

Pianta et al. ( 2002 ) used similar ideas in conjunction with a cosine similarity-based heuristic to produce rankings of the most likely candidate senses. In their work, the ranking was not used to automatically generate a wordnet but merely as an aid to human lexicographers that allowed them to work at faster pace. This methodology was used to create MultiWordNet Italian and later also adopted for the Hebrew WordNet (Ordan and Wintner 2007 ).
 Sathapornrungkij and Pluempitiwiriyawej ( 2005 ) used criteria proposed by Atserias et al. ( 1997 ), and then performed a regression analysis in order to reduce the number of accepted associations and thus increase the accuracy. Since they merely relied on 12 binary criteria rather than numeric scores, they were unable to obtain a higher recall by applying their model to other term-sense pairs not fulfilling one of the chosen criteria.

A more advanced approach that requires only minimal human work lies in using machine learning algorithms based on a large number of scores to identify more subtle decision rules. These decision rules can rely on a number of different heuristic scores with different thresholds. 4 Building wordnets by learning classifications 4.1 General outline In order to build wordnets automatically, we suggest the following approach. Let L N denote the language for which a wordnet is to be constructed, and L 0 denote the language of an existing wordnet that serves as a template for the new one, in our case the English language due to our choice of Princeton WordNet as the template. Acknowledging the caveats pointed out in Sect. 5 , we can treat this existing wordnet as providing an inventory of possible senses.

The most important desideratum obviously are the links from terms in L N to their respective senses. This challenge is tackled by means of translation dictionaries, which we use to obtain translations of terms from L N to terms from L 0 . These translations in turn allow us to construct for each of the original L N -terms a candidate set of synsets that are potentially valid senses.

The central difficulty then is determining which of the candidate synsets to accept and which not. Given the polysemy of terms in L 0 , it often turns out that the majority of the candidate synsets are not acceptable as senses for the L N -term. Our approach relies on a set of training associations between L N -terms and synsets to learn a disambiguation model that can then provide confidence scores indicating how certain we can be about a particular association being correct.

To create this disambiguation model, we compute several numeric scores ( feature values ) for a given association between an L N term t and a candidate synset s , which together constitute a feature vector . Based on a small set of manually established labels for such ( t , s )-pairs, we create the corresponding training set of feature vectors. The disambiguation model can then be derived using well-known classification learning techniques that consider statistical properties of the training vectors. Such a model can be used to make predictions for any other ( t , s )-pair. To create the new wordnet, the model is applied to all pairs ( t , s ) consisting of an L N term t and one of its candidate synsets s . In a final step, one can then import certain relations between synsets from the existing wordnet.

This approach has several advantages compared to the previous work in this field (cf. Sect. 3 ). First of all, the previous automatic approaches were based on hard acceptance criteria X  X ither a ( t , s )-pair satisfies a criterion or not. Many attributes of word senses do not lend themselves easily to such an antagonistic view, e.g. sense relatedness measures produce numeric scores, and thus can be better accommodated in a model that uses real-valued feature vectors. Furthermore, while Atserias et al. ( 1997 ) investigate combinations of two heuristics to arrive at a greater accuracy, a classification learning approach can take into account suitable combinations of even more heuristics, indeed arbitrary linear (or even non-linear) combinations of feature values.

Following this general description of the overall procedure, the following sections will expound on several aspects in much greater detail. 4.2 Candidate sets Given a translation from a term t from L N to a term e from L 0 , it is safe to assume that there is some semantic overlap between t and e , and hence there is a reasonably high probability that some sense of e is also a sense of t .

Our approach makes use of translation dictionaries, however with the constraint of relying on a minimal amount of information specific to L N so that the procedure remains generalizable to as many languages as possible. The dictionary is thus conceived as offering a simple n : m -mapping between terms in L 0 and terms in L , with optional part of speech information, as in the following German-English excerpt:
We thus proceed as follows: for each term t from L N , retrieve the set of translations / ( t ). For each L 0 -translation e in such a / ( t ), retrieve the set of senses r ( e ) from our existing wordnet, e.g. for the German term  X  X  Schulklasse  X  X  the senses of the translations  X  X  class  X  X  and  X  X  form  X  X  would be considered.
 The union t , and our goal will be to determine for each sense s 2 C  X  t  X  whether it is appropriate to consider s a sense of t . This is undoubtedly a very difficult task, as the dictionaries provide only limited information that could aid in determining which of the often many different senses apply, e.g. WordNet lists 9 senses for the word  X  X  class  X  X  and 23 senses for  X  X  form  X  X  . 4.3 Feature computation In our approach, this task of determining the appropriate senses among the candidates is construed as a binary classification problem. A real-valued feature vector x is created for each pair ( t , s ) of a term t from L N and a relevant candidate sense s 2 C  X  t  X  : For example, if t represents  X  X  Schulklasse  X  X , then s could be one of the senses of  X  X  class  X  X . In order to create the feature vectors, a variety of different scores x i are used as features and combined as components of numeric vectors x  X  X  x 1 ; ... ; x m  X 2 R m : These scores x i are intended to quantify some information about the respective term-sense pair. 4.3.1 Sense weighting functions Several features that will be described later on depend on some kind of assessment of the importance of senses s with respect to the particular L N -term t under consideration. We consider the following weighting functions c ( t , s ):  X  c 1 ( t , s ) = 1 is used for unweighted features  X  c lc ( t , s ) represents an estimation of the lexical category compatibility between t  X  c r ( t , s ) considers the ranks of the senses as listed by WordNet for the translations of  X  c f ( t , s ) considers the corpus frequency information provided with WordNet: 4.3.2 Semantic relatedness measures Apart from weighting functions, our approach is fundamentally based on measures of semantic relatedness between senses, e.g. the single sense of  X  X  schoolhouse  X  X  i s relatedness contributes to many of our fitness scores, we shall first introduce several relatedness estimation heuristics.  X  sim f ( s 1 , s 2 ) considers not only whether two senses are identical but also takes  X  sim n ( s 1 , s 2 ) considers the neighbourhood in the graph constituted by WordNet X  X   X  sim m ( s 1 , s 2 ), finally, is a meta-measure that is simply defined as and hence combines the power of sim f ,sim n , and sim c . It is particularly valuable due to thefactthatsim n and sim c are based on very different characteristics of the senses. 4.3.3 Semantic overlap features One important way of making use of the semantic relatedness measures is to exploit that an association should more likely be accepted when a term t has multiple English translations e , and the candidate sense s under consideration is somewhat pertinent to multiple of them. For instance, the German  X  X  Schulklasse  X  X  has the terms students who are taught together but also e.g. to a tax form, only the former of these two senses overlaps semantically with the senses of  X  X  class  X  X  .

Given a term t and a candidate sense s , we integrate scores of the following form into the respective feature vector: where sim( s 1 , s 2 ) represents a semantic relatedness measure and the c ( t , s ) function provides weights as described earlier. The simple identity relatedness function sim id and the constant weighting function c 1 ( t , s ) = 1 make Eq. 3 yield a simple count of how many English terms are mapped to the sense, reminiscent e.g. of the equivalent word matching of Okumura and Hovy ( 1994 ) (cf. Sect. 3 ). By using the above formulae to produce a large number of feature values with all combinations of weighting functions and relatedness measures mentioned in Sects. 4.3.1 and 4.3.2 , we are additionally able to account for cases where the terms are related but do not share senses. 4.3.4 Polysemy-based scores Another set of features are based on the polysemy of the L 0 -translations, i.e. on the idea that an association is more likely correct whenever there are few alternative senses to choose from. Akin to the monosemy heuristic of Okumura et al. (see Sect. 3 ), we can consider for instance the German  X  X  Schulleiter  X  X  with its translation  X  X  headmaster  X  X , which in turn only has one single sense listed in WordNet, so it is rather safe to accept this sense also for the German term. More generally, given a term t and a sense s , several scores can be computed as where c ( t , s ) is a weighting function and C ( t ) stands for the complete candidate set. Another set of scores is computed as and 0 otherwise.

Again, we can use sim id ( s 1 , s 2 ) and c 1 ( t , s ) to illustrate the simplest case: With these choices, Eq. 5 yields the reciprocal of the total number of candidate senses and in Eq. 6 the denominator of each addend becomes 1 whenever the respective term e is monosemous according to WordNet. More advanced scores are computed by 4.3.5 Additional features We further consider a series of other, less essential features, including the following:  X  scores based on the number of translations  X  the ratio  X  a score based on back-translations  X  the number of lexicalizations of the candidate sense, i.e. r 1  X  s  X  ; where r -1 ( s )  X  the ratio of sense lexicalizations that are translations of t , i.e.
  X  indicator values that express whether the candidate sense s is a noun, verb, 4.3.6 Lexical category compatibility Unlike previous work, our study considers all lexical categories (parts of speech) covered by the existing wordnet rather than just nouns. This immediately leads to the problem that the number of candidate senses greatly increases, and we need to come up with some means of preventing a noun from being mapped to a verb sense in WordNet, for instance.

Our solution rests on two pillars. Obviously, whenever the translation dictionary explicitly provides lexical category information, one can simply use hard-coded compatibility indicators, e.g. we give any German adjective a compatibility value of 0.0 with English noun senses, but 1.0 with English adjective as well as adverb senses.

In light of the fact that such explicit information may not always be available, we resort to additional heuristics when necessary, thereby ensuring that our approach remains applicable to a broad range of different scenarios. For each lexical category, a C4.5 decision tree is used to estimate the compatibility based on superficial attributes of the terms such as suffixes and capitalization. In many languages, such attributes provide hints about the part of speech of a word. Growing the trees does not require any manually created training data, because we can leverage terms where all candidate senses share the same lexical category as examples. The features employed are given in the following list. Note that since the terms in L N can be multi-word expressions, much of this information is captured separately for the first and last word of any candidate expression.  X  prefixes of the first and last word up to a length of 10, e.g. for the German verb  X  suffixes of the first and last word up to a length of 10 (without case conversion),  X  capitalization of the first and last word (Boolean features for no capitalization,  X  term length
The decision trees were pruned to have confidence levels of at least 0.25 with at least 2 instances per leaf. The confidence estimations from the leaves can then be used to determine a lexical category compatibility score as a feature in the feature vector. For languages where the predictions are too unreliable, we may instead use a constant value of 0.5. 4.4 Learning the disambiguation model Having defined a feature computation procedure, we can apply well-known classification learning techniques to derive the disambiguation model.

A classification is an assignment of class labels y 2Y to objects x 2X ; and can be formalized as a function b f : XY ! X  0 ; 1 that, given such an x and y , yields a value that provides the degree of confidence in the assignment being correct. We consider only binary problems, where Y X f A ; A g for some class A and its complement A ; and only consider the single label case, where each object is assigned exactly one class. Learning a with low approximation error, given a set of correctly labelled training examples  X  x ; y  X 2XY : In our case, the objects are term-sense pairs x = ( t , s ), and the class y is either A or its complement A ; where A is the class of all ( t , s ) pairs that represent appropriate term-sense associations.

Provided that the objects x 2X are represented in a suitable manner, most commonly as numerical feature vectors x in an m -dimensional Euclidean feature space R m ; one of several learning algorithms can be employed to learn a classification. Support vector machines constitute a class of algorithms based on the idea of computing a decision hyperplane w T / ( x ) ? b = 0 that maximizes the margin between positive and negative training instances in the feature space (Vapnik 1998 ). Such maximum-margin hyperplanes tend to entail lower general-ization errors than other separation surfaces, and the task of finding them leads to a quadratic optimization problem. Additional slack variables may be included to obtain a soft margin solution that is able to cope with training data that cannot be separated cleanly (Cortes and Vapnik 1995 ). The decision surface can be computed using Lagrange multipliers and decomposing techniques such as sequential minimal optimization (Platt 1999 ).

Using a simple dot product, we can then determine the distance f  X  x  X 2 R of a new instance x to this decision hyperplane in the feature space. A sigmoid function these distances, where parameter fitting for a and b is performed using maximum likelihood estimation on the training data (Platt 2000 ; Lin et al. 2007 ). These P  X  y  X  A j x  X  for a given instance x = ( t , s ). 4.5 Generating the wordnet instance We then apply one of the following rules for every ( t , s ) where t is an L N -term from the translation dictionary and s 2 C  X  t  X  is a candidate sense as defined earlier: (a) accept as a weighted connection with weight c ( t , s ) if and only if c ( t , s ) &gt;0,or (b) accept as an unweighted connection if and only if either c ( t , s ) C c min ,or
The first rule results in a weighted statistical wordnet for L N , whereas the second one yields a more conventional unweighted wordnet.

Finally, new connections as well as of course new senses may be introduced manually to make the wordnet more complete. The introduction of new senses is particularly likely to be necessary for terms in L N that had empty candidate sets.
Relational information for new synsets needs to be added manually. For the original synsets from the existing wordnet, we can immediately import a large number of links. Most importantly, hypernym links between synsets that have been found to have lexicalizations in L N can quite safely be transferred to the new wordnet. It should however be noted that certain relations need to be re-interpreted as generic relatedness links between senses (e.g. the derivation relation), or are completely excluded from being imported (e.g. region domains). These issues are discussed in more detail in Sect. 5.4 . 5 Evaluation and analysis of a machine-generated wordnet While our approach is applicable to virtually any language, in the remainder of this article, we will focus on a German-language wordnet produced using our machine learning approach. Princeton WordNet 3.0, which covers around 155,000 English terms and around 118,000 senses, served as the existing template for the new wordnet. We further relied on the Ding German-English dictionary (Richter 2007 ), a large and fairly reliable digital translation dictionary with around 216,000 entries, but not much additional information apart from optional part of speech tags. A linear kernel SVM decision hyperplane was computed using LIBSVM (Chang and Lin 2001 ) and a training set consisting of 1,834 candidate associations (for 350 randomly selected German terms) that were manually classified as correct (22 %) or incorrect. The values c min = 0.5 and c 0 min  X  0 : 45 were chosen as classification thresholds as described in Sect. 4 to generate the German wordnet. In order to obtain unbiased evaluation results, no form of manual revision was performed. 5.1 Accuracy and coverage When evaluating the quality of this wordnet, we cannot rely on existing wordnets because these only provide positive examples but not negative ones, e.g. the fact that GermaNet (Kunze and Lemnitzer 2002 ) does not list the body of artists or sense association is incorrect. Instead, we considered a test set of 1,624 labelled sense associations obtained in the same way as the training set but completely independent from it, and thus not involved in any way in the wordnet building process. One can then evaluate to what degree the generated wordnet corresponds with the test set using standard evaluation measures. Given a test set, the precision is defined as P T P number of true positives, false positives, and false negatives, respectively. Table 1 summarizes the results for our German wordnet, showing the precision and recall with respect to this test set.

The results demonstrate that indeed a surprisingly high level of precision and recall can be obtained with fully automated techniques, considering the difficulty of the task. While the precision might not fulfil the high lexicographical standards many practical applications. Furthermore, one of course may obtain a higher level of precision at the expense of a lower recall by adjusting the acceptance thresholds. Table 2 provides a sample of results obtained using alternative thresholds. For very high recall levels, an increased precision might not be realistic even with purely annotator agreement of 84.73 % for such associations.

In addition to the recall scores in Table 1 , which are based on the test set, Table 3 provides absolute numbers of terms covered by the German wordnet (using the classification thresholds c min = 0.5 and c 0 min  X  0 : 45). While smaller than GermaNet 5.0, this automatically generated wordnet instance is already larger by an order of magnitude than many other manually compiled ones.

Table 4 gives an overview of the polysemy of the terms as covered by our wordnet, with arithmetic means computed from the polysemy either of all terms, or exclusively from terms that are polysemous with respect to the wordnet.

A more qualitative assessment of the accuracy and coverage revealed the following issues:  X  Non-Uniformity of Coverage: While even many specialized terms are included  X  Lexical Gaps and Incongruences: Another issue is the lack of senses for which  X  Multi-word expressions in L N : Certain multi-word translations in L N might be
Of course, the most general and reliable solution to ensure that the wordnet truly captures the typical senses of all terms and is free of incorrect sense associations is to perform a complete manual verification and revision process. 5.2 Comparison with alternative approaches Our technique is further compared to four alternative approaches. We study the first sense heuristic, which involves simply accepting the first sense listed by WordNet for any English term. This heuristic is frequently cited as being more successful than many other methods in word sense disambiguation tasks because the rank reflects the corpus frequency and importance of a sense. We also evaluate existing automatic approaches presented in Sect. 3 . For Rigau and Agirre ( 1995 ), we considered the approach described in the second part of their paper, which was used to obtain a preliminary Spanish WordNet. From the study by Atserias et al. ( 1997 ), we consider the monosemy 1 X 4, variant, as well as the combined brother and polysemy 1/2 criteria. The CD criteria and the field criterion were not applied because their implementation in the original study is mainly based on additional lexical information for the Spanish language apart from the list of translations. The results, presented in Table 5 , demonstrate that our learning-based approach outperforms the existing approaches both in terms of precision as well as in terms of recall. While two previous heuristics arrive at similarly high levels of recall, this occurs at the expense of very low precision scores. By adjusting the c min ; c 0 min confidence thresholds, our method can be made to produce recall scores well above 90 % at such levels of precision (cf. Table 2 ). 5.3 Relational coverage By producing associations with senses of an existing source wordnet, we have the great advantage of immediately being able to import relations between the respective synsets. An excerpt of some of the relations we imported is given in Table 6 .

Lexical relations between particular terms cannot, in general, be transferred automatically, e.g. a region domain for a term in one language, signifying in what geographical region the term is used, will not apply to a second language. However, certain lexical relations such as the derivation relation still provide valuable information when interpreted as a general indicator of semantic relatedness, as can be seen in Table 7 , which shows the results of a human evaluation for several different relation types. Incorrect relations are almost entirely due to incorrect term-sense associations. 5.4 Structural adequacy As mentioned earlier, our machine learning approach is very parsimonious with respect to L N -specific prerequisites, and hence scales well to new languages. Some might contend that using one wordnet as the structural basis for another wordnet does not do justice to the structure of the new language X  X  lexicon.

The most significant issue is certainly that the source wordnet may lack senses for certain terms in the new language or may not make the right sense distinctions, as in the case of the German  X  X  Feierabend  X  X . This point has already been discussed in Sect. 5.1 .It should also be clear that senses without any associated terms are to be considered unlexicalized nodes that do not directly represent the lexicon of the language.
Apart from these two considerations, it seems that general structural differences between languages rarely are an issue. When new wordnets are built independently from existing wordnets, many of the structural differences will not be due to actual conceptual differences between languages, but rather result from subjective decisions made by the individual human modellers (Pianta et al. 2002 ).

Some of the rare examples of cultural differences affecting relations between two senses include perhaps the question of whether the local term for  X  X  guinea pig  X  X  should count as a hyponym of the respective term for  X  X  pet  X  X . For such cases, our suggestion is to manually add relation attributes that describe the idea of a connection being language-specific, culturally biased, or based on a specific taxonomy rather than holding unconditionally.

A more general issue is the adequacy of the four lexical categories (parts of speech) considered by Princeton WordNet. Fortunately, most of the differences between languages in this respect either concern functional words, or occur at very fine levels of distinctions, e.g. genus distinctions for German nouns, and thus are conventionally considered irrelevant to wordnets, though such information could be derived from monolingual dictionaries and added to the wordnet. 6 Applications 6.1 Human consultation One major disadvantage of automatically built wordnets is the lack of native-language glosses and example sentences, although this problem is not unique to automatically-built wordnets. Because of the great effort involved in compiling such information, manually built wordnets such as GermaNet also lack glosses and example sentences for the overwhelming majority of the senses listed. In this respect, automatically produced aligned wordnets have the advantage of at least making English-language glosses accessible.

Another significant issue is the quality of the sense associations. As people are more familiar with high-quality print dictionaries, they do not expect to encounter incorrect entries when consulting a WordNet-like resource.

We found that machine-generated wordnets can instead be used to provide machine-generated thesauri, where users expect to find more generally related terms rather than precise synonyms and gloss descriptions. In order to generate such a thesaurus, we relied on a simple technique that looks up all senses of a term as well as certain related senses, and then forms the union of all lexicalizations of these senses ((Algorithm 6.1 with n = 2, n o = 2, n g = 1). Table 8 provides a sample entry from the German thesaurus resulting from our wordnet, and demonstrates that such resources can indeed be used for example as built-in thesauri in word processing applications. 6.2 Natural language processing In this section, we will discuss some of the possible applications of automatically generated wordnets.

It turns out that the alignment with the English WordNet proves to be a major asset not only for cross-lingual but also for monolingual applications, as one can leverage much of the information associated with the Princeton WordNet, e.g. the included English-language glosses, as well as topical domain information, links to ontologies, and a range of other third-party resources described in more detail in Sect. 2 .

For the task of word sense disambiguation, Patwardhan et al. ( 2003 ) presented an algorithm that maximizes the overlap of the English-language glosses (Patwardhan et al. 2003 ) with promising results, however we were unable to evaluate it more adequately due to the lack of an appropriate sense-tagged test corpus. One issue we noted was that the generated wordnet did not always cover all of the terms and senses to be disambiguated, which means that it is not a perfect sense inventory for word sense disambiguation tasks.

Apart from this, we believe that automatically generated wordnets are well-suited for virtually all other tasks that wordnets can been used for, including conventional information retrieval, multimedia retrieval, cross-lingual information retrieval (Chen et al. 2000 ), text classification, text summarization, coreference resolution (Harabagiu et al. 2001 ), machine translation, as well as semantic relatedness estimation and cross-lingual text classification, which we will now consider in more detail. 6.3 Case study: semantic relatedness Several studies have attempted to devise means of automatically approximating semantic relatedness judgments made by humans, predicting e.g. that most humans consider the two terms  X  X  fish  X  X  a n d  X  X  water  X  X  semantically related. Such relatedness information is useful for a number of different tasks in information retrieval and text mining, and various techniques have been proposed, many relying on lexical resources such as WordNet. For the German language, Gurevych ( 2005 ) reported that Lesk-style similarity measures based on the similarity of gloss descriptions (Lesk 1986 ) do not work well in their original form because GermaNet features only very few glosses, and those that do exist tend to be rather short. With machine-generated aligned wordnets, however, one can apply virtually any existing measure of relatedness that is based on the English WordNet, because English-language glosses and co-occurrence data are available.

We proceeded using the following assessment technique. Given two terms t , t 2 , one estimates their semantic relatedness using the maximum relatedness score between any of their two senses: For the relatedness scores sim( s 1 , s 2 ), we consider three different approaches, described in more detail in Sect. 4.3.2 1. sim n ( s 1 , s 2 ): graph neighbourhood proximity 2. sim c ( s 1 , s 2 ): cosine similarity of extended glosses 3. sim m ( s 1 , s 2 ): maximum (meta-method)
For evaluating the approach, we employed three German datasets (Gurevych 2005 ; Zesch and Gurevych 2006 ) that capture the mean of relatedness assessments made by human judges. In each case, the assessments computed by our methods were compared with these means, and Pearson X  X  sample correlation coefficient was computed. The results are displayed in Table 9 , where we also list the current state-of-the-art scores obtained for GermaNet and Wikipedia as reported by Gurevych et al. ( 2007 ).

The results show that our semantic relatedness measures lead to near-optimal correlations with respect to the human inter-annotator agreement correlations. The main drawback of our approach is a reduced coverage compared to Wikipedia and GermaNet, because scores can only be computed when both parts of a term pair are covered by the generated wordnet.

One advantage of our approach is that it may also be applied without any further changes to the task of cross-lingually assessing the relatedness of English terms with German terms. In the following section, we will take a closer look at the general suitability of our wordnet for multilingual applications.
 6.4 Case study: cross-lingual text classification Text classification is the task of assigning text documents to the classes or categories considered most appropriate, thereby e.g. topically distinguishing texts about thermodynamics from others dealing with quantum mechanics. This is commonly achieved by representing each document using a vector in a high-dimensional feature space where each feature accounts for the occurrences of a particular term from the document set (a bag-of-words model), and then applying machine learning techniques such as support vector machines. For more information, please refer to Sebastiani ( 2002 ).

In comparison with the standard monolingual case, cross-lingual text classifi-cation is a much more challenging task. Since documents from two different languages obviously have completely different term distributions, the conventional bag-of-words representations deliver poor results. Instead, it is necessary to induce representations that tend to give two documents from different languages similar representations when their content is similar.

One means of achieving this is the use of language-independent conceptual feature spaces where the feature dimensions represent meanings of terms rather than just the original terms. We process a document by removing stop words, performing part of speech tagging and lemmatization using the TreeTagger (Schmid 1994 ), and then map each term to the respective sense entries listed by the wordnet instance. In order to avoid decreasing recall levels, we do not disambiguate in any way other than acknowledging the lexical category of a term, but rather assign each sense s a w t , s is the weight of the link from t to s as provided by the wordnet if the lexical category between document term and sense match, or 0 otherwise. We test two another based on a weighted German wordnet ( w t ; s 2 X  0 ; 1 ), as described in Sect. 4.5 . Since the original document terms may include useful language-neutral terms such as names of people or organizations, they are also taken into account as tokens with a weight of 1. By summing up the weights for each local occurrence of a token t (a term or a sense) within a document d , one arrives at document-level token occurrence scores n ( t , d ), from which one can then compute TF-IDF-like feature vectors using the following formula: where D is the set of training documents.
 This approach was tested using a cross-lingual dataset derived from the Reuters RCV1 and RCV2 collections of newswire articles (Reuters 2000a , b ). We randomly selected 15 topics shared by the two corpora in order to arrive at 15 binary classification tasks, each based on 200 training documents in one language, and 600 test documents in a second language, likewise randomly selected, however ensuring equal numbers of positive and negative examples in order to avoid biased error rates. We considered a) German training documents and English test documents and b) English training documents and German test documents. For training, we relied on the SVMlight implementation (Joachims 1999 ) of support vector machine learning (Vapnik 1998 ), which is known to work very well for text classification.

The results in Table 10 clearly show that automatically built wordnets aid in cross-lingual text classification. Since many of the Reuters topic categories are business-related, using only the original document terms, which include names of companies and people, already works surprisingly well, though presumably not well enough for use in production settings. By considering wordnet senses, both precision and recall are boosted significantly. This implies that English terms in the training set are being mapped to the same senses as the corresponding German terms in the test documents. Using the weighted wordnet version further improves the recall, as more relevant terms and senses are covered. 7 Conclusions We have shown that wordnets can be built automatically if we are willing to accept a certain percentage of imprecise sense associations, and that these resources are nevertheless quite useful for various purposes. Our approach to constructing wordnets is based on statistical learning from a number of numeric scores and leads to a better coverage than the hard criteria proposed in previous studies, while simultaneously also allowing for a higher level of accuracy.

We have since conducted further experiments demonstrating that the method presented scales well to new languages (de Melo and Weikum 2009 ), as care was taken to require just a minimal amount of information specific to L N . This enables us to produce a large-scale multilingual wordnet covering many different languages, available at http://www.mpii.de/yago-naga/uwn/ .

Wordnets of this sort greatly facilitate interoperability, as they are aligned to the original Princeton WordNet, and thus also to other resources that are similarly aligned. First of all, of course, the machine-generated wordnets can serve as a valuable starting point for establishing more reliable wordnets, which would involve manually extending the coverage and addressing issues arising from differences between the lexicons of different languages. At the same time, machine-generated wordnets can be used directly without further revision to generate thesauri for human use, or for a number of different natural language processing applications, as we have shown in particular for semantic relatedness estimation and cross-lingual text classification.

In the future, we would like to investigate automatic techniques for extending the coverage of such statistically generated wordnets to senses not covered by the existing wordnets. We hope that our research has contributed to making lexical resources available for languages that previously had not been considered by the wordnet community.
 References
