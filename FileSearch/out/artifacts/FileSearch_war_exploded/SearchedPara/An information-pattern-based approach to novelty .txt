 1. Introduction 1.1. What is novelty detection?
Novelty detection can be viewed as going a step further than traditional document retrieval. Based on the output of a document retrieval system (i.e., a ranked list of documents), a novelty detection system further extracts documents with new information from the ranked list. The goal is for users to quickly get useful infor-mation without going through a lot of redundant information, which is usually a tedious and time-consuming task. The detection of new information is an important component in many potential applications. It is a new research direction, but it has attracted increasing attention in the information retrieval field. The TREC nov-elty tracks, which are related to novelty detection, were run for three years ( Harman, 2002; Soboroff, 2004; Soboroff &amp; Harman, 2003 ).

Novelty detection can be performed at two different levels: the event level and the sentence level. At the event level, a novel document is required to not only be relevant to a topic (i.e., a query) but also to discuss a new event. At the sentence level, a novel sentence should be relevant to a topic and provide new information.
This means that a novel sentence may either discuss a new event or provide new information about an old the work in this paper will focus on novelty detection at the sentence level. 1.2. Motivation and contribution of the work
A variety of novelty measures have been described in the literature ( Allan, Wade, &amp; Bolivar, 2003; Harman, 2002; Zhang, Callan, &amp; Minka, 2002; Zhang, Lin, Liu, Zhao, &amp; Ma, 2003; Soboroff &amp; Harman, 2003 ). The various definitions of novelty, however, are quite vague and seem only indirectly related to intuitive notions of novelty. For example, in Zhang, Callan et al. (2002) , novelty detection was informally defined as the opposite of redundancy. In Allan et al. (2003) , it was described as new information based on new words existing in a sentence. In Harman (2002) and Soboroff and Harman (2003) , no clear definition of novelty was provided.
Usually, new words appearing in an incoming sentence or document contribute to the novelty scores in var-ious novelty measures in different ways. However, new words are not equivalent to novelty (new information).
For example, rephrasing a sentence with a different vocabulary does not mean that this revised sentence con-tains new information that is not covered by the original sentence. Furthermore, new words appearing in material that is not relevant to a user X  X  query do not provide any new information with respect to the user X  X  interest, even though it may contain some new information useful to other users. A more precise definition of novelty or new information is required.

Let us look at a real example to understand the limitations of the related work and why we are proposing a new approach for novelty detection. Topic (query) 306 from the TREC novelty track 2002 is about  X  X  X frican
Civilian Deaths X  X . The user is asking for the number of civilian non-combatants that have been killed in the various civil wars in Africa. Therefore, a number should appear in sentences that are relevant to the query. Let us consider the following four sentences given in Example 1 .
 Example 1. Topic:  X  X  X frican Civilian Deaths X  X   X  Number  X  : 306  X  Title  X  African Civilian Deaths  X  Description  X  : How many civilian non-combatants have been killed in the various civil wars in Africa?
 X  Narrative  X  : A relevant document will contain specific casualty information for a given area , country ,or region . It will cite numbers of civilian deaths caused directly or indirectly by armed conflict. Sentence 1 (Relevant) :  X  X  X t could not verify Somali claims of more than 100 civilian deaths  X  X .
Sentence 2 (Relevant) :  X  X  X atal X  X  death toll includes another massacre of 11 ANC [ African National Con-gress] supporters X  X .

Sentence 3 (Non-relevant) :  X  X  X nce the slaughter began, following the death of President Juvenal Habyari-mana in an air crash on April 6, hand grenades were thrown into schools and churches that had given ref-uge to Tutsi civilians  X  X .

Sentence 4 (Non-relevant) :  X  X  X  Ghana News Agency correspondent with the West African force said that rebels loyal to Charles Taylor began attacking the civilians shortly after the peace force arrived in Monrovia last Saturday to try to end the eight-month-old civil war. X  X 
Each of the four sentences has two query terms (in italics ) that match the key words from the query. How-ever, only the first two sentences are relevant sentences. In addition to the two matching words, each of the first two sentences also has a number, 100 and 11, respectively. Hence, the first two sentences are both topi-cally relevant to the query and have the right type of information with respect to the user X  X  request behind the query. The third sentence and the fourth sentence are not relevant to the query mainly because they do not contain numbers that are required by the user. However, for the example query given above, it is very difficult for traditional word-based approaches to separate the two non-relevant sentences (sentence 3 and sentence 4) from the two relevant sentences (sentence 1 and sentence 2). Even worse, the two non-relevant sentences are very likely to be identified as novel sentences simply because they contain many new words that do not appear in previous sentences.
To solve this problem, a deeper query understanding beyond matching key words and a more precise nov-elty understanding beyond identifying new words is required. This motivated our work on information-pat-tern-based novelty detection. We have investigated if identifying query-related named entity (NEs) patterns in sentences will significantly improve the performance in novelty detection, particularly at top ranks ( Li &amp;
Croft, 2005 ). The basic concept of our NE-pattern-based approach is that each query could be treated as mul-tiple specific NE-questions. Each question is represented by a few query words, and it requires a certain type of named entities as answers.

However, queries (or topics) that can be transformed into specific NE-questions are only a small portion of the possible queries. For example, in TREC 2003, there are only 15 (out of 50) topics that can be formulated into specific NE-questions. The rest of the topics will be called general topics throughout the paper, since they can only be formulated into general questions. New and effective information patterns are needed in order to significantly improve the performance of novelty detection for those general topics. As one of the most inter-esting sentence level information patterns, we have found that the detection of information patterns related to 2006 ). As an example, Topic N1, from the TREC novelty track 2003, is about  X  X  X artial birth abortion ban X  X .
This is a query that cannot be easily converted into any specific NE-questions. However, we know that the user is trying to find opinions about the proposed ban on partial birth abortions. Therefore, relevant sentences are more likely to be  X  X  X pinion sentences X  X . Let us consider the following two sentences.
 Example 2. Topic:  X  X  X artial Birth Abortion Ban X  X   X  Number  X  :N1  X  Title  X  : partial birth abortion ban  X  Toptype  X  : opinion  X  Description  X  : Find opinions about the proposed ban on partial birth abortions.

 X  Narrative  X  : Relevant information includes opinions on partial birth abortion and whether or not it should be legal. Opinions that also cover abortion in general are relevant. Opinions on the implications of pro-posed bans on partial birth abortions and the positions of the courts are also relevant.

Sentence 1 (Relevant and Novel) :  X  X  X he court X  X  ruling confirms that the entire campaign to ban  X  partial birth abortion  X   X  a campaign that has consumed Congress and the federal courts for over three years  X  is nothing but a fraud designed to rob American women of their right to abortion,  X  X  said Janet Benshoof, president of Center for Reproductive Law and Policy.

Sentence 2 (Non-relevant) : Since the Senate X  X  last partial birth vote, there have been 11 court decisions on the legal merits of partial birth bans passed by different states.

Both sentence 1 and sentence 2 have five matched terms (in italics ). But only sentence 1 is relevant to the topic. Note that in addition to the matched terms, sentence 1 also has opinion patterns, indicated by the word  X  X  X aid X  X  and a pair of quotation marks. The topic is an opinion topic that requires relevant sentences to be opin-ion sentences. The first sentence is relevant to the query because it is an opinion sentence and therefore top-word-based approaches to separate the non-relevant sentence (sentence 2) from the relevant sentence (sentence 1). This work tries to attack this hard problem.

For a useful novelty detection system, a unified framework of the pattern-based approach is also required to deal with both the specific and the general topics. We propose a unified information-pattern-based approach to novelty detention (ip-BAND), which includes the following three steps: query analysis, relevant sentence retrieval and new pattern detection.

In summary, this work makes the following three contributions: (1) We provide a new and more explicit definition of novelty. Novelty is defined as new answers to the poten-(2) We propose a new concept in novelty detection  X  query-related information patterns . Very effective infor-(3) We propose a unified pattern-based approach that includes the following three steps: query analysis, rel-1.3. Organization of the paper The rest of the paper is organized as follows. Section 2 gives a review of related work on novelty detection. Section 3 describes data collections from TREC novelty tracks (2002 X 2004). Evaluation measures used in
TREC and the ones we are using are also discussed in this section. Section 4 introduces our new definition of  X  X  X ovelty X  X , and the concept of the proposed information patterns for novelty detection of both specific and general topics. Section 5 gives a thorough analysis of sentence level information patterns including sen-tence lengths, named entities, and opinion patterns. The analysis is performed on data from the TREC 2002 and/or 2003 novelty tracks, which provides guidelines in applying those patterns in novelty detection.
Section 6 describes the proposed unified pattern-based approach to novelty detection for both general and spe-cific topics. Different treatments for specific topics and general topics will be highlighted. Section 7 shows experimental results in using the proposed information patterns for significantly improving the performance of novelty detection for specific topics, general topics, and the overall performance of novelty detection for all topics using the unified approach. Section 8 summarizes the work and indicates some future research directions. 2. Related work
In the following, we give a review of research related to novelty detection at the two different levels, the event level and the sentence level, as well in other applications. Then, we point out the main differences between the pattern-based approach proposed in this paper and other approaches in the literature. 2.1. Novelty detection at the event level
Work on novelty detection at the event level arises from the Topic Detection and Tracking (TDT) research, which is concerned with online new event detection and/or first story detection ( Allan, Lavrenko, &amp; Ward, 2001; Kumaran &amp; Allan, 2004; Stokes &amp; Carthy, 2001; Yang, Pierce, &amp; Carbonell, 1998; Yang,
Zhang, Carbonell, &amp; Jin, 2002 ). Current techniques for new event detection are usually based on clustering algorithms, which include two important issues: event modeling and event clustering. Several models , such as vector space models, language models, lexical chains, etc., have been proposed to represent incoming new stories/documents. Each story is then grouped into clusters . An incoming story will either be grouped into the closest cluster if the similarity score between them is above a preset similarity threshold or it will be used to start a new cluster. A story which starts a new cluster will be marked as the first story about a new topic, or it will be marked as  X  X  X ld X  X  (about an old event) if there exists a novelty threshold (which is smaller than the similarity threshold) and the similarity score between the story and its closest cluster is greater than the novelty threshold.

Temporal proximity has also been considered, e.g., in Yang et al. (1998) and Franz et al. (2001) , based on the observation that news stories discussing the same event tend to be temporally proximate. Yang et al. (2002) proposed a two-level scheme first story detection system. The limitation of this technique is that it needs to predefine the taxonomy of broad topics and to have training data for each broad topic, which makes it inapplicable in the TDT context. Kumaran and Allan (2004) proposed a multi-stage new event detection (NED) system. Stories were classified into categories and NED was performed within categories. Stokes and Carthy (2001) proposed a composite document representation, which combined a free text vector and a lexical chain created using WordNet, and achieved a marginal increase in effectiveness. 2.2. Novelty detection at the sentence level
Research on novelty detection at the sentence level is related to the TREC novelty tracks (which will be described in more detail in the next section). The goal is to find relevant and novel sentences, given a query and an ordered list of relevant documents. Many research groups participated in the three TREC novelty tracks
Hirao, Isozaki, &amp; Maeda, 2003; Kwok, Deng, Dinstl, &amp; Chan, 2002; Litkowski, 2003; Qi, Otterbacher, Winkel, Zhang, Song et al., 2002 ).

Novelty detection at the sentence level can usually be conducted in two steps: relevant sentence retrieval and novel sentence extraction. In current techniques developed for novelty detection at the sentence level, new words appearing in  X  X  X elevant X  X  sentences usually contribute to the scores that are used to rank sentences for novelty. Many similarity functions used in information retrieval are also used in novelty detection. Usually a high similarity score between a sentence and a given query will increase the relevance rank of the sentence, whereas a high similarity score between the sentence and all previously seen sentences will decrease the novelty ranking of the sentence.

The simplest novelty measure, New Word Count Measure ( Allan et al., 2003 ), simply counts the number of new words appearing in a sentence and takes it as the novelty score for the sentence. There are other similar novelty or redundancy measures that consider new words appearing in sentences. One such novelty measure, called New Information Degree (NID), was defined by Jin et al. (2003) . Zhang et al. (2002) used an overlap-based redundancy measure in their work on novelty detection. The redundancy score of a sentence against another sentence is the number of matching terms normalized by the length of the sentence. They set the redundancy threshold to 0.55. Sentences with redundancy scores higher than the threshold were treated as redundant sentences and thus eliminated. Instead of counting new words appearing in sentences, Litkowski (2003) checked all discourse entities in a sentence. Discourse entities are semantic objects and they can have multiple syntactic realizations within a text. A pronoun, a noun phrase and a person X  X  name could all belong to one single entity and they would be treated as the same discourse entity. The sentence was accepted as novel if it had new discourse entities that were not found in the growing history list of discourse entities. Eichmann and Srinivasan (2002) considered the number of new named entities and noun phrases appearing in a sentence.
A sentence was declared novel if the number of new named entities and noun phrases is above a pre-declared number.

Many similarity functions (such as cosine similarity, language model measures etc.) used in information retrieval have also been tried in novelty detection. Tsai et al. (2003) represented sentences as vectors and com-puted similarity scores for a sentence currently being considered with each previous sentence with the cosine similarity function for detecting novel sentences. The Maximal Marginal Relevance model (MMR), intro-duced by Carbonell and Goldstein (1998) , was used by Sun et al. (2003) in their work on novelty detection.
Most novelty detection methods proposed in the past are highly sensitive to the quality of the initial sen-tence retrieval phase. Therefore, improving sentence retrieval performance is one of the major focuses of this paper. 2.3. Novelty detection in other applications
Novelty detection can be also seen in other applications, such as temporal summaries of news topics, doc-ument filtering and minimal document set retrieval, where new information detection is an important compo-nent. Those applications could include novelty detection at the event level, novelty detection at the sentence level, or a combination of both. In the work by Allan, Gupta, and Khandelwal (2001) on temporal summaries of new topics, the task is to extract a single sentence from each event within a news topic, where the stories are presented one at a time and sentences from a story must be ranked before the next story can be considered. In the work by Zhang, Callan et al. (2002) on novelty and redundancy detection in adaptive filtering, the goal is to eliminate redundant documents. A redundant document is defined as the one that is relevant to the user X  X  profile, but only contains information that is covered by previous documents. Other research that performs novelty detection includes subtopic retrieval ( Zhai, Cohen, &amp; Lafferty, 2003 ). The goal of subtopic retrieval is to find documents that cover as many different subtopics as possible. In the recent work by Dai and Srihari (2005) on minimal document set retrieval, the authors tried three different approaches to retrieve and rank document sets with maximum coverage but minimal redundancy of subtopics in each set: a novelty-based algorithm, a cluster-based generation algorithm, and a subtopic extraction based algorithm. 2.4. Comparison of our approach and others
There are three main differences between our pattern-based approach and the aforementioned approaches in the literature. First, none of the work described above gives an explicit definition of novelty, which we believe is essential in novelty detection. Usually new words, phrases or named entities appearing in incoming sentences or documents contribute to the novelty score, though in different ways. Our pattern-based approach is based on the new definition of novelty introduced in this paper, which treats new information as new answers to questions that represented users X  information requests.

Second, in the systems that are related to the TREC novelty tracks, either the title query or all the three sections of a query were used merely as a bag of words, whereas we try to understand a query beyond bag-of-words by transforming it into multiple specific NE-questions or a general question. We believe that a user X  X  information request can be better captured with questions than a few keywords, and a deeper understanding of the user X  X  request helps a novelty detection system to serve the user better.

Third, the proposed information-pattern-based approach is inspired by question answering techniques and is similar to passage retrieval for factoid questions. In our approach, each query could be treated as multiple questions; each question is represented by a few query words, and it requires a certain type of named entities as answers. Answer sentences are defined as sentences with answers to the multiple questions representing a user X  X  information request. Novel sentences are sentences with new answers to those questions. Instead of explicitly extracting exact answers as in factoid question answering systems, we propose to first retrieve answer sentences with certain information patterns that include both query words and required answer types, indicat-ing the presence of potential answers to the questions, and then identify novel sentences that are more likely to have new answers to the questions. 3. TREC novelty track data and evaluation 3.1. Data collections
Currently, there are three data collections from the TREC 2002, 2003 and 2004 novelty tracks officially available for experiments on novelty detection at the sentence level. Table 3.1 summarizes the statistics of the three data collections. There are three main differences among the three sets. (1) The TREC 2003 and 2004 novelty track collections exhibited greater redundancy than the TREC 2002 (2) TREC 2003 and TREC 2004 topics were classified into event topics and opinion topics. This allowed (3) TREC 2002 and 2004 novelty track collections exhibited greater difficulty in terms of relevant sentence
All three data sets are used in the experiments of this paper. The datasets from TREC 2002 and/or 2003 are used for training our system and the TREC 2004 dataset is set aside for testing. 3.2. Evaluation issues
The TREC novelty tracks used three evaluation measures for a novelty detection system: set precision, set recall and the F measure. One problem with set recall and set precision is that they do not average well when the sizes of assessors X  sets vary widely across topics. Harman (2002) illustrated the problem with an example, where a system did precisely the wrong thing but got an average score of 0.5 for both recall and precision. The average of the F measure is more meaningful than the average of set precision or recall when the sizes of the judgment sets vary widely. However, systems with an equal value of the F measures can have a range of pre-cision and recall scores. Hence, the F measure is not very precise in characterizing users X  real requirements, such as a quick search on the Web for obtaining useful information within the top ranked results.
Usually, the number of relevant (or novel) sentences retrieved by the system is determined by their ranking scores. Those sentences whose ranking scores are larger than a threshold are returned. This indicates that the numbers of sentences for different queries may vary dramatically, and the difference between numbers of sen-tences for two topics can be more than a hundred. However, in real applications, a user may only care about how much information within a limited number of sentences, say 10 or 20, retrieved by a system. For instance, many users who search information on the Web only look at top 10 results in the first page returned from a search engine. Few users would read the results beyond the first two pages.

Similarly, the pattern-based approach we propose is a precision-oriented approach designed to meet the aforementioned requirements from users. In improving the precision of relevance and novelty detection, some relevant and novel sentences are filtered out. For example, in dealing with topics that can be turned into multi-evant; other relevant sentences that are not covered by these specific questions are filtered out. In this sense, the task of our pattern-based approach, which is more useful in practice, is different from the tasks defined in the TREC novelty tracks.

Since we apply very strict rules in the selection process, ideally, those sentences that are retrieved by our system are truly relevant sentences or novel sentences, and therefore the precision of the relevant and novel sentence detection should be high, particularly within top ranked results. However, the recall might be low for the same reason. Hence, it is not very meaningful to use set precision, set recall or the F measure to com-pare our approach with the existing methods. Instead, in this paper, we use the precision of relevant or novel sentences at top ranks. We believe that precision values at top ranks (top 5, 10, 15, 20 or 30 sentences) are more useful in real applications where users want to quickly get some required information without going through a lot of non-relevant as well as redundant information. Nevertheless, we thoroughly compare the per-formance of our system with several well-designed baselines, which have been commonly used in novelty detection, using the same evaluation measures. 4. Definitions of novelty and information patterns 4.1. Our novelty definition The definition of novelty or  X  X  X ew X  X  information is crucial for the performance of a novelty detection system.
However, as we have pointed out, novelty is usually not clearly defined in the literature. Therefore, we give our definition of novelty as follows:
This novelty definition is a general one that works for novelty detection with any query that can be turned into questions. In this paper, we show that any query (topic) in the TREC novelty tracks can be turned into either one or more specific NE-questions, or a general question. Specific topics (corresponding to specific NE-questions) are those topics whose answers are specific named entities (NEs), including per-sons, locations, dates, time, numbers, etc. ( Bikel, Schwartz, &amp; Weischedel, 1999 ). General topics (corre-sponding to general questions), on the other hand, require obtaining additional information patterns for effective novelty detection. Other types of questions may be further explored within the framework of this novelty detection. 4.2. Information patterns 4.2.1. Information patterns for specific topics
The identification and extraction of information patterns is crucial in our approach. The information pat-tern corresponding to a specific NE-question (generated from a specific topic/query) is represented by both the query words (of the potential question) and an NE answer type (which requires named entities as its potential answer). They are called NE words patterns , related to the questions about DATE ( X  X  X hen X  X ), LOCATION ( X  X  X here X  X ), PERSON ( X  X  X ho X  X ), ORGANIZATION ( X  X  X hat/who X  X ) and NUMBER ( X  X  X ow many X  X ).

For each type of the five NE-questions, a number of word patterns were constructed for question type iden-tification. Typical NE patterns are listed in Table 4.1 ; some were extracted from the TREC 2002 novelty track queries manually and some were selected from a question answering system ( Li, 2003 ). Each NE word pattern is a combination of both query words (of potential questions) and answer types (which requires named entities as potential answers). 4.2.2. Information patterns for general topics
For a general topic, it is very difficult (if not impossible) to identify a particular type of named entity as its answer. Any type of named entity or even single words or phrases could be part of an answer as long as the answer context is related to the question. Further, in many relevant and novel sentences, no named entities are included. This observation is supported by the data analysis in Section 5 . Simply using named entities seems not very helpful for improving the performance for general topics, even though the performance of novelty fore, the challenging questions are how to effectively make use of these named entities, and what kinds of addi-tional and critical information patterns will be effective for general topics.

After analyzing the TREC data, we have found that the following three kinds of information patterns are very effective for this purpose: sentence lengths, named entity combinations and opinion patterns . We have also in the following, we introduce a particularly effective type of information patterns  X  opinion patterns. 4.2.3. Opinion patterns and opinion sentences
We note that the topics in TREC 2003 and 2004 novelty tracks are either classified as event topics or opin-ion topics. According to Soboroff (2004) ,  X  X  X dentifying sentences that contain an opinion remained a hard problem. Event topics proved to be easier than opinion topics. X  X  As a specific interesting finding, we have found that a large portion of the general questions are about opinions. Opinions can typically be identified by looking at such sentence patterns as  X  X  X XX said X  X ,  X  X  X YY reported X  X , or as marked by a pair of quotation marks. Currently, we have identified about 20 such opinion-related sentence patterns. The full list of opinion patterns is described in Section 5 . These patterns are extracted manually from a training set that is composed of about 100 documents from the TREC 2003 novelty track. The opinion patterns currently used in our sys-tem are individual words or a sequence of words, such as said, say, claim, agree, found that, state that, etc.
Note that the terms remain in their original verbal forms without word stemming, in order to more precisely capture the real opinion sentences. For example, a word  X  X  X tate X  X  does not necessarily indicate an opinion pat-tern, but the word combination  X  X  X tated that X  X  most probably does. If a sentence includes one or more opinion patterns, it is said to be an opinion sentence .

Note that the above information patterns could also be usable to other collections. However, natural lan-guage processing (NLP) is a complicated language understanding problem, and our NLP-like approach only provides the first step to extracting suitable information patterns for novelty detection. Further work is needed to automatically learn and to extract suitable information patterns for adapting to different data collections. 5. Information pattern analysis
Novelty detection includes two consecutive steps: first retrieving relevant sentences and then detecting novel tences, novel sentences and non-relevant sentences. The three information patterns studied here are: sentence lengths, named entities, and opinion patterns. The goal is to discover effective ways to use these information patterns in distinguishing relevant sentences from non-relevant ones, and novel sentences from non-novel ones. 5.1. Statistics on sentence lengths
The first type of pattern we have studied is the length of a sentence. This is probably the simplest feature of a sentence. The hypothesis is that it may include some information about the importance of the sentence. The statistics of sentence lengths in TREC 2002 and 2003 datasets are shown in Table 5.1 . The length of a sentence is measured in the number of words after stop words are removed from the sentence. Interestingly, we have found that the average lengths of relevant sentences from the 2002 data and the 2003 data are 15.58 and 13.1, respectively, whereas the average lengths of non-relevant sentences from the 2002 data and the 2003 data are only 9.55 and 8.5, respectively.
 To conclude, here is our first observation on sentence length (SL) patterns: SL Observation #1. Relevant sentences on average have more words than non-relevant sentences.

The sentence length feature is quite simple, but very effective since the length differences between non-rel-evant and relevant sentences are significant. This feature is ignored in other approaches, mainly because, in the past, sentence retrieval was performed with information retrieval techniques developed for document retrieval, where document lengths were usually used as a penalty factor. A long document may discuss multiple topics and a short document may focus on one topic. Therefore, in document retrieval, a short document is usually ranked higher than a long document if the two documents have same occurrences of query words. But at the sentence level, it turns out that relevant sentences have more words than non-relevant sentence on average.
Therefore, this observation will be incorporated into the retrieval step to improve the performance of relevant sentence retrieval, which will then boost the performance for identifying novel sentences. Further, we have a second observation on sentence length patterns from the statistics:
SL Observation #2: The difference in sentence lengths between novel and non-relevant sentences is slightly lar-ger than the difference in sentence lengths between relevant sentences and non-relevant sentences.
This indicates that the incorporation of the sentence length information in relevance ranking will put the novel sentences at higher ranks in the relevance retrieval step, which increases their chance to be selected as novel sentences. 5.2. Statistics on opinion patterns Topics in TREC 2003 and 2004 novelty track data collections are classified into event and opinion topics.
There are 22 opinion topics out of the 50 topics from the 2003 novelty track. The number is 25 out of 50 for the 2004 novelty track. There are no classifications of opinion and event topics in the 2002 novelty track. We designate a sentence to be an opinion sentence if it has one or more opinion patterns. The list of opinion pat-terns we here identified is shown in Table 5.2 . The list is by no means complete; however we want to show that by making use of this piece of information, the performance of novelty detection can be improved. Intuitively, for an opinion topic, opinion sentences are more likely to be relevant sentences than non-opinion sentences. This hypothesis is tested with a data analysis on the 22 opinion topics from the 2003 novelty track.
We have run our statistical analysis of opinion patterns on the 22 opinion topics from the 2003 novelty track in order to obtain guidelines for using opinion patterns for both 2003 and 2004 data. According to the results shown in Table 5.3 , 48.1% of relevant sentences and 48.6% of the novel sentences are opinion sen-tences, but only 28.4% of non-relevant sentences are opinion sentences. This could have a significant impact on separating relevant and novel sentences from non-relevant sentences for opinion topics, therefore we summa-rize these statistical results as Opinion Pattern (OP) Observations #1 and #2:
OP Observation #1: There are relatively more opinion sentences in relevant (and novel) sentences than in non-relevant sentences.

OP Observation #2: The difference of numbers of opinion sentences in novel and non-relevant sentences is slightly larger than that in relevant and non-relevant sentences.

The number of opinion sentences in the statistics only counts those sentences that have one or more opinion patterns shown in Table 5.2 . We would like to note here that a related work ( Kim, Ravichandran, &amp; Hovy, 2004 ) has been done very recently in classifying words into opinion-bearing words and non-opinion-bearing words, using information from several major sources such as WordNet, World Street Journal, and the General
Inquirer Dictionary. Using opinion-bearing words may cover more opinion sentences, but the accuracy of classifying opinion words is still an issue. We believe that a more accurate classification of opinion sentences based on the integration of the results of that work into our framework could further enlarge the difference in numbers of opinion sentences between relevant sentences and non-relevant sentences. The second observation indicates that novel sentences may benefit more from the use of opinion patterns.

The division of opinion and event topics is given by the TREC task. In realistic situations, this information would not be available. However, it is possible to automatically classify the queries into these categories, par-ticularly when a query with more information than merely a few keywords is provided by a user. 5.3. Statistics on named entities
As we have pointed out, answers and new answers to specific NE-questions are named entities. And for many of the general topics (questions), named entities are also major parts of their answers. Therefore, under-standing the distribution of named entity patterns could be very helpful both in finding relevant sentences and in detecting novel sentences.
 The statistics of all the 21 named entities that can be identified by our system are listed in Table 5.4 on
TREC 2002 and TREC 2003 novelty track data collections. These named entities are also grouped into four categories: Name, Time, Number and Object. In the table, each item lists two measurements: N P ( X ). The former is the number of Y -type ( Y is either R for relevant or NR for non-relevant) sentences, each of which has at least one of X -type named entity ( X is one of the 21 NE types). The latter is the percentage of those sentences among all the relevant (or non-relevant) sentences in each collection. These two measurements are calculated as In Eq. (5.2) , M Y denotes the total number of Y -type (either R for relevant or NR for non-relevant) sentences.
Those numbers are listed in the head of the table. From these statistics, we have found that the five most fre-quent types of NEs are PERSON, ORGANIZATION, LOCATION, DATE and NUMBER. For each of them, more than 25% relevant sentences have at least one named entity of the type in consideration. Note that all the three NE types (PERSON, ORGANIZATION, LOCATION) in the Name category are among the five most frequent types. In the Time category, DATE type is much more significant than TIME and PERIOD types, probably because the significance of a piece of news is characterized by the scale of a day, a month or a year, rather than a specific time of a day, or a period of time. In the Number category, the general Num-ber type (NUMBER) of named entities is overwhelmingly more significant than all the other specific Number types (such as MONEY, POWER, LENGTH, etc.).

Among these five types of NEs, three (PERSON, LOCATION and DATE) of them are more important than the other two (NUMBER and ORGANIZATION) for separating relevant sentences from non-relevant sentences. For example, for TREC 2003, the percentage of relevant sentences that include PERSON type name entities, P R (PERSON), is about 13% more than that of the non-relevant sentences, P The discrimination capability of the ORGANIZATION type in relevance detection is not as significant; P (ORGANIZATION) is just marginally higher than P NR (ORGANIZATION). The insignificant role of ORGANIZATION in relevant retrieval has also been validated by our experiments on real data. One of the reasons is that an NE of this type often indicates the name of a news agency; and names of news agencies often occur frequently in news articles. The role of the NUMBER type is not consistent between the two TREC datasets (2002 and 2003). In Table 5.4 , P R (NUMBER) is higher than P 2002, but is lower for TREC 2003. Therefore only the three effective types will be incorporated into the sen-tence retrieval step to improve the performance of relevance. This leads to our first observation on named enti-ties (NEs):
NE Observation #1. Named entities of the PLD types  X  PERSON, LOCATION and DATE are more effective in separating relevant sentences from non-relevant sentences.

However, the ORGANIZATION type will be also used in the new pattern detection step since a different organization may provide new information. Here is an example Example 3. Organization in new pattern detection  X  docid =  X  X  X YT19980629.0465 X  X  num =  X  X 42 X  X   X   X  ENAMEX TYPE =  X  X  X RGANIZATION X  X   X  Christian Coalition of Florida  X  /ENAMEX  X  director  X  ENA-MEX TYPE =  X  X  X ERSON X  X   X  John Dowless  X  /ENAMEX  X  disagrees with  X  ENAMEX TYPE =  X  X  X ER-
SON X  X   X  Carres  X  /ENAMEX  X   X  spin but agrees that a judge or judges somewhere, state or federal, is liable to strike down the law.

In this example, the organization name  X  X  X hristian Coalition of Florida X  X  gives an indication that new infor-mation may be provided by this sentence. We summarize this into NE Observation #2: NE Observation #2: Named entities of the POLD types  X  PERSON, ORGANIZATION, LOCATION, and
DATE will be used in new pattern detection; named entities of the ORGANIZATION type may provide dif-ferent sources of new information.

Table 5.4 also lists the statistics of those sentences with no NEs at all, with no POLD (Person, Organiza-tion, Location or Date) NEs, and with no PLD (Person, Location or Date) NEs. These data show the follow-ing facts: (1) There are obvious larger differences between relevant and non-relevant sentences without PLD
NEs, than the differences without POLD NEs, or without any NEs. This confirms that PLD NEs are more effective in re-ranking the relevance score (NE Observation #1). (2) There are quite large percentages of rel-evant sentences without NEs or without POLD NEs. Therefore, first, the absence of NEs is not going to be used exclusively to remove sentences from the relevant sentence list. Second, the number of the previously unseen POLD NEs only contributes partly to novelty ranking. 5.4. Named entities in event and opinion topics
As we have discussed, topics in TREC 2003 and TREC 2004 novelty track data collections are classified into two types: opinion topics and event topics. If a topic can be transformed into multiple NE-questions, no matter it is an opinion or event topic, the relevant and novel sentences for this  X  X  X pecific X  X  topic can be extracted by mostly examining required named entities (NEs) as answers to these questions generated from the topic. Otherwise, we can only treat it as a general topic for which no specific NEs can be used to identify sentences as answers. The analysis in Section 5.2 shows that we can use opinion patterns to identify opinion sentences that are more probably relevant to opinion topics (queries) than non-opinion sentences. However, opinion topics only consist of part of the queries ( Table 5.5 ). There are only 22 opinion topics out of the 50 topics from the 2003 novelty track. The number is 25 out of 50 for the 2004 novelty track. Now the question is: how to deal with those event topics?
Since an event topic (query) is usually about an event with persons, locations and dates involved, naturally we turn to named entities about the types of PERSON, LOCATION and DATES for help. Statistics also ver-ify this hypothesis. Table 5.6 compares the difference in the statistics of NEs between event topics and opinion topics for the TREC 2003 novelty track. In Table 5.6 , each item lists two measurements: N P
Y -Z ( X ). The former is the number of Y -type ( Y is either R for relevant or NR for non-relevant) sentences for Z -category topics ( Z is either O for opinion, or E for event), each of which has at least one of X -type named entity ( X is PERSON, LOCATION or DATE). The latter is the percentage of those sentences among all the relevant (or non-relevant) sentences in each category (event or opinion). These two measurements are calculated as tences in the Z category ( Z is either E for event or O for opinion). Those numbers are listed in Table 5.5 . From
Table 5.6 , we can see that for PERSON, LOCATION and DATE types, the relevant to non-relevant percent-about named entities and opinion/event topics:
NE/OP Observation #3: Named Entities of the PERSON, LOCATION and DATE types play a more impor-tant role in event topics than in opinion topics.

This is further verified in our experiments of relevance retrieval. In the equation for NE-adjustment (Eq. (6.5) ), the best results are achieved when a takes the value of 0.5 for event topics and 0.4 for opinion topics.
Note that while opinion patterns in sentences are used for improving the performance of novelty detection for opinion topics, named entities play a more important role for event topics. 6. Pattern-based approach to novelty detection The unified i nformation-p attern-B ased A pproach to N ovelty D etection (ip-BAND) is illustrated in Fig. 1 .
There are three important steps in the proposed approach: query analysis, relevant sentence retrieval and novel sentence extraction. In the first step, an information request from a user will be implicitly transformed into one or more potential questions in order to determine corresponding query-related information patterns. Informa-tion patterns are represented by combinations of query words and required answer types to the query. In the second step, sentences with the query-related patterns are retrieved as answer sentences. Then in the third step, sentences that indicate potential new answers to the questions are identified as novel sentences. We will detail the three steps in Sections 6.1 X 6.3 .
 6.1. Query analysis and question formulation 6.1.1. Query analysis In the first step, a question formulation algorithm first tries to automatically formulate multiple specific
NE-questions for a query, if possible. Each potential question is represented by a query-related pattern, which is a combination of a few query words and the expected answer type. A specific question would require a par-ticular type of named entities (NEs) for answers. Five types of specific questions are considered in the current system: PERSON, ORGANIZATION, LOCATION, NUMBER and DATE .

If this is not successful, a general question will be generated. A g eneral question does not require a particular type of named entity for an answer. Any type of named entities (NEs) as listed in Table 5.4 as well as other single words could be a potential answer or part of an answer, as long as the answer context is related to the question. This means answers for general questions could also be in sentences without any named entities (NEs). However, from our data analysis, the NEs of POLD types ( PERSON, ORGANIZATION, LOCA-TION, DATE ) are the most effective in detecting novel sentences, and three of them ( PERSON, LOCATION,
DATE ) are the most significant in separating relevant sentences from non-relevant sentences. In addition, as we have observed in the statistics in Section 5 , sentence lengths and opinion patterns are also important in relevant sentence retrieval and novel sentence extraction. In particular, we can use opinion patterns to identify opinion sentences that are more probably relevant to opinion topics (queries) than non-opinion sentences.
PERSON, LOCATION and DATE play a more important role in event topics than in opinion topics. There-fore, for a general question, its information pattern includes topic type (event or opinion), sentence length,
POLD NE types, as well as query words. 6.1.2. Question formulation
There are 49 queries in the TREC 2002 novelty track, 50 queries in the TREC 2003 novelty track and 50 queries in the TREC 2004 novelty track. Our question formulation algorithm formulates multiple specific questions for 8 queries from the TREC 2002 novelty track, for 15 queries from the TREC 2003 novelty track, and for 11 queries from the TREC 2004 novelty track, respectively. The remaining queries were transformed into general questions.

Each query from the TREC novelty tracks has three fields: title, description and narrative. Even though not explicitly provided in the format of questions, a significant number of the queries can be transformed into mul-tiple specific questions. As examples, query 306 from TREC 2002 novelty track and query N37 from TREC 2003 are listed here as Examples 4 and 5 .
 Example 4. Question formulation  X  specific questions  X  Number  X  : 306  X  Title  X  African Civilian Deaths  X  Description  X  : How many civilian non-combatants have been killed in the various civil wars in Africa?
 X  Narrative  X  : A relevant document will contain specific casualty information for a given area , country ,or region . It will cite numbers of civilian deaths caused directly or indirectly by armed conflict. Example 5. Question formulation  X  a general question  X  Number  X  : N37  X  Title  X  Olympic scandal Salt Lake City  X  Topic Type  X  event  X  Description  X  : The Salt Lake City Olympic bribery scandal: What were the results?
 X  Narrative  X  : Any mention of effects of the bribery scandal was relevant. Mention of things that should not have been affected, such as, souvenir sales or attendance are relevant. Mention of members being involved in the scandal was relevant. Any mention of kinds of bribery used was relevant. Effects on potential donors or sponsors of the SLC games or future games were relevant.

There are many approaches that can be used for question formulation and pattern determination. In our current implementation, we used a simple algorithm based on word-pattern matching to formulate questions and corresponding information patterns from queries. For each type of the five NE-questions, a number of word patterns, which could be unigram, bi-gram (or further n -gram) word sequences, have been constructed for question type identification. Some word patterns were extracted from the TREC 2002 novelty track queries manually and some patterns were selected from our question answering system ( Li, 2003 ). The patterns have been shown Table 4.1 (in Section 4 ), where, most of them are unigram, but some of them are bi-gram. For a given query, the algorithm will go through the text in both the description and the narrative fields to identify terms that matches some word patterns in the list. The query analysis component first tries to formulate at least two specific questions for each query, if possible, because a single specific question probably only covers a small part of a query, therefore it is translated into a general question.

For the topic in Example 4 , in the Description and the Narrative fields, three word patterns  X  X  X rea X  X ,  X  X  X oun-try X  X  and  X  X  X egion X  X  match the word patterns in the answer type Location, and two word patterns  X  X  X ow many X  X  and  X  X  X umbers X  X  matches the word patterns in the answer type Number. Therefore, two specific questions about  X  X  X here X  X  and  X  X  X ow many X  X  are formulated, hence the topic is identified as a specific topic.
If a query only has terms that match with patterns belonging to one type of question, or it does not have any matched terms at all, then a general question is generated for the query. For a general topic, the topic type (event or opinion), as part of the information pattern, is given in the TREC 2003 and 2004 novelty tracks. A number of expected opinion patterns have been identified manually and are listed in Table 5.2 (in Section 5 ).
For the topic in Example 5 , in the Narrative fields, a word pattern  X  X  X ember X  X  matches the one in the answer type Person. Because only one specific question about  X  X  X ho X  X  is formulated, the topic is classified as a general topic.

Question formulation is more promising for long and well-specified queries. However, this approach is also applicable to short queries with just a few key words. In the latter case, a query will most probably be trans-formed into a general question if sufficient information is not available from such a short query. 6.2. Using patterns in relevance re-ranking 6.2.1. Relevant sentence retrieval overview
The task of relevant sentence retrieval is to retrieve sentences that are likely to have potential answers to the questions transformed from a given topic. It first takes query words of the query, and searches in its data col-lection to retrieve sentences that are topically relevant to the query. Our relevant sentence retrieval module is implemented based on an existing search toolkit  X  X EMUR ( Lemur, 2006 ). The module first uses a  X  X  X FISF X  X  model adopted from TFIDF models in LEMUR for the first round of relevant sentence retrieval. Then it removes the sentences that do not contain any  X  X  X nswers X  X  to the potential question. For a specific question, only a specific type of named entity that the question expects would be considered for potential answers. Thus a sentence without an expected type of named entity will be removed from the list. A list of presumed answer sentences (which contain expected named entities to the question) is generated. For general questions (topics), all types of named entities as well as single words could be potential answers. Therefore, no filtering is per-formed after the first round of sentence retrieval using the TFISF model.

To improve the performance of finding relevant sentences and to increase the ranks for sentences with the required information patterns, sentence-level information patterns, including sentence lengths, Person-Loca-tion-Date NEs, and opinion patterns, are then incorporated in the relevance retrieval step to re-rank the retrieved sentences. The use of information patterns seems to have attracted attention for other applications.
Recent work by Kumaran and Allan (2006) tried a pattern-based approach for document retrieval. By asking users simple questions, frequent bi-grams within a window of eight terms and phrases in queries were identi-fied and used to improve pseudo relevance feedback results. 6.2.2. Ranking with TFISF models
TFIDF models are one of the typical techniques in document retrieval ( Salton &amp; McGill, 1983 ). TF stands for Term Frequency in a document and IDF stands for Inverse Document Frequency with respect to a doc-ument collection. We adopt TFIDF models for the relevant sentence retrieval step in our novelty detection task simply because it was also used in other systems and was reported to be able to achieve equivalent or better performance compared to other techniques in sentence retrieval ( Allan et al., 2003 ). The name of our sentence retrieval model is called the TFISF model, to indicate that inverse sentence frequency is used for sentence retrieval instead of inverse document frequency. The initial TFISF relevance ranking score S for a sentence, modified from the LEMUR toolkit ( Lemur, 2006; Zhai, 2001 ), is calculated according to the following formula: where n is the total number of terms, isf ( t i ) is inverse sentence frequency (instead of inverse document fre-quency in document retrieval), tf s ( t i ) is the frequency of term t of term t i in the query. The inverse sentence frequency is calculated as where N is the total number of sentences in the collection, N term t i . Note that in the above formulas, t i could be a term in the original query (with a weight w(t an expanded query that has more terms from pseudo feedback (with a weight w(t query with pseudo relevance feedback is represented by
With pseudo relevance feedback, the system assumes that top 100 sentences retrieved are relevant to the query and top 50 most frequent terms within the 100 sentences are added to the original query. We have tried various numbers of top ranked sentences and top most frequent terms and have found that 50 top words in the 100 top sentences give the best performance. As in other information retrieval systems, stopword removal (e.g., Zhai et al., 2003 ; Li, 2003 ) and word stemming ( Krovetz, 1993 ) are performed in a preprocessing step for relevant sentence retrieval.

The score S 0 will be served as the baseline for comparing the performance increase with the information patterns we have proposed in relevant sentence retrieval and novelty detection. We have tried both the TFISF model with original queries and the TFISF model with expanded queries by pseudo feedback. The TFISF model with pseudo feedback is used in the experiments reported in the rest of the paper because it provides better performance. 6.2.3. TFISF with information patterns
The TFISF score is adjusted using the following three information patterns: sentence lengths, named enti-ties, and opinion patterns. The length-adjustment is calculated as where L denotes the length of a sentence and L denotes the average sentence length. In the current implemen-tation, the average sentence length is set as the same for all topics in each collection of TREC 2002, 2003 and 2004 novelty tracks. This parameter is not critical as long as the comparison is only among sentences for the same topic. A sentence that is longer than the average will increase the TFISF score. This adjustment follows SL Observations #1 and #2.
 The NEs-adjustment is computed as where F person = 1 if a sentence has at least a person X  X  name, 0 otherwise; F location name, 0 otherwise; and F date = 1 if a sentence has at least a date, 0 otherwise. This adjustment follows
NE Observation #1. The adjustment is applied to both specific topics and general topics, but the parameter a is set slightly different. For specific topics or general but opinion topics, a = 0.4; for general, event topics, a = 0.5. This follows the NE/OP Observation #3.
 Finally, the opinion-adjustment is computed as where F opinion = 1 if a sentence is identified as an opinion sentence with one or more opinion patterns, 0 other-is an opinion sentence. This adjustment follows the OP Observations #1 and #2. This final adjustment step based on opinion patterns is only performed for general opinion topics. 6.2.4. Formula/parameter optimization
All three adjustments are applied to the training data based on TREC 2002 and/or TREC 2003 novelty track to find the best set of parameters, a and b , and the same set of parameters are used for all data sets.
Here is a brief summary of the formula/parameter optimization procedure; details can be found in Li (2006) . (1) First, we have tried different ways of adjusting scores and found that above adjustment mechanism (Eqs. (6.4) X (6.6) ) achieves the best performance.

In incorporating the information of sentences lengths into relevant sentence re-ranking, we have tried the following methods: + Length method (to  X  X  X dd X  X  length factor to the original score),  X  X  X ultiple X  X  a roof-clipped length factor  X  penalty is given to sentences shorter than average) and method (to  X  X  X ultiple X  X  a length factor). They are all compared with the baseline, in which the original TFISF ranking score S 0 is simply normalized by the maximum possible ranking score for all sentences in consider-ation. We carried out performance comparison with different sentence length adjustment methods to 49 topics from TREC 2002, and it turns out that * Length method outperforms the other two significantly. Therefore we use the * Length method (Eq. (6.4) ) in our ip-BAND approach.

The following six methods in incorporating NEs are compared with the baseline with normalized relevant ranking S 0 :+ ALL (to  X  X  X dd X  X  the occurrence of different types of all the NEs), + POLD (to  X  X  X dd X  X  occurrence of the POLD NEs), + PLD (to  X  X  X dd X  X  occurrence of the PLD NEs), different types of all the NEs), * POLD (to  X  X  X ultiple X  X  occurrence of the POLD NEs), occurrence of the PLD NEs). Based on the performance comparison with different named entities adjustment methods to 49 topics from TREC 2002, the * PLD method outperforms the all the other five methods signif-icantly. Note that we have not only compared different ways of applying named entities, i.e.,  X  X  X dding X  X  or  X  X  X ultiplying X  X , but also tried different combinations of named entities. It is clear that using only Person/Loca-tion/Data-type named entities gives the best performance, with the two different calculations. Between the two ways of calculation,  X  X  X ultiplying X  X  method is clearly better. Therefore we use the our ip-BAND approach.

In incorporating the information of opinion patterns into relevant sentence re-ranking, we have tried the following methods: + Opinion method (to  X  X  X dd X  X  a factor if opinion patterns exist) and  X  X  X ultiple X  X  a factor if opinion patterns exist). They are both compared with the baseline with original TFISF ranking score S 0 normalized by the maximum possible ranking score for all sentences in consideration. Based on the performance comparison with the two different opinion pattern adjustment methods to 23 topics from
TREC 2003, the precisions in relevant sentence retrieval by using the two methods are very close. And we have also found that different values of this parameter do not change the performance very much. Therefore either one of them could be chosen into our ip-BAND approach. However, to be consistent with the other two adjustments (the length adjustment and the NE-adjustment), we select the our current implementation. (2) After selecting the most appropriate formulas in the above studies, we applied all the three adjustments and tune the parameters for the best performance of relevant sentence retrieval based on the TREC 2003 dataset.
To simplify the computation, we apply the three adjustments sequentially. The sentence length adjustment is applied first using Eq. (6.4). No parameters need to be tuned for this adjustment. Then the NE-adjustment is applied with different values of the parameter a , ranging from 0.1 to 1.0, with an interval of 0.1, using Eq. (6.5) . It turns out that for general, opinion topics, the best performance is achieved when a = 0.5, and for gen-eral, event topics, or specific topics, a = 0.4 gives the best results.

Finally the opinion pattern adjustment is applied with different values of b , ranging from 0.1 to 1.0, with an interval of 0.1, using Eq. (6.6) . We have found that for specific topics, we could tune a parameter for the best performance of relevant sentence retrieval. However, the performance in the final novelty detection is better without the opinion pattern adjustment. Therefore, the opinion pattern adjustment is not used for specific topic. For general opinion topics, the best performance is achieved when b = 0.5.

Incorporating information patterns at the retrieval step should improve the performance of relevance and thus help in the following novelty extraction step. After applying the above three steps of adjustments on the original ranking scores, sentences with query-related information patterns are pulled up in the ranked list. For the two sentences in Example 2 in Section 1 , the relevant (and novel) sentence (sentence 1) was ranked 14th with the original TFISF ranking scores. It was pulled up to the 9th place in the ranked list after the adjust-ments with the information patterns. The non-relevant sentence (sentence 2) was initially ranked 2nd, but pushed down to the 81st place after the score adjustments. Complete comparison results of novelty detection performance on TREC 2002, 2003 and 2004 are provided in the experiments in the next section.

Even though the parameters are trained on TREC 2002 and/or 2003 data, we have also tested the generality of these parameters. We have found that the best parameters that trained from TREC 2002 and 2003 data are also the best for the TREC 2004 data. This is verified by applying all the different sets of parameters used in the training step to the TREC 2004 data. This indicates that training the parameters with TREC 2004 data will yield the same set of optimal parameters. 6.3. Novel sentence extraction
We start with our treatment for specific topics for easy explanation. For a specific topic with multiple spe-cific NE-questions that are formulated from the topic (query), an answer sentence may have answers to one or more of these specific NE-questions. So named entities related to any one of the specific questions in the answer sentences should be extracted, and are treated as answers. There is an answer pool associated with each topic, which is initially empty. As sentences come in, new answers (i.e., new and specific NEs) will be added to the answer pool when the novel sentence detection module determines that the incoming answers are previ-ously unseen. A sentence will be marked novel if it contains new answers. Sentences without new answers will be removed from the final list provided to the user.
 For a general topic, the novelty score of a sentence is calculated with the following formula: where S n is the overall novelty score of a sentence S, N do not appear in its previous sentences, N ne is the number of POLD-type named entities in S that do not ap-pear in its previous sentences, and x and c are weights for new words and new named entities, respectively.
Note that stopwords have been removed from all sentences in the relevant sentence step, and will not be con-sidered in the process of novelty score calculation. Then the words are stemmed. A sentence is identified as a novel sentence if its novelty score is equal to or greater than a preset threshold. In our experiments, for the general topics, the best performance of novelty detection is achieved when both x and c are set to 1, putting equal weights on the new words and new NEs, and the threshold T for S them in total counts for a novel sentence.

The novelty score formula given in Eq. (6.7) is actually a general one that can also be applied to specific topics. In that case, N ne is the number of the specific answer NEs, and we set x to 0 since only specific answer NEs are counted. The threshold for the novelty score S n is set to 1, indicating the appearance of one specific
NE makes the sentence novel. 7. Experimental results and analysis
In this section, we present and discuss the main experimental results. The data used in our experiments are from the TREC 2002, 2003 and 2004 novelty tracks. The comparison of our approach and several baseline approaches are described. The parameters used in both the baselines and our approaches were tuned with the TREC 2002 data, except the parameter b in Eq. (6.6) with the TREC 2003 data, which is used for sentence re-ranking for opinion topics in our approach. The experiments and analysis include the performance of novelty detection for specific topics, general topics, and the overall performance for all topics. Issues of base-line selection and evaluation measures are also discussed. 7.1. Baseline approaches
We compared our information-pattern-based approach to novelty detection (ip-BAND) to four baselines described in the following subsections. They are: B-NN: baseline with initial retrieval ranking (without novelty detection), B-NW: baseline with new word detection, B-NWT: baseline with new word detection with a thresh-old, and B-MMR: baseline with maximal marginal relevance (MMR). For comparison, in our experiments, the same retrieval system based on the TFISF techniques adopted from the LEMUR toolkit ( Lemur, 2006 ) is used to obtain the retrieval results of relevant sentences in both the baselines and our ip-BAND approach. The evaluation measure used for performance comparison is precision at rank N ( N = 5, 10, 15, 20 and 30 in
Tables 7.1 X 7.12 ). It shows the fraction of correct novel (or relevant) sentences in the top N sentences delivered to a user, which is defined as 7.1.1. B-NN: initial retrieval ranking
The first baseline does not perform any novelty detection but only uses the initial sentence ranking scores generated by the retrieval system directly as the novelty scores (without filtering and re-ranking). One purpose of using this baseline is to see how much novelty detection processes (including NE filtering, relevant sentence re-ranking and new answer detection) may help in removing redundancies.
 7.1.2. B-NW: new word detection
The second baseline in our comparison is simply applying new word detection. Starting from the initial retrieval ranking, it keeps sentences with at least one new word that does not appear in previous sentences as novel sentences, and removes those sentences without new words from the list. As a preprocessing step, all words in the collection were stemmed and stopwords were removed. 7.1.3. B-NWT: new word detection with a threshold
The third baseline (B-NWT) is similar to B-NW. The difference is that it counts the number of new words that do not appear in previous sentences. A sentence is identified as novel sentence if and only if the number of new words is equal to or greater than a preset threshold . The best value of the threshold is 4 in our experi-ments. This means that a sentence is treated as a novel sentence only if more than three new words are included in the sentence. The threshold is selected as the same as for our ip-BAND novel sentence determina-tion for general topics (Eq. (6.7) ). So, roughly speaking, this baseline is comparable to the ip-BAND applied to general topics. 7.1.4. B-MMR: Maximal Marginal Relevance (MMR)
Many approaches to novelty detection, such as maximal marginal relevance (MMR), new word count mea-sure, set difference measure, cosine distance measure, language model measures, etc., were reported in the lit-erature. The MMR approach was introduced by Carbonell and Goldstein (1998) , which was used for reducing redundancy while maintaining query relevance in document re-ranking and text summarization. In our exper-iments, the MMR baseline approach (B-MMR) starts with the same initial sentence ranking used in other baselines and our ip-BAND approach. In B-MMR, the first sentence is always novel and ranked top in nov-elty ranking. All other sentences are selected according their MMR scores. One sentence is selected and put into the ranking list of novelty sentences at a time. MMR scores are recalculated for all unselected sentences once a sentence is selected. The process stops until all sentences in the initial ranking list are selected. MMR is calculated by where S i and S j are the i th and j th sentences in the initial sentence ranking, respectively, Q represents the query, N is the set of sentences that have been currently selected by MMR, and R/N is the set of sentences have not yet selected. Sim 1 is the similarity metric between sentence and query used in sentence retrieval, and Sim 2 can be the same as Sim 1 or can be a different similarity metric between sentences.

We use MMR as a baseline because MMR was reported to work well in non-redundant text summarization topic retrieval ( Zhai et al., 2003 ). Also, MMR may incorporate various novelty measures by using different similarity matrix between sentences and/or choosing different value of k . For instance, if the cosine similarity metric is used for Sim 2 and k is set to 0, then MMR would become the cosine distance measure reported in
Allan et al. (2003) . 7.2. Experimental results and comparisons
Our ip-BAND approach also uses the same initial relevance ranking as all the baselines. The novelty detec-tion performance of our ip-BAND is a combined impact of information patterns in sentence filtering for spe-cific topics, relevant sentence re-ranking and new pattern detection. However, in order to see what is the impact of information patterns for the relevant sentence retrieval step and the novel sentence extraction step, we have also conducted experiments comparing relevant sentence retrieval. In the following subsection, we present the results of the comparisons of the performance of both relevance and novelty detection, and for both specific and general topics. Finally we present the overall performance for all the topics in the three nov-elty track data collections. From Tables 7.1 X 7.13 , Chg% denotes the percent change of the precision of our ip-BAND approach (or the 2nd, 3rd or 4th baseline) compared to the first baseline. The Wilcoxon test is used 7.2.1. Experimental Results on Novelty Detection
A. Specific topics. First, we tested the information-pattern-based approach to novelty detection (ip-BAND) on specific topics. Tables 7.1 X 7.3 show comparisons for the top 5, 10, 15, 20 and 30 sentences. The precision measurements in the tables are the averages of all the specific topics in each data collection at several different ranks. For example, the performance of our approach with specific questions beats the first baseline by more than 30% at rank 15, which are 35.0%, 35.7% and 31.6%, for TREC 2002, 2003 and 2004, respectively. In com-parison, the best results combining all the three baselines (No 2, No 3 and No 4) are only 10.0% (from N-
NWT), 32.1% (from N-NWT) and 10.5% (from both N-NW and N-NWT), respectively. In another words, within the top 15 sentences, our approach obtains more novel sentences than all four baselines. The precision values (see Tables 7.1 X 7.3 ) of our ip-BAND approach are 22.5%, 67.6% and 30.3%, respectively. These indi-cate that within the top 15 sentences, there are 3.4, 10.1 and 4.6 sentences on average that are correctly iden-tified as novel sentences by the ip-BAND approach (from TREC 2002, 2003 and 2004 respectively). The numbers are 1.5, 4.8 and 1.6 for the combinations of the best results of baselines 2 X 4.
 In comparison, New Word Detection with a Threshold (B-NWT) performs slightly better than New Word
Detection (B-NW), but Maximal Marginal Relevance (B-MMR) does not. On the surface, for specific topics, the new word detection baseline approach and our ip-BAND approach use the similar strategy. That is, once a new word (new specific NE) appears in a sentence, it is declared as a novel sentence. However, a new specific
NE in our ip-BAND approach may answer the right question, but a new word in B-NW will often not. There-fore B-NWT is better than B-NW in the sense that it needs more words (4 in our experiments) to declare a sentence to be novel. The performance of the baseline based on Maximal Marginal Relevance in fact is worse to the fact that MMR uses a combined score to measure both relevance and redundancy, which are somewhat in conflict. In reducing redundancy, it might also decrease the performance in relevant ranking, which is very important in novelty detection.
 We have the following conclusion on the experimental results for specific topics:
Conclusion #1. The proposed ip-BAND outperforms all baselines across the three data sets: the 2002, 2003 and 2004 novelty tracks, for specific topics.

B. General topics. Tables 7.4 X 7.6 show the performance comparison of our ip-BAND approach with the four baselines in novelty detection on those general topics that cannot be turned into specific NE-questions. The precision values for the top 15 sentences with our ip-BAND approach for general questions of the
TREC 2002, 2003 and 2004 data are 21.1%, 51.2% and 22.4%, respectively. This represents that on average, 3.2, 7.5 and 3.4 sentences are correctly identified as novel sentences, among the top 15 recalls. Again, the pre-cision is the highest for the 2003 data since this track has highest ratio of relevant to non-relevant sentences.
Compared to the first baseline, the performance increases are 9.2%, 23.4% and 14.9%, respectively and the improvements are significant. In contrast, the best improvements combing all the three baselines (2 X 4) are only 5.0% (from B-MMR), 9.6% (from B-NWT) and 5.4% (from B-NWT), respectively, which are much lower than the results of our ip-BAND approach. In comparison, New Word Detection with a Threshold achieves better performance than New Word Detection for general topics. This is within our expectation because New Word Detection is a special case of New Word Detection with a Threshold when the new word threshold is set to 1.
In addition, both B-NWT and ip-BAND use the same threshold of 4  X  X  X ew words X  X  to declare a relevant sen-tence to be novel, except that ip-BAND also add additional counts for POLD-type NEs. Further, Maximal
Marginal Relevance is slightly better than New Word Detection and New Word Detection with a Threshold on the 2002 data, but it is worse than New Word Detection with a Threshold on the 2003 and 2004 data. We can draw the following main conclusions from the results:
Conclusion #2. Our ip-BAND approach consistently outperforms all the baseline approaches across the three data sets: the 2002, 2003 and 2004 novelty tracks, for general topics.

Conclusion #3. In comparison, the performance of our ip-BAND approach is slighter better for the specific topics than the general topics.

The reason for Conclusion #3 is mainly due to the fact that specific, targeted questions and answers are extracted for specific topics. This also indicates that as a future work, more improvements can be expected if more general topics can be turned into NE or other specific questions.

C. All topics. The overall performance comparisons of the unified pattern-based approach with the four baselines on all topics from the TREC 2002, 2003 and 2004 novelty tracks are shown in Tables 7.7 X 7.9 , respec-tively. The most important conclusion is the following:
Conclusion #4. On average, the unified pattern-based approach outperforms all baselines at top ranks when all topics are considered.

Significant improvements are seen with the 2003 topics. In the top 15 sentences delivered, our approach retrieves 8.42 (= 15 * 0.5613) novel sentences on average, while the four baseline approaches only retrieve 6.60, 7.38, 7.74 and 7.36 novel sentences, respectively. As anticipated, the overall performance for all topics (including both specific and general ones  X  Tables 7.10 X 7.12 ) is slightly better than that for the general topics ones.

This comparison is also summarized in Table 7.10 at top 15 ranks (sentences). In the table, the following measures are listed for each case of specific, general and all topics, and for TREC 2002, 2003 and 2004: (1) the improvements over the first baseline in percentage (Chg%); (2) the number of correctly identified novel sen-tences; (3) the number of relevant but redundant sentences; and (4) the number of non-relevant sentences.
The last three numbers add up to 15. According to the results shown in this table, we have the following important observations that could guide further improvements in both relevant and novel sentence detection for different data collections. (1) There are 3.20 novel sentences, 0.25 redundant sentences and 11.55 non-relevant sentences within the top (2) There are 8.42 novel sentences, 2.78 redundant sentences and 3.8 non-relevant sentences within the top (3) There are 3.62 novel sentences, 3.74 redundant sentences and 7.64 non-relevant sentences within the top 7.2.2. Experimental analysis on relevant sentence retrieval
The second set of experiments is designed to investigate the performance gain of finding relevant sentences with the sentence re-ranking step. For the specific topics, the relevant sentence retrieval module re-ranks the sentences by the revised scores that incorporate the information of sentence lengths and PLD-type named enti-ties (i.e., Person-, Location-and Date-type NEs). In addition, sentences without the required named entities are also removed (i.e., filtered) before re-ranking.

We compare the performance of finding relevant sentences with and without filtering and re-ranking. The comparison results are given in Table 7.11 . From this Table, we can see that the retrieval improvements are noticeable for the TREC 2002 novelty tracks, the improvements for TREC 2003 are moderate, and for TREC 2004 improvements are seen for ranks 15, 20 and 30. One reason for the more significant improvements on
TREC 2002 data collection could be that the impact of NEs and information patterns are more effective since the percentage of relevant sentences in this data collection is very low.

Nevertheless, the results in Tables 7.1 X 7.3 have shown that the pattern-based approach significantly outper-forms all the four baselines at top ranks for identifying novel sentences, for all three novelty data collections.
This indicates that our pattern-based approach makes a larger difference at the step of detecting novel sen-tences than at the step of finding relevant sentences for those specific topics, particularly from the TREC 2003 and 2004 novelty tracks. The reason could be that TREC 2003 and 2004 novelty track collections exhib-ited greater redundancy than the TREC 2002 and thus has less novel sentences, therefore our information pat-terns make a big difference here. In combination, information patterns play a balanced role in the two major steps: relevance retrieval (for TREC 2002 in particular) and novelty detection (for TREC 2003 and 2004 in particular).

We also want to see how information patterns can improve the performance of novelty detection for those general topics that cannot be easily turned into multiple specific NE-questions. As described in Section 6 , all the three types of information patterns are incorporated in the relevance retrieval step of novelty detection for general topics. Table 7.12 gives the performance comparison of relevance retrieval with the original TFISF ranking and with the adjustments using these sentence level information patterns for the TREC 2002, 2003 and 2004 data, respectively.

The main conclusion here is that incorporating information patterns and sentence level features into TFISF techniques can achieve much better performance than using TFISF alone. Significant improvements are obtained for the 2003 topics and the 2004 topics. For example, at rank 15, the incorporation of the informa-tion patterns increases the precision of relevance retrieval by 6.9%, 8.1% and 3.6%, for TREC 2002, 2003 and 2004, respectively. This lays a solid ground for the next step  X  new information detection, and therefore for improving the performance of novelty detection for those general topics.

The overall performance of relevance for all topics is given in Table 7.13 . This table shows that re-ranking sentences with information patterns achieves better performance than using TFISF alone, for all topics. Read-ers can also compare the improvements between the specific and the general topics by comparing the results in
Tables 7.11 and 7.12 . Generally speaking, information patterns play a greater role in relevance retrieval for general topics than for specific topics. We believe this is mostly due to the incorporation of opinion patterns into relevant sentence re-ranking for the general and opinion topics (for TREC 2003 and 2004).
We have also conducted experiments to apply opinion patterns to the relevance re-ranking for specific top-ics, and we have found that the performance is worse than the results without opinion patterns as shown in
Table 7.4 . There could be two reasons. (1) Answers for specific topics are mostly specified by the specific required named entities, no matter the sentences include opinion patterns or not. (2). Answers for general, opinion topics are often indicated by opinion patterns, as have been shown in the statistics of opinion patterns in Section 5 (OP Observations #1 and #2). 7.3. Baselines and evaluation revisited
The experimental results show that the proposed pattern-based approach (ip-BAND) gives the best perfor-mance among all approaches compared. But there is still much room left for further improvements. There are a few factors that may affect the performance of our system. These include: (1) baseline selection and evalu-ation measure by human assessors; (2) misjudgment of relevance and/or novelty by human assessors and dis-agreement of judgments between the human assessors; (3) limitation and accuracy of question formulations.
Discussions of these factors can be found in Li (2006) . Here we want to discuss the issue of baseline selection and evaluation measure; future work on question formulations will be presented in the next section.
Relevance or novelty judgment files by human assessors are usually used as the basis for evaluating the per-formance of different systems. In the TREC novelty tracks, the judgment of novel sentences was based on a particular set of relevant sentences in a presumed order. Unlike relevance measures, a novelty or redundancy measure is asymmetric . The novelty or redundancy of a sentence S ( S , ... , S i 1 ) that the user has seen before this one. For the TREC novelty track data, only the judgments for a particular set of sentences in a presumed order are available. To collect novelty judgments of each sen-tence with respect to all possible subsets of orders, a human assessor has to read up to 2 impossible to collect complete novelty judgments in reality.

There are two potential problems with the current judgment/evaluation file. First, it is not very accurate to evaluate a system X  X  performance if the ranked sentences of a novelty detection system have a different order from the particular set. Second, a relevant sentence, pre-marked redundant by human assessors, could be trea-ted as a novel sentence if it has new information that is not covered by previous sentences in the list provided by a novelty detection system. That is the case where the sentences that make it redundant in the list by human assessors are simply not retrieved by the system.

One possible solution to this problem is to reorder the retrieved sentences following the natural order given by the task. Note that this re-organization of the retrieved sentences was also done by NIST before presenting the sentences to the assessors for novelty assessments of the TREC novelty tracks. It seems that the re-organization of the retrieved sentences is a more appropriate (and stronger) baseline than the B-NN we have used. Further, the input to the novelty detection step of any novelty detection method should be re-organized in the same way. Actually, this re-organization was also done by Allan et al. (2003) before testing the novelty methods (including new word detection method). In our initial experiments, we have also tried the re-organization idea. However, by doing that, the baseline approach without any novelty detection is even better than some novelty methods (e.g., the new word detection method). At a first look, it seems that the problem came from the novelty detection step, but in fact it was mainly due to the low retrieval rate in the relevance retrieval step. The novelty detection step is to find new information from sentences that are supposed to be relevant . However, if many sentences retrieved in the first step are not relevant, the novelty detection precision will be low since some non-relevant sentences may be treated as novel. Re-ordering of the retrieval sentences make the situation worse since the relevance ranking of the reordered sentences does not make sense any more, and the results in novelty detection are really not predictable.

An alternative approach is to deal with the problematic evaluation measures by human assessors. Some work has been done in this respect. For novelty detection data collected and used in their studies, researchers at CMU ( Zhang, Callan et al., 2002 ) initially intended to collect judgments for 50 topics, but unfortunately they could only get assessments for 33 topics. They provide the information on which documents before a doc-ument makes it redundant. The documents must be listed in chronological order. Thus there are problems when evaluating a novelty detection system in which documents are not output in chronological order. As cess of developing new techniques in this area.

One possible solution to a more appropriate novel sentence judgment is to generate dynamic novelty judg-relevant sentences in a presumed order, relevant sentences can be classified into novelty groups . In each group, all sentences basically contain the same information with respect to the topic. Therefore, if a user reads any sentence from a novelty group , the rest of the sentences in the group will become redundant sentences to the user because they do not contain any new information.

To evaluate the performance of a novelty detection system, a dynamic novelty judgment file can be gener-ated at evaluation time. Given the ranked list of sentences from the system and the set of relevant sentences classified in novelty groups for a topic, the dynamic novelty judgment file can be generated by simply scanning the sentences in the ranked list and selecting sentences that are in the novelty groups of the topic and are the first sentence appearing in the ranked list from each novelty group. With the dynamic judgment file, the stan-dard evaluation tools provided by TREC can still be used to evaluate the performance of the novelty detection system. Evaluations with the dynamic novelty judgment file should be more accurate than using a fixed nov-elty judgment file based on a presumed order of sentences. An ideal system should select one sentence from each novelty group and deliver these sentences to the user. The ranked list of sentences provided by such an ideal system will be a complete set of novel sentences for the topic in the sense that it contains all relevant information related to the topic. The performance of the ideal system is then 100% for both precision and recall. 8. Conclusions and discussions
Novelty detection is an important task to reduce the amount of redundant as well as non-relevant informa-tion presented to a user. It is an important component of many potential applications, such as new event detection, document filtering, and cross-document summarization. In this paper, we introduce a new definition of novelty:  X  X  Novelty or new information means new answers to the potential questions representing a user X  X  request or information need . X  X  Based on this definition, a unified pattern-based approach is proposed for nov-elty detection. Queries or topics are automatically classified into specific topics and general topics. Multiple effective query-related information patterns, namely sentence lengths, combinations of NEs, and opinion pat-terns, are identified and considered at both the retrieval step and the novelty detection step. Three sets of experiments were carried out on the data from the TREC novelty tracks 2002 X 2004, for specific topics, for general topics, and for all the topics. The experimental results show that the proposed approach achieves better performance at top ranks than the four baseline approaches on topics from the novelty tracks of all the three years.

The proposed pattern-based approach opens up some further research issues. Even though we have signif-icantly improved the performance of novelty detection for both specific and general topics by using the pro-posed sentence level information patterns, the novelty detection precision for specific topics is much higher. Therefore, our future work will focus on further improving the performance for general topics. First, the proposed three information patterns only capture part of the characteristics of required answers.
Other information patterns, such as the document creation times that we have used in the time-based language model ( Li &amp; Croft, 2003 ), could be helpful in further improving the performance of novelty detection for all topics. Opinion patterns are only used for opinion topics. Finding similar patterns for event topics could fur-ther improve the performance for event topics. An event pattern such as  X  X  X eported by XXX X  X  where  X  X  X XX X  X  could be a person X  X  name or a news agency, might indicate the description of an event is included in a sentence.
Some specific named entity combinations such as a specific time, date and location in a sentence could be another type of event pattern.

Second, exploring the possibilities of turning more topics into multiple specific questions will be of great interest. Currently, only NE-questions that require named entities for answers are considered. However, a topic may be not completely covered by the NE-questions that are automatically transformed at the step of query analysis. For more topic coverage, other types of question should be considered in addition to
NE-questions. Therefore, one direction of our future research is to explore other types of question and dis-cover the related patterns that may indicate the existence of potential answers. One type of question that could be considered is the  X  X  X hy X  X  question that usually asks for the cause of an event or the reason of an opinion.
For this type of question, the occurrence of  X  X  X ecause X  X ,  X  X  X he reason is that X  X ,  X  X  X ue to X  X , or  X  X  X aused by X  X  in a sentence may indicate possible answers. These words or phrases could be used as useful patterns for identify-ing relevant sentences. There are other types of questions that could be considered, such as definition questions ( X  X  X hat X  X  questions) and task questions ( X  X  X ow To X  X  questions). Many other types of questions have been stud-ied in the question answering research community. A close monitoring of the development of question answer-ing techniques and integrating them into the proposed pattern-based approach will further improve the performance of novelty detection.
 Acknowledgements This paper is based on the PhD thesis work of the first author ( Li, 2006 ) at the University of Massachusetts, Amherst. The work was supported in part by the Center for Intelligent Information Retrieval, by SPAW-ARSYSCEN-SD grant numbers N66001-99-1-8912 and N66001-1-8903, and in part by the Defense Advanced
Research Projects Agency (DARPA) under contract number HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsors.
 References
