
During the last decade, we have seen an explosive growth in our capabilities to collect data, thanks to the availability of cheap and effective storage devices. The advances in data collectio n have generated an urgent need for techniques that can intelligently and automa tically analyse and mine knowledge from huge amounts of data. The progress in knowledge discovery brings together the latest research in exciting and rapidly growing field of data mining (Fayyad et al. 1996). a large volume of data to discover interesting and useful information. The core of this process is the application of machine learning-based algorithms to databases. There are two basic ways of performing data minin g and data analysis: the supervised and the unsupervised learning. Supervised learning exploits known cases that show or imply well-defined patterns to find new patterns by means of which generalizations are formed. In unsupervised learning, data patterns are found starting from some characterization of the regularities in a set of data.

Classification is perhaps the most commonly applied data mining technique. It employs a set of preclassified examples to develop a model, which generates a set of grouping rules by means of which a new object may be categorized. There are different classification techniques used to extract relevant relationships in the data, ranging from symbolic learning implementation (Quinlan 1986) to neural networks (Rumelhart et al. 1986). Though these classification tools are algorithmically strong, they require significant expertise to work effectively and do not provide intelligible rules.

The classification problem becomes very hard when the number of possible dif-ferent combinations of parameters is so high that techniques based on exhaustive searches of the parameter space rapidly beco me computationally infeasible. Packard has shown in Breeden and Packard (1992) how learning and optimization algo-rithms can be used to produce optimal modeling of experimental data in the ab-sence of previous theoretical explanations. Moreover, the self-adaptability of Evo-lutionary Algorithms is extremely appealing for information-retrieval applications.
Thus, it is natural to devote attention to a heuristic approach to find a good-enough solution to the classification problem. In this paper, the objective is to exploit the capability of Evolutionary Algorithms to search easily comprehensible classification rules.

The paper is organized as follows: in Sect. 2, a brief review of the state of the art of classification methods is illustrate d. In Sect. 3, an automatic classification system based on an evolutionary algorithm is presented together with implementation details. Section 4 describes the real problem faced, the breast cancer diagnosis, while
Sect. 5 contains the performance of our system compared with that achieved by other methods. In the last section, final remarks and future work are outlined.
Information mining and knowledge discovery from large databases have been rec-ognized as a key research topic in database systems and machine learning. Since the late 1980s, knowledge-based techniques have been used extensively by infor-mation science researchers. These techni ques have attempted to capture searchers X  and information specialists X  domain knowledge and classification scheme knowledge, effective search strategies and query refinem ent heuristics in document retrieval sys-tems design (Chen and Dhar 1991). Despite their usefulness, systems of this type are considered performance systems  X  they only perform what they were programmed to do (i.e., they are without learning ability). Significant efforts are often required to acquire knowledge from domain experts and to maintain and update the knowledge base.

A newer paradigm, generally considered to be the machine learning approach, has attracted attention of researchers in artifici al intelligence, computer science, and other functional disciplines such as engineering, medicine and business (Michalski 1983;
Carbonell et al. 1993; Weiss and Kulikowski 1991). In contrast with performance systems, which acquire knowledge from human experts, machine learning systems acquire knowledge automatically from examples, i.e., from source data. The most frequently used techniques include symbolic, inductive learning algorithms such as
ID3 (Quinlan 1986), which uses a fixed numbe r of generalization values, multiple-layered feedforward neural networks suc h as Backpropagation networks (Rumelhart et al. 1986) that can, in principle, produce many more interpolation values not present in the training cases, and Genetic Algorithms (GAs) (Holland 1975; Goldberg 1989).
Many information science researchers have started to experiment with these evo-lutionary techniques as well (Gordon 1988; Belew 1989; Chen and Lynch 1992;
Chen et al. 1993). A classification of the data mining techniques and a compara-tive study of such techniques can be found in (Holsheimer and Siebes 1994; Chen et al. 1996).
 1996) and it has been studied in statistics, machine learning, neural networks and ex-pert systems (Weiss and Kulikowski 1991). Several classification methods have been proposed. Those based on decision trees (Quinlan 1986, 1993) operate performing a successive partitioning of cases until all subsets belong to a single class. This oper-ating way is impracticable except for trivia l data sets. Other data classification tech-niques include statistical and rough sets approaches (Fayyad et al. 1996; Ziarko 1994) and neural networks (Lu et al. 1995; Hung et al. 2001). Most data mining related
GAs proposed in the literature address the task of rule extraction in propositional and first-order logics (Giordana et al. 1994; Augier et al. 1995; Neri and Gior-dana 1995; De La Iglesia et al. 1996; Anglano et al. 1997; Noda et al. 1999).
A further interesting GA X  X ased method for choosing an appropriate set of fuzzy if X  X hen rules for classification problems can be found in Ishibuchi et al. (1995), while in Salim and Yao (2002), an innovative evolutionary algorithm to knowledge discovery in databases by evolving SQL queries has been presented. Hybrid clas-sification learning systems involve a combination of artificial neural networks with evolutionary techniques ( Yao and Liu 1997) and with linear discriminant models (Fogel et al. 1998), and an integration of rule induction and lazy learning (Lee and
Shin 1999). Furthermore, Genetic Programming (Koza 1992) frameworks for discov-ering comprehensible classification rules have been investigated (Freitas 1997; Ngan et al. 1998; Bojarczuk et al. 1999; Brameier and Banzhaf 2001).
Our aim is the implementation of an evolutionary system able to acquire information from databases and extract in telligible classification rul es for each available class, given the values of some attributes, called predicting attributes. Each rule is consti-tuted by conditions on the predicting attributes. These conditions determine a class description which can be used to construct the classification rule.
 understandable that, for complex classification problems, the number of possible de-scriptions is enormous. An exhaustive search by enumerating all the possible descrip-tions is computationally impracticable. H ence, we appeal to heuristic search tech-niques. In our case, evolutionary approaches based on variants of GAs and Breeder
Genetic Algorithms (BGAs) (M X hlenbein and Schlierkamp-Voosen 1993) have been used.
 senting a single candidate rule, and to gradually improve the quality of these rules by constructing new fitter rules until either rules of sufficient quality are found or no further improvements occur. The major steps of this evolutionary system can be formalized as follows: 1. Generate at random an initial population of r ules representing potential solutions to the classification problem. 2. Evaluate each rule on the basis of an appropriate fitness function. 3. Select the rules to undergo the mechanism of reproduction. 4. Apply the genetic operators, such as r ecombination and mutation, to generate new rules. 5. Reinsert these offspring to create the new current population. 6. Repeat steps 2 to 5 until either correct (see Sect. 3.3) classification rules are found or a fixed maximum number of generations has been reached.

To construct the classification model, da ta is partitioned into two sets: the train-ing and the test sets. The training set contains the known objects used during the evolution process to find one explicit classification rule able to separate an instance of a class from instances of all other classes, while the test set is used to evaluate the generalization ability of the rule found. It should be observed that, for a multiple-class problem, the system needs as man y rules as the number of classes, say c . Thus, the training phase consists in running c times the system in order to find these rules, each of which establishes the related membership class. The found rules are used to predict the class of the examples in the t est set. If for an example only one rule is predicted by the rule. Instead, if more or no rules are applicable, the example is classified as indeterminate by our system.
A single rule is defined by a genetic enc oding, in which each genotype codes for the different attributes. The phenotype is the classification rule itself. This rule is con-stituted by a number of conditional clauses, in which conditions on some attributes are set, and by a predictive clause representing the class. A class together with its description forms a classification rule  X  if &lt; description tional part of the rule is formed by the conjunction (logical conditional clauses. This choice is a limitation to the expressive power of the rule and it is due to the chosen encoding. Actually, this limitation could be overcome by letting the conjunctions evolve within a set containing (  X  ). However this would make the chromosome handling much more troublesome.
In fact, the use of further connectives would require the introduction of delimit-ing symbols such as parentheses in order to ensure rule consistency. Moreover, this would imply variable -sized chromosomes.

It is easily comprehensible that the optimal search of a classification rule includes two tightly coupled subproblems: the search of the more discriminating attributes and the search of the variation interval within the domain of these attributes. Then it is necessary to provide an encoding ab le to represent a rule with conditions on any number of available attributes and to specify which types of conditions we can establish on a generic attribute A i . In our case, the domains can vary in an integer or a real range according to the chosen data base. We have considered four types of possible conditions: where k i 1 and k i 2 are numerical constants related to attribute A have made reference to 0 order logic. Also this is a limitation due to the evolutionary algorithm and to the chosen encoding.
 attribute and, in the latter case, the condition type is to be specified. The geno-type of each individual in the population is r epresented by using two vectors. The first vector, called interval vector , is constituted by a number of loci which is twice the number of attributes. They cont ain in sequence for each attribute A numerical values v i 1 and v i 2 representing the current extremes of the variation in-terval. The second vector, named condition vector , has a number of loci equal to the number of attributes. Each allele of this vector can take on five values indicating five possible variants on the co rresponding attribute condition. Namely, with reference to the aforementioned condition types, if the value in the i th locus is 0, there is absence of the condition for the i th attribute A there is a condition of type ( COND 1) and so on. The values k the conditions are tied to the values v i 1 and v i 2 following relationships: k i 1 = min { v i 1 ,v i 2 } and k last position, the condition vector contains a further element representing the class.
Supposing there are only five attributes, indicated with A and the condition vectors are as in Table 1, the classification rule can be interpreted as follows: where C 2 is the class labelled with the value 2.
As concerns the genetic operators, apart from the crossover and mutation extended to other representation languages with m -ary rather than binary alphabets, recom-bination and mutation operators able to directly deal with real variables have been taken into account.
 Voosen 1993). In particular, as far as the recombination operator is concerned, the
Discrete Recombination (DR), the Extende d Intermediate Recombination (EIR) and the Extended Line Recombination (ELR) have been investigated. For the mutation operator, the Discrete Mutation (DM) and the Continuous Mutation (CM) have been considered. A detailed description of how these operators work can be found in
M X hlenbein and Schlierkamp-Voosen (1993, 1994).
We are looking for classification rules and different criteria can be used to evaluate the fitness of a rule. However, in an evoluti onary search, this fitn ess must encapsu-late as much as possible the desired features. Each individual in the population is objects to classify. Denoting with D the set of all possible descriptions for a given class, to each description d in D corresponds a subset of the training set S ,de-fied, and a size of the class C representing the points where the prediction of the of the negative examples. During the iterative process, the search system will en-counter many incorrect descriptions, yet useful as components for new, and hope-fully better, descriptions. The concept of correctness needs to be extended to be able to select the most promising descriptions out of a set of incorrect ones. Thus, for each description d for a class C , we recall the definition of the accuracy as: and coverage  X  as:
The accuracy of a description represents the probability that an object covered by the description belongs to the class while the coverage is the probability that an object belonging to class C is covered by the description D . Moreover, on the basis of these values, the following kinds of rules can be distinguished:  X  Complete rules : the rule is complete if  X  is equal to 1, that means any object belonging to the class is covered by the description for this class, i.e., C  X  Consistent rules : the rule is consistent if  X  is equal to 1, that is, any object covered by the description belongs to the class, i.e., C  X  Correct rules : the rule is correct if both the classification accuracy and the cov-erage are equal to 1, i.e., if  X  D ( S ) = C .

We can use the correctness-criterion as a fitness function f signed to f c if the description is correct, while its value for any incorrect rule is smaller than 1. Piatesky-Shapiro (1991) proposes principles for the construction of f which assigns a numerical value indicatin g the correctness of any description d in the description space D . The correctness depends on the size of by the description, the size of the class C and the size of their overlapping region  X  ( S )  X  C .

The simplest function to evaluate the fitness of a rule is:
This function can be intuitively understood as the difference between the actual number of examples for which the rule classifies properly and the expected number if C were independent of D . It assumes its maximum value when the examples belonging to C are all and only those that satisfy the condition of D , that is to say, when  X  D ( S ) = C . In this case: a rule is, as suggested in Holsheimer and Siebes (1994). The model proposed is: where  X  D ( S ) is the set of the points in the database in which the conditions are not satisfied while C is the set of the points in which the prediction of the rule is false. Equation (5) assumes its maximum value when that they increase with coverage and accura cy. Consequently, this also guarantees an improvement in terms of completeness and consistency.
 some other factors into account. Keeping in mind that most data mining systems rely on Ockham X  X  razor (Derkse 1993) ( X  X he simpler a description, the more likely it is that it describes some really existing rela tionships in the datab ase X ), we have decided to add further terms to yield a more discriminating fitness function. In particular, we have considered two quantities that take into account in some way the simplicity and the compactness of the description.
 the number of conditions. Namely: where n is the number of the conditions active in the current description and n is the maximum number of conditions that, in our encoding, corresponds to the number of database attributes. Its goal is to prefer the rules with a lower number of conditions.
 the current rule, the ratio between the range of the corresponding attribute and the range of the attribute domain is evaluated. The function f these n ratios divided by their number. This factor varies in indication on the width of the intervals for the conditions present in the rule. The function f 2 can be formalized as follows: where  X  i = ( max i  X  min i ) is the range of the domain of the i th attribute and given by: more restrictive conditions.

The total fitness function f tot considered during the training phase is then the sum of three terms: with where f v corresponds to (3) or (5) for the linear and the logarithmic fitness functions, respectively, f v max represents the best value that f assume values much lower than 1 which is the assigned weight for f in order not to affect too much the evaluation of the description which must take into account the correctness above all. The function f stat
With these choices, the problem becomes a maximisation task. It should be noted that the chosen evaluation mechanism does not guarantee to find the single best rule describing the class under consideration. This is why it is based on some subjective criteria, and even if a perfect evaluation m echanism could be devised, a selection of rules could be necessary for representing different instances of patterns within the database.
In order to exploit the evolutionary approach ability to face a classification task, an evolutionary system has been implemented and applied to one of the most important real problems in the medical domain, i.e., the breast cancer problem. The purpose is to find intelligible rules to classify a tumour as either benign or malignant.
Breast cancer data sets were originally obtained from W.H. Wolberg at the Uni-versity of Wisconsin Hospitals, Madison. We have considered two data sets. The first contains 10 integer-valued attributes, of which the first is the diagnosis class, while the other nine attributes are related to cell descriptions gath ered by microscopic examination (Wolberg and Mangasarian 1990). All these attributes have values in the set { 1 , 2 ,... , 10 } . The data set is constituted by 699 examples, of which 458 are benign examples and 241 are malignant examples. In the following, this database will be denoted as CANCER 1a. It should be noted that this database contains 16 missing attribute values. If we omit the examples with missing attributes, the total number of instances becomes 683, of which 444 are benign and 239 are malignant. This database without missing values will be called and the remaining 212 are known to be malignant. These data have been obtained by means of an image analysis system developed at the University of Wisconsin. First, a fine-needle aspirate (FNA) (Mangasarian et al. 1995) is taken from a lump in a pa-tient X  X  breast. Then the fluid from the FN A is placed onto a glass slide to highlight the nuclei of the cells. An area of the slide is considered to generate a digitized image. Ten real-valued features are comput ed for each cell nucleus. The mean, stan-dard error and worst or largest (mean of th e three largest values) of these features were computed for each image, resulting i n 30 features in addition to the diagnosis class. This database will be called CANCER 2.
 because each multiple-class cl assification problem can be reduced to a two-class problem. In fact, in the case of multiple classes, during the search of the rules pre-dicting a given class, all the other classes can be conceptually thought of as merged into a larger class containing the examples that do not belong to the class predicted.
The breast cancer problem is intended as a te st to evaluate the effectiveness of the approach proposed.
The breast cancer problem has been faced by means of different techniques. As concerns the CANCER 1 data set, initially the classification was performed by linear programming methods (Mangasarian et al. 1990; Bennett and Mangasarian 1992).
Prechelt (1994) showed the results obtained with manually constructed artificial neu-ral networks and Setiono and Hui (1995) used a new neural algorithm called FNNCA.
A comparison with these results is effected by Yao and Liu (1997) who present a new evolutionary system, i.e., EP-Net, for evolving artificial neural networks and compare their results with those attained in Prech elt (1994); Setiono and Hui (1995). These approaches have the disadvantage of lacking explicit rules. In Sherrah et al. (1997) the authors proposed a system that can perform both feature selection and feature construction, but they still do not focus on the discovery of comprehensible rules.
Taha and Ghosh, in Taha and Ghosh (1997), have exploited rule extraction tech-niques from trained feedforward neural n etworks while Pe X a X  X eyes and Sipper, in
Pe X a and Sipper (1999), have combined fuzzy systems and Evolutionary Algorithms to provide comprehensible classification rules.
 1999) and machine learning methods (Hung et al. 2001; Wolberg et al. 1995) have been applied to breast cancer diagnosis and prognosis using the real-valued data set.
The evolutionary system works on the training set only. At the end of the training phase the best rules found are evaluated on the test set. The system allows attaining two rules covering the benign and the ma lignant cases. To achieve these two rules, the evolutionary algorithm is run twice. In practice, we analyse one class at a time.
The training sets must be reasonably sized to ensure adequate population cov-erage. Moreover, as indicated by Prechelt (1994, 1995), it is insufficient to indicate the number of the examples in each of the p artitioned sets because the results may vary significantly for different partitions even when the number of examples in each set is unchanged.
The evolutionary classification system re quires that some control parameters be spec-ified. Preliminary trials have been performed for an appropriate tuning of these pa-rameters, which vary as a function of the problem chosen. For both the problems, the selection mechanism and the fitness f unction chosen have been the same. The tournament selection with a tournament size  X  = 20% has been used. It should be noted that the results remain similar if the parameter  X  is within 15% and 25% of the population. This selection scheme has outperformed the proportional and the trun-cation selections. Furthermore a 1-elitism mechanism has been applied. The fitness function chosen has been (8) where p 1 and p 2 have been derived empirically equal to 0.05. Moreover, it should be pointed out that a linear normalization in has been applied to all the values in the databases to avoid some attribute being more significant than others.
 The values of the other parameters dep end on the problem. For the database
CANCER 1, the population size is equal to 200. Because we have nine attributes plus the class, on the basis of the fixed enc oding, each chromosome is composed of 28 genes. The single-point crossover has been used for both the condition vector and the interval vector, as we are dealing with integer values. This operator has resulted in being more efficient with respect to the uniform crossover. In the interval vector, the mutation operator randomly transf orms with uniform probability the value of an attribute into another value belonging to the domain of that attribute. The mutation rate used was 0.7. For the condition vector th e mutation changes the condition related to a single attribute. Its application probability was 0.3. This last value is not restric-tive. For example, the goodness of the results remains about the same if the mutation probability on the condition vector varies in the range may introduce or destroy new conditions so as to introduce significant variations, while the mutation on the interval vector changes the range of the attribute only and thus its probability can be higher without risk ing the loss of basic information. The evolution process terminates after at most 100 generations if a correct rule is not found before.

As concerns the database CANCER 2, the population size is 300, the search space being larger than in the previous case. Because we deal with 30 attributes plus 1 for the class, the chromosome on the basis of the chosen encoding is constituted by 91 genes.

For the integer-valued condition vector, we have used the single-point crossover while, for the real-valued interval vector , EIR has resulted in being more efficient than ELR and DR. On the basis of their definitions in M X hlenbein and Schlierkamp-
Voosen (1994), for EIR d = 0 . 3, so that the scalar parameter range [ X  0 . 3 , 1 . 3 ] . For the interval vector, DM has had worse performance than CM (M X hlenbein and Schlierkamp-Voosen 1993). Hence, CM with range and  X   X  X  0 . 0 , 1 . 0 ] has been considered. The mutation operator on the condition vector and the mutation rates as well have been the same as in the previous problem. The finding of one correct rule or a number of at most 200 generations has been fixed as termination criterion. In order to determine the validity of our system, let us formulate some definitions.
For each class, we indicate with:  X  T + the number of true positive examples, i.e., the number of the examples cor- X  T  X  the number of true negative examples, that is to say, the number of examples  X  F + the number of false positive examples that are the examples classified incor- X  F  X  the false negative examples, i.e., those e xamples that are incorrectly classified the sensitivity S e and the specificity S p defined as follows: which indicate the rule X  X  ability to classi fy correctly examples as belonging or not belonging to the predicted class, respectively.
 will denote with I 1 and I 2 the indeterminate cases, which include examples satisfying both the rules or no rule, respectively. Moreover, we indicate with CC and UC the total number of examples correctly and incorrectly classified, respectively. Finally, we denote with % Ac the percentage of classification accuracy, with % C and % U the percentage of cases correctly and incorrec tly classified, respectively, and at the end, with % I the percentage of indeterminate examp les. These last values are computed by means of the following formulas: where N V is the number of the examples in the test set. These parameters are tied by the formula: Several experiments have been performed on a SUN workstation for the database
CANCER 1, varying the size of the training and the test sets. Moreover, both the linear and the logarithmic fitness functions proposed in Sect. 3.3 have been tested.
The execution of this algorithm requires about 6 minutes if a correct rule is not found before.
Because the database contains some missi ng values, we have initially decided to merely remove instances with the missing attributes, with the awareness that this approach may lead to serious biases (Little and Rubin 1987). The available 683 instances of the database CANCER 1b have been subdivided into 508 examples for the training set and 175 for the test set. The test set remains unchanged and contains the same 136 benign and 39 malignant examples. The results achieved over 10 runs by using the linear fitness function (3) in (8) are reported in Table 2 in terms of average values A v and standard deviations Std De v .

As can be observed by the reported values, the system shows an average per-centage for accuracy equal to 99.58%, with a standard deviation equal to 0.29%.
This means that over 100 examples for whi ch the system has been able to classify on average more than 99 examples are correctly catalogued. Nevertheless, it is pos-sible to note from the table that there is 4.91% of indeterminate examples. It should be observed that, in many cases, it is better that the system does not classify rather than performs an incorrect classification. Ho wever, the system has correctly classified 94.69% of examples, with an error classification equal to 0.4%.

The best rule found by the system for the malignant cases presents the following conditions: This rule classifies the examples in the test set as shown in Table 3.
The best rule found for the benign cases is: This rule classifies the examples in the test set as in Table 4.

In our case for the malignant rule we have S e = 0 we correctly classify 97% of individuals having the disease and 96% of those truly without disease. For the benign rule, S e = 0 . 98 and S correctly classifies 98% of benign and 100% of malignant cases.

The system with these two rules has % Ac = 99 . 40, % C % I = 4.
 two rules separately and the global table can be understood observing that the number
F + in Table 3 increases either the number of cases satisfying both the rules or the number of examples incorrectly classified, while the number F increases either the number of examples that satisfy no rule or the number of cases incorrectly classified. The same observations are possible for Table 4. discriminant for the diagnosis. For example, during the trials effected, it has been observed that the attributes A 2 and A 3 for the malignant classification rules and A
A 6 and A 8 for the benign classification rules are almost always present. Moreover, the conditions on these attributes are often very similar.
 database. The results achieved over 10 executions are shown in Table 6. behaviour as well are the same as those obtained during the previous test. centage on average equal to 99.35%, with a standard deviation equal to 0.52%.
Nevertheless, it is possible to note by comparing Table 6 with Table 2 that there is a greater percentage of correctly classi fied cases, a lower percentage of indetermi-nate examples but a greater number of incorrectly classified examples. Furthermore, the standard deviations are higher except for one parameter.
 stances into a training set of size 341 and a test set of size 342. Three rule ex-traction techniques from trained feedforw ard networks were applied. Furthermore, a method of integrating the output decisions of both the extracted rule-based system and the corresponding trained network is proposed. The rule evaluation is based on performance measures, among which are the soundness ( T ( F + ). The dimensionality of the breast-cancer input space is reduced from 9 to 6 inputs. Different from Taha and Ghosh, we have used the complete set of attributes without performing any kind of data preprocessing. As regards the performance mea-sures, our single-rule classification system is able to achieve better results in terms of soundness but this is detrimental to the number of false alarms. In fact, in Taha and Ghosh (1997), considering the single best rule for the malignant and the be-nign case, we have an overall classification rate of 92.83 with 21 false alarms. By performing randomly their same subdivision of the instances, the best overall clas-sification rate found by our system over 10 runs is 96.35 with 33 false alarms.
However, Taha and Ghosh obtained better results than ours for their five-rule sys-tem. In particular, their best overall classification rate is 96.63. A simple explanation of all of our above reported results is that this multiple-rule approach is conceived taking in mind that the classification system will be constituted by the conjunction in
OR of more rules. In this way, the aim is to control the number of true positive cases to make the global system more reliable, but this is also detrimental to sim-ple interpretability of the results. Our system provides two easily interpretable rules with good performance. Moreover, it can be noted that it is difficult to try describ-ing complex phenomena by means of single rules able to generalize over the whole data set.

Better results are obtained by Pe X a X  X eyes and Sipper, who present a fuzzy-genetic system (Pe X a and Sipper 1999). They presented very good results for mul-tiple-rule systems, e.g., the overall classification rate for their best system is 97.8%, but the same authors admit that, for this system, there are 39 cases for which they are  X  X omewhat less confident about the output. X  Their best fuzzy one-rule system presents an overall performance of 97 . 07% but no information is given about the diagnostic confidence. Besides, their threshold system is based on the knowledge of the problem at hand, while our results have been obtained without assigning any external value.
The second experiment involved all the 699 instances: the 16 missing attributes have to be replaced. Little and Rubin (1987) describe several approaches to estimate the missing values, but all of the proposed methods are biased because they treat the replacement value as the actual missing va lue. Another replacement strategy based on a Monte Carlo simulation technique ca lled multiple imputation (Shafer 1997) allows the generation of m ultiple values for each missi ng datum. These values are analysed by standard complete-data methods and integrated into a single model. From a practical standpoint, a single replacem ent value must be chosen for each missing datum and this reintroduces bias. A further method, rather than trying to estimate the unknown attribute, treats as unknown a new possible value for each attribute and deals with it as other values (Lee and Shin 1999). We have chosen to replace the missing data with random values within the variation interval of the single missing attribute.

The first three quarters of the data (524 patterns) have been used for the training set and the last 175 for the test set, of which 137 are benign and 38 malignant patterns. We have run the evolutionary sy stem 10 times, each with a different starting random population and using the (3) within the total fitness function (8). The average results are outlined in Table 7. The aver age percentage for accuracy is equal to 98.52%, with a standard deviation equal to 0.8%.

The best rule found for the malignant cases is the following: we have S e = 1and S p = 0 . 96.

This rule classifies the examples in the test set as in Table 9 with S
S previous test. This may be due both to the insertion in the database of the previously discarded examples and to the fact that the evolutionary system provides a number of solutions that are nearly suboptimal from the performance point of view. This does not imply that the provided solutions be genotypically similar. The availability of different rules could represent an a ssistance for a human expert to make a deci-sion.

The system has % Ac = 98 . 83, % C = 96 . 57, % U = 1 . and with the same subdivision for the training and the test set. The average results achieved over 10 executions are shown in Table 11.
 rule we have S e = 0 . 95 and S p = 0 . 99 .

The best rule found for the benign cases is: This rule classifies the examples in the test set as in Table 13. For this rule, we have
S = 0 . 99 and S The results obtained by using both the rules are reported in the global Table 14.
The system has % Ac = 100, % C = 96 . 57, % U = 0and% I
Apart from the best results, it should be noted that the performance of the fit-ness with the linear function is more robust, the standard deviations of the different parameters being lower on average.

In Yao and Liu (1997), the same problem is faced by taking into account the miss-ing attributes. Unfortunately, they do not give any information on how the missing attributes are treated. They have present ed a new evolutionary system for evolving feedforward ANNs applied to this medical diagnosis problem. Their results show a lower percentage of wrong classifications. No indeterminate cases are provided.
Though the percentage of the wrong classifications obtained by our classification system is higher with respect to that attained by Yao and Liu, it is important to emphasize that we provide intelligible rules, unlike they do. This, in our opinion, counterbalances the worse results. However, our system includes indeterminate cases but it is easily explicable by the fact that it is sufficient that one single condition be not satisfied to make the examples classified as indeterminate. Moreover, the limited expressive power of the chosen encoding plays an important role.
 provide, when run more times, rules with different features. For example in our case, apart from the best rules above reported, we have at our disposal several more:
Note that some of these rules have been already found previously. Among them, the first and the third have together an accuracy equal to 100% while the second and the fourth present together the highest percentage of correctly classified examples, i.e., 96.57%.
 validation method has been applied. Considering the database with 699 examples, 524 of which are in the training set and the remaining in the test set, the examples in the two sets are randomly varied over 10 runs. The results averaged over the runs using the linear function in (8) are reported in Table 15.
 cases is higher with respect to the database with the first examples in the training set and the others in the test set. The increase in the indeterminate cases could be ascribed to the presence of the a nomalous examples in the test set.
The system has also been tested on the database CANCER 2. It should be considered that this problem is more co mplex because the search sp ace of the descriptions is much larger. Several experiments have been performed in Hung et al. (2001) con-sidering a training set composed by 455 examples and a test set of 114 examples.
The test set is randomly varied but it always includes 76 benign and 38 malignant examples. We have carried out 10 runs on a SUN workstation with this subdivision and with the linear function in (8). The ex ecution of this algorithm requires about 40 minutes if a correct rule is not found before. Our classification system has produced the results shown in Table 16.

The best rule for the malignant cases is:
This rule produces the results in Table 17 with S e
The rule for the benign cases is:
This rule determines the results in Table 18. In this case, S In Table 19, the results obtained by applying both the rules are shown.
As an example of evolution, the values related to these two best rules during the training phase in terms of the best and average fitness and standard deviation are shown in Fig. 1. The values within the range [0.7, 0.9] have not been reported, while the values in the range [0.9, 1.0] have been magnified to make evident the small variations for the best fitness value.

As can be noted, the increase in fitness values shows an initial remarkable quasi-linear phase, then this increase gets slower. During the final phase, no further fitness improvements are obtained until the end of the run. Because a correct rule is not achieved, the evolution terminates when the fixed maximum number of generations is reached.

In Hung et al. (2001), the classification has been performed by using feedforward neural networks. The network is used to estimate the posterior probabilities of the observations in the test set. According to Mangasarian et al. (1995), a case with malignancy probability between 0.30 and 0.70 is classified as indeterminate, while for values lower than 0.3 as benign and finally malignant for values higher than 0.7.
The paper illustrates several neural network models for classification. The value of the posterior probability is obtained by considering the mean of the outputs of 200 trained networks. Each model allows attaining high correct classification rates, but it is to be pointed out that the best results are obtained by applying a feature selection procedure which results in a model dealing with only 9 variables instead of 30. This reduces the corresponding search space, while we have left out of consideration any kind of preprocessing and postprocessing activity in the database construction. The best results in Hung et al. (2001) outperform those achieved by our system but their classification technique has the disadvantage of lacking in comprehensible rules. The availability of explicit rules is of noti ceable importance because it provides human experts with a further investigation tool.
In this paper we have presented an evolutionary classification system for automati-cally extracting explicit rules. The system has been evaluated on two two-class prob-lems in the medical domain, both related to breast cancer diagnosis. It should be pointed out that this test problem has been chosen only to evaluate the ability of an evolutionary technique in designing an automatic classification system. Naturally, the conceived system is easily applicable to an y other kind of database and generaliz-able to multiple-class problems. We have compared our system with other methods.
Experimental results have demonstrated the effectiveness of the approach proposed in providing the user with comprehensible classification rules.
 their application to different real-world data sets in order to further improve the promising results reported in the present paper. In particular, a Genetic Program-ming approach will be investigated to enhance the expressive power of the extracted rules. This will allow us both to easily introduce a wider set of conjunctions ( and  X  ) and to use higher order logics, i.e., to create clauses containing two attributes.
Furthermore, niching methods (Goldberg and Richardson 1987; Smith et al. 1992) will be exploited with the aim of finding rule sets.
Another interesting task to face will be unsupervised data mining, in which the sification, is not chosen a priori.

