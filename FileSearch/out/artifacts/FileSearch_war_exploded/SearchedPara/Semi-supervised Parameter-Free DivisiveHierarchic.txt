 Clustering categorical data is more challenging than clustering numeric data due to the lack of an inherently meaningful similarity measure. With large amounts of categorical data being generated in real life, clustering of categorical data has been receiving increasing a ttention in recent years [3,7,9 ,10,16,20]. Moreover, in many real-life applications on categorical data, prior knowledge exists in relation to the need or the goal of the data analysis. For example, when a market analyst performs clustering for market segmentation from survey data, he/she will likely want that the customers from the same family be grouped together instead of trivial separation between men and women. None of existing algorithms for clustering categorical data can exploit such prior background knowledge.
Most existing semi-supervised clustering algorithms are non-hierarchical and focus on analyzing numeric data [5]. These non-hierarchical clustering algorithms are derived from the k -means algorithm [2,4,14,18,19]. Besides the k -means vari-ants, labeled instances have been used to help set the parameters of a density-based clustering algorithm in [15], as appropriately setting the parameters is critical but difficult, especially when th e density of the clusters differs widely. However, neither the concept of objective function or Euclidean distance used in the non-hierarchical methods nor the density notion in density-based clustering algorithms is naturally meaningful for categorical data. Moreover, most of exist-ing algorithms have some parameters to tune, these parameter-laden algorithms impose our prejudices and presumptions on the data [12].

Incorporating instance-level constraints in hierarchical clustering is more chal-lenging than in partitioning clustering due to the feasibility problem of satisfying the constraints [5,19]. Some studies suggest that hierarchical algorithms can pro-duce better-quality clusters [11,17]. H owever, little work has been done on the application of background knowledge to hierarchical clustering [4,13], and even these few published hierarchical algorithms are all agglomerative, not divisive. The concept of comparing similarity betw een pairwise instances, which is used in agglomerative method, is not suitable for categorical data [9,20], thus the error-propagation issue of agglomerative method is even more critical in categorical domain. Furthermore, the high time complexity of agglomerative methods pre-vents them from being used on very large data sets.

In this paper, we propose several novel concepts and methods suitable for cat-egorical data, based on which, we propose a semi-supervised clustering algorithm for categorical data. We view semi-supervised clustering of categorical data as an optimization problem with extra instance-level constraints. As the optimiza-tion problem in the clustering is NP hard [1], thereby we propose a heuristic approach to guide the optimization process to a better solution in terms of sat-isfying the constraints. We name the new algorithm SDHCC (Semi-supervised Divisive Hierarchical Clustering of Categorical data). SDHCC is systematic and parameter-free. To our knowledge, SDHCC is the first semi-supervised clustering algorithm for categorical data. In this paper, prior background knowledge is provided as must-link and cannot-link constraints on pairs of instances [18,19]. A must-link constraint indicates that the two instances have to be in the same cluster, while a cannot-link con-straint indicates that the two instances must not be placed in the same cluster.
Since must-link constraints are equivalence relations, they can be used to gen-erate transitive closures. Cannot-link constraints then can be transformed to a cannot-link matrix representing link relations hip between closures. In this pa-per, a transitive closure is also called a constraint closure. Besides the transitive closure, a single instance which is not involved in any must-link constraint but is involved in a cannot-link constraint is also calle d a constraint closure.
The cannot-link matrix is generated from the constraint closures and the cannot-link constraints. Let l be the number of constraint closures; then the cannot-link matrix M is of order l  X  l ,where m ij =1ifclosure c i and c j cannot link, otherwise, m ij =0.Closures c i and c j cannot link if  X  X i  X  c i and  X  X j  X  c j such that ( X i ,X j )isinthesetof cannot-link constraints. Thus the cannot-link matrix M is a symmetric Boolean matrix.

The feasibility of satisfying all constr aints for non-hierar chical and agglom-erative hierarchical clustering has been studied in [4,5]. The feasibility problem is more complex in divisive hierarchical clustering, because a divisive method starts with an all-inclusive cluster containing all the instances, and repeatedly chooses one cluster to split into two sub-clusters, which may make violations of cannot-link constraints unavoidable lower down (closer to the root) in the clus-tering hierarchy. In this paper, must-link constraints are satisfied at all levels of the clustering tree, whereas cannot-link violation is tolerated, especially at the lower levels (closer to the root). Note that we assume the constraints are con-sistent, dealing with the erroneous constraints is out of the scope of this paper. The following definitions help to measure the degree of cannot-link violation in a cluster (under the assumption that all the must-link constraints are satisfied). Definition 1. Given two constraint closures c i and c j , and the cannot-link ma-trix M , the cannot-link weight of the closure c i with respect to c j is where | c j | is the number of instances in closure c j .
 Definition 2. The degree of cannot-link violation of cluster C is defined as: where w ( c i | C )= c The categorical data set  X  = { X i } n i =1 is represented by an indicator matrix, denoted as Z . Each instance X i is a multidimensional vector of m categorical attributes with domains D 1 ,  X  X  X  ,D m , respectively. In the n ew data representa-tion, each categorical value represents a dimension. Let J = m t =1 | D t | be the total number of categorical values, then the indicator matrix Z is of order n  X  J , and the general entry of the indicator matrix Z is as follows: In the following presentation of this paper, we use Z i to denote instance X i according to the indicator m atrix data representation.

We view constraint-free clustering categorical data from an optimization per-spective, and propose a novel objective function, i.e., the sum of Chi-square error (SCE). We set the objective of clustering  X  into K clusters to minimize SCE, which is defined as follows: where K is the number of clusters. d Chi ( Z i ,C k ) is the Chi-square distance be-tween instance Z i and Cluster C k , which is defined as follows: Here  X  kj (1  X  j  X  J )isthe j th element of cluster center of cluster C k ,whichcan be mathematically derived as
For semi-supervised clustering based on the divisive hierarchical approach, we iteratively deal with the constrained optimization problem with K =2. SDHCC starts with an all-inclusive cluster containing all the categorical instances, and repeatedly chooses one cluster to split in to two sub-clusters. Each bisection step consists of three phases: initialization; iterative refinement based on Chi-square distance; and alleviation of the cannot-link violation. 3.1 Initialization The initialization of the bisection consists of two steps, i.e., preliminary split-ting based on multiple correspondence analysis (MCA), and redistributing the instances involved in must-link constraints to satisfy the must-link constraints. The following paragraph describes MCA calculation on indicator matrix of  X  (the calculation on a cluster is the sam e), the interested reader can refer to [8,20] for more details.

Since Z has a total sum of n  X  m , which is the total number of occurrence of all the categorical values, the correspondence matrix is P = Z/nm .Thus the vector of row mass of the correspondence matrix is r = 1 n 1 ,therowmass matrix is D r =(1 /n ) I ; the column mass matrix is D c =(1 /nm ) diag ( Z T Z ), the vector of column mass can be denoted as c = D c  X  1 . Under the null hypothesis of independence, the expected value of p ij is r i c j , and the difference between the observation and the expectation, called the residual value, is p ij  X  r i c j . Normalization of the residual value involves dividing the difference by the square root of r i c j . So the standardized residuals matrix is written as: Hence, the singular value decomposition (SVD) to compute the residuals matrix (5)isasfollows: where U T U = V T V = I .  X  is a diagonal matrix with singular values in descend-ing order:  X  1  X   X  2  X  X  X  X  X  X   X  s &gt; 0, where s is the rank of the residuals matrix. The columns of U are called the left singular vectors, which give us the scale values for the n instances.
 The first step of bisection initializatio n proceeds as follows. To bisect a cluster C
P with | C instance Z i whose first coordinate U ( P ) i 1  X  0 goes to the left child of C P ,which is denoted by C L P ,andeach Z i whose first coordinate U ( P ) i 1 &gt; 0goestotheright child of C P , which is denoted by C R P .

The second step aims to satisfy all the must-link constraints. In fact, the preliminary splitting in first step may distribute the instances in a constraint closure in the two sub-clusters. Each closure c split by the first step is re-assigned to the sub-cluster that holds most of its members; i.e., if | c  X  C L P | X | c  X  C R P | , the closure c goes to the left child C L P , otherwise, the closure c goes to the right child C R P . 3.2 Refinement In the refinement phase, we employ the Chi-square distance to measure the dis-similarity between a single instance, as well as a constraint closure, and a cluster. In fact, after re-assignment of the second st ep of the initialization phase, the in-stances from each closure are assembled together, so the instances in a closure will be treated as an entity in performing the refinement. Therefore, we define the dissimilarity between a closure and a cluster of categorical instances as follows. Definition 3. The dissimilarity between a constraint closure c andacluster C i is the Chi-square distance between them, i.e., where c is in cluster C i ,and  X  c j (1  X  j  X  J ) is the j th element of the center of closure c , i.e.,  X  c j = 1 | c | Z (7) is the same as (3).
 Theorem 1. In a refinement phase (not limited to bisection operation of hier-archical clustering), deciding the membership of a closure according to the Chi-square distance defined in (7) is equivalent to making the decision according to the sum of the Chi-square distance of all the instances in the closure, which is defined in (3).
 The proof of Theorem 1 is omitted here due to lack of space.

Definition (3) and Theorem (1) provide a significant computational advan-tage. They show that it is only necessary to store the statistic features of each constraint closure and those of the two resulting sub-clusters, to speed up the refinement process. In other words, the instances in a closure are treated as an entity, the number of instances for refinement is thus reduced. The cluster fea-tures of C P have two elements: one is the J -dimensional vector of numbers of occurrences of all the cate gorical values, denoted as NO P ; and the other is the number of instances in th e cluster, denoted as | C P | . The features of a closure are the same as those of a cluster. We do not need to revisit all the instances in the cluster to update the cluster features; instead, we just need to record the instances which were relocated to do the u pdate, so the cluster features can thus be updated efficiently after each iteration cycle.

The statistic feature of Chi-square distance requires that individual instance (or closure) be in the measured set. Wh en calculating the distance between an instance Z i and cluster C P ,where Z i is not in C P , we should proceed as if Z i is in cluster C P by updating the cluster features to r P  X  NO P + Z i and r P  X | C P | +1, and after that, restore th e cluster features. Here r P is the balance ratio of cluster size of the two resulting sub-clusters. The balance ratio is set to 1.0 for C L P ,and | C to the distance calculation between a closure c and C P ,where c is not in C P . We use the balance ratio to solve the bias issue in the relocation process. This is because that the rare categorical value of the individual instance/closure has relative high frequency in smaller cluster, which leads to smaller Chi-square distance.
 The refinement of the instances in C L P and C R P proceeds as in Algorithm 1. 3.3 Alleviation of Cannot-Link Violation In this subsection, a novel divide-and-merge method is proposed to alleviate the cannot-link violation in each bisection. It pro ceeds as follows (The pseudo-code is given in Algorithm 2): Dividing operation: in each sub-cluster ( C L P or C R P ), if it has a cannot-link constraint, divide it into two sets. One of these, called the alien set, contains the target constraint closure which is most dissimilar with the sub-cluster, and also the instances that are similar to the target closure (how to choose the target closure will be presented afterward s); the other set, which is called the native set, contains the rest of the instances in the sub-cluster. The pseudo-code for the dividing operation is shown in Algorithm 3.
 Merging operation: After dividing operation, the alien set is merged with the native set of the other sub-cluste r if doing so can decrease the sum of the degree of cannot-link violation of C L P and C R P . The pseudo-code for the merging operation is shown in Algorithm 4.

Algorithm 2. Alleviating the cannot-link violation 1. if C L P has cannot-link violation 2. if C R P has cannot-link violation 3. if( C LR P =  X  or C RR P =  X  ) 4. Repeat steps (1), (2) and (3) until no target closure can be found to 5. Recall Algorithm 1.

The closure which has the largest cannot-link weight in the sub-cluster is cho-sen as the target closure; if in a sub-cluster there is more than one closure with the largest cannot-link weight, choose the one with the greatest Chi-square dis-tance from the cluster. To use the constraint knowledge to guide the assignment of unconstrained instances, the unconstra ined instances which are similar with the target closure are removed together. The divide-and-merge operation on C L P and C R P proceeds iteratively until the sum of the degree of cannot-link violation of C L P and C R P cannot decrease.

We make use of the global clustering quality measure proposed in [3] to de-cide when to terminate splitting in Algorithm 3 and the splitting process in constructing the clustering tree. The termination condition is that either of the two conditions given below is satisfied: 1. Algorithm 1 ends with one cluster, which means that the relocation algorithm 2. Clustering quality does not increase, i.e., Q ( { C t } )  X  Q ( { C tL ,C tR } ), and In the condition (2), Q (  X  ) denotes the global clustering quality measure [3]. In this section, we study the perform ance of SDHCC on real data sets from the UCI Machine Learning Repository and 20-Newsgroup text data. We imple-mented a semi-supervised clustering algorithm for categorical data by combining the semi-supervised algorithm COP-KMEANS in [19] and the k -modes algorithm for categorical data in [16]; the combination is called SKModes in this paper. We also compared our algorithm with SKModes, as well as the state-of-the-art semi-supervised clustering algorithms for numeric data, which are constrained agglomerative hierarchical clustering algorithm in [5], named AggHie in this pa-per, and the algorithm based on Hidden Markov Random Fields [2], named HMRF-Kmeans in this paper. Both complete version (named SDHCC-M-C) and ablated version (named SDHCC-M) of SDHCC are used for comparison. In SDHCC-M, the cannot-link constraints are only used in the termination con-dition to justify whether a cluster should be split or not. We investigate the effectiveness of using instance-level const raint knowledge by comparing the clus-tering results of the semi-supervised algorithms with those of its underlying unsupervised algorithms.

In our performance evaluation, we adopt the F -measure as the clustering validation measure. In hierarchical clustering, the maximum is taken over all clusters i at all levels; in partitioning clustering, the maximum is taken over the K clusters. To evaluate the performance of the algorithms objectively, we used twofold cross-validation on each data set for 20 trials. In each trial, we randomly selected 50% of the instan ces as the test set, and the F -measure was calculated only on the test set. The remaining half of the data set was used as a training set, and the constraints were generated by randomly selectin g pairs of instances from the training set, creating a must-link constraint if the pair instances have the same label and a cannot-link constraint if they have different labels. The clustering algorithms were run on the whole data set, and the results given are the averages of the results of the 20 trials.
 4.1 Data Sets The four UCI data sets used in this paper are the Zoo , Congressional Voting Records ( Votes ), Wisconsin breast cancer ( Cancers )and Mushroom sets. The numbers of categorical values for these sets are J =36,48,91 and 117 respectively.
We also extracted two data sets from the 20-Newsgroup data, and both data sets are preprocessed by removing stop-words and very high-frequency and low-frequency words, the same as the methodology used in [6]. After preprocessing, each data set is prepared with two versions, one is numeric data using TF-IDF weighting method, for details, refer to [6]; the other is categorical (transactional) data, where each article is simply repre sented by collection of the words appear-ing in the article (after preprocessing). The description of the two data sets is as follows.
 Similar-2: this data set consists of 200 articles from two similar topics, i.e., comp.sys.ibm.pc.hardware and comp.sys.mac.hardware, and each topic has 100 articles. The data is represented in 1233 dimensions (words), and in the categor-ical version, J = 1233  X  2 as each word is analogous to one categorical attribute which takes two values indicating inclusion or non-inclusion of the word. Different-3: this data set consists of 300 articles from three different topics, i.e., alt.atheism, comp.sys.ibm.pc.hardw are and talk.politics.mideast, and each topic has 100 articles. The data is represented in 2470 dimensions (words), thus J = 2470  X  2 in the categorical version. 4.2 Results and Discussion Figure 1, 2, 3 and 4 show the clustering results on the Zoo , Votes , Cancers , and Mushroom data sets respectively. For SKModes, we set K =7 on Z oo ; K =3 on Votes and Cancers , as it fails to generate a clustering when the number of constraints is greater than 100 if we set K =2; K =4 on Mushroom for the same reason. For HMRF-Kmeans, we set K to the number of classes on the four data sets. AggHie and HMRF-Kmeans run on the Boolean indicator matrix of the categorical data sets.

From these figures we can see that the use of instance-level prior knowledge produces a remarkable improvement in SDHCC, except on the Cancers data set. The unsupervised algorithm (DHCC) already yields outstanding clustering results on Cancers ,withan F -measure attaining 0.97; thus, the instance-level constraints do not provide much informative knowledge to guide the cluster-ing process. On the Zoo and Votes data sets, SDHCC-M-C and SDHCC-M show quite comparable clustering performance, while on the Mushroom data set, which is of higher dimension and much larger size, SDHCC-M-C outper-forms SDHCC-M by a wide margin. For other three algorithms, none of them can reap potential benefit of prior knowledge on the four UCI data sets, ex-cept SKModes on the simple Zoo data set. Especially for AggHie, the clustering performance deteriorates when the instance-level constraints are incorporated.
Overall, our complete SDHCC algorithm shows superior clustering perfor-mance on the UCI real-life data. The values of F -measure on the four data sets are all greater than 0.9, which means the clusters discovered by SDHCC-M-C closely match the natural classification.

Figure 5 and 6 show the clustering results on Similar-2 and Different-3 data sets respectively. We set K =2 on Similar-2 and K =3 on Different-3 for SKModes and HMRF-Kmeans. In these figures, AggHie-N and HMRF-Kmeans-N indicate the results on numeric data, while AggHie-C and HMRF-Kmeans-C indicate the results on categorical data.

From these figures we can see that our algorithm can reap remarkable improve-ment even on the succinct categorical representation of text data, especially on the Different-3 data set. Similar-2 data impose more challenge than Different-3 on clustering algorithms, since the overlap between the two topics in Simila-2 is sig-nificant. SDHCC-M-C outperforms its ablated version SDHCC-M on the text data. HMRF-Kmeans demonstrates superior performance on numeric TF-IDF data; however, it gains much less improvement on the categorical data. SDHCC-M-C outperforms HMRF-Kmeans by a wide margin on categorical version of Different-3 data set. On the categorical Similar-2 data, SDHCC-M-C performs best in terms of the improvement of clustering quality. AggHie reaps potential benefit from prior knowledge on numeric TF-IDF data, however, its performance on categorical version deteriorates when the instance-level constraints are incor-porated, the same as it does on the UCI data set. For SKModes, the prior instance-level knowledge has little influence on the clustering performance. In this paper, we have proposed a semi-supervised divisive hierarchical cluster-ing algorithm for categorical data (SDHCC).We formalize clustering categorical data as optimizing the objective function, and exploit pairwise must-link and cannot-link constraint knowledge to guide the optimization process to a better solution in terms of satisfying the constraints, which would also be beneficial to the unconstrained instances. Exper imental results on UCI data sets and 20-Newsgroup text data demonstrate that our semi-supervised algorithm shows re-markable improvement over the unsupervised clustering algorithm DHCC. Most important, our semi-supervised clustering algorithm could take advantage of the potential improvement from a small amount of knowledge, which is very useful in real applications, as it is very expensive to provide large numbers of pairwise constraints from human experts.

Our experiments also show that the mains tream semi-supervised clustering al-gorithms for numeric data, such as AggHie and HMRF-Kmeans, are not suitable for categorical data. Incorporating the instance-level constraint knowledge even could deteriorate the clustering quality of the underlying unsupervised clustering algorithm.
