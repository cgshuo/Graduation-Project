 Department of Electrical &amp; Computer Engineering, University of Miami, Coral Gables, FL, USA 1. Introduction
The development of automated text categorization systems often assumes that instances of a relatively small subset of the documents have been pre-classi fi ed by human experts. From the obtained training to label the remaining documents.

One speci fi c aspect of text categorization is that each example can belong to two or more classes  X  a scienti fi c paper may at the same time represent medical studies, clinical research, and data analysis. Some researchers have therefore sought to enhance existing single-label techniques to the multi-label case (e.g. [1] modi fi ed to this end the paradigm of decision trees), but it is much more common to induce a separate binary classi fi er for each class, and then use these classi fi ers in parallel. Figure 1 illustrates the scenario: the original multi-label training set, T , is converted into N binary training sets, T i , one for each class label. In T i , each example is labeled with  X 1 X  or  X 0, X  depending on whether this example in T contains the i -th class label. For induction from T i , researchers have experimented with Bayesian decision theory [2 X 4], instance-based classi fi ers [5], support vector machines [6,7], and boosting approaches [8,9].

However, virtually all this previous work neglected two important factors. First, a text document is usually described by an attribute vector whose each element gives the frequency of a term or word. This means that thousands of attributes are used, most of which are irrelevant for a given class  X  a medical paper will rely on a different vocabulary than a computer-science paper. Since irrelevant attributes are known to impair induction performance of many machine-learning techniques, it is necessary to apply attribute-selection techniques separately to each T i .

The second issue is imbalanced representation of class labels. We observed that the negative examples in T i often signi fi cantly outnumber positive ones. This, too, is known to hurt machine learning [10] divides existing techniques dealing with this aspect into three groups: majority-class undersampling, less common than the other two). In text categorization, oversampling is impractical because the training sets are here already large enough even without the addition of new examples; moreover, its reliance on the replication of examples may lead to data over fi tting [11 X 13]. In view of these considerations, we opted for majority-class undersampling. In essence, the balance between the representation of the classes can be restored by the removal of randomly selected majority-class examples [12,14], an approach which, however, may end up discarding examples that carry valuable information [13,15,16]. We therefore preferred to work with the more sophisticated  X  X ne-sided selection X  (OSS) that seeks to remove preferably those examples that appear to be of lesser value [17].

This paper offers experimental evidence for the claim that, in the domains we dealt with (EUROVOC and RCV1-v2), an explicit treatment of these two aspects improves the performance of two generic techniques: k -nearest-neighbor classi fi ers ( k -NN) and support vector machines (SVM). The former was chosen because it is simple enough for us to explain its behavior under varying conditions; the latter was chosen for its popularity and well-known high performance.

The paper is organized as follows. Section 2 formally speci fi es the problem, including performance criteria. Section 3 provides details of the approach. Section 4 then describes the experiments, and Section 5 summarizes our fi ndings. 2. Formal problem statement and performance criteria
Let R p be an instance space where each example is described by a vector of p numeric attributes, be a fi nite set of documents, let Y be a fi nite set of class labels, and let each document, x i  X  X  ,be is to induce a classi fi er to carry out the mapping g : X X  2 Y in a way that maximizes classi fi cation performance.

The ways to quantify the classi fi cation performance deserve special attention. Suppose Y is the correct set of class labels of a certain document, and suppose that X is the set of class labels returned by the induced classi fi er. In principle, we want X to contain as many labels from Y as possible, and we want to minimize the occurrence in X of labels not in Y . The reader will agree that the classical error rate is ill-suited to capture these aspects. Much more appropriate are metrics commonly used in the fi eld of information retrieval .

To de fi ne these criteria, let us start with the simple case with only two class labels: positive and negative. Let us denote by TP (t rue p ositives) the number of positive examples correctly identi fi ed as such by the classi fi er; by FN (f alse n egatives), the number of positive examples that the classi fi er misclassi fi ed as negative; by FP (f alse p ositives), the number of negative examples misclassi fi ed as positive; and by TN (t rue n egatives), the number of correctly classi fi ed negative examples. These four quantities will de fi ne precision, Pr, and recall, Re, as follows:
Asuccessfulapplication ofan information-retrievalsystemrequires goodunderstanding ofthe interplay of these two variables. Which of them really matters depends on the user X  X  momentary needs and preferences. Most of the time, precision will be more critical because its high value indicates that most of the returned documents are relevant to the user X  X  query. Low precision means that many returned documents are irrelevant to the query, a circumstance that may discourage the user from using the system. By contrast, low recall signals that many relevant documents in the database have been overlooked, a situation that is often more easily tolerated  X  it is much less typical (though it sometimes happens) that the user wants all relevant documents, even if it means that many of the returned ones are irrelevant.
Observing that we often want to maximize both criteria while balancing their values [18], proposed a way to combine precision and recall in a single formula, F  X  , parameterized by the user-speci fi ed  X   X  [0 ,  X  ) that quanti fi es the relative importance ascribed to either metric: The reader can see that  X &gt; 1 apportions more weight to recall while  X &lt; 1 emphasizes precision . Moreover, F  X  converges to recall if  X   X  X  X  ,andto precision if  X  =0 . The situation where precision and recall are deemed equally relevant is re fl ected by the value  X  =1 , in which case F 1 degenerates to the following formula:
As for the needs of multi-label domains [19],proposed two alternative ways how to generalize the above criteria: (1) macro-averaging ,where precision and recall are fi rst computed separately for each category and then averaged; and (2) micro-averaging ,where precision and recall are obtained by summing over all individual decisions. Which of the two averaging methods is to be preferred depends on the speci fi c application: whereas micro-F 1 gives equal weight to each class, macro-F 1 weighs the classes according to their relative frequency. In our own experiments, we preferred the latter. The formulas are summarized in Table 1 where Pr i , Re i ,and F 1 ,i stand for precision , recall ,and F 1 for the i -th class.
For the sake of completeness, we have to mention that other performance metrics have been used in multi-label domains by [9 ,20,21]. We will not employ them here because they are a ppropriate only for classi fi ers that rank the returned documents by the perceived degree of relevance to the user X  X  query. 3. Proposed solution
We worked with two paradigms: k -nearest neighbor classi fi ers ( k -NN) and support vector machines (SVM). As mentioned above, k -NN was chosen because its simplicity facilitate s the explanation of the reasons behind its success or failure under speci fi c circumstances; moreover, its need for good choice of attributes and balanced class representation has been well-established (for the latter, see [17]); fi nally it has been used in text-categorization domains before [22,23]. Also SVM, one of the most powerful classi fi er-induction techniques in existence, has already been used in text-categorization [6,7,24]. While it is known to be much more robust than k -NN, we still wanted to see how it would respond to attribute selection and majority-class undersampling. 3.1. Attribute selection
In text-categorization, the relevance of individual attributes varies from class to class. We therefore applied attribute selection separately to each class.

Many algorithms for attribute selection exist [25]. For our own research, we needed a simple technique that would give good results with low computational costs. This is why we used a method advocated in our earlier research [26]. To be more speci fi c, we ordered all attributes according to the information gain they provide (with respect to the given binary class), and then used only the P highest-ranking ones. 1 How to set the threshold value of P was decided experimentally (see the next section). 3.2. The imbalanced-class problem
We relied on our own minor modi fi cation of the  X  X ne-sided selection X  (OSS) from [17]. Here, the decision which example to remove is based on the concept of Tomek links [28]. Let x and y be two examples such that each has a different class label, and let  X  ( x , y ) be the distance between them. The labels, then ( x , y ) constitutes a Tomek link.

An immediate consequence of two examples participating in a Tomek link is that they are likely to be either noisy or may fi nd themselves in the border region separating the two classes. Both cases are depicted by Fig. 2. The reader can see that a mislabeled example (noise) may confuse the classi fi er, while borderline examples are easily moved to the wrong side of the decision surface by even a small amount of attribute noise. Figure 3 shows a dataset before and after the removal of the noisy and borderline examples participating in Tomek links.

The original OSS [17] removed from the training set all majority-class participants of the Tomek links while retaining all minority class, arguing that the latter are too rare to be wasted, even if some of them are noisy. Figure 4 illustrates a dataset obtained by the removal of majority-class examples in OSS.
However, in our own earlier research [29], we were able to show that the technique treats minority-class examples as if they were all noise-free, an assumption that our text-categorization domains do not satisfy. The approach may thus result in a situation where false positive examples are retained while the possibly correct negative examples surrounding them are removed. We recti fi ed the situation by the following modi fi cation: if the positive example participating in a Tomek link is  X  X ery distant X  from any other positive (training) example, we suspect it is noisy, and therefore do not remove the negative-example part of the Tomek link.

Let us denote by x a positive example participating in a Tomek link, let us denote by y the the positive example nearest to x ,andlet  X  ( x , y ) be the distance between x and y .Let M be the number of negative examples whose distance from x is smaller than  X  ( x , y ) , and let the threshold T ( c ) decide whether the distance between the two nearest positive examples of class c is acceptable. To obtain T ( c ) ,we fi rst compute for each c the centroid of the positive class by averaging the attribute values of all positive examples. Then we order all examples according to their distances from the centroid. Let  X  denote the average number of negative examples between each pair of positive examples in this ordering, and let  X  be the one standard deviation from  X  . The adaptive threshold for class c is de fi ned as follows: where  X  denotes the ceiling function (e.g., 4 . 3 =5 ).
 In our modi fi ed OSS, we remove the negative participant of the Tomek link if and only if M&lt;T ( c ) . Figure 5 illustrates the two different cases: (1) positive example in Tomek link is close to other positive examples; (2) positive participant is suspected to be noisy. 3.3. The resulting system
Our stated intention was to observe to what extent attribute selection and majority-class undersampling would improve the performance of the induced classi fi ers in text-categorization domains. The overall procedure we followed is summarized by the pseudocode in Table 2.

In the fi rst step, we created for each class its own training set, T i . This set was then modi fi ed in the sense that we fi rst removed less relevant attributes, and then undersampled the majority class as explained in the previous sections. We thus obtained N training sets, one for each class. Each of them was used to set of examples and on a different set of attributes.

In the case of k -NN, when the system is presented with a document, x , it submits x  X  X  attribute vector k documents that have the smallest distance from x . 2 In case that there are two or more documents with the same distance, a random selection is made. If the majority of these selected examples are positive, then x is labeled with the i -class.

The SVM approach relies on statistical learning theory and structural risk minimization [30]. The essence is to construct the maximum-margin hyperplane that optimally separates the two classes. Apart from linear classi fi cation, SVM can easily be transformed for the needs of non-linear classi fi ers: in the fi rst step, the non-linear SVM maps input vectors into a higher-dimensional attribute space in which the data of different classes have a higher chance of being linearly separable; in the second step, the approach fi nds a linear decision boundary in this so-called feature space . This is equivalent to fi nding a non-linear separating surface in the input space. The  X  explosion X  in dimensi onality can be avoided by applying appropriate kernel functions to the input space. As a matter of fact, one of the main advantages of SVM is its ability to work with data of high dimensionality. SVM can induce a polynomial classi fi er, function. 4. Experimental evaluation 4.1. Experimental data
We experimented with two real-world domains: EUROVOC from our own earlier research [9], 3 and the publicly available Reuters Corpus, Volume 1, version 2 (RCV1-v2) [31]. This may not seem enough  X  the machine learning community usually expects experiments with a wide array of diverse domains. But while it is true that there are many public-domain datasets with multi-label examples, our choice was greatly constrained by our intention to focus on text categorization with numeric attributes. This is what determined our choice.

The size of the EUROVOC database renders systematic experimentation rather expensive because each single induction run on the complete set takes hours, even days  X  the usual hundreds of experiments needed for statistically justi fi ed conclusions would therefore take an impractical amount of time. We therefore followed the example of [9] and worked with a heavily simpli fi ed database containing only 10,000 documents described by 4,000 attributes, and labeled with 30 different classes. Each attribute speci fi es the frequency of a different term in the document.

The website containing RCV1-v2 has fi ve pairs of training and testing sets [31]. 4 Each training set (and each testing set) contains 3,000 documents described by 4,000 attributes randomly selected from those that have non-zero values in at least 6 documents. There were 101 different class labels. Each document is represented by a cosine-normalized, log TF-IDF attribute vector.

In most of the binary training sets obtained by the procedure summarized in Table 2, negative examples outnumber positive ones. Since the  X  X mbalanced-class X  problem is a signi fi cant aspect of our research, we need an appropriate metric to quantify its extent. Let n denote the total number of examples and let n  X  denote the number of positive examples. The  X  X egree of imbalance X  (recommended by [32]) is calculated as follows:
Note that, according to this formula, if 50% examples belong to one class and 50% to the other (perfect balance), the degree of imbalance is d =0 .Conversely, d approaches 1 in the case of highly imbalanced training sets where n  X  n .

Table 3 gives for each class in the EUROVOC dataset the number of representatives and the degree of imbalance calculated by Eq. (5). Evidently, some classes are only poorly represented. To make sure that N -fold cross-validation makes any sense here, we decided to ignore those classes whose d approaches 1 because these would inevitably be impractically underrepresented in the individual  X  X olds. X  To be more speci fi c, we eliminated the following classes: 10 , 15 , 19 , 20 , 22 , 24 , 26 , 27 .
The degree of imbalance in the RCV1-v2 binary sets is higher than in the EUROVOC data. We chose to work only with those classes that are represented by at least 50 documents. There were 53 such class labels, summarized in Table 4 (note that this table lists only the classes which we worked with). 4.2. Experiments
The parameter T ( c ) used in the  X  X odi fi ed Tomek-link X  technique given to each class c (see Section 3) was de fi ned by Eq. (4). To assist the choice of the concrete value for P , Figure 6 shows the inter-play between macro-F 1 and computational costs of k -NN ( k = 3) and SVM as measured on the two experimental datasets, EUROVOC and RCV1-v2 for different percentages of attributes retained. The reader can see that the run time for induction increases very fast with the growing number of attributes. Moreover, the best performance of both classi fi ers were obtained when the top 25% of attributes are retained. This is why we set P to 25% of the total number of attributes.

To assess statistical signi fi cance, we evaluated all comparisons using t -tests at the 5% signi fi cance level applied to 5-fold cross-validation. 4.2.1. Using the k -NN classi fi er
Our working hypothesis was that attribute selection and majority-class undersampling can improve the performance of the induced classi fi er. The task of the fi rst experiment is to put this hypothesis to test by comparing the following scenarios: (1) each k -NN classi fi er uses all attributes and all training examples; (2) each k -NN classi fi er uses all training examples described by attributes most relevant for this class; (3) only examples that survived the majority-class undersampling mechanism from Section 3 are used, each of them described by the most relevant attributes.
For the case of the EUROVOC data, Table 5 summarizes the performance in terms of macro-precision , macro-recall , and macro-F 1 . Throughout these experiments, the number of the voting nearest neighbors was fi xed at k =3 , a value indicated by auxiliary experiments (see below). The table shows the improvements in performance in two steps: fi rst, after attribute selection; then, after majority-class undersampling. Most important is perhaps the increased value of F 1 : between the subsequent steps (e.g., from k -NN to  X  X ttr+ k -NN X  and from  X  X ttr+ k -NN X  to  X  X ttr+sampling k -NN X ) the improvement proved statistically signi fi cant according to the t -test.

On the other hand, a more detailed look reveals that worsened precision is compensated by improved recall . The user must therefore weigh which of them is more important in the given application.
The results for RCV1-v2 are summarized in Table 6 (for k =5 ). The reader can see that they are very similar to those of the EUROVOC domain. The two steps (attribute selection and majority-class undersampling) led to statistically signi fi cant improvement in terms of F 1 . Again, the price for the considerably improved recall was lower precision .

The k -NN classi fi er X  X  performance usually varies with the value of k  X  the number of  X  X earest neighbors. X  To see how much this is the case in the domains addressed here, we repeated the above experiments for different choices of k and summarized the results in Fig. 7 for the EUROVOC domain, and in Fig. 8 for the RCV1-v2 domain. In the EUROVOC domain, larger values of k tended to reduce the classi fi cation performance. This is why in the previous round of experiments we used k =3 .The reader can see that the classi fi er is fairly robust against varied k in the RCV1-v2 domain, the optimum value is k =5 (this is how we chose this value in the above experiments).

Another aspect that needs justi fi cation is our decision to replace the OSS undersampling technique from [17] with the improved version described in Section 3. The graphs in Fig. 9 indicate that, regardless of the value of k , the adapted OSS always outperforms the original one in terms of the F 1 criterion, mainly because precision and recall have become more balanced. A more detailed look reveals that we obtained somewhat lower recall in exchange for better precision . In the original technique, recall was almost 100% but precision was low. In practical applications, low precision means that many documents returned in response to a user X  X  query will be irrelevant; the user may perceive this as a nuisance.
Figure 10 compares the macro-averaging metrics of the adapted OSS and the original one on RCV1-v2 domain. Again, the new version has higher precision and lower recall than the old one. However, in terms of F 1 , the original OSS performed better when we increased the value of k . We see that its precision increases quickly, counterbalancing the decrease in recall . This can be explained by the much higher sparsity of the examples in RCV1-v2 dataset compared to EUROVOC.

Finally, we wanted to see if it was reasonable to expect that attribute selection and undersampling would be more effective in classes that have a high degree of imbalance as quanti fi ed by Eq. (5). The graphs in Fig. 11 offer some guidance. The horizontal axis represents the degree of imbalance, and the vertical axis represents F 1 . Although the graphs do not offer any strong evidence either way, we do seem to observed somewhat stronger improvement in the case of more imbalanced classes. With RCV1-v2, undersampling seems to greatly in fl uence the performance of classes with degree of imbalance above 0.9. 4.2.2. Using the SVM classi fi er
Here we employed the publicly available system SVM light package [33] 5 where SVM uses the radial basis function as a kernel function. For the sake of simplicity, we used only default parameter setting.
When reporting experimental results, we will rely on the same principle of denotation as before: (1) svm (2) attr+svm and (3) attr+sampling+svm. The results are summarized in Table 7 for the EUROVOC data and in Table 8 for the RCV1-v2 data. In principle, we make the same observation as in the case of k -NN: undersampling improves (in both domains) recall with acceptable loss in precision .The improvements are signi fi cant according to the t -test.

Let us now ascertain whether the adapted OSS offered any improvement over the original OSS (as in the case of k -NN). Tables 9 and 10 summarizes the results of the corresponding experiments. We can see that along the F 1 criterion, the adapted OSS indeed outperforms the original one on both datasets.
Figure 12 shows the effect of attribute selection and undersampling on classes with different degrees of imbalance. In the case of EUROVOC domain, the improvement from attribute selection and un-dersampling does not seem to depend on the degree of imbalance. However, in the case of RCV1-v2 data, the performance of some rare classes did improve after the application of the two methods, though some already showed good results when SVM was used alone. Apparently, SVM is good at minimizing the negative impact of irrelevant attributes, and the technique is therefore much less sensitive to the imbalanced-class issue than other machine learning paradigms  X  the angle and offset of the separating hyperplane are rarely affected by the removal of noisy and borderline examples [34]. 5. Conclusion
The paper reported our experience with classi fi er induction in the fi eld of text categorization with multi-label examples. Classi fi cation performance in domains of this kind is usually evaluated along somewhat different criteria  X  precision , recall , F 1 , and their averages  X  than those typical of other machine-learning applications. Importantly, we concentrated here only on the prevalent (though not the only one possible) scenario where a separate binary classi fi er is induced for each class.
The research sought evidence to support (or refute) the hypothesis that the performance of the induced sets are heavily imbalanced, negative examples outnumbering the positive examples. The fi rst aspect can be addressed by attribute-selectio n techniques; the other is mitigated by majority-class undersampling.
We experimented with two real-world domains. One of them was the EUROVOC domain from our earlier research, the other is the Reuters Corpus, Volume 1, version 2, known underthe acronymRCV1-v2 and commonly used by the community for benchmarking purposes. Both domains are marked by great numbers of attributes and by class labels with diverse degrees of imbalance.

From the wide range of existing machine learning paradigms, we focused on two: nearest-neighbor classi fi ers ( k -NN) and support vector machines (SVM). We found that both of them bene fi t from our techniques, the improvement being essentially brought about by more balanced values of precision and recall . Experiments also showed that our system bene fi tted from the replacement of the original undersampling technique from [17] by our improved version.

The evidence for the principal hypothesis being provided, further research can focus on other ways to improve the performance. Trying to keep things simple, the research reported above relied on rather baseline choices: the Euclidean distance in the k -NN classi fi er, an almost trivial attribute-selection technique, and our own way of reducing the class imbalance in the training sets. It seems more than reasonable to assume that better alternatives can be found.

Moreover, future research should subject to more systematic scrutiny the question of what is the impact of the degree of imbalance on precision and recall , and it should study the ways how to offer the user more fl exible ways to control which of the two is more critical for his or her speci fi c needs.
Another limitation of our work is that it focused only on two paradigms, k -NN and SVM. We strongly advocate that other machine-learning frameworks be investigated along similar lines. Moreover, whereas we concentrated on text categorization, we assume that other domains with similar characteristics can be similarly affected.
 Acknowledgment The research was partly supported by the NSF grant IIS-0513702.
 References
