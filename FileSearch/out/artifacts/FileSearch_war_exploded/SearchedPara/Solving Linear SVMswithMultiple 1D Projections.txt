 We present a new methodology for solving linear Support Vector Machines (SVMs) that capitalizes on multiple 1D projections. We show that the approach approximates the optimal solution with high accuracy and comes with ana-lytical guarantees. Our solution adapts on methodologies from random projections, exponential search, and coordi-nate descent. In our experimental evaluation, we compare our approach with the popular liblinear SVM library. We demonstrate a significant speedup on various benchmarks. At the same time, the new methodology provides a compa-rable or better approximation factor of the optimal solution and exhibits smooth convergence properties. Our results are accompanied by bounds on the time complexity and accu-racy.
 I.5.3 [ Classification ]: Algorithms Support Vector Machines (SVM); Classification; Data Min-ing; Random Projections; Coordinate Descent
Classification is a key task in data analysis, and support vector machines (SVMs) belong to the state-of-art tech-niques for data classification. SVMs can be broadly catego-rized into linear and nonlinear (e.g., kernel-based). Linear SVMs fit linear boundaries in the original attribute space, whereas nonlinear SVMs first transform the data into a new, higher-dimensional space. Because linear boundaries in the transformed space correspond to nonlinear ones in the orig-inal space, nonlinear classification is made possible. Sev-eral research efforts have attested that nonlinear SVMs offer better classification accuracy than the linear ones [34, 17].  X  W ork conducted while at IBM Research -Zurich However, this comes at the expense of significantly higher computation cost [25], which renders them prohibitively ex-pensive for large datasets.

Nonlinear SVMs do not offer additional benefits for datasets having more features than observations, where classes are linearly separable. This scenario is very frequent in genomic studies, where one typically collects only few data samples that contain measurements for thousands of genes.
In general, linear SVMs are better suited: -when the data have more attributes than observations, then we do not really have enough data to fit a complex function, and a simple linear approach is most reasonable; -for Big Data analytics and for exploratory/interactive data analysis because of their superior performance, i.e. their ability to quickly extract rudimentary data statistics; and -in applications that require either a real-time response rate or conservation of energy resources (e.g., sensors). In such cases, obtaining a rough solution quickly may be prefer-able over getting an optimal solution slowly.
 In this work, we present approaches for speeding up linear SVMs further while maintaining accuracy. The main idea is based on the fact that many problems, while difficult in high-dimensional spaces, may indeed be solved optimally and in a simple manner in one dimension. Consider for example, the general K-Means problem; even though it is NP-hard in high dimensions, 2-Means clustering can be solved exactly and efficiently for a single dimension [5].

Support vector machines seek to maximize the margin be-tween the hyperplane that separates two classes. This prob-lem can be solved efficiently in 1D [30]. Therefore by com-bining multiple 1D solutions, we can progressively bound the error of the SVM solution. In general, we make the following contributions : 1. We present a new and simple-to-implement methodol-2. By working on 1D, we can reduce the computational 3. We compare the runtime and quality of these new algo-
In general, our methodology offers new directions for solv-ing linear SVM problems and owing to its low computational demands, it is particularly applicable on very large datasets.
The remainder of the paper is structured as follows: First we give the formulation of linear SVMs in high dimensions and review related work. In Section 3, we show how the linear SVM problem can be solved efficiently in the primal space on a single dimension and offer a complexity analysis. In Section 4, we combine multiple 1D SVMs and provide an error bound on the number of 1D projections that are re-quired. In Section 5, we present a more intelligent approach based on local search. Finally, in the experimental section, we provide scalability and accuracy comparisons against var-ious algorithms from the liblinear SVM library.
We describe the general formulation of linear SVMs and ways to solve the optimization problem. Assume that the training data consist of N points X = { x i | x i  X  R d , i  X  [1 ..N ] } , each point x i having a binary class label y { X  1 , 1 } . The goal is to compute a hyperplane H opt := ( w opt , b opt ) that separates the two classes with maximum margin as shown in Figure 1.
 Figure 1: A Linear Support Vector Machine classi-fi er for two classes.

The class of a point a for a hyperplane H := ( w, b ) is given by the sign of the term w  X  a + b . As point sets are in general not perfectly separable, each point x i is accompanied by a slack variable  X  i that accommodates misclassification of points. Formally speaking, we want to find ( w, b ) with w  X  R d and b  X  R such that we subject to
Parameter c determines the trade-off between misclassify-ing a point x i (which is the case for  X  i &gt; 0) and maximiz-ing the margin. Setting p = 1 we get the L1-SVM and for p = 2 the L2-SVM. Note that L1-SVM is a convex optimiza-tion problem, and L2-SVM is a strictly convex optimization problem.
 Solving high-dimensional SVMs: To solve the above optimization problem efficiently, often the dual formulation is considered. Prevalent methods for solving SVMs include coordinate descent techniques [6, 33, 12], cutting plane tech-niques [14], the finite Newton method [16], alternations be-tween stochastic gradient descent steps and projection steps [29], and the trust region Newton method [21]. Iterative, anytime SVM approaches using Cholesky decompositions are presented in [9]. A survey on large-scale linear classi-fication focusing on L1/L2 regularized L1/L2-loss SVM and logistic regression can be found in [35].

A key design choice is whether to optimize the primal or the dual formulation. Although there are many algorithms based on the dual SVM formulation, there are also argu-ments in favor of using the primal one [7].
 Using Random Projections: In our approach we use multiple 1D random projections. Random projections are often associated with the Johnson X  X indenstrauss Lemma [15]: points in a high-dimensional Euclidean space can be projected into a low-dimensional Euclidean space with tight bounds on distance preservation. A random projection L is a random vector originating from the origin and going to a randomly chosen point in the d -dimensional unit sphere. Random projections have been already used in the field of SVMs. Osadchy et al. [23] assume that the negative class is very large and approximate it with a Gaussian distribution. They separate the (1D) projection of this distribution from the positive samples using a  X  X ybrid X  prior. In the Krish-nan et al. [18] show that by using random projections with k = O (1 / X  2 log N ) dimensions one can approximate the op-timal SVM solution within a factor of (1 +  X  ). Boutsidis et al. [24] demonstrate that when using random projections for linear SVMs, the margin and minimum enclosing ball in the feature space are preserved within a small relative error. Finally, [26] uses random features to approximate nonlin-ear shift-invariant kernel functions with sinusoids randomly chosen from the Fourier transform of the kernel function.
Note also that for the above techniques, the recommended logarithmic (to the number of objects) projected dimension-ality may prove impractically high for Big Data applications. In contrast to the above, we only perform projections on 1D, where all operations (sorting, searching and distance com-putations) are faster and simpler.

To simplify notation and exposition of ideas, we focus on binary SVMs in the remainder of the paper, which can also be used as the basis for constructing multi-class SVMs [8, 28, 32].
W e first describe how linear SVMs can be solved efficiently on one dimension. The process basically resorts to a sorting of points. We use the primal formulation of the optimization problem as described in Section 2. Using a Lagrange multi-plier  X  i for each constraint in (2) and  X  i for each constraint in (3) results in the following Lagrange primal function for minimization l = || w 2 || / 2+ c X The Karush X  X uhn X  X ucker (KKT) conditions [4] provide the necessary conditions to minimize the Lagrange function. They also allow us to formally prove several interesting sim-plifications for the 1D case [30] 1 . The two most important observations are: i Let n + be the number of points of class 1 that are ii Let  X  + be the sum of all  X  i for the positive support Figure 2: One-dimensional SVM with optimal hy-p erplane H opt . To compute H opt , it suffices to con-sider all (up to) N/ 2 pairs of possible support vec-tors. Pair i consists of point i from class 1 and point i from class -1.

Therefore, it suffices for 1D SVMs to perform a linear scan through all sorted points, see Figure 2 for an example. One can commence from the two most distant points from each class, i.e., one point from class 1 and one point from class -1, and assume that they are the two support vectors. For these two points, the weight, the bias and the Lagrangian l are computed as follows:
The 1D weight w 1 d at iteration i is the midpoint of the current support vectors. To compute the bias b , one can use a constraint for the current support vectors, i.e., where P LS i is a support vector. To compute the sum of the slack variables  X  := P i  X  i , we use the equation
I n [30] Observation 4 states that  X  X ll positive support vec-tors have the same coordinate. X , which does not hold in gen-eral for non-linearly separable data. In fact, the authors mean that all points with y i  X  ( x i  X  w + b ) = 1 have the same coordinate. and simplify it using the above observations (i-ii), which leads to where n is the number of points in class 1, and S 1 ( i ) and S  X  1 ( i ) are sums of coordinates of points from class 1 and -1, respectively, as defined in line 5 of Algorithm SVM1D. Once the loss function (Lagrangian) is computed, the pre-viously considered points are removed. From the remaining ones, we choose again the two most distant points from each class. The procedure is repeated for all remaining pairs. The resulting solution is given by the pair of points resulting in the minimum loss l p (4).

The pseudocode of the above procedure is given in Algo-rithm SVM1D.
 Algorithm 1 S VM1D(input: points P , labels LA , projec-tion line L ; output: minimum weight w 1 d min , bias b min 2: Sort projected points P L 17: end if 18: end if 19: end for Now, we analyze the complexity of the SVM1D algorithm. W e prove that the algorithm returns the optimal result for a given direction. If the direction is far from the optimal direction, the solution might also be far from the overall optimal solution because the direction given is not modified by the algorithm.
 Theorem 3.1. Algorithm SVM1D executes in time O ( N log N + N  X  s ) for a projection L with s nonzero entries. It computes an optimal hyperplane ( w 1 d , b ) , with w 1 d for the given projection L .

Proof. Projecting all N points in d dimensions onto random line L with s nonzero entries requires N  X  s time. The sorting of the projected points requires O ( N log N ). Iterating through all N points (lines 7 X 19 in Algorithm SVM1D) requires time O ( N ). This yields an overall time complexity of O ( N log N + N  X  s ). To show correctness, let w opt ( L ) := w 1 d opt  X  L and b opt be an optimal separating hy-perplane for a given projection L minimizing equation (4), where w 1 d opt is the optimal scaling factor. Therefore a point x i s classified as either class 1 or class -1 using Therefore to solve the problem optimally for a given direc-tion L in high-dimensional space, we can derive a classifier for the projected points L  X  x i in one dimension.
Using Algorithm SVM1D, the linear support vectors can b e discovered optimally on 1D. The interested reader can find the proof in [30].

Now a question that naturally arises is: Given multiple 1D SVM solutions, can we somehow combine them to ap-proximate the original, high-dimensional hyperplane? We address this question in the next sections.
Here we examine how to combine multiple solutions of 1D random projections for solving the high-dimensional SVM problem. We perform an initial analysis that reveals a (pes-simistic) upper bound on the number of random projections required for approximating the linear SVM with arbitrarily high accuracy. Later we improve on this solution by per-forming a more intelligent search process.

Assume that we pick a random projection line of unit length, which serves as the normal vector of the hyperplane. We project all points onto the random normal vector. In the preceding section we showed how to solve the linear SVM problem optimally in one dimension. Thus, the problem reduces to finding two scalars for the hyperplane chosen: the optimal bias b opt and the optimal 1D weight w 1 d opt computed by algorithm SVM1D (Section 3). The weight is used to scale the random line chosen.

One can repeat the process of choosing a hyperplane, pro-jecting points and solving the 1D SVM problem such that in each iteration the hyperplane is chosen independently of any previously selected random planes. In this way, the data are projected multiple times, and also the 1D problem is solved multiple times. Random projections are chosen in-dependently, and the error (i.e., the loss function) for each trial is computed. The final result is the projection with the smallest loss. The process is summarized in Figure 3.
Each of the above iterations is very efficient. Choosing a random hyperplane and projecting all points onto the hyper-plane takes ( N +1)  X  d time. The simplest random projection is given by choosing each entry of the vector independently from the standard normal distribution N (0 , 1). There is no need for normalization, as we only consider the direction of the vector and not its magnitude. Computing the support vectors in one dimension is also computationally inexpen-sive, requiring O ( N log N ), as was shown in Theorem 3.1. Error Analysis : Here we provide probabilistic bounds on the trials required so that a chosen vector v is within an angle  X  0 of the optimal vector w opt (see Figure 4). The actual prediction error and its sensitivity depend on the margin of the classifier and therefore on the distribution of the data. Denote  X  ( a, b ) as the angle between vectors a and b .
Theorem 4.1. For any fixed vector a after at most 2 c 0  X  e Figure 3: Illustration of algorithm P rojectSVM for 8 points taken from two classes Algorithm 2 P rojectSVM(input: points X , class labels Y , maximal iterations maxIter ; output: weights w min , bias b 2: for i = 1 ..maxIter do 3: Choose a random projection line L  X  R d with each coor-9: end if 10: end for is such that  X  ( a , v ) &lt;  X  0 with probability 1  X  1 /N arbitrary constant c 0 .
 Proof. The proof can be found in the appendix.

The theorem basically states that the algorithm will find a solution that is arbitrarily close to the desired high-dimensional hyperplane, but the exploration space can be vast: we have an exponential dependence on the number of dimensions. The following section revisits the search pro-cess, and shows that the search space can be substantially pruned by accommodating an intelligent selection of the 1D hyperplanes.
In this section, we present our main algorithm LocalSVM , which improves on the preceding naive approach by selecting Figure 4: The angle  X  b etween the optimal hyper-plane for the point set and the hyperplane guessed by algorithm ProjectSVM . each projection line via intelligent local search. This ensures fast and smooth convergence.

The LocalSVM algorithm follows a similar outline as be-fore: points are projected onto a line, and the problem is solved optimally on that line. Now, however, we implement a local search that sequentially modifies each coordinate on the currently best projection known. This can be regarded as an alternative way of doing coordinate descent. Stan-dard techniques for coordinate descent solve a target prob-lem optimally for a single coordinate while keeping all the remaining coordinates fixed. Our approach, however, differs in the following way: We also modify the value of a single coordinate of the currently best hyperplane known to select a new direction. But then we consider arbitrary magnitudes in the new direction to determine its optimal solution, which modifies all coordinates of the current solution vector rather than a single one. This is illustrated by the example in Fig-ure 5. Note also that while our approach considers potential future directions with arbitrary lengths and small angle to the previous one, standard coordinate descent approaches generally consider rather shorter solutions with larger angle to the previous one. As we show in the experimental section, our approach leads to smoother convergence.

The main motivation for considering each coordinate sep-arately is computational efficiency. Recall that the complex-ity of our naive algorithm ProjectSVM is heavily influenced by the time spent on projecting points. Without any as-sumptions a single projection requires O ( N  X  d ) time. How-ever, we can exploit the linearity of the scalar product to reduce the time complexity. For a projection w composed of two other projections, i.e., w := w min + w  X  and any point x , we have Because w min  X  x k has been precomputed (in a prior iteration or before the first iteration), the time to compute w  X   X  x proportional to the nonzero entries in w  X  . Thus, choosing a sparse vector w  X  has a positive impact on the performance. For a projection onto a singular coordinate, there is only one nonzero entry in w  X  .
 Algorithm LocalSVM : Here we provide more details on the LocalSVM method. Let w denote the currently best direc-Figure 5: On the left, we show three iterations of t raditional coordinate descent techniques for an el-liptic function. The right panel shows two solution vectors of our local search approach. For each direc-tion w 0 , w 1 and w 2 , we compute the optimal solution in 1D, which yields points P 0 , P 1 and P 2 . tion. Each iteration i consists of one or two passes performed for each coordinate. In the first pass, t is added to the i -th coordinate of w to yield w  X  . Then the loss for the projected problem. If it significantly decreases the loss, the new direc-tion is scaled with the weights obtained from SVM1d ( w 1 d If not, a second pass is performed where the i -th coordinate is evaluated in the same way, now by subtracting instead of adding t from the i -th coordinate of w . It is not essential that t is added (or subtracted). A coordinate might as well be multiplied or divided by a value. However, it is important that the value is adjusted in the right manner, i.e. using an exponential scheme. The set of d iterations through all di-mensions denotes one phase . Following that, the algorithm commences the next phase and considers one coordinate af-ter the other, i.e., in the i -th iteration of the j -th phase, we modify the i -th coordinate. During a phase, the value t that is added to (or subtracted from) each coordinate of the solu-tion vector w remains fixed. After every phase, t is reduced or increased by a constant factor c t using an exponential-like search as follows: The value t is multiplied by either a factor 1 /c t or a factor c t . When in the prior phase t was multi-plied by v  X  { 1 /c t , c t } and there was a  X  X ufficient X  decrease of the loss in the current phase, then t is again multiplied by the same value v . If the change was not sufficient then we multiply by 1 /v . Note that if the loss is not sufficiently decreased in two subsequent phases the threshold thres in the Algorithm LocalSVM is decreased substantially to avoid that t repeatedly alternates between two values.
 Convergence : Now, we formulate the convergence proper-ties of the LocalSVM algorithm. We show that our method converges for any strictly convex function, such as the l function [1].

Theorem 5.1. For strictly convex functions f , the algo-rithm LocalSVM converges towards the optimal solution.
Proof. For f a strictly convex function, for any non-optimal point w there must be a t  X  &gt; 0 and a coordinate j such that where e j is the unit vector with all coordinates set to zero and coordinate j set to 1. Let w i be the best solution found up to phase i . Thus, if there is no improvement for Algorithm 3 L ocalSVM(input: points X , class labels Y , nIter ; output: weights w min , bias b min and loss l min 1: t : = 1 4: thres := 1 . 1 { Initial threshold for change in loss function } 8: NoLossDecrease := f alse 9: for i = 0 ..nIter do 10: coord := i mod d + 1 { Coordinate that is modified } 11: { Check if a new phase begins } 12: if coord == 1 and i &gt; 0 then 14: if NoLossDecrease == true then 16: curr := 1 /c t 17: else 18: curr := 1 /curr 19: end if 20: NoLossDecrease := true 21: else 22: NoLossDecrease := f alse 23: end if 24: t := t  X  curr 26: end if 27: isMinDir := f alse 34: isMinDir := true 38: end if 39: end for 40: end for phase i t hen w i = w i  X  1 . Define an unsuccessful phase as a phase in which the loss has not decreased  X  X ufficiently X : tive unsuccessful phases starting at phase i . Let scale i noted as t in the algorithm) and thres i be the values used in phase i of the algorithm LocalSVM. For u = 2, the thresh-old (minus 1) is decreased by c 4 t with c t &gt; 1: thres 1 + ( thres i  X  1) /c 4 t . Furthermore, the current update value by a factor of 1 /c t . To get an upper bound for scale i +2 assume that scale was increased by c t in phase i and de-creased by c t in phase i + 1, thus 0 &lt; scale i +2  X  scale For u = 4, we have that thres and scale are both decreased: 0 &lt; scale i +4  X  scale i /c 4 t and thres i +4 := 1+( thres This pattern repeats for consecutive unsuccessful phases, i.e., for u &gt; 4 we get 0 &lt; scale i + u  X  scale i /c scale must be smaller than t  X  which yields the function f decreased. As thres  X  1 decreases much faster than scale (by a factor c 4 t versus c 2 t for two consecutive unsuccessful phases), we have that thres reaches 1 much faster than scale reaches 0. Eventually, for some phase j , the (rela-Figure 6: Illustration of Algorithm L ocalSVM . After computing the optimal hyperplane for the initial di-rection v , one coordinate of the currently best solu-tion known is modified in each iteration. At Itera-tions 2 and 3, the x -coordinate is changed by a fixed magnitude t . In Iterations 4 and 5, the y coordinate is changed by the same t . At Iteration 6, the process repeats for a smaller t := t/c t = t/ 2 . At Iteration 8, the classifier for v  X  becomes the new best classifier. At Iteration 9, the x -coordinate is changed by t . tive) change of function f fo r some coordinate is larger than Then for some k , scale j + k  X  scale j and all r  X  [ j, j + k ], there exists a coordinate i such that f ( w r + scale r e i )  X  thres &lt; f ( w r  X  1 ) or f ( w r  X  scale r e i )  X  thres &lt; f ( w r  X  1 ). In other words, in each phase r  X  [ j, j + k ], the solution w gets improved. Eventually, thres gets arbitrarily close to 1, and the current best solution w r gets arbitrarily close to the optimal solution w opt .

Above, we showed the convergence for (strictly) convex fu nctions [22, 12, 33, 6]. Note, however, that it is hard and still largely an open problem to prove the rate of convergence for cyclic coordinate search in general [22].

As we will show through extensive evaluations in the fol-lowing section, the runtime of LocalSVM is superior to that of the naive approach ProjectSVM . This makes it an attractive method for solving large-scale linear SVM problems.
Here we evaluate the runtime and accuracy of the pro-posed methods. We compare their performance with that of two algorithms for linear SVMs implemented in the lib-linear framework [10]:
Our algorithms are the naive ProjectSVM and the more intelligent LocalSVM . All code is implemented in Java and experiments have been conducted on a 2.6 GHz Intel CPU with 16 GB RAM 2 .
 Datasets: We consider a variety of datasets that have been used for evaluating linear binary classifiers from the UCI machine learning repository 3 and datasets that come with the liblinear framework [10]. 4 We purposefully focus on dense data, that is, data with (mainly) nonzero values, so as not to give an unfair advantage to our approach; because sparse high-dimensional data can be compressed efficiently using random projections with very little loss of accuracy [2, 20]. A summary of the datasets used in our experiments is given in Table 1.
 Benchmark set-up: For each algorithm, we run each we used the parameters stated in the pseudocodes. For the solvers from liblinear , we retained the default settings.
J ava is a registered trademark of Oracle and/or its affili-ates. Intel is a registered trademark of Intel Corporation or its subsidiaries in the United States and other countries. http://archive.ics.uci.edu/ml/datasets.html www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/ Figure 7: Scalability experiment for increasing num-b er of data objects. Figure 8: Scalability experiment for increasing data d imensionality.
 Scalability: We examine scalability both for increasing number of objects and for increasing data dimensionality using a fixed number of iterations.

Figure 7 illustrates the first experiment, in which we evaluate runtime against increasing object cardinality (for fixed dimensionality). For this experiment, we used the  X  X ecord Linkage Comparison Patterns X  dataset 3 , which con-tains more than 5 million people records. We observe that the runtime of our two methods is not substantially differ-ent from that of the competing approaches. Time complex-ity per object is higher for LocalSVM than for ProjectSVM because of the point sorting involved in the former. Note that for this experiment we do not report the classification accuracy, which may have been different for each approach. We examine this trade-off between speed and accuracy in the following section. The goal of this first experiment was merely to show that the techniques presented exhibit a run-time that is on par with existing approaches.

In the second experiment, we fix the number of objects and test the runtime for increasing data dimensionality. For this experiment, we used the Gisette dataset because it is the one with the highest dimensionality. Figure 8 summarizes the results. We note the excellent performance of our ap-proaches, and in particular of the LocalSVM approach, whose runtime remains at the same level when increasing the num-ber of dimensions. The time complexity of algorithm Pro-jectSVM grows linearly with the number of points and di-mensions, as every additional dimension must be taken into account in every projection (iteration).

In summary, because our techniques always operate on the projected 1D space, their performance is not substantially affected by the data dimensionality. Therefore, we believe that the algorithms presented constitute good candidates for disciplines faced with high-dimensional data (medicine, multimedia, etc).
 Accuracy versus time: The motivation of our work is to provide faster (approximate) solutions to linear SVMs while not compromising the accuracy. Figure 9 shows the trade-off between accuracy and time for four exemplary datasets, and for up to 1024 iterations per run for each method. For all benchmarks, our LocalSVM algorithm outperforms both solvers of the liblinear framework in the accuracy range above 65  X  70%.

What is also noteworthy is that LocalSVM exhibits very smooth convergence properties. This is clearly depicted in all four graphs of Figure 9. In contrast, the general accu-racy behavior of the liblinear solvers exhibits an oscillating pattern. We define the smoothness  X  as follows: For each benchmark, we run each algorithm for a sequence of itera-tions, i.e., 2, 4, 8, etc. This yields a sequence of accuracies a := ( a 0 , a 1 , ..., a m ). The smoothness is given by where max( a )  X  min( a ) is the difference between minimum and maximum accuracy. Table 2 lists the smoothness  X  across all datasets. In the table, we present normalized  X  values, where values closer to 1 (one) are better and higher numbers indicate worse results.

Finally, we run an experiment in which we let each tech-nique run until it reaches up to 1% of the optimal accuracy or until a maximum runtime of 1 min is reached. This is similar to the experiment conducted in [12]. In Table 2, we report normalized accuracy results, where 100 indicates that the method achieved optimal accuracy. Note that for the majority of datasets, LocalSVM achieves a better accu-racy than the other techniques.
Previous work on random projections examined the theo-retical dimensionality on which to project high-dimensional data so that the SVM problem can be solved faster and with provable guarantees [24, 18]. In practice, the recommended logarithmic (to the number of points) projected dimension-ality may prove impractically high for Big Data applications. In this work, we have shown that performing multiple one-dimensional projections provides fast and provably good re-sults for linear SVMs. Further contributions of this work include the following: 1. Analytical bounds on the approximation quality of our 2. Extensive empirical comparison with algorithms from
Finally, our method has two direct and exciting implica-tions: a) Distributed SVM execution, where each execution site holds a different 1D projection, and b) Inherent support for data obfuscation and data privacy. Because each site only has access to its individual 1D projection, it becomes very challenging to make inferences about the original high-dimensional data.
 Acknowledgements: The research leading to these re-sults has received funding from the European Research Council under the European Union X  X  Seventh Framework Programme (FP7/2007-2013) / ERC grant agreement no. 259569. [1] S. Abe. Analysis of support vector machines. In [2] D. Achlioptas. Database-friendly random projections: [3] J. Bentley and A. C.-C. Yao. An almost optimal [4] S. Boyd and L. Vandenberghe. Convex Optimization . [5] P. Brucker. On the complexity of clustering problems. [6] K.-W. Chang, C.-J. Hsieh, and C.-J. Lin. Coordinate [7] O. Chapelle. Training a support vector machine in the [8] K. Crammer and Y. Singer. On the Algorithmic [9] D. DeCoste. Anytime query-tuned kernel machines via [10] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, [11] I. Guyon, S. Gunn, A. Ben-Hur, and G. Dror. Result [12] C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, [13] C.-W. Hsu, C.-C. Chang, C.-J. Lin, et al. A practical [14] T. Joachims. Training linear SVMs in linear time. In [15] W. B. Johnson and J. Lindenstrauss. Extensions of [16] S. S. Keerthi and D. DeCoste. A modified finite [17] S. S. Keerthi and C.-J. Lin. Asymptotic Behaviors of [18] S. Krishnan, C. Bhattacharyya, and R. Hariharan. A [19] B. Laurent and P. Massart. Adaptive estimation of a [20] E. Liberty and S. W. Zucker. The mailman algorithm: [21] C.-J. Lin, R. C. Weng, and S. S. Keerthi. Trust region [22] Y. Nesterov. Efficiency of coordinate descent methods [23] M. Osadchy, D. Keren, and B. Fadida-Specktor. [24] S. Paul, C. Boutsidis, M. Magdon-Ismail, and [25] J. C. Platt. In Advances in Kernel Methods . MIT [26] A. Rahimi and B. Recht. Random features for [27] B. P. Roe, H.-J. Yang, J. Zhu, Y. Liu, I. Stancu, and [28] F. Schwenker. Hierarchical support vector machines [29] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: [30] Y. Su, T. Murali, V. Pavlovic, and S. Kasif. Training [31] A. V. Uzilov, J. M. Keegan, and D. H. Mathews. [32] J. Weston and C. Watkins. Multi-class support vector [33] Z. Yu, J.-B. Thibault, K. Sauer, C. Bouman, and [34] G.-X. Yuan, C.-H. Ho, and C.-J. Lin. An improved [35] G.-X. Yuan, C.-H. Ho, and C.-J. Lin. Recent advances Proof of Theorem 4.1 :
L et a := ( a 0 , ..., a d  X  1 ) with || a || = 1 and let v := ( v 0 , v 1 , ..., v d  X  1 ) be the vector created by choosing each co-ordinate v i independently from N (0 , 1). Then and
V ar [ a  X  v ] = V ar [ X
Consider the Chi-square distribution with k = d degrees of freedom: u := P i  X  0 v 2 i . Using bounds from [19] (see equation (4.3)) it holds for all t &gt; 0. In particular for t = 1, we get
Therefore, with probability larger than 1 / 2.
 For any variable x drawing from N (0 , 1) and z &gt; 0: As a  X  v  X  N (0 , 1), it follows a  X  v &gt; z with probability at
The probability of both a  X  v &gt; z and || v || &lt; p 2 is therefore at least 1 / 2  X  e  X  z 2 . Then it holds that
Let  X  0 &lt; 1 be a small positive value and set  X  =  X  2 0  X  . Using cos  X  1 (1  X   X  )  X  1  X   X  2 0 / 3, i.e., z = (1  X   X  2 0 / 3) p 2 exp(  X  ((1  X   X  2 0 / 3) 2 (2 domly of N (0 , 1). Let X denote the event  X  ( a, v ) &lt;  X  Setting n = 2  X  exp((1  X   X  2 0 / 3) 2 (2 ing to the Binomial distribution for a constant c 0 we expect at least c 0 log( N ) vectors v such that  X  ( a, v ) &lt;  X  the Chernoff bound for X  X  B ( n ; p ) w ith k = 1, we get
From here, it follows that when choosing 2  X  exp((1  X   X  / 3) 2 (2 the probability that at least one of them is such that  X  ( a, v ) &lt;  X  0 is at least 1  X  1
