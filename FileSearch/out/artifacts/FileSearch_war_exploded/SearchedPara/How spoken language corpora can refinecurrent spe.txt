 Spoken language corpora are often accessed by linguists, who need to manipulate specifically de-fined speech stimuli in their experiments. How-ever, this valuable resource of linguistic informa-tion has not yet been systematically applied for the benefit of speech therapy methodologies. This is not surprising, considering the fact that spoken language corpora have only appeared relatively re-cently, and are still not easily accessible outside the NLP community. Existing applications for selecting linguistic stimuli, although undoubtedly useful, are not based on spoken language data, and are generally not designed for utilization by speech therapists per se (Aichert et al., 2005). As a first attempt to bridge this gap, a mechanism is proposed for utilizing the relevant linguistic in-formation to the service of clinicians. In coor-dination with speech pathologists, the domain of speech motor training was identified as an appro-priate area of application. The traditional speech motor programs are based on a rather static inven-tory of speech items, and clinicians do not have access to a modular way of selecting speech tar-gets for training.
 Therefore, in this project, we deal with develop-ing an interactive interface to assist speech thera-pists with constructing individualized speech mo-tor practice programs for their patients. The prin-cipal innovation of the proposed system in re-gard to existing stimuli selection applications is twofold: first, the syllabic inventories are derived from spoken word forms, and second, the selec-tion interface is integrated within a broader plat-form for conducting speech motor practice. 2.1 Speech Motor Disorders Speech motor disorders (SMD) arise from neuro-logical impairments in the motor systems involved in speech production. SMD include acquired and developmental forms of dysarthria and apraxia of speech. Dysarthria refers to the group of disor-ders associated with weakness, slowness and in-ability to coordinate the muscles used to produce speech (Duffy, 2005). Apraxia of speech (AOS) is referred to the impaired planning and program-ming of speech (Ziegler , 2008). Fluency dis-orders, namely stuttering and cluttering, although not always classified as SMD, have been exten-sively studied from the speech motor skill perspec-tive (Van Lieshout et al., 2001). 2.2 Speech Motor Training The goal of speech therapy with SMD patients is establishing and maintaining correct speech mo-tor routines by means of practice. The process of learning and maintaining productive speech mo-tor skills is referred to as speech motor training. An insightful design of speech motor training ex-ercises is crucial in order to achieve an optimal learning process, in terms of efficiency, retention, and transfer levels (Namasivayam, 2008).
 Maas et al. (2008) make the attempt to relate find-ings from research on non-speech motor learning principles to the case of speech motor training. They outline a number of critical factors in the de-sign of speech motor exercises. These factors in-clude the training program structure, selection of speech items, and the nature of the provided feed-back.
 It is now generally agreed that speech motor exer-cises should involve simplified speech tasks. The use of non-sense syllable combinations is a gener-ally accepted method for minimizing the effects of higher-order linguistic processing levels, with the idea of tapping as directly as possible to the motor component of speech production (Smits-Bandstra et al., 2006) . 2.3 Selection of speech items The main considerations in selecting speech items for a specific patient are functional relevance and motor complexity. Functional relevance refers to the specific motor, articulatory or phonetic deficits, and consequently to the treatment goals of the patient. For example, producing correct stress patterns might be a special difficulty for one patient, while producing consonant clusters might be challenging for another. Relative motor com-plexity of speech segments is much less defined in linguistic terms than, for example, syntactic com-plexity (Kleinow et al., 2000). Although the part-whole relationship, which works well for syntactic constructions, can be applied to syllabic structures as well (e.g.,  X  X lake X  and  X  X ake X ), it may not be the most suitable strategy.
 However, in an original recent work, Ziegler presented a non-linear probabilistic model of the phonetic code, which involves units from a sub-segmental level up to the level of metrical feet (Ziegler , 2009). The model is verified on the basis of accuracy data from a large sample of apraxic speakers, and thus provides a quantitive index of a speech segment X  X  motor complexity. Taken together, it is evident that the task of se-lecting sets of speech items for an individualized, optimal learning process is far from obvious, and much can be done to assist the clinicians with go-ing through this step. The syllable is the primary speech unit used in studies on speech motor control (Namasivayam, 2008). It is also the basic unit used for con-structing speech items in current methodologies of speech motor training (Kent, 2000). Since the choice of syllabic tokens is assumed to affect speech motor learning, it would be beneficial to have access to the syllabic inventory of the spoken language. Besides the inventory of spoken sylla-bles, we are interested in the distribution of sylla-bles across the language. 3.1 Syllable frequency effects The observation that syllables exhibit an exponen-tial distribution in English, Dutch and German has led researchers to infer the existence of a  X  X en-tal syllabary X  component in the speech production model (Schiller et al., 1996). Since this hypothesis assumes that production of high frequency sylla-bles relies on highly automated motor gestures, it bears direct consequences on the utility of speech motor exercises. In other words, manipulating syl-lable sets in terms of their relative frequency is ex-pected to have an effect on the learning process of new motor gestures. This argument is supported by a number of empirical findings. In a recent study, Staiger et al. report that syllable frequency and syllable structure play a decisive role with re-spect to articulatory accuracy in the spontaneous speech production of patients with AOS (Staiger et al., 2008). Similarly, (Laganaro, 2008) con-firms a significant effect of syllable frequency on production accuracy in experiments with speakers with AOS and speakers with conduction aphasia. 3.2 Implications on motor learning In that view, practicing with high-frequency sylla-bles could promote a faster transfer of skills to ev-eryday language, as the most  X  X equired X  motor ges-tures are being strengthened. On the other hand, practicing with low-frequency syllables could po-tentially promote plasticity (or  X  X tretching X  ) of the speech motor system, as the learner is required to assemble motor plans from scratch, similar to the process of learning to pronounce words in a for-eign language. In the next section, we describe our study with the Spoken Dutch Corpus, and il-lustrate the performed data extraction strategies. The Corpus Gesproken Nederlands (CGN) is a tains manually verified phonetic transcriptions of 53,583 spoken forms, sampled from a wide vari-ety of communication situations. A spoken form reports the phoneme sequence as it was actually uttered by the speaker as opposed to the canonical form, which represents how the same word would be uttered in principle. 4.1 Motivation for accessing spoken forms In contrast to written language corpora, such as CELEX (Baayenet al., 1996), or even a corpus like TIMIT (Zue et al., 1996), in which speak-ers read prepared written material, spontaneous speech corpora offer an access to an informal, un-scripted speech on a variety of topics, including speakers from a range of regional dialects, age and educational backgrounds.
 Spoken language is a dynamic, adaptive, and gen-erative process. Speakers most often deviate from the canonical pronunciation, producing segment reductions, deletions, insertions and assimilations in spontaneous speech (Mitterer, 2008). The work of Greenberg provides an in-depth account on the pronunciation variation in spoken English. A de-tailed phonetic transcription of the Switchboard corpus revealed that the spectral properties of many phonetic elements deviate significantly from their canonical form (Greenberg, 1999).
 In the light of the apparent discrepancy between the canonical forms and the actual spoken lan-guage, it becomes apparent that deriving syllabic inventories from spoken word forms will approxi-mate the reality of spontaneous speech production better than relying on canonical representations. Consequently, it can be argued that clinical ap-plications will benefit from incorporating speech items which optimally converge with the  X  X ive X  re-alization of speech. 4.2 Syllabification of spoken forms The syllabification information available in the CGN applies only to the canonical forms of words, and no syllabification of spoken word forms exists. The methods of automatic syllabification have been applied and tested exclusively on canonical word forms (Bartlett, 2007). In order to obtain the syllabic inventory of spoken language per se, a preliminary study on automatic syllabification of spoken word forms has been carried out. Two methods for dealing with the syllabification task were proposed, the first based on an n-gram model defined over sequences of phonemes, and the sec-ond based on statistics over syllable units. Both algorithms accept as input a list of possible seg-mentations of a given phonetic sequence, and re-turn the one which maximizes the score of the spe-cific function they implement. The list of possible segmentations is obtained by exhaustively gener-ating all possible divisions of the sequence, satis-fying the condition of keeping exactly one vowel per segment. 4.3 Syllabification Methods The first method is a reimplementation of the work of (Schmid et al., 2007). The authors describe the syllabification task as a tagging problem, in which each phonetic symbol of a word is tagged as ei-ther a syllable boundary ( X  X  X ) or as a non-syllable boundary ( X  X  X ). Given a set of possible segmenta-tions of a given word, the aim is to select the one, viz. the tag sequence  X  b n 1 , which is more proba-ble for the given phoneme sequence p n 1 , as shown in equation (1). This probability in equations (3) is reduced to the joint probability of the two se-quences: the denominator of equation (2) is in fact constant for the given list of possible syllabifica-tions, since they all share the same sequence of phonemes. Equation (4) is obtained by introduc-ing a Markovian assumption of order 3 in the way the phonemes and tags are jointly generated  X  b n 1 = arg max
The second syllabification method relies on statistics over the set of syllables unit and bi-gram (bisegments) present in the training corpus. Broadly speaking, given a set of possible segmen-tations of a given phoneme sequence, the algo-rithm, selects the one which maximizes the pres-ence and frequency of its segments. 4.4 Results The first step involved the evaluation of the two algorithms on syllabification of canonical word forms. Four corpora comprising three different languages (English, German, and Dutch) were evaluated: the CELEX2 corpora (Baayenet al., 1996) for the three languages, and the Spoken Dutch Corpus (CGN). All the resources included manually verified syllabification transcriptions. A 10-fold cross validation on each of the corpora was performed to evaluate the accuracy of our meth-ods. The evaluation is presented in terms of per-centage of correctly syllabified words.
 Table 1 summarizes the obtained results. For the CELEX corpora, both methods produce almost equally high scores, which are comparable to the state of the art results reported in (Bartlett, 2007). For the Spoken Dutch Corpus, both methods demonstrate quite high scores, with the phoneme-level method showing an advantage, especially with respect to correctly syllabified words. 4.5 Data extraction The process of evaluating syllabification of spo-ken word forms is compromised by the fact that there exists no gold annotation for the pronuncia-tion data in the corpus. Therefore, the next step involved applying both methods on the data set and comparing the two solutions. The results re-vealed that the two algorithms agree on 94.29% of syllable boundaries and on 90.22% of whole word syllabification. Based on the high scores re-ported for lexical word forms syllabification, an agreement between both methods most probably implies a correct solution. The  X  X isagreement X  set can be assumed to represent the class of ambigu-ous cases, which are the most problematic for au-tomatic syllabification. As an example, consider the following pair of possible syllabification, on which the two methods disagree:  X  X El-kOm-pjut X  Motivated by the high agreement score, we have applied the phoneme-based method on the spo-ken word forms in the CGN, and compiled a syl-labic inventory. In total, 832,236 syllable tokens were encountered in the corpus, of them 11,054 unique syllables were extracted and listed. The frequencies distribution of the extracted syllabary, as can be seen in Figure 1, exhibits an exponential curve, a result consistent with earlier findings re-ported in (Schiller et al., 1996). According to our statistics, 4% of unique syllable tokens account for 80% of all extracted tokens, and 10% of unique syllables account for 90% respectively. For each extracted syllable, we have recorded its structure, frequency rank, and the articulatory characteristics of its consonants. Next, we describe the speech items selection tool for clinicians.
 Figure 1: Syllable frequency distribution over the spoken forms in the Dutch Spoken Corpus.
 The x-axis represents 625 ranked frequency bins. The y-axis plots the total number of syllable to-kens extracted for each frequency bin. In order to make the collected linguistic informa-tion available for clinicians, an interface has been built which enables clinicians to compose individ-ual training programs. A training program con-sists of several training sessions, which in turn consists of a number of exercises. For each ex-ercise, a number of syllable sets are selected, ac-cording to the specific needs of the patient. The main function of the interface, thus, deals with selection of customized syllable sets, and is de-scribed next. The rest of the interface deals with the different ways in which the syllable sets can be grouped into exercises, and how exercises are scheduled between treatment sessions. 5.1 User-defined syllable sets The process starts with selecting the number of syllables in the current set, a number between one and four. Consequently, the selected number of  X  X yllable boxes X  appear on the screen. Each box allows for a separate configuration of one syllable group. As can be seen in Figure 2, a syllable box contains a number of menus, and a text grid at the bottom of the box.
 Figure 2: A snapshot of the part of the interface allowing configuration of syllable sets Here follows the list of the parameters which the user can manipulate, and their possible values: Once the user selects a syllable type, he/she can further specify each consonant within that syllable type in terms of voiced/unvoiced segment choice and manner and place of articulation. For the sake of simplicity, syllable frequency ranks have been divided in three rank groups. Alternatively, the user can bypass this criterion by selecting  X  X ny X . As the user selects the parameters which define the desired syllable type, the text grid is continuously filled with the list of syllables satisfying these cri-teria, and a counter shows the number of syllables currently in the grid.
 Once the configuration process is accomplished, the syllables which  X  X urvived X  the selection will constitute the speech items of the current exercise, and the user proceeds to select how the syllable sets should be grouped, scheduled and so on. 6.1 Future directions A formal usability study is needed in order to establish the degree of utility and satisfaction with the interface. One question which demands inves-tigation is the degrees of choice that the selection tool should provide. With too many variables and hinges of choice, the configuration process for each patient might become complicated and time consuming. Therefore, a usability study should provide guidelines for an optimal design of the interface, so that its utility for clinicians is maximized.
 Furthermore, we plan to integrate the proposed interface within an computer-based interactive platform for speech therapy. A seamless integra-tion of a speech items selection module within biofeedback games for performing exercises with these items seems straight forward, as the selected items can be directly embedded (e.g., as text symbols or more abstract shapes) in the graphical environment where the exercises take place. Acknowledgments This research is supported with the  X  X osaic X  grant from The Netherlands Organisation for Scientific Research (NWO). The authors are grateful for the anonymous reviewers for their constructive feedback.

