 A large volume of data with different storage systems, multiple formats and all manner of internal complexity can often hide more than it reveals to the data mining techniques, focused on building descriptive or/and predictive computational models [1]. There are several measures of model quality [2], with the accuracy of predictions remaining as a key measure of model quality, rather than the theory that may explain the phenomena . However, many areas require deeper understanding of the phenomena. In addition to the explicitly coded relationships, there often are implicit relationships between the entities described by the data set, especially in the realm of transactions data. The structure of such relationships between the individual entities can be revealed by network models . During recent years there has been an increasing input from physicists [3], mathematicians [4] and organisational scientists [5] to network research, with the focus shifting to large scale networks (with millions of links and nodes), their statistical propertie s and explanatory power, the discovery of such models and their use in explaining different phenomena. This change of scale of the network models indicates a need for change in the analytics approach [4]. 
Network Data Mining (NDM) addresses this challenge. We define network data mining as the process of discovering emergent network patterns and models in large and complex data sets . NDM addresses the  X  X oss of detail X  problem and the  X  X ndependency of attributes X  assumption in predictive modelling. In many areas (e.g. purpose of the analysis as it is often in the detail where the most valuable information is hidden [6]. The  X  X ndependency of attributes X  assumption is accepted in many classifier building algorithms, for example, Na X ve Bayes techniques [7]. The logic is clear: by missing detail or making the wrong assumptions or simply by being unable to define what is normal, an organisation that relies solely upon predictive data mining may fail to discover critical information buried in its data. Further we present a human-centred knowledge discovery methodology, that addresses these issues, and a case study that follows this methodology and illustrates the solutions that the network data mining approach and technology offers. The overall NDM process is illustrated in Fig. 1. The techniques supporting this methodology are implemented in the NetMap visual analytics engine 1 .The main NDM methodological steps and accompanying assumptions approach include: Specify sources of data and modelling : NDM integrates data in order to obtain a single address space, a common view, for disparately sourced data. Having decided the sources, multiple data models are created depending on the input fields. Visualisation and Generation of Visual Models : Visualisation of entities and links consists of a set of various consistent visual models that facilitate the discovery capabilities of the analyst  X  essential when dealing with millions data points.  X  X rain of thought X  analysis : The analyst acts similarly to Donald Sch X n X  X   X  X eflective practitioner X  [8]. Data miners put together visual pieces of information and create new chain of inquiries interacting with the network slices of the data set. Cognition and sense-making : Intuition and cognition are integral, and are harnessed in the analytical process (an argument well supported in [9]). Discovery and remapping : An emergent process, not prescriptive one. To realise its full value the discovery phase needs to be repeated at regular intervals so that new exception detection processes. Reselection and converting patterns to knowledge : Any linkage pattern observed on screen is simply an observation of potential interest. For example, in retail, the perpetrators of a scam had taken it in turns to report levels of refunds always just under the limits no matter what the limits were varied to over an extensive period. NDM depicted collusive and periodic reporting linkages to supervisors. Such patterns are termed scenarios and characterised as definable and re-usable patterns. Their value is that they are patterns that have now become  X  X nown X . Hence they can be defined, stored in a knowledge base, and applied during other data mining processes. The NDM approach is illustrated with an application to a real world case, presented as a reflective analysis of the analyst X  X  steps, following the main methodological steps. Specify sources of data and modelling : The case involved analysis of approximately twelve months of motor vehicle insurance claims from one company. Initially there were no known persons or transactions of interest. Visualisation and Generation of Visual Models : The analyst first built a set of linkages between persons, addresses, claim numbers, telephone numbers, and bank accounts into which claims monies had been paid, as shown in Fig. 2 and Fig. 3. Cognition and sense-making (and  X  X rain of thought X  analysis) : The analyst was able to quickly focus where to look in the myriad of linkages by eliminating the  X  X egular X  small triangles of data comprised of a person, a claim number and an address, all fully inter-linked (most people just had one claim and one address). The  X  X umps X  looked as though they were  X  X rregularities X . The  X  X umps X  comprised persons linked to multiple claims and/or addresses (see Fig. 4). Discovery and remapping : Four emergent groups were identified (see Fig. 5). An emergent group comprises closely inter-related data items; they have more links within the group than outside to any other group.  X  X rain of thought X  analysis : The emergent group on the right in Fig. 5 comprised five people called Simons, three claims and four addresses, all closely inter-related. They were linked across to another group at 11 o X  X lock comprised of more people called Simons and somebody called Wesson. That Wesson (initial A) was linked down to the group at 5 o X  X lock to E Wesson via a common claim. That in turn took the analyst over to the address at 4 o X  X lock and then to an  X  X  Verman X  at 9 o X  X lock. This  X  X rain of thought X  analysis led the analyst to Verman. Following her intuition she wanted to look at Verman more closely (although she could not explain why). Reselection and converting patterns to knowledge : The analyst hypothesised that Verman was a few steps removed from the activity of the Simons. She firstly took Verman and stepped out to obtain all his indirect linkage (see Fig. 6). The analyst then added extra linkage (see Fig. 7, in this case, only two extra fields: bank account information and telephone numbers). The analyst quickly discovered one extra and crucial link that helped her to qualify Verman  X  one of his two telephone numbers was also linked to A Wesson (see Fig. 8). That additional link provided the  X  X ipping point X , the extra knowledge that gave her sufficient confidence to recommend that Verman be investigated. This subsequently led to his conviction on fraud charges. This paper has described the concept of network data mining and presented a case study that illustrates its real-world implementation, its distinction from other analytical technics, and also its distinction from social network analysis. Network data mining involves a human-centred process which harnesses the intuitive powers of the human intellect in conjunction with unique algorithms to facilitate the intuition. The prosecution of Verman, who would have escaped detection with traditional exception detection methods. He had only had one claim, no  X  X ed flag X  information was involved, and nothing particularly anomalous occurred with respect to him. A complementary usage of a frequent pattern mining algorithm could have revealed a rule based on all names occurring more often than expected in the telephone population. The result being that the name Simons would be flagged. Future work aims at deeper integration of both approaches. 
