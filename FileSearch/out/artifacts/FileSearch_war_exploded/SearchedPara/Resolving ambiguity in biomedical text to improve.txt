 1. Introduction and background generating summaries from a full-text source, even when the author has created an abstract: (1) the abstract may not in-when used instead of the articles X  abstracts ( Gay, Kayaalp, &amp; Aronson, 2005 ).

Summarization systems usually work with text-level representations of the document which consist of information that onstrated the benefit of richer conceptual representations ( Fiszman, Rindflesch, &amp; Kilicoglu, 2004; Plaza, D X  which represent documents using concepts instead of words. The representations may be enriched with semantic relations between the concepts (i.e. synonymy, hypernymy, homonymy or co-occurrence) to improve the quality of the summaries.  X 
The Unified Medical Language System (UMLS) ( Nelson, Powell, &amp; Humphreys, 2002 ) has proved to be a useful knowledge
UMLS is used, the vocabulary of the document being summarized has to be mapped onto the concepts it contains. This is 2001 ). For example, the term  X  X  X old X  X  is associated with several possible meanings in the UMLS Metathesaurus including  X  X ommon cold X ,  X  X old sensation X ,  X  X old temperature X  and  X  X old therapy X .

The majority of biomedical summarizers that employ the UMLS Metathesaurus use MetaMap ( Aronson, 2001 ) to translate
Map returns multiple concepts. However, selecting the wrong meaning for ambiguous terms may affect the quality of the summaries generated.

This paper describes the application of various strategies for selecting UMLS concepts from the MetaMap output to im-prove a state-of-art biomedical summarization system. The summarizer ( Plaza et al., 2008 ) is a graph-based method that uses the UMLS Metathesaurus to create conceptual representations. Strategies for selecting concepts from MetaMap include ining their context. We find that using WSD improves the quality of the summaries generated.

The next section describes related work on summarization and WSD and also introduces the resources employed by the summarization and WSD systems used in this work. Section 3 describes our concept-based summarization algorithm. Sec-results. The final section provides concluding remarks and suggests future lines of work. 2. Related work 2.1. UMLS and MetaMap
The Unified Medical Language System (UMLS) ( Nelson et al., 2002 ) is a collection of controlled vocabularies related to biomedicine and contains a wide range of information that can be used for Natural Language Processing (NLP). The UMLS comprises of three parts: the Specialist Lexicon, the Metathesaurus and the Semantic Network.
 tries with one entry for each spelling or set of spelling variants in a particular part of speech.
The Metathesaurus forms the backbone of the UMLS and is created by unifying over 100 controlled vocabularies and clas-sification systems. It is organized around concepts, each of which represents a meaning and is assigned a Concept Unique Identifier (CUI). For example, the following CUIs are all associated with the term  X  X  X old X  X : C0009443  X  X ommon Cold X , C0009264  X  X old Temperature X  and C0234192  X  X old Sensation X .
 The Metathesaurus comprises of several tables containing information about CUIs. These include the
This table lists a range of different types of relations, including sibly synonymous and other related . For example, the MRREL and C0027442  X  X asopharynx X  are connected via the other related
The MRHIER table in the Metathesaurus lists the hierarchies in which each CUI appears, and lists the entire path to the root of each hierarchy for the CUI.
 concepts in the Metathesaurus, along with a set of relationships (or semantic relations) that exist between the semantic types. For example, the concept C0009443  X  X ommon Cold X  is classified in the semantic type  X  X isease or Syndrome X . tions between semantic types, including hierarchical relations ( ciated with and co-occurs with ). For example, the semantic types  X  X isease or Syndrome X  and  X  X athologic Function X  are connected via the is _ a relation in this table.

The MetaMap program ( Aronson, 2001 ) maps biomedical text to concepts in the Metathesaurus. The semantic type for each concept mapping is also returned. MetaMap employs a knowledge intensive approach that uses the Specialist Lexicon in combination with lexical and syntactic analysis to identify noun phrases in text. Matches between noun phrases and
Metathesaurus concepts are computed by generating lexical variations and allowing partial matches between the phrase and concept. The possible UMLS concepts are assigned scores based on the closeness of the match between the input noun (C0234192, C0009443 and C0009264). Weeber et al. (2001) estimated that around 11% of the phrases in Medline abstracts are mapped onto multiple CUIs. 2.2. Summarization of biomedical text
Summarization has been an active area within NLP research since the 1950s. A variety of approaches have been proposed (see Mani (2001) for a review). We focus on graph-based methods here.

Graph-based approaches represent the document as a graph. When text-level representations are used the nodes repre-with the cosine similarity between the sentences. Mihalcea and Tarau (2004) describe a related method in which the sim-ilarity among sentences is measured in terms of word overlaps.

However, text-level representations do not exploit the semantic relations among the words in the text (i.e. synonymy, homonymy or co-occurrence). For example, they are unable to make use of the fact that the phrases myocardial infarction document and domain-specific resources that contain information about the semantic relations between concepts. dad, 1997 ) to deal with concepts instead of terms. Yoo, Hu, and Song (2007) represent a corpus of documents as a graph, where the nodes are the MeSH descriptors found in the corpus and the edges represent hypernymy and co-occurrence rela-tem for biomedical multi-document summarization. It constructs a semantic graph that contains concepts of three types: ontological concepts (general ones from WordNet and specific ones from the UMLS), named entities and noun phrases. It applies a WSD algorithm to select the correct senses in WordNet although they do not describe how they dealt with ambi-guity in the UMLS. 2.3. WSD of biomedical text of approaches have explored the problem in a domain-independent setting, although several researchers have developed Mons (2005) for a review).
 The most popular approaches to WSD in the biomedical domain are based on supervised learning, for example Joshi, approaches. However, they require labeled examples which are often unavailable and can be impractical to create.
Humphrey, Rogers, Kilicoglu, Demner-Fushman, and Rindflesch (2006) describe a supervised approach to WSD, JDI, in the with them and mapping these onto Journal Descriptors. Disambiguation is carried out by examining the context of the able to disambiguate between meanings with the same semantic type. The JDI algorithm is included in recent versions of
MetaMap and can be used to select between mappings when there is more than one possible CUI, as shown in Fig. 1 . Hum-corpus ( Weeber et al., 2001 ).

Knowledge-based approaches to WSD are an alternative to supervised learning that do not require manually-tagged data and have recently been shown to compete with supervised systems in terms of performance ( Ponzetto &amp; Navigli, edge base as a graph which is then analyzed to identify the meanings of ambiguous words. An advantage of this approach is that the entire knowledge base can be used during the disambiguation process by propagating information through the graph.

One such method is Personalized PageRank ( Agirre &amp; Soroa, 2009 ) which makes use of the PageRank algorithm used by knowledge base and create graphs using the entire WordNet hierarchy. The ambiguous words in the document are added as in the graph and the PageRank algorithm is applied to distribute this information through the graph. The meaning of each word with the highest weight is chosen. We refer to this approach as the approach, referred to as  X  X  X ord to word X  X  ( ppr _ w2w
Soroa (2009) show that the Personalized PageRank approach performs well in comparison to other knowledge-based ap-proaches to domain-independent WSD and report an accuracy of around 58% on standard evaluation data sets. Agirre, Sora, and Stevenson (2010) applied Personalised PageRank to the biomedical domain using the UMLS instead of WordNet and re-port precision of 0.681 on a set of 49 terms from the NLM-WSD corpus. 3. Summarizer
This section describes the graph-based conceptual summarization system used in this paper. The summarizer identifies detail. 3.1. Preprocessing
Before starting with the summarization process itself, a preliminary step is undertaken in order to prepare the document for the subsequent steps. This preprocessing involves the following actions:
Acknowledgments and References sections, tables, figures and section headings. 2. If the document includes an Abbreviations section the abbreviations and their corresponding expansions are extracted from it. Any occurrences of the abbreviation in the document are then replaced with the expansion. For example, if the Abbreviations section defines  X  X  X ngiotensin converting enzyme X  X  as the expansion of ACE for a particular document and that document contains the phrase  X  X  X ess than 50% received ACE inhibitor X  X  then that phrase would become  X  X  X ess than 50% received angiotensin converting enzyme inhibitor X  X . 3. Expansions for abbreviations not defined in the document X  X  abbreviation section are identified automatically from the document using a publicly available implementation 1 of the approach described by Schwartz and Hearst (2003) . Abbrevi-ations are then substituted with their expansions in the same way. 4. The text in the body section is split into sentences using GATE 5. Stop words 3 are removed since they are not useful in discriminating between relevant and irrelevant sentences. 3.2. Concept identification
The next stage is to map the terms in the document to concepts from the UMLS Metathesaurus and semantic types from the UMLS Semantic Network. The MetaMap program is run over the text in the body section of the document. Section 4 de-scribes different strategies to select concepts when MetaMap is unable to return a single CUI for a word.
UMLS concepts belonging to very general semantic types are discarded, since they have been found to be excessively
Concept , Functional Concept , Idea or Concept , Intellectual Product , Mental Process , Spatial Concept and Language . 3.3. Document representation
The next step is to construct a graph representing the document. We begin by creating a sentence graph for each sentence archies are removed since they represent concepts with excessively broad meanings and may introduce noise to later processing.

The sentence graphs are then merged to create a single document graph . This graph is extended with more semantic rela-the hypernymy and other related relations between concepts from the Metathesaurus and the tion between semantic types from the Semantic Network. Hypernyms are extracted from the relations from the MRREL table and associated with relations from the with and other related relations that link leaf vertices are added to the document graph.

Fig. 2 shows an example graph for a simplified document consisting of the sentences
Finally, each edge is assigned a weight in [0, 1] as shown in Eq. (1) . The weight of an edge e representing an between two vertices, v i and v j (where v i is a parent of the root of their hierarchy. The weight of an edge representing any other relation (i.e. lated ) between pairs of leaf vertices is always 1.
 graph, the other related link between concepts  X  X oxazosin X  and  X  X hlorthalidone X  is assigned the weight 1. 3.4. Concept clustering and topic recognition that are strongly related in meaning and it is assumed that each set represents a different topic in the document. plex networks in which some nodes are highly connected to other nodes in the network while the remaining nodes are rel-atively unconnected. The highly connected nodes are often called hubs .

The salience of each vertex in the graph ( Yoo et al., 2007 ) is then computed. The salience of a vertex, sum of the weights of the edges that are connected to it. This is shown in Eq. (2) where connect ( e , e connects vertices v i and v j . Salience ranks the nodes according to their structural importance in the graph. connectivity between them. If it is the two HVS are merged. The remaining vertices (i.e. those not included in the HVS) weights of the edges that connect the target vertex to the other vertices in the cluster. 3.5. Sentence selection
The final step of the summarization process consists of selecting the sentences for the summary. A non-democratic voting mechanism ( Yoo et al., 2007 ) is used to compute the similarity between each sentence graph and cluster. Each vertex, within a sentence graph, S j , assigns a vote w k , j , i
We select the sentences for the summary based on their similarities to the clusters. We compute a single score for each scores are then selected for the summary, where N depends on the summary compression rate.
 ondary information. The scoring function selects most of the sentences from the most populated cluster and also includes evant to the user.

Alternative heuristics for sentence selection were explored in previous work ( Plaza et al., 2008 ). 4. WSD strategies for concept identification Since our summarization system is based on the UMLS it is important to be able to accurately map the documents onto
Metathesaurus concepts. The example in Section 2.1 shows that MetaMap does not always select a single concept and it is therefore necessary to have some method for choosing between the ones that are returned. We compare several alternative approaches for concept selection, including ones that make use of WSD. 4.1. First mapping
The first approach used is to take the first mapping returned by MetaMap when it returns more than one concept. No attempt is made to resolve ambiguities. This approach is adopted in previous works ( Plaza et al., 2008; Reeve et al., tially the same as choosing a concept at random from the set of those returned. 4.2. WSD using PPR concept chosen for each term used to create the graph.

The Personalized PageRank algorithm was adapted to assign concepts from the UMLS Metathesaurus following the pro-use both the standard ( ppr ) and  X  X  X ord to word X  X  ( ppr _ uation. The concepts selected by the disambiguation algorithm are then used to create the document graph. 4.3. WSD using JDI The JDI algorithm (see Section 2.3 ) was used as an alternative WSD approach. The implementation included with Meta-this happens the first mapping returned by MetaMap is selected. 4.4. All mappings
Instead of using WSD, all candidate CUIs are used to build the document graph. No attempt is made to resolve ambiguity when there are multiple possible CUIs. 4.5. WSD using weighted mappings
The  X  X  X ll Mappings X  X  strategy made use of all concepts returned by MetaMap and considered them all to be equally impor-tant. The final strategy also uses all concepts returned by MetaMap but weight them using the output of a WSD algorithm. The aim of this strategy is to reduce the effect of WSD errors when the document graph is created.
The function for computing the salience of the vertices in the document graph (Eq. (2) ) is modified to assign greater weight to the concept selected by the WSD algorithm. The modified function is shown in Eq. (5) , where M is the number of possible CUIs returned by MetaMap for a given ambiguous term and salience ( 5. Experimental method 5.1. Evaluation collection
The most common approach to evaluating automatically generated summaries of a document (also known as peers )isto compare them against manually-created summaries ( reference summaries) and measure the similarity between their con-written by humans. To the authors X  knowledge no corpus of reference summaries exists for biomedical documents. However, most scientific papers include an abstract (i.e. the author X  X  summary) which can be used as a reference summary for evaluation.

Our approach was evaluated using a collection of 150 documents randomly selected from the BioMed Central corpus for tions such as background, method, results and conclusion. However, we do not make use of this structure in our approach. 5.2. Evaluation metrics
ROUGE ( Lin, 2004b ) is used to evaluate the summaries by comparing them with the human abstracts for each article in the evaluation corpus. ROUGE is a commonly used evaluation method for summarization which uses the proportion of n-grams between a peer and one or more reference summaries to estimate the content that is shared between them. The tween the peer and model summaries. The following ROUGE metrics were used: ROUGE-1 ( R-1 ), ROUGE-2 ( R-2 ), ROUGE-
SU4 ( R-SU4 ) and ROUGE-W ( R-W ). R-1 and R-2 compute the number of unigrams and bigrams, respectively, that are shared by the peer and reference summaries. R-SU4 measures the overlap of skip-bigrams between the peer and reference summa-ries, allowing a skip distance of 4. Finally, R X  X  computes the union of the longest common subsequences between the peer and the reference summaries by taking the presence of consecutive matches into account.
 community.

A second drawback of ROUGE metrics is that they use lexical matching instead of semantic matching. Therefore, peer summaries that are worded differently but carry the same semantic information may be assigned different ROUGE scores.
This phenomenon is known as paraphrase. Zhou, Lin, Munteanu, and Hovy (2006) present ParaEval, a preliminary approach to automatically evaluate summaries using paraphrases.
 the scores it produces and those provided by human judges from previous Document Understanding Conferences ( Lin, 2004b ). 5.3. Experiments
Automatic summaries are generated by selecting sentences until the summary is 30% of the original document size. This choice of summary size was based on the heuristic that a summary should be between 15% and 35% of the size of the source (i.e. scientific articles) are rich in information.
 ping returned by MetaMap ( First Mapping ), (2) WSD using standard Personalized PageRank ( WSD PPR ), (3) WSD using word-to-word variant of Personalized PageRank ( WSD PPR-W2W ), (4) WSD using JDI ( WSD JDI ), (5) all mappings generated by MetaMap ( All Mappings ) and (6) all mappings generated by MetaMap weighted with WSD output ( WSD Weighted Map-pings ). In addition, a baseline summarizer ( Lead Baseline ) was also implemented. This baseline generates summaries by
A Wilcoxon Signed Ranks Test with a 95% confidence interval is used to test statistical significance of the results. 6. Results and analysis  X  X  X SD Weighted Mappings X  X  approach.

Using WSD improves the average ROUGE scores for the summarizer compared to the  X  X  X irst Mapping X  X  baseline. This improvement is observed for all approaches to WSD. The  X  X  X tandard X  X  (i.e. WSD PPR) version of the Personalized PageRank disambiguation algorithm significantly improves R-1 and R-2 metrics while the  X  X  X ord to word X  X  variant (i.e. WSD PPR-
W2W) significantly improves all ROUGE metrics. Performance using  X  X  X ord to word X  X  PPR is also higher than standard PPR
WSD JDI) are better than those achieved by either Personalized PageRank variant. However, the improvement with respect to WSD PPR-W2W is not statistically significant.
 Results using the simple  X  X  X ll Mappings X  X  approach are comparable to those obtained using the best WSD algorithm, i.e. as well for R-1 and R-W.
 better than when the first mapping and PPR approaches are used for all ROUGE metrics. Although, the improvement over PPR-W2W, JDI and  X  X  X ll Mappings X  X  is not significant.

Finally, it must be noted that performance of all variants of our graph-based summarizer is considerably better than the lead baseline. 6.1. Analysis
The results presented above demonstrate that using WSD improves the performance of our summarizer compared to the first mapping baseline. The WSD algorithms identify the concepts that are being referred to in the documents more accu-od is better able to identify the topics covered in the document and the information in the sentences selected for the summary is closer to the model summaries. However, this improvement is less than expected and this is probably due to errors made by the WSD system (see Section 2.3 ).

On the other hand, the differences between the summaries generated using the  X  X ord to word X  PPR and the JDI algorithms are not significant; the second achieving an improvement of less than 0.5% for ROUGE-1. Both algorithms use a different a similar improvement in the summarization results. This seems to indicate that both the Metathesaurus and the Semantic
Network provide useful information for biomedical WSD, and combining the information from both sources will probably reduce the disambiguation errors.
 Performance improves when all possible concepts for ambiguous terms are used to build the document graph and no representation of the document difficult. The summarizer itself may also be performing WSD implicitly. The graph-based greater.
 suitable way, that is weighting likely concepts rather than removing some from the document graph.
The use of WSD to improve summarization seems similar to previous work which showed that WSD could improve Infor-mation Retrieval performance but only when the disambiguation was accurate enough ( Sanderson, 1994 ). To end with, Table 2 shows the automatic summaries generated by the best disambiguation strategy ( X  X  X SD Weighted open dislocation of the elbow. The lead baseline and the abstract of the paper are also shown. The  X  X  X SD Weighted Map-
The reason for this small difference seems to be that both strategies agree when assigning meanings to the concepts that is achieved despite the fact there is nearly 10% disagreement in the concept mappings for both strategies. 7. Conclusion
This paper explores the integration of WSD algorithms with a graph-based approach to biomedical summarization. The summarizer represents the document as a graph, where the nodes are concepts from the UMLS Metathesaurus and the links Our approach relies on accurate mapping of the document being summarized onto the concepts in the UMLS Metathesaurus.
We found that the commonly used approach of choosing the first mapping from MetaMap could be improved by choosing the concept identified by a WSD system. However, we also found the simple approach of choosing all concepts returned by
MetaMap produced performance comparable to the best WSD approach. The best performance was obtained when all pos-sible concepts from MetaMap were used to build the document graph and information from a WSD system used to weight them.
 limited by the accuracy of the disambiguation. Reducing the errors made by WSD systems may produce better summaries and this will be explored.
 Our future aim is to improve the summarizer to the extent that the abstract writing process can be completely automated. sentences for extraction according to the section in which they appear; and adapting the method to produce query-driven summaries, so that the summaries are biased toward the user X  X  information needs. We also aim to extend the method to produce multi-document summaries.

We are also interested in exploring and using new automatic evaluation strategies. In particular, we would like to mea-&amp; Nenkova, 2008; Saggion, Torres-Moreno, Cunha, &amp; SanJuan, 2010 ).
 Acknowledgments
This research is funded by the Spanish Government through the FPU Program and the Projects TIN2009-14659-C03-01 and TSI 020312-2009-44. Mark Stevenson is grateful for the support provided by the Engineering and Physical Sciences Re-search Council (Grant EP/D069548/1).
 References
