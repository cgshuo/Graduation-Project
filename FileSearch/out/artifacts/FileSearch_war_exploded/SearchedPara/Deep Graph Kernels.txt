 In this paper, we present Deep Graph Kernels, a unified frame-work to learn latent representations of sub-structures for graphs, inspired by latest advancements in language modeling and deep learning. Our framework leverages the dependency information be-tween sub-structures by learning their latent representations. We demonstrate instances of our framework on three popular graph kernels, namely Graphlet kernels, Weisfeiler-Lehman subtree ker-nels, and Shortest-Path graph kernels. Our experiments on several benchmark datasets show that Deep Graph Kernels achieve signif-icant improvements in classification accuracy over state-of-the-art graph kernels.
 H.2.8 [ Database Management ]: Database Applications -Data Min-ing; I.2.6 [ Artificial Intelligence ]: Learning; I.5.1 [ Pattern Recog-nition ]: Model -Statistical R-convolution kernels, graph kernels, deep learning, structured data, string kernels, social networks, bioinformatics
In domains such as social networks, bioinformatics, chemoinfor-matics and robotics, we are often interested in computing similar-ities between structured objects . Graphs, including sequences and trees as special cases, offer a natural way to represent structured-data. To illustrate one example where graph similarity can be use-ful, consider the problem of identifying a sub-community (also re-ferred as subreddits ) on Reddit 1 . To tackle this problem, one can represent an online discussion thread as a graph where nodes rep-resent users, and edges represent whether two users interact, for instance, by responding to each other X  X  comments (see Figure 1
Reddit is a popular content-aggregation website: http:// reddit.com . Image is created with Gephi: http://gephi.github.io .
 Then, the task is to predict which sub-community a discussion thread belongs to based on its communication graph. Similarly, in bioinformatics, one might be interested in the problem of iden-tifying whether a given protein is an enzyme or not. In this case, the secondary structure of a protein is represented as a graph where nodes correspond to atoms and edges represent the chemical bonds between atoms. If the graph structure of the protein is similar to known enzymes, one can conclude that the given graph is also an enzyme [33]. Therefore, computing semantically meaningful simi-larities between graphs is an important problem in various domains. Figure 1: A graph of a random post on http://reddit.com/ r/askreddit .

One of the increasingly popular approaches to measure the simi-larity between structured objects is to use kernel methods. Roughly speaking, kernel methods measure the similarity between two ob-jects with a kernel function which corresponds to an inner product in reproducing kernel Hilbert space (RKHS) [26]. The challenge for kernel methods is then to find a suitable kernel function that captures the semantics of the structure while being computationally tractable. R-convolution [11] is a general framework for handling discrete objects where the key idea is to recursively decompose structured objects into  X  X tomic X  sub-structures and define valid lo-cal kernels between them. In the case of graphs, given a graph G , let  X  ( G ) denote a vector which contains counts of atomic sub-structures, and  X  X  ,  X  X  H denote a dot product in a RKHS H . Then, the kernel between two graphs G and G 0 is given by (a) Dependency of sub-structures Figure 2: Dependency schema of a set of graphlets of size k  X  { 3 , 4 , 5 } where G 39 can be derived from G 15 (similarly, G be derived from G 7 ) by adding a new node and an edge (a). Ex-ponential growth of feature space in graphlets up to size k = 9 (b).

However, this representation does not take a number of impor-tant observations into account. First, sub-structures that are used to compute the kernel matrix are not independent. Let us consider an example on graphlets , a popular sub-structure type that is used for decomposing graphs [23, 28], which are defined as induced, non-isomorphic sub-graphs of size k (see Figure 4). Graphlets exhibit a strong dependence relationship, that is, size k + 1 graphlets can be derived from size k graphlets by addition of nodes or edges (sim-ilarly, size k graphlets can be recovered by deletion of nodes or edges from size k + 1 graphlets). For instance, G 39 can be derived from G 15 by adding a new node and an edge (see Figure 2 (a)). Second, the dimension of the feature space often grows exponen-tially. Figure 2 (b) illustrates the growth of the number of unique features in graphlets as graphlet size k increases. Consequently, as the number of features grows, we encounter the sparsity problem: only a few sub-structures will be common across graphs. This leads to diagonal dominance , that is, a given graph is similar to itself but not to any other graph in the dataset. Figure 3 (a) illustrates such a kernel matrix 3 where diagonal dominance is visibly observable using Weisfeiler-Lehman subtree kernel [27]. Ideally, we would like to have a kernel matrix where all entries belonging to a class are similar to each other, and dissimilar to everything else (see Fig-ure 3 (b)). To alleviate this problem, consider an alternative kernel between two graphs G and G 0 such that where M represents a |V| X |V| positive semi-definite matrix that encodes the relationship between sub-structures and V represents the vocabulary of sub-structures obtained from the training data. Therefore, one can design an M matrix that respects the similarity of the sub-structure space. In cases where there is a strong mathe-matical relationship between sub-structures, such as edit-distance , one can design an M matrix that respects the geometry of the space. In cases where a clear mathematical relationship between sub-structures might not exist, one can learn the geometry of the space directly from data. In this paper, we propose recipes for de-signing such M matrices for graph kernels. For our first recipe, we exploit an edit-distance relationship between sub-structures and directly compute an M matrix. In our second recipe, we propose
In both cases we observe a block matrix since the first 125 entries of the matrix correspond to graphs which are labeled as +1 and the remaining entries correspond to graphs which are labeled as  X  1 . Figure 3: Kernel matrix K for Weisfeiler-Lehman subtree kernel on the MUTAG dataset [7] (a), and its deep variant obtained from our framework (b). Entry K ij encodes the similarity between graph G and graph G j and the color map encodes the degree of the similarity (darker color indicates higher similarity). a framework that computes an M matrix by learning latent repre-sentations of sub-structures. Our contributions are as follows:
The rest of this paper is as follows. In Section 2, we review three popular families of graph kernels for which our framework is appli-cable. In Section 3.1, we design an M matrix for graphlet kernels by exploiting edit-distance relationship between sub-structures. In Section 3.2, we introduce deep graph kernel framework and dis-cuss the connection of our framework to R-convolution kernels. In Section 4, we discuss related work. In Section 5, we compare the classification performance of deep graph kernels to their base vari-ants as well as to other state-of-the-art graph kernels. We report results on classification accuracy on graph benchmark datasets and discuss the run-time cost of our framework. Section 6 concludes the paper.
We first introduce basic concepts and notation that will be used throughout the paper. Then, we discuss three major graph kernel families, namely, graph kernels based on limited-sized subgraphs [13, 28], graph kernels based on subtree patterns [24, 27], and graph kernels based on walks [14, 33] and paths [3]. We briefly discuss each of the above kernels, and recap how they can be viewed as instances of the more general R-convolution framework [11]. Let G = ( V,E ) represent a graph where V is a set of vertices and E  X  ( V  X  V ) is a set of edges . Let G represent a set of n graphs where G = {G 1 , G 2 ,..., G n } and let Y represent a set of labels associated with each graph in G where Y = { y G 1 ,y G 2 ,...,y Given G = ( V,E ) and H = ( V H ,E H ) , H is a sub-graph of G if and only if there is an injective mapping  X  : V H  X  V such that ( v,w )  X  E H if and only if (  X  ( v ) , X  ( w ))  X  E . A graph G is called a labeled graph if there is a function l : V  X   X  that assigns labels from an alphabet  X  to vertices in the graph. A graph G is called an unlabeled graph if individual vertices have no distinct identifications other than their inter-connectivity. Throughout the paper, we will refer K ( G , G 0 ) as a kernel function that measures the similarity between graphs G and G 0 .

Graph classification task considers the problem of classifying graphs into two or more categories. Given a set of graphs set of class labels Y , the task in graph classification is then to learn a model that maps graphs in G to the label set Y . A popular ap-proach is to first use a graph kernel to compute a kernel matrix K of size n  X  n where K ij represents the similarity between G G , and then to plug the computed kernel matrix into a kernelized learning algorithm such as Support Vector Machines (SVM) [12] to perform classification.
A graphlet G is an induced and non-isomorphic sub-graph of size-k (see Figure 4) [23]. Let V k = { G 1 ,G 2 ,...,G set of size-k graphlets where n k denotes the number of unique graphlets of size k . Given two unlabeled graphs G and G graphlet kernel is defined as follows [28]: where f G and f G 0 are vectors of normalized counts, that is, the i component of f G (resp. f G 0 ) denotes the frequency of graphlet G occurring as a sub-graph of G (resp. G 0 ). Furthermore,  X  X  ,  X  X  denotes the Euclidean dot product.
The second family of graph kernels decomposes a graph into its subtree patterns. The Weisfeiler-Lehman subtree kernel [27] be-longs to this family. The key idea here is to iterate over each vertex of a labeled graph and its neighbors in order to create a multiset label . The multiset at every iteration consists of the label of the vertex and the sorted labels of its neighbors. The resultant multiset is given a new label, which is then used for the next iteration. When comparing graphs, we simply count the co-occurrences of labels in both graphs. This procedure is inspired by the Weisfeiler-Lehman test of graph isomorphism, and is equivalent to comparing the num-ber of shared subtrees between two graphs. Formally, given G and G , the Weisfeiler-Lehman subtree kernel is defined as: As before,  X  X  ,  X  X  denotes the Euclidean dot product. If we assume that we perform h iterations of relabeling, then l G consists of h blocks. The i th component in the j th block of l G contains the frequency with which the i th label was assigned to a node in the j th iteration.
The third family of graph kernels decomposes a graph into random-walks [14, 33] or paths [3] and counts the co-occurrence of random-walks or paths in two graphs. Let P G represent the set of all shortest-paths in graph G , and p i  X  P G denote a triplet ( l i s is the length of the path and l i s and l i e are the labels of the starting and ending vertices, respectively. The shortest-path kernel between labeled graphs G and G 0 is defined as [3]: where the i th component of p G contains the frequency of the i triplet occurring in graph G . The vector p G 0 is defined analogously for G 0 .
One can show that all graph kernels summarized above as well as other somewhat sophisticated variants are all instances of the R-convolution framework. In a nutshell, the recipe for defining graph kernels is as follows: First, recursively decompose a graph into its subgraphs. For instance, the graphlet kernel decomposes a graph into graphlets, Weisfeiler-Lehman kernel decomposes a graph into subtrees, and Shortest-Path kernel decomposes a graph into shortest-paths. In next step, the decomposed sub-structures are represented as a vector of frequencies where each item of the vec-tor represents how many times a given sub-structure occurs in the graph. Finally, the Euclidean space or some other domain-specific RKHS is used to define the dot product between the vectors of fre-quencies.
In this section, we first discuss how to compute an M matrix by using the edit-distance relationship between sub-structures. Then, we discuss how to compute an M matrix by learning the similarity between sub-structures inspired by latest advancements in language modeling and deep learning.
When sub-structures exhibit a clear mathematical relationship, one can exploit the underlying similarities between sub-structures to compute an M matrix. For instance, in graphlet kernels, one Figure 5: Undirected edit-distance graph G for graphlets of size k  X  5 (size and colors of the nodes are based on degree). can derive an edit-distance relationship to encode how similar one graphlet to is another (see Figure 2 (a)). Given a graphlet G size k , and a graphlet G j of size k + 1 , let us build an undirected edit-distance graph G by adding an undirected edge from G G j if and only if G i can be obtained from G j by deleting a node of G j (or vice versa, if G j can be obtained from G i by adding a node to G i ). Given such an undirected graph G , one can simply compute the shortest-path distance between G i and G j in order to compute their edit-distance (see Figure 5). While this approach enables us to directly compute an M matrix, the cost of computing the shortest-path distances on G becomes prohibitively expensive as a function of graphlet size k (see Figure 2 (b)). For instance, in order to compute an M matrix at level k = 9 , one needs to compute all pairwise shortest-path distances of an undirected graph having 288 , 267 nodes. On the other hand, one can observe that while the number of unique graphlets grow exponentially, only a few of them will be observed in a given graph. Therefore, instead of computing a complete M matrix of size |V| X |V| , one can design an M matrix of size |V 0 | X |V 0 | with |V 0 ||V| by only taking the observed sub-structures into account. In next section, we discuss an approach that utilizes this observation.
Our second approach is to learn the latent representations of sub-structures by using recently introduced language modeling and deep learning techniques. The learned representations are then uti-lized to compute an M matrix that respects the similarity between sub-structures. Next, we review the related background in language modeling, and then transform the ideas to learn representations of sub-structures.
Traditional language models estimate the likelihood of a sequence of words appearing in a corpus. Given a sequence of training words { w 1 ,w 2 ,...,w T } , n -gram based language models aims to maxi-mize the following probability In other words, they estimate the likelihood of observing w n previous words observed so far.

Recent work in language modeling focused on distributed vector representation of words, also referred as word embeddings . These neural language models improve classic n -gram language models by using continuous vector representations for words. Unlike tra-ditional n -gram models, neural language models take advantage of Figure 6: Architecture for the CBOW and Skip-gram method [20]. w t is the current word, while w t + j are the surrounding words where c is the size of the context, and  X  c  X  j  X  c . Here, CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word. the notion of context where a context is defined as a fixed number of preceding words. In practice, the objective of word embedding models is to maximize the following log-likelihood
Several methods are proposed to approximate Equation 7. Next, we discuss two such methods that we utilize in our framework, namely continuous bag-of-words (CBOW) and Skip-gram models [20].
CBOW model predicts the current word given the surrounding words within a given window. The model architecture is similar to feedforward neural network language model [2] where the non-linear hidden layer is removed and the projection layer is shared for all words (see Figure 6). Formally, CBOW model aims to maximize the following log-likelihood, where c is the length of the context. The probability Pr( w is computed using the softmax, defined as Here, v w corresponds to the input vector representation of w and v t corresponds to the output vector representation of w averaged vector representation from the context is computed as
The Skip-gram model maximizes co-occurrence probability among the words that appear within a given window. In other words, in-stead of predicting the current word based on surrounding words, Figure 7: The learned shortest-path sub-structures on ENZYMES dataset [4] in structures embedded in the cluster marked with  X  are 2  X  2 , 2  X  2 26 , 2  X  2 27 , 2  X  2 28 where first two characters represent the the main objective of the Skip-gram is to predict the surrounding words given the current word (see Figure 6). More precisely, the objective of the Skip-gram model is to maximize the following log-likelihood, where the probability Pr( w t  X  c ,...,w t + c | w t ) is computed as Here, the contextual words and the current word are assumed to be independent. Furthermore, Pr( w t + j | w t ) is defined as where v w and v 0 w are the input and output vectors of word w .
Hierarchical softmax and negative sampling are two efficient al-gorithms that are used in training the Skip-gram and CBOW mod-els. Hierarchical softmax uses a binary Huffman tree to factorize expensive partition function of the Skip-gram model. An alterna-tive to the Hierarchical softmax is negative sampling, which selects the contexts at random instead of considering all words in the vo-cabulary. In other words, if a word w appears in the context of another word w 0 , then the vector representation of the word w is closer to the vector representation of word w 0 comparing to any other randomly chosen words. In practice, one should try both the Skip-gram and CBOW models with Hierarchical softmax and neg-ative sampling algorithms in order to decide which pair is more suitable to the application in hand.

After the training converges, similar words are mapped to similar positions in the vector space. Moreover, the learned word vectors are empirically shown to preserve semantics. For instance, word vectors can be used to answer analogy questions using simple vec-tor algebra where the result of a vector calculation v( X  X adrid X )  X  v( X  X pain X ) + v( X  X rance X ) is closer to v( X  X aris X ) than any other word vector [20].

These properties make word vectors attractive for our task since the order independence assumption provides a flexible notion of  X  X earness X  for sub-structures. A key intuition we utilize in our framework is to view sub-structures in graph kernels as words that are generated from a special language. In other words, different sub-structures compose graphs in a similar way that different words form sentences when used together. With this analogy in mind, one can utilize word embedding models to unveil dimensions of similarity between sub-structures. The main expectation here is that similar sub-structures will be close to each other in the d -dimensional latent space. Figure 7 illustrates shortest-path sub-structures in R 2 learned by our framework. Note that similar sub-structures are close together in latent space.
Our framework takes a list of graphs G and decomposes each graph into its sub-structures. The list of decomposed sub-structures for each graph is then treated as a sentence that is generated from a vocabulary V where vocabulary V simply corresponds to the unique set of observed sub-structures in the training data. However, unlike words in a traditional text corpora, sub-structures do not have a linear co-occurrence relationship. Therefore, one needs to build a corpus where the co-occurrence relationship is meaningful. Next, we discuss how to generate corpora where co-occurrence relation-ship is meaningful on three major graph kernel families. Figure 8: An example of sub-structure regularity in graphlet sub-structure space.

After generating a corpus where a co-occurrence relationship is partially preserved, we simply build the model by using CBOW or Skip-gram algorithms and train them with Hierarchical softmax or negative sampling 4 . Let s represent an arbitrary sub-structure from a vocabulary V , and  X  s represent learned vector representation of
We used Gensim library [25] for all algorithms. s using our framework. Given the vector representations of sub-structures, we compute a diagonal M matrix such that each entry on the diagonal, M ii computed as  X   X  i ,  X  i  X  where  X  i to learned d -dimensional hidden of sub-sequence i and M ij where i 6 = j and 1  X  i  X  |V| (resp. j ). After computing the M matrix, we simply plug it into Equation 2 in order to compute the kernel between each sub-structure.
In a similar fashion, we can plug other graph kernels into our framework such as random-walk kernels [10], labeled version of graphlet kernel [28], subtree kernels [6, 24], cyclic pattern kernels [13] and p -step random-walk kernel [30]. Moreover, our frame-work is applicable to any R-convolution kernel where there is a notion of dependency between sub-structures, such as string ker-nels . String kernels are other popular instances of R-convolution kernels where we are interested in computing a kernel between two sequences . Given an input sequence S over an alphabet V and a number k  X  1 , k  X  spectrum of the sequence S is defined as the set of all k-length contiguous sub-sequences S contains [17]. The feature vector  X  ( S ) is then simply constructed as a frequency vec-tor over sub-sequences in its k-spectrum and the kernel between two sequences are computed via Equation 1. Similar to graph ker-nels, the co-occurrence relationship between sub-sequences are not taken into account. Similar to graph kernels, we treat all length k sub-sequences of a string as co-occurred and learn the hidden rep-resentation of each spectrum using our framework. In case of string kernels, we compute M matrix such that each entry M ij computed as  X   X  i ,  X  j  X  where  X  i corresponds to learned d -dimensional vector of sub-sequence i (resp.  X  j ). The closest work to our paper is the recently proposed model, DeepWalk by [22]. DeepWalk learns social representations of ver-tices of graphs by modeling short random-walks. We distance our-selves from DeepWalk in several aspects. First, instead of learning similarities between nodes we are interested in learning similarities between structured objects , such as graphs and strings. In other words, DeepWalk operates on a single graph, while we are inter-ested in the relationship between multiple graphs. Moreover, in-stead of using random-walks, our framework can be configured to work with any type of sub-structures, including graphlets, shortest-paths, sub-trees and strings.

Many different graph kernels focusing on different types of sub-graphs have been defined in the past which can be categorized into three major families: graph kernels based on limited-sized sub-graphs [13], [28], graph kernels based on subtree patterns [24], [27] and graph kernels based on walks [14] and paths [3]. Our frame-work is complementary to existing graph and string kernels where the sub-structures have a similarity relationship between them.
The aim of our experiments is threefold. First, we want to show that using an M matrix that infers the relationship between sub-structures improves the classification accuracy. Second, we want to show that our framework is robust to random noise. Third, we want to show that the deep kernels are comparable to or outper-form state-of-the-art graph kernels in terms of classification accu-racy, while remaining competitive in terms of computational re-quirements. Next, we discuss bioinformatics and social network datasets that we use in our experiments. Table 1: Properties of the Bioinformatics and Social network datasets used in graph kernel experiments.
 Dataset Size Classes Avg.nodes Labels MUTAG 188 2 17.9 7 PTC 344 2 25.5 19 ENZYMES 600 6 32.6 3 PROTEINS 1113 2 39.1 3 NCI1 4110 2 29.8 37 NCI109 4127 2 29.6 38 COLLAB 5000 3 74.49 -IMDB-BINARY 1000 2 19.77 -IMDB-MULTI 1500 3 13 -REDDIT-BINARY 2000 2 429.61 -REDDIT-MULTI-5K 5000 2 508.5 -
REDDIT-MULTI-12K 11929 11 391.4 -
In order to test the efficacy of our model, we applied our frame-work to real-world datasets from bioinformatics and social net-works (see Table 1 for summary statistics of these datasets).
We applied our framework to benchmark graph kernel datasets, namely, MUTAG, PTC, ENZYMES, PROTEINS and NCI1, NCI109.
 MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds [7] with 7 discrete labels. PTC [31] is a dataset of 344 chemical compounds that reports the carcinogenicity for male and female rats and it has 19 discrete labels. NCI1 and NCI109 [34] datasets (4100 and 4127 nodes, respectively), made publicly available by the National Cancer Institute (NCI) are two subsets of balanced datasets of chemical compounds screened for ability to suppress or inhibit the growth of a panel of human tumor cell lines, having 37 and 38 discrete labels respectively. ENZYMES is a bal-anced dataset of 600 protein tertiary structures obtained from [4] and has 3 discrete labels. PROTEINS is a dataset obtained from [4] where nodes are secondary structure elements (SSEs) and there is an edge between two nodes if they are neighbors in the amino-acid sequence or in 3D space. It has 3 discrete labels, representing helix, sheet or turn .
In order to test the efficacy of our framework on social network domain, we derive several unlabeled graph datasets with different tasks as follows.
We compare our framework against representative instances of major families of graph kernels in the literature. Other than base kernels of our framework, namely, Weisfeiler-Lehman subtree ker-nel [28], graphlet kernel [28], and shortest-path kernel [3], we also compare our kernels with the random walk kernel [10], the subtree kernel [24], and p -step random-walk kernel [30]. The Random-Walk, p -step Random-Walk and Ramon-G X rtner kernels are writ-ten in Matlab and were obtained from the authors of [28]. All other kernels were coded in Python 5 . In order to ensure a fair compari-son, all experiments are performed on the same hardware.

All kernels are normalized to have a unit length in the feature space. Moreover, we use 10-fold cross validation with a binary C -SVM [5] to test classification performance. The C value for each fold is independently tuned using training data from that fold. In order to exclude random effects of the fold assignments, this experiment is repeated 10 times, and average prediction accuracies with their standard deviations are reported.
We chose parameters for the various kernels as follows: the win-dow size and dimension for deep graph kernels is chosen from { 2 , 5 , 10 , 25 , 50 } , the decay factor for random-walk kernels is cho-sen from 10  X  6 , 10  X  5 ,..., 10  X  1 , the p value in the p -step random-walk kernel is chosen from { 1 , 2 ,..., 10 } and the height parame-ter in Ramon-G X rtner subtree kernel is chosen from { 1 , 2 , 3 } . For each kernel, we report the results for the parameter which gave the best classification accuracy. For Weisfeiler-Lehman subtree kernel, we experimented with the height parameter h = 2 due to expo-nentially increasing feature space of the original kernel. For the
Our code and datasets are available at http://web.ics. purdue.edu/~ypinar/kdd corresponding variant derived from our framework. graphlet kernel, we set the size of the graphlets k to be 7 since it exhibits the sparsity problem that we are interested in. We used Nauty [19] to get canonically-labeled isomorphic representations of each graphlet which are then used to construct the feature repre-sentation.
In this section, we apply our framework to several benchmark datasets and compare the classification accuracy of our kernels against their base variants.

Graphlet Kernels under noise We have two variants of graphlet kernels, namely, Edit-distance Graphlet Kernel (EGK) introduced in Section 3.1 and Deep Graphlet Kernel (DGK) introduced in Sec-tion 3.2. Since graphlet kernels do not exploit label information on the vertices and only compare graphs based on their structural similarity, an interesting experiment is to see how our kernels be-have under random noise on the edges. Therefore, we derive noisy variants of the datasets by randomly flipping 10% , 20% and 30% of the edges. Figure 9 (a) shows the comparison between original graphlet kernel and EGK where 0% represents the classification ac-curacy on the original dataset without noise. As can be seen from the figure, EGK outperforms the base kernel in MUTAG, PTC, PROTEINS, NCI1, NCI109, but outperformed by the original ker-nel in ENZYMES dataset. We believe this is due to the fact that EGK only uses a mathematical relationship between sub-structures rather than learning a sophisticated relationship. Therefore, we ap-plied our deep kernel framework on graphlet kernels (see Figure 9) (b). As can be seen from the figure, learning latent representa-tions of the graphlets outperforms its base variant significantly in all datasets except PROTEINS.

Graphlet Kernels on social network datasets Next, we test the efficacy of our framework on several social network datasets us-ing graphlet kernels. As can be seen from Table 2, deep graphlet kernels are able to outperform its base variant in all cases.
Deep Graph Kernels on bioinformatics datasets Table 3 shows the classification accuracy between graph kernels and their deep variants using Graphlet kernel, Weisfeiler-Lehman kernel, and Shortest-Path kernel. As can be seen from the table, our method is compa-rable or outperforms the base variants in all datasets.
 Table 2: Comparison of classification accuracy (  X  standard de-viation) of the Graphlet Kernel (GK) and Deep Graphlet Kernel (DGK) on social network datasets.

Comparison against other kernels Table 4 shows the classifi-cation accuracy of Ramon &amp; G X rtner, p -random-walk and random-walk graph kernels where the first column is constructed by picking the best result of Deep Graph Kernels from Table 3. As can be seen from Table 4, Deep Graph Kernels are able to outperform other graph kernels.
For Edit-distance Graphlet Kernel, computing an M matrix in-volves a one-time computation of the undirected graph between 1253 nodes for k = 7 which empirically takes 7 minutes. Af-ter that, one needs to compute all-pairs-shortest-path distances on the obtained undirected graph which empirically takes 8 seconds. For deep graph kernels, the overhead of computing an M ma-trix involves learning latent representations of the observed sub-structures. The runtime averaged out of all datasets for learning the latent representations is 21.5 seconds for deep graphlet kernel, 4.5 seconds for deep shortest-path graph kernel and 1.75 seconds for deep Weisfeiler-Lehman graph kernel. All runtime experiments use a fixed window size and dimension at 25 and this process is repeated 10 times to eliminate random effects.
As a proof-of-concept, we derive a deep variant of k-spectrum string kernel and perform experiments on benchmark bioinformat-ics datasets. Lehman kernel (WL) to their deep variants on bioinformatics datasets. &gt; 72H indicates that the computation did not finish after 72 hours.
In order to test the efficacy of our model, we applied our method to benchmark datasets in string kernels. SCOP (Structural Clas-sification of Proteins) is a manually-curated database that groups proteins together based on their 3-D structures [1]. The task is then to classify protein sequences into 7 distinct super-families. SCOP database has a 4-level structure-based hierarchy of classes where protein sequences are classified into one of the classes, namely, class , fold , super-family and family . Similar to the setting in [32], we tackled family and super-family classification problems where a family contains proteins with clear evolutionary relationship, and super-family contains the same evolutionary origin without being detectable at the level of sequences [15]. In family-classification problem, we considered NAD(P)-binding Rossmann-fold and (trans)-glycosidases domains where our main task is to classify proteins in a super-family into their families. NAD(P)-Rossmann dataset has 246 sequences having an average length of 218 with 6 classes where (trans)-glycosidases has 95 sequences having an average length of 375 with 2 classes.
 In super-family classification problem, we used Triose Phosphate Isomerase (TIM) beta/alpha-barrel protein fold where we classify each protein to one of the 7 distinct super-families. TIM beta/alpha dataset has 330 sequences having an average length of 332 with 7 classes. In order to derive the labels of each sequence, we used Astral SCOPe 2.04 genetic domain sequence database [9], based on PDB SEQRES records, with less than 95% identity to each other.
Moreover, we derived a new dataset for string kernels using the transcripts of TED.com talks. We collected the transcript of 385 talks having an average length of 9425 from three categories, namely, T echnology, E ntertainment, D esign. The task is then to predict which of the three category a talk belongs to.
The comparison between original k-spectrum string kernel with k = 3 and our method can be seen from Table 5. As shown in Table 5, deep variant of k-spectrum string kernel is able to outperform the base k-spectrum string kernel in all datasets.
 Table 5: Classification accuracy and error reduction for string ker-nel experiments where numbers next to the accuracy results repre-sents the standard deviation.

We presented a novel framework for graph kernels inspired by latest advancements in natural language processing and deep learn-ing. We applied our framework to three popular graph kernels, namely, graphlet kernel, shortest-path kernel, and Weisfeiler-Lehman subtree kernels. We introduced several large graph kernel datasets in social network domain, and showed that our framework outper-forms its base variants in terms of classification accuracy while in-troducing a negligible overhead.

Moreover, while we mainly restricted ourselves to graph kernels in this paper, we discussed that our framework is rather general, and lends itself to many extensions. For instance, it can be plugged directly into any R-convolution kernel as long as there is a depen-dency between sub-structures. We demonstrated one such exten-sion on string kernels and achieved significant improvements in classification accuracy.

An interesting extension of our framework would be applying it to attributed graphs with continuous values. Since a certain di-vergence between attribute values needs to be tolerated, learning hidden representations of the sub-structures would help to obtain a better classification accuracy.
We thank to anonymous KDD reviewers for their constructive comments. We also thank to Hyokun Yun, Jiasen Yang, Joon Hee Choi, Marjan Momtazpour, Parameswaran Raman, Sebastian Moreno, Shihao Ji for reviewing our paper. This work is supported by the National Science Foundation under grant No. #1219015.
