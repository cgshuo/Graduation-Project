 Social media has become a major source of information for many applications. Numerous techniques have been proposed to analyze network structures and text contents. In this paper, we focus on fine -grained mining of contentions in discussion/debate forums. Contentions are perhaps the most important feature of forums that discuss social , political and religious issues. Our goal is to discover contention and agreement indicator expressions, and contention points or topics both at the discussion collection level and also at each indi vidual post level. To the best of our knowledge, limited work has been done on such detailed analysis. This paper proposes three models to solve the problem, which not only model both contention/ agreement expressions and discussion topics, but also, more importantly, model the intrinsic nature of discussions /debates, i.e., interactions among discussants or debaters and topic sharing among posts through quot ing and rep lying relations . Evaluation results using real -life discussion/debate posts from several do mains demonstrate the effectiveness of the proposed models . H.2.8 [ Database Management]: Database Applications  X  Data mining ; H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing . Algorithms, Experimentation Contention Analysis, Debates, Discussions, S ocial Media Social media such as reviews, blogs, comments, discussions and debates contain valuable information that can be used for many applications. The essence of such media is that it enables people from anywhere in the world to express their views and to discuss any issue of interest in online communities. A large part of such discussions is about social, political and religious issues. On such issu es, there are often heated discussions/ debates, i.e., people argue and agree or disagree with one another. In this paper, we model this form of interactive social media. Given a set of discussion /debate posts, we aim to perform the following tasks: 1. Discover expressions that people often use to express 2. Determine contentious topics. First discover discussion topics 
Although there is a large body of literature on social media analysis such as social network analysis [14], sentiment analysis [27, 34], and grouping people into different camps [ 1, 32, 40, 42], to the best of our knowledge , limited research has been done on the fine -grained analysis of discussion/debate forums as proposed in this paper . Th is problem is important because a large part of social media is about discussions/debates of contentious issues, and discovering such issue s is useful for many applications. For example, in a political election, contentious issues often separate voters into different camps and determine their political orientations. It is thus important for political candidates to know such issues. For contentious social topic s, it is crucial for government agencies to be aware of them so that they can address the problems. Even for consumer products /services , contentions about them can be used to identify different types of customer s and to make effective marketing and busi ness decisions. 
In this paper, we use statistical modeling to perform the aforementioned tasks. Three new models are proposed. The first model, called JTE (Joint Topic -Expression model ), jointly models both discussion topics and CA-express ions . It provides a general framework for discovering discussion topics and CA-express ions simultaneously . Its generative process separate s topics and CA-express ions by using maximum-e ntropy priors to guide the separation. However, th is model does not consider a key characteristic of discussions/debates, i.e., authors quote or mention the claims /views of other authors and express contention or agreement on th ose claims /views. That is, there are interactions among authors and topics through the reply -to relation , which is a salient feature of discussion /debate forums . We then extend the JTE model and propose two novel and more advanced models JTE -R and JTE -P which model the interactions of authors and topics in two different ways , based on reply -to relations and author -pair structures respectively . 
Works related to ours are quite different both in application and in modeling . On application, the closely related work to ours is the finding of different camps of people in discussions /debates [ 1, 32, 40, 42 ]. This thread of research , however , does not discover CA -express ions or contention points, which are the objectives of this work. From a modeling point of view, our work is related to topic modeling in general and joint modeling of topics and certain other inform ation in particular. Topic models are a principled way of min ing topics from large text collections . There have been many extensions [5, 7, 13, 29, 35, 43 ] to the initial model s, LDA ( Latent Dirichlet Allocation ) [4 ] and pLSA (Probabilistic latent semantic analysis ) [20 ]. However, these models mine only topics, which are insufficient for our problem. In recent years, researchers have also proposed joint model s of topics and sentiments [ 21, 26 , 30, 47]. Our JTE model is related to these joint models. However, these models treat documents/posts independently, which fail to capture author and topic interactions in discussions/debates , i.e., author s rep ly to and quote each other  X  X  claims /views and express contentions or agreement s. Due to such interaction s, posts are clearly not independent of one another. The proposed JTE -R and JTE -P models capture such interactions. The detailed comparison with these models and other related work appears in  X 5 . 
The proposed models are evaluated both qualitatively and quantitatively using a large number of real -life discussion/ debate posts from four domains. Experimental results show that the two advanced models (JTE -R and JTE -P) outperform the base JTE model significantly , which indicate that the proposed interaction modeling is effective. Experiments using s tatistical metrics such as perplexity and KL -D ivergence also demonstrate that the interaction models fit the discussion/debate data better and can discover more distinctive topics and CA -expressions . This section present s the proposed JTE (Joint Topic Expression) model which lays the ground work for jointly modeling topics and CA-expressions. The JTE model belong s to the family of generative models for text where words and phrases ( n -grams) are viewed as random variables, and a doc ument is view ed as a bag of n -gram s and each n -gram ( word/phrase ) takes one value from a predefined vocabulary. W e use up to 4-grams, i.e., n = 1, 2, 3, 4 , in this work . Note that topics in most topic models like LDA are usually unigram distributions over words and assume words to be exchangeable at the word level. Arguably, this offers a great computational advantage over more complex models taking word order into account for discovering significant n -grams [ 44, 45]. Yet there exist other works which try to post -process discovered topical unigrams to form multi-word phrases using relevance [ 46] and likelihood [ 6] scores. However, o ur goal in this work is to enhance the exp ressiveness of our JTE model (rather than modeling n -gram word order) by considering n -grams and preserving the advantages of exchangeable modeling. Thus, we consider both words and phrases as our vocabulary (more details in  X  4.1 ). For notational convenience, from now on we use terms to denote both words (unigrams) and phrases ( n -grams). Since we are dealing with large corpora, for computational reasons, we only consider terms which appeared at least 30 times in the corpus We den ote the entries in our vocabulary by  X  1 ...  X  number of unique terms in the vocabulary. The entire corpus (document collection) of study is comprised of  X  1 ...  X  A document ( e.g., discussion post)  X  is represented as a vector of terms  X   X  with  X   X  entries.  X  is the set of all observed terms in the corpus with cardinality, |  X  | =  X   X   X  X  X  . The JTE model is motivated by the joint occurrence of CA -expression types ( contention and agreement ) and topics in discussion posts . A typical discussion /debate post mentions a few topics ( using semantically related topical term s) and expresses some viewpoint s with one or more CA-express ion types ( using semantically related contention and/or agreement expressions ). This observation motivates the generative process of our model where documents (posts) are represented as random mixtures of latent topics and CA-express ion types. Each topic or CA-express ion type is characterized by a distribution over terms . Assume we ha ve t = 1,..., T topics and e = 1,..., E expression types in our corpus . Note that in our case of discussion/ debate forum s, based on reading various posts, we hypothesize that E = 2 as i n such forums, we mostly find two expression types: contention and agreement (which we also statistically validate in  X  4.4.1) . However, the proposed JTE and other models are general and can be used with any number of expression types . Let  X   X  ,  X  denote the probability of  X  indicator variable (topic or CA-express ion) for the  X   X   X  ,  X  .  X   X  ,  X  denote s the appropriate topic or CA-express ion type index to which  X   X  ,  X  belongs . We parameterize multinomial s over topics using a matrix  X   X   X   X   X  wh ose elements  X  probability of document  X  exhibiting topic  X  . For simplicity of notation, we will drop the latter subscript ( t in this case) when convenient and use  X   X   X  to stand for the  X  th we define multinomials over CA-express ion types using a matrix  X   X  . The multinomials over terms associated with each topic are parameterized by a matrix  X   X   X   X   X  , whose elements  X  probability of generating  X  from topic  X  . Likewise, multinomials over terms associated with each CA-express ion type are parameterized by a matrix  X   X   X   X   X  . We now define the generative process of JTE (see Figure 1(a) for plate notation ). 1. For each CA -express ion type  X  , draw  X   X   X  ~  X  X  X  X  (  X  2. For each topic  X  , draw  X   X   X  ~  X  X  X  X  (  X   X  ) 3. For each forum discussion post  X   X  {1 ...  X  } : 
We use Maximum Entropy (Max -Ent) model to set  X  Max -Ent parameters can be learned from a small number of labeled topical and CA -expression terms which can serve as good priors. The idea is motivated by the following observation: topical and CA-express ion terms usually play different syntactic roles in a sentence. Topical term s (e.g.  X  U.S. senate X ,  X  X ea level  X ,  X  X arriage  X ,  X  X ncome tax  X ) tend to be noun and noun phrases while CA-expression term s ( X  I refute  X ,  X  X ow can you say  X ,  X  X robably agree  X ) usually contain pronouns, verbs, wh-determiners, and modals. In order to utilize the part -of -speech ( POS ) tag information, we place  X   X  ,  X  (the prior over the indicator variable  X   X  ,  X  ) in the word plate (see Figure 1 (a)) and draw it from a Max -Ent model conditioned on the observed feature vector  X  associated with  X   X  ,  X  and the learned Max -Ent parameters  X  (see  X   X  discriminative. In this work, we encode both lexical and POS features of the previous, current and next POS tags /lexemes of the term  X   X  ,  X  . Specifically, the feature vector, phrasal terms ( n -grams), all POS tags and lexemes of  X  considered as features. To learn the JTE model from data, exact inference is not possible . We thus resort to approximate inference using collapsed Gibbs sampling [18]. W e first derive the joint distribution below and then the Gibbs sampler.

To derive the joint distribution, we factor the joint according to the conditional distributions (causalities) governed by the Bayesian network of the proposed generative model.  X  (  X  ,  X  ,  X  ) =  X  (  X  |  X  ,  X  )  X   X  (  X  |  X  )  X   X  (  X  ) (1) Since we employ a collapsed Gibbs sampler, we integrate out  X  and  X  and obtain the joint as follows. outcome probabilities of the Max -Ent model are given by:  X  1 ...  X  are the parameters of the learned Max -Ent model corresponding to the  X  binary feature functions  X  Ent.  X   X  ,  X   X  X  X  and  X   X  ,  X   X  X  X  d enote the number of times term  X  was assigned to topic  X  and expression type  X  respectively. B ( X ) is the the number of terms in document d that were assigned to topic  X  and CA -expression type  X  respectively.  X   X   X  X  X  ,  X  denote the corresponding row vectors.

We use Gibbs sampling for posterior inference. Gibbs sampling is a form of Markov chain Monte Carlo ( MCMC ) method where a Markov chain is constructed to have a particular stationary distribution . In our case, we want to construct a Markov chain which converges to the posterior distribution over  X  and  X  conditioned on the observed data. We only need to sample  X  and  X  as we use collapsed Gibbs sampling and the dependencies of  X  and  X  have already been integrated out analytically in the joint. Denoting the random variables {  X  ,  X  ,  X  } by singular subscripts {  X  performing the following sampling: where  X  = (  X  ,  X  ) denotes the  X  th term of document  X  and the subscript  X   X  denotes assignment s excluding th e term at  X  . Omission of a latter index denoted by ( X ) represents the marginalized sum over the latter index . The conditional probabilities in ( 3) and (4 ) were derived by applying the chain rule on the joint distribution. W e employ a blocked sampler where we sample  X  and  X  jointly, as this improves convergence and reduces autocorrelation of the Gibbs sampler [37 ]. We now augment the J TE model to encode two kinds of interactions (topic and author ) in discussion/debate forums. We first improve JTE by encoding the reply -to relation s as authors usually reply to each other X  X  viewpoints by explicitly mentioning the user name using @name, and/or by quoting other s X  posts . F or easy presentation, we refer both cases by quoting from now on. Considering the reply -to relation, we call the new model JTE -R (Figure 1 (b)) . This model is based on the following observation: Observation : Whenever a post  X  replies to the viewpoints in some other posts by quoting them,  X  and the posts quoted by d should have similar topic distributions. This observation indicates that the JTE -R model needs to depart from typical topic models where there is usually no topical interaction among documents , i.e., documents are treated as being independent of one another . Let  X   X  be the set of posts quoted by post  X  . Clearly,  X   X  is observed 2 . In order to encode this  X  X eply -to X  relation into our model, the key challenge is to somehow constrain distributions of posts in  X   X  . Specifically, it is how to constrain  X  assignments to documents) during inference while the topic distributions of both  X   X   X  and  X   X   X   X  ,  X   X   X  X  X   X  apriori . Note that this situation is very different from that in [2] where it constrains word assignments to topics apriori knowing that some words are semantically related and probably should belong to the same topic. To solve our problem , we propose a novel solution , which exploits the foll owing salient features of the Dirichlet distribution : 1. Since  X   X   X  ~  X  X  X  X  (  X   X  ) , we have  X   X   X  ,  X   X   X  2. Also, the expected probability mass associated with each Thus, to constrain a post  X   X  X  topic distribution to be similar to the posts whom it replies/quotes (i.e. posts in  X   X  ), we now need functional base measures as it is the base measure that govern s the expected mass associated with each topical dimension in  X  way to employ functional base measures is to draw  X   X  ~  X  X  X  X  (  X   X   X   X  ) , where  X   X  =  X   X  topical distribution of posts in  X   X  ). For posts which do not quote any other post, we simply draw  X   X   X  ~  X  X  X  X  (  X   X  ) . For such a topic model with functional Dirichlet base measures, the sampling distribution is more complicated. Specifically, the document -topic distribution,  X   X   X  is no longer a simple predictive distribution , i.e. , when sampling  X   X   X  , the implication of each quoted document related to  X  by reply -to relations and their topic assignments must be considered because the sampling distribution for  X  document  X  must consider its effect on the joint probability of the entire model. Unfortunately, this too can be computationally expensive for large corpora (l ike ours). To circumvent this issue, we can hierarchically sample documents based on reply -to relation network using sequential Monte Carlo [ 9], or approximate the true Gibbs sampling distribution by updating the original smoothing parameter (  X   X  ) to refl ect the expected topic distributions of quoted documents (  X   X  ,  X   X   X  ), where  X   X  ,  X  is the  X  measure,  X   X  which is computed at runtime during sampling. We take the latter approach (see Eq. (5) ). Experiments show that this approximat ion performs well empirically . The approximate Gibbs distribution for JTE -R while sampling  X   X   X  =  X  is given by : JTE -R builds over JTE by encoding reply -to relation s to constrain a post to have similar topic distribution s to those it quotes. An alternative strategy is to make  X   X  and  X   X  author -pair specific. The idea is motivated by the following observation. Observation : When authors reply to others X  viewpoints (by @name or quoting other authors  X  posts), they typically direct their own topical viewpoints with contentious or agreeing expressions to tho se authors. Such exchange s can go back and forth between pairs of authors. T he discussion topics and CA -expressions emitted are thus caused by the a uthor -pairs X  topical inte rests and their nature of interaction s. Let  X   X  be the author of a post  X  , and  X   X  = [  X  target authors (we will also call them targets for short) to whom  X  replies to or quotes in  X  . The pairs of the form  X  = (  X   X  essentially shape s both the topics and CA -expressions emitted in d as contention or agreement on topical viewpoints are almost always directed towards certain target authors. For example, if  X  claims something,  X   X  quotes the cl aim in his post  X  and then contends/agrees by emitting CA -expressions like  X  X ou have no clue X ,  X  X es, I agree X ,  X  X  don X  X  think, X  etc. Clearly, this pair structure is a crucial featur e of discussions/debate forums. Each pair has its unique and shared topical interests and interaction nature (contention or agreement). Thus, it is appropriate to condition  X   X  and  X   X  over author -pairs. We will see in  X  4.4.1 that this model fits the discussion data better . Standard to pic model s do not consider this key piece of information . Although there are extensions to consider authors [ 37], persona [ 31] and interest [ 24], none of them are suitable for considering the pair structure.

We extend the JTE model to incorporate the pair structure. We call the new mode l JTE -P, which condition s the multinomial distribution s over topics and CA -expression types (  X  authors and targets as pairs rather than on documents as in JTE and JTE -R. In its generative process , for each post, the author  X  and the set of targets  X   X  are observed. To generate each term  X  a target,  X  ~  X  X  X  X  (  X   X  ) , is chosen at uniform from  X  = (  X   X  ,  X  ). Then, depending on the switch variable  X  an expression type index  X  is chosen from a multinomial over topic distribution  X   X   X  or CA-expression type distribution  X  the subscript  X  denotes the fact that the distributions are specific to the author -target pair  X  which shape topics and CA -expressions. Finally, the term is emitted by sampling from topic or CA -expression specific multinomial distribution  X   X 
The graphical model in plate notation corresponding to the above process is shown in Figure 1 ( c). Clearly, in JTE -P, the discovery of topics and CA -expressions are guided by the pair structure of reply -to relations in which the collection of posts w as generated. For posterior inference, we again use Gibbs sampling. Note that as  X   X  is observed, sa mpling c is equivalent to sampling the pair  X  = (  X   X  ,  X  ). Its Gibbs sampler i s given by: assigned to topic  X  and expression type  X  respectively. As JTE -P assumes that each pair has a specific topic and expression distribution , we see that Eq. (6, 7) shares topics and expression types across pairs. It is also worthwhile to note that given  X  authors, there are  X   X  2  X  possible pairs. However, the actual number of pairs (i.e. , where the authors have communicated at least once) is muc h less than  X   X  2  X  . Our experimental data consists of 1824 authors and 7 684 actual pairs. Hence we are only modeling 7684 pairs instead of  X  1824 2  X   X  4 million pairs . 4 We now evaluate the proposed models and compare with baselines . We first qualitatively show the CA-express ions and topics discovered by the models . We then evaluate the models quantitatively based on the two objectives of this work : i) Discovering contention and agreement expressions (or CA-ii) Finding contention points or topics in each contentious post . Experiments are also conducted on statistical metrics such as perplexity and KL -Divergence. They show that the interaction models (JTE -R and JTE -P) fit the discussion/debate data better and find more distinctive topics and CA -expressions than JTE . For our experiments, we used debate/ discussion post s from Volconvo 5 . The forum is divided into various domains: Politics, Religion, Society, Science, etc. Each domain consists of multiple threads. Each thread has a set of posts. For each post, we extracted the post id, author, time, domain, ids of all posts to which it replies /quotes , and the post content. In all, w e extracted 26137, 34986 , 22354 and 16525 posts from Politics, Religion, Soc iety and Science domains respectively . Our final data consists of 5978357 tokens, 1824 author s with an average of 346 words per post , and 7684 author -target pairs . To set the background for our quantitative evaluation of the two tasks in the next two sub-sections, we first show the topics and the CA-expressions discovered by our models, and also compare them with topics found by LDA [ 4] and its varia nt SLDA ( sentence-LDA ) [ 21]. We choose LDA as it is the best -known topic model . We use SLDA as it constrain s words in a sentence to be g enerated from a single topic [21 ]. Since CA-expressions may appear with topics in the same sentence, we want to see how it perform s in min ing topics and CA -expressions although SLDA is unable to separate topical terms and CA -expressions . 
For all our experiments here and the subsequent one s, the hyper -parameters for LDA and SLDA were set to the heuristic proposed models , we set  X   X  = 50/  X  ,  X   X  = 50/  X  ,  X  learn the Max -Ent parameters  X  , we randomly sampled 500 terms from our corpus appearing at least 50 times 6 and labeled them as topical (372) or CA -expressions (128) and used the corresponding feature vector of each term (in the context of posts where it occurs) to train the Max -Ent model. We set the number of topics,  X  = 100 and the number of CA-expression types,  X  = 2 (contention and agreement) as in discussion/debate forums, there are usually two expression types (This hypothesis is further statistically validated in  X 4.4.1) . 
Due to space constraints, we are unable to list the topics discovered by all proposed models. As JTE is the basic model (others build over it) and is also closest to LDA and SLDA, we compare the top terms for 10 topics discovered by JTE, LDA and SLDA in Table 1. The top topic al terms by other models are not so different. However , w e will evaluate all the proposed models quantitatively later using the task of identifying topics (or  X  X oints X ) of contention in each contentious post , which is one of our objective s. From Table 1, we can still observe that JTE is quite effective at discovering topics . Its topic al terms are more specific and contain fewer semantic clustering errors ( marked red in bold) than LDA and SLDA. For example, owing to the generative process of JTE , it is able to cluster phrases like  X  X omo sapiens X ,  X  X arwin X  X  theory , X  and  X  X heory of evolution X  in t Life ), which make s the topic more specific. 
It is important to note that both LDA and SLDA can not separate topics and CA-express ions because the y find only topic s. That is why we need joint modeling of topics and CA-express ions. We can see that the topic s of SLDA do contain some CA-express ion s (marked blue in italics ) because SLDA constrains all words in a sentence to be generated from a single topic [ 21] . Since CA-express ion s can co -occur with topical words in a sentence, they are clustered with topics , which is undesirable. Our proposed models solve this prob lem based on the joint model formulation s. 
Next we look at the discovered CA-express ions . We first list some top CA-express ions found by JTE in Table 2 for qualitative inspection . Since CA-express ions found by JTE -R and JTE -P were quite similar to those of JTE among the top 30 terms, they are omitted here. However, all th ree models are quantitatively evaluated in the next sub-section . From Table 2, we see that JTE can discover and cluster many correct CA-express ions, e.g.,  X  X  disagree,  X   X  X  refute X  and  X  X ompletely disagree X  in contention; and  X  X  accept , X   X  X  agree,  X  and  X  X ou X  X e correct  X  in agreement . It additionally discovers more distinctive expressions beyond those observed in the training data of Max -Ent . For example, we find phrases like  X  X  don X  X  buy your X ,  X  X  really doubt X ,  X  X an you prove X  ,  X  X ou fail to X  , and  X  X ou have no clue X  being clustered in contention and phrases like  X  X alid point X ,  X  X ightly said X ,  X  X  do support X  , and  X  X ery well put X  clustered in agreement. These newly discovered phrases are marked blue (in italics) in Tabl e 2 . 
Lastly, we note that CA -expressions of JTE do contain some errors marked red (in bold). However, this is a common issue with all unsupervised topic models for text and the reason is that the objective function of topic models does not always correlate well with human judgments [11 ]. In our case, the issue is mainly due to unigram CA -expressions like  X  X  X ,  X  X our X ,  X  X o X , etc., which by itself do not signify contention or agreement but show up due to higher frequencies in the corpus. There are also phrase errors like  X  X oesn X  X  necessarily X ,  X  X ot really X , etc. A plausible approach to deal with this is to discover significant n -grams based on multi -way contingency tables and statistical tests along with linguistic clues to pre -process and filter such terms. These issues are worth investigating and we defer them to our future work. We now quantitatively evaluate the discovered CA-express ions by all three proposed models in t hree ways. We first evaluate them directly and then evaluate them indirectly through a classification task. Lastly, we examine the sensitivity of the performance with respect to the amount of labeled data. In this case, we do not have an external baseline method as existing joint topic models c annot discover CA -expressions (S ee  X 5) . However, we will show that the interaction models , JTE -R and JTE -P, are superior to JTE . Since CA-express ion s (according to top terms in  X  JTE, JTE -R, and JTE -P are rankings, we evaluate them using precision @ n ( p@n ), which gives the precision at different rank positions. This measure is commonly used to evaluate a ranking when the number of correct items is unknown, which is our case. For computing p@n , we also investigated multi-rater agreement. Three judges independently labeled the top n terms as correct or incorrect for Contention and Agreement. Then, we marked a term to be correct if all the judges deemed it so which was then used to compute p@n . Multi-rater agreement using Fleiss kappa w as greater than 0.80 for all p@n , which imply perfect agreement. This is understandable because one can almost certainly make out whether a term expresses contention , agreement or none.

Figure 2 show s the precision s of contention and agreement expressions for the top 50, 100, 150 , 200 positions (i.e., p @ 50, 100, 150, 200) in the two rankings . We observe that b oth JTE -R and JTE -P are much better than JTE. JTE -P produces the best results. We believe the reason is that JTE -P X  X  expression models being pair specific (  X   X   X  ) can capture CA -expressions better as contention /agreement expressed by an author is almost always directed to some other author s forming author -pairs. We now use the task of classifying a post as being contentious or agreeing to evaluate the discovered CA -express ions . This classification task is also interesting in its own right. However, we should note that our purpose here is not to find the best classifier to classify posts but to indirectly show that the discovered CA -expressions are of high quality as they help to perform the classification better than the standard word n -grams and part -of -speech (POS) n -gram features for text classification.

To perform this experiment , we randomly sampled 1000 posts from our database and asked our human judges (3 graduate students well versed in English) to classify each of those post s as contentious , agreeing , or other . Judges were made to work in isolation to prevent bias. We then labeled a post as contentious or agreeing if all judges deemed it so. In this way, 532 posts were classified as contentious, 405 as agreeing . We inspected the r est 63 posts which had disagreements . We found that 18 of them were the first posts of threads . We removed them as the first posts of threads usual ly start the discussions and do not express agreement or contention. For the remaining 45 of them, 13 posts were partly contentiou s and partly agreeing (e.g.,  X  Although I doubt your claim, I agree with your point that...  X ), and the rest were mostly statements of views with out contention or agree ment . Since the number of these posts is small ( only 45), we did not use them in classification. That is, we considered two mutually exclusive classes (contentious and agreeing) for post classification. We also conducted a labeling agreement study of our judges using Fleiss multi-rater kappa and obtained  X  Fleiss = 0 .87 which shows perfect agreement among the judges according to the scale 7 [25 ]. This high agreement is not un natural because by reading a post one can almost certainly make out whether it is overall contentious or agreeing.

For supervised learning, a challenging issue is the choice of features. Word and POS n -gram s are traditional features . We now compare such features with CA-express ion s discovered by the proposed models. We used the top 1000 (contention and agreement) terms from the CA-express ion rankings as features . Using classification learning algorithms, we compare a lea rner trained on all w ord and POS n -gram s with those trained on CA -expression s induced by our models. We used SVM, Na X ve Bayes (NB), and Logistic Regression (LR). For SVM, we used SVM (http://svmlight.joachims.org ) and for NB and LR , we used the WEKA implementation s (http://www.cs.waikato.ac.nz/ml/weka ). For SVM, we tried linear, RBF, polynomial and sigmoid kernels, but linear kernel performed the best and hence we only report the results of SVM using linear kernel. Linear kernel has also been show n very effective for text classification by many researcher s, e.g., [22 ]. Table 4 reports the accuracy results. Accuracy is appropriate here as the two classes are not skewed and we are interested in both classes. All results were obtained through 10 -fold cross -validation. As the major advantage of CA -expressions arise from dimensionality reduction and feature selection, we also compared with two state -of -the -art feature selection schemes: Information Gain (IG) and Chi -Square test ( X  2 ). We can observe that SVM performed the best among all learners . The accuracy dra matically increases with CA-expression (  X   X  ) features . JTE , JTE -R, and J TE -P progressively improve the accuracy beyond those obtained by traditional n -gram features. JTE -P p erformed the best. Feature selection schemes also improved performance but the proposed models outperform feature selection schemes as well. All accuracy improvements are significant ( p &lt;0.001) using a two tailed t -test over 10-fold CV. This clearly shows that CA -express ion s are of high quality . We also experimented with different num bers of top CA-expressions as features to see how they affect the accuracy results (Figure 3). Here only SVM results are reported as it performed best . We observe in Figure 3 that when the number of CA -expressions reaches about 1000, the classification accuracies start to level -off for all models . 
In fact , for JT E and JTE -R, since we know the per post distribution of CA -expression type (  X   X   X  ), we can also classify a post directly without supervised learning. For each post, if the probability mass associated with type contention is higher than that of agreement, it is classified as a contentio us post else an agreeing post. The accuracies using this scheme are: JTE: 74.9 %, JTE -R: 75.9% , which are respectable. It is understandable the y are lower than supervised methods because supervised learning uses a large number of features. Note that JTE -P cannot be used directly here as its CA -expression types are conditioned over author pairs (  X   X  ) rather than documents (  X   X   X  ) as in JTE and JTE -R. Having evaluated CA -expressions, we now examine the sensitivity of model performance with respect to the amount of labeled data. In Table 3, we report the p @50, 100 values for all models across contention and agreement on different sizes of labeled term set s used for learning the Max -Ent  X  parameters. We see that with more labeled terms, the results improve which is intuitive as more labeled data will result in more accurate Max -Ent estimates. We used 500 labe led terms in all our experiments. We now turn to the tas k of automatically discovering points of contention in contentious posts. By  X  X oints  X , we mean the topical terms on which the contention has been expressed. We employ the JTE and JTE -R models in the following manner using estimated  X   X   X  . Note that JTE -P cannot be directly used for this task pair specific (  X   X   X  ) rather than post specific. However, since we know the posterior topic assignments of  X   X   X  , we can get a posterior estimate of  X   X   X  for JTE -P using  X   X  ,  X   X  =  X  X  X  X  X  X  X   X 
Given a contentious post  X  , we first select the top  X  topics that are mentioned in  X  according to its topic distribution,  X  contentious expression  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  , we emit the topical terms of topics in  X   X  which appear within a word window of  X  from  X  in  X  . More precisely, we emit the set  X  = {  X  |  X   X  X  X  X   X   X  ,  X  X  X  X   X  , |  X  X  X  X  X  X  (  X  )  X  X  X  X  X  X  X  (  X  )|  X  X  X  } , where  X  X  X  X  X  X  ( X ) returns the position index of the word/phrase in a document  X  . To compute the intersection  X   X  X  X  X  X  X   X   X  , we need a threshold. This is so because the Dirichlet distribution has a smoothing effect which assigns some non -zero probability mass to every term in the vocabulary for each topic  X  . So for computing the intersection, we considered only terms in  X   X   X  which have  X  (  X  |  X  ) =  X  probability masses lower than 0.001 are more due to the smoothing effect of the Dirichlet distribution than true correlation. In an actual application, the values for  X  and  X  can be set according to the user X  X  ne ed. In this experiment , we use  X  = 3 and  X  = 5, which are reasonable because a post normally does not talk about many topics (  X  ), and the contention points ( topic al terms ) appear close to the contentio us expressions. 
For comparison, we also designed a baseline. For each contentious expression  X   X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  , we emit the noun s and noun phrase s within the same window  X  as the points of contention in  X  . This baseline is reasonable because topical terms are usually nouns and noun phrases and are near contentious expressions. But we should note that t his baseline can not standalone as it has to rely on the expression models  X 
Next, to evaluate the performance of these methods in discovering points of contention, we randomly selected 125 contentious posts f rom each domain in our dataset and employed the aforementioned methods on the posts to discover the points of contention in each post. Then we asked two human judges (graduate students fluent in English) to manually judge the results produced by each method for each post. We asked them to report the precision (% of term s discovered by a method which are indeed valid point s of contention in a post) and recall (% of all valid points of contention which were discovered) for each post. In Table 5, we report the average precision and recall for 1 25 posts in each domain by the two human judges J 1 and J 2 methods. Since this judging task i s subjective, the differences in the results from the two judges are not surprising. We observe that across all d omains, JTE, JTE -R and JTE -P progressively improve perform ance over the baseline. Note that it is difficult to compute agreement of two judges using kappa because although the models identify topic terms (which are the same for both judges ), the judges also identify additional terms (for recall calculation) which are not found by the models. We now compare the proposed models across statistical metrics : perplexity and KL -Divergence. To measure the ability of JTE, JTE -R and JTE -P to act as good  X  X enerative X  model s, we computed the test -set (see below) perplexity under estimated parameters and also compared with the resulting values of LDA and SLDA models.

Perplexity, widely used in the statistical language modeling community to assess the predictive power of a model, is algebraically equivalent to the inverse of the geometric mean per -word likelihood. A lower perplexity score indicates a better generalization performan ce. As perplexity monotonically decreases with increase in log -likelihood (by definition), it impl ies that lower perplexity is better since higher log -likelihood on training data means that the model  X  X its X  the data better and a higher log -likelihood on th e test set implies that the model can  X  X xplain X  the unseen data better. Thus , lower perplexity implies that the words are  X  X ess surprising X  to the model. In our experiments we used 1 5% of our data (in  X 4) as our held out test set. As JTE -P requires pair structures, for proper comparison across all models , the corpus was restricted to posts which have at least one quotation . This is reasonable as quoting /replying is an integral part of debates/discussions and we found that about 77% of all pos ts have quoted /replied -to at least one other post (t his count excludes the first posts of threads as they start the discussion s and usually have nobody to quote /reply -to ). The perplexity ( PPX ) of JTE given the learned model parameters  X   X   X  and the state of the Markov chain  X  = {  X  X  X  ,  X  X  X  ,  X  X  X   X   X  } is given by : where  X   X  denotes the total number of terms in  X  better estimate, we average the per -word log -likelihood over S different chains.  X  s denotes the Markov state corresponding to chain s . From the generative process of JTE, we get : where  X   X   X   X  denotes the number of times term  X   X  X  X  occurred in  X   X  accordin g to the query sampler . In a similar way, the perplexities of the other three models can also be derived.

We compare model perplexities across Gibbs iterations with  X  = 100 and  X  = 2 in Table 6 (a). We note the following observations: i) Noise Reduction : The proposed models attain significantly (see caption of Table 6(a)) lower perplexities with fewer iterations than LDA and SLDA showing that the new models fit the debate/discussion forum data better and clustering using the new f ramework contain s less noise . This improvement is attributed to the capabilities of the framework to separate and account for CA-express ions using  X   X  . ii) The number of CA-express ion types , E : In  X 2 and 4 , we hypothesized that for discussions/debates we mostly have two express ion types: contention and agreement. To test this hypothesis , we ran JTE perplexity slightly increases (than E = 2 ) showing performance degradation. It is interesting to note that the number of CA -express ion types E impacts perplexity differently than the number of topics T (as the decrease in perplexity usually slows in inverse proportions with increase in the number of topics [ 4]). We also tried increasing  X  to 5, 10, etc. However, t he performance deteriorated with increase in model perplexity. This result supports our prior hypothesis of  X  = 2 in debate forum s. Another important measure for topic m odels is topic distinctiveness [24 ]. Here, we want to assess how distinctive the discovered topics and CA-express ion s are. To measure topic and CA-express ion distinctiveness, we computed the average topic and CA-express ion distribution (  X   X   X  and  X   X   X  ) separations betw een all pairs of latent topics and CA -express ion types . To measure separations , we choose KL -Divergen ce as our metric as suggested in [24 ]. Clearly, for more distinctive topic and CA-express ion discovery, it is desirable to have higher average KL -Divergence. Table 6(b) shows the comparison results. A gain, a s JTE -P requires pair structures, f or proper comparison, all models were run on the restricted corpus where posts have at least one quotation. We observe that topics discovered by the new models are more distinctive than LDA and SLDA. For both topics and CA-express ions, JTE -R performed better than JTE showing that reply relations are highly beneficial . JTE -P with pair structures performed the best . Table 6(b) also report s the average separations of per document distri bution of topics and expressions (  X   X   X  ) . For models JTE and J TE -R, h aving higher average KL -Divergence for  X   X  and  X   X  implies that documents are well separated based on the estimated topics and two CA-express ion types exhibited. We see that both JTE and JTE -R obtain higher average KL -Divergence for  X   X  than LDA and SLDA . Such separations are particularly useful when topic models are used for performing retrieval [ 19]. Lastly, we look at the average per pair separation of topics and CA-expressions for JTE -P. Clearly, the KL -Divergence values indicate good separations. T his information may be further used to mine contending author -pair s or classify these pairs according to interests , i.e., the posterior on  X   X   X  can be used to make predictions on the interaction nature of any two authors . However, these are beyond the scope of this paper. We will study them in future. Although limited research has been done on fine -grained mining of contentions in discussion /debate forums , there are several general research areas that are related to our work.
 Sentiment analysis : Sentiment analysi s mines positive and negative opinions from text [ 27,34 ]. Agreement and contention are different concepts . S entiments are mainly indicated by sentiment words (e.g., great , good , bad , and poor ), while contention and agreement are indicated by CA -expressions . Sentiment analysis approaches are thus not directly applicable to our tasks and we also need to discover CA-expressions . However, from a modeling perspective, there exist several joint sentiment and topic models which are related to our work and are discussed below . O ther related works in sentiment analysis include pro -con classi fication [3], contradictions [ 23, 36], attitude [38] and negotiations [39 ]. Topic models : Topic models such as Latent Dirichlet Allocation (LDA) [ 4] have been shown effective in mining topics in large text collection s. There have been many extensions to correlated [ 5], supervised [7 ], multi-grain [43 ] and sequential [ 13] topic models. In the context of our JTE -R model, c onstraining document -topic distributions of a post  X  to be similar to its quoted post s in  X  related to topic modeling with network structure [ 12, 17, 28, 33 ] where for each pair of documents, a binary link variable is conditioned on their contents . This requires sampling all  X  which can be computationally very expensive for large corpora. Chang and Blei [10 ] improved the efficiency by treating non -lin ks as hidden. In [40 ], links were assumed to be fixed and topics were conditioned on both the document itself and other linked documents. [29] used a network regularization approach to ensur e topics of neighboring documents in a network are similar. This work is based on p LSA [ 20] and EM rather than LDA and Gibbs sampling . Our approach is simpler. However, all these existing models are mainly used to find topics for corpus exploration. When applied to discussion/debate forums, they are unable to discover contentious topics and CA -expressions at the same time. 
There have also been attempts t o jointly model both topics and opinions in sentiment analysis . For example, the ME -LDA model [47 ] add ed a sentence plate and used m aximum-e ntropy to separate topics and sentiment terms . [26 ] add ed a s entiment variable to LDA and conditioned topic s over sentiments. T he TSM model [ 30] encodes positive, negative and neutral sentiment variables and a background word variable to separate topical words from background words. In the AS UM model [21 ], for each sentence a sentiment is chosen over a multinomial , and a topic is chosen conditioned on the sentiment. However, contentio n and agreeing expressions are emitted different ly. U nlike sentiments and topics which are mostly emitted in the same sentence, contention and agreeing expressions often interleave with user s X  topical viewpoints and m ay not be in the same sentence. Most importantly, JTE -R and JTE -P can capture the key characteristics of discussions : topic and author interactions , using reply relations and pair structures . E xisting joint models are unable to use the m. Support/oppose classification : There have been works aimed at put ting debate author s into support/oppos e camps. In [15 ], conversation s are classified into agree, disagree, backchannel and other classes. In [1], the reply network was used to classify discussion participants into camps . In [32 ], a rule-based approach first classif ies replies in to contention and agreement classes, and max -cut then classifies author s into opposite camps. N one of these works , however, mine s CA -expressions or contentious topics . Stances in o nline debates : In [ 40], a n unsupervised classification method was proposed to recognize stances in debates. In [42 ], speaker stances were mined using a SVM classifier. In [ 8], collective classification techniques were employed . Clearly, our work is different as these existing classification methods do not mine CA -express ions or point s of contentio n in each post. This paper proposed the task of mining contentions in discussion /debate forums , and presented three new models as a principled way to jointly model and mine topics and linguistic expressions indicating agreement s and contentions consider ing topic and author interactions . Existing models are unable to perform these tasks. Specifically, a joint model (JTE) of topics and CA-express ions was first proposed, which was then improved by encod ing two key features of discussions or debates, i.e., topical interactions (using reply -to relations) and author interactions (through pair structures), which yielded the JTE -R and JTE -P models. Experimental results showed that t he proposed models outperformed baselines for our tasks : i) d iscovering topic s and CA-express ion s; and ii) f or each contentious post, discovering the contention points or topics. Statistical e xperiments of perplexity and KL -Divergence were also conducted. They showed that the proposed models fit the data better and discover more distinctive topics and CA-expressions . In all experiments, the interaction models JTE -R and JTE -P consistently gave better results. This work is supported in part by National Science Foundation (NSF) under grant no. IIS -1111092.
