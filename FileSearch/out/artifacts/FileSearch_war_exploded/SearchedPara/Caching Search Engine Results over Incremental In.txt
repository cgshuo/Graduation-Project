 A Web search engine must update its index periodically to incorporate changes to the Web. We argue in this pa-per that index updates fundamentally impact the design of search engine result caches, a performance-critical compo-nent of modern search engines. Index updates lead to the problem of cache invalidation : invalidating cached entries of queries whose results have changed. Na  X   X ve approaches, such as flushing the entire cache upon every index update, lead to poor performance and in fact, render caching futile when the frequency of updates is high. Solving the invalida-tion problem efficiently corresponds to predicting accurately which queries will produce different results if re-evaluated, given the actual changes to the index.

To obtain this property, we propose a framework for de-veloping invalidation predictors and define metrics to eval-uate invalidation schemes. We describe concrete predictors using this framework and compare them against a baseline that uses a cache invalidation scheme based on time-to-live (TTL). Evaluation over Wikipedia documents using a query log from the Yahoo! search engine shows that selective in-validation of cached search results can lower the number of unnecessary query evaluations by as much as 30% compared to a baseline scheme, while returning results of similar fresh-ness. In general, our predictors enable fewer unnecessary in-validations and fewer stale results compared to a TTL-only scheme for similar freshness of results.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Performance, Experimentation Search engine caching, Real-time indexing
Search engines are often described in the literature as building indices in batch mode. This means that the phases of crawling, indexing and serving queries occur in genera-tions, with generation n + 1 being prepared in a staging area while generation n is live. When generation n + 1 is ready, it replaces generation n . The length of each crawl cycle is measured in weeks, implying that the index may represent data that is several weeks stale [8, 9].

In reality, modern search engines try to keep at least some portions of their index relatively up to date, with latency measured in hours. News search engines, e-commerce sites and enterprise search systems all strive to surface docu-ments in search results within minutes of acquiring those documents (by crawling or ingesting feeds). This is realized by modifying the live index (mostly by append operations) rather than replacing it with the next generation. Such en-gines are said to have incremental indices .

Caching of search results has long been recognized as an important optimization step in search engines. Its setting is as follows. The engine dedicates some fixed-size fast mem-ory cache that can store up to k search result pages. For each query in the stream of user-submitted search queries, the engine first looks it up in the cache, and if results for that query are stored in the cache -a cache hit -it quickly returns the cached results to the user. Upon a cache miss -when the query X  X  results are not cached -the engine eval-uates the query and computes its results. The results are returned to the user, and are also forwarded to the cache. When the cache is not full, it caches the newly computed re-sults. Otherwise, the cache X  X  replacement policy may decide to evict some currently cached set of results to make room for the newly computed set.

An underlying assumption of caching applications is that the same request, when repeated, will result in the same response that was previously computed. Hence returning the cached entry does not degrade the application. This does not hold in incremental indexing situations, where the searchable corpus is constantly being updated and thus the results of any query can potentially change at any time. In such cases, the engine must decide whether to re-evaluate re-peated queries, thereby reducing the effectiveness of caching their results, or to save computational resources at the risk of returning stale (outdated) cached entries. Existing search applications apply simple solutions to this dilemma, rang-ing from performing no caching of search results at all to applying time-to-live (TTL) policies on cached entries so as to ensure worst-case bounds on staleness of results.
Contributions. This paper studies the problem of search results caching over incremental indices. Our goal is to se-lectively invalidate the cached results only of those queries whose results are actually affected by the updates to the un-derlying index. Cached results of queries that are unaffected by the index changes will continue to be served. We formu-late this as a prediction problem, in which a component that is aware of both the new content being indexed and the con-tents of the cache, invalidates cached entries it estimates that have become stale. We define metrics by which to mea-sure the performance of these predictions, propose a realiz-ing architecture for incorporating such predictors into search engines, and measure the performance of several prediction policies. Our results indicate that selective invalidation of cached search results can lower the number of queries invali-dated unnecessarily by roughly 30% compared to a baseline scheme, while returning results of equal freshness.
Roadmap. The remainder of this paper is organized as follows. Section 2 surveys related work on search results caching and incremental indexing. Section 3 defines the ref-erence architecture on which this work is based. Section 4 presents schemes for selectively invalidating cached search results as the search index ingests new content. We also discuss in this section the metrics we use to evaluate cache invalidation schemes. Section 5 describes the experimental setup and reports our results. We conclude in Section 6.
Caching of search results was noted as an optimization technique of search engines in the late 1990s by Brin and Page [5]. The first to publish an in-depth study of search re-sults caching was Markatos, in 2001 [17]. He applied classical cache replacement policies (e.g. LRU and variants) on a log of queries submitted to the Excite search engine, and com-pared the resulting hit-ratios, which peaked around 30%. PDC (Probability Driven Caching) [14] and SDC (Static Dynamic Caching) [10] are caching algorithms specifically tailored to the locality of reference present in search engine query streams, both proposed originally in 2003. PDC di-vides the cache between an SLRU segment that caches top-n queries, and a priority queue that caches deeper result pages ( e.g. , results 11-20 of queries). The priority queue estimates the probability of each deep result page to be queried in the near future, and evicts the page least likely to be queried. SDC also divides its cache into two areas, where the first is a read-only (static) cache of results for  X  X ead X  (perpetually popular) queries, while the second area dynamically caches results for other queries using any replacement policy (e.g. LRU or PDC).

The AC scheme was proposed by Baeza-Yates et al. in 2007 [3]. It applies a predictor that estimates the  X  X epeata-bility X  of each query. Several predictors and the features they rely on were evaluated, showing that this technique is able to outperform SDC.

Gan and Suel [12] study a weighted version of search re-sults caching that optimizes the work involved in evaluating the cache misses rather than the hit ratios. They argue that different queries incur different computational costs.
Lempel and Moran studied the problem of caching search engine results in the theoretical framework of competitive analysis [15]. For a certain stochastic model of search en-gine query streams, they showed an online caching algorithm whose expected number of cache misses is no worse than four times that of any online algorithm.

Search results are not the only data cached in search en-gines. Saraiva et al. [19] proposed a two-level caching scheme that combines caching of search results with the caching of frequently accessed postings lists. Long and Suel extend this idea to also caching intersections of postings lists of pairs of terms that are often co-used in queries[16]. Baeza-Yates et al. investigate trade-offs between result and post-ing list caches, and propose a new algorithm for statically caching posting lists that outperform previous ones [2]. It should be noted, however, that in the massively distributed systems that comprise Web search engines, caching of post-ings lists and caching of search results may not necessarily compete on the RAM resources of the same machine. The work of Skobeltsyn et al. describes the ResIn architecture, which lines up a cache of results and a pruned index [20]. They show that the cache of results shapes the query traffic in ways that impact the performance of previous techniques for index pruning, so assessing such mechanisms in isolation may lead to poor performance for search engines.

The above works do not address what happens to the cached results when the underlying index, over which queries are evaluated, is updated. To this effect, one should distin-guish between incremental indexing techniques, that incor-porate updates into the  X  X ive X  index as it is serving queries, and non-incremental settings. Starting with the latter case, we note that large scale systems may choose to not incre-mentally update their indices due to the large cost of update operations and the interference of incremental updates with the capability to keep serving queries at high rates [18, 7]. Rather, they manage content updates at a higher level.
Shadowing is a common index replacement scheme [1, 8]: while one immutable index is serving queries, a second index is built in the background from newly crawled content. Once the new index is ready, the engine shifts its service from the older index to the newly built one. In this approach, indexed content is fully updated upon a new index generation, and the results cache is often flushed at that time.

Another approach, that performs updates at a finer level of granularity than shadowing, uses stop-press or delta in-dices [7, 11, 21]. Here, the engine maintains a large main in-dex, which is rebuilt at relatively large intervals, along with a smaller delta index which is rebuilt at a higher rate and reflects the new content that arrived since the main index was built. When building the next main index, the exist-ing main index and the latest corresponding delta index are merged. Query evaluation in this approach is a federated task, requiring the merging of the results returned by both indices. The main index can keep its own cache, as its results remain stable over long periods of time.

We note that the vast literature on incremental indexing is beyond the scope of this paper. However, we are not aware of any work that addressed the maintenance of the search results cache in such settings. In incremental settings, systems typically either invalidate results whose age exceeds some threshold, or forego caching altogether.
At a high level, Web search engines have three major com-ponents: a crawler , an indexer , and a runtime component that is dominated by the query processor (Figure 1). The crawler continuously updates the engine X  X  document collec-tion by fetching new or modified documents from the Web, and deleting documents that are no longer available. The indexer periodically processes the document collection and generates a new inverted file and auxiliary data structures. Finally, query processors evaluate user queries using the in-verted file produced by the indexer [1, 5].

The runtime component of a Web search engine typically also includes a cache of search results, located between the engine X  X  front-end and its query processor, as depicted in Figure 1. The cache provides two desirable benefits: (1) it reduces the average latency perceived by a user, and (2) it reduces the load on back-end query processors. Such a cache may run in the same machines as query processors or in separate machines. To simplify our discussion, we assume that caches of results reside in separate machines, and that most resources of those machines are available to the cache.
However, as the index evolves, the cached results of certain queries no longer reflect the latest content and become stale. By stale queries, we precisely mean queries for which the top-k results change because of an index update. In order to keep serving fresh search results, the engine must inval-idate those cached entries. One trivial invalidation mecha-nism is to have the indexers indicate whenever the inverted index changes, thereby prompting the cache to invalidate all queries. When the index is updated often, the frequent flushing of the cache severely impacts its hit rate, perhaps to the point of rendering caching worthless.

To efficiently invalidate cache entries, we assume that the indexer is able to propagate information to the runtime com-ponent upon changes to the index. More concretely, we as-sume that even though the crawler continuously updates the document corpus, the indexer only generates a new version every  X  t time. Upon a new version, we assume that a set of documents D have each been either inserted to or deleted from the index. Note that this simple model subsumes in-cremental (real-time) indexing, in the sense that the indexer can index every new or removed document by setting  X  t to a very small value and having D be a singleton set.
We embody the above idea by introducing a new compo-nent to the search engine architecture  X  the Cache Invalida-tion Predictor (CIP).
Cache invalidation predictors bridge the indexing and run-time processes of a search engine, which typically do not in-teract in search engines operating in batch mode, or limit their interaction to synchronization and locking.

When introducing cache invalidation prediction into a sys-tem, the very front end of the runtime system  X  the cache  X  needs to become aware of documents coming into the index-ing pipeline. We thus envision building a CIP in two major pieces, as depicted in Figure 2: The synopsis generator: resides in the ingestion pipeline, e.g. , right after the tokenizer , and is responsible for prepar-ing synopses of the new documents coming in. The synopses may be as robust as the full token stream and other ranking features of each and every incoming document, or as lean as nothing at all (in which case the generator is trivial). The invalidator: implements an invalidation policy. It receives synopses of documents prepared by the synopsis generator, and through interaction with the runtime sys-tem, decides which cached entries to invalidate. The inter-action may be complex, such as evaluating each synopsis-query pair, or simplistic (ignoring the synopses altogether).
Section 4.1 describes various pairings of synopsis gener-ators and invalidators, which together constitute a CIP. In each case we note the computational complexities of both components, as well as the communication between them.
Our architecture allows composing different synopsis gen-erators with different invalidators, yielding a large variety of behaviors. Below we show how the traditional age-based time-to-live policy (TTL) fits within the framework, and proceed to describe several policies of synopsis generators and invalidators, which we later compose in our experiments.
Age-based policies consider each cached entry to be valid for a certain amount of time  X  after evaluation. Each entry is expired, or invalidated, once its age reaches  X  . At the two extremes,  X  = 1 implies no caching as results must be recomputed for each and every query. With  X  =  X  no invalidation ever happens, and results are considered fresh as long as they are in the cache. As the value of  X  increases from 1 to  X  , the number of unnecessary invalidations decreases, whereas the number of missed invalidations increases.
TTL-based policies ignore incoming content. In terms of our architecture, the synopsis generator is null in TTL poli-cies, and no communication is required. The invalidator can be realized with a complexity of O (1) per query.
To improve over TTL, we exploit the fact that the cached results for a given query are its top-k scoring documents. By approximating the score of an incoming document to a query we can try to predict whether it affects its top-k results.
The synopsis generator attempts to send compact repre-sentations of a document X  X  score attributes, albeit to un-known queries. Its main output is a vector of the docu-ment X  X  top-scoring TF-IDF terms [4]  X  these are the terms for which the document might score highly for. To control the length of the synopsis, the generator sends a fraction  X  of each document X  X  top terms in the vector.  X  can range from zero (empty synopsis) to 1 (all terms, full synopsis). Intuitively, selective (short) synopses will lower the commu-nication complexity of the CIP but will increase its error rate, as less information is available to the invalidator.
Another observation, applicable to document revisions, is that insignificant revisions typically do not affect the rank-ings achieved by the document. Consequently, cached en-tries should not be invalidated on account of minor revisions of documents. Hence, we estimate the difference between each document revision and its previously encountered ver-sion, and only produce a synopsis if the difference is above a modification threshold  X  . Concretely, we use the weighted Jaccard similarity [13] as a similarity measure, where the weight of term t in document D is the number of occur-rences of t in D . This measure can be efficiently and ac-curately estimated by using shingles [6]. Increasing  X  will result in fewer synopses being produced, thereby lowering the communication complexity of the CIP, at the cost of failing to invalidate cached entries that have become stale.
Once a synopsis is generated, the CIP invalidators make a simplifying assumption that a document (and hence, a synopsis) only affects the results of queries that it matches. While this is true for most synopses and queries, it does not always hold. For example, a document that does not match a query may still change term statistics that affect the scores of documents that do. With this assumption, an invalidator first identifies all queries (and only those) matched by the synopsis. A synopsis matches query q if it contains all of q  X  X  terms in conjunctive query models, or any term in dis-junctive models. Then, the invalidator may invalidate all queries matched by a synopsis (note that match computa-tion can be efficiently implemented with an inverted index over the cached query set). Alternatively, it can apply score thresholding  X  namely, using the same ranking function as the underlying search engine, it computes the score of the synopsis with respect to cached query q , and only invali-dates q if the computed score exceeds that of q  X  X  last cached result. This score projection procedure, which tries to de-termine whether a new document is in the top-k results of a cached query, is feasible for many ranking functions, e.g. TF-IDF, probabilistic ranking, etc. However, it is inherently imperfect for an incremental index where cached scores can-not be compared with newly computed ones as the index X  X  term statistics drift. We denote by the indicator variable 1 whether score thresholding is applied.

Similarly to TTL, CIP applies age-based invalidation  X  they invalidate all queries whose age exceeds a certain time-to-live threshold, denoted by  X  . This bounds the maximum staleness of the cached results.

Finally, all CIPs invalidate any cached results that include documents that have been deleted. Clearly, all invalidation due to deleted documents are correct.

Table 1 summarizes the parameters of our CIP policies.
Upon processing a new document set D , a Cache Invalida-tion Predictor (CIP) makes a decision whether to invalidate or not each cached query. We say CIP is positive ( p ) about query q when CIP estimates that the ingestion of D by the corpus will change q  X  X  results, and so q  X  X  entry should be invalidated as it is now stale. CIP is negative ( n ) about q when it estimates that q  X  X  cached results do not change with the ingestion of document set D .

For each query, we can compare CIP X  X  decision with an oracle that knows exactly if the ingestion of D by the corpus will change q  X  X  results or not -as if it had re-run every cached query upon indexing D . This leads to four possible cases (depending on whether CIP or the oracle decide positive or negative for the query). Let us call them { pp,pn,np,nn } , where the first letter indicates the decision of the CIP and the second the oracle X  X .

There are two types of errors CIP might make. In a false positive ( pn ), CIP wrongly invalidates q  X  X  results, leading to an unnecessary evaluation of q if it is submitted again. In a false negative ( np ), CIP wrongly keeps q  X  X  results, causing the cache to return stale results whenever q is subsequently submitted until its eventual invalidation. If we have a set of cached queries Q of size Q , we can compute the total number of queries falling in each one of these categories. Let us call these totals PN and NP respectively.
 These two types of errors have very different consequences. The cost of a false positive is essentially computational, whereas false negatives hurt quality of results. Conservative policies, aiming to reduce the probability of users receiving stale results, will focus on lowering false negatives. More ag-gressive policies will focus on system performance and will tolerate some staleness by lowering false positives. This im-plies that CIPs should be evaluated along both dimensions -each application will determine the most suitable compro-mise between false positive and false negatives. We note that modern search engines are conservative, and are will-ing to devote computational resources to keep their results as fresh as possible ( X  X eeping up with the Web X ).
We use the ratio of false positives and false negatives, de-noted FP and FN respectively, as our performance metrics (see Table 2 for definitions). High FP implies many wasteful computation cycles due to unnecessary invalidations. High FN implies many stale results in the cache, leading to po-tentially many of them being returned to the users.
The metrics above were defined with respect to the con-tents of the cache given a single document set D . In an incremental setting, a CIP would receive a sequence of doc-ument sets, D 1 ,D 2 ,... . It is important to note that a false positive made by CIP when processing D t can propagate er-rors (from the users X  standpoint) into the future. Consider a query q , upon which CIP incurs a false negative ( np ) when processing D t , thereby leaving q  X  X  stale results in the cache. Assume that when processing D t +1 , CIP correctly labels q as negative ( nn ) and does not invalidate its results, as the documents in D t +1 indeed do not affect q  X  X  results. While the predictor made a correct point-in-time decision at time t + 1, q  X  X  cached results remain stale, and any user submit-ting q until such time when CIP invalidates q will receive stale results. Let S be the set of cached queries whose re-sults are stale. Note that after processing any document set, | S | X  NP since stale queries may have persisted in the cache from false negatives made on earlier document sets.
False positives and false negatives are asymmetrical also in another aspect: a false positive on query q will incur a single (redundant) re-evaluation of q , so the cost for the engine is irrespective of the query stream. In contrast, the cost of a false negative on q (and any stale query q  X  S in general) depends on the frequency of q in the query stream, as the cache returns stale results for each request of q . We therefore define a Stale Traffic ratio metric ST (see Table 2), in which the cost of each stale query q  X  S is weighted by its frequency, denoted f q . The quantity F in the formula of ST is the sum of all query frequencies F = P q  X  X  f q .
Note that the metrics above are defined irrespective of the cache replacement policy that may be used. In particular, a CIP false negative on q is harmless if the cache replacement policy evicts q before the next request of q . The interaction between cache invalidation due to the dynamics of the un-derlying corpus and cache replacement due to the dynamics of the query stream is subject of future work.
This section presents our evaluation framework. We use a large Web corpus and a real query log from the Yahoo! search engine to evaluate our CIP policies. Note that our setup makes several simplifying assumptions to make tractable the problem of simulating a crawler, an indexer, a cache, and a realistic query load interacting in a dynamic fashion.
As a Web-representative dynamic corpus, we use the his-tory log of the (English) Wikipedia 1 , the largest time-varying dataset publicly available on the Web. This log contains all revisions of 3 , 466 , 475 unique pages between Jan 1, 2006 and Jan 1, 2008. It was constructed from two sources: the lat-est public dump from the Internet Archive 2 , with the infor-mation about page creations and updates, and the deletion statistics available from Wikimedia 3 .

The initial snapshot on Jan 1, 2006 contained 904 , 056 in-dividual pages. We processed Wikipedia revisions in single-day batches called epochs , each containing the revisions that correspond to one day of Wikipedia history. The average number of revisions per day is 41 , 851 (i.e., about 4% of the initial corpus), consisting mostly of page modifications (95 . 22%) and new page creations (4 . 16%). The (uncom-pressed) size of the corpus, with all revisions, is 2.8 TB.
We focus on conjunctive queries (the de facto standard for Web search)  X  i.e. , documents match a query only when containing all query terms. Our experiments use the open-source Lucene search library as the underlying index and runtime engine 4 . Lucene uses TF-IDF for scoring.
We assess the performance of predictors on a fixed rep-resentative set of queries Q , which represents a fixed set of cached queries. The synopsis generator consumes each epoch in turns, sends synopses of its documents to the in-validator, and the invalidator makes a decision on each query q  X  Q . We compute the  X  X round truth X  oracle by indexing the epoch in Lucene and running all queries, retrieving the top-10 documents per query. The ground truth oracle is con-servative and declares a query as invalid upon any change to the ranking of its top-10 results. We record the performance of each CIP relative to the ground truth, and track its set of stale queries. The performance numbers reported in the next section are all averaged, per CIP policy, over a history of 120 consecutive epochs (days) of Wikipedia revisions.
To generate the set of cached queries Q , we performed a uniform sample, with repetitions, of 10 , 000 queries from the Yahoo! Web search log, sampled from a query log recorded on May 4 and May 5, 2008, which resulted in a user clicking on a page from the en.wikipedia.org domain. Q consists of the 9,234 unique queries in the sample. The multiset of queries was used to derive the frequency f q of each q  X  Q , for computing the stale traffic ratio ( ST ).

Our choice of working with a fixed query set stems from our desire to isolate the performance of the CIP policies from the effects of a dynamic cache and its parameters (e.g., cache size and replacement policies). The dynamic study, which is plausible and interesting, is left for future research.
We start by analyzing the results obtained for three stan-dard policies: no caching, no invalidation (static cache), and TTL caching (invalidating all queries after a fixed period of time). Table 3 reports their performance. Not invalidating entries causes the cache to return stale results. Not caching guarantees that no results are stale, but it also forces the engine to process queries unnecessarily as previous work on caching has shown. Using a TTL value improves the overall situation, since it reduces the amount of stale traffic com-pared to not invalidating entries, but it still generates a sig-nificant number of false positives and negatives. Finally, a basic CIP policy with the following parameters is able to re-duce the amount of stale traffic significantly, with very few false negatives -similarly to the  X  X o cache X  case -at the cost of many false positives: Basic CIP:  X  =  X  , X  = 0 , X  = 1 , and 1 s = false
In words, our Basic CIP does not expire queries (  X  =  X  ), does not exclude documents based on similarity (  X  = 0), does not exclude terms (  X  = 1), and does not use score thresholding. The synopsis generator of the Basic CIP es-sentially sends each document in its entirety to the predic-tor, which then invalidates each query whose terms appear in conjunction in any synopsis.

Ruling out a cache is ideal with respect to freshness of results, but it is undesirable from a performance perspec-tive. The Basic CIP is able to achieve a similar degree of freshness, while benefiting from cache hits. We next assess how changing the CIP parameters affects both freshness and performance.
 Dynamics of stale traffic: Over time, errors due to false negatives accumulate, and imply an increasingly high stale traffic ratio (ST). The impact is most severe for frequent queries. A false negative can be fixed by either (1) a CIP positive, either true or false; or (2) an age threshold expira-tion. CIP positives depend on the arrival rate of matching documents: if a match never happens after a false nega-tive, then the latter will persist forever. Consequently, it is critical to augment the CIP with a finite age threshold  X  , not only to bound the maximum result set age, but also to guarantee that ST converges.

Figure 3 shows how stale traffic evolves over time with three CIP instances. The CIP instances in the figure use a synopsis of the top 20% terms (  X  = 0 . 2), employ score thresholding (1 s = true ), and have different  X  values. For  X  =  X  , ST grows, albeit in a declining pace, and eventually exceeds 30% without stabilizing. For  X  = 5 and  X  = 10, ST stabilizes within a few epochs after the first expiration. Infinite  X  is practical only when the predictor X  X  FN ratio is negligible, e.g. , with the Basic CIP.
 Varying  X  and  X  : Figure 4 depicts the behavior of CIP for different values of synopsis size  X  and time-to-live  X  , also em-ploying score thresholding (1 s = true ). In this experiment, we create synopses for all document revisions (  X  = 0). In addition to plotting the TTL baseline, we show 5 CIP plots, each having a fixed value of  X  . The rightmost CIP plot (circle marks) does not apply score thresholding (1 s =false) while the other 4 plots do. The six points in each CIP plot correspond to increments of 0 . 1 in  X  , from  X  = 0 . 5 at the top point of each plot to  X  = 1 . 0 at the bottom. The Basic CIP is the bottom point in the rightmost CIP plot.

Score thresholding reduces false positives but increases the false negatives ratio (FN). The  X  parameter only affects the positive predictions, hence it has no impact on FN. How-Figure 3: Convergence of stale traffic metric for CIP instantiations. For finite age threshold  X  , stale traffic stabilizes shortly after  X  . For infinite  X  , stale traffic grows throughout the evaluation. Table 4: Percentage of transmitted synopses as the modification threshold  X  increases. ever, lowering  X  reduces stale traffic, as frequent age-based invalidation rectifies false negatives from previous epochs and limits their adverse effect on stale traffic. For exam-ple, although the Basic CIP (  X  =  X  , 1 s = false ) achieves the smallest possible FN (0.08%), there are instances (e.g.,  X  = 2 , 1 s = true ) which improve upon it by reducing both stale traffic and false positives (0.35% vs 0.89%, and 59.1% vs 67.8%, respectively). In such configurations, false nega-tives are fixed quickly, causing little cumulative effect.
Finally, shorter synopses (smaller  X  values) reduce false positives and communication, at the expense of more false negatives, and consequently, higher stale traffic.
 Varying  X  and  X  : Figure 5 evaluates the effect of varying the modification threshold  X  . These experiments use com-plete synopses (  X  = 1) and score thresholding (1 s = true ). Each plot fixes a value of  X  , and varies  X  .

Increasing the value of  X  yields a reduction of FP X  X  at the cost of higher FN X  X  and ST. Additionally, eliminating synopses due to minor revisions reduces the communication overhead between the synopsis generator and the invalida-tor. This is particularly useful when the two CIP compo-nents reside on separate nodes. Table 4 shows how the per-centage of generated (and transmitted) synopses drops as the value of  X  increases. Note that we compute the commu-nication overhead here by counting the number of synopses. Best cases: Here we contrast the best individual instances of CIP classes studied in the previous sections against the baseline TTL heuristic. Figure 6 depicts the policy instances that formed the bottom-left envelope of Figure 4 and Fig-ure 5. Our results show that for every point of TTL, there is at least one point of CIP that obtains a significantly lower stale traffic for the same value of false positives. For exam-ple, tolerating 6% of stale traffic requires below 20% of false 1 a suboptimal ST, due to  X  =  X  . Score thresholding ( 1 ), longer timeouts (  X  ), and smaller synopses (  X  ) lead to more aggressive policies. each plot) lead to more aggressive policies. positives, in contrast with TTL X  X  44 . 6%. When high preci-sion is required (low ST), CIP performs particularly well  X  the number of query evaluations is 30% below the baseline.
Cache invalidation is critical for caching query results over incremental indices. Traditional approaches apply very sim-ple invalidation policies such as flushing the cache upon up-dates, which induces a significant penalty to cache perfor-mance. We presented a cache invalidation predictor (CIP) framework, which invalidates cached queries selectively by using information about incoming documents. Our evalua-tion results using Wikipedia documents and queries from a real search engine shows that our policies enable a significant reduction to the amount of redundant invalidations (false positives, or FP) required to sustain the desired precision (stale traffic, or ST). More concretely, for every target ST, the reduction of FP compared to the baseline TTL scheme is between 25% and 30%.

The implication of our results to the design of caching systems is the following. False positives impact negatively the cache hit rate as they lead to unnecessary misses in our setting. Consequently, selecting a policy that enables a low ratio of false positives is important for performance. With our CIP policies, it is possible to select a desired ratio of false positives as low as 0 . 2. Lowering the ratio of false pos-itives, however, causes the ratio of false negatives (and stale traffic) to increase, which is undesirable when the degree of freshness expected for results is high. When designing a caching system, a system architect must confront such a trade-off and choose parameters according to the specific re-quirements of precision and performance. Our CIP policies enable such choices and improve over previous solutions. Figure 6: Stale traffic (ST) vs False Positives (FP) for the best cases. We use:  X  = 1  X  com-plete synopses, 1 s = true  X  score thresholding,  X  = (0% , 0 . 5% , 1%)  X  small modification threshold, and 2  X   X   X  20  X  a variety of age thresholds.
 This work has been partially supported by the COAST (ICT-248036) and Living Knowledge (ICT-231126) projects, funded by the European Community. [1] A. Arasu, J. Cho, H. Garcia-Molina, A. Paepcke, and [2] Ricardo Baeza-Yates, Aristides Gionis, Flavio P. [3] Ricardo Baeza-Yates, Flavio Junqueira, Vassilis [4] Ricardo A. Baeza-Yates and Berthier A. Ribeiro-Neto. [5] Sergey Brin and Lawrence Page. The anatomy of a [6] Andrei Z. Broder, Steven C. Glassman, Mark S.
 [7] Soumen Chakrabarti. Mining the Web -Discovering [8] Junghoo Cho and Hector Garc  X  X a-Molina. The [9] Anirban Dasgupta, Arpita Ghosh, Ravi Kumar, [10] Tiziano Fagni, Raffaele Perego, Fabrizio Silvestri, and [11] Marcus Fontoura, Jason Zien, Eugene Shekita, Sridhar [12] Qingqing Gan and Torsten Suel. Improved techniques [13] Paul Jaccard.  X  Etude comparative de la distribution [14] Ronny Lempel and Shlomo Moran. Predictive Caching [15] Ronny Lempel and Shlomo Moran. Competitive [16] Xiaohui Long and Torsten Suel. Three-level caching [17] Evangelos P. Markatos. On Caching Search Engine [18] Sergey Melnik, Sriram Raghavan, Beverly Yang, and [19] P. Saraiva, E. Moura, N. Ziviani, W. Meira, [20] Gleb Skobeltsyn, Flavio Junqueira, Vassilis [21] Ian Witten, Alistair Moffat, and Timoty Bell.
