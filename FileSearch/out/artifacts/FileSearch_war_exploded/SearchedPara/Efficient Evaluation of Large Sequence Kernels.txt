 Classification of sequences drawn from a finite alphabet us-ing a family of string kernels with inexact matching (e.g., spectrum or mismatch) has shown great success in machine learning. However, selection of optimal mismatch kernels for a particular task is severely limited by inability to compute such kernels for long substrings ( k -mers) with potentially many mismatches ( m ). In this work we introduce a new method that allows us to exactly evaluate kernels for large k , m and arbitrary alphabet size. The task can be accom-plished by first solving the more tractable problem for small alphabets, and then trivially generalizing to any alphabet using a small linear system of equations. This makes it pos-sible to explore a larger set of kernels with a wide range of kernel parameters, opening a possibility to better model se-lection and improved performance of the string kernels. To investigate the utility of large ( k , m ) string kernels, we con-sider several sequence classification problems, including pro-tein remote homology detection, fold prediction, and music classification. Our results show that increased k -mer lengths with larger substitutions can improve classification perfor-mance.
 I.2 [ Artificial Intelligence ]: Learning; I.5 [ Pattern Recog-nition ]: Applications; I.5.2. [ Pattern Recognition ]: De-sign Methodology Algorithms, Design, Measurement, Performance, Experimen-tation sequence classification, string kernels
Analysis of large scale sequential data has become an im-portant task in machine learning and data mining, inspired by applications such as biological sequence analysis, text and audio mining. Classification of string data, sequences of discrete symbols, has attracted particular interest and has led to a number of new algorithms [1, 7, 12, 19]. These al-gorithms often exhibit state-of-the-art performance on tasks such as protein superfamily and fold prediction, music genre classification and document topic elucidation. In particu-lar, string kernel methods with inexact matching (e.g., mis-match) have been successful in solving challenging problems in bioinformatics, text mining, image categorization, music classification, etc (e.g. [15, 17, 16, 12, 7, 4]).
A family of state-of-the-art approaches to scoring similar-ity between pairs of sequences relies on fixed length, sub-string spectral representations and the notion of mismatch kernels, c.f. [7, 12]. There, a sequence is represented as the spectra (counts) of all short substrings ( k -mers) contained within a sequence. The similarity score is established by ex-act or approximate matches of k -mers. Initial work, e.g., [12, 18], has demonstrated that this similarity can be computed for strings X and Y with symbols from alphabet  X  and up to m mismatches between k -mers (the |  X  | notation stands for the length of the sequence).

More recently,in [9] we introduced linear time algorithms with alphabet-independent complexity O ( c k,m ( | X | + | Y | )) for a large class of existing string kernels. In particular, for mismatch kernels, their computation for strings X and Y is based on cumulative pairwise comparison of all substrings  X  and  X  contained in X and Y , with the level of similarity of each pair of substrings (  X ,  X  ) taken as the number of identi-cal substrings their mutational neighborhoods N k,m (  X  ) and N k,m (  X  ) give rise to ( N k,m (  X  ) is the set of all k -mers that differ from  X  by at most m mismatches). This result however requires that the number of identical substrings in ( k, m )-mutational neighborhoods of k -mers a and b (the intersec-tion size) be known in advance, for every possible pair of m and the Hamming distance d between k -mers ( k and |  X  | are free variables). Obtaining the closed form expression for the intersection size for arbitrary k , m is challenging, with no clear systematic way of enumerating the intersection of two mutational neighborhoods. Closed form solutions obtained in [9] were only provided for cases when m is small ( m  X  3).
In this work we introduce a systematic and efficient pro-cedure for obtaining intersection sizes (Sec. 3) that can be used for large k and m and arbitrary alphabet size |  X  | . This will allow us to effectively explore a much larger class of ( k, m ) kernels in the process of model selection. We will investigate performance impact of using large ( k , m ) and il-lustrate it on a number of sequence classification problems i ncluding detecting homology of remotely related proteins, multi-class fold prediction and classification of music sam-ples (Sec. 4.1). We will show importance of kernel parameter selection in Sec. 4.2 and show that using large ( k, m ) ker-nels could further improve performance of the string kernel method.
The key idea of basic string kernel methods is to apply a mapping  X (  X  ) to map sequences of variable length into a fixed-dimensional vector space. In this space a standard ker-nel classifier, such as a support vector machine (SVM) [21], can then be applied. As SVMs require only inner products between examples in the feature space, rather than the fea-ture vectors themselves, one can define a string kernel which computes the inner product in the feature space without ex-plicitly computing the feature vectors: where X, Y  X  D , D is the set of all sequences composed of elements which take on a finite set of possible values from the alphabet  X .

Sequence matching is frequently based on co-occurrence of exact sub-patterns ( k -mers, features), as in spectrum ker-nels [11] or substring kernels [22]. Inexact comparison in this framework is typically achieved using different families of mismatch [12] or profile [7] kernels. Both spectrum-k and mismatch( k , m ) kernel directly extract string features from the observed sequence, X . On the other hand, the profile kernel, proposed by Kuang et al . in [7], builds a profile [3] P
X and uses a similar |  X  | k -dimensional representation, now derived from P X . Constructing the profile for each sequence may not be practical in some application domains, since the size of the profile is dependent on the size of the alphabet set. While for bio-sequences |  X  | = 4 or 20, for music or text data |  X  | can potentially be very large, on the order of tens of thousands of symbols.

Some of the most efficient available trie-based algorithms [12, 18] for mismatch kernels have a strong dependency on the size of alphabet set  X  and the number of allowed mismatches. Both need to be restricted in practice to control the complex-ity of matching algorithms. Under the trie-based framework, the list of k -mers extracted from given input strings is tra-versed in a depth-first search with branches corresponding to all possible  X   X   X . Each leaf node at depth k corre-sponds to a particular k -mer feature (either exact or inexact instance of the observed exact string features) and contains a list of matching features from each string. The kernel ma-trix is updated at leaf nodes with corresponding matching feature counts. The complexity of the trie-based algorithm for mismatch kernel computation for two strings X and Y is O ( k m +1 |  X  | m ( | X | + | Y | )) [12]. The algorithm complexity de-pends on the size of  X , with possible substitutions explicitly drawn from  X  during the trie traversal. Consequently, to control the complexity of the algorithm we need to restrict the number of allowed mismatches ( m ), as well as the al-phabet size ( |  X  | ). Typically, ( k, m ) are set to no more than (5 , 2) for alphabets of size |  X  | = 20.

Recent work in [9] introduced linear time algorithms with alphabet-independent complexity for Hamming-distance based matching. This enables efficient computation of a wide class of existing string kernels for datasets with large |  X  | . The au-thors show that it is possible to compute an inexact ( k, m ) kernel as where a and b are k -mers contained in X and Y , I ( a, b ) is the number of common substrings in the intersection of the mutation neighborhoods of the k -mers a and b , I i is the size of the intersection of k -mer mutational neighborhood for Hamming distance i , and M i is the number of observed k -mer pairs in X and Y having Hamming distance i . The algorithms introduced in [9], however, require as an input the intersection sizes I i (i.e. number of identical substrings in the mutational neighborhoods of k -mers a and b ) for all possible Hamming distances between a and b . No systematic way of obtaining these intersection sizes has been proposed in [9]. Closed-form solutions have been suggested for cases of small m ( m  X  3). For larger values of m ( m &gt; 3), it becomes substantially more difficult to combinatorially enu-merate the intersection of the neighborhoods as the neigh-borhoods grow exponentially with m . In the next section we address these issues and propose a systematic way of com-puting intersection sizes that can be used for larger k and m .
For large values of k and m finding intersection sizes needed for kernel computation can be problematic. This is because while for smaller values of m combinatorial closed form so-lution can be found easily, for larger values of m finding it becomes more difficult due to an increase in the number of combinatorial possibilities as the mutational neighborhood increases (exponentially) in size. On the other hand, direct computation of the intersection by trie traversal algorithm is computationally difficult for large k and m as the com-plexity of traversal is O ( k m +1 |  X  | m ), i.e. it is exponential in both k and m . The above mentioned issues do not allow for efficient kernel evaluation for large k and m . We will further discuss in what follows approaches to computing in-tersection sizes and propose an efficient method that can effectively compute intersection sizes for large k and m and allows to explore more complex kernels (with large k and m ) that as we will show in Section 4.2 can further improve performance of the string kernel method.

The number of k -mers at the Hamming distance of at most m from both k -mers a and b , I ( a, b ), can be found in a weighted form Coefficients w i depend only on the Hamming distance d ( a, b ) between k -mers a and b for fixed k , m , and |  X  | (intuitively, coefficients w i are of combinatorial nature and do not de-pend on the alphabet size). The intersection size I and coef-ficients w i can be precomputed for a given setting ( k, m, |  X  | ) in a number of different ways. In the following, we discuss possible approaches for intersection size computation.
Brute-force . Directly computes intersection sizes for 0  X  d  X  2 m by performing 2 m trie traversals for k -mer pairs a and b over alphabet  X  at distances d = 1 . . . 2 m . This does not compute w i e xplicitly. Brute-force trie traversal for computing intersection sizes is applicable only for moderate values of |  X  | , k , m .

Analytic (closed-form) solution . The set of coefficients w for every possible distance d ( a, b ) and a fixed m can be found in a closed form with k and |  X  | as variables (see [9]). For example, when m = 2 the intersection sizes can be found in closed form as
I ( a, b ) ( m = 2) =
Once the closed form solution is found for every ( m, d ) pair, the intersection sizes for given k , |  X  | can be found in constant time. However, it is difficult to obtain closed form solution for larger k , m . Currently, analytic solutions are known for m &lt; 4, with no known solutions for m  X  4. Finding coefficients by solving linear systems of equations . It is possible to efficiently compute the intersection sizes by reducing ( k, m, |  X  | ) intersection size problem to a set of less complex intersection size computations. We discuss this approach below.
Following Eq. 2, for every Hamming distance 0  X  d ( a, b )  X  2 m , the corresponding set of coefficients w i , i = 0 , 1 , . . . , m satisfies the following linear system of m + 1 equations with each equation corresponding to a particular alphabet size |  X  |  X  { 2 , 3 , . . . , m + 2 } . The left-hand side matrix A is an ( m +1, m +1) matrix with elements a ij = ( i + 1) j  X  1 , i = 1 , . . . , m + 1, j = 1 , . . . , m + 1.
A = The right-hand side I = ( I 0 , I 1 , . . . , I m ) T is a vector of in-tersection sizes for a particular setting of k , m , d , |  X  | = for a pair of k -mers over alphabet size i + 2.

If I i are known, the coefficients w i in w can be fully de-termined by solving the system of equations 4, w = A  X  1 I . Note that, in that case, I i need only be computed for small alphabet sizes, up to m + 2. Hence, this vector can feasibly be computed using a trie traversal for a pair of k -mers at Hamming distance d even for moderately large k as the size of the trie is only ( m + 2) k as opposed to |  X  | k . This allows now to evaluate kernels for large k and m as the traversal is performed over much smaller tries, e.g., even in case of relatively small protein alphabet with |  X  | = 20, for m = 6 and k = 13, the size of the trie is 20 13 / (6 + 2) 13 = 149011 times smaller. Coefficients w obtained by solving Aw = I do not depend on the alphabet size |  X  | . In other words, once found for a particular combination of values ( k, m ), these coefficients can be used to determine intersection sizes for any given finite alphabet |  X  | using Eq. 3.

We summarize the intersection size computation in Algo-rithm 1. The algorithm receives as the input the problem parameters k , m , and |  X  | and returns both the vector of (min( k, 2 m ) + 1) intersection sizes I k,m, |  X  | (lookup table) for a given ( k, m, |  X  | )-problem and the weight matrix W that can be used to obtain intersection sizes for any alphabet size |  X  | . The overall complexity of the algorithm is O ((2 m + 1) k m +1 ( m +2) m ). Compared to O ((2 m +1) k m +1 |  X  | plexity of computing intersection sizes directly using trie traversals, proposed algorithm has lower complexity by a factor of |  X  | m + 2 m (as we show empirically in Sec. 4.3 this agrees with observed running time improvements). We also note that the running time of the proposed algorithm (Al-gorithm 1) is dominated by the trie traversals (lines 3-6 in Algorithm 1) used to obtain intersection sizes I (the right-hand side of the linear system Aw = I ).
 Algorithm 1: In tersection size computation
Input: k ernel parameters k and m , alphabet size |  X  | 10: Set ( d + 1)th row of W k,m to w 11: end for 12: Compute vector of intersection sizes
Output: Weight matrix W k,m of size
Some examples of weight matrices W k ,m for various set-tings of k and m are shown in the Appendix (the negative entries in the weight matrices seem to correspond to terms in Eq. 3 correcting for overcounting). Intersection sizes I can be obtained for a particular alphabet size |  X  | by multi-plying weight matrix W k,m by a vector of ( |  X  |  X  1) powers as in Eq. 3, i.e.
 We note that weight matrices W k,m (steps 1-11 of Algo-rithm 1) can be pre-computed, and then once computed used to solve ( k, m )-mismatch problems for a given alphabet  X  (i.e. weight matrices W k,m are alphabet-size independent ).
We evaluate the utility of large ( k, m ) computations as a proxy for model selection, by allowing a significantly wider range of kernel parameters to be investigated during the selection process. In these evaluations we follow the experi-mental settings considered in [10] and [9].
We use three standard benchmark datasets: the SCOP d ataset (7329 sequences, 54 experiments) [23] for remote protein homology detection, the Ding-Dubchak dataset 1 (27 folds, 694 seqs) [2, 4] for multi-class protein fold recogni-tion, and music genre data 2 (10 genres, 1000 seqs) [13] for multi-class genre prediction. For remote protein homology experiments, we follow standard experimental setup used in previous studies [23] and evaluate average classification per-formance on 54 remote homology experiments, each simulat-ing the remote homology detection problem by training on a subset of families under the target superfamily and test-ing the superfamily classifier on the remaining (held out) families according to SCOP hierarchy (Figure 1). For mu-sic genre classification, we use vector quantization (VQ) to represent original sequences of 13-dim. MFCC vectors as strings over |  X  | = 2048 alphabet, and we use 5-fold cross-validation error to evaluate classification performance. For multi-class fold prediction, we use standard data splits as described in [2, 4]. Data and source code are available at the supplementary website [20].
 Figure 1: SCOP hierarchy. Protein domains orga-n ized into classes, folds, superfamilies, and families. Protein sequences from same superfamily but differ-ent families are considered remote homologs.
In this section, we investigate the impact of kernel param-eters on the classification performance on the three sequence classification tasks. For each task, we evaluate classification performance over a large range of settings for k and m . Such large range evaluation is the first of its kind, made possible by our efficient kernel evaluation algorithm.
 Evaluation measures . The methods are evaluated using 0-1 and top-q balanced error rates as well as F1 scores and pre-cision and recall rates. Under the top-q error cost function, a classification is considered correct if the rank of the correct label, obtained by sorting all prediction confidences in non-increasing order, is at most q . On the other hand, under the balanced error cost function, the penalty of mis-classifying one sequence is inversely proportional to the number of se-quences in the target class (i.e. mis-classifying a sequence from a class with a small number of examples results in a higher penalty compared to that of mis-classifying a se-quence from a large, well represented class). We evaluate remote protein homology performance using standard Re-ceiver Operating Characteristic (ROC) and ROC50 scores. The ROC50 score is the (normalized) area under the ROC h ttp://ranger.uta.edu/~chqding/bioinfo.html http://opihi.cs.uvic.ca/sound/genres curve computed for up to 50 false positives. With a small number of positive test sequences and a large number of negative test sequences, the ROC50 score is typically more indicative of the prediction accuracy of a homology detection method than the ROC score.
 Table 1: Remote homology. Classification per-formance (mean ROC50) of the mismatch kernel method in the supervised setting
Results of mismatch kernel classification for the remote h omology detection problem are shown in Table 1. We ob-serve that larger values of k and m perform better compared to typically used values of k =5-6, m =1-2. For instance, ( k =10, m =5)-mismatch kernel achieves significantly higher average ROC50 score of 53.78 compared to ROC50 of 41.92 and 49.09 for the ( k =5, m =1)-and ( k =5, m =2)-mismatch kernels (with p -values 3 . 1 e -6 and 1 . 8 e -3, respectively). Ta-ble 2 shows for each of the ( k, m ) kernel methods p-values of the Wilcoxon signed-rank test on the ROC50 scores against other ( k , m ) kernels. The utility of such large mismatch kernels was not possible to investigate prior to this study. As can also be seen from comparison with other state-of-the-art methods (including PSI-BLAST, Smith-Waterman similarity-based SVM [14], subsequence [15] and recently proposed spatial sample kernels (SSSK) [8] and sequence learner (SEQL) [5] approaches) in Table 3, large ( k, m ) mis-match kernels display state-of-the-art performance. Table 3: Remote homology prediction. Comparison with state-of-the-art methods method ROC ROC50 PSI-BLAST [7] 74.29 29.25 S VM-Fisher [6, 7] 75.66 31.90 SVM-Pairwise (Smith-Waterman) [14] 89.30 43.40 Subsequence kernel [15] 87.23 40.37 SSSK [8] 91.48 51.18 SEQL [5] 92.20 52.37 Mismatch( k =10, m =5) 91.60 53.78
On the multi-class remote fold prediction problem (Ta-b le 4), larger values of k tend to increase precision and result in higher F1 scores. For instance, top-5 F1 score of 84.00 for the ( k = 11 , m = 6)-mismatch kernel is higher than that of ( k = 5 , m = 1) or ( k = 5 , m = 2) mismatch kernels with F1 scores of 81.68 and 80.88, respectively. For longer k increas-ing the maximum number of allowed mismatches m tends to increase recall rates, while keeping precision rates almost the same. For example, recall rates change from 36.16 to mismatch kernel method in the supervised setting
Recall Top-5 Recall Precision Top-5 Precision F1 Top-5 F1 52.72 for the k = 10 case as m changes from 3 to 6. We also note that using m &gt; k/ 2 compared to m  X  k/ 2 can result in significant drops in precision rates, e.g. ( k = 10 , m = 6)-mismatch kernel has much lower precision rate of 76.32 com-pared to 91.76 of the ( k = 10 , m = 5)-mismatch kernel.
For the music genre classification task (Table 5), param-eter combinations with moderately long k and larger values of m tend to perform better than kernels with small m . As can be seen from results, larger values of m are important for achieving good classification accuracy and outperform setting with small values of m . However, increasing (k,m) does not result in high performance gains, possibly because of significant reduction in recall for these large alphabet se-quences.
In what follows, we discuss approaches for kernel param-eter selection. In particular, we focus on the question of selecting mismatch kernel parameters k and m that would be appropriate for a task at hand. We note that reduced running time requirements of our algorithm open the possi-bility to consider various parameter selection strategies (val-idation, uniform, multiple kernel learning (MKL, e.g., [24])) with a larger set of ( k , m ) kernels. The results presented Table 5: Multi-class music genre recognition. Clas-sification performance of the mismatch method
Kernel Error T op-2 Error F1 Top-2 F1 mismatch(5,1) 34.8  X  3 .49 18.3 65.36 81.95 mismatch(5,2) 32.6  X  2.63 18.0 67.51 82.21 mismatch(6,3) 32.4  X  0.96 19.0 67.79 81.22 mismatch(7,4) 31.1  X  2.07 18.0 68.96 82.16 mismatch(9,3) 31.4  X  1.25 18.0 68.59 82.33 mismatch(9,4) 32.2  X  1.82 17.8 67.83 82.36 mismatch(10,3) 32.3  X  1.3 18.0 67.65 82.12 mismatch(10,4) 31.7  X  1.52 19.1 68.29 81.04 here demonstrate that such large ( k , m ) kernels could lead to significant performance improvements.
One approach to the parameter selection is to select k and m that perform best on average according to the validation set. We demonstrate this approach on remote protein ho-mology detection task within SCOP hierarchy. In SCOP, a manually curated protein data set, sequences are grouped into a tree hierarchy containing classes, folds, superfamilies, and families, from root to leaf (Fig. 1). As validation data is n ot available on the standard benchmark dataset, we split the original training set into two disjoint subsets, a now smaller training subset and a validation subset. Validation subset is obtained by holding out a single family (positive validation examples) and 20% of the negative training se-quences. After training on a now smaller training subset, the performance of a particular combination of values k and m is measured on a validation set. We then select the best performing combination ( k, m ) (according to the validation set) as kernel parameters for the mismatch kernel.
We show results in Table 6 for various choices of k and m (results are average ROC and ROC50 scores for 54 exper-iments). One can observe that according to performance on the validation set larger values of k and m are pre-ferred. For instance, on the validation set, mismatch-( k =9, m =4) and ( k =10, m =5) achieve top two ROC50 scores of 81.57 and 81.30 compared to 77.17 or 79.21 of tradition-ally used mismatch-( k =5, m =2) and ( k =7, m =3). We also observe that best performing kernels according to the vali-dation set also perform best on the test set. The top two kernels with ( k =9, m =4) and ( k =10, m =5) achieve highest average ROC50 scores of 52.59 and 52.96. Similar trends are observed with respect to ROC.
Another approach for setting kernel parameters is to con-sider the kernel equivalent to a combination of multiple ker-nels, with each individual kernel corresponding to a partic-ular choice of k and m . This approach does not select a particular value for k or m , it rather uses a set of ( k, m ) val-ues in a uniform combination. This may be advantageous compared to selecting particular values for k and m as there might not be a single choice of parameter values which would work best in all cases, e.g., for all superfamilies. By not fix-ing the values of k and m , the classifier (e.g., SVM) can learn importance/usefulness of features corresponding to a range of k and m parameter values.

We present selection of results on remote homology detec-tion in Table 7 using a uniform mixture of mismatch kernels with k = 1 ... 13 and m = 1 ... 6. As evident from the results, the performance of the uniform mixture (column 1) is very similar to the performance of the best single kernel as found by the validation (Sec. 4.2.1, Table 6).
Using a weighted combination of multiple kernels may po-tentially give better performance compared to a uniform kernel combination. Here we show results for using mul-tiple kernel learning (MKL) [24] on remote homology detec-tion. We again use the same large pool of mismatch kernels k =5 . . . 13, m =1 . . . 6 and learn a multiple kernel combina-tion for each of the 54 remote homology problems.

The results for multiple kernel learning on remote homol-ogy detection are shown in Table 7 (only a selection of the tasks is shown). While the resulting average ROC50 (col-umn 2 in Table 7) are slightly lower compared to uniform combination (column 1), the results are similar and seem to suggest that for different superfamilies, a different set-ting of parameters k and m may be needed. This is also clear from the best-case results for the selection of param-eter values based on the test set performance (last column in Table 7). We observe that best performing k and m dif-fer across different superfamilies, suggesting the need for a kernel combination.
According to the SCOP hierarchy, (1) a single kernel can be selected for the entire hierarchy, (2) kernels can be se-lected at a superfamily level, i.e. on a per-superfamily ba-sis, (3) kernels can be selected at a family level, i.e. on a per-family basis. Selection itself can be based on train, validation, or test performance. In Table 8 we summarize performance of various kernel selection strategies includ-ing validation-based kernel selection, best-case (test-based) selection, uniform kernel combination and multiple kernel learning. While multiple kernel learning selects kernel mix-ture at a family level, uniform kernel combination is the same for all superfamilies/families, i.e. it is similar to se-lecting a single best kernel based on average performance across all superfamilies/families. For superfamily-level se-lection, a single kernel is selected for a given superfamily based on the average performance across families belonging to the superfamily.
 Table 8: Remote homology. Comparison of kernel selection approaches.
 Validation-based selection (single kernel) 52.59 Validation-based selection (per superfamily) 49.89
As we noted before, results for best-case selection (i.e. for s electing best kernel per superfamily/family based on test set performance) suggest the need for per-superfamily/per-family parameter selection to improve the classification per-formance. The reduced running time requirements of our al-gorithm allows to consider various parameter selection strate-gies (validation, uniform, MKL) with a larger set of ( k , m ) kernels. Our results show (Tables 7, 8) that per-class selec-tion with large ( k , m ) could lead to significant improvements (average ROC50 53.78  X  60.32). However, for practical application (e.g., parameter selection using validation ap-proach) more data is necessary.
We measure the running time for full mismatch kernel ma-trix computations for SCOP and music genre datasets (i.e. we compute 7329  X  7329 and 1000  X  1000 kernel matrices) with all of running time experiments performed on a single 3.0GHz CPU. As can be seen from the running times in Ta-ble 9 our method allows efficient evaluation for large k , m kernels (running times are given for protein (7329 seqs) and music (1000 seqs) datasets). We note that the running times include time required for intersection size computations (Al-gorithm 1).

In Table 10, we report running time improvements of our proposed method for intersection size computation (Algo-rithm 1) over brute-force (trie-based) computation of inter-section sizes. As can be seen from the table, ratios of run-method in the supervised setting (with validation set) prediction accuracy compared to settings with smaller k ,and m . Table 9: Running time for kernel computation on m usic and protein sequence data ning times T t rie /T linear as a function of kernel parameters k , m , and alphabet size |  X  | are as expected from theoretical analysis. For example, the obtained speed-up for computing intersection sizes for k =10, m =5, |  X  | =20 is 208x, while the expected running time ratio |  X  | m + 2 m is (20 / (5 + 2)) We also note that because of high O ( k m +1 |  X  | m ) computa-tional complexity of finding intersection sizes using explicit trie traversal, for m &gt; 3 and large alphabet |  X  | running time is excessively long (for these cases only expected run-ning time ratios are shown in Table 10).
In this work we proposed a new systematic method that al-lows evaluation of inexact string family kernels for long sub-strings k with large number of mismatches m . The method Table 10: Intersection size computation. Running time ratio T trie /T linear (speed-up) of the trie-based method and proposed algorithm as a function of ker-nel parameters k , m , and alphabet size |  X  | . Observed speed-ups are on the order of |  X  | m + 2 m as expected from theoretical analysis finds the intersection set sizes by explicitly computing them for small alphabet size |  X  | and then generalizing this to ar-bitrary large alphabets. We show that this enables one to explore a larger set of kernels corresponding to a wide range of kernel parameter values, which as we demonstrate ex-perimentally can further improve performance of the string kernels. We illustrate impact of the kernel parameters on t he classification performance on a number of sequence clas-sification tasks and evaluate a number of kernel/parameter selection strategies. We demonstrate improved performance on protein remote homology detection, multi-class fold pre-diction, and classification of music samples. [1] J. Cheng and P. Baldi. A machine learning information [2] C. H. Ding and I. Dubchak. Multi-class protein fold [3] M. Gribskov, A. McLachlan, and D. Eisenberg. Profile [4] E. Ie, J. Weston, W. S. Noble, and C. Leslie.
 [5] G. Ifrim and C. Wiuf. Bounded coordinate-descent for [6] T. Jaakkola, M. Diekhans, and D. Haussler. A [7] R. Kuang, E. Ie, K. Wang, K. Wang, M. Siddiqi, [8] P. Kuksa, P.-H. Huang, and V. Pavlovic. Fast protein [9] P. Kuksa, P.-H. Huang, and V. Pavlovic. Scalable [10] P. P. Kuksa and V. Pavlovic. Spatial representation [11] C. S. Leslie, E. Eskin, and W. S. Noble. The spectrum [12] C. S. Leslie, E. Eskin, J. Weston, and W. S. Noble. [13] T. Li, M. Ogihara, and Q. Li. A comparative study on [14] L. Liao and W. S. Noble. Combining pairwise [15] H. Lodhi, C. Saunders, J. Shawe-Taylor, [16] Z. Lu and H. Ip. Image categorization with spatial [17] C. Saunders, D. R. Hardoon, J. Shawe-Taylor, and [18] J. Shawe-Taylor and N. Cristianini. Kernel Methods [19] S. Sonnenburg, G. R  X  atsch, and B. Sch  X  olkopf. Large [20] Supplementary data and code. http://seqam. [21] V. N. Vapnik. Statistical Learning Theory .
 [22] S. V. N. Vishwanathan and A. Smola. Fast kernels for [23] J. Weston, C. Leslie, E. Ie, D. Zhou, A. Elisseeff, and [24] Z. Xu, R. Jin, I. King, and M. R. Lyu. An extended
