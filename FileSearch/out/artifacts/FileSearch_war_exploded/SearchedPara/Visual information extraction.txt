 REGULAR PAPER Yonatan Aumann  X  Ronen Feldman  X  Yair Liberzon  X  Benjamin Rosenfeld  X  Jonathan Schler Abstract Typographic and visual information is an integral part of textual docu-ments. Most information extraction (IE) systems ignore most of this visual infor-mation, processing the text as a linear sequence of words. Thus, much valuable information is lost. In this paper, we show how to make use of this visual informa-tion for IE. We present an algorithm that allows to automatically extract specific fields of the document (such as the title, author, etc.) based exclusively on the vi-sual formatting of the document, without any reference to the semantic content. The algorithm employs a machine learning approach, whereby the system is first provided with a set of training documents in which the target fields are manually tagged and automatically learns how to extract these fields in future documents. We implemented the algorithm in a system for automatic analysis of documents in PDF format. We present experimental results of applying the system on a set of financial documents, extracting nine different target fields. Overall, the system achieved a 90% accuracy.
 Keywords Information extraction  X  PDF analysis  X  Text analysis  X  Wrapper induction 1 Introduction Most information extraction (IE) systems [1 X 5] simplify the structure of the docu-ments they process by ignoring much of the visual characteristics of the document, e.g. font type, size and location, and process the text as a linear sequence. This allows the algorithms to focus on the semantic aspects of the document. How-ever, valuable information is lost. Consider, for example, an article in a scientific journal. The title is readily recognized based on its special font and location but Similarly, for the author names, section headings, running title, etc. Thus, much important information is provided by the visual layout of the document. In this paper, we present an IE system that is based solely on the visual characteristics of the document and show that this visual information alone is sufficient to provide high-accuracy extraction for specific fields (e.g. the title, author names, publica-tion date, etc.). We note that the visual approach provided by this paper is not aimed at replacing the semantic one but rather on complementing it. It can also function as a preprocessor or a converter to other systems.
 the visual layout of the document. The algorithm employs a machine learning ap-proach, whereby the system is first provided with a set of training documents in which the desired fields are manually tagged. Based on these training examples, the system automatically learns how to find the corresponding fields in future doc-uments. 1.1 Problem formulation A document D is a set of primitive elements D ={ e 1 ,..., e n } . A primitive ele-ment can be a character, a line, or any other visual object, depending on the docu-ment format. A primitive element may have any number of visual attributes such as font size and type, physical location, etc. The bounding box attribute, which provides the size and location of the bounding box of the element, is assumed to be available for all primitive elements. We define an object in the document to be any set of primitive elements.
 with a set of target fields F ={ f 1 ,..., f k } to be extracted and a set of train-ing documents T ={ T 1 ,..., T m } , wherein all occurrences of the target fields are tagged. Specifically, for each target field f and training document T , we are pro-vided with the object f ( T ) of T that is of type f ( f ( T ) = X  if f does not appear in T ). The goal is that when presented with an un-tagged query document Q ,to correctly tag the occurrences of the target fields that exist in Q (not all target fields need be present in each document). 1.2 Results and paper organization We present a general framework and algorithm for the VIE task. We show that the VIE task can be decomposed into two subtasks. First, for each document (both training and query) we must group the primitive elements into meaningful objects (e.g. lines, paragraphs, etc.) and establish the hierarchical structure among these stage, the structure of the query document is compared with those of the training detailed in Sect. 3 .
 plates , which are groups of training documents with a similar layout (say, articles from the same journal). Using templates, we can identify the essential features of the page layout, ignoring particularities of any specific document. Templates are detailed in Sect. 3.2 .
 ing financial analyst reports. The documents were in PDF format. Target fields included the title, authors, publication dates, and others. The implementation and the results are detailed in Sect. 4 . 1.3 Related work Information extraction from written text is an area of much research and signifi-cant results. Information extraction is a subfield of natural language processing that is concerned with identifying pre-defined types of information from text. Much of the work on IE was driven by the Message Understanding Conferences (MUC) [1 X 5], which provided a uniform framework for the evaluation and com-parison of different approaches and systems. There are multiple different tasks within the general task of IE. In MUC-7, for example, five tasks have been de-fined: 1. Named Entity (NE) Extraction: Tagging of specific entities mentioned in the 2. Co-reference Resolution: Identification of distinct noun-phases within the text 3. Attribute Assignments (also called Template Elements (TE)): Determining the 4. Relations Extraction (also called Template Relations (TR)): Determining the 5. Scenario Extraction: Extraction of scenarios, which are events, possibly de-Traditionally, IE systems, including all those in the MUC evaluations, base the extraction on the content of the text itself. In fact, the MUC data was provided in raw ASCII format, with essentially no physical layout information. In this paper, in contrast, we base the extraction solely on the physical layout of the text. using wrappers [ 22 ], most commonly for extraction from world-wide-web sources. A wrapper is a procedure for the extraction of specific information from structured format (typically a database schema). Wrappers are commonly used by software agents [ 15 , 17 , 29 ]. A wrapper consists of a set of rules that determine how to identify the relevant information based on the structure of the source. research (see [ 13 ]). To aid with manual creation of wrappers special tool kits have been developed [ 19 , 26 ]. Graphical interfaces for semi-automatic wrapper gen-eration allow for interactive visual definition of wrappers (see [ 8 , 10 ]). Machine learning techniques have been employed extensively for the automatic generation of wrappers (also called wrapper induction ), using a multitude of techniques (see [ 20 , 21 , 24 ]). Several of the algorithms allow the handling of both semi-structured and unstructured text [ 11 , 16 , 30 ].
 wrapper generator. However, the system is fundamentally different from tradi-tional wrappers. Traditional wrappers operate on the textual representation of the document, e.g. the HTML document, and base the extraction on clues provided by this representation, e.g. the HTML tags. Our visual extraction system, in contrast, bases its extraction on the actual visual appearance of the document (as gener-ated by the textual representation). To see the difference, consider, for example, a specific physical layout of a document. This physical layout can be generated in multiple different ways in PDF (two-column text, for example, can be provided column by column or line by line). Using traditional wrappers, these two repre-sentation would necessitate two different wrappers. Our visual extraction system, in contrast, operates on both representation identically, since they both represent the same visual layout.
 is designed to automatically index technical manuals provided in PDF form. The system determines the logical structure of the document (sections, item lists, titles, etc.) using a bottom X  X p process combined with the use of top X  X own (shallow) grammars. Lovegrove and Brailsford [ 23 ] describe a system for analysis of PDF documents based on the blackboard method. Futrelle et al. [ 18 ] describe a system for analysis of diagrams in PDF documents, using support vector machines. Chao et al. [ 12 ] describe a system for the analysis of PDF documents aimed at reuse of its logical elements.
 able in raster format has been studied extensively. Most of these systems are based on rules (or grammars) that represent apriori knowledge on the overall logical structure of document. In this case, the main challenge is to correlate between this apriori structure and the raster information. Several systems for raster doc-ument analysis use machine earning techniques. The system developed as part of the WISDOM project is of special interest in this regard (see [ 6 , 9 , 14 ]). The WISDOM ++ system accepts a scanned document as input and transform it into rich XML format. The processing of the document in WISDOM ++ involves sev-eral steps. The document understanding step is analogous to the task we consider here. The WISODOM ++ system employs a machine learning approach to doc-ument understanding. This is achieved by automatically learning a set of rules, using first-order learning systems. The descriptions of both layout structures and models are given in a suitably defined first-order language, with functions express-ing unary properties such as height and length, and binary predicates and function expressing interrelationships among layout components (e.g., contain, on-top, and so on). The rules are learned from the training data and applied to the test. Our system, in contrast, does not operate by learning the specific rules pertaining to the individual objects but rather by attempting to match the overall structure of the training examples to that of the test document.
 computer vision and for visual object recognition in particular (see [ 25 , 27 ]). The problem we consider here is on the one hand more restricted, as we consider only textual documents, but on the other hand, also more demanding, as the differences between the fields may be subtle. 2 Structural layout analysis Recall that a document is a set of primitive elements such as characters, figures, etc. The objects of a document are sets of primitive elements. Target fields, in gen-eral, are objects. Thus, the first step in the Visual IE task is to group the primitive elements of the documents into higher-level objects. The grouping should provide the conceptually meaningful objects of the document such as paragraphs, head-ings, footnotes, etc. For humans, the grouping process is easy and is generally performed unconsciously based on the visual structure of the document. The goal is thus to mimic the human perceptual grouping process. This process is often called structural layout analysis . 2.1 Problem formulation We model the structure of the objects of a document as a tree, where leaves are primitive elements and internal nodes are composite objects. We call this struc-ture the O-Tree (Object-Tree) of the document. The O-Tree structure creates a hierarchal structure among objects, where higher-level objects consist of groups of lower-level objects. This hierarchal structure reflects the conceptual structure of the document, where objects such as columns are groups of paragraph, which, in turn, are groups of lines, etc. The exact levels and objects represented in the O-Tree are application and format dependent. For an HTML document, for exam-ple, the O-Tree may include objects representing tables, menus, text body, etc., while for a PDF documents the O-Tree may include objects representing para-graphs, columns, lines, etc. Accordingly, for each file format and application we define the Type Hierarchy , H , which determines the set of possible object types , and a hierarchy among these objects. Any Type Hierarchy must contain the type D OCUMENT , which must be at the root of the hierarchy. When constructing an O-Tree for document, each object is labelled by one of the object types defined in the Type Hierarchy, and the tree structure must correspond to the hierarchical structure defined in the hierarchy.
 (DAG) such that:  X  the leaf nodes contain all the possible types for primitive elements;  X  internal nodes contain all the possible types for composite objects;  X  the root node is the type D OCUMENT ;  X  for types x and y , type y is a child of x if an object of type x can (directly) according to H is a tree, O , such that:  X  the leafs of O consist of all primitive elements of D ;  X  internal nodes of O are objects of D ;  X  if X and X are nodes of O (objects or primitive elements) and X  X  X then  X  each node X is labelled by a label from H , denoted label ( X ) ;  X  if X is a parent of X in T then label ( X ) is a parent of label ( X ) in H ;  X  label ( root ) = D OCUMENT . 2.2 Algorithm Given a document, we construct an O-Tree for the document. In doing so, we wish to construct objects best reflecting the true grouping of the elements into  X  X ean-ingful X  objects (e.g. paragraphs, columns, etc.). In doing so, we rely solely on the physical representation of the document and not on any hidden tags that the docu-ment may contain (i.e. we do not rely on XML, HTML or PDF tags describing the nature of the object, such as  X  X eader X ,  X  X itle X , etc.). When constructing an object we consider the following:  X  The elements of the objects are within the same physical area of the page.  X  The elements of the object have similar characteristics (e.g. similar font type, bottom up, layer by layer. In doing so, we always prefer to enlarge existing objects of the layer, starting with the largest object. If no existing object can be enlarged, and there are still  X  X ree X  objects of the previous layer, a new object is created. The procedure completes when the root object, labelled D OCUMENT , is completed. A description of the algorithm is provided in Fig. 1 . 3 Structural mapping Given a VIE task, we first construct an O-Tree for each of the training documents, as well as for the query document, as described in the previous section. Once all the documents have been structured as O-Trees, we need to find the objects of Q (the query document) that correspond to the target fields. We do so by comparing the O-Tree of Q , and the objects therein, to those of the training documents. This we perform in two stages. First, we find the training document that is (visually) most similar to the query document. Then, we map between the objects of the two documents to discover the targets fields in the query document. 3.1 Basic algorithm 3.1.1 Document similarity Consider a query document Q and training documents T ={ T 1 ,..., T n } . We seek to find the training document T opt that is visually most similar to the query docu-ment. We do so by comparing the O-Trees of the documents. In the comparison, we only concentrate on similarities between the top levels of O-Tree. The reason is that even similar documents may still differ in the details.
 the training documents, respectively, and let H be the Type Hierarchy. We define a subgraph of H , called the Signature Hierarchy and denoted by S , consisting of the types in H that determine features of the global layout of the page (e.g. columns, tables). The exact types included in the signature are implementation dependent, but, in general, the signature would include the top one or two levels of the Type Hierarchy. For determining the similarity between objects, we assume similarity between objects of the same type based on the object characteristics such as size, location, fonts, etc. ( sim ( X , Y ) is implementation dependent). in the signature of T we find the object X of Q (of the same type as X )that is most similar to X . We then compute the average similarity for all objects in the signature of T to obtain an overall similarity score between Q and T .We choose the document, T opt , with the highest similarity score. A description of the procedure is provided in Fig. 2 . 3.1.2 Finding the target fields Once the most similar training document, T opt , has been determined, the objects of Q that correspond to the target fields, as tagged in the document T opt , remains to be found. We do so by finding, for each target field f , the object within the O-Tree of Q that is most similar to f ( T opt ) (the object in O ( T opt ) tagged as f ). Finding this object is done in an exhaustive manner, going over all objects of O ( Q ) .We also make sure that the similarity between this object and the corresponding object of T opt is above a certain threshold,  X  , or else we decide that the field f has not been found in Q (either because it is not there, or we have failed to find it). A description of the procedure is provided in Fig. 3 .
 pendently) to the construction of the O-trees. Thus, tagged objects need not appear in the O-tree. If this is the case, line 2 sees to it that we take the minimal object of O ( T 3.2 Templates The aforementioned algorithm is based on finding the single most similar docu-ment to the query document and then extracting all the target fields based on this document alone. While this provides good results in most cases, as we shall see in Sect. 4 , there is the danger that particularities of any single document may reduce the effectiveness of the algorithm. To overcome this problem, we introduce the notion of templates , which allow to compare the query document to a collection of similar documents. A template is a set of training documents that have the same general visual layout, e.g. articles from the same journal, web pages from the same site, etc. The documents in each template may be different in details but share the same overall structure.
 ument (rather than the document most similar). We do so by, for each template, averaging the similarity scores between the query document and all documents in the template. We then pick the template with the highest average similarity. Once the most similar template is determined, the target fields are provided by find-ing the object of Q most similar to a target field in any of the documents in the template. A description of this stage is provided in Fig. 4 . 4 Experimental results We implemented the system for VIE, as described earlier, on documents that are analyst reports from several leading investment banks. 4.1 The data The data consisted of a total of 285 analyst reports from leading investment banks: 71 from BearSterns, 29 from CSFB, 26 from Dresdner, and 159 from Morgan Stanley. All documents were in PDF format. The documents were clus-tered into 30 templates: 7 in the BearSterns data, 4 in the CSFB data, 5 in the Dresdner data, and 14 in the Morgan Stanley data. All documents were manually tagged for the target fields. The target fields included the following G
EOGRAPHY ,I NDUSTRY I NFO . Not all documents included all target fields, but within each template, documents had the same target fields. The data set is avail-able on line at www.cs.biu.ac.il/  X  aumann/datasets/KAIS06.zip. 4.2 Implementation The Type Hierarchy ( H ) used in the system is provided in Fig. 5 . The Signature Hierarchy contained the objects of type COLUMN and PARAGRAPH . The imple-mentation of the fitness function fit (  X  ,  X  ) (for the fitness of one object within the other) takes into account the distance between the objects and the similarity in fonts. For the fitness of a line within an existing paragraph, it also takes into ac-count the distance between lines. The similarity function sim (  X  ,  X  ) , measuring the similarity between objects in different documents, is based on similarity between the sizes and locations of the respective bounding boxes. 4.3 Results Tests were performed using five-fold cross-validation. In each template, the doc-uments were randomly divided into five subsets. We then performed five tests as follows. In each test, for each template, one of the five subsets served for train-ing and the other four were used for the test. (Of course, all test documents were placed together, without distinction of the source template; it was up to the al-gorithm to determine the right template.) We measured the performance of the system with the basic algorithm (Sect. 3.1 ) and with the use of templates (Sect. 3.2 ). The overall average recall and precision values are provided in Fig. 6 .Onthe whole, the introduction of templates improved the performance of the algorithm, increasing the average accuracy from 83 to 91%. We note that for both algorithms the recall and precision values were essentially identical. The reason is that for any target field f , on the one hand, each document contains only one object of type f , and, on the other hand, the algorithm marks one object as being of type f . Thus, for every recall error there is a corresponding precision error. The slight difference that does exist between the recall and precision is due to the cases where the al-gorithm decided not to mark any element, in which case there is a recall error but not a precision error.
 curacy rates for the different target fields. It is interesting to note that while the introduction of templates improves the accuracy in most cases, there are some tar-get fields for which it reduces the accuracy. Understanding the exact reasons for this, and how to overcome such problems, is a topic for further research. 4.4 Performance The system was implemented in C ++ , on a Microsoft Windows platform. We measured the performance of the system using a laptop with a Pentium 4, 1.7 GHZ Centrino processor, with 1 GB of RAM. Figure 8 shows the performance of the system, in seconds per test document, as a function of the number of training documents. As can be seen, the time complexity is essentially linear in the num-ber of training documents. This is expected, since the comparison with the test document is performed separately for each training document. Figure 9 depicts the performance of the system, in seconds per test document, as a function of the number of templates. Here too, the time complexity was linear in the number of templates, though with a much flatter curve. This is again expected, as there is an additional overhead for determining the similarity score for each template, but this process is relatively fast. Throughout, the performance was linear in the number of test documents.
 5 Discussion Typographic and visual information is an integral part of textual documents. All but the most basic document formats provide for extensive representation of vi-sual information (font sizes and types, indentations, etc.). The importance of the visual formatting is not only to please the eye. Rather, the visual information sup-plements the textual one, providing visual clues as to the meaning and/or role of the different parts of the document. The clues can be typographic (e.g. italization), location based (e.g. indentation), or any other form of visual representation (e.g. putting an important piece of text within a box). The text itself, without its visual formatting, is much more difficult to understand. Accordingly, we believe that it is important to consider the visual formatting when analyzing textual documents. tual documents. The algorithm we present employs a machine learning approach, whereby the system is provided with training documents wherein the target fields are manually tagged and automatically learns how to extract these fields in future documents. We present experimental results in implementing the system for PDF documents, achieving accuracy levels of 90%.
 appearance, totally ignoring the semantic content. In this respect, the approach is opposite to most IE systems, which focus only on the sematic content. The results presented here show that the visual information is sufficiently rich to allow for highly accurate extraction of specific fields. Furthermore, we show that the specific visual clues of each target field can be automatically learned by the system. foremost, the visual approach can only capture fields with distinct visual character-istics such as the title, authors, publication date, etc. Semantic elements mentioned within the running text, such as people names, locations, etc., clearly cannot be de-tected by the visual approach. In addition, the learning process presented here only works for features and structures that have a relatively high level of consistency among documents, such as title, author, etc. The method would be less applicable to structures with a high level of variations between documents.
 entire spectrum of available information: semantic, syntactic and visual. In such a system, the visual approach presented here would be one of the components in a combined, integrated approach. The development of such a system is left for future research. We note that Barardi et al. [ 9 ] describe an integrated approach based on the WISDOM system.
 References
