 Similarity search is a key challenge for multimedia retrieval ap-plications where data are usually represented in high-dimensional space. Among various algorithms proposed for similarity search in high-dimensional space, Locality-Sensitive Hashing (LSH) is the most popular one, which recently has been extended to Kernelized Locality-Sensitive Hashing (KLSH) by exploiting kernel similarity for better retrieval efficacy. Typically, KLSH works only with a single kernel, which is often limited in real-world multimedia ap-plications, where data may originate from multiple resources or can be represented in several differen t forms. For example, in content-based multimedia retrieval, a variety of features can be extracted to represent contents of an image. To overcome the limitation of reg-ular KLSH, we propose a novel B oosting Multi-Kernel Locality-Sensitive Hashing (BMKLSH) scheme that significantly boosts the retrieval performance of KLSH by making use of multiple kernels. We conduct extensive experiments for large-scale content-based image retrieval, in which encouraging results show that the pro-posed method outperforms the state-of-the-art techniques. H.3.3 [ Information Storage and Retrieval ]: Retrieval models; H.2.8 [ Database Applications ]: Image databases Algorithms, Experimentation Image Retrieval, High-dimensional indexing, Locality-sensitive hash-ing, Kernel methods
Similarity search, or Nearest Neighbor (NN) search, plays a crit-ical role in Content-Based Image Retrieval (CBIR) systems [ 33, 24]. Typically, images in a CBIR system are represented in a high-dimensional space, and the size of an image database can easily be over millions or even billions for large-scale real-world applica-tions. These two aspects have made CBIR an open grand challenge although it has been extensively studied for decades.

A variety of data structures have been proposed for indexing and searching data points in a low-dimensional space [ 31, 7, 2, 29]. These approaches work well for low dimensional data. But, when the number of dimensions grows, they often become less efficient, a phenomenon known as the curse of dimensionality . Specifically, the time or space requirements of these approaches often grow ex-ponentially w ith the dimensionality.

Since exact NN search is hard to scale for high-dimensional data, recent studies mainly focus on approximation approaches [ 20, 14, 25, 1], which aim to remove the exponential dependence on dimen-sionality. Instead of finding the nearest point p to a query point q , approximate NN search allows to return any point within the dis-tance of (1 + ) times the distance from q to p . Recent studies have shown that by adopting the approximation, the complexity of NN search is reduced from exponential to polynomial in terms of its dependence on the dimensionality. Several recent studies have successfully applied the random projection idea for approximate NN search over high-dimensional data. One of the most well-known techniques in this direction is Locality-Sensitive Hashing (LSH) [ 20, 14, 6], which has been actively studied and success-fully applied to many applications [ 20, 10, 1].

One limitation of regular LSH is that they require explicit vec-tor representation of data points. Kernelized LSH (KLSH) [ 23] addresses this limitation by employing kernel functions to capture similarity between data points without having to know their explicit vector representation. KLSH has been shown effective empirically, but there is no formal analysis of KLSH in theory. In this paper, we first analyze the theoretical property of KLSH to better understand the behavior and capacity of KLSH in similarity search. Second, we address the limitation of KLSH. Despite the success, most exist-ing KLSH techniques only adopt a single kernel function. This sig-nificantly limits its application to many real-world image retrieval tasks [40, 18], where images are often analyzed by a variety of feature descriptors and are measured by a wide class of diverse similarity functions.
 To this end, we propose a novel B oosting Multi-Kernel Locality-Sensitive Hashing (BMKLSH) framework, which improves KLSH by learning a combination of multiple kernels. The key challenge of multi-kernel LSH is to determine an optimal combination of multi-ple kernels by determining appropriate bit size to each of the mul-tiple kernels. To overcome the challenge, we propose a boosting scheme to greedily find a good solution in an efficient approach. Our extensive experiments show that BMKLSH significantly en-hances the performance of KLSH in exploring the power of multi-ple kernels for CBIR.

The rest of this paper is organized as follows. Section 2 reviews related work. Section 3 gives our analysis of KLSH, and Section 4 presents the proposed BMKLSH method. Section 5 discusses our experiments. Finally Section 6 concludes this work.
This section reviews related work in approximate nearest neigh-bor (NN) search with the focus on image retrieval applications.
In literature, developing efficient techniques for indexing high dimensional images has been studied extensively in information retrieval, multimedia, and database communities [ 4, 5, 9]. Spatial data structure approaches (e.g., kd-tree [ 2, 29] or metric tree [ 35]) were used to handle the NN search problem; however, they scale poorly with data dimensionality. In practice, if the number of di-mensions is large enough, kd-tree and other similar data structures require an expensive inspection in the data set, thereby perform no better than an exhaustive linear search that simply compares a query to every data point in the database.

Instead of solving the exact similarity search for high dimen-sional indexing, recent years have witnessed active studies of ap-proximate high-dimensional indexing techniques [ 20, 14, 25, 3, 8, 11]. The key of most techniques is to exploit random projection to tackle the curse of dimensionality issue, such as Locality-Sensitive Hashing (LSH) [ 20], a very well-known and highly successful tech-nique in this area. In general, we can group most of existing ap-proaches into two major categories: linear projection methods and kernel-based methods. Below we briefly review the first category and then focus on discussing the second category.

For the first category, one of the most notable techniques is LSH [ 20], which utilizes a family of locality sensitive hashing functions which map similar items to same bucket with high probability, and dis-similar items to same bucket with low probability. With such a hash function, one can easily search within the bucket a query point belongs to in order to find nearest neighbors of the query. LSH efficiently solves the approximate similarity search problem and achieves query time in the worst case O( dn 1 / ). Gionis et al. [ 14] improved the techniques and achieved significant query time O( dn 1 / (1+ ) ). Recently, many studies have attempted to im-prove upon the regular LSH technique. For example, [ 25]intro-duced multi-probe LSH m ethods that reduce the space requirement of the basic LSH method. Tao et al. [ 34] proposed the locality-sensitive B-tree technique that mainly concerns the performance improvement of I/O disk access. Besides, there are many other ex-isting works that accommodate LSH functions in tackling different issues, including Hamming distance [ 20], inner products [ 6], norms [ 10], normalized partial matching [ 15], learned Mahalanobis metrics [ 27], and so on. Last but not least, there are also many re-cent studies of learning compact binary codes inspired by the idea of LSH [ 39, 16, 36, 22].

Our study is more related to the second category of kernel-based methods. In particular, kernel-based LSH (KLSH) [ 23]wasre-cently proposed to overcome the limitation of the regular LSH tech-nique that often assumes the data come from a multidimensional vector space and the underlying embedding of the data must be ex-plicitly known and computable. KLSH provides a powerful frame-work to explore arbitrary kernel/similarity functions where their underlying embedding only needs to be known implicitly. In [ 28], the authors proposed a variant of KLSH which aims to force the expected Hamming distance between the binary codes of two vec-tors is related to the value of a shift-invariant kernel. In [ 17], the authors proposed an improved algorithm for learning binary codes with kernel to speed up the KLSH technique. Despite being studied actively, most existing kernel-based hashing methods [ 23, 28, 17, 26] only consider a single kernel, which cannot fully explore the potential of multiple kernel functions in a real CBIR application.
Very recently, some emerging study has attempted to apply KLSH by exploring multiple kernels. In [37], they proposed a simple so-lution named MKLSH by applying the existing KLSH algorithm for each specific kernel individually, and then combining the out-puts of these KLSH processes. Our work however differs from their method in several aspects. First of all, their naive approach to combining multiple kernels simply treats each kernel equally, which fails to fully explore the power of combining multiple di-verse kernels in KLSH. Second, their technique is essentially un-supervised, which does not fully explore the data characteristics and thus cannot achieve the optimal indexing performance. In con-trast, our technique is in general supervised and is able to learn the optimal combination of multiple kernels from training data to maximize the indexing performance. In this section, we first review the algorithm of Kernelized Locality-Sensitive Hashing (KLSH), and then present our analysis of KLSH. Let X  X  R d  X  n be the collection of data points to be searched. Given a query q  X  R d , to efficiently find the k nearest neighbors, LSH projects each data point into a low-dimensional binary space, referred to as the hash key. The hash keys are constructed by ap-plying b binary-valued hash functions h 1 ,...,h b to the data points in
X . KLSH generalizes LSH by introducing a kernel function  X  ( x i , x j ) to map a data point x i to a functional space through a nonlinear feature mapping  X  ( x i ) that satisfies the condition  X  (  X  ( x i )  X  ( x j ) . To build the hash function, KLSH first randomly se-lects a subset of p data points from X , denoted as S = { x s and forms a kernel matrix K over the sampled data points; it then generates b random vectors e 1 S ,..., e b S and computes a hashing function for each random vector e k S as where w k =( w k 1 ,...,w k p ) is given by w k = K  X  1 / gorithm 1 outlines the key steps of KLSH, where b is a critical parameter that determines the length of hash key to be constructed in KLSH.

Although KLSH is proved to be effective empirically, no theo-retical analysis is provided for the properties of KLSH. To bound approximation error of KLSH, we introduce a few notations. De-Algorithm 1 KLSH 3: for k =1 ,...,b do 6: construct hash function: 7: end for fine h i  X  R b as and H =( h 1 ,..., h n ) .Let K a =[  X  ( x i , x j )] n  X  matrix for all data points in X ,and K b =[  X  ( x i , x s kernel matrix between data points in X and the sampled data points in S .Let  X  1 ,..., X  n be the eigenvalues of K a ranked in the de-scending order. We first bound the error caused by the random
T HEOREM 1. With a probability at least 1  X   X  , we have
P ROOF . Let us define A as A = 1 b b k =1 e k S [ e k S ] . It is easy to see that E[ A ]= t p I . According to the concentration inequality of linear operator [ 32], we have, with a probability 1  X   X  ,that We complete the proof by the using the fact t p
T HEOREM 2. Assume  X  ( x , x )  X  1 for any x .Let k  X  Z and  X  (0 , 1) be any two numbers satisfying k/ 4  X  p/ [64  X  2  X  =1+ 8log(1 / X  ) . Then, with a pr obability at least 1  X  have 1 n
P ROOF . According to Theorem [ 12], with a probability 1 for any  X  [0 , 1] and k  X  Z that satisfy where  X  =1+ 8log(1 / X  ) ,wehave where | X | F stands for the Frobenius norm. Using the triangle in-equality of Frobenius norm, we have Combining the result from Theorem 1, we have, with a probability 1  X  2  X  , We complete the proof by using the fact | K b K  X  1 K b | As indicated by Theorem 2, on average, kernel similarity  X  ( is well approximated by p tb h i h j , and the approximation error de-creases as the number of bits b increases. Furthermore the approx-imation error is related to the eigenvalues of kernel matrix K particular, the skewed the eigenvalues of K a , the smaller the ap-proximation error.

Theorem 2 shows that different types of kernels will lead to very different approximation errors, depending on their eigenvalue dis-tributions. Hence, some kernels will have good retrieval accuracy but rather poor approximation performance while the others may have the opposites. Thus, combi ning multiple kernels could po-tentially avoid relying too much on a single kernel that could be of neither poor approximation nor low retrieval accuracy. To make a good tradeoff between retrieval accuracy and approximation er-ror, we propose a boosting scheme to combine multiple kernels for KLSH in order to obtain both high retrieval accuracy and low ap-proximation error.
In this section, we propose a framework of learning the combina-tion of multiple kernels for Kernelized Locality-Sensitive Hashing, which aims to boost KLSH by making use of a combination of mul-tiple kernels. One key question is how to determine the weights for kernel combination. A straightforward approach is to assign equal weight to each kernel function, and apply KLSH with the uniformly combined kernel function. Such an approach might not fully explore the power of multiple kernels.

To address this limitation, we propose a scheme to assign each kernel a different number of bits so as to reflect the importance of the kernel. The key challenge of this scheme is thus how to op-timize the bit size allocation with respect to different kernel func-tions. Figure 1 illustrates the proposed framework, which consists of two key steps: (i) bit allocation optimization, and (ii) the multi-kernel hashing as shown in Algorithm 2.
 Figure 1: The proposed framework of learning to combine mul-tiple kernels for improving kernel LSH Algorithm 2 The Multi-Kernel Hashing scheme 1: k =0 3: for l =1 ,...,m do 5: for r =1 ,...,b l do 6: k = k +1 9: construct hash function: 10: end for 11: end for
Unlike the regular KLSH that adopts a single kernel, BMKLSH employs a set of m kernels for the hashing scheme. Besides, a key difference between BMKLSH and some existing Multi-Kernel LSH (MKLSH) [ 37] is the bit allocation optimization step to find the parameter [ b 1 ,...,b m ] that determines the allocations of bit sizes for a set of m kernels. Unlike the existing MKLSH approach [ 37] that simply assigns the same number of bits to each kernel, lead-ing to a uniform combination of multiple kernels, we develop two supervised learning algorithms (WMKLSH and BMKLSH), de-scribed below, that effectively learn the importance of individual kernels, and consequentially determine the appropriate number of bits for each kernel.
We first propose a Weighted Multi-Kernel Locality-Sen sitive Hash-ing (WMKLSH) scheme by a supervised learning approach to de-termine the allocation of bit size, where a kernel is assigned a larger size of bits if it better captures the similarity between data points.
In order to learn the importance weights of different kernels for the retrieval tasks, we assume a small training data set is available for our learning task. The training set consists of a small set of queries and their relevance judgements, which usually can be eas-ily collected in a real-life CBIR system via the relevance feedback mechanism [ 30, 19].

For the WMKLSH algorithm, we begin by testing the retrieval performance of KLSH with a set of m kernels on the given training set. After that, we can obtain the retrieval performance in terms of mean Average Precision (mAP), As a result, we can compute the weights based on the retrieval performance of each kernel, i.e.,  X  = e mAP l ,l =1 ,...,m . Finally, the bit sizes of the kernels are allocated proportionally according to their importance weights.
To further improve the above learning scheme, we propose a boosting scheme, referred to as BMKLSH, to learn the optimal allocation of bit sizes, which adopts the similar idea of boosting algorithms for classification [13]. Let us denote by n q the number performance of applying the l -th kernel  X  l (  X  ,  X  ) to retrieve vectors for the i -th query. In order to find the optimal allocation of bit sizes, we cast it into the following optimization problem: Since each b l is an integer variable, the above problem is essen-tially an integer programming task, which is NP-hard. To find an efficient solution, we approximate the above optimization problem by introducing a set of combination weights  X  l ,l  X  [ m ] , each of which represents the importance of each kernel so as to determine each bit size b i . In order to learn the optimal weights, we turn the above optimization into the following optimization problem Given the learned weights  X  , we assign to the l -th kernel b X  where b is the total number of bits.

To efficiently solve the problem in ( 2), we adopt a boosting based strategy. Following the similar procedure of Adaboost algorithm [ 13], we introduce a distribution of weights D t to indicate the retrieval difficulty of the instances in the training data set. At each boosting round, we measure the retrieval performance (e.g. average preci-sion) of each kernel based on the existing KLSH algorithm (shown in Algorithm 1), and select the best kernel with the largest weighted Average Precision (wAP) performance over the current distribution D , which is defined as: At the end of each boosting round, an importance weight is com-puted for the selected kernel based on its retrieval performance, and the weights of each poorly retrieved query example will be in-creased such that the next selected kernel will focus more on those hardly retrieved examples. The boosting procedure will be repeated T times. Finally, we allocate a bit size to each kernel based on its cumulative weight in all the T boosting rounds. The details of the proposed algorithm are given in Algorithm 3.
First of all, assume the length of hash key b is fixed, both WMKLSH and BMKLSH algorithms have the same querying time cost as that of KLSH and MKLSH. Next we focus on discussing training and indexing time cost.

As supervised methods, WMKLSH and BMKLSH algorithms need to test the performance for all training instances with all kinds of kernels once. For BMKLSH, at each boosting round, it only up-dates the distribution of training instances and computes the weight for each round based on the distribution. The updating step is in general linear. Thus, the main time cost consumed mostly falls into the process of validating the performance of training instances.
We conduct extensive experiments to examine the efficacy of the proposed algorithms for scalable content-based image retrieval.
We perform experiments on two well-known public image databases which have been widely used for benchmark image retrieval tasks. Besides, we downloaded 1,000,000 social images from Flickr to form a background class and combine it with the second database to form a large scale data set. We briefly introduce some details of the two data sets below.

The first data set is the INRIA Holidays data set 1 , which has been widely used for benchmark evaluation of image retrieval per-formance [ 21]. It contains 500 image groups, each of which repre-http://lear.inrialpes.fr/~jegou/data.php Algorithm 3 BMKLSH: Boosting Multi-Kernel LSH algorithm for optimizing the bit size allocation 1: for t =1 ,...,T do 2: for l =1 ,...,m do 6: end for 10: update the distribution of the training instances: 11: end for 13: allocate bits to m kernels [ b 1 ,...,b m ] based on the weights 14: Multi-Kernel Hashing( [ b 1 ,...,b m ] ) sents a distinct scene or object. The first image of each group is the query image and the correct retrieval results are the other images of the group. There are 1491 images in total, including 500 queries and 991 corresponding relevant images.

The second data set is a large-scale data set, which consists of both the ImageCLEF database 2 and the collection of 1,000,000 im-ages crawled from Flickr (named as  X  X lickr1M"). ImageCLEF is a medical image database, which was also used in [ 38]. For all the categories, we randomly select 10% images as the query pool, the other images as database pool. For the Flickr photos, we treat all of them as the background noisy photos, which are mainly used to test the scalability of our algorithms. We denote this data set as the  X  X mageCLEF + Flickr1M" data set or  X  X mageCLEFFlickr" for short.
We present our experimental results by showing the percentage of database items searched with the hashing function instead of measuring the exact search time in order to avoid the unfairness from different implementations of the codes. To achieve this pur-pose, we have to set a parameter for the LSH search, i.e.,  X  ,which is used to control the fraction of nearest neighbors to be linearly scanned.

For performance metric, we evaluate the retrieval performance based on mean Average Precision (mAP) and top-n ( n =1 , 2 ,..., 5 ) retrieval accuracy. The Average Precision (AP) value is the area un-der precision-recall curve for a query. The mAP value is calculated based on the average AP value of all the queries. The precision value is the ratio of relevant examples over the total retrieved ex-amples, while recall is the ratio of the relevant examples retrieved over the total relevant examples in the database. http://www.imageclef.org Our objective is to achieve fast and accurate image retrieval. As LSH can only return a portion of images, only the top returned images (Hit Items) are needed to be evaluated. Based on this con-cern, the mAP performance reported in our experiments is based on the mAP over all the returned items (except for the experiments of parameter sensitivity evaluation, where we adopt the top-10 mAP performance to enable a fair evaluation of different parameter set-tings). It is worth mentioning that in some previous works they usually reported mAP values for the whole data set. As we know, the more the image examples retrieved the higher the mAP value obtained. Thus, it is not totally fair to directly compare the ex-act value of our results with the results of the previous works. In our experiments, we have implemented all the compared methods including our algorithms under the same evaluation criterion to en-able fair comparisons. Nonetheless, from the experimental results, we found that the mAP results we achieved are higher than some of the others X  X  reported results, although we are actually based on a criterion that generates relatively lower mAP values.
As the proposed algorithms need some training data, we split the original query set into two parts, each time we select one part for query, and the other part for training. The final result is obtained by computing the average of the results over the two splits. Finally, we conduct the evaluations of all the algorithms 10 times and average the results over these 10 runs to obtain a stable result.
We adopt both global and local feature descriptors for represent-ing images in our experiments. We have some preprocessing by resizing all images to 500  X  500 pixels while keeping the aspect ratio unchanged. For global features, we extract five kinds of fea-tures, including (1) Color histogram and color moments, (2) Edge direction histogram, (3) Gabor wavelets transform, (4) Local Bi-nary Pattern, and (5) GIST. For local features, we extract the bag of visual words features based on two types of descriptors: SIFT and SURF. In particular, for SIFT: we adopt the Hessian-Affine in-terest region detector with threshold 500 and the SIFT descriptor; for SURF: we use SURF detector with threshold 500 and SURF descriptor. For the clustering, we adopt a forest of 16 kd-trees and search 2048 neighbor to speed up the clustering task. Finally, we use TF-IDF to generate the bag of visual words to represent the local features. In total, with different vocabulary sizes, we ex-tracted four kinds of local features, including SIFT200, SIFT1000, SURF200 and SURF1000.
From the above, we represent each image by 9 types of differ-ent features. We then build kernel functions for these 9 types of features. Before we compute the kernels, we normalize the feature vectors to zero mean (each dimension) and unit length (each point). We adopt the RBF kernel: where d (  X  ,  X  ) is the distance and  X  is selected as the mean of the pairwise distance where the distance is computed using L2 dis-tance. In total, we have a set of 9 kernels for our retrieval tasks. Finally, we note that all kernel matrices are normalized to unit trace to balance different kernels.
To extensively examine the efficacy of the proposed algorithms, we have implemented several different solutions for adopting KLSH with multiple kernels. In particular, we have implemented the fol-lowing algorithms:
We now present the performance evaluation results on the data sets. We measure the performance in terms of top-n ( n =1 , 2 ,..., 5 ) precision and the mAP value of all returned Hit items. For this ex-periment, we fix the parameters as follows:  X  =0.1, b = 300, p = 300, t = 30, and T = 20. We will evaluate the sensitivity of these parameters in the subsequent section. We summarize the experi-mental results of mAP performance of the compared algorithms on the two data sets in Table 2, and illustrate the details of the top-n precision results in Figure 2. Below we discuss the empirical observations from these results.

To examine statistical significance of the comparisons, for the experimental results reported below, we highlight the best result in each group in bold font by conducting student t-tests with the significance level  X  =0 . 05 .

KLSH-Uniform mean 0.58506 0.16902
From the experimental result shown in Table 2 and Figure 2,we can draw several observations. First of all, by comparing the three different KLSH algorithms with diff erent kernels, it seems a bit sur-prising to find that KLSH-Uniform, a simple uniform combination of all kernels, outperformed KLSH-Best, which is based on the best kernel chosen from the training set. But when thinking further, it is not difficult to explain the result as KLSH-best only explores a single kernel, while KLSH-Uniform jointly exploits multiple ker-nels. This result is further verified when we examine the result of KLSH-Weight, which outperform both KLSH-Best and KLSH-Uniform. These observations show that it is very important to ex-plore the power of multiple kernels for KLSH in some real-world applications.

Second, by comparing the proposed WMKLSH and BMKLSH algorithms against the KLSH and MKLSH algorithms, we found that the proposed algorithms generally perform better than the KLSH and MKLSH algorithms, and the proposed BMKLSH algorithm at-tained the best result among all the compared algorithms, which was significantly better than the other algorithms. These promising results showed that the proposed BMKLSH technique is more ef-fective to explore the power of multiple kernels for enhancing the image retrieval performance.
The experimental results of this data set are shown in the last col-umn of Table 2 and the right of Figure 2. By examining the results of the three KLSH algorithms, i.e., KLSH-Uniform, KLSH-Best, KLSH-Weight, we found that the situation is the same as that on the  X  X oliday" data set. Further, by comparing the proposed WMKLSH and BMKLSH algorithms with the KLSH and MKLSH algorithms, we found the similar observation where the proposed BMKLSH achieved the best result among all the compared schemes. This re-sult indicates that by the proposed boosting scheme, the BMKLSH algorithm is to effectively identify the best kernel and achieve a good tradeoff between the best kernel and the optimal combination of multiple kernels.
In this section, we aim to examine the parameter sensitivity of the proposed BMKLSH scheme for image retrieval tasks. Specif-ically, there are several important parameters, including (1)  X  ,the parameter that controls the fraction of nearest neighbors to be lin-early scanned, (2) b , the bit length of hash key, (3) the parameter t used in KLSH, which is to choose t indices for forming vector (4) the parameter p used in KLSH, which is to choose a subset of p examples for computing the kernel matrix in KLSH, and (5) the number of rounds T used in the boosting algorithm.

For the rest of the experiments, when varying one of the param-eters for evaluation, the others will remain fixed at the following default settings:  X  =0.1, b = 300, t = 30, p = 300, and T = 20. We adopt the top-10 mAP performance for evaluation in this section.
The  X  and b are two key parameters for the LSH algorithm. Fig-ure 3 (a) and (b) show the evaluation of two parameters  X  and b , respectively. From the experimental results, it is not difficult to see that increasing the value of  X  in general leads to increase of the top-10 mAP performance. This is not difficult to understand as the larger the  X  value, the more examples in the database will be in-spected, thus more relevant image examples can be likely retrieved. Similarly, we also observe that increasing the value of hash key length b also leads to the increase of the top-10 mAP performance. This is easy to understand as the larger the hash key length, the more information can be encoded, which thus could lead to more accurate results. Finally, similar to the previous observations, for all different values of these two parameters, BMKLSH consistently outperformed the other algorithms.
The parameter p and t are two parameters in the KLSH algo-rithm. Figure 4 (a) and (b) show the evaluation of two parameters p and t , respectively. From the results, we can see that increas-ing the p value in general leads to better performance of all the algorithms. This is reasonable as more examples are sampled we are able to obtain a more accurate estimate of the distribution for KLSH. However, when p is large enough (e.g., p&gt; 200 ), the im-provement of increasing p becomes not significant. In practice, to trade off the performance and efficacy, we can choose any value between 200 and 400 for this situation. On the other hand, for the parameter t , we found that increasing the value of t does not always lead to improvement of the performance. In some cases, a large t value could slightly degrade the performance. Nonetheless, all the algorithms are generally not very sensitive to this parameter.
We now examine how the number of boosting rounds affects the performance of the proposed BMKLSH algorithm. Figure 5 shows the evaluation results. From the results, we can see that the per-formance of BMKLSH algorithm in general increases with respect to the increase of T . The performance of the BMKLSH algorithm becomes saturated when T is sufficiently large, e.g., T&gt; =20 .
Figure 6 illustrates the bit allocation weights by three algorithms on the two data sets. The x, y, z-axis denotes the index of kernel used, the algorithm, and the weight assigned to each kernel, re-spectively. From this figure, we can see that MKLSH assigns equal weights to all the kernels, WMKLSH assigns different weights ac-cording to their performance and the weights are non-zero, while the weights assigned by BMKLSH are sparse, i.e., it focuses on those kernels which are more beneficial to the retrieval tasks. Specif-ically, for the  X  X oliday" dataset, the weights learned by BMKLSH Figure 5: Evaluation of the number of boosting rounds ( T )in the proposed BMKLSH algorithm. are mainly assigned to Color, GIST, SIFT1000 and SURF1000; while for the  X  X mageCLEFFlickr" dataset, BMKLSH allocates all the weights to only GIST and SURF1000. This is reasonable as most of the medical images in the "ImageCLEF" dataset are gray-level images and contain rich texture contents, which thus favor GIST features instead of color features. Moreover, it is interest-ing to observe that for the  X  X oliday" data set, the weight of Color is less than SIFT200 and SURF200 assigned with WMKLSH, but BMKLSH filters SIFT200 and SURF200 while keeps Color. This is also quite reasonable as SIFT1000 and SURF1000 are somewhat redundant with SIFT200 and SURF200, but they are complemen-tary to Color. These observations indicate that BMKLSH can learn an effective and sparse combination of multiple kernels.
Finally, we evaluate the efficiency of all the six algorithms on the  X  X oliday" data set. The experiments were running in Matlab on a Linux machine with 3GHz Intel CPU and 16GB RAM. As we analyzed before, all the compared algorithms share the same querying time given a fixed bit size b . In our experiments, typically for b = 300 , the average retrieval time per query is about 0.65ms for all the compared algorithms. In the following, we focus on the evaluation of training and indexing time efficiency.

Figure 7 shows the evaluation results of the total amount of train-ing and indexing time cost on the holiday data set, which were averaged over 10 runs. Among all the algorithms, it is not surpris-ing that WMKLSH and BMKLSH took more time cost for training and indexing because of the nature of their supervised learning pro-cesses. Such additional overhead is however acceptable since the training process typically is done in an offline manner. To further examine if the training and indexing process is scalable, we exam-ine the relationship of their time cost with respect to the bit size b , and found that they generally follow a linear relationship. Besides, we also vary the number of boosting rounds T for the BMKLSH algorithm. From the results, we can see that the time cost only in-creases slightly w.r.t. the increase of T , which is almost neglected.
Figure 7: Evaluation of training and indexing time efficiency.
Finally, we illustrate the qualitative retrieval performance by ran-domly choosing some query images from the database. Figure 8 shows the retrieval results by different algorithms. This figure in-cludes four retrieval cases of different queries, each of which shows the top-3 retrieved results by three representative algorithms: KLSH-Best, MKLSH, and BMKLSH. From the qualitative results, we can see that the proposed BMKLSH algorithm in general is able to re-turn more relevant results than the other algorithms. This paper investigated a framework of Multi-Kernel Locality-Sensitive Hashing by exploring multiple kernels for efficient and scalable content-based image retrieval. We first analyzed the the-oretical property of kernel LSH (KLSH). We further emphasized that it is of crucial importance to develop a proper combination of multiple kernels for determining the bit allocation task in KLSH, although KLSH and MKLSH with naive use of multiple kernels have been proposed in literature. We thus proposed two new algo-rithms: (i) WMKLSH that combines multiple kernels via a simple weighted combination, and (ii) BMKLSH that employs a boosting-like scheme to optimize the bit allocation of multiple kernels for KLSH. We have conducted an extensive set of experiments to eval-uate the performance of the proposed algorithms, in which the en-couraging results showed that the proposed BMKLSH algorithm using the boosting approach is able to considerably surpass a num-ber of baseline methods. Future work will apply our technique to tackle other problems, such as search-based image annotation. This work was in part supported by Singapore MOE tier 1 project (RG33/11), Microsoft Research grant (M4060936), and US Army Research Office (W911NF-11-1-0383).
