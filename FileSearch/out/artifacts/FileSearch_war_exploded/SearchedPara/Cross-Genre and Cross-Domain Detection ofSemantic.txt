 TechnischeUniversit  X  atDarmstadt HungarianAcademyofSciences Universit  X  atStuttgart UniversityofSzeged TechnischeUniversit  X  atDarmstadt Uncertainty is an important linguistic phenomenon that is relevant in various Natural
Language Processing applications, in diverse genres from medical to community generated, newswire or scientific discourse, and domains from science to humanities. The semantic un-uncertainty cue recognition task.
 the annotation of three corpora and present results with a state-of-the-art uncertainty cue recognition model for four fine-grained categories of semantic uncertainty. also show that even a distant source domain data set can contribute to the recognition and disambiguation of uncertainty cues, efficiently reducing the annotation costs needed to cover a new domain. Thus, the unified subcategorization and domain adaptation for training the models offer an efficient solution for cross-domain and cross-genre semantic uncertainty recognition. 1. Introduction
In computational linguistics, especially in information extraction and retrieval, it is of the utmost importance to distinguish between uncertain statements and factual information.Inmostcases,whattheuserneedsisfactualinformation,henceuncertain propositions should be treated in a special way: Depending on the exact task, the system should either ignore such texts or separate them from factual information. In machinetranslation,itisalsonecessarytoidentifylinguisticcuesofuncertaintybecause the source and the target language may differ in their toolkit to express uncertainty (one language uses an auxiliary, the other uses just a morpheme). To cite another example,inclinicaldocumentclassification,medicalreportscanbegroupedaccording to whether the patient definitely suffers, probably suffers, or does not suffer from an illness.
 literature. We consider propositions to which no truth value can be attributed, given thespeaker X  X mentalstate,asinstancesof semantic uncertainty.Incontrast,uncertainty mayalsoariseatthe discourse level,whenthespeakerintentionallyomitssomeinfor-mation from the statement, making it vague, ambiguous, or misleading. Determining whetheragivenpropositionisuncertainornotmayinvolveusingafinitedictionaryof linguisticdevices(i.e.,cues).Lexicalcues(suchasmodalverbsoradverbs)arerespon-sibleforsemanticuncertaintywhereasdiscourse-leveluncertaintymaybeexpressedby lexicalcuesandsyntacticcues(suchaspassiveconstructions)aswell.Wefocusonfour typesofsemanticuncertaintyinthisstudyandhenceforththeterm cue willbetakento mean lexical cue .
 language processing (NLP) application include the steps of locating lexical cues for uncertainty, disambiguating them (as not all occurrences of the cues indicate uncer-tainty), and finally linking them with the textual representation of the propositions in question. The linking of a cue to the textual representation of the proposition can be performed on the basis of syntactic rules that depend on the word class of the lexical cue, but they are independent of the actual application domain or text type where the cue is observed. The set of cues used and the frequency of their certain and uncertain usages are domain and genre dependent, however, and this has to be addressed if we seek to craft automatic uncertainty detectors. Here we interpret genre asthebasicstyleandformalcharacteristicsofthewritingthatisindependentofitstopic (e.g., scientific papers, newswire texts, or business letters), and domain as a particular field of knowledge and is related to the topic of the text (e.g., medicine, archeology, or politics).

For instance, the mathematical sense of probable is dominant in mathematical texts whereas its ratio can be relatively low in papers in the humanities. The frequency of the two distinct meanings of the verb evaluate (which can be a synonym of judge [an 336 uncertainmeaning]and calculate )isalsodifferentinthebioinformaticsandcellbiology domains.Compare: and we examine the recognition of uncertainty cues in context. We do not address the problemoflinkingcuestopropositionsindetail(see,e.g.,Chapman,Chu,andDowling [2007]andKilicogluandBergler[2009]fortheinformationextractioncase).
 requiredfromvariousdomains.Todate,severalcorporaannotatedforuncertaintyhave been constructed for different genres and domains (BioScope, FactBank, WikiWeasel, and MPQA, to name but a few). These corpora cover different aspects of uncertainty, however,beinggroundedondifferentlinguisticmodels,whichmakesithardtoexploit cross-domainknowledgeinapplications.Thesedifferencesinpartstemfromthevaried applicationneedsacrossapplicationdomains.Differenttypesofuncertaintyandclasses of linguistic expressions are relevant for different domains. Although hypotheses and investigations form a crucial part of the relevant cases in scientific applications, they arelessprominentinnewswiretexts,wherebeliefsandrumorsplayamajorrole.This finding motivates a more fine-grained treatment of uncertainty. In order to bridge the existinggapsbetweenapplicationgoals,thesetypicalcasesneedtobedifferentiated.A fine-grained categorization enables the individual treatment of each subclass, which is less dependent on domain differences than using one coarse-grained uncertainty class.Moreover,thisapproachenableseachparticularapplicationtoidentifyandselect fromapoolofmodelsonlythoseaspectsofuncertaintythatarerelevantinthespecific domain.
 tion of semantic uncertainty in which all the previous corpus annotation works can be placed, and which reveals the fundamental differences between the currently existing resources. In addition, we manually harmonized the annotations of three corpora and performedthefine-grainedlabelingaccordingtothesuggestedsubcategorizationsoas tobeabletoperformcross-domainexperiments.
 features that can be reliably obtained for many different domains and text types, and to craft models that exploit the shared knowledge from different sources as much as possible, making the adaptation to new domains efficient. The study of learning efficientmodelsacrossdifferentdomainsisthesubjectoftransferlearninganddomain adaptation research (cf. Daum  X  e III and Marcu 2006; Pan and Yang 2010). The domain adaptation setting assumes a target domain (for which an accurate model should be learnedwithalimitedamountoflabeledtrainingdata),asourcedomain(withcharac-teristicsdifferentfromthetargetandforwhichasubstantialamountoflabeleddatais available),andanarbitrarysupervisedlearningmodelthatexploitsboththetargetand sourcedomaindatainordertolearnanimprovedtargetdomainmodel.
 of the target and source domains (the two domains should be sufficiently similar to allow knowledge transfer); and (ii) the application of an efficient domain adaptation method (which permits the learning algorithm to exploit the commonalities of the domainswhilepreservingthespecialcharacteristicsofthetargetdomain).
 uncertainty detection, how this impact depends on the distance between target and source domains concerning their domains and genres, and how these differences can bereducedtoproduceaccuratetargetdomainmodelswithlimitedannotationeffort. domainadaptationdifficult, 1 toourknowledgethisisthefirststudytoanalyzedomain differences and adaptability in the context of uncertainty detection in depth, and also thefirststudytoreportconsistentlypositiveresultsincross-training.
 uncertainty phenomena is presented in detail and it is compared with the concept of 338 uncertainty used in existing corpora. A framework for detecting semantic uncertainty isthenpresentedinSection3.RelatedworkoncuedetectionissummarizedinSection4, whichisfollowedbyadescriptionofourcuerecognitionsystemandapresentationof our experimental set-up using various source and target genre and domain pairs for cross-domain learning and domain adaptation in Section 5. Our results are elaborated on in Section 6 with a focus on the effect of domain similarities and on the annotation effortneededtocoveranewdomain.Wethenconcludewithasummaryofourresults andmakesomesuggestionsforfutureresearch. 2. The Phenomenon Uncertainty
In order to be able to introduce and discuss our data sets, experiments, and findings, we have to clarify our understanding of the term uncertainty . Uncertainty X  X n its most generalsense X  X anbeinterpretedaslackofinformation:Thereceiveroftheinformation (i.e.,thehearerorthereader)cannotbecertainaboutsomepiecesofinformation.Inthis respect, uncertainty differs from both factuality and negation; as regards the former, the hearer/reader is sure that the information is true and as for the latter, he is sure that the information is not true. From the viewpoint of computer science, uncertainty emerges due to partial observability, nondeterminism, or both (Russell and Norvig 2010). Linguistic theories usually associate the notion of modality with uncertainty:
Epistemic modality encodes how much certainty or evidence a speaker has for the proposition expressed by his utterance (Palmer 1986) or it refers to a possible state of the world in which the given proposition holds (Kiefer 2005). The common point in these approaches is that in the case of uncertainty, the truth value/reliability of the proposition cannot be decided because some other piece of information is missing.
Thus, uncertain propositions are those in our understanding whose truth value or reliabilitycannotbedeterminedduetolackofinformation.
 classificationofseveraltypesofsemanticuncertainty.Ourclassificationisgroundedon theknowledgeofexistingcorporaanduncertaintyrecognitiontoolsandourchiefgoal hereistoprovideacomputationallinguistics-orientedclassification.Withthisinmind, our subclasses are intended to be well-defined and easily identifiable by automatic tools. Moreover, this classification allows different applications to choose the subset of phenomenatoberecognizedinaccordancewiththeirmaintask(i.e.,wetriedtoavoid anoverlycoarseorfine-grainedcategorization). 2.1 Classification of Uncertainty Types
Several corpora annotated for uncertainty have been published in different domains suchasbiology(MedlockandBriscoe2007;Kim,Ohta,andTsujii2008;Settles,Craven, and Friedland 2008; Shatkay et al. 2008; Vincze et al. 2008; Nawaz, Thompson, and Ananiadou 2010), medicine (Uzuner, Zhang, and Sibanda 2009), news media (Rubin,
Liddy, and Kando 2005; Wilson 2008; Saur  X   X  and Pustejovsky 2009; Rubin 2010), and encyclopedia (Farkas et al. 2010). As can be seen from publicly available annotation guidelines, there are many overlaps but differences as well in the understanding of uncertainty, which is sometimes connected to domain-and genre-specific features of thetexts.Hereweintroduceadomain-andgenre-independentclassificationofseveral types of semantic uncertainty, which was inspired by both theoretical and computa-tionallinguisticconsiderations. 2.1.1 A Tentative Classification. Based on corpus data and annotation principles, the expression uncertainty canbeusedasanumbrellatermforcoveringphenomenaatthe semantic and discourse levels. 3 Our classification of semantic uncertainty is assumed to be language-independent, but our examples presented here come from the English language,tokeepmatterssimple.
 semantics. They cannot be assigned a truth value (i.e., it cannot be stated for sure whethertheyaretrueorfalse)giventhespeaker X  X currentmentalstate.
 (see Figure 1). The main difference between epistemic and hypothetical uncertainty is thatwhereasinstancesofhypotheticaluncertaintycanbetrue,falseoruncertain,epis-temically uncertain propositions are definitely uncertain X  X n terms of possible worlds, hypothetical propositions allow that the proposition can be false in the actual world, butinthecaseofepistemicuncertaintythefactualityofthepropositionisnotknown. nor false: It describes a possible world where the proposition holds but this possible world does not coincide with the speaker X  X  actual world. In other words, it is certain thatthepropositionisuncertain.Epistemicuncertaintyisrelatedtoepistemicmodality: asentenceisepistemicallyuncertainifonthebasisofourworldknowledgewecannot decide at the moment whether it is true or false (hence the name) (Kiefer 2005). The sourceofanepistemicallyuncertainpropositioncannotclaimtheuncertainproposition andbesureaboutitsoppositeatthesametime. 340 determined either and nothing can be said about the probability of their happening.
Propositions under investigation are an example of such statements: Until further anal-ysis, the truth value of the proposition under question cannot be stated. Conditionals canalsobeclassifiedasinstancesofhypotheses.Itisalsocommoninthesetwotypesof uncertain propositions that the speaker can utter them while it is certain (for others or even for him) that its opposite holds hence they can be called instances of paradoxical uncertainty.

Doxastic modality expressesthespeaker X  X beliefs X  X hichmaybeknowntobetrueor false by others in the current state of the world. Necessity (duties, obligation, orders) is the main objective of deontic modality; dispositional modality is determined by the dispositions(i.e.,physicalabilities)ofthepersoninvolved;andcircumstantialmodality is defined by external circumstances. Buletic modality is related to wishes, intentions, plans, and desires. An umbrella term for deontic, dispositional, circumstantial, and buleticmodalityis dynamic modality (Kiefer2005).
 future,theymayhappenbutatthemomentitisnotclearwhethertheywilltakeplace ornot/whethertheyaretrue,false,oruncertain. 2.1.2 Comparison with other Classifications. Thefeasibilityoftheclassificationproposedin this study can be justified by mapping the annotation schemes used in other existing corporatooursubcategorizationsofuncertainty.Thissystematiccomparisonalsohigh-lightsthemajordifferencesbetweenexistingworksandpartlyexplainswhyexamples for successful cross-domain application of existing resources and models are hard to findintheliterature.
 Settles,Craven,andFriedland2008;Shatkayetal.2008;Thompsonetal.2008;Nawaz,
Thompson, and Ananiadou 2010) fall into the epistemic uncertainty class. BioScope (Vincze et al. 2008) annotations mostly belong to the epistemic uncertainty category, withtheexceptionofclausalhypotheses(i.e.,hypothesesthatareexpressedbyaclause headed by if or whether ), which are instances of the investigation class. The probable classofGeniaEvent(Kim,Ohta,andTsujii2008)isoftheepistemically uncertaintype andthe doubtful classbelongstotheinvestigationclass.Rubin,Liddy,andKando(2005) consideruncertaintyasaphenomenonbelongingtoepistemicmodality:Thehigh,mod-erate,andlowlevelsofcertaintycoincidewithourepistemicuncertaintycategory.The speculation annotations of the MPQA corpus also belong to the epistemic uncertainty class,withfourlevels(Wilson2008).The probable and possible classesfoundinFactBank (Saur  X   X  and Pustejovsky 2009) are of the epistemically uncertain type, events with a genericsourcebelongtodiscourse-leveluncertainty,whereasunderspecifiedeventsare classified as hypothetical uncertainty in our system as, by definition, their truth value cannotbedetermined.WikiWeasel(Farkasetal.2010)containsannotationforepistemic uncertainty,butdiscourse-leveluncertaintyisalsoannotatedinthecorpus(seeFigure1 foranoverview).ThecategoriesusedforthemachinereadingtaskdescribedinMorante and Daelemans (2011) also overlap with our fine-grained classes: Uncertain events in their system fall into our epistemic uncertainty class. Their modal events expressing purpose, need, obligation, or desire are instances of dynamic modality, whereas their conditions are understood in a similar way to our condition class. The modality types listed in Baker et al. (2010) can be classified as types of dynamic modality, except for theirbeliefcategory.Instancesofthelattercategoryareeithercertain( It is certain that he met the president )orepistemicordoxasticmodalityinoursystem. 2.2 Types of Semantic Uncertainty Cues
We assume that the nature of the lexical unit determines the type of uncertainty it represents,thatis,semanticuncertaintyishighlylexicalinnature.Thepartofspeechof theuncertaintycuecandidatesservesasthebasisforcategorization,similartotheones foundinHyland(1994,1996,1998)andRizomilioti(2006).InEnglish,modalityisoften associated withmodal auxiliaries (Palmer1979),but,asTable1shows,therearemany other parts of speech that can express uncertainty. It should be added that there are cueswhereitdependsonthecontext,ratherthanthegivenlexicalitem,whatsubclass ofuncertaintythecuerefersto,forexample, may candenoteepistemicmodality( It may rain ...)ordynamicmodality( Now you may open the door ).Thesecategoriesarelistedin
Table1. 3. A Framework for Detecting Semantic Uncertainty
In our model, uncertainty detection is a standalone task that is largely independent of theunderlyingapplication.Inthissection,webrieflydiscusshowuncertaintydetection 342 can be incorporated into an information extraction task, which is probably the most relevant application area (see Kim et al. [2009] for more details). In the information extraction context, the key steps of recognizing uncertain propositions are locating the cues,disambiguatingthem(asnotalloccurrencesofthecuesindicateuncertainty;recall the example of evaluate ), and finally linking them with the textual representation of the propositions in question. We note here that marking the textual representations of importantpropositions(oftenreferredtoas events ininformationextraction)isactually the main goal of an information extraction system, hence we will not focus on their identificationandjustassumethattheyarealreadymarkedintexts.

Here the EVENT mark-up is produced by the information extraction system, and uncertainty detection consists of i) the recognition of the cue word hypothesized ,and determining whether it denotes uncertainty in this specific case (producing the CUE mark-up)andii)determiningwhetherthecue hypothesized modifiestheeventtriggered by inhibits ornot(positiveexampleinthiscase). 3.1 Uncertainty Cue Detection and Disambiguation
The cue detection and disambiguation problem can be essentially regarded as a token labeling problem. Here the task is to assign a label to each of the tokens of a sentence in question according to whether it is the starting token of an uncertainty cue ( B-previousstudiesassumeabinaryclassificationtask,namely,eachtokeniseitherpartof anuncertaintycue,oritisnotacue.Forfine-graineduncertaintydetection,adifferent label has to be used for each uncertainty type to be distinguished. This way, the label sequenceofasentencenaturallyidentifiesalluncertaintycues(withtheirtypes)inthe sentence,anddisambiguationissolvedjointlywithrecognition.
 tainsensesofcuesvaryindifferentdomainsandgenres,uncertaintycuedetectionand disambiguationarethemainfocusofthecurrentstudy. 3.2 Linking Uncertainty Cues to Propositions
The task of linking the detected uncertainty cues to propositions can be formulated asabinaryclassificationtaskoveruncertaintycueandeventmarkerpairs.Therelation holdsandisconsideredtrueifthecuemodifiesthetruthvalue(confidence)oftheevent; it does not hold and is considered false if the cue does not have any impact on the interpretation of the event. That is, the pair (hypothesized, inhibits) in Example (8) is an instanceofpositiverelation.
 pendencygrammarrules(i.e.,theproblemismainlysyntaxdriven).Asthegrammatical propertiesofthelanguagearesimilarinvariousdomainsandgenres,thistaskislargely domain-independent, asopposedtotherecognitionanddisambiguation task.Because ofthis,wesketchthemostimportantmatchingpatterns,butdonotaddressthelinking taskingreatdetailhere. to event markers. For practical implementations of heuristic cue/event matching, see Chapman,Chu,andDowling(2007)andKilicogluandBergler(2009).
 4. Related Work on Uncertainty Cue Detection
Herewereviewthepublishedworksrelatedtouncertaintycuedetection.Earlierstudies focused either on in-domain cue recognition for a single domain or on cue lexicon extractionfromlargecorpora.Thelatterapproachisapplicabletomultipledomains,but doesnotaddressthedisambiguation ofuncertainandothermeaningsoftheextracted cue words. We are also aware of several studies that discussed the differences of cue distributions in various domains, without developing a cue detector. To the best of ourknowledge,ourstudyisthefirsttoaddressthegenre-anddomain-adaptabilityof uncertaintycuerecognitionsystemsandthusuncertaintydetectioninageneralcontext. tainty detection, that is, how to utilize the recognized cues (see, for instance, Kilicoglu andBergler[2008],Uzuner,Zhang,andSibanda[2009]andSaur  X   X [2008]forinformation extraction or Farkas and Szarvas [2008] for document labeling applications), and a recentpilottasksoughttoexploitnegationandhedgecuedetectorsinmachinereading (MoranteandDaelemans2011).Asthefocusofourpaperiscuerecognition,however, weomittheirdetaileddescriptionhere. 4.1 In-Domain Cue Detection
In-domain uncertainty detectors have been developed since the mid 1990s. Most of these systems use hand-crafted lexicons for cue recognition and they treat each oc-currence of the lexicon items as a cue X  X hat is, they do not address the problem of disambiguatingcues(Friedmanetal.1994;Light,Qiu,andSrinivasan2004;Farkasand Szarvas2008;Saur  X   X 2008;Conway,Doan,andCollier2009;VanLandeghemetal.2009).
ConText (Chapman, Chu, and Dowling 2007) uses regular expressions to define cues and  X  X seudo-triggers X . A pseudo-trigger is a superstring of a cue and it is basically used for recognizing contexts where a cue does not imply uncertainty (i.e., it can be regarded as a hand-crafted cue disambiguation module). MacKinlay, Martinez, and
Baldwin (2009) introduced a system which also used non-consecutive tokens as cues (like not+as+yet ).
 tectors have also been developed (to the best of our knowledge each of them uses an in-domain training data set). They use token classification (Morante and Daelemans 2009; Clausen 2010; Fernandes, Crestana, and Milidi  X  u 2010; S  X  anchez, Li, and Vogel 2010) or sequence labeling approaches (Li et al. 2010; Rei and Briscoe 2010; Tang et al. 2010; Zhang et al. 2010). In both cases the tokens are labeled according to whether they are part of a cue. The latter assigns a label sequence to a sentence (a sequence of 344 tokens)thusitnaturallydealswiththecontextofaparticularword.Ontheotherhand, contextinformationforatokenisbuiltintothefeaturespaceofthetokenclassification approaches.  X  Ozg  X  urandRadev(2009)andVelldal(2010)matchcuesfromalexiconthen applyabinaryclassifierbasedonfeaturesdescribingthecontextofthecuecandidate. ally includes surface-level, part-of-speech, and chunk-level features. A few systems havealsouseddependencyrelationtypesoriginatingatthecue(ReiandBriscoe2010; S  X  anchez, Li, and Vogel 2010; Velldal,  X vrelid, and Oepen 2010; Zhang et al. 2010); the
CoNLL-2010SharedTaskfinalrankingsuggeststhatithasonlyalimitedimpactonthe performanceofanentiresystem(Farkasetal.2010),however.  X  Ozg  X  urandRadev(2009) further extended the feature set with the other cues that occur in the same sentence as thecue,andpositionalfeaturessuchasthesectionheaderofthearticleinwhichthecue occurs (the latter is only defined for scientific publications). Velldal (2010) argues that thedimensionalityoftheuncertaintycuedetectionfeaturespaceistoohighandreports improvementsbyusingthesparserandomindexingtechnique.
 detection X  X xploiting weasel tags 4 in Wikipedia articles given by editors. They used syntax-basedpatternstorecognizetheinternalstructureofthecues,whichhasproved usefulasdiscourse-leveluncertaintycuesareusuallylongandhaveacomplexinternal structure(asopposedtosemanticuncertaintycues).
 logicalandmedicaldomains.Allofthesestudies,however,focusononlyonedomain, namely, in-domain cue detection is carried out, which assumes the availability of a trainingdatasetofsufficientsize.TheonlyexceptionweareawareofistheCoNLL-2010
Shared Task (Farkas et al. 2010), where participants had the chance to use Wikipedia data on biomedical domain and vice versa. Probably due to the differences in the annotated uncertainty types and the stylistic and topical characteristics of the texts, very few participants performed cross-domain experiments and reported only limited success(seeSection5.3.2formoreonthis).
 an important aspect of uncertainty detection and that the domain specificity of disam-biguation models and domain adaptation in general are largely unexplored problems inuncertaintydetection. 4.2 Weakly Supervised Extraction of Cue Lexicon
Similar to our approach, several studies have addressed the problem of developing an uncertainty detector for a new domain using as little annotation effort as possible.
The aim of these studies is to identify uncertain sentences; this is carried out by semi-automatic construction of cue lexicons. The weakly supervised approaches start with very small seed sets of annotated certain and uncertain sentences, and use bootstrap-pingtoinduceasuitabletrainingcorpusinanautomaticway.Suchapproachescollect potentially certain and uncertain sentences from a large unlabeled pool based on their similarity totheinstances intheseedsets(Medlock andBriscoe 2007), orbasedonthe known errors of an information extraction system that is itself sensitive to uncertain texts (Szarvas 2008). Further instances are then collected (in an iterative fashion) on the basis of their similarity to the current training instances. Based on the observation that uncertain sentences tend to contain more than one uncertainty cue, these models successfullyextendtheseedsetswithautomaticallylabeledsentences,andcanproduce anuncertaintyclassifierwithasentence-levelF-scoreof60 X 80%fortheuncertainclass, giventhatthetextsoftheseedexamples,theunlabeledpool,andtheactualevaluation datashareverysimilarproperties.
 (setofcues)ofthegivendomain,butareotherwiseunabletodisambiguatethepotential cuewords X  X hatis,todistinguishbetweentheuncertainandcertainusesoftheprevi-ouslyseencues.Thisdeficiencyofthederivedmodelsisinherenttothebootstrapping process, which considers all occurrences of the cue candidates as good candidates for positiveexamples(asopposedtounlabeledsentenceswithoutanypreviouslyseencue words).
 cuelexicon.Theirlinguisticallymotivatedapproachisalsobasedontheweaklysuper-vised induction of a corpus of uncertain sentences. It exploits the syntactic patterns of uncertainsentencestoidentifynewcuecandidates.
 did not tackle the problem of disambiguating the certain and uncertain uses of cue candidates,whichisamajordrawbackfromapracticalpointofview. 4.3 Cue Distribution Analyses
Besidesautomaticuncertaintyrecognition,severalstudiesinvestigatedthedistribution of hedge cues in scientific papers from different domains (Hyland 1998; Falahati 2006;
Rizomilioti2006).Theeffectofdifferentdomainsonthefrequencyofuncertainexpres-sions was examined in Rizomilioti (2006). Based on a previously defined dictionary of hedgecues,sheanalyzedthelinguistictoolsexpressingepistemicmodalityinresearch papers from three domains, namely, archeology, literary criticism, and biology. Her results indicated that archaeological papers tend to contain the most uncertainty cues (which she calls downtoners) and the fewest uncertainty cues can be found in literary criticismpapers.DifferentacademicdisciplineswerecontrastedinHyland(1998)from the viewpoint of hedging: Papers belonging to the humanities contain significantly more hedging devices than papers in sciences. It is interesting to note, however, that inbothstudies,biologicalpapersaresituatedinthemiddleasfarasthepercentagerate ofuncertaintycuesisconcerned.Falahati(2006)examinedhedgesinresearcharticlesin medicine, chemistry, and psychology and concluded that it is psychology articles that containthemosthedges.
 different technical/scientific domains and different genres express uncertainty in gen-eral,andintheuseofsemanticuncertaintyinparticular.Differencesarefoundnotjust in the use of different vocabulary for expressing uncertainty, but also in the frequency of certain and uncertain usage of particular uncertainty cues. These findings underpin the practical importance of domain portability and domain adaptation of uncertainty detectors. 5. Uncertainty Cue Recognition
Inthissection,wepresentouruncertaintycuedetectorandtheresultsofthecross-genre and-domainexperimentscarriedoutbyus.Beforedescribingourmodelanddiscussing the results of the experiments, a short overview of the texts used as training and test 346 datasetswillbegivenalongwithanempiricalanalysisofthesensedistributionsofthe mostfrequentcues. 5.1 Data Sets In our investigations, we selected three corpora (i.e., BioScope, WikiWeasel, and Fact-Bank) from different domains (biomedical, encyclopedia, and newswire, respectively).
Genres also vary in the corpora (in the scientific genre, there are papers and abstracts whereas the other corpora contain pieces of news and encyclopedia articles). We pre-ferredcorporaonwhichearlierexperimentshadbeencarriedoutbecausethisallowed ustocompareourresultswiththoseofpreviousstudies.Thisselectionmakesitpossible toinvestigatedomainandgenredifferencesbecauseeachdomainhasitscharacteristic languageuse(whichmightresultindifferencesincuedistribution)anddifferentgenres also require different writing strategies (e.g., in abstracts, implications of experimental resultsareoftenemphasized,whichusuallyinvolvestheuseofuncertainlanguage). texts from full papers and scientific abstracts; the texts were manually annotated for hedge cues and their scopes. In our experiments, 15 other papers annotated for the
CoNLL-2010 Shared Task (Farkas et al. 2010) were also added to the set of BioScope papers. The WikiWeasel corpus (Farkas et al. 2010) was also used in the CoNLL-2010
Shared Task and it was manually annotated for weasel cues and semantic uncertainty in randomly selected paragraphs taken from Wikipedia articles. The FactBank corpus contains texts from the newswire domain (Saur  X   X  and Pustejovsky 2009). Events are annotated in the data set and they are evaluated on the basis of their factuality from theviewpointoftheirsources.
 set-up,textsbelongingtodifferentgenresalsoplayanimportantrole,dataonabstracts andpapersareincludedseparately. 5.1.1 Genres and Domains. Texts found in the three corpora to be investigated can be categorizedintothreegenres,whichcanbefurtherdividedtosubgenresatafinerlevel ofdistinction.Figure2depictsthisclassification.
 course genre. FactBank texts can be divided into broadcast and written news, and Wikipediatextsbelongtotheencyclopediagenre.
 and encyclopedia. Once again, these domains can be further divided into narrower topicsatafine-grainedlevel,whichisshowninFigure3.Allabstractsandfivepapers inBioScopearerelatedtotheMeSHterms human , blood cell ,and transcription factor ( hbc in Figure 3). Nine BMC Bioinformatics papers come from the bioinformatics domain ( bmc inFigure3),andtenpapersdescribesomeexperimentalresultsontheDrosophila species ( fly ). FactBank news can be classified as stock news, political news, and criminal news. Encyclopedia articles cover a broad range of topics, hence no detailed classificationisgivenhere. 5.1.2 The Normalization of the Corpora. In order to uniformly evaluate our methods in each domain and genre (and each corpus), the evaluation data sets were normalized.
This meant that cues had to be annotated in each data set and differentiated for types ofsemanticuncertainty.ThisresultedinthereannotationofBioScope,WikiWeasel,and
FactBank. 5 In BioScope, the originally annotated cues were separated into epistemic cues and subtypes of hypothetical cues and instances of hypothetical uncertainty not yet marked were also annotated. In FactBank, epistemic and hypothetical cues were annotated: Uncertain events were matched with their uncertainty cues and instances of hypothetical uncertainty that were originally not annotated were also marked in the corpus. In the case of WikiWeasel, these two types of cues were separated from discourse-levelcues.
 in any of the corpora. Although dynamic modality seems to play a role in the news domain, it is less important and less represented in the other two domains we investi-gated here. The other subclasses are more of general interest for the applications. For example,oneofourtrainingcorporacomesfromthescientificdomain,whereitismore important to distinguish facts from hypotheses and propositions under investigation 348 (which can be later confirmed or rejected, compare the meta-knowledge annotation schemedevelopedforbiologicalevents[Nawaz,Thompson,andAnaniadou2010]),and frompropositionsthatdependoneachother(conditions). 5.1.3 Uncertainty Cues in the Corpora. An analysis of the cue distributions reveals some interesting trends that can be exploited in uncertainty detection across domains and genres.Themostfrequentcuestemsinthe(sub)corporausedinourstudycanbeseen inTable3andtheyareresponsibleforabout74%ofepistemiccueoccurrences, 55%of doxastic cue occurrences, 70% of investigation cue occurrences, and 91% of condition cueoccurrences.
 possible , might ,and suggest alsooccurfrequentlyinourdataset.
 uncertaintyclassesineachcorpus,whichispresentedinFigure4.Inmostofthecorpora, epistemic cues are the most frequent (except for FactBank) and they vary the most:
Out of the 300 cue stems occurring in the corpora, 206 are epistemic cues. Comparing the domains, it can readily be seen that in biological texts, doxastic uncertainty is not frequent, which is especially true for abstracts, whereas in FactBank and WikiWeasel they cover about 27% of the data. The most frequent doxastic keywords exhibit some domain-specificdifferences,however:InBioScope,themostfrequentonesinclude puta-tive and hypothesis ,whichrarelyoccurinFactBankandWikiWeasel.Nevertheless,cues belonging to the investigation class can be found almost exclusively in scientific texts (89%ofthemareinBioScope),whichcanbeexpectedbecausetheaimofscientificpub-lications is to examine whether a hypothesized phenomenon occurs. Among the most frequent cues, investigate , examine ,and study belong to this group. These data reveal that the frequency of doxastic and investigation cues is strongly domain-dependent, and this explains the fact that the investigation vocabulary is very limited in Factbank andWikiWeasel. Onlyabout 10cuestemsbelongtothisuncertainty class inthesecor-pora. The set of condition cue stems, however, is very small in each corpus; altogether 18conditioncuestemscanbefoundinthedata,although if and would areresponsible for almost 75% of condition cue occurrences. It should also be mentioned that the percentageofconditioncuesishigherinFactBankthanintheothercorpora.
 ofstemmedforms:Certainverbsinthirdpersonsingular(e.g., expects or believes )occur mostly in FactBank and WikiWeasel. The reason for this may be that when speaking aboutsomeoneelse X  X opinioninscientificdiscourse,thesourceoftheopinionisusually providedintheformofreferencesorcitations X  X suallyattheendofthesentence X  X nd duetothis,theverbisoftenusedinthepassiveform,asinExample(9).
In contrast, impersonal constructions are hardly used in news media, where the ob-jective is to inform listeners about the source of the news presented as well in order to enable them to judge the reliability of a piece of news. Here, a clause including the sourceandacommunicationverbisusuallyattachedtotheproposition.
 dition cues can rarely be found in abstracts, although they occur more frequently in papers (with the non-cue usage still being much more frequent). Another difference is the percentage of cues of the investigation type, which may be related to the structure 350 ofabstracts.Biologicalabstractsusuallypresenttheproblemtheyexamineanddescribe methodstheyuse.Thisentailstheapplicationofpredicatesbelongingtotheinvestiga-tionclassofuncertainty.Itcanbeargued,however,thatscientificpapersalsohavethese characteristicsbutabstractsaremuchshorterthanpapers(generally,theycontainabout 10 X 12 sentences). Hence, investigation cues are responsible for a greater percentage of cues.
 fear , worry ,and rumor are highly unlikely to occur in scientific publications, but they occurrelativelyofteninnewstextsandinWikipediaarticles.Asforlexicaldivergences between abstracts and papers, many of them are related to verbs of investigation and theirdifferentusage.Inthecorpora,verbsofinvestigationsweremarkedonlyifitwas not clear whether the event/phenomenon would take place or not. If it has already happened ( The police are investigating the crime ) or the existence of the thing under investigation can be stated with certainty, independently of the investigation ( The top ten organisms were examined ), then they are not instances of hypotheses, so they were notannotated.Asthedatasetsmakeclear,thereweresomecandidatesofinvestigation verbsthatoccurredintheinvestigationsensemostlyinabstractsbutinanothersensein papers,especiallyinthe bmc dataset(e.g. assess or examine ). Evaluate alsohadaspecial mathematicalsensein bmc papers,whichdidnotoccurinabstracts.
 onlyrelativelyrarely)inabstracts.Thisisespeciallytrueforthe bmc dataset,where can , if , would , could ,and will are among the 15 most frequent cues and represent 23.21% of cueoccurrences,butonly3.85%inabstracts.Itisalsoapparentthattherateofepistemic cuesislowerin bmc papersthaninabstractsorothertypesofpapers.
 papers are compared because their fine-grained domain is the same. Thus, it may be assumed that differences between their cues are related to the genre. The sets of cues usedaresimilar,butthesensedistributionsmaydifferforcertainambiguouscues.For instance, indicate mostly appears in the  X  X uggest X  sense in abstracts, whereas in papers itisusedinthe X  X ignal X  X ense.Anotherdifferenceisthatthepercentagerateofdoxastic cues is almost twice as high in papers as in abstracts (10.6% and 5.7%, respectively). Besidesthesedifferences,thetwodatasetsarequitesimilar.
 ical papers are contrasted. As stressed earlier, bmc papers contain fewer instances of epistemicuncertainty,butconditioncuesoccurmorefrequentlyinthem.Nevertheless, fly and hbc papers are rather similar in these respects but hbc papers contain more investigationcuesthantheothertwosubcorpora.Asregardslexicalissues,thenon-cue usageof possible incomparativeconstructionsismorefrequentinthe bmc datasetthan in the other papers and many occurrences of if in bmc are related to definitions, which were not annotated as uncertain. On the basis of this information, the fly and the hbc domainsseemtobemoresimilartoeachotherthantothe BMC datasetfromalinguistic pointofview.
 behighlightedconcerningthedistributionofuncertaintycuesacrosscorpora.Doxastic uncertainty is of primary importance in the news and encyclopedia domains whereas theinvestigationclassischaracteristicofthebiologicaldomain.Withinthelatter,there is a genre-related difference as well: It is the epistemic and investigation classes that are mainly present in abstracts whereas in papers cues belonging to other uncertainty classes can also be found. Thus, when applying techniques developed for biological texts or abstracts to news texts, for example, doxastic uncertainty cues deserve special attentionasitmightwellbethecasethatthereareinsufficienttrainingexamplesforthis classofuncertaintycues.Theadaptationofanuncertaintycuedetectorconstructedfor encyclopedia texts requires the special treatment of investigation cues, however, if, for instance, scientific discourse is the target genre since they are underrepresented in the sourcegenre. 5.2 Evaluation Metrics
Asevaluationmetrics,weusedcue-levelandsentence-level F  X  = 1 tain class (the standard evaluation metrics of Task 1 of the CoNLL-2010 shared task) and denote them by F cue and F sent , respectively. We report cue-level F individualsubcategoriesofuncertaintyandtheunlabeled(binary) F
Asentenceistreatedasuncertain(inthegoldstandardandprediction)iffitcontainsat least one cue. Note that the cue-level metric is quite strict as it is based on recognized phrases X  X hat is, only cues with perfect boundary matches are true positives. For the sentence-levelevaluationwesimplylabeledthosesentencesasuncertainthatcontained atleastonerecognizedcue. 5.3 Cross-Domain Cue Recognition Model
In order to minimize the development cost of a labeled corpus and an uncertainty detectorforanewgenre/domain,weneedtoinduceanaccuratemodelfromaminimal amount of labeled data, or take advantage of existing corpora for different genres and/ordomainsanduseadomainadaptationapproach.Experimentsinvestigatingthe valueandsufficiencyofexistingcorpora(whichareusuallyout-of-domain)andsimple domainadaptationmethodswerecarriedout.Forthispurpose,weimplementedacue recognitionmodel,whichisdescribedinthissection.
 syntacticfeatures(part-of-speechtagsandchunks)andavoidedtheuseoflexicon-based features listing potential cue words, in order to reduce the domain dependence of the learnedmodels.Nowwewillintroduceourmodel,whichiscompetitivewiththestate-of-the-art systems and focus on its domain adaptability. We will also describe the im-plementationdetailsofthelearningmodelandthefeaturesemployed. Weshouldadd thattheoptimizationofacuedetectorwasnotthemainfocusofourstudy,however. 5.3.1 Feature Set. Weextractedtwotypesoffeaturesforeachtokentodescribethetoken itself, together with its local context in a window of limited size (1, 2, or no window, dependingonthefeature).
 weprovidethelistofthesurfacefeatureswiththecorrespondingwindowsizes: 352 local context. The list of the syntactic features with the corresponding window sizes is thefollowing: 5.3.2 CoNLL-2010 Experiments. TheCoNLL-2010sharedtask Learning to detect hedges and their scope in natural language text focused on uncertainty detection. Two subtasks were definedatthesharedtask:Thefirsttasksoughttorecognizesentencesthatcontainsome uncertain language in two different domains and the second task sought to recognize terms of constituency grammar that covers the part of the sentence that is modified by the cue). The lexical cue recognition subproblem of the second task to the problem setting used in this study, with the only major difference being the typesofuncertaintyaddressed:IntheCoNLL-2010taskbiologicaltextscontainedonly epistemic,doxastic,andinvestigationtypesofuncertainty.Apartfromthesedifferences, the CoNLL-2010 shared task offers an excellent testbed for comparing our uncertainty detectionmodelwithotherstate-of-the-artapproachesforuncertaintydetectionandto comparedifferentclassificationapproaches.Herewepresentourdetailedexperiments usingtheCoNLLdatasets,analyzetheperformanceofourmodels,andselectthemost suitablemodelsforfurtherexperiments.

CoNLL systems. TheuncertaintydetectionsystemsthatweresubmittedtotheCoNLL shared task can be classified into three major types. The first set of systems treats the problem as a sentence classification task, that is, one to decide whether a sentence contains any uncertain element or not. These models operate at the sentence level and are unsuitable for cue detection. The second group handles the problem as a token classification task, and classifies each token independently as uncertain (or not).
Contextual information is only included in the form of feature functions. The third group of systems handled the task as a sequential token labeling problem, that is, de-termined the most likely label sequence of a sentence in one step, taking the informa-tion about neighboring labels into account. Sequence labeling and token classification approaches performed best for biological texts and sentence-level models and token classificationapproachesgavethebestresultsforWikipediatexts(seeTable6inFarkas et al. [2010]). Here we compare a state-of-the-art token classification and sequence labeling approach using a shared feature representation to decide which model to use infurtherexperiments.

Classifier models. We used a first-order linear chain conditional random fields (CRF) model as a sequence labeler and a Maximum Entropy (Maxent) classifier model as a token classifier, implemented in the Mallet (McCallum 2002) package for training the uncertaintycuedetectors.Thischoicewasmotivatedbythefactthatthesewerethemost popular classification approaches among the CoNLL-2010 participants, and that CRF models are known to provide high accuracy for the detection of phrases with accurate boundaries(e.g.,innamedentityrecognition).WetrainedtheCRFandMaxentmodels with their default settings in Mallet for 200 iterations or until convergence (CRF), and alsountilconvergence(Maxent)ineachexperimentalset-up.
 sifies every uni-and bigram as uncertain that is tagged as uncertain in over 50% of the cases in the training data. Hence, it is a similar system to that presented by Tjong KimSang(2010),withouttuningthedecisionthresholdforpredictinguncertainty.
CoNLL results. An overview of the results achieved on the CoNLL-2010 data sets can befoundinTable4.AcomparisonofourmodelswiththeCoNLLsystemsrevealsthat our uncertainty detection model is very competitive when applied on the biological data set. Our CRF model trained on the official training data set of the shared task achievedacue-level F-scoreof81.4andsentence-level F-scoreof87.0onthebiological 354 evaluation data set. These results would have come first in the shared task, with a marginaldifferencecomparedtothetopperformingparticipant.Incontrast,ourmodel is less competitive on the Wikipedia data set: The Maxent model achieved a cue-level
F-score of 22.3 and sentence-level F-score of 58.1 on the Wikipedia evaluation data set, whereas our CRF model was not competitive with the best participating systems.
The observation that sequence-labeling models perform worse than token-based approaches on Wikipedia, especially for sentence-level evaluation measures, coincides with the findings of the shared task: The discourse-level uncertainty cues in the
Wikipedia data set are rather long and heterogeneous and sequence labeling models oftenreverttonotannotatinganytokeninasentencewhenthephraseboundariesare hard to detect. Still, sequence labeling models have an advantage in terms of cue-level accuracy. Thisisnot surprising because CRFisastate-of-the-art model forchunking / sequencelabelingtasks.
 systems for detecting semantic uncertainty (which is closer to the biological subtask), butitislesssuitedtorecognizingdiscourse-leveluncertainty.Inthesubsequentexper-imentsweusedourCRFmodel,whichperformedbestindetectinguncertainty cues in naturallanguagesentences. 5.3.3 Domain Adaptation Model. Insupervisedmachinelearning,thetaskistolearnhow to make predictions on previously unseen, new examples based on a statistical model learned from a collection of labeled training examples (i.e., a set of examples coupled withthedesiredoutputforthem).Theclassificationsettingassumesasetoflabels L ,a setoffeatures X ,andaprobabilitydistribution p ( X )describingtheexamplesintermsof theirfeatures.Thenthetrainingexamplesareassumedtobegivenintheformof pairsandthegoalofclassificationistoestimatethelabeldistribution p ( L beusedlaterontopredictthelabelsforunseenexamples.
 learningtaskhastobesolvedinmultipledomainswhichhavedifferentcharacteristics in terms of their features: The set of features X may be different or the probability dis-tributions p ( X )describingtheinputsmaybedifferent.Whenthetargettasksaretreated asdifferent(butrelated),thelabeldistribution p ( L | X )isdependentonthedomain.That is, given a domain d , the problem can be formalized as modeling p ( L target domain t and a source domain s , with labeled data available for both, and the goal is to induce a more accurate target domain model p ( L source data to acquire an accurate model from just limited target data which are alone insufficienttotrainanaccuratein-domainmodel,andthustoportthemodeltoanew domainwithmoderateannotationcosts.Theproblemisdifficultbecauseitisnontrivial foralearningmethodtoaccountforthedifferentdata(andlabel)distributionsbetween targetandsource,whichcausesaremarkabledropinmodelaccuracywhenitisapplied toclassifyingexamplestakenfromthetargetdomain.
 an adaptation problem as these factors have an impact on both the vocabulary ( p ( X )) confusion in the literature regarding the terminology describing the various domain mismatchesinthelearningproblem.Forexample,Daum  X  eIII(2007)describesadomain adaptationmethodwhereheassumesthatthelabeldistributionisunchanged(wenote herethatthisassumptionisnotexploitedinthemethod,andthatthelabeldistribution changes in our problem), whereas Pan and Yang (2010) uses the term inductive transfer learning to refer to our scenario (in their paper, domain adaptation refers to a different setting). 9 Inthisstudywealwaysusetheterm domain adaptation torefertoourproblem setting,thatis,whereboth p ( X )and p ( L | X )areassumedtochange.
 domains (see Section 5.1.1 for an overview) and applied a simple but effective do-main adaptation model (Daum  X  e III 2007) for training our classifiers. In this model, domain adaptation is carried out by defining each feature over the target and source data sets twice X  X ust once for target domain instances, and once for both the tar-get and source domain instances. Formally, having a target domain t and a source domain s and n features { f 1 , f 2 , ... f n } , for each f and a shared version f i , t + s . Each target domain example is described by 2 n features: { f visedmachinelearningtechniquecanbeusedanditbecomespossibleforthealgorithm tolearntarget-dependentandsharedpatternsatthesametimeandhandlethechanges intheunderlyingdistributions.Thiseasydomainadaptationtechniquehasbeenfound toworkwellinmanyNLP-orientedtasks.WeusedtheCRFmodelsintroducedherein andinthisway,wewereabletoexploitfeature X  X abelcorrespondencesacrossdomains (forfeaturesthatbehaveconsistentlyacrossdomains)andalsotolearnpatternsspecific tothetargetdomain. 5.4 Cross-Domain and Genre Experiments
We defined several settings (target and source pairs) with varied domain and genre distances and target data set sizes. These experiments allowed us to study the po-tential of transferring knowledge across existing corpora for the accurate detection of uncertainlanguageinawidevarietyoftexttypes.Inourexperiments,weusedallthe combinations of genres and domains that we found plausible. News texts (and their subdomains) were not used as source data because FactBank is significantly smaller thantheothercorpora(WikiWeaselorscientifictexts).Asthesourcedatasetistypically largerthanthetargetdatasetinpracticalscenarios,newstextscanonlybeusedastarget data.Abstractswereonlyusedassourcedatabecauseinformationextractiontypically addressesfulltextswhereasabstractsjustprovideannotateddatafordevelopmentpur-poses. Besides these restrictions, we experimented with all possible target and source pairs.
 investigations. In the purely cross-domain (CROSS) setting, the model was trained on the source domain and evaluated on the target (i.e., no labeled target domain data setswereusedfortraining).Inthepurelyin-domainsetting(TARGET),weperformed 356 10-fold cross-validation on the target data (i.e., no source domain data were used). In the two domain adaptation settings, we again performed 10-fold cross-validation on the target data but exploited the source data set (as described in Section 5.3). Here, we eitherusedeachsentenceofthesourcedataset(DA/ALL)oronlythosesentencesthat containedacueobservedinthetargettraindataset(DA/CUE).
 machine learning settings and Table 6 contains the absolute differences between a particularresultandthein-domain(TARGET)results.
 and 8. Table 7 contrasts the coarse-grained F cue with the unlabeled/binary F grained experiments, therefore it quantifies the difference in accuracy due to the more difficult classification setting and the increased sparseness of the task. Table 8 shows the per class F cue scores, namely, how accurately our model recognizes the individual uncertaintytypes.
 investigations. Hence, we performed experiments with different target data set sizes.
We utilized the DA/ALL model (which is more robust for extremely small target data sizes [e.g., 100-400 sentences]) and performed the same 10-fold cross validation on the target data set as in Tables 5-8. For each fold of the cross-validation here, however, we justused n sentences( x axisofthefigures)fromthetargettrainingdatasetandafixed set of 4,000 source sentences to alleviate the effect of varying data set sizes. Figure 5 depictsthelearningcurvesfortwotarget/sourcedatasetpairs. 6. Discussion
As Table 5 shows, incorporating labeled data from different genres and/or domains consistently improves the performance. The successful applicability of domain adap-tation tells us that the problem of detecting uncertainty has similar characteristics across genres and domains. The uncertainty cue lexicons of different domains and 358 genresindeedshareacorevocabularyanddespitethedifferencesinsensedistributions, labeleddatafromadifferentsourceimprovesuncertaintyclassificationinanewgenre and domain if the different data sets are annotated consistently. This justifies our aim to create a consistent representation of uncertainty that can be applied to multiple domains. 6.1 Domain Adaptation Results
The size of the target and source data sets largely influences to what extent external datacanimproveresults.Theonlycasewheredomainadaptationhadonlyanegligible effect (an F-score gain less than 1%) is where the target data set is itself very large.
This is expected as the more target data one has, the less crucial it is to incorporate additional data with some undesirable characteristics (difference in style, domain, certain/uncertainsensedistribution,etc.).
 distanceofthedatasets:Themoredistantthedomainandgenreofthesourceandtarget data sets are, the more the CROSS performance (where no labeled target data is used) degrades, compared with the TARGET model. In general, when the distance between boththedomainandthegenreoftextsissubstantial(++/++and+/+rowsinTables5 and 6), this accounts for a 6 X 10% decrease in both the sentence and cue-level F-scores.
An exception is the case of encyclopedic source and news target domains. Here the performance is very close to the target domain performance. This indicates that these settings are not so different from each other as it might seem at the first glance. The encyclopedicandnewsgenressharequitealotofcommonalities(comparecuedistribu-tionsinFigure4,forinstance).Weverifiedthisobservationbyusingaknowledge-poor quantitativeestimatorofsimilaritybetweendomains(VanAschandDaelemans2010):
Usingcosineasthesimilaritymeasure,thenewswireandencyclopediatextsarefound tobethesecondmostsimilardomainpairinourexperiments,withascorecomparable tothoseobtainedforthepairsofscientificarticletypes bmc , hbc ,and fly . rowsinTables5and6),however,andthedistanceregardingtheotherisjustmoderate, the cross-training performance is close to or even better than the target-only results.
That is, thelarger amount of source training data balances thedifferences between the domains. These results indicate that the learned uncertainty classifiers can be directly appliedto slightly differentdatasets.Thissuitabilityisduetothelearneddisambigua-tion models, which generalize well in similar settings. This is contrary to the findings ofearlierstudies,whichbuilttheuncertaintydetectorsusingseedexamplesandboot-strapping. These models were not designed to learn any disambiguation models for the cue words found, and their performance degraded even for slightly different data (Szarvas2008).
 tation via transferring only source sentences that contain a target domain cue is, on average,comparabletotransferringallthedatafromthesourcedomain.Inotherwords, when we have a small but sufficient amount of target data available, it is enough to account for source data corresponding to the uncertainty cues we saw in the limited targetdataset.Thisobservationhasseveralconsequences,namely: fine-grained distinction results in only a small, or no, loss in performance. The coarse-grainedmodelisslightlymoreaccuratethanthefine-grainedmodel(countingcorrectly 360 recognized but misclassified cues as true positives) in most settings. The most signif-icant difference is observed for the target-only settings, where no out-of-domain data are used for the training and thus the data sets are accordingly smaller. A noticeable exception is when scientific abstracts are used for cross training: In those settings the coarse-grained model performs poorly, due to its lower recall, which we attribute to overfitting the special characteristics of abstracts. The fact that in fine-grained classi-fication the CROSS results consistently outperform the TARGET models (see Table 8) evenfordistantdomainpairs,alsounderlines thattheincreasedsparseness causedby thedifferentiationofthevarioussubtypesofuncertaintyisanimportantfactoronlyfor smaller data sets. The improvement by domain adaptation is clearly more prominent infine-grainedthanincoarse-grainedclassification,however:Theindividualcuetypes benefit by 5 X 10% points in terms of the F-score from out-of-domain data and domain adaptation. Moreover, as Table 8 shows, for the domain pairs and fine-grained classes where a nice amount of positive examples are at hand, the per class F also around 80% and above. This means that it is possible to accurately identify the individual subtypes of semantic uncertainty, and thus it also proves the feasibility of thesubcategorizationandannotationschemeproposedinthisstudy(Section2).Other importantobservationsherearethatdomainadaptationisevenmoresignificantinthe moredifficultfine-grainedclassificationsetting,andthattheconditionclassrepresents a challenge for our model. The performance for the condition class is lower than that for the other classes, which can only in part be attributed to the fact that this is the leastrepresentedsubtypeinourdatasets:asopposedtoothercuetypes,conditioncues aretypicallyusedinmanydifferentcontextsandtheymaybelongtootheruncertainty classesaswell. 6.2 The Required Amount of Annotation
Based on our experiments, we may conclude that a manually annotated training data setconsistingof3,000 X 5,000sentencesissufficientfortraininganaccuratecuedetector for a new genre/domain. The results of our learning curve experiments (Figure 5) illustratethesituationswhereonlyalimitedamountofannotateddata(fewerthan3,000 sentences) is available for the target domain. The feasibility of decreasing annotation effortsandtherealaddedvalueofdomainadaptationaremoreprominentinthisrange. ItiseasytoseethattheTARGETresultsapproachtoDAresultswithmoretargetdata.
TARGET setting outperforms the CROSS model (trained on 4,000 source sentences) is around1,000sentences.Aswementionedearlier,evendistantdomaindatacanimprove thecuerecognitionmodelintheabsenceofasufficienttargetdataset.Figure5justifies this observation, as the CROSS and DA settings outperform the TARGET setting on each source X  X arget data set pair. It can also be observed that the doxastic type is more domain-dependent than the others and its results consistently improve by increasing the size of the target domain annotation (which coincides with the cue frequency investigations of Section 5.1.3). In the news target domain, however, the investigation andepistemicclassesbenefitalotfromasmallamountofannotatedtargetdatabuttheir performance scores increase just slightly after that. This indicates that most of the im-portant domain-dependent (probably lexical) knowledge could be gathered from 100 X  400 sentences. In the biological experiments, we may conclude that the investigation classisalreadycoveredbythesourcedomain(intuitively,theinvestigationcuesarewell representedintheabstracts)anditsresultsarenotimprovedsignificantlybyusingmore target data. The condition class is underrepresented in both the source and target data setsandhencenoreliableobservationscanbemaderegardingthissubclass(seeTable2). genre/domain:(i)Wecanachieveperformancearound60 X 70%byusingcrosstraining dependingonthedifferencebetweenthedomains(i.e.,withoutanyannotationeffort); (ii) By annotating around 3,000 sentences, we can have a performance of 70 X 80%, dependingonthelevelofdifficultyofthetexts;(iii)Wecangetthesame70 X 80%results withannotatingjust1,000sentencesandusingdomainadaptation. 6.3 Interesting Examples and Error Analysis
As might be expected, most of the erroneous cue predictions were due to vocabulary differences, for example, fear or accuse occurred only in news texts, which is why they were not recognized by models trained on biological or encyclopedia texts. Another example is the case of or , which is a frequent cue in biological texts. Still, it is rarely used as a cue in other domains but without domain adaptation, the model trained on biological texts marks quite a few occurrences of or as cues in the news or encyclope-dia domains. Many of these anomalies were eliminated by the application of domain adaptationtechniques,however.
 disambiguate because not only can they refer to several classes of uncertainty, but they typically have non-cue usage as well. For instance, the case of would is rather complicatedbecauseitcanfulfillseveralfunctions: (10) E PISTEMIC USAGE ( X  IT IS HIGHLY PROBABLE  X ):Furtherbiochemicalstudies (11) C ONDITIONAL : X  X freligionwasathingthatmoneycouldbuy,/Therich (12) F UTURE IN THE PAST :ThisAarupcantraceitshistorybackto1500,butit (13) R EPEATED ACTION IN THE PAST ( X  USED TO  X ): X  X ecker X  X asthenextT.V. (14) D YNAMIC MODALITY :Individuals would firsthaveasmalllesionatthe (15) P RAGMATIC USAGE :Althoughsome would disputethefact,thejoke
Theepistemicusesof would areannotatedasepistemiccueswhereasitsoccurrencesin conditionalsaremarkedashypotheticalcues.Thehabitualpastmeaningisnotrelated touncertainty,henceitisnotannotated.Thefutureinthepastmeaning(i.e.,pasttense of will ), however, denotes an event of which it is known that happened later, so it is certain.Thedynamicallymodal would issimilartothefuture will (whichisaninstance of dynamic modality as well), but it is not annotated in the corpora. The pragmatic 362 useof would doesnotrefertosemanticuncertainty(thesemanticvalueofthesentence is, some will/may/might/  X  dispute the fact mean the same). It is rather a stylistic issue to further express uncertainty at the discourse level (i.e., there are some unidentified people who dispute the fact, hence the opinion cannot be associated with any definite source).
 and seem to be characteristic primarily of the news and encyclopedia domains. Thus it is advisable to explore such cases and treat them with special consideration when adaptinganalgorithmtrainedandtestedinaspecificdomaintoanotherdomain.
 frequent cues in each subcorpus, its non-cue usage is rather limited but can be found occasionally in FactBank and WikiWeasel. The following instance of may in FactBank wascorrectlymarkedasnon-cuebythecuedetectorwhentrainedonWikipediatexts.
Ontheotherhand,itwasmarkedasacuewhentrainedonbiologicaltextssinceinthis case,therewereinsufficienttrainingexamplesof may notbeingacue:
Among these examples, only the second one should be annotated as uncertain. POS-tagging seems to provide enough information for excluding the verbal and preposi-tionalusesofthewordbutinthecaseofnominalusage,additionalinformationisalso required to enable the system to decide whether it is an uncertainty cue or not (in this case, the noun in the  X  X ompany X  sense cannot have an argument while in the  X  X orry X  sense, it can have [ about declines ]). Again, the frequency of the two senses depends heavily on the domain of the texts, which should also be considered when adapting thecuedetectortoadifferentdomain.WeshouldmentionthattheroleofPOS-tagging is essential in cue detection because many ambiguities can be resolved on the basis of POS-tags.Hence,POS-taggingerrorscanleadtoaseriousdeclineinperformance. tionandcuedetectionacrossgenresanddomains. 7. Conclusions and Future Work
Inthisarticle,weintroducedanuncertaintycuedetectionmodelthatcanperformwell across different domains and genres. Even though several types of uncertainty exist, available corpora and resources focus only on some of the possible types and thereby only cover particular aspects of the phenomenon. This means that uncertainty models found in the literature are heterogeneous, and the results of experiments on different corpora are hardly comparable. These facts motivated us to offer a unified model of semantic uncertainty enhanced by linguistic and computer science considerations. In accordancewiththisclassification,wereannotatedthreecorporafromseveraldomains andgenresusingouruniformannotationguidelines.
 reasonableperformance (60 X 70%cue-levelF-score)whennoannotateddataisathand foranewdomain.Whensomeannotateddataisavailable(here some meansfewerthan 3,000 annotated sentences for the target domain), domain adaptation techniques are the best choice: (i) they lead to a significant improvement compared to simple cross training, and (ii) they can provide a reasonable performance with significantly less annotation.Inourexperiments,theannotationof3,000sentencesandtrainingonlyon theseisroughlyequivalenttotheannotationof1,000sentencesusingexternaldataand domain adaptation. If the size of the training data set is sufficiently large (larger than 5,000 sentences) the effect of incorporating additional data X  X aving some undesirable characteristics X  X snotcrucial.
 could be attained when the source domain was filtered for sentences that contained cues in the target domain. This tells us that models learn to better disambiguate the cues seen in the target domain instead of finding new, unseen cues. In this sense, this approachcanberegardedasacomplementarymethodtoweaklysupervisedtechniques for lexicon extraction. A promising way to further minimize annotation costs while maximizing performance would be the integration of the two approaches, which we plantoinvestigateinthenearfuture.
 notatedresources),butthedetectionofsuchphenomenaisalsodesirable.Forinstance, dynamically modal events cannot be treated as certain X  X hat is, the event of buying cannot be assigned the same truth value in They agreed to buy the company and They bought the company .Whereasthesecondsentenceexpressesafact,thefirstoneinforms us about the intention of buying the company, which will be certainly carried out in a worldwheremoralorbusinesslawsareobservedbutatthemomentitcannotbestated whetherthetransactiontakesplace(i.e.,thatitiscertain).Hence,inthefuture,wealso intend to integrate the identification of dynamically modal cues into our uncertainty cuedetector.
 Acknowledgments 364 366
