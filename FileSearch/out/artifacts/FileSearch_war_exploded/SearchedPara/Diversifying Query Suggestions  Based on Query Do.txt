 Many domain-specific search tasks are initiated by document-related to a new (query) patent. We call this type of search Query Document Search . In this type of search, the initial query docu-ment is typically long and contains diverse aspects (or sub-topics). Users tend to issue many queries based on the initial document to retrieve relevant documents. To he lp users in this situation, we propose a method to suggest divers e queries that can cover multi-ple aspects of the query document. We first identify multiple que-ry aspects and then provide diverse query suggestions that are effective for retrieving relevant do cuments as well being related to more query aspects. In the experi ments, we demonstrate that our approach is effective in compar ison to previous query suggestion methods. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  Query Formulation, Search Process.
 Diversifying query suggestions; Patent retrieval; Literature search Many domain-specific search tasks can start from document-length initial queries. For example, prior-art search aims to find past relevant patents which may conflict with new patents [7][11]; in academic literature search, academic authors need to find rele-vant papers that should be cite d in their writings. One unique characteristic of these search tasks is more emphasis on recall, i.e., not missing relevant do cuments is more important than placing a relevant document at the top rank. In this paper, we call this type of domain-specific search task Query Document Search (QDS). Note that we use the term  X  X uery document X  to refer to the docu-ment-length initial query in domain-specific searches. Query suggestion (e.g., [12]) can be particularly helpful for QDS. For example, patent examiners use about 15 queries to validate a new patent [11]. In addition, patent engineers have stated that automatic suggestion of search vo cabulary is required for patent search systems [1]. Although a number of existing methods (e.g., [2][13]) can be used, these techniques need improvement for QDS and do not consider diversity. In this paper, to improve query suggestions for QDS, we introduce the concept of diversifying query suggestions based on query documents. Emphasizing diverse que ry suggestions is important because otherwise the system may suggest multiple similar que-ries which would produce near-dup licate search results. In addi-tion, diversified suggestions can help to retrieve more relevant documents related to a query document. Typically a query docu-ment can be quite long (e.g., a patent document can contain thou-sands of terms) and would include several aspects (or sub-topics). So, many relevant documents are re lated to these different aspects, and suggesting queries related to multiple aspects can be effective for retrieving more relevant documents. As an example, Figure 1 shows an example query document. This query document is a United States patent, published in 2002, which describes Infor-mation Retrieval (IR) systems using multiple databases. The pa-tent application mentions several components (or aspects) such as  X  X uery specification X ,  X  X uery execu tion X ,  X  X uery resu lt retrieval X , etc., and the queries suggested for this patent would be more ef-fective if they can cover such query aspects. In fact, many relevant the relevant documents for the query document in Figure 1. In this example, A and B are related to the aspect  X  X uery specification X , whereas C refers to  X  X uery execution X . In addition, D describes report systems, which forms anothe r aspect (i.e.,  X  X eport form X ). Motivated by these types of examples, we propose a method to suggest diverse queries based on query documents. To solve this, we adopt a three-step process: (Step 1) Query Aspect Identifica-tion , (Step 2) Query Generation , and (Step 3) Diversifying Query Suggestions . Given an initial query doc ument, we extract diverse query aspects by defining a  X  X ue ry aspect X  as a set of related terms from the query document and use term clustering algorithms are identified, we generate multiple queries relevant to the identi-fied aspects, and suggest the top k ranked queries. Our experi-ments show that diversified sugg estions are effective for retriev-ing more relevant documents in comparison to existing suggestion methods. The first step is identifying n query aspects by representing a que-ry aspect as a set of related terms from the query document. We address this by using term clustering methods because they can provide query-specific term analysis results (cf. global term analy-sis such as LSA [6] may not provide query-specific results). Spe-cifically, for a query document, we extract m distinct terms using their tfidf weights (stop-words are ignored), and generate  X  X   X   X 1 X   X  2  X  term pairs (the similarity is undirected). By estimating the similarity for each term pair  X   X   X   X ,  X   X  , we can generate a m -by-m symmetric similarity matrix whose diagonal value is 1. Then, we apply a term clustering algorithm us ing this matrix for generating n different term sets. In this paper, we extract 500 terms from each query document, and use a spectral clustering algorithm. Next, we describe how to estimate the similarity for  X   X   X   X ,  X   X  . We define similarity between term s by a mixture of topical relat-edness (or association) and retrieval effectiveness when terms are clustered together. In other word s, we make clustering algorithms group the terms if they are topically associated and are also effec-tive for retrieving relevant documents. To achieve this, we intro-duce the similarity function. where  X   X  and  X   X  is a term pair from a query document. In Eq. (1),  X  X  X   X   X ,  X   X  measures topical relatedness between  X   X  ling parameter. For  X  , we utilize term statistics obtained from the document corpus (e.g., Point-wise Mutual Information (PMI)). To estimate  X  , we leverage the features from query performance predictors (e.g., query clarity [5], query scope [9], etc.). Using the features listed in Tabl e 2, we can rewrite Eq. (1) as: where  X   X  indicates a feature defined in Table 2 and  X  of the k -th feature. To predict more accurate similarity, we employ a supervised learning appr oach. Given a term pair  X   X   X  vised learner estimates its similarity score by learning an optimal value of the feature weights (  X  X   X   X   X   X ,...,  X   X  ). We now generate training exampl es as follows. For each query document, N different term pairs are extracted, and we label each positive if its terms are highly associated and effective for retriev-ing relevant documents; otherwise, the term pair is negative. To determine this, we use the following conditions, and an example is positive if it satisfies every condition; otherwise the example is negative. i) Two terms involve high  X  X etrieval effectiveness X  if they have ii) Two terms are highly  X  X ssociated X  if their PMI estimated from For each relevant document, we generate a unigram language model and assume that the top 100 terms ranked by the language model satisfy the first criteria. For the second constraint, we as-sume that PMI estimated from a relevant document indicates topi-cal association effective for retrieving relevant documents. In this step, based on n identified query aspects, we generate que-ries by exploiting the query generation method proposed in [13]. For each query aspect (i.e., a set of terms), we first retrieve pseu-do-relevant documents ( PRD ) obtained by the terms in the aspect; we use those terms as a query and assume that top k retrieved documents are pseudo-relevant. In addition, we generate an equal number of non-relevant documents ( NRD ) by randomly selecting another k documents from those ranked below the top k. Then, we train binary decision trees using PRD and NRD where the terms in PRD are used as attributes. Once a decision tree is learned, we generate a query by extracting at tributes (terms) on a single path from the root to a positive leaf node (i.e., pseudo-relevance). We define a query as a list of keywor ds (e.g.,{battery, charger, cellu-lar, phone}), and ignore the attri butes associated with negation. See [13] for more details. We define diversifying query suggestions as suggesting k queries that will be effective for findin g relevant and novel documents for a query document. To do this, we exploit the xQuAD diversifica-tion model proposed in [14] and introduce the following probabil-istic query suggestion framework. In this approach, among all generated queries, we select the queries that are more relevant to the query document and novel rela tive to the current suggestion list. Figure 2 describes this framework. iteratively choose the most probable query obtained by: candidate query from  X  . In Eq. (3),  X  X  X P X   X   X  denotes the relevance of  X  to  X  probabilities are optimizing relevance and diversity , controlled by  X  .  X  X  X P X   X   X  can be computed by  X  P LM  X  X  X  X   X   X   X  X  X  gram language mode l estimated from  X   X  , and  X , X P X  estimated using the identified query aspects. By the set of query aspects  X   X  we can marginalize  X , X P X  where  X  X  is a query aspect in  X   X  . Category Features Topical Relatedness Retrieval Effectiveness In Eq. (4), we consider  X  X  X  X P X   X   X  as an importance of an aspect  X  X  for  X   X  , which is estimated by  X  P LM  X  X  X  X   X   X   X  X  X  X  ALGORITHM Diversifying Query Suggestions (DivQS)
INPUT : L (a list of generated queries), k (the number of que-ries to be suggested),  X   X  (query document) OUTPUT :  X  (a list of query suggestions) 
PROCESS : 1:  X   X  X  2: While |  X  |  X  X  do 3:  X   X   X argmax  X  X  X  X  X  X   X   X 1 X   X   X  X  X P X  X   X   X , X P X  X  X  X  X   X   X  X   X   X  4:  X  X  X  X   X   X   X   X  5:  X  X  X  X   X   X   X   X  6: End While 7: Return  X  Figure 2: A framework of Diversifying Query Suggestions. By assuming that the current candidate query  X  is independent of the queries already selected in  X  , P  X   X , X   X  |  X  X   X  can be derived as: retrieval results obtained by  X  ,  X  , and  X  X  . Specifically, we assume that a query X  X  top 100 retrieved documents can represent underly-much of topics in  X  X  are covered by  X  . The equation is given as: following estimation can be given. where  X  X  is a query in  X  and P  X   X  X  |  X  X   X   X  X  X  X   X  X   X  X  Using the above estim ations, we select k queries as suggestions for each query document. We conduct experiments on two domains: the patent and academ-ic domains. For the patent domain, we use the patent corpus pro-vided by [7]. To develop query documents (new patents), we ran-domly selected 102 more recent patents, and consider patents cited in each query patent as  X  X elevant X . For the academic domain, we use the ACL anthology reference corpus [3], and randomly select 150 more recent query docu ments (papers). We regard the articles cited in each query paper as  X  X elevant X . For all query doc-uments, references are hidden, a nd the sentences containing cita-tions are removed. Queries and documents are stemmed by the Krovetz stemmer. To identify quer y aspects and generate diverse suggestions, we perform 5-fold cross-validation with random partitioning. For each query sugg estion, we use the query likeli-hood model implemented by Indri [17]. We assume that the searchers only examine the top 100 of every query result since 100 patents are examined on average [11]. ( Baselines ) For each query document, we generate an initial base-line query (BL0) by the query generation method described in [8], used for evaluating query aspect identification. To evaluate di-verse suggestion results , we employ two different baselines. The first baseline (BL1) is implemen ted by the method in [2] which can suggest relevant n-grams wit hout using query logs. We modi-all n-grams of order 1, 2, 3, 4, and 5 from pseudo-relevant docu-ments obtained by the BL0, rank them by the correlation between candidate n-grams and the terms in the query document, and sug-gest the top k ranked n-grams. The other baseline (BL2) is a query suggestion method proposed in [13]. We generate keyword que-ries by ignoring the terms associated w ith negation. ( Evaluation Measures ) Although there has been considerable research on measuring diversity for search results (e.g., [4]), these previous measures are not appropriate for our search environ-ments; [4] only evaluates the retr ieval results for a single query but we suggest multiple queries for a query document and some multi-query session-based metric is required; in addition, there was no emphasis on recall in session search results. Thus, to eval-uate  X  X iversity X  in multi-query sessions, we create the Session Novelty Recall measure. Session Novelty Recall ( SNR ) is a recall-based metric for multi-query sessions. First, given multip le retrieval results, we ignore relevant documents already foun d by previous suggestions. Sec-ond, following the idea in [10], we discount the documents re-trieved by later suggestions. The computation is given as follows. First, we construct a rank list, L , by concatenating the top 100 documents from each ranked list in a session. Next, in the list, we discard any retrieved documents which are retrieved by any previ-sults. In addition, each retrieved result is labeled by the query which first retrieved it. where  X   X   X  is the document placed at the i -th rank in L and retrieved ments, k is # of queries that the user examines where  X 1 X  , query retrieved every relevant do cument, the value is maximized. In addition, we employ normalized Session DCG ( nSDCG ) [10] to measure retrieval effectiveness of the top k suggested queries. ( Query Aspect Identification Performance ) In this experiment, we hypothesize that more relevant documents are retrieved if the identified query aspect is effective. We measure the retrieval ef-fectiveness of each query aspect by formulating a query using the terms in each query aspect. Table 3 shows the retrieval results of query aspects and baseline. For each query document, 10 query aspects are identified and a single baseline query is used. We measure recall ( R@100 ) in two different ways: (1) selecting the best one among n different query aspects ( Max R@100 ) and (2) aggregating the retrieved relevant documents (within rank 100) by all query aspects ( Agg. R@100 ). We report an average value of each metric over the query documents in each corpus. Table 3: Query Aspect Evaluation.  X  X A X  is our query aspect identification method (using 10 aspects). A * denotes a signifi-cant improvement over  X  X L0 X  (the paired t-test with p &lt; 0.05). 
Metric \ Method R100 0.1091 -0.4452 -Max. R100 -0.1491 * -0.4695 *
Agg. R100 -0.1918 * -0.6369 * First, regarding Max R@100 , our method can generate at least one query aspect which can signif icantly outperform the baseline. Second, from Agg. R@100 we see that significantly more relevant Table 4: Session evaluation using 5 and 10 suggestions. #Q is # of suggested queries. In each row, a significant improvement over each baseline is marked by its number, e.g., improvement over  X  X L 1&amp;2 X  (the paired t-test with p &lt; 0.05). 
Metric #Q BL1 BL2 DivQS 
SN R @100 nSDCG @100
SN R @100 nSDCG @100 documents are retrieved when using all identified aspects. This is a useful result because query aspects can find relevant documents that are missed by BL0 and the query suggestions generated by these aspects should also perform well. ( Diverse Query Suggestion Performance ) We now evaluate diverse query suggestion results in terms of retrieval effectiveness and diversity. For each query document, we suggest 5 and 10 queries by identifying 10 or 20 different query aspects in each query document (i.e., n = 10 or 20). The baselines (BL1&amp;2) gen-erate the same number of query suggestions for the same query document. Table 4 reports retrieva l performance of each method. First, in both domains, BL2 can outperform BL1 in terms of SNR . Second, the queries suggested by our method (DivQS) can pro-vide significantly more diversified results and retrieve more rele-vant documents. SNR verifies that our method is more effective at finding new relevant documents mi ssed by previous queries (since SNR ignores the relevant documents retrieved by any previous queries). Third, considering nSDCG , our method is significantly better at placing relevant docume nts at higher ranks. This is be-cause the queries generated by ou r method contai n more discrimi-native terms from relevant documents. In this paper, we are interested in the diversity between query-suggestion pairs, which has been studied in recent work (e.g., [15][16]). Song et al. [16] selected query candidates from query logs by ranking them in the orde r which maximizes the similarity and diversity between the queries. Sa ntos et al. [15] used the relat-ed queries, from query logs, which contain common clicks or common sessions for diversifying suggestions. However, these methods cannot be used for our task because they are based on proprietary training data (to learn ranking functions) and query logs (to generate suggestions), which are not available. Instead, the query suggestion methods proposed in [2][13] are more easily applied in QDS environments but do not consider diversity. In this paper, we proposed a framework for diversifying query suggestions to help domain-speci fic searchers. We identify di-verse query aspects, generate many queries related to these, and suggest diverse queries based on the identified aspects. Through experiments, we showed that th e suggestions generated by our system produce more diverse and effective search results in com-parison to baseline methods. Our method is easily reproducible and general; we do not require any manually constructed data or external resources, and effectiven ess was verified in two different domains. For future work, we plan to conduct experiments in the legal domain (e.g., findi ng relevant cases). This work was supported in part by the Center for Intelligent In-formation Retrieval. Any opinions, findings and conclusions or recommendations expressed in this material are those of the au-thors and do not necessarily reflect those of the sponsor. [1] Azzopardi, L., Vanderbauwhede, W., and Joho, H. (2010). [2] Bhatia, S., Majumdar, D., and Mitra P. (2011). Query sugges-[3] Bird, S., Dale, R., Dorr, B., Gibson, B., Joseph, M., Kan, M.-[4] Clarke, C. L. A., Kolla, M., Cormack, G. V., Vechtomova, O., [5] Cronen-Townsend, S., Zhou, Y., and Croft, W. B. (2002). [6] Dumais, S. T. (2005). Latent semantic analysis. Annual Review [7] Fujii, A., Iwayama, M., and Kan do, N. (2007). Overview of [8] Ganguly, D., Leveling, J., Magdy, W., and Jones, G. J. F. [9] He, B., and Ounis, I. (2004). Inferring query performance us-[10] Jarvelin, K., Price, S. L., Delcmbre, L. M. L., and Nielsen, M. [11] Joho, H., Azzopardi, L., and Vanderbauwhede, W. (2010). A [12] Jones, R., Rey, B., Madani, O., and Greiner, W. (2006). Gen-[13] Kim, Y., Seo, J., and Croft, W. B. (2011). Automatic Boolean [14] Santos, R. L. T., Macdonald, C., and Ounis, I. (2010). Exploit-[15] Santos, R. L. T., Macdonald, C., and Ounis, I. (2012). Learn-[16] Son g, Y., Zhou, D., and He, L-w. (2011). Post-Ranking Query [17] Strohman, T., Metzler, D., Turtle, H., and Croft, W. B. (2005). 
