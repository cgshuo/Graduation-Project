 Relevance Feedback has proven very effective for improv-ing retrieval accuracy. A difficult yet important problem in all relevance feedback methods is how to optimally balance the original query and feedback information. In the current feedback methods, the balance parameter is usually set to a fixed value across all the queries and collections. However, due to the difference in queries and feedback documents, this balance parameter should be optimized for each query and each set of feedback documents.

In this paper, we present a learning approach to adap-tively predict the optimal balance coefficient for each query and each collection. We propose three heuristics to charac-terize the balance between query and feedback information. Taking these three heuristics as a road map, we explore a number of features and combine them using a regression ap-proach to predict the balance coefficient. Our experiments show that the proposed adaptive relevance feedback is more robust and effective than the regular fixed-coefficient feed-back.
 H.3.3 [ Information Search and Retrieval ]: Relevance feedback, retrieval models Algorithms Adaptive relevance feedback, relevance feedback, learning, prediction, language models
Relevance Feedback has proven very effective for improv-ing retrieval accuracy [27, 25, 28, 21, 32]. Relevance feed-back refers to an interactive process that helps to improve the retrieval performance: when a user submits a query, an information retrieval system would first return an initial set of result documents and then ask the user to judge whether some documents are relevant or not; after that, the system would reformulate the query based on the user X  X  judgments, and return a set of new results. Imagine that in a com-prehensive search task [2], the aim of the user is to find as many documents as possible on a given topic; thus both re-call and precision should be as high as possible. Relevance feedback, in this case, is a natural way to save users from reformulating queries in a trial-and-error manner.
Relevance feedback has been extensively studied, and most of the methods take it as a supervised learning problem with a special treatment of query [27, 25, 28, 32]. How to handle the relation between the query and feedback documents has been a difficult but important problem: we need to care-fully balance the original query and feedback information because if we over-trust the feedback information, we may be biased to favor a particular subset of relevant documents, but under-trusting it would not take advantage of feedback. This problem is especially critical when we have only a few feedback documents, because the over-fitting would cause more damage. In existing feedback methods, the balance is usually controlled by some parameter, which is often set to a fixed value across all the queries and collections. However, due to the variations of queries and feedback documents, this balance parameter presumably should be optimized for each query and each set of feedback documents.

As far as we know, how to optimize the balance of the query and feedback information has not been well studied in previous work. In this paper, we study this novel problem and propose an adaptive relevance feedback method to dy-namically predict an optimal balance coefficient using ma-chine learning. Specifically, we estimate a potentially dif-ferent feedback coefficient for each query and each set of feedback documents, rather than manually set it to a fixed constant. We hypothesize that the proposed method will do better than the current fixed-coefficient approaches.
Besides, we propose three heuristics to characterize feed-back coefficients: (1) discrimination of query: we expect that the more discriminative the query is, the more drifting-tolerant it could be, and thus it would be safe to utilize more feedback information. (2) discrimination of feedback documents: we hypothesize that clearer feedback documents could be trusted more. (3) divergence between query and feedback documents: if the divergence between a query and its feedback documents is large, it possibly means that the query does not represent relevant documents well, thus we may need a larger feedback coefficient. Following these three heuristics, we explored a number of features and combined them using the logistic regression model to predict the feed-back coefficient.

We evaluate our algorithm on two representative TREC data sets. The experimental results demonstrate that our proposed adaptive relevance feedback has shown clear im-provements over a robust baseline system with a well-tuned feedback coefficient, especially when the training data is noisy. It indicates that, the regular fixed-coefficient feedback only works when the training and testing sets are very sim-ilar, while the adaptive relevance feedback method is more noise-tolerant and can adapt to the characteristics of the documents and query effectively.

Our work makes the following contributions: (1) we pro-pose an adaptive relevance feedback algorithm to dynami-cally handle the balance between query and feedback docu-ments in relevance feedback. (2) we propose three heuristics to characterize the balance between original query and feed-back information, based on which, a number of features are explored to dynamically predict the feedback coefficients.
Relevance feedback has been shown to be effective with different kinds of retrieval models [27, 25, 28, 21, 32, 3]. In the vector space model, feedback is usually done by using the Rocchio algorithm, which forms a new query vector by maximizing its similarity to relevant documents and min-imizing its similarity to non-relevant documents [27]. The feedback method in classical probabilistic models is to select expanded terms primarily based on Robertson/Sparck-Jones weight [25]. In the language modeling approaches, relevance feedback can be implemented through estimating a query language model [32, 29] or relevance model [21] through ex-ploiting a set of feedback documents. Recently, a relevance feedback track was organized by TREC to evaluate and com-pare different relevance feedback algorithms [3].
When there are no  X  X eal X  relevance judgments available, alternatively, pseudo relevance feedback [4, 26] may be per-formed, which simply assumes that a small number of top-ranked documents in the initial retrieval results are relevant and then applies relevance feedback. Besides, somewhere in between relevance feedback and pseudo relevance feedback is implicit feedback [19], in which a user X  X  actions in inter-acting with a system are used to infer the user X  X  information need.

All the above work differs from our study in that they used a fixed parameter to control the balance between orig-inal query and feedback information, which often has to be manually set for different queries and feedback documents. Although in Tao and Zhai X  X  work [29], a regularized EM algorithm was proposed to eliminate this balance parame-ter, it turns out that the weight of the original query and the weight of feedback documents are roughly the same (i.e., the balance parameter is fixed to 0 . 5) when the algorithm stops. Our study, however, proposes an adaptive relevance feed-back approach to dynamically predict a potentially different parameter for different query and feedback documents.
Logistic regression [15] is widely used in data mining and machine learning. In IR, it has been used to learn a retrieval function that can be used to rank documents directly [13, 10, 14, 8, 34, 30]. A main difference of our work from these studies and recent work on learning to rank (e.g.,[18, 5, 6]) is that we leverage state-of-the-art language models for rank-ing documents and use logistic regression to optimize an important parameter inside the language modeling frame-work. Since current work on learning to rank has mostly relied on the traditional content-based retrieval models to compute features (e.g., BM25 scores, language model scores) and learn optimal ways to combine these features, our work can also be regarded a novel use of learning techniques to im-prove these features by going  X  X nside X  a traditional retrieval model instead of treating such a model as a black box.
Our work uses some features similar to the measures pro-posed in [11, 31, 7] to characterize queries, documents, as well as their relations (e.g., similarity, distance, and clar-ity). All these studies used these features to predict the query performance or query difficulty. But our work is to predict the balance between query and feedback information and optimize the performance of relevance feedback.
Selective query expansion is closely related to our work, where the idea is to disable query expansion if query expan-sion is predicted to be detrimental to retrieval [1, 12, 31, 17]. However, existing studies on selective query expansion only studied whether to do query expansion or not, while we focus on how much weight we can put on expansion informa-tion. Presumably, selective query expansion would not work for explicit relevance feedback, since almost all the queries benefit from feedback information.
Given a query Q and a document collection C , a retrieval system returns a ranked list of documents L . Let L i denote the i -th ranked document in the ranked list.

We assume that a user would be willing to give explicit feedback information. Our goal is to study how to use these feedback judgments, J  X  X  L 1 , ..., L k } , to effectively re-rank the next r unseen documents: U = { L k +1 , ..., L k + r } to bring more relevant and novel information to the user.

For any relevance feedback model M , we assume that it has a parameter  X  to control the balance of query Q and feedback documents J . A general formula of feedback is: where  X   X  [0 , 1] is the feedback coefficient, and g 1 and g two functions that map a query and a set of relevant docu-ments, respectively, into some comparable representations. For example, in the vector space model, queries and docu-ments are represented as vectors of weighted terms, while in language modeling approaches, they are represented as language models.

Our goal is to optimize the feedback coefficient  X  for differ-ent queries and feedback documents. Formally, we assume there is a function B , that can map a query Q and the cor-responding feedback documents J to the optimal feedback coefficient (i.e.,  X  = B ( Q, J )), and hope to learn such a function B using past queries as training data.

In this study, we explore the problem of adaptive relevance feedback in the popular language modeling framework, par-ticularly the KL-divergence retrieval model and mixture-model feedback method, mainly because language models deliver state of the art retrieval performance [24, 20, 33] and the mixture-model based feedback is one of the most ef-fective feedback techniques which outperforms Rocchio feed-back [32]. However, our methodology could be applicable to other retrieval and feedback models as well, which we leave as future work.
We now review our baseline language modeling frame-work and discuss how we incorporate our feedback prediction function into this framework.
The KL-divergence retrieval model [20] is a generalization of the query likelihood retrieval method [24] and can support feedback more naturally than the query likelihood method. In this model, queries and documents are all represented by unigram language models, which are essentially word distri-butions. Assuming that these language models can be ap-propriately estimated, KL-divergence retrieval model scores a document D with respect to a query Q by computing the negative Kullback-Leibler divergence between the query lan-guage model  X  Q and the document language model  X  D : where V is the set of words in our vocabulary. Clearly, the retrieval performance of the KL-divergence would depend on how we estimate the document model  X  D and the query model  X  Q . The document model  X  D needs to be smoothed and an effective method is Dirichlet smoothing [33].
The query model intuitively captures what the user is in-terested in, and thus would affect retrieval accuracy signifi-cantly. Without feedback,  X  Q is often estimated as p ( w |  X  the query Q , and | Q | is the total number of words in Q .
The query model described above, however, is not very discriminative because a query is typically extremely short. Several different methods have been proposed to improve the estimation of  X  Q by exploiting documents, especially those documents that are used for relevance feedback or pseudo-relevance feedback [20, 21, 32, 29]. In [32], it was proposed that feedback can be implemented in the KL-divergence re-trieval model as updating the query model based on the feed-back documents. Specifically, we can define a two-component mixture model (i.e., a fixed background language model p ( w |C ) estimated using the whole collection and an unknown topic language model to be estimated) and assume that the feed-back documents are generated using such a mixture model. Formally, let  X  T be the unknown topic language model and F  X  X  be a set of feedback documents (in explicit relevance feedback, F is comprised of documents that are judged rel-evant by the user.). The log-likelihood function of the mix-ture model is: L ( F |  X  T ) = where  X   X  [0 , 1) is a mixture noise parameter which con-trols the weight of the background model. Given a fixed  X  , a standard EM algorithm can then be used to estimate p ( w |  X  T ), which is then interpolated with the original query model p ( w | Q ) to obtain an improved estimation of the query model: where  X  is the feedback coefficient to be set manually. As in other existing feedback methods [27, 25, 28], the parameter  X  is generally fixed across all queries and documents. This model has been shown to perform quite well [32, 23] .
In this paper, we will study how to learn a function B to optimize the feedback coefficient  X  and plug it into the mix-ture model feedback to improve the retrieval performance. We view this problem as a prediction problem and propose learning methods for solving it. We now present our method.
We will first identify features possibly correlated to the feedback coefficient. Then we set it up as a learning prob-lem, which can take past queries as training data to learn a function B to map these features to the feedback coeffi-cient  X  . Finally, B can be used to predict new  X  for future queries.
In this section, we propose three heuristics to predict the feedback coefficient: discrimination of query, discrimination of feedback documents, and divergence between query and feedback documents. The three heuristics capture intrin-sic characteristics of the two main components (i.e. query and feedback document set) and the relationship between these two components in a feedback process. We propose a number of features guided by the three heuristics, but our method flexibly allows many other features (e.g., [16]) to be explored in the future work by taking the three heuristics as a road map.
Intuitively, if a query is more discriminative, it could be more drifting-tolerant, and thus it would be safe to utilize more feedback information. So we expect the discrimination of query is correlated with the feedback coefficient. Several measures are proposed to quantify it as follows. (1) Query Length:
A longer query is generally more discriminative than a short one, so our first feature is query length, formally de-fined as:: where c ( w, Q ) is the count of term w in Q . (2) Entropy of Query:
Since the query is often very short, we compute query en-tropy score based on top-N result documents F 0 (Note that we have used a slightly different notation F for relevance feedback documents) in the initial retrieval, defined as fol-lows: where p ( w |  X  F 0 ) is estimated as p ( w |  X  F 0 ) = c ( w, F (3) Clarity of Query:
The  X  X uery clarity X  has shown to predict query difficulty well [11]. Therefore, we expect it to also be useful for pre-dicting feedback coefficient. In the definition, the clarity of a query is the Kullback-Leibler divergence of the query model from the collection model.

To compute query clarity, we need to estimate the query language model first, which, however, involves again an in-terpolation between the original query model  X  Q and the pseudo feedback model  X  F 0 , as well as the setting of an inter-polation coefficient. To avoid this problem, in this paper, we do not estimate an entropy for the interpolated query model directly; instead, we compute two clarity scores based on  X  and  X  F 0 respectively and use them directly as features in a supervised learning framework, leaving the optimization of their combination to the training process.

To reduce the influence of common terms,  X  F 0 is smoothed using the collection language model with the Jelinek-Mercer smoothing method (  X  = 0 . 7) [33]. Following [11], we define relative entropy QEnt R 1 and QEnt R 2 as: where p ( w |C ) is the collection language model.
Intuitively, we would like QEnt R 1 to contribute posi-tively to the measure of the discrimination of query, which simply says that a higher QEnt R 1 implies a more discrim-inative query. However, we can see that QEnt R 1 favors queries that contain high IDF terms; these queries may be extremely specific and over-discriminative. To avoid assign-ing too high discrimination scores to such queries, we would like query discrimination to increase quickly when QEnt R 1 is small but increase slowly as QEnt R 1 is very large. This heuristic is defined formally as follows:
Let Q 1 , Q 2 and Q 3 be three queries, and C 1 , C 2 and C the corresponding query clarity scores computed based on Formula 3. We define d ( Q ) as the discrimination of query Q . If C 1 = C 2  X   X  and C 2 = C 3  X   X  , where  X  is a small constant, then d ( Q 2 )  X  d ( Q 1 ) &gt; d ( Q 3 )  X  d ( Q
To capture this heuristic, we propose another measure by taking a logarithm transformation on the QEnt R 1 to ap-proximate the true discrimination function d ( Q ), as:
Additionally, for QEnt R 2, because we adopt a large  X  to smooth  X  F 0 , which not only  X  X xplains away X  common terms but also decreases the strength of topical terms. To compen-sate the side effect of smoothing, we apply an exponential function to enlarge QEnt R 2 and obtain another feature:
We only utilize documents that are judged relevant by the user for feedback and do not consider negative feedback in this study. Hence, intuitively if feedback documents are more discriminative, it possibly means that they focus more on the relevant topic and far away from noise. Therefore, discriminative feedback documents could be trusted more in the feedback process. (1) Feedback Length:
We also introduce the number of feedback documents, i.e., feedback length | F | , as one feature, which is defined as: where  X  ( d, F ) = 1 if document d  X  F ; otherwise 0. (2) Feedback Radius:
To measure if feedback documents are concentrated on similar topics, we follow previous study [7] to measure the broadness of feedback documents. We define feedback ra-dius as the average divergence between each document and the centroid of the feedback documents, which can be ap-proximated using the Jensen-Shannon divergence [22] among feedback document models, defined below.
 (3) Entropy of Feedback Documents:
Feedback length and feedback radius, as described above, capture the discrimination of feedback documents on the document level, whereas the entropy of feedback documents, which measures the term distribution, is on the term level. Similarly to the computation of query entropy, the entropy of feedback document model  X  F is defined as: where p ( w |  X  F ) is estimated as p ( w |  X  F ) = c ( w, F ) (4) Clarity of Feedback Documents:
Similar to query entropy QEnt A , the computation of feedback entropy FBEnt A is also affected severely by com-mon terms. So, we follow the same idea to smooth  X  F us-ing Jelinek-Mercer smoothing method and then compute the  X  X elative entropy of feedback documents X  (or clarity of feed-back documents) as an alternative feature, which is defined as:
Similarly, to compensate the side effect of smoothing, we apply an exponential function to it and get another feature:
Furthermore, as we have discussed, another method to  X  X xplain away X  common terms is to apply a mixture model to separate the topic model from the background model [32], which, although time-consuming, is applicable in our algo-rithm: since we will compute the topic model for feedback documents in the feedback stage anyway, we can obtain the topic model  X  T here by amortizing the computation cost. The reader can refer to paper [32] for more details. So, we get another feature:
The motivation of divergence between query and feedback documents is that, we may rely on feedback more if the query does not represent relevant information well (i.e., the divergence between the query and its feedback documents is large.) Below, we list two measures to quantify it. (1) Absolute Divergence:
A direct and intuitive way to estimate the divergence is computing the KL-divergence between query model  X  Q and feedback model  X  F .

With respect to  X  Q , usually we interpolate the original query model and the pseudo feedback model to obtain a more accurate query model. However, there are several problems with this method: first, we need to choose an inter-polation coefficient; secondly, terms occurring in the original query will dominate the divergence score and the contribu-tion of other related terms would not be rewarded appropri-ately; thirdly, the probability of common terms (e.g.,  X  X he X ,  X  X nd X , etc.) in the interpolated query model  X  Q is possi-bly much less than its counterparts in the feedback model  X  , which will affect the final score significantly. Based on these observations: we decide to simply use pseudo feedback model  X  F 0 instead of  X  Q to compute the divergence, which is defined as: To prevent zero probability,  X  F 0 is smoothed using the col- X  is set to 1500 . We call this measure  X  X bsolute divergence X  in contrast to the relative divergence to be discussed below. (2) Relative Divergence:
A large absolute divergence value does not necessarily mean a bad query. For example, if the divergence between query and irrelevant documents is much larger than that be-tween query and relevant documents, we can say that the query probably represents relevant information well, no mat-ter what is the absolute divergence value.

To address this problem, we propose another feature to capture relative divergence. Intuitively, if a large portion of top-ranked documents are judged relevant by the user for feedback, it could indicate that the query represents the feedback documents well, suggesting there is possibly a small divergence between query and relevant documents. Moti-vated by this intuition, we adopt a virtual average precision to measure the relative divergence: where r d is the rank of document d , e.g., the rank of the first document is 1 and the second one is 2 ...; prec ( r precision of top r d documents; K is a constant.
With these heuristically defined features, we hope to use some learning techniques to combine them to generate a score for predicting feedback coefficients. In principle, we may use any of the state-of-the-art learning methods. In this paper, we use the logistic regression model, which appears to handle our problem well: it can take as an input, any value from  X  X  X  to  X  , whereas the output is confined to values between 0 and 1.

Logistic regression models are also called maximum en-tropy models and are of the form: where variable z represents some set of features, while f ( z ) represents the probability of a particular outcome, given that set of features. We use f ( z ) as the predicted value for the coefficient  X  , and interpret it as the probability that we would use only the feedback model (as opposed to the original query model) in the mixture model formed by inter-polating the two models. Variable z is a measure of the total contribution of all the features used in the model, which is meric values representing the features, for instance, our fea-tures might include query length | Q | , the entropy of feedback documents, etc. And  X  w represents a set of weights, which indicates the relative weights for each feature. A positive weight means that the corresponding feature increases the probability of the outcome, while a negative weight means that its corresponding feature decreases the probability of that outcome; a large weight means that the feature strongly influences the probability of that outcome, while a near-zero weight means that the feature has little influence on the probability of that outcome.

Typically, we learn these weights from training data, which is described in section 5.1. Because logistic regression mod-els have a global optimum, the choice of learning algorithm is usually of little importance. In our study, we use the statistical package R 1 to train our model.

Once the weight vector  X  w of the equation has been de-rived for a particular data set (e.g., past queries), in our adaptive relevance feedback, the equation can be used to predict feedback coefficients online for new data sets (i.e., future queries).
We used two standard TREC data sets in our study: Ter-abyte (GOV2) and TREC678 (TREC disk 4 and 5 minus Congressional Record). GOV2 data [9] is the largest test set publicly available for ad hoc retrieval with rich relevance judgments. And we used another large data set TREC678 to help further evaluate the adaptability of our algorithm to different training data. Queries were taken from the  X  X itle X  field of the corresponding query topics.

Besides document collections and queries, we also need users X  feedback for each query. We chose to simulate feed-back documents as follows. First, we employed the Lemur toolkit and Indri search engine 2 to index document collec-tions and initially retrieved a document list for each query using our baseline KL-divergence retrieval model. And then, for each query, we assumed all relevant documents on the first result page (i.e., top-10 results) were judged by users for relevance feedback. We only used documents that were  X  X udged X  relevant for feedback and did not consider nega-tive feedback in this study. Since there are a few queries that have no relevant documents in the first result page, we simply removed them from our evaluation data sets. Table 1 shows statistics of the training and testing queries, including the query topics, the number of queries with user X  X eedback X , and the total number of relevance judgments. All the above processing is the same for our training and testing sets. And the preprocessing of documents and queries is minimum, in-volving only stemming with the Porter stemmer. No stop words have been removed.

To train the proposed adaptive relevance feedback, as well as the fixed-coefficient relevance feedback, we need to obtain http://www.r-project.org/ http://www.lemurproject.org/ the training data first. In our study, we split our queries into two parts: we used queries from Terabyte04 (topics 701-750), Terabyte05 (topics 751-800), TREC6 (topics 301-450), TREC7 (topics 351-400), and TREC8 (topics 401-450) as training queries to simulate the  X  X ast X  queries, and took topics from Terabyte06 (topics 801-850) as testing queries to simulate queries in the  X  X uture X . To get the optimal feed-back coefficients for training queries, we used the baseline retrieval model with the mixture model feedback to do rel-evance feedback experiments on training data sets; through we chose the optimal one for each query. For model train-ing, we can train the two feedback methods on any subset of the training queries. The difference between the fixed-coefficient feedback and the adaptive feedback lies mainly in what they can learn from the training data: the former learns a fixed feedback coefficient  X  that leads to the best mean average precision on the training query set, while the latter learns a prediction model that best fits the training query set.

As for the evaluation, we first excluded the top-10 re-sult documents (including both relevant and irrelevant doc-uments) for all related queries from the collection, and then applied relevance feedback methods to retrieve documents from the residue collection. The reason is that we focus on how much the relevance feedback techniques can improve the accuracy of the unseen pages from users X  perspective. Af-ter that, the top-ranked 1 , 000 documents for all runs were compared in terms of their mean average precisions (MAP), which we used as our main evaluation metric. Besides, some other performance measures, such as precision at top-30 doc-uments and recall at 1 , 000 documents, were also considered in our evaluation. Additionally, to make the performance comparable, throughout our experiments, we fixed Dirichlet smoothing prior to 1500 , feedback term count to 100, and mixture noise parameter  X  in the mixture model to 0 . 9, and only left feedback coefficient for tuning.
As we have discussed in Section 3, relevance feedback is usually controlled by an interpolation coefficient  X  . When  X  = 0 , we are only using the original query model (i.e., no feedback), while if  X  = 1 , we ignore completely the original query and rely only on the feedback model. To show the sensitivity of  X  , we plot the MAP of several randomly se-lected queries (TREC topics 757, 776, and 793) in relevance feedback experiments by varying  X  from 0 to 1. The re-sults are shown in Figure 1. We can observe that the setting of  X  can affect the retrieval performance significantly, and the optimal coefficients for different queries could be quite different.
The objectives of this section are to link the features to feedback coefficients and to determine the effect of each fea-ture on predicting optimal feedback coefficients.
We first measure the correlation between features and the optimal feedback coefficient for each query on the Ter-abyte04&amp;05 data, as reported in Table 2. Among these fea-tures, FBRadius , FBEnt R 2, and FBEnt R 3 are by far the most correlated factors to the optimal feedback coeffi-cient, which are highlighted. It may mean that the discrimi-nation of feedback documents plays the key role in predicting Figure 1: Sensitivity to the feedback coefficient (i.e.,  X  ) of several TREC query topics. feedback coefficients. Some other factors, such as QEnt R 1, QEnt R 3, QEnt R 4, | Q | , and QFBDiv A have a lower, but still substantial effect of prediction, which shows that all the three heuristics proposed may be correlated to feedback coef-ficients. However, other factors, such as QEnt A , QEnt R 2, FBEnt A , FBEnt R 1, QFBDiv R , and | F | , have less ef-fect and are thus discarded in our prediction model.
Next, we try to fit the training data using the remain-ing features. The assessment of fit is based on significance tests for the regression coefficients. Table 3 shows the sig-nificance of these features. By removing QEnt R 4 and | Q | one by one, we finally obtain a set of 6 features whose sig-nificance values are all close to or less than 0 . 01, and this feature set also minimizes the AIC (Akaike X  X  Information Criteria) score. These 6 features are highlighted in Table 3, based on which, the following coefficients are derived from Terabyte04&amp;05 data. where we use the absolute value for each feature without any normalization, since there is no predictable value range for each feature. The last row of Table 2 also gives the correlation between the optimal feedback coefficient and z , which shows that the combination of these 6 features could be able to predict the optimal feedback coefficient. Given a new query, we can predict its feedback coefficient directly
From the above formula, we can see that, for two query clarity scores, QEnt R 1 and QEnt R 3, the former is cor-related negatively to the feedback coefficient, while the lat-ter shows a positive correlation. As we know, QEnt R 3 = log( QEnt R 1), so it may mean: (1) when query clarity score QEnt R 1 is relatively small,  X  11 . 83219  X  QEnt R 3 X  domi-nates over  X   X  0 . 87825  X  QEnt R 1 X , and the overall effect of query clarity is positive. This is consistent with our expec-tation that a more discriminative query is more drifting-tolerant and thus it is safe to use a large feedback coeffi-cient. (2) However, when QEnt R 1 is very large, X   X  0 . 87825  X  QEnt R 1 X  will be as important as or even dominate over  X 11 . 83219  X  QEnt R 3 X . In this case, the overall effect of query clarity score will be insignificant or even forced to be negative. One possible explanation is that a very large query clarity score may mean that the query is clear enough, and thus would not benefit much from feedback. Table 2: Pearson and Spearman correlation coef-ficients between features and the optimal feedback coefficients on Terabyte training topics.

The different behaviors of FBEnt R 2 and FBEnt R 3 could be explained as a trade-off between  X  X iscriminative X  and  X  X xtreme X  (i.e., too discriminative). When the discrimi-nation value of feedback documents is relatively small, FBEnt R 3 is often larger than FBEnt R 2 due to the use of smoothing in computing FBEnt R 2, and thus X   X  0 . 12386  X  FBEnt R 2+ 0 . 50930  X  FBEnt R 3 X  usually appears greater than zero; it may suggest that we can trust feedback more and use a higher feedback coefficient if the topic of feedback documents is more discriminative. However, when the discrimination of feedback documents is very large, FBEnt R 2 would domi-nate over FBEnt R 3 due to the use of exponential function in FBEnt R 2, and thus  X   X  0 . 12386  X  FBEnt R 2+0 . 50930  X  FBEnt R 3 X  is often less than zero; it possibly means that we do not need a large feedback coefficient if the feedback is too discriminative, since such feedback information can easily drift away from the original query.

The remaining two features QFBDiv A and FBRadius work similarly as we discussed in Section 4, so we do not duplicate explanations and analysis here.

To further examine our feature selection process, we train three prediction models on Terabyte04&amp;05 data by using three different sets of features respectively: (i) the 6 most significant features, (ii) the 8 features in Table 3, and (iii) all of the proposed features. They are labeled as  X  X RM X ,  X  X RM+ X , and  X  X RM++ X  respectively. Besides retrieval per-formance, we also compute the Mean Absolute Error ( X  X red. Err X ) to show how far off the coefficients used in each method and the optimal coefficients. The results are summarized in Table 4. We can observe that  X  X RM X , although using fewer features, performs better than or comparably to  X  X RM++ X  and X  X RM+ X . Therefore in all the following experiments, we train our adaptive feedback model using the  X  X RM X  feature set.

We have not done feature analysis and selection on TREC678 and just trained our prediction model on it using the same  X  X RM X  feature set. However, it would be interesting to ex-Table 4: Performance comparison of different fea-ture sets. plore and compare feature behaviors on different data sets in the future work.
Once the regression model has been estimated in an of-fline training process, we can predict feedback coefficients directly for  X  X uture X  queries. Thus, the computation of the 6 features takes the major time in our approach. We show below all of these features can be computed efficiently for online prediction.

To compute the features, we only need to estimate the following language models based on the maximum likelihood estimation.
Besides, there are two other models needed:
However we can  X  X orrow X  M 5 from the feedback process without any extra effort, and M 6 is computed offline. All the 6 features can be estimated based on the above six language models: QEnt R 1 = D ( M 1 || M 6 ), QEnt R 2 = log( QEnt R 1), FBEnt R 3 = D ( M 5 || M 6 ), FBEnt R 2 = exp( D ( M 3  X  || M 6 )), FBRadius = 1 | M and QFBDiv A = D ( M 3 || M 2  X  ), where D ( M i || M j ) repre-sents the KL-divergence between M i and M j , and M j  X  in-dicates a smoothed M j (see Section 4 for details).
We evaluate the proposed adaptive relevance feedback in three variant cases: (1) The training set is an  X  X deal X  one, that is, the training data and the testing data are in the same domain; (2) The training set is the  X  X oughest X  one, which is dominated by the data not in the same domain; (3) we have sufficient training data in the same domain, but it is mixed with  X  X oisy X  data.

We choose Terabyte04&amp;05 to simulate an  X  X deal X  training data since Terabyte 04, 05, and 06 share the same docu-ment collection and use similar query topics. We train both the adaptive relevance feedback (AdaptFB) method and the fixed-coefficient relevance feedback (FixedFB) method on this data set. In addition, we also introduce another run (labeled as  X  X ptimalFB X ), in which we manually set the optimal feedback coefficient for each query; it defines an Table 5: Performance comparison of AdaptFB and FixedFB using Terabyte04&amp;05 as training set. Figure 2: Sensitivity to number of queries for train-ing. upper bound of adaptive relevance feedback. The results are reported in Table 5. It shows that both AdaptFB and FixedFB outperform the baseline (without any feedback) significantly. Comparing them with OptimalFB, we can see there is still room to improve the retrieval performance by further optimizing feedback coefficients.

In this ideal environment, although AdaptFB is more ef-fective than FixedFB, the improvement is not significant, suggesting that the regular FixedFB is also a reasonable method when we have consistent training data in the same domain. One possible reason could be that we only have 89 training queries in Terabyte04&amp;05, which may be insuf-ficient to train an accurate prediction model. To examine it, we draw the learning curves w.r.t. the number of queries for training in Figure 2. It is interesting to see an increas-ing trend in the performance of AdaptFB as we increase the number of training queries, while the performance of FixedFB appears to be stable. This observation suggests that AdaptFB would potentially outperform FixedFB more if we have more training data. Further experiments and analysis are needed to understand this better.

Since our learning approach is to minimize the prediction error, rather than to maximize the retrieval performance di-rectly, one interesting question is whether lower prediction errors have enabled AdaptFB to outperform FixedFB. So, we plot the improvement of MAP as a function of the re-duction of prediction error in Figure 3. The reduction of prediction error is formally defined as: |  X  o  X   X  f | X  X   X  where  X  o is the optimal feedback coefficient and  X  f is the well-tuned fixed coefficient. Figure 3 shows clearly a strong positive correlation between these two objective functions, which indicates that the improvement of performance is in-deed due to the reduction of prediction errors.

In reality, we often do not have, or do not have suffi-cient training data in the same domain. We start with an extreme situation when our training data are all in a dif-ferent domain, and we use TREC6&amp;7&amp;8 data to simulate such training data. We then gradually add the Terabyte04 and Terabyte05 data into the training set to mix with the existing TREC6&amp;7&amp;8 data to simulate the scenario when Figure 3: The correlation between the reduction of prediction error and the improvement of MAP.
 Table 6: Comparison of AdaptFB and FixedFB when Terabyte04&amp;05 data is gradually added into the existing training set (i.e., TREC6&amp;7&amp;8). 0 + 0 means that the corresponding improvements over FixedFB are statistically significant. we have more and more training data in the right domain. We compare AdaptFB and FixedFB in Table 6. The re-sults show that AdaptFB yields significantly better results than FixedFB with insufficient training data of the same do-main. We further compare the sensitivity of the two meth-ods in Figure 4. It is clearly observed that, with only a few (e.g., 20) training samples from Terabyte topics, AdaptFB has already reached reasonably good performance. How-ever, even after we add all the Terabyte04&amp;05 data into the training set, the performance of FixedFB is still very poor. This shows that AdaptFB picks up test domain characteris-tics more quickly than FixedFB. One possible explanation is that, FixedFB often over-fits the specified training data; the proposed AdaptFB, however, can really adapt to the char-acteristics of feedback and query effectively due to the use of feedback and query-specific features.
 In the above experiments, although we added all of the Terabyte04&amp;05 data into the training set finally, the per-formance of AdaptFB and FixedFB is still not as good as Figure 4: Sensitivity to the insufficiency of training data in the same domain. Table 7: Comparison of AdaptFB and FixedFB when TREC6&amp;7&amp;8 data is gradually added into the existing training set (i.e., Terabyte04&amp;05). 0 + 0 means that the corresponding improvements over FixedFB are statistically significant. Figure 5: Sensitivity to the noisy training queries (i.e., TREC6&amp;7&amp;8) in the training data. that in the  X  X deal X  case. It indicates that the  X  X oisy X  data (i.e., TREC6&amp;7&amp;8) contributes negatively to the prediction accuracy.

Therefore, we now go to the third question: what is the influence of the noise to two methods if the training set is involved in  X  X oisy X  data? In fact, in real Web search envi-ronment, it is often hard to obtain a training set of the same domain for every query; thus, we will generally have some  X  X oisy X  data in the training set. In order to understand the influence of noise, we keep the current Terabyte04&amp;05 data in our training set, but at the same time we add TREC6, TREC6&amp;7, and TREC6&amp;7&amp;8 data into the training data set respectively to learn new prediction models. The results are reported in Table 7. We can observe that AdaptFB outper-forms FixedFB significantly. To understand the sensitivity of two methods better, we further plot the sensitivity curves w.r.t. the noisy data in Figure 5. We can see that the per-formance of FixedFB decreases dramatically when there is noisy data, but AdaptFB is more stable. Interestingly, it is observed that the performance of AdaptFB is even improved slightly when a little noisy data is introduced. Both Table 7 and Figure 5 show that FixedFB only works when the train-ing and testing sets are very similar, and it would not work well when the training set is noisy; while AdaptFB is robust and effective in both cases. It means that AdaptFB is more noise-tolerant than FixedFB.

To make the comparison between AdaptFB and FixedFB more illustrative, we also plot the average precision of the two methods in Figure 6, where both of the methods are trained on a mixture of Terabyte04&amp;05 data and TREC6&amp;7 data. We can see that AdaptFB outperforms FixedFB for most of the queries with only a few exceptions. In addition, AdptFB appears to work especially effectively for difficult queries. Further experiments are needed to study and un-derstand this interesting observation better.
Figure 6: MAP plot for AdaptFB and FixedFB.
In this paper, we proposed an adaptive relevance feed-back algorithm to optimize the balance between query and feedback information. First, three heuristics were proposed to characterize the balance between query and feedback in-formation, including discrimination of query, discrimination of feedback documents, and divergence between query and feedback documents. Then, taking these three heuristics as a road map, we explored a number of features and com-bined them using the logistic regression model to predict the balance coefficient. Finally, we did extensive experi-ments to evaluate our algorithm from different perspectives; our experiments show that the three heuristics are all very important, each capturing one aspect of information to pre-dict the feedback coefficient, and the proposed adaptive rel-evance feedback is more robust and effective than the reg-ular fixed-coefficient feedback, especially when the training data is noisy: it suggests that, the regular fixed-coefficient feedback only works when the training and testing sets are very similar; while the adaptive relevance feedback is more noise-tolerant and can adapt to the general characteristics of feedback and query effectively.

There is still room to explore in the future work. First, the proposed method relies on explicit user feedback for training, so it would be interesting to study how to adaptively exploit pseudo and implicit feedback. Secondly, we only evaluated the proposed methodology of adaptively learning feedback coefficient for the mixture-model feedback method, so we also hope to apply our idea to other feedback approaches, e.g., Rocchio feedback, to examine its performance. Thirdly, the training in the proposed method needs target values for the feedback coefficients, and a possibly more natural strat-egy which would be done in our future work could be to learn the coefficient so as to optimize some measure related to the MAP criterion, e.g., using a learning to rank method, which would remove the need for computing desired coeffi-cient values. Fourthly, we should study more effective and robust features in the future. Fifthly, it would also be inter-esting to explore and incorporate negative feedback into the proposed adaptive relevance feedback method.
We thank the anonymous reviewers for their useful com-ments. We also thank Wan Chen for her help to improve the English in this paper. This material is based upon work supported by the National Science Foundation under Grant Numbers IIS-0347933, IIS-0713581, and IIS-0713571. [1] Giambattista Amati, Claudio Carpineto, and Giovanni [2] Anne Aula. Query formulation in web information [3] Chris Buckley and Stephen E. Robertson. Relevance [4] Chris Buckley, Gerard Salton, James Allan, and Amit [5] Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, [6] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and [7] David Carmel, Elad Yom-Tov, Adam Darlow, and [8] Ben Carterette and Desislava Petkova. Learning a [9] Charles Clarke, Nick Craswell, and Ian Soboroff. [10] William S. Cooper, Fredric C. Gey, and Daniel P. [11] Stephen Cronen-Townsend, Yun Zhou, and W. Bruce [12] Stephen Cronen-Townsend, Yun Zhou, and W. Bruce [13] Norbert Fuhr and Chris Buckley. A probabilistic [14] Fredric C. Gey. Inferring probability of relevance using [15] Trevor Hastie, Robert Tibshirani, and Jerome [16] Claudia Hauff, Djoerd Hiemstra, and Franciska [17] Ben He and Iadh Ounis. Combining fields for query [18] T. Joachims. Optimizing search engines using [19] Diane Kelly and Jaime Teevan. Implicit feedback for [20] John D. Lafferty and Chengxiang Zhai. Document [21] Victor Lavrenko and W. Bruce Croft. Relevance-based [22] Jianhua Lin. Divergence measures based on the [23] Yuanhua Lv and ChengXiang Zhai. A comparative [24] Jay M. Ponte and W. Bruce Croft. A language [25] Stephen E. Robertson and Karen Sparck Jones. [26] Stephen E. Robertson, Steve Walker, Susan Jones, [27] J. J. Rocchio. Relevance feedback in information [28] Gerard Salton and Chris Buckley. Improving retrieval [29] Tao Tao and ChengXiang Zhai. Regularized [30] Zuobing Xu and Ram Akella. A bayesian logistic [31] Elad Yom-Tov, Shai Fine, David Carmel, and Adam [32] ChengXiang Zhai and John D. Lafferty. Model-based [33] ChengXiang Zhai and John D. Lafferty. A study of [34] Zhaohui Zheng, Keke Chen, Gordon Sun, and
