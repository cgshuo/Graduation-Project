 The stochastic approximation method supplies the theoretical underpinnings behind many well-studied algorithms in machine learning, notably policy gradient and tempo ral dif-ferences for reinforcement learning, inference for tracking and filtering, on-line learn-ing [1, 17, 19], regret minimization in repeated games, and parameter estima tion in prob-abilistic graphical models, including expectation maximization (EM) and the co ntrastive divergences algorithm. The main idea behind stochastic approximation is simpl e yet pro-found. It is simple because it is only a slight modification to the most basic o ptimization method, gradient descent. It is profound because it suggests a fundamentally different w ay of optimizing a problem X  X nstead of insisting on making progress toward the so lution at every iteration, it only requires that progress be achieved on average .
 Despite its successes, people tend to steer clear of constraints on the parameters. W hile there is a sizable body of work on treating constraints by extending established o ptimization techniques to the stochastic setting, such as projection [14], subgradient (e.g . [19, 27]) and penalty methods [11, 24], existing methods are either unreliable or suited only to s pecific types of constraints. We argue that a reliable stochastic approximatio n method that handles constraints is needed because constraints routinely arise in the mathematical fo rmulation of learning problems, and the alternative approach X  X enalization X  X s often unsatisfact ory. Our main contribution is a new stochastic approximation method in which each step is the solution to the primal-dual system arising in interior-point methods [7]. Our method is easy to implement, dominates other approaches, and provides a general solution to constra ined learning problems. Moreover, we show interior-point methods are remarkably w ell-suited to stochastic approximation, a result that is far from trivial when one consi ders that stochastic algorithms do not behave like their deterministic counterparts (e.g. Wolfe condi tions [13] do not apply). We derive a variant of Widrow and Hoff X  X  classic  X  X elta rule X  fo r on-line learning (Sec. 5). It achieves feature selection via L 1 regularization (known to statisticians as the Lasso [22] and to signal processing engineers as basis pursuit [3]), so it is well-suited to learning problems with lots of data in high dimensions, such as the problem of fil tering spam from your email account (Sec. 5.2). To our knowledge, no method has been proposed that reliably achieves L 1 regularization in large-scale problems when data is processed on-line or on-demand. Finally, it is important that we establish convergence guara ntees for our method (Sec. 4). To do so, we rely on math from stochastic approximation a nd optimization. In their 1952 research paper, Robbins and Monro [15] examined the problem of tuning a control variable x (e.g. amount of alkaline solution) so that the expected outcome of the experiment F ( x ) (pH of soil) attains a desired level  X  (so your Hydrangea have pink blos-soms). When the distribution of the experimental outcomes is unknown to the stat istician or gardener, it may be still possible to take observations at x . In such case, Robbins and Monro showed that a particularly effective way to achieve a response level  X  = 0 is to take a (hopefully unbiased) measurement y k  X  F ( x k ), adjust the control variable according to for step size a k &gt; 0, then repeat. Provided the sequence { a k } behaves like the harmonic series (see Sec. 4.1), this algorithm converges to the solution F ( x  X  ) = 0.
 Since the original publication, mathematicians have extended, generalized, and further weak-ened the convergence conditions; see [11] for some of these developments. Kiefer and Wol-fowitz re-interpreted the stochastic process as one of optimizing an unconstrained objective ( F ( x ) acts as the gradient vector) and later Dvoretsky pointed out that each measur ement y is actually the gradient F ( x ) plus some noise  X  ( x ). Hence, the stochastic gradient algo-rithm. In this paper, we introduce a convergent sequence of nonlinear systems F ( x ) = 0 and interpret the Robbins-Monro process { x k } as solving a constrained optimization problem. We focus on convex optimization problems [2] of the form where c ( x ) is a vector of inequality constraints, f ( x ) and c ( x ) have con-tinous partial derivatives, and mea-surements y k of the gradient at x k are noisy. The feasible set, by contrast, should be known exactly. To simplify our exposition, we do not consider equality constraints; techniques for handling them are discussed in [13].
 Convexity is a standard assumption made to simplify analysis of stocha stic approximation algorithms and, besides, constrained, non-convex optimization raises unresolved com plica-tions. We assume standard constraint qualifications so we can legitimately identify optimal solutions via the Karush-Kuhn-Tucker (KKT) conditions [2, 13].
 Following the standard barrier approach [7], we frame the constrained optim ization problem as a sequence of unconstrained objectives. This in turn is cast as a sequence of root-finding problems F ( x ) = 0, where &gt; 0 controls for the accuracy of the approximate objective and should tend toward zero. As we explain, a dramatically more effective strat egy is to solve for the root of the primal-dual equations F ( x, z ), where z represents the set of dual variables. This is the basic formula of the interior-point stochastic a pproximation method. Fig. 1 outlines our main contribution. Provided x 0 is feasible and z 0 &gt; 0, every subsequent cient decrease condition on k F ( x, z ) k or suitable merit function; this is not needed in the stochastic setting. Our stochastic approximation algorithm requires a s lightly non-standard treatment because the target F ( x, z ) moves as changes. Fortunately, convergence under non-stationarity has been studied in the literature on tracking and adaptive filt ering. The next section is devoted to deriving the primal-dual search direction ( X  x,  X  z ). We motivate and derive primal-dual interior-point methods starting from t he logarithmic barrier method. Barrier methods date back to the work of Fiacco and McCormick [6 ] in the 1960s, but they lost favour due to their unreliable nature. Ill-conditioning was lo ng considered their undoing. However, careful analysis [7] has shown that poor conditioni ng is not the problem X  X ather, it is a deficiency in the search direction. In the next section, we exploit this very analysis to show that every iteration of our algorithm pro duces a stable iterate in the face of: 1) ill-conditioned linear systems, 2) noisy observati ons of the gradient. The logarithmic barrier approach for the constrained optimization problem (2 ) amounts to solving a sequence of unconstrained subproblems of the form where &gt; 0 is the barrier parameter, and m is the number of inequality constraints. As becomes smaller, the barrier function f ( x ) acts more and more like the objective. The philosophy of barrier methods differs fundamentally from  X  X xterior X  penalty m ethods that penalize points violating the constraints [13, Chapter 17] because the l ogarithm in (3) prevents iterates from violating the constraints at all, hence the word  X  X arr ier X . The central thrust of the barrier method is to progressively push to zero at a rate which allows the iterates to converge to the constrained optimum x  X  . Writing out a first-order Taylor-series expansion to the optimality conditions  X  f ( x ) = 0 about a point x , the Newton step  X  x is the solution to the linear equations  X  2 f ( x )  X  x =  X  X  X  f ( x ). The barrier Hessian has long been known to be incredibly ill-conditioned X  X his fact becomes apparent by writing out  X  2 f ( x ) in full X  X ut an analysis by Wright [25] shows that the ill-conditioning is not harmful under the right conditions. The  X  X ight conditions X  are that x be within a small distance 1 from the central path or barrier trajectory , which is defined to be the sequence of isolated minimizers x  X  satisfying  X  f ( x  X  ) = 0 and c ( x  X  ) &lt; 0. The bad news: the barrier method is ineffectual at remaining on the barrier trajectory X  X t pushes iterates too clo se to the boundary where they are no longer well-behaved [7]. Ordinarily, a convergence test is conducted for each value of , but this is not a plausible option for the stochastic setting. Primal-dual methods form a Newton search direction for both the primal vari ables and the Lagrange multipliers. Like classical barrier methods, they fail catastrophi cally outside the central path. But their virtue is that they happen to be extremely good at remaining o n the central path (even in the stochastic setting; see Sec. 4.2). Primal-dual metho ds are also blessed with strong results regarding superlinear and quadratic rates of converg ence [7]. The principal innovation is to introduce Lagrange multiplier-like variables z i  X   X  /c i ( x ). By setting  X  x f ( x ) to zero, we recover the  X  X erturbed X  KKT optimality conditions: where Z and C are matrices with z and c ( x ) along their diagonals, and J  X  X  X  x c ( x ). Forming a first-order Taylor expansion about ( x, z ), the primal-dual Newton step is the solution to on constrained optimization), and H is the Hessian of the objective or an approximation. Through block elimination, the Newton step  X  x is the solution to the symmetric system where  X   X  C  X  1 Z . The dual search direction is then recovered according to Because (2) is a convex optimization problem, we can derive a sensible update rule f or the barrier parameter by guessing the distance between the primal and dual objectives [2]. This guess is typically =  X   X z T c ( x ) /m , where  X  &gt; 0 is a centering parameter. This update is supported by the convergence theory (Sec. 4.1) so long as  X  k is pushed to zero. First we establish conditions upon which the sequence of iterates generated by the al gorithm to infinity. Then we examine the behaviour of the iterates under finite-precision arithmetic. 4.1 Asymptotic convergence A convergence proof from first principles is beyond the scope of this paper; we build upo n the martingale convergence proof of Spall and Cristion for non-stationary sy stems [21]. Assumptions: We establish convergence under the following conditions. They may be weakened by applying results from the stochastic approximation and optimizat ion literature. Proposition: Suppose Assumptions 1 X 7 hold. Then  X   X   X  ( x  X  , z  X  ) is an isolated (locally unique within a  X  -neighbourhood) solution to (2), and the iterates  X  k  X  ( x k , z k ) of the feasible interior-point stochastic approximation method (Fig. 1) con verge to  X   X  almost surely; that is, as k approaches the limit, ||  X  k  X   X   X  || = 0 with probability 1.
 Proof: See Appendix A. 4.2 Considerations regarding the central path The object of this section is to establish that computing the stochastic primal -dual search direction is numerically stable. (See Part III of [23] for what we mean by  X  X  table X .) The concern is that noisy gradient measurements will lead to wildly perturbed search directions. As we mentioned in Sec. 3, interior-point methods are surprisingly stable prov ided the iterates remain close to the central path, but the prospect of keeping close to the path seems particularly tenuous in the stochastic setting. A key observation is tha t the central path is itself perturbed by the stochastic gradient estimates. Following arguments similar to those given in Sec. 5 of [7], we show that the stochastic Newton step (6,7 ) stays on target. with gradient estimate y  X   X  f ( x ) +  X  . Suppose we are currently at point  X  ( ,  X  ) = ( x, z ) One way to assess the quality of the Newton step is to compare it to the tangent line of the noisy central path at ( ,  X  ). Taking implicit partial derivatives at ( x, z ), the tangent line is with y  X   X  X  X  f ( x ) +  X   X  . Since we know that F ( x, z ) = 0, the Newton step (5) at ( x, z ) with In conclusion, if the tangent line (8) is a fairly reasonable approximation t o the central path, then the stochastic Newton step (10) will make good progress toward  X  (  X  ,  X   X  ). Having established that the stochastic gradient algorithm closely follow s the noisy central path, the analysis of M. H. Wright [26] directly applies, in which round-off erro r (  X  machine ) is occasionally replaced by gradient noise (  X  ). Since stability is of fundamental concern X  particularly in computing the values of W  X  J T  X  J , the right-hand side of (6), and the solution to  X  x and  X  z  X  X e elaborate on the significance of Wright X  X  results in Appendix B. In this section, we apply our findings to the problem of computing an L 1 -regularized least squares estimator in an  X  X n-line X  manner; that is, by making adjustments to eac h new example without having to review all the previous training instances. While this pro blem only involves simple bound constraints, we can use it to compare our method to existing approaches such as gradient projection. We start with some background behind the L 1 , motivate the on-line learning approach, draw some experimental comparisons wi th existing methods, then show that our algorithm can be used to filter spam.
 y . (The notation here is separate from previous sections.) Assuming a linear model and centred coordinates, the least squares estimate  X  minimizes the mean squared error (MSE). Linear regression based on the maximum likelihood estimator is one of the bas ic statistical tools of science and engineering and, while primitive, generalizes to many popular sta tistical estimators, including linear discriminant analysis [9]. Because the least squa res estimator is unstable when m is large, it can generalize poorly to unseen examples. The standard cure is  X  X egularization, X  which introduces bias, but typically produces estimators tha t are better at predicting the outputs of unseen examples. For instance, the MSE with an L 1 -penalty, not only prevents overfitting but tends to produce estimators that shrink many of the components  X  j to zero, resulting in sparse codes. Here, kk 1 is the L 1 norm and  X  &gt; 0 controls for the level of regularization. This approach has been independently studied for many problems, including statistical regression [22] and sparse signal recons truction [3, 10], precisely because it is effective at choosing useful features for prediction.
 We can treat the gradient of MSE as a sample expectation over responses of the fo rm  X  x i ( y i  X  x T i  X  ), so the on-line or stochastic update improves the linear regression with only a single data point ( a is the step size). 2 This is the famed  X  X elta rule X  of Widrow and Hoff [12]. Since standard  X  X atch X  learning requires a full pass through the data for each gradient evaluation, the on-line update (12) may b e the only viable option when faced with, for instance, a collection of 80 million images [16]. On-line learning for regression and classification X  X ncluding L 2 regularization X  X s a well-researched topic, particularly for neural networks [17] and support vector machines (e.g. [1 9]). On-line learning with L 1 regularization, despite its ascribed benefits, has strangely avoided study. (The only known work that has approached the problem is [27] using subgradient methods .) We derive an on-line, L 1 -regularized learning rule of the form and where &gt; 0 is the barrier parameter,  X  =  X  pos  X   X  neg , z pos and z neg are the Lagrange multipliers associated with the lower bounds  X  pos  X  0 and  X  neg  X  0, respectively, and a is a step size ensuring the variables remain in the positive quadrant. Multiplicat ion and division in (13) are component-wise. The remainder of the algorithm (Fig. 1) consists of choosing and feasible step size a at each iteration. Let us briefly explain how we arrived at (13). Figure 2: (left) Performance of constrained stochastic gradient methods for different step size sequences. (right) Performance of methods for increasing levels of variance in the dimensions of the training data. Note the logarithmic scale in the vertical axis.
 because the L 1 norm is not differentiable near zero. The trick is to separate the coefficients into their positive (  X  pos ) and negative (  X  neg ) components following [3], thereby transform-ing the non-smooth, unconstrained optimization problem (11) into a smooth problem with convex, quadratic objective and bound constraints  X  pos ,  X  neg  X  0. The regularized delta rule (13) is then obtained from direct application of the primal-dual interior-po int Newton search direction (6,7) with a stochastic gradient (see Eq. 12), and ident ity in place of H . 5.1 Experiments We ran four small experiments to assess the reliability and shrinkage effect o f the interior-point stochastic gradient method for linear regression with L 1 regularization; refer to Fig. 1 and Eq. 13. 3 We also studied four alternatives to our method: 1) a subgradient method, 2) a smoothed, unconstrained approximation to (11), 3) a projected gradient metho d, and 4) the augmented Lagrangian approach described in [24]. See [18] for an in-depth di scussion of the merits of applying the first three optimization approaches to L 1 regularization. All these methods have a per-iteration cost on the order of the number of features.
 Method. For the first three experiments, we simulated 20 data sets following the procedure described in Sec. 7.5 of [22]. Each data set had n = 100 observations with m = 40 features. We defined observations by x ij = z ij + z i , where z i was drawn from the standard normal the inverse Gamma with shape 2 . 5 and scale  X  = 1. (The mean of  X  2 j is proportional to  X  .) each block [22]. Outputs were generated according to y i =  X  T x i +  X  with standard Gaussian noise  X  . Each method was executed with a single pass on the data (100 iterations) wit h step sizes  X  a k = 1 / ( k 0 + k ), where k 0 = 50 by default. We chose L 1 penalty  X /n = 1 . 25, which tended to produce about 30% zero coefficients at the solution to (11). The augment ed Lagrangian required a sequence of penalty terms r k  X  0; after some trial and error, we chose r step size parameter k 0 , the inverse Gamma scale parameter  X  , and the L 1 penalty parameter  X  . In Experiment 4, each example y i in the training set x i had 8 features, and we set the true coefficients were set to  X  = (0 , 0 , 2 ,  X  4 , 0 , 0 ,  X  1 , 3) T .
 Results. Fig. 2 shows the results of Experiments 1 and 2, with error 1 n k  X  exact  X   X  on-line k 1 mate obtained after 100 iterations of the on-line or stochastic gradient met hod. With a large enough step size, almost all the methods converged close to  X  exact . The stochastic interior-point method, however, always came closest to  X  exact and, for the range of values we tried, its solution was by far the least sensitive to the step size sequence and level of var iance in the ob-servations. Experiment 3 (Fig. 3) shows that even with well-chosen step sizes for all methods, the stochastic interior-point method still best approximated the exact so lution, and its per-formance did not degrade when  X  was small. (The dashed vertical line at  X /n = 1 . 25 in Fig. 3 regularized estimates of Experiment 4. After one pass through the data (middle)  X  X quivalent to a single iteration of an exact solver X  X he interior-point stochastic gradient method shrank some of the data components, but didn X  X  quite dis-card irrelevant features altogether. After 10 vis-its to the training data (right) , the stochastic al-gorithm exhibited feature selection close to what we would normally expect from the Lasso (left) . 5.2 Filtering spam Classifying email as spam or not is most faith-fully modeled as an on-line learning problem in which supervision is provided after each email has been designated for the inbox or trash [5]. An effective filter is one that mini mizes mis-classification of incoming messages X  X hrowing away a good email being considerabl y more deleterious than incorrectly placing a spam in the inbox. Without any prior knowledg e as to what spam looks like, any filter will be error-prone at initial stages of deployment. Spam filtering necessarily involves lots of data and an even larger number of featur es, so a sparse, stable model is essential. We adapted the L 1 -regularized delta rule to the spam filtering problem by replacing the linear regression with a binary logistic regres sion [9]. The To our knowledge, no one has investigated this approach for on-line spam filtering, t hough there is some work on logistic regression plus the Lasso for batch class ification in text corpora [8]. Needless to say, batch learning is completely impractical in this s etting. Method. We simulated the on-line spam filtering task on the trec2005 corpus [4] contain-ing emails from the legal investigations of Enron corporation. We compar ed our on-line clas-sifier (  X  = 10,  X  = 1 2 ,  X  a i = 1 1+ i ) with two open-source software packages, SpamBayes 1.0.3 and Bogofilter 0.93.4. (These packages are publicly available at spambayes.sourceforge.net and bogofilter.sourceforge.net .) A full comparison is certainly beyond the scope of this paper; see [5] for a comprehensive evaluation. We represented each email as a vector of norm alized word frequencies, and used the word tokens extracted by SpamBayes. In the end, we had an on-line learning problem involving n = 92189 documents and m = 823470 features.
Results for SpamBayes Table 1: Contingency tables for on-line spam filtering task on the trec2005 data set. Results. Following [5], we use contingency tables to present results of the on-line spam filtering experiment (Table 1). The top-right/bottom-left entry of each tabl e is the number of misclassified spam/non-spam. Everything was evaluated on-line. We tagged an ema il for deletion only if p ( y i = spam)  X  97%. Our spam filter dominated SpamBayes on the trec2005 corpus, and performed comparably to Bogofilter X  X ne of the best spam filters to date [5]. Our model X  X  expense was slightly greater than the others. As we found i n Sec. 5.1, assessing sparsity of the on-line solution is more difficult than in the exact cas e, but we can say that removing the 41% smallest entries of  X  resulted in almost no ( &lt; 0 . 001) change. Our experiments on a learning problem with noisy gradient measurements and bound con-straints show that the interior-point stochastic approximation alg orithm is a significant improvement over other methods. The interior-point approach also has the virt ue of being much more general, and our analysis guarantees that it will be numerically st able. Acknowledgements. Thanks to Ewout van den Berg, Matt Hoffman and Firas Hamze.

