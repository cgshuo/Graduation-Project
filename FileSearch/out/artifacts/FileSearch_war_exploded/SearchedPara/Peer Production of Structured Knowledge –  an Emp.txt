 Creating and maintaining semantic structures such as ontologies on a large scale is a labor-intensive task, which a sole individual cannot perform. Established automa ted solutions for this task do not yet exist. Peer production is a promising approach to create structured knowledge: Members of an online community create and maintain semantic structures collaboratively. To motivate members to participate and to ensure the quality of the data, rat-ing-based incentive mechanisms are promising. Members mutu-ally rate the quality of their contributions and are rewarded for good contributions and truthful ratings. Until now, there has been no systematic evaluation of such rating mechanisms in the context of structured knowledge. We have developed a platform for the collaborative creation of semantic structures. To evaluate the ef-fect of ratings and incentive m echanisms on the quality of peer-produced data, we have conducted an extensive empirical study in an online community. We show that ratings are a reliable measure of the quality of contributions by comparing user ratings with an ex post evaluation by experts. Fu rther experimental results are that incentive mechanisms increas e the quality of contributions. We conclude that ratings and in centive mechanisms are promising to foster and improve the peer production of structured knowledge. H.5.3 [ Information Interfaces and Presentation ]: Group and Organization Interfaces  X  collaborative computing, computer-supported cooperative work, evaluation . Design, Experimentati on, Human Factors. Ratings, Incentive Mechanisms, Cooperation and Collaboration, Ontologies, Empirical Study. Peer production [3] is gaining acceptance for the creation of struc-tured knowledge and semantic meta data. Peer production refers to a web-based model of production where a large number of con-tributors work on a common project. As opposed to hierarchical organizations, peer production doe s not require a coordinating authority that decides what has to be done and by whom. This approach has the advantage that it is both scalable when adding further users and robust because individual users can be replaced. Examples of projects created by peer production include Wikipe-dia, IMDB, and Slashdot. Several tools for the peer production of structured knowledge have been proposed, see [15] for an over-view. online community, as opposed to an organization with a coordi-nating authority, two questions arise: (1) How can users be moti-vated to contribute? The interest on the topic of a community is not always sufficient to guarantee the active participation of all users. In fact, many online communities suffer from under-contri-bution [2], i.e., only a small fraction of the members contributes. In particular, this question is unsolved for online communities with only few members, e.g., online communities within corpora-tions. Because of their small size, these communities need a rela-tively high percentage of active contributors to reach a critical mass. In contrast to large communities, they cannot tolerate a high number of free riders. (2) How to ensure and assess the quality of the peer-produced data? Compared to, say, Wikipedia, the prob-lem of quality assurance becomes even more pressing for semantic metadata, which is used for query processing or automated reasoning. Thus, the absence of a coordinating authority must not compromise the quality of data. to assure the quality of structured knowledge created collabora-tively. Users review contributions created by other users and rate them according to the quality perceived. To motivate users to contribute to the community, we reward them with points ac-cording to the extent and quality of their contributions. We com-pute the quality of contributions, in turn, based on their ratings. Finally, we convert the gathered points into external rewards, e.g., gift coupons like at Epinions or system privileges like at Slashdot. the ratings themselves have to be of high quality. We deem ratings to be of high quality if they represent the honest opinion of raters, as opposed to uninformed one s or ratings that simply copy the majority opinion. To gain such high-quality ratings, incentive mechanisms for truthful ratings have been proposed, e.g., Bayes-ian Truth Serum [17] or the Peer Prediction Method [14]. How-ever, we are not aware of empirical studies that evaluate these mechanisms in real online commun ities. In particular, there have been no systematic evaluations to which extent incentive mecha-nisms can increase the quality of data and stimulate contributions to a base of structured knowledge. Quantifying the effect of rat-ings on the quality of contributi ons requires an objective assess-ment of the quality. Such an assessment is not obvious. ance, we have developed a platform for the peer production of structured knowledge. Its design allo ws for fine-grained ratings of the semantic structures while the underlying incentive mechanism can easily be exchanged. This lets us investigate to which extent incentive mechanisms stimulate contributions and ratings increase the quality of data. To compare different settings, we had to con-trol the usage of ratings and incentive mechanisms. We do not see how we could have achieved this by studying existing online communities solely based on, say, th e analysis of log files. More specifically, we make the following contributions:  X  Development of a platform for the peer production of  X  Evaluation of ratings and incentive mechanisms in an  X  Experimental results. Over four weeks, 120 members of an  X  Lessons learned. We have experienced various difficulties Paper outline: Section 2 discusses related work. Section 3 presents our collaboration platform. Section 4 motivates rating-based incentive mechanisms. Section 5 presents the empirical study and discusses experimental resu lts. Section 6 concludes. Different tools for the collaborative creation of structured knowl-edge have been proposed. They range from full-fledged ontology editors enhanced with collaborativ e features [18], [19] over wiki-based approaches [1] for semantic data to tools that facilitate tag-ging folksonomies to create lightweight ontologies [13], [20]. There also are commercial tools for the collaborative creation of structured knowledge, notably Freebase (www.freebase.com). Some of these tools feature rating mechanisms. However, we are not aware of any systematic attempts to measure the effect of rat-ings on the quality of the metadata created. group investigates contribution behavior in online communities. They have conducted many of th eir experiments in an online community for movie recommendations called MovieLens (movielens.umn.edu). To study the question how to elicit contributions in an online community, they have deployed theories from social science. Ho wever, none of their publications investigates incentives to rate the quality of contributions. proaches have been proposed. Bayesian Truth Serum [17] deals with scoring systems for eval uating individual and collective judgment in domains where no exte rnal truth criterion is avail-able. The scoring system rewards honest judgments even if they do not represent the majority opinion. Peer Prediction [14] deals with online product ratings and is described in Section 4. Both approaches develop incentive mech anisms based on scoring rules. The mechanisms make announcing the truthful (subjective) opin-ion a Nash equilibrium, instead of favoring the consensus answer. Neither approach has been subject to an empirical evaluation in the context of structured knowledge. This section describes our tool Consensus 2.0 which allows for the collaborative creation of structured knowledge. We have inte-grated the tool into an existing online community for students of our department. Consensus 2.0 allows modeling the contents of our lectures as semantic structures. The structures should be of help for students preparing for their exams or consulting the course material later in their studies or in their professional career. well-suited for direct manipulation by humans, as opposed to standards like RDF or OWL that are designed for automatic proc-essing. Anyhow, the choice of the data format is of minor impor-tance. The functionality of Consensus 2.0 is applicable to other data formats for structured knowledge as well. and occurrences. Topics represent subjects or entities of the real world, e.g., Professor and Lecture . A topic can have one or more types that are topics themselves, e.g., Professor is of type Instructor . Associations describe relationships between association, each playing a certain role, e.g., Professor &lt;reads&gt; Lecture . A topic can have one or more occurrences that represent links to external web resources or documents, e.g., to the personal website of Professor . For a detailed descrip-tion of Topic Maps, we refer to [16]. and maintain Topic Maps collabora tively. First, the tool supports the visualization of Topic Maps and allows browsing topics and associations . Us ers can s earch for topics and create new topics and associations. Second, topics are detailed on individual pages (cf. Figure 1), which display all data relevant for the current topic. Besides the creator and version of the topic, the details page con-tains a description of the topic. Furtherm ore, it s hows the as socia-tions where the current topic play s a role, as well as its attributes , types, and occurrences. In our context, occurrences link to course material s uch as lecture s lides . Third, a us er can view pers onal sta tistic s suc h as his sc ore , the sc ores of other participants, topics nism, which we describe in the ne xt section, com putes the s cores . Topic Map. Cosley et al. show that even though users prefer a finer-grained rating s cale, the granularity of the rating scale has no adverse effect on user behavior [6] . Therefore, we have decided for the well-known five-star rating scale, which is widely used by online com munities such as Am azon or Epinions. Cosley avoid this effect, the cons ens us rating of a concept is not shown until the user rates the concept him self. For this purpose, we used a rating panel that shows grey stars while the user has not rated a concept. As soon as the user rate s the concept, a bar that shows the average rating as well as the num ber of ratings replaces the rating panel. actions in the Topic Map with points. The design of these incen-we use ratings to compute rem unerations for good contributions. We refer to the remuneration for a good contribution as commis-sion . A user obtains a commission if other users rate his contribu-tion favorably . A linear function dependent on the value of a rating determ ines the com mission. F or exam ple, us er Alice creates as sociation X, and us er Bob gives X a four-star rating, then Alice obtains a com mission of .75 points. If the rating was three stars instead of four, the commission would be .5 points ins tead. the tool into the portal of an existing online com munity community contains pages for th e lectures of our department. Students who attend those lectures can join the com munity . Each page contains a variety of tool s for course management and col-courses, check recent announcem ents, download lecture m aterial, and chat using a discussion form . To achieve a s eam les s integra-tion of the Topic Maps tool Consensus 2.0 into the online com -munity , we have facilitated navigation between the Topic Map and the other tools of the community . For example, users can link lecture s lides as occurrences of topics . F ollowing the general ap-proach, us ers can als o rate th e validity of thes e occurrences . Since ratings are critical, we want to elicit high-quality ratings as opposed to uninformed ones or ratings that copy the majority opinion. (This does not exclude the majority opinion from being the right one.) To gain such ratings one has to overcome two challenges: under contribution and honesty . Rating data items is costly . It requires the user to i nvest time and effort to understand the rating scale and to consciously select the appropriate value for ratings can overcome this obstacl e. However, a simple reward, i.e., one remuneration unit per ra ting, does not suffice. This is becaus e it does not provide any incentive for the rater to gather information before issuing his rating and to respond truthfully . classes20.ipd.uka.de that rewards truthful ratings. In particular, we have chosen the Peer Prediction (PP) method [14] by Miller et al. PP has been designed for online product ratings where buy ers perceive a noisy signal about the quality of a product and rate the product accord-ing to their perception. PP uses a central processing facility sim -and com putes a s core for each rating. noisy perception of the product X  X  quality as his signal S quality of the product rem ains fixed and defines the product type. PP assumes a finite number of product types t . Let S = { s be the set of possible signals, and let Pr( s j | t ) = Pr( S probability that a buy er receives signal j when the true ty pe of the product is t . Pr(.| .) is assumed to be common knowledge, and  X  center as ks them to s ubm it ratings according to their s ignals si-multaneously . Subsequently , the center s cores every rating sub-mitted by com paring it with another rating called the reference rating. The buy ers receive a pay ment which is proportional to their s core. Let r i  X  S denote the rating subm itted by buy er i , and let r ref(i)  X  S denote the rating subm itted by the reference rater ref( i)  X  i . To com pute the pay ment  X  ( r i , r mechanism uses a proper scoring rule R(.| .): proper if buy er i receives his m axim al expected score if and only if he reports his signal received truthfully , i.e., r scoring rules studied extensively are:  X  Logarithmic scoring rule:  X  Quadratic scoring rule: detailed discussion of scoring rules see [5] . pay ment expected by buy er i is the weighted sum of the pay ments over all possible values of the reference rating: R( r ref(i) |r i ), the expected pay ment of a rater is uniquely maxim ized by reporting the truth. That m akes truth telling a Nash equilibrium of the PP m echanism : if every other rater is telling the truth, i can only maximize his pay -off by telling the truth as well. dicts the reference rater X  X  rating. W ith each new rating, the center updates the distributions needed to predict the reference rater X  X  reference rating relative to the updated distribution. If the refer-ence raters X  rating is honest, every rater can maximize his ex-pected pay ment by announcing his subjective beliefs. Note that the remuneration for giving ratings is a countermeasure against the under-contribution of ratings. Since every positive affine transformation of a strictly proper scoring rule is also a strictly proper s coring rule, the center can s cale the pay ments . T hereby it can induce raters to exert effort. PP m echanism . Other equilibria like rating alway s high or rating alway s low exist as well. Clearly , these equilibria are not wanted. [14] outlines som e counterm easures in order to prevent the system from reaching one of these equilibria, but without any evaluation. such countermeasures. This question is an interesting one which our experim ents will address as well. im plem entation of Peer Prediction. F or each concept of the Topic Map, we maintain two probability distributions: the prior distribu-tion of the ty pes Pr( t ) and the signal distribution Pr( s modeled Pr( t ) as initially uniform ly distributed. We update Pr( t ) with the new ratings subm itted. W e have m odeled Pr( s multinomial distributions. Thus, we assume the perception of the individual ty pes to follow a binomial distribution with a mean of their res pective value of t . We assume the ty pes and signals to be possible X , and 2, 3 and 4 represent the values in between. subm itted their rating and scores them sim ultaneously . This pro-cedure is im practical in our s cenario, s ince we prefer to provide users with scores as soon as possi ble. Instead, we use an extension of the PP method, which allows for sequential ratings. That is, instead of computing the scores for ratings after all users have subm itted their rating, we put subs equent ratings into groups of size three and com pute scores w ithin that group. Miller et al. show that this procedure yields the sam e results as the original PP method for a minimal group size of three [14] . We determine the reference ratings required to co mpute the scores by a random permutation of the ratings with in a group. After computing the scores for a group, we update the prior distributions Pr( t ) with the new ratings from that group. had to guarantee non-negative rem unerations. In order to avoid the infinite range of a logarithmic scoring rule, we use a quadratic scoring rule with a finite range. The previous sections have de scribed our platform for the collaborative creation of Topic M aps. It allows us ers to create concepts such as topics , as sociations , and occurrences cooperatively . We use ratings and incentive mechanisms to motivate users and to ensure the quality of the modeled data. To evaluate the effectiveness of our approach, we have conducted an extensive experimental study in an online community . Before designing the experiment, we ha ve formulated a number of falsifiable hy potheses to be tested. Rating mechanisms are alway s based on the assumption that indi-vidual ratings can be aggregated to a meaningful measure of qual-ity . Before aggregating ratings, we want to make sure that ratings are indeed suitable to m easure th e quality of contributions. Thus, we formulate the first hy pothesis: contributions. Cosley et al. [7] have conducted an experimental study in an online community where participants reviewed the contributions of other participants. Their results show that this reviewing pro-cess improves the quality of contributions. We hypothesize that the same effect occurs when the explicit reviewing of contribu-tions is replaced by the broader concept of rating mechanisms. Thus, our second hypothesis states that the usage of rating mecha-nisms has a positive impact on the quality of contributions: (H2) The usage of rating mechanisms increases the quality of contributions. Many online communities use extr insic incentives to motivate their members to contribute to the community. Some communities like Epinions reward users not only for their time and effort, but also for the quality of their work. We use the consensus rating as measure of the quality of a contribution and reward users for creating high-quality contributions. In order to test whether re-wards actually increase the quality of contributions, we formulate the third hypothesis: (H3) Commissions for good contributions increase the quality of contributions. To evaluate these hypotheses, we have designed and conducted an experiment which we desc ribe in the following. In the present study, we have examined the influence of incentives and rating mechanisms on the quality of the contributions of an online community. To this end, we have varied both the usage of ra ting mechanisms and of the commissions for the creation of good contributions. These two factors are the independent variables :  X  Usage of a rating mechanism  X  Commissions for good contributions We have analyzed two factors (rating mechanism and commis-sions) with two stages each (ratings vs. no ratings and commis-sions vs. no commissions), resulti ng in a 2x2-factorial design. Note that commissions and ratings are not orthogonal to each other  X  since commissions depend on ratings, this second factor requires the presence of a rating mechanism. 120 members, of which 60 people visited the lectures regularly. With an estimated participation of 50% of the regular visitors, we had anticipated about 30 active members. Dividing those 30 peo-ple in four experimental groups would have resulted in rather small groups, so we have opted for only two groups. We divided the groups randomly and without the knowledge of the partici-pants. In the following, we call the group in which we varied the independent variables the experimental group (EG). The control group (CG) in turn has served as the baseline for the experiment. experiment into two phases. In th e first phase, we varied the vari-able  X  X sage of a rating mechanism X  to analyze its effect on the quality of contributions. In the second phase, we manipulated the variable  X  X ommissions for good c ontributions X  to test whether rewards increase the quality of contributions. Only one factor could be manipulated in each phase, so both groups used a rating mechanism in the second phase. Table 1 summarizes the experi-mental design. In order to test the hypotheses, we had to measure the effects of manipulating the independent variables on the dependent ones. Since we could not measure some of the dependent variables di-rectly during the experiment, we conducted an online question-naire after the completion of the experiment. The present study focuses on the following dependent variables :  X  Number of contributions  X  Quality of contributions  X  Number of ratings  X  Rating averages  X  Anti-social behavior of community members  X  Usability of the tool Students who attended the lectures of our department were invited to sign up for the online community and to create an anonymous account. Anonymous accounts raise the problem of Sybil attacks [9] where users forge multiple identities to gain larger influence. To counter those attacks, we applied validation techniques to make sure that each person created only one account. For this purpose, we distributed activa tion keys among the students and manually ensured that each person obtained at most one key. This activation key was prompted during the creation of an account and synchronized with the database. Deployment X  (DBE) as a Topic Ma p and, in the second phase, the lecture  X  X ata Warehousing and Mi ning X  (DWM). The students of both lectures overlap almost completely, so all participants had the same background knowledge. Both groups created their own Topic Map in each phase, resulti ng in four independent Topic Maps. At the beginning of the expe riment, we gave a short intro-duction to Topic Maps and demons trated our tool. Between the first and the second phase, we inst ructed students how to use rat-ing mechanisms. In particular, we told the students that honest ratings maximize their expected scores and defined the following criteria for the quality of contributions:  X  Correctness of the contributions  X  Adequacy in the context of the lectures  X  Usefulness for exam preparation To test the hypotheses that ratings and incentive mechanisms in-crease the quality of contributions (H2 and H3), we had to find a way to assess the contributions obj ectively. For this purpose, we recruited two teams of domain experts, one for each experimental group. Each expert team consisted of three academic staff mem-bers of our department who inde pendently rated concepts of the Topic Maps. We instructed the experts in the same way as the participants of the experiment. That is, we gave them the same instructions regarding Topic Maps and rating criteria, and they used the same user interface. We randomly assigned expert teams to experimental groups, so the experts did not know which group limited time, they rated a random sample of 100 contributions for each phas e. In order to aggregate the expert ratings , all experts of a team us ed the s ame sample. the Topic Maps. The rewards in the form of points should be pro-portional to the effort induced by the individual contributions to the com munity . W e identified the rating of an elem ent as the smallest contribution possible and set the respective maxim al reward to one point. As described in Section 4, rewards for ratings vary depending on the likelihood of the ratings. We normalized rating an elem ent, we es tim ated the extent of the other actions in relative term s and defined their rewards accordingly . Topic Maps. duct a lottery of six exam bonuses after the completion of the experim ent. The intention behind the lottery was to m otivate par-ticipants with low scores as well. The high scores of other partici-pants would probably deter those participants if only the top k members received a reward. The points gathered s erved as num ber of lots for the lottery . Therefore, the chance of winning an exam bonus was proportional to the score in the experiment. Since members of the control group could not reach as m any points as the ones of the expe rimental group, we actually conducted two drawings of three bonuses each. 
Figure 2. Scores of active users in th e exp erimen tal grou ps The whole experim ent was built upon the collaboration tool Con-sensus 2.0 , so we firs t checked whether it is suitable for the col-laborative creation of structured knowledge. Table 3 illustrates the number of contributions to the individual Topic Maps. While the number of topics created in the first phase is about the same for both groups, the experimental group created more topics than the control group in the second phase. Regarding associations, the control group was considerably more active in the first phase, whereas the experimental group created more associations in the second phase. In terms of occurren ces, the situation was just the other way around. Note that the total number of contributions over both phases was about the sa me for both groups. Overall, 32 participants actively used the tool and have created over 3500 ontology concepts. Thus, we deem Consensus 2.0 suitable for the collaborative creation of structured knowledge. Regarding scores, the control group was more active in the first phase while the experimental gr oup was more active in the second phase. Note that the experimental group could gather more points than the control group. This is because the experim ental group was additionally rewarded for hone st ratings and for the creation of good contributions. Figure 2 illustrates the scores of users who gathered more than 100 points in the control and experimental group. The diagram of the control group shows that a few mem-bers were extremely involved and gathered significantly more points than their colleagues. This observation also applies to the experimental group, but to a lower extent. Recall that our goal was to com pare the quality of contributions between the experimental and the control group. For this purpose, we com pared the average ratings of the dom ain expert for the respective topic m aps of each group. In order to com pute reliable averages of the individual expert ratings , a significant agreem ent between the experts of each team is neces sary. Cohen X  X  kappa measure [10] is a statistical method to measure the agreement between raters. A value of less than zero stands for only random agreem ent, while a value of one m eans perfect agreem ent. Table 4 shows the kappa measure for both team s of experts . The achieved values of .14 to .27 indicate only a low agreem ent between the raters. However, higher values c ould not be expected when using five rating categories [4] . W e com puted the critical values z in order to determine the significan ce level of the Kappa m eas ure. The values z &gt; 3.0 show that the agreement was highly significant ( p &lt; .01). Becaus e of the s ignifi cant agreem ent between the ex-perts of both teams, we deem the expert ratings an objective measure of the quality of contributions. In order to analyze whether ratings are suitable to measure the quality of contributions, we compared user ratings with expert ratings. Since the ratings of each contribution are averaged, as many ratings as possible are re quired to generalize from the sub-jective opinions of individual member s. To have at least as many user ratings as expert ratings, we considered only contributions with at least three ratings from different users. An analysis of the data shows that there is a medium correlation ( r = .46, p &lt; .01) between the average user and expert ratings. It is worth mention-ing that the average of the user ratings ( M = .66) differs consid-erably from the average of the expert ratings ( M = .56). In other words, the experts were much tougher than the users regarding the ratings. Because of the significant correlation between user and expert ratings, the first hypothesis is supported: contributions. After the kappa measure had been computed, the experts dis-cussed the controversial ratings with a standard deviation SD &gt; .25 in a final meeting. The result was a consensus between the experts on the ratings of each concept in the random sample. Table 5 illustrates the ratings of the contributions after both teams of experts agreed on their ratings. Although the standard deviation is relatively similar in all four conditions, there are considerable differences between the average ra tings of both groups. In the first phase, the contributions of the experimental group were rated This is evidence that the manipulation of the experimental group has increased contribution quality. Thus, H2 is supported: (H2) The usage of rating mechanisms increases the quality of contributions. In the second phase, the difference between the expert ratings of the experimental group and the control group was not significant ( p &gt; .10). Even more, the difference between the ratings of the experimental group in Phase 1 and the control group in Phase 2 (presumably due to the learning effect) was higher than the differ-ence between the two groups in Phase 2 (because of the manipu-lation). Thus, the third hypothesis was not supported: (H3) Commissions for good contributions increase the quality of contributions. phase, only members of the experimental group could give ratings since the control group did not use a rating mechanism. The divi-sion into experimental groups was performed without the knowl-edge of the participants, so we could not instruct students how to use rating mechanisms publicly in the lectures. This might be the reason for the low number of ratings ( N = 119) in the first phase of the experiment. Despite of the rare usage of the rating mecha-nism, the experts assessed the T opic Map of the experimental group as better than the Topic Map of the control group. That is, solely the presence of a rating mechanism improves the quality of contributions. This observation is in line with the results of Cosley et al. [7] that oversight improves the quality of contribu-tions. Thus, the explicit reviewing of contributions can actually be generalized to the broader concept of rating mechanisms. their ratings. When giving ratings , members of the experimental group had to consider that they grant points to other members  X  and possible competitors  X  by rating their contributions. An analysis of the results shows that members of the experimental group were much more critical ( M = .59, SD = .39) with their ratings than members of the control group ( M = .72, SD = .32). This fact conflicts with the ratings by the experts who assessed the Topic Map of the experimental group as better than the one of the control group. Apparently, th e experimental group was biased towards more critical ratings and displayed a more competitive behavior compared to the control group. After the completion of the experiment, we invited the partici-pants of the experiment to take part in a questionnaire. This ques-tionnaire contained 30 questions re garding rewards, ratings, rating mechanisms, and the usability of Consensus 2.0 . The participants could choose between five possible answers from the interval  X  X trongly disagree X  to  X  X trongly agree X . Participants received an additional 200 points for completing the questionnaire. of exam bonuses after the comple tion of the experiment. The par-ticipants stated that they were highly motivated by this kind of reward and that they consider rewards in general an adequate means to stimulate contributions in an online community. Inter-estingly, there was no correlation between the motivation of the participants as stated in the questionnaire and the score reached in the experiment ( r = .05, p &gt; .10). The participants were also asked if they consider the lottery of re wards as fair, or if they would have preferred deterministic rewards for the top k participants. There was no correlation between the fairness of the lottery and the desire for deterministic rewards ( r = -.11, p &gt; .10). However, participants with higher scores d eem the lottery less fair and show a stronger desire for deterministic rewards. A combination of a lottery and deterministic reward s would probably appeal to both risk averse and risk tolerant pa rticipants and will be our first choice in future experiments. pants pursued when giving ratings. The vast majority of the par-ticipants stated that they have rated truthfully and have not joined the consensus rating. There was a strong negative correlation be-tween rating truthfully and joining the consensus rating ( r = -.59, p &lt; .01). That is, the participants considered truthful and consen-sus-oriented ratings as mutually exclusive. Furthermore, there was a group difference regarding truthful ratings. Members of the experimental group rather stated that they had rated truthfully, compared to members of the control group. This is inconsistent with the data measured during th e experiment: the experimental group assigned lower ratings during the second phase, even though the quality of their Topic Map was assessed as better by the experts. Members of the expe rimental group also reported that they tried to keep the scores of their fellow members low and assessed the community as more competitive than members of the control group. This confirms our observation that there is a con-flict between truthful ratings and commissions for good contribu-tions. We discuss possible impli cations in the next section. the tool. The majority of the pa rticipants found the tool intuitive and easy to use. As expected, pa rticipants with higher scores deemed the tool more intuitive and coped better with the creation of Topic Maps. This might indicate a steep learning curve for our tool. Finally, the majority of the pa rticipants stated that they plan to use the ontologies built for the preparation of their exams. In this section, we discuss the e xperimental results and present the lessons learned while conducting the experiment. Overall, the hypotheses which we have formul ated at the beginning of this section were analyzed empirically. The data measured supported Hypotheses (H1) and (H2) ; Hypothesis (H3) could not be vali-dated with the current experiment . This is consis tent with the experimental design since all hy potheses build upon each other. If ratings were not suitable to measure the quality of contributions, it would not be possible to make statements about quality improve-ments. Furthermore, rewards for the creation of good contribu-tions are dependent on the effectiveness of the rating mechanism since we assessed the quality of contributions using ratings. pants randomly into two groups, to manipulate the independent variables. Because some participants who knew each other in real life were assigned to different groups, the division into groups could not really be kept a secret. When participants asked why their friends could rate contribu tions while they could not, we claimed that only a couple of par ticipants used rating mechanisms during the first phase in order to calibrate system parameters. We could not explain the usage of ra ting mechanisms in the lectures before the experiment since pa rticipants from both groups were present. Hence, we had to use private messages to instruct partici-pants. The low number of ratings in the first phase might indicate that instructions should rather be announced publicly. create a large portion of the content and contribute considerably to the value of the community. T hus, it is important to tie those outstanding members to the community by offering them appro-priate rewards. In future experiments we plan to combine determi-nistic rewards with a lottery by rewarding the k members with the highest score deterministically and draw l additional rewards for the other members by lot. In doing so, both the most active mem-bers are rewarded while the other members still have the chance of receiving a reward. That is, such an incentive might attract both risk averse and risk neutral participants. crease the quality of contributions. Since the commissions were based on ratings, there was a conflict between truthful feedback and commissions for good contributi ons. Honest rating has been shown to be a Nash equilibrium [11] in the Peer-Prediction method. Similarly, a Nash equilibrium exists when the majority of users agrees on rating all contribu tions as good or bad. This case might occur when members are rewarded based on the ratings of their contributions. To avoid giving points to possible competi-tors, it might be conceivable that users  X  tacitly or explicitly  X  agree on rating all contributions as bad, while still being rewarded by the Peer-Prediction method. Howe ver, an analysis of the data shows that no user has issued bad ratings only. Thus, the unfavor-able lying equilibria of Peer Prediction did not occur here. ratings is three. That is, the incentive mechanism needs at least three ratings to compute scores. To gather at least three ratings for every item of the Topic Map has turned out to be difficult and is unlikely to take place without a dditional measures. Compared to other contributions, the ratings were sparse, and thus many ratings remained unscored. In the work at hand, we have investigated the peer production of structured knowledge as a means to create semantic structures collaboratively. We have proposed the usage of ratings and in-centive mechanisms to stimulate contributions and increase the quality of data. To evaluate our approach, we have developed a platform for the collaborative creation of structured knowledge. Using this platform, we conducte d an extensive experimental study to analyze the influence of ratings and incentive mecha-nisms on the contributions of a real online community. productive tool that 118 participants used to create over 3500 ontology concepts in the course of the experiment. Students of our department still use this tool to model the content of the lectures. The results of the experimental st udy show that our platform is suitable for the collaborative creation of semantic structures. Rat-ings prove to be a reliable measure for the quality of contributions in an online community. Furthermore, the usage of rating mecha-nisms increases the quality of cont ributions. However, an increase in the quality of contributions because of commissions for good contributions could not be show n. We will conduct further ex-periments to reinvestigate this hypothesis. online communities. If the intrinsic motivation is not sufficient to motivate users to contribute, extrinsic incentives can be provided. Such incentives are important to tie committed and thus valuable members to the community. Our experimental study confirms that rewards are a potential means to motivate users to contribute to an online community and create high-quality data. members of an online community based on the ratings of contri-butions. Collaborative filtering algorithms can identify members with similar interests. We will also investigate how to compute personalized ratings for individual contributions to point users to interesting contributions. Finally, we plan to address the issue of sparse ratings by introducing an adaptable rating granularity. ment and helpful feedback. We al so thank the staff members of the Institute for Program Structures and Data Organization (IPD) for supporting us in the evaluation of the experiment. [1] Auer, S., Dietzold, S., and Riechert, T. 2006. OntoWiki -A [2] Beenen, G., Ling, K., Wang, X ., Chang, K., Frankowski, D., [3] Benkler, Y. 2006. The Wealth of Networks . How Social Pro-[4] Cohen, J. 1968. Weighted Kappa: Nominal Scale Agreement [5] Cooke, R. M. 1991. Experts in Uncertainty : Opinion and [6] Cosley, D., Lam, S. K., Albert, I., Konstan, J. A., and Riedl, [7] Cosley, D., Frankowski, D., Ki esler, S., Terveen, L., and [8] Counterman, C., Golden, G., Gollub, R., Norton, M., Sever-[9] Douceur, J. 2002. The Sybil Attack. In Proceedings of the [10] Fleiss, J. 1981. Statistical Methods for Rates and Propor-[11] Fudenberg, D. and Tirole, J. 1991. Game theory . MIT Press, [12] Heckhausen, H. 1989. Motivation und Handeln . Second [13] Hotho, A., J X schke, R., Schmitz, C., and Stumme, G. 2006. [14] Miller, N., Resnick, P., and Zeckhauser, R. 2005. Eliciting [15] Noy, N. F., Chugh, A., and Alani, H. 2008. The CKC Chal-[16] Pepper, S. 2000. The TAO of Topic Maps -Finding the Way [17] Prelec, D. 2004. A Bayesian Truth Serum for Subjective [18] Sunagawa, E., Kozaki, K., Kitamura, Y., and Mizoguchi, R. [19] Tudorache, T. and Noy, N. 2007. Collaborative Prot X g X . In [20] Zacharias, V. and Braun, S. 2007. SOBOLEO -Social Book-
