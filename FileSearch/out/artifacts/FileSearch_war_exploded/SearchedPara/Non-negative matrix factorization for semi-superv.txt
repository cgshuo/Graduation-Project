 Yanhua Chen  X  Manjeet Rege  X  Ming Dong  X  Jing Hua Abstract Traditional clustering algorithms are inapplicable to many real-world problems where limited knowledge from domain experts is available. Incorporating the domain knowl-edge can guide a clustering algorithm, consequently improving the quality of clustering. In this paper, we propose SS-NMF: a semi-supervised non-negative matrix factorization frame-work for data clustering. In SS-NMF, users are able to provide supervision for clustering in terms of pairwise constraints on a few data objects specifying whether they  X  X ust X  or  X  X annot X  be clustered together. Through an iterative algorithm, we perform symmetric tri-factorization of the data similarity matrix to infer the clusters. Theoretically, we show the correctness and convergence of SS-NMF. Moveover, we show that SS-NMF provides a gen-eral framework for semi-supervised clustering. Existing approaches can be considered as special cases of it. Through extensive experiments conducted on publicly available datasets, we demonstrate the superior performance of SS-NMF for clustering.
 Keywords Non-negative matrix factorization  X  Semi-supervised clustering  X  Pairwise constraint 1 Introduction Clustering or unsupervised learning is a generic name for a variety of procedures designed to find natural groupings, or clusters, in multidimensional data, based on measured or perceived information from unlabeled data. Applications of data clustering are found in many fields, such as information discovery, text mining, web analysis, image grouping, medical diagnosis, and bioinformatics. In general, the clustering algorithms can be categorized into two popular techniques: hierarchical clustering and partitional clustering.

Hierarchical clustering [ 33 , 35 ] aims to obtain a hierarchy of clusters, called dendrogram, that shows how the clusters are related to each other. The clustering result can be obtained by cutting the dendrogram at a desired level. Amongst these the agglomerative methods create the cluster dendrogram in a bottom-up fashion, starting with each data object (or sample) in its own cluster and merging clusters successively according to a similarity measure till a cluster dendrogram in a top-down divisive fashion, where all the data objects initially belong to a single cluster to begin with. This cluster is then split successively according to some measurement till a convergence criterion is reached [ 6 , 12 , 17 ].

Partitioning methods divide the data in a given number of clusters directly and are typ-ically used more frequently in real-world applications. These methods attempt to obtain a partition which minimizes the within-cluster scatter or maximizes the between-cluster scat-ter. Amongst these density-based algorithms model clusters as dense regions, use different heuristics to find arbitrary-shaped high-density regions in the input data space and group points accordingly. Well-known methods include Denclue, which tries to analytically model the overall density around a data object [ 24 ], and WaveCluster, which uses wavelet-transform to find high-density regions [ 48 ]. Density-based methods typically have difficulty scaling up to very high dimensional data. Mixture-based methods assume that the data objects in a clus-ter are drawn from one of several distributions (usually Gaussian) and attempt to estimate the parameters of all these distributions. The introduction of the expectation maximization (EM) algorithm in Dempster et al. [ 9 ] was an important step in solving the parameter estimation problem. Mixture-resolving methods have a high computational complexity and make rather strong assumptions regarding the distribution of the data. Most mixture-based methods view example, the k -means algorithm [ 44 ] assumes every cluster has a compact shape. Since the actual underlying distribution of the data can be different, these methods are susceptible to their a priori assumptions.

Clustering based on spectral graph partitioning has emerged as a popular method over the data objects as vertices of a weighted graph with edge weights representing the similarity between two data objects. Clustering is then obtained by  X  X utting X  the graph vertices into different partitions. Partitioning of the graph is obtained by solving an eigenvalue problem where the clustering is inferred from the top eigenvectors. Although, all of the above methods have contributed greatly to the problem of data clustering, they are completely unsupervised. That is, they are inapplicable to many real-world problems where limited knowledge from algorithm, consequently improving the quality of clustering.

Semi-supervised clustering uses class labels or pairwise constraints on data objects to aid initial labeled data as well as unlabeled data in order to modify the existing set of categories available to a semi-supervised clustering method: the similarity distance measurement in unsupervised clustering and class labels or some pairwise constraints. For semi-supervised clustering to be profitable, these two sources of information should not completely contra-dict each other. Existing methods for semi-supervised clustering based on source information generally fall into two categories: distance-based and constraint-based methods. In distance-based approaches, an existing clustering algorithm that uses a distance measure is employed; supervised data [ 31 , 52 ]. In constraint-based approaches, the clustering algorithm itself is to combine the constraint-based with distance-based approaches.

In this paper, we propose a non-negative matrix factorization (NMF) [ 37 , 38 ] based framework to incorporate prior knowledge into data clustering. Under the proposed Semi-Supervised NMF (SS-NMF) methodology, user is able to provide pairwise constraints on a few data objects specifying whether they  X  X ust X  or  X  X annot X  be clustered together. We derive an iterative algorithm to perform symmetric non-negative tri-factorization of the data simi-larity matrix. The correctness and convergence of the algorithm are proved by showing that the solution satisfied the KKT optimality and the algorithm is guaranteed to converge. We also prove that SS-NMF is a general and unified framework for semi-supervised clustering by establishing the relationship between SS-NMF and other existing semi-supervised clustering algorithms. Experiments performed on various publicly available datasets demonstrate the superior performance of the proposed work.

The rest of the paper is organized as follows. We review related work on semi-supervised theoretical results are presented in Sect. 3 . Experimental results appear in Sect. 4 . Finally, we conclude in Sect. 5 . 2 Related work In this section, we provide a review of related works on using user provided information to improve data clustering. We first discuss some algorithms in which prior knowledge is in the form of labeled data. Next, we describe other algorithms for which pairwise constraints are required to be known a priori.

SS-constrained-Kmeans [ 51 ] and SS-seeded-Kmeans [ 3 ] are the two well-known algo-rithms in semi-supervised clustering with labels. The SS-constrained-Kmeans seeds the k -means algorithm with the given labeled data and keeps that labeling unchanged through-out the algorithm. Moreover, it is appropriate when the initial seed labeling is noise-free, or if the user does not want the labels of the seed data to change. On the other hand, the SS-seeded-Kmeans algorithm changes the given labeling of the seed data during the course the seed labels to remain unchanged during the clustering iterations and can therefore aban-don noisy seed labels after the initialization step. Semi-supervised clustering with labels has been successfully applied to the problem of document clustering. Hotho et al. [ 25 ] proposed incorporating background knowledge into document clustering by enriching the text features using WordNet. 1 In Jones et al. [ 30 ], some words per class and a class hierarchy were sought from the user in order to generate labels and build an initial text classifier for the class. A similar technique was proposed in Liu et al. [ 41 ], where the user is made to select inter-esting words from automatically selected representative words for each class of documents. These user identified words were then used to re-train the text classifier. Active learning approaches have also found applications in semi-supervised clustering. Godbole et al. [ 18 ] has proposed to convert a user recommended feature into a mini-document which is then used to train an SVM classifier. This approach has been extended by Raghavan et al. [ 47 ]which adjusts SVM weights of the key features to a predefined value in binary classification tasks. Recently, Huang and Mitchell [ 26 ] presented a probabilistic generative model to incorporate extended feedback that allows the user and the algorithm to jointly arrive at coherent clusters and Joachims [ 29 ] proposed methods where the user provided class labels a priori to some of the documents. These algorithms use the labeled data to generate seed clusters that ini-tialize a clustering algorithm, and use constraints generated from the labeled data to guide the clustering process. Proper seeding biases clustering towards a good region of the search space, while simultaneously producing a clustering similar to the specified labels.
However, in certain applications, supervision in the form of class labels may be unavail-able. For example, complete class labels may be unknown in the context of clustering for speaker identification in a conversation [ 2 ], or clustering GPS data for lane-finding [ 51 ]. In some domains, pairwise constraints occur naturally, e.g., the database of interacting pro-teins (DIP) dataset contains information about proteins co-occurring in processes, which can be viewed as must-link constraints during clustering. Similarly, for document clustering, user knowledge about which few documents are related or unrelated can be incorporated to improve the clustering results. Moreover, it is easier for a user to provide feedback in the form of pairwise constraints than class labels, since providing constraints does not require the user to have significant prior knowledge about the categories in the dataset. Amongst the various methods proposed for utilizing user provided constraints for semi-supervised clus-tering [ 3 , 4 ], two of the well known include the semi-supervised kernel k -means (SS-KK) [ 36 ] and semi-supervised spectral clustering with normalized cuts (SS-SNC) [ 28 ]. While, SS-KK transforms the clustering distance measure by weighted kernel k -means with reward and penalty constraints to perform semi-supervised clustering of data given either as vectors or as a graph, SS-SNC utilizes supervision to change the clustering distance measure with pairwise information by spectral methods. The SS-NMF framework presented in this paper, allows the user to provide pairwise constraints on a small percentage of the data points. Specifically, these constraints specify whether the two data points should belong to the same cluster or should strictly belong to different clusters. 3 Semi-supervised non-negative matrix factorization for clustering In this section, we first formulate the SS-NMF model in Sect. 3.1 and derive it in Sect. 3.2 .We prove the correctness and convergence of the algorithm in Sect. 3.3 . Equivalence of SS-NMF to SS-KK and SS-SNC is proven in Sect. 3.4 , followed by a discussion of advantages of SS-NMF in Sect. 3.5 . 3.1 Model formulation We assume the data consists of n objects, and that m features have been extracted from each of the objects. Correspondingly, the data can be represented using a matrix X  X  R m  X  n where columns index the data objects to be clustered and rows denote the features. An entry x fi in this matrix denotes the value of feature f for object i .

We propose a SS-NMF model for data clustering. NMF has received much attention recently and proved to be very useful for applications such as face recognition, text min-ing, multimedia analysis, and DNA gene expression grouping. It was initially proposed for  X  X arts-of-whole X  decomposition [ 37 , 38 ], and later extended to a general framework for data clustering [ 10 ]. It can model widely varying data distributions and accomplish both hard and soft clustering simultaneously. When applied to the data matrix X , NMF factorizes it into two non-negative matrices [ 53 ], where P  X  R m  X  k is cluster centroid, Q  X  R n  X  k is cluster indicator, and k is the number of clusters.

In the proposed model, we perform symmetric non-negative tri-factorization of the simi-larity matrix A = X T X  X  R n  X  n as, association of object x i with cluster h . The cluster membership of an object is given by finding the cluster with the maximum association value. S  X  R k  X  k is the cluster centroid matrix that gives a compact k  X  k representation of X .

Supervision is provided as two sets of pairwise constraints on the data objects: must-link that the two objects should belong to different clusters. The constraints are accompanied by associated violation cost matrix W .Anentry w ij in this matrix denotes the cost of violating ( x , x j )  X  C CL . The model relies on a distortion measure D : R m  X  R , to compute distance cluster representatives is (locally) minimized according to the given distortion measure D , while constraint violations are kept to a minimum. 3.2 Algorithm derivation We define the objective function of SS-NMF as follows: { w x . S  X  R k  X  k is the cluster centroid, and G  X  R n  X  k is the cluster indicator.
We propose an iterative procedure for the minimization of Eq. ( 3 ) where we update one factor while fixing the others. The updating rules are, Thus, the SS-NMF algorithm for document clustering can be illustrated in Algorithm 1. 3.3 Algorithm correctness and convergence We now prove the theoretical correctness and convergence of SS-NMF. Motivated by Long function and several matrix inequalities. Algorithm 1 SS-NMF Algorithm 3.3.1 Correctness First, we prove the correctness of the algorithm, which can be stated as, Proposition 1 If the solution converges based on the updating rules in Eqs. ( 4 ) and ( 5 ) ,the solution satisfies the KKT optimality condition.
 Proof Following the standard theory of constrained optimization, we introduce the Lagrangian multipliers  X  1 and  X  2 to minimize the lagrangian function,
Based on the KKT complementarity conditions, we obtain the following two equations, Applying the Hadamard multiplication on both sides of Eqs. ( 7 )and( 8 )by S and G , respec-tively, and using KKT conditions of where denotes the Hadamard product of two matrices, we can prove that if S and G are a local minimizer of the objective function in Eq. ( 6 ), the following equations are satisfied, BasedonEqs.( 9 )and( 10 ), we derive the proposed updating rules of Eqs. ( 4 )and( 5 ). If the updating rules converge, the solution satisfies the KKT optimality condition. Proof is completed. 3.3.2 Convergence Next, we prove the convergence of the algorithm. In Propositions 2 and 3 , we show that the objective function decreases monotonically under the two updating rules. This can be done by making use of an auxiliary function similar to that used in Lee and Seung [ 38 ]. Proposition 2 If G is a fixed matrix, then J ( S ) = A  X  GSG T 2 = Tr ( A T A  X  2 G T A T GS + G GSG T GS T ) decreases monotonically under the updating rule of Eq. ( 4 ) .
 F ( S L ( S ( t + 1 ) ) . Thus, L ( S ( t ) ) is monotonic decreasing (non-increasing).
 we write and show that, is an auxiliary function of L ( S ( t + 1 ) ) .
 in L ( S ( t + 1 ) ) because of the inequality S L ( holds.
 S obtained by setting, Thus, we can derive the updating rule of Eq. ( 4 )as S ih  X  S ih 2 ( G T AG ) ih ( updating rule, J ( S ) decreases monotonically. Proof is completed. Proposition 3 If S is a fixed matrix, J ( G ) = A  X  GSG T 2 = Tr ( A T A  X  2 G T A T GS + G GSG T GS T ) decreases monotonically under the updating rule of Eq. ( 5 ) .
 F ( we write and show that, is an auxiliary function of L ( G ( t + 1 ) ) .
 setting  X  F ( G G pleted. 3.4 Equivalence of SS-NMF and other semi-supervised clustering methods We now show that SS-NMF is a general and unified framework for semi-supervised cluster-ing by establishing the relationship between SS-NMF and other well-known semi-supervised clustering algorithms, i.e., semi-supervised kernel k -means (SS-KK) [ 36 ]andsemi-supervised spectral clustering with normalized cuts (SS-SNC) [ 28 ]. In fact, both these algo-rithms can be considered to be special cases of SS-NMF.
 Proposition 4 Orthogonal SS-NMF clustering is equivalent to SS-KK clustering.
 Proof The SS-NMF objective function is, The equation can be written as, J SS  X  NMF = A  X  GSG T 2 = A  X  G G T 2 = Tr ( A T A  X  2 G T AG + G T G ) if let S = Q T Q and G = GQ T .SinceTr ( A T A + G T G ) is a constant, the minimization of J becomes a maximization problem as, The SS-KK objective function is [ 36 ], squared Euclidean distances among the data points, W the constraint matrix and G the cluster indicator. Equation ( 19 ) becomes the minimization of the following function, We can convert the minimization of Eq. ( 20 ) to a maximization of the problem, where K = A + W and A the similarity matrix.

It is clear that the objective function of SS-NMF (Eq. ( 18 )) is equivalent to that of SS-KK Proof is completed.
 Proposition 5 Orthogonal SS-NMF clustering is equivalent to SS-SNC clustering. Proof The objective function of SS-SNC is [ 28 ], where A = A  X  W reward  X  W penalty is the pairwise similarity matrix with constraints, D = vector z h = D 1
It can be shown that the minimization of Eq. ( 22 ) becomes a maximization problem as, Also, it can be seen that Eq. ( 18 ) is equivalent to Eq. ( 23 )if A =  X  A . Moreover, the G in Eq. ( 18 ) represents the same clustering as Z of Eq. ( 23 ) does. Proof is completed.
From the above two proofs, we can see that SS-NMF, SS-KK, and SS-SNC are mathemat-ically equivalent. However, notice that in SS-NMF, the matrix A might have some negative is to perform some normalization techniques to guarantee non-negative values. Alternatively, we can simply relax the non-negative constraint to allow negative values as in Semi-NMF [ 40 ]. In either of the approaches, the clustering result will not get affected. In SS-NMF, the cluster indicator G is near-orthogonal and can produce soft clustering results. The cluster centroid S can provide good characterization of the quality of data clustering because the residue of the matrix approximation J = min A  X  GSG T is smaller than J = min A  X  GG T .On the other hand, for SS-KK and SS-SNC, if input matrix is added with constraint weight W ,in order to ensure positive definiteness, certain additive constraints need to be enforced. More-to be orthogonal, leading to only hard clustering results. Hence, both SS-KK and SS-SNC can be viewed as special cases of SS-NMF with orthogonal space constraints. Thus, SS-NMF essentially provides a general and unified mathematical framework for semi-supervised data clustering. 3.5 Advantages of SS-NMF In this section, we further illustrate the advantages of SS-NMF using a toy dataset shown in Fig. 1 a, which follows an extreme distribution consisting of 20 data points forming two natural clusters: two circular rings with 10 data points each. Traditional unsupervised clustering methods, such as (kernel) k -means, spectral normalized cut or NMF, are unable to produce satisfactory results on this dataset. However, after incorporating knowledge from the user in the form of constraints, we are able to achieve much better results.
 Unlike SS-SNC, SS-NMF maps the samples into a non-negative latent semantic space. Moreover, SS-NMF does not require the derived space to be orthogonal. Figure 1 b, c shows, the data distributions in the two spaces for SS-NMF and SS-SNC, respectively. Data points belonging to the same cluster are depicted by the same symbol. For SS-NMF, we plot the data points in the space of two column vectors of G , while for SS-SNC the first two singular vectors are used. Clearly, in the SS-NMF space, every data point takes non-negative values in both the directions. Furthermore, in SS-NMF space, each axis corresponds to a cluster, and all the data points belonging to the same cluster are nicely spread along the axis. The cluster label for a data point can be determined by finding the axis with which the data point has the largest projection value. However, in the SS-SNC space, there is no direct relationship between the axes (singular vectors) and the clusters.

Ta b l e 1 shows the difference of cluster indicator between the hard clustering of SS-KK and soft clustering of SS-NMF. An exact orthogonality in SS-KK means that each row of cluster indicator G has only one nonzero element, which implies that each data object belongs to each data object could belong fractionally to more than 1 cluster. This can help in knowledge discovery in the cases where the data point is evenly projected along the different axes. For two clusters.

SS-NMF uses an efficient iterative algorithm instead of solving a computationally expen-sive constrained eigen decomposition problem as in SS-SNC. The time complexity of SS-NMF is O ( tkn 2 ) where k is the number of clusters, n is the number of documents, SS-KK clustering algorithm. However, compared to SS-KK, SS-NMF algorithm is simple as it only involves some basic matrix operations and hence can be easily deployed over a distributed computing environment when dealing with large datasets. Another advantage in favor of SS-NMF is that a partial answer can be obtained at intermediate stages of the solution by specifying a fixed number of iterations.

In Fig. 2 , we demonstrate the computational speed of SS-NMF with respect to SS-KK and SS-SNC. This experiment was performed on a machine with a 3 GHz Intel Pentium 2 processor with 2 GB RAM. As the number of data samples increase, SS-SNC turns out to be the slowest of the three algorithms. SS-KK is the quickest with SS-NMF closely following it. In the next section, we show the superior performance of SS-NMF in terms of clustering accuracy in comparison with other clustering algorithms. 4 Experiments and results In this section, we empirically demonstrate the performance of SS-NMF for data cluster-ing. we present the details of our experiments, starting with the descriptions of the data sets (Sect. 4.1 ), the methodology and evaluation metrics (Sect 4.2 ), followed by thorough per-formance comparisons with leading unsupervised and semi-supervised clustering algorithms (Sect 4.3 ). 4.1 Data description We have thoroughly evaluated the proposed algorithm on a variety of datasets, with number of classes ranging from 2 to 10, having between 27 and 500 data samples, and the dimensionality (attributes) ranging from 4 to 12,600. These datasets represent applications from different domains such as text mining and bioinformatics. 1. Text datasets 2. Gene expression datasets 3. UCI datasets 4.2 Methodology and evaluation metrics We compared the performance of SS-NMF model on all the 15 datasets with the following NMF, (5) SS-KK, (6) SS-SNC. The first four methods are the most popular unsupervised data clustering methods, whereas SS-KK and SS-SNC are the representative semi-supervised ones. Through these comparison studies, we demonstrate the relative position of SS-NMF with respect to unsupervised and semi-supervised approaches in real-world data clustering. We evaluated the clustering results using confusion matrix and the accuracy metric AC. Each entry ( i , j ) in the confusion matrix represents the number of objects in cluster i that belong to true class j . The AC metric measures how accurately a learning method assigns labels  X  y i to the ground truth y i , and is defined as, where n denotes the total number of objects in the experiment, and  X  is the delta function the global minimum, it is beneficial to run the algorithm several times with different initial values and choose one trial with a minimal objective value. In reality, usually a few number of trials is sufficient. In the case of NMF and k -means, for a given k , we conducted 20 test runs. Three trials are performed in each of the 20 test runs and final accuracy value is the average of all the test runs. 4.3 Results 4.3.1 Document clustering We first performed comparison of the four unsupervised clustering approaches with SS-NMF having pairwise constraints on only 3% pairs of all the possible document pairs, which is ments. If both the documents have the same class label ( must-link ) , then the constraint is assigned maximum weight in the document-document similarity matrix. On the other hand, if they belong to different classes ( cannot-link ), then the minimum weight in the similarity matrix is used for the constraint. For kernel k -means, we used a Gaussian (exponential) ker- X  = 0 . 01 for more than two clusters. In Table 5 , we compared the algorithms on all the datasets using AC values. The performance of the first three methods is similar with NMF proving to be the best amongst the unsupervised methods. However, the accuracy of NMF greatly deteriorates and is unable to produce meaningful results on datasets having more than two clusters. On the other hand, the superior performance of SS-NMF is evident across all the datasets. We can see that in general a semi-supervised method can greatly enhance the document clustering results by benefitting from the user provided knowledge. Moreover, SS-NMF is able to generate significantly better results by quickly learning from the few pair-wise constraints provided. Table 6 demonstrates the performance of SS-NMF when varying amounts of pairwise constraints were available a priori. We reported the results in terms of the confusion matrix C and the cluster centroid matrix S . As the available prior knowl-edge increases from 0 to 5%, we can make the following two key observations. Firstly, the confusion matrices tend to become perfectly diagonal indicating higher clustering accuracy. Second observation pertains to the cluster centroid matrix S which represents the similarity or distance between the clusters. Increasing values of the diagonal elements of S indicate higher inter-cluster similarities. As expected, when the amount of prior knowledge available is more, the performance of the algorithm clearly gets better.
 In Fig. 3 a, the sparsity pattern of a typical document-document matrix A = X T X ( England-Heart in the figure) before clustering is shown. The SS-NMF algorithm is applied to the columns of the matrix. Figure 3 b,c shows the  X  A matrices for England-Heart and Fbis5 datasets after clustering with 5% pairwise constraints. Document clusters are indicated by the dense sub-matrices in these matrices. We now compare SS-NMF with the other two semi-supervised clustering approaches. As before, for SS-KK, a Gaussian kernel was used. In Fig. 4 , we plotted the AC values against increasing percentage of pairwise constraints available, for the algorithms on all the datasets. On the whole, all three algorithms perform better as the percentage of pairwise constraints increases. While the performance of SS-KK is close to that of SS-SNC on the mainly because of the fact that SS-KK is unable to maintain its accuracy when producing more than two clusters. While, the performance of SS-SNC is head-to-head with SS-NMF on Fbis2 and Fbis3 , it is consistently outperformed by SS-NMF on the rest of the datasets. Another noticeable fact is that the curve for SS-KK and SS-SNC might take a slow rise in some cases indicating that they need more amount of prior knowledge to improve the performance. Comparatively, SS-NMF gets better accuracy than the other two algorithms even for minimum percentage of pairwise constraints. 4.3.2 Gene expression clustering We now present the comparison of SS-NMF with the other algorithms on real-world gene expression datasets. We first compared the four unsupervised clustering approaches with SS-NMF having pairwise constraints on only 3% pairs of all the possible sample pairs. For kernel k -means, we used a Gaussian (exponential) kernel, with variance  X  = 0 . 00001 for ALL /AML and Colon Tumor datasets and a polynomial kernel K ( x 1 , x 2 ) = ( 1 + x 1  X  x 2 ) p with polynomial parameter p = 1 for the other datasets. In Table 7 , we have compared the algorithms on all the five gene expression datasets with AC values. As was the case with document clustering, SS-NMF performs to be the best across all the datasets. It is evident that the algorithm learns quickly in spite of having few constraints. Table 8 demonstrates the performance of SS-NMF improves when the number of pairwise constraints on the gene expression datasets increase from 0 to 5%. These results are reported in terms of the confusion matrix C and the normalized cluster centroid matrix S as before.

Next, we compare SS-NMF with the other two semi-supervised clustering approaches on the gene expression datasets. Figure 5 shows a plot of the AC values against increas-ing percentage of pairwise constraints for the three semi-supervised algorithms on all the five datasets. All three algorithms perform better as the percentage of pairwise constraints percentage of constraints when distinguishing between tumor and non-tumor samples, as in Figs. 5 b-c. Also, for clustering subtypes of tumors, although the differences are small, SS-NMF outperforms the other two algorithms as seen from Figs. 5 a, d-e. 4.3.3 UCI datasets clustering Ta b l e 9 shows the comparison of SS-NMF with the unsupervised clustering algorithms on all three UCI datasets. As before, for kernel k -means, we used a Gaussian (exponential) kernel with variance  X  = 1for Iris data and polynomial kernel with polynomial parameter p = 1 for the other datasets. As can be seen, with just 5% constraints, SS-NMF yields significantly better results than the unsupervised approaches. For instance, on Soybean data, SS-NMF improves the accuracy over 25%. Similar trends can also be observed for other two datasets.
Figure 6 illustrates the performance of SS-NMF and the two semi-supervised algorithms SS-NMF clustering always produces best accuracy performance when the dimensionality of the datasets is high (Fig. 6 b,c). However, it is unable to achieve quality clustering on low dimensionality datasets for fewer constraints. For Iris dataset which has dimensionality SS-KK as the percentage of pairwise constraints increase. This shows that SS-NMF is a viable proposition for low-dimensional data as well but needs higher percentage of constraints. 5 Conclusions We presented SS-NMF: a semi-supervised approach for clustering based on non-negative matrix factorization. In the proposed framework, users are able to provide supervision in ( a ) (c) (e) terms of must-link and cannot-link pairwise constraints on the data objects. We derived an iterative algorithm to perform symmetric tri-factorization of the data similarity matrix. We have mathematically shown the correctness and convergence of SS-NMF. Moveover, we proved that SS-NMF provides a general and unified framework for semi-supervised data clustering. Existing approaches can be considered as special cases of it. Empirically, we showed that SS-NMF outperforms well-established unsupervised and semi-supervised clustering methods in grouping publicly available datasets. ( a ) (c) References Author Biographies
