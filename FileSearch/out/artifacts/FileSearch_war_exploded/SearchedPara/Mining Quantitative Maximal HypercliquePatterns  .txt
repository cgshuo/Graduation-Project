 A hyperclique pattern [9, 4] is a new type of association pattern that contains items which are highly affiliated with each other. More specifically, the pres-ence of an item in one transaction strong ly implies the presence of every other item that belongs to the same hyperclique pattern. Conceptually, the problem of mining hyperclique pattern in transaction data sets can be viewed as finding approximately all-one sub-matrix in a 0-1 matrix where each column may corre-spond to an item and each row may correspond to a transaction. For the rest of this paper, we refer to this problem as the binary hyperclique mining problem.
However, in many business and scientific domains, there are data sets which contain quantitative attributes (e.g. income, gene expression level). How to de-fine and efficiently identify hyperclique patterns in data sets with quantitative attributes remains a big challenge in the literature. To this end, the focus of this paper is to address the quantitative hyperclique pattern mining problem.
To the best of our knowledge, there is no previous work on developing al-gorithms for finding quantitative maximal hyperclique patterns. Our approach for mining quantitative hyperclique patterns is built on top of the normalization scheme [7]. A side effect of the normalization scheme is that there is no support pruning for single items. To meet with this computational challenge, we design a clique pruning method to dramatically remove a large number of items which are weakly related to each other, and thu s effectively improving the overall com-putational performance for finding quantitative hyperclique patterns. We adopt structures of three popular association pattern mining algorithms including FP-tree [5], diffEclat [10], and Mafia [3] as the bases of our algorithms. The purpose of these algorithms is to find quantitative maximal hyperclique pattern, which is a more compact representation of quantitative hyperclique patterns and is desirable for many applications, such as pattern preserving clustering [8]. A hy-perclique pattern is a maximal hyperclique pattern if no superset of this pattern is a hyperclique pattern. Finally, we briefly introduce the results of using our approach on some real-world data sets. Normalization. In this paper, we adopt the normalization method proposed in [7]. For a vector x = &lt;x 1 ,x 2 ,...,x n &gt; , our normalization will turn the vec-After data normalization, we define the support of every individual item i ,  X  normalized value of item j in the transaction i .

One advantage of this normalization is that the resulting support is a number between 0 and 1. Such normalization is natural in many domains, e.g., text documents. However, a side-effect of this is that individual items can no longer be pruned using a support threshold since all single items have a support of 1. Quantitative Hyperclique Patterns. A traditional binary hyperclique pat-tern [9] is a frequent itemset with the a dditional constraint that every item in the itemset implies the presence of th e remaining items with a minimum level of confidence known as the h-confidence. Specifically, we have the following: Definition 1. A set of attributes, X , forms a hyperclique pattern with a partic-ular level of h-confidence, where h-confidence is defined as Where  X  is the standard support function [1].
 H-confidence, just like standard support, is in the interval [0 , 1] and it has the anti-monotone property; that is, the h-confidence of an itemset is greater than or equal to that of its any superset. Also, hyperclique patterns have the high affinity property, i.e., items in a pattern with a high h-confidence are guaranteed to have a high pairwise similarity as measured by the cosine metric. Additionally, there is an important relationship between h-confidence of binary hyperclique patterns and the support function  X  min , L 2 alent to standard support for binary data, we can substitute  X  min , L 2 standard support function  X  ( X ) in Equation 1. It is then interesting to note that if we normalize all attributes to have an L 2 norm of 1, i.e.,  X  min , L 2 for all items i , then, by Equation 1, hconf( X )=  X  min , L 2 ization sets the support of all the item to 1, we get max i  X  X {  X  ( i ) } = support (i) =1.

In a nutshell, finding continuous hyperclique patterns first proceeds by nor-malizing the attributes to have an L2 norm of 1. Then, for each row, we take the minimum of the specified attributes. Finally, we square each of these values and add them up. The resulting value is the h-confidence and is a lower bound on the pairwise cosine similarity. Here, we present the algorithms for mining quantitative maximal hyperclique patterns. Our algorithms are built on top of three state-of-the-art association pattern mining algorithms including FPTree [5], diffEclat [10], and Mafia [3]. Clique Pruning. We design a clique pruning method for eliminating weakly re-lated single items. Specifically, we first compute h-confidence of all item pairs on the normalized data. For each item, we then identify the maximum h-confidence value among all pairs including this item. Finally, for a user-specified threshold, we prune all items whose maximum h-confidence is less than this threshold. Algorithm based on FP-Tree. FP-Tree [5] is a compact tree structure which allows to identify frequent patterns without generating the candidate patterns. Here, we adopt the FP-tree algorithm for finding quantitative maximal hyper-clique patterns. First, we store float valu es instead of integer values, since the support of the normalized data are continuous. Second, the support values should be squared before added to the FP-Tree since they have an L 2 Norm. Finally, we need to split squared transactions a nd make the support of preceding item not less than the successor item, before adding them into the FP-Tree. Algorithm based on MAFIA. MAFIA [3] is a depth-first searching algo-rithm for mining maximal frequent patterns. For the data set with continuous attributes, we change the algorithm to store not only the tidset, but also the support (normalized data) for each transaction. For this purpose, the algorithm needs a float vector to store the support information. Each element in the vector presents the support for ea ch transaction in order.
 Algorithm based on DiffEclat. DiffEclat uses a vertical data representation, called diffset , for efficiently mining maximal frequent patterns[10]. The diffset only store the different set of transaction ID between the pattern and its parant pattern. The key modification that we made is to store both transaction IDs and the support information. However, for diffset, we store the support different between a pattern and its parent pattern instead of the support itself. Experimental Setup. Our experiments were performed on two real-life gene expression data sets, Colon Cancer and NCI [2, 6]. Table 1 shows some charac-teristics of these gene expression data sets.
 A Performance Comparison. Figure 1 (a) shows the running time of three algorithms on the Colon Cancer data set. As can be seen, when the h-confidence threshold is less than 0.35, the FP-Tree can be an order of magnitude faster than Mafia and DiffEclat is not very efficient and become unscalable when the h-confidence threshold is low. Also, Figure 1 (b) shows the performance of the proposed algorithms for mining sample patterns on the NCI data set. Similar to the observation from the Colon Cancer data set, we can also observe that when the h-confidence threshold is less than 0.5, the FPTree can be an order of magnitude faster than Mafia. However , MAFIA has a better performance when the h-confidence threshold is high. Another observation is that the performance DiffEclat is not scalable when the h-confidence threshold is low.
 The Effect of Clique Pruning. Figure 2 demonstrate the effect of clique pruning on Colon and NCI data sets using the algorithm based on Mafia. As can be seen from both figures, with the increase of the clique pruning ratio, the running time is reduced significantly. The running time can be orders of magnitude faster if we target on hyperclique patterns with high affinity. Another benefit is that, the proposed algorithm can even identify patterns at a very low level support when the clique pruning ratio is at a certain level. In this paper, we addressed the problem of mining quantitative maximal hyper-clique patterns in the data sets with cont inuous attributes. Instead of mapping continuous attributes into binary attributes, we applied a data normalization method. Also, we provided algorithms for finding quantitative maximal hyper-clique patterns. These algorithms are built on top of three state-of-the-art asso-ciation pattern mining algorithms and have included a clique pruning method to perform pruning for individual items. Finally, the performance of the algorithms have been demonstrated us ing real-world data sets.
 Acknowledgement. This paper was partially supported by NSF grant #ACI-0305567 and NSF grant #CCF-0514796.

