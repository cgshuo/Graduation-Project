 Kilho Shin yshin@cmuj.jp Tetsuji Kuboyama kuboyama@gakushuin.ac.jp Gakushuin University, Mejiro, Toshima-ku, Tokyo, Japan Haussler X  X  convolution kernel (Haussler, 1999) has been used as a general framework to tailor known primitive kernels to the context of specific applications. In this section, we first review a degenerated form of Haussler X  X  convolution kernel, which proves in fact to be equivalent to the general form of Haussler X  X  con-volution kernel (see 2.2). Let each data point x in a space  X  be associated with a finite subset  X  0 x of a com-mon space  X  0 . Furthermore, we assume that a kernel k :  X  0  X   X  0  X  R is given. Then, Haussler X  X  convolution kernel K :  X   X   X   X  R is defined as follows (see 2.2). Haussler proved that, if k ( x 0 , y 0 ) is positive semidefi-nite, then so is K ( x, y ). Haussler X  X  convolution kernel is known to have a wide range of application (Lodhi et al., 2001; Collins &amp; Duffy, 2001; Suzuki et al., 2004). On the other hand, the mapping kernel is a natural generalization of Haussler X  X  convolution kernel, and is defined by Eq. (2) for { M x,y j  X  0 x  X   X  0 y | ( x, y )  X   X  2 } . The problem that the present paper addresses is to determine whether the mapping kernel is positive semidefinite.
 The main contribution of the present paper is to present a necessary and sufficient condition for the mapping kernel K ( x, y ) defined by Eq. (2) to be pos-itive semidefinite for all possible choices of positive semidefinite k ( x 0 , y 0 ). More specifically, we prove that the condition is that the mapping system { M x,y | ( x, y )  X   X  2 } is transitive , i.e. , ( x 0 , y 0 )  X  M M y,z  X  ( x 0 , z 0 )  X  M x,z . Haussler X  X  convolution kernel is indeed the special case of the mapping kernel for { M x,y =  X  0 x  X   X  0 y } , which is apparently transitive. We see plural instances of the mapping kernel in the literature, and some of them were mistreated in re-spective manners.
  X  Although the elastic tree kernel (Kashima &amp; Koy- X  The codon-improved kernel (Zien et al., 2000) was That is to say, the positive semidefiniteness of the aforementioned kernels were concluded on wrong grounds, and in fact, the conclusion regarding the codon-improved kernel is wrong  X  in reality, it is not necessarily positive semidefinite.
 The kernels introduced in (Menchetti et al., 2005) and (Kuboyama et al., 2006) are also instances of the map-ping kernel. In contrast to the elastic and codon-improved kernels, their positive semidefiniteness was properly investigated, albeit in specific manners. This is the first paper that recognizes the mapping kernel as a generic class of kernels, and presents a nec-essary and sufficient condition that a mapping kernel becomes positive semidefinite. Furthermore, the con-dition is simple, intuitive and easy to check, and there-fore, would make engineering of new instances of the mapping kernel easier, more efficient and more effec-tive to a large extent.
 As the second contribution of the present paper, we take advantage of the mapping kernel, and present a way to augment a couple of well-known frameworks to engineer similarity functions for discretely structured objects ( e.g. strings, trees, general graphs). It is known that the maximum sizes of shared substruc-tures of the objects can be used as a good measure of similarities of the objects. The maximum agreement subtree is a good example. Also, the edit distance has been applied to various types of objects. An edit dis-tance between two data objects is generally defined as the minimum cost of edit scripts that transform one object into the other.
 These two frameworks are common in that they only focus on the maximum/minimum values of the similar-ity measures ( i.e. the sizes of shared substructures and the costs of edit scripts), and therefore, only those sub-structures with the maximum sizes or those edit scripts with the minimum costs can contribute to the similar-ity functions. It is, however, reasonably presumable that distributional features of the measurements may carry useful information with regard to similarities of objects, and more accurate similarity functions can be engineered by evaluating the distributional features. Based on the aforementioned consideration, we intro-duce two novel classes of kernels (similarity functions) each evaluating the distributional features of the sizes of shared substructures or the costs of edit scripts. Also, we show a general way to view them as mapping kernels. By virtue of our simple criteria for positive semidefinite mapping kernels, we can easily determine whether instances of the new kernel classes are positive semidefinite, and, if they are, we can take advantage of sophisticated classifiers such as support vector ma-chines (SVM). In 3.1 and 3.2, we see that the examples of distribution-based similarity functions derived from maximum agreement subtrees and general tree edit distances are positive semidefinite, while those derived from maximum refinement trees (Hein et al., 1996) and less-constrained tree edit distance (Lu et al., 2001) are not. In this section, as a preliminary, we quickly review the positive semidefinite kernel (2.1) and Haussler X  X  convolution kernel (2.2). Then, we describe our main theorem with regard to the mapping kernel (2.3). 2.1. The Positive Semidefinite Kernel A kernel K :  X   X   X   X  X  X  R is said to be positive semidef-inite, if, and only if, for arbitrary x 1 , . . . , x n  X   X  , the corresponding Gram matrix G = [ K ( x i , x j )] i,j =1 ,...,n is a positive semidefinite matrix. Positive semidefi-niteness of kernels is a critical condition for reproduc-ing kernel Hilbert spaces to exist. In simpler cases where a data point space  X  is finite, this condition is equivalent to the property that there exists a mapping  X  :  X   X  X  X  R N such that K ( x, y ) =  X ( x ) X ( y ) T . In this paper, by a positive semidefinite matrix, we mean a real symmetric matrix ( i.e. A T = A ) that sat-isfies one of, hence, all of the mutually equivalent con-ditions stated below, where dim A = n .  X  ( c 1 , . . . , c n ) A ( c 1 , . . . , c n ) T  X  0 for  X  ( c  X  A has only non-negative real eigenvalues.  X  There exists an n -dimensional orthogonal matrix  X  A = B T B for some m  X  n real matrix B .
 2.2. Haussler X  X  Convolution Kernel Hausler X  X  theorem (Haussler, 1999, Theorem 1) as-serts the positive semidefiniteness of Haussler X  X  R -convolution kernel, and Theorem 1 presents its special case for D = 1.
 Theorem 1. Let k :  X  0  X   X  0  X  R be a positive semidefi-nite kernel. Given a relation R j  X  0  X   X  , K :  X   X   X   X  R defined by Eq. (3) is also positive semidefinite. It is interesting to note that Haussler X  X  theorem for D &gt; 1 is obtained as a corollary to Theorem 1. Corollary 1. (Haussler, 1999) Let k d :  X  0 d  X   X  0 d  X  X  X  R be positive semidefinite kernels for d = 1 , . . . , D . Given a relation R  X   X  0 1  X  X  X  X  X   X  0 D  X   X  , the kernel K :  X   X   X   X  X  X  R defined below is also positive semidefinite. K ( x, y ) = 2.3. Definition and Main Theorem an equivalent form of Eq. (3). On the other hand, the mapping kernel is defined so that ( x 0 , y 0 ) moves over a subset M x,y of  X  0 x  X   X  0 y rather than the entire cross product  X  0 x  X   X  0 y (Eq. (2)).
 The present paper shows that the mapping kernel is positive semidefinite for all possible choices of posi-tive semidefinite underlying kernels k , if, and only if, { M x,y | x, y  X   X  } is transitive (Definition 2). Therefore, for an arbitrary non-transitive { M x,y } , a positive semidefinite underlying kernel k ( x 0 , y 0 ) exists such that the resulting K ( x, y ) is not positive semidef-inite (4.1.2). On the other hand, K ( x, y ) may be posi-tive semidefinite even for a non-transitive { M x,y } and a positive semidefinite k ( x 0 , y 0 ) (Example 1). Example 1. The ( k, m )-mismatch kernel K ( k,m ) ( x, y ) is positive semidefinite (Leslie et al., 2004). When x and  X  0 y denote the sets of k -mers in x and y , K ( k,m ) ( x, y ) can be regarded as a mapping kernel for the non-transitive { M x,y } defined as follows. The result is formalized as follows.
 Definition 1. A mapping system M is a triplet Definition 2. A mapping system (  X , {  X  0 x } , { M x,y } ) is said to be transitive , if, and only if, ( x 0 1 , x 0 M for arbitrary x i  X   X  and x 0 i  X   X  0 x Definition 3. An evaluating system E for a mapping system (  X , {  X  0 x } , { M x,y } ) is a triplet (  X  0 , k, {  X   X  } ) with a positive semidefinite underlying kernel k :  X  0  X   X  0  X  R and projections  X  x :  X  0 x  X   X  0 . Definition 4. For a mapping system M = (  X , {  X  0 x } , { M x,y } ) and an evaluating system E = (  X  0 , k, {  X  x } ) for M , the mapping kernel with respect to M and E is defined by Eq. (4).
 Now, our main theorem is described as follows, and its proof is given in Section 4.
 Theorem 2. For a mapping system M , the following are equivalent to each other. 1. M is transitive. 2. For an arbitrary evaluating system E for M , the It is possible to prove (1)  X  (2) of Theorem 2 as a corollary to Theorem 1. Nevertheless, our direction in the present paper is opposite  X  we like to view Theorem 1 as a trivial corollary to Theorem 2. In fact, we will prove Theorem 2 without assuming Theorem 1 in Section 4. In this section, we introduce two new classes of the mapping kernel. The kernels are expected to im-prove the classification performance of known simi-larity measurements by evaluating their distributional features. 3.1. Size-of-index-structure-distribution When some structures are commonly derived from two data objects, the structures may carry information with regard to similarities between the data objects. In this paper, we call such structures index structures . The agreement subtree is a good example of the in-dex structure, when data objects are represented as trees. An agreement subtree between plural input trees is usually defined as a subtree homeomorphically included in all the input trees (Berry &amp; Nicolas, 2004). In the present paper, we assume that the input trees are a pair of trees. Even when we fix the input tree pair, there may exist more than one agreement sub-tree, and the maximum size of the agreement subtrees can be naturally viewed as a measure of similarities be-tween the input trees. The maximum agreement sub-trees (MAST) problem is the problem to determine at least one agreement subtree with the maximum size among the possible agreement subtrees for the input trees. The MAST problem has been extensively stud-ied from the application point of view ( e.g. evolution-ary trees (Hein et al., 1996; Berry &amp; Nicolas, 2004), shape-axis trees (Pelillo, 2002)) as well as from the algorithm efficiency point of view.
 When using the size of the maximum agreement sub-trees as a similarity measurement between trees, we discard those agreement subtrees smaller in size than the maximum ones, and therefore, they do not con-tribute to the final evaluation at all. It is, however, reasonable to think that distributional features of the sizes of agreement subtrees may carry useful informa-tion with regard to similarities of the trees. Based on the aforesaid consideration, we introduce the kernel of Eq. (5), which evaluates distributional fea-tures of the sizes of agreement subtrees. In Eq. (5), we let AST ( x, y ) denote the set of the agreement subtrees between x and y , and f : N  X  R + = { y  X  0 | y  X  R } be an increasing function.
 If x and y are rooted trees of bounded degree, and if f ( n ) =  X  n or f ( n ) = n , for example, there ex-ist polynomial-time efficient algorithms to calculate K ( x, y ).
 Beside the advantages due to the distributional fea-tures, the kernel could provide the advantage of using sophisticated classifiers such as SVM (Cristianini &amp; Shawe-Taylor, 2000). In fact, our contribution asserts that K ( x, y ) is positive semidefinite as follows. First, K ( x, y ) can be viewed as a mapping kernel under the following notation.  X   X  0 x is the set of the subtrees of x .  X  k ( x 0 , y 0 ) = It is easy to see that { M x,y } is transitive and k ( x 0 , y 0 ) is positive semidefinite. Hence, K ( x, y ) = is positive semidefinite by Theorem 2.
 Besides the maximum agreement subtree, the max-imum refinement subtree (Hein et al., 1996; Berry &amp; Nicolas, 2004), maximum subtree isomorphism (Pelillo, 2002; Aoki et al., 2003) and maximum agree-ment supertree (Jansson et al., 2005) are also used as index structures for trees. As for general graphs, the maximal common clique included in an input pair of graphs is also studied in association with MAST in (Pelillo, 2002).
 For each of those index structures, we can define ker-nels in the same way as for MAST. We have only to replace AST ( x, y ) in Eq. (5) with the set of the re-spective index structures. Moreover, except for the maximum refinement subtree, through the same dis-cussion as for MAST, the kernels prove to be positive semidefinite.
 Interestingly, Theorem 2 also implies that the kernels defined based on the minimum refinement subtree are not necessarily positive semidefinite. The minimum refinement subtree for x 0 j x and y 0 j y is defined as the minimum tree t such that both x 0 and y 0 can be derived from t through a sequence of edge contrac-tions , and the maximum refinement subtree problem ( a.k.a. the maximum compatible tree problem) is the problem to find a minimum refinement subtree with the largest size. Different from the agreement subtree, the relation of having a refinement is not an equiva-refinement subtrees, x 0 and z 0 do not necessarily have a refinement subtree. This implies that the correspond-ing M x,y is not necessarily transitive. Therefore, The-orem 2 asserts that the corresponding K ( x, y ) is not necessarily positive semidefinite. 3.2. Edit-cost-distribution Kernels The Edit distance is also used as an effective measure of similarities between discrete data structures ( e.g. (Wagner &amp; Fischer, 1974) for strings, (Barnard et al., 1995) for trees, (Bunke, 1997) for general graphs). Let x be an object consisting of one or more compo-nents. For example, a string consists of one or more characters which are laid out on a line. For another example, a graph consists of one or more vertices and edges, and each edge connects a vertex to another. We first give a general definition of edit operations , edit scripts , edit costs and edit distances for such objects. An edit operation is an operation on a component of x , and is one of (i) substituting a component b for a component a of x (denoted by  X  a  X  b  X  ), (ii) deleting a component a of x (denoted by  X  a  X   X  X  ), and (iii) inserting a component a into x (denoted by  X  X  X  X  a  X  ). An edit script is a sequence of zero or more edit opera-tions which transforms an object into another. When a cost  X   X  a  X  b  X   X  R is given for each edit operation  X  a  X  b  X  1 , the cost  X  (  X  ) of an edit script  X  is the sum of the costs of the edit operations that comprise  X  . Fi-nally, an edit distance d ( x, y ) between objects x and y is defined by: Therefore, those edit scripts with larger costs than the minimum cost do not contribute to the final edit dis-tance. In contrast, by introducing kernels by Eq. (6) with a decreasing function f : R +  X  R + , we try to take advantage of the information that those discarded edit scripts potentially carry.
 It is important to note that there exists a natural in-terpretation of Eq. (6). In a natural setting where the cost  X   X  a  X  b  X  is defined as the negative logarithm of the probability that the substitution of b for a ( a or b could be  X  ) would occur ( e.g. (Li &amp; Jiang, 2005; Salzberg, 1997)), we let f ( x ) = e  X  x . For an edit script  X  =  X  x 0 1  X  y 0 1  X  X  X  X  X  x 0 n  X  y 0 n  X  transforming x into y , f (  X  (  X  )) is evaluated as follows. Hence, K ( x, y ) by Eq. (6) equals the total probability that x would be transformed into y .
 Usage of sophisticated classifiers such as SVM is an-other potential advantage of the kernels of the form of Eq. (6). In fact, as shown below, the kernels can be viewed as mapping kernels, if we can pose the following four assumptions. 1. The cost function is symmetric ( i.e.  X   X  a  X  b  X  = 2. We let f ( x ) = e  X  cx for some positive constant c . 3. In order to avoid calculating infinite sums, we take 4. If two irreducible edit scripts differ from each For  X  =  X  x 0 1  X  y 0 1  X  . . .  X  x 0 n  X  y 0 n  X  , we assume that i and y 0 i are respectively components of x and y , if, and only if, i  X  { 1 , . . . , m (  X  ) } , and call  X  x 0 K ( x, y ) are evaluated as follows.  X  (  X  ) = In Eq. (7), the first two factors of the right-hand side are functions of x and y , and therefore, we denote them by g ( x ) and g ( y ), respectively. On the other hand, the define M x,y as follows.

M x,y = { (( x 0 1 , . . . , x 0 m ) , ( y 0 1 , . . . , y Then, the following holds
K ( x, y ) = g ( x )  X  g ( y )  X  In particular,  X  K ( x, y ) is a mapping kernel, and K ( x, y ) is positive semidefinite, if, and only if, so is  X  K ( x, y ), since g ( x ) cannot take the value 0. The kernel  X  K ( x, y ), however, is not necessarily positive semidefinite, even if k ( x 0 , y 0 ) is positive semidefinite, since { M x,y necessarily transitive. We will investigate this problem taking the tree edit distance as an example.
 For the tree edit distance, the edit operations act on some irreducible tree edit script, it is necessary and sufficient that  X  defined by  X  ( x 0 i ) = y 0 i preserves the ancestor-descendent relation and the sibling (left-to-right) relation (Tai, 1979). Therefore, M x,y for the general tree edit distance is defined as follows, where It is straightforward to verify that { M x,y } is transi-tive. Therefore, Theorem 2 asserts that, if k ( x 0 , y 0 ) is positive semidefinite, so is  X  K ( x, y ) for this { M x,y On the other hand, two subclasses of the general tree edit distance have been proposed. They are con-strained ( a.k.a. structure-preserving) tree edit distance (Zhang, 1995) and less-constrained ( a.k.a. alignable) tree edit distance (Lu et al., 2001).
 Those subclasses of the general tree edit distance de-termine respective M x,y , which are generally proper subsets of those define by (8). Since { M x,y } for the constrained tree edit distance is easily verified to be transitive, the resulting  X  K ( x, y ) turns out positive semidefinite by virtue of Theorem 2. In contrast to the constrained edit distance, { M x,y } for the less-constrained tree edit distance is not transitive. There-fore, Theorem 2 implies that  X  K ( x, y ) is not necessarily positive semidefinite. 4.1. Key Lemma Let X ij be m -dimensional square matrices parameter-ized by ( i, j ) = { 1 , . . . , n } 2 , and let X denote the de-the ( m ( i  X  1) + k, m ( j  X  1) + l )-element of X , denoted by X ij kl , is defined to be the ( k, l )-element of X ij . Furthermore, for an m -dimensional square matrix A , smry A ( X ) denotes the n -dimensional square matrix smry A ( X ) is given by Eq. (9). Proposition 1. For an m -dimensional square matrix A , the following are equivalent to each other. 1. A is positive semidefinite. 2. smry A ( X ) is positive semidefinite for an arbitrary Proof. First, we prove the assertion assuming that A is diagonal, whose I -th diagonal element is  X  I . The condition 2 implies 1, since we see  X  I  X  0 for any I by letting X be the sparse matrix such that X kl is 1, if k = l = I , and 0, otherwise.
 On the other hand, the converse follows from Eq. (10), since smry A ( X ) = Z T Z holds for the m 2 n  X  n matrix is an mn -dimensional matrix such that X = Y T Y . The general cases for non-diagonal A reduces to the diagonal case, since, for P such that P T AP is di-agonal, smry A ( X ) = smry P T AP (  X  X ) holds for  X  X = [ P 4.2. (1) Implies (2) Investigating whether K is positive semidefinite is equivalent to investigating whether the Gram matrices for finite subsets of  X  are positive semidefinite. There-fore, without any loss of generality, we may assume that  X  is a finite set { x 1 , . . . , x n } . Since M x We slightly extend the definition of (  X  0 , k, {  X  x } ) by adding a new element  X   X   X  0 such that k (  X  ,  X  ) = Even after the extension, (  X  0 , k, {  X  x } ) still remains an evaluating system for M .
 Next, we define  X   X  0 ,  X  M and {  X   X  x } as follows:  X  disjoint union  X   X  and  X   X  x (  X  x 0 ) =  X  , otherwise. Then, the mapping kernel K with respect to M and E is rewritten as follows. Furthermore, K ( x i , x j ) = tr( A T X ij ) holds, when we define m -dimensional matrices A and X ij for  X   X  0 = {  X  x 0 otherwise; X ij kl = k (  X   X  x To show the assertion, it suffices to prove A is posi-is positive semidefinite by definition). A is symmet-ric, since ( x 0 , y 0 )  X  M x,y  X  ( y 0 , x 0 )  X  M y,x holds. The hypothesis that { M x,y } is transitive implies that { 1 , . . . , m } is decomposed into U 1 t X  X  X t U M such that: U a  X  U b =  X  , if a 6 = b ; (  X  x 0 k ,  X  x 0 l )  X  if, k, l  X  U a for some a  X  { 1 , . . . , M } . Therefore, A = semidefinite, since so are A [ U a ]. 4.3. (2) Implies (1) We prove the cotraposition of the assertion. If M is not transitive, A includes at least one of the following sub-matrices (without any loss of generality, we may dimensional matrix whose (  X ,  X  )-element is A i Note that any of them has a negative eigenvalue, since det A &lt; 0 holds.
 We will see that there exists an instance of E = (  X  0 , k, {  X  x } ) such that smry A ( X ), which is the Gram matrix for  X  , is not positive semidefinite, if any of the above three cases occurs. In the remaining of this section, we will give a proof only for the case where Eq. (13) holds. The assertion for the simpler cases, that is, where either Eq. (11) or (12) holds, can be proved in almost the same way.
 Let i , j and a denote the indices such that x 0 k  X   X  0 x as the disjoint union of  X  0 x for x  X   X  ). The indices are not necessarily different from each other. Further, let column vectors ~e 1 , ~e 2 and ~e 3 be an orthogonal basis of We assume  X  1 &lt; 0 without any loss of generality, and define positive semidefinite K as follows.
  X  tr( A [ k, l, b ] T K ) Now, we define E = (  X  0 , k, {  X  x } ) as follows.  X   X  0 = { X  ,  X ,  X ,  X  }  X   X   X  x ( x 0 ) = Below, we investigate three cases: the indices take the same value, that is, i = j = a ; two of the indices coincide with each other, where we can assume i = j 6 = a without loss of generality: the indices are different from one another, that is, i 6 = j 6 = a 6 = i . For each case, we see that some diagonally located submatrix of smry A ( X ) is not positive semidefinite. This implies that smry A ( X ) itself is not positive semidefinite. Case i = j = a : The submatrix smry A ( X )[ i ] is not Case i = j 6 = a : We will show that smry A ( X )[ i, k ] is Case i 6 = j 6 = a 6 = i : For  X ,  X  = 1 , 2 , 3, the (  X ,  X  )-Aoki, K. F., Yamaguchi, A., Okuno, Y., Akutsu, T., Ueda, N., Kanehisa, M., &amp; Mamitsuka, H. (2003).
Efficient tree-matching methods for accurate carbo-hydrate database query. Genome Informatics , 14 , 134  X  143.
 Barnard, D., Clarke, G., &amp; Duncan, N. (1995). Tree-to-tree correction for document trees (Technical Re-port 95-375). Queen X  X  University, Kingston, Ontario K7L 3N6 Canada.
 Berry, V., &amp; Nicolas, F. (2004). Maximum Agreement and Compatible Supertrees (Extended Abstract). CPM (pp. 205 X 219).
 Bunke, H. (1997). On a relation between graph edit distance and maximum common subgraph. Pattern Recognition Letters , 18 , 689 X 694.
 Collins, M., &amp; Duffy, N. (2001). Convolution kernels for natural language. Advances in Neural Infor-mation Processing Systems 14 [Neural Information
Processing Systems: Natural and Synthetic, NIPS 2001] (pp. 625 X 632). MIT Press.
 Cristianini, N., &amp; Shawe-Taylor, J. (2000). An in-troduction to support vector machines and other kernel-based learning methods . Cambridge Univer-sity Press.
 Haussler, D. (1999). Convolution kernels on discrete structures UCSC-CRL 99-10). Dept. of Computer Science, University of California at Santa Cruz. Hein, J., Jiang, T., Wang, L., &amp; Zhang, K. (1996). On the complexity of comparing evolutionary trees. Discrete Applied Mathematics , 71 , 153  X  169. Jansson, J., Ng, J. H. K., Sadakane, K., &amp; Sung, W. K. (2005). Rooted maximum agreement supertrees. Al-gorithmica , 293  X  307.
 Kashima, H., &amp; Koyanagi, T. (2002). Kernels for semi-structured data. the 9th International Conference on Machine Learning (ICML 2002) (pp. 291 X 298). Kuboyama, T., Shin, K., &amp; Kashima, H. (2006). Flexi-ble tree kernels based on counting the number of tree mappings. Proc. of Machine Learning with Graphs . Leslie, C. S., Eskin, E., Cohen, A., Weston, J., &amp;
Noble, W. S. (2004). Mismatch string kernels for discriminative protein classification. Bioinformat-ics , 20 .
 Li, H., &amp; Jiang, T. (2005). A class of edit kernels for svms to predict translation initiation sites in eukary-otic mrnas. Trans. on Comput. Syst. Bio. II , LNBI 3680 , 48  X  58.
 Lodhi, H., Shawe-Taylor, J., Cristianini, N., &amp;
Watkins, C. J. C. H. (2001). Text classificatio us-ing string kernels. Advances in Neural Information Processing Systems , 13 .
 Lu, C. L., Su, Z.-Y., &amp; Tang, G. Y. (2001). A New Measure of Edit Distance between Labeled Trees.
LNCS (pp. pp. 338 X 348). Springer-Verlag Heidel-berg.
 Menchetti, S., Costa, F., &amp; Frasconi, P. (2005). Weighted decomposition kernel. Proc. of the 22nd International Conference on Machine Learning . Pelillo, M. (2002). Matching free trees, maximal cliques, and monotone game dynamics. IEEE Trans-actions on Pattern Analysis and Machine Intelli-gence , 24 , 1535  X  1541.
 Salzberg, S. L. (1997). A method for identifying splice sites and translational staqr sites in eukaryotic mrna. Computer Applications in the Biosciences , 13 , 365  X  376.
 Suzuki, J., Isozaki, H., &amp; Maeda, E. (2004). Con-volution kernels with feature selection for natural language processing tasks. Proc. of the 42nd An-nual Meeting of the Association for Computational Linguistics (ACL) (pp. 119 X 126).
 Tai, K. C. (1979). The Tree-to-Tree Correction Prob-lem. JACM , 26 , 422 X 433.
 Wagner, R., &amp; Fischer, M. (1974). The string-to-string correction problem. JACM , 21 , 168 X 173.
 Zhang, K. (1995). Algorithms for the constrained edit-ing distance between ordered labeled trees and re-lated problems. PR , 28 , 463 X 474.
 Zien, A., R  X atsch, G., Mika, S., Sch  X olkopf, B.,
Lengauer, T., &amp; M  X uller, K. R. (2000). Engineer-ing support vector machne kernels that recognize translation initiation sites. Bioinformatics , 16 , 799
