 The usage of data in many commercial applications has been growing at an unprecedented pace in the last decade. While successful data mining efforts lead to major business ad-vances, there were also numerous, less publicized efforts that for one or another reason failed. In this paper, we discuss practical lessons based on years of our data mining experi-ences at Yahoo! and offer insights into how to drive the data mining effort to success in a business environment.
We use two significant Yahoo X  X  applications as illustrative examples: shopping categorization and behavioral target-ing; and reflect on four success factors: methodology, data, infrastructure, and people.
 H.3.3 [ Information Search and Retrieval ]: Clustering, Information filtering, Relevance feedback, Retrieval models, Selection process Algorithms, Experimentation, Performance Industrial practice and experience, large-scale statistical mod-eling, advertising and optimization, classification and clus-tering  X  This work was conducted at Yahoo! Labs, 701 First Ave, Sunnyvale, CA 94089.  X  This work was conducted at Yahoo! Labs, 701 First Ave, Sunnyvale, CA 94089.

Over the past decade, machine learning and data mining have become enabling forces of the Internet businesses, from search, advertising, communication, to online marketplace, commerce, and social network. The objective of this paper, however, is to inject  X  X  dose of reality X  into the tremen-dous efforts and successes made along the course, from a retrospective and practical point of view. We observed at least two remarkable gaps between an academic setting and the industrial practices. First, the problem formulated, par-ticularly the objective function to be optimized, does not necessarily embody what really matters to users and other stakeholders. One good example is using the click-through rate (CTR) as a relevance measure for ads. We would argue that no less amount of efforts should be devoted to defin-ing a faithful and holistic measurement of the problem un-der study. Second, the large scale of a real-world problem, particularly the massive data on the Web, has become a touchstone of the viability and applicability of academic de-velopments. Making sense of such Web-scale datasets not only raises computational challenges, but also presents in-teresting theoretical questions. For example, it is less obvi-ous than some rules of thumb how many data examples are needed to fit a linear model with millions of parameters.
Successful data mining efforts lead to major business ad-vances, as illustrated by a profusion of algorithmic innova-tions, open source code development and corporate invest-ment focused on data mining. In the offline universe, the oft-mentioned stock market prediction efforts or fraud de-tection mechanisms come to mind. We are most interested in the online universe, which comes with a host of chal-lenges that rarely allow for a straightforward application of any data mining algorithm. Successes in this arena are much publicized, from recommender systems popularized by Ama-zon [23] and used by Netflix [4], to classification techniques used by the various comparison shopping sites such as Ya-hoo! Shopping [25] and predictive algorithms used in ad targeting by companies like Yahoo! [11, 10] and Google [2].
However, for every such success follows tens of failed at-tempts. The vast, unorganized and fast-changing nature of the online world that requires applications to be ultra re-sponsive is a major cause. In this paper, we will discuss these challenges in detail and outline the lessons we have learned in overcoming them.

We will use the following applications to illustrate the discussion: 1. Name: Yahoo! Shopping Categorization [25]. 2. Name: Yahoo! Behavioral Targeting (BT) [11].
Keep in mind that machine learning and data mining is today empowering a myriad of business applications at Ya-hoo!, and the ones above are chosen for illustrative purposes.
Many companies fail to take full advantage of their data because they do not apply data mining techniques to study, manage and learn from their data. While using humans as editors or as document classifiers to work with small sets of data, it can not scale to the terabytes, or even petabytes of data that companies are amassing today. In the online world, not only is the data huge in size, it is also rapidly changing. Users may be interested in travel deals this week and in mortgage loans the next. The smart method of both managing the large-scale datasets and taking full advantage of changes in user interests, is to apply data mining methods that model this shift well.
 Consider the Yahoo! Shopping categorization example. At the dawn of its existence, Yahoo! Shopping applied rules that would match particular terms and phrases in the text of the product to drive categorization. In this scenario, the product is assigned a category implied by the first (high-est ranked) rule that matched the product. This approach worked well when the number of categories was restricted to a dozen or so. But when the taxonomy grew to more than 1000 nodes, it was clear that maintaining rules, their excep-tions, exceptions from those exceptions etc., was hideously expensive, not to mention unmaintainable. Hence, data mining methods became a necessity. Today, the categoriza-tion for Yahoo! Shopping is fully automated and relies on a scalable classifier, trained on available labeled merchant data and vast domain knowledge to provide high degree of accuracy and coverage of products.
In this section we will use the BT application for illustra-tion but the main concepts translate to other applications as well. A typical process of data mining in practice is shown in Figure 1.
 Figure 1: A practical data mining process with both offline and online horizons.

Although the above data mining framework is crystallized from our experiences in large-scale data mining in an indus-trial environment, it fits well into the state-of-the-art KDD (Knowledge Discovery in Databases) process [19], in particu-lar the leading data mining methodology in practice, that is, the CRISP-DM (Cross Industry Standard Process for Data Mining) framework [27]. One key difference of our process from the classical frameworks is that we explicitly call out the dichotomy between the offline horizon and the online perspective, since our experiences show that bridging the gap between these two presents critical challenges, often-times can dominate the ultimate success or the reverse.
It really is not obvious to an audience outside the data mining community that data rules, and not any preconceived notions or even offline stereotypes. Companies habitually rely on their  X  X ut feelings X  instead of relying on the data to drive decision-making.

That being said, one should not underestimate the impor-tance of domain knowledge. We argue that domain knowl-edge should guide empirical investigation, especially at the exploratory stage; and serve as a sanity check on empirical results.

In the BT application for example, one might expect search queries to be predictive of ad clicks since they are self-expressed user interests, which has also been confirmed in the sponsored search domain [26]. However, we found em-pirically that in the BT scenario, user queries are much less effective than historical ad clicks in predicting future clicks, even inferior to page view features. In one of the empirically optimal models for BT [11], search queries only account for 30K features out of a total of 150K, after cross-validated fea-ture selection. On the other hand, one extreme of the data-driven approach would use all features available from data, assuming we have sufficient amount of data to fit them, and this would lead to 8M features mostly comprised of queries. Our experiments have shown that this is an unnecessary computational challenge to address. Aggressively reducing the number of search queries, which otherwise is unbounded, not only has computational advantages, but also results in better model performance in prediction accuracy.
The data mining process starts with data preprocessing, or so-called ETL (extract, transform and load), during which raw user data logs go through a series of perturbations and get loaded into a data warehouse (DW). For the purpose of BT, data is joined from multiple feeds from the DW to produce a comprehensive and consistent picture of user ac-tivities across time. The joined data is processed (also called datagen for data generation) to compute feature-vector rep-resentation of the user data across time. The feature-vector data is made available for data analysis, modeling and eval-uation, resulting in a model which is subsequently deployed in production.

Many data mining projects fail because of the lack of ade-quate data. Below we touch upon the major issues we came across and explain why they may have a devastating effect on the data mining project:
In general and at Yahoo!, the availability of sufficient data is a myth: you either require manually labeled data leading to imbalance, as with shopping categorization, or need to estimate CTR on categories with very few clicks available in historical data.

For manual categorization, unless you are using the wis-dom of crowds, e.g., Amazon X  X  M-turk [29], you need to secure budget to pay for manual labeling work. And be warned, those people will make errors, have slow turnaround and ignore instructions. Unfortunately, this is a necessary step.

As to CTR estimation, you have to get all positive ex-amples you possibly can (meaning the clicks in the target category) and deal with their low quantity. You can argue that if the number of views is small as well, then these ad categories have little demand and can be ejected from your modeling exercise. But in the alternate case of high views and low clicks, you need to figure out how to boost the num-ber of clicks for modeling. The standard practice is to use the above mentioned business intelligence in a form of pos-tulating rules. For example, if certain events happen in the user history, then the user is automatically qualified for a category, hence in a live deployment she will be shown the ads in this category; and if the manual guess was correct, more clicks will come. Then these clicks are fed into a ma-chine learning algorithm to learn a model on a wider set of features than just the rules, the model is deployed and the cycle of click harvesting is repeated.

This discussion nicely brings us to the topic of down-sampling which in our view one should avoid as much as possible. It may be that you would like to do exploratory data analysis, then the properly selected random sample of data is fine. If you do not have a Hadoop cluster or budget to rent the cloud from Amazon, or the real-time produc-tion system has a strict memory or time constraints, and your computation will not scale unless you down-sample, then perform sampling with extra caution such as cross-validation. But, in general, if you can afford working with all data you got, do it. In BT for example, ad CTR is extremely low, typically a fraction of a percent, thus a pre-dictive model to estimate clicks is inevitably  X  X ata-hungry X . As shown in Figure 2, the BT model performance in terms of relative CTR lift over a baseline increases monotonically as the size of training data increases from 32 to 512 buckets (one bucket is a 1/512 random sample of the full user data). The CTR is measured at the current production operating point of view recall and the relative CTR lift is the CTR ob-tained from the proposed model subtracted by the one from the baseline model, the BT model is a non-negative Poisson regression with granular features (individual ad clicks and views, page views, and search queries) [11], and the base-line is the current production model using sign-constrained linear regression with categorized features [14].

That said, stratified sampling is a different story. Since the click-through data used for BT is extremely sparse, it is advantageous theoretically as well as computationally to sample negative sub-population (examples with zero clicks and views) independently, while preserving positive exam-ples (examples with non-zero clicks or views) to the largest extent that computation is still tractable. In fact, for such a highly skewed dataset without stratified sampling, any clas-sifier would tend to trivially label all examples to be negative to get a reasonably low classification error. Figure 3 shows the benefit of aggressively sampling the negatives while keep-ing all the positives.
The real-world data, even after ETL, is going to be a long way far from what you expect after reading textbooks where the core assumptions are  X  X tationary and i.i.d. X  or what you get to analyze in the UCI Mushroom dataset [6]. This means that you will need to go the extra mile, or maybe 10, to get to the production solution.

In practice, the samples are not i.i.d., nor are their dis-tribution stationary. For instance, the ad-serving in behav-iorally targeted graphical ads takes the predictions of a user behavioral model into account, but on top of it adds a thick layer of business logics, such as delivery constraints, bud-gets, time of year, to name just a few. Often times, these Figure 2: The dependence of model performance in relative CTR lift on training data sampling ratio. The CTR lift was measured from a Poisson regres-sion over a linear regression as the baseline. The training data was randomly sampled at ratios of: 0.0625 (32 buckets), 0.125 (64 buckets), 0.25 (128 buckets), 0.5 (256 buckets), and 1 (512 buckets) [11]. business logics may go completely orthogonal to model X  X  pre-dictions. It may be that none of the model X  X  predictions are honored just today, because, say, of the end of quarter, but tomorrow things are going to be back to normal. How can you reliably measure things under these circumstances and how can you use this data for modeling is a big challenge. A brute-force approach is to update your model very often, possibly incrementally, and filter out some of the data that you know is contaminated.

Due to the non-stationarity of data discussed above, the actual data sent in production to the classifier or the predic-tor may be quite different from the data it was trained on. Consider, for instance, the real-life example of a bumper-sticker merchant who sent its data feed to Yahoo! Shop-ping for categorization and whose product title was each the sticker said, and not a bumper sticker, as one would expect. Imagine the results of the classifier that obtained product titles such as  X  X f you can read it you are too close X  and  X  X  X  X  rather be flying X .

Several solutions are possible here: idenitfy merchants whose products are distributed across all categories (com-pute for example the empirical entropy over the set of all categories), and mark them for manual review. This avoids amusing users and embarassing the company. The catego-rization outcome can be repaired by smart feature engineer-ing: consider for instance a merchant ID feature that you put into a classifier. As long as you have some manual sam-ples from this merchant, this feature plays a role of a terrific regularizer by effectively giving low weight to all categories Figure 3: The dependence of model performance in relative CTR lift on negative sampling ratio (the ra-tio of negative examples randomly sampled for train-ing), while keeping all positive examples. Here a negative example is defined as a training example with zero ad clicks and views, and a positive exam-ple contains non-zero ad clicks or views [11]. that this merchant X  X  products were never observed to be-long to. Rightly so: there are very few mall-like merchants relative to the boutique ones.

A similar problem arises if the data was not properly col-lected as a random sample of the population. For example, an ad targeting model can be viewed a system that gener-ates user ad clicks. It is safe to assume that the labeling this system produces is biased by the assumptions made in the existing model and hence is not a random sample. If, in the extreme case, the system chooses to only target those who clicked on an ad before, then clearly the resulting system will have a quite limited recall and using the data for train-ing will likely yield a suboptimal classifier. Even with the merchant ID example above, if the manual labeling focuses only on the high-entropy merchants, you may not be doing a fair random-enough job labeling all the data. Hence, it is imperative to supplement the biased labeling with truly random one.

Another possibility is to only focus on the decision bound-ary estimation for a classification task, so that labeling near-boundary examples will make most sense as opposed to do-ing a broad labeling that may not affect the boundary, or a social implementation of active learning [15].

Recall that in both of our applications, we are not in-terested in simply obtaining best possible accuracy on the training data, we want our models to generalize well beyond it. Thus understanding the underlying data distribution and compensating for any bias is essential.
One na  X   X ve approach, especially with the availability of cloud computing, by a researcher is to set up thousands of experiments exploring the entire model space. But in light of the issues just mentioned, overfitting of the data becomes a potential trap. In this context, we would also like to pitch the  X  X ook at and know your data X  message. By no means can brute force experimenting be a substitute for understanding the data.
 Remember also that every domain application is unique. But basic machine learning philosophies [18], including Oc-cam X  X  razor and no-free-lunch theorem, hold true. Occam X  X  razor prefers simpler models, and so is production  X  it is faster and easier to maintain. No free lunch essentially trans-lates into a necessity to perform due diligence with every new dataset and project you get in terms of exploratory analysis and modeling, it is unique and so is the solution that you may need to pursue.

We applaud the start-ups that venture into projects such as content-match, search or behavioral targeting without data. Ad-networks would be a great example: in case of CTR estimation, they lack the massive volumes of data that Yahoo! has. How do they go about it? Well, they buy the data from the data distributors  X  those companies willing to share their traffic. However again given $1, what data should we buy that would translate best into the features for my application?
In industrial data mining, building a well performing model is typically part of a bigger optimization problem. The biggest reason of failure of modeling, based on our experi-ence, is inadequate understanding of how the model is going to be used when deployed. Often though, this is not a tech-nical problem but a lack of transparency with business. For example, if BT model predicts the probability of click in a particular ad category, but most user sessions will be served by a higher priority demographic, geographic or property targeting, then this needs to be accounted for (e.g., to avoid cannibalization) by modeling and, subsequent to modeling, optimization. In other words, one needs to answer a ques-tion of when it makes most business sense for BT to handle the user versus any other targeting method. Another exam-ple is relevance ranking in search: even if you have the ideal predictive relevance model per URL, showing two identical very relevant PDFs in response to a query will result in a bad user experience.

We use the term problem overfitting to refer to a scenario in which the model works well on the test data but badly in the holistic optimization setting, which contains the data mining model as one of the ingredients. In this context, it is useful to emphasize that once the model is built and is ready for deployment, in most cases the A/B testing [22] or bucket testing used to test the quality of the new pipeline against the older version is very expensive. The cost comes not only from building and maintaining expensive bucket-testing infrastructure but also from having to take a  X  X ite X  from the revenue-sensitive live traffic. A practical solution on which business will invariably insist, is simulation of the real-world optimization and serving envirnoment in which the model will be deployed, in other words, a faithful offline evaluation system.
We learned so far that data is important and QA is cru-cial. Using data to build high quality and high yield models requires the cost of substantial infrastructure to support the entire process, from ETL to testing and evaluating of the built models, as elaborated below.

First, we need hardware, in terms of clusters of machines, preferably running Hadoop [1] and plenty of disk space to store and process multiple sets of data and models, that is, a platform of both computation and storage in the scale of terabytes or even petabytes.

Second, the software is required for the data operations, model training, evaluation, and so forth (recall the data min-ing process diagram in Figure 1). Note that the models and optimizations built in highly unscalable R and Weka interfaces will work for exploratory data analysis, but they are not designed and thus will not scale beyond moderate datasets. Hence the underlying algorithms will also have to be coded and available on the chosen platform.
 Note that any evaluation of a model must be realistic. That is, evaluation of models should be over data as available in a production environment with all its myriad constraints. For instance, an architectural constraint amounting to in-crementality of the model with respect to the input data may exist in production. This implies, in BT for example, that as soon as an activity of a user is observed, it is im-mediately priced into the user score by applying the model, incrementing his or her score and subsequently discarding the obsolete one. This significantly limits the space of al-lowable models, with linear models being one possible class (but impractical for kernel SVM), while saves a lot of online time and space. In such a case, evaluation software should fully respect these constraints so model performance is truly reflective of the reality.

Ideally, in a production environment, results need to be re-producible and operations need to be repeatable, with mini-mum human involvement; so that a person or ideally a script can launch training over newly obtained data to produce a new model. This means that the code needs to be bug-free (if such a state is at all achievable), well documented and QA-certified which hardly comes out of any research exer-cise.

For a practical guideline, we highlight four metrics that are of importance in training and production deployment: 1. Model offline training time  X  recall that you may want 2. Model online prediction time  X  the amount of time 3. Model size or memory footprint  X  the size of the model
Ideally, we would like our model to be compact, fast to train and run in production and perform well. The require-ment of model deployability collected from the data mining exercise may result in not only upper bounds on the model training or online prediction time, model size or memory consumption, but also on what features may be allowed in the model. In the BT application, certain activities are ex-tremely frequent, taking for instance ad views (every page on Yahoo! may have half a dozen of various ads) or Yahoo! Mail or Yahoo! Frontpage (www.yahoo.com) page views. It will not necessarily be a wise move to discard these activities from the model like it is customary to do with the stopwords in text categorization, as the former may bear important in-formation. In production processing, however, any frequent request like this may in turn require additional hardware to meet the online time constraint which in turn translates into investment. It is often, and understandably why, preferred to have a scientist come up with a good model subject to constraints (or argue why it cannot be done) than to buy more hardware that translates not only into capital expen-diture but also future electricity and maintenance cost.
Given the desired infrastructure, such as the Hadoop grid at Yahoo! and the distributed file system (DFS) [21] and the MapReduce framework [16] at Google, the philosophy of algorithm design and implementation should also change, particularly in parallelization or so-called gridification of tra-ditional algorithms.

It is almost certain that, at a Web scale, offline machine learning and data mining workloads are IO-bound or specif-ically scan-bound [3], where the running time is dominated by file IOs for sequential scans. In-memory computation, even as expensive as sorting, becomes a secondary term. The first recipe for a IO-bound computing task is to dis-tribute the data, which is trivially offered by a cluster like Hadoop. But still, every effort should be made in designing and implementing the algorithm to minimize the number of full scans of the data (note that a DFS as in Hadoop does not support random access), because the scalability achieved by simply increasing the number of nodes can never exceed linear. Given for instance 1TB data distributed across 1000 nodes, each node is then allocated 1GB data to process; and hence a polynomial or exponential time algorithm may still not be practical. To minimize the number of file IOs and data scans, one important design trick is to cache the input data in memory and process it in batch. The computation sequence in the algorithm should be organized in such a way that it leverages the data locality (in memory) to the largest possible extent. In BT for example, to fit a Poisson regres-sion model, for each example x we need to compute the dot products with all relevant weight vectors w &gt; x and a tensor product with its target vector yx &gt; , as well as some sum-mary statistics such as the total count of the j th feature event P i x ij where i indexes examples. All these compu-tations should be realized when the example x is in cache, or incrementally performed for global tasks like calculating summary statistics. Caching input data is especially appeal-ing in a Hadoop cluster, since the default data block size is 128MB and fits well into memory.

The viewpoint of complexity analysis of a large-scale learn-ing algorithm should also be changed. First, we agree with Daniel Spielman X  X  recent argument at NIPS 2008 that a smoothed analysis (typical-case analysis) [28] should play a more significant role than the traditional worst-case anal-ysis. Second, unlike the traditional run-time analysis, the constant term in the big O notation should be seriously taken into account. Given two algorithms both having a linear-time complexity, O (1 n ) and O (4 n ) may differ by days in execution time, as in our BT X  X  case. This is because we really care less about the asymptotic case than the almost worst case at hand.

It is yet to be fully investigated whether the state-of-the-art parallel programming paradigms [9] are suitable for the popular machine learning algorithms. One challenge we ob-served is that most optimization methods used for large-scale learning, such as conjugate gradient descent and EM, involve iterative algorithms and thus synchronizations be-tween consecutive iterations. This leads to the so-called fine-grained or coarse-grained parallelism, which is consid-ered non-trivial to parallelize. One rule of thumb, we found works very well in practice, is to make the synchronization step as light-weight as possible. This requires an intelligent design of parallelizing the computation tasks to achieve the final goal efficiently. In BT for example, it is tempting to distribute the calculations by some business concepts such as category, and hence reaching a suboptimal parallelization suffering from the highly skewed traffic distribution across categories (bad load balance), e.g., a  X  X inance X  category is much more trafficked than a  X  X ife Staging X  one. A good par-allelization should be oblivious of and immune to the likely undesirable domain data distribution. One general guideline is to distribute the computation tasks at a fine granularity, and thus better randomization. Our parallel BT learning algorithm implemented in Hadoop MapReduce framework, for instance, achieves this by distributing the updates of the weight matrix by cell or the target/feature pair indexed by ( k,j ). The resulting system can easily run on 500-1000 nodes without any parallelization bottleneck.
Finally, in this Infrastructure section, we would argue the right principle of scaling up algorithms to terabytes or even petabytes of data. The state-of-the-art parallel computing frameworks such as Hadoop and Google X  X  MapReduce imple-mentation perform an outstanding job in distributing large-scale data into logical blocks and hence facilitate processing vast amounts of data in parallel. This design paradigm is extremely suitable for simple operations on large datasets, such as aggregation and filtering; this is also why some high-level SQL-like languages on top of MapReduce have become popular, examples are Pig Latin [24], DryadLINQ [30], and Jql [5]. On the other hand, learning at the Web scale largely prohibits complicated models, at least at the current level of computing art. The Poisson regression used in BT is a generalized linear model [11], and hence it is fast to learn and inference. Nevertheless, we should note that the cur-rent parallel computing infrastructures have not put great efforts to optimize per-operation efficiency, which we believe should be taken as the foremost step in scaling. This is even more so when the underlying models become complicated such as Bayesian methods [20] and probabilistic latent vari-able models [7], and the problem domains exhibit special structures such as a very high level of sparsity.
 We had surprising yet reflective findings in this regard. In one recent work of applying the GaP model [8] to ad targeting [12], a highly efficient single-machine implementa-tion with sparse matrix operations yielded above 100 Mflops, which is in the same order of magnitude as a parallel im-plementation using a 250-node Hadoop cluster. Therefore, although many researchers and practitioners assume that large-scale parallelization is the best way to deal with scal-ing, we argue that in fact implementation issues (e.g., cache efficiency and data encapsulation) still amount to orders-of-magnitude differences in computational performance and may even overwhelm the additional nodes. The right prin-ciple, and also a resource-conservative one, is to start with single nodes that achieve optimal per-operation efficiency (especially for floating-point and matrix operations), for in-stance exceeding 100 Mflops, with sparse arithmetic opera-tions. It might be possible to achieve this with the current parallel computing frameworks such as Hadoop, but it will require some customization (low-level implementation) and likely new data structures (untagged numerical data blobs).
As you can see from the lessons above, the data contains value but mining it amounts to an exercise requiring good knowledge, skill and proper infrastructure. This is the rea-son why data mining specialists with interdisciplinary back-ground have been in high demand in technological job mar-ket  X  many of these tasks are difficult to completely auto-mate. Talking about people involvement we will use the following loose notation to illustrate our point: Here, technology transfer is the process of taking a success-ful prototype created in the incubated environment on a potentially sterile subset of data and making it work in a production environment on the larger un-massaged dataset.
First, the product managers and the scientists have to work together in defining and detailing the business prob-lem so that (1) the scientists build a model that will indeed generate revenue for the company, and (2) the product man-agers understand proper utilization of the model. There is no well-defined methodology for this step. The two sets of people have to converse and comprehend each other.

Second, the production engineering team is engaged so that the scientists understand the nature and limitations af-forded by the production pipeline. Most of these issues have been outlined earlier under the Data and the Infrastructure sections. Only then, can the scientists get to their tasks of understanding the data and building a model that will in-deed solve the business problem and be usable within the engineering framework. This is not an easy task. Scientists should push the boundaries of the engineering infrastruc-ture to build better models, while the engineers will have to constrain the scientists to reality. In doing so, friction and tension may arise between the two sets of people, which needs to be carefully, properly and decisively managed for success.

Finally, once a model is built, measured and deemed ready for deployment, operations and reporting teams on the en-gineering and business sides need to be trained so the model is properly evaluated and the revenue impact is faithfully measured. Again, this requires a lot of communications be-tween the engineering and operations teams as well as the modeling and reporting teams so a feedback flow is estab-lished. People are involved in every step along the way in a manner that simply cannot be automated. The success of the entire operation depends on how well communications and coordination between the various teams is managed and nourished.
One of the most valuable categories of people making a real difference, are the scientists developing the technology to solve business problems. Funding theoretical and long-term academic research is an important, oft-overlooked com-ponent of technology development. Engaging this wide com-munity of researchers is a necessary means of solving the toughest business problems, if the process of engagement is well-defined.

Some ten years ago or so, the art of machine learning, data mining and knowledge discovery was already fairly well de-veloped in the research community: graduate machine learn-ing programs were open under the umbrella of AI in all ma-jor universities, conferences like KDD, CIKM, ICML, NIPS had seen years of submissions and ever growing attendance. All major companies with research labs, such as IBM, Mi-crosoft, Google, Yahoo!, eBay, AT&amp;T, NEC hosted groups of researchers working on KDD problems with a business angle. Nonetheless, the number of successful business de-ployments of KDD techniques was quite small and due to several reasons.

The data sharing with academic world was miniscule. This was mostly because companies were not able to find a mu-tually beneficial agreement with academia so that academia obtains access to valuable industrial datasets while the busi-nesses gain sole access to proficient models. Companies had to establish policies and mechanisms for a successful collab-oration. One such mechanism has been known and used for a while: inviting professors and their students to work with a company, e.g., on an internship or on a contract. While these relationships were highly productive, the policies for wider data sharing with the research community have only recently become a trend: for instance, Netflix or Microsoft challenges that bear not only data sharing but also a mone-tary reward for the winners, and promise of better business for the companies. It is worth a final mention that this trend is highly welcome by the research community and it would be valuable to have more companies open up their data stores, even if the final delivery to academia undergoes transformation via a number of anonymization filters.
In many cases the research done in the universities is highly disjoint from the architectural and real-world con-straints of the business, making research results less usable. Even given a nice incubated result, it is a long road before the model can be put into use in a real-world environment. Such unproductive collaborations, along with a lack of data and IP sharing policies, have made companies wary of aca-demic collaboration, preventing them from investing in data mining research.

As our experiences shows, the only way to arrive to a suc-cessful revenue-bearing data mining application is for all in-volved parties to work together: modeling sciences are done by research scientists and their academic advisors, engineers are involved in the process from day one so that various ar-chitectural constraints are brought to light and accounted for in research, product managers are engaged so the re-searchers understand the actual business problem at hand as well as the relevant domain knowledge, and the product managers in turn understand proper utilization of the model, and most importantly, proper offline and online metrics are put in place to quantify success. We would like to thank John Canny, Padhraic Smyth, Usama Fayyad and the whole Yahoo! Research Labs team for many productive discussions, support and advice. [1] http://hadoop.apache.org/. [2] S. Agarwal, P. Renaker, and A. Smith. Determining [3] D. Andersen. Using a fast array of wimpy nodes. In [4] R. M. Bell and Y. Koren. Lessons from the Netflix [5] K. Beyer. Jaql. Hadoop Summit 2008 . [6] C. L. Blake and C. J. Merz. UCI repository of [7] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [8] J. F. Canny. GaP: a factor model for discrete data. [9] E. Chang. Scalable collaborative filtering algorithms [10] Y. Chen, D. Pavlov, P. Berkhin, and J. F. Canny. [11] Y. Chen, D. Pavlov, and J. F. Canny. Large-scale [12] Y. Chen, D. Pavlov, J. F. Canny, and E. Manavoglu. [13] C.-T. Chu, S. K. Kim, Y.-A. Lin, Y. Yu, G. Bradski, [14] C. Y. Chung, J. M. Koran, L.-J. Lin, and H. Yin. [15] D. Cohn, L. Atlas, and R. Ladner. Improving [16] J. Dean and S. Ghemawat. Mapreduce: Simplified [17] K. Dixon. Yahoo cuts data retention to three months. [18] R. O. Duda and P. E. Hart. Pattern Classification and [19] U. M. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. [20] A. Gelman, J. B. Carlin, H. S. Stern, and D. B. [21] S. Ghemawat, H. Gobioff, and S.-T. Leung. The [22] R. Kohavi, R. M. Henne, and D. Sommerfield.
 [23] G. Linden, B. Smith, and J. York. Amazon.com [24] C. Olston, B. Reed, U. Srivastava, R. Kumar, and [25] D. Pavlov, R. Balasubramanyan, B. Dom, S. Kapur, [26] M. Richardson, E. Dominowska, and R. Ragno.
 [27] C. Shearer. The CRISP-DM model: the new blueprint [28] D. A. Spielman and S.-H. Teng. Smoothed analysis of [29] Q. Su, D. Pavlov, J.-H. Chow, and W. Baker.
 [30] Y. Yu, M. Isard, D. Fetterly, M. Budiu, U. Erlingsson,
