 Predicting CTR of ads on the search result page is an ur-gent topic. The reason for this is that choosing the right advertisement greatly affects revenue of the search engine and advertisers and user X  X  satisfaction. For ads with the large click history it is quite clear how to predict CTR by utilizing statistical data. But for new ads with a poor click history such approach is not robust and reliable. We sug-gest a model for predicting CTR of such new ads. Contrary to the previous models of predicting CTR of new ads, our model uses events -clicks and skips 1 instead of the observed CTR. In addition we have implemented several novel fea-tures, that resulted into the increase of the performance of our model. Offline and online experiments on the real search engine system demonstrated that our model outperforms the baseline and the approaches suggested in previous papers. Categories and Subject Descriptions: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval I.2.6 [Artificial Intelligence]: Learning General Terms: Economics, Experimentation, Perfor-mance, Algorithms, Measurement Keywords: click-through rate, sponsored search, paid search, Web advertising, CTR, CPC
Contextual advertising is a major source of the income of search engines. It is crucial for search engine companies to select advertisements properly, because it seriously affects both revenue and user X  X  experience.

There are several forms of online advertising, but in this paper we will restrict ourselves to the most common model: pay-per-click billing. It means that the search engine is paid every time the ad is clicked by a user. Such model implies that a search engine chooses advertisements that have the highest expected revenue, i.e. estimation of the probability of the click (CTR) multiplied by the cost-per-click of the ad. Here and further skip means an absence of a click There are two types of slots of sponsored search ads: the block of ads placed on the right side of the search results (East ads), and the block placed above the search results (North ads). In this paper we will restrict ourselves to the North ads only.

In the case of ads with large click history the probability of click might be estimated directly by utilizing statistical data. In the case of new ads with a poor click history such a straight-forward approach is not appropriate: empirical measures are not persistent and reliable. The problem may be solved by utilizing other properties of the ads: title, text, url (i.e. landpage of the ad), keywords, click history of sim-ilar ads and other. From here on, an ad will refer to a com-bination of a particular ad presentation with a particular keyword.
 Regelson et al. [1] solved the problem of the predicting CTR of new ads as following. They clustered ads by the tex-tual similarity of the keywords in hierarchial clusters. Next, for predicting CTR of the ad they used such features as av-erage CTR of the ads in the same cluster as the given ad and average CTR of the ancestor clusters. Richardson et al. [2] suggested more sophisticated approach. They calcu-lated several types of features of each ad: the CTR of the phrase of the given ad (here and further  X  X hrase of the ad X  means the ad X  X  keywords), the CTR of similar phrases, fea-tures, that describe appearance of the ad, CTR of the land-page X  X  domain, relevance of the text of the advertisement to its phrase and others. Then they extracted collection of ads with the observed CTR (displayed more than 100 times) and learned the logistic regression model over described fea-tures on train set. Their approach demonstrated high per-formance on the test set in terms of Mean Squared Error and KL-divergence between observed and predicted CTR.
The problem of the experiment described above is that the model is trained and tested on the ads with large click history. But the task is to predict CTR of new advertise-ments. It raises a number of research questions: do new advertisements have any principal differences with the ad-vertisements with some click history? (Research Question 1) Is it sensible to learn a model using the ads with large click history as training examples and to apply it to new ads? (Research Question 2) How to prove that the learned model works well (the problem is we can not measure accuracy of the predictions of the CTR of the new ads, because there is no true(observed) values of the CTR of new ads) (Research Question 3) ? How to change the approach, suggested by Richardson, in order not to shift the collection to the ads with large history? (Research Question 4)
In this paper we demonstrate the difference between typ-ical new ads and typical ads with large history (RQ 1), and describe the problem this difference causes (RQ 2). Then we describe our algorithm that is based on a different target function -it predicts events (a click or absence of a click) instead of predicting CTR. This change allows us to include new ads into the training collection (RQ 4). We also describe several novel features of ads that we implemented. Next we describe offline and online experiments (RQ 3) on a real search engine, that proves that our algorithm outperforms the baseline and the approach suggested by Richardson et al.[2] Figure 1: Dependance of the CTR on the size of click history
High CTR of ads with large click history. We have implemented an algorithm similar to the algorithm sug-gested by Richardson et al. and conducted an online exper-iment. Detailed results are described in Section 4. Now we will focus on one peculiarity: the CTR values of the new ads predicted by the algorithm were statistically higher than the observed values. Indeed, both search engine and adver-tiser are interested to keep displaying ads with high CTR, so worthless and spam ads are blocked or deleted soon, or just displayed rarely. This is why ads with large click his-tory are mostly ads with high CTR. Figure 1 demonstrates the difference between CTR of new ads and ads with large click history. The algorithm that was learned on the ads with high CTR is likely to predict higher CTR. Such a bias of an algorithm might result into serious revenue loss: new ads get higher estimations of their CTR than they actually have. Consequently some of the good ads which have been successfully and repeatedly displayed are replaced with new ads which are, in fact, worse. Finally, users click less than they could, search engine earns less money, advertisers at-tract less potential clients, users are less satisfied.
Target function. Richardson et al. [2] used ads with observed CTR (i.e. large click history) as the objects we need to make predictions for. They used CTRs of such ads as target labels. Such an approach has the aforementioned problems. On the contrary we suggest to use impressions of ads as input objects and clicks (and absences of clicks) as the target labels. Generally, an ad may have several impres-sions over time, sometimes they are clicked and sometimes they are not. We aggregate all such cases into two types of records: where W eight 1 is the number of times when the ad was clicked, W eight 0 is the number of times when the ad was shown, but not clicked.

Now we can learn any classification algorithm that al-lows for weights of training examples and provides the confi-dence level of its prediction. Each new unseen advertisement should be input in the algorithm, confidence level returned by the algorithm is an approximation of the CTR of the given advertisement.

We have tried two classificators: logistic regression and stochastic gradient boosting over regression trees. The lat-ter demonstrated higher performance, so we used it in all experiments described further.
This section describes features that we used in our im-plementation of algorithm of Richardson et al. and in the implementation of our algorithm. Richardson et al. sug-gested the following groups of features: We have implemented all of them and found out that the last two groups of features do not give significant increase of performance, therefore we did not include them in our real-ization of Richardson X  X  algorithm. Other features mentioned above were included into feature sets of our algorithm and into feature set of our implementation of the algorithm of Richardson et al. Now we will describe our new features not mentioned by Richardson et al.

IDF of the words of the phrase. Richardson et al.[2] suggested to use the number of words of an ad X  X  phrase as one of the features. This is quite a strong factor: the more words the higher the CTR. We suggest to take into account IDF (inverse document frequency) of the words of the phrase. Inspired by this we implemented three features: sum, product and average IDF of the words of the keywords. Table 1 demonstrates high correlation of CTR and sum of IDF of the keywords. Here and further we replaced real values of CTR with the propotional values due to the pro-prietary nature of such information.

Statistics of the right side advertisements. As we mentioned above, in this paper we predict CTR of new ads Table 1: The dependence of the CTR of the IDF sum of keywords placed above the search results. Sometimes the advertise-ment has large click history of the right side impressions to the search results, but the history of the impressions above the search results is poor. Therefore we suggest to use his-tory of the right side impressions to build a few features. We propose the following three features: the number of im-pressions,the overall number of clicks of the ad on the right side, and the CTR of the ad on the right side. Our measures proved that these features have positive correlation with the CTR of the ad, but we do not present the details here due to the space limitations.

Ads similar by phrase and by text. Richardson et al. utilized average CTR of ads which had similar keywords. We additionally took into account the ads which are textually similar in terms of their titles and bodies. Moreover we used another definition of textual similarity. The Richardson X  X  definition is based on the number of shared words in the texts which are compared. Our definition is similar, but we take into account IDFs of the keywords. So we used features based on average CTR among ads with similar keywords, titles and bodies. Such features significantly improved the quality of our prediction model.
Before providing a description of the experiments we will shortly describe the scheme of predicting CTR of advertise-ments (any of them, not only new) in our search engine. The prediction model is based on a set of features, and one of the strongest of them is an empirical CTR smoothed with a pri-ori value. Currently search engine uses a simple approach; the a priori value is a linear function of two values: a number of words in the ad X  X  phrase and level of relevance of the ad X  X  phrase to the ad X  X  text. In our experiments, both offline and online, we used the estimation of CTR provided by our algo-rithm as a priori value of CTR of the ad. Smoothing formula is structured in such a way that the shorter the click history the stronger the effect of the a priori value. Essentially the most significant improvement was obtained on ads with a short click history; it will be demonstrated further.
Offline experiments. For our experiments, we used in-formation of two million ads that were displayed more than 250 times. This collection was used to calculate features based on the CTR of similar ads. Then we collected click logs for the week period for one million random ads. This collection was used as a training set. Next we collected click history of the next month period. This collection of several millions events was used as a test set. For each record of the test set our algorithm provided the value -confidence of a click. These values were used as a priori values of the CTR in the CTR predicting scheme described above.

Predicting scheme provides estimations of CTR, i.e. float numbers while true labels are observed events (clicks or ab-sences of clicks), i.e. binary numbers: zeros and ones. We suggest two ways of measuring accuracy of the predictions -mean squared error (MSE) and Pearson correlation (PC). Denote { y i } N i =1  X  binary sequence of events (clicks and not clicks), { w i } N i =1  X  corresponding weights, i.e. number of re-peats of the given event in the collection, and { ctr } N corresponding predicted CTRs. Then MSE =
In the experiments we used Pearson correlation as the pri-mary measure of performance as we empirically know that it is strongly correlated with the revenue of our search engine. In the same time Pearson correlation is invariant to a linear modifications of inputs, that is why we tried to maximize Pearson correlation and also required that implementing of our algorithm did not result into increase of MSE. This re-quirement was fulfilled in all the experiments described fur-ther. Performance of each model was measured on four var-ious subsets of the test set. First three subsets contain the ads with a specific number of impressions: not more than 10, 20, 50 impressions. The last subset is a full test set.
Groups of features We examined effects of various groups of features described above: 1. features of the phrase of the ad 2. features that describe relevance of the text of the ad-3. features of the ads similar by phrase 4. features of the ads similar by text 5. statistics of the  X  X ight side advertisements X  6. statistics of the landpage X  X  domain We sequentially, one by one in the order given above added each of the groups to the feature set of our model. At each step the model was trained. Figure 2 presents the effect of each group of features on various subsets of the test set.
The most significant is the first histogram which demon-strates results on the newest ads (with 10 or less impressions in the click history). Each group of factors increases the quality of the algorithm.

Filtering of the training set. As was described in 2 the ads with large history have higher CTR than the new ads. Since the task is to predict CTR of new ads we tried different filterings of the train set in order to improve per-formance. We sequentially filtered out ads with more than 10, 20, 30, 50, 75, 100, 150, 200 impressions. It resulted in 5% improvement of quality on test set. The best results were obtained with a filter that ignores ads with more than 100 impressions. The results are presented in Figure 3. Here and further we replaced real values of linear correlation with the propotional values due to the proprietary nature of such information. Figure 2: Results on different feature sets on differ-ent subsets of the test set. Figure 3: Results on different feature sets on differ-ent subsets of the test set.

Online experiments. We compared quality of click pre-diction of our model with the baseline model. Our model ouperforms the baseline. The results are presented in Table 2. This experiment was conducted offline using search logs. Offline experiments are only able to measure the accuracy of the predictions, but they are not able to estimate the global effect (for exapmle overall CTR and average cost-per-click) of the implementation of a new algorithm. That is why we conducted an online experiment described below.

We compared three predicting models described above: the original model with a priori value of the CTR calculated by a simple linear approach, the one with a priori value provided by our algorithm, and the one with a priori value provided by algorithm suggested by Richardson et al. More exactly, we conducted two experiments: in the first exper-iment we compared Richardson X  X  algorithm versus original Table 2: Quality of click prediction of our model. Improvements comparing to the baseline model on different subsets of the test set.
 Filterings by number of impressions MSE, % Lin. corr., % Table 3: Results of the online experiment. Improve-ments comparing to the baseline model.
 Table 4: Results of the online experiment on new ads. Improvements comparing to the baseline model.
 model, and in the second we compared our algorithm ver-sus original. Each experiment was organized by the same scheme. For one part of searches the original formula is used to choose ads to display, and for another part of searches the rival formula is used. We compared models by the overall average CTR. The results are presented in Table 3. Our model demonstrated higher performance. Table 4 provides more detailed overview of the effect of our model on new ads. We divided new ads into two groups by the number of impressions in click history. For both groups we observed in-crease of the average CTR and decrease of the click-per-cost, the latter change is usually welcomed by the advertisers.
We suggested an algorithm for predicting CTR of new ads with sparse click history. The algorithm is not biased to the ads with large click history, and consequently avoids overes-timation of CTR. We also suggested several novel features and implemented them in our model. Offline and online experiments demonstrated that our model outperforms the baseline and the approaches suggested in the previous pa-pers. [1] M. Regelson and D. Fain. Predicting click-through rate [2] M. Richardson, E. Dominowska, and R. Ragno.

