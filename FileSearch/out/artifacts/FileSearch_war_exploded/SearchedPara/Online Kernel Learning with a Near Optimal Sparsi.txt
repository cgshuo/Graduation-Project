 Lijun Zhang zhanglij@msu.edu Jinfeng Yi yijinfen@msu.edu Rong Jin rongjin@cse.msu.edu Ming Lin lin-m08@mails.tsinghua.edu.cn Xiaofei He xiaofeihe@cad.zju.edu.cn Kernel methods ( Sch  X olkopf &amp; Smola , 2002 ), such as support vector machine (SVM) ( Burges , 1998 ) and kernel logistic regression(KLR) ( Roth , 2001 ), are widely used in statistical learning. Many on-line learning algorithms have been proposed to im-prove the computational efficiency of kernel meth-ods ( Freund &amp; Schapire , 1999 ; Kivinen et al. , 2002 ; Cheng et al. , 2007 ). Typically, online kernel learn-sequel, and generates a sequence of kernel classifiers { f 1 , . . . , f T } based on the observed examples. The fi-nal solution  X  f is obtained by taking the average of the intermediate classifiers, i.e.,  X  f = 1 procedure often referred to as online-to-batch conver-sion ( Cesa-Bianchi et al. , 2004 ). The main problem of online kernel learning is that the number of support vectors used to construct the intermediate f t may grow unboundedly, leading to a large storage requirement and a high computational cost for both training and testing.
 The focus of this work is online sparse kernel learning that learns a kernel classifier with a limited number of support vectors. To generate sparse kernel classifiers, a common approach is to use a non-smooth loss function (e.g. hinge loss) whose derivative becomes zero when a data point is classified correctly with sufficiently large margin ( Kivinen et al. , 2002 ). The drawback of this approach is that it is unable to provide a formal bound on the number of support vectors.
 An alternative approach for online sparse kernel learn-ing is based on sampling ( Zhang et al. , 2012 ). It online determines if a training example ( x t , y t ) is a support vector based on p ( y t | f t ( x t )), the probability of cor-rectly classifying ( x t , y t ). The larger the probability support vector. Similar to the approach of using non-smooth loss function, this method is not equipped with a formal bound on the number of support vectors. Budget online learning methods ( Cavallanti et al. , 2007 ; Dekel et al. , 2008 ) were proposed to control the number of support vectors. It maintains through the iterations, a sequence of kernel classifiers with a fixed number of support vectors. The main shortcoming of budget online learning is that although the number of support vectors is bounded for each intermediate clas-sifier f t , it is usually not the case for the final solution, which is computed as the average of the intermediate classifiers. This is verified by our empirical study. In this paper, we develop an online sparse kernel learn-ing algorithm by utilizing a sampling approach and a smooth loss function  X  ( y, z ). We note that we slightly abuse the term  X  X mooth X  as the smooth loss func-tion defined in this work is slightly different from the conventional definition. The key idea of the pro-posed algorithm is to measure the difficulty in clas-sifying a training example ( x t , y t ) by the derivative  X   X  ( y t , f t ( x t )), instead of the loss  X  ( y t , f more chance for a  X  X ifficult X  example to be a support vector than an  X  X asy X  one via a sampling procedure. We choose the derivative for the difficulty measure-ment because it leads to an unbiased estimate of the gradient, an important property for our analysis. Using a smooth loss function may sound counter-intuitive because it usually leads to dense kernel clas-sifiers. One nice property of a smooth loss function is that its derivative directly reflects the degree of misclassification. As a result, given kernel classifier f ( ) learned from a smooth loss, if we randomly se-lect its support vector ( x t , y t ) based on the derivative ficult to be classified by f ( ) will be kept, which al-lows us to preserve the core of f ( ). More specifically, our theoretical analysis reveals the following important properties of the proposed algorithm compared to the available methods for online kernel learning:  X  Unlike the existing approaches for sparse online  X  Unlike budget online learning that only bounds  X  The proposed algorithm provides a flexible mech-In this section, we briefly review the existing work on sparse learning.
 Sparse Kernel Learning in Batch Setting A number of algorithms have been developed for batch sparse kernel learning ( Burges &amp; Sch  X olkopf , 1997 ; Lee &amp; Mangasarian , 2001 ; Zhu &amp; Hastie , 2001 ; Keerthi et al. , 2006 ). In post-processing based ap-proach ( Cotter et al. , 2013 ), a non-sparse kernel clas-sifier is first learned from the training examples, and a sparse solution is then computed to approximate the dense solution. An alternative approach is to refor-mulate the kernel learning problem such that a sparse solution is guaranteed ( Wu et al. , 2006 ).
 Online Sparse Kernel Learning Most online sparse kernel learning algorithms are built upon non-smooth loss functions, and none of them is able to pro-vide explicit bound on the number of support vectors that our work is closely related to the recent work on sparse kernel logistic regression ( Zhang et al. , 2012 ) in that both adopt a sampling strategy for reducing the number of support vectors, they differ significantly in the sampling procedure. More importantly, the pro-posed algorithm here achieves a bounded number of support vectors while ( Zhang et al. , 2012 ) did not. Online Kernel Learning on a Budget The ob-jective of budget online kernel learning is to generate a sequence of kernel classifiers with a fixed number of support vectors. In ( 2002 ), Kivinen et al. consider on-line kernel learning for regularized losses, where the coefficients of support vectors are shrunk by a small constant at each iteration. To improve the sparsity, they propose to drop the support vectors with small-est coefficients. Forgetron ( Dekel et al. , 2008 ) applies a similar idea and turns kernel perceptron into a bud-get kernel learning algorithm. In randomized budget perceptron ( Cavallanti et al. , 2007 ) and bounded on-line gradient descent ( Zhao et al. , 2012 ), random sam-pling approaches are developed to remove support vec-tors when the number of support vectors exceeds the budget.
 In ( 2004 ), Crammer et al. develop a heuristic ap-proach that removes redundant support vectors. A similar but more sophisticated strategy is developed in Projectron ( Orabona et al. , 2008 ), which introduces a new support vector only when it cannot be well approximated by the existing ones. Recently, Wang et al. present budgeted stochastic gradient descent for kernel SVM through several budget maintenance strategies ( Wang et al. , 2012 ).
 Since budget online learning typically uses both inser-tion and deletion operations to control the number of support vectors, the final solution, i.e., the average of intermediate classifiers, is usually dense in the number of support vectors. In contrast, our algorithm only allows insertion operation, leading to a sparse kernel classifier even for the final solution.
 Online Learning for Sparse Linear Models Sev-eral online methods have been proposed to learn sparse linear models ( Langford et al. , 2009 ; Duchi &amp; Singer , 2009 ). These approaches cannot be applied directly to kernel learning because they rely on the  X  1 regulariza-tion and are developed specifically for linear classifica-tion. Before we describe our algorithm, we first define a few notations that will be used throughout the paper. Let  X  ( x , x  X  ) : X  X  X 7 X  R be a kernel function, and H  X  be the reproducing kernel Hilbert space (RKHS) endowed with  X  . For simplicity, we assume  X  ( x , x )  X  1 for any x  X  X . Let B = { f  X  H  X  : k f k H solution domain, where R &gt; 0 specifies the domain size. We use  X  B ( f ) for the projection of a function f ( )  X  H  X  into the domain B , and sgn( x ) for the sign function that outputs +1, 0, and  X  1 when x is positive, zero, and negative, respectively.
 Let  X  ( y, z ) be a non-negative loss function convex in the second argument. Similar to most online learning algorithms, we assume  X  ( y, z ) to be Lipschitz continu-ous in the second argument, i.e., where Z is the domain for the predicted value 2 . Be-sides being Lipschitz continuous, we also assume that the magnitude of the derivative is upper bounded by the loss, i.e., where L &gt; 0 is a constant independent from y and z . Remark 1 It is the assumption A2 that makes it possible to bound both the regret and the sparsity of the kernel classifier simultaneously. It is straightfor-ward to check that assumption A2 holds for logit loss  X  ( y, z ) = ln(1 + exp(  X  yz )) because Table 1 shows a few examples of loss functions that satisfy condition A2 . We note that condition A2 is closely related to the conventional definition of H -smooth loss function. This is because, using Lemma 2.1 from ( Srebro et al. , 2010 ), it is easy to show that for any H -smooth loss function  X  ( y, z ), if the absolute value of the derivative |  X   X  ( y, z ) | is bounded from below by a constant G 0 for domain Z , it will satisfy assump-tion A2 with L = 4 H/G 0 . Algorithm 1 shows the detailed steps of the proposed Online Sparse Kernel Learning (OSKL) algorithm. At each iteration, it first a binary variable Z t with where parameter G is introduced to adjust the sam-pling probability. Training example ( x t , y t ) is added to the kernel classifier as a support vector only when Z t = 1. It is this sampling scheme that allows us to control the number of support vectors.
 Note that we choose the derivative |  X   X  ( y t , f t ( x t This is because using the derivative based sampling, the resulting gradient g ( ) computed in ( 1 ) will be an Algorithm 1 Online Sparse Kernel Learning (OSKL) Input: step size  X  , domain size R , and parameter G  X  G 1 1: Initialize f 1 ( x ) = 0 2: for t = 1 , . . . , T do 3: Receive an example ( x t , y t ) 4: Compute the derivative  X   X  ( y t , f t ( x t )) 5: Sample a binary random variable Z t with 6: Update the classifier by 7: end for unbiased estimate of the true gradient  X   X  ( y t , f t ( x i.e., This property is the key to the analysis of the re-gret bound and the sparsity for the proposed algo-to be bounded by  X  ( y t , f t ( x t ) in condition A2 , using derivative for sampling may result in a smaller number of support vectors.
 Evidently, the number of support vectors of  X  f is given by P T t =1 Z t . The following theorem shows that both the regret and the number of supported vectors can be bounded by the cumulative loss of the optimal kernel classifier.
 Theorem 1. Assume that loss function  X  ( y, z ) satis-fies the assumptions A1 , A2 , and T  X  18 / [ G ln(1 / X  )] . For a fixed  X   X  (0 , 1) , we set  X   X   X / ( LG ) . Let f , . . . , f T be the sequence of classifiers generated by by Algorithm 1 . With probability at least 1  X  2  X  , for any f  X   X  B , we have and where c = max 2 p G 1 , and m =  X  log 2 ( G 1 T 2 )  X  .
 Remark 2 Note that although in ( 3 ) we bound the number of support vectors P T t =1 Z t of support vectors to the loss of the optimal classifier f  X  using the bound in ( 2 ). To better understand the structure of the bounds in ( 2 ) and ( 3 ), we set  X  =  X / ( LG ), leading to the following bounds for the regret and the number of support vectors  X  G and
X It is not surprising to observe that the smaller the G , the smaller the regret and the larger the number of support vectors. Thus, parameter G allows us to com-promise between classifier sparsity and classification accuracy.
 To check the tightness of the bounds for Algorithm 1 , we examine if it is always possible to devise an al-gorithm that achieves a similar regret bound as Al-gorithm 1 but with a significantly smaller number of support vectors. The answer to this question, as re-vealed by the following theorem, is no. We defer the proof to the supplementary document.
 Theorem 2. For any fixed integers n and T , there al-ways exists a sequence of training examples { x t , y t } T such that  X  the optimal classifier f  X  has n support vectors and  X  the sequence of kernel classifiers f 1 , . . . , f T re- X  the regret of any sequence of kernel classifiers The result in Theorem 2 indicates that the bound for the number of support vectors achieved by Algorithm 1 is optimal up to a poly (ln T ) factor. In this section, we present the analysis that leads to Theorem 1 . To simplify the notations, we define  X  t = sgn(  X   X  ( y t , f t ( x t ))) , and A T = In the analysis below, we consider two different sce-narios, i.e., A T  X  1 /T and A T &gt; 1 /T . 4.1. Bounds When A T  X  1 /T Under this condition, we have where we set  X  = TR/ In addition, we can also bound the number of support vectors, i.e., P T t =1 Z t , by using Bernstein X  X  inequality for martingales ( Cesa-Bianchi &amp; Lugosi , 2006 ). Lemma 1. Assume A T  X  1 /T and T  X  18 / [ G ln(1 / X  )] . With probability at least 1  X   X  , we have The proof is provided in the supplementary document. As a result, with probability at least 1  X   X  , we have 4.2. Bounds When A T &gt; 1 /T We first consider bounding the number of support vectors P T t =1 Z t . Similar to the previous analy-sis, we derive an upper bound for P T t =1 GZ t  X  tight upper bound for A T , so we cannot follow the approach used in the proof of Lemma 1 . In-stead, we combine the Bernstein X  X  inequality for martingales ( Cesa-Bianchi &amp; Lugosi , 2006 ) with the peeling process introduced in ( Bartlett et al. , 2005 ), to give an upper bound involving the overall loss P t =1  X  ( y t , f t ( x t )). To this end, we have the follow-ing lemma.
 Lemma 2. Assume A T &gt; 1 /T . With probability at least 1  X   X  , we have X where m =  X  log 2 ( G 1 T 2 )  X  .
 The proof is provided in the supplementary document. Following Lemma 2 , with probability at least 1  X   X  , we have Now, we give the analysis of the regret bound. Using the standard analysis of online learning ( Cesa-Bianchi &amp; Lugosi , 2006 ), we have By adding the inequality in ( 8 ) over all the iterations, we have  X  Following ( 7 ), we can bound the second expression on the R.H.S of the above inequality as where the last inequality follows from the condition  X   X   X LG . Similar to Lemma 2 , we develop the follow-ing Lemma to bound the last expression in ( 9 ). Lemma 3. Assume A T &gt; 1 /T . With probability at least 1  X   X  , we have where m =  X  log 2 ( G 1 T 2 )  X  .
 We skip the proof since it is identical to that for Lemma 2 . From Lemma 3 , we have  X   X  Combining the bounds in ( 9 ), ( 10 ) and ( 11 ), we have, with probability at least 1  X  2  X  ,  X  which implies Using the definition of c in ( 4 ), we obtain ( 2 ) by com-bining ( 5 ) and ( 12 ), and ( 3 ) by combining ( 6 ) and ( 7 ). In this section, we perform classification experiments to demonstrate the advantage of the proposed method. We use 3 benchmark data sets which are summa-rized in Table 2 . Magic is available at UCI Ma-chine Learning Repository ( Frank &amp; Asuncion , 2010 ), while the others are downloaded from LIBSVM data sets ( Chang &amp; Lin , 2011 ). For Magic and Covtype, we randomly select 80% data for training and repeat the experiments 5 times with different training and testing splits. For Adult, we use the training and testing splits provided by the authors, and also re-peat the experiments 5 times by processing the train-ing data in different random permutations. For un-normalized data sets, we linearly scale each feature to the range [0 , 1]. We choose the Gaussian ker-kernel width  X  to the 20-th percentile of the pair-wise distances ( Mallapragada et al. , 2009 ). We choose the logit loss for our Online Sparse Kernel Learning (OSKL) algorithm. 5.1. Comparison with online sparse kernel We first compare the proposed OSKL with three online sparse kernel learning algorithms:  X  Margin and Auxiliary , two online learning al- X  Pegasos , an online learning method for kernel Besides, we also report the result of Baseline , which applies stochastic gradient descent to solve kernel lo-gistic regression. The regularization parameter  X  in the parameter R in other four algorithms is searched their values are determined by cross validation. The parameters  X  and  X  in OSKL are set to be 0 . 9 and  X /G , respectively. In the experiments, we formally defined sparsity as the ratio between the number of non-support vectors and the number of received train-ing examples ( Zhang et al. , 2012 ).
 Table 3 shows the average as well as the standard devi-ation of the classification accuracy, the sparsity (SP), the number of support vectors (SVs), and the train-the other fours algorithms are able to generate sparse classifiers, that dramatically reduce the training time, and at the same time still maintain high classification accuracy. We can see the classifier generated by OSKL achieves the highest sparsity among all the methods in comparison. Based on the results of OSKL on different data sets, we also observe that the easier the classifi-cation task, the higher the sparsity. This is consistent with our analysis, i.e., the number of support vectors is bounded by the loss of the optimal classifier. Finally, we observe that increasing the value of G improves the sparsity measure, but at the cost of small reduction in classification accuracy. 5.2. Comparison with budget online learning In this section, we compare OSKL with For-getron ( Dekel et al. , 2008 ), which is a classic bud-get online learning algorithm, and bounded on-line gradient descent using nonuniform sampling (BOGD++) ( Zhao et al. , 2012 ), which has shown to be superior to other budget online learning algorithms, such as Projectron ( Orabona et al. , 2008 ). To make the result comparable, we set the budget size to be the number of support vectors used by OSKL.
 In the first comparison, we evaluate the performance of the last classifier f T generated by the two budget online learning algorithms, and summarize the classi-fication performance in Table 4 . We omit the sparsity and the number of support vector of f T from Table 4 , because they are the same as OSKL and can be found from Table 3 . Since the budget is set as the number of support vectors used by OSKL, it is not surprising that the training time of Forgetron and BOGD++ is overall comparable to that of OSKL. However, the classifica-tion accuracy of the last classifiers f T generated by the budget online learning algorithms is significantly worse than that of OSKL in most cases. Besides, the perfor-mance of the last classifier is unstable, as indicated by the large standard deviation.
 Second, we evaluate the performance of the average classifier  X  f = 1 converts online learning solutions into a batch learning solution. Table 5 summarizes the classification accu-racy, the sparsity and the number of support vectors for the average classifier. The training time was omit-ted from Table 5 as it is already listed in Table 4 . We observe that, using the average classifier, both For-getron and BOGD++ achieve similar classification ac-curacy as OSKL. Compared to the results in Table 4 , we observer that the performance of the average clas-sifier is significantly more stable than the last classi-fier generated by the budget online learning. However, compared to the results in Table 3 , the average clas-sifier is significantly denser in the number of support vectors than the solution returned by OSKL, making it less efficient in testing. In general, the number of support vectors increases as the budget size decreases. That is because using a smaller budget size, the online learning algorithm tends to make more mistakes. In this paper, we developed an algorithm for online sparse kernel learning. The key idea is to reduce the number of support vectors by performing stochastic updating. By setting the sampling probability to be proportional to the derivative of a smooth loss func-tion, we are able to show theoretically that the sparsity bound achieved by the proposed algorithm is near op-timal. Experimental results show that the proposed algorithm is very effective in finding a both accurate and sparse classifier, and thus reduces the computa-tional cost dramatically.
 In the future, we plan to combine the strength of the proposed approach with methods for budget online learning to further improve the sparsity of online ker-nel learning.
 This work is partially supported by Office of Navy Research (ONR Award N00014-09-1-0663 and N000141210431), National Basic Research Program of China (973 Program) under Grant 2009CB320801, and National Natural Science Foundation of China (Grant No: 61125203).
 Bartlett, P.L., Bousquet, O., and Mendelson, S. Local rademacher complexities. Ann. Stat. , 33(4):1497 X  1537, 2005.
 Burges, C.J.C. A tutorial on support vector machines for pattern recognition. Data Min. Knowl. Discov. , 2(2):121 X 167, 1998.
 Burges, C.J.C. and Sch  X olkopf, B. Improving the accu-racy and speed of support vector learning machines. In NIPS 9 , pp. 375 X 381, 1997.
 Cavallanti, G., Cesa-Bianchi, N., and Gentile, C.
Tracking the best hyperplane with a simple budget perceptron. Mach. Learn. , 69(2-3):143 X 167, 2007. Cesa-Bianchi, N. and Lugosi, G. Prediction, Learning, and Games . Cambridge University Press, 2006. Cesa-Bianchi, N., Conconi, A., and Gentile, C. On the generalization ability of on-line learning algorithms. IEEE Trans. Inf. Theory , 50(9):2050 X 2057, 2004. Chang, C. and Lin, C. LIBSVM: A library for support vector machines. ACM Trans. Intell. Syst. Technol. , 2(3):27:1 X 27:27, 2011.
 Cheng, L., Vishwanathan, S.V.N., Schuurmans, D.,
Wang, S., and Caelli, T. Implicit online learning with kernels. In NIPS 19 , pp. 249 X 256, 2007. Cotter, A., Shalev-Shwartz, S., and Srebro, N. Learn-ing optimally sparse support vector machines. In ICML , 2013.
 Crammer, K., Kandola, J., and Singer, Y. Online clas-sification on a budget. In NIPS 16 , pp. 225 X 232, 2004.
 Dekel, O., Shalev-Shwartz, S., and Singer, Y. The forgetron: A kernel-based perceptron on a budget. SIAM J. Comput. , 37(5):1342 X 1372, 2008.
 Duchi, J. and Singer, Y. Efficient online and batch learning using forward backward splitting. J. Mach. Learn. Res. , 10:2899 X 2934, 2009.
 Frank, A. and Asuncion, A. UCI machine learning repository, 2010.
 Freund, Y. and Schapire, R.E. Large margin classifica-tion using the perceptron algorithm. Mach. Learn. , 37(3):277 X 296, 1999.
 Keerthi, S.S., Chapelle, O., and DeCoste, D. Building support vector machines with reduced classifier com-plexity. J. Mach. Learn. Res. , 7:1493 X 1515, 2006. Kivinen, J., Smola, A.J., and Williamson, R.C. On-line learning with kernels. In NIPS 14 , pp. 785 X 792, 2002.
 Langford, J., Li, L., and Zhang, T. Sparse online learn-ing via truncated gradient. J. Mach. Learn. Res. , 10: 777 X 801, 2009.
 Lee, Y. and Mangasarian, O.L. Rsvm: Reduced sup-port vector machines. In SDM , 2001.
 Mallapragada, P.K., Jin, R., Jain, A.K., and Liu, Y. Semiboost: Boosting for semi-supervised learning.
IEEE Trans. Pattern Anal. Mach. Intell. , 31(11): 2000 X 2014, 2009.
 Orabona, F., Keshet, J., and Caputo, B. The projec-tron: a bounded kernel-based perceptron. In ICML , pp. 720 X 727, 2008.
 Roth, V. Probabilistic discriminative kernel classi-fiers for multi-class problems. In Proceedings of the 23rd DAGM-Symposium on Pattern Recogni-tion , pp. 246 X 253, 2001.
 Sch  X olkopf, B. and Smola, A.J. Learning with kernels : support vector machines, regularization, optimiza-tion, and beyond . MIT Press, 2002.
 Shalev-Shwartz, S., Singer, Y., and Srebro, N. Pega-sos: primal estimated sub-gradient solver for SVM. In ICML , pp. 807 X 814, 2007.
 Srebro, N., Sridharan, K., and Tewari, A. Smoothness, low noise and fast rates. In NIPS 23 , pp. 2199 X 2207, 2010.
 Wang, Z., Crammer, K., and Vucetic, S. Breaking the curse of kernelization: Budgeted stochastic gradient descent for large-scale svm training. J. Mach. Learn. Res. , 13:3103 X 3131, 2012.
 Wu, M., Sch  X olkopf, B., and Bak X r, G. A direct method for building sparse kernel learning algorithms. J. Mach. Learn. Res. , 7:603 X 624, 2006.
 Zhang, L., Jin, R., Chen, C., Bu, J., and He, X. Effi-cient online learning for large-scale sparse kernel lo-gistic regression. In AAAI X 12 , pp. 1219 X 1225, 2012. Zhao, P., Wang, J., Wu, P., Jin, R., and Hoi, S.C.H.
Fast bounded online gradient descent algorithms for scalable kernel-based online learning. In ICML , pp. 169 X 176, 2012.
 Zhu, J. and Hastie, T. Kernel logistic regression and the import vector machine. In NIPS 13 , pp. 1081 X 
