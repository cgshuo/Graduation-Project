 Every piece of textual data is generated as a method to con-vey its authors X  opinion regarding specific topics. Authors deliberately organize their writings and create links, i.e., references, acknowledgments, for better expression. There-after, it is of interest to study texts as well as their relations to understand the underlying topics and communities. Al-though many efforts exist in the literature in data clustering and topic mining, they are not applicable to community dis-covery on large document corpus for several reasons. First, few of them consider both textual attributes as well as re-lations. Second, scalability remains a significant issue for large-scale datasets. Additionally, most algorithms rely on a set of initial parameters that are hard to be captured and tuned. Motivated by the aforementioned observations, a hi-erarchical community model is proposed in the paper which distinguishes community cores from affiliated members. We present our efforts to develop a scalable community discov-ery solution for large-scale document corpus. Our proposal tries to quickly identify potential cores as seeds of communi-ties through relation analysis. To eliminate the influence of initial parameters, an innovative attribute-based core merge process is introduced so that the algorithm promises to re-turn consistent communities regardless initial parameters. Experimental results suggest that the proposed method has high scalability to corpus size and feature dimensionality, with more than 15% topical precision improvement com-pared with popular clustering techniques.
 H.4.m [ Information Systems ]: Miscellaneous; D.2 [ Software ]: Software Engineering; H.3.3 [ Information Systems ]: In-formation Search and Retrieval X  Clustering  X  This work was done when the first author was visiting Microsoft Research Asia.
 Algorithms, Design.
 Community Mining, Clustering, Relational Data.
Texts are generated by their authors as a media to express and interchange information, opinions, ideas and comments regarding specific topics. In authors X  writings, relations to external resources, such as references, acknowledgements, are deliberately created for better understanding of the men-tioned topics. In social sciences, communities constructed on topics and interests have been extensively studied to un-derstand the causality of events and roles of actors. Corre-spondingly, it is of interest to find communities organized based on latent topics from documents, which can be taken as an accurate reflection of the social communities behind the scene. Imagine that a researcher wants to make a com-prehensive survey of a research domain (e.g., data mining), she might issue the following queries:
A traditional information retrieval system may find it dif-ficult to support such queries. However, experts in a specific domain are aware of such information. They will leave an-swers to these topic-based questions in their writings. There-fore, communities constructed on top of document collec-tions can be used as an effective utility to assist the query processing in intelligent information retrieval systems.
Community discovery can be taken as a special clustering problem. However, text datasets have their unique charac-teristics which prevent many state-of-the-art clustering tech-niques from being feasible. First, the size as well as the feature dimensionality are usually very large for text collec-tions. The corpus can grow into a size of millions or billions. Meanwhile, either we follow the  X  X ag of words X  principle by
In this paper, relation denotes an association between ob-jects and we use it interchangeably with relationship. taking unigrams as features or use k-gram approach, a huge vocabulary size is always expected. Second, inter-document relations are generated and maintained intentionally by doc-ument creators, making relations a useful clue in locating and relating topics. These characteristics make community discovery on large data collections an open and challenging problem.

Current clustering methods can be classified into three categories. (1) Textual attribute clustering, e.g., LDA [3] and pLSI [14]. This category of clustering methods utilizes texts to find implicit topics. Although proved effective in discovering latent topics, text-based clustering methods suf-fer from severe scalability issues. For online archives such as Libra 2 and CiteSeer 3 , the document corpus contains more than 1 , 000 , 000 records, which requires highly scalable meth-ods for topic mining. Additionally, such methods usually require solid prior knowledge regarding the parameter set-tings, which is difficult to be captured. Even worse, these algorithms are sensitive to these parameters. Any update on the document collection may put previously tuned para-meters out-of-date and a severe degradation of performance. (2) Relation-based clustering. This category of methods re-lies purely on relations between documents to discover com-munities. However, without considering attributes, it is dif-ficult to interpret the meaning of relations. Probing only based on relations can include too many topics into a com-munity and thus cannot yield good results. (3) Hybrid clus-tering. Some works in the literature propose to consider both attributes and relations. However, these approaches either inherit the scalability issue from the aforementioned attribute-based clustering techniques or fail to recognize the important role of relations in conveying topics. These lim-itations create a critical obstacle for these methods to be effective.

In this paper, we address the aforementioned issues of community discovery on relational data and propose a solu-tion that utilizes both textual attributes and relations. Our approach is unsupervised, scalable to dataset size and fea-ture dimensionality, and consistent with input parameters. To improve scalability, a filtering process based on relation analysis is first used to find representative community cores. Afterwards, an innovative core merge process is adopted so that consistent communities are returned regardless the ini-tial parameters. The merge step maps cores into virtual bounding boxes in a low-dimension feature space to ana-lyze similarity between cores. Textually-relevant commu-nity cores are then merged to fo rm topically coherent ones. Finally, community cores are propagated via relations to form communities and a text-based classification procedure is used to improve the topical precision within communities.
In summary, our contributions are four-fold: http://libra.msra.cn http://citeseer.ist.psu.edu
The rest of the paper is organized as follows. We give a review on related works in Section 2. An extensive analysis to the performance of LDA, a popular textual topic mining algorithm, is given in Section 3. Steps of the proposed so-lution for community discovery is given in Section 4. The experimental results are presented in Section 5. Finally, the concluding remarks are given in Section 6.
In this section some state-of-the-art clustering methods on data attributes and relations are reviewed. We focus the discussion on textual data clustering techniques. The first category of data clustering techniques is Textual Attribute Clustering , which utilizes texts to discover topics and then uses topics to cluster data objects. This category of approaches (e.g., LSI [7], pLSI [14]) does not consider re-lations and only relies on attributes. As one of the popular methods applied to textual data collections, Latent Dirichlet Allocation (LDA) [3] is a generative probabilistic model for generating large text corpus. LDA is a three-level model, in which for each document a distribution of topics  X  is sam-pled and for each word in the document a topic z is sampled from  X  and the word is sampled according to p ( w n | z,  X  ), a multinomial probability conditioned on z .Inordertouse LDA, a group number K needs to be provided and each document will be transformed into a K -dimension vector to represent its location in the topic space. However, shown by literature studies [12], the level of topic coherence of clusters returned by LDA is sensitive to K . In addition, its scalabil-ity remains an issue when the document corpus is very large. The performance analysis to LDA is detailed in Section 3.
This category of data clustering methods does not rely on data attributes. They explore object adjacency rela-tions and graph topology to cluster objects. For example, the HITS algorithm finds web object communities [11, 16], which are characterized by a set of authoritative pages linked by hub pages. Academic community discovery has been per-formed [20] by expanding from some key papers as centroids of clusters and including affiliated papers. Traditionally, re-lational clustering methods often use both link analysis and graph partition to locate communities [5, 8, 9, 10]. Random walks [13], for instance, are utilized on a weighted directed graph to update the weights of inter-cluster edges and intra-cluster edges in order to distinguish clusters. Relation-based clustering methods can effectively generate topologically-coherent clusters. However, simply studying links cannot guarantee to find topically-coherent communities because multiple topics can exist within a group of tightly-linked documents.
In the literature, there have been some discussions regard-ing hybrid approaches in text clustering, many of which are extended from generative language models such as LDA. In [2], the authors use a probabilistic generative model to train soft clusters, in which each object is provided with a Dirich-let random variable. When it comes to decide the relation-ship between two objects, each object will sample a topic distribution from its given Dirichlet random variable, which are supplied to a Bernoulli distribution to determine the re-lationship. LDA is also extended to include author, group variables for cluster mining [18, 21, 22]. MMRC [17] gen-erates a latent indicator vector to represent the class of a single data object, based on which attributes, homogeneous and heterogeneous relations are modeled. In another piece of work, PLSA is combined with PHITS [6] to provide a unified view of topics in generating texts and links between documents under the assumption that an underlying doc-ument generation process exists, which creates both words and hypertext links. However, because these models are ex-tended from generative language models, they also inherit the issues we have discussed in Section 2.1.

In [19], the authors identify the problem of discovering an unknown number of clusters on attribute and relational data, trying to find clusters that are compact inside and dis-tinctive from their neighborhoods in a connecting graph. A two-phase solution is proposed. Although both aim at re-moving the limitation of input parameters, unlike the prob-lem definition of Connected X Clusters Problem [19], which utilizes relations to define clus ter boundaries, we use the re-lations to provide topic clues regarding linked documents. Also, scalability remains a critical issue for large datasets.
In summary, although many alternative methods are avail-able for clustering relational data, as far as we know, none of them can be used to effectively and efficiently solve the problem of community discovery for large text collections.
As introduced in Section 2, LDA is a popular language model for topic mining. LDA relaxes the assumption by al-lowing multiple topics for one document and thus has demon-strated its effectiveness in machine learning tasks on docu-ment sets. In terms of text-based topic mining, LDA yields very good performance. Meanwhile, many state-of-the-art hybrid topic mining approaches are extended from LDA. Thereafter, in this section, we exploit the applicability and performance of LDA in clustering large-scale text collections. Two aspects of LDA performance are studied: 1) its scalabil-ity to dataset size, and 2) its sensitivity to initial parameter settings.

In this experiment, the computer science publication dataset extracted from CiteSeer is used, which includes abstracts of 575 , 598 computer science papers published before 2006. This dataset contains 238 , 028 unique terms in the vocabu-lary,aswellasthe1 , 438 , 596 citation links between publi-cations. We use the LDA implementation made available by its authors 4 to mine K topics and conduct all experiments on a Linux server with dual AMD Opteron Processor 252 CPUs (1 G Hz), 4 GB main memory. The parameter settings used in the experiments are summarized in Table 1: http://www.cs.princeton.edu/ blei/lda-c/ Parameter Value Iterations of variational inference 20 Convergence criteria for variational inference 1  X  10  X  6 Iterations of variational EM 30 Convergence criteria for variational EM 1  X  10  X  4
In the scalability experiment, each time a subset of docu-ments, ranging from 20 , 000 to 100 , 000, is extracted and the time it takes to generate 20 clusters is measured. During our text parsing step, 2-grams are collected and put into the vo-cabulary instead of unigrams to improve clustering precision. The results are given in Figure 1(a), from which it is found that the overhead basically grows up with the dataset size. Furthermore, we note that with the increase of collection size, the overhead growth rate also increases accordingly. This can be explained by the fact that the vocabulary size grows with the dataset size to make the feature dimension higher. When LDA is applied to the complete dataset, due to the huge size of the document collection, LDA failed to return any results and ran out of memory in initialization. This scalability limitation makes LDA unable to be applied in real systems for topic mining. (a) LDA scalability to corpus size.
In the next step, the sensitivity of LDA to K , the topic number parameter, is studied. In this study, two non-overlapping datasets, computer science publications in 2001 and 2003, are used. These datasets include 29 , 869 and 17 , 085 records respectively. For each dataset, LDA is performed using dif-ferent K settings.

By studying the ACM metadata record of each document, such as publication venue, author affiliation, etc, each doc-uments is classified into one or several of 17 major com-puter science research topic categories 5 .Eachdocument d is assigned with a 17-dimension topic vector z i .Asoft-classification approach is adopted by allowing a paper d to belong to multiple topic categories. After clustering, we use the scripts provided by LDA authors to generate top N ( N is set to be 100 in our analysis) representative key-words for each cluster, which are manually compared with ACM classification keywords and labeled in the same 17-dimension topic space, i.e, each cluster C i is assigned a topic vector Z i . Based on the LDA cluster topic vectors and doc-ument topic vectors, we define a metric  X  PCS for clustering precision evaluation, which represents the topical precision for the community discovery task.  X  PCS is also used in the http://www.acm.org/class/1998/overview.html evaluation section to compare community qualities.
HIT i =
In Equation 1 and 2, d ik denotes the k -th document in C and z ik is the topic vector of d ik . The topic of each docu-ment is then compared with its affiliated cluster topic label. The result is summarized in PCS i to represent the topical precision of C i . We average PCS i over all communities. The result, denoted as  X  PCS ,isusedasourevaluationmetrics.
Figure 1(b) presents the change of  X  PCS over K for the two experiment datasets. It is observed that the two curves display different patterns. LDA gets the best clustering pre-cision when K is around 120 for the 2001 dataset and 180 for the 2003 dataset. This findi ng suggests that there is not a best setting of K for all datasets. On the contrary, it needs to be tested repeatedly and tuned to get the best clustering precision, which is around 65% in the tests. Ad-ditionally, it is found that both curves in Figure 1(b) do not change monotonically with K . It is observed the  X  PCS value decreases when K grows from 80 to 100 and starts to increase when K continues to grow to 120 for the 2003 dataset. In summary, to better utilize LDA for topic clus-tering, prior knowledge regarding the approximate number of topics in the underlying dataset is required because the clustering precision is very sensitive to the topic number. Otherwise many trials and manual comparisons are needed to infer the best K value. This is not applicable for real datasets, considering the scalability issue of LDA.
In this section, we first propose a hierarchical commu-nity model for textual datasets, based on which a solution for community discovery is introduced. The proposed solu-tion builds internal hierarchy of communities simultaneously when it defines community boundary. This section is orga-nized as follows. First, Section 4.1 illustrates the community model conceptually. Section 4.2 gives an overview of the pro-posed solution using an example. Afterwards, Section 4.3 introduces essential steps involved in community discovery.
In the literature, there are definitions already given for communities [4, 9, 15], most of which claim that a com-munity consists of a set of relevant objects sharing similar interests or topics. A study to Web communities [8] gives a definition by comparing Web linkage with social groups and classifies webpages into cores and fans .Frombothempir-ical observation to real world communities and definitions given by social sciences, we believe that a community con-sists of a set of topically relevant members with similar at-tributes and internal relations. Inside a community, there exist core objects that are representative and influential in scoping the topic of the community. Remaining members of a community have their roles and ranks inside a community, which forms a hierarchy. It is noted that an object can cross the boundary of communities to make it an interdisciplinary member.

Figure 2 gives an illustration of the proposed community model, in which two communities ( C 1 and C 2 )areshown. Figure 2: Inner structure of communities for rela-tional data, in which C i represents a community and K i represents the core members of C i .
 Definition 1 gives the formal definition of our community model:
Definition 1. A community C i is defined as a set of ob-jects { d i | i =1 , 2 , ..., n, d i = &lt;A i ,R i &gt; } the attributes of d i and R i is the set of relations connecting with d i . The topics associated with C i are denoted as Z Within C i , there exists a set of core members K i that deter-mines and thus best describes Z i .Affiliatedmembersof C i are ranked by their relevance to Z i , which creates a hierarchy within C i .
Based on the proposed community model in Definition 1, we develop a solution to support scalable community dis-covery on large document collection, utilizing text contents as well as relations. The proposed solution first quickly de-composes the data collection into smaller units by exploring relations. Compared with textual attributes, relations be-tween documents convey consistent topics because they are constructed deliberately by their creators. Handling rela-tions first may reduce the overhead in interpreting and sum-marizing texts significantly so that the solution has a high scalability regardless the dataset size. Through a relation topology analysis, a set of preliminary community cores is returned and later expanded into communities. To reduce the effects of parameter setting used in the first step and produce consistent communities, a core merge step is added on the preliminary cores. Afterwards, attributes are used within a community, whose size is much smaller compared with the original data collection, to determine the relevance of each community member that is included into the commu-nity through relation propagation. Non-relevant documents, although connected with the community, are eliminated.
Figure 3 illustrates the steps taken for community dis-covery in the solution. In this figure, a given collection of documents ( d a to d l ) are represented by solid circles and labeled from a to l . Directed relations are also given by solid arrows. Basically, the algorithm consists of four steps, namely: 1. Core Probing : In this step, the solution starts from 2. Core Merge : In this step, core s are merged based Figure 3: Illustration of community discovery steps. 3. Affiliation : After cores are constr ucted, relations are 4. Classification : Since linked documents may not be
In the following section, we give algorithmic details to the community discovery steps listed above.
Based on Definition 1, cores dictate the formation and top-ics of communities. Previous approaches usually use simple heuristics such as relation frequency to locate cores. We ar-gue, however, finding core documents of a community via simple relation count or content analysis does not necessar-ily yield good results. As suggested in [16], hub objects tend to have many outgoing relations, which usually cover many communities and are thus too generic in topics. On the other hand, attribute analysis can possibly find documents that are typical in a specific topic but without actual influence, which is both expensive in computation and inaccurate.
To obtain more topically coherent cores without incurring high computational overhead, in our solution core candi-dates are discovered by co-occurrence analysis. We believe that when multiple objects are linked simultaneously by oth-ers, they are more likely to be able to define a coherent topic scope. Definition 2 specifies the community cores definition.
Definition 2. The core K of a community C is defined as a set of members that are simultaneously linked by many affiliated members in C . It is not required that objects in the core of a community should be tightly linked to each other.
Based on Definition 2, the co-occurrence relation analy-sis can be used to find community cores for textual datasets. We do not require that core members have many direct links in between because it is often observed that several indepen-dent topics eventually merge into one coherent new topic over time. For each document d i , the list of all outgoing relations R out i = { r ij } is generated. Thus, the problem of finding core documents is reduced to a problem of calcu-lating frequent itemsets in the associate rule algorithm [1]. Since finding all possible combinations of community mem-bers is extremely expensive, we start from short itemsets, which will be reused to find longer itemsets. By using re-lation analysis, the probing step can efficiently find a set of cores that is very small compared with the whole document collection, which achieves very high scalability.

The core-discovery algorithm is based on the Apriori al-gorithm but different in the following two aspects:
The formal description of the core-probing algorithm is outlined in Algorithm 1.
 Algorithm 1 Extracting core members of a community. Input: L 1 : a large 1-item set of objects { d j | d j is linked by adocument d i in the corpus } t : filtering threshold Output: asetofcores, K for m =2; L m  X  1 =  X  ; m ++ do end for return K = m i =1 L i ;
In Algorithm 1, a filtering threshold t needs to be pro-vided, based on which cores are probed. Intuitively, a lower t value can create more cores than a higher t value, which places a similar problem as with LDA, which needs users to supply the number of topics. To overcome this problem, a core merge process is introduced, which tries to analyze top-ical similarity using textual analysis and reduce the effects of improper t settings.

Remember that t stands for the number of co-occurrences for frequent itemsets. It is observed that after applying two different thresholds t 1 and t 2 ( t 1 &lt;t 2 ) every core returned by t 2 can find a corresponding core in the result returned by t , which can be formally summarized as:
Property 1. Given two core probing thresholds t 1 and t 2 ( t 1 &lt;t 2 ), the core probing algorithm returns two sets of core candidates respectively, namely K 1 = {K (1) i K
Property 1 suggests that low t values can always find a more complete core candidate set and the overhead re-mains much lower than LDA. For example, if { d 1 ,d 2 } returned in K 2 ,theremustbeacorein K 1 that is the su-perset of { d 1 ,d 2 } . However, the negative effect is that too many candidates are returned by lower t values and many of them contain overlapping members such as { d 1 ,d 2 ,d 3 } { d 1 ,d 2 ,d 4 } . For such cases, it is often found actually d d 4 are really relevant documents. It would be good if these two sets can be merged as { d 1 ,d 2 ,d 3 ,d 4 } . Another problem arises here: How can we decide when to merge these similar cores? Simply relying on the overlapped entries in the two sets cannot promise good results because d 3 and d 4 in the above sets may discuss two totally different topics whereas d 1 and d 2 are only two documents providing tools or back-grounds. A core merge step is introduced to identify core candidate sets that can be merged to generate more com-plete and consistent cores. A hybrid policy is used, which not only studies the overlapping level of cores but also uses their topical distributions to determine when to merge cores.
Given two cores K i and K j , it is required that they have at least one member in common to make them qualified for merging. Namely,
Afterwards, the candidate cores are sent to study their topical similarities by text analysis. LDA is applied on all core candidate documents, whose size is much smaller than that of the whole collection. LDA is used not to find com-munities. On the other hand, it is used as a utility to reduce the dimension of the feature space for documents. Although the original term space can be used, this approach builds a feature space that is as large as the size of the textual vocabulary of the collection, which is expensive in compu-tation. Moreover, the term-document matrix is very sparse. Among all terms in the vocabulary, only a very small subset has strong discrimination effect in topic clustering, which is very difficult to capture without global-scale distributional study. On the other hand, as proven in [3], LDA can effec-tively reduce the dimensionality of text representation and keep good discriminative information.

LDA is told to generate n topics, in which n should be an integer that is large enough. The low-dimensional doc-ument representation is used to discover the similarity be-tween documents. n will not influence the merge results very much, which will be shown by experiments in Section 5. After dimension reduction, each document in the dataset is represented as a n -tuple vector.

For each core K i , in the low-dimension feature space, the Figure 4: Example core merge in a two-dimension view. following coordinates are calculated:
In the above formulae, v min ij and v max ij stand for the min-imal and maximal value of feature dimension j for all docu-mentsthatbelongtothecore K i , respectively. Thus, p min and p max i generate a bounding box for each core in the fea-ture space. For each pair of cores K i and K j ,wecalculate their intersection, denoted as R ij , based on their p min p max . Notice that there is no need to calculate R ij if the value of any dimension of p min i is larger than the correspond-ing dimension value of p max j since they cannot overlap at all. This initial condition is very helpful in eliminating unnec-essary comparisons. Additionally, in our implementation, two indices for p min and p max are maintained respectively to facilitate the calculation.

If two cores overlap in the feature space ( R ij =  X  ). The significance of their overlapping is analyzed by checking if either  X  p i or  X  p j is bounded by R ij . We select to merge and K j if  X  p i  X  R ij or  X  p j  X  R ij . After two cores, are merged, the new core is formed as K ij = K i  X  X  j . p min and p max of K ij are updated using Equation 3 and Equation 4 respectively. However, instead of updating  X  p using the topic centroid of all members of K ij , the separate topic centroids of K i and K j are kept. When it is found any topic centroid of K ij falls in its intersection region with other cores, the two cores can merge. Formally speaking, we select to merge two core K i and K j if and only if:
A stronger merge condition can be placed by requiring all topic centroids of involved cores to fall into K i  X  X  j ,which is described as:
From Equation 6 and Equation 7, the following property of the core merge process can be found, in which the operator  X   X  represents the core merge operation.

Property 2.
Based on Property 2, it is found the sequence of selecting the core candidates for merge does not have any effect on the final results, which is denoted as Associativity of the core merge operation. So the algorit hm will start from arbitrary pairs of core candidates and apply the merge procedure on them. This process is performed iteratively until no merge is available.

The process can be illustrated with Figure 4, in which there are originally three core candidate sets ( K 1 , K 2 K ) in Figure 4(a). The p min and p max values are shown as small solid circles on the corners of bounding boxes. Also, the topic centroid of each core is plotted in the figure with a small triangle. It is evident that  X  p 2 lies within the in-tersection area of K 1 and K 2 , which makes them qualified for merge according to Equation 6. On the other hand, al-though K 2 and K 3 have an intersection area in the figure, they cannot be merged because neither have its topic cen-troid in the shadowed intersection region. After merge, the newly constructed core K 12 is then compared again with K (Figure 4(b)). It is found there is none of  X  p 1 ,  X  p in the shadow region of Figure 4(b). So the merge process terminates. Otherwise, if we choose the policy described in Equation 7, K 1 and K 2 cannot be merged either.
 The whole process is specified in Algorithm 2.
 Algorithm 2 Merging core candidates.
 Input: S : initial core candidates returned by core probing Output: a refined set of cores S
S 0 =  X  ; while S 0 = S do end while return S = S ;
By applying the core merge step to the initial cores, more succinct yet complete cores can be generated for later ex-pansion. More importantly, this step guarantees that lower t values can generate a collection of cores that include all cores that would be retu rned by using a higher t values. In the core probing process, for each core K i we can record its frequency f i of co-occurrences in the collection and rank communities based on f i . The following property stands for the community ranking:
Property 3. Given two core probing thresholds t 1 and t 2 ( t 1 &lt;t 2 ), the core merge algorithm returns two sets of core candidates respectively, namely K 1 = {K (1) i K 2 = {K (2) j } .Coresin K 1 and K 2 are ranked according to their co-occurrence frequencies respectively and recorded in the ranking sequence.  X K (2) i  X  K 2 ,  X  X  (1) j  X  K 1 ,s.t. K
According to Property 3, the frequency-based community ranking is consistent regardless t values. For K 1 and K ( t 1 &lt;t 2 ), the top-k ( k = K 2 ) rankings of communi-ties based on their co-occurrence frequencies are consistent. Higher t values can promise to return a small set of rep-resentative cores and lower t values can generate a larger size of cores with representative cores promoted. Therefore, Property 3 guarantees a high consistency of returned cores regardless the initial clustering setting.
After finding cores K of a community C , all remaining documents { d i | d i  X  X  X  X } are taken as affiliated members, whose affiliation relationship can be determined through re-lation propagation. C is initialized to be K . Iteratively, for each document d i in C , we find all documents in the cor-pus that link to d i and add them to C . To avoid infinity loops caused by relation circles, we can limit the number of iterations. The propagation process terminates if no new documents are added to C in an iteration.

Among affiliated members, interdisciplinary members D can be identified by checking the common member sets be-tween any two communities. If it is found that a member belongs to multiple communities, it is identified as interdis-ciplinary member of C .

During the affiliation propagation process, internal hier-archy of a community is built simultaneously. We evaluate the relevance of a member to the affiliated community with its closeness to a community core. The closer a member is to K , the higher its rank is. In the community propaga-tion process, every time a new document is inserted into a community, the iteration value is recorded to represent its distance to the cores.
Finding communities solely based on relation propagation can generate false hits because of weak relations with topi-cal ambiguity. After the relation study, in this stage, com-munities are refined by filtering out these false hits from community candidates using attribute analysis.

Theoretically, this pruning process can be viewed as a spe-cial classification task. Given a community C i = { d i 1 with its core K i , we want to classify C i into two collections C i and  X  C i so that ting, each document d j in the core K i is taken as a positive example, which suggests d j belongs to C i .Wealsosample documents from other community cores, which have dra-matic different latent topic distributions, as negative exam-ples. LDA is applied to the dataset to reduce dimensionality ( n ). After clustering, each document in C i as well as all sampled documents will have a feature vector to represent its topical position in the feature space, which is denoted as z = &lt;v 1 ,v 2 , ..., v n &gt; . The absolute value of the LDA re-sult is not used as a document X  X  actual topic because we do not tune n in this process. On the other hand, the vector representation of each document is taken as features used in the classification process to reveal the topical similarity between documents.

After dimension reduction, the training set, including K i and documents from other communities, is supplied to Sup-port Vector Machine (SVM) to train a binary classifier. All documents in C i are classified and negative labeled docu-ments are removed from C i .

From the previous introduction to the community discov-ery process it is observed that the necessary attributes we defined in Definition 1 (internal ranking, interdisciplinary members, and community cores) are constructed during the process, which do not require a second run to the dataset.
In this section, we perform an empirical evaluation of our proposal. Section 5.1 describes the experiment settings and the testing methods. Section 5.2 reports our experiment resultsaswellasanalysis.
We conduct our evaluations based on an academic dataset extracted from the CiteSeer computer science digital library Papers and their associated citations are extracted for com-munity discovery. For each paper d i ,itsabstract,title,au-thor, venue, as well as citation relations are collected. Table 2 gives the summary of the dataset as well as the default settings of parameters, which are used unless explicitly spec-ified.

We use two metrics, namely mining time overhead and  X  PCS , both discussed and used in Section 3. For each ob-tained result set, 10% of communities are randomly sampled. The topic label of a community ( Z i ) is obtained manually by checking the metadata records of its core papers.
First, to test the scalability of proposed algorithm, we vary the size of datasets and filtering thresholds.
Figure 5, which plots the time cost corresponding to dif-ferent sizes of datasets ranging from 50 , 000 documents to 500 , 000 documents, shows that the algorithm finished the community discovery task quickly despite of dataset sizes. It takes about 1 , 200 seconds for the complete dataset (500 , 000 documents) and only around 400 seconds for a dataset of 50 , 000 papers. Compared with LDA which takes about 20 , 000 seconds to process the dataset of 50 , 000 documents (as shown in Figure 1(a)), the proposed algorithm is much faster (i.e., in more than two orders of magnitude). Re-gardless the dataset size, our algorithm can quickly focus itself on a small subset of core documents and make textual analysis on them. Based on the trends of curves in these two figures, we also find our proposal is much more scalable than LDA. More importantly, when applied to the complete testing dataset, our algorithm does not suffer from mem-ory limitation whereas LDA fails to return any result in our testing platform.

The follow-up study checks the algorithmic cost in find-ing communities for different filtering threshold settings on the complete dataset. The results are plotted in Figure 6. In this test, the filtering threshold t is varied from 11 to 134. Basically, a low threshold can create more commu-nity core candidates for the later merge process. Thereafter, the overhead in discovering communities with low thresholds is expected to be higher, which is confirmed in the figure. http://citeseer.ist.psu.edu/oai.html However, for the lowest t value in the figure, the overall mining time is only 1 , 502 seconds. Figure 6 also shows the overhead in each of the four steps in community discovery. The step-wise comparison reveals the major cost difference is caused by the core probing phase and core merge phase. After merging, the overheads of the algorithm under differ-ent threshold settings are similar, suggesting the consistency in merged communities for different parameter settings. Figure 6: Time overhead to discover communities with different filtering thresholds.
In this section, we try to compare our proposed algorithm with several typical textual data clustering approaches. Namely, we choose LDA as the representative of textual attribute clustering technique, which has been introduced in Section 3. Meanwhile, we choose a relational clustering model, Concentric-Circle Model [23], which identifies cores of communities only via relation study. Due to the scalability problem of LDA, we use the document set containing all 29 , 869 papers pub-lished in 2001 as the test dataset. The proposed algorithm, together with the Concentric-Circle Model, uses the default parameter setting listed in Table 2. As of LDA, we use the topic number that generates best community topic precision in our tests in Section 3, 120, as the i nput parameter. Our experimental results are given in Figure 7. From Figure 7(a), we find our proposed solution (labeled as Ours ) generates the best quality communities. Its  X  PCS is better than the other two approaches for more than 15%. One of the reasons for high  X  PCS can be found in Figure 7(b), which suggests the proposed solution creates communities with smallest sizes. On the other hand, LDA predefined the number of topics whereas the Concentric Model includes too many non-relevant documents in communities because no attribute analysis is performed in its clustering process.
In this section, we perform a sensitivity test of the re-turned community quality (in terms of topic precision) against various parameter settings.

First of all, we conduct an experiment to see the effect of the filtering threshold t on the community quality as well as the effect of the merge process. Figure 8 gives the compari-son results. As shown, although different filtering threshold t values result in different number of communities and differ-ent community precisions, the  X  PCS values are very stable, achieving over 80% for most cases. Figure 8 also shows that the core merge process is very effective in reducing the nega-tive effects of filtering threshold settings. For example, after merging, the number of cores is reduced from 4 , 264 to 2 , 209 ( t = 14) and from 3 , 144 to 1 , 839 ( t = 16). The gap between the sizes of the core collections returned by these two tests is greatly narrowed, suggesting that a very consistent com-munity discovery regardless the initial t settings. Compared with the experiment on LDA (as shown in Section 3), our community discovery solution provides a larger sweet zone for its input parameters. On the other hand, if an improper filtering threshold is set, our algorithm can still promise a high consistency in popular communities due to Property 3 in Section 4.3.2. Figure 8: Effects of filtering threshold t and the merge process.

Meanwhile, it can be observed that the merge process may lower the topic precision slightly because documents that are originally assigned to different communities may be put into one community after merge. The topic coverage is thus larger and the distance of outlier documents to the commu-nity centroid is extended.

Next,wecontinuetostudytheimpactofdifferentmerge policies to determine the best strategy in merging cores. In the experiment, four differe nt core merge policies are put into comparison, which are (1) OR Policy , as introduced in Equation (6); (2) AND Policy , which requires both topic centroids of target cores to be within the overlapping topic area, as described in Equation (7); (3) Overlap Policy , which solely based on the co-occurrence of common mem-bers between two candidate cores and neglect the attributes. Two cores are merged as long as they have any common member; and (4) Distance-Based Policy , which check the distance  X  d ij between the two topic centroids of cores. To avoid introducing new distance threshold, we find a pair of documents from K i and K j respectively which have the longest distance between each other, denoted as d max ij .If is less than half of d max ij , the two cores are taken as relevant and are merged afterwards. The experimental results are given in Figure 9. From Figure 9(a) and Figure 9(b), it is observed that the AND-policy is the strongest, sacrificing the merge opportu-nities for higher precision. On the other hand, the Overlap-policy is the weakest policy and its overall topic precision is thus the lowest. The OR-policy and the Distance-based policy yields similar results. However, the Distance-based policy requires pair-wise distance computation between doc-uments in cores, which is very expensive for high dimension feature spaces. Thereafter, since the OR-policy is very sim-ple and effective, it is used as the default merge policy in the following experiments.

In the core merge process, as described in Section 4.3.2, each core document is transformed into a vector in a low-dimension feature space using LDA. Thus, a new variable, n , is introduced. A natural question arises correspondingly,  X  X ill the selection of n greatly impact the final community precision like what we already see in the LDA clustering analysis in Section 3? X  To answer this question, a series of experiments are performed, using different n values to test the community precision. The results are given in Figure 10. Figure 10: Effects of n to community precision.

In this experiment, n isvariedfrom5to50toobserveits impact on the precision of returned communities. Basically, as shown in the figure, the difference is very small and thus is negligible. Extremely small n values ( n =5)canlower  X  PCS by 5%. For remaining tests, the community quality remains stable regardless n values. It is also observed that higher n does not necessarily generate better community results in terms of  X  PCS .

Finally, we show the effects of the member classification phase in community discovery. Figure 11 gives the change of  X 
PCS as well as the average size of communities. It is found the classification phase is very effective in promoting the topic consistency of a community by eliminating topi-cally non-relevant documents. A significant improvement in the  X  PCS value is found for each test (about 10%). From the average community size distribution in Figure 11, the average community size increases as the filtering threshold increases, whether it X  X  before o r after classification. This is because a higher filtering threshold can find hot community topics that attract many followers. However, after classifi-cation, the average size of communities becomes relatively stable for each test.
In this paper, we exploit the problem of current cluster-ing techniques for community discovery on textual datasets. A new hierarchical community model is proposed to cap-ture the observation that a community typically consists of a set of tightly relevant members with common latent top-ics. Based on this model, a community discovery solution is developed to discover communities from large-scale docu-ment corpus. Our solution overcomes the scalability issue by limiting analysis scope to a small subset using relation analy-sis. A core merge step is employed to make the solution less sensitive to initial parameter settings. Evaluation results validate our ideas and show that our solution outperforms existing community discovery techniques. The sensitivity tests further demonstrate our approach works very well in discovering useful and accurate communities, with low costs. In the future, we plan to study the dependency of communi-ties and their causality, which can be used to predict future trends and transitions.
