 1. Introduction
The experimental evaluation of Information Retrieval (IR) systems relies on a well-known and widely established meth-odology, dating back to the Cranfield experiments in the 1960s and usually referred to as TREC-like evaluation (see, e.g., San-derson (2010) for a recent survey). One issue in current TREC-like test collection initiatives is the cost related to relevance assessment: assessing requires resources in terms of infrastructure, organization, time, money, and does not scale up easily. Indeed, in recent years, there has been some trend on trying to save assessment resources: there is a vast body of literature on reducing the number of documents pooled and/or judged ( Aslam, Pavlu, &amp; Yilmaz, 2006; Carterette, Pavlu, Kanoulas, effort is obviously a problematic issue when building your own test collection.

Our position is that crowdsourcing can be a reliable alternative to  X  X  X lassical X  X  assessors. In this paper we propose to use crowdsourcing platforms like Amazon Mechanical Turk for relevance assessing; we also support this position by some exper-imental data. Stated briefly and informally, our research question is:  X  X  X an we get rid of TREC assessors? X  X  More specifically, we set to understand if, how, and to what extent it is possible to replace TREC-like relevance assessors with workers from Amazon Mechanical Turk.
 The paper is organized as follows. Section 2 introduces the main ideas behind crowdsourcing and Amazon Mechanical Turk, presents related work in relevance evaluation by crowdsourcing-based experiments, and briefly discusses the relevant issue of disagreement between relevance assessors. Section 3 presents the experimental design and Section 4 reports on its results. Conclusions and future work are presented in Section 5 .  X  2. Related work 2.1. Crowdsourcing and Amazon Mechanical Turk examples of crowdsourcing can be found on the Web; the most known  X  and successful  X  is probably Wikipedia, but open source software, Re-CAPTCHA, or the ESP game and, more generally, all GWAP (Games With A Purpose) are other notable instances. Moreover, several websites and services have been successful and concern different kinds of activities like, e.g., t-shirt design (threadless.com), photography (iStockphoto), software development (vworker.com, formerly known as rent-acoder.com), research and development (Innocentive), and so on. In the context of relevance evaluation, crowdsourcing in-volves asking the crowd (the world) to perform different relevance tasks like assessment, relevance criteria elicitation, etc. service that gives developers the ability to include human intelligence as a core component of their applications. Developers use a web services API to submit tasks, approve completed tasks, and incorporate the answers into their software applica-tions. To the application, the transaction looks very much like any remote procedure call. The application sends the request, and the service returns the results. People come to the web site looking for tasks and receive payment for their completed work. In addition to the API, there is also the option to interact using a dashboard that includes several useful features for prototyping experiments.
 sign up to perform work is described in the system as a worker . The unit of work to be performed is called a HIT (Human
Intelligence Task). Each HIT has an associated payment and an allotted completion time; workers can see sample HITs, along with the payment and time information, before choosing whether to work on them. Sometimes, the requester needs multiple answers to the same question. To this aim, a HIT can have multiple assignments : this means that it will be executed a number of times. A worker can only accept a HIT once, so a HIT with multiple assignments is guaranteed to be performed by multiple (like a HIT) that the worker must answer to qualify and therefore work on the assignments. Details on the API and user inter-face are available in the developer documentation ( http://aws.amazon.com/mturk/ ). 2.2. Crowdsourcing and IR evaluation used by companies, researchers, and start-ups to test and evaluate different techniques. Although in its infancy, this is a 2011 ). A specific example is the TERC crowdsourcing approach that uses an editorial approach on a larger scale ( Alonso, Rose, &amp; Stewart, 2008 ). The main benefits of this approach are fast turnaround, low cost, high quality, and flexibility.
MTurk workers and TREC assessors on a single topic (topic number 011, concerning space program). We tried several alter-natives: we used both binary and graded relevance, we repeatedly modified the instructions (for instance, by simplifying the showed that workers were as good as the original assessors, and in some cases they were able to detect assessors X  errors in port on a larger study, built on the basis of the experience gathered by these five preliminary experiments. sults. Grady and Lease (2010) focused their work in human factors for crowdsourcing assessments with the goal of measur-ing accuracy, time, and expense. Alonso and Mizzaro (2009a) showed how MTurk can be used to elicit relevance criteria.
Nowak and R X ger (2010) compared image annotation in ImageCLEF by experts and MTurk workers, finding comparable quality. Kazai, Kamps, Koolen, and Milic-Frayling (2011) studied the effect of different strategies on obtaining relevance assessments by MTurk workers within the INEX Book track. The usage of crowdsourcing to gather relevance labels, in order to replace and/or complement classical assessors, is also being experimented in practice: at TREC 2010, within the Blog track ( McCreadie, Macdonald, &amp; Ounis, 2010, 2011 ), and at TREC 2011 where a crowdsourcing track is being experimented (and conservative in their judgments compared to the average participant.
 methodology. One of the first attempts to discuss design and implementation issues with crowdsourcing is described in
Alonso and Baeza-Yates (2011) . An in-depth tutorial on crowdsourcing in the context of information retrieval is presented in Alonso and Lease (2011) .
 by Snow, O X  X onnor, Jurafsky, and Ng (2008) shows the quality of workers in the context of four different language tasks such as affect recognition, word similarity, textual entailment, and event temporal ordering. Callison-Burch (2009) shows how Mechanical Turk can be used for evaluating machine translation. One of the first user studies with MTurk was performed by Kittur, Chi, and Suh (2008) . 2.3. Assessors disagreement
Assessor errors and disagreement have been recognized as an important research issue in IR evaluation by means of test collections since the first test collections in the 1960s. Some amount of disagreement among assessors is taken for granted. A key question is whether assessor disagreement is so high and/or biased to imply some undesired effect, like an unreliable measurement of retrieval effectiveness within the test collection paradigm. To this aim, some experiments have been per-formed in the TREC environment: Voorhees (2000) found high correlations among system rankings in TREC-4 when using different relevance judgments sets. Cormack, Palmer, and Clarke (1998) , while studying alternative pooling methods, re-as-sessed a part of TREC-6 collection, and found again high correlations despite a not so high agreement. Soboroff, Nicholas, and Cahan (2001) showed that even with random judgments some conclusions on systems ranking can be drawn.

More recently, Carterette, Bennet, Chickering, &amp; Dumais (2008) studied relevance judgments based on preference (i.e.,  X  X  X ocument A is more relevant than document B X  X ), and they also compared disagreement when using preference judgments with disagreement when using graded judgments. Recent work also shows that system rankings in evaluation exercises can Costello, 2009 ), after having been generally dismissed as too expensive.

Our approach is based on multiple and redundant assessors and, as we discuss in the following, it turns out that having multiple assessors is both reliable and cheap in MTurk. Also, in the following of this paper we will rely on the figures ob-tained by some of the above mentioned authors as a sort of baseline against which to compare the level of agreement ob-tained by MTurk workers. 3. Experiments
After the encouraging results of the first experiment described in Alonso and Mizzaro (2009b) and summarized in Sec-tion 2.2 , we set up to seek a confirmation of the results on a higher number of topics. We also made several adjustments in the interface design to make sure we ask the right questions in the best way possible. We followed standard guidelines for questionnaire design ( Bradburn, Sudman, &amp; Wansink, 2004 ) and well-known usability techniques ( Nielsen, 1993 ). The assessment was done in October 2009. 3.1. Experimental design We selected ten TREC topics among those re-assessed by Sormunen on a four-level relevance scale ( Sormunen, 2002 ). nale for these choices is twofold: on the one side, we were interested in studying the differences between binary and graded relevance as well (see below), and Sormunen X  X  dataset provides graded assessment; on the other side, in this way we had a rather objective criterion for topics selection.

For each topic, we selected at most 22 documents (see the exact numbers in Table 1 , third column). To have a reasonable mix, for each topic we aimed at selecting 16 relevant documents (divided into 4 highly relevant, 6 relevant, and 6 marginally relevant) and 6 not relevant ones. We used again an objective criterion for documents selection: we ranked alphabetically the documents in the pool (accordingly to their identifiers) and ran a script that picked up the first 4 that were assessed as number of documents in each relevance category could not be reached (for example, only one document has been assessed as highly relevant for the last topic, 410): in those cases, we have fewer than 22 documents, as shown in Table 1 . by workers (the number of assessments for each topic is shown in the fourth column in Table 1 ), thus 1030 assessments.
Each HIT included just one relevance question and therefore collects one assessment per worker, and we used multiple with a cost of $26. Then, we also re-run the same experiment to collect graded relevance assessments, in the same number and with the same cost. In all experiments we used a qualification test that consisted of a few questions about the topics.
Fig. 1 shows a HIT as seen by the workers in the graded relevance experiment. The only difference in the binary version is the relevance assessment question (bottom part of the figure): 3.2. Worker activity often want such distribution when working in our experiments, and it is more difficult to be obtained when relying on stu-dents or researchers, that tend to be more homogeneous. That said, it is important to note that not everybody does the same amount of work. Fig. 2 shows a graph of how many workers versus how many tasks in the two experiments. As we can see, this resembles a power law distribution, as expected: many workers do very few HITs (lower right corner of the graph) and a few ones do a lot of HITs (upper left). This is consistent with other previous research work on crowdsourcing: the population follows a power-law. 4. Results eral respects (Section 4.1 ); we repeat the analysis for graded relevance (Section 4.2 ); we compare our agreement levels with those reported in the literature and discuss whether disagreement is caused by worker errors, or if conversely the errors were made by the original TREC assessors (Section 4.3 ); and finally, we discuss more qualitative results like workers com-ments (Section 4.4 ).
 4.1. Binary relevance
Several definitions and measures of agreement exist; we rely on both an intuitive and graphical analysis and the following statistical numerical figures: This is by no means the only possible choice, and several alternative agreement definitions and measures could be used. Since there is no agreement on a single agreement measure, in the following we will often show a set of values, of course selecting only the measures that are meaningful for each case. 4.1.1. Individual agreement
A first analysis concerns the overall individual agreement between each worker and the corresponding TREC assessor, i.e., representation of the results. It can be said that overall there is a 68% agreement (51% on relevant/relevant plus 17% on not relevant/not relevant): in 68% of the cases, the worker agreed with the corresponding original TREC assessor. When comput-ing Fleiss X  kappa, we get a 0.229 value, which can be seen as a fair but not high agreement. Same value is obtained when computing alpha (0.229). The agreement among the five workers judging the same &lt;topic,document&gt; pair is even lower: Fle-iss X  kappa is 0.195, which is considered a slight agreement, and alpha is 0.195.
 Table 2 shows how agreement depends on the topic. When working on single topics, agreement varies from 57% to 84%. Also note that having higher values on the documents judged relevant by TREC assessors (than to those judged not relevant) simply depends on having more relevant than not relevant documents (this is a well known effect of percentage agreement, see e.g., ( Stemler, 2004 )).
 In general, individual agreement is not bad but not excellent. However, this is not the agreement we are interested in. Since we are seeking to replace TREC assessors with a group of workers assessing the same &lt;topic,document&gt; pair, what is really needed and important is if and how the group of workers working on one pair agrees with the original TREC assessor working on the same pair, rather than if the single worker agrees with the TREC assessor. In other terms, we need to aggre-gate/group the workers working on the same pair. We now turn to analyze this agreement. 4.1.2. Group agreement
As described in Section 3.1 , a group of five workers is assessing the same &lt;topic,document&gt; pair. One intuitive way to reduce a group of assessments to a single assessment is the average assessment. By casting the relevance labels into a ratio 1. This means that the error (i.e., the difference between TREC assessor and the group of workers) can be: the cases where all the five workers agreed with the TREC assessor (22%). The small errors (inside the inner rectangle) are not worrying: they represent the cases where just one out of five workers did not agree with the other four ones and with the TREC assessor, and sum up, with the previous zero errors, to 55%.

This average voting analysis can be used to understand what happens if a majority voting is used, i.e., if we take as the assessment by the group of five workers the assessment expressed by at least three out of five of them (this would corre-spond to the mode on the original relevance scale): the errors within the outer dashed rectangle are not large enough to cause a classification error if a majority voting is used. By summing up we get an overall 77% of cases where the majority of a group of workers agreed with the TREC assessor, which is significantly higher than the 68% individual agreement of pre-rying striking errors , were at least 4 out of five workers disagreed with the original TREC assessor.

Since percentage agreement can overestimate agreement due to the chance of random agreement, we also compute Co-hen X  X  kappa statistics. It cannot be computed for the average voting because the scales are different; for the majority voting, Cohen X  X  kappa figure is 0.478, which is considered a moderate agreement (higher than the individual agreement). Alpha is 0.475.

Also, Fig. 4 shows that agreement is higher on documents judged not relevant by TREC assessors, and confirmed as not relevant by workers X  X r, conversely, that errors are higher on the right hand side of the figure, i.e., for documents judged relevant by TREC assessors. Most disagreement comes from workers judging as not relevant what TREC assessors judged i.e., that about 50% of documents assessed as relevant by TREC assessors are reassessed as just marginally relevant. 4.1.3. Agreement over number of workers
Another issue is whether the agreement level varies (increases) with the number of workers. If this is the case, higher agreement and accuracy can be obtained simply by adding more workers. Fig. 5 a X  X  shows the error distribution when using this experiment we kept the original order of HITs execution, since there might be a difference between the first workers and the last ones, and this effect would be lost if we randomized the workers. Similarly to what has been done above for group ger than 0.5. Fig. 5 f shows the distribution and trend of these large errors: the linear trend line emphasizes the clear trend towards a lower rate of large errors, although there are some fluctuations. 4.2. Graded relevance
As anticipated above, we were also interested in studying workers X  assessment when using graded relevance. The graded assessments were provided by the work at the University of Tampere (UTARELS) ( Sormunen, 2002 ). The graded workers data were obtained by means of the second MTurk experiment; this second experiment was similar to the previous one: we used the same topics and documents, and we collected five assessments for each &lt;topic,document&gt; pair. The only difference was that workers were asked to assess the relevance on a four-level relevance scale. On the basis of the experience gathered dur-from Sormunen X  X , mainly because Sormunen X  X  assessors were trained, and that was not feasible in our experiment. There-fore, the scale we used was a mixture between Sormunen wording and the labels we used in our previous experiment, with some minor changes that led to the scale shown in Fig. 1 (bottom).

Fig. 6 a shows the individual agreement between Sormunen X  X  assessors and graded workers (and the table in Fig. 6 b shows the values more clearly). There is some agreement, since the bars  X  X  X long the diagonal X  X  tend to be higher, but by comparing to Fig. 3 , it probably seems that the agreement is lower in this case. The agreement is low even if we group the 0 and 1 judg-ments (as not relevant) and the 2 and 3 (as relevant), see Fig. 6 c: by doing so, we get a 59% agreement, lower than the 68% of Fig. 3 . An agreement comparable to X  X ctually, higher than X  X hat in Fig. 2 can be obtained by grouping 0 as not relevant and 1, liberal notion of relevance in TREC mentioned at the end of Section 4.1.2 .

We now turn to the more pragmatic topic of group agreement. As done above, we group all the five workers working on ing; therefore we compute the average assessment of each group of workers. 1 This average can of course be compared with possible error distributions. In the six subfigures we consider all the six possible agreements, between: simply multiplying by three the [0,1] values of binary relevance). Also note that the vertical axis has different scales in the subfigures.

Fig. 7 b is of particular interest. Graphically, the agreement is rather low; however, the situation is less worrying when analyzing the errors as done above for the binary case. The maximum error happens when the UTARELS assessor judged a document as maximally relevant (not relevant) and all five workers judge it as not relevant (relevant). As in the binary case corresponds to either correct classification or misclassification in the adjacent category). Although worse than the binary case, the agreement is anyway promising.

A noticeable difference between graded and binary cases is that in Fig. 7 b most of the errors are on the left side, which corresponds to the less relevant documents according to UTARELS assessors (whereas in Figs. 4 and 7 a most of the disagree-ment was on the right side). This can be ascribed again to the more liberal notion of relevance in TREC than in UTARELS, as confirmed by Fig. 7 d.

Fig. 7 d also shows that the disagreement between binary (qrels) and graded (UTARELS) original assessors is not negligible at all. This is also confirmed by the numerical figures. To compute the statistics, we can convert the graded into binary in the three cases, Cohen X  X  kappas are 0.087, 0.409, 0.773, respectively, and Krippendorff X  X  alphas are 0.209, 0.363, 0.774, respectively. As expected, the liberal case has higher agreement. between binary and graded workers. Being it left skewed, we have yet another instance of a liberal notion of relevance: bin-ary workers tend to judge relevant what graded workers tend to judge partially relevant. 4.3. Disagreement and error analysis sors. The first is that by comparing the agreement figures presented in the previous sections and those reported in other studies in the literature, we find that the assessor-worker agreement is not so different from the agreement between TREC  X  or anyway more expert  X  assessors. Voorhees (2000) studied assessors different from those that originated the topic. She measured agreement by means of overlap of the relevant documents set (this is another agreement measure defined, for two assessors, as the number of documents judged relevant by both assessors divided by the number of documents judged rel-evant by at least one assessor), and she obtained overlap values of about 0.4 X 0.5 (see Table 1 in her paper). By computing the overlap on our data (presented in Section 4.1.1 ), we get 0.6, a higher value indicating higher agreement. Cormack et al. (1998) relied on trained assessors and also worked on binary relevance. They obtained a percentage agreement of 78% (this can be computed by the data in Fig. 1 in their paper). This value is higher than our individual agreement of 68%, and this suggests that workers tend to disagree with original assessors more than trained assessors. However, we got almost the same value when relying on group agreement (77%, see Section 4.1.2 ).
 of Carterette, Bennet et al. (2008) : when using a 5-points relevance scale they got a 43% individual agreement, that increases to 69% and 78% when grouping the five categories into two (values computed from Table 1 in their paper). Our values (see Section 4.2 ) are lower: 36%, 59%, and 73%. The difference is even more important when considering that we used a 4-points scale, which should have a higher agreement. However, again, the situation improves when using group agreement: our va-lue of 68% is more similar to the baseline. To summarize, by using five redundant workers we have been able to get agree-ment levels similar to those obtained by expert assessors, especially in the case of binary relevance.

The second observation concerns the analysis of errors. Since our previous experiments ( Alonso &amp; Mizzaro, 2009b ) hinted that sometimes workers might be more correct than TREC assessors (and since it is well known that TREC assessment is far from perfect), we performed a more specific error analysis. We examined the most striking errors, i.e., those where at least four workers disagreed with the corresponding TREC assessor. This corresponds to the two rightmost bars and the two left-description of the topics), the error, the original TREC assessment, the average assessment of the group of workers, the UTA-RELS assessment, and, in the last two columns, another assessment and who is correct between TREC and workers, both according to ourselves. From the table, we observe that:
This analysis is a further confirmation of the good reliability of workers. Of course, we are not claiming that workers are more correct than TREC assessors: a more complete and independent experiment would be needed for this, and we would also need an unbiased sample of documents. However, this is not our aim: we are just claiming that when workers and TREC assessors disagree, the disagreement is often a reasonable one  X  it cannot be taken for granted that workers are wrong. 4.4. Qualitative results
As part of the HIT design, we were interested in getting more user feedback on each task. One way is to ask an optional open-ended question at the end of the task, which is often a justification for the answer provided. Our interest in exploiting this feature is that we can know for the same cost why a document is relevant or not. It is important to note that none of the TREC assessments contains any information about why a particular document was relevant or not.
 139.76 (characters). Some examples of worker feedback (unedited) are: understanding of what the workers thought about a document. We show a few examples, containing  X  X  X ints X  X  about what the document is all about: tions that term  X  X  X alkland X  X  is not on the document and therefore it should not be relevant, but the document contains the term  X  X  X alvinas X  X , the alternative name for the islands: 5. Conclusions and future work the original TREC assessor is not high when measured individually, but it increases when workers are grouped. When dis-agreement exists, in the majority of cases TREC assessors X  assessments are at least questionable: workers not only are accu-rate in assessing relevance but in some cases were as precise as the original expert assessors, if not more. Also, workers tend to disagree with the original assessors slightly more when the document is relevant. This probably calls for a more precise and less liberal notion of relevance in TREC, and/or for graded relevance.
 not have data to answer this, which is an interesting research question left for future work.
 redundant judgments, and they are a good indication that MTurk can be a reliable, quick, and cheap alternative for relevance assessment in TREC-like initiatives. Also, these results seem to hold for both binary and (to a lesser extent) graded relevance assessment. Finally, MTurk seems useful for researchers building in-house evaluation collections as well. For example, a col-lection with 10 topics and 200 assessed documents per topic could be built with a budget of about $200 (with the above setting). Although we do not claim that MTurk, or rather crowdsourcing in general, are replacements for traditional mech-anisms for gathering assessments, we think that crowdsourcing is an interesting alternative approach and, taken as a whole, can be a very useful tool for researchers and practitioners.

After this experience, we can also draw some suggestions on how to run crowdsourcing experiments. It is important to design the experiments carefully. Mapping TREC assessment instructions to MTurk is not trivial and some modifications have to be made. The TREC-7 guidelines is a 4-page document that has to be summarized in a few sentences for reading on-line, since the worker sees a screen with instructions and task to be completed. It is important to be concise, precise, and clear about how to evaluate the relevance of a document. The usage of some basic usability design considerations for pre-tent and clear instructions are, among other items, necessary conditions for getting attention from workers and therefore good quality results at the end. In our experience, all experiments without qualification tests were completed in less than 48 h. Once qualification test was involved, the completion rate per worker was much higher. This is expected as certain workers may not feel like going through a qualification test to get work done. The number of workers required to assess each document can have an impact on the duration.

This paper covered several issues, but a lot is left as future work. Although we have some indication that increasing the cardinality of a group of workers leads to higher agreement with TREC assessors (see Fig. 5 f), we only studied a maximum cardinality of five. The time spent by each worker on his/her HIT could be taken into account, for instance to study its cor-worker assessment, perhaps simply by assuming that the longer the time, the more precise the answer. More in general, some quality analysis could be performed to single out spammers and thus get more precise results. The disagreement anal-ysis could be extended to analyze the topic and document features (document length, presence of synonyms, etc.), if any, that can lead to disagreement. Finally, the problem of higher noise level of workers with respect to expert assessors could be solved by exploiting other techniques, like using more topics, different pool depths, etc. The combination of these differ-ent techniques with MTurk assessment is another interesting research direction.
 Acknowledgments
The authors would like to thank Ellen Voorhees for answering many questions about TREC assessment at NIST, Eero Sorm-unen for providing the UTARELS graded assessments, and the anonymous reviewers for providing very constructive comments.
 References
