 Question answering systems increasingly need to deal with complex information needs that require more than simple factoid answers. The evaluation of such systems is usually carried out using precision-or recall-base d system performance metrics. Previous work has demonstrated that when users are shown two search result lists side-by-side, they can reliably differentiate between the qualities of the lists. We investigate the consistency between this user-based approach and system-oriented metrics in the question answering environment. Our initial results indicate that the two methodologies show a high level of disagreement. H.3.4 [ Information Storage and Retrieval ]: Systems and Software ---Performance evaluation Experimentation, Huma n Factors, Performance TREC, ciQA, human preference and judgement. In many situations, users have questions that can only be answered by a set of related facts. The complex interactive question answering (ciQA) task of the TREC Question Answering track provides an experimental framework for such a real-life information search task. A typical example of a complex question (from TREC 2007) is:  X  X hat evidence is there for transport of [illegal immigrants] from [Croatia] to [the European Union]? X  A question answering (QA) system ai ms to provide a set of text fragments (or answer strings) th at, taken together, contain as many different answer aspects as possible. ciQA is a challenging task, and since state-of-the-art QA systems are not yet able to supply precise lists of answers, allowing user interaction is a promising approach for impr oving system performance. The performance of QA systems can be measured with a variety of system performance metric s, including Nugget Precision, Average Precision, and Pyramid F-Score [1]. However, to apply these measurements, a lot of effort is required to examine each retrieved answer string. Thomas and Hawking have proposed a methodology for extracting user preferences between different sets of search results: users are presented with tw o ranked lists of search results, displayed side-by-side on a screen [4]. In their work, they used answer lists returned by the G oogle search engine, and compared lists consisting of answers in rank positions 1 X 10 with the answers in positions 6 X 15, and 21 X 30. For both cases, users were reliably able to choose the better list. In this paper, we apply the list-comparison methodology to the complex question answering task, and investigate whether a user X  X  preference of a list is corre lated to the quality of the list as judged in a TREC style. This study is our first attempt to find a reliable way that would take less human effort to compare two systems X  performance on the QA task. These experiments were conducted as part of our participation in the TREC 2007 ciQA task. The interaction design included 30 complex questions; for each, participants could submit a URL to an interactive system. NIST assessors (only the primary assessors, who developed the questions) spent five minutes interacting with the system for each question. The interface for list preference choice displayed two panels; for each question, these panels were populated using answer lists generated from two different back -end systems. The lists were assigned randomly to the left or th e right panel, with the two lists having an equal chance to be on either side for 30 questions. Assessors were instructed to browse the answer lists, and to select their preferred answer list by clicking a button located on top of each list. After each query, assesso rs were given a questionnaire to gather additional information about why one list was preferred to the other. Given the per-query time limitations, we displayed only the ten top-scoring answer sentences from each list. To create answer lists, we used the Lemur toolkit Mercer smoothing (  X  = 0.5) to retrieve the top twenty documents for each question. These documents were then parsed into sentences, which were ranked according to a combination of: the longest span of matched query words; the total number of matched query words; and the num ber of matched distinct query words. The difference between the two back-end systems arises from the query construction method: for Sy stem 1, those words embedded in brackets in a question template were run as a query; for System 2, the query was expa nded manually, with additional words from the narrative field of a question topic (not including introductory words such as  X  X he analyst would like to know of X ). http://www.lemurproject.org These words may give additiona l information about what is counted as an answer. The main effort required for the evaluation of ciQA systems is the process of judging answer strings [1]. First, the answer strings for a question are pooled from all submitted runs. A primary assessor then identifies a list of  X  X uggets  X  X  X he sought-after concepts or relationships specified in the ques tion. Nuggets are classified into two types: vital, representing con cepts that must be present in a good answer, and okay, consisting of useful but not essential (including a second judgement from the primary assessor) independently judge each nugget as vital or okay. Answer strings in each submitted run are then labeled to indicate whether a nugget is present in an answer string. From this, system performance is measured by cal culating a nugget pyramid score for each answer string, by averaging and normalizing the nine votes. A pyramid F-score for each list can be calculated based on the pyramid scores of the answer strings [2]. Across all 30 questions, there is no significant difference between our System 1 and 2 runs (0.365 and 0.361, respectively). Figure 1: Preference choice versus average precision. In the x-axis: 1, System-1 preferred; 2, System-2; 0, equal; and x, no choice made. Topic numbers are indicated under preferences. To investigate whether the side-by-side user preferences agreed with scoring of the lists based on individual nuggets we used the average precision scoring scheme as opposed to the nugget F-score, which includes a recall component. We reasoned that users judging truncated lists in a side-by-side manner would use a precision style scheme, rather than a recall based scheme. Average precision (AP) is calculated as usual, with the precision of individual items calculated as a length allowance based on the number of both vital and okay nuggets returned [2]. The assessors of our interactive systems examined 30 questions. For 16 questions they preferred one system over the other; for 12 questions they found no difference between the two systems; and for 2 questions no preference choice was made. Figure 1 shows the relationship between assessor preferences (labeled on the x-axis) and the difference in AP between System 1 and 2. The results are surprising: out of the 28 preference choices made, 20 do not agree with the difference in system AP. For example, if System 1 has higher performance than System 2 based on AP, represented by a positive y-axis va lue, then a disagreement occurs if the choice made using the preference interface was for System 2 or  X  X o difference X . The figure also shows the difference in system AP when measured us ing only the judgments of the primary assessors. Here, 17 out of 28 items still show disagreement, in this case between the same individual when judging a system based on comparing to side-by-side answer lists, compared to AP calculated from the judging of individual answer strings. Similar results hold for other system performance metrics such as P@10. The 30 questions of the ciQA track covered 5 different question  X  X ypes X . We didn X  X  observe any correlation between the question types and the mismatch of assessors X  preference with system performance differences. However we did notice some assessor effects: for some assessors, preferences tended to agree more with the system differences. Our investigation of whether side-by-side user preferences agree with system evaluation metrics for complex questions indicates that, unlike for a web search task , in the majority of cases list preference does not agree with system performance metrics. This may be due to a variety of r easons. First, making relevance judgments for complex QA may be more complicated than for topic-finding ad hoc search tasks. The former requires not only a judgement about the relevance of an answer string at a surface level, but also requires an identification of whether each answer string contributes extra informati on. This is likely to impose a higher cognitive load on assessors, something that was commented on in our system surveys. Second, the quality of answer lists shown in Figure 1 includes the primary assessor X  X  judgment after the interaction. Ideally, the interactive assessor X  X  judgment at the individual nugget level should be obtained at the same time that they interact with the side-by-side interface, since judgments may change as a t opic becomes more and more familiar [3]. This is also borne out by a related experiment: as part of our participation in the ciQA task, we also used another interface to gather assessor judgments of individual answer strings. These judgments were onl y obtained for 15 questions per system, making pair-wise compar ison impossible. However, we observed that out of 300 answer strings, our interactive assessors disagreed with their own post-interaction judgments for 129 sentences: during the interacti on, assessors judged 127 answer strings as  X  X efinitely an answer X , while after the interaction they labeled only 32 of them as containing nuggets. To clarify the cause of disagreem ent between user list preferences and system performance metrics, we plan to conduct further experiments, testing more users per topic, and collecting their own judgment at the individual nugget level as close to the interaction as possible. We also plan to investigate the effect of varying rank position, and nugge t re-occurrence, to better understand how users judge a QA answer list as a set. [1] H. T. Dang and D. Kelly and J. Lin. Overview of the TREC-[2] J. Lin and D. Demner-Fushman. Will Pyramids built of [3] I. Ruthven and L. Azzopardi et al. Intra-assessor consistency [4] P. Thomas and D. Hawking. Evaluation by Comparing 
