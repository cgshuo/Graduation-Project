 Researchers have focused on finding experts in individual domains, such as emails, forums, question answering, blogs, and microblogs. In this paper, we propose an algorithm for finding experts across these different domains. To do this, we propose an expertise framework that aims at extracting key expertise features and building an unified scoring model based on SVM ranking algorithm. We evaluate our model on a real World dataset and show that it is significantly better than the prior state-of-art.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation, Theory Expertise finding; Social media; Enterprise search
The existing efforts on expertise discovery have targeted a specific data source for finding experts, such as emails [4], question answering [11], blogs [7] and microblogs [9]. There is little prior work (except [5]) that models expertise by com-bining data from these web sources -primarily because of challenges in gathering such data and disambiguating users across website (e.g. finding Twitterers on Facebook) can be impossible. Prior work [5] considered data within an En-terprise that allows its employees to publish documents in several data sources internally. They proposed an expertise model that indexes all the content in Lucene and uses the relevant documents retrieved through Lucene to compute users X  expertise. There are several shortcomings of this ap-proach. First it assumes that all documents are reflective c  X  of the expertise of its authors, which need not be the case. Second, it uses a weighted sum of document relevance and popularity to compute users X  expertise. This aggregation strategy can bias the model towards authors that have large number of less relevant documents.

Our expertise framework systematically addresses the short-comings of prior work. It first analyses documents on various aspects, such as language , questions , topics , indicator of ex-pertise with a goal of extracting key features from the exper-tise documents of an author. We then propose IR techniques (such as a proximity based document relevance computa-tion, concept filtering based) beyond a basic Lucene search to retrieve highly relevant documents. Finally, we propose a sophisticated rank aggregation framework that takes the rel-evant documents as input and computes the expertise score of their authors. Evaluation over a large real World dataset shows the improved performance of our approach in com-parison to prior methods.
Expertise discovery is a heavily researched area (see [1] for a comprehensive overview). Dom et al. [4] considered an email network for expertise identification. They evalu-ated several graph based algorithms and found PageRank algorithm to outperform HITS. Zhang et al. [11] presented an expertise discovery algorithm to rank experts in a ques-tion answering website by examining the number of ques-tions answered by the user along with the number of persons that were answered by the user. They showed that a sim-ple model could outperform classical expert finding models like PageRank and HITS in the QA domain. Java et al. [7] modeled the spread of influence in the blogosphere in order to select an influential set of bloggers such that they would maximize the spread of information in the blogosphere. Pal et al. [9] proposed a feature-based algorithm for finding topi-cal authorities in microblogs. They proposed features to cap-ture topical interest and prominence for users and showed that their model out-performs graph based models.

There is little work on expert finding across different web-based data sources, with a notable exception of [5]. Guy et al. [5] proposed a Lucene based model to index multi-domain documents and combined document relevance, and popularity to compute user expertise. While somewhat sim-ilar to this prior work, our approach differs in important ways. First, we incorporate a number of components and algorithms with the goal of extracting key features (topics, language) from documents that are indicative of users X  ex-pertise . Then we systematically improve the retrieval engine through key modules such as, proximity based document rel-evance computation and rank aggregation. We crawled online participation data of employees within IBM. We selected 20,000 employees randomly and crawled their complete online activity which includes publishing blogs, microblogs, wikis, forum posts. Additionally we crawled the profiles of the employees which includes their job title, role and self-reported expertise and past experience. Table 1 presents the summary statistics of the crawled dataset. Fig. 1 presents an overview of our expertise framework. It consists of several core components that are geared to-wards extracting key features, relevance engine that aims at picking the relevant documents to a query, and a ranking component to select top experts.
We describe the core components that are used for ex-tracting key features from user documents.

Language Detection : Our dataset consists of docu-ments written in several languages. However the technical jargon (such as products, tools) are shared across all lan-guages. Ignoring this multi-lingual aspect of the documents could retrieve experts versed in different languages -thereby confusing the end users. We use ngram based classifier [3] to select only the documents written in English as it is the most predominant language of our dataset (80%).

Topic Estimation : We use the Latent Dirichlet Alloca-tion algorithm (LDA) [2] to estimate the documents X  topics. To estimate the number of topics for LDA, we use Bayesian Information Criteria (BIC). We use Jensen-Shannon diver-gence [8] to compute the similarity between the documents.
Question Modeler : Users employ ingenious methods to ask questions in different data sources, e.g. a user can create a wiki page with a question and the users can answer by editing the wiki. We trained a linear SVM classifier over the ngrams of a set of labeled documents (question or not a question). Only the first four sentences of a document are considered for feature extraction as beyond that several documents have questions that do not merit any answer, e.g. if you have further questions, please contact the author .
Document Sensing : A crucial component for expertise discovery is identifying documents that reflect expertise of their authors. E.g. talk announcements, webpage summary, scribe notes, financial reports, can provide misleading indi-Algorithm 1 DocSense ( D u ) cators for users X  core expertise. For filtering such documents, we propose DocSense algorithm that takes documents au-thored by a user as input and filters them on 4 criteria: 1) non-relevance : document contains specific keywords (sum-mary, announcement, call for) in its opening paragraph, 2) url summary : document summarizing a url, 3) duplicate : document matching other documents of the author (useful for filtering financial reports, reminders, etc.), and 4) non-topical : document not pertinent to user X  X  core interest area.
Table 2 lists the document features. Most of these features are common across the different document types. Since we treat an user X  X  information such as role, bio, etc as a user document, so some of the content features are relevant to it.
We carried out language analysis using Linguistic Inquiry and Word count (LIWC) tool. LIWC provides scores on 80-90 features for the input text. We used JS divergence to compute the similarity between a post and its replies.
We use Apache Lucene to index the documents. Sepa-rate indexes are maintained for each domain; ensuring that the retrieval engine doesn X  X  get biased towards any specific data source (such as microblogs, which typically have shorter length). It also provides a flexibility of picking same number of documents per source irrespective of their relevance. We propose several components besides Lucene that are aimed towards improving the relevance of the retrieved documents.
Query Expansion : Before a query is passed to Lucene, we first use the probabilistic query expansion model [10] to expand it by adding relevant terms. This is done so that all the topically-relevant documents are retrieved. The query expansion model depends on a word-word similarity the-saurus which we build using the JS divergence over the Algorithm 2 DocRel ( d  X  document, Q  X  expanded query) words X  topical distribution. The model ensures that the terms that bind closely with the query keywords are se-lected. The expanded query is then used to retrieve 1000 documents per data source.

Document Relevance : Lucene X  X  relevance scores are based on tf-idf normalized weights of the query terms in the documents. This scheme is not ideal for two reasons: (1) restriction on the length of documents differ for differ-ent data sources, which could bias this relevance, and (2) it does not consider the proximity of the query keywords within the documents -altering the context of the docu-ments altogether. E.g. a document containing machine and learning far apart, might pertain to systems domain and not artificial intelligence. We propose DocRel algorithm that takes into consideration the minimum distance between query keywords (vector of vector due to query expansion) in the documents to compute the document relevance. The al-gorithm also takes care of the scenario where not all query keywords are present in the document.

Concept Filtering : A user query partitions the docu-ments into relevant and non-relevant bucket. The relevant documents can be further partitioned into concepts buckets. E.g. a query machine learning might draw documents from concepts, such as finance, math and statistics, computer sci-ence, software engineering. The idea here is to eliminate concepts that are not well represented by examining the un-derlying topic distribution of the document. We use Gaus-sian Mixture Model (GMM) over the document topics for partitioning them into concept space and discard document that belong to low frequency clusters.
Let the set of relevant documents provided by the retrieval engine be D = { D wiki ,D microblogs ,... } . Let d  X  D s cate document d  X  X  features (Table 2). First we estimate the relative expertise score ( R s d ) of the documents given all other relevant document from the same source.
 where n ( d ) is the number of features in d , d ( i ) indicates the i th component of d , N is a Gaussian distribution and w s i the weight on the i th feature. The model parameters  X  s are estimated using the observed D s . We get, Weights over individual features in R s d is computed through a Gaussian CDF function which is a monotonically increas-ing function and hence well suited to our scoring problem as we prefer a higher value for all features. If a lower value is preferred, then negation can be applied to that feature before applying the above formula.

In order to learn the weights w i on different features, we consider a training dataset with  X  R s d as the true score of the document. Following optimization is used to get optimal w . arg min The above optimization problem is quadratic in w s and hence can be computed using ElasticNet regression. It leads to shrinking in the values of w as well as induces sparsity. Another advantage is that amongst a block of highly corre-lated features -only few features have non-zero weights.
Once we have relative expertise of documents, we can com-pute users X  expertise per data source. Let u ( D )  X  D indicate the documents in D authored by u . Then we compute the source specific expertise ( E s u ) of a user as follows. We get expertise score for each user per data source. To combine these scores, we use SVM rank aggregation algo-rithm [6]. Let E u = [ E wiki u ... ] T be the vector of expertise score of the user u . Let ( u,v )  X  X  be the ranked set of the users, indicating that user u has higher expertise than user v . The rank aggregation problem can be written as:-The constraints are geared towards finding a ranking func-tion that is consistent with the ranked set R .  X  indicates soft-margin penalties to the separation constraints, without which the minimization problem might be infeasible (in case of non-linearly separability). The optimal weights a used to sort the users in decreasing order of a T E u value. We compared our model with several baseline models:
B1 : This model uses the prior state-of-art approach [5] for discovering experts across multiple domains.

B2 : This model adapts B 1 to use the several components proposed in our framework. The final expertise of users is aggregated the same way as suggested by B 1.

B3 : This model uses the document retrieval technique proposed by our expertise framework. Then users are se-lected with probability proportional to the relevance of the documents published by them.

All the models were trained using a small labeled dataset of the experts for two hand picked queries. In order to evalu-ate our approach, we conducted a user study in which results from our model were compared to those from three baseline models. We selected a set of 10 popular queries (such as Dis-tributed Scrum , Commercial Finance , Topic Modeling ) and picked top 10 authors per model per query. Then 40 authors per query (usually less due to some commonality) are shown to five coders in a random order. The coders were shown au-thors X  profile and the relevant documents published by them and they were asked to rate on a 5-point Likert scale, the expertise of the authors on the query. Inter rater agreement between the coders was 0.6, indicating moderate agreement.
Table 3 shows the average survey rating received by the top 10 authors. We note that the authors discovered by our model received 37% higher rating than the prior work ( B 1). It also performed 10% better than B 2 indicating the effectiveness of our expertise framework in comparison to a simple aggregation technique. All improvements were sta-tistically significant using one-sided t-test with 95% CI.
Table 4 shows the discounted cumulative gain (DCG) of the four models. We note that the DCG of our model is statistically significantly better for all queries, indicating that it surfaced top experts at a higher position in the ranked list, in comparison to the baselines.
 Table 4: Discounted cumulative gain of the models.
In practice, an end-user is looking for connecting with one expert and hence they would be satisfied if the model recom-mends at least one expert that matches their requirements. To compare the models in this practical setting, we consider the ratings given to the best-rated author per model by each coder (Table 5). We observe that authors of our model re-ceived heist ratings  X  indicating that an end-user would be most satisfied with our model. Additionally we see that the best author is surfaced within top 2 of the ranked list, which makes it easier for an end-user to locate the top expert. Table 5: (a) Best ratings and (r) rank of best author.
The previous results show that our expertise framework improves over the baselines, which is primarily due to the various components proposed in this paper. Here we es-timate the exact contribution of the different components towards model performance.
DCG works in our case as we are retrieving equal number of experts from each model Table 6: Drop in DCG by removing a component.

Recall that the authors retrieved by the full model are coded for expertise. We consider the rank of these authors and their ordering when computing the DCG of the model. In this case, we consider the DCG of the full model and consider the drop in DCG for the partial models.

Table 6 shows the drop in average DCG received by our model without a specified component. We note that a com-ponent can affect the model performance in several ways. E.g. if the language analysis component is turned off, then several non-English documents would be retrieved at the cost of relevant English documents. These documents could reduce the score of top experts, thereby reducing their ranks and giving prominence to sub-par authors. We observe that the document sensing is in particular the most effective com-ponent of the model, without which the model would retrieve authors with 17% less ratings. We note that document sens-ing, question classification and document sensing contribute substantially towards improving the model performance.
This paper presents a framework for expertise finding that aggregates content from several data sources. We introduced several novel components pivotal for building an effective expertise search engine, such as language classifier, question classifier, concept filtering scheme, and expertise rank ag-gregation. Our experiments show improved performance in comparison to models that ignores these aspects.

