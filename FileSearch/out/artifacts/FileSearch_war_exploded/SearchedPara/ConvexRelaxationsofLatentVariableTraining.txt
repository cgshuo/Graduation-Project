 Few algorithms are better kno wn in machine learning and statistics than expectation-maximization (EM) [5]. One reason is that EM solv es a common problem X learning from incomplete data X that (mar ginal observ ed or conditional hidden lik elihood). Only one problem is due to the algorithm quickly in man y circumstances [12, 13]. The main problems attrib uted to EM are not problems nential family). Non-con vexity immediately creates the risk of local minima, which unfortunately still unable to infer useful syntactic models of natural language from raw unlabeled text. ation of EM cannot succeed, we subsequently sho w that the problem can be overcome by working problem, the y do not immediately yield a con vex EM formulation, because the underlying estima-tion principles for directed graphical models have not been formulated in these terms. Our main useful con vex relaxation of EM becomes possible. 1.1 EM Variants where x refers to the observ ed part of the data and y refers to the unobserv ed part; and let w missing variables and optimizing the parameters of the model (conditional EM update) y ( k +1) = arg max the more common form of EM X contrib uting the very name expectation-maximization X is given by (mar ginal EM update) q ( k +1) where q max q result, it tends to nd better local maxima. Mar ginal EM has subsequently become the dominant thus these iterations are only guaranteed to nd local maxima.
 vated. In fact, for most applications we are more interested in acquiring an accurate conditional EM pose nearly identical optimization problems: (joint EM objecti ve) arg max (mar g. EM objecti ve) arg max where q variable case and assume a x ed set of random variables Y set of variables X missing value case howe ver. posed by joint EM the challenge is deeper than this. In the hidden variable case X when the same variables are miss-ing in each observ ation X there is a complete symmetry between the missing values. In particular , devastating consequences for any con vex relaxation: Assume one attempts to use any jointly con-vex relaxation f ( q assignment y has been relax ed into a continuous probabilistic assignment q the global minimum of f , ( q Proof: Assume ( q permutation of the missing values, , such that the alternati ve ( q 0 q y 6 = q y con vexity of f , we then have f ( q f ( q y ; w ) , for 0 &lt; &lt; 1 , contradicting the global optimality of f ( q y ; w ) . Therefore, any con vex relaxation of (1) that uses a distrib ution q vexied. (Note that standard coordinate descent algorithms simply break the symmetry arbitrarily and descend into some local solution.) This negative result seems to imply that no useful con vex directed models X that includes mixture models and discrete Bayesian netw orks as special cases. A directed model denes a joint probability distrib ution over a set of random variables Z conditional distrib utions P ( z j w ) = Q n family representation for the conditional distrib utions and of the child-parent conguration indicator feature for each local conguration ( a; b ) .
 poses into an independent sum of local loglik elihoods Thus the problem of solving for a maximum lik elihood set of parameters, given complete training Z . To simplify notation, consider one of the log-linear regression problems in (2) and drop the subscript j . Then, using a matrix notation we can rewrite the j th local optimization problem as examples, v is the number of possible values for the child variable, c is the number of possible Y and are indicator matrices that have a single 1 in each row, where Y indicates the value of the presentation belo w.) We also use the notation log normalization factor is given by A ( W; vector with a single 1 in position a .
 posteriori (MAP) form of the problem specic variable values in specic examples X which will lead to a trivial outcome if we attempt dependence and replace it with a dependence only on equi valence relationships. The rst step in reformulating (3) in terms of equi valence relations is to deri ve its dual. Lemma 2 An equivalent optimization problem to (3) is the Fenchel conjug ate of A it can be sho wn that (3) is equi valent to max min Finally , the inner minimization can be solv ed by setting W = 1 &gt; ( Y ) , yielding (4). kernel matrix is in fact an equi valence relation between parent congurations: is a 0-1 indicator matrix with a single 1 in each row, implying that K each of the parent variables. Let Y p 2 f 0 ; 1 g t v p indicate the value of a parent variable Z training example. That is, Y p variable Z to variable Z equi valence relation over complete parent congurations, K = &gt; , is equal to the component-K = &gt; = M 1 M 2 M p , since K ij = 1 iff M 1 ij = 1 and M 2 ij = 1 and ... M p ij = 1 . that 0 , 1 = 1 , and Y = . (Note that 2 IR t v , for v &lt; t , and therefore is lar ger than v any , some must exist that achie ves Y = . Then we can relate the primal parameters to this the y can only express the same realizable set of parameters W .) To simplify notation, let B = I then it is possible to sho w that an equi valent optimization problem to (3) is given by where K = &gt; and M = Y Y &gt; are equi valence relations on the parent congurations and child values respecti vely . The formulation (5) is now almost completely expressed in terms of equi valence relations over the data, except for one subtle problem: the log normalization factor equi valence relation matrix M alone.
 Lemma 3 A ( B; consists of columns copied from Y . That is, for all j , M child value in example j . Let y ( j ) denote the child value in example j and let P a exp Using Lemma 3 one can sho w that the dual problem to (5) is given by the follo wing. Theor em 1 An equivalent optimization problem to (3) is wher e K = M 1 M p for par ent variables Z Proof: This follo ws the same deri vation as Lemma 2, modied by taking into account the extra (5) is equi valent to max Finally , the inner minimization on B can be solv ed by setting B = I , yielding (6). EM variants for directed models. In particular , by exploiting Theorem 1, we can now re-e xpress hidden variable values min = min where h ranges over the hidden variables, and K j = M j 1 M j p for the parent variables Z introduced. Another nice property of the objecti ve in (8) is that is it concave in each in each M h indi vidually (a maximum of con vex functions is con vex [2]). Therefore, (8) appears as though it might admit an efcient algorithmic solution. Ho we ver, one dif culty in solving the con vex, there is a natural con vex relaxation suggested by the follo wing.
 and dropping the noncon vex rank constraint, yielding in terms of equi valence relations, and do not depend on the specic values of hidden variables in any way, this formulation is not subject to the triviality result of Lemma 1.
 a con vex EM training algorithm for various applications, such as for mixture models for example (see the note regarding continuous random variables belo w). Second, if there are multiple hidden are connected in any way, either by sharing a parent-child relationship or having a common child, are multiple hidden parents Z M p 1 M p ` is a Hadamard product of the indi vidual matrices. A con vex formulation can be the set of linear constraints ~ M to approximate the componentwise 'and'. A similar relaxation can also be applied when a child is hidden concurrently with hidden parent variables.
 Continuous Variables The formulation in (8) can be applied to directed models with continuous random variables, pro vided that all hidden variables remain discrete. If every continuous random the situation where there are continuous hidden variables.
 Reco vering the Model Parameters Once the relax ed equi valence relation matrices f M h g have been obtained, the parameters of the y underlying probability model need to be reco vered. At an Table 1: Results on synthetic and real-w orld Bayesian netw orks: average loss standard deviation f the relationship W can be immediately reco vered. For hidden variables, we rst need to compute a rank v of
M h . Let V = U 1 = 2 where U and are the top v h eigen vector and eigen value matrices of the centered matrix HM h H , such that H = I 1 k-means on the rows of V and construct the indicator matrix. A more ele gant approach would be to use a randomized rounding scheme [6], which also produces a deterministic ^ Y guarantees about how well ^ Y of Y h where the row vectors have been re-centered on the origin in a rotated coordinate system. mean back to the simple x center and rotation the coordinates back into the positi ve orthant. data. Our experiments are conducted using both synthetic Bayesian netw orks and real netw orks, while measuring the trained models by their logloss produced on the fully observ ed training data we used 10 random restarts for Viterbi EM to help avoid poor local optima.
 For the synthetic experiments, we constructed three Bayesian netw orks: (1) Bayesian netw ork 1 (Synth1) is a three layer netw ork with 9 variables, where the two nodes in the middle layer are 6 edges, where a node with 2 parents and 2 children is pick ed as hidden variable; (3) Bayesian the hidden variable. The parameters are generated in a discriminati ve way to produce models with apparent causal relations between the connected nodes. We performed experiments on these three synthetic netw orks using varying training sizes: 50, 100 and 150. Due to space limits, we only we also ran experiments using real UCI data, where we used Nai ve Bayes as the model structure, UCI data sets.
 Here we can see that the con vex relaxation was successful at preserving structure in the EM ob-in the case (Synth1) where there was two hidden variables. Not surprisingly , supervised training on the complete data performed better than the EM methods, but generally demonstrated a lar ger gap between training and test losses than the EM methods. Similar results were obtained for both synthetic netw orks, sho wing good results again for the con vex EM relaxation. and Asian (do wnloaded from http://www .norsys.com/netw orklibrary .html). We pick ed one well connected node from each model to serv e as the hidden variable, and generated data by sampling from the models. Table 1 sho ws the experimental results for these three Bayesian netw orks. Here we can see that the con vex EM relaxation performed well on the Cancer and Alarm netw orks. Since we only pick ed one hidden variable from the 37 variables in Alarm, it is understandable that any experimental comparisons to a standard joint EM algorithm (Viterbi EM), on both synthetic and the symmetry breaking problem that blocks nai ve con vexication strate gies from working. One obtained, and this too is the subject of further investig ation.
 [1] J. Borwein and A. Le wis. Con vex Analysis and Nonlinear Optimization . Springer , 2000. [2] S. Bo yd and L. Vandenber ghe. Con vex Optimization . Cambridge U. Press, 2004. [3] S. Chen. Models for grapheme-to-phoneme con version. In Eur ospeec h , 2003. [4] T. De Bie and N. Cristianini. Fast SDP relaxations of graph cut clustering, transduction, and [5] A. Dempster , N. Laird, and D. Rubin. Maximum lik elihood from incomplete data via the EM [6] M. Goemans and D. Williamson. Impro ved approximation algorithms for maximum cut and [7] S. Goldw ater and M. Johnson. Bias in learning syllable structure. In Proc. CONLL , 2005. [8] D. Klein and C. Manning. Corpus-based induction of syntactic structure: Models of depen-[9] B. Merialdo. Tagging text with a probabilistic model. Comput. Ling . , 20(2):155 X 171, 1994. [11] J. Nocedal and S. Wright. Numerical Optimization . Springer , 1999. [12] R. Salakhutdino v, S. Ro weis, and Z. Ghahramani. Optimization with EM and expectation-[14] M. Wainwright and M. Jordan. Graphical models, exponential families, and variational infer -[15] L. Xu, J. Neufeld, B. Larson, and D. Schuurmans. Max mar gin clustering. In NIPS 17 , 2004.
