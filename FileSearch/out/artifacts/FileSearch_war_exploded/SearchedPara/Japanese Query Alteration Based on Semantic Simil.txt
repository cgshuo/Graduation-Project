 Web search query correction is an important prob-lem to solve for robust information retrieval given how pervasive errors are in search queries: it is said that more than 10% of web search queries contain errors (Cucerzan and Brill, 2004). English query correction has been an area of active research in re-cent years, building on previous work on general-purpose spelling correction. However, there has been little investigation of query correction in lan-guages other than English.

In this paper, we address the issue of query cor-rection, and more generally, query alteration in Japanese. Japanese poses particular challenges to the query correction task due to its complex writ-ing system, summarized in Fig. 1 1 . There are four main character types, including two types of kana (phonetic alphabet -hiragana and katakana ), kanji (ideographic -characters represent meaning) and Roman alphabet; a word can be legitimately spelled in multiple ways, combining any of these character sets. For example, the word for  X  X rotein X  can be spelled as  X  X  X  X  X  X  X  X  X  (all in hiragana),  X  X  X  X  X  X   X  (katakana+kanji),  X  X  X  X  (all in kanji) or  X  X  X  X  (hiragana+kanji), all pronounced in the same way ( tanpakushitsu ). Some examples of these spelling variants are shown in Fig. 1 with the prefix Sp: as is observed from the figure, spelling variation occurs within and across different character types. Resolv-ing these variants will be essential not only for in-formation retrieval but practically for all NLP tasks.
A particularly prolific source of spelling varia-tions in Japanese is katakana. Katakana charac-ters are used to transliterate words from English and other foreign languages, and as such, the variations in the source language pronunciation as well as the ambiguity in sound adaptation are reflected in the katakana spelling. For example, Masuyama et al. (2004) report that at least six distinct translitera-tions of the word  X  X paghetti X  (  X  X  X  X  X  X  X  X  X  ,  X  X  X  X   X  X  X  X  , etc. ) are attested in the newspaper corpus they studied. Normalizing katakana spelling varia-tions has been the subject of research by itself (Ara-maki et al., 2008; Masuyama et al., 2004). Similarly, English-to-katakana transliteration (e.g.,  X  X edex X  as  X  X  X  X  X  X  X  X  fedekkusu in Fig. 1) and katakana-to-English back-transliteration (e.g.,  X  X  X  X  X  X  X  X  back into  X  X edex X ) have also been studied extensively (Bi-lac and Tanaka, 2004; Brill et al., 2001; Knight and Graehl, 1998), as it is an essential component for machine translation. To our knowledge, however, there has been no work that addresses spelling vari-ation in Japanese generally.

In this paper, we propose a general approach to query correction/alteration in Japanese. Our goal is to find precise re-write candidates for a query, be it a correction of a spelling error, normalization of a spelling variant, or finding a strict synonym in-cluding abbreviations (e.g., MS  X  X  X  X  X  X  X  X  X   X  X icrosoft X , prefixed by Abbr in Fig. 1) and true synonyms (e.g.,  X  X  X  (translation of  X  X eat X )  X  X  X  X  (transliteration of  X  X eat X , indicated by Syn in Fig. 1) 2 Our method is based on previous work on English query correction in that we use both spelling and se-mantic similarity between a query and its alteration candidate, but is more general in that we include al-teration candidates that are not similar to the original query in spelling. In computing semantic similar-ity, we adopt a kernel-based method (Kandola et al., 2002), which improves the accuracy of the query al-teration results over previously proposed methods. We also introduce a novel approach to creating a dataset of query and alteration candidate pairs effi-ciently and reliably from query session logs. The key difference between traditional general-purpose spelling correction and search query cor-rection lies in the fact that the latter cannot rely on a lexicon: web queries are replete with valid out-of-dictionary words which are not mis-spellings of in-vocabulary words. Cucerzan and Brill (2004) pi-oneered the research of query spelling correction, with an excellent description of how a traditional dictionary-based speller had to be adapted to solve the realistic query correction problem. The model they proposed is a source-channel model, where the source model is a word bigram model trained on query logs, and the channel model is based on a weighted Damerau-Levenshtein edit distance. Brill and Moore (2000) proposed a general, improved source model for general spelling correction, while Ahmad and Kondrak (2005) learned a spelling error model from search query logs using the Expectation Maximization algorithm, without relying on a train-ing set of misspelled words and their corrections. Extending the work of Cucerzan and Brill (2004), Li et al. (2006) proposed to include semantic sim-ilarity between the query and its correction candi-date. They point out that adventura is a common misspelling of aventura , not adventure , and this can-not be captured by a simple string edit distance, but requires some knowledge of distributional similar-ity. Distributional similarity is measured by the sim-ilarity of the context shared by two terms, and has been successfully applied to many natural language processing tasks, including semantic knowledge ac-quisition (Lin, 1998).

Though the use of distributional similarity im-proved the query correction results in Li et al. X  X  work, one problem is that it is sparse and is not avail-able for many rarer query strings. Chen et al. (2007) addressed this problem by using external informa-tion (i.e., web search results); we take a different ap-proach to solve the sparseness problem, namely by using semantic kernels.

Jones et al. (2006a) generated Japanese query al-teration pairs from by mining query logs and built a regression model which predicts the quality of query rewriting pairs. Their model includes a wide variety of orthographical features, but not semantic similar-ity features. 3.1 Problem Formulation We employ a formulation of query alteration model that is similar to conventional query correction mod-els. Given a query string q as input, a query correc-tion model finds a correct alteration c  X  within the confusion set of q , so that it maximizes the posterior probability: where C is the set of all white-space separated words and their bigrams in query logs in our case 3 , and CF( q )  X  C is the confusion set of q , consisting of the candidates within a certain edit distance from q , i.e., CF( q ) = { c  X  C | ED( q, c ) &lt;  X  } . We set  X  = 24 using an unnormalized edit distance. The detail of the edit distance ED( q, c ) is described in Section 3.2. The query string q itself is contained in CF( q ) , and if the model output is different from q , it means the model suggests a query alteration. Formulated in this way, both query error detection and alteration are performed in a unified way.

After computing the posterior probability of each candidate in CF( q ) by the source channel model (Section 3.2), an N-best list is obtained as the ini-tial candidate set C 0 , which is then augmented by the bootstrapping method Tchai (Section 3.4) to cre-ate the final candidate list C ( q ) . The candidates in C ( q ) are re-ranked by a maximum entropy model (Section 3.5) and the candidate with the highest pos-terior probability is selected as the output. 3.2 Source Channel Model Source channel models are widely used for spelling and query correction (Brill and Moore, 2000; Cucerzan and Brill, 2004). Instead of directly com-puting Eq. (1), we can decompose the posterior probability using Bayes X  rule as: where the source model P ( c ) measures how proba-ble the candidate c is, while the error model P ( q | c ) measures how similar q and c are.

For the source model, an n -gram based statisti-cal language model is the standard in previous work (Ahmad and Kondrak, 2005; Li et al., 2006). Word n -gram models are simple to create for English, which is easy to tokenize and to obtain word-based statistics, but this is not the case with Japanese. Therefore, we simply considered the whole input string as a candidate to be altered, and used the rel-ative frequency of candidates in the query logs to build the language model:
For the error model, we used an improved chan-nel model described in (Brill and Moore, 2000), which we call the alpha-beta model in this paper. The model is a weighted extension of the normal Damerau-Levenshtein edit distance which equally penalizes single character insertion, substitution, or deletion operations (Damerau, 1964; Levenshtein, 1966), and considers generic edit operations of the form  X   X   X  , where  X  and  X  are any (possibly null) strings. From misspelled/correct word pairs, alpha-beta trains the probability P (  X   X   X  | PSN) , conditioned by the position PSN of  X  in a word, where PSN  X  X  start of word, middle of word, end of word } . Under this model, the probability of rewrit-ing a string w to a string s is calculated as: which corresponds to finding best partitions R and T in all possible partitions Part( w ) and Part( s ) . Brill and Moore (2000) reported that this model gave a significant improvement over conventional edit dis-tance methods.

Brill et al. (2001) applied this model for ex-tracting katakana-English transliteration pairs from query logs. They trained the edit distance between character chunks of katakana and Roman alphabets, after converting katakana strings to Roman script. We also trained this model using 59,754 katakana-English pairs extracted from aligned Japanese and English Wikipedia article titles. In this paper we al-lowed |  X  | , |  X  | X  3 . The resulting edit distance is obtained as the negative logarithm of the alpha-beta probability, i.e., ED  X   X  ( q | c ) =  X  log P  X  X  ( q | c ) .
Since the edit operations are directional and c and q can be any string consisting of katakana and En-glish, distance in both directions were considered. We also included a modified edit distance ED hd for simple kana-kana variations after converting them into Roman script. The distance ED hd is essen-tially the same as the normal Damerau-Levenshtein edit distance, with the modification that it does not penalize character halving ( aa  X  a ) and doubling ( a  X  aa ), because a large part of katakana vari-ants only differ in halving/doubling (e.g.  X  X  X  X  X  X  X 
The final error probability is obtained from the minimum of these three distances: where every edit distance is normalized to [0, 1] by multiplying by a factor of 2 / ( | q || c | ) so that it does not depend on the length of the input strings 5 . 3.3 Kernel-based Lexical Semantic Similarity 3.3.1 Distributional Similarity
The source channel model described in Sec-tion 3.2 only considers language and error models and cannot capture semantic similarity between the query and its correction candidate. To address this issue, we use distributional similarity (Lin, 1998) es-timated from query logs as additional evidence for query alteration, following Li et al. (2006).
For English, it is relatively easy to define the con-text of a word based on the bag-of-words model. As this is not expected to work on Japanese, we de-fine context as everything but the query string in a query log, as Pas  X ca et al. (2006) and Komachi and Suzuki (2008) did for their information extraction tasks. This formulation does not involve any seg-mentation or boundary detection, which makes this method fast and robust. On the other hand, this may cause additional sparseness in the vector representa-tion; we address this issue in the next two sections.
Once the context of a candidate c i is de-fined as the patterns that the candidate co-occurs with, it can be represented as a vector c i = [pmi( c the number of context patterns and x 0 is the trans-position of a vector (or possibly a matrix) x . The el-ements of the vector are given by pointwise mutual information between the candidate c i and the pattern p , computed as: where | c i , p j | is the frequency of the pattern p j in-stantiated with the candidate c i , and  X * X  denotes a wildcard, i.e., | c i ,  X  X  =  X  c | c, p j | similarity can be calculated as cosine similarity. Let  X c be the L2-normalized pattern vector of the candi-date c i , and X = {  X c i } be the candidate-pattern co-occurrence matrix. The candidate similarity matrix K can then be obtained as K = X 0 X . In the follow-ing, the ( i, j ) -element of the matrix K is denoted as K ij , which corresponds to the cosine similarity be-tween candidates c i and c j . 3.3.2 Semantic Kernels
Although distributional similarity serves as strong evidence for semantically relevant candidates, di-rectly applying the technique to query logs faces the sparseness problem. Because context patterns are drawn from query logs and can also contain spelling errors, alterations, and word permutations as much as queries do, context differs so greatly in represen-tations that even related candidates might not have sufficient contextual overlap between them. For example, a candidate  X  X ouTube X  matched against the patterns  X  X ouTube+movie X ,  X  X ovie+YouTube X  and  X  X ou-Tube+movii X  (with a minor spelling er-ror) will yield three distinct patterns  X #+movie X ,  X  X ovie+# X  and  X #+movii X  6 , which will be treated as three separate dimensions in the vector space model.
This sparseness problem can be partially ad-dressed by considering the correlation between pat-terns. Kandola et al. (2002) proposed new kernel-based similarity methods which incorporate indirect similarity between terms for a text retrieval task. Al-though their kernels are built on a document-term co-occurrence model, they can also be applied to our candidate-pattern co-occurrence model. The pro-posed kernel is recursively defined as:
K =  X X 0  X  GX + K,  X  G =  X X  X  KX 0 + G, (7) patterns and  X  is the factor to ensure that longer range effects decay exponentially. This can be in-terpreted as augmenting the similarity matrix K through indirect similarities of patterns  X  G and vice versa. Semantically related pairs of patterns are ex-pected to be given high correlation in the matrix  X  G and this will alleviate the sparseness problem. By solving the above recursive definition, one obtains the von Neumann kernel :
This can also be interpreted in terms of a random walk in a graph where the nodes correspond to all the candidates and the weight of an edge ( i, j ) is given by K ij . A simple calculation shows that K ij equals the sum of the products of the edge weights over all possible paths between the nodes corresponding c i and c j in the graph. Also, K t probability that a random walk beginning at node c i ends up at node c j after t steps, assuming that the en-tries are all positive and the sum of the connections is 1 at each node. Following this notion, Kandola et al. (2002) proposed another kernel called expo-nential kernel , with alternative faster decay factors: They showed that this alternative kernel achieved a better performance for their text retrieval task. We employed these two kernels to compute distribu-tional similarity for our query correction task. 3.3.3 Orthographically Augmented Kernels
Although semantic relatedness can be partially captured by the semantic kernels introduced in the previous section, they may still have difficulties computing correlations between candidates and pat-terns especially for only sparsely connected graphs. Take the graph (a) in Fig. 2 for example, which is a simplified yet representative graph topology for candidate-pattern co-occurrence we often encounter. In this case K = X 0 X equals I , meaning that the connections between candidates and patterns are too sparse to obtain sufficient correlation even when se-mantic kernels are used.

In order to address this issue, we propose to aug-ment the graph by weakly connecting the candidate and pattern nodes as shown in the graph (b) of Fig. 2 based on prior knowledge of orthographic similarity about candidates and patterns. This can be achieved using the following candidate similarity matrix K + instead of K :
K + =  X S C + (1  X   X  ) X 0 [  X S P + (1  X   X  ) I ] X (10) where S C = { s c ( i, j ) } is the orthographical similar-ity matrix of candidates in which the ( i, j ) -element is given by the edit distance based similarity, i.e., s ( i, j ) = exp [  X  ED( c similarity matrix of patterns S P = { s P ( i, j ) } is de-fined similarly, i.e., s P ( i, j ) = exp[  X  ED( p i , p j Note that using this similarity matrix K + can be interpreted as a random walk process on a bipar-tite graph as follows. Let C and P as the sets of candidates and patterns. K + corresponds to a sin-gle walking step from C to C , by either remaining within C with a probability of  X  or moving to  X  X he other side X  P of the graph with a probability of 1  X   X  . When the walking remains in C , it is allowed to move to another candidate node following the candi-date orthographic similarity S C . Otherwise it moves to P by the matrix X , chooses either to move within P with a probability  X S P or to stay with a probabil-ity 1  X   X  , and finally comes back to C by the matrix X this process t times. Using this similarity, we can de-fine two orthographically augmented semantic ker-nels which differ in the decaying factors, augmented von Neumann kernel and exponential kernel: 3.4 Bootstrapping Additional Candidates Now that we have a semantic model, our query correction model can cover query-candidate pairs which are only semantically related. However, pre-vious work on query correction all used a string dis-tance function and a threshold to restrict the space of potential candidates, allowing only the orthographi-cally similar candidates.

To collect additional candidates, the use of context-based semantic extraction methods would be effective because semantically related candidates are likely to share context with the initial query q , or at least with the initial candidate set C 0 . Here we used the Tchai algorithm (Komachi and Suzuki, 2008), a modified version of Espresso (Pan-tel and Pennacchiotti, 2006) to collect such candi-dates. This algorithm starts with initial seed in-stances, then induces reliable context patterns co-occurring with the seeds, induces instances from the patterns, and iterates this process to obtain cat-egories of semantically related words. Using the candidates in C 0 as the seed instances, one boot-strapping iteration of the Tchai algorithm is executed to obtain the semantically related set of instances C 1 . The seed instance reliabilities are given by the source channel probabilities P ( c ) P ( q | c ) . Finally we take the union C 0  X  C 1 to obtain the candidate set C ( q ) . This process is outlined in Fig. 3. 3.5 Maximum Entropy Model In order to build a unified probabilistic query al-teration model, we used the maximum entropy ap-proach of (Beger et al., 1996), which Li et al. (2006) also employed for their query correction task and showed its effectiveness. It defines a conditional probabilistic distribution P ( c | q ) based on a set of feature functions f 1 , . . . , f K : where  X  1 , . . . ,  X  K are the feature weights. The op-timal set of feature weights  X   X  can be computed by maximizing the log-likelihood of the training set.
We used the Generalized Iterative Scaling (GIS) algorithm (Darroch and Ratcliff, 1972) to optimize the feature weights. GIS trains conditional proba-bility in Eq. (13), which requires the normalization over all possible candidates. However, the number of all possible candidates C obtained from a query log can be very large, so we only calculated the sum over the candidates in C ( q ) . This is the same ap-proach that Och and Ney (2002) took for statistical machine translation, and Li et al. (2006) for query spelling correction.

We used the following four categories of func-tions as the features: 1. Language model feature , given by the logarithm of the source model probability: log P ( c ) . 2. Error model features , which are composed of three edit distance functions:  X  ED  X  X  ( q | c ) ,  X  ED  X  X  ( c | q ) , and  X  ED hd ( q, c ) . 3. Similarity based feature , computed as the loga-rithm of distributional similarity between q and c : log sim( q, c ) , which is calcualted using one of the following kernels (Section 3.3): K,  X  K,  X  K,  X  K + , to [0, 1] after adding a small discounting factor  X  = 1 . 0  X  10  X  5 . 4. Similarity based correction candidate features , which are binary features with a value of 1 if and only if the frequency of c is higher than that of q , and distributional similarity between them is higher than a certain threshold. Li et al. (2006) used this set of features, and suggested that these features give the evidence that q may be a com-mon misspelling of c . The thresholds on the nor-malized distributional similarity are enumerated from 0.5 to 0.9 with the interval 0.1. 4.1 Dataset Creation For all the experiments conducted in this paper, we used a subset of the Japanese search query logs sub-mitted to Live Search (www.live.com) in November and December of 2007. Queries submitted less than eight times were deleted. The query log we used contained 83,080,257 tokens and 1,038,499 unique queries.

Models of query correction in previous work were trained and evaluated using manually created query-candidate pairs. That is, human annotators were given a set of queries and were asked to provide a correction for each query when it needed to be re-written. As Cucerzan and Brill (2004) point out, however, this method is seriously flawed in that the intention of the original query is completely lost to the annotator, without which the correction is often impossible: it is not clear if gogle should be cor-rected to google or goggle , or neither  X  gogle may be a brand new product name. Cucerzan and Brill therefore performed a second evaluation, where the test data was drawn by sampling the query logs for successive queries ( q 1 , q 2 ) by the same user where the edit distance between q 1 and q 2 are within a cer-tain threshold, which are then submitted to annota-tors for generating the correction. While this method makes the annotation more reliable by relying on user (rather than annotator) reformulation, the task is still overly difficult: going back to the example in Section 1, it is unclear which spelling of  X  X rotein X  produces the best search results  X  it can only be em-pirically determined. Their method also eliminates all pairs of candidates that are not orthographically similar. We have therefore improved their method in the following manner, making the process more automated and thus more reliable.

We first collected a subset of the query log that contains only those pairs ( q 1 , q 2 ) that are issued suc-cessively by the same user, q 2 is issued within 3 min-utes of q 1 , and q 2 resulted in a click of the resulting page while q 1 did not. The last condition adds the evidence that q 2 was a better formulation than q 1 . We then ranked the collected query pairs using log-likelihood ratio (LLR) (Dunning, 1993), which mea-sures the dependence between q 1 and q 2 within the context of web queries (Jones et al., 2006b). We ran-domly sampled 10,000 query pairs with LLR  X  200 , and submitted them to annotators, who only confirm or reject a query pair as being synonymous. For ex-ample, q 1 = nikon and q 2 = canon are related but not synonymous, while we are reasonably sure q 1 = ipot and q 2 = ipod are synonymous, given that this pair has a high LLR value. This verification process is extremely fast and consistent across annotators: it takes less than 1 hour to go through 1,000 query pairs, and the inter-annotator agreement rate of two annotators on 2,000 query pairs was 95.7%. We annotated 10,000 query pairs consisting of alpha-numerical and kana characters in this manner. After rejecting non-synonymous pairs and those which do not co-occur with any context patterns, 6,489 pairs remained, and we used 1,243 pairs for testing, 628 as a development set, and 4,618 for training the max-imum entropy model. 4.2 Experimental Settings The performance of query alteration was evaluated based on the following measures (Li et al., 2006). The input queries, correct suggestions, and outputs were matched in a case-insensitive manner.  X 
Accuracy : The number of correct outputs gener-ated by the system divided by the total number of queries in the test set;  X 
Recall : The number of correct suggestions for al-tered queries divided by the total number of al-tered queries in the test set;  X 
Precision : The number of correct suggestions for altered queries divided by the total number of al-terations made by the system.

The parameters for the kernels, namely,  X  ,  X  , and  X  , are tuned using the development set. The finally employed values are:  X  = 0 . 3 for  X  K ,  X  K , and  X  K + ,  X  = 0 . 2 for  X  K + ,  X  = 0 . 2 and  X  = 0 . 4 for  X  K + , and  X  = 0 . 35 and  X  = 0 . 7 for  X  K + . In the source channel model, we manually scaled the language probability by a factor of 0.1 to alleviate the bias toward highly frequent candidates.

As the initial candidate set C 0 , top-50 instances were selected by the source channel model, and 100 patterns were extracted as P 0 by the Tchai iteration after removing generic patterns, which we detected simply by rejecting those which induced more than 200 unique instances. Finally top-30 instances were induced using P 0 to create C 1 . Generic instances were not removed in this process because they may still be alterations of input query q . The maximum size of P 1 was set to 2,000, after removing unreliable patterns with reliability smaller than 0.0001. 4.3 Results Table 1 shows the evaluation results. SC is the source channel model, while the others are maxi-mum entropy (ME) models with different features. ME-NoSim uses the same features as SC, but con-siderably outperforms SC in all three measures, con-firming the superiority of the ME approach. Decom-posing the three edit distance functions into three separate features in the ME model may also explain the better result. All the ME approaches outper-formed SC in accuracy with a statistically significant difference ( p &lt; 0 . 0001 on McNemar X  X  test).
The model with the cosine similarity (ME-Cos) in addition to the basic set of features yielded higher recall compared to ME-NoSim, but decreased accu-racy and precision, which are more important than recall for our purposes because a false alteration does more damage than no alteration. This is also the case when the kernel-based methods, ME-vN (the von Neumann kernel) and ME-Exp (the expo-nential kernel), are used in place of the cosine sim-ilarity. This shows that using semantic similarity does not always help, which we believe is due to the sparseness of the contextual information used in computing semantic similarity.
 On the other hand, ME-vN+ (with augmented von Neumann kernel) and ME-Exp+ (with augmented exponential kernel) increased both accuracy and pre-cision with a slight decrease of recall, compared to the distributional similarity baseline and the non-augmented kernel-based methods. ME-Exp+ was significantly better than ME-Exp ( p &lt; 0 . 01 ).
Note that the accuracy values appear lower than some of the previous results on English (e.g., more than 80% in (Li et al., 2006)), but this is because the dataset creation method we employed tends to over-represent the pairs that lead to alteration: the simplest baseline (= always propose no alteration) performs 67.3% accuracy on our data, in contrast to 83.4% on the data used in (Li et al., 2006).
Manually examining the suggestions made by the system also confirms the effectiveness of our model. For example, the similarity-based models altered the query ipot to ipod , while the simple ME-NoSim model failed, because it depends too much on the edit distance-based features. We also observed that many of the suggestions made by the system were actually reasonable, even though they were differ-ent from the annotated gold standard. For example, ME-vN+ suggests a re-write of the query 2tyann as 2  X  X  X  X  X  X  X  ( X 2-channel X ), while the gold standard was an abbreviated form 2  X  X  X  X  ( X 2-chan X ).

To incorporate such possibly correct candidates into account, we conducted a follow-up experiment where we considered multiple reference alterations, created automatically from our data set in the fol-lowing manner. Suppose that a query q 1 is corrected as q 2 , and q 2 is corrected as q 3 in our annotated data. If this is the case, we considered q 1  X  q 3 as a valid alteration as well. By applying this chaining oper-ation up to 5 degrees of separation, we re-created a set of valid alterations for each input query. Note that directionality is important  X  in the above ex-ample, q 1  X  q 3 is valid, while q 3  X  q 1 is not. Table 2 shows the results of evaluation with multiple refer-ences. The numbers substantially improved over the single reference cases, as expected, but did not af-fect the relative performance of each model. Again, the differences in accuracy between the SC and ME models, and ME-Exp and ME-Exp+ were statisti-cally significant ( p &lt; 0 . 01 ). In this paper we have presented a unified approach to Japanese query alteration. Our approach draws on previous research in English spelling and query correction, Japanese katakana variation and translit-eration, and semantic similarity, and builds a model that makes improvements over previously proposed query correction methods. In particular, the use of orthographically augmented semantic kernels pro-posed in this paper is general and applicable to other languages, including English, for query alteration, especially when the data sparseness is an issue. In the future, we also plan to investigate other meth-ods, such as PLSI (Hofmann, 1999), to deal with data sparseness in computing semantic similarity. This research was conducted during the first au-thor X  X  internship at Micorosoft Research. We thank the colleagues, especially Dmitriy Belenko, Chris Brockett, Jianfeng Gao, Christian K  X  onig, and Chris Quirk for their help in conducting this research.
