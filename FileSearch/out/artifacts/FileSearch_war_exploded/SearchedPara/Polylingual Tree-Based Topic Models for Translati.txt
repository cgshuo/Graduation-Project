 Probabilistic topic models (Blei and Lafferty, 2009), exemplified by latent Dirichlet alloca-tion (Blei et al., 2003, LDA ), are one of the most popular statistical frameworks for navigating large unannotated document collections. Topic models discover X  X ithout any supervision X  X he primary themes presented in a dataset: the namesake topics.
Topic models have two primary applications: to aid human exploration of corpora (Chang et al., 2009) or serve as a low-dimensional representa-tion for downstream applications. We focus on the second application, which has been fruitful for computer vision (Li Fei-Fei and Perona, 2005), computational biology (Perina et al., 2010), and information retrieval (Kataria et al., 2011).
In particular, we use topic models to aid statisti-cal machine translation (Koehn, 2009, SMT ). Mod-ern machine translation systems use millions of examples of translations to learn translation rules. These systems work best when the training corpus has consistent genre, register, and topic. Systems that are robust to systematic variation in the train-ing set are said to exhibit domain adaptation .
As we review in Section 2, topic models are a promising solution for automatically discover-ing domains in machine translation corpora. How-ever, past work either relies solely on monolingual source-side models (Eidelman et al., 2012; Hasler et al., 2012; Su et al., 2012), or limited modeling of the target side (Xiao et al., 2012). In contrast, machine translation uses inherently multilingual data: an SMT system must translate a phrase or sen-tence from a source language to a different target language, so existing applications of topic mod-els (Eidelman et al., 2012) are wilfully ignoring available information on the target side that could aid domain discovery.

This is not for a lack of multilingual topic mod-els. Topic models bridge the chasm between lan-guages using document connections (Mimno et al., 2009), dictionaries (Boyd-Graber and Resnik, 2010), and word alignments (Zhao and Xing, 2006). In Section 2, we review these models for discover-ing topics in multilingual datasets and discuss how they can improve SMT .

However, no models combine multiple bridges between languages. In Section 3, we create a model X  X he polylingual tree-based topic models (pt
LDA ) X  X hat uses information from both external dictionaries and document alignments simultane-ously. In Section 4, we derive both MCMC and variational inference for this new topic model.
In Section 5, we evaluate our model on the task of
SMT using aligned datasets. We show that pt LDA offers better domain adaptation than other topic models for machine translation. Finally, in Sec-tion 6, we show how these topic models improve SMT with detailed examples. Before considering past approaches using topic models to improve SMT , we briefly review lexical weighting and domain adaptation for SMT . 2.1 Statistical Machine Translation Statistical machine translation casts machine trans-lation as a probabilistic process (Koehn, 2009). For a parallel corpus of aligned source and target sen-tences ( F , E ) , a phrase  X  f  X  F is translated to a phrase  X  e  X  E according to a distribution p w (  X  e |  X  One popular method to estimate the probability p w (  X  e | Lexical Weighting In phrase-based SMT , lexi-cal weighting features estimate the phrase pair quality by combining lexical translation probabil-ities of words in a phrase (Koehn et al., 2003). Lexical conditional probabilities p ( e | f ) are maxi-mum likelihood estimates from relative lexical fre-quencies c ( f,e )/ count of observing lexical pair ( f,e ) in the train-ing dataset. The phrase pair probabilities p w (  X  e |  X  are the normalized product of lexical probabili-ties of the aligned word pairs within that phrase pair (Koehn et al., 2003). In Section 2.2, we create topic-specific lexical weighting features.
 Cross-Domain SMT A SMT system is usu-ally trained on documents with the same genre (e.g., sports, business) from a similar style (e.g., newswire, blog-posts). These are called domains . Translations within one domain are better than translations across domains since they vary dra-matically in their word choices and style. A correct translation in one domain may be inappropriate in another domain. For example,  X   X   X   X  in a newspa-per usually means  X  X nderwater diving X . On social media, it means a non-contributing  X  X urker X . Domain Adaptation for SMT Training a SMT system using diverse data requires domain adap-tation . Early efforts focus on building separate models (Foster and Kuhn, 2007) and adding fea-tures (Matsoukas et al., 2009) to model domain information. Chiang et al. (2011) combine these approaches by directly optimizing genre and col-lection features by computing separate translation tables for each domain.

However, these approaches treat domains as hand-labeled, constant, and known a priori . This setup is at best expensive and at worst infeasible for large data. Topic models provide a solution where domains can be automatically induced from raw 2.2 Inducing Domains with Topic Models Topic models take the number of topics K and a collection of documents as input, where each docu-ment is a bag of words. They output two distribu-tions: a distribution over topics for each document d ; and a distribution over words for each topic. If each topic defines a SMT domain, the document X  X  topic distribution is a soft domain assignment for that document.

Given the soft domain assignments, Eidelman et al. (2012) extract lexical weighting features condi-tioned on the topics, optimizing feature weights us-ing the Margin Infused Relaxed Algorithm (Cram-mer et al., 2006, MIRA ). The topics come from source documents only and create topic-specific lexical weights from the per-document topic distri-bution p ( k | d ) . The lexical probability conditioned on the topic is expected count e k ( e,f ) of a word translation pair under topic k , where c d (  X  ) is the number of occurrences of the word pair in document d . The lexical probability conditioned on topic k is the unsmoothed probabil-ity estimate of those expected counts from which we can compute the phrase pair proba-abilities and normalizing as in Koehn et al. (2003).
For a test document d , the document topic dis-tribution p ( k | d ) is inferred based on the topics learned from training data. The feature value of a phrase pair (  X  e,  X  f ) is a combination of the topic dependent lexical weight and the topic distribution of the document, from which we extract the phrase. Eidelman et al. (2012) compute the resulting model score by combining these features in a linear model with other standard SMT features and optimizing the weights.

Conceptually, this approach is just reweighting examples. The probability of a topic given a docu-ment is never zero. Every translation observed in the training set will contribute to p k ( e | f ) ; many of the expected counts, however, will be less than one. This obviates the explicit smoothing used in other domain adaptation systems (Chiang et al., 2011).
We adopt this framework in its entirety. Our contribution are topics that capture multilingual information and thus better capture the domains in the parallel corpus. 2.3 Beyond Vanilla Topic Models Eidelman et al. (2012) ignore a wealth of infor-mation that could improve topic models and help machine translation. Namely, they only use mono-lingual data from the source language, ignoring all target-language data and available lexical semantic resources between source and target languages.
Different complement each other to reduce ambi-guity. For example,  X   X   X   X  in a Chinese document can be either  X  X obbyhorse X  in a children X  X  topic, or  X  X rojan virus X  in a technology topic. A short Chinese context obscures the true topic. However, these terms are unambiguous in English, revealing the true topic.

While vanilla topic models ( LDA ) can only be applied to monolingual data, there are a number of topic models for parallel corpora: Zhao and Xing (2006) assume aligned word pairs share same topics; Mimno et al. (2009) connect different lan-guages through comparable documents. These models take advantage of word or document align-ment information and infer more robust topics from the aligned dataset.

On the other hand, lexical information can in-duce topics from multilingual corpora. For in-stance, orthographic similarity connects words with the same meaning in related languages (Boyd-Graber and Blei, 2009), and dictionaries are a more general source of information on which words share meaning (Boyd-Graber and Resnik, 2010).
These two approaches are not mutually exclu-sive, however; they reveal different connections across languages. In the next section, we combine these two approaches into a polylingual tree-based topic model. In this section, we bring existing tree-based topic models (Boyd-Graber et al., 2007, t LDA ) and polylingual topic models (Mimno et al., 2009, p
LDA ) together and create the polylingual tree-based topic model (pt LDA ) that incorporates both word-level correlations and document-level align-ment information.
 Word-level Correlations Tree-based topic mod-els incorporate the correlations between words by encouraging words that appear together in a con-cept to have similar probabilities given a topic. These concepts can come from WordNet (Boyd-Graber and Resnik, 2010), domain experts (An-drzejewski et al., 2009), or user constrains (Hu et al., 2013). When we gather concepts from bilin-gual resources, these concepts can connect different languages. For example, if a bilingual dictionary defines  X   X   X   X  as  X  X omputer X , we combine these words in a concept.

We organize the vocabulary in a tree structure based on these concepts (Figure 1): words in the same concept share a common parent node, and then that concept becomes one of many children of the root node. Words that are not in any concept X  uncorrelated words  X  X re directly connected to the root node. We call this structure the tree prior .
When this tree serves as a prior for topic models, words in the same concept are correlated in topics. For example, if  X   X   X   X  has high probability in a topic, so will  X  X omputer X , since they share the same parent node. With the tree priors, each topic is no longer a distribution over word types, instead, it is a distribution over paths, and each path is associated with a word type. The same word could appear in multiple paths, and each path represents a unique sense of this word.
 Document-level Alignments Lexical resources connect languages and help guide the topics. How-ever, these resources are sometimes brittle and may not cover the whole vocabulary. Aligned document pairs provide a more corpus-specific, flexible asso-ciation across languages.

Polylingual topic models (Mimno et al., 2009) assume that the aligned documents in different lan-guages share the same topic distribution and each language has a unique topic distribution over its word types. This level of connection between lan-guages is flexible: instead of requiring the exact matching on words and sentences, only a coarse document alignment is necessary, as long as the documents discuss the same topics.
 Combine Words and Documents We propose polylingual tree-based topic models (pt LDA ), which connect information across different lan-guages by incorporating both word correlation (as in t LDA ) and document alignment information (as in p LDA ). We initially assume a given tree struc-ture, deferring the tree X  X  provenance to the end of this section. Generative Process As in LDA , each word to-ken is associated with a topic. However, tree-based topic models introduce an additional step of select-ing a concept in a topic responsible for generating each word token. This is represented by a path y d,n through the topic X  X  tree.

The probability of a path in a topic depends on the transition probabilities in a topic. Each concept i in topic k has a distribution over its children nodes is governed by a Dirichlet prior:  X  k,i  X  Dir (  X  i ) . Each path ends in a word (i.e., a leaf node) and the probability of a path is the product of all of the transitions between topics it traverses. Topics have correlations over words because the Dirichlet parameters can encode positive or negative correla-tions (Andrzejewski et al., 2009).

With these correlated in topics in hand, the gen-eration of documents are very similar to LDA . For every document d , we first sample a distribution over topics  X  d from a Dirichlet prior Dir (  X  ) . For every token in the documents, we first sample a topic z dn from the multinomial distribution  X  d , and then sample a path y dn along the tree according to the transition distributions specified by topic z dn . Because every path y dn leads to a word w dn in lan-guage l dn , we append the sampled word w dn to document d l both languages; monolingual documents only have words in a single language.

The full generative process is: 1: for topic k  X  1 ,  X  X  X  ,K do 2: for each internal node n i do 3: draw a distribution  X  ki  X  Dir (  X  i ) 4: for document set d  X  1 ,  X  X  X  ,D do 5: draw a distribution  X  d  X  Dir (  X  ) 6: for each word in documents d do 7: choose a topic z dn  X  Mult (  X  d ) 8: sample a path y dn with probability Q 9: y dn leads to word w dn in language l dn 10: append token w dn to document d l
If we use a flat symmetric Dirichlet prior instead of the tree prior, we recover p LDA ; and if all docu-ments are monolingual (i.e., with distinct distribu-tions over topics  X  ), we recover t LDA . pt LDA con-nects different languages on both the word level (us-ing the word correlations) and the document level (using the document alignments). We compare these models X  machine translation performance in Section 5. Figure 1: An example of constructing a prior tree from a bilingual dictionary: word pairs with the same meaning but in different languages are con-cepts; we create a common parent node to group words in a concept, and then connect to the root; un-correlated words are connected to the root directly. Each topic uses this tree structure as a prior. Build Prior Tree Structures One remaining question is the source of the word-level connections across languages for the tree prior. We consider two resources to build trees that correlate words across languages. The first are a multilingual dic-tionaries ( dict ), which match words with the same meaning in different languages together. These re-lations between words are used as the concepts in the prior tree (Figure 1).

In addition, we extract the word alignments from aligned sentences in a parallel corpus. The word pairs define concepts for the prior tree ( align ). We use both resources for our models (denoted as pt
LDA -dict and pt LDA -align ) in our experiments (Section 5) and show that they yield comparable performance in SMT . Inference of probabilistic models discovers the pos-terior distribution over latent variables. For a col-lection of D documents, each of which contains N d number of words, the latent variables of pt LDA are: transition distributions  X  ki for every topic k and internal node i in the prior tree structure; multi-nomial distributions over topics  X  d for every docu-ment d ; topic assignments z dn and path y dn for the n th word w dn in document d . The joint distribution of polylingual tree-based topic models is p ( w , z , y ,  X  ,  X  ;  X , X  ) =
Exact inference is intractable, so we turn to ap-proximate posterior inference to discover the latent variables that best explain our data. Two widely used approximation approaches are Markov chain Monte Carlo (Neal, 2000, MCMC ) and variational Bayesian inference (Blei et al., 2003, VB ). Both frameworks produce good approximations of the posterior mode (Asuncion et al., 2009). In addition, Mimno et al. (2012) propose hybrid inference that takes advantage of parallelizable variational infer-ence for global variables (Wolfe et al., 2008) while enjoying the sparse, efficient updates for local vari-ables (Neal, 1993). In the rest of this section, we discuss all three methods in turn.

We explore multiple inference schemes because while all of these methods optimize likelihood be-cause they might give different results on the trans-lation task. 4.1 Markov Chain Monte Carlo Inference We use a collapsed Gibbs sampler for tree-based topic models to sample the path y dn and topic as-signment z dn for word w dn , where  X ( s ) represents the word that path s leads to, N k | d is the number of tokens assigned to topic k in document d and N i  X  j | k is the number of times edge i  X  j in the tree assigned to topic k , exclud-ing the topic assignment z dn and its path y dn of current token w dn . In practice, we sample the la-tent variables using efficient sparse updates (Yao et al., 2009; Hu and Boyd-Graber, 2012). 4.2 Variational Bayesian Inference Variational Bayesian inference approximates the posterior distribution with a simplified variational distribution q over the latent variables: document topic proportions  X  , transition probabilities  X  , topic assignments z , and path assignments y .

Variational distributions typically assume a mean-field distribution over these latent variables, removing all dependencies between the latent vari-ables. We follow this assumption for the transi-tion probabilities q (  X  |  X  ) and the document topic proportions q (  X  |  X  ) ; both are variational Dirichlet distributions. However, due to the tight coupling between the path and topic variables, we must model this joint distribution as one multinomial, q ( z , y |  X  ) . If word token w dn has K topics and S paths, it has a K  X  S length variational multino-mial  X  dnks , which represents the probability that the word takes path s in topic k . The complete variational distribution is q (  X  ,  X  , z , y |  X  ,  X  ,  X  ) =
Our goal is to find the variational distribution q that is closest to the true posterior, as measured by the Kullback-Leibler (KL ) divergence between the true posterior p and variational distribution q . This induces an  X  X vidence lower bound X  ( ELBO , L ) as a function of a variational distribution q : L =
E = + + + H [ q (  X  )] + H [ q (  X  )] + H [ q ( z , y )] , (6) where H [  X  ] represents the entropy of a distribution. Optimizing L using coordinate descent provides the following updates:  X  dnkt  X  exp {  X (  X  dk )  X   X (  X  where  X  0 ( w dn ) is the set of all paths that lead to word w dn in the tree, and t represents one particular path in this set. I [ i  X  j  X  s ] is the indicator of whether path s contains an edge from node i to j . 4.3 Hybrid Stochastic Inference Given the complementary strengths of MCMC and VB , and following hybrid inference proposed by Mimno et al. (2012), we also derive hybrid infer-ence for pt LDA .

The transition distributions  X  are treated identi-cally as in variational inference. We posit a varia-tional Dirichlet distribution  X  and choose the one that minimizes the KL divergence between the true posterior and the variational distribution.
For topic z and path y , instead of variational updates, we use a Gibbs sampler within a document. We sample z dn and y dn conditioned on the topic and path assignments of all other document tokens, based on the variational expectation of  X  , q ( z dn = k,y dn = s | X  z dn ,  X  y dn ; w )  X  (10) This equation embodies how this is a hybrid algo-rithm: the first term resembles the Gibbs sampling term encoding how much a document prefers a topic, while the second term encodes the expecta-tion under the variational distribution of how much a path is preferred by this topic, E q [log p ( y dn | z dn ,  X  ) p ( w dn | y dn )] = I [ X ( y
For every document, we sweep over all its to-kens and resample their topic z dn and path y dn conditioned on all the other tokens X  topic and path assignments  X  z dn and  X  y dn . To avoid bias, we discard the first B burn-in sweeps and take the following M samples. We then use the empirical average of these samples update the global varia-tional parameter q (  X  |  X  ) based on how many times we sampled these paths  X  For our experiments, we use the recommended set-tings B = 5 and M = 5 from Mimno et al. (2012). We evaluate our new topic model, pt LDA , and exist-ing topic models X  LDA , p LDA , and t LDA  X  X n their ability to induce domains for machine translation and the resulting performance of the translations on standard machine translation metrics.
 Dataset and SMT Pipeline We use the NIST MT Chinese-English parallel corpus ( NIST ), excluding non-UN and non-HK Hansards portions as our train-ing dataset. It contains 1 . 6 M sentence pairs, with 40 . 4 M Chinese tokens and 44 . 4 M English tokens. We replicate the SMT pipeline of Eidelman et al. (2012): word segmentation (Tseng et al., 2005), align (Och and Ney, 2003), and symmetrize (Koehn et al., 2003) the data. We train a modified Kneser-Ney trigram language model on English (Chen and Goodman, 1996). We use CDEC (Dyer et al., 2010) for decoding, and MIRA (Crammer et al., 2006) for parameter training. To optimize SMT system, we tune the parameters on NIST MT 06, and report results on three test sets: MT 02, MT 03 and MT 05. 2 Topic Models Configuration We compare our polylingual tree-based topic model (pt LDA ) against tree-based topic models (t LDA ), polylingual topic models (p LDA ) and vanilla topic models ( LDA ). 3 We also examine different inference algorithms X  Gibbs sampling ( gibbs ), variational inference ( variational ) and hybrid approach ( variational-hybrid ) X  X n the effects of SMT performance. In all experiments, we set the per-document Dirichlet parameter  X  = 0 . 01 and the number of topics to 10 , as used in Eidelman et al. (2012).
 Resources for Prior Tree To build the tree for t
LDA and pt LDA , we extract the word correla-tions from a Chinese-English bilingual dictio-using the NIST vocabulary, and keep entries map-ping single Chinese and single English words. The prior tree has about 1000 word pairs ( dict ).
We also extract the bidirectional word align-ments between Chinese and English using GIZA ++ (Och and Ney, 2003). We then remove the word pairs appearing more than 50 K times or fewer than 500 times and construct a second prior tree with about 2500 word pairs ( align ).

We apply both trees to t LDA and pt LDA , denoted as t LDA -dict , t LDA -align , pt LDA -dict , and pt LDA align . However, t LDA -align and pt LDA -align do worse than t LDA -dict and pt LDA -dict , so we omit t LDA -align in the results.
 Domain Adaptation using Topic Models We examine the effectiveness of using topic models for domain adaptation on standard SMT evalua-tion metrics X  BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). We report the results on three different test sets (Figure 2), and all SMT results are averaged over five runs.

We refer to the SMT model without domain adap-tation as baseline . 5 LDA marginally improves ma-chine translation (less than half a BLEU point). on MT 03 using variational and variational-hybrid inferences. Polylingual topic models p LDA and tree-based topic models t LDA -dict are consistently better than LDA , suggesting that incorporating additional bilin-gual knowledge improves topic models. These im-provements are not redundant: our new pt LDA -dict model, which has aspects of both models yields the best performance among these approaches X  X p to a 1 . 2 BLEU point gain (higher is better), and -2 . 6 TER improvement (lower is better). The BLEU improve-ment is significant (Koehn, 2004) at p = 0 . 01 , 6 except on MT 03 with variational and variational-hybrid inference.

While pt LDA -align performs better than base-line SMT and LDA , it is worse than pt LDA -dict , possibly because of errors in the word alignments, making the tree priors less effective.
 Scalability While gibbs has better translation scores than variational and variational-hybrid , it is less scalable to larger datasets. With 1 . 6 M NIST training sentences, gibbs takes nearly a week to run 1000 iterations. In contrast, the parallelized variational and variational-hybrid approaches, which we implement in MapReduce (Dean and Ghemawat, 2004; Wolfe et al., 2008; Zhai et al., 2012), take less than a day to converge. In this section, we qualitatively analyze the trans-lation results and investigate how pt LDA and its cousins improve SMT . We also discuss other ap-proaches to improve unsupervised domain adapta-tion for SMT . 6.1 How do Topic Models Help SMT ? We present two examples of how topic models can improve SMT . The first example shows both LDA and pt LDA improve the baseline . The second exam-ple shows how LDA introduce biases that mislead SMT and how pt LDA  X  X  bilingual constraints correct these mistakes.

Figure 3 shows a sentence about a company pt pt
LDA (bottom). The Chinese translations are in parenthesis. introducing new technology gadgets where both LDA and pt LDA improve translations. The base-line translates  X   X   X   X  to  X  X et X  (red), and  X   X   X   X  to  X  X ith X  (blue), which do not capture the reference meaning of a add-on device that works with com-patible games. Both LDA and pt LDA assign this sentence to a business domain, which makes the translations probabilities shift toward correct trans-lations: the probability of translating  X   X   X   X  to  X  X ompatible X  and the probability of translating  X   X   X   X  to  X  X it X  in the business domain are both signif-icantly larger than without the domain knowledge; and the probabilities of translating  X   X   X   X  to  X  X ith X  and the probability of translating  X  X et X  to  X   X   X   X  in the business domain decrease.

The second example (Figure 4) illustrates how pt
LDA offers further improvements over LDA . The source sentence discusses foreign affairs . The baseline correctly translates the word  X   X   X   X  to  X  X ffect X . However, LDA  X  X hich only takes mono-lingual information from the source language X  assigns this sentence to economic development . This misleads SMT to lower the probability for the correct translation  X  X ffect X ; it chooses  X  X mpact X  instead. In contrast, pt LDA  X  X hich incorporates bilingual constraints X  X uccessfully labels this sen-tence as foreign affairs and produces a softer, more nuanced translation that better matches the refer-ence. The translation of  X   X   X   X  is very similar, except in this case, both the baseline and LDA produce the incorrect translation  X  X he commitment of X . This is possible because the probabilities of translating  X   X   X   X  to  X  X romised to X  and translat-ing  X  X romised to X  to  X   X   X   X  (the correct transla-tion, in both directions) increase when conditioned on pt LDA  X  X  correct topic but decrease when condi-tioned on LDA  X  X  incorrect topic. 6.2 Other Approaches Other approaches have used topic models for ma-chine translation. Xiao et al. (2012) present a topic similarity model based on LDA that produces a fea-ture that weights grammar rules based on topic compatibility. They also model the source and tar-get side of rules and compare the target similarity during decoding by projecting the target distribu-tion into the source space. Hasler et al. (2012) use the source-side topic assignments from hidden topic Markov models (Gruber et al., 2007, HTMM ) which models documents as a Markov chain and assign one topic to the whole sentence, instead of a mixture of topics. Su et al. (2012) also apply HTMM to monolingual data and apply the results to machine translation. To our knowledge, however, this is the first work to use multilingual topic mod-els for domain adaptation in machine translation. 6.3 Improving Language Models Topic models capture document-level properties of language, but a critical component of machine translation systems is the language model, which provides local constraints and preferences. Do-main adaptation for language models (Bellegarda, 2004; Wood and Teh, 2009) is an important avenue for improving machine translation. Models that si-multaneously discover global document themes as well as local, contextual domain-specific informa-Figure 4: Better SMT result using pt LDA compared to
LDA and the baseline. Top row: the source sentence pt tion (Wallach, 2006; Boyd-Graber and Blei, 2008) may offer further improvements. 6.4 External Data The topic models presented here only require weak alignment between documents at the document level. Extending to larger datasets for learning topics is straightforward in principle. For exam-ple, pt LDA could learn domains from a much larger corpus like Wikipedia and then apply the extracted domains to machine translation data. However, this presents further challenges, as Wikipedia X  X  do-mains are not representative of newswire machine translation datasets; a flexible hierarchical topic model (Teh et al., 2006) would better distinguish useful domains from extraneous ones. Topic models generate great interest, but their use in  X  X eal world X  applications still lags; this is par-ticularly true for multilingual topic models. As topic models become more integrated in common-place applications, their adoption, understanding, and robustness will improve.

This paper contributes to the deeper integration of topic models into critical applications by present-ing a new multilingual topic model, pt LDA , com-paring it with other multilingual topic models on a machine translation task, and showing that these topic models improve machine translation. pt LDA models both source and target data to induce do-mains from both dictionaries and alignments. Fur-ther improvement is possible by incorporating topic models deeper in the decoding process and adding domain knowledge to the language model.
 We would like to thank the anonymous reviewers, Doug Oard, and John Morgan for their helpful com-ments, and thank Junhui Li and Ke Wu for insight-ful discussions. This work was supported by NSF Grant IIS-1320538. Boyd-Graber is also supported by NSF Grant CCF-1018625. Any opinions, find-ings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor.

