 HAN-PING SHEN, CHUNG-HSIEN WU, and PEI-SHAN TSAI , Due to globalization, multilingual communication has become more and more popular in recent years. With the increase of multilingual speech data and the need for wide applications, the development of bilingual or multilingual automatic speech recogni-tion (ASR) systems has become more and more important. In bilingual or multilin-gual ASR, non-native speakers usually pronounce speech with accents of their mother tongues. For an ASR system, accents produced by non-native speakers will dramati-cally deteriorate recognition performance. How to overcome the degradation problem caused by accents has become one of the most important research issues in bilingual or multilingual speech recognition.

In past years, much research has been conducted dealing with accent or disor-dered articulation problems in order to improve recognition performance. Pronunci-ation variation for inter-or intra-speakers was discussed in Strik and Cucchiarini [1999]. For accent modeling, existing methods vary from collecting data in a specific accent for training a recognizer to adapting the native speech recognizer. The results have shown that the use of accent-specific data for adaptation recognition rates ef-fectively. Traditional methods, such as maximum likelihood linear regression (MLLR) and maximum a posteriori (MAP) estimation [Dempster et al. 1997; Young et al. 2006], for acoustic model adaptation were most widely used. Wang et al. [2003] and Wang and Schultz [2003] reported the effectiveness of using a polyphone decision tree spe-cialization method and acoustic modeling adaptation for non-native speech recogni-tion. Tomokiyo and Waibel [2001] focused on adaptation for lower-proficiency speakers, exploring how acoustic data can be put to most effective use. Livescu and Glass [2000] explored modifications to the lexicon to incorporate non-native pronunciations, which attempted to derive rules from the data. Bouselmi et al. [2007] used not only acous-tic adaptation but also pronunciation modeling for non-native speech recognition. Bouselmi et al. [2008] further proposed several approaches based on phonetic confu-sion and acoustic adaptation to investigate the feasibility of multi-accent, non-native speech recognition without accent detection. Oh et al. [2007] investigated pronunci-ation variability between native and non-native speakers and proposed an acoustic model adaptation method based on pronunciation variability. Wu et al. [2011] investi-gated articulation-disordered speech recognition using speaker-adaptive acoustic mod-els and personalized articulation patterns. Nevertheless, these methods can only train or adapt the acoustic models to recognize specifically accented or disordered speech according to the collected training or adaptation data. Collecting all types of accented speech is time consuming and almost impossible.

Furthermore, Stefan [2003] modeled prototypical foreign-accented pronunciation at the phonetic transcription level using accent rewrite rules. Zhang et al. [2008] used a state-level, bilingual model modification approach to improving non-native speech recognition accuracy with great variation in accented pronunciations, espe-cially when a substitution occurs. Chen et al. [2011] proposed an informative dialect recognition system that learned phonetic rules for dialect identification. In Chen X  X  approach, a hidden Markov model (HMM) was used to align reference phones with dialect-specific pronunciations to characterize when and how often substitutions, in-sertions, and deletions occur. Yang et al. [2008] presented a hybrid method considering expert knowledge and data-driven techniques to model accent-specific pronunciation variation through pronunciation dictionary adaptation. Fung and Liu [2005] studied the effects of phonetic and acoustic confusion in accented speech. They proposed a likelihood ratio for measuring phonetic confusion and an asymmetric distance for mea-suring acoustic confusion. Only accent-specific phonetic units with low acoustic confu-sion were used in an augmented pronunciation dictionary, while the phonetic units with high acoustic confusion were reconstructed using decision tree merging. Liu and Fung [2004] incorporated pronunciation modeling into acoustic models with high dis-criminative ability and low complexity to improve spontaneous speech recognition ac-curacy. In order to deal with the low-resource problem, Qian et al. [2011] presented a data borrowing strategy combining a subspace Gaussian mixture model (SGMM). The result showed that SGMMs were more robustly estimated by borrowing the data from the non-target language at the acoustic-state level. Some other interesting ap-proaches related to bilingual or multilingual speech recognition, such as phone cluster-ing [Lebese et al. 2012], senone clustering [Yeh et al. 2011], and language identification [Weiner et al. 2012], have been proposed. In addition, code-switching issues in bilin-gual or multilingual speech recognition have also beeen discussed [Vu et al. 2012; Wu et al. 2012; Yeh et al. 2012].

In accented speech extraction, phone recognizers based on acoustic features were generally used to extract accented speech segments [Fukada et al. 1999; Mokbel and Jouvet 1998; Oh et al. 2007; Ravishankar and Eskenazi 1997; Strik and Cucchiarini 1999; Torre et al. 1997; Williams and Renals 1998]. Nevertheless, the extraction re-sults were usually affected by the recognition accuracy of the recognizer. Different ex-planations for the poor performance of HMM-based recognizers on spontaneous speech based on acoustic features and reasons why the articulatory features might help in overcoming the encountered problems have been presented by several researchers. Ostendorf [1999] argued that pronunciation variability in spontaneous speech is the main reason for poor recognition performance. She also thought that the use of linguistically-motivated distinctive features could provide the necessary granularity to better deal with pronunciation variants using context-dependent rules that describe the value changes of the features. Kirchhoff [2000] also acknowledged that it is easier to model pronunciation variants with the help of articulatory features. She pointed out that articulatory features exhibit a dual nature, because they have a relation to the speech signal as well as to higher-level linguistic units. In addition, St  X  uker et al. [2003a] claimed that articulatory features are robust to inter-language variabil-ity. Based on previous research, articulatory features, containing manner and place of articulation for consonants and tongue positions for vowels, can provide a universal acoustic characterization of all spoken languages. Furthermore, Zhang et al. [2011] proposed an attribute-based approach to accented speech recognition based on auto-matic speech attribute transcription with high detection accuracy of articulatory fea-tures. The results showed that the detection-based system improved the recognition rate in recognizing accented speech using articulatory features. Felps et al. [2012] pro-posed a concatenative synthesis approach to the problem of foreign accent conversion. From the results, we can see that the change of articulatory features is highly related to accents. In addition to data-driven approaches, knowledge-based methods stated that some of the accent problems can be solved by adding new pronunciation rules into the pronunciation dictionary. Nevertheless, accents appear in many ways and are therefore difficult to model well.
 In this study, a new approach to acoustic model generation of accented English speech using highly-accented speech segment verification and transforma-tion function-based model generation is proposed. This study focuses on Mandarin-English ASR, and the goal of this study is to generate the accented English models and by extension to generate the unseen English models which are unlikely to be obtained using insufficient training/adaptation data. Because accents from different speakers are very diverse, it is impossible to collect all different kinds of accent data for training accented acoustic models. A systematic method for automatically predict-ing and generating potential accented acoustic models for accented speech recognition becomes inevitable. The system diagram for accented phone model generation for bilin-gual speech recognition is shown in Figure 1. In the first step, the state-level speech segments with high accent are first identified using a two-stage verification method [Lee et al. 2007]. Both acoustic features and articulatory features are used for highly-accented speech segment verification. In accented segment verification using statis-tical hypothesis testing, the likelihood/score can be normalized to reduce undesired variation from acoustically mismatched conditions. Many score normalization tech-niques have been presented in the literature, for both speaker-and test-dependent variability compensation [Bimbot et al. 2004], while most of these approaches used a cohort of impostors to compensate the variability. The normalized cross-entropy measure [Campbell et al. 2006] is an alternative method for score normalization. How-ever, one difficulty with cross-entropy is that it is difficult to interpret directly. In this study, the cohort normalization, which uses a small representative set of models and has been successfully used in speaker verification [Chao et al. 2007; Rosenberg et al. 1992], is employed to estimate the likelihood of alternative hypothesis in hypothesis testing to achieve better verification performance. After verification, each successfully-verified, highly-accented speech segment and its corresponding native/less-accented speech segments are regarded as a Native-Accented speech segment pair. Linear transformation functions constructed based on the Native-Accented pairs are thus constructed. For the sparse data case, the unseen accented model is unable to be generated using insufficient training data from the unseen accented model. Accord-ingly, articulation-related questions are adopted to construct a decision tree for the classification of the transformation functions in order to deal with the data sparseness problem. For a native state-based acoustic model, the decision tree is used based on the articulation-related questions to retrieve the most suitable transformation function for generating the accented models as well as the unseen accented models which have no Native-Accented speech segment pairs for transformation function construction. Even though all accented state-based models can be generated using the retrieved trans-formation function, if all the generated accented models are used in bilingual ASR directly, the recognition accuracy will degrade dramatically because of the high ambi-guity among the generated accented and native phone models. Therefore, a discrim-inative function is applied to verify the discriminability of each generated accented phone model. Finally, the verified accented phone models are combined with the na-tive phone models to form a universal phone model set. Based on the expanded acoustic phone model set, the bilingual ASR system can improve word recognition accuracy for Mandarin-English speech recognition.

The rest of this article is organized as follows. Section 2 describes the variation of articulatory features in accented speech. Section 3 introduces the extraction of highly-accented speech segments. Section 4 presents accented phone model generation. Section 5 describes the discriminative phone model verification. Section 6 presents the evaluation and experimental results. Conclusions are finally drawn in Section 7, along with future directions for bilingual speech recognition. By definition, articulatory features illustrate the properties of a speech sound based on its voicing or its place or manner of articulation in the vocal tract, as voiceless, nasal, or anterior. Appendix A shows the Sound Pattern of English (SPE)-based artic-ulatory features [Chomsky and Halle 1968]. As mentioned in the previous section, the accented and native speech utterances of the same phone have different articulatory features, and the property variation of articulatory features is highly related to accents [Felps et al. 2012]. For example, phoneme  X / X / X  in the word  X  X ad /h X d/ X  is frequently pronounced as  X /  X  / X  by Taiwanese, and the original articulatory features X  X ontinuant, low, tense, vocalic, and voice X  X f phoneme  X / X / X  are thus changed into  X  X ontinuant, tense, and voice, X  which are the same as the articulatory features of phoneme  X / Another example is that phoneme  X /  X  / X  in the word  X  X hese / as  X /l/ X  by Taiwanese, and thus the original articulatory features X  X nterior, consonan-tal, continuant, coronal, and voice X  X f phoneme  X /  X  / X  become  X  X nterior, consonantal, continuant, coronal, high, and voice, X  which are the same as the articulatory features of phoneme  X /l/. X  Based on the investigation, articulatory features are very likely adopted for accented speech segment verification to improve the robustness of verification. This study proposes a two-stage verification method, including acoustic and articula-tory feature-based verification, to extract the state-level, highly-accented speech seg-ments for accented model generation. Figure 2 shows the block diagram for accented speech segment extraction. In the first stage for verification, since the word transcrip-tions of the training speech corpus are available, forced alignment-based ASR is per-formed to estimate the likelihood for each state-level speech segment using acoustic features. Accordingly, the potential state-level, highly-accented speech segments with low likelihood can be identified. Since the ASR is error prone, articulatory feature-based verification is employed in the second stage to further verify if the extracted highly-accented candidate speech segments are either misrecognized or potentially ac-cented. In the second stage, an AF detector [Siniscalchi et al. 2008; St  X  uker et al 2003] is constructed to extract the AFs of each speech segment. If the AFs of the extracted candidate speech segment are different from those of the corresponding native/less-accented speech segment, the candidate speech segment is regarded as a true highly-accented speech segment. On the contrary, if the AFs of the candidate segment are similar to those of the corresponding native/less-accented speech segment, the candi-date speech segment is regarded as a native/less-accented speech segment with low likelihood obtained from incorrect forced alignment. The detailed verification process is described in the following sections. In the first verification stage, given the acoustic features of the state-level speech seg-ment O S obtained from the ASR system in forced alignment mode, the acoustic like-lihood between O S and its corresponding native state-based acoustic model  X  used to verify if O S is a highly accented candidate speech segment. Eqs. (1) and (2) are used to estimate the likelihood of the state-level speech segment O where g (.) is the likelihood function for forced alignment and  X  based acoustic model. A set of cohort models, {  X  Cohort i are the N Cohort best native speech models for speech segment O statistical hypothesis testing. If O S is a native/less-accented speech segment, the value of g ( O S |  X  native ) will be high, while the value of g to a high value of G veri ( O S ) . On the other hand, if O segment, the value of g ( O S |  X  native ) will be low, while the value of g high. This leads to a low value of G veri ( O S ) .When G threshold, speech segment O S is judged to pass the verification and is regarded as a highly-accented candidate speech segment. In this study, the successfully verified highly-accented English speech segments will be used to generate the accented English model. Thus, only the native English models are used as the cohort models in first-stage verification. In order to avoid misverification due to incorrect forced alignment in the first stage, state-based AFs are used to further verify the extracted highly-accented candidate speech segments. The basic assumption on the proposed AF-based verification is that if an aligned speech segment has low likelihood, there are two possibilities: (1) The speech segment is actually pronounced with weak accent but recognized with low likelihood, while the detected AFs of the speech segment are similar to the AFs of the native/less-accented speech segment. In this case, a false alarm occurs in the first stage because of incorrect forced alignment. The speech segment is thus re-garded as the native/less-accented speech segment. (2) The speech segment is actually pronounced with high accent and has low likelihood, and the AFs of this speech seg-ment are different from that of the native/less-accented speech segment. The candidate speech segment is reasonably regarded as a highly-accented speech segment. For ap-plying the AFs to accented speech verification, 14 AFs listed in Table I are used. The AF-based verification process is shown in Figure 3. A recurrent artificial neural net-work (RNN) with feedback connections is adopted to construct the AF detector. The extracted MFCC features along with the RNN outputs of the preceding frame are fed to the input layer of the RNN. In the RNN, each hidden layer contains 600 hidden nodes which are determined empirically. The AF detector is trained using the TIMIT database with AF labels for estimating the posterior probability of a state-based speech segment belonging to each AF category. The softmax activation function is used to ob-tain the output values of the RNN. Eq. (3) is used to estimate the posterior probability output vector E AF ( O s ) of the AF categories E given speech segment O where M is the number of AF categories, z ( t ) is the input vector for the t th frame and is composed of a bias, which contains the acoustic feature vector x ( t ) with 39-dimensional MFCCs of the t th speech frame and the AF posterior probability output vector u ( t of the t-1 st speech frame. w Ei is the interconnection weight vector between the i th AF posterior probability output and the hidden nodes in the RNN-based AF detector. The inclusion of  X 1 X  in z ( t ) is used to apply a bias to the nonlinearity. N total frame number in speech segment O S . The structure of the RNN is shown in Figure 4.

After the extraction of AFs, an AF-based verification model is constructed by a bi-nary support vector machine (SVM). Each binary SVM is employed for one state. Thus, around 3,000 binary SVMs are constructed using the radial basis function kernels [Joachims 2002; Joachims et al. 1999]. A supervised training process is performed for AF-based verification model training. Only a small part of the EAT (English Across Taiwan) corpus [English Across Taiwan 2005] was used for training. For labeling the training database, the highly-accented speech segments in EAT were extracted with the help of the first-stage verifier. These extracted speech segments were further man-ually judged by two researchers at National Cheng Kung University, Taiwan, to check if they are highly-accented or native/less-accented. Before the training process, AF vec-tors of the identified native/less-accented and highly-accented speech data belonging to the same state-based model are extracted using the TIMIT-trained AF detector. The native/less-accented data were used as the positive training data of SVM. Conversely, the highly-accented data were used as the negative training data of SVM. Finally, the SVMs are used to verify if a state-level segment belongs to a positive or negative class. If a state-level segment belongs to a positive class, the segment is regarded as less ac-cented. Positive class means the candidate state-level segment has similar AFs as its corresponding native state-level segment. On the other hand, a state-level segment is highly accented if it is classified as the negative class using the binary SVM. The iden-tified highly-accented segment and its corresponding native/less-accented speech seg-ment are regarded as a Native-Accented pair for a specific state-based model. Finally, all the Native-Accented speech segment pairs will be used to generate the accented state-based models. After speech segment verification, the identified highly-accented speech segments are used to construct the accented models. Nevertheless, accented model construc-tion generally suffers from the problem of data sparseness. Using the extracted Native-Accented speech segment pairs in the corpus for a specific state-based model, linear transformation functions between the state-level native/less-accented speech segments and the highly-accented speech segments are constructed. The linear trans-formation functions can also be extended to generate the unseen accented models for the native models with similar acoustic and articulatory features but without Native-Accented speech segment pairs for constructing the transformation function. In order to deal with the problem for sparse Native-Accented speech segment pairs, a decision tree is constructed using the articulation-related questions listed in Appendix B to retrieve the nearest transformation function for the native state-based models with no Native-Accented speech segment pairs. The nodes of the decision tree contain the questions related to articulatory features. The native/less-accented model and its cor-responding linear transformation functions are stored in the leaf nodes. In the process of linear transformation function selection, the canonical articulatory features of a na-tive state-based model are first used as the input to retrieve a leaf node in the decision tree. Since there are several transformation functions in each leaf node, the acoustic features are further used to select the closest linear transformation function in the re-trieved leaf node. After retrieving the transformation function, the accented model and even the unseen accented model can be generated using the retrieved transformation function. In order to precisely generate the potential accented state-based models, the Gaus-sian component-level linear transformation functions between the Native-Accented speech segment pairs are constructed. However, there is no direct relationship be-tween the Gaussian components in the highly-accented models and the native/less-accented models. This study uses a hierarchical Gaussian component mapping method to find the correspondence between the Gaussian components in the highly-accented models and the native/less-accented models for transformation function construction. The structure of the hierarchical Gaussian component mapping method is shown in Figure 5.

The procedure starts by calculating the centroid of the means of all Gaussian com-ponents from the native state-based models denoted as N 11 means of all Gaussian components from the accented state-based models denoted as AS 11 , as shown in Figure 5(a). The transformation function f can be obtained using the Expectation-maximization (EM) algorithm. In Figure 5(b), the Gaussian components in the native state-based models are transformed using f such that the new centroid N 11 of the transformed Gaussian components coincides with AS 11 . After transformation, in Figure 5(c), we next split the Gaussian compo-nents into two clusters and calculate the centroid of each cluster for the clusters in the native and accented state-based models. The splitting process begins by finding two Gaussian components that are most far apart in the cluster for splitting and use these two Gaussian components as the new centroids for the two new clusters. Using these notations, we denote these two centroids for the accented state-based models by AS 2 i , i = 1,2, respectively. Each of the transformed data is then assigned to one of the two centroids, AS 21 and AS 22 , according to the nearest-neighbor principle in order to generate two new Gaussian component clusters. Based on the same splitting process, the Gaussian components in the native state-based models are assigned to one of two new centroids, denoted by N 2 i , i = 1, 2. The mean values for these four clusters are N 21 , N 22 , AS 21 ,and AS 22 . The cluster with mean value N corresponds to the cluster with mean value AS 22 , and the cluster with mean value N corresponds to the cluster with mean value AS 21 .

It can be observed that N 21 and N 22 correspond to N 21 and N to transformation using f 1 . In Figure 5(d), we can estimate the linear transformation functions f 1 2 and f 2 2 using N 21 , N 22 , AS 21 ,and AS be applied to the already transformed Gaussian components. This procedure repeats until the overall distance between every two centroids of all the paired Gaussian com-ponent clusters is below a threshold. Finally, the Gaussian component clusters for the native and paired accented state-based models are used for transformation function construction. The paired Gaussian components in the native and accented state-based models are used as parallel data to construct the linear transformation function. The transfor-mation function transforms each vector of the source dataset in the target dataset { y k } . A Gaussian mixture model (GMM) of the joint probabil-ity density of the model parameters between the paired Gaussian components in the native and the accented state-based models is trained. The GMM corresponds to the Gaussian mixture distribution that represents the probability distribution of obser-vations in the overall population, with N ( x ;  X  , ) denoting the p -dimensional normal distribution with mean vector  X  and covariance matrix . The following parametric form is assumed for the transformation function.
 The transformation function T (.) is defined by the p -dimensional vectors v p  X  p matrices the i th mixture of the GMM. Because linear transformation functions are constructed for the paired Gaussian components, Eq. (7) can be reduced to Eq. (8).
 where v and are, respectively, the mean target vector v = covariance matrix of the source and target vectors = E [ ( y script T denotes transposition. The derivation details can be found in Stylianou et al. [1998]. For dealing with the data sparseness problem, a decision tree is constructed to classify the transformation functions with the same articulatory features. The decision tree is used to select a suitable transformation function for Gaussian component generation of the accented state-based model from the Gaussian components of the native state-based model. The nodes of the decision tree contain the articulation-related questions listed in Appendix B. If a split happens, the generation error from the Gaussian com-ponents in two child nodes should be lower than the generation error from their parent node. The generation error in each leaf node is defined in Eq. (9).
 where y k is the mean vector of the k th target Gaussian component parameters of the accented speech model in a leaf node, x k is the mean vector of the Gaussian component parameters of the native speech model, and T ( x k ) is the transformation result. K is the total number of Gaussian components in the node. Finally, the Native-Accented Gaussian component pair and the corresponding linear transformation functions are stored in the leaf nodes. The Gaussian components for the native state-based model which are classified into the same leaf node have the same articulatory features and similar acoustic feature. In order to achieve the best split, the generation error should be minimized. The generation error reduction ( GE-Reduction ) is computed by Eq. (10). where GenErr p is the generation error of the parent node and GenErr tion error of the i th child node. K i is the number of data samples in the i th child node and K p is the number of data samples in the parent node.
 The Gaussian components of the native state-based model without a Native-Accented Gaussian component pair in the training corpus can select a specific trans-formation function through the decision tree based on the articulatory features and acoustic features. Once reaching a leaf node, a prestored transformation function can be selected according to the acoustic similarity. The selected transformation function is then used to transform the input Gaussian component of a native state-based model to generate a Gaussian component of its corresponding accented state-based model. Finally, the generated accented state-based models replace the original native state-based models to construct an accented phone model (a phone model contains three state-based models). Figure 6 shows the decision tree for transformation function selection. For accented phone model construction, each potential accented phone model is con-structed by accepting/rejecting the generated state-based model for each of the three states of the native phone model. With the generated accented and the original native phone models, a bilingual speech recognizer can be obtained. However, not all of the generated accented phone models are robust for speech recognition. Models with low discriminability among phone models will degrade the recognition accuracy. Also, the complexity increases if a large number of phone models are used for recognition. This study adopts a discrimination function along with a dynamic threshold to verify if a phone model is discriminative for bilingual speech recognition.

In this study, verification of the generated accented models can be divided into two steps. The first step is to test if a generated accented phone model is discriminative to all native phone models. A discrimination score of the i th generated accented phone model is defined in Eq. (11). where g (.) is the recognition likelihood function.  X  Y model. Y i denotes the acoustic features of the highly-accented speech data, and the value of g ( Y i |  X  Y corpus corresponding to the i th generated accented phone model. phone model set.  X  m is the m th native phone model. The lower the value of d the higher the discriminability of the generated accented phone model. Conversely, a dynamic discrimination threshold of the i th generated accented phone model to its corresponding native phone model is defined in Eq. (12). where  X  Y speech data.  X  m represents all the native phone models except  X  of d accent N i is lower than that of d native N i , the generated accented phone model will be more discriminative than the original native phone model. Hence, this model is regarded as a candidate accented phone model.

After the first step of discriminability verification, the second step is adopted to test whether a specific candidate accented phone model is discriminative to all phone mod-els, including the candidate accented phone models and native phone models. In this step, a discrimination score of the i th candidate accented phone model is defined in Eq. (13). where  X  Y highly-accented speech data for the i th candidate accented phone model.  X  all the native phone models native and candidate accented phone models  X   X  the original native phone model is defined in Eq. (14). where  X  m represents all the native phone models and candidate accented phone models except  X  Y candidate accented phone model will pass the verification and can be accepted as an accented phone model. In this study, three Mandarin or English corpora, TCC300, TIMIT [Fisher et al. 1986], and EAT (English Across Taiwan), were used to train a Mandarin-English speech recognition system. The Mandarin speech corpus, TCC300, which has been widely used for Mandarin speech processing, was utilized to train the native Mandarin acous-tic models. This corpus contains about five hours of speech data, including words, short sentences, and long sentences. The numbers of the collected sentences total 2,500 from 50 males and 2,500 from 50 females. Convesely, TIMIT was used to train the initial native English acoustic models and the AF detector. There are 6,300 sentences in the corpus. Six hundred and thirty speakers, 438 males and 192 females, participated in the TIMIT recording project. Since there is no information about which dialect of English is the most  X  X orrect X  English, all the data collected in TIMIT were used. The TIMIT corpus was collected from eight major dialect regions of the United States and was assumed native with respect to Mandarin-accented English speech. The goal of this study is to design a robust Mandarin-English ASR system which can alleviate the recognition accuracy degradation problem due to accents produced by native Man-darin speakers. Hence, the EAT corpus was adopted for performance evaluation. The EAT corpus was collected during 2004 X 2005. It contains long English sentences, short English sentences, and Chinese-English mixed sentences spoken by Taiwanese peo-ple. In EAT, 42,071 sentences from 572 English major students (166 males and 406 females) and 40,972 sentences from 592 non-English major students (368 males and 224 females) were recorded. In order to evaluate performance improvement using the accented speech model generation method, a small training corpus instead of a large amount of accented speech data was selected to construct the accented speech model. For accented speech labeling, the highly-accented speech segments in EAT were ex-tracted with the help of the first-stage verifier described in Section 3.2. These sentences with extracted highly-accented speech segments were further manually judged by two researchers at National Cheng Kung University, Taiwan, to check if they are highly-accented speech or native/less-accented speech. This manual check is much easier and consistent, since the extracted segments were short and only the highly-accented or native/less-accented speech utterances were labeled. The utterances with uncertain or inconsistent judgment were not included in this study. In total, 450 English utter-ances spoken by English major students were regarded as less-accented speech. This contained 194 and 256 sentences from males and females, respectively. Conversely, they also selected 450 English utterances spoken by non-English major students re-garded as highly-accented speech. This contained 241 and 209 speech utterances from males and females, respectively. Finally, the 900 selected utterances with about one hour of speech data formed a subcorpus. In order to focus on the accented issues, only the sentences without code-switching were used in this study. Furthermore, in order to reduce the mismatch among TCC300, TIMIT, and EAT corpora, the 450 selected less-accented utterances in EAT were used to adapt the TIMIT-trained initial English phone models. The average frame-level accuracy of the TIMIT-trained AF detector in detecting the less-accented speech in EAT was around 88.4%. For constructing a Mandarin-English ASR system, the HTK toolkit was applied to train the acoustic models. In order to combine Mandarin and English phone sets, IPA is employed for phone set definition. Based on IPA, Mandarin and English phones are mapped into 74 phones, including one silence. Each phone model, characterized by a three-state triphone HMM (around 10,000 triphone HMMs for Mandarin and English speech), is composed of three state-based models, and each state-based model contains 16 Gaussian components [Hwang and Huang 1992]. 39-dimensional MFCCs were used as the acoustic features. The trained Mandarin state-based model set and the verified English accented state-based model set were combined into a universal model set for bilingual speech recognition. In this study, even though a specific English phone and Mandarin phone can be mapped into the same IPA, they are regarded as two different basic phones. The Mandarin and English phones employed in this study are separately shown in Table II, where the superscript m is attached to each Mandarin phone if the phone is also used in English. In the test phase, bilingual recognition is performed using the phone models of English and Mandarin.
 A bigram language model was employed and trained using the sentences in TCC300, TIMIT, EAT, and Chinese Gigaword. The vocabulary contains about 39,000 words, in-cluding roughly 34,000 Mandarin and 5,000 English words. According to the previous settings, around 3,000 state-based models were generated. Finally, about 5,500 tri-phone models can be obtained from the generated state-based models. For performance evaluation, nine-fold cross-validation was employed in the following experiments. 6.3.1. Evaluation on Different Verification Methods. This section discusses the effect of using different accented speech verification methods and parameter settings. The proposed verification method can be divided into two steps, acoustic feature-based verification (AC) and articulatory feature-based verification (AF). The fused method considering AC and AF is denoted as AC + AF.

The first experiment was conducted on the effect of the number of cohort mod-els in acoustic feature-based accented speech segment verification. The recognition accuracies using nine-fold cross-validation with different numbers of cohort models (1  X  7) are shown in Figure 7. The goal of this experiment is to find a parameter set-ting that achieves a balanced and good result in recognizing both highly-accented and native/less-accented speech. From Figure 7, the effect of the number of cohort models is not obvious, yet the best recognition accuracy was achieved when the co-hort model number was set to 6. Using this setting, the word recognition accuracies were 80.6%, 80.1%, and 67.8% for native Mandarin, native/less-accented English, and highly-accented English speech, respectively. Hence, the cohort model number was set to 6 in the following experiments.
 The evaluation results of the proposed verification methods are shown in Figure 8. Recognition experiments on the methods using AC, AF, and AC segment verification were evaluated individually. The recognition rates based on nine-fold cross-validation for native Mandarin, native/less-accented English, and accented English speech were obtained. The results are presented for three categories, including native English, native Mandarin, and accented English. Among the three proposed methods, the AC + AF method outperformed the other two methods in recognizing the utterances from the three corpora. From the results, we can conclude that the proposed two-step method is the most robust approach among these three verification methods. 6.3.2. Evaluation on the Effectiveness of Discriminative Functions. Before model verification, the number of generated accented phone models was more than 55,000 for each set in the training data. It can be easily seen that if all the generated accented models were used for speech recognition, the efficiency and the results would be poor due to the huge amount of models. In order to evaluate the effectiveness of the discriminative functions, the following experiments were conducted based on (1) native and all gener-ated accented models, (2) native and generated accented models passing the first step in discriminative phone model verification, and (3) native and generated accented mod-els passing the first and second steps in the discriminative phone model verification. The average recognition rate in recognizing highly-accented English speech using na-tive and all generated accented models was as low as 7.7%, which shows that a large part of the generated accented models are not reliable for recognition. The average recognition rate for recognizing the highly-accented English speech using the native and generated accented models passing the first step in discriminative phone model verification achieved 60.5%, while the average recognition rate for the recognizer using native and generated accented models which passed the first and second steps in dis-criminative phone model verification was 67.8%. About 93% of the generated accented models were removed in the first step of discriminative phone model verification. After the first step of discriminative phone model verification, the recognition accu-racy improved significantly. However, the result was not as good as the result achieved using native models only. That is, even though the generated accented models were discriminative to all native models, the generated candidate accented models and ac-cented models trained from the speech corpus will interfere with each other. Thus, the second step of discriminative phone model verification was used to prevent the con-structed model set from the interference effect. With the second step verification, the result could be improved to 67.8%. Finally, the native phone models and the verified accented phone models were integrated into one global model set. The average number of generated accented phone models was about 1,800. 6.3.3. Comparison to Traditional Adaptation-Based Methods. For fair comparison, the MLLR-adapted phone models and MAP-adapted phone models were built by adapt-ing the native phone model using the labeled 450 highly-accented utterances. In or-der to find the optimal MLLR-adapted phone models, the word recognition accuracies for the systems using traditional MLLR-adapted phone models with different regres-sion classes (1, 2, 4, 8, 16, and 32) are shown in Table III. The results reveal that MLLR-adapted phone models with eight regression classes achieved the highest accu-racy. Thus, in the following experiment, the number of regression classes was set to eight for MLLR-adapted phone models. Furthermore, the results for the system us-ing native + MLLR-adapted phone models and native + MAP-adapted phone models are also shown to compare with our proposed expanded phone models.

The following two experiments conducted comparisons on word recognition ac-curacy of the systems using native phone models, MLLR-adapted phone models, native + MLLR-adapted phone models, MAP-adapted phone models, native adapted phone models, and the proposed expanded phone models. As previously mentioned, the goal of this study is to design a speech recognition system that can reduce the negative effects of accents and keep good word accuracy in recognizing native speech. Figure 8 shows the recognition results of using native phone models, MLLR-adapted phone models, native + MLLR-adapted phone models, MAP-adapted phone models, native + MAP-adapted phone models, and the expanded phone mod-els in recognizing English and Mandarin speech. From the results, we can see that there is no apparent difference in native Mandarin speech recognition using these six methods. This is because the generated English accented models were discrimi-native, and the Mandarin word recognition accuracy was not highly affected by these generated English models. In native and accented English speech recognition, it is clear that the MLLR-adapted phone models, native + MLLR-adapted phone models, MAP-adapted phone models, and native + MAP-adapted phone models outperformed the native phone models. Compared with the recognition results obtained using na-tive models only, the substitution error rate obtained using adapted models was reduced significantly. For example, using MLLR-adapted models, about 80% of the improvement was contributed by the reduction of substitution errors, and almost all of the remaining 20% improvement was obtained by the reduction of insertion errors. Furthermore, the expanded phone models outperformed the MLLR-adapted phone models, native + MLLR-adapted phone models, MAP-adapted phone models, and native + MAP-adapted phone models. The results reveal that the proposed expanded phone models not only improved word accuracy in recognizing highly-accented English speech but also in recognizing native/less-accented English speech. 6.3.4. Evaluations on Recognition using Native and Expanded Phone Models. The following experiment was conducted to show phone recognition accuracy for native speech data and accented speech data in recognizing the 450 highly-accented utterances. In this experiment, speech segments with strong accents were identified using the extraction method described in Section 2. The recognition accuracies for recognizing native/less-accented and highly-accented speech data using the expanded phone models are il-lustrated in Figure 9. The results show that there is no significant difference in phone recognition accuracy between the methods using native phone models and expanded phone models in recognizing native/less-accented English speech. But in highly-accented speech recognition, the proposed expanded phone models significantly outperformed traditional native phone models. The phone accuracy of the system using the expanded phone models achieved 23.3% recognition rate in highly-accented phone recognition, which was 16.5% higher than that using native phone models. Further-more, the recovery and mis-recognition rates, which are defined in Eqs. (15) and (16), for native/less-accented and highly-accented phone recognition using expanded phone models are shown in Figure 10.
 where E nor denotes the number of phones which were misrecognized using native phone models. R exp denotes the number of phones recovered successfully using the expanded phone models while these phones were misrecognized by the native phone models. C nor denotes the number of phones which were correctly recognized using the native phone models. E exp denotes the number of phones misrecognized using the ex-panded phone models while these phones were correctly recognized using the native phone models. The result shows that the recovery and misrecognition rates were 1.2% and 2.7% in native phone recognition. Conversely, it is obvious that the phone recovery rate of highly-accented English phones achieved 18.4%, which was 16.5% higher than the misrecognition rate. Although few misrecognition errors occurred after adding ac-cented phone models into the recognition phone set, the recovery rate still can cover those errors. This result reveals that the expanded phone models can improve phone accuracy in recognizing both native/less-accented and highly-accented speech. 6.3.5. Comparison with Speaker-Adaptation Method with VTLN. In order to evaluate that the improvement was contributed by the generated and verified accented phone models or the speaker-adaptation methods, the vocal track length normalization (VTLN) method was applied to extract the speaker-normalized MFCC features in the feature extrac-tion stage. Then, the extracted features were used to generate the accented phone model using the proposed method. In addition, the extracted normalized MFCC fea-tures were also used to build the MLLR-adapted model. The recognition accuracies achieved using our proposed method and MLLR-adapted models in recognizing highly-accented speech were 71.1% and 66.9%. The results reveal that there still remains room for improvement using both methods after removing the factors of gender and speaker. Since accents are diverse, the improvement of using the adaptation-based method is limited. However, the proposed method is not an adaptation-based method but a generation-based method which can be used to generate discriminative accented models for diverse accents. Therefore, the improvement obtained using the proposed method is higher than that using the adaptation-based method after removing gender and speaker information. The aim of this study focuses on generating robust accented phone models for the recognition of Mandarin-English speech with non-native accent. In highly-accented speech segment verification, state-based acoustic models are used. Articulatory features are integrated with acoustic features to verify the potential state-level highly-accented segments in the speech data. Linear transformation functions between the native/less-accented and the highly-accented speech segments are constructed. These transformation functions are used to generate the accented state-based models for constructing the accented phone models. A two-step discriminability verification pro-cedure is performed to remove the generated phone models with low discriminability.
In the experiments, the proposed method achieved relative improvements of 3.8% and 1.8% over acoustic feature-based and articulatory feature-based methods in recog-nizing highly-accented English speech, respectively. Other experimental results reveal that the proposed ASR speech recognition system using expanded phone models can improve the ability to recognize highly-accented speech and keep comparable perfor-mance in recognizing native/less-accented speech. The word accuracy of the system using the proposed expanded phone models is 4.1% higher than that using traditional native phone models, 2.7% higher than that using MLLR-adapted phone models, 3.2% higher than that using native + MLLR-adapted phone models, 1.8% higher than that using MAP-adapted phone models, and 2.4% higher than that using native adapted phone models in recognizing highly accented English speech. In conclusion, the proposed method can generate robust accented phone models, and is effective in recognizing highly-accented and native/less-accented speech for Mandarin-English speech recognition.

