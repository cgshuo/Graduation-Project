 A core issue in sensory coding is to seek out and model statistical regularities in high-dimensional data. In particular, motivated by developments in information theory, it has been hypothesized that modeling these regularities by means of redundancy reduction constitutes an important goal of early visual processing [2]. Recent studies conjectured that the binary spike responses of retinal ganglion cells may be characterized completely in terms of second-order correlations when using a maximum entropy approach [13, 12]. In light of what we know about the statistics of the visual input, however, this would be very surprising: Natural images are known to exhibit complex higher-order correlations which are extremely difficult to model yet being perceptually relevant. Thus, if we assume that retinal ganglion cells do not discard the information underlying these higher-order correlations altogether, it would be a very difficult signal processing task to remove all of those already within the retinal network.
 Oftentimes, neurons involved in early visual processing are modeled as rather simple computational units akin to generalized linear models, where a linear filter is followed by a point-wise nonlinearity. For such simple neuron models, the possibility of removing higher-order correlations present in the input is very limited [3].
 Here, we study the role of second-order correlations in the multivariate binary output statistics of such linear-nonlinear model neurons with a threshold nonlinearity responding to natural images. That is, each unit can be described by an affine transformation z k = w T k x +  X  followed by a point-wise signum function s k = sgn ( z k ) . Our interest in this model is twofold: (A) It can be regarded a parsimonious model for the analysis of population codes of natural images for which the Figure 1: Similarity between the Ising and the DG model. A+C: Entropy difference  X  H between the Ising computational power and the bandwidth of each unit is limited. (B) The same model can also be used more generally to fit multivariate binary data with given pairwise correlations, if x is drawn from a Gaussian distribution. In particular, we will show that the resulting distribution closely resembles the binary maximum entropy models known as Ising models or Boltzmann machines which have recently become popular for the analysis of spike train recordings from retinal ganglion cell responses [13, 12].
 Motivated by the analysis in [12, 13] and the discussion in [10] we are interested at a more gen-eral level in the following questions: are pairwise interactions enough for understanding the sta-tistical regularities in high-dimensional natural data (given that they provide a good fit in the low-dimensional case)? If we suppose that pairwise interactions are enough, what can we say about the amount of redundancies in high-dimensional data? In comparison with neural spike data, natural images provide two advantages for studying these questions: 1) It is much easier to obtain large amounts of data with millions of samples which are less prone to nonstationarities. 2) Often differ-ences in the higher-order statistics such as between pink noise and natural images can be recognized by eye. In order to study whether pairwise interactions are enough to determine the statistical regularities in high-dimensional data, it is necessary to be able to compute the maximum entropy distribution for large number of dimensions N . Given a set of measured statistics, maximum entropy models yield a full probability distribution that is consistent with these constraints but does not impose any Figure 2: Examples of covariance matrices ( A+B. ) and their learned approximations ( C+D ) at m = 10 for additional structure on the distribution [7]. For binary data with given mean activations  X  i =  X  s i  X  and correlations between neurons  X  ij =  X  s i s j  X   X   X  s i  X  X  s j  X  , one obtains a quadratic exponential probability mass function known as the Ising model in physics or as the Boltzmann machine in machine learning.
 Currently all methods used to determine the parameters of such binary maximum entropy models suffer from the same drawback: since the parameters do not correspond directly to any of the mea-sured statistics, they have to be inferred (or  X  X earned X ) from data. In high dimensions though, this poses a difficult computational problem. Therefore the characterization of complete neural circuits with possibly hundreds of neurons is still out of reach, even though analysis was recently extended to up to forty neurons [14].
 To make the maximum entropy approach feasible in high dimensions, we propose a new strategy: Sampling from a  X  X ear-maximum X  entropy model that does not require any complicated learning of parameters. In order to justify this approach, we verify empirically that the entropy of the full probability distributions obtained with the near-maximum entropy model are indistinguishable from those obtained with classical methods such as Gibbs sampling for up to 20 dimensions. 2.1 Boltzmann machine learning For a binary vector of neural activities s  X  X  X  1 , 1 } m and specified  X  i and  X  ij the Ising model takes the form  X  s  X  X  s j  X  =  X  ij . Unfortunately, finding the correct parameters turns out to be a difficult problem which cannot be solved in closed form.
 Therefore, one has to resort to an optimization approach to learn the model parameters h i and J ij from data. This problem is called Boltzmann machine learning and is based on maximization of the likelihood can be computed in terms of the empirical covariance and the covariance of s i and s j as produced by the current model: The second term on the right hand side is difficult to compute, as it requires sampling from the model. Since the partition function Z in Eq. (1) is not available in closed form, Monte-Carlo methods such Figure 3: Random samples of dichotomized 4x4 patches from the van Hateren image data base (left) and from as Gibbs sampling are employed [9] in order to approximate the required model average. This is computationally demanding as sampling is necessary for each individual update. While efficient sampling algorithms exist for special cases [6], it still remains a hard and time consuming problem in the general case. Additionally, most sampling algorithms do not come with guarantees for the quality of the approximation of the required average. In conclusion, parameter fitting of the Ising model is slow and oftentimes painstaking, especially in high dimensions. 2.2 Modeling with the dichotomized Gaussian Here we explore an intriguing alternative to the Monte-Carlo approach: We replace the Ising model by a  X  X ear-maximum X  entropy model, for which both parameter computation and sampling is easy. A very convenient, but in this context rarely recognized, candidate model is the dichotomized Gaussian distribution (DG) [11, 5, 4]. It is obtained by supposing that the observed binary vector s is generated from a hidden Gaussian variable Without loss of generality, we can assume unit variances for the Gaussian, i.e.  X  ii = 1 , the mean  X  and the covariance matrix  X  of s are given by where  X ( x, y,  X  ) =  X  2 ( x, y,  X  )  X   X ( x ) X ( y ) . Here  X  is the univariate standardized cumulative Gaussian distribution and  X  2 its bivariate counterpart. While the computation of the model param-eters was hard for the Ising model, these equations can be easily inverted to find the parameters of the hidden Gaussian distribution: Determining  X  ij generally requires to find a suitable value such that  X  ij  X  4 X (  X  i ,  X  j ,  X  ij ) = 0 . This can be efficently solved by numerical computations, since the function is monotonic in  X  ij and has a unique zero crossing. We obtain an especially easy case, when  X  i =  X  j = 0 , as then  X  It is also possible to evaluate the probability mass function of the DG model by numerical integra-tion, where the integration limits are chosen as a i = 0 and b i =  X  , if s i = 1 , and a i =  X  X  X  and b i = 0 , otherwise.
 In summary, the proposed model has two advantages over the traditional Ising model: (1) Sampling is easy, and (2) finding the model parameters is easy too. In the previous section we introduced the dichotomized Gaussian distribution. Our conjecture is that in many cases it can serve as a convenient approximation to the Ising model. Now, we investigate how good this approximation is. For a wide range of interaction terms and mean activations we verify that the DG model closely resembles the Ising model. In particular we show that the entropy of the DG distribution is not smaller than the entropy of the Ising model even at rather high dimensions. 3.1 Random Connectivity We created randomly connected networks of varying size m , where mean activations h i and interactions terms J ij were drawn from N (0 , 0 . 4) . First, we compared the entropy H I =  X  P s P I ( s ) log 2 P I ( s ) of the thus specified Ising model obtained by evaluating Eq. 1 with the en-tropy of the DG distribution H DG computed by numerical integration 1 from Eq. 6 (twenty parameter sets). The entropy difference  X  H = H I  X  H DG was smaller than 0 . 002 percent of H I (Fig. 1 A, note scale) and probably within the range of the numerical integration accuracy. In addition, we computed the Jensen-Shannon divergence D JS [ P I k P DG ] = 1 2 ( D KL [ P I k M ] + D KL [ P DG k M ]) , where M = 1 2 ( P I + P DG ) [8]. We find that D JS [ P I k P DG ] is extremly small up to 10 dimensions (Fig. 1 B). Therefore, the distributions seem to be not only close in their respective entropy, but also to have a very similar structure.
 Next, we extended this analysis to networks of larger size and repeated the same analysis for up to twenty dimensions. Since the integration in Eq. 6 becomes too time-consuming for m  X  20 due to the large number of states, we used a histogram based estimate of P DG (using 3  X  10 6 samples for m &lt; 15 and 15  X  10 6 samples for m  X  15 ). The estimate of  X  H is still very small at high dimensions (Fig. 1 C, below 0 . 5% ). We also computed D JS , which scaled similarly to  X  H (data not shown).
 In Fig. 1 C,  X  H seems to increase with dimensionality. Therefore, we investigated how the estimate of  X  H is influenced by the number of samples used. We computed both quantities for varying num-bers of samples from the DG distribution (for m = 7 , 10 ). As  X  H decreases according to a power law with increasing m , the rise of  X  H observed in Fig. 1 C is most likely due to undersampling of the distribution. 3.2 Specified covariance structure To explore the relationship between the two techniques more systematically, we generated covari-ance matrices with varying eigenvalue spectra. We used a parametric Toeplitz form, where the n th diagonal is set to a constant value exp(  X   X   X  n ) (Fig. 2A and B, m = 7 , 10 ). We varied the decay parameter  X  , which led to a widely varying covariance structure (For eigenvalue spectra, see Fig. 2E and F). We fit the Ising models using the Boltzmann machine gradient descent procedure. The co-variance matrix of the samples drawn from the Ising model resembles the original very closely (Fig. 2C and D). We also computed the entropy of the DG model using the desired covariance structure. We estimated  X  H and D JS [ P G k P DG ] averaged over 10 trials with 10 5 samples obtained by Gibbs sampling from the Ising model.  X  H is very close to zero (Fig. 2G, m = 7 ) except for small  X  s and never exceeded 0 . 05% . Moreover, the structure of both distributions seems to be very similar as well (Fig. 2H, m = 7 ). At m = 10 , both quantities scaled qualitatively similair (data not shown). We also repeated this analysis using equations 1 and 6 as before, which lead to similar results (data not shown).
 Our experiments demonstrate clearly that the dichotomized Gaussian distribution constitutes a good approximation to the quadratic exponential distribution for a large parameter range. In the following section, we will exploit the similarity between the two models to study how the role of second-order correlations may change between low-dimensional and high-dimensional statistics in case of natural images. We now investigate to which extent the statistics of natural images with dichotomized pixel inten-sities can be characterized by pairwise correlations only. In particular, we would like to know how the role of pairwise correlations opposed to higher-order correlations changes depending on the di-mensionality. Thanks to the DG model introduced above, we are in the position to study the effect of pairwise correlations for high-dimensional binary random variables ( N  X  1000 or even larger). We use the van Hateren image database in log-intensity scale, from which we sample small image patches at random positions. The threshold for the dichotomization is set to the median of pixel intensities. That is, each binary variable encodes whether the corresponding pixel intensity is above or below the median over the ensemble. Up to patch sizes of 4  X  4 pixel, the true joint statistics can be assessed using nonparametric histogram methods. Before we present quantitative comparisons, it is instructive to look at random samples from the true distribution (Fig. 3, left), from the DG model with same mean and covariance (Fig. 3, middle), and from the corresponding independent model (Fig. 3, right). By visual inspection, it seems that the DG model fits the true distribution well. In order to quantify how well the DG model matches the true distribution, we draw two independent sets of samples from each ( N = 2  X  10 6 for each set) and generate a scatter plot as shown in Fig. 4 A for 4  X  4 image patches. Each dot corresponds to one of the 2 16 = 65536 possible different binary patterns. The relative frequencies of these patterns according to the DG model (red dots) and according to the independent model (blue dots) are plotted against the relative frequencies obtained from the natural image patches. The solid diagonal line corresponds to a perfect match between model and ground truth. The dashed lines enclose the regions within which deviations are to be expected due to the finite sampling size. Since most of the red dots fall within this region, the DG model fits the data distribution very well.
 We also systematically evaluated the JS-divergence and the multi-information I [ S ] = P k H [ S k ]  X  H [ S ] as a function of dimensionality. That is, we started with the bivariate marginal distribution of two randomly selected pixels. Then we incrementally added more pixels of random location until the random vector contains all the 16 pixels of the 4  X  4 image patches. Independent of the dimension, the JS-divergence between the DG model and the true distribution is smaller than 0.015 bits. For comparison, the JS-divergence between the independent model and the true distribution increases with dimensionality from roughly 0.2 bits in the case of two pixels up to 0.839 bits in the case of 16 pixels. For two independent sets of samples both drawn from natural image data the JS-divergence ranges between 0.006 and 0.007 bits for 4  X  4 patches setting the gold standard for the minimal possible JS-divergence one could achieve with any model due to finite sampling size. Carrying out the same type of analysis as in [12], we make qualitatively the same observations as it was reported there: as shown above, we find a quite accurate match between the two distributions. Figure 5: Random samples of dichotomized 32x32 patches from the van Hateren image data base (left) and Furthermore, the multi-information of the DG model (red solid line) and of the true distribution (blue dots) increases linearly on a loglog-scale with the number of dimensions (Fig. 4 B). Both findings can be verified only up to a rather limited number of dimensions (less than 20). Nevertheless, in [12], two claims about the higher-dimensional statistics have been based on these two observations: First, that pairwise correlations may be sufficient to determine the full statistics of binary responses, and secondly, that the convergent scaling behavior in the log-log plot may indicate a transition towards strong order.
 Using natural images instead of retinal ganglion cell data, we would like to verify to what extent the low-dimensional observations can be used to support these claims about the high-dimensional statistics [10]. To this end we study the same kind of extrapolation (Fig. 4 B) to higher dimensions (dashed lines) as in [12]. The difference between the entropy of the independent model and the multi-information yields the joint entropy of the respective distribution. If the extrapolation is taken seriously, this difference seems to vanish at the order of 50 dimensions suggesting that the joint entropy of the neural responses approaches zero at this size X  X ay for 7  X  7 image patches (Fig. 4 C). Though it was not taken literally, this point of  X  X reezing X  has been pointed out in [12] as a critical network size at which a transition to strong order is to be expected. The meaning of this assertion, however, is not clear. First of all, the joint entropy of a distribution can never be smaller than the joint entropy of any of its marginals. Therefore, the joint entropy cannot decrease with increasing number of dimensions as the extrapolation would suggest (Fig. 4 C). Instead it would be necessary to ask more precisely how the growth rate of the joint entropy can be characterized and whether there is a critical number of dimensions at which the growth rate suddenly drops. In our study with natural images, visual inspection does not indicate anything special to happen at the  X  X ritical patch size X  of 7  X  7 pixels. Rather, for all patch sizes, the DG model yields dichotomized pink noise. In Fig. 5 (right) we show a sample from the DG model for 32  X  32 image patches (i.e. 1024 dimensions) which provides no indication for a particularly interesting change in the statistics towards strong order. The exact law according to which the multi-information grows with the number of dimensions for large m , however, is not easily assessed and remains to be explored.
 Finally, we point out that the sufficiency of pairwise correlations at the level of m = 16 dimensions does not hold any more in the case of large m : the samples from the true distribution at the left hand side of Fig. 5 clearly show much more structure than the samples from the DG model (Fig. 5, right), indicating that pairwise correlations do not suffice to determine the full statistics of large image patches. Even if the match between the DG model and the Ising model may turn out to be less accurate in high dimensions, this would not affect our conclusion. Any mismatch would only introduce more order in the DG model than justified by pairwise correlations only. We proposed a new approach to maximum entropy modeling of binary variables, extending maxi-mum entropy analysis to previously infeasible high dimensions: As both sampling and finding pa-rameters is easy for the dichotomized Gaussian model, it overcomes the computational drawbacks of Monte-Carlo methods. We verified numerically that the empirical entropy of the DG model is com-parable to that obtained with Gibbs sampling at least up to 20 dimensions. For practical purposes, the DG distribution can even be superior to the Gibbs sampler in terms of entropy maximization due to the lack of independence between consecutive samples in the Gibbs sampler.
 Although the Ising model and the DG model are in principle different, the match between the two turns out to be surprisingly good for a large region of the parameter space. Currently, we are trying to determine where the close similarity between the Ising model and the DG model breaks down. In addition, we explore the possibility to use the dichotomized Gaussian distribution as a proposal density for Monte-Carlo methods such as importance sampling. As it is a very close approximation to the Ising model, we expect this combination to yield highly efficient sampling behaviour. In summary, by linking the DG model to the Ising model, we believe that maximum entropy modeling of multivariate binary random variables will become much more practical in the future.
 We used the DG model to investigate the role of second-order correlations in the context of sen-sory coding of natural images. While for small image patches the DG model provided an excellent fit to the true distribution, we were able to show that this agreement breakes down in the case of larger image patches. Thus caution is required when extrapolating from low-dimensional mea-surements to higher-dimensional distributions because higher-order correlations may be invisible in low-dimensional marginal distributions. Nevertheless, the maximum entropy approach seems to be a promising tool for the analysis of correlated neural activities, and the DG model can facilitate its use significantly in practice.
 Acknowledgments We thank Jakob Macke, Pierre Garrigues, and Greg Stephens for helpful comments and stim-ulating discussions, as well as Alexander Ecker and Andreas Hoenselaar for last minute ad-vice. An implementation of the DG model in Matlab and R will be avaible at our website http://www.kyb.tuebingen.mpg.de/bethgegroup/code/DGsampling .

