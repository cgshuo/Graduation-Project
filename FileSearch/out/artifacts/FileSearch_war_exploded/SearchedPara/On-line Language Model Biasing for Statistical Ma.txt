
While much of the focus in developing a statistical machine translation (SMT) system revolves around the translation model (TM), most systems do not emphasize the role of the language model (LM). The latter generally follows a n -gram structure and is es-timated from a large, monolingual corpus of target sentences. In most systems, the LM is independent of the test input, i.e. fixed n -gram probabilities de-termine the likelihood of all translation hypotheses, regardless of the source input.
 Some previous work exists in LM adaptation for SMT. Snover et al. (2008) used a cross-lingual infor-mation retrieval (CLIR) system to select a subset of target documents  X  X omparable X  to the source docu-ment; bias LMs estimated from these subsets were interpolated with a static background LM. Zhao et al. (2004) converted initial SMT hypotheses to queries and retrieved similar sentences from a large monolingual collection. The latter were used to build source-specific LMs that were then interpo-lated with a background model. A similar approach was proposed by Kim (2005). While feasible in off-line evaluations where the test set is relatively static, the above techniques are computationally expensive and therefore not suitable for low-latency, interac-tive applications of SMT. Examples include speech-to-speech and web-based interactive translation sys-tems, where test inputs are user-generated and pre-clude off-line LM adaptation.

In this paper, we present a novel technique for weighting a LM corpus at the sentence level based on the source language input. The weighting scheme relies on a measure of cross-lingual similarity evalu-ated by projecting sparse vector representations of the target sentences into the space of source sen-tences using a transformation matrix computed from the bilingual parallel data. The LM estimated from this weighted corpus boosts the probability of rele-vant target n -grams, while attenuating unrelated tar-get segments. Our formulation, based on simple ideas in linear algebra, alleviates run-time complex-ity by pre-computing the majority of intermediate products off-line.
 We propose a novel measure of cross-lingual simi-larity that evaluates the likeness between an arbitrary pair of source and target language sentences. The proposed approach represents the source and target sentences in sparse vector spaces defined by their corresponding vocabularies, and relies on a bilingual projection matrix to transform vectors in the target language space to the source language space.
Let S = { s resent the source and target language vocabularies. Let u represent the candidate source sentence in a M -dimensional vector space, whose m th dimension u sentence. Similarly, v represents the candidate tar-get sentence in a N -dimensional vector space. Thus, u and v are sparse term-frequency vectors. Tra-ditionally, the cosine similarity measure is used to evaluate the likeness of two term-frequency repre-sentations. However, u and v lie in different vector spaces. Thus, it is necessary to find a projection of v in the source vocabulary vector space before sim-ilarity can be evaluated.

Assuming we are able to compute a M  X  N -dimensional bilingual word co-occurrence matrix  X  from the SMT parallel corpus, the matrix-vector product  X u =  X v is a projection of the target sen-tence in the source vector space. Those source terms of the M -dimensional vector  X u will be emphasized that most frequently co-occur with the target terms in v . In other words,  X u can be interpreted as a  X  X ag-of-words X  translation of v .

The cross-lingual similarity between the candi-date source and target sentences then reduces to the cosine similarity between the source term-frequency vector u and the projected target term-frequency vector  X u , as shown in Equation 2.1:
In the above equation, we ensure that both u and  X u are normalized to unit L over-or under-estimation of cross-lingual similarity due to sentence length mismatch.

We estimate the bilingual word co-occurrence matrix  X  from an unsupervised, automatic word alignment induced over the parallel training corpus P . We use the GIZA++ toolkit (Al-Onaizan et al., 1999) to estimate the parameters of IBM Model 4 (Brown et al., 1993), and combine the forward and backward Viterbi alignments to obtain many-to-many word alignments as described in Koehn et al. (2003). The ( m, n ) th entry  X  the number of times source word s word t In traditional LM training, n -gram counts are evalu-ated assuming unit weight for each sentence. Our approach to LM biasing involves re-distributing these weights to favor target sentences that are  X  X im-ilar X  to the candidate source sentence according to the measure of cross-lingual similarity developed in Section 2. Thus, n -grams that appear in the trans-lation hypothesis for the candidate input will be as-signed high probability by the biased LM, and vice-versa.

Let u be the term-frequency representation of the candidate source sentence for which the LM must be biased. The set of vectors { v represent the K target LM training sentences. We compute the similarity of the source sentence u to each target sentence v
The biased LM is estimated by weighting n -gram counts collected from the j th target sentence with the corresponding cross-lingual similarity  X  ever, this is computationally intensive because: (a) LM corpora usually consist of hundreds of thou-sands or millions of sentences;  X  uated at run-time for each of them, and (b) the entire LM must be re-estimated at run-time from n -gram counts weighted by sentence-level cross-lingual similarity.

In order to alleviate the run-time complexity of on-line LM biasing, we present an efficient method for obtaining biased counts of an arbitrary target n -gram t . We define c the indicator-count vector where c j ased count of t in target sentence j . Let  X  = [  X  1 , . . . ,  X  K ] lingual similarity between the candidate source sen-tence and each of the K target sentences. Then, the biased count of this n -gram, denoted by C  X  ( t ) , is given by Equation 3.2:
The vector b of target n -gram t in the source space. Note that b independent of the source input u , and can therefore be pre-computed off-line. At run-time, the biased count of any n -gram can be obtained via a simple dot product. This adds very little on-line time com-plexity because u is a sparse vector. Since b nically a dense vector, the space complexity of this approach may seem very high. In practice, the mass of b t is concentrated around a very small number of source words that frequently co-occur with target n -gram t ; thus, it can be  X  X parsified X  with little or no loss of information by simply establishing a cutoff threshold on its elements. Biased counts and proba-bilities can be computed on demand for specific n -grams without re-estimating the entire LM. We measure the utility of the proposed LM bias-ing technique in two ways: (a) given a parallel test corpus, by comparing source-conditional target per-plexity with biased LMs to target perplexity with the static LM, and (b) by comparing SMT performance with static and biased LMs. We conduct experi-ments on two resource-poor language pairs commis-sioned under the DARPA Transtac speech-to-speech translation initiative, viz. English-Dari (E2D) and English-Pashto (E2P), on test sets with single as well as multiple references.
 Data set E2D E2P TM Training 138k pairs 168k pairs LM Training 179k sentences 302k sentences Development 3,280 pairs 2,385 pairs Test (1-ref) 2,819 pairs 1,113 pairs Test (4-ref) -564 samples 4.1 Data Configuration Parallel data were made available under the Transtac program for both language pairs evaluated in this pa-per. We divided these into training, held-out devel-opment, and test sets for building, tuning, and evalu-ating the SMT system, respectively. These develop-ment and test sets provide only one reference trans-lation for each source sentence. For E2P, DARPA has made available to all program participants an additional evaluation set with multiple (four) refer-ences for each test input. The Dari and Pashto mono-lingual corpora for LM training are a superset of tar-get sentences from the parallel training corpus, con-sisting of additional untranslated sentences, as well as data derived from other sources, such as the web. Table 1 lists the corpora used in our experiments. 4.2 Perplexity Analysis For both Dari and Pashto, we estimated a static trigram LM with unit sentence level weights that served as a baseline. We tuned this LM by varying the bigram and trigram frequency cutoff thresholds to minimize perplexity on the held-out target sen-tences. Finally, we evaluated test target perplexity with the optimized baseline LM.

We then applied the proposed technique to es-timate trigram LMs biased to source sentences in the held-out and test sets. We evaluated source-conditional target perplexity by computing the to-tal log-probability of all target sentences in a par-allel test corpus against the LM biased by the cor-responding source sentences. Again, bigram and trigram cutoff thresholds were tuned to minimize source-conditional target perplexity on the held-out set. The tuned biased LMs were used to compute source-conditional target perplexity on the test set.
Witten-Bell discounting was used for smoothing all LMs. Table 2 summarizes the reduction in target perplexity using biased LMs; on the E2D and E2P single-reference test sets, we obtained perplexity re-ductions of 12.3% and 11.3%, respectively. This in-dicates that the biased models are significantly better predictors of the corresponding target sentences than the static baseline LM. 4.3 Translation Experiments Having determined that target sentences of a parallel test corpus better fit biased LMs estimated from the corresponding source-weighted training corpus, we proceeded to conduct SMT experiments on both lan-guage pairs to demonstrate the utility of biased LMs in improving translation performance.
 We used an internally developed phrase-based SMT system, similar to Moses (Koehn et al., 2007), as a test-bed for our translation experiments. We used GIZA++ to induce automatic word alignments from the parallel training corpus. Phrase translation rules (up to a maximum source span of 5 words) were extracted from a combination of forward and backward word alignments (Koehn et al., 2003). The SMT decoder uses a log-linear model that com-bines numerous features, including but not limited to phrase translation probability, LM probability, and distortion penalty, to estimate the posterior proba-bility of target hypotheses. We used minimum error rate training (MERT) (Och, 2003) to tune the feature weights for maximum BLEU (Papineni et al., 2001) on the development set. Finally, we evaluated SMT performance on the test set in terms of BLEU and TER (Snover et al., 2006).
 The baseline SMT system used the static trigram LM with cutoff frequencies optimized for minimum perplexity on the development set. Biased LMs (with n -gram cutoffs tuned as above) were estimated for all source sentences in the development and test Test set BLEU 100-TER E2D-1ref-tst 14.4 14.8 29.6 30.5 E2P-1ref-tst 13.0 13.3 28.3 29.4 E2P-4ref-tst 25.6 26.1 35.0 35.8 sets, and were used to decode the corresponding in-puts. Table 3 summarizes the consistent improve-ment in BLEU/TER across multiple test sets and language pairs. Existing methods for target LM biasing for SMT rely on information retrieval to select a comparable subset from the training corpus. A foreground LM estimated from this subset is interpolated with the static background LM. However, given the large size of a typical LM corpus, these methods are unsuitable for on-line, interactive SMT applications.

In this paper, we proposed a novel LM biasing technique based on linear transformations of target sentences in a sparse vector space. We adopted a fine-grained approach, weighting individual target sentences based on the proposed measure of cross-lingual similarity, and by using the entire, weighted corpus to estimate a biased LM. We then sketched an implementation that improves the time and space ef-ficiency of our method by pre-computing and  X  X par-sifying X  n -gram projections off-line during the train-ing phase. Thus, our approach can be integrated within on-line, low-latency SMT systems. Finally, we showed that biased LMs yield significant reduc-tions in target perplexity, and consistent improve-ments in SMT performance.

While we used phrase-based SMT as a test-bed for evaluating translation performance, it should be noted that the proposed LM biasing approach is in-dependent of SMT architecture. We plan to test its effectiveness in hierarchical and syntax-based SMT systems. We also plan to investigate the relative usefulness of LM biasing as we move from low-resource languages to those for which significantly larger parallel corpora and LM training data are available.
