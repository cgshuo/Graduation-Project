 ORIGINAL PAPER Dominique Rivard  X  Eric Granger  X  Robert Sabourin Abstract Some of the fundamental problems faced in the design of signature verification (SV) systems include the potentially large number of input features and users, the lim-ited number of reference signatures for training, the high intra-personal variability among signatures, and the lack of forgeries as counterexamples. In this paper, a new approach for feature selection is proposed for writer-independent (WI) off-line SV. First, one or more preexisting techniques are employed to extract features at different scales. Multiple feature extraction increases the diversity of information produced from signature images, allowing to produce signa-ture representations that mitigate intra-personal variability. Dichotomy transformation is then applied in the resulting feature space to allow for WI classification. This alleviates thechallengesofdesigningoff-lineSVsystemswithalimited number of reference signatures from a large number of users. Finally, boosting feature selection is used to design low-cost classifiers that automatically select relevant features while training. Using this global WI feature selection approach allows to explore and select from large feature sets based on knowledge of a population of users. Experiments performed with real-world SV data comprised of random, simple, and skilledforgeriesindicatethattheproposedapproachprovides a high level of performance when extended shadow code and directional probability density function features are extracted at multiple scales. Comparing simulation results to those of off-line SV systems found in literature confirms the viabil-ity of the new approach, even when few reference signatures are available. Moreover, it provides an efficient framework for designing a wide range of biometric systems from lim-ited samples with few or no counterexamples, but where new training samples emerge during operations.
 Keywords Biometrics  X  Handwriting recognition  X  Writer-independent signature verification  X  Feature extraction  X  Feature selection  X  Boosting  X  Decision tree classification  X  Incremental learning 1 Introduction Biometricshasemergedfromitsextensiveuseinlawenforce-ment and forensic sciences and is increasingly being adopted in a wide variety of civilian applications for enhanced secu-rity and privacy [ 1 ]. Biometric systems perform the recog-nition of individuals based on their physiological (i.e., face and fingerprint traits) and behavioral (i.e., voice print and handwritten signature) characteristics. Biometric traits are intrinsic to a person, and as such cannot be lost, stolen, or forgotten as with security tokens and secret knowledge [ 2 ]. Among the numerous biometric traits considered so far, handwritten signatures have long been established as one of the most widespread means for authenticating a person X  X  identity by administrative and financial institutions. The pro-cedure for acquisition of signature samples is familiar and noninvasive [ 3 ].

Biometric systems provide three recognition functions: identification, screening, and verification. Identification seeks to establish a person X  X  identity by matching his biometric sample against all user templates in the system database. Screening discreetly determines whether the bio-metric sample of an individual whose enrollment procedure is not typically well-defined matches any of the system X  X  watch list of identities. Finally, verification authenticates the claimed identity of an individual by comparing his biometric sample to his template stored in the system database [ 4 ].
Given its behavioral nature, one of the main difficulties in handwritten signature verification is that an individual X  X  signature can vary significantly from one sample to the next. In addition, a forger may attempt to reproduce signatures to bypass the system. Forgeries are usually divided into three types X  X andom, simple, and simulated. A random forgery occurs when the forger does not know both the writer X  X  name and the signature X  X  morphology. It can also happen when a genuine signature presented to the system is mislabeled to another user. When the forger knows the writer X  X  name but not the signature X  X  morphology, the forger can only produce a simple forgery using a style of writing of his liking. A sim-ulated forgery occurs when the forger has access to a sample to produce a reasonable imitation of the genuine signature.
Features extracted from handwritten signatures are broadly divided into two categories, static and dynamic, according to the acquisition method [ 5 ]. Static features are extracted by an off-line acquisition device after the writ-ing process has been completed, while dynamic features are extracted by an online acquisition device during the writing process. By extension, automatic signature verification sys-tems are either off-line or online. In either case, a neural or statistical classifier applied to signature verification is often trained using a limited number of training samples collected from a complex underlying distribution.

Two approaches have been proposed for off-line signa-ture verification X  X riter-dependent (WD) and writer-inde-pendent (WI). The former approach models the signature of each individual from his samples, and a specialized clas-sifier is trained for each writer. The WI approach uses a classifier to match each input questioned signature to one or more reference signatures, and a single classifier is trained for all writers [ 6 ]. Most automatic signature verification sys-tems found in literature follow the WD approach. However, as in most biometric applications, the performance of these systems declines with large numbers of users and with lim-ited number of reference signatures per user. For instance, in off-line verification of bank cheque signatures, the number of bank customers can easily reach the tens of thousands. In most cases, acquiring a sufficient number of reference signatures from each writer is not practical.

In contrast, the WI approach employs the dichotomy transformation to alleviate the difficulties of designing a verification system with a limited number of reference signa-tures from a large number of users. Off-line SV systems that follow the WI approach are designed in a space that is rep-resentative of the domain according to a population of writ-ers (using some learning or development database) and hold several practical advantages. For one, this approach allows to exploit a system with only one signature per user. In addi-tion, since input feature vectors are transformed into a dis-tance space between signatures, the number of users is of little consequence. The signature of all writers is authen-ticated using a single two-class classifier. This classifier should be trained from a sufficient set of previously col-lected genuine signatures, although writers populating this learning set do not necessarily need to be enrolled to the system used during operations. The underlying hypothesis is that these dataset signatures are representative of the entire population of legitimate users enrolled to the verification system.

Several studies suggests that the accuracy and reliability of a biometric system can be improved by integrating the evi-dence obtained from multiple different sources of informa-tion [ 2 ]. Systems that employ multiple techniques to extract different feature types at multiple resolutions or scales may uncover diversified information from a given biometric sam-ple. Since useful information may go undetected by using a single feature type and scale, these systems may improve the overall recognition rate [ 7 ]. In this respect, handwrit-ten signature is a promising candidate since several power-ful feature extraction techniques have been proposed in the literature [ 5 , 8 ]. Biometric sources of information are typi-cally integrated at the sensor (raw biometric data), feature, score, and decision levels. Since the features extracted from sensor measurements contain richer information content about a biometric trait than scores, integration at the feature level should provide higher level of accuracy than at other levels.

This paper presents a novel approach for feature selection that is effective for the design of WI off-line signature verification systems. It is based on the combination of multiple feature extraction, dichotomy transformation, and boosting feature selection. Multiple feature extraction is adopted to extract several diverse handwritten signature rep-resentations from a signature image, using one or more preexisting feature extraction techniques at different lev-els of resolution or scales. Even though the approach applies with a wide range of feature extraction techniques (c.f., [ 5 ]), this paper considers that the representation and analysis of signature images are achieved by extracting features at multiple grid scales using two well-known grid-based techniques X  X xtended shadow code (ESC) [ 9 ] and directional probability density functions (DPDF) [ 10 ]. While ESC extracts information about the spatial distribu-tion of the signature, DPDF extracts information about the orientation of the strokes. These feature extraction tech-niques are seen as complementary, and once combined into a single feature subset, they may provide a powerful multi-scale and spatio-directional representation of signa-tures.

In this paper, off-line signature verification is performed in a WI framework derived from a forensic document exami-nation approach [ 11 ] and compared to the performance of state-of-the-art results using a database composed of 168 writers. Writer-independence is achieved by the verifica-tion system using the dissimilarity between each questioned signature and the reference signatures. Using the multiple feature extraction and dichotomy transformation provides signature representations with a large number of distance features and may therefore reduce the impact on perfor-mance caused by intra-personal variability. However, fea-ture selection must be performed in the distance space to avoid the curse of dimensionality. Boosting feature selection is employed to efficiently select discriminant feature sub-sets from the potentially large number of features in the dis-tance space, while training the classifier [ 12 ]. By virtue of the global WI (dissimilarity-based) approach, the proposed system selects features with knowledge of a population of users, which is difficult to achieve with a WD (feature-based) approach.

The specialized WI feature selection approach proposed in this paper allows learning a discriminant representation space from a corpus of signatures (i.e., development data-base) sampled from an independent population of writers that are not enrolled to the system evaluated during opera-tions (or testing phases on some exploitation dataset). The approach proposed in this paper may be seen as an exten-sion of the one presented in [ 13 ]. However, the approach proposed in this paper is based on the selection of repre-sentation spaces, not on the selection of classifiers, and the fusion of information is performed at the feature level instead of at the confidence score level. Moreover, the approach in this paper does not require selecting the best grid scale or set of grid scales. Boosting feature selection allows to perform low-cost feature learning, and several image zones of differ-ent sizes may be selected, with extraction techniques that are well adapted to the type of projection, type of stroke direc-tions, etc. Finally, the dichotomy transformation employed with this approach allows to improve performance over time through incremental learning of new signature references and features during operations (exploitation phase), without hav-ingtoretrainasystemfromthestartonallcumulativetraining data.

The paper is organized as follows. Before presenting the multiple feature extraction and selection approach proposed in this paper (Sect. 3 ), Sect. 2 provides a survey of related WI systems that are suitable for off-line signature verification. In Sect. 4 , the experimental methodology, including datasets, protocols, and performance metrics, are defined. In Sect. 5 , simulation results are presented and discussed. 2 State-of-the-art in writer-independent signature verification This section presents the state-of-the-art in WI systems for off-line signature verification. As with the standard WD case, WI systems take as input handwritten signatures and output verification results. However, as depicted by Fig. 1 , when acquiring a new reference signature S r (e.g., during enroll-ment), the corresponding feature vector x r extracted from the signature image is stored for later use in the system X  X  knowl-edge base. In verification mode, the image of a questioned signature S q is presented to the system and its feature vector x , along with the reference set { x r } R 1 of signatures of the users enrolled to the knowledge base, are presented to the dichotomy transformation module. Then, the dichotomizer (two-class classifier) takes as input distance vectors { u each questioned signature S q and produces the correspond-ing set of confidence scores { f ( u r ) } R 1 that are combined to output a final decision g ( x q ) .

The rest of this section describes the dichotomy transfor-mation [ 14 ], followed by its application to SV [ 11 ]. Then, an approach to WI signature verification based on dichotomizer ensembles [ 13 ]isreviewed. 2.1 Dichotomy transformation A dichotomy transformation [ 14 ] allows to transform K -class pattern recognition problems where K is a large or unspecified value into a 2-class problem. In this context, the handwritten SV problem is formulated as follows. Given a reference signature and a questioned signature, the objective is to determine whether the two signatures were produced by the same writer. Formally, let x q and x r be two feature vectors from the feature domain labeled y q and y r , respectively, and let u r be the distance vector in the distance domain resulting from the dichotomy transformation: u where |  X  | is the absolute value. It is important to emphasize that each component of vector u r equals the distance between the corresponding components of vectors x q and x r , thus dis-tance vector and feature vectors have the same dimensional-ity. In the distance domain, independently of the number of writers, there are only two classes: the within class  X   X  and the between class  X  . The distance vector u r is assigned the label v r according to: v =
Intuitively, signatures from the same writer should be near one another in the feature space, thus clustering near the origin in distance space, whereas signatures from different writers should be distant from each other in the feature space and thus be scattered away from the origin in the distance space.

As for the number of distance vectors generated by the dichotomy transformation, if K writers provide a set of R references each, ( 1 ) generates up to KR 2 different distance vectors. Of these, K R 2 are of the within class and K 2 R of the between class. Thus, using a small sample of references from each writer, the dichotomy transformation generates an appreciable quantity of samples in the distance domain.
Figure 2 presents an example to illustrate the dichotomy transformation. Suppose a set of three writers, {  X  1 , X  2 and each writer provides three signatures. Some feature extraction technique produces a vector of two features ( x vectors of signatures into the feature space. The dichotomy transformation calculates the distance between the features of each pair of signatures to form vectors ( u 1 u 2 ) T in the distance space, as depicted in Fig. 2 b.

The dichotomy transformation affects the geometry of dis-tributions. In this example, multiple boundaries are needed to separate the three writers in the feature space as opposed to only one in the distance space. Also, the vectors in the distance space are always nonnegative since they consist of distances. Finally, the dichotomy transformation augments the number of samples in the distance space because they are made up of every pair of signatures.

To illustrate how the verification process is independent from the writer being verified, let x q , x r be a questioned and a reference feature vectors, respectively, both from new writer  X  4 . The dichotomy transformation ( 1 ) computes the distance vector u r from x q and x r . As it can be seen in the distance space (Fig. 2 b), that distance vector u r is located in the within region defined by the dichotomizer, which means that it authenticates both questioned and reference signatures as belonging to the same writer. On the other hand, the fea-ture space boundaries (Fig. 2 a) fail utterly by classifying one signature ( x q ) to writer  X  2 and the other ( x r ) to writer  X  . In fact, it is impossible for the feature domain model to adequately classify the signatures as belonging to writer since this writer did not contribute to the training set. Hence, the writer-independence is provided by the distance domain model.

One drawback of the dichotomy transformation is that perfectly clustered writers in feature domain may not be per-fectly dichotomized in distance domain. In other words, the broader the spread of the feature distributions among the writers, the less the dichotomizer is able to detect real differ-ences between similar signatures [ 14 ]. Thus, the performance of a dichotomizer is considerably affected by the choice of a feature set extracted from the handwritten signatures. More-over, since the dichotomy transformation affects the spatial geometry of distributions, the best feature set may not be the same in feature domain as in the distance domain. The approach proposed in this paper (see Sect. 3 ) seeks to extract a large set of potential features and efficiently selects a small set of discriminant features in the distance domain. 2.2 Extension of dichotomy transformation for questioned The Questioned Document Expert X  X  approach [ 11 ]isan extension to the dichotomy transformation that applies when users have more than one template stored in the knowledge base. The idea is to emulate the expert X  X  approach, which consists of comparing the questioned signature input to the SV system to a set of genuine signatures. Each comparison leads to a partial decision from the expert, his/her final deci-sion being based on all partial decisions. Intuitively, the more reference signatures that are available for comparison with the questioned signature, the more accurate the final decision will be.

Formally, the dichotomy transformation is applied bet-ween the questioned signature X  X  feature vector x q and the user X  X  reference set { x r } R 1 from the knowledge base and pro-duces the set of distance vectors { u r } R 1 . The dichotomizer evaluates each distance vector individually and outputs a set of confidence values { f ( u r ) } R 1 representing the partial deci-sions from the expert. The final decision of the system about the questioned signature is based on the fusion of all con-fidence values by a function g (  X  ) . The choice of the fusion function is dependent on the nature of the dichotomizer X  X  output. For instance, if the output of the dichotomizer is a label, then the majority vote is an appropriate fusion strat-egy. On the other hand, if the output of the dichotomizer is a probability, then a wider range of fusion strategies is avail-able such as the sum, mean, median, max, and min functions, to name a few. 2.3 Ensemble of writer-independent dichotomizers In [ 13 ], the original framework for WI off-line signature ver-ification is improved by replacing the dichotomizer by an ensemble of dichotomizers. To achieve ensemble diversity, support vector machines (SVMs) are trained on a learning set of 40 writers using 16 different scales of the segmen-tation grid during feature extraction. The same four grid-based feature extraction techniques of [ 11 ] are used except for stroke curvature information, which is extracted based on cubic Bezier curves. Thus, a pool of 64 SVMs is overpro-duced, from which a genetic algorithm chooses a subset of base classifiers to form the final ensemble of dichotomizers. These dichotomizers are combined at the confidence score level using the sum rule fusion strategy.

Different objective functions are applied with the genetic algorithm and the authors conclude that maximizing the area under the receiver operating characteristic (ROC) curve, or AUC, is the most suitable. In all cases, the fitness of the objective function is evaluated on an independent validation set of 20 writers. The influence of the number of reference signatures in the validation set is evaluated by increasing their numbers from 3 to 15, repeating the ensemble optimization every time. The authors conclude that the authentication rate depends on a trade-off between the number of references and the intra-class variability of the reference set.

Overall, authentication using an ensemble of dichotom-izers shows an improvement over 2-class classification by a single dichotomizer. However, this approach complicates the verification system, specifically in [ 13 ], ensembles count an average of 13 SVMs, using a total of 2,300 features and thus increasing the use of resources while reducing recognition speed. Moreover, the ensemble optimization process being stochastic in nature, each run may lead to a different ensem-ble, as demonstrated by their results. An improvement to the authentication rate is sought with the approach proposed in this paper (see Sect. 3 ). Results should improve by extract-ing different feature types at different scales and with clas-sifier ensembles where the measurements are integrated at the confidence score level. However, ensembles are typically more effective to integrate different sources of information as early as possible in the verification system [ 2 ]. Integrating combined information at feature level should yield improve-ments since at this level information about the signatures is richer. The WI approach proposed in this paper is based on the selection of representation spaces, and the fusion of infor-mation occurs at the feature level. Moreover, this approach does not require selecting the best grid scale or set of grid scales. Boosting feature selection allows to perform low-cost feature learning, and several image zones of different sizes may be selected. 3 A framework for multi-feature extraction and selection This section introduces a new approach for multi-feature extraction and selection that is efficient in WI off-line sig-nature verification. This novel system uses an ensemble of dichotomizers to combine features across several scales and feature extraction techniques, leading to low cost and accu-rate SV. As depicted in Fig. 3 , the proposed approach may be viewed in terms of the generic system for WI off-line SV shown in Fig. 1 .

However, as described in the following subsections, it uses (i) the multiple feature extraction at different scales and (ii) the boosting feature selection (BFS) technique for classification.

Multiple feature extraction can be applied to reference sig-natures S r and questioned signatures S q using one or more preexisting feature extraction techniques, producing a large set of features. In this paper, the representation of signature images is achieved by using two grid-based techniques X  ESC and DPDF. During a preliminary design phase, the BFS algorithm performs feature learning on a reference set of distance vectors and produces a committee of stumps, where each stump corresponds to a selected feature. During oper-ations, the committee of stumps takes as input the distance vectors { u r } R 1 for each questioned signature S q . It produces the corresponding set of the committee X  X  confidence scores, {
F ( u g ( x q ) . 3.1 Multiple feature extraction For handwritten signatures, it is important for the feature extraction process to be text insensitive. In other words, the measurements taken on signature must not rely on the seg-mentation of specific letters, which can be a very difficult task especially if the signature is highly personalized [ 9 ]. A practical alternative is to partition the signatures using a virtual grid and to take local measurements in each of the grid cells. By varying the scale of the virtual grid, purely global to very local features are extracted. In the literature, grid-based approaches generally tend to find a grid scale suitable to their signature database. Here, the proposed approach is to extract features at multiple scales and let the classifier select the most suitable ones.

When a new signature is used for enrollment or opera-tions, it is presented to the system as a gray-level image. From there, two preprocessing steps are necessary in order to pre-pare the signature for feature extraction. First, the signature is automatically segmented from its background using Otsu X  X  threshold selection method from gray-level histograms [ 15 ]. According to questioned document experts, the proportion and orientation of handwritten signatures are intrinsic char-acteristics of the writer when guided by a form [ 16 ]. Con-sequently, the second preprocessing step corrects the binary signature images in translation by aligning their centroid with the center of the feature extraction grid.

The extended shadow code (ESC) [ 9 , 17 ] consists in the superposition of a bar mask array over the binary image of a handwritten signature as depicted by Fig. 4 a. Each bar is assumed to be a light detector related to a spatially con-strained area of the 2D signal. A shadow projection is defined as the simultaneous projection of each black pixel into its closest horizontal, vertical, and diagonal bars. A projected shadow turns on a set of bits distributed uniformly along the bar. After all the pixels on a signature are projected, the num-ber of on bits in each bar is counted and normalized to the range [ 0 , 1 ] before features are extracted. Given a virtual grid composed of I rows by J columns, the cardinality of the ESC feature vector is equal to x ESC = 4 IJ + I + J . (3)
Directional probability density functions (DPDF) [ 10 ] have been used as a global shape factor for automatic off-line handwritten signature verification. The rationale of this approach is that the stroke orientation of handwritten signa-tures is stable enough to properly discriminate writers. Thus, this technique extracts features based on the frequency dis-tribution of the orientation of the gradient at the edge of the signature. Gradient features are used by other signature verification systems, for instance [ 18 ]. In this work, local DPDF are extracted from within each cell of a virtual grid placed over the handwritten signature image. This way, local information is extracted from different parts of the signature, consequently increasing its discriminating power. Moreover, the information extracted from the signature is complemen-tary to that extracted using the ESC technique. While the ESC extracts information about the spatial distribution of the signature, DPDF extracts information about the orien-tation of the strokes. Since the same grid scale is used for both techniques, this leads to a powerful spatio-directional representation of handwritten signatures.

The gradient is computed from the binarized version of the signature image after it has been smoothed using a Gaussian low-pass filter to reduce the impact that residual noise can have on the two key derivatives used for gradient computa-tion. Computing the gradient on a smoothed binary image has the definitive advantage that the intensity of the image is already normalized; consequently, there is no need of a threshold to detect the edges of the image. In fact, the back-ground segmentation process has already managed to detect the edges of the signature and thus, the remaining task is to determine their orientation. This work uses Sobel operators to compute the gradient key derivatives.

Figure 4 b illustrates the gradient of a handwritten signa-ture image using arrows to indicate its direction and magni-tude at each pixel location. Since the signature consists of a binary image, gradient is null in the background of the image and within the strokes of the signature where intensity is con-stant. Gradient is non-null along the edges of the signature and its direction varies perpendicularly to the contour of the signature.

In order to obtain a fixed number of features, gradient directions are quantized into an even number of ranges. A greater results into a more exact representation of the gradient of the signature, thus increasing between-writers discrimination. However, the more exact the representation, the more sensitive it is to intra-personal variance, thus low-ering generalization capabilities. As a trade-off, this work uses = 8. It is important to realize that a quantized value  X   X  1 , 2 ,..., value  X  + 2 . Thus, after quantization, gradient magnitudes are summed according to each stroke orientation for every individual cell of the virtual grid, leading to a feature vector of cardinality x
Note that both feature extraction techniques can be exe-cuted in parallel to reduce computation time. 3.2 Boosting feature selection Boosting is a machine-learning procedure, which combines the performance of many weak classifiers into a power-ful committee. The rationale behind boosting is that find-ing many moderately inaccurate rules of thumb using many simple classifiers can be easier than finding a single highly accurate prediction rule using a more elaborate learning algo-rithm. Boosting methods have proven to be very competitive in terms of generalization in a variety of applications [ 19 ]. The general idea of boosting is to form a committee of weak classifiers iteratively by adding one weak classifier at a time. At the beginning of the training procedure, a uniform weight-ing is assigned to the patterns of the training data set. Each time, a new classifier is added to the committee, the samples in the training data are re-weighted to reflect the performance of this weak classifier, assigning more importance to misclas-sified samples. Thus, the next weak classifier focuses on more difficult samples, and the procedure ends after a predefined number of weak classifiers have been trained.

The problem of feature selection is defined as follows: given a set of potential features, the objective is to select the best subset under some classification objectives. This proce-dure has three goals: (i) to reduce the cost of extracting fea-tures, (ii) to improve the classification accuracy, and (iii) to improve the reliability of the estimate of performance [ 20 ]. The boosting feature selection algorithm [ 12 ] (and further studied in [ 21 ]) explicitly incorporates feature selection into AdaBoost [ 22 ], the most commonly used variant of boosting. Boosting feature selection is performed by designing a weak classifier that selects the single most discriminant feature of a set of potential features and finds a threshold to separate the two classes to learn, effectively a decision stump. Conse-quently, features are selected in a greedy fashion according to the weighting while learning is conducted by the boosting algorithm. Given a very large set of features, the result is a committee built on the best subset of features representing the training data.
The next subsections describe the boosting algorithm and the weak classifier used in this work and are followed by a complexity study of the resulting committee. 3.2.1 Gentle AdaBoost The problem of handwritten signature verification can have a significant class overlap, especially between genuine signa-tures and simulated forgeries, and as mentioned previously, the dichotomy transformation can exacerbate this phenom-enon. Also, it has been observed by several authors that AdaBoost is not an optimal method on very noisy problems [ 23  X  25 ]. By design, Adaboost focuses on misclassified sam-ples and this may results in fitting the noise.

Several boosting methods address the overfitting prob-lem, mostly by adjusting the weighting scheme. For instance, MadaBoost [ 26 ] bounds the weight assigned to each sam-ple by its initial probability, Gentle AdaBoost [ 27 ] takes adaptive Newton steps to update the weights more slowly, BrownBoost [ 28 ] uses a non-monotone weighting function decreasing the weight of samples far from the margin, Ada-Boost  X  [ 24 ] and AdaBoost  X   X  [ 29 ] both use the concept of soft margin to regularize by allowing for misclassifications, SmoothBoost [ 25 ] constructs smooth distributions, which do not put too much weight on any single sample, and Nada-Boost [ 30 ] prevents high weight values by thresholding.
Moreover, validation sets have long been used in machine learning to limit overfitting, and, as noted by the authors of Adaboost [ 31 ], a validation set could be used for early stop-ping. This work makes use of Gentle Adaboost and early stopping to address the significant class overlap problem. Early stopping is implemented using a holdout validation set. The early stopping criterion is based on the maximiza-tion of area under the receiver operating characteristics curve (AUC) on the holdout validation set.

Receiver operating characteristics (ROC) curves are graphs plotting the true positive rate of a classifier in func-tion of its false positive rate. The points composing the curve are obtained by varying the decision threshold of the classi-fier (see Algorithm 1 of [ 32 ] for an efficient method for the generation of ROC points). ROC curves have an attractive property: they are insensitive to change in class distribution [ 32 ]. If the proportion of genuine signatures and forgeries changes between the design of a system and its exploitation, the ROC curves will not change. It is the case with signa-ture verification applications, as the proportions of fraud for real applications are likely to vary in time and from place to place. The AUC of a ROC curve has an important statistical property: it is equivalent to the probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance [ 32 ]. As such, the AUC is invariant to the decision threshold optimized by Gentle AdaBoost, which is a significant advantage in the context of this work since the decision threshold is learned using ran-dom forgeries as counterexamples when in fact the commit-tee is tested against random, simple, and simulated forgeries. Moreover, the decision threshold is a function of the priors and the classification cost, both of which are likely to vary in a signature verification application.

Figure 5 describes the Gentle AdaBoost algorithm with early stopping. Let L = { u l ,v l ,w l } L 1 be a learning set of L feature vectors u  X  R D labeled to v l  X  X  X  1 , 1 } and weighted by the distribution w l  X  X  0 , 1 ] , L l = 1 w l = 1. Similarly, let H = { u h ,v h } H 1 be a holdout validation set of H non-weighted samples. Let also T L , T H  X  N be the maximum iteration stopping criterion and the early stopping criterion, respectively. The first half of the algorithm implements the four steps of the Gentle AdaBoost algorithm: (i) train a new decision stump f t ( u ) based on L , (ii) add f t ( u ) to the com-mittee F ( u ) , (iii) update the weights w l according to the responseof f t ( u l ) ,and(iv)renormalizetheweightstoensure a distribution. The second half implements a holdout val-idation scheme using the AUC for criterion. The AUC is computed using Algorithm 2 described in [ 32 ]. If the algo-rithm reaches T H iterations without increase in the AUC, it early stops. The Gentle AdaBoost with early stopping pro-cedure outputs the committee F ( u ) composed of T decision stumps. 3.2.2 Decision stumps Decision trees classify a pattern through a sequence of ques-tions [ 33 ]. Each question tests a single feature of the data and is represented by a tree node, the first question being the root of the tree and each possible decision spanning a branch to new node (i.e., the next question) and so on until a terminal node, called a leaf, is reached and the pattern is classified. Each decision outcome is called a split since it effectively splits the data into subsets, binary decisions being referred to assinglesplitsandhighernumberofdecisionsasmulti-splits.
Decision stumps are one-level, single split trees [ 34 ]. A decision stump f t ( u ) is composed of four parameters: d dimension to split,  X  t the splitting threshold in that dimen-sion, and  X  left t and  X  right t the weighted means of the response for the left and right leaves, respectively. Figure 6 illustrates the decision stump learning algorithm. The first step in the algorithm is to compute W  X  , W the positive, and nega-tive weight totals, respectively. Then, the algorithm inde-pendently searches each problem dimension to find the best split point ( d t , X  t ) . For a given dimension, the samples are first sorted by increasing feature values. Then, the algorithm computes w  X  , w the positive, and negative weight cumula-tive distribution functions, respectively. Based on the CDFs and the weight totals, the splitting threshold is selected to minimize the probability that a training sample would be misclassified. Once the split point is optimized, the algorithm computes the weighted means of the response for both leaves.
When presented a sample u to classify, the decision stump f ( u ) thresholds the feature d ple to the corresponding leave. Formally: f ( u ) =
Decision stumps typically have high bias and low vari-ance. However, boosting algorithms are capable of reducing both bias and variance, hence the increase in performance from committee of stumps [ 27 ]. Moreover, when boosting implements a re-weighting strategy (as opposed to re-sam-pling), like it is the case for Gentle AdaBoost, decision stumps cause boosting to become deterministic in the sense that multiple runs on the same learning set will result in iden-tical committees. Also, the order of the presentation of the samples and of the features of the learning set do not affect the resulting committees. Finally, following ( 5 ), a decision stump classifies a pattern using a single feature. This means that boosted decision stumps will greedily select informative features while building the committee, ignoring redundant and irrelevant features. It is worth noting that the committee may learn several stumps based on the same feature, each with a different decision threshold and response.  X  X ppendix A X  presents a detailed analysis of the time com-plexity for feature learning of a learning dataset with BFS and for classification using a committee of decision stumps. Dur-ing operations, the total worst-case time required to classify a distance vector using the committee of stumps is O ( T ) , mak-ing for very fast classifications. When training with larger databases ( L T ) as considered in this research, the total average case time required to learn with Gentle AdaBoost and early stopping, including the time for quicksort, train-ing decision stumps, and computing the area under the ROC curve is O ( DLT ) , resulting in a fast learning algorithm that scales linearly. 4 Experimental methodology The objective of this experimental protocol is to assess and compare the performance of the new approach proposed in Sect. 3 for feature selection for WI off-line SV. WI verifica-tion implies that there is only one classifier for all writers. Therefore, the protocol employs two disjoint sets of writers for system design (during development phase) and system testing (during exploitation phase). The underlying hypothe-sis is that the set of writers used for training is representative of the set of writers encountered during exploitation.
In most real-world applications, few genuine signatures are available for each writer, and random forgeries are typi-cally the only type of counterexample available for designing a SV system. Consequently, only random forgeries were included in the training database, although the system is tested against random, simple, and simulated forgeries.
The following subsections describe the signature data-base, feature sets, and protocols used to evaluate perfor-mance. In fact, there is a generic protocol that consists in usinganincreasingnumberofreferencesignaturesperwriter, and this protocol is evaluated under different settings of fea-tures and their combination. These settings allow to evaluate performance for (1) single-scale representations, (2) infor-mation fusion at the feature extraction level, (3) information fusion at the confidence score level, and (4) fast incremental learning. 4.1 Signature database The signature database used in this work is composed of 168 writers divided into a 108 writer development database D and a 60 writer exploitation database E . As described in [ 13 ], the signatures were provided by 168 under-graduated students in four different sessions, ten samples at a time, once a week during one month, for a total of 40 genuine signatures per writer. The signatures were collected on a white A4 sheet of paper with no overlap and then scanned in gray level with 300 dpi. Regarding forgeries, ten people with no experience in producing forgeries were selected as forgers, to produce one simple and one simulated forgery for the 60 first writers. Simple forgeries were produced by supplying only the name of the writer to the forger. Simulated forgeries were produced by showing the forger four genuine signatures of the writer.
To build a WI classifier, distance vectors must be com-puted from the feature vectors using the dichotomy trans-formation as explained in Sect. 2.1 . The learning set L and holdout validation set H are both generated from the devel-opment database D . To do so, the 40 genuine signatures of each writer in D are partitioned into subsets of 30 and 10 signatures denoted D 1 and D 2 , respectively. The signatures populating D 1 are selected randomly.

The learning set L is generated using exclusively the gen-uine signatures of subset D 1 .The within class samples are computed using all genuine signatures from every writer, giv-ing 108  X  30  X  29 2 = 46 , 980 distance vectors. To generate an equivalent number of counterexamples, the dichotomy trans-formation is applied, for each writer, to 29 genuine signatures used as signatures of references against 15 random forgeries selected from the genuine signatures of 15 other writers. The result is 108  X  29  X  15 = 46 , 980 between class distance vec-tors. Thus, the learning set is defined as L = { u l ,v l with equiprobable priors.

The holdout validation set H is generated using the genu-ine signatures of subset D 1 as signatures of reference against the genuine signatures of subset D 2 . Since each writer has 30 references signatures in D 1 and 10 genuine signatures in the number of within class samples is equal to 108  X  30  X  32 , 400. To generate an equivalent number of counterexam-ples,foreachwriter,10randomforgeriesareselectedfrom10 different writers in D 2 . The random forgeries are compared to the 30 references from D 1 , giving 108  X  30  X  10 = 32 between class distance vectors. Thus, the holdout validation To perform a WI evaluation of the system, both reference R and questioned Q sets are generated from the exploitation database E whose writers are unknown to the verification system. The reference set R is composed of 30 randomly selected genuine signatures from each writer of the exploi-tation database E . Thus, the reference set is defined as { x remaining genuine signatures and simple and simulated forg-eries from each writer plus 10 random forgeries selected from the genuine signatures of 10 different writers. Thus, the ques-tioned set is defined as Q = x q , y q 2 4.2 Feature sets ESC feature vectors and DPDF vectors are extracted from both signature databases D and E . Resolution depends on the sizeoftheextractiongrid;thesmallerthegridcells,thehigher the resolution. The highest resolution used in this work is a cell of 20  X  20 pixels. Since the width of a stroke measures an average of 10 pixels, higher resolutions would results mostly in saturated cells and empty cells. On the other hand, the low-est resolution is limited by the size of the image. Since the signature images are 400 pixels high by 1,000 pixels wide, the lowest resolution consists of a single cell of that size. Let I ={ 1 , 2 , 5 , 10 , 20 } be a set of 5 horizontal scales defined by their number of grid rows and J ={ 1 , 3 , 6 , 12 , 25 be a set of 6 vertical scales defined by their number of grid columns. The Cartesian product I  X  J results in the 30 sin-gle scales used in this work. Finally, multiple features are achieved by combining feature sets from every scale, for a total of 15,457 and 14,744 features for ESC and gradient histogram, respectively, and a grand total of 30,201 features when both techniques are combined. 4.3 Single-scale representations In order to independently characterize both ESC and DPDF feature extraction techniques, single-scale committees are designed from the development database D according to the following protocol. For each available feature set, the Gentle AdaBoost algorithm with early stopping (see Fig. 5 ) builds a committee of decision stumps on the learning set L using the holdout validation set H to prevent overfitting. The early stopping criterion T H = 100 and the maximum iteration stopping criterion T L = 100 , 000.

The performance of the WI committees is evaluated with the exploitation database E , using reference signatures from new writers in the set R to authenticate the questioned sig-natures of set Q . To measure the impact of the cardinality of the reference set, reference subsets containing 1 , 3 ,..., randomly selected signatures are used for authentication. To simulate the effect of enrolling new signatures over time, previously selected reference signatures are kept and new signatures are added to increase the size of the references subsets. This procedure is repeated 100 times for variance estimation.

ROC analysis is used to compare the performance of the committees on the questioned set. Additionally, the commit-tees are evaluated using their error rates on the questioned set. To do so, the decision threshold minimizing the zero-one loss is used as it permits a characterization of the questioned set and also a comparison with previous systems. 4.4 Multi-feature information fusion at the feature Biometric sources of information are typically integrated at the sensor (raw biometric data), feature, score, and decision levels. Contrary to other WI systems [ 13 ], which implement information fusion at the confidence score level, the proposed system implements information fusion at the feature extrac-tion level. Since the features extracted from sensor measure-ments contain richer information content about a biometric trait than scores, integration at the feature level may provide higher level of accuracy. Three multi-feature committees are compared: (i) using a multi-feature feature set only from the ESC technique, (ii) using a multi-feature feature set only from the DPDF technique, and (iii) using a multiple feature set from both ESC and DPDF techniques. Multiple feature sets are achieved by concatenating appropriate single-scale feature sets. To permit a straightforward result comparison, training and evaluation protocols are the same as for single-scale feature set, as described previously. 4.5 Multi-feature information fusion at the confidence In order to compare the approach proposed in this paper to the overproduce and choose approach used in [ 13 ], the latter is shown here using the same feature extraction techniques and database partitions used in this paper. Since fusion is per-formed at the confidence score level, committees designed from single-scale feature sets are combined into an ensemble of committees by summing their individual confidence lev-els. Ensembles of committees are optimized using a genetic algorithm based on bit representation, one-point crossover, bit-flip mutation, and roulette wheel selection with elitism. Parameters are set as in [ 13 ], with a population = 100, number of generations = 300, probability of crossover = 0.7, and probability of mutation = 0.03. The chromosomes are composed of 60 bits, that is, one bit per single-scale com-mittee and for a given chromosome. Bits with a value of  X 1 X  indicate the selected committees. The fitness function is the maximization of the AUC on the holdout validation set H . Once an ensemble of committees is optimized, it is evaluated on the questioned set using the evaluation protocol described in Sect. 4.3 . This procedure is also repeated 100 times for variance estimation, thus allows direct comparison with the other approaches explored in this work. 4.6 Fast incremental learning In order to demonstrate the modularity of the proposed sys-tem, a third experimental protocol implements a fast incre-mental learning of handwritten signature representations. For instance, suppose that domain experts extract new represen-tations from the design database. As new signature represen-tations become available, they are learned incrementally by the verification system in order to increase its recognition rate.

A greedy incremental learning scheme is implemented to demonstrate the modularity of the proposed system. Single-scale signature representations are presented to the system one at a time. For each representation, a committee of stumps is built and combined with previous committees to form an ensemble of committees. Then, the AUC of the ensemble of committees is evaluated on the holdout validation set H and the newly added committee is kept if it improves the AUC of the ensemble or dismissed otherwise. The 60 single-scale representations are presented in random order and this procedure is replicated 100 times to evaluate the variance of the learning scheme.

Figure 7 details this experimental protocol. Given a sig-nature representation p , the learning set L p and holdout val-idation set H p are both generated from the design database D and the reference set R p and questioned set Q p are gen-erated from the exploitation database E whose writers are unknown to the verification system. Committees of stumps are the result of the Gentle AdaBoost algorithm with early stopping, described at Fig. 5 , and they are evaluated accord-ing to the evaluation protocol described in Sect. 4.3 .
Finally, Fig. 8 describes the experimental protocol designed to evaluate the impact of the quantity of signature representations on the boosted feature selection algorithm with early stopping. Let L , H , Q , R be the learning, hold-out, questioned, and references sets, respectively. They are all initialized as empty sets. Then, the 60 signature repre-sentations are randomly selected one at a time and added to the sets. When the sets contain 1, 5, 10, 15, 20, 25, 30, 40, 50, and 60 representations, the committee of stumps F p is built from sets L and H using the boosted feature selection algorithm with early stopping and then tested on set Q using 1, 3, 5, 7, 9, 11, 13, 15 references from set R . This protocol is repeated 10 times to show replicability. 5 Results and discussion 5.1 Single-scale representations First, the results from committees built on single-scale fea-ture sets are presented. Table 1 presents the committee size and ratio of features selected for single scale with the ESC representations. Since no committee has reached T L itera-tions, early stopping has occurred for every scale. The lower selection rate obtained at higher resolutions indicates the presence of redundant and irrelevant features. Best overall error rates from ESC representations are obtained at scale 2  X  3(seeTable 2 ). Results are presented as the mean error rate over 100 replications, along with one standard deviation (in parenthesis).

Table 3 presents the committee size and ratio of selected feature from single-scale DPDF representations. Again, no committee has reached T L iterations, and early stopping has occurred for every scale. When compared to ESC representation committees, the DPDF representation com-mittees are usually larger and have a higher ratio of selected features that indicate that DPDF representations generally contain less redundant and irrelevant information. Best over-all error rates from DPDF representations are obtained at scale 20  X  6(seeTable 4 ). Compared to best ESC represen-tation committee, the DPDF representation committee pro-vides a lower error rate. 5.2 Information fusion at feature level This subsection presents results from committees built on multi-scale ESC representation, committees built on multi-scale DPDF representation, and committees built on a multi-feature representation (both multi-scale ESC and DPDF representations).

The committee built on multi-scale ESC representation is composed of 1,095 terms. Table 5 details the individual ratio of selected features at each scale. Features from every scale have been selected for a total of 818 features out of 15,457, resulting in an overall selected features rate of approximately 5%. Table 6 presents the mean error rates over 100 replica-tions with one standard deviation for the multi-scale ESC rep-resentation committee. Using the multi-scale approach leads to lower overall error rates compared to single-scale ESC representation. This is explained by the greater quantity of features available to build committees.

The committee built on DPDF multi-scale representation is composed of 1,288 terms. Table 7 details the individ-ual ratio of selected features for each scale. Features from every scale have been selected for a total of 888 features out of 14,744, resulting in an overall selected features rate of approximately 6%; a result similar to the one obtained with multi-scale ESC representation.

Table 8 presents the mean error rates (in %) of the 100 replications with one standard deviation for the multi-scale DPDF representation committee. Similarly to the ESC multi-scale representation, the multi-scale approach leads to lower error rates compared to the single-scale DPDF representa-tion. However, the multi-scale DPDF representation com-mittee provides lower error rates than the multi-scale ESC representation committee.

The committee built a multi-feature ESC+DPDF repre-sentation (multi-scale representations with ESC and DPDF) is composed of 679 terms. Table 9 details the features selec-tion rate for each scale. Features from every scales have been selected for a total of 555 features out of 30,201 resulting in an overall selected features rate of less than 2%. By provid-ing more diversified information to BFS results in a lighter committee using less features.

Table 10 presents the mean error rates over the 100 repli-cations for the committee built on ESC+DPDF multi-feature representations. The error rates are lower than those obtained from the committees based only on one of the two multi-scale representations. This confirms that ESC+DPDF multi-feature representation are complementary and that together, they provide greater diversity to BFS. 5.3 Information fusion at the confidence score level This section presents the results obtained from the overpro-duce and choose approach. A mean of 19.81 committees are selected per replication. Each committee uses a mean of 1736.93 ESC features and 2004.12 DPDF features for a total of 3741.05 features. Table 11 details the individual ratio of selected features at each representation. Of the 30 resolutions, 4 are systematically selected for both types of representation and 11 are systematically discarded. Interest-ingly, the remaining fifteen (that is, half of the resolutions available) are equally shared between both types of repre-sentation, indicating the complementarity of the two feature extraction techniques.

Table 12 presents the mean error rates over 100 replica-tions for the ensemble of committees. The overall error rates arelowerthanthoseobtainedfromthecommitteesbasedonly on one of the two multi-scale representations, but higher than those obtained from the committee based on both multi-scale representations. 5.4 Fast incremental learning This section presents results from the forward incremental learning scheme. Figure 9 a presents the mean error rate as a function of both the number of references per writer and the numberofrepresentationspresentedtothesystem.Theactual number of selected representations is indicated on Fig. 9 b using the mean number of features as a function of the num-ber of representations that has been presented to the system.
The mean error rate decreases monotonically according to both the number of representations and the number of references. In both cases, there seems to be a limit to the improvement provided by adding new references and new representations since the improvement lessens as more refer-ences or representation are added. However, the figure clearly shows that adding new representations leads to a greater impact on accuracy than by adding new reference signatures.
Figure 10 a presents the mean error rate as a function of both the number of references per writer and the number of representations used by the verification system. The mean error rate decreases monotonically for both the number of representations and the number of references. In both cases, there is a limit to improvements provided by adding new references and representations. However, the figure clearly shows that adding new representations has a greater impact on accuracy than adding new references.

Figure 10 b compares the mean error rates when applying information fusion at the feature level compared to informa-tion fusion at the confidence score level. Committees result-ing from information fusion at feature level obtain a lower error rate, which is explained by the richer information con-veyed by features rather then scores. 5.5 Discussion Table 13 summarizes the results presented in this paper, bold indicating lowest error rates among the compared SV systems. Results indicate that the proposed multi-feature ESC+DPDF approach provides a high level of performance, yet is faster than the optimal overproduce and choose approach [ 13 ]. This is true despite the fact that the pro-posed approach uses features extracted with two simple tech-niques (ESC and DPDF), while the optimal overproduce and choose approach uses more sophisticated graphomet-ric features adapted to signature traces. Compared to [ 13 ], the improvements are due to a combination of larger over-all feature spaces provided by multi-feature extraction and of boosting feature selection. With the WI feature selection approach proposed in this paper, many samples are gener-ated in a large feature space, and feature learning with BFS allows for personalized feature selection, focusing on the more relevant features. Regarding both best single-scale rep-resentations, DPDF are significantly more discriminant than ESC representations. However, an interesting fact is that ESC performs better at low resolution while DPDF provides bet-ter performance at higher resolution. In this respect, the two feature extraction techniques are complementary.

Figure 11 presents a notched box and whisker plot of the error rates of the different approaches explored in this work. The notches represent a robust estimate of the uncertainty about the medians for box-to-box comparison. Boxes whose notches do not overlap indicate that the medians of the two groups differ at the 5% significance level. All error rates significantly differ except for the multi-scale DPDF and the approach combining all 60 independent committees, whose notches overlap.

The proposed solution is highly modular in the sense that each new representation can generate an independent clas-sifier, which in turn can be integrated to the classification module for increased performance. For instance, if all independent committees built from every representations extracted in this work are combined to form ensemble of 60 committees, this  X  X ll Representations X  (see Table 13 )system provides highest level of performance of any system built on a single-scale representation.

The ensemble of committees can also be optimized using a genetic algorithm to further improve performance by filter-ing out redundant and irrelevant representations. The draw-back of this approach is that the optimization process must be repeated each time a new representation is available. In this case, an incremental learning strategy is more appropriate. Results show that even forward incremental selection, argu-ably the simplest incremental selection scheme, provides a viable means for filtering representations and increase per-formance. As shown in Fig. 9 a, there is more to be gained from extracting new representations than by sampling new references. Consequently, such a verification system can run with a few signatures of reference if it is composed of ade-quate representations.

The combination of independent committees into ensem-ble of committees result in the fusion of information at the confidence score level. When committees are built across multiple representations, the information fusion occurs at the feature level and results small committees with better gen-eralization performance. Both multi-scale ESC and multi-scale DPDF committees outrank their single representation counterparts and use only 5 and 6% of all available features, respectively. This result is even more convincing for a multi-scale committee built across all 60 representations; the com-mittee uses even less features (2%) and provides a lower error rate. 6 Conclusion This paper presents a practical solution to some of the fundamental problems encountered in the design of off-line signature verification (SV) X  X he large number of users and features, the limited number of reference signatures, the high intra-personal variability of the signatures, and the lack of forgeries as counterexamples. A new approach for feature selection is proposed for cost-effective design of writer-independent (WI) off-line SV systems. It combines multiple feature extraction, dichotomy transformation, and boosting feature selection (BFS). Computer simulations performed on real-world signature data (comprised of random, simple, and skilled forgeries) indicate that this approach provides enhanced performance when extended shadow code and directional probability density function features are extracted at different scales.

The multi-feature extraction and selection approach pro-posed in this paper involve dichotomy transformation to mit-igate the effects of designing a system with many users and a limited number of reference signatures. The global WI approach allows to explore and select from a large set of features by incorporating prior knowledge of a population of users. Experimental results show the writer-independence by training and testing the system on two disjoint sets of writ-ers and allows for signature verification from only a single reference signature per writer. Results further demonstrate the viability of using random forgeries to train a classifier in the distance space of the dichotomy transformation, thus addressing the lack of skilled forgeries.

The high intra-personal variability of handwritten signa-tures is dealt with by extracting a large diversified set of fea-tures, using one or more preexisting techniques (such as ESC and DPDF) at different scales. Simulation results have shown that these two complementary feature extraction techniques provide a powerful multi-scale and spatio-directional repre-sentation of signature images. Given the large number of fea-tures, BFS allows to select features while learning. Originally proposed for traditional feature vectors, results also indicate the effectiveness of BFS with distance vectors resulting from the dichotomy transformation. Further, this approach result in low cost, efficient classifiers that are suitable for real-time applications.

Another significant advantage of the proposed framework resides in the modularity of its classification architecture. Using the properties inherited from the WI approach, new samples and signature representations can be added to the system during operations. A single classifier may be built off-line, using all available signature representations, thereby fusing information at the feature level. In contrast, one clas-sifier may be built per representation and then grouped into an ensemble of classifiers, thereby fusing information at the confidence score level. The former approach yields the more efficient classifiers, yet requires all representations to be available during the design phase, while the latter allows to design the system using a single representation and then updates it incrementally when new representations become available.ThereisnoneedtoretraintheWIclassifierfromthe start using all cumulative references signatures. Results indi-cate that after starting with a single representation and a sin-gle reference signature, the accuracy of the system improves the most by adding representations rather than references. The proposed framework is therefore suitable to applications where few reference samples are available.

One issue of off-line signature verification research is the availability of large-scale data sets. Future research will include assessing performance over a wider range of feature extraction techniques, resolutions, and data sets. Other fea-ture extraction techniques and scales would be considered to increase information diversity, and thus system accuracy. An analysis should uncover the impact of different feature types and scales on performance. Future work will also focus on adapting the classification function dynamically to the spe-cific writer or signature for authentication, and thus combin-ing the advantages of both WI and WD approaches. Finally, significant improvement in learning time is expected from distributed computing.
 Appendix A complexity analysis for BFS Suppose a committee composed of T decision stumps is built from a two-class D -dimensional problem with training and validation datasets of L =| L | and H =| H | patterns, respec-tively. Let t 1 be the time taken to perform an addition, sub-straction, or comparison, and t 2 the time for a multiplication, division, or exponentiation. For the purpose of this analy-sis, suppose that later operations are an order of magnitude greater than the former such as t 2 = 10 t 1 .

During testing, a decision stump classifies ( 5 ) an input distance vector regardless of the number of features and thus has a constant time complexity of t 1 . A committee of stumps repeats this operation T times and then sums the T  X  1 responses from the stumps. Thus, the total worst-case time required to classify an input vector using the committee t growing rate, valid when T 1, is O ( T ) , making for very fast classification during operations. By comparison, RBF-SVM classification time complexity scales linearly with the number of features and support vectors O ( DN s ) [ 35 ], where N s is the number of support vectors. For noisy problems such asWIsignatureverification,theset N s increasesdramatically and causes a major slowdown for SVM during operation. On the other hand, the boosting approach does not suffer this inconvenience. Moreover, when working with high-dimen-sional databases such as in this work, D T , which makes the boosting approach an attractive alternative to SVMs.
During training, the total worst-case time required to learn with Gentle AdaBoost and early stopping includes the time for quicksort, training decision stumps, and computing the area under the ROC curve. Using quicksort algorithm [ 36 ], the worst-case time required to sort the values of one feature is defined as: t Once values are sorted, training a decision stump (see Fig. 6 ) has a worst-case time of t The algorithm to compute the area under an ROC curve [ 32 ] has a worst-case time of t Thus, the total worse-case time required to perform the Gen-tle AdaBoost with early stopping in a normalized format (see Fig. 5 ) is expressed by: t t t and the corresponding growth rate, when D , L , T 1, is: O ( DL 2 + TH 2 + DT L ). (10) By comparison, the time complexity of computing a radial basis function kernel matrix for a support vector machine scales to O ( DL 2 ) [ 35 ], not including the time spent on parameters selection.

It is worth noting that quicksort does much better in the average case with t ave sort  X  L log L . Thus, the average case time complexity of the Gentle AdaBoost with early stopping is
O ( DLT ) . In computer simulations 1 , the authors have observed the average case analysis to be more representa-tive of the reality than the worst-case analysis. Consider-ing this, when working with large databases such as in this work, L T , which makes the boosting approach a method of choice since O ( DLT ) grows significantly slower than O ( DL 2 ) making the Gentle AdaBoost with early stopping a fast learning algorithm that scales linearly. Our simulations with SVMs were executed on the same machine as for BFS, a dual-core Opteron 875 running at 2.2 GHz with 32 GB of memory, using the small and medium single-scale resolu-tion datasets presented in this work at Sect. 4 . By comparing execution durations, the SVM approach increased the time complexity by two and three orders of magnitude for learn-ing and testing, respectively, thus confirming the theoretical complexity analysis presented herein. Knowing that the BFS approach took 2 days for learning and classifies around 4,800 samples per second on the largest of the multi-feature data-sets, by projection, the SMV approach would require over 6 months for learning and the resulting SVM classifier would only classify up to 5 samples per second.
 References
