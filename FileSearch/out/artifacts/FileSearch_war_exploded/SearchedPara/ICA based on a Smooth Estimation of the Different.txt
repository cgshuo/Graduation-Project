 observations of unknown linear functions of that vector. Assume a data S  X  R d is generated via d independent sources. We observe X = AS where A is an unknown square matrix called the mixing transformation A and the sources s 1 , ..., s n that generated our data x i = As i . Given the minimal statement of the problem, it has been shown [6] that one can recover the origi-nal sources up to a scaling and a permutation provided that at most one of the underlying sources is Gaussian and the rest are non-Gaussian. Upon pre-whitening the observed data, the problem reduces to a search over rotation matrices in order to recover the source and mixing matrix in the sense de-scribed above [10]. We will assume henceforth that such pre-processing has been done. Specifying distributions for the components of X , one obtains a parametric model that can be estimated via maximum likelihood [3, 4]. Working with W = A  X  1 as the parametrization, one readily obtains a gradient or fixed-point algorithm that yields an estimate  X  W and provides estimates of the latent components via  X  S =  X  WX [10].
 In practical applications the distributions of the d components of X are unknown. Therefore it is preferable to consider the ICA model as a semiparametric model in which the distributions of the components of X are left unspecified. The problem is then, obviously, to find a suitable contrast function , i.e. a target function to be minimized in order to estimate the ICA model. The earliest ICA algorithms were based on contrast functions defined in terms of expectations of a single fixed nonlinear function, chosen in ad-hoc manner [5]. More sophisticated algorithms have been obtained by careful choice of a single fixed nonlinear function, such that the expectations of this function yield a robust approximation to the mutual information [9].
 Maximizing the likelihood in the semiparametric ICA model is essentially equivalent to minimizing the mutual information between the components of the estimate  X  S =  X  WX [4]. The usage of the motivated, quite apart from the link to maximum likelihood [6]. Estimating MI from a given finite sample set is difficult. Several modern approaches rely on k -nearest neighbor estimates of entropy and mutual information [12, 16]. Recently the Vasicek esti-mator [17] for the differential entropy of 1D random variables, based on k -nearest neighbors statis-tics, was applied to ICA [8, 13]. In addition ICA was studied by another recently introduced MI be computed and therefore the optimization of such numerical criteria can not be based on gradient techniques. Also the result numerical criteria tend to have a non-smooth dependency on sample values. The optimization therefore should involve computation of contrast function on a whole grid of searched parameters.
 In addition, such estimators do not utilize optimally the whole amount of data included in the sam-ples of random vectors. Therefore they require significant artificial enlargement of data sets by a technique called data augmentation [13] that replaces each data point in sample with R-tuple (R is usually 30) of points given by an statistical procedure with ad-hoc parameters. An alternative is the Fourier filtering of the estimated values of the evaluated MI estimators [16].
 In the present paper we propose new smooth estimators for the differential entropy, the mutual in-formation and the divergence. The estimators are obtained by a novel approach averaging k -nearest They fully utilize the amount of data comprised into a random variable sample. The estimators pro-vide a novel geometrical interpretation for the entropy. When applied to ICA problem, the proposed estimator leads to the most precise results for many distributions known at present. The rest of the paper is organized as follows: Section 2 reviews the k NN approach for the entropy the mutual information and the divergence. Section 4 describes the application of the proposed estimators to the ICA problem and Section 5 describes conducted numerical experiments. entropy of X is defined as: We describe the derivation of the Shannon differential entropy estimate of [11, 18]. Our aim is distance between x i and its k -th nearest neighbor (the probability is computed over the positions the  X  -ball centered at x i by p i (  X  ) , i.e. p i (  X  ) = we obtain: It can be easily verified that indeed log p i (  X  ) according to the distribution P ik (  X  ) is: where  X  ( x ) is the digamma function (the logarithmic derivative of the gamma function). To verify the last equality, differentiate the identity around x i , we obtain: d/ 2) for Euclidean norm). Substituting Eq. (4) into Eq. (3), we obtain: which finally leads to the unbiased k NN estimator for the differential entropy [11]: unbiasedness and consistency of the k NN estimator is found at [15].
 A similar approach can be used to obtain a k NN estimator for the Kullback-Leibler divergence [19]. drawn independently from the densities p and q respectively. By definition the divergence is given by: The distance of x i to its nearest neighbor in { x j } j 6 = i is defined as We also define the distance of x i to its nearest neighbor in { y j } Then the estimator of [19] is given by The authors established asymptotic unbiasedness and mean-square consistency of the estimator (10). The same proofs could be applied to obtain k -nearest neighbor version of the estimator: Being non-parametric, the k NN estimators (6, 11) rely on the order statistics. This makes the ana-estimator value as a function of the sample coordinates. One also should mention that finding the k -nearest neighbor is a computationally intensive problem. It becomes necessarily to use involved approximate nearest neighbor techniques for large data sets. We propose a novel approach for the entropy estimation as a function of sample coordinates. It is can be also extracted from a mean of several estimators corresponding to different values of k . Next we consider all the possible values of order statistics k from 1 to n  X  1 : Exchanging the order of summation, the last sum adds for each sample point x i the sum of log of entropy can be written as: where the constant depends just on the sample size and dimensionality. We dub this estimator, the MeanNN estimator for differential entropy. It follows that the differential entropy (approximation) has a clear geometric meaning. It is proportional to log of the products of distances between each two points in a random i.i.d. sample. It is an intuitive observation since a higher entropy would lead to a larger scattering of the samples thus pairwise distances would grow resulting in a larger product of all distances. Moreover, the MeanNN estimator (13) is a smooth function of the sample coordinates. Its gradient can be easily found. The asymptotic unbiasedness and consistency of the estimator follow from the same properties of the k NN estimator (6). Obviously, the same method gives the mean estimator for the mutual information by usage of well known equality connecting the mutual information and marginal and joint entropies: We demonstrate the MeanNN estimator for the entropy in the case exponential distributed random variable f ( x,  X  ) = 1 as
H = log  X  +1 . We compared the performance of the MeanNN estimator with k -nearest neighbor estimator (6) for various values of k . Results are given in Table 1. One may see that the mean square error of the MeanNN estimator is the same or worse for the traditional k NN estimators. But the standard deviation of the estimator values is best for the MeanNN estimator. Further we will apply MeanNN for optimization of a certain criterion based on the entropy. In such cases the most important characteristics of an estimator is its monotonic dependency on the estimated value and the prediction of the exact value of the entropy is less important. Therefore one may conclude that MeanNN is better applicable for optimization of entropy based numerical criteria. Table 1: Performance of MeanNN entropy estimator in comparison with k NN entropy estimators. 100 samples of random variable, 10 various values of  X  parameter, 100 repetitions. To obtain the estimator for the divergence we apply the same mean approach to estimator (11) setting m = n  X  1 : The mean estimator for the divergence has a clear geometric interpretation. If the product of all distances inside one sample is small in comparison with the product of pairwise distances between the samples then one concludes that divergence is large and vice versa. As many approaches do, we will use a contrast function J ( Y ) = Considering Y as linear function of X , Y = WX , it is easily verified [3, 7, 10] that the logarithm of the Jacobian of the transformation. As we will assume the X  X  X  to be pre-whitened, W will be restricted to rotation matrices, therefore log( | W | ) = 0 and the minimization of J ( Y ) reduces to finding expression as a function of W : Then we can plug the MeanNN entropy estimator into Eq. (19) to obtain (after omitting irrelevant constants) an explicit contrast function to minimize: The gradient of the contrast function S ( W ) with respect to a rotation matrix W may be found with matrix W  X  R d  X  d is represented by a product of d ( d  X  1) / 2 plane rotations: where G st is a rotation matrix corresponding to a rotation in the st plane by an angle  X  st . It is rotation matrix by ( s, s ) , ( s, t ) , ( t, s ) , ( t, t ) for which It can easily verified that the gradient of the contrast function (20) is given by where  X  G uv =  X  The contrast function S ( W ) and its gradient  X  row w t is perpendicular to a vector x i  X  x j . To overcome this numerical difficulty we utilize a smoothed version of the contrast function S ( W,  X  ) and give the expression for its gradient: For the optimization of the contrast function we apply the conjugate gradient method. The algorithm is summarized in Figure 1. Input : Data vectors x 1 , x 2 , ..., x n  X  R d , assumed whitened Output : Mixing matrix W
Method : First we study the set of 9 problems proposed by [2]. Each problem corresponds to a 1D probability distribution q ( x ) . One thousand pairs of random numbers x and y are mixed as x 0 = x cos  X  + We applied the conjugate gradient methods for the optimization of the contrast function (25) with from [1].
 it is highly recommended to use dataset augmentation. This is a computationally intensive technique for the dataset enlargement by replacing each data set point with a fixed number (usually 30) new method gives smooth results without any additional augmentation due to its smooth nature (see Eq. (13)). pdfs MILCA MILCA Aug RADICAL RADICAL Aug KernelICA MeanNN ICA Table 2: Amari performance (multiplied by 100) for two-component ICA. The distributions are: (a) Student with 3 degrees of freedom; (b) double exponential; (c) Student with 5 degrees of freedom; (d) exponential; (e) mixture of two double exponentials; (f) symmetric mixtures of two Gaussians; (g) nonsymmetric mixtures of two Gaussians; (h) symmetric mixtures of four Gaussians; (i) non-symmetric mixtures of four Gaussians.
 In the explored cases the proposed method achieves the level of a state-of-the-art performance. This is well explained by the inherent smoothness of MeanNN estimator, see Figure 2. Here we presented of possible rotations angles for the mixture of two exponentially distributed random variables (case e ). The contrast function corresponding to the order statistics k = 10 generally coincides with the MILCA approach. Also the contrast function corresponding to the order statistics k = 30 '  X  n generally coincides with the RADICAL method. One may see that MeanNN ICA contrast function leads to much more robust prediction of the rotation angle. One should mention that the gradient based optimization enables to obtain the global optimum with high precision as opposed to MILCA and RADICAL schemes which utilize subspace grid optimization.
 Application of the gradient based optimization schemes also leads to a computational advantage. The number of needed function evaluations was limited by 20 as opposed to 150 evaluations for grid optimization schemes MILCA and RADICAL. Figure 2: Convergence analysis for a mixture of two exponentially distributed random variables. Contrast function dependence on a rotation angle for different entropy estimators. 1000 samples, 0.01 radian grid.
 We also studied the application of MeanNN ICA to multidimensional problems. For that purpose we chose at random D (generally) different distributions, then we mixed them by a random rotation and ran the compared ICA algorithms to recover the rotation matrix. The results are presented at Table 3. MeanNN ICA achieved the best performance. dims MILCA MILCA Aug RADICAL RADICAL Aug KernelICA MeanNN ICA
Table 3: Amari index (multiplied by 100) for multidimensional ICA. 1000 samples, 10 repetitions We proposed a novel approach for estimation of main information theoretic measures such as dif-ferential entropy, mutual information and divergence. The estimators represent smooth differential functions with clear geometrical meaning. Next this novel estimation technique was applied to the ICA problem. Compared to state-of-the-art ICA methods the proposed method demonstrated supe-rior results in the conducted tests.
 Studied state-of-the-art approaches can be divided in two groups. The first group is based on exact entropy estimation, that usually leads to high performance as demonstrated by MILCA and RADI-CAL. The drawback of such estimators is the lack of the gradient and therefore numerical difficulties in optimization. The second group apply different from entropy criteria, that benefit easy calcula-tion of gradient (KernelICA). However such methods may suffer from deteriorated performance. MeanNN ICA comprises the advantages of these two kinds of estimators. It represents a contrast function based on an accurate entropy estimation and its gradient is given analytically therefore it may be readily optimized.
 Finally we mention that the proposed estimation method may further be applied to various problems in the field of machine learning and beyond.

