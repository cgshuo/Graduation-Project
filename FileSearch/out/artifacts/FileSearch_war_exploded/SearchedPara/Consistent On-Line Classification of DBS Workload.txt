 An important goal of self-managing databases is the au-tonomic adaptation of the database configuration to evol-ving workloads. However, the diversity of SQL statements in real-world workloads typically causes the required ana-lysis overhead to be prohibitive for a continuous workload analysis. The workload classification presented in this pa-per reduces the workload analysis overhead by grouping si-milar workload events into classes. Our approach employs clustering techniques based upon a general distance func-tion for DBS workload events. To be applicable for a con-tinuous workload analysis, our workload classification spe -cifically addresses a stream-based, lightweight operation , a controllable loss of quality, and self-management. H.2.4 [ Database Management ]: Systems Management
Many configuration decisions for a DBS depend on its workload, e.g. the physical design, the bufferpool size, and the optimizer level. As the workload typically evolves over time, a DBA must continuously monitor the workload and adapt the DBS configuration to possible changes. This task causes high administration costs. To decrease administrat i-on costs, DBMS vendors have started to integrate on-line self-management functionality into their products, e.g. a u-tonomic memory management and statistics collection.
In the future, an increasing number of administration as-pects will be under the control of on-line self-management functions. Due to the large impact of the workload on the DBS configuration, a majority of these functions will have to consider the workload in their analysis. But the size and diversity of real-world workload information usually caus es a high analysis overhead. In order to reduce this overhead, a workload classification technique is needed, which assign s similar workload events to common classes.
DBS workload classification refers to placing workload events into classes by considering some of the event charac-teristics, but ignoring others. Only the class information is passed on to the subsequent workload analysis function. To make the workload classification applicable for a variety of analysis functions, it must be possible to select a set of diff e-rent DBS workload characteristics, e.g. CPU and I/O usage, execution times, or SQL statement characteristics ( Varying Workload Characteristics ). To assure that the workload ana-lysis is not harmed by the workload characteristics that are ignored for classification, the acceptable loss of quality m ust be definable ( Loss of Quality ). An upper limit for the num-ber of classes is required to restrict the workload analysis overhead ( Class Limitation ). As self-management functions in a DBS are intended to lower the administration effort for a DBS, the workload classification may not impose additio-nal effort ( Self-Management ). The DBS workload has to be observed and analysed continuously in on-line self-manage -ment functions. Thus, it can be seen as a stream of incoming workload events, which have to be assigned to classes imme-diately ( Stream-based Processing ). To avoid biased workload analysis results, the workload classification must assure t hat one type of event is mapped to the same class throughout the classification X  X  lifetime ( Consistency ).

Traditional classification techniques (Bayes classifiers, de-cision trees) would require the DBA to provide training data. The effort for this manual definition and assignment of workload classes therefore prevents their usage for DBS workload classification. The task of unsupervised assignme nt of observations into groups instead corresponds to the sub-ject of clustering techniques. Hence, our DBS workload clas -sification approach is based on using clustering to identify groups of similar statements in the DBS workload. These groups then define the classes for all workload events. How-ever, major challenges still have to be solved to meet the requirements described above: First, the similarity of eve nts cannot be determined by usual distance metrics like the Euclidean distance, because features of workload events ma y have non-numerical values (e.g. table names). Second, the appropriate number of classes must be automatically deter-mined according to the loss of quality requirements. Third, the usual clustering techniques expect the entire set of dat a objects as input, which is not the case for DBS workloads.
Feat. Type Description Feature Type Value Examples nominal-qualitative ordinal-qualitative discrete-quantitative continuous-quantitative
Related research approaches do not meet the requirements of DBS workload classification. In [3] the classification of database workload into OLTP and DSS is described. The approach is intended only to distinguish between these two workload types and cannot be used for DBS workload clas-sification in general. Chaudhuri et al. in [1] compress SQL workload using clustering techniques. However, the distan -ce function strictly focuses on index selection, the entire workload must be known in advance, and sampling is per-formed instead of classification. Though stream-oriented, the approach in [6] does not guarantee consistency of classifica ti-on results or a class. Data streams clustering approaches li ke [7] often employ on-line versions of the k-Means algorithm. As these continuously adapt the clusters to the observed events, they violate the consistency requirement. More ad-vanced approaches like [2] follow a two-phase scheme, where the stream is mapped to a grid on-line, whereas the clusters are identified off-line. Also these approaches lack support f or consistency, do not support a quality loss limit, and are not suited for nominal features.
A general-purpose workload classification must support all different features that may be extracted from DBS workload events. In particular, it must be possible to use non-nume-rical, i.e. nominal and ordinal, feature types, which are pr e-sent in SQL statement texts, for example. Tab. 1 provides an overview of different feature types [5]. As these feature types may not only occur as atomic values, the column fea-ture type value defines the type of their complex values. The actual selection of features heavily depends on the analysi s goal and should be subject of an experimental evaluation. Hence, the examples in Tab. 1 only illustrate that all fea-ture these types must be considered in the distance metric, because there might be features of these types.

Usual distance metrics cannot be used on the nominal feature types of DBS workload events, because arithmetic operations cannot be applied. Hence, we use the generalized Minkowski metric [5] instead. In this approach, nominal-qualitative feature type values are generally represented as sets, whereas the other feature types values are intervals. It defines the distance  X  between two features A k and B k as where | A k  X  B k | denotes the number of elements in the union of A k and B k (or the length of the unified interval, respec-tively), and | A k  X  B k | denotes the number of elements in the intersection. Using  X  ( A k , B k ), [5] defines the generalized Minkowski distance d p for two events A and B as where | U k | denotes the number of possible values of the fea-ture k (or the interval length, resp.). Dividing the feature distance  X  ( A k , B k ) by | U k | normalizes the feature values to the interval [0; 1], thus avoiding an implicit weighting o f the features by different dimensions. The normalized fea-ture distances may be weighted by the coefficient c k , where P k =1 c k = 1. Parameter p defines the order of the metric.
As the generalized Minkowski metric supports all relevant feature types from Tab. 1, it fulfils the requirement of Va-rying Workload Characteristics . The Self-Management re-quirement is not violated, because all parameters, i.e. the feature dimensions k = 1 ..d , the feature weights w k , and the Minkowski order p depend on the goal of the workload analy-sis only, but not on a particular DBS environment. However, due to the Stream-based Processing , the domain size of a fea-ture representing the names of the relations, for example, is affected by new relations. Hence, the distance calculatio n could determine different distance values for SQL statement s in this case and so could violate the Consistency . A simple solution to ensure Consistency while supporting Stream-Based Processing would be to store the distance va-lues between known feature vectors. The domain size could then be changed without affecting the consistency of pre-vious distance calculations. But as an enlarged feature do-main size will lead to smaller normalized distance values fo r previously unobserved feature vectors, the validity of the tri-angle inequality for the distance metric could not be ensure d. For this reason, the value of the domain sizes | U k | may not be changed during workload classification. We have therefor e designed a simple lifecycle model for the workload classific a-tion, which freezes all | U k | after a learning phase (see Sec. 4). However, an evolving workload may still may cause feature distances  X  ( A k , B k ) greater than the original U k . So distan-ce values in the interval [0; 1] can only be assured during the learning phase, and may be exceeded afterwards.
Using the distance function described in Sec 3, clustering techniques can be used to derive the classification rules. Pa r-titional clustering algorithms are best suited for groupin g DBS workload events, because they can be efficiently im-plemented and identify isotropic clusters. We have chosen the the partitional k-Means algorithm because of its attrac -tive time and space complexity. Several constraints apply t o the selection of the number of classes k : It must be derived Algorithm 1 : Number of Workload Classes Algorithm: Classification Input : Workl. W , Max. Qual. Loss  X  , Class Limit k max
Output : Classes C if | W | X  COMP R RAT IO  X  k max then k min  X  1; k max  X  min( k max , | W | ); while k min &lt; k max do end automatically ( Self-Management ), it must consider the ac-ceptable Loss of Quality , and it must regard the Class Limit .
The prerequisite for choosing the right k is a measure for the quality loss caused by the classification. This loss of qu a-lity is caused by assigning the workload events to clusters, where the medoids then represent all events in the clusters. So the loss of quality is the dissimilarity between the clas-sified event v i and its medoid m i , which can be quantified by the distance function d p . Hence, the overall quality loss ql mse , which is caused by classification C of workload W using k classes, can be computed as the mean squared error
Due to the definition of d p , the value range for ql mse dependent on the order p chosen for the Minkowski me-tric: Assuming the maximum normalized distance value 1 for every feature, the maximum distance for two events is ( P d k =1 c p k ) 1 /p . For example, the maximum event distance for order 2 and ten equally weighted features is  X  0 , 32. So we normalize the quality loss measure to the maximum event distance
The workload classification then has to find the minimum class number k such that ql norm ( C ( W, k ))  X   X  . For this task we follow the approach described in [1] and perform the binary search described in lines 5 -13 of Alg. 1. As for large workloads it is likely that the quality loss is exceeded even when the class limit is reached, we avoid the effort for a binary search in this case ( 1 -4 ).

Alg. 1 still makes no provisions for a stream-based clas-sification with consistent results. To achieve this goal, we have designed a lifecycle for the workload classification: I t is initialized in state learning , where the classification ru-les are learned from the observed workload. Afterwards, the workload classification switches to state stable . In this state no changes are made to the classes anymore, i.e. the medo-ids are frozen. Each element from the stream of events is assigned to the cluster (class) with the nearest medoid. The task of the learning phase is to find a classification Algorithm 2 : Learning Phase Algorithm: LearningPhase
Input : Min. Learn. t , Check Interval ci , Threshold  X  l  X  t / ci ; /* ring-buffer length */ for i = 0 to l  X  1 do end n 1  X  buffer [0]; n 2  X  buffer [ i ]; while n 2 &gt; ( n 1  X  (1 +  X  )) do end settleFeatureDomainSizes ( W );
Classification( W ,  X  , k max ) ; /* see Alg.1 */ for the DBS workload. This includes the decision on the adequate number of classes, the identification of the clus-ter medoids, and the determination of the feature domain sizes. The observed workload information is not passed on to the self-management function in this phase. The major challenge in the learning phase is to freeze the classificati -on rules as soon as the classification sufficiently well reflect s the DBS workload. The obvious solution would be to execu-te the cluster analysis in short intervals, ending the learn ing phase when there are no or only little changes in the results. Due to the large effort for this continuous analysis, we have designed a learning phase that requires the cluster analysi s only once (see Alg. 2). First (lines 1 -5 ) the workload is read in regular check intervals ci and buffered for the duration t . After t , the algorithm regularly checks the ratio of new feature vectors during the last period t ( 6 -13 ). When the value drops below a threshold  X  , the learning phase of the classification rules ends, and the feature domain sizes are settled. Then, the classification rules are derived from the workload ( 15 ), and the classified events are passed on.
After the classification rules have been determined in the learning phase, the workload classification switches to the stable state. In this state, all incoming feature vectors ar e classified by assigning them to the cluster with the nearest medoid. Only the medoid information is passed on to the subsequent self-management function, whereas the origina l feature vector is discarded. This classification is perform ed until the workload classification receives an external orde r to start a new learning phase by the self-management function.
For evaluation purposes, we have applied the described workload classification concepts to our workload shift dete c-tion (WSD) self-management function [4]. The WSD identi-fies significant changes in the DBS workload by monitoring the workload at the statement level. The classification pro-totype therefore first converts the observed SQL statements into feature vectors by extracting the features described i n Tab. 2. Afterwards, the feature vectors are classified accor -ding to Sec. 4. All components are built upon an implemen-tation of the distance function described in Sec. 3. Table 2: Feature Selection for SQL Statements
Feature F.-Type Value Weight relation names nom.-qual. set 0.50 selection columns nom.-qual. set 0.25 projection columns nom.-qual. set 0.13 statement type nom.-qual. atomic 0.06 no. of subqueries disc.-quant. atomic 0.03 aggregate functions nom.-qual. set 0.01 grouping nominal-qual. set 0.02 Table 3: Workload Shift Detection Timestamps
To assess different workload classification configurations, we have simulated various workload shift scenarios: The TPC-C/H scenario starts with a workload that is composed of 70% TPC-C and 30% TPC-H statements, and then slow-ly shifts to a workload composed of 30% TPC-C and 70% TPC-H. In the ( TPC-W ) scenario, the workload starts with the SQL from the TPC-W Browsing Mix, then slowly shifts to the TPC-W Shopping Mix, and finally to the TPC-W Or-dering Mix. The Custom scenario comprises two workloads with a large number of randomly structured SQL. For each of these scenarios, Tab. 3 reports when the workload shifts have been detected ( WSD ) and how many classes have be-en created ( #IC ) for different quality loss thresholds  X  . For small  X  values (0 . 08  X   X   X  0 . 13) the number of workload classes is effectively reduced, while the workload shifts ar e still detected. Fig. 1(a)-1(c) additionally illustrate th e per-plexity values in the WSD component for these scenarios. Informally, the perplexity is an indicator that describes h ow well the current workload matches the typical workload of the DBS (details are given in [4]): High perplexity values indicate that the observed workload does not match the mo-del of the typical workload in the WSD component and vice versa. In Fig. 1(a)-1(c) the perplexity values without clas -sification, with a quality loss threshold of 0 . 13, and with a quality loss threshold of 0 . 20 are plotted (configuration used: p = 2, k = 1000,  X  = 0 . 05).

The effort analysis has shown that the assignment of clas-ses in the stable phase is fast. For a workload resulting in 1000 distinct feature vectors and 200 classes, the average classification time per vector was 0.18 ms (2.2 GHz, 2 GB). Significant overhead is only caused by the cluster analysis at the end of the learning phase (k-Means execution).
Our workload classification approach reduces the analysis overhead for DBS workload by grouping similar workload events into classes. We have shown how the classification rules have to be managed in order to realize a stream-ba-sed classification. By using a general distance function for mixed-type feature vectors and by offering a quality loss li-mit, we have made our approach generally applicable for the classification of different workload events and analysis goa ls. [1] S. Chaudhuri et al. Compressing SQL workloads. In [2] Y. Chen and L. Tu. Density-Based Clustering for [3] S. Elnaffar et al. Automatically Classifying Database [4] M. Holze and N. Ritter. Autonomic Databases: [5] M. Ichino and H. Yaguchi. Generalized Minkowski [6] P. Kolaczkowski. Compressing Very Large Database [7] L. O X  X allaghan et al. Streaming-Data Algorithms for
