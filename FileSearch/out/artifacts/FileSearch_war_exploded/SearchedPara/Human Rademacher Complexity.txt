 Many problems in cognitive psychology arise from questions of capacity . How much information can human beings hold in mind and deploy in simple memory tasks [19, 15, 6]? What kinds of functions can humans easily acquire when learning to classify items [29, 7], and do they have bi-ases for learning some functions over others[10]? Is there a single domain-general answer to these questions, or is the answer domain-specific [28]? How do human beings avoid over-fitting learning examples when acquiring knowledge that allows them to generalize [20]? Such questions are central formal mathematical footing [7, 9, 5].
 Machine learning offers a variety of formal approaches to measuring the capacity of a learning sys-tem, with concepts such as Vapnik-Chervonenkis (VC) dimension [27, 25, 12] and Rademacher complexity [1, 13, 24]. Based on these notions of capacity, one can quantify the generalization performance of a classifier, and the danger of over-fitting, by bounding its future test error using complexity X  X an be measured in humans, based on their performance in a  X  X earning the noise X  pro-cedure. We chose Rademacher complexity (rather than the better-known VC dimension) because it is particularly amenable to experimental studies, as discussed in Section 5. We assess whether human capacity varies depending on the nature of the materials to be categorized, and empirically test whether human generalization behavior respects the error bounds in a variety of categorization tasks. The results validate Rademacher complexity as a meaningful measure of human learning capacity, and provide a new perspective on the human tendency to overfit training data in category learning tasks. We note that our aim is not to develop a new formal approach to complexity, but rather to show how a well-studied formal measure can be computed for human beings. Background and definitions. Let X be a domain of interest, which in psychology corresponds to a stimulus space. For example, X could be an infinite set of images parametrized by some continuous parameters, a finite set of words, etc.. We will use x  X  X to denote an instance (e.g., an image or a word) from the domain; precisely how x is represented is immaterial. We assume there is an underlying marginal distribution P training and testing. For example, P Let f : X 7 X  R be a real-valued function. This corresponds to a hypothesis that predicts the label can come up with.
 Rademacher complexity (see for example [1]) measures the capacity of the hypothesis space F by how easy it is for F to fit random noise. Consider a sample of n instances: x i.i.d. from P probability. For a given function f  X  X  , its fit to the random numbers is defined as | P n of as random labels, and { ( x match the random labels on the training sample: if f perfectly predicts the  X   X  X , or completely the the  X   X  X , the fit is minimized at 0.
 The fit of a set of functions F is defined as sup easier to find a hypothesis f  X  F that matches the random labels, and its fit will be large. On the f ( x i ) will match  X  i , and its fit will be close to zero.
 Finally, recall that { ( x sample of size n , there always exists some f  X  F (which may be different each time) that matches very definition is to learn noise. This is captured by taking the expectation over training samples: Definition 1 (Rademacher Complexity) . For a set of real-valued functions F with domain X , a distribution P where the expectation is over x = x values  X  1 .
 Rademacher complexity depends on the hypothesis space F , the domain X , the distribution on the domain P noting that Rademacher complexity is independent of any future classification tasks. For example, in Section 4 we will discuss two different tasks on the same X (set of words): classifying a word by its emotional valence, or by its length. These two tasks will share the same Rademacher complexity. In general, the value of Rademacher complexity will depend on the range of F . In the special case when F is a set of functions mapping x to { X  1 , 1 } , R ( F , X ,P A particularly important property of Rademacher complexity is that it can be estimated from random samples. Let { ( x (1) Section 3, these will correspond to m different subjects. The following theorem is an extension of Theorem 11 in [1]. The proof follows from McDiarmid X  X  inequality [16]. Theorem 1. Let F be a set of functions mapping to [  X  1 , 1] . For any integers n,m , Theorem 1 allows us to estimate the expectation in (1) with random samples, which is of practical importance. It remains to compute the supremum in (1). In Section 3, we will discuss our procedure to approximate the supremum in the case of human learning.
 Generalization Error Bound. We state a generalization error bound by Bartlett and Mendelson (Theorem 5 in [1]) as an important application of Rademacher complexity. Consider any binary task is characterized by a joint distribution P data consist of instance-label pairs ( x,y ) iid  X  P X otherwise. On a training sample { ( x f can generalize to future test data: e ( f ) = E allows us to bound the true error using training sample error as follows.
 P 1  X   X  , every function f  X  F satisfies The probability 1  X   X  is over random draws of the training sample. That is, if one draws a large number of training samples of size n each, then (3) is expected to hold on 1  X   X  fraction of those the confidence parameter  X  and training sample size n . When the bound is tight, training sample bound requires the Rademacher complexity to be close to zero. On the other hand, if the Rademacher error bound on four different human classification tasks in Section 4. Our aim is to measure the Rademacher complexity of the human learning system for a given stimulus space X , distribution of instances P the set of binary classification functions that an average human subject can come up with on the domain X , under the experiment conditions described below. We will denote this set of functions F with H We make two assumptions. The first is the assumption of universality [2]: every individual has the same H For instance, F could be defined as the set of functions that a particular individual or group can A second assumption is required to compute the supremum on H and compare the performance of every single function contained in H when making their classification judgments, participants use the best function at their disposal X  X o that the errors they make when tested on the training instances reflect the error generated by the best-performing function in H assumption is that participants are doing their best to perform the task. With the two assumptions above, we can compute human Rademacher complexity for a given stimulus domain X , distribution P sample { ( x search within H f  X  = argmax ask the subject to classify the same training instances { x sification labels will be f  X  ( x mum as follows: sup complexity to reflect actual learning capacity on the set H from simply doing rote learning. With these considerations, we propose the following procedure to estimate human Rademacher complexity.
 Procedure. Given domain X , distribution P we generate m random samples of size n each: { ( x (1) x and-pencil based, and consists of three steps:
Step 1. Participant j is shown a printed sheet with the training sample { ( x ( j ) each instance x ( j ) ter; they have three minutes to study the sheet; and later they will be asked to use what they have learned to categorize more instances into  X  X  X  or  X  X  X .

Step 2. After three minutes the sheet is taken away. To prevent active maintenance of training tion/subtraction questions.

Step 3. The participant is given another sheet with the same training instances { x ( j ) and is encouraged to guess if necessary. There is no time limit.
 view where the subject reports any insights or hypotheses they may have on the categories. Materials To instantiate the general procedure, one needs to specify the domain X and an associated marginal distribution P the corresponding domain. We conducted experiments on example domains. They are not of spe-cific interest in themselves but nicely illustrate many interesting properties of human Rademacher complexity: (1) The  X  X hape X  Domain . X consists of 321 computer-generated 3D shapes [3]. The large x produces smooth ones. A few instances and their parameters are shown in Figure 1(a). It is important to note that this internal structure is unnecessary to the definition or measurement of Rademacher complexity per se . However, in Section 4 we will define some classification tasks that  X  X ord X  Domain . X consists of 321 English words. We start with the Wisconsin Perceptual At-tribute Ratings Database [18], which contains words rated by 350 undergraduates for their emotional valence. We sort the words by their emotion valence, and take the 161 most positive and the 160 most negative ones for use in the study. A few instances and their emotion ratings are shown in Figure 1(b). Participants have rich knowledge about this domain. The size of the domain for shapes and words was matched to facilitate comparison.
 divided evenly into eight groups. Each group of m = 10 subjects worked on a unique combination of the Shape or the Word domain, and training sample size n in 5, 10, 20, or 40, using the procedure defined previously. Results . Figures 1(c,d) show the measured human Rademacher complexities on the domains X =Shape and Word respectively, with distribution P ple sizes n . The error bars are 95% confidence intervals. Several interesting observations can be made from the data: Observation 1: human Rademacher complexities in both domains decrease as n increases . This is anticipated, as it should be harder to learn a larger number of random labels. Indeed, when n = 5 , our interviews show that, in both domains, 9 out of 10 participants offered some spurious rules of the random labels. For example, one subject thought the shape categories were determined by whether the shape  X  X aces X  downward; another thought the word categories indicated whether the they believed the labels to be random, as spurious  X  X ules X  are more difficult to find. Observation 2: human Rademacher complexities are significantly higher in the Word domain than domain interviews: (i) Some participants created mnemonics. For example, one subject received the training sample (grenade, B), (skull, A), (conflict, A), (meadow, B), (queen, B), and came up with the following story:  X  X  queen was sitting in a meadow and then a grenade was thrown (B = before), to motel service, X  or  X  X hysical vs. abstract. X  We speculate that human Rademacher complexities on knowledge about the domain.
 Observation 3: many of these human Rademacher complexities are relatively large . This means that under those X ,P prone to overfit on real (i.e., non-random) tasks. We will present human generalization experiments We reiterate the interpretation of human Rademacher complexity for psychology. It does not predict  X  e (how well humans perform when training for a given task). Instead, Theorem 2 bounds e  X   X  e , the  X  X mount of overfitting X  (sometimes also called  X  X nstability X ) when the subject switches from We now present four non-random category-learning tasks to illustrate these points. Materials. We consider four very different binary classification tasks to assess whether Theorem 2 holds for all of them. The tasks are: (1) Shape-+ : Recall the Shape domain is parametrized by easy task on the domain. (2) Shape-+-: This task is also on the Shape domain, but with a nonlinear decision boundary. The negative class is on both ends while the positive class is in the middle: however, that the two shape tasks share the same Rademacher complexity, and therefore have the same bound for the same n . (3) WordEmotion : This task is on the Word domain. P ( y = 1 | x ) = 0 if word x has a negative emotion rating in the Wisconsin Perceptual Attribute Ratings Database, and P ( y = 1 | x ) = 1 otherwise. (4) WordLength : P ( y = 1 | x ) = 0 if word x has 5 or less letters, semantics and the other on orthography, but they share the same Rademacher complexity and thus the same bound (for the same n ), because the underlying domain is the same.
 in addition to the training instances { x ( j ) { x ple error as  X  e ( f ( j ) ) = 1 /n P n Participants were 40 additional students, randomly divided into 8 groups of five each. Each group worked on one of the four tasks, with training sample size n =5 or 40.
 We make two more observations: Observation 4: Theorem 2 holds for every participant . Table 1 provides empirical support that our  X  = 0 . 05 , Theorem 2 allows the bound to fail on about two (5% of 40) participants  X  which did not happen. Of course, some of the  X  X ound e  X  are vacuous (greater than 1) as it is well-known that bounds in computational learning theory are not always tight [14], but others are reasonably tight (e.g., on Shape-+ with n = 40 ).
 Observation 5: the larger the Rademacher complexity, the worse the actual amount of overfitting averaged over the two different tasks with the same domain and n , as noted in the graph). The bound seems to be true regardless of the classification task. For example, the Word domain and n = 5 has a large Rademacher complexity 1.76, and both task WordLength and task WordEmotion severely suggesting that their good performance on the training items reflects overfitting. Accordingly, the  X  X hings you can go inside, X  while subject 114 thought the class indicated an odd or even number of (scream, -1), and concluded that class 1 is  X  X nything related to omitting[sic] light, X  and proceeded to classify the test items as such. Is our study on memory or learning ? This distinction is not necessarily relevant here, as we adopt an abstract perspective which analyzes the human system as a black box that produces labels, and both learning and memory contribute to the process being executed in that black box. We do have evidence from post-interviews that Figure 1 does not merely reflect list-length effects from memory studies: (i) participants treated the study as a category-learning and not a memory task  X  they were not told that the training and test items are the same when we estimate R ; (ii) the memory load was identical in the shape and the word domains, yet the curves differ markedly; (iii) participants were able to articulate the  X  X ules X  they were using to categorize the items.
 Much recent research has explored the relationship between the statistical complexity of some cate-gorization task and the ease with which humans learn the task [7, 5, 9, 11]. Rademacher complexity of the learner in domain X (note Y does not appear in Rademacher complexity). Greater complex-On the other hand, our definition of Rademacher complexity depends only on the domain, distribu-instructions, motivation, time to study, and should probably be incorporated into the complexity. Human Rademacher complexity has interesting connections to other concepts. The VC-dimension [27, 25, 12] is another capacity measure. Let { x the domain. Let ( f ( x by some f  X  F . If F is rich enough such that its members can produce all 2 m vectors: arbitrarily large subsets. Unfortunately, human VC-dimension seems difficult to measure experi-given subset. Second, to determine that the VC-dimension is m , one needs to show that no subset of size m + 1 can be shattered. However, the number of such subsets can be infinite. A variant,  X  X ffective VC-dimension X , may be empirically estimated from a training sample [26]. This is left for future research. Normalized Maximum Likelihood (NML) uses a similar complexity measure for a model class [21], the connection merits further study ([23], p.50).
 Human Rademacher complexity might help to advance theories of human cognition in many ways. First, human Rademacher complexity can provide a means of testing computational models of hu-man concept learning. Traditionally, such models are assessed by comparing their performance to human performance in terms of classification error. A new approach would be to derive or empiri-cally estimate the Rademacher complexity of the computational models, and compare that to human Rademacher complexity. A good computational model should match humans in this regard. Second, our procedure could be used to measure human Rademacher complexity in individuals or special populations, including typically and atypically-developing children and adults. Relating in-dividual Rademacher complexity to standard measures of learning ability or IQ may shed light on the relationship between complexity, learning, and intelligence. Many IQ tests measure the partici-pant X  X  ability to generalize the pattern in words or images. Individuals with very high Rademacher complexity may actually perform worse by being  X  X istracted X  by other potential hypotheses. Third, human Rademacher complexity may help explain the human tendency to discern patterns in memory X  effect [22]. These effects may be viewed as spurious rule-fitting to (or generalization of) the observed data, and Human Rademacher complexity may quantify the possibility of observing such an effect.
 Fourth, cognitive psychologists have long entertained an interest in characterizing the capacity of different mental processes such as, for instance, the capacity limitations of short-term memory [19, learning system.
 Finally, human Rademacher complexity can help experimental psychologists to determine the propensity of overfitting in their stimulus materials. We have seen that human Rademacher complex-ity can be much higher in some domains (e.g. Word) than others (e.g. Shape). Our procedure could be used to measure the human Rademacher complexity of many standard concept-learning materials in cognitive science, such as the Greebles used by Tarr and colleagues [8] and the circle-and-line stimuli of McKinley &amp; Nosofsky [17].

