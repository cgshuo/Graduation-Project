 Associating labels with online products can be a labor-intensive task. We study the extent to which a standard  X  X ag of visual words X  image classifier can be used to tag products with useful information, such as whether a sneaker has laces or velcro straps. Using Scale Invariant Feature Transform (SIFT) image descriptors at random keypoints, a hierarchical visual vocabulary, and a variant of nearest-neighbor classification, we achieve accuracies between 66% and 98% on 2-and 3-class classification tasks using sev-eral dozen training examples. We also increase accuracy by combining information from multiple views of the same product.
 Categories and Subject Descriptors: H.3.1 [Informa-tion Storage and Retrieval]: Content Analysis and Index-ing; I.4.8 [Image Processing and Computer Vision]: Scene Analysis X  Object recognition General Terms: Algorithms, Experimentation Keywords: Content-based tagging, image classification, bag of visual words
Online shoppers benefit from being able to filter their search results according to product characteristics. For in-stance, in browsing through the women X  X  high-heeled shoe section on Amazon.com, a consumer may wish to view only shoes with a pointy toe. This ability to refine search results is a prominent feature of specialty product-search websites such as Like.com.

Manual labeling of product traits could be expensive for large numbers of items. In order for a company like Ama-zon to add currently unlabeled descriptions to its products, it would likely need an automated classifier, one of whose in-puts could be images of the product. The computer-vision community has made impressive advances in supervised im-age classification over the past several years (e.g., [1 ]), so we investigate how well a standard implementation of such techniques would perform on the product-tagging task. In addition to assessing raw classification accuracy, we explore how many manually annotated training examples are nec-essary and whether performance can be improved by using multiple image views of a single product.
 way. Using this tree, we transform each of our images into a  X  X ag of words X  by associating each of the image X  X  SIFT vectors with the words in the tree to which it is closest. The result is a histogram of frequency counts for each word, to which we can apply standard information-retrieval tech-niques like term frequency-inverse document frequency (TF-IDF) weighting and cosine similarity [2 ]. (For further details on the classifier, including parameter settings, see [ 6].)
We classify test images using a distance-weighted variant of k -nearest neighbor, in which each training image  X  X otes X  for its own category label in proportion to how much closer it is to the test image than the average training image. When we have multiple views of a test product image from dif-ferent angles, as is commonly available for items like shoes on Amazon.com, we compute distances from each of these views to the training images separately and then apply our distance-weighted nearest-neighbor classifier to the entire re-sulting set of distances at once. Views of the product that are more informative in the sense of having smaller average distances to the training images have a bigger influence on the classification decision because of the distance weighting.
We collected approximately 3,500 training images from the shoe and men X  X  shirt departments of Amazon.com and manually labeled them with characteristics visible from the product images alone. 1 We created five classification prob-lems: velcro vs. laced sneakers, pointy vs. nonpointy high-heeled shoes, short-vs. long-sleeved shirts, ballet vs. boat-ing vs. baseball shoes, and collar vs. v-neck vs. crew shirts. For each, we report class-size-adjusted accuracies, i.e., the within-class accuracy rates averaged over each class.
Figure 2 shows mean accuracies over 5 folds of cross-validation. Performance improves with increasing numbers of product training examples from each category and with increasing numbers of image views of each product. (Where not experimentally manipulated, the number of training im-ages and product views used was maximal.) The problems clearly vary in their difficulty, with some being relatively easy; for instance, we can distinguish short-vs. long-sleeved shirts to 90% accuracy with only 5 training images.
Given training sets containing labeled images of several dozen consumer-product items, we achieved accuracies be-tween 66% and 98% on 2-and 3-class classification tasks. Our classifier was relatively simple, and we expect that higher accuracies could be achieved with more advanced methods (e.g., [ 7]). In particular, going  X  X eyond bags of fea-tures X  [ 8] by using local image information could be helpful, especially for items like pointy-toed shoes where the rele-vant visual information is contained in a small region of the image. Finally, we note that our classification performance reflects a relatively clean test set, in which each image be-longed to one of the two or three main categories. A com-mercial system would likely need to handle miscellaneous items that don X  X  fit any of the training-set labels.
If interested in using this data set, please see http://www.sccs.swarthmore.edu/users/09/btomasi1/ tagging-products.html
