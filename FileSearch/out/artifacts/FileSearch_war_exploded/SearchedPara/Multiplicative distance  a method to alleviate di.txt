 Jafar Mansouri  X  Morteza Khademi Abstract Recently, it has been shown that under a broad set of conditions, the commonly used distance functions will become unstable in high-dimensional data space; i.e., the dis-tance to the farthest data point approaches the distance to the nearest data point of a given query point with increasing dimensionality. It has been shown that if dimensions are indepen-dently distributed, and normalized to have zero mean and unit variance, instability happens. In this paper, it is shown that the normalization condition is not necessary, but all appro-priate moments must be finite. Furthermore, a new distance function, namely multiplicative distance, is introduced. It is theoretically proved that this function is stable for data with independent dimensions (with identical or nonidentical distribution). In contrast to usual dis-tance functions which are based on the summation of distances over all dimensions (distance components), the multiplicative distance is based on the multiplication of distance compo-nents. Experimental results show the stability of the multiplicative distance for data with independent and correlated dimensions in the high-dimensional space and the superiority of the multiplicative distance over the norm distances for the high-dimensional data. Keywords Distance instability  X  High-dimensional data  X  Minkowski and fractional norms  X  Multiplicative and additive distances 1 Introduction Nowadays, due to the great advances in computers, high-capacity storage disks, multimedia transmission, and compression systems, there are many applications which require high-dimensional data analysis. One important issue in many of these applications is determining distances between data and query points. Beyer et al. [ 4 ] have proved that under a broad set of conditions, distances between data and query points are meaningless or unstable in the high-dimensional space. This means that the ratio of the distances of the nearest and farthest neighbors to a given query point approaches 1 for a wide variety of data distributions and distance functions when dimensionality increases toward infinity, e.g., for data with independently and identically distributed (i.i.d.) dimensions with Minkowski distance. Of course, the instability of Euclidean distance under some conditions has already been stated by Demartines [ 10 ]. This instability is also called distance concentration phenomenon [ 26 ]. The commonly used distance functions become unstable in the high-dimensional space. Weber etal.[ 39 ]havereportedthatwhenthedimensionalityofdataismorethan10,thisphenomenon can occur. In such cases, the concepts of proximity and similarity are not meaningful because of poor discrimination between the nearest and furthest neighbors. This instability can greatly affect many applications like information retrieval and data indexing [ 5 , 15 , 20 , 40 ], gene
Three solutions have been more addressed for conquering distance instability in high-47 ]. In these methods, data are divided to some partitions and the nearest neighbor or distance is obtained over these partitions. These methods have some drawbacks with increasing the dimensionality like significant increase in the number of partitions and complexity, and the performance degradation. Moreover, some of these methods are just applicable for the nearest neighbor search and cannot give the exact value of distance.

The second solution, which is widely used, is to apply some dimension reduction tech-ness that the reduced-dimension data may be high-dimensional again, for example, in the processing of high-quality images and videos. For example, if singular value decomposition matrices is 352  X  288. If rows of one of these matrices are concatenated to form a vector, dimensionality of this vector is 101,376. Suppose that this vector is used as a feature vector (e.g., for clustering), and a dimension reduction technique is applied on it. Since the original dimensionality is very high, the reduced-dimension vector is high-dimensional again (e.g., dimension reduction technique to obtain a reduced-dimension vector with low dimensional-ity, definitely a large amount of important information is lost and this can result in the poor performance of the system. So, dimension reduction techniques are not useful for data with a very high dimensionality in practice.

Since the above two methods have drawbacks and limitations for some applications, other solution should be considered. The third solution is to define a meaningful distance function. expectation, namely Pearson variation, does not tend to zero as the dimensionality grows to the stability of a distance function can be examined. As will be discussed in Sect. 2 ,thefew presented distance functions for conquering distance instability have some drawbacks. Also, is addressed and a new distance function is proposed.

This paper has the following contributions: In [ 14 ], Francois et al. have proved that norm distances are unstable for data with independent dimensions provided that dimensions are normalized, i.e., zero mean and unit variance. This paper proves that normalization condition is not necessary, but dimensions must have finite appropriate moments. Also, we introduce the multiplicative distance as a new distance function which is theoretically proved for data with independent dimensions (with identical or nonidentical distribution), its Pearson variation does not tend to zero when dimensionality increases to infinity. Simulation results show the stability of this distance function for correlated data, too. As an application, it is shown that this distance function has better performance than the norm distances for clustering. Also, the multiplicative distance can be used in the low-dimensional space in applications like clustering. A list of symbols used in this paper is given in Table 1 .
 This paper is organized as follows. Section 2 gives an overview on the related works. In Sect. 3 , a theorem on the instability of the norm distance functions for data with independent dimensions with identical or nonidentical distribution is presented. Section 4 introduces the multiplicative distance and details its properties and stability for the high-dimensional data. The experimental results are reported in Sect. 5 . Finally, some conclusions are drawn in Sect. 6 . 2 Related works In this section, we survey the previous works that studied instability of different dis-tance functions and defined new distance functions. In [ 10 ], Demartines has shown that if X = ( x 1 , x 2 ,..., x m ) is an m -dimensional random vector with i.i.d. dimensions with any dis-certain conditions on the distance distribution (induced by data distribution and distance func-tion), as dimensionality increases, the distance to the nearest data point approaches the dis-named relative contrast, converges to 0. One important case given in Beyer et al. [ 4 ]isfor data and query points with i.i.d. dimensions for Minkowski norm.

Aggarwal and Yu [ 2 ] have proposed a grid-based approach to determine the similarity function based on Minkowski norms. In this method, each dimension is divided into k equi-Y = ( y 1 ,..., y m ) be two records and m be number of dimensions. For dimension i , if both x on dimension i . The entire set of dimensions in which the two records lie in the same range and l i be the upper and lower bounds for the corresponding range in the dimension i in which the records X and Y are in proximity to each another. Then, for a given pair of records X and Y , and a level of discretization k , the similarity between the records is given by: The presence of u i  X  l i in the denominator normalizes the distances with respect to the different ranges of the coordinates. The limitation of the above function is that this function leads to ignoring the exact value of dissimilarity on the dimensions in which two records are not in the same range. Moreover, since the algorithm is based on equi-depth ranges, if some new data are added to or removed from dataset, ranges can change significantly. In other words, u i and l i are very sensitive to the number of data. Also, the stability has not been theoretically proved for this function.

In [ 18 ], Hinneburg et al. have obtained the difference of maximum and minimum of the L p -norm for i.i.d. random vectors and for an arbitrary distribution when dimensionality grows to infinity. Aggarwal et al. [ 1 ] introduced fractional norms, an extension of Minkowski norm with a positive exponent less than one, to alleviate the instability effect. They detailed the Hinneburg X  X  work for the relative contrast for uniform distribution and then extended the results to the fractional norm. They also concluded that fractional norms are usually more suitable than Minkowski norms in terms of the relative contrast for the uniform distribution. It should be noted that fractional norms are not metric since the triangle inequality does not hold in general.

In [ 14 ], Francois et al. have shown that the instability of the norms in high-dimensional spaces is really an intrinsic instability property of the norms and not a side effect of the finite sample size. In addition, all fractional distances become unstable in high-dimensional spaces. They also have proved that the root of Pearson variation is a strictly decreasing function of p ( 0 &lt; p &lt;  X  ) when the dimensions are distributed in accordance with an i.i.d. uniform distribution over the interval [0, 1]. However, there exist some distributions for which fractional norms are not always less unstable than higher-order norms. Moreover, Francois et al. have proved that if dimensions are independent (but not necessarily identically distributed), Minkowski and fractional norms are unstable provided that the dimensions are normalized, i.e., zero mean and unit variance. They have experimentally shown that usually high-dimensional data that present a lot of correlation or dependencies between dimensions will be much less unstable than that if all dimensions are independent. Furthermore, they have shown with some examples that it cannot be said which one of the Minkowski or fractional norms is more stable, except in special cases.

As stated before, in [ 11 ]and[ 19 ], authors have proved the necessary and sufficient con-dition for the stability of distances in the high-dimensional data space. In [ 19 ], Hsu and Chenhaveintroducedshrinkage-divergenceproximity(SDP)functionforalleviatingdistance general form of SDP is defined as: where where d 1 is a distance function for one-dimensional data. The weighting parameter w i ,which reflects particular characteristics of the dataset, is determined by the domain knowledge or statistical characteristics among dimensions, subject to the importance of dimension i to the application X  X  needs. The parameters s i 1 and s i 2 are called the shrinkage threshold and diver-gence threshold of dimension i , respectively. These parameters are obtained empirically in terms of standard deviation of the dimension i . The SDP function is not metric since the trian-does not imply X = Y . One limitation of this distance function is that SDP is sensitive to its chosen parameters, and they are obtained empirically. The other limitation of the above distance function is that it cannot give the exact value of distance on the dimensions with d ( x i , y i )&lt; s i 1 . Also, the stability of this function has not been theoretically proved.
In[ 11 ],DurrantandKabanhaveshownthatforaclassofdatadistributionshavingnon-i.i.d. dimensions, namely the family of linear latent variable models, the Euclidean distance will not be unstable as long as the amount of relevant dimensions grows no slower than the overall data dimensions. An irrelevant dimension in linear latent variable model will only contain noise. In linear latent variable models, each dimension is defined based on a linear combination of the latent factors, and there is a special kind of correlation between dimensions. The limitation is that this condition is not often met in practice. It should be mentioned that the existence of correlation between dimensions does not guarantee the stability of norm distances; e.g., Beyer et al. have given an example in [ 4 ] that there is a correlation between dimensions, but the norm distance is unstable.

In [ 26 ], Ledoux has stated that the Hamming distance will be concentrated around a mean value on the high-dimensional Boolean cube equipped with counting probability measure. Also, the instability of cosine similarity has been explored by Radovanovi X  X  et al. in [ 34 ] dimensional data in a distribution-free manner and in this case has obtained a lower bound on the probability that distances become unstable. 3 The instability of norms Many additive distance functions become unstable in high-dimensional data space for a wide range of data and query distributions. Francois et al. have proved one case of instability in the below theorem.
 Theorem 1 (Francois et al. [ 14 ]) If dimensions of the data points are independent (but not necessarily identically distributed) and normalized, then fractional and Minkowski norms are unstable when dimensionality tends to infinity.

Normalization here means that for each random variable x k , subtract the variable from its mean and divide it by its standard deviation so that E [ x k ]= 0andvar [ x k ]= 1, k = 1 , 2 ,..., m . In this section, it is proved that the condition of normalization is not necessary if dimensions have finite appropriate moments and number of dimensions with nonzero variances tends to infinity as dimensionality tends to infinity. First, Theorem 2 is stated which is used in the proof of Theorem 3 : Theorem 2 ([ 11 ]and[ 19 ]) Sufficient and necessary condition of instability: Let p be a constant ( 0 &lt; p &lt;  X  ) , and X i  X  F, i = 1 , 2 ,..., n be a set of random vectors chosen independently from the query point of Q  X  F q , and d m be a distance function. The distance d ( X i , Q ) is unstable if and only if its Pearson variation tends to zero when dimensionality tends to infinity, i.e., lim function is unstable in high-dimensional space if its Pearson variation of distance distribution approaches 0 with increasing dimensionality.
 Theorem 3 In the high-dimensional space if dimensions are independent (identically or not identically distributed), fractional and Minkowski norms are unstable when dimensionality grows to infinity provided that all the appropriate moments are finite (i.e., up to the 2 p  X  X h moment, 0 &lt; p &lt;  X  ) and number of dimensions with nonzero variance ( X  2 = var [| x | p ] ) tends to infinity.
 vectors with m independent dimensions. Each dimension has a specific distribution, i.e.,  X  i : x i , k  X  F k k = 1 , 2 ,..., m . As in Francois et al. [ 14 ], we use the origin as the query considerably. Thus, we have: In accordance with Theorem 2 , for checking the instability, the Pearson variation of the distance distribution must be calculated. As in Beyer et al. [ 4 ], we can write:  X  absolute values  X ). Assume that number of dimensions with  X  2 k = 0 would be  X  m .Forthese zero, and this is in contradiction to our assumption. So, we have: if m  X  X  X  , then, according to the assumption,  X  m  X  X  X  . Therefore, var [ d p m ] ( It is observed that this proof is free of the normalization condition.

Note that in fact, Theorem 1 is a special case of Theorem 3 . In other words, this theorem is true for any kind of normalization, too. Notice that the condition of zero mean in Theorem 1 for obtaining Pearson variation,  X  k is calculated for the absolute value of the normalized data. With Theorem 3 , the set of conditions under which the additive distances, like Minkowski and fractional norms, become unstable is extended in comparison with Theorem 1 . 4 Multiplicative distance From Theorem 3 and the related works in Sect. 2 , it can be concluded that many distance functions, like Minkowski and fractional norms, are unstable in the high-dimensional space for many data distributions. As stated before, the similarity and dissimilarity functions defined in Aggarwal and Yu [ 2 ] and Hsu and Chen [ 19 ] have their own limitations. Therefore, defining propose a new distance function, named multiplicative distance, which can resist to instability in high-dimensional space for a wide range of data distributions. Compared to the additive distance, which comprises of the addition of elements, the multiplicative distance contains the productofelements.Inthissection,thedefinitionofthemultiplicativedistance,itssimilarities and differences with additive distances, its stabilities and some other characteristics of it are considered. 4.1 Multiplicative distance versus additive distances Definition Let X = ( x 1 , x 2 ..., x m ) be a random vector with x k  X  F k , k = 1 ,..., m and Q form of the multiplicative distance of X from Q is defined as: where c k &gt; 0 is named  X  X ontrol power, X  which controls the effect of each z k on the distance. z k is defined as the distance component. If on the distance. In the simple and usual form of the multiplicative distance, we have  X  k : c k = = 44.

Now, multiplicative distances can be compared with additive distances. In additive dis-tances like Minkowski norms m i = 1 | t i | p 1 / p , the distance comprises of the addition of distance components. Here, distance components are t p k where t k =| x k  X  q k | .Butthemul-tiplicative distance comprises of the product of distance components. The reason of adding  X 1 X  to | x k  X  q k | is that a distance function, like a norm distance function, must be non-descending; i.e., when a new dimension is added, the distance must be increased or remained unchanged. In the similar way, in the multiplicative distance when a new dimension is added, component, in some cases with adding a new dimension, the distance can be decreased. To | x k  X  q k | . In the summation, the neutral element is 0, while in the multiplication, the neutral element is 1. These facts are seen in the norm and multiplicative distance formulas. Since in the multiplicative distance, distance components are multiplied, the value of the multi-plicative distance is much larger than the value of the additive distance in which distance components are added. In other words, the multiplication intensifies the value of the distance when distance components are larger than 1.

There is weighted Minkowski or fractional norm m i = 1 w i | t i | p 1 / p , in which each w k controls the effect of the corresponding distance component on the distance. Similarly, in the multiplicative distance  X  X ontrol power, c  X  performs this task but with a difference. In Minkowski or fractional norms, if  X  k : w k = w , then, different values of w do not affect the Pearson variation, while in the multiplicative distance (as will be described later) if  X  k : c k = c , different values of c directly affects the Pearson variation.

It can be thought that by taking logarithm or exponent of the multiplicative distance, it is similar to L 1 -norm or SDP function without threshold, respectively. However, these are not true since logarithm and exponent are nonlinear operators, and they can change the expected So, we  X  X annot X  apply logarithm or exponent to the multiplicative distance. Furthermore, the previous distances are based on summation of components, but the multiplicative distance is based on the product, and the nature of the product is different from the nature of the summation. Moreover, the multiplicative distance function gives the exact value of distance in contrast to the functions defined in Aggarwal and Yu [ 2 ] and Hsu and Chen [ 9 ]. 4.2 Multiplicative distance and its stability in high-dimensional space Now, in the following theorem, it is proved that for data with independent dimensions (but not necessarily identically distributed), the multiplicative distance is stable in the high-dimensional data space, in contrast to additive distances which are unstable (Theorem 3 ). Theorem 4 If the set of random vectors X i = ( x i , 1 ,..., x i , m ) ,i = 1 ,..., n are data 1 from all X i , then the multiplicative distance of X i from Q is stable when dimensionality tends to infinity.
 Proof For stability, the Pearson variation of the multiplicative distance must be examined as the dimensionality rises to infinity. Under the independent assumption of x i , k , z i , k = 1 +| x i , k  X  q k | are independent variables. when dimensionality tends to infinity, definitely, a does not tends to zero. Therefore, it is sufficient to show that independent, where g k (.) is a deterministic function. Thus, the Pearson variation can be written: Therefore, the Pearson variation can be written: It is shown that when m  X  X  X  ,ifall  X  2 i are not very close to zero, the Pearson variation does not tend to zero, so the multiplicative distance is stable.
 with q k  X   X  F is the query point chosen independently from all X i , then the multiplicative distance of any random vector to the query point is stable when dimensionality grows to infinity. 4.3 More on the multiplicative distance Figure 1 depicts the L 1 norm, L 2 norm, L 0 . 79 norm, and the multiplicative distance with representing the curves, they are shown in two sub-figures. The sub-figures are depicted just for the first region of Cartesian coordination. The other three regions can be depicted by symmetry. The reason for choosing the L 0 . 79 norm is that its curve is similar to the ( a ) (b) multiplicative distance for c = 1 (simple form). In these sub-figures, the multiplicative distance is denoted by MD. As shown in Fig. 1 , with decreasing c , the range of x 1 and x 2 is increased.

As an important point, since the multiplicative distance is the product of distance compo-nents,whenthenumberofdimensionsisincreased,therecanbeacaseinwhichforcalculating the distance, overflow occurs. To remove this problem, the small c can be used. The overflow depends on the programming language, number of dimensions, and distribution of data. So, usually, the value of c cannot be determined in advance. Moreover, the multiplicative distance is not metric since the triangle inequality does not hold in general. However, the influence of the triangle inequality may be insignificant in many applications such as clustering, espe-cially those for high-dimensional space [ 12 , 22 ]. For a metric distance, number of distance calculations can be reduced in the fast nearest neighbor search using the triangle inequality. But since the triangle inequality does not hold for the multiplicative distance, all distances from data points to the query point should be calculated for the multiplicative distance. In the following, we consider the effect of c on the Pearson variation.
 the Pearson variation of the multiplicative distance is an increasing function of c. Proof For simplicity, we set MD ( X , Q ) = m k = 1 z c k  X  1 = Z c  X  1 = Z c 1 .So,thePearson variation of the multiplicative distance equals to E Z 2 c 1 following equation: where Y = log ( Z ) and m Y ( c 1 ) = E e c 1 Y is the moment generating function of Y . A well-known fact is that a moment generating function is log-convex on the interval where it exists. and an increasing function of c .
 To show the effect of c on the Pearson variation, the following experiment is performed. The three synthetic 300-dimensional sample datasets were generated as follows: For the first uniform distribution U(0,1). Similarly, for the second and third datasets, entries were ran-domly sampled from a normal distribution N(0,1) and an exponential distribution Exp(0.5), respectively. Number of data points for each dataset is 1,000. Figure 2 shows the effect of c on the Pearson variation. As it can be observed, the Pearson variation is increased with increasing c .

Since Pearson variation of the multiplicative distance is an increasing function of c ,there is no global or local optimal value for the Pearson variation with respect to c .Thelarger c is, the larger Pearson variation. But large c can make overflow. Also, when there is no superiority between dimensions, the  X  c  X  is selected the same for all dimensions. So, usually, c = 1 is selected for simplicity.

Major properties of the multiplicative distance function are listed as follows:  X  Stability of the multiplicative distance is proved theoretically for data with independent  X  Pearson variation of the multiplicative distance is an increasing function of c .  X  Multiplicative distance is not a metric. A nonnegative function d :  X   X   X   X  R ( R is the  X  Multiplicative distance is not homogeneous, i.e., MD ( X  X , X  Q ) =  X  MD ( X , Q ) . 5 Experimental evaluations To provide a practical perspective and check the stability/instability of the multiplicative and norm distances for data with independent and correlated dimensions, some simulations are conducted on both synthetic (independent) and real (correlated) datasets. Section 5.1 illustrates some characteristics of datasets and experimental setup. In Sect. 5.2 , the Pearson variation and the relative contrast of datasets are considered for the multiplicative distance and some norm distance functions. A statistical test is also used for assessing the stability of distances. In Sect. 5.3 , these distance functions are applied to the k-means clustering algorithm to have a comparison between these functions for a real application. The effect of noise on distances is also considered on the clustering application. 5.1 Experimental setup To compare the performance and stability of the multiplicative distance with some well-known distances, various datasets with different properties in diverse domains were selected. The datasets were chosen from UCI machine learning repository 1 and LIBSVM dataset. 2 These domains (in order of their appearance in Table 2 ) are iris plant prediction, liver patient prediction, multispectral satellite image detection, sonar signal detection, handwritten text recognition, synthetic dataset, confusable digit handwritten detection, gene monitoring for diseases, and farm animal advertisement detection. Except Madelon (synthetic dataset), other datasets are real. Datasets have different number of classes and instances. Table 2 summarizes these datasets and some of their characteristics. Specifically, some of datasets have few fea-tures to compare the multiplicative distance with other distances even for the low-dimensional data.

Features of data can be measured in different units. Therefore, normalization should be performed before calculating distances. Generally speaking, there are two kinds of normal-izations. In the first kind, features are subtracted from their mean ( X  i ) and divided by their standard deviation ( X  i ) so that the normalized variables have zero mean and unit variance; i.e., In the second kind of normalization, each variable ranges from zero to one; i.e., Both of these normalizations are considered in this paper for evaluating the effect of normal-ization on the results. 5.2 Stability checking To asses the stability of the multiplicative distance and compare it with norm distances, first, the Pearson variation and relative contrast criteria are addressed [ 4 ]. The query point is the origin. The Pearson variation and the respective relative contrast for datasets with two types of in Tables 3 , 4 , 5 ,and 6 . Generally, in the simulation the simple form of the multiplicative distance ( c = 1 ) is used. But for some datasets, selecting c = 1 leads to the overflow. Hence, for these datasets and for simplicity, the largest value of the negative integer powers of 10, i.e., 0.1, 0.01, ... , that overflow does not happen, is selected as the control power. For Iris, Liver, Satellite, Sonar, Usps, and Madelon, c = 1, for Gissete, c = 0 . 1, and for Leu and Farm, c = 0 . 01, were selected based on the experiments.

Tables 3 , 4 , 5 ,and 6 show the Pearson variation and the relative contrast for the norm and multiplicative distances for two kinds of normalizations. It is shown that Madelon, which is an artificial dataset with independent dimensions, has very small Pearson variation and relative contrast for both types of normalizations for norm distances. This shows the instability of norm distances for independent data. Other datasets with high dimensionality usually have small Pearson variation and relative contrast for norm distance functions. Since other datasets are real, there is correlation between their dimensions. This correlation causes that datasets with higher dimensionality, and their Pearson variations and relative contrasts are larger than the case in which dimensions are independent (Madelon dataset). Consequently, norm distances are less unstable for other (real) datasets.

Furthermore, it is observed that Pearson variation and relative contrast for Madelon is large for both types of normalizations for the multiplicative distance. These results show the stability of the multiplicative distance for independent data in the high-dimensional data space. For other datasets with high dimensionality, which there exists correlation between their dimensions, the multiplicative distance has large Pearson variation and relative contrast for both types of normalizations. These results demonstrate the stability of the multiplicative distance in the high-dimensional data space for the correlation case. Of course, as it is observed in the proof of Theorem 4 ,iftherearedatathatall  X  2 i are very close to zero, the multiplicative distance becomes unstable.

All datasets have large Pearson variation and relative contrast for the multiplicative dis-tance in comparison with the norm distances. In contrast to the norm distances which are usually unstable for high-dimensional data, the multiplicative distance is stable. The multi-plicative distance is also stable for low-dimensional data (Iris dataset) as the norm distance is stable.

The type 2 of normalization does not always have smaller Pearson variation and relative rather than the type 1 of normalization, although the range of values in the type 2 is usually smaller than the type 1. Regarding Pearson variation, there is no superiority for two kinds of normalizations over each other for distance functions. From the relative contrast point of view, this matter is true for the norm distances. However, for the multiplicative distance, the first type of normalization gives larger relative contrasts. Therefore, the first type of normalization is better than the second type. Nevertheless, both of these normalizations have a large Pearson variation and relative contrast for the multiplicative distance and can be used in high-dimensional space.

The reason that the Pearson variation is large for the multiplicative distance is that, as it is seen in the proof of Theorem 4 (e.g., for data with independent dimensions), the Pearson variation is approximately equal to the product of values larger than one. For this reason, when dimensionality is increased, the Pearson variation is increased. Of course, the Pearson variation is also dependent on c and large c makes large Pearson variation. For example, if for Farm dataset for c = 1 overflow did not occur, its Pearson variation would be larger than in the case of c = 0 . 01.

Another way for assessing distance instability/stability is the statistical test proposed in Kaban [ 24 ] that gives a finite-dimensional characterization of the distance instability phenomenon based on the following theorem.
 Theorem 6 [ 24 ] Let X i , i = 1 ,..., n, be independently drawn m-dimensional sam-ple points from some distribution F, and Q be the query point. Denote DMIN m ( n ) = min
P { DMAX m ( n )&lt;( 1 +  X ) DMIN m ( n ) } X  1  X  where ( u ) + = max ( 0 , u ) and
It follows that this bound ( 17 ) is tight in the neighborhood of probability 1 when the probability of distance instability P { DMAX m ( n )&lt;( 1 +  X ) DMIN m ( n ) } is high. In other words, it means that for a specific value of  X  , what is the lower bound on the probability that instability happens? This makes this bound appropriate as a statistical test to identify whether a given distance function suffers from the instability problem in some unknown data distribution. This probability bound requires that the true Pearson variation of the distance distribution induced by F is known. However, F is most often unknown in practice. Instead, we have data samples drawn from F . For this reason, the following theorem is used in which the true Pearson variation is replaced with its sample estimate.
 Theorem 7 [ 24 ] Let X 1 ,..., X n  X  F be random samples, n  X  2 , and Q be the query let Y 1 ,..., Y r  X  F be observed data samples, r  X  2 . Assume P { d m ( X 1 , Q ) = ... = d ( X n , Q ) = d m ( Y 1 , Q ) = ... = d m ( Y r , Q ) }= 0 . Then, where PV m , r ( p ) is the estimated Pearson variation from the dataset Y 1 ,..., Y r .
Figure 3 shows the lower bounds on the probability that norm and multiplicative distances become unstable for datasets in the underlying unknown data distributions, for n = 2 , p = 1 [ p in the formula ( 17 )], plotted against a range of deviation  X  .InFig 3 , N1 and N2 represent first and second types of normalizations, respectively. The reference query point is the origin. Again, it is observed that for norm distances, even for small values of  X  , the probability of instability is usually high. This shows that norm distances are unstable for the dataset with independent dimensions and usually for the datasets with correlated dimensions. However, the lower bounds on the probability of instability remain zero for the considerable range of  X  datasets (with independent and correlated dimensions) and for two kinds of normalizations. ( a ) (b) (c) ( d ) (e) (f) ( g ) (h) visible because it coincides with the curve of type 2 of normalization for this distance. Also, from the figures it is clear that the most unstable case is for Madelon for norm distances. 5.3 Clustering application For the comparison of multiplicative distance with other distance functions, the performance of the aforementioned distances is investigated on a real application. For this purpose, we choose clustering. There are many clustering algorithms but maybe the most popular one is k-the optimization process of grouping them into k clusters so that the global criterion function: distance function, the global criterion must be minimized. The steps of k-means algorithm are as follows: (a) Select k initial cluster centroids. (b) For each point of the dataset, compute the clustering criterion function with each cluster (c) Recalculate k centroids based on the documents assigned to them. (d) Repeat Steps (b) and (c) until convergence.
 A number of clustering evaluation techniques exist in the literature. The most commonly used as the result of a query, whereas each preclassified set of points can be considered as the cluster j for each class i can be calculated. If n i is the number of the members of the class i , m j is the number of the members of the cluster j ,and k ij is the number of the members of the class i in the cluster j ,then P ( i , j ) and R ( i , j ) can be defined as: The overall clustering performance is measured by F -measure value. In other words, the F -measure is a harmonic combination of the precision and recall values. The F -measure corresponding F -measure F ( i , j ) for P ( i , j ) and R ( i , j ) is defined as: where  X  is the relative importance of clustering precision versus clustering recall. In this paper,  X  is chosen to be 1, assigning equal importance to cluster precision and recall. Then, the F -measure for the whole clustering result is defined as [ 29 , 43 ]:
The F -measure values are in the interval [0, 1]. The larger F -measure value indicates the higher clustering quality. In Tables 7 and 8 ,the F -measures have been shown for the datasets for two types of normalizations.

BasedonTables 7 and 8 , the multiplicative distance outperforms norm distances for high-dimensional clustering. Furthermore, it can be observed that for norm distances, usually the second normalization has better results; but for the multiplicative distance, the first kind of normalization has superiority to the second type. Moreover, the multiplicative distance has a good performance even for the low-dimensional data.

It is useful to note that although changing the control power affects the Pearson variation and relative contrast, it does not have an effect on the F -measure for the multiplicative distance provided that with changing the control power, the aforementioned stability criteria (Pearson variation and relative contrast) have large values. In this situation, slightly changing the centroids does not change the points belonging to each centroid unless the changes of the query points are considerable; or the relative contrast or the Pearson variation are near to zero. For considering the effect of the control power on the clustering performance, different values of c have been applied on the multiplicative distance for Madelon (synthetic) and Farm (real) datasets. The results are displayed in Tables 9 and 10 . As it is seen, when the control power changes, until Pearson variation is large, the F -measure remains constant. When small c is used such that Pearson variation is small, the performance decreases.

For considering the effect of noise on these distance functions, a white Gaussian noise with zero mean and unit variance is added to each feature of the un-normalized data and then normalization is performed. The F -measure results for noisy datasets are displayed in Tables 11 and 12 . These results have been obtained by averaging on 100 experiments. The values in the parentheses represent standard errors in percentage.
In the noisy case, usually, the multiplicative distance function has better F -measures in comparison with norm distance functions. Based on the experiments, it is concluded that both kinds of normalizations yield good results for the multiplicative distance in terms of clustering performance, although the first type of normalization slightly outweighs. 6Conclusion In the high-dimensional data space, many distance functions become unstable under a broad set of conditions. In this paper, it was proved that for independent dimensions with finite appropriate moments, norm distance functions become unstable without any condition on thenormalizationofdimensions.Moreover,astabledistancefunctioninthehigh-dimensional data, named multiplicative distance, was introduced. This distance function is based on the multiplication of distance components, contrarily to the usual distances functions which are based on the summation of distance components. It was theoretically proved that the multi-plicative distance function was stable for data with independent dimensions (with identical or nonidentical distribution) in high-dimensional data space. Experimental results showed the superiority of the multiplicative distance over the norm distance functions in terms of stability and clustering results for data with independent and correlated dimensions in the high-dimensional space. Furthermore, the multiplicative distance functions could be used in low-dimensional space in applications such as clustering.
 References
