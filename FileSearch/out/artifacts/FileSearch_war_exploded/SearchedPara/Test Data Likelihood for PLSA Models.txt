 1. Introduction Probabilistic Latent Semantic Analysis is a statistical latent class model (or aspect model) that recently has shown excellent results in several IR related tasks (Gildea and Hofmann 1999, Hofmann and Puzicha 1998, Hofmann 2000). It has the positive feature of assigning probability distributions over classes to documents and words, unlike other clustering tech-niques that do a hard assignment to one class. Aspect models are also successfully used in other areas of natural language processing, like language modeling (Saul and Pereira 1997) or parsing (Rooth et al. 1999).
 proposes zero probabilities for unseen documents during training and then ignores this fact during testing when calculating probabilities conditioned on the test document.
As we will show, one of the two alternative formulations of the Expectation-Maximization algorithm for PLSA does not assume zero-probabilities for test documents, which favors derive a new test data likelihood substitute and compare it empirically with three existing and previously used likelihood substitutes. Our new likelihood substitute has two advantages ove r previously used measures: first, we show that its maximum is achieved by the same 182 BRANTS parameter setting as the true but unknown likelihood, and second, the empirical comparison reveals a good qualitative match between the new likelihood substitute and task results, which makes it best suited to determine the number of EM steps for training PLSA in an unsupervised setting.

As an additional result of our comparison we find evidence that PLSA is very robust previously been reported to be very sensitive to overfitting when applied to information retrieval tasks (Blei et al. 2001, Gildea and Hofmann 1999, Hofmann and Puzicha 1998, Hofmann 2000) even with small numbers of classes. Therefore, the studies usually proposed using tempered EM in order to avoid overtraining. On the other hand, studies using aspect models for language modeling and parsing report these models to be relatively robust against ov ertraining when the number of classes is much smaller than the size of the vocabulary (Saul and Pereira 1997, Rooth et al. 1999). Our experiment plotting the number of EM steps vs. retrieval and segmentation task results show that PLSA is not sensitive to overfitting in these two tasks and therefore render techniques like tempered EM for PLSA unneccary.
The rest of the paper is structured as follows. Section 2 shortly introduces the PLSA model. Section 3 presents an alternative formulation and points out its advantage of avoiding zero probabilities for test documents. Section 4 introduces our new method of calculating substitutes for PLSA test documents likelihoods and also presents three existing methods. Section 5 empirically compares the four likelihood measures against each other and against results in document retrieval and text segmentation. Section 6 contains conclusions. 2. The PLSA model Probabilistic Latent Semantic Analysis (PLSA) is a statistical latent class model or aspect model (Hofmann 2000, Hofmann 1999). The model is fitted to a training corpus by the Expectation Maximization (EM) algorithm (Dempster et al. 1977). It assigns probability distributions over classes to words and documents and thereby allows them to belong to PLSA represents the joint probability of a document d and a word w based on a latent class v ariable z 1 : PLSA has the following view of how a document is generated: first a document d  X  D (i.e., its dummy label) is chosen with probability P ( d ). For each word in document d ,a latent topic z  X  Z is chosen with probability P ( z | d ), which in turn is used to choose the word w  X  W with probability P ( w | z ).
 A model is fitted to a document collection D by maximizing the log-likelihood function L : T EST DATA LIKELIHOOD FOR PLSA MODELS 183 The E -step in the EM-algorithm is and the M -step consists of The parameters are either randomly initialized or according to some prior knowledge.
The parameters P ( w | z ) obtained in the training process are used to calculate P ( z | q ) for new documents q (queries) with the folding-in process. Folding-in uses Expectation-Maximization as in the training process; the E -step is identical, the M -step keeps all the P ( w | z ) constant and re-calculates P ( z | q ). Usually, a very small number of iterations is sufficient for folding-in. 3. Alternative PLSA formulation 3.1. Zero-probability documents probabilities used for folding-in of query documents are P ( z | q ). Bayes X  formula allows the reverse conditioning A subtle but crucial point of PLSA is that we do not know P ( q ). Even worse, the model as it is usually formulated (Hofmann 2000) (and as it is shortly presented in the previous is the training collection. This can be derived from Eq. (5). For any given z  X  Z ,wehave 184 BRANTS
The PLSA formulation based on P ( d | z ) simply ignores this zero-probability problem 3.2. PLSA reformulation We now switch to a formulation of EM for PLSA that was used in Gildea and Hofmann (1999) and Hofmann (2001). These publications presented the reformulation without giving a motivation for preferring one version or the other, and without addressing implications. We now use probabilities P ( z | d ) instead of P ( d | z ). The E -Step is and the M -step consists of
This section investigates the differences between the two formulations. The log-likelihood function (2) can be written as because of Both parts of Eq. (12) can be optimized independently of each other (cf. Hofmann and d  X  D in order to obtain valid probabilities conditioned on d . And they require in order to reserve probability mass to unseen documents q (i.e., having P ( q ) &gt; 0 for T EST DATA LIKELIHOOD FOR PLSA MODELS 185 unseen documents, but the model now accounts for them and no longer postulates zero training because it does not show up in the EM steps. We only need probabilities conditioned on d ,s ow e assume P ( d ) &gt; 0 and P ( q ) &gt; 0.
 i.e., folding-in of q proceeds without information about other test documents q .
To sum up, the reformulation maximizes the conditional part of the likelihood function can assume P ( q ) &gt; 0 for documents q not in the training set. To our knowledge, this motivation for the particular PLSA model formulation has not been published before. likelihoods is very useful when determining the number of EM-steps on a held-out data set during unsupervised learning. 4. Likelihood calculation The EM formulation for PLSA in Section 3.2 eliminates the weakness of assigning zero probabilities to unseen documents. However, it does so by avoiding to assign a particular likelihood to any of the documents. Neither the usual formulation nor the reformulation can assign a likelihood to an unseen query document. Previously, three different test data for unseen documents based on folding-in. This method is derived from the PLSA likelihood maximization definition. The method does not give us the true likelihood values, but we show that it is maximized at the same points in the parameter space as the true likelihood. We then present the three existing likelihood substitutes.
 Fo r the sake of brevity, we make two simplifying assumptions without losing generality. First, we do not use logarithms here (for an implementation, however, logarithmic values need to be used). Second, we treat documents as multi-sets; if a term occurs several times, it is assumed to be repeated in q . This saves us from keeping track of frequency counts f ( q ,w ) and lets us concentrate on the differences of the four alternatives. 4.1. Likelihood based on folding-in on the model parameters  X  , which is in analogy to (2): 186 BRANTS In the following, we will omit  X  from the notation. (15) can be re-written as where | q | is the number of words in test document q . The difficult part is coming up with the calculation.

Further separating the elements in (17) yields The left part in square brackets is not dependent on the model parameters P ( w | z ) and independently in order to find the product X  X  maximum. In case we are interested in finding (conditional) part unseen test collection Q uses (20): where P fi marks the probabilities obtained by folding-in, the others are obtained from the maximazition we find exactly the same parameter setting as if we had maximized the true likelihood, which follows from the decomposition in (19).
 T EST DATA LIKELIHOOD FOR PLSA MODELS 187 4.2. Likelihood based on marginalizing over the training documents Another method estimates P ( q )b y marginalizing over all training documents d , yielding: This method was used in Blei et al. (2001) for the comparison of different models. Its view that a particular training document d generates all words w of the test document and then av erages over all training documents. Likelihood based on marginalizing over the training documents conditions word probabilities on each training document separately and then av erages. Unfortunately, this does not match very well the view of PLSA that a document is generated by its own (and possibly unique) mixture of topics z .W e therefore expect this likelihood estimate in general to be too low. 4.3. Likelihood based on a word unigram model marginalizing out d and z from the PLSA model and then calculates test document likelihood by multiplying the word probabilities: As with the previous methods, this way of calculating a likelihood ignores the PLSA view of how a document is created: select a document, select a mixture of topics for the document, select the words based on the mixture of topics. 4.4. Likelihood based on partial prediction The fourth method is based on a split of the test data and using one part to predict the w ords of the other part. Each document q in the test collection Q is split into two disjoint terms w in the second part  X  2 ( q ) based on P ( z |  X  1 ( q )): 188 BRANTS In other words, this method does not calculate the probability of a complete new document, bu t instead it calculates the probability of additional words given that it already knows some of the words in the document. This yields the following likelihood based on partial prediction:
This method of calculating a likelihood substitute was used in the work by Hofmann e.g. (Hofmann 2001, Hofmann and Puzicha 1998). 2 Their plots of the perplexity (which is e.g. those presented in figures 5 and 6 of Hofmann and Puzicha (1998), or those presented in figure 7 of Hofmann (2001). 4.5. Concluding remarks on the four methods PLSA does not provide a way of calculating test data likelihoods. Our newly presented method, calculating the likelihood based on folding-in, circumvents the problem by calcu-lating only a part of the likelihood, but, as we showed, this part is maximized by the same arguments that maximize the true likelihood. The other three methods try to circumvent the problem by marginalizing over one or two parameters, or by predicting one part of a test document based on another part. Likelihood based on folding-in directly follows the view of PLSA of how a document is generated ( d  X  z  X  w ). The other three methods deviate substantially from the PLSA generative view and we therefore expect the folding-in likelihood to be best suited.
 Sections 4.1 to 4.4 presented the likelihood methods using notational simplifications. Practical applications would perform the calculations using logarithms, and they would not simply repeat multiple occurrences of the same word but directly model the frequency f ( d ,w ). We therefore repeat the four likelihood formulas, but now are using logarithms and f ( d ,w ).
 F olding-in log-likelihood (cf. Eq. (21)): Document-marginal log-likelihood (cf. Eq. (23)): Document-marginal log-likelihood is the most difficult to calculate of the four methods. Because of the sum within the logarithm the product over all words in q needs to be actually T EST DATA LIKELIHOOD FOR PLSA MODELS 189 calculated, which means one has to take care of floating point underflows for long test documents.
 W ord unigram log-likelihood (cf. Eq. (25)): Likelihood based on partial prediction (cf. Eq. (28)): 5. Experiments The experiments reported in this section aim at aligning the four likelihood measures pre-sented in the previous section with error rates obtained in information retrieval tasks. The first task is a standard retrieval task, the second task evaluates on text segmentation. 5.1. Corpora and pre-processing Fo r the retrieval task, we used the MED collection, consisting of 1,033 document abstracts from the National Library of Medicine, 15 queries, and manually assigned relevance judge-ments for each of the 15 queries.

Fo r the segmentation task, we used the ModApte split of the Reuters-21578 Corpus. The training set consists of 7,769 news articles with approx. 1.1 million tokens. The test set consists of 3,019 articles with approx. 400,000 tokens. The corpus was prepared according to the description in Li and Yamanishi (2000). Exactly two original documents from the test part are concatenated to form one test document, one from one of the 10 larger categories, one from a randomly chosen other category. In total, 500 test documents are generated. The task is to find the boundary of the two original documents. More details about the segmentation task can be found in Hearst (1997) and Choi et al. (2001). The error metric is described in Beeferman et al. (1999). The use of PLSA for the segmentation task is described in Brants et al. (2002).

In both cases, the corpora were stemmed, stopwords and words with frequency 1 were removed, the rest was mapped to lower case. 5.2. Likelihood calculation ranging from 16 to 8,192 in powers of 2, and repeated all experiments using several different random EM initializations. Results using the different numbers of classes and initializations where very similar. We therefore present as representatives details of the experiments with 190 BRANTS 32 classes and 128 classes. The PLSA models were randomly initialized. After each EM step, to 4 show the negative log likelihoods for steps 1 to step 300 (please note that lower values mean higher likelihood and vice versa since we are plotting the negative log likelihood). four plots, then flattens out and changes only very slighly afterwards.
 and then rapidly decreases towards 0 (with the exception of the graph for the Reuters Corpus with 32 classes is slightly different; we further increased the number of EM steps and found the likelihood is down to random level after 20 X 80 EM iterations, i.e. at the same level where it was after initialization. The steep decrease in document marginal likelihood can be explained by Eq. 23 and that with larger numbers of classes PLSA tends to move most T EST DATA LIKELIHOOD FOR PLSA MODELS 191 are (almost) identical to one of the training documents. The flaw is that this likelihood uses the probability that any training document has generated the vector of words of the document has generated them, P ( w | q ).

W ord unigram likelihood L w test increases sharply from random level for only very few EM iterations (in some cases for one iteration only), and stays almost constant thereafter. The Reuters Corpus graph shows a small decrease around 10 iterations, the MED graph levels out immediately. Using this as a model performance measure would mean that the model MED case. This way of calculating likelihood suffers from its very restricted underlying model, word unigrams, which is not the way PLSA assigns probabilities. 192 BRANTS
P artial prediction likelihoods show similar characteristics as document marginal likeli-hoods, with a slightly earlier maximum (after the initial 5 to 15 iterations) and a sharper increase thereafter. 5.3. Recall/precision and error rate We now present results on IR tasks for PLSA models that are trained with varying numbers of EM iterations. The goal is a qualitative comparison with the four methods of likelihood calculation. Therefore, we do not use additional methods like tempered EM or interpolation with other models to improve results but report plain PLSA results.

The task performed on the MED corpus is document retrieval. The test set consists of 15 queries, each consisting of one line of text. The goal is to find relevant documents in the collection. Automatically retrieved documents are compared against manual judgements T EST DATA LIKELIHOOD FOR PLSA MODELS 193 on the documents. We report the recall/precision breakeven point, i.e., we take as many documents from the top of the ranked retrieved list of documents until recall and precision have the same value. The recall/precision breakeven point is plotted against the number of EM used for training the PLSA model. Results are shown at the bottom of figures 1 and 2. We do not find any overfitting during the first 300 iterations for the model with 32 classes. relatively constant after 50 iterations. This result is a surprise since PLSA was previously reported to require tempered EM to avoid overfitting (e.g., Hofmann (2000) and Blei et al. (2001)).

The second task is text segmentation, performed on the Reuters corpus. Results (error rates) are shown in the bottom part of figures 3 and 4. Again, we find improving results no sign of overtraining within 300 iterations. 194 BRANTS
Qualitatively comparing the graphs of the four likelihood measures with the retrieval and segmentation results, the curves of folding-in likelihood and task performance match best in the overall shape. Document marginal likelihood and word unigram likelihood have very different curves. Folding-in likelihood even predicts the slight decrease in recall/precision around iteration 20 for the retrieval task with 128 classes (figure 2). The newly presented folding-in likelihood was derived from the PLSA likelihood definiton as it is used for the Expectation-Maximization algorithm and empirically matches recall/precision and error view of PLSA. Determining the task performance at the early stopping points for the four less than 0.1%) also shows the good fit of folding-in likelihoods (cf. Table 1). In all four e xperiments, folding-in likelihood comes closest to the optimum.

Our empirical results might also hint towards an explanation why Hofmann et al. found better results for tempering than for early stopping in their experiments on the MED corpus. Fo r likelihoods based on partial prediction, maximum likelihood on the test or held-out data wa s achieved too early. As an example, partial-prediction log-likelihood in figure 2 reaches the maximum of  X  97,792 after 7 iterations. At this point, the recall-precision break-even point is at 55.99%. Recall/precision maximum is at a later point: it is 57.23% after 16 iterations. When using the partial-prediction likelihood, tempering seems necessary in order to go beyond the too early stopping point. However, folding-in likelihood does not need this heuristic addition of tempering. It achieves the maximum of  X  17,358 after 17 iterations and therefore comes very close to the optimal performance.

Similarly, document marginal likelihoods reach the maximum too early: in figure 2, document marginal log-likelihood reaches the maximum of  X  19,270 after 9 iterations. Results also favor folding-in likelihood in the other cases: partial-prediction likelihoods and document marginal likelihoods have their maximum too early. Tempering might still be useful in order to escape unfavorable local maxima, but it is not necessary to avoid ov erfitting in the two presented tasks. 6. Conclusions Investigating the two alternative formulations of EM for PLSA we observed that the use of T EST DATA LIKELIHOOD FOR PLSA MODELS 195 documents. However, both formulations do not assign a particular likelhood value to test documents which makes it necessary to resort to substitutes.

We presented a new test data likelihood substitute that we derived from the PLSA likeli-hood definition as used for the Expectation-Maximization algorithm. This was named the folding-in likelihood.

We plotted graphs of the folding-in likelihood as well as three other likelihood substitutes: document-marginal likelihood, word unigram likelihood, and partial-prediction likelihood. sidered, document marginal likelihood and partial-prediction likelihood both suggest that there is heavy overfitting, word unigram likelihood seems unusable because it drops for v ery few (sometimes only one) iterations and then stays almost constant.

Claims in the literature that PLSA models in retrieval are very likely to overfit when using (untempered) EM cannot be confirmed; more to the contrary, PLSA models seem v ery robust. To our knowledge, this is the first investigation that systematically increases the number of EM steps for a PLSA model and directly reports recall/precision/error rate are poor performance predictors. Earlier claims that early stopping yields poor results where based on the use of partial-prediction likelihood or document-marginal likelihood which we showed to predict the maximum too early.

Comparision to recall/precision in a retrieval task and error rate in a segmentation task showed that the best predictions are made by folding-in likelihood both from the overall This suggest that folding-in likelihood is well suited to determine the number of EM steps in an unsupervised learning setting.
 Acknowledgments Ia mv ery grateful to Francine Chen, Thomas Hofmann, Jay Ponte, Kris Popat, Stefan Riezler, and Prateek Sarkar for valuable comments and discussions on the topic of this paper.
 Notes References 196 BRANTS
