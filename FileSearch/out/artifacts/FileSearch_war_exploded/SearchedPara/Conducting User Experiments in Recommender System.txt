 There is an increasing consensus in the field of recommender systems that we should move beyond the offline evaluation of algorithms towards a more user-centric approach. This tutorial teaches the essential skills involved in conducting user experi-ments, the scientific approach to user-centric evaluation. Such experiments are essential in un covering how and why the user experience of recommender systems comes about. H.1.2. [Models and principles] : User/Machine Systems X  software psychology ; H.5.2 [Information Interfaces and Presentation] : User Interfaces X  evaluation/methodology ; H.4.2. [Information Systems Applications] : Types of Systems X  decision support Measurement, Experime ntation, Human Factors, Standardization. User experiments, recommender systems, user experience, user-centric evaluation. From a methodological perspective, the evaluation of recom-mender systems has undergone an interesting development [8]. The recent focus on  X  X ser-centric X  evaluation [4, 11] is inspired by the suggestion that higher accuracy does not always mean higher user satisfaction [9], and that the algorithm accounts for only a small part of the real-world relevance of a recommender system. Other aspects such as the presentation and interaction have a signification impact on the user experience [2, 5, 10]. To make inferences about the users X  experience, we need to move beyond measuring their behavior, and measure their subjective valuations as well [6]. Moreover, as users X  interaction with recommender systems is highly cont ext-dependent [1, 3], personal and situational characteristics also need to be taken into account. In Knijnenburg et al. [6] we pr esent a framework for the user-centric evaluation of recommender systems that takes all these aspects into consideration (Figure 1). This framework can be used as a guideline for user experiments to reveal how and why the user experience of recommender systems comes about. However, conducting such experiments is a complex endeavor. How does one test whether a certain system aspect has a significant influ-ence on e.g. users X  satisfaction with the system? How does one measure a subjective concept like  X  X ser satisfaction X  to begin with? Although it may make intuitive sens e to test the user experience of a recommender system in a holistic fashion, such  X  X est every-thing at once X  evaluations cannot discern the specific causes of the user experience. Good user e xperiments instead try to single out the effects of specific aspects of the system. To single out the effect of an aspect, one needs to manipulate that aspect by creating two or more conditions (versions of the aspect). The tutorial covers adequa te manipulations, and di scusses the pros and cons of between-subjects (i.e. each participants gets to see only one condition) and within-subjects (i .e. each participant gets to see all conditions) experiments. Measuring user behavior is insufficient to make inferences about the user experience. Behavior is highly context-dependent and difficult to interpre t. Subjective valuations, gathered through questionnaires, typically provide a more robust measurement of the users X  experience with the recommender system. Moreover, subjective evaluations are better predictors of longer-term system goals such as adoption and user retention. This part of the tutorial teaches the art of creating questionnaire items, typically presented as statements to which users can agree or disagree on a 5-or 7-point s cale. Currently, researchers typical-ly use one such questionnaire item for each concept (e.g. satisfac-tion, perceived control, understa ndability) that they want to measure. This tutorial instead makes the case for creating multi-item measurements for each concept. It presents factor analysis as a statistical method to turn such multi-item measurements into robust unidimensional scales. Statistical analysis of user-centric resear ch typically involves correlations, t-tests and linear regressions. This part of the tutorial presents structural equation models as a more sophisticated and modern statistical method to make causal inferences. Structural equation models can test complex causal structures, such as whether a certain manipulation (e.g. a different algorithm) has a significant influence on users X  pe rceptions (e.g. perceived recom-mendation quality), and whether this perception in turn influences their experience (e.g. system effectiveness), and behavior (e.g. item ratings). The tutorial concludes by returning to the Knijnenburg et al. [6] evaluation framework. This framework for the user-centric evaluation of recommender systems can be used to develop causal hypotheses, to select and constr uct subjective measures, and to integrate new and existing user-centric research on recommender systems. If you work on recommender systems X  X s a system developer, an algorithms researcher, or a user interface designer X  X ser-centric evaluations are the way to go. This tutorial presents user experi-
