 ORIGINAL PAPER Cristina Giannone  X  Roberto Basili  X  Paolo Naggar  X  Alessandro Moschitti Abstract In this paper, we present models for mining text relations between named entities, which can deal with data highly affected by linguistic noise. Our models are made robust by: (a) the exploitation of state-of-the-art statistical algorithms such as support vector machines (SVMs) along with effective and versatile pattern mining methods, e.g. word sequence kernels; (b) the design of specific features capa-ble of capturing long distance relationships; and (c) the use of domain prior knowledge in the form of ontological con-straints, e.g. bounds on the type of relation arguments given by the semantic categories of the involved entities. This prop-erty allows for keeping small the training data required by SVMs and consequently lowering the system design costs. We empirically tested our hybrid model in the very com-plex domain of business intelligence, where the textual data are constituted by reports on investigations into criminal enterprises based on police interrogatory reports, electronic eavesdropping and wiretaps. The target relations are typi-cally established between entities, as they are mentioned in these information sources. The experiments on mining such relations show that our approach with small training data is robust to non-conventional languages as dialects, jargon expressions or coded words typically contained in such text. 1 Introduction Mining relational patterns is a core activity of data mining testified by important previous work, e.g. [ 16 , 17 , 48 ]. From human being perspective, such research assumes a particular interest when the involved data are natural language doc-uments and the relationships are defined between entities described in text, e.g. [ 23 , 24 , 34 , 47 ].

The relation extraction (RE) from text has been standard-ized by the automatic content extraction (ACE) program [ 15 ] as the task of finding relevant semantic relations between pairs of entities. Table 1 shows part of a document from ACE 2004 corpus (i.e. a collection of news articles). The text expresses the relation between the entity person, i.e. the president and the entity organization, NBC X  X  entertainment division , where the person holds a managerial position.
Previous work has been devoted to automatically extract relations according to the ACE data and definitions. These models make use of machine learning and similarity mea-sures over different features, which often take the form of kernel functions [ 35 ]. Such work has shown interesting and promising extraction systems, e.g. [ 10 , 13 , 38 , 40 , 41 , 44 ]. However, there are important aspects that need to be fur-ther developed and studied: first of all, the usual target language is English and there is a lack of indications if the previous approaches are cross-language or some modifica-tions of them must be applied to deal with different language phenomena, such as richer morphology or freer argument syntax.

Secondly, the previous work on ACE provides results on standard texts, i.e. news items, whose complexity cannot be compared with more difficult data, e.g. non-standard free text, web pages, blogs and so on.

Thirdly, the relations between entities of the ACE pro-gram do not contribute to define any global information or aspect of its target domain, e.g. to determine/understand how the domain and its relationships are structured. For example, although there can be a bunch of relations related to Jeff Zucker (see Table 1 ), in the labelled ACE documents, we cannot hope to find all the relations between the employees of NBC, or the relations between the main actors of such TV broadcast domain, e.g. managers, anchor men, an so on.
Moreover, the use of academic benchmarks indirectly pre-vents to study models capable to deal with real application scenarios, e.g. noisy documents, whose content has been altered by unpredictable external conditions.

Finally, work on ACE only focuses on the extraction of the target relations and assumes that the set of named enti-ties (NEs) participating to relations is already given in the targeted texts. This is not realistic since an effective system should automatically recognize, in general, the NEs from which the target relations can be extracted. The step of rec-ognizing NEs, although can be automatically carried out with reasonably high accuracy, increases the complexity of RE.
In this paper, we carry out a study on the above-mentioned issues focusing on relation extraction (RE) from a real-world application perspective. In particular, we focused on complex relations between entities in textual documents from the investigative domain, where:  X  the language of the application domain is Italian. To our  X  The textual data are constituted by reports on investiga- X  The available text documents are manual transcriptions  X  Our automatic extractor is supposed to be used to speed-
Our approach follows the typical machine learning set-ting for relation extraction: we used state-of-the-art statistical algorithms such as support vector machines along with effec-tive and versatile pattern mining methods, e.g. word sequence kernels. However, the above-mentioned new characteristics of our application domain do negatively impact the accuracy of standard models forcing us to study and apply suitable solutions:  X  first of all, the large presence of noise in the language  X  Since previous approaches cannot deal with some instan- X  Most noticeably, to reduce the amount of training data
It should be also noted that our application scenario (and most likely many real-world scenarios) already includes an ontology of the relations expressed in the target domain. This ontology is simply generated by the analysts, who try to understand the content structure of the domain. Although the domain relation types are available, the work of an auto-matic relation extractor is still extremely important to derive relational information contained in new documents. In our case, the ontology is expressed by the relational schema of the database employed by analysts to store relations manu-ally extracted by them while working in investigation. Such manual work also provided us with the needed training and test data.

We carried out several experiments to assess our mod-els in the conditions of different levels of noise and dif-ferent amount of training data. The results show that it is possible to build a relation miner, which is robust to non-conventional languages as dialects, jargon expressions or coded words typically contained in our target intelligence text. Moreover, the experiments using different bins of train-ing data show that our approach, based on ontological con-straints, can achieve high accuracy even over small training data sets. In the remainder of this paper, Sect. 2 describes our target RE task along with the available corpus, whereas Sect. 3 introduces our RE system based on SVMs, kernel methods, innovative domain, specific features and ontolog-ical constraints. Section 4 illustrates our system evaluation along different dimensions: kernels, features, training data size and NE recognition. Finally Sect. 5 derives the conclu-sive remarks. 2 Relation extraction from investigative text The research we present has been conducted in conjunc-tion with the Italian Public Prosecutor X  X  Office with the aim of designing automatic tools supporting countering orga-nized crime. The investigation activities consist in analyz-ing huge amount of documents every day. These come from the investigative districts located in the whole national terri-tory and report all the actions performed during investigation (e.g. arrests, trials, questioning reports and so on). A pool of experts analyze each document for finding evidence of rela-tions among entities (e.g. person knows a person or person owns a car ) with the intent to build connection nets among subjects, useful for intelligence activities. Such experts were also employed to build a computational corpus from which automatic REs can be learned and tested.

In addition to interesting characteristics of real scenario such as the relationship structure, which describes document content and provide information useful for carrying out inves-tigative processes, there is the presence of several noise types. These range from utterance transcriptions to the use of jargon and disfluencies, which impact the lexicon and syntax of the target documents in a way similar to more traditional types of noise, e.g. mistakes in OCR processes. In the next section, we describe our task, corpus and the source of complexities also coming from the above-mentioned noise. 2.1 Domain relationships Our relation extraction (RE) framework is formally identical to the one proposed in ACE. The task can be formalized as follows: let O and R = R / 2 denote the finite set of entity types and the binary relation types, respectively, and let t stands for a generic relevant text fragment observable in a document. The recognition of a given binary relation r  X  R for a text t ij , including mentions to two entities e i and e whose types are T i , T j  X  O respectively, formally corre-sponds to the function: f ( e i , T i , e j , T j , t ij )  X  R  X  X  X } (1) where the special symbol  X  means no relation holds. In our definition, we stress the types of NEs, since in our domain they play an important role in the assignment of possible rela-tions than in ACE. The analysts, which are domain experts, already know the relation types that are described by means of a conceptual schema, representing concepts and relation-ships of investigative interest.

The experts X  task is to populate a DB during the analy-sis step. Figure 1 shows a subpart of the employed entity-relationships schema (used in our research). The relations described by such schema are briefly depicted in Table 2 . For example, r 5 provides some interesting information for understanding the role of the different criminal organizations like Corleonesi includes Calderone .

Database instances (i.e. the relation mentions) are popu-lated with the data extracted from text. For example, the text fragment in Fig. 2 (from questioning transcription reports) includes two statements: 1 (a) Just when I was entering the way out street, I saw (b) I can certainly claim that the Ve r d i family includes:
From the first statement, the relation r 4 : Mario belongs to Verdi can be extracted. Moreover, another entity, Bian-chi Giuseppe , is explicitly mentioned in the excerpt. From it, we understand that the two people above are traveling together in a car. Consequently, we can extract the relation r : Mario knows Bianchi Giuseppe . It is worth noticing that this last relationship requires the interpretation of two subsequent sentences.

Another complex case is shown by the sentence (b) in which a list describes people belonging to a criminal enter-prise. This relational structure can remarkably enlarge the distance between two related entities (we have observed enterprise X  X  name and person at a distance up to 100 tokens). This is a critical difference with respect to other typical appli-cation domains of relation extraction. In the investigative domain, simplifying assumptions typical of standard RE as defined in ACE [ 18 ] are no longer valid.

The more general form on which relation can be realized is just one aspect of the complexity of our RE task. In the next sections, we describe all the other sources of complexity in detail. 2.2 Corpus construction The design of RE corpus is a product of our collaboration with the Italian Public Prosecutor X  X  Office. The same pool of experts, who manually analyzes documents for finding evidence of relations among entities, were also employed to carry out in line annotation, following guidelines similar to those adopted in the ACE program.

Our referring domain corpus is entirely written in Italian, and it is composed by rather heterogeneous types of doc-uments as illustrated by Table 3 . There are four different types: Informative Reports, Printout Transcriptions, Direct Questioning Report and Summary of Questioning, where Questioning Reports, i.e. reports about a respondent person replying to specific questions or making spontaneous decla-rations, constitutes the majority of the documents.
The writing style is deeply different from the declar-ative prose largely characterizing ACE domains or from bioinformatics texts, which generally includes short and well-structured sentences. Our data consist of complex doc-uments (e.g. printout transcriptions of mobile calls or the records of interrogatories), which do not follow any jour-nalistic rule and are much more syntax free: the targeted relationship instances very often appear within an ill-formed sentence, which is full of syntactically illegal phenomena, or, even worse, they span across more than one such problematic sentences.

Moreover, the declarations made by involved actors can be rather vague or incomplete with also a large usage of jar-gon. The latter in the specific Italian case refers to dialectal expressions, which characterize declarations with a specific terminology and syntax. Given the large number of Italian dialects, a massive use of different linguistic varieties can be also found. This affects almost all document types, but in particular the printout transcriptions of mobile conversations.
Beside the complexity given by genuine linguistic phe-nomena, the targeted transcriptions also include large volumes of noise affecting the audio channel. Although we use manual transcription for our experiments, noise in the radio communication and dialect prevents human annotators to assess many fragments of conversations. 2.2.1 Corpus consistency The team of analysts who has assisted us in this work was composed by six experts of domain (policemen and detec-tives). The process of corpus annotation was similar to clas-sical annotation made by analysts during their work: for each instance of relationships found within text, they marked the involved entities and the text span in which the relation-ship is lexically realized. Although trained through very spe-cific guidelines, annotators often do not follow them strictly. This is largely due to the combinatorial explosion of some phenomena, which are difficult to fully consider. Conse-quently, some cases are neglected, thus reducing coverage. An example of such inconsistent behavior is the analysis of an excerpt like the following: All X  X ncontro a Roma erano presenti: Andrea, Barbara, Claudio, Daniela, Ettore e Francesca. 3
It is obviously true that this sentence suggests binary rela-tions between all pairs of the mentioned PP (hence, according to the annotation rules, we should have 6  X  ( 6  X  1 )/ 2 = instances of the knows relation) and between people and the location (i.e. Rome, with 6 instances of hang out relation between PP s and place ). One annotator pointed out for this sentence only the last 6 relations.

In order to handle the above problems, a quality test over the annotations was carried out. The analyst team was split in two different groups, each one annotated all test set docu-ments. This allows us to compare the annotation 4 by means of various metrics, e.g. the inter-annotator agreement.
As discussed in Sect. 2.3 , some complex problems affect the annotation phase. In order to evaluate the quality of the annotated material produced by the analysts as well as for evaluating the consistency of the test material, we evalu-ated inter-annotator agreement indices. Seven test documents were annotated by a second team (with analysts not included in the first team), which was trained according to the same modalities of the first team. After a short training on sep-arate documents, they replicated the annotation so that all test cases were doubly annotated. The measure of the inter-annotator agreement observable between the two teams was the Cohen X  X  Kappa [ 11 ], computed as:  X  = P where P ( A ) is the observed agreement among raters of the two teams, and P ( E ) is the expected agreement, that is, the probability the raters agree by chance. The values of  X  lie within the interval [ X  1 , 1 ]:  X  = 1 means that the raters are in perfect agreement,  X  = 0 that they agree by chance while  X  = X  1 expresses total disagreement. 5 The inter-annotator agreement measures are reported in Table 5 according to  X  values for each individual relation; high  X  values are obtained for almost all relations.

The results show that annotation of relation r 1 (i.e. knows ) is much more controversial between the two teams. This is due to its combinatorial nature, which implies a large number of diverging choices or missing cases (for both teams). Notice that relation r 1 is also the most likely in the data sets (see Table 4 ). This confirms the complexity of the targeted relation extraction task even for expert analysts.
A posterior analysis of the inter-annotator agreement scores suggested us that most of the disagreement was due to missed relations. Thus, in our final test set, we consider relations if almost a team has accepted it. Note that in this domain multiple relations between entities hold. 2.3 Complexity and noise in investigative data The complexity of the document types is unfortunately fully reflected into the textual realizations of the targeted rela-tionships. In the following sections, we outline three kinds of complexities: (1) structural, which concerns the content structure of the domain, (2) noise, which regards the quality of the sources of information and (3) the impact of the latter on the linguistic consistency, e.g. at syntactic and semantic level. 2.3.1 Structural complexity The relationships useful for investigation and analysis are typically more complex than those defined in benchmarks. A prosecutor is interested to know whether there is a link between two people under investigation, even if this rela-tion is broken down in two text fragments appearing in distant document points. Thus, a relation quotation usually includes a large number of tokens between the two men-tions of the entities involved in the underlying relationship. Table 6 reports the statistics about the number of such tokens for individual relationships, which can be compared with the corresponding figures in the ACE 2004 training set (last row). Every relationship class of the investigative domain has a larger token distance, ranging from 16 to 32 tokens on average. This reflects the fact that relationships usually span over more than one sentence. Note that relationship r 2 ( identifies PP ) is the only exception with an average token distance of 5. This happens as r 2 describes the situation in which a person A identifies another person B through the visual inspection of one of B X  X  pictures, as provided by the detectives (this information is usually expressed with very short sentences). In contrast, the ACE relationships are real-ized in much shorter sentence fragments, typically with just one sentence. Indeed, the average distance between related entities is about 13 tokens: in particular, 44% entity pairs are not farer than 10 tokens, i.e. they belong to very short fragments.
 Another complexity dimension relates to interpretation. This is even more open to subjectivity than standard natural language text. For example, a sentence like Ne parlai con Mario e Giorgio (I spoke to Mario and
Giorgio about it). was treated differently by individual annotators. One detects three instances of the relation knows between the speaker, Mario and Giorgio , and produced, in this way, three annotations for the three pairs of physical persons (
PP ): (speaker,Giorgio) , (speaker,Mario) and (Giorgio,Mario) . This interpretation clearly assumed that a meeting had taken place between the three. In con-trast, a second annotator outlined that no information could be found in the sentence confirming that the speaker met both persons at the same time. This alternative interpretation results into just two annotations between the speaker and each PP . 2.3.2 Noise types The kind of noise affecting the investigative domain is dif-ferent from the traditional noise present in other domains, e.g. speech data, OCR or user-generated contents [ 25 , 36 ]. The main difference is that in our case the noise is produced by human beings. Consequently, its analysis tends to be more complex in terms of the semantics of the noisy text frag-ments, e.g. sometime odd words can be considered noise pro-duced by wrong transcriptions whereas in other cases the odd lexicals are just out-of-standard vocabulary words. In gen-eral, they may also assume a valid meaning by considering a specific pragmatic level.
Noise in our work reflects two major categories: Recog-nition or Transmission Errors due to the automatic process-ing of human-generated linguistic contents and Uncertainty in the Communication process , largely due to non-tradi-tional and heterogeneous communication forms, e.g. similar to those in blogs or SMS-based communication. In the inves-tigative domain, Recognition or Transmission Errors are due to the involvement of online sources that can be recorded in an imperfect environment:  X  Transcription from telephone conversations, as included  X  A specific form of noise is introduced by human beings,
In line with other notion of noise, as those studied in the literature (e.g. [ 25 , 36 ]), the investigative texts analyzed by this study are also affected by large amounts of noise due to the Uncertainty in the Communication process . In particular:  X  The conversational nature of the questioning sessions  X  The terminological variability of the targeted textual phe- X  An important form of noise emerges when a relation-2.4 Noise impact to linguistic complexity The noise described in the previous section deeply impacts the linguistic processors that we can use to implement our RE system. We characterize such impact in the following analysis.

First of all, we note that the natural language phenom-ena occurring in our texts are highly heterogeneous. Most of the linguistic problems are related to the use of specific forms as dialectal and jargon expressions that open a vari-ety of ambiguities to the interpretation or to clerical errors during interrogations or audiotypings. This suggests that the application of a syntactic parser is unhelpful as for coverage at the level of lexical and grammatical phenomena.

As an example, let us consider the following short sen-tence of our corpus:
Sono stato combinato pe X  rituali e il mio padrino  X  sta-to il John Doe (I have been introduced through the usual rituals and my godfather was John Doe)
The words in bold show dialectal lexicon phenomena that affect the syntax of the entire sentence and produce an incor-rect syntactic interpretation. In Italian, the word combinato has no meaning related to the ceremony for initiating an indi-vidual to a group, expressed mostly by the verb iniziare (i.e. to initiate ). Moreover, also the expression pe X  rituali is odd; it is a jargon expression for the meaning attraverso i (soliti) rituali (through the (usual) rituals). Here pe X  plays the role of preposition and should be interpreted as a prepositional modifier of the main verb ( combinato ).

Figure 3 reports the syntactic parse of the fragment above, automatically generated by our Italian parser called CHAOS [ 3 ]. The text fragment, Sono stato combinato pe X  ritual-i... in the morpho-syntactic boxes (i.e. 2nd level boxes), shows that the words pe X  and rituali are interpreted as nouns ( POStag = NC ). Consequently, the interpretation of their dependency with the main verb combinato (i.e. initiated )is wrong, i.e. pe X  is interpreted as a direct object (V_obj).
In more detail, all three grammatical relations generated from the fragment combinato pe X  rituali are wrong, as they fail to capture the prepositional attachment between com-binare (i.e. to initiate ) and rituali (i.e. rituals ): no semantic interpretation is thus made available for correctly detecting the initiation event.

Moreover, given the entire syntactic graph, it is even dif-ficult to establish a relationship between the speaker (i.e. the intended subject of the main verb ( I X  X e been initiated ) and John Doe , as the graph is not fully connected. This makes the extraction of the semantic relationship r 1 ( PP knows PP between the respondent (i.e. the implicit person of the Direct Questioning report) and John Doe , impossible. 3 A relation extraction system for investigative text analysis Our mining system architecture follows the classical relation extraction (RE) models, e.g. [ 9 , 13 , 22 , 40 , 45 ]. We automati-cally learn the RE function f in Eq. 1 from data. This decides if two target entities e i and e j are in a target relationship.
For this purpose, the entity pair is mapped into a vector x ij of properties expressing different types of features of the text unit t ij (i.e. a potential quotation) in which they appear. A binary classifier for each relation r k can be learned from existing repositories of annotated examples. We combine the set of binary classifiers in the multiclassifier f with the one vs. all method [ 32 ]. To map t ij in vectors, we use manually designed features (linear kernels) as well as implicit mapping given by sequence kernels, e.g. [ 9 ].

In addition to the two above standard methods, we:  X  carry out experiments with both gold standard and auto- X  encode prior knowledge in the classification system by
In the next sections, we described our models in more detail. 3.1 Kernels and support vector machines Kernel methods (e.g. see [ 35 ]) refer to a large class of learn-ing algorithms based on inner product vector spaces, among which support vector machines (SVMs) [ 37 ] are one of the most well-known algorithms. SVMs learn a hyperplane H ( x ) = w  X  x + b = 0, where x is the feature vector repre-sentation of a classifying object o , w  X  R n and b  X  R are parameters [ 37 ]. The classifying object o is mapped into x by a feature function  X  . The kernel trick allows us to rewrite the decision hyperplane as i = 1 ,..., l y i  X  i  X ( o i ) X ( o where y i is equal to 1 for positive and -1 for negative examples,  X  i  X  R + , o i  X  i  X  X  1 ,..., l } are the training instances and the product K ( o i , o ) =  X ( o i ) X ( o kernel function associated with the mapping  X  . Note that we do not need to apply the mapping  X  , we can use K ( o i , directly. This allows us, under the Mercer X  X  conditions [ 35 ], to define abstract kernel functions that generate implicit fea-ture spaces, where the SVM optimization algorithm is guar-anteed to converge to a global optimum according to the geometric interpretation of margin maximization.

Moreover, kernel methods have the advantages that com-binations of kernel functions can be easily integrated into SVM as they are still kernels. The choice of the kernel can be also based on prior knowledge about the problem and on the noisy nature of the data. We can carry out two simple operations on kernels: K 1 + K 2 or K 1  X  K 2 . These combina-tions are very useful to mix the knowledge provided by the original features, for example acting on different perspec-tives (e.g. lexical vs. syntagmatic) on the original objects, e.g. textual units.

In next section, we illustrate a sequence kernel func-tion that counts the number of word sequences in common between two sentences, in the space of n -grams (for any n ) by also considering gaps. 3.1.1 Word sequence kernels The Word Sequence Kernels that we consider count the num-ber of subsequences of words containing gaps shared by two sequences, i.e. some of the symbols of the original sequence are skipped. Gaps modify the weight associated with the tar-get subsequences as shown in the following.

Let be a finite alphabet,  X  =  X  n = 0 n is the set of all word sequences. Given a string s  X   X  , | s | denotes the length of the strings and s i its compounding symbols, i.e s = s 1 ,..., s | s | , whereas s [ i : j ] selects the sub-u is a subsequence of s if there is a sequence of indexes I = ( i u = s between the first and last character of the subsequence u in s , i.e. d ( I ) = i | u |  X  i 1 + 1. Finally, given s 1 , indicates their concatenation.

The set of all substrings of a text corpus forms a fea-ture space denoted by F ={ u 1 , u 2 ,... } X   X  .Tomapa string s in R  X  space, we can use the following functions: the number of occurrences of u in the string s and assign them a weight  X  d ( I ) proportional to their lengths. Hence, the inner product of the feature vectors for two strings s 1 and s 2 the sum of all common subsequences weighted according to their frequency of occurrences and lengths, i.e.

SK ( s 1 , s 2 ) = where d (.) counts the number of characters in the sub-strings as well as the gaps that were skipped in the original string. It is worth noting that (a) longer subsequences receive lower weights; (b) some characters can be omitted, and gaps determine a weight since the exponent of  X  is the number of characters and gaps between the first and last character.
Characters in the sequences can be substituted with any set of symbols. In our study, we preferred to use words so that we can obtain word sequences. For example, given the sentence: Mario Rossi is affiliated to the Corleone X  X  family sample sub-strings, extracted by the Sequence Kernel (SK), are: Rossi is affiliated Corleone X  X  , Rossi affiliated family , Rossi affiliated , Rossi Corleone X  X  ,etc. 3.2 Specific representation for investigative data Section 2 has shown that in our domain there are linguistic complexities and noise, which prevent the use of parser and of other standard kernels. We hereafter provide our relation representations, which make RE applicable to our data. 3.2.1 Sequence Kernels Word Sequence Kernels compute the similarity between instances according to their common sparse subsequences as observed in the targeted textual units t ij and t ij used to represent them. Learning proceeds through the matching of subsequences as they are exhibited by training examples. In particular, in our work, we use an approach similar to [ 9 ], who builds three different sequences for any quotation:  X  X  Fore-Between segment ( FB ) is made by the first n words  X  X he Between segment ( B ) is made by the words appearing  X  X he Between-After segment ( BA ) includes the first n
These three sequences are used to feed three differ-ent sequence kernels, whose final contribution is summed together. Unfortunately, the relations of our domain tend to be realized between entities even at a very long distance in the text as we pointed out in Sect. 2.3.1 . To cope with this higher complexity, we only use the two FB and BA sequences. Note that, by choosing an enough big n the two sequences include the three original sequences. In case of very long distances, e.g. entities spanned over multiple sentences, the resulting kernel only acts on text fragments that are local to e i and e , i.e. our kernel tends to capture shallow (local) syntactic information. It will be hereafter referred as K Seq .
It should be noted that (1) in our experiments the two-window-based kernels outperform the original three-window representations (see discussion in Sect. 4.4 ) and (2) the infor-mation between the two entities, which is lost in case of long distance NEs, is partially recovered by our manually designed features presented in the next section. 3.3 Manually designed features Along with kernel methods, we manually design a set of effective attributes illustrated hereafter: Lexical units. Words in texts are expressed through their surface representations (tokens) or through the correspond-ing lemmatized forms (lemmas).
 Entity Types. In order to increase the generalization power of individual features, entities (e.g. Mario , Roma ) are substi-tuted by their corresponding class (e.g. person). For example in the excerpt Lui ha abitato a Roma per un periodo (He has lived in
Rome for a while) the active tokens in the representation are { PP , ha, abitato, a ,
Pl , per, un, periodo } , where PP and Pl indicate physical person and place, respectively.
 Distance between mentions to entities. We use the distance between the two entities as a filter for candidate pairs and we also define three different features: short, medium and large distance associated with the three main percentile, 33, 66 and 100% on the distributions of distance values of cor-rect instances, respectively. Thus, each feature characterizes valid relations, with a decreasing precision.
 Punctuation. Punctuation in the Fo re , Between and After text portions is represented via special features. These account for the position of each punctuation mark with respect to the two entities. For example, a comma in the Fo re text component (i.e., before the entity e i ) is denoted by #,F , while #,B is reserved for commas appearing between the two entity men-tions. Moreover, each feature is weighted by its frequency. Ordering of mentions. This is a boolean feature, Ord , which indicates if the entities appear in the text fragment with the same order indicated by the target relation r k . For exam-ple, the hangs out relation is clearly orientated from people PP to places Pl , whereas the fragment A Roma l X  X ncontro con Mario si protrasse sino a tarda notte (In Rome, the meeting with Mario lasted until late night) expresses the two entities in the reverse order with respect to r k : in this case, the feature Ord assumes the false value. 3.4 Kernel combinations Our baseline model is based on the linear kernel, K applied to vectors built with only lexical units. When all the other manually designed features are included in the vector, we call the related model, K XBOW , i.e. the extended kernel. We can combine the functions above with sequence kernels. This way, lexical and syntagmatic spaces are modeled inde-pendently, via K BOW (or K XBOW ) and K Seq respectively. The overall kernel is defined through the usual sum: K ( X 3.5 Prior knowledge via ontological constraints Disfluencies and jargon expressions occurring in the target text highly increase the complexity of the textual model that statistical methods have to learn. Thus, reliable classifiers can be only achieved using large amount of training data. This is a major drawback in practical applications since costs and time constraints prevent to rely on large corpora. To reduce the amount of the required data, we exploit background knowl-edge under the form of relational schema 7 of the underlying database containing the relational instances.
 In more detail, our ontological filters work as follows. Our annotated text corpus contains instances of a relation-ship class r  X  R . These are gathered as the set of positive training examples for the relation r . Moreover, every positive instance for a relation, r k , is also a negative example for every relation r l ( l = k ) that insists on the same entity pairs for example, every accepted instance of the knows relation (between PP pairs) is also a negative instances for the iden-tifies relation (see schema in Fig. 1 for a full description).
However, negative training examples also include pairs of entities that are not in a relationship. In order to build the full set of negative examples for a target relation, we computed all possible entity pairs from a document that: (1) are not positive examples of such relation and (2) appear in at least one relationship class in the domain schema (i.e., there is an entry in Table 3 ). This assumption states that every candidate quotation, which is a feasible candidate entity pair in the DB schema, is a negative example only when no annotation is available for it. Note that the above assumption limits the set of candidate pairs, although they may proliferate in long documents.

Additionally, to deal with the above-mentioned prolif-eration, we impose a thresholds on the maximal distance allowed between two entities e i and e j . The analysis of the annotated corpus showed that most of the entity pairs in valid relation instances generally occurred within a limited distance. 9 The distribution of valid relations allowed us to define a criteria (statistical filter hereafter) that filter out the ( e , e over the training set. 10 As different relations produce differ-ent distributions, different thresholds have been adopted for each relationship class. The statistical filter is then clearly applied in the training (to gather useful negative examples) as well as in the test phase. 3.6 Related work To identify semantic relations using machine learning, three learning settings have mainly been applied, namely supervised methods, e.g. [ 13 , 22 , 26 , 40 , 45 ], semi-supervised methods, e.g. [ 1 , 8 ], and unsupervised method, e.g. [ 21 ]. In a supervised learning setting, representative related work can be classified into generative models, e.g. [ 26 ], fea-ture-based, e.g. [ 22 , 33 , 44 , 45 ] or kernel-based methods, e.g. [ 10 , 13 , 38 , 40  X  42 ].

The learning model employed in [ 26 ] used statistical pars-ing techniques to learn syntactic parse trees. It demonstrated that a lexicalized, probabilistic context-free parser with head rules can be effectively used for information extraction. Generally, feature-based approaches often employ various kinds of linguistic, syntactic or contextual information and integrate them into the feature space. Roth and tau Yih [ 33 ] applied a probabilistic approach to solve the problems of named entity and relation extraction with the incorpora-tion of various features such as words, their part-of-speech, and semantic information from WordNet. Kambhatla [ 22 ] employed maximum entropy models with diverse features including words, entity and mention types and the number of words (if any) separating the two entities.

Recent work on Relation Extraction has mostly employed kernel-based approaches over syntactic parse trees. Kernels on parse trees were pioneered by Collins and Duffy [ 12 ]. This kernel function counts the number of common subtrees, weighted appropriately, as the measure of similarity between two parse trees. Culotta and Sorensen [ 13 ] extended this work to calculate kernels between augmented dependency trees. Zelenko et al. [ 40 ] proposed extracting relations by com-puting kernel functions between parse trees. Bunescu and Mooney [ 10 ] proposed a shortest path dependency kernel by stipulating that the information to model a relationship between two entities can be captured by the shortest path between them in the dependency graph.

Although approaches in RE have been dominated by ker-nel-based methods, until now, most of the research in this line has used the kernel as some similarity measures over diverse features [ 10 , 13 , 38 , 40 , 41 ]. A recent approach suc-cessfully employs a convolution tree kernel over constituent syntactic parse tree [ 42 , 46 ]. The combination of such kernel with others based on grammatical relations from dependency structure was successfully modeled in [ 30 ].

As shown in the Sect. 2.3 , we cannot apply any kind of parsing in our target domain; this prevents the use of the find-ings in most part of the mentioned researchers since they are deeply based on syntactic trees. 4 Experiments We evaluated the impact of our relational miner on a real test collection. Our objectives were: (a) to provide a comparative analysis of different learning algorithms and representation models by measuring the reachable accuracy; (b) to study the properties of robustness to noise and lack of training data of our approach; (c) to measure the impact of fully automatic RE based on named entity recognition; and (d) to measure the impact of our ontological constraints. 4.1 Setup The experimental corpus was derived from two collections of public judicial acts related to the legal proceedings against the same large criminal enterprise. It is constituted by 96 documents, annotated by analysts. We used 82% (i.e., 79 documents) for training and 8% (7 documents) for testing. The remaining 10% (10 documents) has been used as a devel-opment set to optimize the parameter settings for all the com-pared algorithms. Note that we double the annotation on test documents to produce a more accurate test set. This prevents us to carry out cross-validation as the quality of the training data is much lower and cannot be used to reliably measure accuracy. 11
Although the manual annotation was carried out on 15 dif-ferent relationship classes, some of them were rather rare; this prevented us to use them. Thus, the experimentation was only focused on the seven relations reported in Table 3 .Skewed distributions can be observed, where some relations are much more common in documents like PP hangs out at a Pl or
PP knows PP and others are very infrequent as Asset is connected to a Place . Some of the relations, although high relevant for investigation, were not well represented in the training data.

The experimental corpus is described in Table 7 .It shows the overall number of instances available for training (column 2) and testing (column 3) over each individual rela-tion: percentages are relative to the number of positive cases that were used for training. Notice how the first two rows (relations knows and identifies ) have the same number of cases: they in fact operate on the same number of candidate pairs, as their semantic signature (i.e., ( PP  X  PP )) coincides.
A final very important remark regards the application of ontological constraints as explained in Sect. 3.5 . We always apply such constraints in all our experiments. We attempted to disable such feature but we obtained very low Micro-average F1, i.e. about 40%. This confirms that the use of back-ground knowledge is extremely important in case of scarce training data availability. 4.2 Comparative analysis In this section, we present comparative experiments to ana-lyze the impact of different feature models across a set of learning algorithms.

We applied two well-known learning algorithms, i.e. C4.5 decision trees [ 31 ] and Naive Bayes to our data sets. 12 systems were run over the feature set characterizing the K
XBOW kernel (i.e., bag-of-words extended with the domain features discussed in Sect. 3.3 ). Additionally, we provide a baseline accuracy given by random choices across the candi-date pairs (filtered according to the 90th percentile statistics).
All the algorithms were optimized over the same devel-opment set and then tested against the data shown in Table 7 . For evaluation, we used the classical evaluation metrics: Precision (i.e. the percentage of correctly recognized relation instances against the total number of accepted test cases), Recall (i.e., the percentage of correctly recognized rela-tion instances against the total number of true relationship instances present in the test documents) and the F-measure (F1), as the harmonic mean between Precision and Recall (with equal balancing among the two). Micro-average is used to summarize the results of individual relations. Accuracy was also measured as the percentage of correct recognition inferences, thus including the acceptance of correct candi-dates and the rejection of false candidates.

The comparative evaluation of different algorithms trained with the best parameterization (over a held-out set) is shown in Table 8 . The last three rows represent the systems trained with different kernels. 13 The results show that the Precision of Decision Tree and NaiveBayes is better than the one of K BOW (i.e. the SVM simple model) but their F1s, i.e. 0.31 and 0.42, are lower thantheF1of K BOW , i.e. 0.45. This is due to the higher generalization power of SVMs. The best models are K XBOW and K XBOW + K Seq , which can exploit our extended fea-tures and sequence kernels, reaching the interesting values of 0.75 and 0.80, respectively. 4.3 Feature analysis The good results obtained through the different kernels, as shown by Table 8 , inspired an analysis of the impact of the different models over the individual relations. As discussed in Sect. 3.3 , the extended features that characterize some con-ceptual-and task-specific properties of the individual text units t ij are used to augment the kernel expressiveness and generalization power. This is shown by the extension of the BOW model through the XBOW one.

Note how the extended features have several variants that imply several learning configurations to be evaluated. For example, lemmas and tokens can be used, and concep-tual labels can be adopted to generalize the names of entity instances. In order to find the best variants, we ran several tests. The best trade-off between Precision and Recall scores was achieved with the following feature configuration:  X  Lexical Units : tokens.  X  Entity Types : textual mentions to entities (e.g. Mario )  X  Distance : number of tokens between the two involved  X  Punctuation : expressed only for marks appearing  X  Ordering of mentions : applied as boolean feature.
In Table 9 , the F-measure scores as obtained for individual relations according to the above XBOW model are reported. Most of the relations obtain an ex cellent score, reaching in some case an F1 of 1. On some more complex relationship classes, as PP knows PP and PP hangs out at a Pl ,the K
XBOW kernel achieves lower performance, basically due to the presence of dialectal or syntactically odd expressions. The combination of the two kernels (last column of Table 9 ) seems to overcome most of these problems.

Notice that the weakest relation is r 1 ( knows ) where also experts show a very high disagreement. It seems that, although relatively shallow features are adopted and no syn-tactic parsing is applied, the trained SVM performs on most of the phenomena similarly to humans: relation detection exhibits a similar behavior where complex cases are hard for both. In particular, if the K XBOW + K Seq kernel is only applied to the 335 cases (that is the 65% of the overall test set) where full agreement among the annotator teams is observed, its F1 achieves the much higher value of 0.82 (vs. 52 %).
As a final test, we computed the Precision/Recall curve for K XBOW + K Seq model, reported in Fig. 4 . The Preci-sion/Recall curves were built varying the learning param-eter J of SVM-light-TK, i.e. the relative weight to positive instances with respect to negative instances. The plot shows a regular shape and suggests that parameter tuning can be effec-tively applied to capture the required trade-off between the suitable coverage and the required accuracy of the method. Notice that optimizing coverage can be a much more critical requirement within the investigative domain. 4.4 Analysis of learning ability and robustness to noisy As previously discussed in Sec. 3.2.1 , our models provide an important contribution in case of noisy and complex data which cannot be managed with the classical data representa-tion used for academic benchmarks. In fact, the use of only two token windows instead of three allows to learn over examples not otherwise computable. In order to prove the robustness of our model, two further kernel representations have been tested.

These representations are based on the standard represen-tation of quotations through three text windows as defined in [ 9 ]. In these experiments, the problem of long distance among entities is overcome by substituting the Bet w een window with a special window when the former is larger than a fixed threshold. Two special windows have been adopted:  X  Break window: when the distance between entities  X  Random window: the Bet w een segment is formed by a
As shown in Table 10 , the combinations of the above ker-nels with XBOW do not improve the results obtained with our word sequence kernel (applied to the two text windows centered in the target NEs); in other words, these represen-tations do not provide the same robustness on complex data.
To study the benefit of our hybrid model on training data requirement, we report the learning curves of the seven rela-tion classifiers in Fig. 5 . It is interesting to note that with 50% of training data (corresponding to 40 documents) all the clas-sifiers almost reach a plateau, with an F1 ranging from 50% to 100%. The most interesting aspect is that the maximum accuracy, which is also the state-of-the-art for such complex mining task, can be reached with relatively few training doc-uments.
 4.5 End-to-end system evaluation Another important evaluation regards the fully automatic relation extraction, i.e. where also the named entities are automatically derived. We used our re-implementation of the well-known NER Identifinder [ 4 ]. This is based on a simple hidden Markov model with smoothing and multiple back-offs. The adopted features are accurately described in [ 4 ]. They mainly characterize the strings constituting the lan-guage model (e.g the string is in upper case, the initial letter of the string is capitalized, the string is alphanumeric and so on). The training instances for our NER are generated from the same data used for training the RE systems: each relation indeed has necessarily annotated its arguments, i.e. the two NEs.

The last row of Table 10 , i.e. NER + K XBOW + K Seq , shows the F1 (0.72) of our best RE model, when NEs are automatically extracted. This datum can be compared with the result using gold standard NEs, i.e. 0.80. Table 11 shows the F1 of the classifiers for the target seven relations. The results demonstrate that our approach is robust to the noise produced by the NER since the F1 of the different relations are very near to those achieved with gold standard NEs (see Table 9 ). This is also due to the good accuracy of our NER on the target domain, as shown in Table 12 . 5 Conclusive remarks One interesting data mining problem is relation extraction between entities in textual documents. We have presented robust models for linguistic relation mining from a business intelligence domain, where reports on criminal investigation, police interrogatory, electronic eavesdropping and wiretap constituted the typical data. The relations to be mined occur between subjects mentioned in documents. This application scenario is highly affected by linguistic noise and by the complexity of natural language data.

Our solution is based on (1) supervised approaches, i.e. support vector machines along with effective and versatile pattern mining methods, e.g. sequence kernels; (2) design of new specific features to deal with the generality of the target application domain; and (3) the exploitation of the ontological information extracted by the relational schema of the underlying database used by the manual investigative approach.

Our collaboration with the investigative team allowed us to leverage the previous manual work to derive the ontology and the annotated data 14 that we used to design and test our models.

To measure the impact of our models, we carried out several experiments: (1) for measuring the complexity of our produced corpus by means of the inter-annotator agree-ment score; (2) to compare different models using different kernels; (3) to study the robustness with respect to data avail-ability and noise.

The results show that:  X  The sequence kernel along with the manually designed  X  The learning curves show that the best model needs only  X  The fully automatic task, which includes the use of a  X  The ontological constraint proved to be essential as when
It should be noted that our approach is state-of-the-art for this task since the best models in relation extraction, e.g. [ 9 , 13 , 21 , 22 , 43 ], are not applicable in our case. Indeed, disfluencies (make dependency and constituent parsing dif-ficult to apply) and types of relations (e.g. spanning differ-ent paragraphs) require a completely different design of the above-mentioned approaches in order to be applied to our data.

However, we do believe that the design of robust approaches able to exploit deeper syntax and shallow seman-tics is an interesting research line. Advanced representations based on predicate argument structures, e.g. [ 19 , 20 , 28 , 29 ], may result robust to noise and provide the required syn-tactic and shallow semantic information as it has been already shown in [ 27 ]. Additionally, term similarity kernels, e.g. [ 2 , 5 ], will be likely to improve relation generalization, especially when combined syntactic and semantic kernels are used, i.e. [ 6 , 7 ].
 References
