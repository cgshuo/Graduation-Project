 Boqing Gong boqinggo@usc.edu Kristen Grauman grauman@cs.utexas.edu Fei Sha feisha@usc.edu Learning algorithms often rely heavily on the assump-tion that data used for training and testing are drawn from the same distribution. However, the validity of this assumption is frequently challenged in real-world applications. For example, in computer vision, recent studies have shown that object classifiers optimized on one benchmark dataset often exhibit significant degra-dation in recognition accuracy when evaluated on an-other one ( Torralba &amp; Efros , 2011 ; Perronnin et al. , 2010 ). The culprit is clear: the visual appearance of even the same object varies significantly across dif-ferent datasets as a result of many factors, including imaging devices, photographers X  preferences, or illumi-nation. These idiosyncrasies often cause a substantial mismatch between the training and the testing distri-butions. Similarly, in text analysis, we might want to train a document classifier on one corpus (e.g., product reviews on kitchen appliances) and apply to another one such as reviews on books ( Blitzer et al. , 2007 ). The two corpora thus have mismatched distributions of words and their usages, such that the trained clas-sifier would not perform well.
 How can we build classifiers that are robust to mis-matched distributions? This is the domain adaptation problem, where the training data comes from a source domain and the testing data comes from a different target domain ( Shimodaira , 2000 ; Daum  X e &amp; Marcu , 2006 ; Pan &amp; Yang , 2010 ; Gretton et al. , 2009 ). When some labeled data from the target is accessible, the problem is similar to semi-supervised learning and is referred to as semi-supervised domain adapta-tion ( Daum  X e et al. , 2010 ; Bergamo &amp; Torresani , 2010 ; Saenko et al. , 2010 ). In contrast, when there is no labeled data from the target domain to help learn-ing, the problem is called unsupervised domain adap-tation ( Blitzer et al. , 2007 ; 2006 ; Gopalan et al. , 2011 ; Gong et al. , 2012 ; Chen et al. , 2011 ).
 Unsupervised domain adaptation is of great impor-tance to real-world applications. For instance, sup-pose we want to allow mobile phone users to take pictures and recognize objects in environments spe-cific to their lifestyles. While both the camera phones and the users X  environments introduce idiosyncrasies in the images, it would be highly desirable if users did not have to label any captured image data; the recognition system ought to adapt automatically from existing labeled vision datasets, such as LabelMe or ImageNet ( Russell et al. , 2008 ; Deng et al. , 2009 ). While appealing, unsupervised domain adaptation is especially challenging. For example, the common prac-tice of discriminative training is generally not appli-cable. Without labels, it is not even clear how to de-fine the right discriminative loss on the target domain! Similarly, it is also difficult to perform model selection (e.g., tuning regularization coefficients).
 Thus, to enable domain adaptation, we need to de-termine how domains are related ( Pan &amp; Yang , 2010 ; Quionero-Candela et al. , 2009 ). One extensively stud-ied paradigm is to assume that there is a domain-invariant feature space ( Blitzer et al. , 2007 ; 2006 ; Gopalan et al. , 2011 ; Blitzer et al. , 2011 ; Chen et al. , 2011 ; Ben-David et al. , 2007 ; 2010 ; Pan et al. , 2009 ). In this space, the source and target domains have the same (or similar) marginal distributions, and the pos-terior distributions of the labels are the same across domains too. Hence, a classifier trained on the la-beled source would likely perform well on the tar-get. Several ways of measuring distribution simi-larities have been explored and theoretical analysis shows that the performance of the classifier on the tar-get is indeed positively correlated with those similari-ties ( Ben-David et al. , 2010 ; Mansour et al. , 2009a ; b ). Despite such progress, existing approaches so far have only been limited to macroscopically examining the distribution similarity by tuning to statistical prop-erties of the samples as a whole  X  when comparing distributions, all the samples are used. This notion is stringent, as it requires all discrepancies to be ac-counted for and forces learning inefficiently (or even erroneously) from  X  X ard X  cases that might be just out-liers to the target domains.
 In contrast, we will leverage the key insight that not all instances are created equally in terms of adaptability . Thus, we will examine distribution sim-ilarity microscopically at the instance level; our ap-proach plucks out and exploits the most desirable in-stances to facilitate adaptation. Identifying those in-stances requires comparing all possible subsets from the source domain to the target. We show how this can be addressed with tractable optimization. In what follows, we summarize the main idea behind our ap-proach. After describing it in detail in section 2 , we contrast it to related work in section 4 .
 Main idea Our approach centers around the notion of landmarks . Landmarks are defined as a subset of labeled instances from the source domain. These instances are distributed similarly to the target do-main. Thus, they are expected to function as a con-duit connecting the source and target domains to fa-cilitate adaptation. As an intuitive example, suppose we want to recognize objects placed in two types of environments: homes (as the source) and offices (as the target). Conceivably, only certain images from the source  X  such as those taken in home offices  X  could also be regarded as samples from the target domain. Such landmark images thus might have properties that are shared by both domains. These properties in turn can guide learning algorithms to search for invariant features. Fig. 2 displays several discovered landmark images for the vision datasets we use in this work. Leveraging the existence of landmarks and their prop-erties, we create a cohort of auxiliary tasks where land-marks explicitly bridge the source and target domains. Specifically, in those auxiliary tasks, the original tar-get domain is augmented with landmarks, blurring the distinction across domains. Thus, those tasks are eas-ier to solve than the original problem. We show this is indeed true both theoretically and empirically. The auxiliary tasks offer multiple views of the orig-inal problem. In particular, each task differs by how its landmarks are selected, which in turn is determined by how the similarity among instances is measured. In this work, we measure similarities at multiple scales (of distances). Thus, each view provides a different per-spective on the adaptation problem by being robust to idiosyncrasies in the domains at different granularities. The solutions of the auxiliary tasks give rise to multi-ple domain-invariant feature spaces that can be char-acterized by linear positive semidefinite kernel func-tions. We parameterize invariant features for the orig-inal adaptation problem with those auxiliary kernels. We show how the corresponding learning problem is equivalent to multiple kernel learning. We learn the kernel discriminatively to minimize classification er-rors on the landmark data instances, which serve as a proxy to discriminative loss on the target domain. Fig. 1 schematically illustrates the overall approach. Contributions We contribute to domain adapta-tion by proposing a novel landmark-based approach. The key insight is to use landmarks to create auxil-iary tasks that inform the original problem. We show i) how to automatically identify landmarks; ii) how to construct easier auxiliary domain adaptation tasks; iii) how to combine the solutions of auxiliary tasks dis-criminatively to solve the original domain adaptation problem (cf. section 2.3 ); iv) strong empirical results on standard benchmark datasets for object recognition and sentiment analysis, outperforming state-of-the-art algorithms by a significant margin (cf. section 3 ). The key insight of our approach is that not all in-stances are equally amenable to adaptation. In par-ticular, only certain instances bridge the source and target domains, owing to their statistical properties. We aim to identify and exploit them for adaptation. To this end, we propose a landmark-based approach that consists of three steps that will be described in turn: i) identifying and selecting the landmark in-stances; ii) constructing multiple auxiliary tasks using landmarks and inferring the corresponding domain-invariant feature spaces, one for each auxiliary task; iii) discriminatively learning the final domain-invariant feature space that is optimized for the target domain. 2.1. Landmarks Landmarks are data points from the source domain; however, given how they are distributed, they look like they could be samples from the target domain too (cf. Fig. 1 for a schematic illustration, and Fig. 2 in sec-tion 3 for exemplar images of visual objects identified as landmarks in vision datasets). The intuition behind our approach is to use these landmarks to bridge the source and the target domains.
 How can we identify those landmarks? At the first glance, it seems that we need to compare all possible subsets of training instances in the source domain to the target. We will show in the following this seem-ingly intractable problem can be relaxed and solved with tractable convex optimization.
 Let D S = { ( x m , y m ) } M m =1 denote M data points and their labels from the source domain. Likewise, we use D T = { x n } N n =1 for the target domain.
 Landmark selection To identify landmarks, we use M indicator variables  X  = {  X  m  X  { 0 , 1 }} , one for each data point in the source domain. If  X  m = 1, then x m is regarded as a landmark. Our goal is to choose among all possible configurations of  X  = {  X  m } such that the distribution of the selected data instances are maximally similar to that of the target domain. To determine whether the two distributions are sim-ilar, we use a non-parametric two-sample test (other approaches are also possible, including building den-sity estimators when the dimensionality is not high). Specifically, we use a nonlinear feature mapping function  X  (  X  ) to map x to a Reproducing Kernel Hilbert Space and compare the difference in sample means ( Gretton et al. , 2006 ). We choose  X  such that the difference is minimized, namely, min Furthermore, we impose the constraint that labels be balanced in the selected landmarks. Concretely, where y mc is an indicator variable for y m = c . The right-hand-side of the constraint is simply the prior probability of the class c , estimated from the source. We stress that the above criterion is defined on land-marks, which are a subset of the source domain, as the sample mean is computed only on the selected in-stances (cf. the denominator P m  X  m in eq. ( 1 ) ). This is very different from other approaches that have used similar nonparametric techniques for comparing distri-butions ( Pan et al. , 2009 ; Gretton et al. , 2009 ). There they make stronger assumptions that all data points in the source domain need to be collectively distributed similarly to the target domain. Furthermore, they do not impose the balance constraint of eq. ( 2 ). Our re-sults will show that these differences are crucial to the success of our approach.
 Eq. ( 1 ) is intractable due to the binary constraints on  X  m . We relax and solve it efficiently with con-vex optimization. We define new variables  X  m as  X  plex  X  = {  X  : 0  X   X  m  X  1 , P m  X  m = 1 } . Substituting {  X  m } into eq. ( 1 ) and its constraints, we arrive at the following quadratic programming problem: kernel matrix computed between the source domain points and target domain points. The optimization is convex, as the kernel matrix A is positive semidefinite. We recover the binary solution for  X  m by finding the support of  X  m , ie,  X  m = threshold (  X  m ). In practice, we often obtain sparse solutions, supporting our mod-eling intuition that only a subset of instances in the source domain is needed to match the target domain. Multiscale analysis The selection of landmarks de-pends on the kernel mapping  X  ( x ) and its parame-ter(s). For theoretical reasons, we use Gaussian RBF kernels, defined as follows:
K ( x i , x j ) = exp { X  ( x i  X  x j ) T M ( x i  X  x j ) / X  where the metric M is positive semidefinite. We ex-perimented with several choices  X  details in section 3 . The bandwidth  X  is a scaling factor for measuring dis-tances and similarities between data points. Since we regard landmarks as likely samples from the target do-main,  X  determines how much the source and the tar-get are similar to each other at different granularities. A small  X  will attenuate distances rapidly and regard even close points as being dissimilar. Thus, it is likely to select a large number of points as landmarks in or-der to match distributions. A large  X  will have the opposite effect. Fig. 2 illustrates the effect of  X  . Instead of choosing one  X  in the hope that one scale fits all, we devise a multiscale approach. We use a set {  X  q  X  [  X  min ,  X  max ] } Q q =1 . For each  X  q , we compute the kernel according to eq. ( 4 ) and solve eq. ( 3 ) to obtain the corresponding landmarks L q = { ( x m , y m ) :  X  m = 1 } . Using multiple scales adds the flexibility of modeling data where similarities cannot be measured in one homogeneous scale. For example, the category of grizzly bear is conceivably much closer to grey bear than to polar bear , and so similarities among all three are better modeled at two scales.
 Each set of landmarks (one set per scale) gives rise to a different perspective on the adaptation problem by suggesting which instances to explore to connect the source and the target. We achieve this connection by creating auxiliary tasks, as we describe next. 2.2. Auxiliary tasks Constructing auxiliary tasks Imagine we create a new source domain D q S = D S \ L q and a new target domain D q T = D T S L q , where the L q is removed from and added to the source and target domains, respec-tively. We do not use L q  X  X  labels at this stage yet. Our auxiliary tasks are defined as Q domain adapta-tion problems, D q S  X  D q T . The auxiliary tasks differ from the original problem D S  X  D T in an impor-tant aspect: the new tasks should be  X  X asier X , as the existence of landmark points ought to aid the adap-tation. This is illustrated by the following theorem, stating that the discrepancy between the new domains is smaller than the original.
 Theorem 1 Let P S ( X ) and P T ( X ) denote the distri-butions of the original source and the target domains, respectively. For the auxiliary task, assume the new target distribution is modeled as a mixture distribution Q T ( X ) = (1  X   X  ) P T ( X ) +  X P S ( X ) where  X   X  [0 , 1) . In other words, the landmarks increase the component of P S ( X ) in the target domain. Thus, KL ( P S ( X ) k Q T ( X ))  X  (1  X   X  ) KL ( P S ( X ) k P where KL (  X k X  ) stands for the Kullback-Leibler diver-gence. In words, the new target distribution is closer to the source distribution.
 The proof appeals to KL-divergence X  X  convexity in its arguments. Details are in the Supplementary Material. With the reduced discrepancy between P S ( X ) and Q
T ( X ), we can apply the analysis in ( Mansour et al. , 2009b ) (Lemma 1) to show that classifiers applied to Q
T ( X ) attain a smaller generalization error bound than those applied to P T ( X ). Intuitively, the in-creased similarity between the new domains is also closely related to the increased difficulty of distin-guishing which domain a data point is sampled from. More formally, if we were to build a binary classifier to classify a data point into one of the two categories source versus target , we would expect the accuracy to drop when we compare the original to the auxiliary tasks. The accuracy  X  also named as A-distance  X  is closely related to how effective domain adaptions can be ( Blitzer et al. , 2007 ). A high accuracy is in-dicative of a highly contrasting pair of domains, and thus is possibly due to many domain-specific features capturing each domain X  X  individual characteristics. These insights motivate our design of auxiliary tasks: they conceivably have low accuracy for binary classi-fication as the landmarks blend the two domains, dis-couraging the use of domain-specific features. We de-scribe next how to extract domain-invariant ones using the solutions of those easy problems as a basis . Learning basis from auxiliary tasks For every pair of auxiliary domains, we use the geodesic flow ker-nel (GFK), a state-of-the-art algorithm for unsuper-vised domain adaptation ( Gong et al. , 2012 ), to com-pute domain-invariant features. The GFK is partic-ularly adept at measuring domain-invariant distances among data points, as exemplified by its superior per-formance in nearest-neighbor classifiers. Thus, it is especially suitable for the final stage of our approach when we use Gaussain RBF kernels to compose com-plex domain-invariant features (cf. 2.3 ).
 We give a brief description of that method in the fol-lowing. (We omit the index q for brevity in notation.) The GFK technique models the domain shift by mod-eling each domain with a d -dimensional linear sub-space and embedding them onto a Grassmann man-ifold. Specifically, let P S , P T  X  R D  X  d denote the basis of the PCA subspaces for each of the two domains, respectively. The Grassmann manifold G ( d , D ) is the collection of all d -dimensional subspaces of the feature vector space R D . We infer the optimal d with the au-tomatic procedure in ( Gong et al. , 2012 ). The geodesic flow {  X  ( t ) : t  X  [0 , 1] } between P S and P
T on the manifold parameterizes a path connecting the two subspaces. Every point on the flow is a basis of a d -dimensional subspace. In the beginning of the path, the subspace is similar to P S =  X  (0) and in the end of the flow, the subspace is similar to P T =  X  (1). We project the original feature x into these subspaces and view the flow as a collection of infinitely many features varying gradually from the source to the target domain: z  X  = {  X  ( t ) T x : t  X  [0 , 1] } . Using the new feature representation for learning will force the classifiers to be less sensitive to domain dif-ferences and to use domain-invariant features. Partic-ularly, the inner products of the new features give rise to a positive semidefinite kernel:
G ( x i , x j ) = h z  X  i , z  X  j i (6) The matrix G can be computed efficiently using sin-gular value decomposition ( Gong et al. , 2012 ). Note that computing G does not require any labeled data. The domain-invariant feature space is extracted as the mapping  X  q ( x ) = p G q x . In the following, we de-scribe how to integrate the spaces  X  one for each aux-iliary task  X  discriminatively so that the final feature space is optimal for the target. 2.3. Discriminative learning In this final step, we reveal the second use of landmarks beyond constructing auxiliary tasks. We will use their labels to learn discriminative domain-invariant fea-tures for the target domain. Concretely, we compose the features for the original adaptation problem with the auxiliary tasks X  features as a basis.
 We scale and concatenate those features {  X  w Learning { w q } is cast as learning a convex com-bination of all kernels G q ( Lanckriet et al. , 2004 ), We use the kernel F in training a SVM classifier and the labels of the landmarks {L q } , i.e., D train = P q L q to optimize { w q } discriminatively. We use D dev = D S \D train be a validation dataset for model selection. Since D train consists of landmarks that are distributed similarly to the target, we expect the classification er-ror on D train to be a good proxy to that of the target. 2.4. Summary To recap our approach: i) at each granularity  X  q , we automatically select landmarks  X  individual instances that are distributed most similarly to the target; ii) we then construct auxiliary tasks and use their solutions as a basis for composing domain-invariant features; iii) we learn features discriminatively , using classification loss on the landmarks as a proxy to the discriminative loss on the target. We evaluate the proposed method on benchmark datasets extensively used for domain adaptation in the contexts of object recognition ( Gopalan et al. , 2011 ; Gong et al. , 2012 ; Saenko et al. , 2010 ; Kulis et al. , 2011 ) and sentiment analysis ( Blitzer et al. , 2007 ). We compare the proposed method to several competitive ones. Empirical results show that our method outper-forms all prior techniques in almost all cases. 3.1. Object recognition We use 4 datasets of object images: cal-tech ( Griffin et al. , 2007 ), amazon , webcam , and dslr ( Saenko et al. , 2010 ). Each dataset is treated as a separate domain: images in amazon came from on-line catalogs, images in dslr and webcam were cap-tured by a digital SLR camera and a webcam with high and low resolutions, respectively. 10 object classes are common to all 4 datasets. The number of images per class ranges from 15 (in dslr ) to 30 ( webcam ), and to 100 ( caltech and amazon ). Due to its small size, dslr is not used as a source domain. We experiment extensively on the remaining 9 possible domain pairs. We follow the previously reported protocols for prepar-ing features ( Saenko et al. , 2010 ). SURF features are quantized into an 800-bin histogram with codebooks computed via K-means on a subset of images from amazon . The histograms are standardized such that each dimension is zero-mean and unit-standard devia-We compare to several leading approaches and vari-ants of our own approach. We follow the standard procedures for selecting models and tuning hyperpa-rameters. Whenever other approaches do not state clearly the tuning process, we give them the benefit of the doubt by reporting their best results by revealing the target domain labels to those algorithms. (Our method does not use those labels to tune its models.) The bandwidth parameters  X  q for the Gaussian RBF kernels used for selecting landmarks (cf. section 2.1 ) The  X  0 is the median distance computed over all pair-wise data points, cf. eq ( 4 ). The metric M for com-puting the distances is chosen to be the kernel from the GFK method ( Gong et al. , 2012 ) using all instances. Recognition accuracies Table 1 reports object recognition accuracies on the target under 9 pairs of source and target domains. We contrast the proposed approach ( landmark ) to the methods of transfer component analysis ( tca ) ( Pan et al. , 2009 ), geodesic flow sampling ( gfs ) ( Gopalan et al. , 2011 ), the GFK ( gfk ) ( Gong et al. , 2012 ), structural cor-respondence learning ( scl ) ( Blitzer et al. , 2006 ), ker-nel mean matching ( kmm ) ( Huang et al. , 2007 ), and a metric learning method ( metric ) ( Saenko et al. , 2010 ) for semi-supervised domain adaptation, while label information (1 instance per category) from the target domains is used. We also report the baseline results of no adaptation , where we use source-only data and the original features to train classifiers. Our approach landmark clearly performs the best on almost all pairs, even the metric method which has access to labels from the target domains. The only significant exception is on the pair webcam  X  dslr . Error analysis reveals that the two domains are very similar, containing images of the same object instance with different imaging resolutions. As such, many data points in webcam have been selected as landmarks, leaving very few instances for model selection during the discriminative training. Addressing this issue is left for future work.
 Detailed analysis on landmarks The notion of landmarks is central to our approach. In what follows, we further examine its utility in domain adaptation. We first study whether automatically selected land-marks coincide with our modeling intuition, ie, that they look like samples from the target domain. Fig. 2 confirms our intuition. It displays several land-marks selected from the source domain amazon when the target domain is webcam . The top-left panels dis-play representative images from the headphone and mug categories from webcam , and the remaining pan-els display images from amazon , including both land-marks and non-landmarks.
 When the scale  X  is large, the selected landmarks are very similar in visual appearance to the representative images. As the scale decreases, landmarks with greater variance start to show. This is particularly pronounced at 2  X  3  X  0 . Nonetheless, they still look far more likely to be from the target webcam domain than non-landmark images (see bottom-right panels). Note that the non-landmark images for the headphone category contain images such as earphones, or headphones in packaging boxes. Similarly, non-landmark images in the mug category are more unusually shaped ones. In Table 2 , we contrast our method to some of its vari-ants, illustrating quantitatively the novelty and signif-icance of using landmarks to facilitate adaptation. First, we study the adverse effect of selecting incorrect images as landmarks. The row of rand. sel. displays results of randomly selecting landmarks, as opposed to using the algorithm proposed in section 2.1 . (The number of random landmarks is the average number of  X  X rue X  landmarks chosen in landmark .) The aver-aged accuracies over 10 rounds are reported (Standard errors are reported in the Suppl). landmark outper-forms the random strategy, often by a significant mar-gin, validating the automatic selection algorithm. The swap row in Table 2 gives yet another strong in-dication of how landmarks could be viewed as samples from the target. Recall that landmarks are used as training data in the final stage of our learning algo-rithm to infer the domain-invariant feature space (cf. section 2.3 ). Other instances, ie, non-landmarks in the source, are used for model selection. This setup follows the intuition that as landmarks are mostly similar to the target, they are a better proxy than non-landmarks for optimizing discriminative loss for the target. When we swap the setup, the accuracies drop signifi-cantly, except on the pair A  X  D (compare the rows swap and landmark ). This once again establishes the unique and extremely valuable role of landmarks. We also study the usefulness of the class balancing constraint in eq. ( 2 ), which enforces that the selected landmarks obey the class prior distribution. Without it, some classes dominate and would result in poor clas-sification results on the target domain. This is clearly evidenced in the row of unbalanced where accuracies drop significantly after we remove the constraint. Finally, we study the effect of using GFK to measure distribution similarity, as in eq. ( 4 ). The row of euc. sel. reports the results of using the conventional Eu-clidean distance, illustrating the striking benefit of us-ing GFK (in the row of landmark ). While using nonparametric two-sample tests to measure distribu-tion similarity has been previously used for domain adaptation (e.g., kernel mean matching, cf. the row of kmm in Table 1 ), selecting a proper kernel has received little attention, despite its vital importance. Our com-parison to euc. sel. indicates that measuring distri-bution similarity across domains is greatly enhanced with a kernel revealing domain-invariant features. 3.2. Sentiment analysis Next, we report experimental results on the task of cross-domain sentiment analysis of text. We use the Amazon dataset described in ( Blitzer et al. , 2007 ). The dataset consists of product reviews on kitchen appliances, DVDs, books, and electronics. There are 1000 positive and 1000 negative reviews on each prod-uct type, each of which serves as a domain. We reduce the dimensionality to use the top 400 words which have the largest mutual information with the labels. We have found this preprocessing does not reduce perfor-mance significantly, while being computationally ad-vantageous. We use bag-of-words as features. In Table 3 , we compare our landmark method to leading methods for domain adaptation, including tca ( Pan et al. , 2009 ), gfs ( Gopalan et al. , 2011 ), gfk ( Gong et al. , 2012 ), scl ( Blitzer et al. , 2006 ), kmm ( Huang et al. , 2007 ), metric ( Saenko et al. , 2010 ), as well as the baseline no adaptation . Note that while scl and kmm improve over the base-line, the other three methods underperform. Nonethe-less, our method outperforms almost all other meth-ods. Most interestingly, our method improves gfk sig-nificantly. We attribute its advantages to two factors: using multiple scales to analyze distribution similarity while gfk uses the  X  X efault X  scale, and using land-marks to discriminatively learn invariant features. 3.3. Summary of supplementary material We provide more detailed results including standard errors and comparison to other methods. We also study the benefits of having multiple auxiliary tasks. We show that while individual auxiliary tasks can lead to improved performance in adaptation, combin-ing multiple of them with the multiple kernel learning framework (cf section 2.3 ) improves further. Namely, constructing auxiliary tasks using multiple scales to re-flect similarities at different granularities yields differ-ent views of the adaptation problem, and the learning framework successfully exploits them. Learning domain-invariant feature representa-tions has been extensively studied in the litera-ture ( Ben-David et al. , 2007 ; Blitzer et al. , 2006 ; 2007 ; Daum  X e , 2007 ; Chen et al. , 2011 ; Pan et al. , 2009 ). However, identifying and using instances that are distributed similarly to the target to bridge the two domains has never been explored before.
 Our approach is also very different from transductive-style domain adaptation meth-ods ( Bergamo &amp; Torresani , 2010 ; Chen et al. , 2011 ). We partition the source domain into two disjoint subsets, only once for each auxiliary task. In those methods, however, the target and the source domains are merged iteratively.
 Kernel mean matching has previously been used to weigh samples from the source ( Huang et al. , 2007 ; Pan et al. , 2009 ; Gretton et al. , 2009 ) to correct the mismatch between the two domains. We select sam-ples as our landmarks. Those prior works typically do not yield sparse solutions (of the weights), and thus do not perform selection . Additionally, the inclusion of the balancing constraint in our formulation of eq. ( 3 ) is crucial, as evidenced in our comparison to those meth-ods in experimental studies (cf. Table 1 and Table 3 ). Without it, some classes could be underrepresented in selected landmarks, leading to poor performance. Distribution similarity is central to learning invariant features across domains. While existing approaches fo-cus on treating all samples as a whole block, we have proposed an instance-based approach. At the core is the idea of exploiting landmarks, which are data in-stances from the source that are distributed similarly to the target. The landmarks enable analyzing distri-bution similarity on multiple scales, hypothesizing a basis for invariant features, and discriminatively learn-ing features. On benchmark tasks in both vision and text processing, our method consistently outperforms others, often by large margins. Thus, our approach has broad application potential to other tasks and do-mains. For future work, we plan to advance in this direction further, for example, proposing other mech-anisms to identify and select landmarks.
 This work is partially supported by DARPA D11AP00278 and NSF IIS-1065243 (B. G. and F. S.), and ONR ATL #N00014-11-1-0105 (K. G.).

