 sentations of high dimensional data. T yp ically , this high dimensional data is represented in the form challenge is to compute lo w dimensional representatio ns that a re consistent with observ ed measure-ments of local proximity . F or e xample, in robot path mapping, the robot X  s locations must be inferred from the high dime nsional description of its state in terms of sensorimotor input. In this setting, we e xpect similar state descriptions to map to similar locations. Lik e wise, in sensor netw orks, the locations of indi vidual nodes must be inferred from the estim ated distances between nearb y sensors. In general, it is possi ble to formulate these problems as simple optimizations o v er the lo w dimen-F or this reason, lar ge-scale p roblems cannot be reliably solv ed in this manner . A more promising approach reform ulates these problems as con v e x optimizations, whose global minima can be e f ficiently computed. Con v e xity is obtained by recasting the problems as optimiza-approach. Fi rst, only lo w rank solutions for the inner product matrices X yield lo w dimen sional con v e x relaxations are not guaranteed to yield the desired lo w dim ensional solutions. Second, the of thousands of ro ws and simi larly lar ge numbers of constraints.
 manifold learning [12] and nonlinear dimensionalit y reduction [14]. This w ork has s ho wn that while the rank of solutions from SDPs cannot be directly constrained, lo w rank solutions often emer g e nat-urally by computing maximal trace sol utions that respect local distance constraints. Maximizing the trace of the inner product matrix X has the ef fect of maximizing the v arian ce of the lo w dimensional Here, we adopt the name maximum v ariance unfolding (M VU) which seems to be currently ac-cepted [13, 15] as best capturi ng the underlying intuition.
 This paper addresses the second problem mentioned abo v e: ho w to solv e very lar g e problems in MVU. W e sho w ho w to solv e such problem s by approximately f a ctorizing the lar ge n  X  n matrix X as X  X  QYQ ! where Q is a pre-com puted n  X  m rectangular matrix with m # n . The f actorization lea v es only the muc h smaller m  X  m matrix Y to be optimized wi th respect to local distance con-straints. W ith this f actorization, and by collecting constraints using the Schur complement lemma, the smaller matrix Y . This SDP can be solv ed v ery quickly , yielding an accurate approxi mation to by (non-con v e x) conjug ate gradie nt descent in the v ectors { ! x i } .
 problems in MVU. Where does the f actorization come from? Either implicitly or e xplicitly , all problems of this sort specify a graph whose nodes represent the v ectors { ! x i } and whose edges represent local distance constraints. The matrix f actorization is obtained by e xpanding the lo w dimensional represe ntation of these nodes (e.g., sensor locations) in terms of the m # n bottom (smoothest) eigen v ectors of t he graph Laplacian. Due to the local distance constraints, one e xpects the lo w dimensional representation of these nod es to v ary smoothly as one tra v erses edges in the graph. The presumption of smoothness justifies the partial o rthogonal e xpansion in terms of the bottom eigen v ectors of the graph Laplacian [5]. Similar ideas ha v e been widely applied in graph-based approaches to semi-supervised learning [4]. Matrix f actorizations of this type ha v e also been pre viously studied for manifold le arning; in [11, 15], though, the local distance constraints were not properly form ulated to permit the lar ge-scale applications considered here, while in [8 ], the approximation w as not con sidered in conjunction with a v ari ance-maximizing term to f a v or lo w dimensional representation s.
 The appr oach in this paper applies generally to an y setting in which lo w dimensional representa-tions are deri v ed from an SDP that maximizes v ariance subject t o local distance constraints. F or concreteness, we illustrate the approach on the problem of localization in lar ge scale sensor net-w orks, as recently describ ed by [1]. Here, we are able to solv e optimization s in v olving tens of thousands of nodes in just a fe w minutes. Similar applications to the SDPs that arise in manifold This paper is or g anized as follo ws. Section 2 re vie ws the problem of localization in lar ge scale sensor netw orks and its formulation by [1] as an SDP that maximizes v arian ce subject to local distance constrain ts. Section 3 sho ws ho w we solv e lar ge prob lems of this form X  X y approximating the inner product matrix of sensor locat ions as the product of smaller matrices, by solving the smaller SDP that results from this approximation, and by refining the solu tion from this smaller SDP using local search. Section 4 presents our e xperimental results on se v era l simulated netw orks. Finally , section 5 concludes by discus sing further opportunities for research. The problem of sensor localization is best il-lustrated by e xample; see Fig. 1. Imagine that sensors are locate d in major c ities throughout the continental US, and tha t nearby sensors can estimate their distances to one another (e.g., via radio transmitters). From only this local infor -mation, the problem of sensor localization is to compute the indi vidual sensor locations and to identify t he whole netw ork topology . In purely mathematical terms, the problem can be vie wed as computing a lo w rank embedding in tw o or three dimensional Euclidean space subject to local distance constraints.
 W e assume there are n sensors distrib uted in the plane and formulate the problem as an optimization o v er their planar coordinates ! x , . . . , ! x n  X  % 2 . (Sen sor localization in three dimensional space can be solv ed in a similar w ay .) W e d efine a neighbor relation i  X  j if the i th and j th sensors are suf ficiently close to estimate their pairwise distance via limited-range radio transmission. From such (noisy) estimates nates { ! x i } . W ork on this problem has typically focused on minimizing the sum-of-squares loss function [1] that penalizes l ar ge de viations from the estimated distances: this w ork we consider the s cenario where no such  X  X nchor points X  are a v ailable as prior kno wledge, and the goal is simply to position the sensors up to a global rotation, reflection, and translation. Thus, to the abo v e optimizatio n, without loss of generality we can add the centering con straint: It is straightforw ard to e xtend our approach to incorporate anchor points, which generally leads to e v en better solutions. In this case, the centering constraint is not needed.
 that is much more tractable [1]. This is done by re writing the optimization in eqs. (1 X 2) in terms of the elements of the inner pro duct matrix X ij = ! x i  X  ! x j . In this w ay , we obtain: The first constrain t centers the sensors on the origin, as in eq. (2), while the second constraint decomposition.
 The con v e x relaxa tion of the optimization in eqs. (1 X 2) drop s the constraint that that the v ectors ! x lie in the % 2 plane. Instead, the v ectors will more generally lie i n a subspace of dimensionality their tw o dimens ional subspace of maximum v ari ance, o btained from the top tw o ei gen v ectors of X . Unfortunately , if the rank of X is high, this projection l oses information. As the error of the part of a con v e x optimization.
 Mindful of this problem, the approach to sensor localization in [1] borro ws an idea fr om recent w ork to the v ariance assuming that the sensors a re centered on the origin, since tr ( X ) = % the observ ation that a flat piece of paper has greater diameter than a crumpled one. F ollo wing this intuition, we consider the fol lo wing optimization: The parameter  X  &gt; 0 balances the trade-of f between maxi mizing v ariance and preserving local be kno wn as maximum variance unfoldin g (MVU) [9, 15, 13].
 As demonstrated in [1, 9, 6, 14], these types of optimization s can be written as semidefinite programs solutions are prohibiti v ely e x pensi v e. This leads us to consider the methods in the ne xt section. Most SDP solv ers are ba sed on interior -point method s whose time-comple xity sc ales cubically in we must therefore reduce the m to SDPs o v er small matrices with small numbers of constraints. 3.1 Matrix factorization The sensor netw ork defines a connected graph whose edges represent local pairwise connecti vity . Whene v er tw o nodes share an edge in this graph, we e xpect the locations of these nodes to be nodes of this graph. Because the edges represent loca l d istance constraints, we e xpect this function best understood by analogy . If a smooth function is defined on a bounded interv al o f % 1 , then from real analysis, we kno w that it can be well approximated by a lo w order F ourier series. A similar type of lo w order approximation e xists if a smooth function is defined o v er th e nodes of a graph. This lo w-order approximation on g raphs will enable us to simplify the SDPs for MVU, just as lo w-order F ourier e xpansions ha v e been us ed to re gularize man y problems in statistical estimati on. Function approximations on graphs are most naturally deri v ed from the eigen v ectors of the graph Laplacian [5]. F or unweighted graphs, the graph Laplacian L computes the quadratic form on functions f  X  % n defined o v er the nodes of the graph. The eigen v ectors of L pro vide a set of basis functions o v er the nodes of the graph, ordered by smoothness. Thus, sm ooth functions f can be well approximated by line ar combinations of the bottom eigen v ectors of L .
 Expanding the senso r locations ! x i in terms of these eigen v ectors yie lds a compact f actorization for the inner product matrix X . Supp ose that ! x i  X  % m rectangular matrix Q store the m bottom eigen v ectors of the graph Laplaci an (e xcluding the uniform computed from the unweighted connecti vit y graph of the sensor netw ork, while the v ectors ! y  X  play the role of unkno wns that depend in a complic ated w ay on the local distance estimat es d ij . Let Y denote the m  X  m inner product matrix of the se v ectors, with el ements Y  X  X  = ! y  X   X  ! y  X  . From the lo w-order approximation to t he sensor locations, we obtain the matrix f actorization: Eq. (6) approximates the inner product matrix X as the product of much smaller matrices. Using this approximation for localization in lar ge scale netw orks, we can solv e an optimizati on for the much smaller m  X  m matrix Y , as opposed to the original n  X  n matrix X .
 The optimization for the matrix Y is obtained by substituting eq. (6) where v er the matrix X appears in eq. (4). Some s implifications o ccur due to the structure of the matrix Q . Because the columns include the uniform eigen v e ctor in Q , it follo ws that QYQ ! automatically satisfies the centering that QYQ ! ) 0 . W ith these simplifications , we obtain the follo wing optimization: Eq. (6) can alternately be vie wed as a form of re gulari zation, as it constrai ns neighboring sensors noise). Similar forms of graph re gularization ha v e been widely used in semi-supervised learning [4]. 3.2 F ormulation as SDP As noted ear lier , our strate gy for solving lar ge problem s in MVU depends on casting the required optimizations as SDPs o v er small matrices with fe w c onstraints. The matrix f actorization in eq. (6) leads to an optimizatio n o v er the m  X  m matrix Y , as opposed to the n  X  n matrix X . In thi s section, we sho w ho w to cast this optimiza tion as a correspondingly small S DP . This requires us to reformulate the quadratic optimization o v er Y ) 0 in eq. (4) in terms of a linear objecti v e function with linear or positi v e semi definite constraints.
 matrix Y . Let Y  X  % m 2 denote the v ector obtained by concatenating all t he columns of Y . W ith this notation, the objecti v e f unction (up to an additi v e constant) tak es the form the trace term in the object i v e function, tr ( Y ) , is absorbed by the v ector b . W ith the abo v e notation, we can write the optimization in eq. ( 7) as an SDP in standard form. As in [8], this is done in tw o steps. First, we introduce a dummy v ariable # that serv es as a lo wer bound matrix inequality via the Sc hur complement lemma. Combining these steps, we obt ain the SDP: to denote the matrix square root. Thu s, via the Schur lemma, this co nstraint e xpresses the lo wer bound #  X  Y ! A Y , and the SDP is seen to be equi v alent to the optimization in eqs. (7 X 8). The SDP in eq. (9) repre sents a drastic reduction in comple xity from the optim ization in eq. (7). The only v ariables of the SDP are the m ( m + 1) / 2 elements of Y and the unkno wn scalar # . The m 2  X  m 2 . Note that the comple xity of this SDP does not depend on the number of nodes or edg es in the network. As a result, this approach sca les v ery well to lar ge problems in sensor localization. In the abo v e formulation, it is w orth noting the important role played by quadr atic penalties. The use of the Schur lemma in eq. (9) w as conditioned on the quadratic form of the objecti v e function in eq. (7). Pre vious w ork on MVU has enforced the distance constraints as strict equalities [12], as one-sided inequalities [9, 11], and as soft con straints w ith linear penalties [14]. Expressed as SDPs, these earlier formula tions of MVU in v olv ed as man y constraints as edges in the underlying approaches are not merely due to graph re gularization, b ut more precisel y to its use in conjunction with quadratic penalti es, all of which can be co llected in a single linear matrix ineq uality via the Schur lemma. 3.3 Gradient-based impr o v ement While the matrix f actorization in eq. (6) leads to much more tractable optimizations, it only pro vides an approximation to the global minimum of the original lo ss function in eq. (1). As suggested in [1], minima. In this setting, h o we v er , the solution of the SDP in eq. (9) pro vides a highly accurate initialization. Though no theoretical gu arantees can be made, in practice we ha v e observ ed that this initialization often lies in the basin of attraction of the true global minimum.
 Our most rob ust results we re obtained by a tw o-step process. First, s tarting from the m -dimensional boundaries. It seems generally dif ficult to r epresentation such boundaries in ter ms of the bottom eigen v ectors of the graph Laplacian. Ne xt, w e projected the results of this first step into the % 2 plane and use conjug ate gradient met hods to minimize the loss function in eq. (1). This second step and/or the rank constraint is not well modeled by MVU. W e e v aluated our algorithm on tw o simulated sensor netw orks of dif ferent size and topology . W e did not a ssume an y prior kno wledge of sensor locations (e.g., from anchor points). W e add ed white noise to each local distance measurement with a standard de viation of 10% of the true local distance. Figure 2: Sensor locat ions inferred for n = 1055 lar gest cities in the continental US. On a v er age, each sensor estimat ed local distances to 18 neighbors, with measurements corrupted by 10% G aus-sian noise; see t e xt. Left: sensor locations obtained by solving the SDP in eq. (9) using the m = 10 bottom eigen v ectors of the graph Laplacian (computation tim e 4 s ). Despite the ob vious distortion, the solution pro vides a good initial starting point for gradient-based impro v ement . Right: sensor locations after post-process ing by conjug ate gradient descent (additional computati on time 3 s ). Figure 3: Resu lts on a simulated netw ork with n = 20000 uniformly distrib uted nodes inside a centered unit square. See te xt for details.
 cities in the continental U S. Each node estimated the local distance to up to 18 other nodes within a radius of size r = 0 . 09 . The SDP in eq. (9) w as solv ed using the m = 10 bottom eigen v ectors of the graph Laplacian. Fi g. 2 sho ws the solution from this SDP (on the lef t), as well as the final clump nodes together , especially near the boundaries. After gradient -based impro v ement, ho we v er , SDP required 4 s of total computation ti me on a 2.4 GHz Pentium 4 desktop computer , while the post-processing by conjug ate gradient descent took an additional 3 s . The second simulated netw ork, sho wn in Fig. 3, placed nodes at n = 20000 uniformly sampled points insi de the unit square. The nodes were then centered on the origin. Each node estimated the lo-cal distance to up to 20 o ther nodes within a radius of size r = 0 . 06 . The SD P in eq. (9) w as solv ed using the m = 10 bottom eigen v ectors of the graph Lapla-cian. The computation time to construct and solv e the SDP w as 19 s . The follo w-up conjug ate gradi-ent optimization required 52 s for 100 line searches.
 Fig. 3 illustrates the absolute positional errors of the sensor locations computed in three dif ferent w ays: the solution from the SDP in eq. (8), the refined so-lution obtain by conjug ate gra dient descent, and the  X  X aseline X  solution obtaine d by conjug ate gradient descent from a random in itialization. F or these plots, the sen sors were col ored so that the ground truth positioning re v eals the w ord C O N V E X in the fore-ground with a radial color gradient i n the background. The refined solution in the third panel is seen to yield highly accurate results. (Note: the repres entations in the second and fourth panels were scaled by f actors of 0.50 and 10 28, respecti v ely , to ha v e the same size as the others.) eq. (8) as a function of m . It also plots the computat ion time required to create and solv e the SDP . The figure sho ws that more eigen v ectors lead to be tter solu tions, b ut at the e xpense of increased computation time. In our e xperience, t here is a  X  X weet spot X  around m  X  10 that be st manages accurate initialization for rapid con v er gence of subsequent gradient-based methods. Finally , though not reported here due to space c onstraints, we also tested our approach on v arious data sets in manifold learning from [12]. Our approach generally reduced pre vious computation times of minutes or hours to seconds with no noticeable loss of accurac y . In thi s paper , we ha v e p roposed an a pproach for solving lar ge-scale problems in MVU. The approach mak es use of a matrix f actorization computed from the bottom eigen v ectors of the gr aph Laplacian. The f actorization yields accurate approximate solutions which can be further refined by local search. The po wer of the approa ch w as illustrated by simulated results on sensor localization. The netw orks in section 4 ha v e f ar more nodes and edges than could be analyzed by pre viously formulated SDPs for these types of problems [1, 3, 6, 14]. Be yond the pr oblem of sensor localization, our approach applies quite gene rally to other settings where l o w dimensional representations are inferred from local distance constraints. Thus we are hopeful that the ideas in this paper will find further use in areas such as robotic path m apping [3], protein clustering [6, 7], and manifold learning [12]. Ackno wledgments This w ork w as supported by NS F A w ard 0238323.

