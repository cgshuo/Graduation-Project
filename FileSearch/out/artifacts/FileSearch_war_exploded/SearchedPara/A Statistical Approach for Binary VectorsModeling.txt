 Learning techniques and in particular statistical methods are widely used in data mining applications. One of the fundamental problems when using statis-tical approaches, is the quality of the data to model. The data may contain a large amount of irrelevant or low-quality information. Indeed, if the data are too few comparing to its dimensionality or the measurement dimensions are too are improving computational efficiency and clustering effectiveness.
Feature selection, as a cla ssical problem, has been the subject of intensive cognitive approaches have been proposed (See [10], for instance), also.
In this work, we are interested in a difficult problem which is unsupervised number of clusters are known and the pr oblem of feature selection become more for unlabeled binary data vectors, we propose a theoretically sound model-based approach, using a finite multivariate Bernoulli mixture model, which supports learning of this model is based on maximum a posteriori (MAP) approach. Be-which we use for model selection.
 Section 4. Finally, Section 5 ends the p aper with some concluding remarks. ing N images (or documents). Each vector X n ,n =1 ,...,N is D -dimensional ument). By supposing that X is composed of M clusters, each vector X n can be modeled as a finite mixture of multivariate Bernoulli distributions: where  X  M = {{  X  j } , P } is the set of parameters defining the mixture model,  X  0  X  p
As we have mentioned in the previous section, often in the case of high-performance. Thus, these irrelevant features should be removed or associated with small weights. A common used approach is to suppose that a given feature written as follows p ( X Bernoulli distribution considered as a common background model to explain non-is relevant  X  d = p (  X  d = 1), straightforward manipulations of Eq. 2 give us p ( X n |  X  )= be removed. Indeed, the added parameters can be viewed as additional degrees of freedom for the model to avoid overfitting the data. 3.1 Parameters Estimation An important step when using finite mixtures for modeling is the estimation of the associated parameters. A standard procedure is to estimate the parameters by maximizing the likelihood function: p ( X|  X  )= The main approaches for parameters estimation are the maximum likelihood (ML) and the maximum a posteriori (MAP) 1 . In the case of discrete data, and binary data in particular, it is generally better to apply smoothing approaches the presence of small number of observations [16]. Thus, we have used the MAP approach given as the following the different model X  X  parameters.
 1 } . Thus, a natural choice, as a prior, for P is the Dirichlet distribution which the multinomial distribution): parameters are defined on the compact support [0,1], then prior probabilities. Then, p (  X  )= By maximizing log p ( X|  X  )+log p (  X  ) (Eq. 5), we obtain the following  X   X   X  where p ( j | X n )= belongs to class j ). Note that the previous equations are reduced to the ML estimates when all hyperparameters are set to one. 3.2 Model Selection Different criteria have been proposed to determine how many clusters should be minimum description length [18], and minimum message length [19]. The ma-principle, for the selection of the model order M , which has been shown to be instance) and can be viewed as the shortest description length of the data and tic complexity is defined as minus logarithm of the integrated (or marginal) likelihood given by p (
X| M ) is called also the evidence and can be viewed as an information the-ory measure. Computing the integrated likelihood is analytically intractable in practice and different approximations have been proposed [24]. A well known approximation to stochastic complexity is the Bayesian Information Criterion (BIC) [25], which is equivalent to the first version of the minimum description length (MDL) proposed by Rissanen [18]. A better approach that we adopt in this paper is the Cheeseman-Stutz approximation used in the AutoClass system [26] which suggests the use of the complete data evidence. The complete data for our model in Eq. 3 is class j and 0 otherwise. Thus, the complete data evidence is given by p ( X , Z , X  )= p ( X , Z , X  |  X  )  X  (  X  ) d X  =  X   X   X   X  where n j is the number of vectors in cluster j .
 Having Eq. 18 in hand, our complete algorithm can be summarized as follows:
Algorithm For each candidate value of M : 1. Set  X  t  X  0 . 5, t =1 ,...,T and initialization using k-Means algorithm. 2. Iterate the two following steps until convergence: 4. Select the optimal model that yields the smallest stochastic complexity. 4.1 Real Data the zoo data set available from the UCI Machine Learning Repository [27, the data set contributed by Richard Forsyth]. The data set describes 101 instances of animals in terms of 17 attributes ( X  X nimal name X , 15 boolean attributes, and one numeric attribute called  X  X egs X  and  X  X ype X  which indicates the class membership of the instance). Table 1 shows an example input vector for the instance  X  X lephant X . One animal,  X  X rog X  appears twice, so we eliminate one of them. The zoo data set contains 7 classe s (each class represents a given type of animals) and the numbers of animals in each class are: 41, 20, 5, 13, 3, 8, name X  and  X  X ype X  and translate the numeric attribute,  X  X egs X , into six binary features corresponding to 0, 2, 4, 5, 6, and 8 legs, respectively.
Figure 1 shows the number of clusters found by our algorithm with and with-lect the exact number of clusters whic his7anddoesnotidentifythecluster representing type 5 because of its small number of instances (only 3 animals). 5 clusters because it does not succeed to identify cluster representing type 3 (with only 5 animals). Tables 2 and 3 represent the confusion matrices with the different features, with their standard deviation over 20 runs are shown in figure 2.

We also evaluated our model on the advertisement data set, available from the UCI machine learning repository [27]. The data set represents 3279 images large number of their surrounding features. In order to represent each image in was represented by 1555-dimensional binary vector. We split the advertisement data set into a training set of 1779 vectors and a test set of 1500 vectors. We run our algorithm 20 times and the average error rate without feature selection was 10.33  X  2.68 as opposed to 7.23  X  2.03 obtained when we apply feature selection. 4.2 Binary Images Classification: Handwritten Digit Recognition lot of opportunities to extract useful information. At the same time this grow-ing content presents a lot of challenges [29]. Indeed, an important problem in computer vision is to obtain efficient s ummaries of image databases. In this application we use binary image classification to handle the handwriting digit recognition problem which has many application scenarios such as auto mail were obtained on the well-known UCI database [27] which contains 5620 objects. are processed to extract normalized bitmaps of handwritten digits. Each nor-malized bitmap includes a 32  X  32 matrix (each image is represented then by 1024-dimensional binary vector) in which each element indicates one pixel with value of white or black. Figure 3 shows an example of the normalized bitmaps. here are obtained over 20 runs of our algorithm and are rounded. Figure 4 shows process appear to improve the discrimination between models. Table 5 gives the confusion matrix when we use our fe ature selection model. The number of images misclassified was 287 (an error of 5.10 percent) as compared to 9.30 percent without feature selection (the number of misclassified images was 523, See table 6).
 In this paper we have considered the problem of unsupervised learning of binary have proposed an algorithm that iterates simultaneously between clustering and feature selection until finding a compact meaningful statistical representation of the data. We have also given a closed form expression to approximate the stochastic complexity of our model. Through some experimental results, we have shown that our binary selection model has good modeling capabilities. The completion of this research was made possible thanks to the Natural Sci-ences and Engineering Research Counc il of Canada (NSERC), and a NATEQ Nouveaux Chercheurs Grant.
