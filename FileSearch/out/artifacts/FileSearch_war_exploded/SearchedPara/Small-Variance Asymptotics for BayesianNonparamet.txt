 many applications since these models can perform automatic model selection (HDP) [ 2 ] are two of the most popular BNP models. The DPM, where number of mixture model. The HDP, where number of topics is computed based on data additional knowledge about the relationship between instances. For example, in image clustering we know that two images, one of which portrays a stone  X  X ridge X . In topic modeling, one may prefer that the words  X  X bama X ,  X  X nited the symbols of USA. These additional knowledge could be represented in the tigated the use of constraints in BNP models.
 encode constraints using Markov random field in DPM models. These existing makes it even harder to formulate an inference algorithm for the BNP models of BNP models with constraints.
 the maximum a posterior probability (MAP) estimate of BNP models when con-the optimization function from MAP estimate by using the generic exponential tion equation, we develop a deterministic clustering algorithm for DPM with must-links. The only one parameter of our algorithm is the tradeoff between archical models with constraints. Our constrained hard HDP allows users to Our contributions are: prove that data with must-links are exchangeable in chunklets;  X  We formulate DPM with must-links as an optimization problem using small tribution for mixture model. Cannot-links are handled via an approximation;  X  We formulate a similar optimization problem for HDP with constraints;  X  We derive efficient clustering algorithms for both DPM and HDP with con-straints;  X  We validate both the algorithms with extensive experimentation. 2.1 Exponential Family Distributions An exponential family distribution is defined as [ 13 ] p h ( x )), where  X  := {  X  j } d j =1 is a natural parameter and  X  h ( x )) d x is the log-partition function. The expected value and covariance are family distribution is defined as [ 13 ] p (  X  |  X ,  X  ) = exp( where  X  and  X  are the hyperparameters. Given the conjugate prior above, the posterior p (  X  | x , X , X  ) has the same form as the prior.
 A bijection between Bregman divergence and exponential families was estab-lished in [ 14 ]. In particular, given a convex set S  X  R x , y  X  S is define as D multinomial distributions are two widely used distributions. The correspond-using the expected value  X  as [ 14 ]: where  X  is the Legendre-conjugate function of  X  , f  X  ( x g (  X ,  X  ) = exp(  X  (  X ,  X  )  X  m (  X ,  X  )). 2.2 Dirichlet Process Mixture Model We briefly review the Dirichlet process mixture model (DPM). In DPM, each data point is drawn from one of mixture distributions. The likelihood of one  X  as, where  X  = {  X  k } K k =1 is the mixture proportion (1  X  k and K k =1  X  k = 1). In DPM we do not need to specify the number of clusters upfront. A latent variable z indicates the component from which the data generated. The collapsed Gibbs sampling could be used to infer 2.3 Must-Link and Cannot-Link Must-link implies that two data points must belong to the same cluster and to find out the transitivity between constraints. For ML constraints, ( M when ( u, w )  X  X  and ( w, v )  X  X  are given, where M is the set of ML constraints. For CL constraints, ( u, v )  X  X  when ( u, w ) given, where C is the set of CL constraints.
 closure above. Consider a data set x = { x i } N i =1 , where data points. The transformed data X = { X j } J j =1 ( J  X  N distinct chunklets; ( X j , X j )  X  X  denote that the pair chunklets ( CL constraints only exist between chunklets. We hope to buildup DPM with ML and CL constraints. Vlachos et al. [ 7 ]pro-instances are generated by the same component and cannot-linked instances always by different components. However, its Gibbs sampling inference treats blocked Gibbs sampling [ 16 ] for DPM with constraints (C-DPM). Like Section 2.3, we first construct the chunklet representation of data. Let indicator for chunklet X j . z  X  j is the cluster indicators for chunklets blocked Gibbs sampling for a chunklet X j is presented, p ( z The equation has the same form with DPM. We expand it, the current chunklet X j ) currently assigned to the cluster as DPM does. For ( X j , X j )  X  X  , p ( z j = k | z  X  j , are assigned to the cluster k . Due to the slow Gibbs sampling of C-DPM, we derive an efficient inference algorithm for DPM with constraints.
 ing approaches based on the assumption that data are exchangeable. However, with the introduction of constraints the data become non-exchangeable and in ing to de Finetti X  X  Theorem, an infinite sequence z 1 ,z 2 variables is exchangeable if, property of a sequence with ML and CL constraints.
 Lemma 1. Data with must-links are not exchangeable Proof. We prove it by contradiction. Suppose that there are 4 customers x , x x , x 4 and their table indicators are Then, p ( z 1 =1 ,z 2 =2 ,z 3 =1 ,z 4 =1)=  X   X  +1  X  1  X  +2 centration parameter of DP. We exchange the sequence of x p ( z =1 ,z 3 =2 ,z 2 =1 ,z 4 = 1) = 0. Obviously p ( z 1 =1 ,z 1) = p ( z Addressing this we introduce  X  X hunklet X  which is a set of must-linked data tables with the probability proportional to the number of chunklets already in that table or sit at a new table proportional to a constant ( propose proposition 1 below.
 Proposition 1 . Data with must-links are exchangeable in chunklets ngeable.
 4.1 Small-Variance Asymptotic DPM with Must-Links tions with natural parameters  X   X  =  X   X  as in [ 12 ]. With the scaling of  X   X  =  X  X  ,Eq.( 1 ) and ( 2 ) become, Let K be the total number of clusters. According to Proposition 1, the joint of
J chunklets together: where S Jk = J j =1 z jk is the number of chunklets belonging to cluster generated from an exponential family distribution with the mean parameter The likelihood of a chunklet set X = { X j } J j =1 given terior: arg max K, z ,  X  p ( z ,  X  | X ) . Further, as p ( X reformulated as maximizing the joint likelihood: arg max K, likelihood p ( X , z ,  X  ) is expanded as follows, p ( X , z ,  X  )= p ( X | z ,  X  ) p ( z 1: J, 1: K |  X  ) p (  X   X  exp  X  The prior  X  is set as the function of  X  ,  X  ,  X  as [ 12 ]: We then substitute the prior  X  into Eq.( 8 ). It becomes 1 log partition function in the Equation above. p ( X , z ,  X  K which minimize J ( z ,  X  , X  ) when  X   X  X  X  2 .
 equivalent to the following optimization problem: where  X  is the tradeoff between the likelihood and the model complexity. We function of the DP-means with ML constraints (also termed  X  X -DP-means X ) can be expressed as the sum of Bregman divergence between the chunklet to function, we propose a deterministic clustering solution for C-DP-means. The algorithm is illustrated in Alg. 1 .
 Lemma 2. The Alg. 1 decreases the objective given in Eq.( 9 ) to local convergence The proof of Lemma 2 is similar with [ 17 ]. We also provide one generalized that we replace distances of points to elements of T with  X  X j D  X  ( x i , X j ). Next, we discuss the data with cannot-links. Algorithm 1. Constrained DP-means with ML constraints Lemma 3. Data with cannot-links are not exchangeable Proof. We assume cannot-links exist between x 1 and x 4 , ability of the first permutation p ( z 1 =1 ,z 2 =1 ,z 3 =2 exchange x 2 and x 3 . The probability of this permutation 2 ,z 1 ,z 4.2 One Heuristic for DPM with Must-Links and Cannot-Links for constrained DPM with cannot-links. We can unify must-links and cannot-links in one optimization function as follows, lihood. The indicator  X  z ik ,z i k = 1 if and only if z (
X except that we search legal clusters to comply CL constraints. For data with heuristic helps to improve the clustering performance. between documents. A fundamental assumption underlying hierarchical topic modeling is that words among one document are conditionally independent. (unconstrained hard HDP), which solves the scalability of Bayesian HDP. How-ever, unconstrained Hard HDP still lacks a mechanism for merging such domain knowledge. It motives us to develop hard HDP with constraints. Here we briefly extend the the small-variance asymptotic analysis to HDP based on chunklets.
 Assume that we have Y data sets,1 , 2  X  X  X  ,y,  X  X  X  Y . Data point data point i from the set y . X yj refers to the chunklet X are associated with local cluster indicators z yj .The z yj the chunklet j in data set y is assigned to local cluster associated to global clusters { l 1 ,  X  X  X  ,l k ,  X  X  X  l K } chical models with constraints, where  X  top is the penalty of creating a new global cluster. number of local clusters and global clusters respectively. if z to interactive hierarchical modeling. 6.1 DP-means with Constraints We compare our algorithm with Gibbs sampling algorithm of C-DPM and COP-Kmeans [ 4 ]. We use NMI to measure the performance of algorithms. from 3 Gaussians (See Fig. 1 (a)). For C-DPM, we sample the hyperparameter We report the average NMI.
 whereas C-DPM produces more than 3 clusters. In Fig. 1 (b), we can see that C-DP-means performs better than C-DPM in NMI score. COP-Kmeans just improves a little. We also show the decrease of the objective for C-DP-means and the loglikehood of C-DPM in Fig. 1 (c). Evidently, C-DP-means converges faster than C-DPM.
 with no constraint. However, most of constraint sets have no improvement for the C-DP-means and some even make the performance lower. The similar trend improve the performance [ 20 ].
 Mixed Constraints: The results in Fig. 1 (d) show that our algorithm is effective and performs better than the baselines.
 Clustering UCI datasets . We use six UCI datasets: Iris(150/4/3), Wine (178/13/3), Glass(214/9/6), Vehicle(846/18/4), Balance scale (625/4/3), Seg-mentation(2310/19/7). We assume that all data sets are generated from a mix-algorithms when ML constraints are given. C-DP-means achieves higher NMI on 5 out of 6 data sets in comparison to C-DPM, 6 out of 6 in comparison to COP-Kmeans, while it requires much less running time than the Gibbs sampling based C-DPM. We observe the similar result when mixed constrains are used that using only ML constraints.
 6.2 Hard HDP with Constraints We now consider interactive hard HDP . We guide our topic modeling by pro-viding must-links and cannot-links among a small set of words. We randomly frequency words with less than 10 occurrences, which finally produce 17,937 words in vocabulary, 1,728,956 words in total. We first run hard HDP with no to food topic and  X  X zz george bush X  is assigned to company topic. We apply Merge ( X  X ompany X ,  X  X illion X ,  X  X tock X ) ( X  X ompany X  and  X  X illion X  are the two key words for the company topic), which is compiled into must-links between ( X  X ompany X , X  X resident X ), which are compiled into cannot-links between these words. Food and company topics both become more consistent than before. hardly change and have not been listed. Hard-HDP with constraints converges estimate HDP model, each costing 200 seconds. from MAP directly. One deterministic K-means type algorithm is derived. We also provide an appropriate heuristic for DPM with cannot-links. We further proposed algorithms.

