 Much of the progress in Natural Language Process-ing can be attributed to defining problems of broad interest (e.g. parsing and machine translation); col-lecting or creating publicly available corpora that en-code meaningful  X  input, output  X  samples (e.g. Penn TreeBank and LDC Parallel Corpora); and devising simple, objective and computable evaluation met-rics to automatically assess the performance of algo-rithms designed to solve the problems of interest, in-dependent of the approach or technology used (e.g. ParseEval and Bleu).

As robots become increasingly ubiquituous, we need to learn to interact with them intelligently, in the same manner we interact with members of our own species. To make rapid progress in this area, we propose to use an intellectual framework that has the same ingredients that have transformed our field: ap-pealing science problem definitions; publicly avail-able datasets; and easily computable, objective eval-uation metrics.
 In this paper, we study the problem of Human-Robot Natural Language Communication in a set-ting inspired by a traditional AI problem  X  blocks world (Winograd, 1972). After reviewing previous work (Section 2), we propose a novel Human-Robot Communication Problem that is testable empirically (Section 3.1) and we describe the publicly available datasets (Section 3.2) and evaluation metric that we devised to support our research (Section 6). We then introduce a set of algorithms for solving our problem and we evaluate their performance both objectively and subjectively (Sections 4 X 8). Most research on Human-Robot Interac-tion (Klingspor et al., 1997; Thompson et al., 1993; Mavridis, 2015) bridges the gap between natural language commands and the physical world via a set of pre-defined templates characterized by a small vocabulary and grammar. Progress on language in this area has largely focused on ground-ing visual attributes (Kollar et al., 2013; Matuszek et al., 2014) and on learning spatial relations and actions for small vocabularies with hard-coded ab-stract concepts (Steels and Vogt, 1997; Roy, 2002; Guadarrama et al., 2013). Language is sometimes grounded into simple actions (MacMahon et al., 2006; Yu and Siskind, 2013) but the data, while multimodal, is relatively formulaic, the vocabularies are small, and the grammar is constrained. Although robots have significantly increased their autonomy and ability to plan, that has not resulted, to date, in more flexible human-robot communication protocols that would enable robots to understand free-er form language and/or acquire simple and complex concepts via human-robot interactions.
Recently the connection between less formulaic language and simple actions has been explored suc-cessfully in the context of simulated worlds (Brana-van et al., 2009; Goldwasser and Roth, 2011; Brana-van et al., 2011; Artzi and Zettlemoyer, 2013; An-dreas and Klein, 2015) and videos (Malmaud et al., 2015; Venugopalan et al., 2015). However, to our knowledge, there is no body of work that focuses on understanding the relation between natural language and complex actions and goals or on explaining flex-ibly the actions taken by a robot in natural language utterances. As observed by Klingspor (1997), there is a big gap between the formulaic interactions that are typical of state-of-the-art human-robot commu-nications and human-human interactions, which are more abstract. 3.1 Problem Definition Problem-Solution Sequences. In order to build models that understand the ambiguous and com-plex language used by people when communicat-ing to solve a task, we adopt the Problem-Solution Sequence (PSS) framework proposed by Bisk et al. (2016). Problem-Solution Sequences provide high and low level descriptions of actions in service of a goal; more specifically, they are sequences of images that encode what a robot might see as it goes about accomplishing a goal. In this paper, we work with PSSs specific to a simple world that has blocks placed on a table and a robot that can visually in-spect the table and manipulate the blocks on it.
Figure 1 shows four intermediate block configura-tions of a PSS the robot observes as it transforms the initial state block configuration (random) into the fi-nal one (the number five). The PSS makes explicit the natural language instructions that a human may give to a robot in order to transform the configura-tion in an Image i into the configuration in an Image j -the two configurations may correspond to one robot action (for adjacent states in the sequence) or to a se-quence of robot actions (for non-adjacent states). To account for language variance, each simple action or sequence of actions is associated with a set of alter-native natural language instructions. For example, nine descriptions of the same action ( t 8  X  t 9 ) from Figure 1 are shown in Table 1.

We see some structural similarity between the ut-terances, but they require different amounts of in-ference to understand, use different (potentially syn-onymous) language, and choose different blocks as contextual anchors for proper interpretation. Despite this, they each describe the action with equal preci-sion. The natural language instructions encode im-plicitly partial or full descriptions of the world ( X  X n line with X  or  X  X irst open space X ).
 Simple Instruction/Command Understanding. The problem definition we focus on in this paper is that of simple instruction/command understanding: given a state of the world, Image i , and a human-like natural language command, C , we would like to in-fer the target world, Image i +1 , that a robot should construct if it understood C . If we assume, for sim-plicity, that only one block can be moved at a time, command understanding has a straightforward se-mantics: understanding a command amounts to in-ferring that the block at location ( x,y,z ) S needs to be moved and the location ( x,y,z ) T where the block needs to be moved. The rest of the blocks are not affected by the move. 3.2 Data We follow Bisk et al. (2016) X  X  methodology and col-lect PSSs specific to both goal oriented and random goal oriented data, wherein blocks are used to draw configurations/scenes that look like the digits zero through nine. To create these abstract drawings in a diverse and natural manner, configurations are de-rived from actual hand-written digits in the MNIST corpus (LeCun et al., 1998). These digits provide an easily recognizable target goal when arranging blocks. To create a sequence of actions that draws out these digits, the MNIST images were sharpened and down-sampled until each had at most 20 active pixels which could be replaced with blocks. The blocks were either decorated with brands (as shown above) or with the numbers one through twenty.
These block configurations were placed into a vir-tual world and scrambled until the board X  X  initial state was unrecognizable. This was achieved by ran-domly relocating adjacent blocks to new locations in the world. Once every block had been placed at a new location, the world appears random (for ex-ample, Image 0 in Figure 1), but when these random actions are played in reverse, the sequence of moves recreates the digit in a deliberate and ordered man-ner. Each of these actions are then shown to Ama-zon Mechanical Turkers. To ensure that our robot learns to understand human-like commands, turkers were asked to provide instructions they would give to another person in order to transform a block con-figuration corresponding to a first image ( t i ) into a second block configuration corresponding to an im-age ( t i +1 ). Each image pair was presented to three turkers, each of whom had to provide three different instructions for achieving the same goal.

A total of 100 digits sequences were annotated (10 drawings for every digits). The sequences were split so that half were decorated with numbers and half with logos. The sides of every block have a dif-ferent color and a logo or number is overlayed on every side. Eight sequences for every digit are in-cluded in Training, one for development and two for testing. Overall, the training data has 11,871 com-mands, while the development and test corpora each have 1,719 and 3,177 commands, respectively.
For each learning example, we thus have access to an input that consists of an Image i (what the robot sees), the ( x,y,z ) coordinates of each block in Image i (a discrete representation of the world corre-sponding to the image), and a natural language com-mand that a robot needs to understand in order to operate on the world in Image i . The output consists of an Image i +1 (what the robot should build) and the ( x,y,z ) coordinates of each block in Image i +1 .
The training/development/test sections of the data contain  X  177K/31K/48K tokens for the decorated blocks. The overall lexical type and token counts for our data are presented in Table 2. To compute statistics all text was lower-cased and tokenized us-ing Stanford X  X  CoreNLP (Manning et al., 2014). For the MNIST configurations, digits 0-9 are present in the test data as drawn with both logos and numbered blocks. In contrast, only half the digits appear with a given decoration in the development data.

Perhaps because annotators were not constrained or told they were giving instructions to a robot, the breadth of constructions and variance in command length is substantial. For example, the length of the commands varies wildly. Table 3 shows the num-ber of training commands in a given length range. Some commands span multiple sentences. The av-erage command length is 15 tokens with a standard deviation of 8.

The free form nature of the language and task al-lows for both utilitarian descriptions as well as more flowery instructions which utilize the full three-dimensional world: Random Blank Blocks. In both Table 2 and 3 we also present statistics on a second much more chal-lenging dataset of blank blocks used to build ran-dom configurations. For this data, blocks were ran-domly placed alongside each other, on top of one an-other, or randomly scattered in the space. Addition-ally, these blocks have no identifying labels so they are more difficult to describe, making the ground-ing problem difficult. This leads to more interest-ing language with more spatial cues and counting (e.g. third block from the top... ). This manifests as much longer descriptions averaging 23.5 words with a standard deviation of 9 words. We see this length bias in Table 3. Finally, this data also presents the challenge of stack creation in the third-dimension. As capturing the language and phenomena of this data is largely out of the scope of our current work, we present baseline results to demonstrate its dif-ficulty in the hope of motivating future research. The complete blank blocks corpus consists of 2,493 training commands, 360 for development and 720 for testing or a total of  X  58K/8K/17K tokens. Implicitly encoded in our data are three tasks with varying amounts of abstraction and context-specific language: Entity grounding, Spatial Rela-tion grounding, and Planning.
 Entity Grounding. As one would expect based on Gricean maxims, there are many ways an object might be referred to in everyday speech. These are context specific and depend on the perceived am-biguity of a scene. For example, the decoration of blocks with logos allows for easy indexing ( X  X vidia block X ) which uniquely identifies the referent. If a human feels the brand is not sufficiently recogniz-able they may choose to describe Texaco as  X  X he star block X , or Mercedes as  X  X hree lines in a circle X . In these cases, the speaker is appealing to more basic geometric knowledge in lieu of brand recognition.
The introduction of numbered blocks complicates the grounding as many actions also contain mea-sures of how far to move a block: In this case, the user has decided to denote the block IDs with the numerals 0-9, but distances by spelling out the number. As is to be expected, most strategies do not hold across users, and an individual user may be very inconsistent: move block seven two spaces to the right of block 6 This inconsistency, while difficult for a learner, in-troduces no actual ambiguity for a fluent speaker.
Finally, blank block descriptions use lots of spa-cial references the involve often complicated recur-sive structure:
It follows that an important subtask for our mod-els/algorithms is to correctly ground the entities ref-erenced in naturally occurring commands. In gen-eral this may require a thorough understanding of syntax and scoping.
 Spatial Relation Grounding. Another subtask is that of understanding spacial relations. Again, we are presented with lots of linguistic ambiguity which can be resolved by the shared context and references of the speaker and listener. For example, the first command in Table 1 is simply a list of three brands: This statement on its own is meaningless, but in the context of an image that contains the first two brands in sequence from left to right, the list implies that the final logo should be appended to the existing line.
Naturally occurring commands also assume basic knowledge of physics. The final description in Ta-ble 1, asks that we place a block in the  X  X irst open space to the right ... X . Implicit in this statement is a shared understanding that two blocks cannot occupy the same space. This implies that a human or robot knows to search for the open space, which may be arbitrarily far away. Knowledge of physics or ba-sic geometric shapes appears to be common in the instructions and injecting this knowledge into our models may be helpful.
Finally, when analyzing the data, we found that command givers would often create an ad hoc grid to assist in specifying where a block should be placed. This was particularly common when placing the first block ( t 0  X  t 1 ) to start the drawing. This initial block may need to be placed in a position that is not near any existing blocks. Common solutions in-clude specifying midpoints between blocks to center a conversation: Often the referenced blocks may be very far apart but appear opposite one another and equidistant from a useful reference point in space.
 Plan Recognition. The third problem of interest is plan recognition. The annotation of sequences of ac-tions shows that natural commands are also used in a manner that assumes the ability to plan and execute individual and complex actions: The models we introduce in this paper have diffi-culty dealing with these kind of commands. In order to correctly interpret commands in context, we need models that ground entities and understand spatial relations, shapes, and the compositionality of language. This is a large and fertile space for ex-ploration. In contrast with previous work which at-tempts to produce deep semantic interpretations of commands (Kim and Mooney, 2012; Dukes, 2014), in this paper we explore the degree to which we can solve our communication problem using seman-tic free models. We are quick to note though that our framework can also assess the performance of semantic-heavy approaches.

We outline here three basic neural models that provide a set of reasonable baselines for other re-searchers interested in solving this problem. Each approach assumes less knowledge injection than the previous. As discussed in Section 3.1, in all three models, the eventual output is a tuple specifying where to find the block to move and where to move it: ( x,y,z ) S and ( x,y,z ) T . For each model pre-sented below we will try both simple feed-forward and recurrent neural network architectures for en-coding the input utterance. 5.1 Model Architecture The goal of our models is to convert an utterance and world state into a location prediction in the world. We tackle this problem by breaking the problem into four steps: Encoding, Representation, Grounding, Prediction. Components of this pipeline can be trained independently (Sections 5.2 and 5.3) or jointly as a single End-to-End model (Section 5.4). This division of labor also allows for differing amounts of human intervention both during training and in the interpretation of actions and bears some resemblance to (Andreas et al., 2016). Specifically, we will first present results where the model predicts a fixed semantic interpretation of actions which are easily human interpretable (Encoder + Representa-tion). In this setting, the experimenter/human then must convert the semantics to actions in the world. Second, we remove the human interpreter and train a model for Grounding and Predicting from our se-mantic representation. Finally, we maintain our ar-chitecture but remove the human entirely, forcing the model to both converge to and interpret its own internal semantic representation.

The model architecture, regardless of how it is trained, at least implicitly, encodes our beliefs about the best way to solve the learning problem: per-forming single actions requires identifying anchors in the world that can be used as spacial referents from which a target location can be offset. 5.2 Discrete Predictions of a Fixed Semantics Our first model assumes a setup with very simple se-mantics. Despite all blocks existing in a real-valued world, we will assume that a final location is param-eterized by knowledge of a reference block and the direction from the reference to the target position. For example, in the simple command above, we can distill three pieces of relevant information:
By assuming a grid world, BMW can be con-verted to its location in the world ( x,y,z ) BMW , which we shift east by changing the y component to yield: ( x,y +  X ,z ) . In practice we define a set of nine relative positions:
First our model produces an encoding of the sen-tence. We present two approaches: Feed-Forward Neural Network (FFN): This model produces a sentence encoding by concatenat-ing one-hot word vectors as input to a hidden layer. We pad sentences so all inputs are the same length. Recurrent Neural Network (RNN): In contrast, the RNN encoder consumes the full sentence, each word passing through a hidden layer one at a time, before returning a final representation.

Additionally, in both encoding approaches, words which only occur a single time during training are replaced with an UNK token.

We use a single hidden layer architecture with a softmax for prediction and and train with cross en-tropy loss. We train a separate model for each pre-diction (The Encoder and Representation stages of Figure 2). Once three versions of the model have been used to predict the Source, Reference and Di-rection, this triple is used to compute both the source ( x,y,z ) S and target ( x,y,z ) T locations. The for-mer is computed via a simple look-up table, while the latter amends the reference look-up with the ap-propriate offset from the aforementioned grid.
When the model predicts a reference block which is not on the board (not all configurations use all 20 blocks) we set the reference location to the center of the board and then apply the relative position trans-formation to this hallucinated block location. 5.3 Continuous Valued Predictions From a At this point, we have constructed a model for pre-dicting a specific semantic triple, but rely on the human to convert its output to physical locations given the current state of the world. To address this, we train a simple architecture which is shown the world and automatically learn direction offsets (the Grounding and Prediction stages of Figure 2).
The model takes knowledge of the Source, Refer-ence and Direction and passes them to two hidden layers. One is multiplied by the world (a 3  X  20 matrix of coordinates) and then both are summed to produce a final ( x,y,z ) prediction. The world ma-trix columns are the locations of each block, with a fixed ordering. If any block is missing from the configuration the matrix is padded with [-1,-1,-1]. This component of the network is then trained with a mean-square error regression loss.

Running this model on the predicted representa-tion of Section 5.2 creates a simple pipeline from Sentence and World representations to location pre-dictions, with no human intervention. We are par-ticularly interested in this model X  X  performance be-cause its intermediary representation is forced to conform to the simple, interpretable semantics we chose for this domain and task. 5.4 End-to-End Model Finally, we present a single model which takes as input the sentence and world as before and predicts either the location of the block to move or its final location. This corresponds to training the neural ar-chitecture (Figure 2). We train the model twice, once to predict the source location ( x,y,z ) S and a second time to predict the target location ( x,y,z ) T . While the model architecture implicitly assumes the pres-ence of an internal semantics we do not train it di-rectly, but rather rely on the model to discover one based solely on its prediction error in the world. This approach is likely to allow for easier future exten-sions that encode finer-grained direction information and scaling (see analysis in Section 8). In our formulation, understanding a command C in the context of a world configuration Image i amounts to inferring the block ( x,y,z ) S that needs to be moved and the target location ( x,y,z ) T where the block needs to be moved to. Given that we control the manner in which the data is generated, we al-ways have access to the gold interpretation of com-mand C . Therefore, it is trivial to measure the per-formance of various command understanding algo-rithms by tracking two metrics: (i) we measure our ability to identify the block to be moved (and ref-erence/direction information when available) using standard accuracy figures; (ii) and we measure our ability to select and place the block to be moved by measuring the distance between our predicted loca-tions and the gold locations.

The first evaluation is presented in Table 4 under the S , R , and D columns. The distance errors are computed in terms of block lengths and we present the mean and median errors both for the source block X  X  initial and final location. Since the gold annotations make explicit only the block to move and its target location, the Fixed Se-mantics models do not have gold training data for predicting the reference block used for anchoring or the direction to offset. To remedy this, we use a simple string matching heuristic that chooses a ref-erence block during training and that computes a  X  X old X  direction from its location. The reference is chosen as the closest block mentioned in the sen-tence, other than the source.
 Oracle. To evaluate the strength of this heuristic, we perform an oracle evaluation (Table 4): we as-sume perfect knowledge of the source block that is moved; we apply our string matching heuristic to choose a reference block; and then assume perfect knowledge of the quadrant in which we place the block that is moved. For the blank blocks, our string matching heuristic fails, so we simply use the closest block to the target location to the reference location. This, unsurprisingly, leads to higher error. Human Performance. We randomly sampled 50 utterances from each dataset to evaluate human per-formance. The participants in our experiment were not affiliated with the project and were not provided any guidance about the task; for example, they were not told about the high-level goal of drawing a num-ber. Despite this, Table 4 shows human performance is very similar to Oracle performance. Although hu-mans did not place blocks in line  X  X erfectly X , they were comparable to or outperformed the oracle. Baselines. Finally, Table 4 also shows the results obtained by two baseline models. One (Center) has perfect knowledge of the source block to move, but always places it in the center of the table. The sec-ond baseline (Random), chooses random values for the source, reference, and direction. As expected, the performance of these baselines is abysmal. The results in Table 4 show that there is a massive difference in performance between block configura-tions that use blocks marked with identifiers (logos and digits) and those without. When the blocks are marked with clearly identifiable logos, all models outperform our baselines by a wide margin. How-ever, when blocks are blank the situation is flipped.
The results in Table 4 also highlight a notice-able gap in performance between the simplest Dis-crete model and the two location predicting mod-els. The comparable performance of the Con-tinuous and End-to-End models on labeled blocks seems to imply that the End-to-End model is captur-ing/discovering similar anchoring information with-out being explicitly told to do so. On the blank block data, the End-to-End model performs best by learn-ing its own more appropriate representation. Parameters. Where appropriate, we used 256 unit hidden-layers, 0.5 dropout, and the Adam optimizer with a learning rate of 0.001. With the exception of the FFN Discrete Predictions model, SGD parameter grid-search did not yield an improvement. 8.1 Subjective Error Analysis In Table 5, we collected 50 of our models worst er-rors on the decorated blocks data and categorized them into five classes of error. Eliminating most of these errors require more knowledge or a richer rep-resentation than currently afforded by our simple se-mantic triples. This is often due to the use of multi-ple reference blocks, but grammatical ambiguity and a knowledge of some basic geometric primitives also account for many of the mistakes. 8.2 Future Work on Blank Blocks One of the most jarring results we present is the the clear performance gap between easily grounded blocks (MNIST data) and the Blank blocks (Ran-dom) which require a much richer understanding of the world. We do not believe this is due to additional complexity in the types of relations present in the data, but rather the difficulty in grounding the refer-ences. When analyzing the data we see that much of the data still follow a very simple (Source, Ref-erence, Direction) paradigm, but automatically ex-tracting that semantics is now more difficult and the purview for future work with scene understanding.
To remove the possibility that this performance difference is due to sparsity, we down-sampled the training data from the decorated blocks to match that of the blank ones. We found the development errors grew (Average 0.27 and 1.35 on source and target, respectively) but were still substantially lower than those observed with blank block data.

Because extracting the semantics for training is so difficult, a particularly nice result is that while the End-to-End model was slightly weaker than the oth-ers on the MNIST based data, it actually performs best in this domain, where we cannot provide an ex-plicit training signal for the representation.
The nature of the language in the blank blocks dif-fers quite dramatically due to this grounding diffi-culty. Table 6 shows the two sentences we perform best (and worst) on in the development data and that make use of a reference and direction. We showed how human-robot communication can be attacked within an empirical framework that supports alternative models to be evaluated and compared using objective metrics. We intro-duced a set of simple algorithms for human-robot, in-context command/instruction understanding that should serve as strong baselines for future research in this field. The datasets present unique and impor-tant challenges for NLU, in which the interpretation of the language has varying amounts of dependence on the world in which it is uttered. The datasets we created in support of this work are made publicly available and should support the development of in-creasingly sophisticated models and algorithms for solving the problem defined in this paper, as well as additional problems that concern human-robot com-munication.
 This work was supported by Contract W911NF-15-1-0543 with the US Defense Advanced Research Projects Agency (DARPA) and the Army Research Office (ARO).
