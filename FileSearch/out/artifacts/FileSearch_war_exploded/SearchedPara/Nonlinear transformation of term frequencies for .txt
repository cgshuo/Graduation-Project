 1. Introduction ments such as news articles, technical papers, corporate web sites and university web pages on the internet or intranets into a set of predefined categories with minimum possible error and hence decrease the human effort that is required for this purpose. Bag-of-words is the most widely used document representation approach. In this technique, each document is represented as a feature vector consisting of thou sands of entries where each entry corresponds to a different word or term in the vocabulary ( Sebastiani, 2002 ). Frequency based representation of each term corresponds to using the number of times a term occurs (term frequency) as the value of the rel evant feature entry. On the other hand, binary representation sets the entries in the feature vector to one for terms that exist in the document under concern and zero for the rest. For a better representa tion, the features are generally represented using term weights that are defined as the product of term frequency factor (or, local weight) and the collection frequency factor (or, global weight) ( Lan et al., 2009 ; Dumais, 1990 ). The raw form of term frequencies has been traditionally used as the term frequency factor ( Lan et al., 2009 ) whereas the inverse document frequency ( idf ) is considered as the collection frequency factor ( Debole and Sebastiani, 2004 ; Diederich et al., 2003 ).
While idf is appropriate for information retrieval, it is not the best choice for text classification ( Soucy and Mineau, 2005 ) since term weighting, which is based on the idea that some terms are more discriminative compared to the others, aims to assign larger weights to the discriminative ones ( Lan et al., 2009 ). Researchers studied alternative collection frequency factors for replacing the idf multiplier and various schemes are proposed for this purpose where improved F 1 scores are reported ( Lan et al., 2009 ; Liu et al., 2009b ; Tsai et al., 2008 ; Erenel et al., 2011 ). In computing the collection frequency, almost all term weighting schemes take into account the document frequencies. More specifically, the number of documents that contain a given term is considered rather than the overall sum of the frequency counts of a given term inside documents across the collection. Relevance frequency ( RF ) based weighting  X  tf RF  X  that takes into account document frequencies is recently proposed and shown to deliver the best results on several benchmark datasets ( Lan et al., 2009 ). Following this study, Erenel et al. (2011) proposed a novel scheme  X  exp  X  a , y  X  X  based on the ratio of term occurrence probabilities and it is shown to surpass various existing schemes including RF on four benchmark datasets.

With the use of the term frequency factor, high frequency terms are expected to have greater impact on the decision compared to those having lower frequencies. In fact, this is generally considered as desirable where the direct use of tf is supported by the experi-mental results ( Lan et al., 2009 ). However, it is also observed that binary representation performs better on some categories. This means that overstating high frequency terms is harmful in some cases. With the use of a transformation, large swing in the feature valueofatermduetoonlyamodestchangeinthe tf value can be avoided ( Lan et al., 2009 ). Consequently, a compromising represen-tation providing a better aver age performance when multiple categories are considered will be obtained. The use of the logarithm function, log  X  1  X  tf  X  for this purpose is studied mainly in information retrieval and then in text categorization ( Radovanovic and Ivanovic, 2006 ; Bisht et al., 2010 ). However, successful implementations in text categorization are quite limited. In fact, Lan et al. (2009) have recently argued that, when used alone with a linear SVM, there is no significant difference between tf and log  X  1  X  tf  X  .Ontheotherhand, Hassan et al. (2006) have shown that the logarithmic mapping of tf performs significantly better c ompared to its raw form on three other benchmark datasets. These conflicting observations clearly indicate that the effectiveness of a nonlinear mapping on tf values depends on the dataset and the distribution of tf values in particular. It can be roughly argued that, if the documents include terms having large tf values, the system benefits from the logarithmic transforma-tion since overstatement of these terms is avoided. However, this argument should be further explored to be verified by empirical results.

The collection frequency factor is generally computed using document frequencies. More specifically, the collection frequency factor of a term depends on the number of positive and negative documents that contain it. Incorporating tf values into the collec-tion frequency factor was considered in information retrieval by Jones (1973) and it is recently studied for text categorization by Maleki (2010) . Nevertheless that factor does not utilize the category information. Moreover, the performance achieved is shown by Maleki (2010) to be worse compared to RF . We believe that the contribution of term frequency in formulating the collection frequency factor for text categorization has not been fully investigated yet.

In this study, the use of a nonlinear mapping in computing the term frequency factor is firstly addressed. The use of the raw form of term frequency (i.e. identity mapping) and log  X  1  X  tf  X  as the term frequency factors are evaluated where the document lengths are normalized using cosine normalization. In order to make sure that the characteristics of different mappings are not affected by the normalization operation, a linear approach is also applied. In particular, the logarithm of the document length is used for the normalization of log  X  1  X  tf  X  as the third scheme which is pre-viously shown to be effective in representing the relations of terms and concepts in concise semantic analysis for text categor-ization ( Zhixing et al., 2011 ). The experiments conducted on five benchmark datasets namely, Reuters-21578 ModApte Top10 split, WebKB, CSTR 2009 , e-News and 7-Sectors have revealed that the relative performances of these mappings depend on the distribu-tion of tf values.

It is also questioned whether the number of occurrences of the terms in individual documents is useful for computing collection frequency factors. A novel collection frequency factor which uses the term frequencies for this purpose is proposed. This novel scheme is used together with all three forms of the term frequency factors. The experiments reveal the fact that, when compared to using the term frequency factor alone, the gain in performance by using the proposed collection frequency factor depends on the relative individual performance of the mappings used in the term frequency factors. In particular, on CSTR e-News datasets where the identity mapping is found to be superior or equivalent to the logarithmic mapping, the proposed collection frequency factor surpasses RF and exp  X  a , y  X  . On the other hand, RF and exp  X  a , y  X  which omit tf are observed to be useful when overstating the terms having large tf values by using the identity mapping in the term frequency factor provides worse
F scores compared to log  X  1  X  tf  X  . Experiments have also shown that the proposed collection frequency factor, which takes into consideration the damped forms of the term frequencies, provides a better compromise in representation and hence the best F scores on four out of the five datasets when the best-fitting mapping is used in term frequency factors.

Section 2 presents a brief literature review about text categor-ization. The main motivation of the current study is presented in Section 3 . The proposed collection frequency factor is described in
Section 4 . In order to verify the significance of using tf in the collection frequency factor, experiments are conducted on five benchmark datasets. The datasets used, the experiments con-ducted and the results obtained are presented in Section 5 . The last part, Section 6 summarizes the conclusions drawn from this study. 2. A review of related work
The text categorization problem involves the solution of several independent binary classification subproblems. The posi-tive class includes the documents from the target category whereas the negative class includes those from the remaining categories. In this approach, feature selection and weighting are done separately for each category. This local policy of system generation leads to locally tuned parameter sets for each cate-gory. In particular, feature selection corresponds to computing the best-fitting subset of terms for each category where a given term can receive different weights in different categories ( Zheng et al., 2004 ). Moreover, the relative importance of two different terms in a given category depends on the weighting scheme. This means that one term may be more important than the other according to one scheme whereas the contrary is true for another.
As a matter of fact, the relative performances of different term selection and weighting schemes generally depend on the cate-gory under concern ( Alt X nc -ay and Erenel, 2010 ).

The first phase of designing an automated text categorization is term (feature) selection. If all terms in the vocabulary are considered, the number of features in the document vectors become extremely large which leads to computationally demand-ing classification schemes. Moreover, some terms are not infor-mative for the classification of documents ( Chen et al., 2006 ).
Because of this, feature selection is applied to select a subset of features ( Chen et al., 2005 , 2009 ; Yang and Pedersen, 1997 ; Zheng et al., 2004 ). Various measures have been investigated in the last decade for this purpose. The most popular feature selection measures are mutual information (MI), chi-square  X  w 2  X  and odds ratio ( OR )( Liu et al., 2009b ; Mladenic and Grobelnik, 2003 ; Sebastiani, 2002 ; Yang and Pedersen, 1997 ; He et al., 2003 ).
The following step is the term weighting. In order to avoid a large swing in the feature value of a term due to only a modest change in the tf value, several nonlinear transformations are proposed. Consider a term t j having the term frequency value tf  X  d , t j  X  for a given document d k . Instead of using the binary or identity mapping (i.e. raw form of tf ) on term frequencies, a linear or nonlinear mapping, M  X  X  is used which maps the term frequencies in the interval  X  0 , tf max onto  X  0 , M  X  tf 1 o M  X  tf max  X  o tf max . With the use of such a mapping, terms having higher frequencies will contribute more to the overall decision compared to those having lower without being over-stated. The use of such transformations on tf is extensively studied in information retrieval ( Manning et al., 2008 ; Salton and Buckley, 1988 ; Singhal et al., 1995 ). The most common approach is to define the nonlinear mapping with the use of the logarithm function. Transformations of the form log  X  tf  X  ,  X  1  X  log  X  tf  X  X  and log  X  1  X  tf  X  are studied ( Buckley et al., 1994 ).
Another logarithm based transformation used in information retrieval is defined as ( Manning et al., 2008 ) b tf j  X  where tf avg is the average of the tf j values of the terms within the document. It should be noted that the terms tf , tf j and tf  X  t be used interchangeably in this study depending on the context they appear.
 terms in the document are considered. The general form of this linear scheme is b tf j  X  a  X  X  1 a  X  where the tf values are scaled to lie in the interval  X  a , 1 ( Salton and Buckley, 1988 ). The constant  X  X  a  X  X  is generally selected as zero or 0.5. The main disadvantage of this scheme is the effect of the outlier terms having large number of occurrences which are not representative for the document under concern ( Manning et al., 2008 ).
 factor is more popular in text categorization, some of these transformations have already been evaluated. For instance, Yang and Liu (1999) observed that the performance of log  X  tf  X  is almost indifferent from tf in text categorization. Following this study, Lan et al. (2009) have shown that, when considered individually, tf and log  X  1  X  tf  X  perform equivalently for 5000 terms. The linear transformation given in Eq. (2) is studied by Liu et al. (2009b) for a  X  0 in text categorization but a comparison with the raw form of tf is not presented. On the other hand, Hassan et al. (2006) have shown that the logarithmic mapping of tf performs significantly better compared to its raw form on three other benchmark datasets when SVM is used. These results show that the relative performances of different mappings are categorization problem dependent. We believe that the use of log  X  1  X  tf  X  should be studied on a wider set of benchmark datasets together with he state-of-the-art collection frequency factors to analyze and clarify the potential reasons for their problem dependent relative performances.
 the document under concern. More specifically, as the length of a document increases, a particular term is expected to be repeated further in the text. In order to eliminate the effect of differences in length, document length normalization is generally considered as the next step even if the tf values are scaled down using the techniques listed above. Cosine normalization is the most popular document length normalization scheme ( Miao and Kamel, 2011 ).
In fact, the terms such as max i tf i and tf avg in Eqs. (1) and (2) are correlated with the document lengths. Consequently, further normalization for document lengths is not necessary in such cases. Both normalized and unnormalized forms are used in information retrieval. Liu et al. (2009b) have recently shown that cosine normalization may deteriorate the performance when  X  1  X  log  X  tf  X  X  is used together with idf . It is clear that, when the document lengths are roughly the same, normalization is not necessary. On the other hand, normalization is expected to be meaningful when the document lengths are different.
 computed using document frequencies of the terms for which the whole collection of documents is taken into account. More specifically, the number of documents that contain the given term is taken into account in weight estimation where the frequency counts of occurring terms in each document are not used.
Asymmetric weighting schemes favor positive terms (terms that appear mostly in the positive documents) over negative ones, but the symmetric schemes give high leverage to predominantly negative terms as well ( Erenel et al., 2011 ; Chen et al., 2006 ).
Relevance frequency ( RF ) is an asymmetric technique that is recently proposed and shown to deliver the best results on several benchmark datasets ( Lan et al., 2009 ). It is defined as the function of the information elements A and C as
RF  X  t j  X  X  log 2  X  A max f 1 , C g ,  X  3  X  where A and C denote the number of positive and negative documents which contain t j , respectively.

Following this study, a symmetric scheme, exp  X  a , y  X  is pro-posed by Erenel et al. (2011) that is based on the ratio of the term occurrence probabilities. It is shown to surpass various existing schemes including RF on four benchmark datasets. Let p  X  j denote the percentage of positive and negative documents that contain t j , respectively. Then exp  X  a , y j  X  X  where y j is defined as arctan  X  p  X  j = p j  X  and a is a design parameter to be tuned. It is specified that a  X  3 is a good choice on a wide set of categories ( Erenel et al., 2011 ).

RF is not studied together with log  X  1  X  tf  X  ( Lan et al., 2009 ). It is shown that tf RF is superior to both tf and log  X  1  X  tf  X  on two datasets, but the performance of log  X  1  X  tf  X  RF is not studied ( Lan et al., 2005 ). Both tf idf and log  X  1  X  tf  X  idf are used for text categorization where the performances obtained are gener-ally worse compared to the state-of-the-art scheme, tf RF ( Lan et al., 2005 ). However, it is shown that log  X  1  X  tf  X  idf performs better compared to tf idf on four datasets in another study ( Hassan et al., 2006 ). These two studies had one common dataset namely, 20 Newsgroups where both studies have provided better scores in favor of log  X  1  X  tf  X  idf . These results show that non-linear mappings deserve further attention to be considered in term weighting.

It should also be noted that cosine normalization is a nonlinear transformation where the normalized frequency value of a term depends not only on its tf but also on the tf values of other terms within the document. As an example, assume that d 1 and d documents where the vocabulary includes only three terms. Let d  X  X  3 ; 4 , 0 T and d 2  X  X  3 ; 2 , 2 T which indicate that the first term occurs three times in both d 1 and d 2 . After cosine normalization, we obtain d 1  X  X  3 = 5 ; 4 = 5 ; 0 T and d 2  X  X  3 means that the first term is represented by a larger value in the second document after cosine normalization. This means that, when cosine normalization is applied on transformed term frequencies, the one-to-one relation between tf  X  X  and values obtained after the transformation will not exist any more. In order to make sure that the characteristics of differen t mappings are not affected by the normalization operation, it can be done in a linear way as well. The logarithm of the document length can also be used for the normal-ization of the transformed tf values in logarithmic transformation ( Zhixing et al., 2011 ). In this study, normalization is applied on term frequency factors. The collection frequency factors are then multi-plied with them to compute the term weights.

The last step in designing a text categorization system is training the classification scheme. Experiments conducted on various domains have shown that, due to its robustness in very high dimensional feature spaces, support vector machine (SVM) is the strongest classifier for text categorization ( Lan et al., 2009 ; Miao and Kamel, 2011 ). 3. Main motivation
The performances of different mappings used in term frequency factors are domain dependent as mentioned above. In order to explain this behavior, consider the CSTR 2009 dataset where the problem is described as the classification of technical report abstracts having limited lengths. In this case, it is expected that the term frequencies will be small. If the term frequency factor is used alone as term weights, smoothing tf  X  X  by applying a nonlinear transformation will further reduce the differences among the terms, leading to a possible deterior ation of the performance when compared to using raw forms of t erm frequencies. On the other hand, on a different categorization problem, the documents are including repeated terms as in WebKB. In this dataset, documents are formed using the web pages of computer science departments from several universities. The documents are related mainly to students, courses, projects and f aculty members. Since the topics are very specific, some terms are repeated a number of times. In such a dataset, it can be argued th at overstatement of terms having large tf  X  X  due to using in raw form will lead to worse performance. Hence, smoothing is expected to be useful.

The collection frequency factors described above and others considered in the literature are based on document frequencies where the occurrence frequency of each term is taken as either zero or one for each document to calculate the information elements ( Erenel et al., 2011 ). The main motivation for defining a new collection frequency factor that takes into account term frequencies is the categorization problem dependent performances of different mappings used in term frequency factors as described above. In particular, if the use of the raw form of tf values in term frequency factor provides better scores compared to the smoothed version on some datasets, it can be argued that considering term frequencies should also be useful in computing c ollection frequency factors in such cases. Consider two terms, t i and t j . Assume that the term t occurs only once in the documents in which it is contained whereas t occurs several times in every document it is contained. It can be argued that t j isamoreimportanttermsinceitcannotbeconsidered as an incidentally occurring term. In such a case, the term frequency values will be useful in computing t he collection frequency factor. To the best of our knowledge, this has not been considered in any well-known scheme.

In this study, we hypothesize that the problem dependent performances of different mappings are mainly due to the differ-ences in the distribution of term frequency values in different text categorization tasks. In particular, shrinking the term frequencies in cases when they are large using a transformation is advanta-geous. We also hypothesize that the use of term frequencies in collection frequency factors is beneficial when the term frequen-cies are small. However, as in the case of term frequency factor, if some terms have high frequencies, they will be overstated if included in the collection frequency factor. In such a case, using document frequencies as in conventional approaches will be more appropriate. Based on these hypotheses, we propose a novel collection frequency factor which makes use of the term frequen-cies. The merit of the hypotheses and the effectiveness of the proposed scheme are investigated by a rich set of experiments on five different datasets. 4. Proposed collection frequency factor
The proposed factor is based on a recent study by Zhixing et al. (2011) . They defined the relation between a term t concept c i as
H  X  c i , d k  X  log  X  1  X  tf  X  d k , t j  X  X  log  X  1  X  length  X  d where N denotes the total number of training documents and
H  X  c , d k  X  X  1 if the document d k belongs to the concept c i and zero otherwise. Instead of cosine normalization, the denominator is used to normalize the length of the documents. The length of the document is defined as the number of terms within the docu-ment. We will refer to this as term-count normalization in the following context. It can be seen that the equation involves the transformation log  X  1  X  tf  X  . Inspired from this expression, we define a mapping on term frequencies as b tf  X  M  X  tf j , k  X  9 where the cosine normalization is omitted. This is due to the fact that the denominator corresponds to linear document length normalization. Consequently, the one-to-one relation between tf  X  X  and term frequency factors is preserved. Hence, the character-istic of logarithmic mapping is not modified due to normalization.
Because of these properties, this mapping can also be used as a term frequency factor in term weighting. The transformed term frequency value can be re-written as b tf
This means that the base of the logarithm is set to be  X  1  X  length  X  d
In fact, various base values are used before for term weighting such as 2, e and 10 in text categorization ( Zhou and Zhang, 2008 ). Since the ultimate goal is not ranking of terms but setting their relative importance, the choice of the base is important in weight calcula-tion. The representation given above sets the base of the logarithm as one plus the number of terms in the document. It should be noted that, for a given tf , the linearly normalized value will be the same for the documents having the same length and it will not be affected by the frequencies of other terms in the document.

The collection frequency factor of each term is computed based on the transformation given in Eq. (7). The proposed scheme is sym-metric which means that the terms which mainly occur in the negative class are regarded as equally valuable as terms that occur mostly in the positive class. The main motivation for this is the superiority of the symmetric schemes on four different benchmark datasets ( Erenel et al., 2011 ). Let S  X  denote the set of training samples belonging to the positive class having the cardinality N  X  . The negative class generally involve s samples from several categories.
Let S i denote the set of negative training samples belonging to the i th category having the cardinality N i . In other words, there are N  X 
P term frequency value of t j is firstly computed for the positive class: b tf  X 
Similarly, the average value of the transformed term frequency for t computed for each category in the negative class as follows: b tf  X  The collection frequency factor is defined as
W  X  t
 X  X  where max i b tf , i j , avg denotes the maximum mapped term frequency value obtained when the categories in the negative class are considered. E is a small constant which is used to avoid division by zero. It can be seen in Eq. (10) that the weight of a term grows in proportion to the number of positive documents it exists in and also in line with its frequency in each of these documents. Similarly, if the term under concern is related to a pa rticular category in the negative class, the weight of the term grows in proportion to the number of documents in that category containing the term and also in line with its frequency in each of these documents.
 factor is the employment of term frequencies which is not the case in any of the widely used schemes. The performance of the proposed scheme can be associated with the relative performance of different mappings on tf when the term frequency factor is used alone for term weighting. Assume that only the term frequency factor is used for term weighting where diminishi ng the term frequencies that are greater than one using log  X  1  X  tf  X  is observed to perform better compared to the raw form for a particular dataset. In other words, avoiding overstatement of high frequency terms is found to have a significant effect. In such a case, a collection frequency factor which considers only document frequencies is a reasonable choice. How-ever, if the term frequency values are generally small, applying a nonlinear transformation will further reduce the differences among the terms leading to a possible deterioration of the performance when compared to the raw form of tf . In this case, the raw form of tf  X  X  should be used as the term frequency factor and the use of tf  X  X  in collection frequency factor is expected to be beneficial as well. For this reason, the proposed scheme is expected to surpass conven-tional collection frequency factor s particularly in cases where their performances are poor.
 by analyzing the distribution of the term frequency values separately for each category in the negative class. In order to understand the intuitive reasoning behind this, consider a term which commonly exists only in one specific category of the negative class. Assume that it rarely appears in the target category. If this term is available in a test document, then it is an important indicator that the document is a negative one.
Therefore, it deserves a large weight. If the categories in the negative class were not to be treated separately and all negative documents were considered as a whole, the average transformed term frequency would be smaller due to large N and hence the generated weight would also be smaller. As a result, the negative discriminative terms would be undermined.

Fig. 1 illustrates the nonlinear mapping given in Eq. (7) for five different document lengths in the interval  X  10 ; 50 . It can be seen in the figure that, for a given tf j , k value, b tf j , k document length grows. This is the inherent normalization used in the nonlinear transformation to eliminate the effect of varying document lengths in document representation.

The proposed collection frequency factor can be used together with cosine normalized identity mapping, cosine normalized loga-rithmic mapping and term-count normalized term frequency fac-tors. In our simulations presented in the following section, we evaluated the proposed scheme on all three types of term frequency factors. 5. Experiments 5.1. Datasets
In this study, five widely used datasets are used for evaluating the proposed scheme. The main characteristics of these datasets are presented in Table 1 . The ModApte split of top 10 classes of
Reuters-21578 ( Debole and Sebastiani, 2004 ) is the first where the negative classes are defined to include documents which belong to one or more of the remaining nine categories. The same policy is applied on all datasets for forming the negative class. Due to its highly imbalanced category distribution, Reuters-21578
ModApte Top10 is a significant dataset among others. WebKB is another popular dataset that is a collection of web pages which belong to seven categories ( Craven et al., 1998 ). They were collected by the Carnegie Mellon University Text Learning Group from several universities in 1997. Four of the categories namely,  X  X  X tudent X  X ,  X  X  X aculty X  X ,  X  X  X ourse X  X  and  X  X  X roject X  X  which contain totally 4199 documents are generally used in text categorization experi-ments ( Alt X nc -ay and Erenel, 2010 ). The training and test sets of
Reuters-21578 dataset are defined. For WebKB dataset, fourfold cross validation is applied in the experiments and the average results are reported. The number of training documents in each fold is presented in the third column of Table 1 . In the same way, fourfold cross validation is applied on the remaining three datasets. The third dataset, 7-Sectors contains 4581 web pages. Each page belongs to one parent category and one subcategory in hierarchical order. Similar to the previous experiments in the literature ( Nunzio, 2009 ), seven parent categories are used. CSTR 2009 dataset is composed of 625 abstracts, each belonging to a single category, from technical reports in four research areas published in the Department of Computer Science at the Uni-versity of Rochester between 1991 and 2009. 2 The e-News dataset is collected from four newspapers. 3 It includes five categories, namely  X  X  X usiness X  X ,  X  X  X ducation X  X ,  X  X  X ntertainments X  X ,  X  X  X port X  X  and  X  X  X ravel X  X .

The fourth column in Table 1 presents the total number of terms in each of the datasets. When all terms are considered, the average document lengths and the means of average term frequency values in the documents are also presented in the last two columns of the table. It should be noted that, for a given document, the average term frequency is computed as the average tf  X  X  of the terms that are present in the corresponding document. As seen in the table, there are considerable differences in these datasets. In particular, the web documents in WebKB and 7-Sectors are on the average longer compared to the documents in the other three datasets. Moreover, there are larger number of different terms in these datasets compared to the others. Although the e-News dataset includes longer documents com-pared to CSTR 2009 and Reuters-21578, the average term frequency is smaller which means that it includes documents that have larger number of different terms, each of which are repeated smaller number of times in the documents. 5.2. Experimental setup
In the implementation of a text categorization system, the first step is the removal of stopwords. In this study, SMART stoplist is used for this purpose ( Buckley, 1985 ). Then, Porter stemming algorithm is applied ( Porter, 1980 ). SVM light with default para-meters and linear kernel is employed as the classification scheme ( Joachims, 1998 , 1999 ). For the evaluation of different approaches, precision ( P ), recall ( R ) and F 1 score are computed separately for each category. Precision is defined as the percen-tage of documents which are correctly labeled as positive. Recall provides the percentage of correctly classified positive docu-ments. However, since a categorization system can be tuned to maximize either precision or recall at the expense of the other, their harmonic mean named as F 1 score is generally used ( Liu et al., 2009a ) where F  X  P  X  TP TP  X  FP , R  X  TP TP  X  FN :  X  11  X  TP , FP and FN denote true positives, false positives and false negatives, respectively, for the category under concern ( Wu and Tsai, 2009 ). Macro-F 1 score is then computed as the average of individual F 1 scores ( Sebastiani, 2002 ). On the other hand, Micro-F 1 score is calculated based on the total values of TP , FP and FN over all categories. In a recent study, it is observed that the F scores of most weighting schemes plateau after 5000 features for SVM ( Lan et al., 2009 ). Because of this, top 5000 features ranked by w 2 are considered in the experiments. However, in the CSTR dataset, the total number of processed terms is less than 3500 in onefold. Because of this, top 3000 terms are used for this dataset in all four folds. In order to investigate the performances using smaller number of features (i.e. 1000 and 2000 terms), additional experimental results are presented at the end of Section 5.3 . In all experiments, the value of E in Eq. (10) is set as 0.001. 5.3. Experimental results
The first stage of the simulations covers the use of different term frequency factors on their own. Table 2 presents the macro-
F and micro-F 1 scores obtained. Note that cosine normalization is applied on tf and log  X  1  X  tf  X  values whereas the third factor has its inherent normalization. On CSTR 2009 dataset, tf performs better compared to the nonlinear mappings. On e-News dataset, the relative performance depends on the normalization scheme. As a matter of fact, it can be argued that there is not any remarkable gain due to smoothing of tf  X  X  by applying a nonlinear transforma-tion. On the other hand, it is evident from the table that nonlinear transformation provides significant improvements on the other datasets, especially on WebKB and 7-Sectors. These results verify the dataset dependent performance of different mappings.
It is hypothesized in Section 3 that this dependence is mainly due to the differences in the distribution of term frequency values in different text categorization problems. This difference is evident in
Table 1 which is obtained using all terms in the vocabulary. In order to evaluate the validity of this hypothesis after feature selection, consider Table 3 where the mean and standard deviations of the average tf values of the selected terms (3000 in CSTR 2009 the other datasets) in the positi ve and negative documents are presented for the largest and smallest categories of each dataset.
More specifically, for each document, the averages of the tf  X  X  of the appearing terms are firstly computed. Then, the mean and standard deviation of these values over all positive and negative documents are calculated. As seen in the table, documents in WebKB and 7-Sectors contain terms having hig her frequencies compared to the others. As a matter of fact, employing the raw form of tf results in overstatement of these terms leading to poor performance as presented in Table 2 .
 documents as a function of the average term frequencies for all categories in e-News and 7-Sectors, respectively, for further clarification of the differences between the two categorization tasks. The histograms are computed using the training documents in the first fold. As seen in the figures, the average term frequencies in the vast majority of documents in e-News dataset are smaller than 2 whereas the average term frequencies are above 2 in the great majority of documents in 7-Sectors dataset. the experiments are performed for all three term frequency factors given above. As the collection frequency factors, asym-metric RF and symmetric exp  X  a , y  X  are also considered for refer-ence purposes. Table 4 presents the macro-F 1 and micro-F obtained when cosine normalized tf is used as the term frequency factor. As it can be seen in the table, on CSTR 2009 and e-News datasets where tf  X  X  are generally smaller, using tf  X  X  in collection frequency factor is favorable where better F 1 scores are achieved compared to RF and exp  X  a , y  X  . On the other three datasets where logarithmic transformation is found to be effective when term frequency factor is used alone, RF and exp  X  a , y  X  provide better scores. This is consistent with our hypothesis about the use of term frequencies in collection frequency factor.

The experiments are repeated for the term frequency factor log  X  1  X  tf  X  and the results are presented in Table 5 . The results are consistent with the case when tf is the term frequency factor. This means that, whatever the term frequency factor is, the choice of the collection frequency factor still depends on the distribution of term frequency values in the dataset. It should be noted that the proposed weighting scheme provides the best scores on WebKB as well. Although the use of raw form of tf in term frequency factor is found to provide inferior results compared to logarithmic mapping, the use of tf  X  X  in collection frequency factor is still useful. In fact, this observation is valid for b tf j , k where the best macro-F 1 scores are obtained by the proposed weighting scheme on four datasets.

It should be noticed that, as seen in Tables 4 X 6 , the highest macro-F 1 scores are achieved by the proposed collection fre-quency factor on four datasets (82.67 on CSTR 2009 , 97.04 on e-News, 91.27 on Reuters-21578 and 92.66 on WebKB). In order 10 20 30 40 50 60 Number of documents
Sport to assess the statistical significance of the improvements in the macro-F1 scores provided by the proposed approach, hypothesis tests are performed using the t -test approach. The null hypothesis is defined as  X  X  X 0  X  mean of the improvement is equal to zero X  X  and the alternative hypothesis is defined as  X  X  X 1  X  mean of the improvement is greater than zero X  X . The tests are performed between log  X  1  X  tf  X  W and baseline tf RF system. The null hypothesis is rejected at significance level of a  X  0 : 06, with p -values 0.055, 0.030, 0.054 and 0.00052, respectively, for CSTR 2009 ,e-News,WebKBand 7-Sectors datasets.

It is already shown by Lan et al. (2009) that the use of insufficient number of terms slightly degrades the categorization scores. In order to evaluate the performance of the proposed term weighting scheme on smaller number of features, experiments are repeated for 1000 and 2000 terms. The results are presented in Table 7 . As seen in the table, the scores are slightly degraded on all systems but proposed factor scheme provides the best scores on majority of the datasets in both cases.

As a final remark, it should be noted that the datasets considered have unique characteristics that can be easily exploited to determine the best-fitting type of the collection frequency factor to maximize the categorization performance. As given in Table 3 ,onthreedatasets, the means of the average term frequency values are found to be around 1.5 whereas, on the other two, the means are around 2.5 or above and the standard deviations are considerably higher. This suggests that some terms may be downgraded by far whereas some terms can be overemphasized if an inappropriate transformation is selected. Consequently, after analyzing the training data, one can easily decide whether the term freq uencies should be considered in the calculation of collection frequency factors or not.
Utilities Number of documents Number of documents
Financial 6. Conclusions
In this study, it is shown that each text categorization problem has different characteristics in terms of the distribution of term frequencies which affects the performance of the mappings considered in term frequency factors. When the term frequencies are small, it is shown that raw term frequencies provide better scores compared to logarithmic mapping. On the other hand, it is observed that if high frequency terms prevail, logarithmic trans-formation prevents their overstatement, leading to better scores compared to using them in raw form.

Based on these observations, a novel collection frequency factor is proposed which uses term frequencies. Experiments on five datasets have verified that the proposed scheme provides better scores on categorization problems where the term fre-quencies are usually small. It is also shown that achieving better scores on the problems where greater tf values appear is also possible.

The proposed collection frequency factor uses the logarithm of term frequencies. On the other hand, the conventional approaches take into account the number of documents that contain the term.
The logarithm function can be considered as an intermediate mapping between the two extremes: using the raw term fre-quency values or the document frequencies. The logarithm func-tion may not be the best choice for all datasets. As a future work, we believe that alternative functions should be investigated which may be in parametric forms. It is conceivable that the parameters can be estimated from the distribution of the term frequencies in the datasets.
 References
