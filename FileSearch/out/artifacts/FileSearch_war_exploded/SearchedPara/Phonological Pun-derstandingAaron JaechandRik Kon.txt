 From the high culture of Shakespeare X  X  plays (Tanaka, 1992), to the depths of the YouTube com-ments section, from advertising slogans (Keller, 2009) to conversations with nerdy parents, puns are a versatile rhetorical device and their understand-ing is essential to any comprehensive approach to computational humor. Humor has been described as  X  X ne of the most interesting and puzzling research areas in the field of natural language understanding X  (Yang et al., 2015). Puns, in particular, offer an in-teresting subject for study since their humor derives from wordplay and double-meaning.

An important class of puns, known as parono-masic puns, are those where one entity, the pun, is phonologically similar to another, the target (Joseph, 2008). Consider an example from Crosbie (1977): Here,  X  X lothed X  is the pun and  X  X losed X  is the tar-get. Paronomasic puns are distinguished from ho-mographic puns such as which puns on the two definitions of the word  X  X ie X . When the pun and target are homophonic this is called a perfect pun, and when nearly homophonic an imperfect pun (Zwicky and Zwicky, 1986) (or a heterophonic pun (Hempelmann, 2003)). The fo-cus of this work is to propose and evaluate a model for target recovery of both perfect and imperfect paronomasic puns, assuming that the location of the pun word or word sequence.

Ritchie (2005) classifies puns in terms of whether they are self-contained , i.e., based on general knowl-edge and humorous in a variety of circumstances, or contextually integrated , i.e. relying on a specific context such as a visual context, knowledge of a re-cent event or discussion. Many puns of this type are associated with cartoons or images, e.g. a cartoon with pies and cakes in the street having the caption (desserted/deserted). Contextually integrated puns lose their humor out of context because the pun is difficult to detect. However, target recovery is often still possible, and thus this distinction does not play a major role in the current study.

If a listener fails to recover the target of the pun then the statement fails in its humor. The two chief clues that the listener must rely on to perform tar-get recovery are the phonetic information and lan-guage context. This is analogous to the way in which someone listening to speech uses the acoustic infor-mation and language context to recover a sequence of words from an audio source. In the words of Hempelmann (2003),  X  X he recovery of the target in heterophonic puns is just a specific case of the com-plex task of hearing. X  The similarity between hear-ing and target recovery suggests the use of meth-ods from automatic speech recognition in building a model for automatic target recovery.

The goal of this paper is to develop and test a computational model for target recovery in puns. Sentences with the position of the pun marked are given as input and the model must output the tar-get word sequence. As the focus is on paronomasic puns, the relationship between the pun and the target is primarily phonological, but surrounding language context is also important for recovering the target. This work has applications in natural language un-derstanding of texts that contain humor. Further-more, the insights gained from our model are use-ful for improving pun generation in computational humor systems. Zwicky and Zwicky (1986) provided an analysis of the properties of paronomasic puns, especially with regard to the markedness of phonological segments. They assembled a corpus of 2140 instances of seg-mental relationships from imperfect pun/target pairs for their analysis. By counting how many times each phoneme was used in a pun or target, the authors observe a behavior they refer to as ousting, a strong asymmetry in phoneme substitution likelihood. For instance, punners will rarely replace a  X  X  X  phoneme (IPA t ) in the target word with  X  X H X  ( T ) in the pun, but regularly replace  X  X H X  with  X  X . X  An example of such a pun is (fit/fifth)). Because of this asymmetry, we say that  X  X  X  ousts  X  X H. X  Zwicky and Zwicky correlate the ousting behavior evident in their pun data with the phonological notion of markedness . Markedness can be defined (if oversimplified) as  X  X he tendency for phonetic terms to be pronounced in a simple, natural way X  with regard to physiological, acoustic, and perceptual factors ( marked segments are more complex) (Anderson and Lightfoot, 2002). They conclude that marked segments tend to oust un-marked segments (e.g. voiced stops oust their voice-less counterparts).

This is followed-up by Sobkowiak (1991), whose manual alignment of the phoneme sequences of 3,850 pun/target pairs allows for a more careful study of the ousting behavior. This corpus, where whole sequences of phonemes are aligned between puns and targets, is a much richer resource for an-alyzing ousting than the segment-only data used in (Zwicky and Zwicky, 1986). Sobkowiak X  X  improved data provides evidence against the conclusions of Zwicky and Zwicky (1986). Rather,  X  X t seems that it is not the case that  X  X arked ousts unmarked X  in paronomasic puns. X  Sobkowiak goes on to show that puns more frequently involve changes to vowels than consonants, noting that their information load (i.e. contribution to target recoverability) is lighter. Our data corroborates Sobkowiak X  X  claims regarding the role of markedness in punning as well as the mu-tability of vowels, and provides more details about the specific nature of substitutions in a large corpus of puns.

Building off of Sobkowiak X  X  work, Hempelmann (2003) studies target recoverability, arguing that a good model for target recovery provides necessary groundwork for effective automatic pun generation. He proposes a preliminary phonetic edit cost table to be one part of a scoring system. The model is based on the phoneme edit counts from Sobkowiak (1991) with an ad-hoc formula for transforming the counts into substitution costs. However, Hempel-mann makes no effort to empirically test his model at the recovery task. The model uses a subset of 1,182 puns from the 3,850 identified by Sobkowiak. This subset is the data used for training our phonetic edit models and we use Hempelmann X  X  cost function as a baseline.

The task of automatic target recovery of parono-masic puns has not been previously attempted. Re-cently, Miller and Gurevych (2015) studied meth-ods for automatic understanding of homographic puns using methods from word-sense disambigua-tion. Paronomasia is intentionally excluded from their data. Following the convention of Miller and Gurevych (2015), we assume that the position of the pun in the input sentence is known. The target recovery task is to identify the pun target given the pun and its left and right word contexts. 3.1 Model The model has three parts: a phonetic edit model, a phonetic lexicon and a language model. The re-covered target T  X  is the word (or words) with the maximum probability given the pun P according to T  X  = argmax where p ( P | T ) is the phonetic edit model and p ( T ) is the language model. The factorization into a lan-guage model and a phonetic edit model is similar to the classic approach to automatic speech recogni-tion.

The implementation of the model uses weighted finite-state transducers (WFSTs) which have been adopted as a useful structure for speech decoding due to their ability to efficiently represent each of the relevant knowledge sources, i.e. phonetic infor-mation, phonetic lexicon and language model, in a single framework (Mohri et al., 2002; Hori et al., 2007). Finite-state transducers are finite-state ma-chines with an input and an output tape. We will use WFSTs for our pun target recovery model. Each of the phonetic edit model (PEM), phonetic lexicon (L) and language model (LM) can be represented as WFSTs, which are joined together by applying the composition operation. The weights on the PEM and LM are negative log-likelihoods, and the lexi-con has no weighting. Each of these models will be explained in further detail below. The target is given by the shortest path in the WFST: where P is the pun and LC and RC are the left and right word contexts. The symbols  X  1 ,  X  and  X  de-note the inverse, composition, and concatenation op-erations respectively.

The sequence of operations P  X  L  X  1  X  PEM  X  L converts a pun to its phonetic form, expands it to a lattice based on phonetic confusions, and then con-verts the phone lattice to a lattice of possible target words. By concatenating the left and right word con-texts and composing with the language model, each path through the WFST is a target word sequence with a weight equal to the combined phonetic edit and language model scores. The ability to handle multi-word puns and/or targets (e.g., the word se-quence  X  X o bell X  can be matched to  X  X obel X , using an example from (Yang et al., 2015)) is made possi-ble because the lexicon WFST L allows multiword sequences.

Since the scores are negative log likelihoods, the target hypothesis is just the shortest path in the WFST. We score the model based on its accuracy at identifying the target which must be an exact match, ignoring punctuation. We disallow the pos-sibility that the pun is hypothesized as a target, i.e. homographic puns, in order to focus on the class of puns whose relationship with their targets is primar-ily phonological. The OpenFst library is used to per-form all of the WFST operations (Allauzen et al., 2007). 3.2 Phonetic Edit Model The purpose of the phonetic edit model is to esti-mate the probability of the pun phoneme sequence given a candidate target phoneme sequence. We pre-fer to learn a model from the data rather than adopt an existing model that relies on phonetic features and edit costs that were derived by hand (Kondrak, 2000). As shown by Ristad and Yianilos (1998), a memoryless WFST model can learn a probabil-ity distribution over edit operations with a princi-pled objective, namely, to maximize the likelihood of the source/target sequences in the training data. In our case, the training data is pun/target phoneme sequences. Memoryless, in this context, refers to the fact that the model is not conditioning on pre-vious symbols, i.e. there is only a single state in the WFST. The model assumes that the phoneme se-quence of the pun is generated through the stochastic application of insertions, deletions, and substitutions to the target phoneme sequence. During learning, the model estimates the function p ( y | x ) where y is a phoneme from the pun and x is a phoneme from the target. When x = this is an insertion of y and when y = it is a deletion of x .
Training uses the expectation-maximization algo-rithm following the equations given by Oncina and Sebban (2005) to estimate the conditional probabil-ity distribution instead of the joint one as originally derived by Ristad and Yianilos. The model is trained to maximize the probability of the pun targets given their sources subject to the constraint that the model encode a probability distribution over all possible string pairs. In the expectation step, puns are aligned with their targets given the current model. Then, the maximization step re-estimates the edit proba-bilities given the current alignment. Edit probabil-ities are initialized by giving a high probability to keeping the same phoneme and uniform small prob-abilities to all possible edits. (We initialized by giv-ing ten times the probability to preserving the same phoneme as to any possible edit operation.) Follow-ing the convention of Sobkowiak (1991), vowels can not align with consonants and vice-versa.

The training data consists of the 1,182 target pun pairs taken from Appendix E of Hempelmann (Hempelmann, 2003). These are a subset of the puns that Sobkowiak took mostly from Crosbie X  X   X  X ictio-nary of Puns X  and analyzed in his work. 3.3 Phonetic Lexicon The lexicon models the pronunciation of each word in the vocabulary. Pronunciations come from the CMU pronunciation dictionary (Weide, 1998). This dictionary has an inventory of 39 phonemes. If a pun is not in the vocabulary of the dictionary, for ex-ample if it is not a word, then its pronunciation is generated automatically using the LOGIOS lexicon are unknown beforehand. Thus, when the lexicon is used to map puns to phonemes the vocabulary size is essentially unlimited. But, when it is used to map the phoneme lattice into a word lattice of potential targets then the fixed vocabulary from the language model is used.

The CMU dictionary includes multiple pronunci-ations for some words. All pronunciations are used with unweighted parallel paths. The version of the dictionary used here includes stress markers and syl-lable boundaries (Bartlett et al., 2009). In the sim-plest version of our model, this information is ig-nored in order to reduce the number of learned pa-rameters in the PEM.

After composing with the phonetic edit model and the lexicon, we do a conservative pruning of the WFST to remove highly improbable word sequences based on the phonetic score and run epsilon removal on the resulting lattice. This reduces the memory footprint and allows use of a larger language model. 3.4 Language Model A 230 million word corpus was formed from com-ments obtained from Reddit, an online discussion forum. These comments were collected from a wide variety of forums, known as subreddits. Reddit con-tributors tend to use a casual conversational style that is a good match for the language used in com-mon puns. All of the text data from Reddit was tokenized using the NLTK tokenizer (Bird et al., 2009). The tokenizer splits contractions into two to-kens but we kept these as a single token to match the pronunciation dictionary. We remove case infor-mation. Punctuation is removed when evaluating the correctness of the hypothesized targets, but punctu-ation symbols are included in the language model. It provides a useful context break in punning rid-dles where the pun/target typically follows a ques-tion mark or other punctuation.

The vocabulary is set by intersecting the vocab-ulary from the CMU pronunciation dictionary with the set of tokens that occur at least 30 times in the language model training data. This gives us a 36,175 word vocabulary. As Sobkowiak (1991) observed, the target tends to have a much higher unigram prob-ability than the pun. This means that the vocabulary size need not be too large to cover most of the tar-gets.

The language model is a trigram model with mod-ified Kneser-Ney smoothing (Chen and Goodman, 1999). Entropy pruning is used to reduce the size of the language model (Stolcke, 2000). It is impor-tant to perform the determinization and minimiza-tion operations on the LM after converting it into the FST representation, in order to reduce the size of the model (Mohri et al., 2008). Because we are us-ing a trigram model, only two words of context are needed on each side of the pun. 3.5 Extending the Phoneme Edit Model with For a listener to recognize the phonological distinct-ness of the pun and the target, they should prefer-ably differ in a perceptually salient position such as a stressed syllable. In particular, we expect that puns would take advantage of the increased acoustic en-ergy in the onset and nucleus of a stressed syllable and utilize these positions for phoneme changes.
To analyze this effect we used a syllabified ver-sion of the CMU pronunciation dictionary (Bartlett et al., 2009). We took the 1-best phonetic align-ment of the training data and split it according to the syllable boundaries of the pun. Then we computed the probability of a phoneme change according to the position in the syllable and the stress. Conso-nants in the onset of a stressed syllable have a 40.3% probability of changing between the target and pun. The nucleus of a stressed syllable of has a 36.8% probability of substitution. This is compared to a 31.8% probability of substitution for phonemes in unstressed syllables and coda positions.

To incorporate this into an extension of the pho-netic edit model, we created a three state model. There is one default state and two special states for the stressed syllable onset and nucleus respectively. The phoneme edit probabilities p ( y | x ) were scaled according to the state s and renormalized so that p ( y | x,s ) is a valid probability distribution. When a substitution does occur, we assume that the choice of target phoneme is independent of the syllable po-sition and stress. The net effect of the syllable ex-tension to the PEM is to encourage substitution of onsets and nuclei of stressed syllables and discour-age it otherwise. 4.1 Data We collected 75 puns from various joke websites such as Tumblr, Reddit, and Twitter and soliciting collected without reference to the sources used by Sobkowiak to assemble the puns used in building the phonetic edit model. This data was used for test data only and is completely separate from the training data used by the language model and the phonetic edit model. (It would be nice to have used some of the 1,182 puns from Sobkowiak for test data but only the isolated pun/target pairs were provided without the necessary word contexts.) Pun locations were marked in each sentence as the minimal set of words that change between the pun and the target.

Note that the phonetic edit model is trained on ex-clusively imperfect puns but it is tested on both per-fect and imperfect puns (24 perfect and 51 imper-fect). This creates a mismatch between the training data and the test data. Target recovery is harder on imperfect puns but having a mix of both types better reflects what is commonly found in the wild. 4.2 Baseline Model As a baseline model we replace our phonetic edit model with the cost function proposed by Hempel-mann, which we replicated based on details given in Appendix G of his thesis (Hempelmann, 2003). This cost function, which Hempelmann refers to as  X  X reliminary, X  is the only published phonetic edit model for paronomasic puns. The cost table is based on the phoneme pair alignment counts from the 1,182 training pairs that were aligned by hand. The phoneme alignment counts are converted to costs by using simple ad-hoc equations. Vowel pair counts are transformed to costs using cost = 0 . 3  X  0 . 3  X  Our model improves upon the baseline by avoiding heuristic transformations. Since the phoneme sym-bol set used by Hempelmann (based on (Sobkowiak, 1991)) differs from that used in the CMU dictio-nary, the Hempelmann costs are mapped to match the CMU inventory. 4.3 Results We report the performance of our model in Table 1 using accuracy and mean reciprocal rank as met-rics. If the correct target did not appear in our n-best list then we use a value of zero for its recipro-cal rank. Ties are broken randomly. The baseline uses Hempelmann X  X  phonetic cost model plus the LM, and we include two ablation models that use just the LM or just the PEM. The other two models use the LM with either the memoryless PEM or the PEM with the syllable extension. Accuracy
Using the language model only gives poor perfor-mance. It is only able to recover the target when the target happens to be an idiomatic expression. The PEM-only model does significantly better than using the LM only, highlighting the importance of phonet-ics in paronomasic puns. In the full system, Hempel-mann X  X  cost matrix does not fare well compared to the PEM model. The Hempelmann cost matrix does a poor job of separating likely targets from the rest of the vocabulary. Thus, many times the true tar-get is pruned before the application of the language model.

The LM + PEM model recovers the target more than two-thirds of the time and has a mean reciprocal rank of 0.729. When using the syllable extension to the PEM, the results agree on the rank of the target for all but five puns. For those five, the model with the syllable extension improves the rank compared to the basic PEM. Two puns from that set of five are (a mall/ X  X m all and Hebrews/he brews, respectively). The hypothesized targets  X  X mmoral X  and  X  X e abuse X  outrank the true target for these puns in the basic PEM model but not in the syllable one because they change more phonemes in unstressed syllables.
Perfect puns are easier to recover than imperfect ones. The LM + PEM model does well on both per-fect and imperfect puns. As to be expected, the PEM only model does very poorly on imperfect puns and the LM only model does equally poorly on both per-fect and imperfect.

Table 2 shows the top ranked hypothesis for a sample pun using the LM + PEM model, where the cost in this table is the negative log-likelihood. In this case, the top ranked hypothesis was correct. The second highest ranked hypothesis is a misspelling of the target that is common enough for the language model to also give it a high score.

An example where the model makes a mistake is: The pattern of  X  X ne thing . . . another X  is common in English but, in this case, the target  X  X nother X  is too far away from  X  X ne thing X  for the relationship to be captured by the tri-gram language model. A consequence of using a stochastic model for pho-netic edit costs is that there is a non-zero edit cost between a phoneme and itself and that cost is differ-ent depending on the phoneme. This highlights the fact that we model the edit (or transformation) prob-abilities of the pun/target corpus rather than phono-logical similarity (which would be a symmetric cost function). The analysis below shows that the edit model is in fact capturing more than simple phono-logical similarity. 5.1 Phoneme Edit Probabilities Figures 1 and 2 show the probability of observing a source phoneme (i.e. a phoneme appearing in the pun) given each target phoneme for the vowel and consonant pairs respectively. The numbers are to be interpreted as percentages and values less than 1 are not shown. The  X . X  symbol is used for epsilon transi-tions and indicates segment insertions and deletions. Vowels. Regarding vowels, our data corroborates some of the findings of Sobkowiak (1991). The lower numbers along the diagonal in Figure 1 rel-ative to Figure 2 indicates the violability of vowels (39%) relative to consonants (34%) in paronoma-sic puns. Our data also confirms that, where puns are concerned, marked segments do not necessarily oust unmarked. According to Zwicky and Zwicky (1986), if  X  X arked ousts unmarked X  we would ex-pect to see that tense vowels oust lax vowels. Rather, what we find in this data is that the majority of vow-els and diphthongs are ousted by AH ( @ ), widely considered an unmarked vowel. Puns demonstrating this phenomenon include (pun/pen and sediment/said he meant, respectively). We hypothesize that the low cost for substituting AH for other vowels is due in part to the fact parono-masic puns originally were a spoken phenomenon, and so the substitution possibilities for a given tar-get vowel depend significantly on the variety of re-alizations of that vowel in speech. It is well at-tested cross-linguistically that vowels undergo re-duction in unstressed positions (Crosswhite, 2004). In English running speech, this reduced form most closely resembles AH (Burzio, 2007). The data pre-sented here suggests that punners take advantage of the commonality of running speech vowel reduction when considering target recoverability, resulting in the availability of AH as a replacement for most tar-get vowels. Our model captures this fact by assign-ing low cost to such a substitution.

Our data provides other insights into the nature of vowel substitutions in puns. For instance, we see that IY ( i ) is less likely to change in a pun than other monophthongs, indicating a significant perceptual distance between IY and its neighbors. Most likely to change are the vowels AO ( O ) and UH ( U ). The violability of AO is likely due to the AO/AA merger present in many American English Dialects (Labov et al., 2006), and in fact we see targets with AO fre-quently mapping to puns with AA. The mutability of UH is also interesting: while it, like other monoph-thongs, is ousted by AH, it is also ousted by a closely articulated marked counterpart, UW ( u ). This seems to be the sole example in our data of marked vowels ousting an unmarked vowel to a significant degree.
Another interesting feature of this data is the sub-stitutability of ER ( 3~ ) and OY ( OI ), as in puns like (earl/oil and loining/learning, respectively). These edits seem to indicate punners X  awareness of rhotic-ity variation among English dialects (Labov, 1972). Consonants. In Figure 2, we see several expected trends. D ( d ) ousting DH ( D ), T ( t ) ousting TH ( T and V ( v ) ousting W ( w ) are all as expected accord-ing to the  X  X arked ousting unmarked X  hypothesis of Zwicky and Zwicky (1986). Yet we also see S ( s ) ousting Z ( z ), which is an instance of the unmarked voiceless alveolar fricative ousting the voiced, as well as N ( n ) ousting NG ( N ), an instance of the unmarked coronal ousting the marked velar. These oustings, like the case of AH ousting other vowels, are likely conditioned by segment frequency. For N ousting NG, syllable structure may play a role as well, as NG is restricted to codas whereas N is not.
An interesting feature of our consonant data is the extreme violability of interdentals TH and DH, which are more likely to map to T and D respectively than to retain their identity in a paronomasic pun. In addition to the  X  X it of whiskey X  example mentioned earlier, we have (disgusting/this gusting). This phenomenon is known as  X  X h-stopping X  and is a common dialectal feature of many variants of English, from those of Philadelphia and New York to the Caribbean (Wells, 1982). This substitution supports the hypothesis that the substitution possibilities for a segment depend on the realizations of that segment which are com-monly encountered in speech. Notably, T is signifi-cantly more likely to replace TH than is F ( f ), despite that the articulatory and acoustic similarity between TH and F is greater.
 5.2 Correlation with Human Ratings Our phonetic edit model allows us to empirically verify an untested assumption with respect to phono-logical similarity from Fleischhacker (2002)  X  X hat degree of representation in the pun corpus corre-lates with pun goodness. X  In another paper, she repeats this assumption and adds the explanation  X  X ruly funny puns are generally those in which the phonological relationship between pun and target is . . . subtle but quickly recognizable X  (Fleischhacker, 2005). Hempelmann writes that this assumption is  X  X nlikely X  to be true.

We conducted a survey of native English speak-ers where respondents were asked to rate 17 puns on a five point scale: hilarious, funny, okay, bad, ter-rible. The puns were selected from the test set to have a variety of phonemic edit distances. Respon-dents also had the option to indicate that they did not understand the pun, in which case their answer was ignored. A phonetic edit score was calculated for each pun-target pair by averaging the log-likelihood value from our model over the phonemes in the pun. The order of the questions was randomized for each respondent. Advertising for the survey was done us-ing /r/SampleSize, a Reddit forum for recruiting sur-vey participants. The 435 respondents gave us 7,135 ratings in total.

The relationship between phonetic edit cost and goodness was measured using ordinal regression, with clustered standard errors to account for the fact that responses from the same person are not in-dependent. We use the RMS package in R (Har-rell Jr., 2015). The regression coefficient indicates that decreased phonetic edit cost is indeed asso-ciated with higher perceived goodness of the pun with p &lt; 0 . 0001 . For visualization purposes we mapped the categorical goodness ratings onto a nu-meric scale from 1-5 to create an average goodness rating for each pun. In Figure 3, we depict the pho-netic edit cost vs. the average goodness rating for each of the 17 puns. The line shown in that figure is an outlier resistant linear regression.

The biggest outlier is The pun is on  X  X our mother X  with  X  X nother X  as the target. This pun has the highest phonetic edit cost in our sample but it makes up for it with more interest-ing semantics than average. The quality of our phonetic edit model is evident from its performance at the target recovery task, as well as the fact that it captures known linguistic phe-nomena such as vowel reduction and dialectal fea-tures. Furthermore, by collecting human ratings we are able to empirically verify the previously untested assumption that lower phonetic edit costs in puns correlate with pun goodness.

The strength of the model can be leveraged to im-prove the quality of pun generation and humor clas-sification systems that have used weaker phonetic edit models (Binsted, 1996; Valitutti, 2011; Raz, 2012). Some pun generation systems are limited to exact homophones. In this work, we did not con-sider homographic puns. In principle, our algorithm can handle these by introducing an LM weight to control the balance of PEM/LM scores. Pun genera-tion is much more complicated than target recovery as reflected in the complexity of proposed systems for humor generation. However, improved under-standing of puns by way of progress in the target recovery task should also lead to corresponding im-provements in the task of pun generation.

Our syllable extension to the PEM gave the best performance, but only by a small margin. Extend-ing the edit model further is a fruitful area for future work but will likely require additional data.
In this work, we assume that the pun is given. Of interest for future work is joint recognition of the pun and its target. Preliminary experiments indicate that the unigram word probabilities are a somewhat strong feature for pun recognition but further work is needed. For contextually-integrated puns, iden-tifying the pun is likely to be more difficult, and for some cases it would be useful to integrate image cues.
 We thank Arjun Sondhi for his assistance in design-ing and analyzing the survey and Hope Boyarsky for her help assembling our pun corpus. We also would like to thank Nathan Loggins and the anonymous re-viewers for their feedback on this work.

