 In the situation of information explosion that characteriz es todays world, the need for automatic tools for data analysis is more than obvious. Here, we focus on an un supervised analysis of data that can be organized in matrix form. Clearly, this broad definition cov ers various types of data. For instance, in text analysis data, the rows of a matrix correspond to words, the columns to different documents, and entries indicate the number of occurrences of a particular w ord in a specific document. In a matrix of gene expression data, rows correspond to genes, columns t o various experimental conditions, rows correspond to viewers, columns to movies, and entries i ndicate ratings made by the viewers. Finally, for financial data, rows correspond to stocks, colu mns to different time points, and each entry indicates a price change of a particular stock at a give n time point.
 While the text analysis case is a classical example of co-occu rrence data, the remaining examples are not naturally interpreted that way. Typically, a normal ized words-documents table is used as an estimator of a words-documents joint probability distri bution, where each entry estimates the probability of finding a given word in a given document, where as the words are assumed to be inde-pendent of each other [1, 2]. By contrast, values in a financia l data matrix are a general function of stocks and days and in particular might include negative num erical values. Here, the data cannot be regarded as a sample from a joint probability distribution o f stocks and days, even if a normalization is applied. Though it can be argued that each entry of the matr ix is a sample from a joint probability distribution of three variables: stock, day, and price chan ge -the degenerate nature of this sample must be taken into account: the rate change of a given stock on a given day occurs only once and no by direct sampling. A similar argument applies to survey dat a like movie ratings. In this case the sample might be even more degenerate, with many  X  X issing val ues X , since not all the viewers rate all the movies. Although gene expression data can be conside red as a repeatable experiment, very often different experimental conditions correspond to onl y one single column in the matrix. Thus once again a single data point represents the joint statisti cs of three variables: gene, condition, and expression level.
 Nevertheless, in most such cases there is a statistical rela tionship within rows and/or within columns and movies with similar characteristics give rise to simila r rating profiles. Such relationships can be exploited by clustering algorithms that group together sim ilar rows and/or columns, and furthermore make it possible to complete the missing entries in the matri x [3].
 The existing clustering techniques can be classified into th ree major categories. (i) Similarity, or points and possibly to new points as well. The nature of the di stance measure is crucial to these techniques and inherently requires expert knowledge of the application domain, which is often un-chosen to describe the data. As before, an a priori choice of an appropriate model is far from ob-Information Bottleneck (IB) approach [4] and its extension s. Instead of defining the clustering ob-jective through a distortion measure or data generation pro cess, the approach suggests using relevant variables. A tradeoff between compression of the irrelevan t and prediction of the relevant variables is then optimized using information theoretic principles. Importantly, the definition of the relevant variable is often natural and obvious for the task at hand, an d in turn the method yields the optimal relevant distortion for the problem.
 Since the original work in [4], multiple studies have highli ghted the theoretical and practical im-portance of the IB method, in particular in the context of clu ster analysis [2, 5, 6, 7, 8]. However, the original formulation is based on the availability of co-occurrence data. In practice a given co-occurrence table is treated as a finite sample out of a joint di stribution of the variables; namely row and column indices. Unfortunately, as mentioned above, this assumption does do not fit many realistic datasets, thus preventing a direct application o f the IB approach in various domains. To address this issue, in [9] a random walk over the data point s is defined, serving to transform non co-occurrence data into a transition probability matri x that can be further analyzed via an IB algorithm. In a more recent work [10] the suggestion is made t o use the mutual information between different data points as part of a general information-theo retic treatment to the clustering problem. The resulting algorithm, termed the Iclust algorithm, was demonstrated to be superior or comparable to 18 other commonly used clustering techniques over a wide r ange of applications where the input data cannot be interpreted as co-occurrences [10]. However , both of the approaches have limitations  X  Iclust requires a sufficient amount of columns in the matrix for reli able estimation of the informa-tion relations between rows, and the Markovian Relaxation a lgorithm involves various non-trivial steps in the data pre-process [9].
 Here, we suggest an alternative approach, inspired by the multivariate IB framework [8]. The mul-tivariate IB principle expands the original IB work to handl e situations where multiple systems of clusters are constructed simultaneously with respect to di fferent variables and the input data may correspond to more than two variables. While the multivariat e IB was originally proposed for co-occurrence data, we argue here that this framework is rich en ough to be rigorously applicable in the new situation. The idea is simple and intuitive: we look f or a compact grouping of rows and/or columns such that the product space defined by the resulting c lusters is maximally informative about the matrix content, i.e. the matrix entries. We show that thi s problem can be posed and solved within the original multivariate IB framework. The new choice of re levance variable eliminates the need presented here). Moreover, when missing values are present , the analysis suggests an information theoretic technique for their completion.
 We explore the application of this approach to various domai ns. For gene expression data and finan-cial data we obtain clusters of comparable quality (measure d as coherence with manual labeling) to those obtained by state-of-the-art methods [10]. For movie rating matrix completion, performance is superior to the best known alternatives in the collaborat ive filtering literature [11]. 2.1 Problem Setting We henceforth denote the rows of a matrix by X , the columns by Y , and the matrix entries by Z (and small x, y and z for specific instances). The number of rows is denoted by n , and the number of columns by m . We regard X and Y as discrete coordinate space and Z ( X, Y ) as a function. Generalization to higher dimensions and continu ous coordinates is readily possible, but not discussed here. For a given matrix, a row x , and a column y , the value of z ( x, y ) is assumed to be deterministic. This can be relaxed as well.
 The objective is to find  X  X ood X  partitions of the X -Y space that will be informative with respect to the function values Z ( X, Y ) . The partitions are defined by grouping of rows into clusters of rows C and grouping of columns into clusters of columns D . The complexity of such partitions is measured by the sum of the weighted mutual information val ues nI ( X ; C ) + mI ( Y ; D ) . For the hard partitions considered in the paper this sum is the nu mber of bits required to describe the partition (see [12]). The informativeness of the partition is measured by another mutual information, information level about the matrix values Z . This can be expressed via the minimization of the following functional: column clusters d , and  X  is a Lagrange multiplier controlling the tradeoff between c ompression and accuracy.
 describe a sequential algorithm for its minimization. We wi ll stick to the following notation con-ventions: p is used for distributions that involve only input parameter s and hence do not change during the analysis,  X  p is used for empirical distributions, q for the sought mapping distributions and  X  q for empirical distributions dependent on the sought mappin gs. By the definition of the mutual information [12]: We define the indicator function: and denote the total number of populated entries (which is ou r sample size) by: N = P Then: I ( Y ; D ) ,  X  p ( y ) , and  X  q ( d ) are defined similarly.
 We assume Z is a categorical variable, thus: In the special case of complete data matrices 1 the general case considered in this paper X and Y (and thus C and D ) are not independent. 2.2 Sequential Optimization minimization functional L minimize L suggested in [13]. This algorithm is quite simple: Due to monotonic decrease in L anteed to converge to some local minima of (1). Multiple rand om initializations may be used to improve the result. This simple algorithm is by far not the on ly way to optimize (1), but in practice it was shown to achieve very good results on similar optimiza tion problems [2].
 The complexity of the algorithm is analyzed in the complemen tary material, where it is shown to be and | C | , | D | are cardinalities of the corresponding variables. 2.3 Minimal Description Length (MDL) Formulation The minimization functional L plications they may be given (e.g. the desired number of clus ters), there are cases when they also require optimization (as in the example of matrix completio n in the next section). To perform such optimization, the Minimum Description Length (MDL) princi ple [14] is used. The idea behind MDL is that models achieving better compression of the training data -when the compression includes a model description -also achieve better generalization on t he test data.
 The following compression scheme is defined: | C | row and | D | column clusters define | C || D | sec-Thus the total description length is nI ( X ; C ) + mI ( Y ; D ) + N H ( Z | C, D ) + | Z || C || D | Since H ( Z | C, D ) = H ( Z )  X  I ( C, D ; Z ) and H ( Z ) is constant the latter can be omitted from optimization, which results in total minimization functio nal Observe that constrained on | C | , and | D | , L i.e. the optimal tradeoff  X  = N is uniquely determined. Since in practice F both C and D , the optimal values for these two parameters may be easily de termined by scanning. 2.4 Relation with the Multivariate Information Bottleneck Multivariate Information Bottleneck (IB) [8] is an unsuper vised approach for structured data ex-ploration. Its core lies in combining the Bayesian networks formalism [15] with the Information Bottleneck method [4]. Multivariate IB searches for a meani ngful structured partition of the data, defined by compression variables (in our case these are C and D ). Two graphs, G defined. The former specifies the relations between the input (data) variables  X  in our case, X , Y , and Z  X  and the compression variables. The latter specifies the inf ormation terms that are to be preserved by the partition. A tradeoff between the multi-in formation preserved in the input struc-ture I G in (which we want to minimize) and the multi-information expre ssed by the target structure I out (which we want to maximize) is then optimized. For a set of var iables V = V 1 , .., V n , a multi-information I G ( V ) is defined as: where P a G ( V The graphs G X and Y in G direction.) The corresponding optimization functional is : them from the above optimization and obtain exactly the opti mization functional defined in (1), only with equal weighting of I ( X ; C ) and I ( Y ; D ) .
 Thus the approach may be seen as a special case of the multivar iate IB for the graphs G defined in Figure 1. An important distinction should be made t hough: unlike the multivariate IB excluding the term I ( X, Y ; Z ) from the optimization functional. 2.5 Relation with Information Based Clustering A recent information-theoretic approach for cluster analy sis is given in [10], and is known as infor-applicable to co-occurrence as well as non co-occurrence da ta. In the following we highlight the re-lation of our work and this earlier contribution.
 s ( x 1 , x 2 ) used in [10] as: s ( x 1 , x 2 ) = X Table 1: Clusters coherence for the ESR and S&amp;P stock datasets. The table provides the coher-ence of the achieved solutions for N Iclust algorithm at the same settings are shown in brackets a longside the results of our algorithm. For the ESR data an average coherence according to the three G Os is shown. Separate results for Substituting this in the optimization functional of [10], c hanging maximization to minimization by flipping the sign, and substituting T = 1 measured through pairwise distances and not based on a centr oid model.
 Importantly, in order to be able to evaluate I ( Z of columns to be available, whereas our approach can operate with any amount of columns given. Alternately, even when the data contain many columns, but ar e relatively sparse, i.e. , have many missing values, evaluating I ( Z tersection of non-missing observations for z the contrary, the approach is designed to cope with this kind of data and resolves the problem by simultaneous grouping of rows and columns of the matrix to am plify statistics. We first compare our algorithm to I-Clust, as it was shown to be superior/comparable to 18 other describe an experiment on matrix completion. Another appli cation to a small dataset is provided in the supplementary material 1 . In the last two cases Iclust is not directly applicable. The multivariate IB is not directly applicable to all the provided examples. 3.1 One Dimensional Clustering -Comparison to I-Clust We focus on two applications reported in [10]. For purposes o f comparison we restrict our algorithm to cluster only the rows dimension of the matrix by setting th e number of column clusters, | D | , equal to the number of columns, m . This simplifies the objective functional defined in equatio n (1) to L values for the  X  parameter.
 The first dataset consists of gene expression levels of yeast genes in 173 various forms of envi-ronmental stress [16]. Previous analysis identified a group of  X  300 stress-induced and  X  600 stress-repressed genes with  X  X early identical but opposit e patterns of expression in response to the environmental shifts X  [17]. These 900 genes were termed the yeast environmental stress response (ESR) module. Following [10] we cluster the genes into | C | = 5 , 10 , 15 , and 20 clusters. To assess the biological significance of the results we consider the coherence [18] of the obtained clusters with respect to three Gene Ontologies (GOs) [19]. Specifical ly, the coherence of a cluster is defined as the percentage of elements within this cluster that are gi ven an annotation that was found to be significantly enriched in the cluster [18]. The results achi eved by our algorithm on this dataset are comparable to the results achieved by I-Clust in all the veri fied settings -see Table 1. Poor (S&amp;P) 500 list 2 , during 273 trading days of 2003. As with the gene expression data we take evaluate the coherence of the ensuing clusters we use the Glo bal Industry Classification Standard 3 , group, industry, and subindustry. As with the ESR dataset ou r results are comparable with the results of I-Clust for all the configurations -see Table 1. 3.2 Matrix Completion and Collaborative Filtering Here, we explore the full power of our algorithm in simultane ous grouping of rows and columns of a matrix. A highly relevant application is matrix completio n -given a matrix with missing values we would like to be able to complete it by utilizing similarit ies between rows and columns. This problem is at the core of collaborative filtering applicatio ns, but may also appear in other fields. We test our algorithm on the publicly available MovieLens 10 0K dataset 4 . The dataset consists of 100,000 ratings on a five-star scale for 1,682 movies by 943 us ers. We take the five non-overlapping splits of the dataset into 80% train on 20% test size provided at the MovieLens web site. We stress that with this division the training data are extremely spar se -only 5% of the training matrix entries are populated, whereas 95% of the values are missing.
 To find a  X  X ood X  bi-clustering of the ratings matrix, minimiz ation of F by scanning cluster cardinalities | C | and | D | and optimizing L pair of | C | , | D | . The minimum of F sensitivity to small changes in | C | and in | D | both in F supplementary material 1 for visualization of the solution at | C | = 4 and | D | = 3 . To measure the accuracy of our algorithm we use mean absolute error (MAE) metrics, which the median of z values within each section c, d .
 Note that our algorithm is general and does not directly opti mize the MAE error functional. Nev-ertheless we obtain 0.72 MAE (with a deviation of less than 0. 01 over multiple experiments). This confidently beats the  X  X agic barrier X  of 0.73 reported in the collaborative filtering literature [11]. The root mean squared error (RMSE) measured for the same clus tering with a mean of z values within each section c, d taken for prediction yields 0.96 (with a deviation below 0.0 1). This is much better than 1.165 RMSE reported for a dataset 20 times larger [20] and quite close to 0.9525 RMSE reported by Netflix for a dataset 1000 times larger of a simila r nature 5 . A new model independent approach to the analysis of data give n in the form of samples of a function Z ( X, Y ) rather than samples of co-occurrence statistics of X and Y is introduced. From a theo-retical viewpoint the approach is a much required extension of the Information Bottleneck method that allows for its application to entirely new domains. The approach also provides a natural way for bi-clustering and matrix completion. From a practical view point the major contribution of the paper is the achievement of the best known results for a wide range o f applications with a single algorithm. As well, we improve on the results of prediction of missing va lues (collaborative filtering). Possible directions for further research include generali zation to continuous data values, such as those obtained in gene expression and stock price data, and r elaxation of the algorithm to  X  X oft X  than clustering, as occurs in IB when applied to continuous v ariables [21].
 The proposed framework also provides a natural platform for derivation of generalization bounds for missing values prediction that will be discussed elsewh ere.

