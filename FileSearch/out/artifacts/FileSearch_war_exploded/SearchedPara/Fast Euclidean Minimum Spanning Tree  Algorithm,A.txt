 The Euclidean Minimum Spanning Tree problem has appli-cations in a wide range of fields, and many efficient algo-rithms have been developed to solve it. We present a new, fast, general EMST algorithm, motivated by the clustering and analysis of astronomical data. Large-scale astronomical surveys, including the Sloan Digital Sky Survey, and large simulations of the early universe, such as the Millennium Simulation, can contain millions of points and fill terabytes of storage. Traditional EMST methods scale quadratically, and more advanced methods lack rigorous runtime guaran-tees. We present a new dual-tree algorithm for efficiently computing the EMST, use adaptive algorithm analysis to prove the tightest (and possibly optimal) runtime bound for the EMST problem to-date, and demonstrate the scalability of our method on astronomical data sets.
 F.2.0 [ Analysis of Algorithms and Problem Complex-ity ]: General; I.5.3 [ Clustering ]: Algorithms Algorithms, Theory Adaptive Algorithm Analysis, Euclidean Minimum Span-ning Trees
We present a new algorithm for the fundamental and widely applied Euclidean Minimum Spanning Tree (EMST) prob-lem. Given a set of points S in R d , our goal is to find the lowest weight spanning tree in the complete graph on S with edge weights given by the Euclidean distances between points. With references in the literature as early as 1926, the MST problem is one of the oldest and most thoroughly stud-ied problems in computational geometry [36]. In addition to this long-standing theoretical and algorithmic interest, the MST is useful for many practical data analysis problems. Many optimization problems can be posed as the search for the MST in a network [36]. The MST is also used as an approximation for the traveling salesman problem [24], in document clustering [48], analysis of gene expression data [15], wireless network connectivity [46], percolation analyses [8], and modeling of turbulent flows [44], among other areas. These problems are commonly solved in the Euclidean set-ting. In this case, the computational bottleneck in both tra-ditional MST algorithms like Kruskal X  X  [28] and Prim X  X  [37] and more advanced methods is finding the nearest neigh-bor of components in a spanning forest. We propose a new method to overcome this obstacle and demonstrate its the-oretical and experimental superiority.

In particular, we are interested in using the EMST to compute hierarchical clusterings [20, 53]. One such cluster-ing is obtained by deleting all edges longer than a specified cutoff in the MST, generating a clustering through the re-maining connected components. By varying the scale of the cutoff, this generates a hierarchical clustering. In the clus-tering literature, this is often referred to as a single-linkage clustering and is frequently represented by a dendrogram . While the single-linkage clustering is very simple and can be sub-optimal for many applications, it can form the basis of more insightful clusterings. The single linkage clustering can be pruned to obtain more useful astronomical results [4]. MST X  X  also form the inner loop for methods to identify non-parametric clusters in noisy data [49]. Furthermore, theoretically optimal clusterings can be obtained efficiently from the single-linkage clustering [3].

In astronomy, EMST-based clustering is used to analyze deep-space surveys and simulations of the early universe. Each level of single-linkage clustering is known as a friend-of-friends clustering [40, 2]. The EMST is used to identify dark matter haloes in simulations, which are believed to be crucial to galaxy formation [29]. Clustering is also applied to sky surveys to identify the super-large scale structure of the universe, which sheds light on the conditions of the early universe and the mechanisms of galaxy formation [4].
The volume of data produced in the astronomy commu-nity has grown explosively in recent years. Recent large surveys include the Las Campanas Redshift Survey (26,418 objects) [42], the 6dF Galaxy Survey (125,071 galaxies) [25], the 2dF Galaxy Redshift Survey (382,323 objects) [13], and the Sloan Digital Sky Survey (over 230 million objects) [52]. In addition, our understanding of cosmology has benefit-ted from large-scale simulations of the formation of galaxies and conditions in the early universe. For example, the Mil-lennium Simulation [43] contains over 1 billion points and produces terabytes of output. The analysis of the current structure of the universe as revealed in the large sky surveys and comparison to the predictions of theories in simulations are the keys to understanding the origins of the cosmos and validating new models, including verification of dark mat-ter and dark energy. This in turn requires the ability to compute minimum spanning trees quickly and accurately for very large data from a variety of distributions.
Adaptive Analysis. Traditional approaches to algo-rithm analysis use the running time for the worst possible input as an upper bound for the running time of all in-stances. This often leads to overly pessimistic bounds due to a few pathological inputs. Adaptive analysis seeks to improve these results by considering properties of the in-puts in the analysis. By bounding the runtime in terms of these properties, one can obtain tighter and more informa-tive bounds. Adaptive analysis has been successfully applied to many fundamental problems including searching in lists [6], merging arrays [14], sorting [16], and the convex hull problem [27]. Despite these successes, the difficulty of char-acterizing the inputs in relation to the problem has limited the number of applications.

Our Contribution. We present a new Euclidean mini-mum spanning tree algorithm, DualTreeBoruvka .Using the dual-tree algorithmic framework [22], we can efficiently compute the shortest edge between components in a span-ning forest, thus overcoming the bottleneck of most EMST methods. We show:
Paper Outline. We review MST algorithms, both gen-eral and Euclidean, and their runtime bounds in section 2. In section 3, we define certain inherent properties of a dataset that are necessary to characterize the difficulty of computing the MST. In section 4, we describe the DualTreeBoru-vka algorithm and bound the running time of the cover-tree-based version in section 5. In section 6, we compare our algorithm to several competing methods with empirical timings for synthetic data and astronomical surveys, and we conclude in section 7.
Many MST algorithms rely on Tarjan X  X  blue rule [45], which says the minimum weight edge across any edge cut is in the minimum spanning tree. This allows us to greedily form cuts in the graph and add the minimum weight edge across each. Algorithms using this rule include Kruskal X  X  [28] and Prim X  X  [37], which require O ( m log n )and O ( m + n log n ) time, respectively, on a graph with n points and m edges. Both algorithms maintain one or more components in span-ning forest and use the cut between one component of the forest and the rest of the graph, adding the edges found in this way one at a time.

Boruvka X  X  Algorithm. In this work, we focus on the earliest known minimum spanning tree algorithm, Bor  X  uvka X  X  algorithm, which dates from 1926. See [32] for a transla-tion and commentary on Boruvka X  X  original papers. As in Kruskal X  X  algorithm, a minimum spanning forest is main-tained throughout the algorithm. Kruskal X  X  algorithm adds the minimum weight edge between any two components of the forest at each step, thus requiring N  X  1 steps to com-plete. Bor  X  uvka X  X  algorithm finds the minimum weight edge incident with each component, and adds all such edges, thus requiring at most log N steps and a total running time of O ( m log n ). We define the nearest neighbor pair of a com-ponent C as the pair of points q  X  C, r  X  C that minimizes d ( q, r ). Finding the nearest neighbor pair for each compo-nent and adding the edges ( p, q ) to the forest is called a Boruvka step . Boruvka X  X  algorithm then consists of forming an initial spanning forest with each point as a component and iteratively applying Boruvka steps until all components are joined.

General MST Algorithms. More recently, sophisti-cated algorithms have been developed for the MST problem on general graphs. Fredman &amp; Tarjan [17] showed a bound of O ( m log  X  n ), which was soon improved to O ( m log log [19]. Yao further improved the bound to O ( m log log n ) [50]. Chazelle showed O ( m X  log  X  ), where  X  ( m, n ) is a functional inverse of Ackermann X  X  function [11]. Chazelle [12] and Pet-tie [34] improved this to O ( m X  ). The current tightest bound, due to Pettie &amp; Ramachandran [35] in 2002, is O ( T  X  where T  X  is bounded from below by  X ( m )andaboveby O ( m  X   X  ). All these general algorithms are insufficient for large, metric problems because they depend linearly on the number of edges. In the Euclidean case, the edge set con-sists of all pairs of points. Therefore, linear scaling in m corresponds to quadratic scaling in the number of points, and thus we need to consider other approaches.

Euclidean MST Algorithms. Shamos &amp; Hoey [41] ap-plied the Voronoi diagram to constructing the MST in the Euclidean plane. The Voronoi diagram can be constructed in O ( N log N )timefor N points and contains O ( N )edges. Since the MST is a subset of the edges in the dual of the Voronoi diagram, the MST can be found in O ( N log N )time using one of the algorithms above. This bound worsens to O ( N 2 log N ) in three or more dimensions, fundamentally limiting this method to two dimensional cases. Preparata and Shamos [36] give a lower bound for the EMST problem of  X ( N log N ), which is the tightest known lower bound.
Bentley and Friedman [5] developed an EMST algorithm using kd -tree-based nearest neighbor searches to find the next edge to add in Prim X  X  algorithm. While their method lacks a formally rigorous bound, they estimate that it re-quires O ( N log N ) time for most distributions of points. An alternate implementation of this approach is given in [33]. In 1982, Yao gave a bound of O ( N 2  X  a ( k ) (log N ) 1  X  a a ( k )=2  X  ( k +1) for points in a k -dimensional metric space, along with a O (( N log N ) 1 . 8 ) bound for points in three di-mensions [51]. Agarwal et al. (1991) relate d the running time to the bichromatic closest pair (BCP) problem. Given a set of red and a set of blue points, the bichromatic closest pair is the red point r and blue point b such that d ( r, b )is minimized. They showed a bound of O ( F d ( N, N )log d ( N )), where F d ( N, M ) is the time to solve the BCP problem with N blue and M red points in d dimensions [1].
 WSPD-based Methods. Callahan &amp; Kosaraju X  X  Well-Separated Pair Decomposition (WSPD) [10] forms the basis of the most recent EMST algorithms. The WSPD is defined as a set of pairs of nodes in a space-partitioning tree such that for each pair of points ( p, q ), we have p  X  P, q  X  for exactly one pair of nodes ( P, Q ), and the the nodes in any pair are farther apart than the diameter of either node. It can be shown that the WSPD has O ( N ) pairs of nodes, and that the MST is a subset of the edges formed between the closest pair of points in each pair of nodes. In [9], the authors use the WSPD to improve Agarwal and coworker X  X  1991 bound to O ( F d ( N, N )log N ). Their algorithm uses the WSPD-based nearest neighbor algorithm to compute neigh-bors of components for Boruvka X  X  algorithm. The method identifies a list of pairs in the WSPD, for which bichromatic closest pair computations are performed to find edges of the MST. This algorithm is superficially similar to our method, but only locates neighbors for small components in each iteration. It also requires bookkeeping and connectedness queries which are not factored into the analysis, and no ex-perimental results are shown.

Narasimhan et al. [31] implement a variant of this method, which they attribute to [9]. In this algorithm, GeoMST , they compute the BCP for each pair in the WSPD, then apply Kruskal X  X  algorithm to the resulting edge set. They improve this method by postponing and avoiding some BCP computations and refer to the resulting algorithm as Ge-oMST2 . This method can be successfully applied to point sets of any dimensionality; however, the constant in the O ( N ) size of the WSPD grows exponentially in the dimen-sion and is often very large in practice. The authors ar-gue that the algorithm has an expected O ( N log N ) running time, but do not prove this rigorously. They also demon-strate favorable running times on several data sets.
These algorithms are the most sophisticated methods for the EMST problem in terms of both theoretical analysis and practical performance. The runtime bound in terms of the bichromatic closest pairs problem is the tightest available given optimistic runtimes for bichromatic closest pairs, but it is incomplete without bounding F d . Bentley and Fried-man X  X  kd -tree-based method and the tree-and WSPD-based GeoMST2 are the most practically viable algorithms. We return to these methods in our experimental analysis.
We now consider the problem of computing the EMST in relation to properties of the data. We present three objective parameters, independent of any algorithm and argue that these parameters capture difficulties in computing the MST. Expansion Constant. The expansion constant , due to Karger and Ruhl [26], bounds the maximum increase in the density of points as a function of the distance from any point, and was used in adaptive analysis of nearest neighbors in previous work [7, 38].

Definition 1. Let S be a set of points in a metric space ( X, d ). Let B S ( p, r )= { q  X  S : d ( p, q )  X  r } . Then, the expansion constant c of S is defined as the smallest c  X  2 such that for all p  X  X and all r&gt; 0 While the expansion constant depends only on the pair-wise distances between points, the MST has a  X  X igher-order X  structure. In other words, the MST depends on distances between clusters of points in addition to distances between the individual points. Since the expansion constant does not capture this structure, we define two new parameters: the cluster expansion constant and linkage expansion constant .
Boruvka Clustering. We first require a definition of clusters. Independently of how they are computed, succes-sive Boruvka steps define a hierarchical clustering of the data. We can therefore define and use the Boruvka clus-tering without reference to any method for computing it.
Definition 2. Given a point set S ,the Boruvka clustering at level i , D i , is the clustering obtained from applying a single Boruvka step to the clustering D i  X  1 . D 1 consists of each point as its own cluster.

Cluster Expansion Constant. Given the Boruvka clus-tering, we define the new expansion constants. Let B c i ( q, r ) be the set of all components C p with a point p  X  C p such that d ( q, p )  X  r .Usingthis component-wise ball , we define the cluster expansion constant .

Definition 3. The cluster expansion constant is the small-est real number c p such that for all points q  X  S ,distances r&gt; 0, and each level of the Boruvka clustering D i .

Linkage Expansion Constant. Let C 1 and C 2 be two clusters in the Boruvka clustering at level i and let S 1 and S 2  X  C 2 .Let B l i ( S 1 ,S 2 ,r )bethesetofallpairs( p, q ) such that p  X  S 1 , q  X  S 2 ,and d ( p, q )  X  r .
Definition 4. The linkage expansion constant is the small-est real number c l such that for all levels of the Boruvka clustering D i ,clusters C 1 C 2 at level i , subsets S 1  X  C 1 ,S 2  X  C 2 , and distances r&gt; 0.
Intuition for New Parameters. The time to com-pute the EMST for any algorithm is ultimately governed by the number of distance computations needed to distin-guish edges that belong in the MST from those that do not. If the incorrect edges can be excluded from consideration with few computations, then it may be possible to compute the MST efficiently. On the other hand, if there are very many possible nearest neighbors of a given component, it may be impossible to avoid computing the distances to all such neighbors to find the nearest.

One possible case where the correct MST edge may be difficult to identify is given in Figure 1(a). Since Q is nearly (a) Large expansion
Figure 1: Cases illustrating expansion constants. equidistant from the points in R , there are many edges with nearly the same length as the edge in the MST. In this case, the expansion constant must be at least as large as the num-ber of points in R .
 Figure 1(b) shows an analogous possibility. Here, the MST must have an edge from the component Q to one of the components in the ring R . As before, the possible edges have nearly the same length, and it may be difficult to find the shortest edge. However, the expansion constant may still be small relative to the number of points. If Q contains |
R | /c points, then it is possible for the expansion constant to be bounded by c ,evenif R has O ( N ) points. The cluster expansion constant is large in this case, so it captures this possible source of difficulty in computing the MST.
Figure 1(c) shows another case where finding the correct edges of the MST is difficult. There are many possible nearest neighbor pairs, and therefore many possible near-est neighbor edges, between Q and R . As in Figure 1(b), the expansion constant may still be small. Since this set has only two components, the cluster expansion constant is small as well. Here, the linkage expansion constant is large, capturing the difficulty of identifying the MST edge.
As we contended above, the EMST depends on distances between groups of points as well as the pairwise distances. We also argued that the expansion constant alone is insuffi-cient to quantify the amount of computation needed to iden-tify edges in the MST, thus motivating the need for further parameters to understand the behavior of any MST algo-rithm. We now turn to the definition and adaptive analysis of our new algorithm, DualTreeBoruvka ,andshowthat the combination of all three types of expansion constant does contain enough information about the data to obtain a more precise analysis of the EMST problem.
The running time of Bor  X  uvka X  X  algorithm depends on an efficient method to find the nearest neighbor pair of each component. Here, we describe a method to compute all nearest neighbor pairs simultaneously by amortizing some computations across different points. This allows us to im-plement Boruvka X  X  algorithm more efficiently than previous methods.

Dual-Tree Algorithms. Dual-tree algorithms are a com-putational framework that has been applied to many prob-lems in computational statistics, physics, and machine learn-ing. These algorithms are the overall fastest known methods for many problems, including all nearest neighbors [22], ker-nel density estimation [23], mean shift [47], kernel discrimi-nant analysis [39], and n -point correlation functions [22, 30].
For concreteness, consider the all nearest neighbors prob-lem: we are given a query set Q and a reference set R of points in Euclidean space, each of O ( N ) size. The goal is to find, for each point q  X  Q ,thepoint r  X  R such that d ( q, r ) is minimized. A brute-force solution to this problem con-sists of two nested  X  X or X  loops which compute all pairwise distances and requires O ( N 2 )time. Wecanimproveon this algorithm with a space partitioning tree (e.g. a kd -tree) built on the set of references. For each query, we descend the tree, expanding reference nodes closer to the query first. We store the smallest distance d ( q, r ) found so far at each stage of the algorithm, which provides an upper bound on the true nearest neighbor distance. Given this upper bound, we can prune nodes in the tree that are far enough away to not contain the true nearest neighbor, thus reducing the total number of distance computations. This single-tree method, described in [18], requires roughly O ( N log N )time.
A dual-tree method further improve sthisbyconstructing another tree on Q . We consider both a query and reference node, and expand both. When the upper bounds allow, we can then prune a distant reference node for an entire node of queries with a single distance computation. This delivers tremendous speedups in practice and has been shown to re-quire O ( N ) time after tree construction with a cover tree [7]. We extend this idea to efficiently find the nearest neighbor pair for each component in a spanning forest.

New Algorithm. Our new algorithm, DualTreeBoru-vka , uses a dual-tree method to find the nearest neighbor pair for each component. Algorithm 1 gives the description of the outer loop. The subroutine UpdateTree handles the propagation of any bo unds up and down the tree and resets the upper bounds d ( C q ) to infinity. We also make use of a disjoint set data structure [45] to store the connected com-ponents at each stage of the algorithm. Our algorithm is independent of the particular space partitioning tree used. In this paper, we present experimental results on two instan-tiations of the algorithm. Algorithm 2 uses a kd -tree, and algorithm 3 uses the cover tree[7].

For the remainder of this work, we assume that we are given a set S of N points in R d .Furthermore,wemake the standard assumption that all pairwise distances between points are unique. We make use of the following notation: Algorithm 1 Dual-Tree Bor  X  uvka (Tree root q ) 3: FindComponentNeighbors ( q, q, e ) 6: end while
DualTreeBoruvka on a kd -tree. The kd -tree [36] is a binary space-partitioning tree which maintains a bounding box for all the points in each node. The root consists of the entire set. Children are fo rmed recursively by splitting the parent X  X  bounding box along the midpoint of its largest dimension and partitioning the points on either side. In the kd -tree version of DualTreeBoruvka ,eachnode Q maintains an upper bound d ( Q )=max q  X  Q d ( C q )and records whether all the points belong to the same component of the spanning forest. A node where all points belong to the same component is referred to as fully connected .Withthese records, we can prune when the distance between the query and reference is larger than d ( Q )orwhenallthepointsin Q and R belong to the same component.

Theorem 4.1. The FindComponentNeighbors routine in Algorithm 2 returns the correct nearest neighbor pairs.
Proof. The algorithm can only prune in two ways. If Q and R are fully connected, then no edges ( q, r )with q  X  and r  X  R can be nearest neighbor pairs. The distance-based prune only occurs when for all q  X  Q , d ( C q ) &lt;d ( Q, R ). Therefore, all components with points in Q must have a candidate neighbor closer than any point in R , which again implies that no edge ( q, r ) can be a nearest neighbor pair. So, for each q  X  Q , the correct Boruvka neighbor r of the component C q cannot be pruned and must be found in the base case.

DualTreeBoruvka on a Cover Tree. A cover tree is a data structure introduced by Beygelzimer et al. [7] for practically and theoretically efficient nearest-neighbor com-putations. In previous work, a proof of linear running time for the dual-tree all nearest neighbor algorithm used cover trees [38]. Here, we give a brief overview of the properties of a cover tree used in this paper.

A cover tree consists of a set of nested sets C i ,eachata scale i . A node in the cover tree consists of a single point and links to the node X  X  children. The root is a single point at level  X  . As we descend the tree, the scale decreases, until C  X  X  X  contains the entire set of points. For convenience, we index nodes in the cover tree with the node X  X  point and use p to denote the node indexed by p at level i of the tree. Algorithm 2 FindComponentNeighbors ( kd -tree node Q , kd -tree node R ,Edgeset e ) 3: else if d ( Q, R ) &gt;d ( Q ) then 6: for all q  X  Q, r  X  R, r  X  q do 9: end if 12: else 15: FindComponentNeighbors ( Q.left,R.right,e ) 18: end if As in [7], we consider two representations of a cover tree. Conceptually, an algorithm descends from the root C  X  to the set of all points at level C  X  X  X  , touching every level in between. Each point in level C i has itself as a child in level C  X  1 along with any other children. We refer to this idea ofthetreeasthe implicit representation and make use of it in the algorithm description and proof. The explicit repre-sentation allows us to use the cover tree in practice. In the implicit representation, there are many levels where a node has only itself as a child. To create the explicit represen-tation, we combine all such nodes. Therefore, a node is a single point, and contains pointers to all its children. The explicit representation has O ( N ) nodes [7].

A cover tree has the following invariant properties: 1. Nesting: C i  X  C i  X  1 2. Covering: For every p  X  C i  X  1 ,thereexistsa q  X  C i 3. Separation: For all p, q  X  C i , d ( p, q ) &gt; 2 i .
The cover tree version of FindComponentNeighbors (Algorithm 3) follows the all nearest neighbor pseudocode given in [38]. The reference set R i contains all points at level i that may have a nearest neighbor of a descendant of q as one of their descendants. Therefore, points are pruned from R i  X  1 in line 12 only when they are too distant to pro-vide a neighbor. All descendants of q j are within 2 j +1 and all descendants of points in R are within 2 i of a point in R by the covering invariant. Therefore, any point out-side the bound in line 12 cannot be a nearest neighbor for descendants of q j .

Theorem 4.2. The FindComponentNeighbors routine in Algorithm 3 returns the correct nearest neighbor pair.
Proof. For a query q j being considered at level j ,the algorithm must guarantee that it finds the nearest neighbor pair both for the component C q and for all components C q where q is a descendant of q j . Pruning a fully-connected node can never delete the true nearest neighbor pair. Algorithm 3 FindComponentNeighbors (Cover tree node q j , Reference Set R i ,Edgeset e ) 3: for all q that are descendants of q j and r  X  R i with 6: end if 9: // reference descend 12: R i  X  1 = { r  X  R : d ( q j ,r )  X  d +2 i +2 j +2 } 15: else 18: FindComponentNeighbors ( p j  X  1 ,R i ,e )
We then consider distance-based pruning. As before, we use the nearest neighbor of the component C q that the al-gorithm has seen up to this point in the execution. This candidate neighbor can be either a previously found nearest neighbor of another point in C q (in which case d = d ( C apoint r  X  R ( d = d ( q j ,r )), or an inferred descendant of a connected point r ( d = d ( q j ,r )+2 i ). If q j  X  r but q then r must have a descendant r that is not connected to q By the covering invariant, d ( q j ,r )  X  d ( q j ,r )+2 i fore, d is a valid upper bound for C q . Since the distance between any point in R and any descendant is bounded by 2 , any ancestor of the true nearest neighbor of q j must be within d +2 i , so the algorithm can never prune the ancestor of this neighbor.

We must also show that d is a valid bound for any descen-dant q of q .If q and q are in the same component, then this is clearly true, since bounds are shared across components. Otherwise, q is a candidate neighbor for q and d ( q, q ) 2 +1 . Therefore, we can be sure that d ( C r be the correct neighbor for q ,andlet r be the ancestor of r in R . Then, d ( q j ,r )  X  d ( q j ,q )+ d ( q ,r )+ d ( r ,r ) 2 +1 +2 j +1 +2 i =2 j +2 +2 i . Therefore, the distance prune cannot remove the neighbor of any descendant of q . In this section, we prove our main theoretical result:
Theorem 5.1. For a set S of N points in a metric space with expansion constant c , cluster expansion constant c p and linkage expansion constant c l ,the DualTreeBoruvka algorithm using a cover tree requires time (where  X  ( N ) is defined below).

Proof. Since each Bor  X  uvka step reduces the number of components in the spanning forest by a factor of at least two, the entire algorithm requires at most log N iterations. The construction of the cover tree takes O ( N log N )time(proved in [7]) and only needs to be done once as a preprocessing step. Bookkeeping and cleanup in the tree in between calls to
FindComponentNeighbors requires a single depth-first traversal, which takes O ( N )time.

Adding edges requires at most O ( N ) Union operations on the disjoint-set structure, each of which requires O (  X  ( N )) time, with  X  ( N ) defined as follows. Let A k ( j )= A ( and let A 0 ( j )= j + 1. Then, define Therefore, in order to complete the proof, we only need to show that the FindComponentNeighbors subroutine on a cover tree requires O ( N X  ( N )) time.

We first require two lemmas about cover trees, proven in [7]. Both lemmas assume the cover tree is built on a set of N points in a metric space with expansion constant c .
Lemma 5.2. (Width Bound) The number of children of any node in the cover tree is bounded by c 4 .

Lemma 5.3. (Depth Bound) The maximum depth in the tree of any point in the explicit representation is O ( c
We now apply our adaptive analysis to bounding the run-time of FindComponentNeighbors .
 Theorem 5.4. Under the assumptions of Thm. 5.1, the FindComponentNeighbors algorithm on a cover tree (Al-gorithm 3) finds the nearest neighbor of each component in time bounded by:
Proof. We show that the amount of work done in each line of the algorithm during the entire execution is at most O (max i | R i | N X  ( N )). We complete the proof by showing max i | R i | depends only on c , c p ,and c l .

Base Case. The base case (lines 1 through 7) is executed at most once for each explicit query node. Each base case requires max i | R i | Find operations, each of which requires
Query Descends. Each query node in the explicit repre-sentation is expanded at most once (line 18), so this step requires O ( N ) time overall.

Reference Descends. On the other hand, a reference node may be expanded more than once. When a query node is expanded, its reference cover set R i needs to be duplicated for each child of the query. By the width bound, this cre-ates at most c 4 duplications. Therefore, the total number of reference nodes considered in Line 12 is O ( c 4 N ).
At each level, | R | X  c 4 max i | R i | . Since the maximum depth of a node is O ( c 2 log N ) (depth bound), the num-ber of nodes considered in Line 14 is O ( c 6 max i | R i Considering possible duplication across queries, the total number of calls to Line 14 is at most O ( c 10 max i | R i Computing d in each reference descend involves checking the connectedness of q j and r ,whichrequires O (  X  ( N )) time, for a total running time of O ( c 10 max i | R i | log N X  ( N )).
Bounding | R i | . For a given query q j and reference cover set R i , we compute the upper bound distance d . Then, R  X  1 = { r  X  R : d ( q j ,r )  X  d +2 i +2 j +2 } .Since j&lt;i in this part of the algorithm, and since the query and reference trees are identical, j = i  X  1. Therefore, B ( q j ,d +2 i B ( q j ,d +2 i +1 +2 i ).

Consider two cases: first let d  X  2 i +2 . Then, as in [7], we bound number of balls of radius 2 i  X  2 that can be packed into B ( q j ,d +2 i +1 )by: Each ball of radius 2 i  X  2 can contain at most one point in C  X  1 by the separation invariant. Therefore, the number of points in B ( q j ,d +2 i +1 +2 i )  X  C i  X  1  X  R i  X  1 is at most c
Consider the other case where d&gt; 2 i +2 . Without loss of generality, assume that we have computed k previous it-erations. First note that all points within B ( q j ,d  X  must be connected to q j . Otherwise, let q be a point in B ( q j ,d  X  2 i +1 ) that is not connected to q j . Then, q has a grandparent q at level C i  X  1 such that d ( q ,q )  X  Therefore, d ( q j ,q )  X  d ( q j ,q )+ d ( q ,q ) &lt;d  X  2 i +1 +2 Therefore, d ( q j ,q )+2 i &lt;d and q j q , which contradicts the definition of d in line 11.

The number of components that q j may have to search is bounded by As noted above, all points within d  X  2 i +1 of q j are connected to q j , so the only component in B c k ( q j ,d  X  2 i +1
We now bound the number of points within a component that q j may have to consider. Let C r be a component dis-tinct from C q .Let L ( q j ) denote the set of all leaves that are By the above argument, there can be at most one pair in B ( C q  X  L ( q j ) ,C r ,d ). Therefore, there are at most c in C r contained in B ( q j ,d +2 i +1 +2 i ). In the worst case, each of these points is at level C i  X  1 ofthetreeandmust be considered in R i  X  1 . There are at most c 2 p components C r that can contribute points, so the maximum number of points in R i  X  1 is c 2 p c 2 l .

Combining these cases, we have max i | R i | X  max { c 6 ,c Therefore, the running time is: which completes the proof.
 Theorem 5.4 shows that each call to FindComponent-Neighbors requires at most O ( N X  ( N ) time. By combin-ing this with the observation above that DualTreeBoru-vka requires at most log N calls to FindComponentNeigh-bors , we arrive at the runtime stated in Thm. 5.1, namely
Algorithms Implemented. We present results for kd -tree-based and cover-tree-based DualTreeBoruvka .For comparsion, we implemented the other fast EMST methods mentioned in section 2. Specifically, we compare against the single-fragment EMST algorithm from Bentley and Fried-man [5], which is an implementation of Prim X  X  algorithm. The algorithm uses a single-tree algorithm on a kd -tree to find the next edge to add at each step. We also show results for the WSPD-based algorithm GeoMST2 [31], described above. Finally, we compare against a na  X   X ve implementation of Boruvka X  X  algorithm in which nearest neighbor pairs are computed by iterating over all pairs of points.

Datasets . The experiments here are on four datasets: one synthetic and three sets of astronomy data. The syn-thetic data are drawn from a mixture of ten evenly weighted Gaussians placed uniformly at random in the unit cube in three dimensions. Figure 2(a) compares timing results on these data. Figure 2(b) shows runtimes on four dimensional samples of spectral data from the Sloan Digital Sky Sur-vey. Table 1 has results for two other astronomy datasets: a 40,000 point, 3,840-dimensional set of color spectra from the SDSS, and a million point, 3 dimensional set of ( x, y, z ) coordinates from a galaxy-formation simulation.

Implementation Details. We implemented all algo-rithms in the FASTlib C++ library [21]. The code was compiled with gcc version 4.1.2 with the -O2 flag. All ex-periments were performed on a 3.0 GHz Intel Xeon processor with 8GB of RAM running Linux.

Results. We attempted to run all the algorithms on all the sets of data. However, the na  X   X ve experiments were lim-ited by time, since the brute-force algorithm scales quadrat-ically. Thus results are missing for the larger sets. The Ge-oMST2 algorithm is limited by available memory. Although the WSPD contains O ( N ) pairs of nodes, the constant factor can be very large. The constant in the O ( N ) analysis scales exponentially with the dimension [10], so the storage bottle-neck becomes tighter with higher-dimensional data. Missing timings for GeoMST2 indicate that the available memory was exceeded. In our experiments, the Bentley-Friedman algorithm is more efficient than either of these.
In both the synthetic data (Figure 2(a)) and the SDSS data (Figure 2(b)), DualTreeBoruvka on a kd -tree is the fastest method, by a factor of 2.8 and 4.6 over the Bentley-Freidman method, respectively. On both figures, we plot the slope of the predicted N log N performance, scaled to align with the timings for our method. Figure 2: EMST computation times on log-log scale. All timings are in seconds.
 Table 1: Comparison of DualTreeBoruvka and Bentley-Friedman timings. Timings are in seconds.
Our results also consider dimensionality of the data. In the three-and four-dimensional data given in figure 2, the kd -tree based DualTreeBoruvka is fastest. Unlike most EMST algorithms, our method can also efficiently handle high-dimensional data, as shown in table 1. For the high-dimensional SDSS data, the two methods using kd -trees re-quire roughly the same time. DualTreeBoruvka on a cover tree, however, is faster by a factor of 2 . 9. We presented a new algorithm for the EMST problem, DualTreeBoruvka . Wealsopresentthefirstadaptive analysis of this long-standing problem. Combining these, we obtain the tightest runtime bound to-date for computing the EMST -O ( N log N X  ( N ))  X  O ( N log N )-whichissep-arated from the best lower bound only by the overwhelm-ingly slowly-growing function  X  ( N )  X  O (1). We leave for future work whether our bound is optimal or if a rigorous O ( N log N ) algorithm can be shown. Our analysis is also the first to avoid explicit (and exponential) dependence on the dimension of the input. We demonstrate the practical utility of our method for astronomical problems with experiments on data from the Sloan Digital Sky Survey and simulations of galaxy formulation. Comparison against algorithms in the literature shows our method to be the considerably faster on these sets. These experiments also support our theoretical analysis and demonstrate the applicability of our algorithm to both low-and high-dimensional data. The authors would like to thank Dongryeol Lee and Ryan Riegel for helpful comments on the proof. [1] P. K. Agarwal et al. Euclidean minimum spanning [2] G.J.BabuandE.D.Feigelson. Astrostatistics .
 [3] M. Balcan, A. Blum, and S. Vempala. A [4] J. D. Barrow, S. P. Bhavsar, and D. H. Sonoda. [5] J. Bentley and J. Friedman. Fast Algorithms for [6] J. Bentley and A. Yao. An almost optimal algorithm [7] A. Beygelzimer, S. Kakade, and J. Langford. Cover [8] S. P. Bhavsar and R. J. Splinter. The superiority of [9] P. Callahan and S. Kosaraju. Faster algorithms for [10] P. B. Callahan and S. R. Kosaraju. A Decomposition [11] B. Chazelle. A faster deterministic algorithm for [12] B. Chazelle. A minimum spanning tree algorithm with [13] M. Colless et al. The 2dF Galaxy Redshift Survey: [14] E. Demaine, A. L  X  opez-Ortiz, and J. Munro. Adaptive [15] M. B. Eisen et al. Cluster analysis and display of [16] V. Estivill-Castro and D. Wood. A survey of adaptive [17] M. Fredman and R. Tarjan. Fibonacci heaps and their [18] J. H. Friedman, J. L. Bentley, and R. A. Finkel. An [19] H. Gabow et al. Efficient algorithms for finding [20] J. C. Gower and G. J. S. Ross. Minimum spanning [21] A. Gray et al. Mlpack, 2008. [22] A. Gray and A. W. Moore. N-body problems in [23] A. G. Gray and A. W. Moore. Rapid Evaluation of [24] M. Held and R. M. Karp. The traveling-salesman [25] D. H. Jones et al. The 6dF Galaxy Survey: samples, [26] D. R. Karger and M. Ruhl. Finding Nearest Neighbors [27] D. G. Kirkpatrick and R. Seidel. The ultimate planar [28] J. B. Kruskal. On the shortest spanning subtree of a [29] C. Lacey and S. Cole. Merger rates in hierarchical [30] A. Moore et al. Fast Algorithms and Efficient [31] G. Narasimhan, M. Zachariasen, and J. Zhu.
 [32] J. Nesetril. Otakar Boruvka on minimum spanning [33] O. Nevalainen, J. Ernvall, and J. Katajainen. Finding [34] S. Pettie. Finding Minimum Spanning Trees in [35] S. Pettie and V. Ramachandran. An optimal minimum [36] F. P. Preparata and M. I. Shamos. Computational [37] R. C. Prim. Shortest connection networks and some [38] P. Ram et al. Linear time algorithms for pairwise [39] R. Riegel, A. Gray, and G. Richards. Massive-Scale [40] S. Schmeja and R. S. Klessen. Evolving structures of [41] M. Shamos and D. Hoey. Closest-point problems. In [42] S. Shectman et al. The Las Campanas Redshift [43] V. Springel et al. Simulations of the formation, [44] S. Subramaniam and S. B. Pope. A mixing model for [45] R. Tarjan. Data Structures and Network Algorithms . [46] P. J. Wan et al. Minimum-energy broadcast routing in [47] P. Wang et al. Fast Mean Shift with Accurate and [48] P. Willett. Recent trends in hierarchic document [49] W.-K. Wong and A. Moore. Efficient algorithms for [50] A. Yao. An O ( | E | log log | V | ) algorithm for finding [51] A. Yao. On constructing minimum spanning trees in [52] D. York et al. The Sloan Digital Sky Survey: Technical [53] C. Zahn. Graph-theoretical methods for detecting and
