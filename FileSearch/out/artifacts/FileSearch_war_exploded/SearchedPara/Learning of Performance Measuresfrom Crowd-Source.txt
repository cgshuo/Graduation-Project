 The goal of data mining is to automatically identify  X  X nteresting X  patterns in a dataset. Data mining algorithms therefore utilize an interestingness measure , a function that assigns a numerical score to a given pattern, to evaluate and rank patterns. Several interestingness measures have been proposed, surveyed, estingness measure depends on the specific domain since a pattern can exhibit multiple desirable attributes which must be traded-off against each other. Designing an interestingness measure for a specific domain is challenging and typically requires a domain expert to create a new function and identify a set of features that can be calculated from the dataset attributes [ 22 ]. As an alternate approach, we propose a method to learn an interestingness measure from crowd-sourced data collected from end-users in the domain community. In our approach, domain users are presented with pairs of candidate patterns and are asked to rank one over the other. Pairwise ranking is a non-arduous way for domain users to share preference information. It also facilitates the combining of preference information from multiple users. The collected pairwise rankings are then provided as input to a learning-to-rank algorithm to learn a model of user preference which can be used as an interestingness measure. The features in the learning model are previously proposed interestingness measures for the domain. The result is a custom measure that represents  X  X eal human interest X  [ 22 ]inthe domain as expressed by its users.
 domain of finance, specifically the task of learning an investment performance measure that reflects the preferences of investment professionals. Investment preference rankings are collected from users of online discussion forums com-prised of quantitative analysts and traders. The model features that are used in the learning-to-rank algorithm include currently used investment performance metrics and ratios. The learned model achieves an accuracy of 80% for pre-dicting the domain users X  preference, while the highest accuracy of any single existing performance measure is 77%.
 domain since there is a large number of investment choices. For instance, the United States has over 5,000 exchange-traded stocks and over 7,000 mutual fund choices. Our proposed approach can enable individuals to locate investments that match their specific interests. Moreover, the learned interestingness measure can also be used as an objective function for portfolio selection and optimization. 1. We propose a novel approach based on learning-to-rank algorithms that 2. We evaluate this approach in the domain of investment ranking and show 3. We provide all data collected as part of this study to encourage further Ohsaki et al. [ 22 ] experimentally compared interestingness measures against real human interest in medical data mining. They generated prognosis-prediction rules from a clinical dataset on hepatitis. They then had a medical expert eval-uate rules as Especially-Interesting , Interesting , Not-Understandable ,and Not-Interesting . Carvalho et al. [ 5 ] build on [ 22 ] with evaluations on eight datasets. They presented nine rules to each expert for each interestingness measure: the best three, the worst three, and three in the middle. Experts were asked to assign a subjective degree of interestingness to each rule. Tan et al. [ 26 ] studied ways to select the best interestingness measure for association rules  X  instead of using actual experts to rank contingency tables, they consider a held-out measure as the expert (and repeat over all measures). None of these works attempt to learn an interestingness measure from domain experts as we propose in this work. To the best of our knowledge, no work has been published on compar-ing investment performance measure rankings against real human interest. For related work in finance, we summarize publications that describe the relative per-formance of different evaluation measures in this domain. Justification for these proposed measures is axiomatic, based on the properties of the measures [ 1 , 17 ]. Farinelli et al. [ 11 ] compare eleven performance ratios. Their work includes a lim-ited empirical simulation, evaluating how well each ratio performed forecasting five stock indexes. They find that asymmetrical performance ratios work bet-ter and recommend that more than a single performance ratio be used. Cogneau and H  X  ubner [ 7 ] survey over 100 investment performance measures. They provide a taxonomy and classification of measures based on their objectives, proper-ties, and degree of generalization. Bacon [ 2 ] also provides a thorough survey of measures grouped into categories.
 Some of the current research indicates that different performance metrics produce substantially the same rank orders. Hahn et al. [ 14 ]used10perfor-mance measures to rank data from two proprietary trading books and found high values of Spearman X  X  rank correlation. Eling and Schuhmacher [ 10 ] find high rank correlation (0.96) between 13 performance measures that were used to rank the returns of 2,763 hedge funds. Eling [ 9 ] confirmed the high rank correla-tion between measures when applied to 38,954 mutual funds from 7 asset classes. On the other hand, Zakamouline [ 28 ] describe several less correlated measures and suggest the use of Kendall X  X  tau instead of Spearman X  X  rho for measuring rank correlation. None of these four studies considered the Pain, Ulcer, and Martin-related measures discussed in Section 3.3 . Investment performance measures are designed to weigh the risk as well as the reward, and are therefore called  X  X isk-adjusted returns. X  Metrics are structured as ratios, with return on investment in the numerator, and risk in the denom-inator. In this way, a single metric can compare two investment options with different risk profiles.
 While return on investment is a standard measure of reward, there are mul-tiple measures of risk and hence consensus has not yet been reached as to which performance measure is best [ 11 ]. New performance metrics continue to be pro-posed [ 7 , 21 ], and investors have to choose from among them [ 2 ]. We first describe equity graphs which provide a visualization of asset per-formance, followed by a summary of performance measures that will be used as features in our learning model.
 3.1 Equity Graphs Historical performance is often presented as an equity graph , which shows the value of one X  X  investment account over time. Equity graphs enable domain experts to rapidly evaluate historical performance. While there are different types of equity graphs, in our work we use the common variant where the graph presents a cumu-lative sum of daily returns. This is equivalent to assuming exactly one dollar was invested each day, with profits removed from the account. Such a graph is easy to examine, since the ideal is a straight line from the lower left corner to the upper right corner. Examples are shown in Figures 1 , 2 ,and 3 . 3.2 Distribution-Based Measures Many performance measures calculate risk based on the distribution of returns . For a time series R , the return on investment for each period, R where S t is the asset value at time t .
 ratio, the Sharpe ratio [ 23 ]. The Sharpe ratio is widely used [ 9 ], with surveys showing its use by up to 93% of money managers [ 2 ]. This performance measure is  X  X ptimal X  if the return distribution is normal. The Sharpe ratio is closely related to the t -statistic for measuring the statistical significance of the mean differential return [ 24 ].
 ment in period t , R Bt the return of the benchmark security (commonly the risk-free interest rate) in period t ,and D t the differential return in period t : Let  X  D be the average value of D t from period t = 1 through T : and  X  D be the standard deviation over the period: The Sharpe Ratio ( S h )is: Given that asset returns are often non-normal, researchers have developed mea-sures that incorporate higher moments of the distribution [ 17 ]. The Sortino ratio [ 25 ] is similar to the Sharpe ratio, except it uses the semi-standard devia-tion (downside risk) in the denominator. Other measures consider only the very worst returns in the tail of the return distribution [ 1 , 8 ]. 3.3 Multi-Period-Based Measures Shape-based measures focus on multi-period drawdowns instead of return dis-tributions. The Maximum Drawdown is defined as the maximum peak-to-valley decline in the equity graph. Figure 1 shows how two orderings of returns can have very different maximum drawdowns while still having the same daily Sharpe ratio. The chart on the right has an unappealing drawdown of 22%, yet it has the exact same distribution of returns as the chart on the left (with a drawdown of only 6%).
 Drawdown can also be defined as a string of consecutive negative returns. Many performance measures consider aspects of the distribution of such draw-downs instead of returns, including the mean, standard deviation, and selected number of worst drawdowns.
 The Martin ratio , or  X  X lcer performance index X  has the same numerator as the Sharpe ratio, but has the Ulcer index as the denominator. Using the notation in Bacon [ 2 ], let D i be the drawdown since the previous peak in period i .The Ulcer index is then defined as: Figure 2 shows an equity graph with each D i shown in black. The Ulcer index penalizes long drawdowns.
 inator is the Pain index , a modified form of the Ulcer index: The Pain index also penalizes long drawdowns but does not penalize deep draw-downs as severely as the Ulcer index.
 the longest horizontal line that can be drawn between two points on the graph, as shown in Figure 3 . We introduce it here because it is not found in the literature, and we find it ranks highly in our experiments. We now describe our approach to learn an investment performance measure with higher rank prediction accuracy than the current performance measures, using crowd-sourced domain user input. The steps of our approach are as follows: 1. Generate equity graphs simulating reasonable investment performance. 2. Collect preference data for the generated equity graphs from domain users 3. Use learning-to-rank algorithms with individual performance measures as 4.1 Generating Equity Graphs Our approach uses equity graphs as a means for enabling domain experts to rapidly compare two strategies or investments. We generated (synthetic) equity graphs that follow a log-normal random walk. In this model, the asset price, S follows the stochastic differential equation: where  X  is the constant drift,  X  is the constant volatility, and dW process.
 We generated discrete differential simple returns representing five years with 252 business days per year. The returns are normally distributed with a mean of 0 . 125 and a standard deviation of 1. These values were chosen to lead to a broad distribution of Sharpe ratios centered around 2. Of these, only graphs with Sharpe ratios between 1 . 5 and 2 . 5 are retained. This range corresponds to the range of Sharpe ratios typically encountered. Ratios below 1 . 5 are unattractive as an investment, and ratios greater than 2 . 5 are very rare in practice. In total, we generated 2,000 charts.
 For each graph, we normalize the set of returns to sum to 1. Normalizing the cumulative return enables domain experts to directly compare risk metrics (such as the maximum drawdown) on the same scale. 4.2 Collection of Ranking Data One of our innovations is the collection of domain expert preferences in the form of pairwise rankings. We believe that it is easier for a participant to choose between two equity graphs than to decide on a numeric score for every individual graph. In particular, numeric scores require that these be normalized before aggregating scores to account for the different preference scales of participants. This normalization would be difficult for cases where a participant only labeled a small number of charts. In contrast, our pairwise ranking-based method is fast for human users with median ranking time between 3 and 4 seconds.
 We created a web page that described our research goal and presented two randomly chosen equity graphs side-by-side. A participant is asked which of these two investments is more attractive to invest in for the future. We requested participation from domain experts in two online forums. The first forum targets quantitative analysts and risk managers. The second forum targets individual traders, although some members run small hedge funds or are commodity trading advisors. 66 different anonymous people from these forums ranked a total of 1,004 chart pairs. We believe that the participation of many professionals is validation of community interest in improving investment performance measurement. One author also ranked 1,659 equity graph pairs, including a re-ranking of every pair ranked by the community. In order to estimate self-consistency of rankings, the author later re-ranked each of the same 1,659 graph pairs. The estimate of self-consistency is 90%. In all rankings and re-rankings, the equity graph positions (i.e., left or right side) were chosen randomly. 4.3 Data Quality Ensuring quality of crowd-sourced data is a recognized problem [ 18 ]. As expected, we found that some of the crowd-sourced data was of low quality. In this section, we describe the steps performed to derive a higher quality data subset from the crowd-sourced annotations.
 ranking as either  X  X lose call X  (81%) or  X  X lear choice X  (19%). A  X  X lear choice X  tag indicates that the author X  X  preference was strong and this view was likely to reflect universal preferences. The author was 100% self-consistent when re-ranking  X  X lear choice X  equity graph pairs.
 ing to the following characteristics:  X  Small median time between clicks  X  A high fraction of times the participant clicked the same button (i.e., left or  X  A systematic preference for the chart with the lower Sharpe ratio  X  A relatively high fraction of rankings that contradict the author X  X   X  X lear a data quality filter is subjective, we also ran all experiments on the unfiltered dataset in addition to making the data publicly available. 4.4 Learning-to-Rank A learning-to-rank algorithm predicts the order of two objects given training data consisting of partial orders of objects (and their features). We use the learning-to-rank algorithm proposed by Herbrich et al. [ 15 ]. In this method, the ranking task is transformed into a supervised binary classification task by considering the difference between corresponding features. This transformation also enables the use of other learning algorithms in addition to support vector machines as originally proposed by Herbrich et al. [ 15 ].
 1. Logistic regression, with L 1 -norm regularization [ 12 ] 2. Random forests [ 4 ] 3. SVM with linear and RBF kernels [ 6 ] based on their respective features. It is redundant to include both A&gt;B and B&gt;A (with negated feature differences) when training a model. In order to ensure balanced numbers of classes for the model to learn, we chose one of either A&gt;B or B&gt;A for each instance such that there were equal numbers of positive and negative instances in the training data. Balancing the training data also ensures that the intercept or bias term will be zero for logistic regression. Features. The features we use as inputs to the machine learning models include relevant risk and performance metrics found in Bacon X  X  comprehensive survey [ 2 ] which also provides descriptions of each measure using uniform notation. Note that for our normalized charts, risk metrics produce identical rank orderings as their respective performance measures. We nevertheless include both, because models such as logistic regression use linear combinations of features, and we do not know a priori which feature will combine best with other features. In our experiments, we consider the following three datasets: 1. The full set of all 1,004 community rankings (ACR) 2. The filtered set of 875 community rankings (FCR) 3. The set of 1,659 author rankings (AR) Each experiment followed these steps for evaluation: 1. Randomly shuffle the data 2. Separate 25% of the data for testing 3. Choose optimal hyper-parameters using 5-fold cross-validation on the train-ing data 4. Test the accuracy of the final model on the held-out test data We performed each experiment 8 times and averaged the test accuracies. All models were trained and tested on the same random shuffle of the data to better compare their accuracies.
 In order to estimate the impact of the number of pairwise rankings needed for training on the accuracy of the learned performance measure, we tested pro-gressively increasing amounts of training data. The data was not reshuffled as training instances were added, i.e., for n = 200, the first 100 data points are the same ones used for n = 100. Figure 4 shows accuracies obtained for each of the three datasets, using each of the models, trained with an increasing number of pairwise ranking samples. Each point on the graphs represents the average of 8 runs. For reference, we show the most commonly used performance measure as a baseline, the monthly Sharpe ratio. In addition, we also show the performance of the ex post facto best measure for each dataset, although in practice which measure would perform the best on a given dataset would not be known. mance measures in this domain is able to fully predict domain expert pref-erences. Our performance measure trained from domain expert preferences is able to achieve better prediction accuracy. For the filtered community ranking dataset, the random forests approach narrowly outperformed logistic regression, with 80% accuracy. The best baseline for this dataset is the monthly Pain index, with 77% accuracy. For the dataset containing all community rankings, logistic regression has the best performance, with 74% accuracy. The best baseline for this dataset is the daily Pain index, with 74% accuracy. For the dataset contain-ing author rankings, logistic regression again has the best performance, with 86% accuracy. Note that for this dataset, the same author performed each pairwise ranking twice. As these two sets of rankings have an agreement rate of 90%, this forms an upper bound for any model X  X  predictive accuracy. The best baseline for this dataset is the daily Martin ratio, with 79% accuracy.
 author dataset. This is because community members have idiosyncratic prefer-ences, contributing inconsistency to the community training and test data. more equity graph pairs would not lead to higher accuracies, given the models and features we have chosen. A small number of rankings (approximately 300) is adequate to learn a trader X  X  preferences. Given median ranking times between 3 and 4 seconds, a trader would likely spend 15 to 20 minutes ranking 300 chart pairs. We presented a novel method using crowd-sourcing to learn a domain-specific performance measure. This method uses pairwise learning-to-rank algorithms with previously proposed performance measures as input features. We demon-strated and evaluated this approach for the case of learning a performance mea-sure to rank investments. Our experimental results showed that machine learning algorithms can find linear combinations of performance measures that improve accuracy in this domain.
 to encourage further study. With the data, we also include a table unable to fit in this paper, showing the accuracy of the individual baseline performance measures on each dataset.
