 In the field of information retrieval, one is often faced with the problem of computing the correlation between two ranked lists. The most commonly used statistic that quantifies this correlation is Kendall X  X   X  . Often times, in the information retrieval community, discrepancies among those items hav-ing high rankings are more important than those among items having low rankings. The Kendall X  X   X  statistic, how-ever, does not make such distinctions and equally penalizes errors both at high and low rankings.

In this paper, we propose a new rank correlation coeffi-cient, AP correlation (  X  ap ), that is based on average pre-cision and has a probabilistic interpretation. We show that the proposed statistic gives more weight to the errors at high rankings and has nice mathematical properties which make it easy to interpret. We further validate the applicability of the statistic using experimental data.
 Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Perfor-mance Evaluation General Terms: Experimentation, Measurement, Algo-rithms Keywords: Evaluation, Kendall X  X  tau, Average Precision, Rank Correlation
Most of the research in the field of information retrieval depends on ranked lists of items. The output of search en-gines are ranked list of documents, the search engines them-selves are also ranked based on their performance accord-ing to different evaluation criteria, the queries submitted to search engines are again ranked based on their difficulty, and so on.
We gratefully acknowledge the support provided by NSF grant IIS-0534482.

Since most of the research in IR is based on ranked lists of items, it is often the case that we need to compare two ranked lists and report the correlation between them. Two of the most commonly used rank correlation statistics are Kendall X  X   X  [7] and Spearman rank correlation coefficient [15].
The Spearman correlation coefficient is equivalent to the traditional linear correlation coefficient computed on ranks of items [15]. The Kendall X  X   X  distance between two ranked lists is proportional to the number of pairwise adjacent swaps needed to convert one ranking into the other.

Kendall X  X   X  has become a standard statistic to compare the correlation between two ranked lists. When various methods are proposed to rank items, Kendall X  X   X  is often used to compare which method is better relative to a  X  X old standard X . The higher the correlation between the output ranking of a method and the  X  X old standard X , the better the method is concluded to be. Pairs of rankings whose Kendall X  X   X  values are at or above 0.9 are often considered  X  X ffectively equivalent X  [13], at least empirically.
For example, Soboroff et al. [12] propose a new method for system evaluation in the absence of relevance judgments and use Kendall X  X   X  to measure the quality of their method. Buckley and Voorhees [2] propose a new evaluation measure, bpref, to evaluate retrieval systems and use Kendall X  X   X  to show that this measure ranks systems similar to average pre-cision. Similarly, Aslam et al. [1] propose a new method for evaluating retrieval systems with fewer relevance judgments. They compare their method with the depth pooling method by comparing the Kendall X  X   X  correlation of the rankings of systems obtained using both methods with the actual rank-ings systems to show that their method is better than the depth pooling method. The Kendall X  X   X  statistic is also used to compare the rankings of queries based on their estimated difficulty with the actual ranking of queries [14]. Melucci et al. [8] provides an analysis of places where Kendall X  X   X  is used in information retrieval.

In most of the places where Kendall X  X   X  is used, authors aim for a Kendall X  X   X  value of 0.9 and conclude that their method produces  X  X ood X  rankings if they obtain a  X  value greater than this threshold [17, 3, 9].

Although Kendall X  X   X  seems to be a reasonable choice for comparing two rankings, there is an important problem with this statistic, at least in the context of IR. Kendall X  X   X  equally penalizes errors that occur at any part of the list. In other words, it does not distinguish between the errors that occur towards the top of the list from the errors towards the bottom of the list [8]. However, in almost all cases in information retrieval we care more about the items that are ranked towards one end of the list (either top or bottom). For example, in TREC, the goal is often to correctly iden-tify the best systems, i.e., those at the top of a list ranked by performance. When the goal is to predict query diffi-culty, it is typically more important to identify the most difficult queries. Similarly, when comparing the outputs of two search engines, the differences towards the top of the two rankings typically matter more than the differences towards the bottom of the rankings.

As an example of the aforementioned problem, consider 8 different retrieval systems. Let X  X  assume that their actual ranking is &lt; 1 2 3 4 5 6 7 8 &gt; . Suppose that there are two alternate methods to evaluate these systems, and one would like to compare the relative quality of these two methods. Let X  X  assume that when the first method is used, the systems are ranked &lt; 4 3 1 2 5 6 7 8 &gt; and when the second method is used, they are ranked &lt; 1 2 3 4 8 7 5 6 &gt; . The former ranking has the first four systems in inverse order compared with the actual ranking, while the latter the last four system in inverse order. The Kendall X  X   X  correlation of each rankings with the actual ranking of the systems is the same (in both cases equal to 0 . 6429). Hence, based on the Kendall X  X   X  values, the two methods are equivalent in terms of how they rank the systems. Note, however, that in many IR contexts it is much more important to get the  X  X op half X  of the list  X  X ight X  than the  X  X ottom half X . Thus, we might well much prefer the latter ranking as compared to the former.
In the real world, we are often times faced with ranked lists in which there are many mistakes in terms of the rank-ings of the top (best) items. Figure 1 and Figure 2 show two such cases for TREC 8. In the figures, the leftmost plots show the mean average precision (MAP) values of sys-tems using depth-1 and depth-6 pooling versus the actual MAP values. It can be seen in Figure 1 that the Kendall X  X   X  correlation between the rankings of systems induced by depth-1 MAP values and the actual rankings of systems is 0 . 733. Hence, based on the Kendall X  X   X  value, there is a pos-itive correlation between the two rankings. However, if we only consider the top (best) 10 systems in TREC 8 (middle plot), it can be seen that these systems are ranked almost in reverse order among themselves. On the other hand, if we only focus on the worst 10 systems (right plot), it can be seen that these systems are ranked almost perfectly with re-spect to each other. Hence, even though the depth-1 pooling method is quite poor at identifying the best systems, since Kendall X  X   X  does not make any distinction between errors towards the top versus the errors towards the bottom, the overall Kendall X  X   X  value is still 0 . 733. The same behavior can be seen in Figure 2.

Many researchers are aware of this flaw in using Kendall X  X   X  [8] and have tried to use some alternatives to Kendall X  X   X  . Voorhees [14] makes use of a new measure for estimating query difficulty by gradually removing the items of inter-est and comparing how the average values of items change with respect to average values of actual items. However, this method cannot be used to measure correlation between ranked lists. To compare rankings of retrieval systems, Wu et al. [16] use an accuracy measure which is the total number of items that are common in the top-n of both lists, divided by n , where n is the number of items one is interested in. This approach has the problem that the best possible accu-racy measure is obtained as long as the top-n of both lists contain the same items, even if the top items are ranked in reverse order in two lists.

Shieh [11] recently devised an extension of Kendall X  X   X  where the errors in different ranks are penalized by different weights. However, this requires assigning arbitrary weights to these errors beforehand and defining such weights is not easy. Similarly, Fagin et al. [5] proposed an extension to Kendall X  X   X  for comparing top-k lists. Their extension is also based on defining arbitrary penalties when there are errors in rankings. Furthermore, their approach still gives equal weights to errors within the top-k lists and is not very applicable for comparing the entire ranked lists while giving more weight to the errors at the top. Haveliwala et al. [6] used the Kruskal-Goodman  X  statistic (a statistic very sim-ilar to Kendall X  X   X  ) to compute correlations in the regions they are interested in (e.g. the top). However, this approach also suffers from the same problems as the former method in that it is not very applicable for comparing all the items in ranked lists at once while giving more weight to the errors at the top.

In this paper, we first show that the problem of evalu-ating the correlation between two ranked lists is analogous to the problem of evaluating the quality of a search engine, concluding that similar ideas can be used in both cases. We propose a new rank correlation coefficient, AP correlation (  X  ap ), that is based on average precision and has a prob-abilistic interpretation similar to Kendall X  X   X  , while giving more weight to the errors nearer the top of the list, as in AP. The proposed statistic has two nice properties: (1) When ranking errors are randomly distributed across the list, the AP correlation value is equal to Kendall X  X   X  , in expecta-tion. (2) If there are less (more) errors towards the top of the list, the AP correlation value is higher (lower) than the Kendall X  X   X  value, as desired. These two properties make the AP correlation coefficient easy to interpret (by compar-ing with Kendall X  X   X  ) and use. We further demonstrate the applicability of the statistic through experimental data, and we demonstrate with a real example how AP correlation can point out errors that might otherwise be incorrectly ignored when using Kendall X  X   X  .
The Kendall X  X   X  measure is one of the most commonly used measures employed to compute the amount of corre-lation between two rankings. Given two lists of length N , let C be the total number of concordant pairs (pairs that are ranked in the same order in both rankings) and D the total number of discordant pairs (pairs that are ranked in opposite order in the two rankings). Then, the Kendall X  X   X  value between the two lists is defined as
Note that given a ranked list with N objects, there are  X  = N ( N  X  1) / 2 pairs of items among them (the denom-inator in the formula). Hence, the Kendall X  X   X  value has the following nice probabilistic interpretation: Consider the following experiment, 1. Pick a pair of items at random. 2. Return 1 if the pair is ranked in the same order in both systems. systems Let p be the expected outcome of this random experiment, i.e., the probability that a randomly chosen pair of items is ranked in the same order in both lists. Note that p = C/ [ N ( N  X  1) / 2], and thus Kendall X  X   X  is As such, Kendall X  X   X  is a monotonic function of the proba-bility that a randomly chosen pair is ordered identically in both rankings.

Note that if the two rankings are identical ( p = 1), then the Kendall X  X   X  value is 1, while if the two rankings perfectly disagree ( p = 0), then the Kendall X  X   X  value is  X  1, and if the two rankings are statistically independent ( p = 1 / 2), then the Kendall X  X   X  value is 0.

In information retrieval, many different evaluation mea-sures have been developed to assess the quality of the out-put of a search engine, typically a ranked list of documents. Since the documents retrieved towards the top of the list are more important than the others, evaluation measures that give more importance to the documents retrieved towards the top of the ranking have been proposed. Note that the problem of evaluating the correlation between two ranked lists (when the top items are more important than the oth-ers) is a similar problem to the problem of evaluating the quality of a search engine. As such, one can use similar ideas to the ones used in evaluating the quality of a search engine in order to compare rankings.

Average precision is perhaps the most commonly used evaluation measure. It has a nice probabilistic interpreta-tion that assigns a meaning to the measure. In the following section, we show that average precision can be defined based on preferences. Based on this preference based version of average precision and the probabilistic interpretation of the measure, we then propose a new rank correlation coefficient ( AP correlation ) that also has a nice probabilistic interpreta-tion and that distinguishes between the errors made towards the top of a ranked list.
Average precision can be defined as the average of the pre-cisions at relevant documents, where the precisions at unre-trieved documents are assumed to be zero. It is also known to be an approximation of the area under the precision-recall curve.

Since documents that are retrieved at the top of a list are more important than the documents towards the bottom, average precision assigns more weight to the errors made towards the top of a ranking than the errors towards the bottom.

An intriguing property of average precision is that these weights are assigned such that the value of the measure has a nice probabilistic interpretation.

Recently, Yilmaz and Aslam [17] proposed a new proba-bilistic interpretation of average precision and used this in-terpretation to estimate the value of average precision when the relevance judgments are incomplete. The average pre-cision according to this interpretation can be computed as the expected outcome of a random experiment, 1. Pick a relevant document at random. Let r be the Figure 3: (left) AP correlation coefficient vs. Kendall X  X   X  for all possible permutations of a list of length 8 (right) Symmetrized AP correlation co-efficient vs. Kendall X  X   X  for all possible permutations of a list of length 8. 2. Pick a second document at or above rank r , at random. 3. Return the binary relevance of this second document. Average precision is the expected outcome of this random experiment: the expected outcome of Steps 2 and 3 is the precision at rank r (containing a relevant document), and in expectation, Step 1 corresponds to the average of these precisions.

Note that the aforementioned probabilistic interpretation of average precision holds in the case of binary relevance judgments (i.e., a document can be either relevant or non-relevant). It can be altered to hold in the case of preferences as well (i.e., one document is more relevant than the other and so on) as follows: 1. Pick a relevant document at random. Let r be the 2. Pick a second document at or above rank r , at random. 3. Return 1 if the second document is at least as relevant Average precision is the expected outcome of this random experiment. Note that the above definition of average preci-sion gives us a way to compute average precision of a ranked list of documents given the actual ranked list of documents.
As mentioned before, in information retrieval, we often face the problem of comparing two ranked lists of items where we care more about the top items than the items towards the bottom and Kendall X  X   X  does not distinguish between such cases. Average precision itself is such a top-heavy measure. Hence, one could use a rank correlation statistic based on average precision. In the following sec-tion, we propose a new rank correlation statistic based on the probabilistic interpretation of average precision, and we show how this new statistic compares to the Kendall X  X   X  statistic.
One can alter the probabilistic interpretation of average precision [17] slightly to obtain a new rank correlation statis-tic that gives more importance to the items towards the top of the list. The main idea behind the AP correlation coeffi-cient (  X  ap ) is that since the rankings at the top part of the list are more important, given an item, one mainly cares if this item is ranked correctly with respect to the items above this current item. Hence, the correlation is based on comput-ing the probability that each item is ranked correctly with respect to the items above this current item and averaging over all items.

Let list1 and list2 be two lists of items of length N and suppose list2 is the actual ranking of items and list1 is a ranking of items whose correlation with the actual ranking ( list2 ) we would like to compute. Consider the following random experiment: 1. Pick any item from list1 , other than the top ordered 2. Pick another item from this list that is ranked above 3. Return 1 if this pair of documents are in the same In mathematical terms, the expected outcome of this ran-dom experiment can be written as follows: where C ( i ) is the number of items above rank i and correctly ranked with respect to the item at rank i in list1 .
Note that this random experiment is very similar to the random experiment upon which Kendall X  X   X  is based; the only difference is that instead of comparing an item with any other randomly chosen item, it is only compared with items above.

It is easy to see that the value of p 0 falls between 0 and 1, where 1 means that all items ranked by list1 are ranked in the same order as the items ranked by list2 and 0 means that all items ranked above another item are ranked incorrectly according to list2 (complete negative correlation). Note that if there are items that are not shared by both lists then p is computed based only on the common items.

We define the AP correlation coefficient as a function of the expected outcome of the above random experiment, in much same way that Kendall X  X   X  is defined as a function of the outcome of an analogous random experiment, so that its value will fall between  X  1 and +1, the range of values com-monly used by correlation coefficients. The AP correlation  X  ap is defined as  X  ap = p 0  X  (1  X  p 0 ) = 2 p 0  X  1 = Given the two rank correlation statistics, Kendall X  X   X  and AP correlation coefficient, one might wonder how the value of the two statistics compare with each other for different types of lists. In this section, we show how these two statis-tics compare with each other when the distribution of error changes in lists.

Theorem 1. When the errors are uniformly distributed over the list, Kendall X  X   X  and AP rank correlation coefficient are equivalent.

Proof. Suppose you are given an ordered list of items, list1 and a second list with the actual ranking of items. Suppose that the probability of obtaining a correct pair in list1 is p and all errors are uniformly distributed over the list.

First consider the traditional Kendall X  X   X  value. Based on the given setup, the probability that any randomly picked pair is concordant or discordant is p or 1  X  p respectively. Hence, when the errors are uniformly distributed over the list with probability 1  X  p , the value of Kendall X  X   X  is 2 p  X  1.
Now consider the AP rank correlation coefficient value for this setup. Since the errors are uniformly distributed over the entire list with probability 1  X  p , at rank i , the expected number of concordant items above rank i with respect to the item at rank i is p ( i  X  1). Hence, the value of AP rank correlation coefficient can be computed as:
Therefore, when the error is uniformly distributed with probability 1  X  p over the entire list, the values of Kendall X  X   X  and AP rank correlation coefficient are the same and are both equal to 2 p  X  1. As mentioned before, when the two rankings of items are completely independent of each other, the Kendall X  X   X  value is 0. This corresponds to the case where p = 0 . 5 and empirically verifies the theorem above, since in this case both the Kendall X  X   X  and the AP rank correlation coefficient are 0.

Theorem 2. If the probability of error is increasing with rank (more errors towards the bottom of the list when com-pared to the top of the list), then the Kendall X  X   X  is always less than the AP rank correlation coefficient. Similarly, if the probability of error is decreasing with the rank, then the Kendall X  X   X  is always less than the AP rank correlation co-efficient.

Proof. Suppose you are given an ordered list of items, list1 and a second list with the actual ranking of items. Suppose that the probability of obtaining a correct pair in list1 is varying with the rank and for an item at rank i , the probability that the items ranked above item i are ranked correctly with respect to item i is p i .

First, consider the value of the traditional Kendall X  X   X  in this setup. The expected number of items that are ranked above item i and are ranked correctly with respect to item i is p i  X  ( i  X  1). For each rank i , we can compute the total number of concordant items above this rank with respect to the item at rank i and sum these values to obtain the total number of concordant items in the list. Therefore the expected Kendall X  X   X  value is:
Now consider the AP rank correlation coefficient value in the same setup. Using the same idea for Kendall X  X   X  , the AP rank correlation coefficient value can be written as: To demonstrate how the Kendall X  X   X  and AP rank correla-tion coefficient change with respect to each other with the probabilities p i , the difference between the AP rank corre-lation coefficient and the Kendall X  X   X  is computed: The constant factor 2 / ( N ( N  X  1)) can be ignored. We can expand the remaining formula and rewrite it in a simpler form as, (( N  X  2) p 2 + ( N  X  4) p 3 + ( N  X  6) p 4 +  X  X  X  + 2 p 0 p This summation can be written as Note that this summation is always greater than zero when the probabilities are decreasing and is always smaller than zero when the probabilities are increasing. Hence, when the probability of error in the list increases (or decreases) as we go lower in the ranking, the AP rank correlation coefficient is always larger (or smaller) than Kendall X  X   X  .

Note that the above proof shows that if the probability of error is decreasing by rank (more errors at the top), then E [  X   X   X  ap ] &lt; 0 (or vice versa). However, this does not im-ply that if E [  X   X   X  ap ] &lt; 0 then it is always the case that the probability of error is decreasing by rank. In fact, one can identify some particular distributions of error where this statement does not hold. However, in the next sections, we show using real data that it is often the case that the lists for which  X  ap  X   X  , the errors are quite random over the en-tire list and for the ones  X  ap &gt;  X  , there are fewer mistakes towards the top of the list.

The left plot in Figure 3 shows the value of AP correlation coefficient vs. Kendall X  X   X  for all possible permutations of a list of length 8. It can be seen that for a given value of Kendall X  X   X  , there are many lists that have a different  X  vice versa. In what follows, we show that for lists for which  X  ap  X   X  , the errors are quite random within the list and for those for which  X  ap &gt;  X  , there are fewer mistakes towards the top of the list as proved. Furthermore, we show that the proposed statistic can produce more desirable assessments of correlations among rankings, in that it produces higher values when the top ranked items are correctly identified and lower values when they are not. systems become worst and worst systems become best)
Note that the AP correlation coefficient is not a symmetric statistic. It assumes that there is an actual ranked list ( list2 ) of items and an estimated ranked list ( list1 ) of items and one would like to compute the correlation among these two lists (  X  ap ( list 1 | list 2) : AP correlation of list1 given list2). However, if we were to compute the AP correlation of list2 given list1, we obtain a different value for the statistic. In information retrieval, it is often the case that we would like to compare the correlation between an estimated ranked list given an actual ranked list (e.g. actual rankings of systems). Hence, in such situations, AP correlation can be used.
However, in some cases one would simply like to compute the correlation among two ranked lists where we do not have the notion of the  X  X ctual X  rankings. In such cases, a sym-metrized version of the statistic ( symmetrized AP correlation coefficient ) could be used. The symmetrized AP correlation coefficient can be computed based on the same idea used in defining the symmetrized version of the KL distance mea-sure [4]: symm X  ap ( list 1 , list 2) = (  X  ap ( list 1 | list 2)+  X  Hence, the symmetrized AP correlation coefficient is the av-erage of the AP correlation coefficients when each list is treated as the actual list.

Right plot in Figure 3 shows the value of AP correlation coefficient vs. Kendall X  X   X  for all possible permutations of a list of length 8. It can be seen that the shape of the curve is similar to the shape for AP correlation coefficient (Figure 3).
We have shown theoretically that when the errors are ran-domly distributed the entire list,  X  ap =  X  , when there are more errors towards the top of the list,  X  ap &lt;  X  , and when there are fewer errors towards the top of the list,  X  ap &gt;  X  . What this means is that this new measure can correctly or-der correlations among items (lists with fewer errors towards the top have higher AP correlations and so on.)
Figure 4 demonstrates this behavior in practice. The fig-ure shows three different rankings with identical Kendall X  X   X  values. The leftmost plot illustrates the ranking of sys-tems according to depth1 pooling versus the actual ranks of systems in TREC 8 (the top right corner of the plots refer to the top ranked systems). The rightmost plot shows the rank of the systems in TREC 8 based on depth-1 pooling when the rankings are reversed (worst systems become the best and best systems become the worst). Note that the rightmost plot corresponds to rotating the leftmost plot by 180  X  . The middle plot corresponds to a random ranked list that has random errors along the entire list such that the Kendall X  X   X  of the list is equal to the Kendall X  X   X  of depth-1 pooling on that TREC. It can be seen that when the errors are randomly distributed over the list (middle plot), the AP correlation coefficient is approximately equal to Kendall X  X   X  as expected in expectation. In the leftmost plot, where there are more errors towards the top of the list than the bottom, it can be seen that the AP correlation is less than Kendall X  X   X  . Similarly, when the top systems are ranked mostly correctly (rightmost plot), the value of the AP cor-relation coefficient increases and is higher then the Kendall X  X   X  value. Hence, even though all three plots are equivalent in terms of their Kendall X  X   X  value, the proposed correlation coefficient correctly identifies the distinction among all these three cases, as desired.

The fact that Kendall X  X   X  gives equal weight to all errors in the rankings also has practical implications for IR research. For example, as mentioned before, in information retrieval, pairs of rankings whose Kendall X  X   X  values are at or above 0.9 are often considered effectively equivalent [13]. As also men-tioned by Sanderson et al. [10], this threshold has implicitly or explicitly been used in many places to reach conclusions regarding the quality of rankings. Work by Carterette et al. [3], Sanderson et al. [9], and Yilmaz et al. [17] are just a few examples of the many such research papers.

In Sanderson et al. [9], for example, the authors claim that since manual runs identify most of the relevant docu-ments, instead of judging the entire depth pool, one could only judge the outputs of manual runs and use these judg-ments to evaluate the quality of retrieval systems. In order to test their claim, they take a manual run out, form a qrel by using only judgments for documents that are retrieved by this run and are also contained in the actual TREC qrels. Then, using these qrels, they evaluate the quality of retrieval systems by mean average precision and compare the induced rankings of systems with the actual rankings using Kendall X  X   X  . They report that for most of the manual runs, the  X  cor-relations are at or above 0.9 and therefore conclude that qrels formed from the output of a single manual run can be effectively used to evaluate the quality of retrieval systems. (bottom) CL99SDopt2 in TREC 8, compared to actual rankings.
Figure 5 shows an example of these experiments. The upper and lower plots show how qrels formed from manual runs Brkly26 from TREC 7 and CL99SDopt2 from TREC 8 evaluate retrieval systems in these TRECs, respectively. The x axis in these plots is the actual MAP value for the systems and the y axis shows the MAP value of the systems as using these qrels.

The leftmost plots show the rankings of systems for only the automatic runs submitted to these TRECs whereas the middle plot contains rankings for all systems (manual and automatic). The rightmost plots focus only on the top 12 systems (sorted by the actual mean average precision value) in these TRECs and show how the manual qrels rank these top systems as compared to their actual rankings.
First consider the plots for TREC 7. By looking just at the Kendall X  X   X  values, one would conclude that qrels formed from this manual run are very effective at evaluating systems (both when only automatic runs are included  X   X  value of 0.9555  X  and when all runs are included  X   X  value of 0.9543). Although this may be a reasonable conclusion when only automatic runs are considered, the conclusion is much less valid when all runs are considered, since the topmost systems are not ranked correctly (middle plot). As shown by the rightmost plot, when all runs are included the best system according to these qrels is actually the 11th best system, the 2nd best system is actually the 3rd best system, and so on. Furthermore, although the correlations in the middle plot looks worse than the correlations in the left plot, the  X  values are very similar. However, the AP correlation (or symmetrized AP correlation) correctly takes these changes into account and correctly distinguishes these two cases, correctly pointing out that the correlation in the middle plot is worse than in the left plot.
 Similar behavior can be seen for qrels formed from CL99SDOpt2 in TREC 8. In this case, even when only automatic runs are considered (left plot), the rankings of the top-most systems are not perfect. Therefore, the AP correlation is not so high even for this case where Kendall X  X   X  is 0.9396. When all runs are included (middle plot), the Kendall X  X   X  value is 0.9454 although the correlations among the top systems become even worse (top system according to the actual qrels are identified as the 4th best system us-ing these qrels, and the best system according to these qrels is actually the 7th best system if actual qrels were used as seen by the the rightmost plot). Surprisingly, the Kendall X  X   X  value in this case is higher than the Kendall X  X   X  value when only automatic runs are included. Once again, the AP correlation correctly distinguishes these cases. Both of these examples show that even though the Kendall X  X   X  statistic shows very good correlations, the qrels formed from a single manual system cannot be reliably used to evaluate the quality of all other systems (including other manual systems); the AP correlation statistic points out this issue more reliably than Kendall X  X   X  . This problem with the Kendall X  X   X  statistic was also discussed by Sanderson and Soboroff in a similar context [10].

To further demonstrate the applicability of the AP corre-lation statistic against Kendall X  X   X  , we generated 100 random lists of length 50 that have a Kendall X  X   X  value of 0.9 (using dynamic programming), and we scatter plot all such lists versus the actual ranking (1 2 . . . 50), in a manner similar to the middle plot of Figure 4 superimposing all 100 scatter plots on top of each other. In this way, we can see the trend of error, i.e., which part of the ranking is the error likely to occur at. The left plot in Figure 6 shows the result of this experiment. It can be seen that for a list with Kendall X  X   X  value of 0 . 9, the errors are likely to occur at any part of the list, hence the top items may not be identified correctly. Similarly, to see the trend of error in the lists that have an AP correlation value of 0 . 9, we generated 100 random lists Figure 6: (Left) 100 randomly generated lists of length 50 that have a Kendall X  X   X  value of 0.9. (Right) 100 randomly generated lists of length 50 that have a AP correlation value of 0.9. of length 50 that have an AP correlation value of 0 . 9. The right plot in Figure 6 shows the result of this experiment. It can be seen that the lists that have an AP correlation of 0 . 9 are very unlikely to have errors towards the top of the list, and most of the errors tend to be towards the bottom, rein-forcing our conclusion that AP correlation coefficient can be effectively used in the cases where we are mostly interested in identifying these top ranked items.
We propose a new rank correlation coefficient that has a nice probabilistic interpretation and more heavily weights the errors towards the top of the ranking. The statistic has the nice property that (1) if errors are uniformly dis-tributed over the entire ranking, the statistic is equal to Kendall X  X   X  in expectation and (2) if there is more error towards the top/bottom of the list, the value of the statis-tic is lower/higher than Kendall X  X   X  . We theoretically and experimentally show through TREC data the applicability of this measure and conclude that in the context of infor-mation retrieval, where the goal is often to identify the best systems while still having a good overall ranking, this statis-tic can be better used to compute the correlation between two rankings.

The statistic as defined in this paper is not symmetric, hence it is not a metric. Although we show how this statis-tic can be extended to be symmetric, throughout the paper we mainly focus on the behavior of the nonsymmetric ver-sion as it is simpler to analyze. The overall behavior of the symmetric version of the statistic is the subject of future work. Furthermore, the Kendall X  X   X  statistic has an associ-ated probability distribution, enabling one to do hypothesis testing. In the future, we are planning to investigate the dis-tribution of the proposed statistics to enable making better inferences about the values of these statistics. [1] J. A. Aslam, V. Pavlu, and R. Savell. A unified model [2] C. Buckley and E. M. Voorhees. Retrieval evaluation [3] B. Carterette and J. Allan. Incremental test [4] T. Cover and J. Thomas. Elements of Information [5] R. Fagin, R. Kumar, and D. Sivakumar. Comparing [6] T. H. Haveliwala, A. Gionis, D. Klein, and P. Indyk. [7] M. Kendall. A new measure of rank correlation. [8] M. Melucci. On rank correlation in information [9] M. Sanderson and H. Joho. Forming test collections [10] M. Sanderson and I. Soboroff. Problems with kendall X  X  [11] G. S. Shieh. A weighted kendall X  X  tau statistic. [12] I. Soboroff, C. Nicholas, and P. Cahan. Ranking [13] E. M. Voorhees. Evaluation by highly relevant [14] E. M. Voorhees. Overview of the TREC 2004 robust [15] D. D. Wackerly, W. Mendenhall, and R. L. Scheaffer. [16] S. Wu and F. Crestani. Methods for ranking [17] E. Yilmaz and J. A. Aslam. Estimating average
