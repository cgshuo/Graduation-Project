 Dependency parsing is a fundamental task for language processing which has been investigated for decades. Among a variety of dependency parsing models, graph-based models are attractive for their ability of scoring the parsing decisions on a whole-tree basis. Recently, neural network models have been successfully introduced into graph-based dependency parsing and obtained state-of-the-art results. [16] presented a simple feed-forward network model which uses only atomic features such as word unigrams and POS tag unigrams. [18] proposed utilizing word representations learned by Bidirectional LSTM network to support parsing decisions and further improved their model by segment embeddings. cabulary due to its standard lookup-based word representations, which lead to difficulty in recovering dependencies involving out-of-vocabulary words. On the other hand, this standard lookup-based word representations capture only dis-tributional semantics of words, which in essence encodes useful information in the surrounding contexts of the concerned word. However, for a more complete word representation, compositional semantics of a word would be necessary as well, which can be derived by combining meaning of word parts. This is especial-ly important for languages like Chinese. The characters making up words bear meanings of their own and usually determine the meanings of words to a certain extent.
 to graph-based dependency parsing. Our model includes two levels of Bidirec-tional LSTM network: one at the character level and one at the word level. The character-level LSTM aims to capture compositional semantics of word, which is then combined with the standard lookup-based word embedding to produce a more complete representation of word. The introduction of compositional rep-resentation also makes our model more robust to OOV words which are usually not well represented by the lookup-based word embeddings. The word-level L-STM aims to enrich the word representation and capture potential long range contextual information to support parsing decisions.
 dependency parsing. [5] proposed replacing lookup-based word representation-s with character-based representations which obtained by Bidirectional LSTM. They show that character-level information is helpful for morphologically rich language. However, simply using character-based representations results in their model performing poorly for English and Chinese. Different from their work, lookup-based word representations and character-based representations are com-bined and a word-level Bidirectional LSTM is further utilized to capture richer contextual information in our work. The combined word representation used in our work captures both distributional and compositional semantic information. Treebank, our model achieves state-of-the-art accuracy on Chinese Penn Tree-bank and competitive accuracy on English Penn Treebank. In this section, we describe the architecture and the training of our neural net-work model in detail. 2.1 Word Representation Given an input sentence s = w 1 ,...,w n together with the corresponding POS tags p 1 ,...,p n , a hierarchical LSTM network is utilized to learn word represen-tations, which is summarized in figure 1. A character-level Bidirectional LSTM is used to compute character-based embeddings of words. We then concatenate four vectors: the forward character-level LSTM hidden vector (  X  X  X  c ); the backward character-level LSTM hidden vector (  X  X  X  c ); the word embedding ( e ( w i )) and the POS tag embedding ( e ( p i )). A linear transformation w e is performed and passed though an element-wise activation function g (ReLU is used as our activation function): represent words in sentence. Hierarchical LSTM networks allow each word rep-resentation v i to capture information regarding the character level, the word form and POS tag, as well as the sentential context it appears in. In addition, combining character-level information allows our model to avoid the problem of out-of-vocabulary words. 2.2 Score Model A neural network model is utilized to score dependency arcs. We use the same architecture and the same features as [18].
 word representations for head word h and the modifier word m , distance between them (distance features are encoded as randomly initialized embeddings), and segment embeddings for the dependency pair ( h,m ). A sentence is divided into three parts ( prefix , infix and suffix ) by head word h and modifier word m, these parts are called segments. An extra forward LSTM layer is placed on top of the word representations and segment embeddings are learned by using subtraction between the forward LSTM hidden vectors.
 transformation is utilized to model edge direction: where a i is the feature embedding, W d h which indicates the direction between head and modifier. dependency arcs: Where Score ( h,m )  X  R L is the output vector, L is the number of dependency types. Each dimension of the output vector is the score for each kind of depen-dency type of head-modifier pair. 2.3 Neural Training We use the Max-Margin criterion to train our model. Parameter optimization is performed with the diagonal variant of AdaGrad [9] with minibatchs (batch size = 20). To mitigate overfitting, we apply dropout [12] on the hidden layer of the score model with 0.2 rate.
 size = 100, POS tag embedding size = 50, character embedding size = 50, hidden layer size = 200, character-level LSTM hidden vector size = 50, word-level LSTM hidden vector size = 100, character-level LSTM layers = 1, word-level LSTM layers = 2, regularization parameter  X  = 10  X  4 .
 [10] and [18], we use a variant of the skip n-gram model introduced by [13] on Gigaword corpus [11]. We also experimented with randomly initialized embed-dings, where embeddings are uniformly sampled from range [  X  0 . 3 , 0 . 3]. All other parameters are uniformly sampled from range [  X  0 . 05 , 0 . 05]. In this section, we present our experimental setup and the main results of our work. 3.1 Experiments Setup We conduct our experiments on the English Penn Treebank (PTB) [1] and the Chinese Penn Treebank (CTB) [2] datasets.
 the Penn Treebank. Dependencies generated from version 3.3.0 2 of the Stanford converter [15], we call it Penn-SD In the following section. We followed standard practice and used sections 2-21 for training, section 22 for development, and section 23 for testing. The Stanford POS Tagger [17] with ten-way jackknifing of the training data is used for assigning POS tags (accuracy  X  97.2%). cies are converted using the Penn2Malt 3 tool with the head-finding rules of [21]. And following [21, 23], we use gold segmentation and POS tags for the input. 3.2 Experiments Results Following previous work, UAS (unlabeled attachment scores) and LAS (labeled attachment scores) are calculated by excluding punctuation 4 . Table 1 lists the performances of our model as well as previous state-of-the-art models on CTB5. As we can see, word representations combining character-level information do improve model X  X  performance compared with [18]. Moreover, the LAS of our model achieves state-of-the-art accuracy.
 Table 2 lists the performances of our model as well as previous state-of-the-art models on Penn-SD. As we can see, the improvement on Penn-SD is lower than CTB5. On one hand, the OOV rate of Penn-SD (2.2%) is much lower than CTB5 (10.4%). On the other hand, character-level information of English reflects more morphological information which is also encoded in treebank POS tags, while character-level information of Chinese supplements semantic information within words to support parsing decisions. Moreover, our model outperforms [5] by a substantial margin on both Chinese and English since our word representations capture richer information rather than simple character-level information. out-of-vocabulary words, we compare the UAS and LAS of out-of-vocabulary words between our hierarchical LSTM network model and bidirecitonal LSTM network model proposed by [18] on CTB5. As shown in table 3, incorporating character-level information makes our model achieve better UAS and LAS of out-of-vocabulary words. We observed a 3.96% rise in UAS and 5.88% rise in LAS respectively on recovering dependencies involving out-of-vocabulary words. A t-test on the difference shows the improvement is statistically significant. for the performance of our parser. As shown in table 4, our basic model uses on-ly lookup-based word representations and shows the worst performance. Using character-level information and POS tags do improve our basic model and lets our model achieve competitive accuracy. Moreover, we find that character-level information makes greater improvement when POS tags are not provided. Again we observe a much bigger improvement in Chinese than in English when intro-ducing the character-level representation of words. In addition, although using character-level information could improve model to a certain extent, POS tags is still necessary for dependency parsing. It seems that the introduction of POS information contributes more in Chinese than in English as well. This is, how-ever, not strictly comparable, since we use gold standard POS tag for Chinese and automatically generated POS tag for English as most previous work did. achieves 93.52% UAS / 91.23% LAS on Penn-SD and 86.62% UAS / 85.11% LAS on CTB5. Using pre-trained word embeddings can obtain around 0.6%  X  1.1% improvement. In this paper, we propose a hierarchical LSTM network model for graph-based dependency parsing. Word-level and character-level LSTM networks are utilized to capture information regarding the character level, the word form and POS tag, as well as the sentential context it appears in, which allows for a improvement on Chinese and lets our model achieve state-of-the-art accuracy. Moreover, our model is still a first-order model using standard Eisner algorithm for decoding, the computational cost remains at a lowest level among graph-based models. and test our model on morphologically rich languages which are often nonpro-jective dependencies.
 This work is supported by National Key Basic Research Program of China under Grant No.2014CB340504 and National Natural Science Foundation of China under Grant No.61273318. The Corresponding author of this paper is Baobao Chang.

