 Adam Laud LAUD @ UIUC . EDU Gerald DeJong DEJONG @ CS . UIUC . EDU Reinforcement Learning (RL) is a popular and effective technique for learning to act in stochastic domains with delayed feedback. Empirically, learning is often slow in the sense that many episodes may be required before a good policy emerges. To speed learning, researchers have explored the use of shaping rewards. In essence, shaping employs an augmented reward structure as a medium to convey prior knowledge to an otherwise-conventional RL system. These artificial rewards can help to shepherd the reinforcement learner toward policies believed to be good or away from policies believed to be bad. Dorigo and Colombetti (1993) use shaping to convey suggestions from a trainer to a small mobile robot. Mataric (1994) uses successive levels of shaping to achieve better final performance during fixed training intervals of a robot foraging task. Randl X v and Alstr X m (1998) show how a shaping reward function can be employed to learn how to drive a bicycle toward a goal. The shaping reward structure is often static in the sense that additional rewards are fixed prior to any domain observations. Laud and DeJong (2002) explore dynamic rewards, in which the additional rewards depend in part on characteristics derived from initial observations of the world. They apply dynamic shaping to bipedal walking. The addition of shaping rewards changes the Markov decision process (MDP) to be solved. However, this transformation need not alter the optimal policy. Ng, Harada, and Russell (1999) show necessary and sufficient conditions for a policy to remain optimal under addition of a shaping reward structure. However, they, like other shaping investigators demonstrate accelerated learning empirically. The question of what sort of shaping policy will speed rather than delay convergence to the optimal policy is left largely unanalyzed. This is the focus of the present work. A policy is a mapping or assignment of the highest utility action to each respective state. Thus, conceptually, policy acquisition in reinforcement learning can be viewed as solving a set of classification learning tasks but with several significant complications. A primary complication is that training feedback on a state classification is delayed and ambiguous. Executing an action is similar to obtaining a training example with partial feedback. This feedback is fully captured though a sequence of (possibly discounted) rewards extending on from the action decision. The reward sequences for successive action decisions overlap. The later that a reward appears in a sequence from an action, the greater the number of intervening action decisions, and the less certain we can be that this reward represents significant evidence about the original action decision. It seems reasonable to consider that the learning rate of a reinforcement learner, like other learners, is significantly influenced by the quality of the feedback provided. Two factors degrade the quality of the feedback. First is the stochastic nature of the underlying rewards. Even for the same state-action-state triple, the observed reward may vary significantly. A higher-variance reward distribution makes any estimate of the average reward less confident. The second factor is the potential delay in rewards (Tesauro, 1992). As delay is increased, the number of reachable states and potentially relevant rewards grows exponentially. This translates into a much more complicated distribution of feedback sequences, and again a greater number of samples are required for any reliable characterization. We introduce the notion of a reward horizon to summarize these difficulties in an MDP. The reward horizon is a measure of the number of decisions a learning agent must make before experiencing accurate feedback. We view shaping as reducing the reward horizon. Shaping provides artificial reward signals which embody more informative feedback than do the native rewards. We also introduce a critical region of an MDP. These are the states with a significant probability of being visited by some near-optimal policy. Learning to behave within the critical region is sufficient for good behavior in general. The reward horizon defines a boundary to the search for a good decision and can therefore serve as a mechanism to approximate the MDP X  X  critical region. We provide two arguments for the importance of these ideas. Of primary significance is a first effort at formalizing these notions. It is difficult to analyze their effects on conventional reinforcement algorithms. But we show that with them one can construct a simple policy-learning algorithm. The experience complexity of this algorithm, while exponential in the reward horizon, is sub-quadratic in the size of the critical region and independent of the number of states in the MDP. Second, we provide a brief empirical investigation showing that a conventional reinforcement learning algorithm (Q-learning with  X  -greedy exploration) can naturally benefit from reducing the reward horizon. We begin developing our reward-based perspective with a discussion of our formalization of the reinforcement learning task, and the essential definitions and notation for the rest of the paper. We will focus on a modified version of the traditional Markov decision process (MDP), the ideal critical region, and the reward horizon. 2.1 A Modified Approach In addition to the standard description of a Markov decision process, we further characterize the process with a few new properties. We consider domains that are episodic, starting from one initial state. This choice models the repetition that is necessary for learning explicitly, rather than identifying cycles within the domain. Based on this property, we will be able to use the number of episodes until convergence as a clear metric for measuring the time complexity of a learning process. The choice of a single initial state is for simplicity; our results readily extend to a set of initial states. Another property of the learning task we choose to highlight is the task length. Definition 2.1 The task length , L, of an MDP is the maximum number of actions that may be executed in any policy. This length sets a limit on the size of all potential policies. Policies that do not reach terminal states during L actions are still considered, but will only be evaluated on the return of the first L actions. The task length allows us to distinguish policies in a way other than their expected return, and in a manner that is closely related to the learning complexity of the problem. Based on the idea of limited size episodes, we define an MDP as a collection of seven elements: the set of all states S, the set of all actions A, the set of reward distributions R, the set of state s 0 , and the task length L. In this framework, the task for reinforcement learning is to compare all policies leaving s 0 , that continue on in M for at most L steps, and to choose the one that will encounter the most reward on average. If we execute the resulting policy repeatedly, it will visit some subset of the state space. This set of states is the ideal critical region (CR) for the MDP. Definition 2.2 The ideal critical region , CR, of an MDP, in the initial state of M, and executing the optimal policy. The ideal critical region is another way of viewing convergence to the optimal policy. We have an optimal solution to the problem if we can (1) identify all states in the ideal critical region, and (2) make the optimal decision for each of these states. Notice that this region is small in many cases; its size is simply the task length in the deterministic case, and otherwise grows with the randomness imparted by the transition probabilities. We focus on convergence by learning the CR in steps, looking at a local piece of the original MDP to optimize one state and to refine a working set that approximates the region. We call the local areas sub-MDPs, and they are determined by a start state anywhere in the original MDP, and a task length at most that of the original. Formally, the sub-MDP of an MDP M = {S, A, R, T,  X  , s 0 T,  X  , s, H}. 2.2 The Reward Horizon The central idea that enhances the process of learning is restricting the task length of the sub-MDPs to a minimal, yet still informative value. The reward horizon is the property of the reward structure that determines this bound. For any state in the CR, solving a sub-MDP centered on that state with task length equal to the reward horizon will yield the optimal first action for that state. Furthermore, after deciding on a reasonable chance of failure, we can find the solution to the sub-problem efficiently. The reward horizon has two characteristics that guarantee its ability to put forth a useful sub-MDP task length. The first is that solving problems within the horizon will find the optimal first action. The second is that this solution process can be done efficiently. Inherently, the second characteristic implies that the distinction between the expected return of the optimal policy and the expected return of its closest competitor in any sub-MDP may be determined with reasonable experience in the domain. Let the mean and variance of the return of the optimal policy in the sub-MDP m be competitor policy have order to make it easy to select the optimal, we would like a large difference in means, mean ratio (vmr) to mathematically formulate these ideas. Definition 2.3 The variance-mean ratio , vmr, is In order that the optimal solution is found with a reasonable amount of experience, it must be the case that the variance-mean ratio be small. Based on these characteristics we define reward horizon with two necessary and sufficient conditions for the MDP. Definition 2.4 A MDP M has a reward horizon of H if and only if: We have argued that the reward horizon explains the success of shaping by describing a minimal search boundary. We now look to use knowledge of the reward horizon to derive the amount of work needed to converge to an approximately optimal policy. Based on this analysis, we will show that a simple algorithm can learn efficiently, even while disregarding parts of the state space. Any given MDP will have a reward horizon as long as there is one optimal policy. The reward horizon may be as long as the task length, meaning that there is no locality to the rewards; all information is delayed until the end. On the other extreme, the reward horizon may be one, meaning that the feedback from a single transition is enough to determine the best decision. More commonly, we expect that shaping can transform a reward structure with a large horizon to one with a lesser value. This ability of shaping with prior knowledge is what motivates our research on the reward horizon. We wish to investigate the effects of learning while exploiting the reward horizon. The approach we propose is to grow a set of known states that will eventually include enough states to know reliably that the policy we compute from these states is very close to the optimal. Marking a state known indicates that we have sufficient information to reliably decide on the best action for that state. We develop known states by solving the relevant sub-MDPs with length equal to the reward horizon. Growing the set is a forward-chaining operation; as early actions become fixed, later elements in the CR are visited more frequently and subsequently become known. Once enough states are known, we continue to follow the current policy until enough evidence is gathered to support termination with a good policy. The next sections discuss in detail how to make a state known, how to learn a sufficient portion of the ideal critical region, and how to bound the total amount of work required to achieve termination. These ideas build the foundation for the algorithm in the next section. 3.1 Creating Known States A known state is a state for which we believe we have found the optimal action with some level of confidence. In the case where we have a reward horizon H in an MDP M, a state s becomes known when we solve the sub-MDP m
H (s). Once m H (s) is solved, by the definition of reward horizon, we have the optimal first action in M, which is the best action for s. Therefore, our approach will be to find enough members of the ideal critical region by solving each sub-MDPs based on the reward horizon. We outline a simple, policy-based procedure for solving sub-MDPs. Namely, we will acquire a number of samples of every policy within the sub-MDP and choose the policy with the highest sample mean. Because the true mean of a policy is estimated from experience, it is not possible to know the best action with zero probability of error. We require a sampling method that will correctly choose the optimal policy within a reasonable chance of error. The following results derive such a method. Theorem 3.1 Consider two policies,  X  1 and  X  2 . Let the mean and standard deviation of the return of the policies be  X  1 ,  X  2 ,  X  1 ,  X  2 respectively, with  X  1 &gt;  X  the higher sample mean over samples of each policy will result in choosing  X  1 better policy with probability at least 1- X  . We use  X  to denote the cumulative distribution function for the standard normal distribution. vmr  X  n Proof. Let R 1 denote the random variable reporting the total discounted reward for one execution of  X  1 , and R likewise for  X  2 . Let D = R 1  X  R 2 , with mean  X  standard deviation  X  D . Let D be the sample mean of n samples of D. We wish to ensure that n is large enough that the probability that the sample mean of R 1 is larger than that of R 2 is at least 1- X  . In other words, Here we apply the central limit theorem in the case that R and R 2 are not normal. Note that  X  D =  X  1 - X  We can easily generalize these results to the comparison of several policies. Corollary 3.2 Consider m policies,  X  1 ...  X  m . Let the mean and standard deviation of the return of the policies be  X  ...  X  m , and  X  1 ...  X  m respectively, with  X  1 maximum. Then choosing the highest sample mean of samples of each policy will result in choosing  X  1 best policy with probability at least 1- X  . Proof. We define R 1 ... R m as before. We require p  X  1- X  , or equivalently p ~  X   X  X  X   X   X  . We can bound p ~ with the sum of the probability of the individual comparisons. Then p  X  p ~  X  since the events exclusive across i. For all i, let Then p  X  p ~  X   X  which, by the previous theorem, requires n samples of each  X  i , where Corollary 3.2 provides an effective sampling method, given we know the number of policies being compared, and the means and variances of each. In order to count the maximum number of policies within a given horizon, we must characterize the randomness of the domain. To this end, we introduce a parameter k, the maximum number of successor states for any state-action. The maximum decisions at the start state, and k|A| decisions branching from each step in the policy up to H. We will not assume specific knowledge of the means and variances of policy returns, but rather make use of the second part of the definition of reward horizon. This part guarantees a bound on the variance mean ratio, call it VMR. Substituting VMR and the maximum number of policies into the previous results, yields the following corollary. Corollary 3.3 A state s in an MDP M, with horizon H becomes known with  X  chance of failure after sampling every policy in m H (s) evenly, with at most visits to the state s. 3.2 Learning the Critical Region A major task of learning, under our framework, is identifying a sufficient portion of the ideal critical region such that we meet a high level of performance with a small chance of failure. We might think of learning the entire CR. In this case, we would achieve optimal returns with no chance of failure. The drawback is that we must learn every state, regardless of how improbable, or unrewarding. In fact, the difficulty in reaching unknown states in the CR plays a major role in the time it takes to learn. In order to reduce the number of unlikely states to learn, we choose to learn a subset of the critical region. The idea of this approach is to learn a number of states in the CR, such that our chance of visiting only known states for an entire episode is high. Once this probability is high enough, we will experience many trials of the current policy which visit no unknown states. In other words, we have a policy which, although not optimal, executes the optimal actions for all of the most common paths. Each time an execution introduces no unknown states, we gain evidence to develop a confidence interval around the return of the current policy. If there is sufficient number of such observations, we can conclude that the current policy is very likely to be approximately optimal. The following results outline the construction of the confidence interval, by defining a sufficient condition for its existence. We derive the number of episodes a given policy must visit only known states, in order that this policy be classified as approximately optimal with high degree of probability. We know that the return of a policy visiting only known states is high because it executes only n optimal actions within the specified chance of failure. However, we must limit the chances that this policy would, in the future, visit unknown states whose payoff is enough to warrant the current policy as not acceptable. Theorem 3.4 Let  X  be a policy for which the return has standard deviation  X  . Then  X  has an expected return within  X  of the optimal with probability 1- X  , if we observe sample episodes of  X  , and each episode performs only optimal actions.
 Proof. Let R be the random variable for the return experienced during one episode while executing  X  . Applying the central limit theorem, we obtain the standard confidence interval based on n samples of R. Setting the confidence interval width equal to  X  and solving for n yields the stated results. Because the samples obtained for R were found executing only optimal actions, R also reflects a sample mean for the optimal policy. Therefore, we obtain the desired confidence interval around R with n samples from  X  . The expected return of  X  must be within  X  of the optimal 1- X  of the time. The idea of gaining confidence in the current policy is powerful because it gives purpose to every episode. That is, on any given episode, we either: (1) visit an unknown state, which refines the current policy, or (2) visit only known states, which builds confidence in the current policy. Once the confidence interval justifies the current policy as approximately optimal with high probability, we can end the learning process, even though some members of the CR are left unknown. However, Theorem 3.4 relies on the variance of the return of a policy, which is generally unknown. The following result places a bound on such variances. Theorem 3.5 Let any reward, r, from one transition in an MDP be bounded: -r MAX  X  r  X  r MAX , and let the maximum standard deviation of r be  X  MAX . Then the maximum variance for any policy in the MDP with task length L is Proof. Let the random variable for the policies return be R, coming from the probability density function f with mean  X  . The policy visits any path i, which occurs with probability p i , has return drawn from f i with mean  X  standard deviation  X  i . The variance of R is we obtain the maximum value for the variance. Substituting this bound for the variance in Theorem 3.4, we develop a termination condition for the learning process. Corollary 3.6 Let  X  be a policy for which we have observed consecutive episodes visiting only known states. Then  X  has an expected return within  X  of the optimal with probability 1- X  . We would like to determine a subset of the critical region such that we can reach the termination condition in Corollary 3.6. However, this is possible for any nonempty subset that contains a complete path. To specify an approximate critical region, we need another parameter,  X  . This parameter denotes the probability that the next n terminal episodes will visit only known states  X  the chance of immediate termination with a near-optimal policy. Definition 3.1 The (approximate) critical region , CR  X  any subset of the critical region such that the probability of visiting only its elements is at least terminal given episode, executing a policy which knows the optimal action for every element. The notion of  X  is an artificial one; we are not interested in specifying it, but rather we use it to illustrate the amount of approximation that will occur. Furthermore, it is a parameter that will need to be optimized to determine the minimum amount of time until convergence. For example, learning CR 1 requires a great deal of work to explore and optimize all the states in the critical region, and very little work to gain confidence in the resulting policy. On the other hand, learning CR 0.1 may be a much easier subset to optimize, but requires on average 10(n terminal ) episodes to achieve termination. Rather than continually add to the approximate critical region until termination. This idea will automatically determine an appropriate  X  . If several states in the critical region are very unlikely, we will tend toward a lower alpha. If most states are easily explored,  X  will tend to be close to 1.  X  n
R n
CR is known.  X  1 3.3 Reaching Convergence The final part of our analysis is to count the number of episodes needed to achieve termination with a near optimal policy. Conceptually, the work that occurs for termination has two parts. The first is meeting the condition of Corollary 3.3 for a sufficient number of states to create the approximate critical region. Next, we must gain confidence in the policy created by optimizing the approximate critical region. This implies meeting the termination condition of Corollary 3.6. Theorem 3.7 Let CR  X  be a subset of the critical region reaching the least probable state within CR  X  . Then, with probability 1- X  , every state in CR  X  will be known in episodes. F is the cumulative distribution function for the probability of success p. Proof. The number of episodes, t, it will take for the least probable state to become known is a negative binomial random variable with n known number of successes, and probability of success equal to p. For t, a success is simply a visit to that state. We divide our acceptable probability of failure,  X  , into two parts. Half is the chance that solving a sub-MDP will not find the best action, and the other half is the chance that a given state is not visited enough. Because any state in the approximate critical region may fail the second part, this error is divided by the size of the region. And the least probable state will become known with  X  /(2|CR  X  |) chance of error in episodes. Since all other states are more probable, n represents an upper bound for any state in the approximate critical region. Thus, we may proceed for |CR  X  | n episodes, which must make all the required states known with acceptable probability. Theorem 3.8 Let CR  X  be an approximate critical region in which all states are known, and the probability to experience n terminal successive episodes staying only within within CR  X  will occur with probability 1- X  in episodes. G is the cumulative distribution function for the geometric distribution with  X  probability of success. Proof. Let one trial be a sequence of n terminal trial is successful if it visits only known states. The number of trials for a success is a geometric random variable with chance of success  X  . We bound the number of trials with the acceptable chance of error,  X  , and multiply by the length of a trial to obtain the stated result. We must apportion of the acceptable chance of failure to its various potential causes. We have discussed 4 areas of failure, and denote them as follows. Let  X  1 be the chance of error for not finding the correct action during the time given for a state to become known. This error must further be shared with L other members within any given episode, so that the total chance of executing a non-optimal action during an episode is  X  1 . Let  X  2 chance of termination without being within  X  of optimal. Let  X  3 be the chance that any state in the approximate critical region is not made known because it was not reached enough times. This error must be shared between all members of the approximate critical region. Let  X  the chance of not reaching termination within the given time, after the critical region is learned. We divide  X  evenly to all four modes of failure. With this notation, we consolidate our results into the main result of this section: the total number of episodes for the learning process. Table 1 explains all the parameters of Corollary 3.9 and the relevant terms from our previous results. Each parameter is given with its highest order term showing its contribution to the total running time. Corollary 3.9 The total number of episodes to learn a policy, by solving reward horizon bounded sub-MDPs, with expected return within  X  of the optimal with probability 1- X  is bounded by for any  X  . F is the cumulative distribution function for the probability of success p. G is the cumulative distribution function for the geometric distribution with  X  probability of success. In this section, we present a simple method that implements the reward horizon learning scheme according to the results in the previous section. The intent of this algorithm is to take advantage of the reward horizon for more efficient learning. In this way, we can illustrate the potential for shaping through a specific technique that exploits the reward horizon. The behavior of H ORIZON _L EARN is to undergo the process of making known states, until eventually, enough states are known such that we reach the termination requirement of Corollary 3.6. The method to achieve this is very simple  X  build up a policy  X  by solving sub-MDPs based on the reward horizon. At any time,  X  is either exploited, because the state is known, and we wish to explore other members of the critical region, or we execute any policy from a sub-MDP that is needed to make the state known. The number of times we must cycle through this procedure follows the Corollary 3.9. H ORIZON _L EARN (MDP M, Reward Horizon H) H
ORIZON _L EARN is guaranteed to meet the requirements for both convergence, and approximate optimality with a polynomially-bounded amount of work given a fixed horizon. The reward horizon serves to guide exploration and focus it on the critical region. This process allows the algorithm to take advantage of any case where the critical region is smaller than the state space. The algorithm continues to learn and expand its knowledge of the critical region until enough confidence is gained that the policy it currently executes is good enough. This process will automatically terminate with some level of approximation for the critical region,  X  . Convergence occurs as proven in Corollary 3.9, which guarantees an approximately optimal policy with a reasonable amount of work. We have given a theory identifying the reward horizon as an integral parameter relating the reward structure to the speed of reinforcement learning. This relationship has been motivated by discussion, and proven for an algorithm that makes explicit use of the reward horizon. In this section, we explore the role of the reward horizon empirically, using standard Q-learning with an  X  -greedy exploration strategy. This simple exploration strategy makes no direct use of the reward horizon, and is only loosely guided by rewards. The following experiment tests the claim that lower reward horizons correspond to faster learning in a more general setting. Our experiment operates on a walking task for a bipedal mechanism simulation, as described in our earlier work on shaping (2002). The learning process attempts to maximize the distance traveled over a fixed time interval requiring approximately 30 actions per episode. Taking the best shaping function from our previous work, we vary the reward horizon by changing the delay between applying the average value of the shaping function over different intervals. The shaping function is accumulated over intervals of one, five, and fifteen, actions, and the average shaping reward is applied at the end of the interval. Because the shaping function is very successful, its feedback must be fairly accurate, and thus the interval size will correspond to the reward horizon of the problem. Figure 2 shows the learning curves as the reward horizon is varied. Each curve is the average of ten experiments. Each shaping strategy is given with a number indicating the delay (interval size) during which no shaping signal is applied until the last interval. Thus 5-shape has four actions without the shaping signal, and the fifth action receives the average shaping signal for all five actions. As the shaping signal is made successively more immediate from the no shape case to the 1-shape case, we observe accelerated learning. Specifically, if we observe the first episode for which distance walked averages at least 11m, 1-shape meets this performance after 1400 episodes, 5-shape after 2700, 15-shape after 4500, and No Shape, after 7000. This data verifies that algorithms that explore based on reward feedback have the potential to be accelerated by reducing the reward horizon. We have formulated an explanation of the potential of reward shaping to accelerate reinforcement learning with a reward-based analysis. The central parameter that characterizes the difficulty of a reward structure is the reward horizon. Given a reward horizon, we showed that a simple algorithm can learn an approximately optimal policy in polynomial time in all parameters except the reward horizon. This strong dependency upon the reward horizon demonstrates why shaping can be very powerful; reductions in the reward horizon result in large savings in the time it takes to learn. We are indebted to the other members of the Explanation-Based Learning group at Illinois: Mike Cibulskis, Arkady Epshteyn, Valentin Moskovich, and Qiang Sun, and to the three anonymous reviewers for helpful suggestions. This material is based upon work supported by the Office of Naval Research under Award No. ONR N00014-01-1-0063. Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of the Office of Naval Research. Abramowitz and Stegun. (1972). Handbook of Mathematical Functions , 9th edition. Dover Publications, New York. Dorigo, M., and Colombetti, M. (1993) Robot Shaping: Developing Situated Agents through Learning.
 Technical Report TR-92-040, International Computer Science Institute, Berkeley, CA. Hogg and Tanis. (2001). Probability and Statistical Inference . Prentice-Hall, New Jersey. Kearns and Singh. (1998) Near-optimal reinforcement learning in polynomial time. Proceedings of the Fifteenth International Conference on Machine Learning (pp. 260-268). Morgan Kaufmann, CA. Laud and DeJong. (2002). Reinforcement Learning and 
Shaping: Encouraging Intended Behaviors. Proceedings of the Nineteenth International Conference on Machine Learning (pp. 355-362). Morgan Kaufmann, CA. Mataric, M. J. (1994). Reward functions for accelerated learning. In Cohen, W. W. and Hirsh, H. (Eds.). 
Proceedings of the Eleventh International Conference on Machine Learning . Morgan Kaufmann, CA. Ng, A., Harada, D., &amp; Russell, S. (1999). Policy invariance under reward transformations: Theory and application to reward shaping. Proceedings of the Sixteenth International Conference on Machine Learning . Bled, Slovenia: Morgan Kaufmann. Randl X v, J., and Alstr X m, P. (1998). Learning to drive a bicycle using reinforcement learning and shaping. In Shavlik, J. (Ed.). Proceedings of the Fifteenth 
International Conference on Machine Learning (pp. 463-71). Morgan Kaufmann, CA. Tesauro, G.J. (1992). Practical issues in temporal difference learning. Machine Learning , 8, 257-277. Watkins, C.J.C.H. (1989). Learning from Delayed Rewards . Ph.D. thesis, Cambridge University. 
