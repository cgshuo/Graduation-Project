 We study in this paper the problem of bridging the semantic gap between low-level image featur es and high-level semantic concepts, which is the key hindrance in content-based image retrieval. Piloted by the rich textual information of Web images, the proposed framework tries to l earn a new distance measure in the visual space, which can be used to retrieve more semantically relevant images for any unseen query image. The framework differentiates with traditional di stance metric learning methods in the following ways. 1) A ranking-based distance metric learning method is proposed for image retr ieval problem, by optimizing the leave-one-out retrieval performance on the training data. 2) To be scalable, millions of images together with rich textual information have been crawled from the Web to learn the similarity measure, and the learning framework particularly considers the indexing problem to ensure the retrieval efficiency. 3) To alleviate the noises in the unbalanced labels of images and fully utilize the textual information, a Latent Dirichlet Allocation based topic-level text model is introduced to define pairwise semantic similarity between any two images. The learnt distance measure can be directly applied to applications such as content-based image retrieval and search-based image annotation. Experimental results on the two applications in a two million Web image database show both the effectiv eness and efficiency of the proposed framework. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  retrieval models Algorithms, Experimentation Content-based image retrieval, ranking-based distance metric learning, search-based image annotation With the prevalence of digital cameras and the Internet, there are more and more digital images on the Web. The explosion of digital images necessitates effective and efficient image retrieval techniques. Currently, there are mainly two image retrieval frameworks: text-based image retrieval and content-based image retrieval (CBIR). In traditional text-based systems, images are manually annotated by human labe lers and then searched using annotated keywords. The main di sadvantage of traditional text-based approach is that manually annotating large quantity of images is too tedious and time-co nsuming for common labelers. To overcome the disadvantages in traditional text-based image retrieval systems, content-based image retrieval was introduced in the early 1980s. In CBIR, images are indexed by their visual content, such as color, texture, and shapes. For comprehensive surveys on CBIR please refer to [12][14]. Although CBIR has been extensively studied since 1990s, the semantic gap between low-level image features and high-level semantic concepts is still the key hindrance in the effectiveness of CBIR systems. In contrast to billions of images that are freely available online, most of existing works are restricted to limited size of image dataset as well as limited textual labels. It will be very helpful if we can forward towards bridging the semantic gap in CBIR in a larger scale, i.e. Web-scale image set with unlimited textual information. In the past decade, with the prevalence of the Web, text-based Web image search engines such as Google have become more and more popular. Since Web images have rich metadata such as filename, URL and surrounding text for indexing and searching, different from traditional text-based approach, no manually labeling work is needed in current Web image search engines. The success of text-based image search engines has shown the power of textual information associated with Web images. However, we argue that, image search engines could be further improved if they can leverage vi sual information of images. We also see a huge potential of levera ging the textual information to reduce the semantic gap in CBIR on the Web. In this work, we target at the problem of bridging the semantic gap in content-based image retrieval on the Web. Generally, in a Web image database, both textual feature and visual feature can be extracted for each image. However, in many cases, a query image usually has no additional text ual information. Therefore, content-based image retrieval ha s to be conducted only in the visual feature space, but the performance is evaluated in the textual feature space. This motivates us to learn a new distance measure in the visual space to approximate the distance measure in the textual space. The learnt distance measure can be used in a This work was performed at Microsoft Research Asia. content-based image retrieval system to retrieve more semantically relevant images. This target is closely related to the goal of distance metric learni ng in machine learning field. Distance metric learning is to lear n a distance metric for the input space of data from a given co llection of labeled points that preserves the distance relation among the training data. For a comprehensive survey please refe r to [18]. In spite of many successful attempts, existing algorithms cannot be easily generalized to solve the problem we target at in this paper. First, we need to face the scalability pr oblem, i.e. unlim ited data points (millions of images) and unlimited classes (thousands of keywords). To build an online content-based image search engine, more aspects such as indexing need to be considered. The indexing strategy will be a st rong constraint to the learning algorithm. Second, most of existing works focus on multi-class problem, while what we are faci ng is a multi-label problem, in which an image could contain multiple objects and therefore be labeled as multiple classes. Third, most of existing works learn distance metrics from well labeled training set, while we directly regard the surrounding text of Web images as their textual labels, which affirmatively have more noi se. In fact, it is usually impossible to manually label or refine labels for millions of images. Finally, CBIR is essentially a ranking problem, rather than a classification problem as treated in traditional distance metric learning algorithms [9][15][18] . Thus, it will be helpful if we can design a ranking-based dist ance metric learning algorithm. In this paper, a novel content-ba sed image retrieval framework is framework, millions of images together with rich textual information have been crawled from the Web for experiments. This image collection is indexed based on K-means-based indexing method [8] using visual features. Second, to alleviate the noises in the unbalanced labels of images and fully utilize the textual information, a tf-idf based term-level text model i.e. cosine measure and a Latent Dirichlet Allocation [3] based topic-level text model are introduced to define pairwise semantic similarity between any two images and build semantic maps for each local model. Finally, to bridge the se mantic gap using the constructed semantic maps, a ranking-base d distance metric learning algorithm is proposed. In partic ular, different from traditional classification-based distance metric learning algorithms, to learn each local Mahalanobis distance metric, the algorithm directly minimizes the leave-one-out retrieval cost on the training set, based on a ranking-based probabilistic model. Moreover, to guarantee the comparability of different local models, a scale-invariant distance measure is used to make the cost function be scale-invariant. Furthermore, the learnt distance measure can be directly applied to applications such as content-based image retrieval and search-based image annotation (SBIA) [16] [17]. SBIA is differentiated from traditional image annotation me thods [1][2][4][7][11] by the utilization of search techniques as well as Web-scale data set. Different from existing SBIA met hods which simply isolate the (visual) search stage from the (textual) mining stage, the proposed annotation algorithm fully utilizes the textual information during both search and mining stages. Experimental results on the two applications in a two million Web image database show both the effectiveness and efficiency of the proposed framework. The remainder of this paper is organized as follows. In Section 2, we present the proposed CBIR fra mework as well as detailed algorithms. In Section 3, the proposed framework is applied to search-based image annotation pr oblem. Section 4 reports the experimental results. Finally, we c onclude this paper in Section 5. In this section, we present th e proposed CBIR framework. First, let us state the content-based Web image retrieval problem. In the training set  X  , we have N Web images  X  ={ I visual features  X  ={ x 1 ,..., x N }  X  X   X  and textual information  X  ={ d 1 ,..., d N }, where x i is a b  X 1 column vector and d the textual description (document) of the i -th image I define different features for a document d i based on different text models later. Thus, an image I i  X  X  can be represented by a without any textual information as I= ( x ,  X  ). The goal of CBIR is th at, given a query image I optimal retrieval system should return a ranking list  X  * that orders the images in  X  according to their relevance to the query image. This is a typical ranking problem. Thus, we need to create a ranking function f from the training set  X  to output ranking scores f ( I | I q ) for each image I i  X  X  given the query image I q There are several problems we need to tackle. First, in order to learn the ranking function f from the training set, we need to know the truly semantic similarity be tween images. To alleviate the noises in the textual information  X , we propose to leverage text models to measure the semantic similarity between different images. We will introduce this part of algorithms in Section 2.1. Second, we have to handle the scalability problem. Although it is theoretically resolvable to learn a global linear [9] or non-linear [15] algorithm, on the one hand, a single linear classifier cannot well separate the different classes in this complicated scenario; on the other hand, there is usuall y scalability problem using non-linear methods. A natural remedy is then to have an ensemble of locally adaptive models. In Secti on 2.2, 2.3 and 2.4, we present respectively the placement of m odels, the learning process of a single model i.e. the proposed ranking-based distance metric learning algorithm, and the fusion of the models. The framework of the proposed approach is illustrated in Figure 1. It mainly consists of four steps: 1. Calculate semantic similarity of the training set based on 2. Place local models. Input: training set  X  and text model  X  3. for k = 1: K 4. Fuse local models. Input: local models {  X   X  , k =1... K } and In this section, we introduce two text models to measure semantic similarity of images: a tf-idf based term-level model and a Latent Dirichlet Allocation base d topic-level model. Let us first define a vocabulary indexed by {1,..., v }. In this model, each document d  X  X  is represented by the a v -dimension term frequency  X  inverse document frequency ( tf-idf ) vector: d =( tfidf 1 , tfidf 2 , ..., tfidf v ), where tfidf i term in the vocabulary. The cosine measure between two documents d p and d q is defined as: Here we use d p and d p to represent a document and its feature. Although tf-idf scheme reduces documents of arbitrary length to fixed-length lists of numbers, the dimension of documents is still quite large and it reveals little in the way of inter-or intra-document statistical structure. To address the above shortcomings, researchers have proposed severa l other dimensionality reduction techniques, such as Latent Semantic Indexing (LSI) [6], probabilistic LSI (pLSI) [10], and Latent Dirichlet Allocation (LDA) [3]. We choose LDA as our topic-level text model due to its generalization ability in text mining and information retrieval. Latent Dirichlet Allocation (LDA) is a generative probabilistic model of a corpus. The basic idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. Two parameters  X  (prior Dirichlet parameter) and  X  (word probabilities given certain topic) are estimated by LDA. Through the inference, each document d i  X  X  is finally represented by a Dirichlet distribution over the mixture of la tent topics, with the optimized posterior parameter  X  i . For the details of LDA, please refer to [3]. In this topic-level model, two documents are considered to be relevant if they have similar dist ributions over latent topics. The most natural way to measure the similarity of two distributions is the Kullback-Leibler (KL) divergence. The KL divergence of two k -dimensional Dirich let distributions Dir (  X ; X   X  ) and Dir (  X ; X  where  X  X  X  X  X  is the Gamma function.  X  X  X  X  X  is the first derivative of log  X  X  X  X  X  , which can be approximated vi a Taylor expansion. As the KL divergence is asymmetry, the semantic similarity between two documents d p and d q is finally defined as:  X  X  X   X  X  X   X  X   X   X ,  X   X  X  X xp X  X  X  The estimated parameters  X  ,  X  , and document-specific parameters  X   X   X  X  X  X 1,...,  X  X  are kept in the text model  X   X  for calculating the similarity between two documents. Although some methods such as the Gaussian Mixture Model and Responsibility Mixture Model [5] have been proposed for isolating local structures, they still suffer from the efficiency problem and the indexing problem wh en dealing with millions of data with unlimited classes. For efficiency, we use a model placement scheme based on K-mean s-based indexing method [8]. Based on the K-means clustering al gorithm in the visual space, (clusters). Let us denote the sub visual feature set of the k -th cluster as  X   X  . We also denote the corresponding sub image set and sub textual document set as  X   X  and  X   X  respectively. For each sub document set  X   X  ( k =1,..., K ), a local semantic map S is constructed based on the text model  X   X  . By denoting the size of  X   X  as N k and denoting the p -th document in  X  obtain the N k  X  N k matrix S k as follows: where  X  X  X   X  X  X  X  X  X  X  X  X  X  X  represents  X  X  X   X  X  X  X  X  X  X  X  or  X  X  X   X  X  X  depending on the type of text model  X   X  . It should be noticed that the elements on the diagonal of the matrix S k have been set to be zero for succinct deduction in Section 2.3. The indexing information such as cluster center and membership is kept in the placement information  X   X  for further use. In this section, we describe th e proposed ranking-based distance metric learning algorithm in detail. As aforementioned, the input in this step is the sub image set  X   X  (with its size N semantic map S k . The output is the model  X   X  , which need to be learnt. To facilitate the deduction, we replace  X   X  , S  X  , S , and N in this section, respectively. Given a feature set  X  ={ x 1 ,..., x N }  X  X   X  and the pairwise semantic similarity map S , we want to learn a Mahalanobis (quadratic) distance metric M , which can always be represented by a symmetric positive semi-definite matrix. Let M = A T A , where A is a b  X  b matrix. We estimate M by learning a linear transformation A of the input space such that in the transformed space, the retrieval performance is good. Thus, for x , y  X   X  , we have need to design a ranking-based cost function to optimize the leave-one-out retrieval performance on the training set: where  X  X  X  X   X  X  X  X  X  X  X  X  X  X   X   X   X  is the retrieval cost of x considered as the query point (image). w q is a weight to indicate the importance of x q . We provide a probabilistic model for image retrieval problem. The basic idea is that there is uncertainty in the prediction of the ranking lists of images based on current visual distances of retrieved images to the query image. In other words, any possible ranking lists could be the final list with certain probability. Assume that we have a basic sample space  X   X  = {1, 2, ..., N }. Let  X   X  = { P 0 ( j ) =p j : j  X  X   X  } be a discrete probability distribution, where P 0 ( j ) represents the probability of randomly choosing sample j from  X   X  . We assume p j is strictly positive. Here comes the problem. That is, given  X   X  , how we can generate an n -length ranking list from the samples in  X   X  . Here we assume that there cannot be repeated samples in the ranking list. Let us define the n -order permutation sample space  X  any possible n -length ranking list sampled from  X   X   X  . That is, in a database having N samples, if we only consider the top n search results,  X   X  will represent all possible top n ranking lists. For example, if we set N =3, then  X   X  ={{(1), (2), (3)},  X  (2,1), (2,3), (3,1), (3,2)}, and  X   X  ={(1,2,3), (1,3,2), (2,1,3), (2,3,1), (3,1,2), (3,2,1)}. The n -order permutation probability distribution on the space  X   X  is denoted as  X   X  ={ P n (  X  ):  X   X  X  following expression for P n (  X  ) with simple deductions: where  X   X   X   X  represents the j -th sample in the ranking list  X  . randomly generated from the sample space  X   X  . Here we use the aforementioned samples in  X   X  = {1, 2, ..., N } to identify the images in the set  X  ={ x 1 ,..., x N }  X  X   X  image q , we could design the basic visual distance distribution  X  distance defined between images q and j according to current transformation matrix A . Thus, the elements {  X  } in the permutation sample space  X   X  become all possible n -length retrieved list according to query q . After fixing the distance measure Dist ( q , j ), any retrieved image list  X   X  X  returned to users with probability P n (  X  | q )  X  X   X   X  X  X  X  X  X  X  X  is the n -order permutation probability distribution defined based on  X   X   X  X  X  X  X  X  X  X  according to Equation 6. Similar to the basic visual distance distribution  X   X   X  X  X  X  X  X  X  X  also define a basic semant ic similarity distribution  X  { Q similarity defined between image q and j according to the normalized to satisfy the condition of a probability distribution. We could therefore obtain the n -order semantic permutation probability distribution  X   X   X  X  X  X  X  X  X  X  X  X  X  based on  X   X   X  X  X  X  X  X  X  X  X  X  X  Given a query q , the elements in  X   X   X  X  X  X  X  X  X  X  and  X  ranking lists of retrieved images m easured by visual distance and semantic similarity respectively. Notice that if a ranking list  X  has large semantic similarity probability Q n (  X  | q ), its visual distance probability P n (  X  | q ) should be suppressed. Thus, we design the retrieval cost as follows: Therefore, we have the following leave-one-out retrieval cost: Notice that the query itself in the re trieved list will be ignored by setting either  X   X  = 0 or Sim ( q , q ) = 0 in Section 2.3.2. Since we optimize the leave-one-out retrieval performance on the training set, all que ry images belong to  X   X  in training. Therefore,  X   X  X  X  X  X  X  X  X  ,  X   X   X  X  X  X  X  X  X  X  X  X  X  and w q are defined as follows: probability distribution. Here we assume that all elements in the above two distributions are positiv e. This assumption could be  X   X  1) if it is equal to zero, followed by a normalization stage to make  X   X   X  X  X  X  X  X  X  X  and  X   X   X  X  X  X  X  X  X  X  X  X  X  be probability distributions. In fact, the distance measur e defined in Equation 9 is a scale-invariant measure due to its independence of the scale of A . Let and Cost( A )=Cost(  X  A ). Due to space limitation, we omit the proofs of the above properties. The scale-invariant properties guarantee that different local dist ance metrics learnt from different local data sets are scale-comparable when we fuse local models in Section 2.4. It is easy to know that the size of  X   X  is N !/( N -n )!, which might be intractable in practice when n is large. For efficiency, in this paper, we implemented a 2-order algorithm by setting the order n to be 2. There will be some computational simplification since all query points are all from the same sample space  X   X  in training. We first calculate the leave-one-out retrieval cost and the gradient of the cost function with respect to the transformation matrix A . Then, we can use a gradient-based optimizer such as Quasi-Newton or conjugate gradients to optimize A . A will be kept in the local model  X   X  for further use. With some deductions , we can obtain: where  X   X  X  X   X  X  X  X  X   X   X , X   X  and  X   X  X  X   X  X  X  X   X   X , X   X  , which have been defined in Equation 9 and 10, respectively. It is apparent that  X  Cost ( A )/  X  A could be explicitly expressed as a function of A . Due to space limitation, we omit the expression of  X  Cost ( A )/  X  A. We can use conjugate gradients methods to obtain optimized transformation matrix A . Given the placement information  X   X  and K local transformation matrices A k associated with each local model  X   X  K }, we could obtain the final model  X  ={ f } under a probabilistic framework, where f ( I i | I q ) ourtputs the ranking score of I an unseen query image I q . For each i  X  X  ={1, ..., N }, the ranking function f is given as follows: where  X  X  X  X   X   X |  X   X  represents the probability of I q belonging to local model  X   X  , and  X   X   X  X   X   X  X   X  , X   X   X  represents the visually similarity score between  X   X  and  X   X  measured by the local model  X   X  X  X   X   X  X   X   X  and  X   X   X  X   X   X  X   X  , X   X   X  could be given as follows: where center k is the center of the k -th local cluster, and x visual feature of I q . It is easy to know that, for a large scale data set, there will be efficiency problem when we retrieve images exactly based on Equation 13. To be efficien t, we have to make some simplifications when we perform similarity search in our implementation. We define  X   X   X   X  ={the indices of the most n clusters in  X   X   X   X  }. First, given a query I q , we reduce the search scope from the whole similarity search. We could achieve it by adding the constraint i  X  X   X   X   X  in Equation 13 and replacing  X  X  X  with  X  X  X  Equation 14. Second, we assume the relation between an image I model. We could conduct this simplification by replacing  X  X  X  In this implementation, we set n m and n respectively by considering both the effectiveness and efficiency. Furthermore, for the efficiency of online search, the features in the transformed space, i.e.  X   X   X   X  (for i  X  X  ,  X  X  X   X  offline calculated and indexed. Notice that by setting n only one additive transformed feature needs to be stored for each visual feature. We could also rem ove the original features to save storage space. All transformed features are indexed in the original K-means-based indexing framework to speed up the online search. Therefore, given a query image I q , the system first uses the ranking function f to output a score  X  X  X   X   X  |  X  the image database (or reduced database), and then returns the reordered images to users according to their ranking scores. Image annotation is an active re search topic in CBIR. Among various approaches, search-based image annotation (SBIA) has demonstrated its potential of so lving image annotation problem by leveraging search techniques as well as Web-scale data set. Existing SBIA works [16][17] differentiate with traditional image annotation methods in the followi ng ways: 1) Web-scale data set for training, 2) unlimited lexicon for annotation, and 3) real-time speed. In SBIA, image annotation problem is formulated in two steps: 1) searching for visually similar images us ing content-based image retrieval (CBIR), and then 2) mining annotations from the textual information of the retrieved images. In spite of the strongpoint compared with traditional image annotations methods, th e effectiveness of current SBIA algorithms is constrained to some extent, since the CBIR stage is simply information is utilized when CBIR is conducted. We argue that the rich textual information could help find semantically relevant images, since existing SBIR algorithms rely on it in the annotation mining stage. Therefore, we directly extend the proposed CBIR algorithm to the search-based image annotation framework to solve image annotation problem. Two novel SBIA algorithms using term-level and topic-level text models are implemented, which are named as SBIA-RDML-term and SBIA-RDML-topic respectively. Here RDML represents Ranking-based Distance Metric Learning. In SBIA-RDML-term , the distance metric learnt based on term-level text model as introduced in Section 2 is used to retrieve similar images. The annotation mi ning stage could be the same as existing SBIA algorithms, since they are all term-level mining. In SBIA-RDML-topic , the CBIR process is similar to SBIA-RDML-term , except that the topic-level text model is used to learn the distance metric. In the annot ation mining stage, we provide a topic-level mining scheme. Recal l that in LDA model, through the inference, each document d i  X  X  is finally represented by a Dirichlet distribution over the mixture of latent topics, with the optimized posterior parameter  X  i . In LDA, we can consider that the j -th element in  X  i is in proportion to the probability of document d i belonging to topic z j [3], denoted as P ( z assume that the parameter  X  in LDA (see Section 2.1.2) is stored as a v  X  k matrix, where v is the size of the vocabulary and k is the number of topics. Thus, the ( m, j )-th element in  X  is just the probability of z j containing word t m [3], denoted as P ( t we can provide the score of a word t m given both the query image I and the similar image set  X   X  X  X  X  X  retrieved by the CBIR stage:  X  X  X  X  X   X  X  X  X   X  X   X   X  X   X  , X   X  X  X  X  X   X  X  X  X   X  X   X   X  |  X   X  X  X  X  X   X   X  X   X  We can choose the top several word s with the highest scores as the final annotations. In this section, we evaluate the framework on content-based image retrieval and search-based image annotation problems. In this experiment, we try to re duce the semantic gap in content-based image retrieval by leveraging text information of images. We have collected more than 2 million images from several photo forum sites, e.g. photosig 1 , as the image database. Images of such sites have rich textual information, such as title and http://www.photosig.com photographer X  X  description. As shown in Figure 2, the textual information reflects the content of the image to some extent. To represent an image, a 64 dimensional color feature [16] was extracted. The whole dataset was indexed to be 2000 clusters according to visual features. U. Washington dataset 2 (UW) is a content-based image retrieval database, which has been widely used in CBIR and SBIA works [13][16][17]. For the sake of evaluation, we also choose this collection as the query image se t. There are 1109 images and each image contains about 5 manually labeled annotations as ground truth on average. The images in UW are used as query images to retrieve semantically relevant images in the crawled Web-scale database. Since it is impractical to manually label millions of images in the training set, we need to design a measure to evaluate the retrieval results based on the textual info rmation of retrieved images. Based on different text models introduced in Section 2.1, two precision measures are used to evaluate image retrieval results.  X  X  X  X  X  X  X _ X  X  X   X   X  X  X  X  X  X  X  X _ X  X  X   X  where  X   X  represents the query image set, and  X   X  images retrieved by certain CBIR algorithm given query image I d is the ground truth annotations of I i and  X  semantically relevant if they have at least one identical label, while pre_topic assumes that two images are relevant if they have similar distributions over latent topics. In this implementation, we set the topic number to 100 when the LDA model was trained. As mentioned in Section 1, existing distance metric learning algorithms cannot be directly generalized to solve the problem we are facing, due to the scalability problem and the noises in the rich textual labels. However, to ev aluate the proposed ranking-based distance metric learning algorithm, we have developed a localized Neighbourhood Components Analysis (LNCA) algorithm, which is the same as the proposed al gorithm with term-level model, except that the local transformation matrix A is learnt by the Neighbourhood Components Analysis (NCA) algorithm [9]. Therefore, four algorithms were considered and compared: 1) the proposed ranking-based distance metric learning algorithm with term-level model ( RDML-term ), 2) the proposed algorithm with topic-level model ( RDML-topic ), 3) CBIR without distance metric learning ( NoDML ), and 4) CBIR using the LNCA algorithm. The retrieval performance of di fferent algorithms is shown in Figure 3 and 4. Four conclusions could be drawn from the results. First, the performance of LNCA is comparable or even worse than that of NoDML . One explanation for the poor performance of LNCA may be that, the NCA algorithm used for learning local transformation matrices focuses on classification problem rather than ranking problem, which results in the use of a scale-sensitive http://www.cs.washington.edu/res earch/imagedatabase/groundtru th/ cost function (based on softmax function). Although it works well in a single dataset [9], it will fail when thousands of locally unbalanced distance metrics are fused together. In this case, a cluster with very small scale transformation matrix may dominate the retrieval process, since the distances of the query image to the images in this cluster are more likely to be smaller than those in other clusters, measured by co rresponding problematic metrics. Second, both RDML-term and RDML-topic outperform other algorithms evaluated by both of the two measures. The scale-invariant measure and ranking cost function could avoid the unbalance of local distance metrics, since the change of the scale of the transformation matrix will not influence the ranking cost therefore as well as the optimized matrix itself. Third, RDML-term and RDML-topic outperform each other when measured by the corresponding text model as they used in training. It is reasonable that the algorithm will perform well, when the objective of learning is consistent with the evaluation measure. Fourth, the performance measured by cosine measure ( Sim cosine much worse than that measured by LDA model Sim LDA the cosine measure is a very strict measure which will decrease the performance a lot when either of the compared documents is long. On the other hand, Sim LDA does not restrict that two documents should have ce rtain identical terms. In CBIR process, the two text models, i.e. term-level model and topic-level model, produce two ki nds of semantic measures for images as ground truth. It is not a trivial task to evaluate the two text models in CBIR process its elf. We will provide some comparisons of them in image an notation problem in Section 4.2. The LDA model, semantic ma ps and local transformation matrices are all built and optimized offline. The LDA model cost about 2.5 hours on a cluster consis ting of nine computers with 3.00 GHz Intel Xeon hyper-threade d CPU and 8 G memory. The semantic maps and local transformation matrices cost about 3 days in a single computer wit hout special optimization. The average online retrieval time is less than 0.1 second for a query image on a single computer, leav ing out of account the feature extraction and image display processes. We apply the proposed CBIR fram ework to search-based image annotation problem as described in Section 3. As existing SBIA works, UW dataset is used as the query image set, and the crawled Web-scale image set is considered as the training database. Annotation precision and recall were used as evaluation measures: where m is the number of returned annotations. correct ( I number of correctly annotated words for the testing image I automatic ( I i ) is the number of automatically annotated words for I g roundtruth ( I i ) is the number of ground truth annotations of I A SBIA algorithm named as SBIA-NoDML is developed as a baseline, in which textual information is not used in the CBIR stage, as the same as in [16][17]. In the implementation, we also fix the number of retrieved images to be 500, since it is the propositional number in [16] due to its good performance for the noisy Web-scale dataset. In the annotation mining stage, the term frequency in the textual descriptions of the mo st visually similar images is used to rank annotations. By replacing the CBIR process with LNCA algorithm in SBIA-NoDML , a new SBIA algorithm named as SBIA-LNCA is also developed. Two proposed SBIA algorithms i.e. SBIA-RDML-term and SBIA-RDML-topic as described in Section 3 are compared with the former two ones. We also implemented a mixed version of SBIA-RDML-term and SBIA-RDML-topic , which use the topic-level model to help the CBIR process, and use the term-l evel model in the annotation mining process. We denote it as SBIA-RDML-topic&amp;term . Figure 5 and Figure 6 show the comparison results. We could draw the following conclusions from the two figures. First, SBIA-LNCA cannot benefit the annotation results compared with SBIA-NoDML , since LNCA does not work well in the CBIR process. Second, all the three methods SBIA-RDML-term , SBIA-RDML-topic , and SBIA-RDML-topic&amp;term significantly outperform SBIA-NoDML , which has shown the significant benefit provided by applying the proposed framework to SBIA problem. Third, Figure 3. Retrieval precision measured by term-level model 
Figure 4. Retrieval precision measured by topic-level model both SBIA-RDML-topic and SBIA-RDML-topic&amp;term outperform SBIA-RDML-term , which indicates that the semantic measure based on topic-level model may be more reasonable than the cosine measure based on term-level model. Since cosine measure cannot distinguish polysemy or synonymy, it will consider two semantically irrelevant images with the same polysemous word as relevant, while deem two semantic ally relevant ima ges without an identical word as totally irrelevant. On the other hand, the topic-level model, i.e. L DA model, may provid e a more reasonable semantic similarity measure by projecting documents in the original term space to the topic space. Figure 5 and 6 also show that, SBIA-RDML-topic&amp;term is not so powerful as SBIA-RDML-topic , although it does outperform SBIA-RDML-term a little. This result exhibits that it is very useful to utilize topic-level text model during annotation mining process, if the semantic maps are built upon topic-level model in the former CBIR process. In this paper, we have presente d a novel framework to reduce the semantic gap in large scale content-based image retrieval and annotation. To be scal able, millions of images together with rich textual information were crawled from the Web for experiments. To alleviate the noises in the unbalanced labels of Web images and fully utilize the textual information, a Latent Dirichlet Allocation based topic-level text model was introduced to define pairwise semantic similarity between any two images. Piloted by the rich textual information of Web images, a novel ranking-based distance metric learning method was proposed to learn a new distance measure in the visual space, which could be used to retrieve more semantically rele vant images for any unseen query image. The proposed framework was directly applied to content-based image retrieval and s earch-based image annotation problems. Experimental results on the two applications in a two million Web image database showed both the effectiveness and efficiency of the proposed framework. Changhu Wang is supported in part by the National Nature Sciences Foundation of China 60672056. [1] Barnard, K., Duygulu, P., Forsyth, D., de Freitas, N., Blei, D. [2] Blei, D. M. and Jordan, M. I. 2003. Modeling annotated data. [3] Blei, D. M., Ng, A. Y., and Jordan, M. I. 2003. Latent [4] Carneiro, G., et al. Supervised learning of semantic classes [5] Dai, J., Yan, S., Tang, X. a nd Kwok, J. T., Locally adaptive [6] Deerwester, S., et al. Indexing by latent semantic analysis. [7] Feng, S. L., Manmatha, R., and Lavrenko, V. Multiple [8] Ferhatosmanoglu, H., Tuncel, E., and etc. Approximate [9] Goldberger, J., Roweis, S., Hi nton, G., and Salakhutdinov, R. [10] Hofmann, T. Probabilistic latent semantic indexing. In [11] Lavrenko, V., Manmatha, R., and Jeon, J. "A model for [12] Lew, M. S., et al. Content-based multimedia information [13] Muller, H., et al. Learning from user behavior in image [14] Smeulders, A. W. M., et al. Content-based image retrieval at [15] Torresani, L. and Lee, K. Larg e margin component analysis. [16] Wang, C., Jing, F., and et al. Scalable search-based image [17] Wang, X. J., Zhang, L., Jing, F., and Ma, W. Y. AnnoSearch: [18] Yang, L. and Jin, R. (2006). Distance metric learning: A 
