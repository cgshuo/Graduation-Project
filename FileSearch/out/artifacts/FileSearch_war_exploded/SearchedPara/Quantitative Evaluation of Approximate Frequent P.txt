 Traditional association mining algorithms use a strict defi nition of support that requires every item in a frequent itemset to occ ur in each supporting transaction. In real-life datasets, this l imits the recovery of frequent itemset patterns as they are fragmente d due to random noise and other errors in the data. Hence, a number of methods have been proposed recently to discover approxim ate frequent itemsets in the presence of noise. These algorithm s use a relaxed definition of support and additional parameters, s uch as row and column error thresholds to allow some degree of  X  X rro r X  in the discovered patterns. Though these algorithms have be en shown to be successful in finding the approximate frequent it em-sets, a systematic and quantitative approach to evaluate th em has been lacking. In this paper, we propose a comprehensive eval ua-tion framework to compare different approximate frequent p attern mining algorithms. The key idea is to select the optimal para meters for each algorithm on a given dataset and use the itemsets gen erated with these optimal parameters in order to compare different algo-rithms. We also propose simple variations of some of the exis ting algorithms by introducing an additional post-processing s tep. Sub-sequently, we have applied our proposed evaluation framewo rk to a wide variety of synthetic datasets with varying amounts of noise and a real dataset to compare existing and our proposed varia tions of the approximate pattern mining algorithms. Source code a nd the datasets used in this study are made publicly available.
 H.2.8 [ Database Management ]: Database Applications X  Data Min-ing ; D.2.8 [ Software Engineering ]: Metrics X  performance mea-sures ; I.5 [ Pattern Recognition ]: Miscellaneous Algorithms, Experimentation, Performance, Verification Association analysis, approximate frequent itemsets, err or toler-ance, quantitative evaluation
Traditional association mining algorithms use a strict defi nition of support that requires every item in a frequent itemset to o ccur in each supporting transaction. In real-life datasets, thi s limits the recovery of frequent itemset patterns as they are fragmente d due to random noise and other errors in the data.

Motivated by such considerations, various methods [11, 7, 8 , 6, 5, 2] have been proposed recently to discover approximate fr equent itemsets (often called error-tolerant itemsets (ETIs)) by allowing itemsets in which a specified fraction of the items can be miss ing. Please see figure 1 for a conceptual overview. The most basic a p-proach is to require only that a specified fraction of the item s in a collection of items and transactions be present. However, s uch a  X  X eak X  ETI [11] provides no guarantees on the distribution o f the items within this  X  X lock, X  i.e., some rows or columns could b e com-pletely empty. To address this issue, a  X  X trong X  ETI was defin ed [11], which required that each row must have at most a specifie d fraction of items missing. The support of strong ETIs is simp ly the number of transactions that support the pattern, as in th e tradi-tional case, but support does not have the anti-monotone pro perty, i.e., support can increase as the number of items increases. Thus, a heuristic algorithm for finding strong ETIs was developed.
Indeed, the lack of an anti-monotone support measure is one o f the factors that has made the construction of algorithms for finding approximate itemsets very challenging. One solution that l eads to an anti-monotone support measure is to allow only a fixed numb er of missing items in each row [7]. This approach does not enfor ce any constraint on the number of items missing in a column, and is unappealing in that bigger itemsets should be allowed more m iss-ing items than the smaller ones. Another potential solution is an approach that uses  X  X ense X  itemsets [8]. The support of an ap prox-imate itemset is defined in terms of the minimum support of eve ry subset and is anti-monotone, but one may argue whether the de fini-tion of an approximate pattern used by this approach is as app ealing as some of the other definitions since different subsets of it ems may be supported by different transactions.
 Both strong and weak ETI patterns can have empty columns. To handle this situation, approximate frequent itemsets (A FI) [5] were proposed. AFIs enforce constraints on the number of mis s-ing items in both rows and columns. One of the advantages of AFIs over weak/strong ETIs is that there is a limited version of an anti-monotone property that helps prune the search space. H ow-ever, this algorithm cannot guarantee that no AFIs are overl ooked since a heuristic approach is used to verify the column const raint. Although AFIs might seem like the most natural and complete d ef-inition of an approximate itemset pattern, one might argue t hat it is easier and perhaps more meaningful to find approximate items ets if at least some transactions contain all the items comprisi ng an approximate itemset. This formed the motivation for AC-CLO SE, which finds AFIs based on the notion of core itemsets [2].
Although these approaches have shown to be successful in find -ing the approximate frequent itemsets, a systematic and qua ntita-tive approach to evaluate the patterns they generate is stil l lacking. Indeed, most of the papers on approximate itemsets have prov ided only limited comparisons with other work: The paper that int ro-duced AFIs [5] only presented a comparison to strong ETIs [11 ], while AC-CLOSE [2] only compared against AFIs [6, 5].

Contributions of this paper: The source codes of all the algorithms as well as synthetic an d real data sets used in this study are available at the following we bsite: www.cs.umn.edu/  X  kumar/ETI/ .

Organization: The remainder of the paper is organized as fol-lows. In Sections 2 , we briefly review the existing algorithms for approximate frequent pattern mining and we also propose sim ple variations of some of them. Section 3 gives the details of our pro-posed evaluation methodology, including the definition of m easures and how to select optimal parameters for each algorithm on a g iven dataset. We discuss our experimental results in Section 4 an d con-clude with final remarks and future work in Section 5.
Below, we briefly define different generalizations of freque nt itemsets, which can generally be viewed as a hierarchy as sho wn in Figure 1. It is important to note that each of these definiti ons Figure 1: A conceptual hierarchy of approximate pat-terns/algorithms will lead to a traditional frequent itemset if their error-t olerance is set to 0 . However, first we define a set of common terminology. Assume, we have a binary transaction database consisting of a set I= { i 1 , . . . , i m } of items, and a set T= { t 1 , . . . , t where each t i has a subset of items from I . It is useful to think of such a database as a n -by-m binary matrix D , with each column corresponding to an item, and each row corresponding to a tra ns-action. Thus D j,k = 1 if i k  X  t j , and 0 otherwise. An itemset (or of transactions it appears in, |{ T  X  : I  X   X  T  X  }| ), is more than some user-specified threshold denoted by minsup .
The concept of weak and strong ETI was defined in [11]. An itemset I  X  at support minsup is said to be a weak ETI with tol-erance  X  if  X  T  X   X  T such that | T  X  | X  minsup and the following
It is difficult to find all weak ETIs, since we effectively have to search the entire pattern space without any pruning. Also, w e can have both rows and columns included that consist entirely of zeros, since there is no constraint as to where the 0 X  X  can occur with in the itemset. An itemset I  X  at support minsup is said to be a strong ETI with tolerance  X  if  X  T  X   X  T such that | T  X  | X  minsup
This implies that for a given set of parameters, any strong ET I is also a weak ETI (figure 1). Also, the definition of strong ETI helps to eliminate the possibility of adding spurious trans actions. A greedy approach for computing strong ETIs is also proposed [ 11].
This algorithm was developed by Sepp X nen and Mannila and named (somewhat misleadingly) dense itemsets [8]. The idea was to add a recursive condition to the definition of a weak ETI in o rder to overcome weak ETIs inherent susceptibility to spurious i tems. transactions for the subset of items do not necessarily need to be the same. While this may seem to be a drawback, it still guarantee s that any set of items within the itemset are related to one another . This algorithm also has an advantage of apriori-like pruning mea ning if an itemset is not a recursive weak ETI, no superset of it can po ssi-bly be a recursive weak ETI. We denote this algorithm as  X  X W X  i n this paper (see figure 1 for its relationship to other algorit hms).
The concept of an Approximate Frequent Itemset (AFI) was de-veloped in [5], although it was earlier introduced in [9]. Th e idea is to extend the concept of strong ETI to include separate row an d col-umn constraints (  X  r and  X  c respectively). An itemset I minsup is said to be an AFI with tolerance  X  r and  X  c if such that | T  X  | X  minsup and the following two conditions hold:
From the above properties, it can be seen that AFIs are a sub-set of strong ETIs (see figure 1). One of the advantage of AFIs over weak/strong ETIs is that a relaxed version of an anti-mo notone property holds for this pattern.
AC-CLOSE [2] uses a core pattern constraint in addition to ro w (  X  ) and column (  X  c ) error thresholds to find frequent approximate patterns. Specifically, this algorithm uses a parameter trol the percentage of supporting transactions that must ha ve all the items in an itemset. This essentially further filters out pat terns gen-erated by the algorithm  X  X FI X  (see figure 1). In [2], an efficie nt top-down mining algorithm was also proposed to discover app roxi-mate frequent patterns with core patterns as the initial see ds. In this paper, we apply the additional parameter  X  as a post-processing step to  X  X FI X  patterns to obtain  X  X C-CLOSE X  patterns. Conce ptu-ally, the patterns should be equivalent to the original AC-C LOSE, our implementation is not as efficient as the original  X  X C-CL OSE X  implementation.
In order to overcome the possibility that  X  X ETI X  can pick spu -rious items, we propose a variation of  X  X ETI X  which uses an ad di-tional parameter (  X  c ) to make sure every item in each ETI generated by the algorithm  X  X ETI X  also satisfies the column constraint .
In order to overcome the drawback of both strong ETIs and re-cursive weak ETIs, we added a post-processing step in algori thm  X  X W X  to make sure that all recursive weak ETIs also meets a cer -tain column threshold (say  X  c ). Hence, patterns generated using  X  X W-PP X  lie in the intersection of  X  X W X  and strong ETI (figure 1).
Since the recursive weak definition seems to work well in prac -tice [8], a natural step is to define a recursive strong ETI, wh ere each subset must also be a recursive strong ETI (figure 1).
The evaluation approach given in [3] compares the result qua lity of different noise-tolerant models using precision and rec all. An-other evaluation framework for frequent sequential patter n mining [4] considers recoverability, spuriousness, redundancy a nd num-ber of extraneous items as quantitative measures and influen ced the design of our evaluation framework. Building upon the de f-inition of recoverability and spuriousness given in [4], be low we describe the evaluation measures in more detail. Note, we de fine B = { b 1 , . . . , b m } to be the set of base itemsets ( X  X rue X  patterns) and F = { f 1 , . . . , f n } to be the set of found patterns.
This quantifies how well an approximate pattern mining algo-rithm recovers the base patterns. In this definition, recove rability is similar to recall. To measure the recoverability of the tr ue pat-terns using the found patterns, we create a matrix of size fb ij , i.e, the number of common items in found pattern base pattern B j . We consider the recoverability of any base pattern B j to be the largest percent of the itemset found by any pattern that is associated with B j . For illustration purposes, as shown in table 1, for each base pattern (each column in table 1), we put a  X  on an entry which is maximum in that column. If there is a tie among found patterns for the maximum value, we put  X  on mul-tiple entries. For computing the recoverability of the base pattern B , we take any entry with a  X  on it and divide it by the size of When we have more than one true pattern in the data, we need to combine the recoverability for each pattern into one measur e. This is done by taking a weighted average (bigger patterns count m ore than smaller patterns) over all base patterns.
Although recoverability gives a good idea of what fraction o f patterns are recovered by an algorithm, it does not give a com plete picture. It is possible that a pattern is recovered solely be cause a found pattern contained a large number of items, not all nece ssarily related to the base pattern. Thus just as precision is comple men-tary to recall, we need another sibling measure of recoverab ility (recall) that measures the quality of the found patterns. Th e quality of a pattern can be estimated using the spuriousness measure which computes the number of items in the pattern that are not assoc iated with the matching base pattern (i.e., are spurious). Hence, preci-sion of a found pattern can be defined as 1  X  spuriousness illustration purposes, as shown in table 1, for each found pa ttern (each row in table 1), we put a # on an entry which is maximum in that row. If there is a tie among base patterns for the maxim um value, we put a # on multiple entries in a row. For computing the spuriousness of the found pattern F i , we take any entry with a numerous found patterns, the spuriousness of a set of found p at-terns is equivalent to the number of spurious items over tota l items found. spuriousness ( F ) = Table 1: Illustration of the matrix formed by found patterns and base (or true) patterns
Based on the two measures, recoverability and spuriousness that are defined above, we define a measure called  X  X ignificance of t he found patterns X  that combines the two just as F-measure comb ines precision and recall [10]. significance ( F ) = This measure balances the trade-off between useful and spur ious information in the generated patterns.
As mentioned above, many approximate pattern mining algo-rithms produce huge number of patterns that are often a small vari-ation of one another. Hence, it is important to quantify how m any of the found patterns are actually useful. For example, an it emset of size 10 has 45 subsets of size 8. If we recovered all of these , we would have a recoverability of 0.8, and a spuriousness of 0, b ut we would quickly be overwhelmed by the number of patterns. To me a-sure the extent of the redundancy in the found patterns, we cr eate a matrix R of size | F | X | F | , whose ij th element ( i column) is represented as ff ij , i.e, the number of common items in patterns F i and F j . We then take the sum of the upper triangular matrix R excluding the diagonal to estimate the redundancy of a set of patterns. redundancy ( F ) =
Note, we do not take the average in the computation of redun-dancy to differentiate between the algorithms that generat e unequal number of patterns but have the same average pairwise overla p of items. Hence, this definition of redundancy indirectly take s into account the number of patterns generated by a mining algorit hm.
It is also important to compare approximate pattern mining a l-gorithms based on their sensitivity to the input parameters . Some of the algorithms are more sensitive to these parameters tha n oth-ers and the quality of the patterns changes drastically if th e optimal parameters are not used. Another parameter that these algor ithms are usually sensitive to is the percentage of noise in the dat a. Ide-ally, an evaluation framework should evaluate the algorith m not only on the basis of the quality of patterns (based on signific ance which is defined as the combination of recoverability and spu rious-ness) but also on the basis of the size of the parameter space f or which this algorithm generates patterns of acceptable qual ity. This parameter sensitivity analysis quantifies the robustness o f an algo-rithm, which is a very important criterion as optimal parame ters for a real life dataset will not be known. To do this, we explor e a reasonable three-dimensional parameter space of support thresh-old ( minsup ), row constraints (  X  r ) and column constraint ( each algorithm and choose the top k % combinations of parameters for which the values of the significance measure are highest. We then take the mean and variance of these top k % significance val-ues. While the mean denotes the performance of the algorithm in terms of quality of the patterns, variance denotes how sensi tive it is to the selection of parameters. Ideally, one wants the mea n to be high and variance to be low. However, in real-life applica tions, the unavailability of ground truth makes it inconceivable t o obtain optimal parameters. Hence, one would like to choose an algor ithm that consistently shows low variance (high stability) in to p nificance values even at some cost of pattern quality, when te sted on synthetic datasets.
As mentioned earlier, different approximate pattern minin g algo-rithms generate best results on different optimal paramete rs. To do a fair comparison among them, it is very important to compare the results obtained by each on its own optimal parameters setti ng. De-pending on the application, one may use different evaluatio n mea-sures to choose optimal parameters. For a given dataset, we d efine the optimal parameters for an algorithm to be the ones that gi ve the best value of significance. In case there is a tie among mul tiple parameter combinations, any parameter combination can be u sed. Figure 2: Images of the base (no noise) synthetic datasets us ed in the experiments. All datasets have 1000 transactions and 50 items.
We implemented the algorithms  X  X FI X  and  X  X ETI X  and used the publicly available version of  X  X W X . We also implemented the vari-ations of  X  X ETI X  and  X  X W X  as discussed earlier. For  X  X C-CLOS E X , we use the patterns generated by  X  X FI X  and identify the ones t hat also have a core (completely dense) block of at least  X   X  minsup support. Though we also implemented and tested the algorith m  X  X S X , we exclude its results because performance is general ly worse than  X  X W X . We then apply our proposed evaluation framework o n both synthetic and real datasets to compare the performance of these approximate pattern mining algorithms. Synthetic da tasets are used because it is easier to evaluate the merits/demerit s of dif-ferent algorithms when the ground truth is available. We als o used zoo dataset [1] as an example of a real dataset since a number of previously published studies have also used zoo dataset [ 5, 3]. Moreover to avoid the problem of huge number of patterns, we o nly compare the maximal patterns generated by different algori thms.
Various synthetic datasets were generated keeping in mind d if-ferent characteristics of a real-life dataset. These chara cteristics in-clude: 1) Noise : Almost all real-life datasets are noisy and finding true patterns but not just the confounding groups of noisy at tributes is a non-trivial task; 2) Types of Patterns : There are variety of pat-terns that a real-life data set may have depending on the appl ication domain. For example, patterns may be overlapping (either in items or in transactions or both) or non-overlapping, of differen t sizes (in terms of number of items or in terms of transactions).

We generated 8 synthetic datasets (shown in figure 2) based on the above data characteristics. All the generated datasets have items and 1000 transactions. Note that the datasets shown in figure 2 are only the base datasets with 0% noise level but we also gen-erated noisy versions of each of them by uniformly adding ran dom noise in fixed increments. Following is the brief descriptio n of each of the synthetic datasets.

Data 1 -Single embedded pattern of 10 items with a support of 200 . Data 2 -Two non-overlapping embedded patterns of 6 items with a support of 200 and 150 respectively. Data 3 -Two em-bedded patterns of 6 items each ( 3 overlapping items) with a sup-port of 200 and 150 respectively. Data 4 -Two embedded patterns of 6 and 5 items and with a support of 200 and 150 respectively ( 50 overlapping transactions). Data 5 -Two embedded patterns of 6 items each ( 3 overlapping items) with a support of 200 and respectively ( 50 overlapping transactions). Data 6 -Three embed-ded patterns of 6 , 5 and 6 items with a support of 200 , respectively. While patterns 1 &amp; 2, and patterns 2 &amp; 3 overlap in 2 items and 50 transactions, there is no overlap of either items or transactions in patterns 1 &amp; 3. Data 7 -Four embedded patterns of 5 , 4 , 4 and 3 items with a support of 200 , 150 , 100 and respectively. Patterns 1 &amp; 2 overlap in 2 items and 50 patterns 3 &amp; 4 overlap in 1 item and 50 transactions, and patterns 2 &amp; 3 overlap in 50 transactions but no items. Data 8 data 7 except that the patterns are generated in a different way. Al l the rows and columns in the data are randomly shuffled before a new pattern is embedded. Figure 2 shows the shuffled data afte r patterns are embedded.

Given a base (no noise) synthetic dataset, we first add random noise by flipping its elements with a probability of means that (100  X  n )% is the probability of any element remain-ing constant. We vary the value of n to obtain noisy versions of the base synthetic dataset. We then run the algorithms on eac h of the noisy dataset using a wide range of parameters like sup port threshold ( minsup ), row (  X  r ) and column (  X  c ) tolerance. While we use 0, 0.05, 0.10, 0.20, 0.25 and 0.33 as 6 different values for row (  X  ) and column (  X  c ) tolerance; the range of minsup is selected based on the size of the implanted true patterns in the synthe tic data. Moreover, as the noise is random, we repeat the complet e process of adding noise and running the algorithms on all pos sible parameter combinations 5 times and report average results. To give an illustration of the computational complexity, consider applying the AFI, which uses both row (  X  r ) and column (  X  c ) tolerance, on a single dataset. To cover the parameter space defined by nois e, support threshold ( minsup ), row (  X  r ) and column (  X  c we need to run this algorithm 5 (number of runs) x 5 (assuming 5 different noise levels) x 5 (assuming 5 different values of sup-port parameter) x 6 ( # of values of  X  r ) x 6 ( # of values of = 4500 times. As the true patterns for all synthetic datasets are known, we run all the algorithms on a wide range of parameters to select the best combination (refered to as the optimal param eters) for each. The performance of the algorithms is then compared with each other using optimal parameters.
Due to the space constraints, we only show results on synthet ic data 6 (figure 3) and synthetic data 8 (figure 4). Results on other datasets are similar and are available on the website that co ntains all the source codes and the datasets (see section 1). Also, T ables 2 and 3 shows the optimal parameters selected by each algorit hm for these datasets at different noise levels. Sometimes the re are multiple parameter values for which the generated patterns show the same performance measured in terms of significance. In su ch cases, we show the parameter combinations corresponding to min-imum and maximum support within such cases. Noise level is va r-ied from 0% to 16% in increments of 4% for synthetic data from 0% to 8% in increments of 2% for synthetic data 8 . Again, the maximum amount of noise level introduced in the data is gover ned by the size of the smallest implanted pattern.

In both figure 3 and figure 4, we can see that the performance of the  X  X PRIORI X  algorithm (measured in terms of significanc e) falls most rapidly as the random noise in the data increases. Al-though as seen from table 2,  X  X PRIORI X  uses low support thres hold ( minsup = 100 ) for all noise levels to recover true patterns, due to the rigid definition of support, overall recoverability a nd hence significance is low.

Generally speaking, the performance of all the algorithms f alls as expected when the random noise is increased in the data. As the noise increases, recoverability goes down, spuriousne ss goes up and as a result, the significance of the patterns goes down. Al-though, every algorithm chooses optimal parameters corres pond-ing to the best value of significance, the effect of random noi se is different on each algorithm. Broadly, these algorithms cou ld be di-vided into two groups: one that uses single parameter  X  and one that uses two parameters  X  r and  X  c . Usually, single parameter al-gorithms  X  X W X  and  X  X ETI X  pick more spurious items than those that uses two parameters. This is because these single param eter algorithms only require each supporting transaction to hav e at least (1  X   X  r ) fraction of items. They do not impose any restriction on the items in the column. On the other hand, algorithms  X  X FI  X ,  X  X C-CLOSE X  and our proposed variations  X  X ETI-PP X  and  X  X W-P P X  pick fewer spurious items and hence have a better significanc e val-ues. AFI uses two parameters  X  r and  X  c and hence additionally requires each item in a pattern to be supported by at least fraction of total supporting transactions.  X  X C-CLOSE X  fur ther re-quires that pattern should have a core (completely dense) bl ock of support at least  X   X  minsup , where  X   X  [0 , 1] .  X  X ETI-PP X  and  X  X W-PP X  uses another parameter  X  c in addition to the parameter used by the algorithms  X  X ETI X  and  X  X W X  to check if all the item s in each pattern have more than (1  X   X  c ) fraction of supporting transac-tions. This helps in filtering some of the patterns that have s purious items. Hence,  X  X ETI-PP X  and  X  X W-PP X  finds the patterns with a flavor similar to the ones generated by  X  X FI X .

As can be clearly seen from significance plots in figure 3 and figure 4, generally  X  X FI X ,  X  X ETI-PP X , and  X  X W-PP X  have simil ar performance. However, the optimal parameters used by these algo-rithms are different as shown in Tables 2 (for data 6) and 3 (fo r data 8 ). For instance at a noise level of 8% in synthetic data PP X  can find the patterns at minsup = 150 , but  X  X FI X  and  X  X W-PP X  can only find them at minsup = 125 . Similarly at a 6% noise level in table 3,  X  X W-PP X  finds same quality patterns at either para meters minsup = 90 ,  X  r = 0 . 25 , and  X  c = 0 . 05 or at minsup = 100  X  = 0 . 25 , and  X  c = 0 . 10 . Therefore, by relaxing  X  c to 0 . 10 ,  X  X W-PP X  was able to find same quality patterns at higher support. All such cases, where multiple optimal parameter v alues are possible, are shown in the optimal parameters tables.
Our results here demonstrate that differences amongst most of these algorithms are not very large when optimal parameters are used. This finding is not consistent with some of the conclusi ons in previous work ([5, 2]). In [5]  X  X FI X  and  X  X ETI X  were compared on a simple synthetic dataset with one embedded pattern (simila r to our synthetic data 1) and  X  X FI X  was found to outperforms  X  X ETI X  b y a huge margin both in terms of recoverability and spuriousnes s. Fol-lowing are some of the possible reasons for this inconsisten cy: (1) Parameter space ( minsup ,  X  r ,  X  c ) is not explored in [5] to choose the optimal parameters for each algorithm, and (2) an exact m atch-ing criterion between the found pattern and the true pattern might have been used. In [2]  X  X FI X  and  X  X C-CLOSE X  were compared on synthetic datasets generated using the IBM data generator. Ground truth is defined to be the traditional dense patterns obtaine d using the APRIORI algorithm in the noise-free version of the data. This truth appears to favor the algorithm  X  X C-CLOSE X , which requ ires a pattern to have a core block of support at least  X   X  minsup
We also compare the algorithms based on their sensitivity to the input parameters because in real datasets where the ground t ruth is not available, optimal parameters cannot be estimated. Ide ally for a real dataset, one would like to choose an algorithm which gi ves acceptable performance as measured in terms of significance and yet be less sensitive to the input parameters. Figures 3 and 4 shows the variance of the top k % significance values obtained on different parameter combinations for dataset 6 and 8 respectively. Th e mean of the top k % significance values is also shown to indicate the over-all performance. Note, we do not show  X  X PRIORI X  in the plots o f mean and variance of top k % values because  X  X PRIORI X  only uses one minsup parameter while parameters  X  r and  X  c are 0 by de-sign. Also remember, variance does not indicate the perform ance of the algorithm, it only indicates how consistently an algo rithm generates the patterns with similar quality on different pa rameter settings. It is more meaningful to compare only those algori thms on this measure, which show acceptable significance values. We set k = 16 . 67 ( 1 note that  X  X ETI X  and  X  X W X , which require only one parameters apart from minsup have fewer number of runs in comparison to algorithms  X  X FI X ,  X  X C-CLOSE X ,  X  X ETI-PP X  and  X  X W-PP X , whic h require two parameters  X  r and  X  c apart from minsup and hence have more number of runs. Figure 3 and 4 shows the mean and variance of top 4 (out of total 24 ) and top 24 (out of total nificance values for these two sets of algorithms respective ly. We notice that although the difference in variance is not too hi gh on these datasets,  X  X C-CLOSE X  shows relatively high variance (hence less robustness) than others. This may be because of the requ ire-ment of specifying fourth parameter  X  , which makes it difficult to estimate the optimal parameters.
In the Zoo dataset [1], there are 101 instances (animals) wit h 15 boolean attributes (e.g. aquatic, tail, hair, eggs etc.) an d a class label (mammal, bird etc.). For approximate pattern mining, we consider transactions to be animals and items to be differen t fea-tures that characterizes them. Finding frequent itemsets i n this data provides the ability to predict the class of an animal. In gen eral, approximate itemsets are more suited for this problem becau se not all instances of the same class have all the common features. For example, though most mammals produce milk, are covered in ha ir, and are toothed, platypus lack teeth and dolphin lack hair.
We only focused on three classes (mammals, birds and sea-cre atures) as they have more than 10 instances each. As we saw from the results on synthetic datasets,  X  X ETI-PP X  and  X  X W-PP X  usual ly out-performs  X  X ETI X  and  X  X W X  respectively, we only show the resu lts of  X  X FI X ,  X  X ETI-PP X  and  X  X W-PP X  (see table 4). Our results in di-cate that all the algorithms discussed in this paper (except of course  X  X PRIORI X ) can find the itemsets that defines the two classes, mam-mals and birds, almost perfectly. An itemset of size-7 is supported (animal) from any other class supports it. Similarly, an ite mset that is only supported by the instances of the bird X  X  class can be f ound. For example,  X  X ETI X  finds an itemset of size-6 at minsup = 18 and  X  r = 0 . 09 , which is supported by all (and only) the instances from the bird X  X  class. The inconsistency of these results wi th those in [5, 3] is due to the difference in selection of optimal para meters.
The instances of the third class sea-creatures share 3 common features but the same 3 features are also shared by some instances from mammals and birds class. Hence, an itemset comprising o f Table 4: Parameters ( sup,  X  r ,  X  c ) for different algorithms on zoo data these 3 features alone cannot be used to predict the instances of the class sea-creatures. Truly speaking, sea-creatures di stinguish themselves from other classes because they lack some featur es that instances from other classes have. Association pattern min ing in general does not find patterns to capture such information. T his requires generalizing the definition of patterns to not only include patterns like ( A and B and C ) but also like (( A or B ) but this generalization is beyond the scope of this paper.
In this section, we compare the efficiency of different algor ithms for varying amount of noise in the dataset. Considering the f act that different parameters will result in different run-time for each algo-rithm, we run the algorithms on all the different parameter c ombi-nations and use the total run-time to compare them. All the al go-rithms are run on a linux machine with 8 Intel(R) Xeon(R) CPUs (E5310 @ 1.60GHz) (with 10 processes). Because  X  X ETI-PP X  an d  X  X W-PP X  are the variations of  X  X ETI X  and  X  X W X  respectively, we only report results on  X  X ETI-PP X  and  X  X W-PP X . Also, the run-time of  X  X C-CLOSE X  algorithm is not included. In table 5, we repor t the run-times (in seconds) of the algorithms  X  X ETI-PP X ,  X  X W-PP  X  and  X  X FI X  on synthetic data 8 with noise levels varied from 0% in increments of 2% . Note, this is the total time taken by the algo-rithm for 144 paramater combinations ( 4 different minsup and 6 different  X  r and  X  c values). It is interesting to see the dif-ferences among the algorithms in terms of the increase in run -time as the noise increases. Though,  X  X FI X  is computationally mo re ef-ficient than  X  X ETI-PP X  when noise in the dataset is low, it is v ery expensive when noise in the dataset is high. However, it is im por-tant to note that this is also due to high value of row and colum n error threshold. It is also interesting that run-time of  X  X W -PP X  only increase marginally when the noise is increased from 0% to after which it also shows rapid increase.
 Table 5: Comparison of run-times of different algorithms fo r varying amount of random noise.
In the experiments shown so far, random noise added to the syn -thetic data was limited by the size (number of transaction) o f the smallest base (or true) pattern. This was done (1) to make sur e that the truth remains meaningful and (2) to make the paramet er space search feasible for mining algorithm AFI, which is com pu-tationally expensive when the random noise in the data is hig h. However, in some real-life applications, small but true pat terns are hidden in even larger amounts of random noise. In such cases, it is still desired that algorithms could recover as many signi ficant and useful patterns from the dataset as possible. Hence, the trade off between recoverability and spuriousness becomes even m ore challenging. However, as the computational efficiency of th e algo-rithms decreases as the noise increases, we designed the fol lowing two schemes to make the comparison among algorithms feasibl e:  X 
Scheme 1: For algorithms that could finish in time  X  (  X  = 1 in our experiments), we search the whole parameter space.  X 
Scheme 2: For algorithms that could not finish certain parame ter combinations in time  X  , we search the whole parameter space in the order of complexity. Also, the row and the column thresho lds were set equal.  X  X tricter X  parameter combinations (high su pport threshold, small error tolerance) take less time to finish an d hence will be tested before the  X  X ess strict X  ones that have small s upport threshold and high error tolerance. Any combination of para meters, support threshold and error tolerance, for which the algori thm does not finish in time  X  , is not considered while selecting the optimal parameters.

We compare the performance of different algorithms under mo re noise only on synthetic data 8 since it appears to be closest to a real dataset. In addition to the original noise levels of 0%, 2%, 4%, 6%, and 8% considered in the previous experiment, noise leve ls of 10%, 12%, and 14% were also tried. As the algorithm  X  X FI X  could not finish certain parameter combinations at noise lev el of 14% within time  X  , we use the second scheme above to search the parameter space. Although other algorithms also took longe r, they were able to finish all the runs in time  X  . Figure 5 shows the results on synthetic data 8 including more noise levels 10%, 12%, and 14%. It is clear the performance of all the algorithms suffer s due to more random noise in the data. However, performance of  X  X W  X  seems to suffer more than the others as it is picking more spur ious items. Also as  X  X FI X  could not finish certain runs at noise lev el in time  X  and hence had to choose the optimal parameters from a smaller parameter space, its performance falls at noise lev el Moreover, as we derive  X  X C-CLOSE X  patterns from the  X  X FI X  pa t-terns using the core block constraint, performance of  X  X C-C LOSE X  falls as well. A more efficient implementation of  X  X C-CLOSE X  may not have this problem. Interestingly, in this case  X  X ETI  X  and  X  X ETI-PP X  have better performance than  X  X W-PP X  but as we see from the variance of the top k % (where k = 16 . 67 ( 1 runs)) significance values (figure 5),  X  X ETI X  and  X  X ETI-PP X  s eems to be less robust as the noise in the data increases.
In this paper, we have proposed an evaluation framework and showed its applicability to compare different approximate pattern mining algorithms on both synthetic and real datasets. Foll owing are the general conclusions of our evaluation study:  X 
Our results suggest that enforcing the column error toleran ce (as introduced in AFI algorithm [5]) over the concepts of str ong ETI or recursive weak ETI, makes them much more effective in terms of finding the true patterns with less spurious informa tion.  X 
All the existing ( X  X FI X  and  X  X C-CLOSE X ) as well as our propos ed variations ( X  X ETI-PP X  and  X  X W-PP X ) of the algorithms, whic h use the column error tolerance  X  c perform similarly when the optimal parameters are selected. This is because adding an addition al con-straint helps filter out patterns with spurious items.  X 
The computational efficiency of the variations  X  X ETI-PP X  an d  X  X W-PP X  seems to be much better than  X  X FI X  specially at highe r noise levels. Moreover, their performance in terms of the qu ality of the patterns is also comparable to those generated by  X  X FI X .
These conclusions are in contrast to some of the previously p ub-lished studies ([6, 5, 2]). [6, 5] compared only  X  X ETI X  and  X  X  FI X  and suggested  X  X FI X  significantly outperforms  X  X ETI X . We sh owed that although this is true, the difference is not that signifi cant. More-over  X  X ETI-PP X , a simple variation of  X  X ETI X  we proposed per -forms comparable to  X  X FI X . [2] compared only  X  X C-CLOSE X  and  X  X FI X  and suggested  X  X C-CLOSE X  performs better than  X  X FI X . How-ever, we observed no significant differences in  X  X FI X  and  X  X C -CLOSE X . We believe these differences are partly due to the fact that p revious studies did not select the optimal parameters for each algor ithm and partly because of the choice of the datasets.

This comparative study, though far more comprehensive than other previous studies, has several limitations. Most of th e patterns considered in this paper are simple embedded patterns in the syn-thetic datasets and hence may not reflect various aspects of c omplex real datasets. Even the real zoo dataset is not very complex i n terms of its size and availability of ground truth. Though, it woul d be better to apply the evaluation framework on a complex real da taset, lack of ground truth knowledge makes this much harder.
 There are still many interesting problems that need to be stu died. In the evaluation framework, it would be interesting to inco rporate redundancy and other measures in the process of optimal para meter selection. On the algorithm side, extending approximate pa ttern mining algorithms to work with categorical and continuous v alued data could prove to be very beneficial to many application dom ains. This work was supported in part by NSF and by the University of Minnesota Rochester Biomedical Informatics and Computati onal Biology Program traineeship award and research seed grants . Ac-cess to computing facilities was provided by the Minnesota S uper-computing Institute.
