 A large number of subspace methods have been proposed for processing high dimen-sional data in last decades. Among these m ethods, Principal Component Analysis (henceforth PCA), which is to find an optimal set of projection directions in the sample space and maximize the covariance of the total scatter across all samples, has difficulties in solving nonlinear problems. Linear Discriminant Analysis (LDA), which attempts to maximize inter-class distances and minimize intra-class distances simultaneously, al-ways suffers from a small sample size (SSS) problem especially for high dimensional data. Null Space Linear Discriminant Analysis (NLDA) in which the null space of the intra-class scatter matrix S W is preserved and then projected to the inter-lass scatter matrix S B [1], can obtain a high classification accuracy than PCA and LDA because of saving more discriminant information. However, a disadvantage of the NLDA algo-rithm as well as LDA, is that each class is implicitly assumed to subject to Gaussian distributions with equal covariance.
 On the other hand, mixture models based approaches, such as Multiple Discriminant Analysis (MDA) [2] and Multimodal Oriented Discriminant Analysis (MODA) [3], as-sume that training samples of each class are generated from a mixture model constituted of multiple Gaussian subclasses with different covariances, and extract discriminant in-formation from the subclasses which are obtai ned by cluster analysis. While these meth-ods obtain better discriminant performance than the traditional single model ones, there are still some drawbacks among them. Firstly, the number of subclasses of training data needs to be manually assigned. Secondly, the employed iterative algorithm has a high computational complexity and low conver gence rate. Thirdly, the multimodal methods suffer by the SSS problem more seriously when the number of training samples is small enough (e.g. =2 ).

To address the aforementioned issues, we propose the SNLDA algorithm. First of all, a new spectral clustering method proposed by Lihi Z.M. &amp; Pietro P. [5] is introduced for automatical detecting the number of s ubclasses of each class. Then, covariances from different subclasses are unified for m odeling a null space. Finally, principal fea-ture vectors are extracted from the null space with eigen-solution approach. Without the iterative procedure employed by the MDA and MODA, the proposed SNLDA algorithm builds a system with higher recognition performance and less computational complex-ity. Meanwhile, the SSS problem which cannot be solved by the mentioned multimodal methods is circumvented through the SNLDA algorithm.
 The rest of the paper is organized as follows: Section 2 is the details of the proposed SNLDA method. The experiment results are reported in Section 3. In Section 4, we end up this paper with a conclusion. In this section, a new spectral clustering algorithm that can automatically detect the number of clusters (or subclasses) in each class is first introduced. Then the details of deriving the unified null space will be proposed. Finally, a pseudo-code of the proposed algorithm and some refinements will be given. 2.1 Spectral Clustering Generally speaking, an underlying distribution of data can be properly approximated by a mixture of Gaussian models with cluster analysis. To achieve this task, the MODA algorithm introduces multiclass spectral clustering [4] which employs an iterative pro-cedure with non-maximum suppression and u ses singular value decomposition (SVD) to recover the rotation R . However, the method requires the pre-assignment of the num-ber of clusters and easily gets stuck in local minima [4].
 In this section, a new spectral clustering method proposed by Lihi Z.M. &amp; Pietro P. [5] is introduced. Assuming that the eigenvector X  X  n  X  C in an ideal case is polluted by a linear transformation R  X  C  X  C , the method can recover the rotation R through a gradient descent scheme, the corresponding cost function J to be minimized is defined as: Where Z is the rotated eigenvector, n is the number of points, C means the possible group number, M i =max j Z ij , i and j denote the row and the column of matrix Z , respectively. As a result, the number of clu sters in each class could be automatically estimated without local minimum. It is noti ceable that the spectra l clustering method can perform quite well even for small sample sizes. The details on the introduction of the method can be seen in [5]. 2.2 Deriving the Unified Null Space sian clusters. To measure the distance between two different normal distributions N fined as follows [3]: Where x  X  d is the training sample,  X  and  X  denote mean and covariance, the super-script and subscript of each symbol indicate t he index of cluster and class, respectively. For example,  X  r 1 i and  X  r 1 i denote the mean and the covariance of the r 1 -th cluster in the i -th class. The symbol  X  tr  X  denotes the trace of matrix. Our aim is to find a linear transformation B  X  d  X  k (i.e. normalization factor) so that for all clusters, can maximizes the KL divergence among different clusters under the low dimensional subspace, namely: It is difficult to directly optimize the en ergy function in Eq. (3) [3], because second-order type of gradient methods do not scale well for a large size of matrix. As a result, the MDA algorithm applies the EM algorithm in the optimization procedure. De la Torre &amp; Kanade proposed a bound optimization method called Iterative Majorization for monotonic reducing the value of the energy function [3]. However, when using these EM-like iterative algorithms, complexities in time and storage are quite high. Also, if data belong to a single sample cluster, discriminant information will be lost due to the fact that all zero intra-cluster scatter matrix will be produced.

Eq.(3) cannot be solved by an eigen-solutio n like the traditional LDA because there are many different normalization factors ( B T  X  r 1 i B )  X  1 [3]. In order to eliminate the influence of the normalization factors, our basic idea is to first project different intra-cluster scatter matrix of each cluster to a uni fied null space, then get the feature vectors by maximizing the inter-cluster scatter matrix in the null space of the intra-cluster one. For the sake of unifying each intra-cluster scatter matrix, a proposition must be pro-posed first: Proposition. Suppose the training set is composed of a total of C clusters. Let  X  de-note the sum covariance of all clusters  X  = C n =1  X  n . If the orthnormal bases
B can project the sum covariance  X  to its null space, namely, B T  X B =0 and ( B T B = I ) , it can also project each sub covariance  X  n to its null space, which is B T  X  n B =0 .
 A proof on the proposition can be seen in the appendix. For each covariance matrix  X  r i , which belongs to the r -th cluster in the i -th class, we have then we can get the orthonormal bases B which can project  X  to its null space Under the mentioned propos ition, the orthonormal bases B can also project each co-variance  X  r i to its null space Therefore, we can modify the objective function Eq.(3) as
It is obviously that the different normalization factor ( B T  X  r 1 i B )  X  1 has been re-placed by the unified null space B T  X B =0 , and the orthonormal bases B can min-imize each covariance  X  r i onto zero and avoid the numerical influence of the ratio with Eq. (7). Meanwhile, an eigen-solution way can be applied for optimizing the en-ergy function instead of the EM-like algorithms. Therefore, the proposed method has less computational complexity than MDA and MODA which use iterative optimization strategy. 2.3 Further Refinements and a Pseudo-code of the SNLDA Algorithm If a cluster consists of only one sample which is often happened in many databases (e.g. face recognition), all the elements in the intra-cluster scatter matrix will be equal to zero. Hence a modification in the defin ition of cluster covariance is proposed: where x is the only data point of the r -th cluster in the i -th class,  X  i indicates the mean of the i -th class. Through the replacement, th e rank of the modified intra-cluster covariance will not become 0 but 1 , and discriminant information can be preserved. It should be pointed out that, because the sp ectral clustering demands the number of samples in each class is not less than 2 , we do not take the instance of a single training sample per class into account in this paper.

Furthermore, some of the other literatures still mention that the null space of the inter-class matrix is no use for discriminan t analysis [6], and therefore project the ob-servation space to the null space of the intra-class scatter matrix. Under this conception, we redefine the total inter-cluster covariance S as S raw . The details of the SNLDA al-gorithm are described in Tab.1:
It also should be mentioned that, while the proposed unified null space removes the different covariances components of each cluster, the SNLDA algorithm can still solve the issue of non-Gaussian covariance. We have designed a simulated database to test the non-Gaussian problem in Section 3.2, the experimental results can support our viewpoint. In this section, four databases, including a multi-view UMIST face database [7], two FERET face databases [8], and a simulated database, are used for evaluating the perfor-mance of the proposed SNLDA algorithm. Some examples of the three face databases are illustrated in Fig.1. In the UMIST database, 10 subjects with 54 images per per-son are randomly selected and each image is resized to 32  X  30 pixels. And both of the two FERET databases include 40 subjects, each with 10 images of the face. The first database (FERET1) is mainly composed of frontal images while in the second one (FERET2), large pose variation is introduced. The images in FERET1 are cropped to 30  X  30 pixels for removing the influences of background and hairstyle. In the FERET2 database, images are zoomed into 24  X  36 pixels followed by ellipse masking. Mean-while, all the face images are roughly and manually aligned.

For all the experiments, each database is ra ndomly divided into a training set and a test set without overlapping. The NN (nearest neighbor) algorithm will be applied for classification as soon as the dimension reduction is achieved. All the reported results is the average of 20 repetitions under the mentioned procedure. 3.1 The UMIST Database In the first subsection, we attempt to employ the UMIST database to compare the ac-curacy between the SNLDA algorithm and the MODA algorithm in which the number of clusters is manually set to be 2 , 3 , 5 and 8 , respectively. The results on the average recognition rate versus the number of training samples of each class are shown as in Fig.2(a).

It can be seen from Fig.2(a) that when the MODA algorithm is applied, the highest recognition rates do not always correspond to a fixed number of clusters. Compared with the MODA algorithm, SNLDA can be self-tuned as the number of training samples varies and sounds more stable. What X  X  more, the accuracy of the refined SNLDA op algorithm is always better than those of the other algorithms even if the number of training samples is fewer. It is clear that the mentioned disadvantage of the SNLDA or algorithm is partially overcame by the the SNLDA op .

For better understanding the clustering algorithm in the SNLDA algorithm, a curve on the average detected number of clusters for the database is also illustrated in Fig 2(b). From the figure it can be found that as the number of training samples increases, the number of clusters also fluctuates. Also, none of the average cluster number is close to 1 or the upper bound of the class which is an important condition for ensuring the stability of subsequent classification. Therefore, the spectral clustering method used by the SNLDA algorithm is quite fit for data with small sample size and the proposed SNLDA approach is better in accuracy than the traditional MODA one. 3.2 Non-gaussian Simulated Database In this subsection, a simulated database is generated for evaluating the discriminant abil-ity of the proposed SNLDA algorithm under t he condition of non-Gaussian covariance. Here 200 samples from five different 200 -dimensional ( d = 200 ) Gaussian classes were generated. Each sample of the c -th class is generated as x i = B c c +  X  c + n , where x i  X  200 . And each element of random matrix B c  X  200  X  60 is gener-ated from N (0 , I ) , c  X  X  60 (0 , I ) , n  X  X  200 (0 , I ) . The means of five classes are  X   X  Gaussian covariance, we give a contrast of the classical PCA (Principal Component Analysis) algorithm, the MODA 2 algorithm and the SNLDA op algorithm. The ex-perimental results are shown in Tab.2.

In the aspect of recognition rate, the SNLDA algorithm outperforms PCA by an average of 10% and the MODA algorithm by an average of 2 . 8% . An exception is that in the first column of the table, the accu racy of the three algorithms is almost the same. It is shown that when the number o f training samples are small (e.g. leq 2 ), the accuracy of the three mentioned algorithms are similar. The experiment results show that the SNLDA algorithm can get better performance than others even in non-Gaussian covariance conditions. 3.3 The FERET Databases Finally, a comparative experiment among PCA, LDA (Linear Discriminant Analysis), NLDA (Null-space Linear Discriminant Analysis), MODA and the proposed SNLDA algorithm is carried out on the two FERET face databases. The results are shown in Fig.3. Considering the limitation of the paper X  X  size, furthermore, only a table on recognition rates with standard deviations of the FERET-2 database is tabulated in Tab.3. The results about the FERET-1 database are similar to those about the FERET-2 database.

From Fig. 3 and Tab.3, we can see that the performance of SNLDA op is always the best. When the number of samples is over 5 , furthermore, the performance of two SNLDA algorithms are higher than the traditional methods. When 3 or 4 samples per subject are regarded as training samples, however, the multi-classes method MODA and SNLDA or are not ideal due to the influence of many single sample clusters. In addition, the MODA algorithm can outperform the SNLDA or algorithm in these con-ditions, that ascribe to the iterative procedure which can extract much information from the different normalization factors. Totally, the recognition rate of the SNLDA op al-gorithm is 4%  X  7% higher than the traditional MODA and NLDA, almost 17% higher than PCA and LDA.
 In this paper, we propose a spectral clustering based null space linear discriminant anal-ysis algorithm. A main contribution is that we generalize the NLDA algorithm into multiple clusters through the combination o f the spectral clustering and the proposed unified null-space technique. Considering SSS problem and the properties of null-space, meanwhile, two further refinements on the definition of covariance and the null-space are proposed. The experimental results on face databases and simulated database show that the proposed SNLDA approach can help classifiers to obtain higher recognition rate than the mentioned traditional discriminant analysis approaches.

Recently, M. L. Zhu and A. M. Martinez have introduced the spect ral clustering method for LDA [9] and get the appropriate cluster number by considering the angle information of the eigenvectors of the covariance matrix. We will try to combine the clustering process with the oriented discr iminant analysis method and give a compari-son with this method in the further researches.
 This work is partially sponsored by NSFC (60635030), NSFC (60505002), and the State Key Laboratory of Rail Traffic Control and Safety (Beijing Jiaotong University), China. Part of the research in this paper uses the Gray Level FERET database of facial images collected under the FERET program.
 Proof. By definition, the covariance of the n -th cluster is formulated as:
