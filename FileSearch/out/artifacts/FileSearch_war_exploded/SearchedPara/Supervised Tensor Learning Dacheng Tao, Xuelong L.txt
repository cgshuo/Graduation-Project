
This paper aims to take general tensors as inputs for supervised learning. A supervised tensor learning (STL) framework is established for convex optimization based learning techniques such as support vector machines (SVM) and minimax probability machines (MPM). Within the STL framework, many conventional learning machines can be generalized to take n th  X  X rder tensors as inputs. We also study the applications of tensors to learning machine design and feature extraction by linear discriminant analysis (LDA). Our method for tensor based feature extraction is named the tenor rank X  X ne discriminant analysis (TR1DA). These generalized algorithms have several advantages: 1) reduce the curse of dimension problem in machine learning and data mining; 2) avoid the failure to converge; and 3) achieve better separation between the different categories of samples. As an example, we generalize MPM to its STL version, which is named the tensor MPM (TMPM). TMPM learns a series of tensor projections iteratively. It is then evaluated against th e original MPM. Our experiments on a binary classification problem show that TMPM significantly outperforms the original MPM.
Supervised learning [1 X 3] is an important topic in machine learning and data mining and their applications. Generally, only one X  X imensional vectors are accepted as inputs by existing supervised learning machines, such as support vector machines (SVM) [2], and minimax probability machines (MPM) [3]. However, in the real world many data items such as images are represented by 2 nd  X  X rder or high X  X rder tensors rather than the one dimensional vectors. We develop a supervised tensor learning (STL) framework in order to apply convex optimization based supervised learning techniques to tensor data. 
The motivation for the STL framework comes from the following observations. z Data structures in the real world: in many computer vision applications objects are represented as 2  X  X rder, 3 rd  X  X rder, or higher X  X rder tensors. For example, grey level face images [5] are usually represented as 2 nd  X  X rder tensors; an attention region [6], which is used in natural image understanding [4], is represented as a 3 rd  X  X rder tensor; another 3 rd tensor example is the bi X  X evel down sampled silhouette images used to represent human gait [7]; and moreover, video sequences [8] are 4 th  X  X rder tensors. 
Data structures for conventional learning methods are restricted to one X  X imensional vectors, i.e. the 1 order tensors as inputs. Learning machines such as MPM and SVM obtain top X  X evel performances in machine learning and data mining; however, they have to convert many data items naturally represented by high  X  X rder tensors to one X  X imensional vectors in order to comply with their input requirements. As a result of this conversion, much useful information in the original data is destroyed, leading to an increased number of classification errors. In addition, the conversion to vector input usually leads to the so called curse of dimension problem since the dimension of the feature space becomes much larger than the number of training samples. If the data are represented in their natural, high X  X rder tensors, then this curse of dimension problem is usually reduced. 
The main contributions of this paper are as follows. z Tensor X  X lane (a set of projection orientations): by an iterative scheme, the STL X  X ased classifier X  X  optimal tensor X  X lane can be obtained. Our method aims to approach an optimization criterion iteratively; and this criterion can be any convex functions, such as, the margin maximization (SVM) and the probability of correct classification of future data maximization (MPM). z Kernelization: a kernel space representation of general tensor inputs is developed. z Tensor MPM (TMPM): as an example, MPM is generalized to its STL version, named the tensor MPM (TMPM). TMPM is then compared with MPM on a sample image classification problem. In addition, TMPM is kernelized. z Stability analysis: in a series of experiments we observe that TMPM converges quickly within a very few iterations (around 5). Furthermore, unlike many other learning models, it is stable under different choices of initial values [11], i.e. it does not become trapped in a local minimum . z Feature extraction: we study tensor X  X ased feature extraction based on linear discriminant analysis (LDA). In this effort [18], we inherit the merits from both the defined differential scatter based discriminant criterion (DSDC) and the rank X  X ne (or rank X  n ) tensor decomposition; as a result DSDC can extract features from tensors. The new established feature extraction algorithm is named the tenor rank X  X ne discriminant analysis (TR1DA). 
This section introduces some basic concepts in convex optimization based supervised learning; establishes a STL framework for general tensors with an iterative procedure; and also deduces a kernel extension of the STL framework for high X  X rder tensor pattern classification using convex optimization. 2.1. Convex optimization based learning 
The intrinsic connections between machine learning methods and convex optimization techniques have been well studied. Examples include quadratic programming [1] in SVM and second order cone programming [3] in MPM. More recently, some convex optimization tools, such as semi X  X efinite programming [12], have become popular because they are simple and yet powerful. Below, i x ( 1 x in dd ) and i y ( 1 y in dd ) are tensors belonging to the positive class and the negative class respectively. A. Support Vector Machines (SVM) 
SVM [2] is an effective bi X  X ategory classification algorithm with sound theoretical foundations and a good generalization ability. It aims to maximize the margin between the positive and negative samples as: where w is the optimal projection orientation for bi-category classification. B. Minimax probability machines (MPM) classification for future data, or alternatively, minimizes the maximum of the Mahalanobis distances of the positive and negative samples. It is a second order cone programming: 6 are the covariance matrices of x and y , and w is the optimal projection orientation for classification . MPM has a solid theoretical foundation based on the powerful Marshall and Olkin X  X  theorem [15]. MPM can outperform SVM consistently [3]. 2.2. Supervised tensor learning framework 
Motivated by the successes of convex optimization based learning algorithms and the effective image representation by general tensors [13] [14], we propose a tensor framework for this type of learning algorithm. Generally, convex optimization based learning can be written as: where x i C and y j C are linear or quadratic constrained functions. It is not difficult to show that (1) and (2) can be unified into (3). The optimal hyper X  X lane for the classification can thus be written as: Here, if 1 gz , then z belongs to the positive class; otherwise, it belo ngs to the negative class. 
Based on (3) and (4), we can use the tensor idea to re X  X epresent convex optimization learning with n th order tensors as inputs by: and the optimal classification function is: where kk w u is the mode X  k product in tensor analysis [14]. It is defined as: kk B Aw u . Here, we use a set of projection orientations 1 | n kk w ( optimal tensor plane ) for classification. Table 1. Alternating procedure (AP) for STL. Input: Output Step 1.
 Step 2. 
Step 3. Step 4. 
According to the definition of this novel tensor oriented supervised learning framework, we can directly represent objects in their original format for machine learning and data mining applications in computer vision etc. However, so far, no closed X  X orm solutions to (5) are known. In the next section, an alternating approach is developed to solve (5). 2.3. Alternating approach 
In our tensor based supe rvised learning framework, we define and study an optimal tensor plane ( 1 | n ii w , a set of projection orientations) to approach the objective function with some const rained functions, which are related to the positive and negative samples. A close X  form solution does not exist, so we develop an alternating procedure (AP) for the framework, listed in Table 1. In this table, Steps 2 and 3 finds the optimal tensor plane 1 | n ii w and Step 4 finds the optimal bias b . 2.4. Kernelization 
As for many other machine learning and data mining algorithms, the STL framework can be kernelized directly. The kernelized model is: and the optimal classification function is: where k u is the mode X  k product in tensor analysis [14] and M  X  is a nonlinear mapping function. 
The key issue in kernelization is how to calculate tensor rank X  X ne decomposition, i.e. x can be decomposed into 11 the kernel space the decomposition can be written as: 
With this representation, the kernel trick can be utilized directly: where k K  X  is a typical kernel function, such as the Gaussian function and the polynomial function. By this kind of representation, we can have n kernel functions for the tensor learning framework in the kernel space with the n th  X  X rder tensors as inputs. 
The alternating procedure can also be kernelized because the l th kernel tensor projection orientation is ww MDM  X  , which is similar to vector based kernel algorithms, such as kernel discriminant analysis. Here, i l D is the linear combination coefficient for the l direction of the i th training sample. Based on these observations, it is straightforward to outline the kernel AP. In this paper, we do not focus on the kernel method, because the kernel parameter, defined in kernel functions, tuning is still a problem and the number of the kernel parameters is much more than that in the 1 st  X  X rder form. 2.5. LDA X  X ased feature extraction: TR1DA 
TR1DA extends the STL framework for feature extraction based on linear discriminant analysis (LDA). Because TR1DA includes many variables, we first define the variables. , k ij X is the i th object in the j
XX . Moreover, , k ij X is an M th  X  X rder tensor. MX is the j th class mean tensor in the k th training iteration and u is the j th direction base vector for decomposition in the i th training iteration. With these definitions, TR1DA is defined by the following equations: 
From the definition of the problem, which is given by (10) and (11), we know that TR1DA can be calculated by a greedy approach, because of the lack of the closed form solution for the problem. The greedy approach is illustrated in Figure 1. The calculation of u are obtained by the alternating least square (ALS) method. In ALS, we obtain the optimal base of the algorithm is given in Figure 1 and the detailed procedure for TR1DA is given in [18] due to length constraints. More extensions, such as a graph embedding extension and a kernel extension for TR1DA, are also given in [18]. With TR1DA, we can obtain 11 | dM rd u iteratively. tensor X . For recognition, the prototype p X for each individual class in the database and the test tensor to be classified are projected onto the bases to get the prototype weight vector 1 | kR p k O and test weight vector O . The test tensor class is found by minimizing the distance 
Unlike existing tensor extensions of discriminant analysis, TR1DA can converge (all 1 1 | drR rdM u dd change any more during training stage). We can check the convergence through is a small number. If projection orientation in the t th iteration is equivalent to the ( t+1 ) th iteration. 
We can easily generalize existing STL based convex optimization based machine learning and data mining algorithms. In this section, we generalize MPM to tensor MPM (TMPM) as an example. MPM is recently built and reported to achieve a top X  X evel performance [3]. Moreover, MPM has a very strong probability theory foundation based on the powerful Marshall and Olkin X  X  theorem [15]. TMPM aims at maximizing the probability of the correct classification for future data points according to: where yw y w u u  X  , and 6 are the covariance matrices of kk x w u and yw u respectively. Table 2. Alternating procedure (AP) for tensor minimax probabili ty machines (TMPM). Input : Same as in Table 1. Output Step 4.

The optimization problem for learning in (12) is a sequential second X  X rder cone programming in convex optimization. Based on the procedure developed in Section 2.3, to solve (12) is straightforward. In Table 2, we have only listed Step 3 and Step 4, which require more detail, while Step 1 and Step 2 are the same as in the procedure shown in Table 1. 
We now turn to analyze the computational complexity of this generalized TMPM. If the second order cone programming is solved by the primal X  X ual interior X  X oint method, th e computational complexity of TMPM is 3 convergence. 
Regarding kernelization, the key issue in a kernel tensor minimax probability machine (KTMPM) is the adaptation of (12) to the kernel trick. Here, we present our results for the positive class only. For the negative class, the deduction process is similar. We define then we have: and 1 1 n xw n defined in Section 2.4. With (13), (14), and (15), the KTMPM can be implemented according to AP, which is similar to the procedure listed in Table 2. Figure 2 illustrates TMPM for learning with n th order tensors as inputs. All the data in the figure come from the experiment section below. First, we have a set of samples to train TMPM. We then get three 1 st  X  X rder tensors as the projection orientations for classification. To represent the outer product of the three 1 st  X  X rder tensors conveniently, we only do the outer product for the first and second projection 1 st  X  X rder tensors, the first and third projection 1 st  X  X rder tensors, and the second and third projection 1 st  X  X rder tensors, respectively. The results of each production are shown as a 2 X  X imensional tensor plane. With these tensor planes, we can project the training or testing data onto a real axis easily through 112 2 ... ii nn x ww w O u u uu . After projections, we can calculate the optimal bias according to the criterion of the 0 th  X  X rder TMPM. 
With the established STL framework, TMPM is developed as an example of a generalized learning machine. In this section, its performance is examined with a binary image classification problem. The experimental results show that the STL version outperforms the original vector based version in terms of the ability for generalization and the stability for different initial parameters 1 | n ii w as referred to in Abstract. 4.1. Sample binary classification problem 
To categorize images into groups based on their semantic contents is a very important and challenging issue. The fundamental task is binary classification. A hierarchical structure can be built using a series of binary classifiers. As a result, this semantic image classification [19] can make the growing image repositories easy to search and browse [17]; moreover, the semantic image classification is of great help for many other applications. 
In this STL based classification environment, two groups of images are separated from each other by a trained TMPM. Inputs (representing features) of TMPM are the regions of interest (ROIs) within the pictures, which are extracted by the attention model [6] and represented as 3 rd  X  X rder tensors. 
The attention model [6] is capable of reproducing human X  X evel performances for a number of pop X  X ut tasks [16]. A target  X  X ops  X  X ut X  from its surroundings when it has it X  X  a unique orientation, color, intensity, or size. Pop X  X ut targets are always easily noticed by an observer. Therefore, utilizing the attention model to describe an image X  X  semantic information is reasonable. 
As shown in Figure 3, to represent an attention region from an image consists of several steps: 1) extract the salient map as introduced by Itti et al. in [6]; 2) find the most attentive region, whose center has the largest value in the salient map; 3) extract the attention region by a square (called ROI) in size of 64 64 u ; and 4) finally, represent this ROI in the hue, saturation, and value (HSV) perceptual color space. Consequently, we have a 3 rd  X  X rder tensor for the image representation. 
Note that although we only select a small region from the image, the size of the extracted 3 rd  X  X rder tensor is already as large as 64 64 3 uu ; if we vectorize it, the dimensionality of the vector will be 12288 . From the next subsection, we will be aware that the numbers of elements in the training and test sets are only of hundreds, much smaller than 12288 .Therefore, the small samples size (SSS) problem is always met when a 3 rd  X  X rder tensor is converted to a vector for input to a conventional learning machine. In contrast, our tensor oriented supervised learning scheme can reduce the SSS problem and at the same time represent the ROIs much more naturally. 4.2. Training/test data sets 
The training set and the test set for the following experiments are built upon the Corel photo gallery [17], from which 100 images are selected for each of the two sets. These 200 images are processed to extract the 3  X  X ensor attention features for TMPM. 
We choose the  X  X iger X  category and the  X  X eopard X  category for binary classification experiments since it is a very difficult task for a machine to distinguish between them. The  X  X iger X  and  X  X eopard X  classification is carried out in the next subsection. We choose the top N images as a training set according to the image IDs, while all the images are used to form the corresponding test set. Table 3. TMPM vs. MPM.

STS TMPM MPM TMPM MPM 10 0.0000 0.5000 0.4250 0.4900 15 0.0667 0.4667 0.3250 0.4150 20 0.0500 0.5000 0.2350 0.4800 25 0.0600 0.4800 0.2400 0.4650 30 0.1167 0.5000 0.2550 0.4600 
We introduced 3 rd  X  X rder tensor attention ROIs, which can mostly be found correctly from the images. Some successful results, respectively extracted from the  X  X iger X  category and the  X  X eopard X  category, are shown in Figure 4. By this means, the underlying data structures are well kept for the next step: classification. However, we should note that the attention model sometimes cannot depict the semantic information in an image. This is because the attention model always surroundings and thus might be  X  X heated X  when some complex or bright background exists. Some unsuccessful ROIs in the  X  X iger X  (top) and  X  X eopard X  (bottom) categories are shown in Figure 5. It should be emphasized that in order to keep the following comparative experiments fair and automatic, these wrongly extracted ROIs were not excluded from each training set. 4.3. Binary classification performance 
We carried out the binary classification ( X  X iger X  and  X  X eopard X ) experiments on the above training/ test sets. The proposed tensor based TMPM algorithm is compared with the original MPM. The experimental results are shown in Table 3. Error rates for both training and testing are reported, as the size of the training set (STS) increases from 5 to 30. 
From the training error rates in Table 3, it can be seen that the traditional method (MPM) cannot learn a satisfactory model for classification when the size of the sample set is small. However, the learning algorithm TMPM under the proposed STL framework has a good characteristic on the volume control according to the computational learning theory and its real performances. 
Also from Table 3, based on the testing error rates of the comparative experiments, the proposed TMPM algorithm more effectively represents the intrinsic discriminant information (in forms of 3 rd  X  X rder ROIs). TMPM learns a better classification model for unseen data classification than MPM and thus has a better performance on the testing set. It is observed that the TMPM error rate is a decreasing function of the size of learning theory. 
In this section we study two important issues in machine learning and data mining, namely, the convergence property and the insensitiveness to the initial values. 
We carry out a series of experiments based on the same database employed in Section 4. Experimental results prove that TMPM converges well. In addition, it is insensitive to the initial values and thus has a good stability. 
Figure 6 shows tensor projected position values ii nn x ww w O u u uu of the original general tensors with an increasing number of learning iterations using 10 training samples for each class. We find that the projected values converge to stable values. As shown in Figure 6, five to six iterations are usually enough to achieve convergence. 
Figure 7 shows TMPM is insensitive to the initial values. Many learning algorithms converge to different local minima with different initial parameter values for w . This is the so X  X alled local minimum problem. However, our new TMPM does not slump into this local minimum problem, which is proved by a set of experiments, with different initial parameters, 10 Theoretically, this is also true, because each sub X  optimization problem to optimize i w is convex. Figure 7. TMPM is stable with different initial values in 10 learning iterations (20 training samples in each class). 
In this paper, we have developed a supervised tensor learning (STL) framework to generalize convex optimization based schemes so that they accept n th order tensors as inputs. We have also studied the tensor rank X  X ne discriminant analysis ( TR1DA ) method for extracting features from tensors. Under this STL framework, a novel approach called tensor minimax probability machines (TMPM) has been developed to learn a series of projections for classification. TMPM has the following properties: 1) it can reduce the curse of dimension problem meaningfully and directly by using the tensor based representation, which reduces the number of parameters in the learning procedure; 2) it makes a better use of the information on the input than vector based discriminant analysis, efficiently; 3) it converges within a few training iterations; and 4) it is insensitive to the initial parameter values. The stability of the proposed scheme has also been analyzed. 
