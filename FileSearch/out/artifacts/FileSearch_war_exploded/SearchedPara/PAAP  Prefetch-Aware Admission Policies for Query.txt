 Caching query results is an efficient technique for Web search engines. Admission policy can prevent infrequent queries from taking space of more frequent queries in the cache. In this paper we present two novel admission policies tailored for query results cache. These policies are based on query results prefetching information. We also propose a demote operation for the query results cache to improve the cache hit ratio. We then use a trace of over 5 million queries to evaluate our admission policies, as well as traditional poli-cies. Experimental results show that our prefetch-aware ad-mission policies can achieve hit ratios better than state-of-the-art admission policies.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  search process ; H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  Perfor-mance evaluation (efficiency and effectiveness) Performance, Experimentation Web search engine, admission policy, caching, prefetching
Caching is an effective technique to improve the perfor-mance of large scale Web search engines. Results cache stores the previous search results which were recently com-puted to resolve future queries. Web search engines may also prefetch some search engine results pages which are the  X 
Supported by the Youth Foundation of CNCERT (No.2013QN-18), the National Key Technology R&amp;D Pro-gram (No.2012BAH42B01) and the National Natural Sci-ence Foundation of China (No.61300206) listing of Web pages returned by the Web search engines in response to a query that it predicts to be requested in the near future [1]. In addition to these approaches, query results cache admission policy is employed to prevent infre-quent queries from taking space of more frequent queries in the cache.

Web search engine cache admission policy has been stud-ied by a number of researchers. As we know, most of earlier admission policies use either stateless features, such as the length of the query in words, the length of the query in char-acters, which depend only on the query, or stateful features, such as the past frequency of the query, which are based on historical information.  X  X t is hence an open problem if there exists a feature (or combination of features) that can achieve a performance as good as the one of stateful features. X  pre-sented by Baeza-Yates et al.[2] is a research challenge in query results cache admission policy.

We take into consideration the fact that prefetching query results can result in infrequent queries in the cache. This pa-per concentrates on results cache admission policy, and in particular discusses the application of query results prefetch-ing information in the context of Web search engine results cache admission policy. Our goal is to design new admis-sion policies for query results cache that exploit the query results prefetching information to prevent infrequent or even singleton queries from polluting the cache. To do this, we divide results cache into two areas: controlled cache and un-controlled cache, and evaluation functions based on query results prefetching information were used to decide whether a query is infrequent or not and store in which part of the results cache. Generally speaking, the controlled cache will contain those queries that are likely to be hits. Ideally, with a perfect admission policy, there would be no need for the uncontrolled cache. However, implementing such an admis-sion control policy is very difficult. In addition, we propose a demote operation for the query results cache to improve the cache hit ratio.
We use a query log from a famous Chinese Web search en-gine which contains around 28 million queries of about 5.8 million people for a period of 2-months (from 09/01/2006 to 10/31/2006) to explore Web search engine query charac-teristics. The analysis of the search engine query log shows that query reused distance obeys Zipf law which can be ex-pressed as y = Kx  X   X  , x represents the rank of query reused distance and the value of parameter  X  is 0.49 in our dataset.
Let X  X  recall how Web search engine works. It can be sum-marized in the following procedure: (1) Web search engine user formulates a query according to his information need, and the query request includes user query interests; (2) Web search engine receives the request, and do index retrieval, similarity calculation, ranking, generating query results page with results URL content snippet; (3) Returns final query results page to the user. It is note that the query result is returned in units of pages. When a user submit an ini-tial query to Web search engine, the first page, namely page number 1, would be returned to the user, and the user would decide whether to submit subsequent page number request based on the quality of current returned query results page. Figure 1: Some query requests about  X  X i Yuchun X  ( in Chinese) (From a famous Chinese Web search engine)
Here, we divide queries submitted by users into the fol-lowing two categories: (1) Initial Query: The first query request on a topic in a session; (2) Follow-up Query: All query request following the initial query in a session. Figure 1 shows several query sessions about the topic  X  X i Yuchun X  a famous female singer in China from a famous Web search engine query log on September 1, 2006 (Note that these are only a part of queries about the topic  X  X i Yuchun X ). Column 5 is the query results page number and follow-up query is marked  X  X  X  in the tail. Taking the queries about the topic  X  X i Yuchun X  submitted by the user  X 1151472524296 X  for exam-ple, after an initial query, the user submits three follow-up queries with query results page number 2, 3 and 5. We find that this query results page access pattern is widespread in Web search engines and we define this pattern as follow-up pattern. Therefore, prefetching in advance subsequent query results pages can reduce the latency of future query requests.
Follow-up pattern means that Web search engines have good spatial locality, and prefetching is useful to improve system performance. We define query as Query ( Topic,Page When the request with page number Page no is received by Web search engine, it would prefetch query results page Figure 2: Hit ratio of various prefetch queue capac-ities ranged from Page no + 1 to Page no + M to query results cache to reduce the response time of follow-up queries. Fig-ure 2 shows the experimental results when prefetching pa-rameter M is 2 (Note that the results with other parame-ters are similar). In the figure, the horizontal axis represents the ratio of total query results cache capacity for prefetch-ing queue and the vertical axis represents the hit ratio of the query results cache. It shows that the hit ratio of the query results cache with prefetching is better than without prefetching (the ratio of total cache for prefetching queue is 0), and prefetching query results is very necessary for the performance of the query results cache.
 100% Miss ratio
From the above analysis, we can see that prefetching is an important technology to improve the Web search engine system performance. Next, we explore the characteristics of the queries submitted to the core of the Web search engine. Figure 3 shows the statistics result of cache miss page num-ber when cache capacity is 200K query results pages and M is 4. The cache miss ratio of initial queries is 63%, because initial query usually can X  X  fetched in advanced, and it has high cache miss ratio. The follow-up queries page number ranged from 2 to 5 have low cache miss ratio because of prefetching. Due to prefetching parameter M is 4, query results page number 6 has high cache miss ratio. The access behavior of the subsequent query results pages follows this rule. As a result of prefetching technology using by query results cache, it makes the cache miss ratio of page numbers much different.
Query results cache evaluates the value of a query through cache admission policy to decide whether the corresponding query results page is infrequent or not. The approach of evaluating the value of a query is the core issue, and we define it as evaluation function. Figure 4 shows the cache admission policy framework. When a query request is re-ceived by the cache, it decides the query results page entry into the controlled cache or the un-controlled cache based on the evaluation function. When a query has a high score as evaluated by the function, the query results page is stored in the controlled cache, otherwise stored in the un-controlled cache. Controlled cache and un-controlled cache can use the same cache replacement policy, and also can use different replacement policies. Generally speaking, controlled cache only admits those queries that the evaluation function clas-sifies as future cache hits, and uncontrolled cache admits the rest of all queries. Ideally, with a perfect admission policy, there would be no need for the uncontrolled cache.
 Next, we give a formal definition of evaluation function. Given a query sequence Q s , the definition of query evalua-tion function is f : Q s  X  X  0 , 1 } , if f ( q ) = 1 then the query results page is stored in the controlled cache; if f ( q ) = 0 then the query results page is stored in the un-controlled cache. Figure 4: Query results cache admission policy fram ework Figure 4: Query results cache admission policy framework
In the query results cache admission policy framework, choosing an effective evaluation function is the key issue. Baeza-Yates et al.[1] summarized the features those were proposed in previous studies, including the frequency of the query, the length of the query in words, the length of the query in characters and so on. We find that the prefetching technology makes the cache miss ratio of query results page numbers much different. Therefore, we choose prefetching information as the feature of the evaluation function.
According to the behavior of prefetching in the query re-sults cache, we propose two types of features: Non-Memory Prefetching Feature (PAAP-NMPF) : It needn X  X  maintain the information of past queries, and the decision of the evaluation function is based on current state. This is a stateless feature. The evaluation function with non-memory feature is as follows: Memory Prefetching Feature (PAAP-MPF) : It needs maintain the information of past queries, and the decision of the evaluation function is based on historical information. Memory feature cost much more memory space than non-memory feature. This is a stateful feature. We define the total number of the past queries for results page number Page i as S ( Page i ) and the total number of the past queries for Page i through prefetching as P ( Page i ). Therefore, the ratio of Page i through prefetching is: The evaluation function with memory feature is as follows: f ( q ) =
Next, we will discuss the impact of demote operation on queries results cache. Demote operation : it is used to transfer evicted query results pages from the controlled cache to the uncontrolled cache rather than out of the query results cache directly. The motivations of demote operation is as follows: making those queries that the evaluation func-tion classifies as future cache hits stay in the cache longer. Table 1 shows the impact of demote operation on the query results cache with PAAP-NMPF admission policy. The hit ratio of the query results cache with demote operation is higher than without demote operation.
 Table 1: Impact of demote operation on query re-sults cache
Capacity Non-Demote(%) Demote(%) Improve(%) 200K 51.96 52.22 0.26 400K 55.99 56.28 0.29 600K 58.46 58.75 0.29 800K 60.02 60.26 0.24 1000K 61.10 61.32 0.22
The dataset is from a famous Chinese Web search engine from 09/01/2006 to 10/31/2006. We chose a period of query logs that contained 5 million queries. The first 1 million queries constitute the training set to warm up the cache. The remaining about 4 million queries are reserved as the test set.
Query results cache is a key technology to improve Web search engine system performance. It usually uses cache hit ratio as its measure. Due to the capacity limitation of the query results cache, capacity factor need to be considered. When the cache is full, it needs a policy to replace some of the query results in the cache. There are many replace-ment policies, such as LRU, LIRS, 2Q, MQ and so on. LRU was adopted in our experiments, and the impact of various replacement policies is our future work.

In order to evaluate the effectiveness of our admission poli-cies, we used the state-of-arts policies as the baseline, in-cluding the past frequency of the query (PastF), the length of the query in words (LenW) and the length of the query in characters (LenC). We used least recent used (LRU) as query results cache replacement policy and pre-SDC[3, 4] as query results prefetching policy.
We conducted our experiments on the query results cache with the prefetch-aware admission policies: (1) PAAP-NMPF and (2) PAAP-MPF. Cache capacity is given as the num-ber of query results pages that are cached. The capacity of the cache is from 200K query results pages to 1000K query results pages, and the number of prefetching query results pages (parameter M in pre-SDC) is 2 in our experiments. It is noteworthy that the experimental results with other M values are similar, therefore, we would not discuss the im-pact of the parameter M . We allocate a  X  fraction of the query results cache for the controlled cache and the rest for the un-controlled cache. The experiments were conducted with  X  = 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 and 1.0.
First of all, we give the experimental results of the base-line such as admission policy with PastF, LenW and LenC in the paper. Table 2  X  4 show the results when the ca-pacity of the cache is 800K query results pages. We found the admission policy with PastF is higher than the hit ra-tio of the admission policy with LenW and LenC. Table 2 shows the results of the admission policy with PastF, and optimal cache hit ratio is 59.35% when the threshold of the query frequency is 3. Because most of the queries in our Web search engine logs are in Chinese, we used a Chinese word segmentation tool  X  ICTCLAS released by Institute of Computing Technology, Chinese Academy of Sciences to preprocess the query logs. Table 3 shows the results of the admission policy with LenW, and optimal cache hit ratio is 56.56% when the threshold of the query length in words is 5. Table 4 shows the results of the admission policy with LenC, and optimal cache hit ratio is 56.97% when the threshold of the query length in characters is 25.
 Table 2: Cache hit ratio with PastF admission policy Table 3: Cache hit ratio with LenW admission policy Table 4: Cache hit ratio with LenC admission policy
Figure 5 is the comparison of the query results cache with the PAAP-NMPF, PAAP-MPF admission policies and tra-ditional policies. As can be seen from the experimental re-sults, the PAAP-NMPF and PAAP-MPF admission poli-cies are much better than those traditional policies. Com-pared with the LenC and LenW admission policies, the PAAP-NMPF admission policy can get 6.38%  X  11.99% performance increase. Compared with the PastF admission policy, the MPF admission policy can get 7.51%  X  9.93% performance increase. Compared with the PAAP-NMPF admission pol-icy, the PAAP-MPF admission policy can get 4.44%  X  5.95% performance increase. As can be seen from the above anal-ysis, the hit ratio of the PAAP-MPF admission policy is higher than the PAAP-NMPF admission policy, but the PAAP-MPF admission policy uses more storage space to record the information of query results pages. According to a previous study [5], the queries whose request page num-ber less than 10 account for 93.59%, so the storage overhead is as follows: ( number of queries )*( 10 pages )*( storage space 45 49 53 57 61 65 Hit ratio (%) Figure 5: Comparison of the hit ratios for all re-quests
In this study, we introduce prefetch-aware admission poli-cies for query results cache. Experimental results show that the new admission policies can effectively improve the per-formance of Web search engines. The future work involves the impact of various query results cache replacement poli-cies and query results prefetching approaches on these ad-mission policies. [1] S. Jonassen, B.B. Cambazoglu, and F. Silvestri. [2] R. Baeza-Yates, F. Junqueira, V. Plachouras and H.F. [3] T. Fagni, R. Perego, F. Silvestri and S. Orlando. [4] H.Y. Ma and B. Wang. User-aware caching and [5] Y.N. Li, S. Zhang, B. Wang and J.T. Li.

