 The grammars used by modern parsers are ex-tremely large, rendering exhaustive parsing imprac-tical. For example, the lexicalized grammars of Collins (1997) and Charniak (1997) and the state-split grammars of Petrov et al. (2006) are all too large to construct unpruned charts in memory. One effective approach is coarse-to-fine pruning, in which a small, coarse grammar is used to prune edges in a large, refined grammar (Charniak et al. , 2006). Indeed, coarse-to-fine is even more effective when a hierarchy of successive approximations is used (Charniak et al. , 2006; Petrov and Klein, 2007). In particular, Petrov and Klein (2007) generate a se-quence of approximations to a highly subcategorized grammar, parsing with each in turn.

Despite its practical success, coarse-to-fine prun-ing is approximate, with no theoretical guarantees on optimality. Another line of work has explored A  X  search methods, in which simpler problems are used not for pruning, but for prioritizing work in the full search space (Klein and Manning, 2003a; Haghighi et al. , 2007). In particular, Klein and Man-proved the search in the lexicalized grammar, with provable optimality. However, their bottleneck was clearly shown to be the exhaustive parsing used to multi-pass way to speed up the computation of such complex heuristics.

In this paper, we address three open questions regarding efficient hierarchical search. First, can hierarchical coarse-to-fine pruning? We show that McAllester, 2007) can naturally be applied to both the hierarchically refined grammars of Petrov and Klein (2007) as well as the lexicalized grammars of Klein and Manning (2003a). Second, what are methods? We show that coarse-to-fine is generally much faster, but at the cost of search errors. 1 Below optimal. Finally, when and how, qualitatively, do as the slack increases between the heuristic bounds and the true costs. On the other hand, coarse-to-fine prunes unreliably when the approximating grammar is very different from the target grammar. We em-pirically demonstrate both failure modes. Our primary goal in this paper is to compare hi-(CTF) pruning methods. Unfortunately, these two algorithms are generally deployed in different archi-tectures: CTF is most naturally implemented using a dynamic program like CKY, while best-first al-agenda-based parsers. To facilitate comparison, we would like to implement them in a common architec-ture. We therefore work entirely in an agenda-based setting, noting that the crucial property of CTF is not the CKY order of exploration, but the pruning of unlikely edges, which can be equally well done in an agenda-based parser. In fact, it is possible to closely mimic dynamic programs like CKY using a best-first algorithm with a particular choice of prior-ities; we discuss this in Section 2.3.
 Felzenszwalb and McAllester (2007), we present here a specialization to the parsing problem. We first review the standard agenda-driven search frame-2.1 Agenda-Driven Parsing A non-hierarchical, best-first parser takes as input a PCFG G (with root symbol R), a priority function p (  X  ) and a sentence consisting of terminals (words) scoring (Viterbi) tree structure which is rooted at R and spans the input sentence. Without loss of gen-erality, we consider grammars in Chomsky normal form, so that each non-terminal rule in the grammar has the form r = A  X  B C with weight w r . We assume that weights are non-negative (e.g. negative log probabilities) and that we wish to minimize the sum of the rule weights.

The objects in an agenda-based parser are edges e = I ( X , i, j ) , also called items , which represent parses spanning i to j and rooted at symbol X. We denote edges as triangles, as in Figure 1. At all times, edges have scores  X  e , which are estimates of their Viterbi inside probabilities (also called path costs ). These estimates improve over time as new derivations are considered, and may or may not be correct at termination, depending on the properties of p . The parser maintains an agenda (a priority queue of edges), as well as a chart (or closed list in search terminology) of edges already processed. The fundamental operation of the algorithm is to pop the best (lowest) priority edge e from the agenda, put it into the chart, and enqueue any edges which can be built by combining e with other edges in the chart. The combination of two adjacent edges into a larger edge is shown graphically in Figure 1 and as a weighted deduction rule in Table 1 (Shieber et al. , 1995; Nederhof, 2003). When an edge a is built from adjacent edges b and c and a rule r , its cur-rent score  X  a is compared to  X  b +  X  c + w r and up-dated if necessary. To allow reconstruction of best parses, backpointers are maintained in the standard way. The agenda is initialized with I ( T i , i, i + 1) for i =0 . . . n  X  1 . The algorithm terminates when I ( R , 0 ,n ) is popped off the queue.
 Priorities are in general different than weights. Whenever an edge e  X  X  score changes, its priority p ( e ) , which may or may not depend on its score, may improve. Edges are promoted accordingly in the agenda if their priorities improve. In the sim-plest case, the priorities are simply the  X  e estimates, which gives a correct uniform cost search wherein the root edge is guaranteed to have its correct inside score estimate at termination (Caraballo and Char-niak, 1996).
 cial case of such an agenda-driven parser in which the priority function p takes the form p ( e )=  X  e + h ( e ) , where e = I ( X , i, j ) and h (  X  ) is some approx-imation of e  X  X  Viterbi outside cost (its completion cost ). If h is consistent , then the A  X  algorithm guar-antees that whenever an edge comes off the agenda, its weight is its true Viterbi inside cost. In particular, this guarantee implies that the first edge represent-ing the root I ( R , 0 ,n ) will be scored with the true Viterbi score for the sentence. to come from a black box. For example, Klein and Manning (2003b) precomputes most heuristics of-fline, while Klein and Manning (2003a) solves sim-pler parsing problems for each sentence. In such cases, the time spent to compute heuristics is often non-trivial. Indeed, it is typical that effective heuris-tics are themselves expensive search problems. We computation of the heuristics themselves. Hierar-quence (or hierarchy ) of m +1 PCFGs G 0 . . . G m , where G m is the target grammar and G 0 . . . G m  X  1 are auxiliary grammars . Each grammar G t has an in-ventory of symbols  X  t , hereafter denoted with capi-tal letters. In particular, each grammar has a distin-input and a root symbol R t .

The grammars G 0 . . . G m must form a hierarchy in which G t is a relaxed projection of G t +1 . A grammar G t  X  1 is a projection of G t if there exists some onto function  X  t :  X  t $ X   X  t  X  1 defined for all symbols in G projection is a relaxation if, for every rule r = A t  X  B t C t with weight w r the projection r $ =  X  t a target grammar G m and a projection function  X  m , it is easy to construct a relaxed projection G m  X  1 by minimizing over rules collapsed by  X  m :
Given a series of projection functions  X  1 . . .  X  m , we can construct relaxed projections by projecting G that by construction, parses in a relaxed projection give lower bounds on parses in the target grammar (Klein and Manning, 2003b).
 First, it tracks not only standard inside edges e = I ( X , i, j ) which represent derivations of X  X  T i . . . T j , but also outside edges o = O ( X , i, j ) which represent derivations of R  X  I ( VP , 0 , 3) denotes trees rooted at VP covering the span [0 , 3] , O ( VP , 0 , 3) denotes the derivation of the  X  X est X  of the structure to the root. Where inside edges e have scores  X  e which represent (approxima-tions of) their Viterbi inside scores, outside edges o have scores  X  o which are (approximations of) their Viterbi outside scores. When we need to denote the inside version of an outside edge, or the reverse, we write o =  X  e , etc. from all levels of the hierarchy on a single, shared agenda, so that all items compete (see Figure 3). While there is only one agenda, it is useful to imag-ine several charts, one for each type of edge and each grammar level. In particular, outside edges from one level of the hierarchy are the source of completion costs (heuristics) for inside edges at the next level. and represented graphically in Figure 2. The IN rule we can combine two adjacent inside edges using a binary rule to form a new inside edge. The new twist is that because heuristics (scores of outside edges from the previous level) are also computed on the fly, they may not be ready yet. Therefore, we cannot carry out this deduction until the required outside edge is present in the previous level X  X  chart. That is, fine inside deductions wait for the relevant coarse outside edges to be popped. While coarse outside edges contribute to priorities of refined inside scores (as heuristic values), they do not actually affect the queue refined terminal edges until their outside scores are ready. The IN-BASE rule specifies the base case for a grammar G t : we cannot begin un-til the outside score for the terminal symbol T is ready in the coarser grammar G t  X  1 . The initial queue contains only the most abstract level X  X  terminals, I the inside edge I ( R m , 0 ,n ) , represting root deriva-tions in the target grammar, is dequeued.

The deductions which assemble outside edges are deductions take larger outside edges and produce smaller sub-edges by linking up with inside edges, as shown in Figure 2(b). The OUT-BASE rule states that an outside pass for G t can be started if the in-side score of the root symbol for that level R t has been computed. The OUT-L and OUT-R rules are the deduction rules for building outside edges. OUT-L states that, given an outside edge over the span [ i, j ] and some inside edge over [ i, k ] , we may con-struct an outside edge over [ k, j ] . For outside edges, the score reflects an estimate of the Viterbi outside score.
 agenda with a priority equal to their path cost (inside score) and some estimate of their completion cost (outside score), now taken from the previous projec-tion rather than a black box. Specifically, the priority function takes the form p ( e )=  X  e +  X   X  e ! , where  X  is the outside version of e one level previous in the hierarchy.

Outside edges also have priorities which combine path costs with a completion estimate, except that the roles of inside and outside scores are reversed: the path cost for an outside edge o is its (outside) score  X  o , while the completion cost is some estimate of the inside score, which is the weight  X  e of o  X  X  complementary edge e =  X  o . Therefore, p ( o )=  X  o +  X  .

Note that inside edges combine their inside score estimates with outside scores from a previous level (a lower bound), while outside edges combine their outside score estimates with inside scores from the same level , which are already available. Felzen-szwalb and McAllester (2007) show that these choices of priorities have the same guarantee as stan-off the queue, its path cost is optimal. 2.3 Agenda-driven Coarse-to-Fine Parsing with an alternate priority function of our choosing. In doing so, we may lose the optimality guarantees nificant increases in performance. We do exactly this in order to put CTF pruning in an agenda-based framework. An agenda-based implementation al-highlighting the effectiveness of the various parsing strategies and normalizing their implementations.
First, we define coarse-to-fine pruning. In stan-dard CTF, we exhaustively parse in each projection level, but skip edges whose projections in the previ-ous level had sufficiently low scores. In particular, an edge e in the grammar G t will be skipped entirely if its projection e $ in G t  X  1 had a low max marginal :  X  root derivation  X  R ! . Formally, we prune all e where  X 
The priority function we use to implement CTF in our agenda-based framework is: Here,  X  t  X  0 is a user-defined threshold for level t and  X  R mar G t . These priorities lead to uniform-cost explo-ration for inside edges and completely suppress out-side edges which would have been pruned in stan-dard CTF. Note that, by the construction of the IN rule, pruning an outside edge also prunes all inside edges in the next level that depend on it; we there-fore prune slightly earlier than in standard CTF. In any case, this priority function maintains the set of states explored in CKY-based CTF, but does not nec-essarily explore those states in the same order. 3.1 Evaluation Our focus is parsing speed. Thus, we would ideally evaluate our algorithms in terms of CPU time. How-ever, this measure is problematic: CPU time is influ-enced by a variety of factors, including the architec-ture of the hardware, low-level implementation de-tails, and other running processes, all of which are hard to normalize.

It is common to evaluate best-first parsers in terms of edges popped off the agenda. This measure is used by Charniak et al. (1998) and Klein and Man-ning (2003b). However, when edges from grammars of varying size are processed on the same agenda, the number of successor edges per edge popped changes depending on what grammar the edge was constructed from. In particular, edges in more re-fined grammars are more expensive than edges in coarser grammars. Thus, our basic unit of measure-ment will be edges pushed onto the agenda. We found in our experiments that this was well corre-lated with CPU time. 3.2 State-Split Grammars We first experimented with the grammars described in Petrov et al. (2006). Starting with an X-Bar gram-mar, they iteratively refine each symbol in the gram-mar by adding latent substates via a split-merge pro-cedure. This training procedure creates a natural hi-erarchy of grammars, and is thus ideal for our pur-poses. We used the Berkeley Parser 2 to train such grammars on sections 2-21 of the Penn Treebank (Marcus et al. , 1993). We ran 6 split-merge cycles, producing a total of 7 grammars. These grammars range in size from 98 symbols and 8773 rules in the unsplit X-Bar grammar to 1139 symbols and 973696 rules in the 6-split grammar. We then parsed all sen-tences of length  X  30 of section 23 of the Treebank with these grammars. Our  X  X arget grammar X  was in all cases the largest (most split) grammar. Our pars-ing objective was to find the Viterbi derivation (i.e. fully refined structure) in this grammar. Note that this differs from the objective used by Petrov and Klein (2007), who use a variational approximation to the most probable parse. presented by Klein and Manning (2003b), an aux-iliary grammar can be used, but we are restricted to only one and we must compute inside and out-side estimates for that grammar exhaustively. For our single auxiliary grammar, we chose the 3-split grammar; we found that this grammar provided the best overall speed.
 auxiliary grammars from the hierarchy as desired. Ideally, we would find that each auxiliary gram-mar increases performance. To check this, we per-formed experiments with all 6 auxiliary grammars (0-5 split); the largest 3 grammars (3-5 split); and only the 3-split grammar.
 Figure 4 shows the results of these experiments. As a baseline, we also compare with uniform cost search (UCS) (A  X  with h =0 ). A  X  provides a speed-up of about a factor of 5 over this UCS base-need not exhaustively parse the 3-split grammar be-fore beginning to search in the target grammar. it increases performance by another 25%. However, using all 6 auxiliary grammars actually decreases performance compared to using only 3-5. This is be-relaxed projections of the target grammar. Since the weights of the rules in the smaller grammars are the minimum of a large set of rules in the target gram-mar, these grammars have costs that are so cheap that all edges in those grammars will be processed long before much progress is made in the refined, more expensive levels. The time spent parsing in the smaller grammars is thus entirely wasted. This is in sharp contrast to hierarchical CTF (see below) where adding levels is always beneficial.

To quantify the effect of optimistically cheap costs in the coarsest projections, we can look at the degree to which the outside costs in auxiliary gram-mars underestimate the true outside cost in the target grammar (the  X  X lack X ). In Figure 5, we plot the aver-age slack as a function of outside context size (num-ber of unincorporated words) for each of the auxil-iary grammars. The slack for large outside contexts gets very large for the smaller, coarser grammars. In Figure 6, we plot the number of edges pushed when bounding with each auxiliary grammar individually, against the average slack in that grammar. This plot shows that greater slack leads to more work, reflect-can be exponential in the slack of the heuristic. using the grammars of Petrov et al. (2006). It is important to note, however, that we do not use the same grammars when parsing with these two al-gorithms. While we use the same projections to coarsen the target grammar, the scores in the CTF case need not be lower bounds. Instead, we fol-low Petrov and Klein (2007) in taking coarse gram-mar weights which make the induced distribution over trees as close as possible to the target in KL-divergence. These grammars represent not a mini-mum projection, but more of an average. 3 is shown in Figure 4. CTF represents a significant shown here, is that, where the work saved by us-saved with CTF increases with the addition of highly coarse grammars. Adding the 0-through 2-split grammars to CTF was responsible for a factor of 8 speed-up with no additional search errors.

Another important property of CTF is that it Figure 7 shows a plot of edges pushed against sen-tence length. This is not surprising in light of the in-crease in slack that comes with parsing longer sen-tences. The more words in an outside context, the more slack there will generally be in the outside es-timate, which triggers the time explosion.

Since we prune based on thresholds  X  t in CTF, we can explore the relationship between the number of search errors made and the speed of the parser. While it is possible to tune thresholds for each gram-mar individually, we use a single threshold for sim-plicity. In Figure 8, we plot the performance of CTF using all 6 auxiliary grammars for various values of  X  . For a moderate number of search errors ( &lt; 5% ), nearly 100 times faster than UCS. However, below a certain tolerance for search errors ( &lt; 1% ) on these 3.3 Lexicalized parsing experiments We also experimented with the lexicalized parsing model described in Klein and Manning (2003a). This lexicalized parsing model is constructed as the product of a dependency model and the unlexical-ized PCFG model in Klein and Manning (2003c). We constructed these grammars using the Stanford Parser. 5 The PCFG has 19054 symbols 36078 rules. The combined (sentence-specific) grammar has n times as many symbols and 2 n 2 times as many rules, where n is the length of an input sentence. This model was trained on sections 2-20 of the Penn Tree-bank and tested on section 21.

For these lexicalized grammars, we did not per-form experiments with UCS or more than one level used in Klein and Manning (2003a). This grammar differs from the state split grammars in that it factors into two separate projections, a dependency projec-tion and a PCFG. Klein and Manning (2003a) show that one can use the sum of outside scores computed in these two projections as a heuristic in the com-bined lexicalized grammar. The generalization of effective. We therefore treated the dependency pro-jection as a black box and used only the PCFG pro-use the sum of the two projections X  outside scores as our completion costs. This is the same procedure as Klein and Manning (2003a). For CTF, we carry out a uniform cost search in the combined space where we have pruned items based on their max-marginals in the PCFG model only.

In Figure 9, we examine the speed/accuracy trade off for the lexicalized grammar. The trend here is the reverse of the result for the state split grammars: thresholds which produce many search errors. This is because the heuristic used in this model is actu-ally an extraordinarily tight bound  X  on average, the slack even for spans of length 1 was less than 1% of the overall model cost. We have a presented an empirical comparison of search, the extra levels of the hierarchy are dramat-ically more beneficial for CTF. This is because, in CTF, pruning choices cascade and even very coarse projections can prune many highly unlikely edges. so loose as to not rule out anything of substance. In addition, we experimentally characterized the fail-as heuristics loosen and only outperforms CTF when either near-optimality is desired or heuristics are ex-tremely tight.

