 important statistical distribution on lower dimensi ons for high dimensional data. Methods such as Principal Component Analysis (PCA) do not make use of the cla ss information, and Linear Discriminant Analysis (LDA) could not be performed efficiently in a scalable wa y. In this paper, we propose a novel highly scalable supe rvised subspace learning algorithm called as Supervised Kampong Measure (SKM). It assigns data points as cl ose as possible to their corresponding class mean, simultaneously assigns data points to be as far as possible from the other class means in the transformed lower dimensional subspace. Theoretical derivation shows that our algorithm is not limited by the number of class es or the singularity problem faced by LDA. Furthermore, our algorithm can be executed in an incremental manner in which learning is done in an online fashion as data streams are received. Experimental results on sever al datasets, including a very large text data set RCV1 , show the outstanding performance of our proposed algorit hm on classification problems as compared to PCA, LDA and a popular feature selection approach, Informat ion Gain (IG). research has witnessed a growing interest in subspa ce learning [7] and its applications, such as Web docu ment classification [11], face recognition [16] and data clustering [1, 12]. Among various subspace learning approaches, linear algorithms are of great interest ing due to their efficiency and effectiveness. Principal Co mponent Analysis (PCA) [2, 13] and Linear Discriminant Anal ysis (LDA) [4] are two of the most widely used tradition al linear subspace learning algorithms. unsupervised algorithm, aims at finding out the geometrical structure of the data set and projectin g the data along the directions with maximal variances. However, it discards the class information which is significant for classification tasks. Linear Discri minant Analysis (LDA), which is a supervised algorithm als o called as Fisher Discriminant Analysis (FDA), is a traditional supervised subspace learning algorithm. It searches for the projection on which the data point s of different classes are far from each other and, at t he same time, the data points of the same class are close t o each other. Nevertheless, the available subspace dimensi on in LDA is limited by the number of classes, and the singularity problem limits the application of LDA [ 10]. batch algorithms, which mean that the training data must be available in advance. As a result, they cannot s atisfy the requirements of online applications on data str eam [3, 6, 14] . Furthermore, when the dimensionality of the features is high, the computational complexity and the storage requirement grow dramatically. Thus in this paper we propose a novel algorithm which can satisfy: (1) , it is supervised which is more suitable to classification problems than the common used unsupervised one, PCA ; (2), it has no calculation limitations such as the class number limitation and singularity problem faced by LDA, in addition it can give the same or better performa nce than LDA for real classification tasks; (3), the computa tional complexity is lower than the traditional matrix decomposition based algorithms PCA and LDA; (4), it can deal with data streams in a scalable way. learning approach with incremental learning as Supe rvised Kampong Measure (SKM). Note SKM is just a name which has no physical meaning. Intuitively in the h istory of human being, to avoid hurts of animals, the resi dents of the same remote antiquity Kampong always live toget her as close as possible, on the other hand, to avoid w ar and get enough food, the residents of different kampong s always live as far as possible. Similar to the cust om of kampong residents, we design SKM aim at assigning d ata points as close as possible to their corresponding class means, and simultaneously, assigning data points as far as possible to the other class means in the low dimens ional subspace.
 our proposed measure does not depend on the number of training data X  X  classes; and it has no singularity problem. Moreover, it outperforms PCA in classification prob lems since PCA discards the valuable class information. Our experimental results on various data sets show that this algorithm is effective for classification problems compared to PCA or LDA. Particularly, experiments performed on Reuters Corpus Volume 1 (RCV1) [8], whose dimension is about 300 X 000 and the samples number is more than 800 X 000, demonstrate the scalab le property of SKM on a very large scale dataset. We t ake unsupervised PCA as a baseline of SKM on the large scale data. In addition, based on the comparative s tudy of [17], supervised IG is the best one among several f eature selection algorithms for text categorization proble ms. Thus we also utilize IG as baseline of SKM. Since i t is very hard to perform LDA on such a large dataset, w e only conduct LDA on general datasets. This large sc ale experiment shows that the F1 value of our algorithm in a 3-dimensional subspace could outperform PCA and IG in even 500-dimensional subspaces. Section 2, we introduce some necessary background knowledge on subspace learning, such as the PCA and LDA algorithms. In Section 3, we will give the form al problem statement. Following that, we present the derivation of our proposed approach and its increme ntal learning algorithm in Section 4. In Section 5, we demonstrate the experimental results on the synthet ic datasets and the real data. Conclusion of this pape r is given in Section 6. Some detailed proof could be fo und in the appendix. in real tasks such as Web document classification a nd face recognition nowadays. It aims at finding a projecti on matrix which could efficiently project the data fro m the original high-dimensional feature space to a much l ower dimensional representation under a particular crite rion. Different criterion will yield different subspace l earning algorithm with different properties. Principal Comp onent Analysis (PCA) and Linear Discriminant Analysis (LD A) are two most widely used linear subspace learning approaches. are d -dimensional column vectors, where N is the number of samples, and that U is the sample matrix with ( ) u i as its i th column. PCA aims to find a subspace whose basis vectors correspond to the directions with maximal variances. It projects the original data into a p-dimensional ( p &lt;&lt; d ) subspace. The new p -dimensional feature vector can be computed as T y W u = , where the projection matrix and its column vectors correspond to the p leading eigen-vectors of the centralized covariance matrix T C UU = . Here uppercase T stands for transpose of matrix. least square error, and finds out the most representative features. The objective function of PCA is, In other words, the projection matrix W is achieved by maximizing the objective function ( ) J W . has effective incremental learning algorithm [2] which could process large scale streaming data. However, it ignores the class label information which is very valuable for general classification tasks. Discriminant Analysis (FDA), was proposed to pursue a low dimensional subspace that can best discriminate the samples from different classes. Suppose d p W R  X   X  is the linear projection matrix; LDA aims to maximize the so-called Fisher criterion, Where are called the Inter-class scatter matrix and the I ntra-class scatter matrix respectively, where c is the number of classes, m is the mean of all samples, i m is the mean of the samples belonging to class i and i p is the prior probability for a sample belonging to class i . The projection matrix W can be obtained by maximizing the objective function ( ) J W . Through simple mathematical derivation, it is to solve the following generalize d eigen-vector decomposition problem: the samples and is a supervised algorithm. There ar e at most c -1 nonzero eigen-values, so the upper bound of p is c -1; and at least d c + sample data is required to make it possible that w S is not singular. These limit the application of LDA. Furthermore, it is difficult fo r LDA to handle large-sized datasets when the feature-spa ce dimension is high. For example, as in the Reuters C orpus Volume 1, the feature dimension is about 300,000 wh ere it is impossible to conduct the generalized eigen-v ector decomposition on a computer with moderate configura tion. learning approaches on a group of synthetic data. T he stars and triangles are two dimensional data points belong to two different classes. The straight line is the one dimensional subspace found by PCA and the broken li ne is the one dimensional subspace found by LDA. It is obvious that if we project the data into the subspa ce calculated by LDA, they will be separated easily. However, if we project the data into the subspace calculated by PCA, the samples of two classes will be mixed together. As a conclusion, the unsupervised P CA project the data based on the overall distribution of dataset while supervised LDA project the data based on the class distribution of dataset. This is one of the reasons why supervised subspace learning approaches are always better than unsupervised ones on some classification tasks . learning approaches attempt to find a projection ma trix which could efficiently project the data from the o riginal feature space to a target representation under a ce rtain optimization criterion. In mathematical terms, supp ose that we are given N training data ( ) d u i R  X  , 1, 2,... i N = the number of classes. The mean of class i is i problem then is to give an objective function ( ) J W satisfies the properties listed below. We can then solve this objective function ( ) J W in order to produce the subspace. The properties we wish the objective function ( ) J W to satisfy are: words, it should outperform at least PCA and at the same time, perform the same or better than LDA for classification problems; the dimension of subspace should not be limited by the class size. Furthermore, the computation should no t be constrained by the singularity problems; i.e. suitable for incremental learning with much lo wer complexity than batch calculation. In other words, it should support both batch as well as online learnin g. of animals, the residents of the same remote antiqu ity Kampong always live together as close as possible, on the other hand, to avoid war and get enough food, the residents of different kampongs always live as far as possible. We are motivated to make use of a similar idea in subspace learning. This is the reason why we cal l our criterion SKM. We measure the distance between a da ta point and all the class centers for subspace learni ng. In other words, we wish to make the distance between a data point and its corresponding class center as short a s possible in the extracted low dimension subspace, a t the same time, we wish to make the distance between dat a points and the inhomogeneous class centers as far a s possible. algorithm for subspace learning. For a high dimens ional data point that belongs to class 2, suppose P1, P2, P3 are three possible positions after projected to the 2 d imension subspace. Under our motivation, possible position 2 (P2) is better than possible position 1 (P1) since it is closer to its own class mean. Moreover, P3 is better than P2 due to the reason that P3 is one of the closest positions to its own class mean among the three possible positions, and is the furthest to the other class means. 
Figure 2. An intuition explanation of our proposed points are d -dimensional column vectors in Euclidean Space, thus the distance between the th i sample data and the mean of class j could be represented by, dis u i m u i m u i m u i m =  X  =  X   X  (1) ( ( ) ) ( ( ) ) {( ( ) )( ( ) ) } T T j j j j u i m u i m tr u i m u i m  X   X  =  X   X  (2) where { } tr  X  is the trace of a given matrix. In other words, we have Thus we can use the trace of scatter matrix ( ( ) )( ( ) ) T j j u i m u i m  X   X  to measure the distance between Suppose that d p W R  X   X  is the linear projection matrix to be found by our algorithm through which a new p -dimensional feature vector can be computed as T y W u = . The distance between the th i sample data and the mean of class j in the low dimensional subspace then could be denoted by, class has different prior probability. Intuitively, the stronger a kampong is, to avoid conflict and hurt, the farther other kampong X  X  residents should live away from it. In terms of subspace learning, the larger prior pro bability of a class is, the larger distance between its clas s mean and a data point of other class should be. This lea ds to a weighted form of formula (4). Suppose that i l stands for the class label of ( ) u i and different class centers with a determined data poin t (we discuss how to set these weights in the next sectio n). Then the measurement of distance between ( ) u i and j m is: kampongs except for his own, we should maximize the distance between a data point ( ) u i and all other class centers in the low dimensional subspace, i.e. we sh ould maximize, between a data point and its corresponding class ce nters after being projected on a low dimensional subspace , we minimize, To get a unified formula, we define a  X  -function as, Then we can combine formular (6) and (7) as, Note that if formula (9) gets its maximum val ue, then our original motivation i.e. assigning data points as close as possible to their corresponding class means, mea nwhile, assigning data points as far as possible to the inhomogeneous class means is satisfied for a given data point ( ) u i . And then for all the sample data, the full criterion could be write as, ( ) { ( { ( , ) ( ( ) )( ( ) ) }) }
J W tr W E l j e u i m u i m W where { } E  X  is the expectation of a random variable and learning is transformed into an optimization proble m. In the equation (11), we exercised freedom to multiply W with some nonzero constant. Thus, we additionally r equire that W consists of unit vectors, i.e. W w w w = L and 1 T k k w w = . In addition, we require the projection matrix W to be orthogonal matrix in Euclidean space, i.e. 0 T k l w w = if k l  X  . Then the optimization problem of the proposed objective func tion (10) is translated to the following constraint opti mization problem: where I is the identity matrix. The problem can be restate d as, objective function (10). Note i j l  X  indicates that the sample ( ) u i does not belong to class j , in which situation we want to separate ( ) u i from samples of class j dimensional subspace. As demonstrated in section 4. 1, the more samples in class j , the larger the weights are needed for class j , i.e. the longer the distance is needed to separat e them. Alternatively, when i j l = , we wish to associate ( ) u i with samples of class j in the p -dimensional subspace. Intuitively from the global perspective, if a kampo ng has a few residents, then everyone is very important to t his society. All people should live very close for safe ty. However if a kampong has a lot of residents, people can live a little far away from each other since the lo ss of a single person will not affect the kampong much. Thu s the more samples in class j , the smaller the weights should be for class j . the following constraints: (a), if ( ) u i does not belong to class j , the weight , with the prior probability of class j ; (b), in contrast, if ( ) u i belongs to class j , the weight , decreasing with the prior probability of class j ; (c), the third is a common constraint that all weights must satisfy together with his own kampong is more important tha n live far away from other kampongs. In other words, the distance between a data point and its corresponding class mean should be more important than the distance bet ween the data point and all the other class centers, i.e . e e these constraints. As an example, we assign the wei ghts which can satisfy (a) ~ (d) by Theorem 1. Theorem 1 , the weights satisfies all the constraints (a) ~ (d) discussed a bove when 1 a  X  , where M is a positive real const used to normalize the weights (a simple proof is given in a ppendix) Without loss of generality, we could use to represent the weights in our problem. This is du e to the reason that multiplying by a constant will never af fect the projection matrix, mathematically arg max { } arg max { ( ) } function as, and then the objective function can be rewritten as , ( ) { ( { ( , )( ( ) )( ( ) ) }) }
J W tr W E q l j u i m u i m W arg max T p k k c k w S w =  X  subject to some constraints. By introducing a Lagrangian function below, where k  X  are the Lagrange multipliers. At the saddle point, the derivatives of L must vanish, leading to c k k k Thus, the columns of W are eigen-vectors of the criterion matrix c S . Therefore, ( ) J W is maximized when W is composed of the first p leading eigen-vectors of c S . proposed algorithm has no singularity problem since it need not to calculate the inversing of a matrix. I n addition, our proposed algorithm is not limited by the number of classes. classical Singular Value Decomposition (SVD), but t he the smaller value between the sample number and the data dimension. However, in real applications, we allow data to be streamed from a data source, such that the da ta are incrementally received. Furthermore, when the dimen sion of the data set is high, both the computational and storage costs grow fast. Thus, an incremental method is hig hly desired to compute an adaptive subspace when the da ta arrive sequentially. as { ( )} u n , where n =1, 2.... Our algorithm aims to dimension of the transformed data, i.e. the final s ubspace dimension. The criterion scatter matrix of step n after learning from the first n samples can be written as, 
S n q l j u i m i u i m i where ( ) j m i is the center of class j at step i . value of A is  X  and the corresponding eigen-vector is  X  . approximate iterative eigen-vector computation formulation with v  X  X  = : v n S n n Then the eigen-vector can be directly computed as / v v  X  = . For iterative calculation, let formulation: ( ) ( , )( ( ) ( ))( ( ) ( )) v n q l j u i m i u i m i Through simple algebra derivation, we can get ( ) ( 1) (22) v n v n For initialization, we set (1) u as the first sample. So, it helps to generate  X  X bservations X  only in a complementary space for computation of the higher o rder eigen-vectors. To compute the ( 1) th j + eigen-vector, we vector from the data, orthonormalization is avoided and the orthogonality is always enforced when the convergence is reached, although not exactly so at early stages. A very sim ilar convergence proof of this algorithm could be found in [15]. Then the full algorithm is computing (22) (23 ) iteratively from the initially value. input samples is ( ) Ncdp O , where c is the number of classes, d is the dimension of the original data space, and p is the target dimension, which is linear with each factor. Furthermore, when handling each input sample, SKM only need to keep the learned eigen-space and sever al first-order statistics of the past samples, such as the mean and the counts. Hence, SKM is scalable which can ha ndle large scale and continuous data. of experiments. In the first set, we tested the inc remental algorithm on a very high dimensional and large data set RCV1 whose dimension is about 300,000. In the seco nd experiment, we used a synthetic dataset generated u sing a normal distribution. The purpose of this experimen t is to illustrate the subspaces learned by LDA, PCA and SK M algorithms follow our initial intuition. In the thi rd set of experiments, we applied our method to some UCI data sets [5] to compare the classification performance with other approaches. SKM on a high dimensional and large scale data set, we tested our algorithm on the Reuters Corpus Volume 1 (RCV1). We show the performance of a= 1 in this test; we observed that using other values of a resulted in similar performance. algorithm with other subspace learning algorithms, we constructed classification experiments on RCV1. The dimension of each sample data is about 300,000, whe re each dimension is defined by a keyword. codes (CCAT, ECAT, GCAT, and MCAT) in the  X  X opic Codes X  hierarchy, which contains 789,670 documents. Then we applied a five-fold cross validation on the data. We split them into five equal-sized subsets, where in each experiment four of them are used as the training se t and the remaining one is left as the test set. The expe rimental results reported are the average of the five runs. The detailed experiments following the 5 steps listed b elow, evaluation measure for text classification, of the incremental algorithm using Support Vector Machine (SVM) as the classifier. In this experiment, the Information gain (IG) and Incremental Principal Component Analysis (IPCA) algorithms are used as baselines. subspace dimension. For example  X  X G500 X  means the 500-dimension subspace found by IG. The y-axis of Figure 3 are Micro-F1 and Macro-F1 respectively. Th e larger F1 value is, the better classification perfo rmance is achieved. The x-axis is the number of training samp les used in our incremental algorithm. Note that we use all the training data when we train the baseline algorithms for dimension reduction. From this experiment, we can observe that the 3 -dimensional subspace calculated by incremental SKM is much better than even the 500-dimensional subspace of Principal Component Analysis. Moreover, for IG, one of the most popular dimensionality reduction approache s for large scale text data, SKM on 3-dimensional subspac e outperforms it on the same scale and SKM3 has comparable performance with IG500.

SKM outperforms PCA for classification problems since the former is supervised approach while the l atter is unsupervised which ignores the valuable class label information. SKM outperforms IG due to the reason t hat IG is feature selection approach. It directly selec ts features from the original data space while SKM use a transformation (projection) matrix to reduce the ve ry high dimension of data. Since the text data are sparse d ataset, feature selection approaches always reduce many dif ferent data points to the same zero vectors when the reduc ed dimension is low. 
Table 1 gives the comparison of all the different approaches considered by us.
 SKM YES YES YES YES PCA NO YES YES YES LDA YES NO YES NO (A) , supervised approach which is usually better than unsupervised approaches for classification; scale dataset; large scale sparse text data to very low dimensiona l space; structure. For example, for a large scale dataset w ith the dimension more than 10 X 000, if the number of classe s is only 3, the subspace dimension by LDA could not bey ond 2. by generating a 3-dimensional dataset with two clas ses for intuition. As an example, each class consists of 10 0 samples following the norm distribution with means (0,0,0) and (5,5,5). Figure 4-(a) shows a scatter plot of t he data set. (b), (c), (d) (e) (f) are low dimensional proj ection of original data by different subspace learning approa ches. In (b) we project the data into 1-dimensional subspace by PCA; in (c) and (d) we project the data into 1-dime nsional subspace by SKM and LDA respectively; in (e) and (f ) we project the data into 2-dimensional subspace by PCA and SKM respectively. mixes these two classes of data in both one dimensi on and two. Though LDA subspace could separate them in fig ure (d), the dimension of LDA subspace could not beyond one, i.e. we can not project the data into 2-dimensional subspace due to the limitation of class number. (c) and (f) show that our proposed approach could separate thes e two class in any dimensional subspace for this syntheti c dataset. is not optimal for classification tasks and mixes t he two classes; the subspace dimension of LDA could not be yond one due to its limitation. It is clear that our pro posed algorithm outperform PCA and do not limited by the number of classes. The UCI machine learning dataset is a reposit ory of databases, domain theories and data generators that are used for the empirical analysis of machine learning algorithms. For each UCI dataset that do not provid e training-testing split, we used repeated holdout me thods by repeatedly separating them into two folds random ly. SKM, PCA and LDA are then applied to the training data to find the subspace. The k-nearest neighbor classifier is used to classify these testing data. By using the same classifier, w e use classification error rate to evaluate the performan ce of different subspace learning algorithms for classifi cation tasks. 6 UCI subsets are utilized in this paper for experiments. dimension of data is 4. 26 classes. The dimension of data is 617. dimension of data is 13. 
The dimension of data is 16. classes. The dimension of data is 6. dimension of data is 2. Figure 5 shows the picture of error rate with subspace dimension on these subsets of UCI. It seem s that supervised subspace learning approaches outperform unsupervised PCA most of the time (a), ( b), (c), (d), (e). However, the subspace dimension was limited for LDA. Moreover, SKM still could outstand even on the dataset which PCA is more suitable than
LDA (d). Note we choose a=1,2,3 in this paper and plot the average solution. To measure the convergence ability of incremental S KM, we show the convergence curve on two subsets of UCI , IRIS and Monk X  X  Problem, for intuition. Since correlation between two unit eigen-vectors is repre sented by their inner product, and the larger the inner pr oduct is, the more similar the two eigen-vectors are. Figure 6 shows the inner product between eigen-vectors found by th e proposed incremental algorithm in each step and the eigen-vectors found by the batch approach. The x-ax is is the number of training data and the y-axis is the i nner-product. From this Figure we can see that, incremen tal SKM can converge very fast. The CCIPCA [13] is the incremental PCA algorithm involved in this paper wh ich has been used in section 5.1. 
In this paper, we proposed a novel supervised subsp ace learning algorithm called as Supervised Kampong Measurement. The incremental algorithm of this new criterion is also presented. In contrast to traditi onal LDA, the available subspace dimension with this measurem ent is not limited by the number of the classes and it is efficient and has no singularity problem in computation. More over, it is a highly scalable algorithm. In other words, it can process large scale data incrementally. The extensi ve experiments on both synthetic and real datasets demonstrated that it outperforms PCA and even LDA o n classification tasks. One of our future work is to give an algorithm which can learn the optimal parameter a automatically in the designing of weights. [1]Al-Harbi, S.H. and Rayward-Sm, V.J. The Use of a Supervised k-Means Algorithm on Real-Valued Data wi th Applications in Health. Lecture Notes in Computer Science , Volume 2718 / 2003 . 575 -581. [2]Artae, M., Jogan, M. and Leonardis, A., Incremen tal PCA for On-line Visual Learning and Recognition. In Proceedings of the 16th International Conference on Pattern Recognition , (Quebec City, QC, Canada, 2002), 781-784. [3]Asai, T., Arimura, H., Abe, K., Kawasoe, S. and Arikawa, S., Online Algorithms for Mining Semi-structured Data Stream. In Proceedings of the Proceedings of the 2002 IEEE International Conferen ce on Data Mining (ICDM'02) , (Maebashi City, Japan, 2002), 27. [4]Balakrishnama, S. and Ganapathiraju, A. Linear Discriminant Analysis -A brief Tutorial, Institute for Signal and Information Processing, MS, 1998. [5]C.L., B. and C.J., M. UCI Repository of machine learning databases Irvine. CA: University of California, Department of Information and Computer Science. [6]Domingos, P. and Hulten, G., Mining High-Speed D ata Streams. In Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , (Boston, MA, 2000), 71-80. [7]La, F.D., Torre and J.Black, M. A Framework for Robust Subspace Learning. International Journal of Computer Vision , 54 (1-3). [8]Lewis, D., Yang, Y., Rose, T. and Li, F. RCV1: A new benchmark collection for text categorization resear ch. Journal of Machine Learning Research . [9]Li, H., Jiang, T. and Zhang, K., Efficient and R obust Feature Extraction by Maximum Margin Criterion. In Proceedings of the Advances in Neural Information Processing Systems 16 , (Vancouver, Canada, 2004), MIT Press. [10]Martinez, A.M. and Kak, A.C. PCA versus LDA. IEEE Transactions on Pattern Analysis and Machine Intelligence , 23 (2). 228-233. [11]Torkkola, K., Linear Discriminant Analysis in Document Classification. In Proceedings of the , (2001), 800-806. [12]Wagstaff, K., Cardie, C., Rogers, S. and Schr, S., Constrained K-means Clustering with Background Knowledge. In Proceedings of the ICML , (Williams College, Williamstown, MA, USA, 2001), 577-584. [13]Weng, J., Zhang, Y. and Hwang, W.-S. Candid Covariance-free Incremental Principal Component Analysis. IEEE Trans. Pattern Analysis and Machine Intelligence , 25 (8). 1034-1040. [14]Wu, A., Mining Data Streams: A Survey of Algorithms and Applications. In Proceedings of the http: //www-courses.cs.uiuc.edu/~cs412/slides/Stream1AWu. pdf . [15]Yan, J., Zhang, B.Y., Yan, S.C., Chen, Z., Fan, W.G., Yang, Q., Ma, W.Y. and Cheng, Q.S., IMMC: Increment al Maximum, Marginal Criterion. In Proceedings of the To Appear in SIGKDD'04 , (Seattle,WA, 2004). [16]Yu, H. and Yang, J. A direct LDA algorithm for high-dimensional data with application to face recogniti on. Pattern Recognition , 34. 2067-2070. [17]Yang, Y. and Pedersen, J.O., A comparative Stud y on Feature Selection in Text Categorization. In Proceedings of the 14th International Conference on Machine Learning (ICML) , (1997), 412-420 Theorem 1 , Weights (13) satisfies all the constraints listed in section 4.2 when 1 a  X  , Proof: Since 1 a  X  and 0 1 j p  X   X  , we can obviously seen from (13) that if i j l  X  , on the other hand, if i j l = , when j p increases. Thus the first two constrains are satisfied. The third constrain is a common one. to 1 a  X  and M is a positive number used to normalize the weights. For the latest one, since 1 j p =  X  and The latest constrain is satisfied if and only if 1 a  X  . 
