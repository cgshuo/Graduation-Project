
We describe a content-based recommendation framework that explores the relative benefits of different sources of data for modelling curated collections and user profiles.
In Scoop.it each topic is a collection of curated pages and has an associated topic title and description. Each page in a topic has a URL, a title, and a summary (snippet) descrip-tion. In addition we can also extract a set of content terms from the page directly. This means that we can represent and index topics at different levels of granularity: 1. TopicTitleDesc -The terms in the topic X  X  title and 2. PageTitleSumm -The combination of terms from the 3. PageContent -The combination of the top-N terms
In addition to representing topics with raw terms as above we also produce feature-based descriptions by apply-ing LDA [1] and LSI [3] to both page-level and content-level terms to produce LSIPageTitleSumm , LSIPageCon-tent , LDAPageTitleSumm , LDAPageContent representa-tions.

This produces 7 different content types that can be used as the basis of collection indexing and user profiling. In the case of the former we use a standard indexing approach in which each topic is represented as a document vector based on the different content types. So in the case of TopicTitleDesc each topic ( t i ) is represented as a vector of the topic X  X  title and description terms; we refer to this as I ( t i , T opicT itleDesc ).

Users in Scoop.it are associated with the topics they cu-rate (create) and other topics they choose to follow. There-fore we can profile a user u i based on her curated topics ct as the source of the profile data. And then we can repre-sent each of these profile types using one of the 7 different approaches above. In other words, if we profile u j from her curated topics using LSIPageContent then the profile will be made up of the LSI features extracted from the page content terms of all pages in the user X  X  curated topics only; we refer to this as P ct ( u j , LSIP ageContent ).
We adopt a simple retrieval-based content recommenda-tion approach, treating user profiles as queries against the topic index to recommend the n most similar topics. In fact, we take a weighted retrieval approach using Lucene X  X  TF-IDF weighting metric during retrieval [6]; for a given user u , we score a topic t i based on those terms/features it shares with the user X  X  profile P = P src ( u j , type u ), in proportion to their frequency in u j  X  X  profile and inversely proportional to their frequency in the topic index I = S t whole.

Score ( u j , t i , src, type u , type I ) = X
Once again it is worth stressing that our aim here is not to propose a novel recommendation technique but rather establish an approach that facilitates a like-for-like compar-ison between different types of profile/index representation and profile source. The above combinations of profile rep-resentation and source, and indexing data, accommodate many different combinations of recommendation techniques: 3 sources of profiling data, 7 types of profile, and 7 types of index for a total of 147 recommendation configurations.
The purpose of this evaluation is to consider how the dif-ferent types of indexing data and profiling sources impact overall recommendation quality when it comes to suggesting new topics to users. To do this we ran an offline, training-test style recommendation study over Scoop.it data scraped from their API during October-November 2012. This data include 22,000 unique topics covering more than 2 million pages and we focus on a subset of 845 active users who fol-low at least 20 topics; on average these users follow 90 topics and have created 5 topics of their own, each of which con-tains at least 100 pages. In this study the training instances are made up of user profiles containing all of the user X  X  cre-ated topics plus random selections of 10% of their followed topics. The remaining followed topics serve as test instances. We do this 10 times to select different sets of followed topics for training and average our results across these folds.
For each set of training-test data we create all possible configuration combinations (7 x 7 x 3) of profile and recom-mendation index as described above. For each user a set of n ( n = 5, 10, 20, 30) recommendations is made, and compared to the followed topics in the test profile in order to compute the average precision . Of course there are other standard measures we could apply here such as recall , F1-measure or normalised discounted cumulative gain (NDCG) ; due to limited space however, we only consider precision at this point. The results are presented in Figure 2; the rows repre-sent different types of profile data and the columns different types of index data. There are actually 3 sets of graphs pre-sented in this figure: (1) the main core of 7 x 7 graphs shows the individual results of each of the (7 x 7 x 3) configura-tions that represent our recommendation design space; (2) the bottom row of 7 graphs ( average topic index ) represent summary precision data, for a given index configuration, av-eraged across the profile configurations (the average of the columns); and (3) the rightmost column ( average user pro-file ) shows a similar averaging across the rows, each profile configuration averaged across the index configurations.
There is a lot of result data presented but a number of interesting patterns can be observed. Focusing on the core graphs we can see that there is considerable precision vari-ation across the different profiling and indexing configura-tions; precision results vary from &lt; 10% to almost 30%. And so the choice of profiling and indexing data matters. Gener-ally speaking better results are observed when using curated topics versus followed or curated plus followed topics. Even though a given profile is only made up of a small fraction of curated topics this information provides a much clearer recommendation signal. For example, the results when us-ing page title and summary information for profiling show a clear benefit for curated topics with precision ranging from about 0.25 to 0.15 (as n increases) compared to 0.2 -0.1 for the followed and curated plus followed source configuration. Interestingly, this result aligns with the findings in [2] which show that the content in a user X  X  own tweets (on Twitter) are a better signal of her information seeking preferences, than the content in the tweets of the people she follows.
The row and column averages help us to draw some gen-eral conclusions. We can see that extracting LDA features rarely helps, at least not as much as using LSI. And generally speaking basing recommendations on topic-level information from the topic title and description produces limited preci-sion results. At the same time, using detailed page content (i.e. extracting actual term-content from pages) performs similarly poorly when used as the profile query but performs well when used to index topics; compare the page content configurations in the average user profile summary column to the average topic index summary row. Generally speaking it appears that page-level content, in the form of page title &amp; summary configurations (using terms and LSI derived fea-tures) provides the right level of representational clarity to deliver the best performing recommendations. Specifically building a term-based index from page title &amp; summary data and using LSI to extract page title &amp; summary profiles gives the best overall recommendation results.
Our evaluation using Scoop.it data covered a comprehen-sive design space of recommendation configurations from which some general conclusions can be drawn. First and foremost, we demonstrated that relevant recommendations can be delivered even when evaluated against a tradition-ally conservative precision metric that focuses on those top-ics/collections only already followed by users. We often achieve precision scores up to 0.3 indicating that 30% of the recommended topics were found to be relevant in the sense that the user already followed them. Generally speak-ing, curated topics (those topics the user has created them-selves) provided a strong recommendation signal than those they followed and delivered better precision, despite their relative sparsity compared to followed topics. And finally, the representational granularity offered by the intermediate page title &amp; summary data provided superior precision re-sults, although indexing (but not profiling) using detailed page content information also performed well.
 This work is supported by Science Foundation Ireland under grant number 07/CE/I1147, Ministry of Education Malaysia and Universiti Teknikal Malaysia Melaka. The INSIGHT Centre for Data Analytics is supported by Science Founda-tion Ireland under Grant Number SFI/12/RC/2289. [1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [2] J. Chen, R. Nairn, L. Nelson, M. Bernstein, and E. Chi. [3] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. [4] K. Duh, T. Hirao, A. Kimura, K. Ishiguro, T. Iwata, [5] D. Greene, F. Reid, G. Sheridan, and P. Cunningham. [6] E. Hatcher and O. Gospodnetic. Lucene in action . [7] Z. Saaya, M. Schaal, R. Rafter, and B. Smyth.

