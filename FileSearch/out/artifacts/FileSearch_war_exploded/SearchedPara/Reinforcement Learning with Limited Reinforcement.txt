 Finale Doshi FINALE @ MIT . EDU Massachusetts Institute of Technology, Boston, USA McGill University, Montreal, Canada Massachusetts Institute of Technology, Boston, USA
Partially Observable Markov Decision Processes (POMDPs) have succeeded in planning domains that require balancing actions that increase an agent X  X  knowledge and actions that increase an agent X  X  re-ward. Unfortunately, most POMDPs are defined with a large number of parameters which are difficult to specify only from domain knowledge. In this paper, we present an approximation approach that allows us to treat the POMDP model parameters as additional hidden state in a  X  X odel-uncertainty X  POMDP. Cou-pled with model-directed queries, our planner actively learns good policies. We demonstrate our approach on several POMDP problems. Partially Observable Markov Decision Processes (POMDPs) have succeeded in many planning do-mains because they can reason in the face of uncertainty, optimally trading between actions that gather information and actions that achieve a desired goal. This ability has made POMDPs attractive in real-world problems such as dialog management (Roy et al., 2000), but such problems often require a large number of parameters that are difficult to specify from domain knowledge alone. Recent advances can solve POMDPs with tens of thousands of states (Shani et al., 2007), but learning in POMDPs remains limited to small problems (Jaulmes et al., 2005).
 Traditional reinforcement learning approaches (Watkins, 1989; Strehl et al., 2006; Even-Dar et al., 2005) to learning in MDP or POMDP domains require a reinforcement signal to be provided after each of the agent X  X  actions. If learning must occur through interaction with a human expert, the feedback requirement may be undesirable. The traditional approach also does not guarantee the agent X  X  performance during training. We identify and address three limitations in the traditional approach in this work: 1. Gathering sufficient training data for supervised learni ng may be prohibitively expensive. 2. Most approaches require the agent to experience a large penalty (i.e., make critical mistakes) to discover the con-sequences of a poor decision. 3. Accurate numerical reward feedback is especially hard to obtain from people, and inverse reinforcement learning (identifying the reward model without explicit reinforce-ment) poses its own challenges (Ng &amp; Russell, 2000). Our objective is to propose a framework for simultaneous learning and planning in POMDPs that overcomes the lim-itations above, allowing us to build agents that behave ef-fectively in domains with model uncertainty.
 We now discuss how our approach will address each of these three issues. To address the issue of long training periods, we adopt a Bayesian reinforcement learning ap-proach and express model-uncertainty as additional hid-den state. Bayesian methods (Dearden et al., 1999; Strens, 2000; Poupart et al., 2006; Jaulmes et al., 2005) have re-ceived recent attention in reinforcement learning because they allow experts to incorporate domain knowledge into priors over models. Thus, the system begins the learn-ing process as a robust, functional (if conservative) agent while learning to adapt online to novel situations. The do-main knowledge specified as a prior can also provide the agent with a basic understanding of potential pitfalls. Our work builds on previous Bayesian reinforcement learning approaches in that we provide both practical approximation schemes as well as guarantees on correctness and conver-gence.
 To ensure robustness toward catastrophic mistakes, we de-velop an active learning scheme that determines when addi-tional training is needed (typically active learning invol ves asking for a few labels from unlabeled data; in this work, the  X  X abel X  corresponds to asking for the optimal action at a particular point in time). If the agent deems that model uncertainty may cause it to take undue risks, it queries an expert regarding what action it should perform. These queries both limit the amount of training required and al-low the agent to infer the potential consequences of an ac-tion without executing it. Depending on the domain, we can imagine that different forms of information are most readily available. For example, in a navigation task, it may be straight-forward to query a state oracle (i.e., a GPS sys-tem) for a location. Similarly, rewards may be easy to mea-sure based on quantities such as energy usage or time to goal. However, in other domains X  X articularly when work-ing with human-robot interaction and dialog management systems X  X olicy information may be more accurate; a hu-man user may know what he wishes the agent to do, but may be unable to provide the agent with an accurate state representation (which is often complex, for optimization purposes). In these domains, asking for policy informa-tion, instead of a traditional reward signal, also side-ste ps the issue of getting explicit reward feedback from a human user, which can also be inaccurate (Millet, 1998). In this work, we deal exclusively with policy-based queries. We are still left with the inverse reinforcement learning problem, as the user X  X  response regarding correct actions provides only implicit information about the underlying re -ward. To date, Bayesian reinforcement learning has suc-ceeded in learning observation and transition distributio ns (Jaulmes et al., 2005; Poupart et al., 2006), where updates have closed forms (such as updating Dirichlet counts); pre-vious inverse reinforcement learning work (Ng &amp; Russell, 2000) does not extend to the partially observable case. To overcome this issue, we use a non-parametric approach to model distributions over POMDPs; we demonstrate our ap-proach on several standard problems.
 We describe two practical contributions. First, we pro-pose an approximation based on minimizing the immedi-ate Bayes risk for choosing actions when transition, obser-vation, and reward models are uncertain. The Bayes risk criterion avoids the computational intractability of solv ing large, continuous-valued POMDPs; we show it performs well in a variety of problems. Second, to gather informa-tion about the model without assuming state observabil-ity, we introduce the notion of meta-queries . These meta-queries accelerate learning and help the agent to infer the consequences of a potential pitfall without experiencing i ts effects. They are a powerful way of gaining information, but they make the strong assumption that they will be an-swered. Fortunately, a number of decision-making prob-lems exist where this assumption is reasonable, particular ly in collaborative human-machine tasks (e.g. automated dia-logue systems and shared robot control scenarios). A POMDP consists of the n-tuple { S , A , O , T ,  X  , R , A , and O are sets of states, actions, and observations. The transition function T ( s  X  | s, a ) is a distribution over the states the agent may transition to after taking action a from state s . The observation function  X ( o | s, a ) is a distribution over observations o that may occur in state s after taking action a . The reward function R ( s, a ) specifies the imme-diate reward for each state-action pair. The factor  X   X  [0 , 1) weighs the importance of current and future rewards. In the POMDP model, the agent must choose actions based on past observations; the true state is hidden. The belief, a probability distribution over states, is a sufficient stat istic for a history of actions and observations. The belief at time t + 1 can be computed from the previous belief, b t , the last action a , and observation o , by applying Bayes rule: b t +1 ( s )= X ( o | s, a ) where P r ( o | b, a )= P If the goal is to maximize the expected discounted reward, then the optimal policy is given by: where the value function V ( b ) is the expected discounted reward that an agent will receive if its current belief is Q ( b, a ) is the value of taking action a in belief b . The exact solution to equation 3 is only tractable for tiny problems, s o we use a point-based approximation (Pineau et al., 2003). We assume that the sets S , A , and O are known. The POMDP learning problem is to determine the parameters of
T ,  X  , and R that describe the dynamics and objective of the problem domain. A Bayesian approach is attractive in many real-world settings because we may have strong no-tions regarding certain parameters, but the value of those parameters may be difficult to specify exactly. We place a prior over the model parameters to express our domain knowledge, and improve upon this prior with experience. If the state, action, and observation sets are discrete, T  X  are collections of multinomial distributions. As conju-gate priors, Dirichlet distributions are a natural choice o f prior for T and  X  . We use a uniform prior over expert-specified ranges for the reward function R . Together these priors specify a distribution over POMDP models. To build a POMDP that incorporates the model parameters into the hidden state, we consider the joint state space S  X  = S  X  M where M is the space of models as described by all valid values for the model parameters. Although S  X  is contin-uous and high dimensional, the transition model for M is simple (assuming the true model is static).
 The formulation above makes the agent aware of the un-certainty in the model parameters, and by trying various actions, it will be able to reduce uncertainty both in its state and in the parameters. However, the model informa-tion provided by the standard actions may be weak, and we would like the agent to be able to explicitly reduce model uncertainty in a safe manner. To allow for active learning, we augment the action space A of our original POMDP with a set of meta-queries { q an oracle (e.g., a domain expert) for the optimal action at a particular time step. We assume that the expert has ac-cess to the history of actions and observations (as does the agent), as well as the true POMDP model, and thus can ad-vise the agent on the optimal action at any particular time. The agent begins by confirming the action it thinks is best:  X  X  think a If the oracle answers to the negative, the agent follows with what it thinks is next best:  X  X hen I think a until it receives an affirmative response. The ordered list o f actions helps give the expert a sense of the agent X  X  uncer-tainty; if the agent is uncertain, the expert might advise it to Meta-queries may be applied in situations where an expert is available to guide the agent. Unlike the oracle of Jaulmes et al. (2005), the meta-queries ask for policy information, not state information, which can be important if optimiza-tion procedures make the state-space unintuitive to the use r (e.g., Williams and Young (2005)). In human-robot interac-tion, it may also simply be more natural to ask  X  X  think you want me to go to the coffee machine. Should I go there? X  which may be more natural than  X  X lease enter your most re-cent statement X  or  X  X lease enter our position coordinates.  X  We can think of these meta-queries simply as additional actions and simply attempt to solve the model-uncertainty POMDP with this augmented action space. However, such an approach quickly becomes intractable. Therefore, we will treat the meta-query as a special action to be taken if the other actions are too risky. We take the cost querying the user to be a fixed parameter of the problem. Table 1 summarizes our two-part approach to solving the model-uncertainty POMDP. First, given a history of ac-tions and observations, the agent must select the next ac-tion. Second, the agent must update its distribution over the model parameters given additional interactions with th e environment. In the most general case, both steps are in- X  Sample POMDPs from a prior distribution.  X  Complete a task choosing actions based on Bayes risk:  X  Once a task is completed, update prior (Section 4.2):
Performance and termination bounds are in 4.3 and 4.4. 4.1. Bayes-Risk Action Selection Let the loss L Q m ( b, a )  X  Q belief b according to model m . Given a belief p models, the expected loss E BR ( a ) = where M is the space of models, b lief according to model m , and a  X  tion for the current belief b a = arg max a  X  A BR ( a ) be the action with the least risk. In the passive learning scenario, our agent just performs If the risk of the least-risky action a  X  is large, the agent may still incur significant losses. We would like our agent to be sensitive to the absolute magnitude of the risks that it take s. In the active learning scenario, the agent performs a meta-loss is more than the cost of the meta-query. The series of meta-queries will lead us to choose the correct action and thus incur no risk.
 Intuitively, our criterion selects the least risky action n ow and hopes that the uncertainty over models will be resolved at the next time step. We can rearrange equation 4 to get: BR ( a )= The second term is independent of the action choice; to maximize BR ( a ) , one may simply maximize the first term: The Bayes risk criterion is similar to the Q MDP heuris-tic (Littman et al., 1995), which uses the approximation V ( b ) = max P s Q ( s, a ) b ( s ) to plan in known POMDPs. In our case, the belief over states b ( s ) is replaced by a be-lief over models p states Q ( s, a ) is replaced by an action-value function over beliefs Q ( b that the uncertainty over states will be resolved after the next time step. Our Bayes-risk criterion may be viewed as similarly assuming that the next action will resolve the uncertainty over models.
 Though similar, the Bayes risk action selection criterion differs from Q tions come from POMDP solutions and thus do fully con-sider the uncertainty in the POMDP state. Unlike Q we do not act on the assumption that our state uncertainty will be resolved after taking the next action; our approx-imation supposes that only the model uncertainty will be resolved. Thus, if the model stochasticity is an important factor, our approach will take actions to reduce state uncer -tainty. This observation is true regardless of whether the agent is passive (does not ask meta-queries) or active. In the active learning setting, the second difference is the meta-query. Without the meta-query, while the agent may take actions to resolve state uncertainty, it will never take actions to reduce model uncertainty. However, meta-queries ensure that the agent rarely (with probability takes a less than  X  -optimal action in expectation. Thus the meta-queries make the learning process robust from the start and allow the agent to resolve model uncertainty. Approximation and bounds: The integral in equation 4 is computationally intractable, so we approximate it with a sum over a sample of POMDPs from the space of models: There are two main sources of approximation that can lead to error in our computation of the Bayes risk:  X 
Error due to the Monte Carlo approximation of the in-tegral in equation 4: Note that the maximum value of the Q ( b the Hoeffding bound with sampling error  X  dence  X  , we will require n  X 
Error due to the point-based approximation of Q ( b
The difference Q ( b up to  X  density of the belief points. This result is directly from the error bound due to Pineau et al. (2003). To obtain a confidence  X  when calculating if the Bayes risk is greater than  X   X  , we combine these bounds, setting  X  samples n from equation 8. We note however that the Ho-effding bounds used to derive this approximation are quite loose; for example in the shuttle POMDP problem, we used 200 samples, whereas equation 8 suggested over 3000 sam-ples may have been necessary even with a perfect POMDP solver. 4.2. Updating the Model Distribution As described in Section 3, we initially placed Dirichlet pri -ors over the transition and observation parameters and uni-form priors over the reward parameters. As our agent in-teracts with the environment, it receives two sources of in-formation to update its prior: a history h of actions and observations and a set of meta-queries (and responses) Q . Given h and Q , the posterior p where Q and h are conditionally independent given m be-cause they are both computed from the model parameters. The history h is the sequence of actions and observations since p queries asked (and the expert X  X  responses). Each source poses a different challenge when updating the posterior. If the agent were to have access to the hidden under-lying state, then it would be straightforward to compute p
M | h ( m | h )  X  p ( h | m ) p M ( m ) counts to the appropriate Dirichlet distributions. Howeve r, when the state sequence is unknown, the problem becomes more difficult; the agent must use its belief over the state sequence to update the posterior. Thus, it is best to perform the update when it is most likely to be accurate. For ex-ample, in a robot maze scenario, if the robot is lost, then estimating its position may be inaccurate. However, once the robot reaches the end of the maze, it knows both its start and end position, providing more information to recover its path. We focus on episodic tasks in this work and update the belief over models at the completion of a task. The meta-query information poses a different challenge: the questions provide information about the policy, but our priors are over the model parameters. The meta-queries truncate the original Dirichlet as models inconsistent wit h meta-query responses have zero likelihood. We approxi-mate the posterior with a particle filter. Recall that sequential Monte Carlo techniques let us repre-sent a distribution at time t using a set of samples from time t  X  1 using the following procedure (Moral et al., 2002): where K ( m, m  X  ) is an arbitrary transition kernel and p the probability of the model m under the true posterior. Sampling a new model requires solving a POMDP, which is computationally expensive and thus may be undesirable while an agent is in the process of completing a task. Thus, we do not change our set of samples during a task (that is, K ( m, m  X  ) =  X  m ( m  X  ) where  X  () is the Dirac delta func-tion). We begin at time t  X  1 with a set of models and weights w els. If a meta-query occurs at time t , then p p ( Q t | m ) p M,t  X  1 ( m ) , and the weight update reduces to In theory, the p ( Q | m ) should be a delta function: either the model m produces a policy that is consistent with the meta-query ( p ( Q | m ) = 1 ), or it does not ( p ( Q | m ) = 0 In practice, approximation techniques used to compute the model X  X  policy are imperfect (and expert advice can be in-correct) so we do not want to penalize a model that occa-sionally acts incorrectly. We model the probability of see-ing k incorrect responses in n trials as a binomial variable with parameter p a meta-query due to the approximate solver. This value is hard to characterize, of course, and is problem-specific; we used p Over time, samples taken from the original prior may no longer represent the posterior well. Moreover, if only a few high weight samples remain, the Bayes risk may appear smaller than it really is because most of the samples are in the wrong part of the space. We also need to update the models based on the history information, which we have ignored so far. We do both these steps at the end of a task. Action-Observation Histories: Dirichlet Update. We first discuss how to update the posterior p ( m | h ) in closed form. Recall that updating the Dirichlet counts given ac-tions and observations requires knowing the underlying state history, and our agent only has access to history of actions and observations. We therefore update our parame-ters using an online extension of the standard EM algorithm (Sato, 1999). In the E-step, we estimate a distribution over state sequences in the episode. In the M-step, we use this distribution to update counts on our Dirichlet priors. On-line EM guarantees convergence to a local optimum. For the E-step, we first estimate the true state history. Two sources of uncertainty are present: model stochasticity an d unknown model parameters. To compute the expectation with respect to model stochasticity, we use the standard forward-backward algorithm to obtain a distribution over states for each sample. Next, we combine the distributions for each sample based on the sample X  X  weight. For exam-ple, suppose a sampled model assigns a probability p being in state s . Then the expected probability  X  p ( s ) ing in state s is  X  p ( s ) = P n our distribution over models, so this sum approximates an expectation over all models.
 Next, we update our Dirichlet counts based on both the probability that a POMDP assigns to a particular state and the probability of that POMDP. Given an action a and ob-servation o corresponding to time t , we would update our Dirichlet count for  X  for each state s . This update combines prior knowledge about the parameters X  X he original value of  X  new information from the current episode,  X  p ( s ) . Resampling Models. As is standard in sequential Monte Carlo techniques, we begin by resampling models accord-ing to their weights w may get selected many times for inclusion in the resam-pled set of models, while a model with low weight may disappear from the sample set since it is no longer repre-sentative of the posterior. Once resampled, each model has equal weight. Before we begin the next task, we perturb the models with the following transition kernel:  X  Draw a sample m  X  from p  X 
With probability p  X 
With probability 1  X  p the model parameters of m and m  X  so that m  X  = p  X  m  X  + (1  X  p )  X  m with the convexity parameter being chosen uniformly at random on [0 , a ] .
 We reduce the probability p actions continue, encouraging large exploration earlier o n and fine-tuning in later interactions. We set a to 0.2 in our experiments. Based on this sampling procedure, the weight (keeping in mind that after resampling, all models had equal weight) of the transitioned model m  X  is given by: where, K ( m, m  X  ) = p sampled model and and K ( m, m  X  ) = (1  X  p if we perturb m via a convex combination. 4.3. Performance Bounds model. From our risk criterion, the expected loss at each action is no more than  X  . However, with probability  X  , in the worst case, the agent may choose a bad action that takes it to an absorbing state in which it receives R To determine the expected discounted reward, we consider a two-state Markov chain. In state 1, the  X  X ormal X  state, the agent receives a reward of R  X   X  , where R is the value the agent would have received under the optimal policy. In state 2, the agent receives R transitions in this simple chain and the values of the states : Solving this system and noting that the agent begins in state 1 with probability 1  X   X  and state 2 with probability  X  , the 4.4. Model Convergence Given the algorithm in Table 1, we would like to know if the learner will eventually stop asking meta-queries. We state that the model is converged if BR ( a  X  ) &gt;  X   X  for all histories (where  X  is the cost of a meta-query). Our conver-gence argument involves two steps. First, let us ignore the reward model and consider only the observation and tran-sition models. As long as standard reinforcement learning conditions X  X eriodic resets to a start state and informatio n about all states (via visits or meta-queries) X  X old, the pri or will peak around some value (perhaps to a local extremum) in a bounded number of interactions from the properties of the online EM algorithm (Sato, 1999). We next argue that once the observation and transition parameters have converged, we can bound the meta-queries required for the reward parameters to converge.
 Observation and Transition Convergence. To discuss the convergence of the observation and transition distribu -tions, we apply a weaker sufficient condition than the con-vergence of the EM algorithm. We note that the number of interactions bounds the number of meta-queries, since we ask at most one meta-query for each normal interac-tion. We also note that the counts on the Dirichlet pri-ors increase monotonically. Once the Dirichlet parameters are sufficiently large, the variance in the sampled models will be small; even if the mean of the Dirichlet distribution shifts with time, no additional meta-queries will be asked. The specific convergence rate of the active learning will de-pend heavily upon the problem. However, we can check if k additional interactions are sufficient such that the proba-bility of asking a meta-query is p do so, we will sample random beliefs and test if less than a p -proportion have a Bayes risk greater than  X  . 1. Sampling a Sufficient Number of Beliefs. To test if k interactions lead to a probability p queries with confidence  X  for n p q n b beliefs require meta-queries after k interactions, we accept the value of k . We sample from the posterior
Dirichlet given k interactions and estimate  X  p
We desire  X  p ability  X  we set  X  2. Computing Bayes Risk from a Conservative Poste-rior. We next compute the Bayes risk for each belief given a hypothesized set of k interactions. We do not know a priori the response to the interactions, so we use the maximum-entropy Dirichlet posterior to compute the posterior Bayes risk (that is, assign the k counts to as-sign an equal number of counts to each variable). We compute the Bayes risk of each belief from this posterior and accept k if  X  p 3. Correction for Approximate Bayes Risk. Recall that we approximate the Bayes risk integral with a sum over sampled POMDP models, and the number of models n required is given by equation 8. We must correct for the error induced by this approximation. Section 4.1 tells us if a belief b has risk BR ( a ) &lt;  X   X  with confidence  X  . Suppose we sample n b beliefs, and the true fraction of beliefs in which meta-queries are asked is p misclassifications, however, the expected value we will observe is only (1  X   X  ) p
Chernoff bound to determine that with probability  X  , no more than 2(1  X   X  ) n be the minimum fraction of beliefs queries we expect to observe requiring meta-queries if the true fraction is p Thus, to test if k interactions lead to a probability of for meta-queries with confidence  X  equation 19, sample n update the Dirichlet posteriors to be maximum-entropy posteriors, sample the n nally compute the posterior Bayes risk for each belief. If less than a p then k is an upper bound on the number of remaining meta-queries with probability p Reward Convergence. The cost of a meta-query limits the reward resolution. Suppose a POMDP P has an opti-mal policy  X  with value V . If we adjusted all the rewards by some small  X   X  bound on the optimal policy in the new POMDP. Thus, a POMDP with all its rewards within (1  X   X  )  X  of P will have a policy of value V  X   X  . In this way, the value  X  imposes a minimal level of discretization over the reward space. The rewards are bounded between R reward space has d dimensions, then our discretization will cretization involves limiting the precision of the sampled rewards.) Since each meta-query invalidates at least one POMDP, we must eventually stop asking meta-queries. We first solve a discretized model-uncertainty POMDP solved directly to show the utility of meta-queries. We next couple the meta-queries with our Bayes-risk criterion for learning with with continuous-valued unknown parameters. 5.1. Learning Discrete Parameters In domains where model uncertainty is limited to a few, dis-crete parameters, we may be able to solve for the complete model-uncertainty POMDP using standard POMDP meth-ods. We consider a simple POMDP-based dialog manage-ment task (Doshi &amp; Roy, 2007) where the reward is un-known. We presume the correct reward is one of four (dis-crete) possible levels and that the meta-query had a fixed associated cost. Figure 1 compares the performance of the optimal policy with meta-queries (left column), an opti-mal policy without meta-queries (middle column), and our Bayes risk policy with meta-queries (right column). The difference in median performance is small, but the variance Unfortunately, discretizing the model space does not scale ; increasing from 4 to 48 possible reward levels, we could no longer obtain high-quality global solutions using standar d techniques. Next, we present results using our Bayes-risk action selection criterion when we no longer discretize the parameter space and instead allow the parameters to take on continuous values within prespecified ranges.
 5.2. Learning Continuous Parameters Table 2 shows our approach applied to several standard POMDP problems (Littman et al., 1995). For each prob-lem, between 50-200 POMDP samples were initially taken from the prior over models. The sampled POMDPs were solved very approximately, using relatively few belief points (500) and only 25 partial backups. The policy oracle used a solution to the true model with many more belief points (1000-5000) and 250 full backups. We took this so-lution to be the optimal policy. During a trial, which con-tinued until either the task was completed or a maximum number of iterations was reached, the agent had the choice of either taking a normal action or asking a meta-query and then taking the supplied optimal action. POMDPs were re-sampled at the completion of each trial.
 The non-learner (control) always used its initial samples t o make decisions, using the Bayes-risk criterion to select an action from the policies of the sampled models. Its prior did not change based on the action-observation histories that it experienced, nor did it ask any meta-queries to gain additional information. The passive learner resampled its POMDP set after updating its prior over transitions and ob-servations using the forward-backward algorithm. The ac-tive learner used both the action-observation histories an d meta-queries for learning. None of the systems received ex-plicit reward information, but the active learner used meta -queries to infer information about the reward model. The Hallway problem was too large for the agent to learn (af-ter 50 repetitions, it still queried the oracle at nearly eve ry step); in these results we provided the agent with possible successor states. The smarter prior seemed reasonable as a map and may be easier to obtain for a new environment than to the robot X  X  dynamics. Depending on the problem, tasks required an average of 7 to 32 actions to complete. Problem States Control Passive Active Tiger 2 46.5 50.7 33.3 Shuttle 8 10.0 10.0 2.0 Gridworld-5 26 33.1 102 21.4 Hallway 57 1.0 1.0 0.08 Figure 2 shows the performance of the three agents on the shuttle problem (a medium-sized standard POMDP). In each case, the agent began with observation and tran-sition priors with high variance but peaked toward the cor-rect value (that is, slightly better than uniform). We creat ed these priors by applying a diffusion filter to the ground-truth transition and observation distributions and using t he result as our initial Dirichlet parameters. All reward pri-ors were uniform between the minimum and maximum re-ward values of the ground-truth model. The active learner started (and remained) with good performance because it used meta-queries when initially confused about the model. Thus, its performance was robust throughout.
 One recent approach to MDP model learning, the Beetle al-gorithm (Poupart et al., 2006), converts a discrete MDP into a continuous POMDP with state variables for each MDP parameter. However, their analytic solution does not scale to handle the entire model as a hidden state in POMDPs. Also, since the MDP is fully observable, Beetle can easily adjust its prior over the MDP parameters as it acquires ex-perience; in our POMDP scenario, we needed to estimate the possible states that the agent had visited. Recently the authors have extended Beetle to partially observable do-mains (Poupart &amp; Vlassis, 2008), providing similar ana-lytic solutions to the POMDP case. The work outlines effi-cient approximations but results are not provided. Prior work in MDP and POMDP learning has also con-sidered sampling to approximate a distribution over un-certain models. Dearden et. al. (1999) discusses sev-eral approaches for representing and updating priors over MDPs using sampling and value function updates. Strens (2000) shows that in the MDPs, randomly sampling only one model from a prior over models, and using that model to make decisions, is guaranteed to converge to the op-timal policy if one resamples the MDP sufficiently fre-quently from an updated prior over models. More recently, in the case of POMDPs, Medusa (Jaulmes et al., 2005) avoids the problem of knowing how to update the prior by occasionally requesting the true state based on model-uncertainty heuristics. It converges to the true model but may make several mistakes before convergence. Our risk-based heuristic and policy queries provide correctness and convergence guarantees throughout the learning process. We developed an approach for active learning in POMDPs that can robustly determine a near-optimal policy. Meta-queries X  X uestions about actions that the agent is think-ing of taking X  X nd a risk-averse action selection criterion allowed our agent to behave robustly even with uncer-tain knowledge of the POMDP model. We analyzed the theoretical properties of our algorithm, but also included several practical approximations that rendered the method tractable. Finally, we demonstrated the approach on sev-eral problems from the POMDP literature. In our future work, we hope to develop more efficient POMDP sampling schemes X  X s well as heuristics for allocating more compu-tation to more promising solutions X  X o allow our approach to be deployed on larger, real-time applications.
