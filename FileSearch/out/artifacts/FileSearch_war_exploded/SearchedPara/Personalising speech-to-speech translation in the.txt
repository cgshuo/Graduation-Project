 A mobile real-time speech-to-speech translation (S2ST) device is one of the grand challenges in natural language processing (NLP). It involves several important NLP research areas: auto-matic speech recognition (ASR), statistical ma-chine translation (SMT) and speech synthesis, also known as text-to-speech (TTS). In recent years significant advance have also been made in rele-vant technological devices: the size of powerful computers has decreased to fit in a mobile phone and fast WiFi and 3G networks have spread widely to connect them to even more powerful computa-tion servers. Several hand-held S2ST applications and devices have already become available, for ex-ample by IBM, Google or Jibbigo 1 , but there are still serious limitations in vocabulary and language selection and performance.

When an S2ST device is used in practical hu-man interaction across a language barrier, one fea-ture that is often missed is the personalization of the output voice. Whoever speaks to the device in what ever manner, the output voice always sounds the same. Producing high-quality synthesis voices is expensive and even if the system had many out-put voices, it is hard to select one that would sound like the input voice. There are many features in the output voice that could raise the interaction expe-rience to a much more natural level, for example, emotions, speaking rate, loudness and the speaker identity.

After the recent development in hidden Markov model (HMM) based TTS, it has become possi-ble to adapt the output voice using model trans-formations that can be estimated from a small number of speech samples. These techniques, for instance the maximum likelihood linear regres-sion (MLLR), are adopted from HMM-based ASR where they are very powerful in fast adaptation of speaker and recording environment characteristics (Gales, 1998). Using hierarchical regression trees, the TTS and ASR models can further be coupled in a way that enables unsupervised TTS adaptation (King et al., 2008). In unsupervised adaptation samples are annotated by applying ASR. By elimi-nating the need for human intervention it becomes possible to perform voice adaptation for TTS in almost real-time.

The target in the EMIME project 2 is to study unsupervised cross-lingual speaker adaptation for S2ST systems. The first results of the project have been, for example, to bridge the gap between the ASR and TTS (Dines et al., 2009), to improve the baseline ASR (Hirsim  X  aki et al., 2009) and SMT (de Gispert et al., 2009) systems for mor-phologically rich languages, and to develop robust TTS (Yamagishi et al., 2010). The next step has been preliminary experiments in intra-lingual and cross-lingual speaker adaptation (Wu et al., 2008). For cross-lingual adaptation several new methods have been proposed for mapping the HMM states, adaptation data and model transformations (Wu et al., 2009).

In this presentation we can demonstrate the var-ious new results in ASR, SMT and TTS. Even though the project is still ongoing, we have an initial version of mobile S2ST system and cross-lingual speaker adaptation to show. The baseline ASR systems in the project are devel-oped using the HTK toolkit (Young et al., 2001) for Finnish, English, Mandarin and Japanese. The systems can also utilize various real-time decoders such as Julius (Kawahara et al., 2000), Juicer at IDIAP and the TKK decoder (Hirsim  X  aki et al., 2006). The main structure of the baseline sys-tems for each of the four languages is similar and fairly standard and in line with most other state-of-the-art large vocabulary ASR systems. Some spe-cial flavors for have been added, such as the mor-phological analysis for Finnish (Hirsim  X  aki et al., 2009). For speaker adaptation, the MLLR trans-formation based on hierarchical regression classes is included for all languages.

The baseline TTS systems in the project utilize the HTS toolkit (Yamagishi et al., 2009) which is built on top of the HTK framework. The HMM-based TTS systems have been developed for Finnish, English, Mandarin and Japanese. The systems include an average voice model for each language trained over hundreds of speakers taken from standard ASR corpora, such as Speecon (Iskra et al., 2002). Using speaker adaptation transforms, thousands of new voices have been created (Yamagishi et al., 2010) and new voices can be added using a small number of either su-pervised or unsupervised speech samples. Cross-lingual adaptation is possible by creating a map-ping between the HMM states in the input and the output language (Wu et al., 2009).

Because the resources of the EMIME project have been focused on ASR, TTS and speaker adaptation, we aim at relying on existing solu-tions for SMT as far as possible. New methods have been studied concerning the morphologically rich languages (de Gispert et al., 2009), but for the S2ST system we are currently using Google trans-late 3 . 3.1 Monolingual systems In robust speech synthesis, a computer can learn to speak in the desired way after processing only a relatively small amount of training speech. The training speech can even be a normal quality recording outside the studio environment, where the target speaker is speaking to a standard micro-phone and the speech is not annotated. This differs dramatically from conventional TTS, where build-ing a new voice requires an hour or more careful repetition of specially selected prompts recorded in an anechoic chamber with high quality equip-ment.

Robust TTS has recently become possible us-ing the statistical HMM framework for both ASR and TTS. This framework enables the use of ef-ficient speaker adaptation transformations devel-oped for ASR to be used also for the TTS mod-els. Using large corpora collected for ASR, we can train average voice models for both ASR and TTS. The training data may include a small amount of speech with poor coverage of phonetic contexts from each single speaker, but by summing the ma-terial over hundreds of speakers, we can obtain sufficient models for an average speaker. Only a small amount of adaptation data is then required to create transformations for tuning the average voice closer to the target voice.

In addition to the supervised adaptation us-ing annotated speech, it is also possible to em-ploy ASR to create annotations. This unsu-pervised adaptation enables the system to use a much broader selection of sources, for example, recorded samples from the internet, to learn a new voice.

The following systems will demonstrate the re-sults of monolingual adaptation: 1. In EMIME Voice cloning in Finnish and En-2. In EMIME Thousand voices map the goal is 3. The models developed in the HMM frame-3.2 Cross-lingual systems In the EMIME project the goal is to learn cross-lingual speaker adaptation. Here the output lan-guage ASR or TTS system is adapted from speech samples in the input language. The results so far are encouraging, especially for TTS: Even though the cross-lingual adaptation may somewhat de-grade the synthesis quality, the adapted speech now sounds more like the target speaker. Sev-eral recent evaluations of the cross-lingual speaker adaptation methods can be found in (Gibson et al., 2010; Oura et al., 2010; Liang et al., 2010; Oura et al., 2009).

The following systems have been created to demonstrate cross-lingual adaptation: 1. In EMIME Cross-lingual Finnish/English 2. In EMIME Real-time speech-to-speech mo-3. The morpheme-based translation system for The research leading to these results was partly funded from the European Communitys Seventh Figure 3: EMIME Real-time speech-to-speech mobile translation demo Framework Programme (FP7/2007-2013) under grant agreement 213845 (the EMIME project).
