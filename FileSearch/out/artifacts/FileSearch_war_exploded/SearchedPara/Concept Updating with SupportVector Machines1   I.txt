 Learning is obtaining an underlying rule by using training data sampled from the environment. Support vector machines(SVMs)[1]are trainded by solving a quadratic optimization problem which needs on the order of 2 memory and time resources to solve, where is the number of training examples. In many completed. One of the common approaches is to add training data to the SVM retraining SVMs.
 mental batch updating method which uses an incremental updating model simi-we formulate the standard Support Vector Machine for Regression(SVR). Then we give the updating model and explain in more detail each of its main steps new algorithm on toy and real datasets to standard batch SVR learning. set S . The Derivation of the SVR equations can be found in [1] and will not a nonlinear mapping from input space to some high-dimensional feature space. to minimize subject to (for  X  i  X  X  1 ,  X  X  X  , } )  X  i and  X  function over the training set: the dual optimization problem. In this section, we (1) introduce an updating method for support vector re-proposed updating method. 3.1 Formulation of Concept Updating Method quadric programming problem as the updating step subject to idea is to construct a Lagrange function from both the objective function(it at the optimal solution. We construct the Lagrange function as follows: have to vanish for optimality. subject to (10) can be rewritten as follows and therefore conditions which is not further discussed here. 3.2 Updating Procedure In fact, the updating procedure can be described as following 3.3 Relation to Standard SVR w k  X  1 +  X  w . So we can rewrite the updating formula as follows. subject to From above rewriting formula, we can see that updating method has the same in the  X  -tube.
 positive definition function, and new data set D k satisfies condition then  X  (  X  ) ki =0 ,i =1 ,  X  X  X  , k .
 We decomposed it into two items D = D 1 + D 2 ,where rewrite the D 2 as follows D then  X  (  X  ) ki =0 ,i =1 ,  X  X  X  , k also is unique and optimal solution. based on the LibSVM 1 . All our experiments are done in a Celoren 1.7Ghz ma-chine with 256MB memory running on Windows2000. 4.1 Evaluations on Synthetic Toy Data Sets We demonstrate the advantages of our approach in comparison with standard SVR in following synthetic toy data sets first.
 1 , g are given below(see Fig. 1): 1. Sinc Function 2. Simple Interaction Function 3. Radial Function 4. Harmonic Function 5. Additive Function 6. Complicated Interaction Function ( x with each component an independent uniform random variable on [0 , 1]. And  X  X  are i.i.d N (0 , 1).
 Parameter Settings: In our experiments, both standard SVR and updating method use Gaussian kernel with width parameter  X  = 1, regularized parameter C = 1 and all other parameters are the default parameters in LibSVM tools. Experimental Results: To test the accuracy of the updating method, a test of x ir =(2 i variance of unexplained(FVU), which is given by expectations with averages over the grid of test set values. the results, we observe that updating procedure improve the performance on updating method wins five noisy cases, but half noiseless cases. 4.2 Evaluations on Other Data Sets from the UCI machine learning repository. We compared updating method with standard SVR with Gaussian kernel. The parameter C for both updating and standard SVR was tuned via cross validations, so was the width parameter in the Gaussian kernel for SVR. We use one of three training data for updating and keep the the parameters unchange during updating. The final performance evaluation results.
 mance and decrease the iterative number of algorithm dramatically on the all set. And The evaluations on these standard bench X  X ark data sets demonstrate that it is worth considering available data to updating trained SVR. We propose a novel updating method for SVR. It is shown that updating pro-cedure is to solve an convex quadratic programming the same as the standard will not change. Comparison to the SVR trained with all data in one batch, plications.
 This research was supported by Ningbo Doctor Science Fund grant 2005A610002.
