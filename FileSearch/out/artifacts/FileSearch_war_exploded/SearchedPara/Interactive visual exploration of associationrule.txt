 REGULAR PAPER Julien Blanchard  X  Fabrice Guillet  X  Henri Briand Abstract On account of the enormous amounts of rules that can be produced by data mining algorithms, knowledge post-processing is a difficult stage in an as-sociation rule discovery process. In order to find relevant knowledge for decision making, the user (a decision maker specialized in the data studied) needs to rum-mage through the rules. To assist him/her in this task, we here propose the rule-focusing methodology, an interactive methodology for the visual post-processing of association rules. It allows the user to explore large sets of rules freely by fo-cusing his/her attention on limited subsets. This new approach relies on rule in-terestingness measures, on a visual representation, and on interactive navigation among the rules. We have implemented the rule-focusing methodology in a pro-totype system called ARVis . It exploits the user X  X  focus to guide the generation of the rules by means of a specific constraint-based rule-mining algorithm. Keywords Knowledge discovery in databases  X  Association rules  X  Post-processing  X  Interactive visualization  X  Rule focusing  X  Constraint-based mining  X  Interestingness measures  X  Neighborhood of rules 1 Introduction Among the knowledge models used in knowledge discovery in databases (KDD), association rules [ 2] have become a major concept and have received signif-icant research attention. Association rules are implicative tendencies X  X  Y where X and Y are conjunctions of items (boolean variables of the form databaseAttribute = value ). The left-hand side X is the antecedent of the rule and the right-hand side Y the consequent. Such a rule means that most of the records that verify the antecedent in the database verify the consequent too. For instance, in market basket analysis where the data studied are the customers X  transactions in a supermarket, an association rule { pizza , crisps } X  X  beer } means that if a cus-tomer buys a pizza and crisps then (s)he most probably buys beer too. Since the pioneering algorithm of Agrawal, called Apriori [ 3 ], many algorithms have been proposed for association rule mining (cf. Hipp et al. [ 44 ] for a survey). They gen-erally produce very large amounts of rules. This is due to the unsupervised nature of association rule discovery. Indeed, because the user does not know precisely enough what (s)he is looking for to express it with the data terminology, (s)he does not make his/her goals explicit and does not specify any endogenous vari-able. Thus, the algorithms search all the valid associations existing in the database and generate an amount of rules exponentially growing with the number of items.
 knowledge for decision making. Because of the oversized amounts of rules, the post-processing stage often turns out to be a second mining challenge called  X  X nowledge mining. X  While data mining is automatically computed by combi-natorial algorithms, the knowledge mining stage is manually done by the user (a decision maker specialized in the data studied). In practice, it is very difficult for users to rummage through the rules and find interesting ones in a corpus that can hold hundreds of thousands of rules, or even millions of rules with large business databases.
 and interactive and requires user involvement [ 29 , 74 ]. In particular, Brachman and Anand [ 20 ] have pointed out that in order to efficiently assist the users in their search for interesting knowledge, the KDD process should be considered not from the point of view of the discovery algorithms but from that of the users X , as a human-centered decision support system. The human-centered approaches aim at creating a retroaction loop between the user and the system which constantly takes into account the information processing capacities of the user (cf. [ 12 ]for examples of applications). Adopting Brachman and Anand X  X  point of view, in this article, we propose the rule-focusing methodology, a human-centered methodol-ogy for the post-processing of association rules. The rule-focusing methodology allows the user to explore large sets of rules by focusing his/her attention on suc-cessive limited subsets. The methodology relies on several neighborhood rela-tions that connect the rules among them according to the user X  X  semantics. With these relations, the user can navigate freely among the subsets of rules and thus drive the post-processing. In this way, a voluminous set of rules is explored subset by subset so that the user does not need to appropriate it entirely. Our approach combines:  X  rule interestingness measures to filter and sort the rules,  X  a visual representation to make comprehension easier,  X  interactivity based on the neighborhood relations to guide the post-processing. applied after association rule mining, as a pure post-processing technique. This can be applied during association rule mining, as an interactive mining tech-nique conducted by the user. Effectively, the rule-focusing methodology induces a constraint-based rule-mining algorithm. A constraint-based rule-mining algo-rithm exploits constraints that the user gives to specify which kind of rules (s)he wants to find (cf., for example [ 34 , 53 , 65 , 66 , 80 ]). Syntactic constraints (con-straints specifying the items that must occur or not in the rule) and interestingness measure threshold constraints are the most commonly used constraints, but more general studies concern the so-called anti-monotone and succinct constraints [ 65 ], and the monotone constraints [ 18 , 35 ]. Constraints allow to significantly reduce the exponentially growing search space of association rules. 1 Thus, the constraint-based algorithms can mine dense data more efficiently than the classical Apriori-like algorithms (the FP-growth-based algorithms of [ 40 ] can mine dense data too, but they use a condensed representation of the data and require that it holds in memory). Besides, with appropriate constraints, the constraint-based algorithms can discover very specific rules which cannot be mined by the Apriori-like algo-rithms (the constraint-based algorithms can use low support thresholds for which Apriori-like algorithms are intractable). These rules are often very valuable for the users because they were not even thought of beforehand [ 31 ]. For these reasons, we use a constraint-based rule-mining algorithm to implement the rule-focusing methodology in the prototype system described in this paper. This specific algo-rithm extracts the rules interactively according to the user X  X  focus. Note that using the rule-focusing methodology as a pure post-processing technique or as an in-teractive mining technique is only a choice of implementation. The methodology does not depend on it.
 present a survey on association rule evaluation, exploration, and visualization. Then we describe the information visualization field of research, and in partic-ular we compare 2D and 3D visualizations. Section 4 is dedicated to the study of cognitive constraints of the user during rule post-processing. From these con-straints, in Sect. 5, we define the rule-focusing methodology. Section 6 describes the prototype system implementing our methodology: ARVis , a visual tool for as-sociation rule mining and post-processing . In Sect. 7, we give an example of rule post-processing with ARVis . It comes from a study made with the firm Perfor-manSe SA on human resource management data. Finally, we give our conclusion in Sect. 8. 2 Survey on association rule evaluation, exploration, and visualization At the output of the data mining algorithms, the sets of association rules are simple text lists. Each rule consists of a set of items for the antecedent, a set of items for the consequent (sets of items are called itemsets), and the numerical values of two interestingness measures, support and confidence [ 2 ]. Support is the proportion of records which verify a rule in the database; it evaluates the generality of the rule. Confidence (or conditional probability) is the proportion of records which verify the consequent among those which verify the antecedent; it evaluates the validity of the rule (success rate). association rules:  X  the user can filter and order the rules with other interestingness measures;  X  the user can browse the large sets of rules with interactive tools or query lan- X  the user can visualize the rules. 2.1 Rule interestingness measures It is now well known that the support X  X onfidence framework is rather poor to evaluate the rule quality [ 9 , 76 , 82 ]. Numerous rule interestingness measures have been proposed to complement this framework. They are often classified into two categories: the subjective (user-oriented) ones and the objective (data-oriented) ones. Subjective measures take into account the user X  X  a priori knowl-edge of the data domain [ 59 , 67 , 75 ]. On the other hand, the objective measures do not depend on the user but only on objective criteria such as data cardinali-ties or rule complexity. Depending on whether they are symmetric (invariable by permutation of antecedent and consequent) or not, they evaluate correlations or rules.
 directed relations and therefore can be considered as neutral or non-existing [ 15 ]:  X  X he independence , i.e., when the antecedent and consequent are independent;  X  what we call the equilibrium , i.e., when examples and counter-examples are terestingness: the deviation from independence and the deviation from equilib-rium. The objective measures of interestingness can be classified into two classes [ 16 , 17 ]:  X  the measures of deviation from independence, which have a fixed value at  X  the measures of deviation from equilibrium, which have a fixed value at equi-create the same preorder on rules) [ 15 , 16 ]. A rule can have a good deviation from independence with a bad deviation from equilibrium, and conversely. Regarding the deviation from independence, a rule A  X  B with a good deviation means  X  X hen A is true, then B is more often true X  (more than usual, i.e., more than without any information about A ). On the other hand, regarding the deviation from equilibrium, a rule A  X  B with a good deviation means  X  X hen A is true, then B is very often true. X  Deviation from independence is a comparison relatively to an expected situation, whereas deviation from equilibrium is an absolute statement. The measures of deviation from independence are useful to discover associations between antecedent and consequent (do the truth of A influence the truth of B ?), while the measures of deviation from equilibrium are useful to take decisions or make predictions about B (knowing or supposing that A is true, is B true or false?) [ 15 ]. 2.2 Interactive browsing of rules Interactive tools of the type  X  X ule browser X  have been developed to assist the user in the post-processing of association rules. First, Klemettinen et al. [ 55 ] present a browser with which the user reaches interesting rules by adjusting thresholds on interestingness measures and applying syntactic constraints (templates). Secondly, Liu et al. [ 58 ] propose a rule browser which is based on subjective interestingness measures and exploits the user X  X  a priori knowledge of the data domain to present the rules. The user expresses his/her knowledge under the form of relations, and then the tool classifies the rules in different categories according to whether they confirm or not the user X  X  beliefs. Finally, in [ 61 ], the user explores a summary of the rules. (S)he can access the rules by selecting elements in the summary. which does not suit the study of large amounts of rules described by numerous interestingness measures.
 presented in [ 33 ]. It allows to filter the rules with syntactic constraints that are more or less general, since they can take into account an item taxonomy. The tool also enables the user to program any interestingness measure to order and filter the rules. Besides, the user can save the rules that (s)he judges interesting during the exploration. Another rule browser is presented in [ 84 ],butitisnotageneric tool. It is dedicated to the analysis of gene expression data coming from DNA microarrays, and relies on a very complete system of syntactic constraints which can take into account a gene taxonomy.
 guages have been proposed, such as DMQL [ 38 ], MINE RULE [ 63 ], MSQL [ 52 ], or XMINE [ 21 ]. They allow to mine (by means of constraint-based algorithms) and post-process rules interactively under the user X  X  guidance. However, as re-gards rule post-processing, the query languages are not very user friendly (cf. [ 19 ] for an experimental study). 2.3 Visualizing the rules Visualization can be very beneficial to KDD [ 30 ]. Visualization techniques are in-deed an effective means of introducing human subjectivity into each stage of the KDD process while taking advantage of the human perceptual capabilities. The information visualization techniques can either be used as knowledge discovery methods on their own, which is sometimes called  X  X isual data mining X  [ 54 ], or they can collaborate with data mining algorithms to facilitate and speed up the analysis of data, intermediate results, or discovered knowledge [ 1 , 42 , 73 ]. Asso-ciation rule visualization comes within this latter case. It must be noticed that the methods and tools presented later are generally supplied with basic functionalities for ordering and filtering the rules on items and on a few interestingness measures. mann and Wilhelm [ 47 ] and the Quest research group [ 4 ], as well as the software programs DBMiner [ 39 ], SGI MineSet [ 23 ], DB2 Intelligent Miner Visualization [ 50 ], and Enterprise Miner [ 71 ], give different implementations of it. In an item-to-item matrix (Fig. 1), each line corresponds to an antecedent item and each column to a consequent item. A rule between two items is symbolized in the matching cell by a 2D or 3D object whose graphical characteristics (generally size and color) represent the interestingness measures. This visualization technique has been im-proved into rule-to-item matrices [ 88 ] whose cluttering is lower and which allow a more efficient representation of rules with more than two items. The main limit of these approaches is that the matrices reach considerable sizes in case of large sets of rules over numerous items.
 [ 39 , 50 , 55 , 69 ], the nodes and edges respectively representing the items and the rules (cf. Fig. 2 where letters denote items). The interestingness measures are sym-bolized on the edges, for instance with color or thickness. The graph representa-tion is very intuitive but it has two main drawbacks. First, it makes transitivity appear among the rules, whereas, in the general case, the rules are not transitive (with most measures, rule interestingness does not spread transitively). Secondly, it does not suit the visualization of large sets of rules over numerous items ei-ther. Indeed, the graph is then overloaded with nodes and crossing edges, all the more when rules with more than two items are considered. To improve the rule visualization, the same representation method has been used in 3D with a self-organization algorithm to guarantee a more efficient graph layout [ 43 ]. Also, in [ 57 ], we have proposed a dynamic rule graph that is a subgraph of the itemset lattice. In this graph, the nodes do not represent the items but the itemsets so that arule AB  X  C is symbolized by an edge between the nodes AB and ABC (Fig. 3). The resulting graph is acyclic with more nodes but fewer edge crossings. The user can dynamically develop the graph as (s)he wishes by clicking on the nodes. items). A different approach is proposed in [ 85 ], where the representation is based on interestingness measures. This representation is a scatter plot between support and confidence where each point is colored according to density estimation. The user can query any point to display the names of the rules represented by the point (rules with close supports and confidence). The main advantage of such a representation is that it can contain a great number of rules. However, several rules can be represented by one and only one point, which does not facilitate the task of the users when they search for rules using items as criteria. This approach is the closest to the one we propose in this paper, which also uses a spatial mapping to highlight the interestingness measures.
 less, they do not deal with the visualization of the whole rule set but with the visualization of a pattern of rules (a group of rules with given items in antecedent and consequent). These methods allow a thorough study of a restricted number of rules, making their interpretation easier and helping to understand their occur-rence context. We can quote for example Hofmann et al. X  X  mosaic plots (2000) for rules with categorical attributes (Fig. 4), or [ 32 ]and[ 42 ] for numerical rules. Also some techniques inspired from parallel coordinates have been considered to visualize patterns of classification rules [ 41 ] or association rules [ 56 ]. 3 Information visualization 3.1 Context Information visualization [ 24 , 79 ] consists in representing abstract data under a visual form in order to improve cognition for a given task, that is to say the ac-quisition and use of new knowledge. The core of information visualization is vi-sual encoding, i.e., the mapping of data tables to visual structures in a 2D or 3D space [ 24 ]. The visual structures have several graphical properties such as posi-tion, length, area, hue, brightness, saturation, shape, texture, angle, curvature, etc. They can be zero-dimensional (points), one-dimensional (lines), two-dimensional (surfaces), or three-dimensional (volumes).
 which ones are appropriate according to the data variables to be represented. Among these works, those of Cleveland and McGill [ 27 ], Tufte [ 83 ], and then Wilkinson [ 87 ] are references for statistical graphs (charts). A second trend stems from cartography, with the works of McEachren [ 62 ] and Bertin [ 10 ] whose Semi-ology of Graphics is considered as the first and most influential structural theory of graphics [ 87 ]. As regards the visual representation of quantitative variables, the two trends agree that the best encodings are done with position [ 10 , 24 , 27 , 62 , 87 ]. However, the two trends mainly differ about the use of surfaces to represent quan-titative variables: this use is not advisable with statistical graphs, whereas it is standard practice in cartography. In particular, Cleveland and McGill [ 27 ] propose a hierarchy of visual encodings saying that surface is little appropriate (less than length) to represent quantities. This point of view is based on Stevens X  X  law in psychophysics according to which the perceived quantities are not linearly related to the actual quantities with surface [ 7 ]. On the other hand, Bertin points out that before the variation of length, the variation of surface is the sensitive stimulus of the variation of size [ 10 ].
 tistical graph: this is a 3D virtual world. With the increase in the capacities of personal computers, the 3D virtual worlds have become common in information visualization [ 26 ]. Associated with navigation operators (viewpoint controls), they have shown to be efficient for browsing wide information corpuses such as large file system hierarchies with Silicon Graphics X  FSN (re-used in MineSet for the visualization of decision trees), hypertext document graphs with Harmony [ 6 ], or OLAP cubes with DIVE-ON [ 5 ](cf.[ 26 ] for other examples of applications). While a 2D representation is restricted to the two dimensions of the screen, the additional dimension in a 3D virtual world offers a viewpoint towards infinity, cre-ating a wide workspace capable of containing a large amount of information [ 24 ]. In this workspace, the most important information can be placed in the foreground (most visible objects) and thus be highlighted compared to the less important in-formation placed behind it (less visible objects). This is the reason why the 3D representations are sometimes considered as focus + context approaches. More-over, 3D enables to exploit volumes as objects in the visualization space. It allows to benefit from more graphical properties for the objects and thus to represent even more information. 3.2 2D or 3D? The choice between 2D and 3D representations for information visualization is still an open problem [ 24 , 26 ]. This is especially due to the fact that the efficiency of a visualization is highly task-dependent [ 25 ]. Besides, while 3D representations are often more attractive, 2D has the advantage of a long and fruitful experience in information visualization. In fact, few research works are dedicated to the compar-ison between 2D and 3D. As regards the static (non-interactive) visualization of statistical graphs, the 3D representations have generally not been advisable since the influential publications of Tufte [ 83 ] and Cleveland and McGill [ 27 ]. Never-theless, the psychophysics experiments of Spence [ 78 ] and Carswell et al. [ 25 ] show that there is no significant difference of accuracy between 2D and 3D for the comparison of numerical values. In particular, Spence points out that this is not the apparent dimensionality of visual structures which counts (2 for a surface, 3 for a volume) but the actual number of parameters that show variability [ 78 ]. In his experiments, whatever the apparent dimensionality of visual structures, Stevens X  X  law is almost always the same when only one parameter actually varies (Stevens X  X  law exponents are very close to 1). Under some circumstances, information may even be processed faster when represented in 3D rather than in 2D. As regards the perception of global trends in data (increase or decrease), the experimental results of Carswell et al. [ 25 ] also show an improvement in the answer times with 3D but to the detriment of accuracy.
 alization. Cockburn and McKenzie [ 28 ] study the storage and retrieval of book-marked web pages in a 2D or 3D visualization space. With the 2D interface, the processing times of the users are shorter but not significantly. On the other hand, the subjective assessment of the interfaces shows a significant preference for 3D (which Spence [ 78 ] and Carswell et al. [ 25 ] also sense but without assessing it). Finally, Ware and Franck [ 86 ] compare the visualization of 2D graphs and 3D graphs. Their works show a significant improvement in intelligibility with 3D. More precisely, their experiment consists in asking users whether there is a path of length two between two nodes randomly chosen in a graph. With the 3D graphs, the error rate is reduced by 2.2 for comparable answer times. With stereoscopic stereoscopy allows fully exploiting the characteristics of the 3D representations. 4 Cognitive constraints of the user during rule post-processing 4.1 User X  X  task During the post-processing of the rules, the user is faced with long lists of rules described by interestingness measures. The user X  X  task is then to rummage through the rules in order to find interesting ones for decision making. To do so, (s)he needs to interpret the rules in the business semantics and to evaluate their quality. The two decision indicators are therefore the rule syntax and the interestingness measures. The user X  X  task is difficult for two reasons. First, the profusion of rules at the output of the data mining algorithms prevents any exhaustive exploration. Secondly, on account of the unsupervised nature of association rule discovery, it is generally not feasible for the user to obviously formulate constraints which would isolate relevant rules directly. 4.2 Cognitive hypotheses of information processing On account of the human  X  X ounded rationality X  hypothesis [ 77 ], a decision pro-cess can be seen as a search for a dominance structure. More precisely, the de-cision maker faced with a set of multiattribute alternatives tries to find an alter-native (s)he considers dominant over the others, i.e., an alternative (s)he thinks better than the others according to his/her current representation of the decision situation [ 64 ]. This type of models of decision process can be transferred to the post-processing of association rules by considering the rules as a particular kind of alternatives with items and interestingness measures as attributes. According to Montgomery, the decision maker isolates a limited subset of potentially use-ful alternatives and makes comparisons among them. This can be done iteratively during the decision process. More precisely, he has pointed out that:  X  X he decision process acquires a certain directionality in the sense that certain alternatives and attributes will receive more attention than others [ ... ] The directionality of the process may be determined more or less consciously. Shifts in the directionality may occur several times in the process, particularly when the decision maker fails to find a dominance structure. X  posed in [ 11 ]. It results from experimental data concerning the user X  X  behavior in the discovery process. This methodology is based on a filter which automatically detects a small number of potentially interesting attributes. The filter guides the user X  X  attention on a small, and therefore more intelligible, subset of the database. The importance of focusing on a small number of attributes in human informa-tion processing has also been widely confirmed with works on decision strategies (cf., for example, the moving basis heuristics in [ 8 ]). Indeed, on account of his/her limited cognitive abilities, the decision maker examines only a small amount of information at each moment. three principles on which our rule-focusing methodology relies: P1. enabling the user to focus his/her attention on a limited subset of rules with a P2. enabling the user to make comparisons among the rules in the subset, P3. enabling the user to shift the subset of rules (s)he is focusing on at any time 5 Rule-focusing methodology The idea of developing the rule-focusing methodology has arisen from our earlier works on the visualization of rule sets by graphs [ 57 ]. The methodology consists in letting the user navigate freely inside the large set of rules by focusing on succes-sive limited subsets via a visual representation of the rules and their measures. In other words, the user gradually drives a series of visual local explorations accord-ing to his/her interest for the rules (Fig. 5). Thus, the rule set is explored subset by subset so that the user does not need to appropriate it entirely. At each navigation 2 step, the user must make a decision to choose which subset to visit next. This is the way subjectivity is introduced into the post-processing of the rules. The user acts here as an exploration heuristics. Exploiting a human heuristics is coherent, since the function to be optimized, i.e., the user X  X  interest, is subjective. in the following ways:  X  Relations allow to focus on the subsets and to navigate among them (principles  X  The user visualizes the subsets to visit them, and in particular to compare the Both the neighborhood relations and the visualization technique must take into account the two decision indicators involved in the user X  X  task: the rule syntax and the interestingness measures (cf. Sect. 4.1). 5.1 Neighborhood relations among rules The neighborhood relations determine the way the subsets of rules are focused on (cognitive principle P1) and the way the user can go from one subset to another (cognitive principle P3). They are a fundamental element of the rule-focusing methodology, since they are the vectors of the navigation for the user. These rela-tions are defined in the following way: a neighborhood relation associates each bors (Fig. 6). So with x relations, the user can reach x neighboring subsets of rules from one rule, and from a subset containing y rules (s)he can reach x . y possible neighboring subsets. To navigate from one subset to another, the user must make two choices: which neighborhood relation to apply, and on which rule.
 over the complete set R of the rules extracted by the data mining algorithms. Still with the aim of being appropriate to the user X  X  task, we choose neighbor-This introduces user semantics into the navigation among the rules. Any rela-tion could be considered provided it makes sense for the user. Consequently, the relations have to be defined with his/her help before starting the rule post-processing.
 neighborOf ( r 1 , r 2 ) : 1. r 1 is neighbor of r 2 if and only if r 1 and r 2 have the same conclusion; 2. r 1 is neighbor of r 2 if and only if r 1 is an exception of r 2 ; 3. r 1 is neighbor of r 2 if and only if the antecedent of r 1 is more general than that 4. r 1 is neighbor of r 2 if and only if r 1 has the same support and confidence as r 2 4 is based on two interestingness measures. Furthermore, relation 1 is an equiv-alence relation, whereas relation 2 is neither reflexive, nor symmetric, nor transi-tive. Relation 3 is only transitive, and relation 4 is reflexive and symmetric but not transitive.
 (s)he can reach the subset S of all the rules that are neighbors of r according to . We call r the  X  X ransitional rule X  because it allows to navigate from one subset to another. Depending on the reflexivity of the relation chosen, S can or cannot contain the transitional rule r .
 ration techniques (described Sect. 2.2) mainly lies in the concept of neighborhood relation. With a query language or an interactive interface like a rule browser, the user can reach any subset of rules but (s)he must explicitly specify the constraints which delimit it. With the rule-focusing methodology, the constraint specification is implicit, since it is hidden in the neighborhood relations. Actually, the neigh-borhood relations can be seen as generalizations of constraints (classes of con-straints). We think that the user X  X  task is made easier with neighborhood relations than with explicit constraints. For example, let us imagine the following explo-ration scenario: (S)he thinks that the combination of these four items is very pertinent but (s)he would like to change the order of the items and verify whether the rules A B D  X  C, AC D  X  B, and BC D  X  A (and why not the rules A B  X  CD, AC  X  BD and so on as well) are better evaluated by the interestingness measures. just one interaction. (S)he only needs the neighborhood relation  X  r 1 is neighbor of r if and only if r 1 and r 2 have the same items. X  On the other hand, with a query language or a rule browser, the user has to write a series of appropriate queries or to specify a series of constraints manually with the graphical interface. This can be a tedious and time-consuming task. 5.2 Rule visualization In order to help the user to visit the subsets of rules, we provide him/her with a visual representation instead of poorly intelligible textual lists. The visual repre-sentation facilitates and speeds up comprehension, and in particular it makes the comparisons among the rules easier (cognitive principle P2). Most of the tech-niques proposed for rule visualization have been developed to represent the whole set of rules produced by the data mining algorithms. Nevertheless, in the rule-focusing methodology, we can take advantage of the user X  X  focus strategy by rep-resenting only the current subset of rules at each navigation step. This reduces the number of rules to draw and above all largely improves the representation intelli-gibility. Visually, the user X  X  point of view on the complete rule set is thus always local.
 dle interestingness measures as additional information (except for the approach of Unwin et al. [ 85 ], cf. Sect. 2.3). However, the interestingness measures are also decision indicators fundamental to the user X  X  task. So that the user can quickly assess and compare the rules, the representation must highlight the interesting-ness measures and make the best rules clearly recognizable. Also the visualization must be able to integrate numerous measures (not only support and confidence as it often happens), to dynamically filter the rules according to thresholds set by the user, and to support large amounts of rules having any number of items inside. Finally, the visualization must integrate interactive operators allowing the user to trigger the neighborhood relations.
 perception of a scene, before taking an interest in details. This is what motivated the development of the approaches named overview + details and focus + context [ 24 ]. Thus, in the rule-focusing methodology, the user has to be able to easily change between global and detailed views of the rules by interacting with the visualization. 6 ARVis , a visual tool for association rule mining and post-processing In this section, we present ARVis ( association rule visualization ), an experimental prototype implementing the rule-focusing methodology. It was originally devel-oped for the firm PerformanSe SA in order to find knowledge for decision support in human resource management. ARVis considers rules with single consequents (one item only in the consequent). This choice is usual in association rule discov-ery. Indeed, in association rule discovery in general and in our applications with PerformanSe SA in particular, the users are often interested spontaneously in this kind of rules because they are more intelligible than rules with multi-item conse-quents. However, considering only rules with single consequents is not a limitation to our approach. This choice could be easily changed.
 dence [ 2 ], and entropic implication intensity (respectively noted sp , cf ,and ei i ). We choose support and confidence because they are the basic indexes to assess association rules. As for implication intensity, it is an asymmetric probabilistic index that evaluates the statistical significance of the rules by quantifying the un-likelihood of the number of counter-examples [ 36 , 37 ]. The entropic version of this index also takes into account the imbalances between examples and counter-examples for both the rule and its contrapositive [ 14 ]. The entropic implication intensity is a powerful measure, since it takes into account both the deviation from independence and the deviation from equilibrium. This is the reason why we have chosen to integrate it into ARVis . But here again, this choice of measures is not a limitation to our approach, and others can be added. Each measure is associ-ated to minimum and maximum thresholds set by the user: min sp , min cf , min eii , max sp , max cf , max eii . Although most of the tools for association rule mining do not provide them, the maximum thresholds improve the user X  X  focus. For example, rules with high support and high confidence are often already known by the users; removing them allows highlighting more interesting rules.
 for a visualization technique mainly based on interestingness measures. We think this is the way to the most user-friendly solutions for rule exploration. 6.1 Neighborhood relations Eight neighborhood relations are implemented in ARVis , most of them being generalization-type relations or specialization-type relations. Two of the most fun-damental human cognitive mechanisms for generating new rules are indeed gen-eralization and specialization (cf. the study of the reasoning processes in [ 48 ]). X  X  y where X is an itemset X  X  I and y is an item y  X  I  X  X . The complete set of rules with single consequents that can be built with the items of I is noted R . In order to simplify the notations, we note X  X  y instead of X  X  X  y } and X  X  y instead of X  X  X  y } . For the same simplicity reason, we define the neighborhood relations not as binary relations over R but as functions from R to 2 R which associate each rule with the subset composed of its neighbors: straints:  X  syntactic constraints, which specify the items that must occur or not in the  X  interestingness measure constraints, which specify minimum and maximum other hand, the interestingness measure constraints are shared by all the relations. We group them together into the boolean function interesting () : A rule is said to be interesting if the three measures respect the minimum and maximum thresholds. 6.2 Specialization-type relations agreement specialization ( X  X  y ) = exception specialization ( X  X  y ) = kinds of complementary rules: exception rules and agreement rules. Exception rules aim at explaining the counter-examples of the general rule, while agreement rules aim at better explaining the examples. For instance, a rule  X  X f  X  is a dog then  X  is friendly X  can be specialized into the rules  X  X f  X  is a dog and  X  is muz-zled then  X  is mean X  and  X  X f  X  is a dog and  X  is not muzzled then  X  is friendly. X  The interest of exception rules in KDD has been widely confirmed (cf. for ex-ample [ 49 , 81 ]). On the basis of these two kinds of specialization, we propose the neighborhood relations agreement specialization and exception specialization 3 in ARVis . The third specialization-type relation is inspired by forward chaining in in-ference engines for expert systems: when a rule X  X  y is fired, the concept y becomes active and can be used with X to fire new rules and deduce new concepts z . Backward chaining cannot be considered with rules with single consequent. Generalization-type relations generalization ( X  X  y ) = X  X  z  X  y z antecedent generalization ( X  X  y ) = The generalization relies on the condition-simplifying generalization mechanism described in [ 48 ]. This relation is complementary to agreement specialization and exception specialization . It consists of deleting an item in the antecedent. The relation antecedent generalization is complementary to forward chaining .After applying forward chaining on a rule r , one can effectively come back to r by applying antecedent generalization . 6.3 Other relations same items ( X  X  y ) = and change the consequent, or vice versa. The relation same items allows to re-order the items in a rule. All the rules produced by this relation concern the same population of records in the database. 6.4 Quality-oriented visualization Each subset of rules is visualized in a 3D space, which we call a world . The rep-resentation is built in the following way: each rule is symbolized by an object composed of a sphere perched on top of a cone. Three straightforward graphi-cal characteristics are thus obtained to represent the interestingness measures: the sphere diameter, the cone height, and the color. The representation size depends only on the number of rules in the subset and not on the amount of items. In order to facilitate the navigation (viewpoint control) inside the world, a ground and a sky are represented. As pointed out by Chen [ 26 ], such visual landmarks make the navigation task easier by facilitating the acquisition of spatial knowledge, and more generally by facilitating the building of the cognitive map by the user (men-tal model of the world).
 position [ 24 ]. Therefore, in order to be emphasized, the interestingness measures which are fundamental for decision making are represented by the object position in the world. Since several rules can present the same interestingness, only two measures can be symbolized by spatial position, so that the third dimension is free for scattering the objects. All things considered, we have chosen to use only one axis to place the objects in space and so to spatially represent only one interest-ingness measure. Indeed, the objects are laid out in the 3D world on an arena (a transparent half-bowl), which means that the further an object is, the higher it is placed (Fig. 7). This arena allows a better perception of the depth dimension and reduces occultation of objects by other objects. It can hold around 250 objects at most. A similar choice is made in the document manager Data Mountain of Microsoft Research, where web pages are laid out on an inclined plane [ 70 ]. sent each subset of rules by highlighting the interestingness measures (Fig. 8):  X  the object position represents the entropic implication intensity,  X  the sphere visible area represents the support,  X  the cone height represents the confidence,  X  the object color is used redundantly to represent a weighted average of con-This visual metaphor facilitates comparisons among the rules. It stresses the good rules whose visualization and access are made easier compared to the less good rules. More precisely, a large red sphere perched on top of a tall cone placed at the front of an arena, on the lower steps, represents a rule whose support, confidence and entropic implication intensity are high. On the other hand, a little blue sphere perched on top of a small cone placed at the back, on the upper steps, represents a rule whose three measures are weak. This metaphor is a choice among the many possible combinations. It can be adapted by the user. One can choose, for instance, to change the mappings, or to represent more interestingness measures with color or by using two axes for the spatial mapping.
 the name of the corresponding rule. They provide the numerical values for support, confidence and entropic implication intensity (noted  X  X II X ) too and thus complete the qualitative information given by the representation. 6.5 Interactions with the user The user can interact in three different ways with the visual representation: by visiting a subset of rules, by filtering the rules on the interestingness measures in a subset, and by navigating among the subsets to discover new rules.
 3D world in front of the arena so that (s)he benefits from an overall and synthetic view of the rules. With this comprehensive vision, it is easy to locate the best rules. Then the user can wander freely over the world to browse the rules, and zoom in on them to examine them more closely. (S)he just has to click on an object to move in front of it. In each 3D there also exist predefined viewpoints providing overall visions of the arena (cf., for instance, the viewpoint from the top in Fig. 7C). If the user looks for a rule with particular items in it, (s)he can search it in a menu (Fig. 7A) which lists all the rule names of the subset and allows to move directly in front of the object wanted. In the final analysis, ARVis enables the user to find the rules that interest him/her in a subset whether his/her search criteria are based on interestingness measures or on items.
 interestingness measures by adjusting the thresholds min sp , min cf , min eii , max sp , max cf ,and max eii (Fig. 9A). Only the rules whose measures respect the thresholds are represented. This makes objects appear or disappear in the world.
 the eight neighborhood relations (Fig. 10). By applying a neighborhood relation world is replaced by a new world, which gives the impression of virtually moving inside the whole set of rules. At any time during the navigation, the user can go back to the previous subsets (and worlds) with the  X  X ack X  operator.
 generates a new subset S = ( r ) containing all the rules neighbors of r according to .In ARVis , we systematically add the transitional rule r into the new subset S . Visually, the transitional rule r can be easily located in the world, since its ob-ject flashes. This enables the user to make comparisons between the transitional rule r and its neighbor rules. For instance, with the neighborhood relation agree-ment specialization , it is interesting to compare a rule r to its neighbors in order to see whether or not the addition of a new item in r tends to improve the rule interestingness. Reciprocally, with the relation generalization , comparing a rule r to its neighbors allows to detect the superfluous items in r (those whose removal does not reduce the quality of the rule).
 first subset to focus on with an exploration initialization interface (Fig. 9B). This interface is an  X  X temset browser X  working with inclusive templates: it enables to build the itemset of one X  X  choosing and then to display the world of the rules that include this itemset in the antecedent, or in the consequent, or in both. Further-more, the exploration initialization interface allows to choose the database and the table to be studied, and to choose the set I of the items to be used during the exploration. 6.6 ARVis implementation 6.6.1 Constraint-based rule-mining algorithm When the user triggers a neighborhood relation, ARVis runs a constraint-based algorithm which dynamically computes the appropriate subset of rules with the interestingness measures. As seen in Sect. 6.1, each of the eight neighborhood re-lations induces two kinds of constraints: syntactic constraints and interestingness measure constraints. These constraints are  X  X ushed X  into association rule mining to reduce the exponentially growing search space. The general structure of the constraint-based algorithm is given as follows. no database scan, since it deals only with the syntax of the rules. The syntactic con-straints induced by the neighborhood relations of ARVis are powerful constraints which drastically reduce the number of rules to be produced. Effectively, the syn-tactic constraints are verified by at most | I | rules, whatever the relation chosen. It is easy to enumerate these candidate rules and therefore to enumerate all the item-sets that must be counted in the database during step 2 (Ng et al. [ 65 ] pointed out the interest of such itemset enumeration procedures in constraint-based itemset mining). Thus, whatever the neighborhood relation chosen, the whole constraint-based algorithm has a polynomial complexity 4 in O ( | I | ) .
 improve the response times of the algorithm, a progressive save system is imple-mented in the procedure RetrieveCardinalities: each time an itemset is counted, its cardinality is saved to avoid counting it another time during the remainder of the exploration. In this way, the greater number of times the algorithm is run over the same database, the more the itemset cardinalities are saved, and the more probable it is that the algorithm runs faster. The cardinalities of the itemsets are saved in the database in specific tables. There is one table for each itemset length (number of items in the itemset), so that all the itemsets of the same length are saved in the same table. For each table, the retrieval of the cardinalities uses a B-tree index. based algorithm can be optimized by  X  X ushing X  the interestingness measure constraints into step 1. With the progressive save system, one can indeed an-ticipate that some candidate rules do not respect the thresholds. Eliminating these candidate rules allows to reduce the number of itemsets that must be counted in the database during step 2. For example, with the neighborhood re-lation same consequent , it is useless to generate a candidate rule antecedent  X  consequent if the cardinality of antecedent has already been counted and does not respect cardinality  X  n  X  min sp / max cf and cardinality  X  n  X  max sp / min cf where n is the number of transactions in the database. 6.6.2 Architecture ARVis is built on a client/server architecture with a thin client (Fig. 11). The main block is a CGI program in Perl divided into two parts:  X  the constraint-based algorithm which dynamically extracts the subset of rules  X  a procedure which dynamically generates the corresponding 3D world in The user visits the worlds with a web browser equipped with a VRML plug-in. The exploration initialization interface is a series of web pages generated by the CGI program. 6.6.3 Response time Figure 12 shows the response times obtained on three datasets (presented Table 1) by executing an exploration scenario with ARVis , i.e., a series of neigh-borhood relations. For each relation triggered by the user, the response time is the time required by ARVis to compute the subset of rules with the constraint-based al-gorithm and to display the corresponding world on the screen. The minimum and maximum thresholds chosen in the scenarios are given in Table 2. For the exper-iments, the server of ARVis was running on an SGI Origin 2000 server equipped with four 250-MHz RISC R10000 processors and with 512-MB memory. The DBMS was PostgreSQL. The tables storing the itemset cardinalities were empty at the beginning of the scenarios.
 It is small but it is known to be highly correlated. The exploration scenario that was used with this dataset is given in Table 3. The two other datasets are two large synthetic ones: T10.I4.D100k and T20.I6.D100k. They were generated by the procedure proposed by Agrawal and Srikant [ 3 ] (the number of patterns was set to 1000). The dataset T20.I6.D100k is deliberately very dense (on average, each transaction contains 43% of the items). The exploration scenarios for these two datasets are similar to the one given in the Table 3 but we do not present them, since the data have no real meaning.
 unfolds. This is due to the progressive save system of the constraint-based algo-rithm of ARVis . The peaks in the response time curve (for example for t = 6and t = 11 in the MUSHROOMS scenario) appear when the algorithm needs lots of itemsets that have not been counted yet. In this case, like in any classical proce-dure for frequent itemset mining, the algorithm has to scan the database, which is time consuming.
 mine dense data. In particular, during this experiment, very specific rules contain-ing up to 15 items and presenting a support of 0.07% have been computed. With a levelwise exhaustive algorithm, such specific rules could never be extracted from a dense database. 7 Example of rule exploration with ARVis The example presented in this section comes from a case study made with the firm PerformanSe SA on human resource management data. The data are a set of workers X  psychological profiles used to calibrate decision support systems. It contains around 4000 individuals described by 20 categorical attributes (Table 4). Each attribute has three possible values:  X +, X   X 0, X  and  X -. X  In the example, since flashing objects cannot be seen on the figures, a transitional rule is represented in the worlds by an object with a white sphere.
 By means of the exploration initialization interface, he displays the world that contains the rules with the itemset { ext =+ , po w =+} in the antecedent (Fig. 13A). The user explores the world. There are three rules with high confidence and high entropic implication intensity at the bottom of the arena, and one of them especially interests the user: { ext =+ , po w =+} X  X  rec = X  X  . To know more characteristics of this not very receptive population, he applies the neighborhood relation forward chaining on this rule. The newly displayed world contains the rules with { ext =+ , po w =+ , rec = -} in the antecedent (Fig. 13B). The user finds a rule which he thinks very pertinent: { ext =+ , po w =+ , rec = -} X  { mem =+} . To know the other rules verified by these extrovert, not very recep-tive, and motivated by power and membership people, he applies the neighborhood relation same items on the rule. In the new world, the user sees the four rules that can be built with the four items (Fig. 13C). One is the transitional rule, two oth-ers are bad rules, and the fourth is a little better than the transitional rule: this is { ext =+ , mem =+ , rec = X  X  X  X  po w =+} . To know whether all the items in the antecedent are useful in this rule, he applies the relation generalization on it. In the new world (Fig. 13D), there is the rule { ext =+ , mem =+} X  X  po w =+} that is as good as the transitional rule, which means that the item { rec = X  X  was superfluous. Next, the user continues his exploration by examining the exceptions of the rule (Fig. 13E).
 with the world containing the rules with { rig =+} in the antecedent (Fig. 14A). His attention is drawn by the rule: { rig =+} X  X  anx =+} . This is quite a good rule, but he wants to verify whether other characteristics could better predict strong anxiety. To do so, he applies the neighborhood relation same consequent . The new world contains the rules that conclude on { anx =+} and shows that there is no better rule than { rig =+} X  X  anx =+} (Fig. 14B). So the user comes back to the previous world and applies the relation agreement specialization on { rig =+} X  X  anx =+} to know whether an additional item could improve strong anxiety prediction. The new world presents some better rules effectively (Fig. 14C). 8Conclusion In this article, we have presented the rule-focusing methodology for the post-processing of association rules. It enables the user to explore large sets of rules freely by focusing his/her attention on interesting subsets. The methodology re-lies on:  X  rule interestingness measures which filter and sort the rules,  X  a visual representation which speeds up comprehension and makes the com- X  several neighborhood relations which connect the rules among them and un-rule-focusing methodology by means of a 3D representation, of neighborhood relations meaningful for the user, and of a specific constraint-based rule-mining algorithm. ARVis takes advantage not only of the rule syntax, used in the neigh-borhood relations, but also of the interestingness measures, highlighted in the rep-resentation. This dual approach is original compared to the other rule visualization methods. Moreover, ARVis generates the rules dynamically along the exploration by the user. Thus, the user X  X  guidance during association rule post-processing is also exploited during association rule mining to reduce the search space and avoid generating huge amounts of rules.
 one hand, strengthen the user in certain hypotheses and, on the other hand, provide the user with new ideas. In particular, lots of unknown rules that the user meets along the exploration arouse his/her curiosity and influence the rest of the naviga-tion. Our future works will mainly consist in developing additional neighborhood relations among the rules. For example, a project we have with the French med-ical research center INSERM on cardiac pathology data requires neighborhood relations which rely on item hierarchies and generate rules with multi-item con-sequents. Moreover, we think that the analysis of the exploration logs of ARVis should reveal some  X  X atterns of navigation X  useful to create new neighborhood relations.
 References Author biographies
