 This paper presents a novel two-stage information filtering model which combines the merits of term-based and pattern-based approaches to effectively filter sheer volume of infor-mation. In particular, the first filtering stage is supported by a novel rough analysis model which efficiently removes a large number of irrelevant documents, thereby addressing the overload problem. The second filtering stage is empow-ered by a semantically rich pattern taxonomy mining model which effectively fetches incoming documents according to the specific information needs of a user, thereby addressing the mismatch problem. The experimental results based on the RCV1 corpus show that the proposed two-stage filtering model significantly outperforms other types of  X  X wo-stage X  information filtering models.
 H.3.3 [ Information Search and Retrieval ]: Information Filtering Decision, Experimentation,Theory
The mismatch and overload problems are still not com-pletely resolved by the IF research community. Mismatch refers to the case that some relevant or interesting informa-tion is missed out, and so it impairs the recall of information. On the other hand, overload means too much irrelevant in-formation is returned, and so it impairs precision.
Traditional IF systems do not explicitly deal with the is-sues of mismatch and overload in separate stages; some sys-tems could be effective in handling overload but weak in addressing the issue of mismatch, while others could be the another way round. Existing IF systems are also weak in dealing with issues such as feature selection (e.g., removing noisy and non-relevant features), or threshold setting (e.g., learning the optimal filtering threshold).

To resolve the fundamental issues of mismatch and over-load mentioned before, and improve the effectiveness of IF systems, the initial idea of a two-stage (i.e., topic filtering stage and pattern matching stage) IF framework has been examined in the previous work [7]. In this paper, we have extended this work including refinement of the two-stage IF model and carrying out more experiments for testing differ-ent combinations of term-based and pattern-based IF mod-els. The pattern mining IF model which includes pattern weighting method and pattern mining algorithms has been discussed in our previous works [7, 10]. Due to page limit, we will not discussed it in this paper.
 The main contributions of this research work are twofold. First, a novel rough sets based optimal filtering threshold calibration method has been developed. It was found that a good  X  X alance X  must be found between reducing the  X  X oise X  at the first stage, and at the same time, effectively matching incoming documents with a semantically rich user profile at the second stage. With the help of the first topic filtering stage, pattern mining and matching can be conducted effi-ciently and applied to realistic IF settings. The second con-tribution is that extensive experiments based on the RCV1 corpus have been performed to evaluate the proposed two-stage filtering model. The experimental results confirm that the proposed two-stage IF model which combines the ad-vantages of term-based and pattern-based filtering performs significantly better than other state-of-the-art IF models.
The remainder of the paper is organized as follows. Sec-tion 2 provides a brief review of related work. Section 3 illus-trates the rough sets based filtering threshold optimization method to address the issue of overload at the first filtering stage. The experimental results are reported in Section 4. Finally, concluding remarks are sketched in Section 5.
The term-based approaches for IF have been proposed to address the problems of overload and mismatch over the past decades. The term-based IF systems used terms to repre-sent the user profiles. Such profiles are the most simplest and common representation of the profiles. For examples: the probabilistic models [2], BM25 [9], rough set-base mod-els [11] and ranking SVM [8] based models used the term-based user profiles. The advantage of term-based model is efficient computational performance as well as mature the-ories for term weighting, which have emerged over the last couple of decades from the IR and machine learning com-munities. However, term-based models suffer from the prob-lems, such as, the relationship among the words can not be reflected and also, only considering single words as features is the semantic ambiguity.

In the presence of these setbacks, sequential closed pat-terns used in data mining community have turned out to be a promising alternative to phrases [4]. Pattern mining techniques can be used to find various text patterns, such as co-occurring terms and multiple grams, maximal frequent patterns, and closed patterns, for building up a representa-tion with these new types of features. In [1], data mining techniques have been used for text analysis by extracting co-occurring terms as descriptive phrases from document collections. However, the effectiveness of the text mining systems using phrases as text representation showed no sig-nificant improvement. Mining maximal frequent patterns [3] was also proposed to reduce the time complexity of mining all frequent patterns, where an itemset (or a pattern) was maximal frequent if it had no superset that was frequent.
To consider the very important semantic relationships be-tween the terms, a pattern taxonomy model (PTM) for IF has been proposed in [10]. Pattern taxonomy is a tree-like hierarchy that reserves the sub-sequence (that is,  X  X s-a X ) rela-tionship between the discovered sequential patterns. These pattern based approaches have shown encouraging improve-ments on effectiveness, but at the expense of computational efficiency. Another challenging issue for PTM is to deal with low frequency patterns because the measures used in data mining to learn profiles turn out be not suitable in the fil-tering tasks. To deal with the uncertainty issues, a Rough Set-based IF model(RSIF) has been developed in [6]. There are two key tasks in developing a RSIF model. The first one is us-ing discovered rough patterns to represent the topic profiles. The second task is deciding an optimal threshold based on the obtained topic profiles. In this paper, only the posi-tive documents will be used to represent the user profile as a rough set. It is less efficient to use the features of the non-relevant documents for an information filter [5] since coverage of the feature descriptions for negative documents can be very large.
A set of terms is referred to as a termset . Given a positive document d i and a term t , tf ( d i ,t ) is defined as the number of occurrences of t in d i . A set of term frequency pairs is referred to as an initial r-pattern (rough pattern) in this paper.

Let termset ( p ) = { t | ( t,f )  X  p } be the termset of r-pattern p . In this paper, r-pattern p 1 equals to r-pattern p 2 only if termset ( p 1 ) = termset ( p 2 ). Two initial r-patterns can be composed if they have the same termset . In this paper, we use the composition operation,  X  , that defined in [6] to compose r-patterns. For example, (Notice:  X  is also suitable for patterns with different termsets, e.g., { ( t 1 , 2) , ( t 2 , 5) } X  X  ( t 1 , 1) } = { ( t 1
Based on the above definitions, for a given set of positive documents D + = { d 1 ,d 2 ,...,d n } , there are n corresponding tial r-patterns that have the same termset into clusters and use their composition, a r-pattern, to represent the cluster. Therefore, the training set of positive documents, D + , is de-scribed as a set of r-patterns, RP = { p 1 ,p 2 ,...,p r } , where r  X  n , and n = | D + | is the number of positive documents in D . We write this process as RP = { p 1 ,p 2 ,...,p  X  ( {  X  d 1 ,  X  d 2 ,...,  X  d n } ).

Let cluster ( p i ) be the set of documents (initial patterns) that are composed to generate p i . We can define the support of a r-pattern p i as follows:
Theorem 3.1.1. Let RP = { p 1 ,p 2 ,...,p r } be the set of r-patterns discovered in D + . We have Proof.

For any two r-patterns p i and p j , we have cluster ( p cluster ( p j ) =  X  since the documents in the different r-patterns have the different termset. Therefore, we have | cluster ( p i ) | + | cluster ( p j ) | = | cluster ( p Based on this equation and Eq. 1, we also have
Up to now, the positive documents in the training set have been represented as r-patterns. In the topic filtering stage, discovered r-patterns are employed to filter out most irrele-vant documents rather than to identify relevant documents.
Formally the relationship between r-patterns and terms can be described as the following association mapping if we consider term frequencies: such that where p i  X  RP is a r-pattern; and w i = f i P k
We call  X  ( p i ) the normal form of r-pattern p i in this pa-per. The association mapping  X  can derive a function for the weight distribution of terms on T in order to show the importance of terms in the positive documents, which satis-fies: for all t  X  T .

Theorem 3.2.1. Let RP be the set of discovered r-patterns, then pr  X  is a probability function on T if  X  ( p i ) be the normal form of all r-pattern p i  X  RP .
 Proof. Based on Eq. 3 and Theorem 3.1.1, we have
Based on the above discussion, a positive document d can be described as an event that represents what users want with the probability value. Therefore, the weight of a positive document d i is
To work out the suitable thresholds, it is assumed that document d is irrelevant if it is not closed to the common feature of the topic profiles in the training set. For a given topic, it consists of a set of the positive document, D + document d i has a weight W d i . To capture the common fea-ture of the topic from the training data, the distributions of the document weights for a given topic must be first under-stood.

Many simplistic models assume normal distribution, that is, the data is symmetric about the mean. The normal dis-tribution has a skewness of zero. It is reasonable to as-sume that the scores of the document follow a normally dis-tributed pattern. Using the mean of the Rough Set weights as a threshold would be a good initial choice, because the mean represents the  X  X ommon feature X . According to the statistical approach, if the distributions of the weights of the documents is assumed as a normal distribution then the common feature,  X  j for a topic can be modelled as: where n is the number of the positive documents, n = | D + | . In fact,  X  j is the mean, m , of the probabilities of the positive documents in D + . The thresholds, therefore, can be simply determined as threshold =  X  j .

However, real data points are not always perfectly sym-metric. Skewness is a measure of the asymmetry of the prob-ability distribution of a real-valued random variable. By ob-servations from the experiments conducted in this study, the distributions of the weights of the documents have exhibited a high degree of skewness. To obtain the  X  X eal X  common fea-ture, both the standard derivation and the skewness must be taken into consideration for modeling the document weights. The following features have been used to characterize a his-togram in this paper.
  X  is the standard deviation of the probabilities of positive documents. It is given by:  X  is the skewness of the probabilities. The skewness is given by:
A linear discriminated function is used to make a decision based on features obtained from the above analysis. There-fore, the threshold can be determined as follows: where  X  is an experimental coefficient obtained from specific data sets. It is an empirical value. When the user profiles are specific, a lower value of  X  can be used and allow more documents into the second stage filtering. Likewise, a higher value of  X  will potentially limit the irrelevant documents into next stage. If adaptive filtering will be the study,  X  can be a dynamic parameter (reducing) as the user profiles become more certain.
The Reuters Corpus Volume 1 (RCV1) has been selected to test the effectiveness of the new two-stage information filtering model. All 100 TREC-11 topics have been used in our experiments. F 1 = 2 PR ( P + R ) matrix is used and the paired two-tailed t-test is applied over all 100 topics on F 1 scores.
The proposed two-stage filtering model (T-SM) integrates two types of filtering models: a term-based and a pattern mining-based model. In the first stage, a threshold setting method is developed based on the rough set analysis. In the second stage, a document ranking model is developed based on the pattern taxonomy model. The major objectives of the experiments are to show how the rough threshold model of the first stage can affect the performance of the two-stage fil-tering system and how the pattern mining method can help improve the performance in the second stage. Hence, to give a comprehensive investigation for the proposed model, our experiments involve comparing the filtering performance of the different threshold setting methods and the differ-ent combinations of term-based and pattern-based filtering models.
The two-stage models use threshold min developed in [6] and threshold (see Eq. 6)newly developed in this study at the first stage, respectively. The results on F 1 matrix and p  X  value of t-test display in Table 1.

As can been seen from Table 1, the performance of the two-stage model using threshold is better than the two-stage Table 2: Different integrations of filtering models Table 3: T-SM vs other types of two-stage models model using threshold min on F 1 over F 1 matrix. For the t-test, the p  X  value is less than 0.0001. This is considered to be extremely statistically significant. With regarding to the threshold setting methods, the newly developed threshold setting method significantly outperforms than the threshold setting method developed in [6].
Table 2 illustrates the possible combinations of a term-based model integrated with the pattern-based model or a term-based model integrated with another term-based model, where for the efficiency issue, the model that used for the first stage should be a term-based model. In this section, the topic filtering model developed in this study is called a Topic Filtering Model(TFM). This term-based model will be inte-grated with other term-based model to form a term-based + term-based two stage models.

The results shown in Table 3 are the comparisons of T-SM with other possible two-stage models. The BM25 and SVM based term weight methods are used in the first stage for the BM25 + PTM and SVM + PTM models.

The results show that T-SM significantly outperforms all other two-stage models. The purpose of the topic filtering stage is to move the  X  X oisy X  and prepare more  X  X lean X  data for pattern-mining stage. As can be seen from the above results TFM can achieve this goal because the design ob-jectives of TFM are different from the traditional filtering models. The TFM has an excellent performance for deter-mining non-relevant information comparing with the tradi-tional models that focus on the performance for determining relevant information.
This paper illustrates a novel two-stage filtering model which combines rough sets based reasoning and pattern tax-onomy mining to alleviate the problems of information over-load and information mismatch. The proposed method has been evaluated using the standard TREC routing frame-work and the RCV1 benchmark corpus.

Compared with some other possible types of  X  X wo-stage X  models, the experimental results confirm that the proposed two-stage IF model (T-SM) significantly outperforms the other models. In previous works, our two-stage model com-pared with the term-based and pure pattern based single stage models such as Rochio, BM25, SVM and PTM model. We have concluded that the proposed two-stage IF model (T-SM) is superior than all other single stage IF models. The substantial improvement is mainly due to the rough sets based threshold optimization method applied to the first stage and the  X  X emantic X  based patterns mining and matching applied to the second stage. This research work has delivered a very promising methodology for developing effective and efficient filtering systems based on positive rel-evance feedback. [1] H. Ahonen, O. Heinonen, M. Klemettinen, and A. I. [2] R. Baeza-Yates and B. Ribeiro-Neto. Modern [3] J. Bayardo. Efficiently mining long patterns from [4] N. Jindal and B. Liu. Identifying comparative [5] R. Kaptein, J. Kamps, and D. Hiemstra. The impact [6] Y. Li and N. Zhong. Mining ontology for [7] Y. Li, X. Zhou, P. Bruza, Y. Xu, and R. Y. Lau. A [8] T. Qin, X.-D. Zhang, D.-S. Wang, T.-Y. Liu, W. Lai, [9] S. E. Robertson and I. Soboroff. The trec 2002 [10] S.-T. Wu, Y. Li, and Y. Xu. Deploying approaches for [11] X. Zhou, Y. Li, P. Bruza, S.-T. Wu, Y. Xu, and
