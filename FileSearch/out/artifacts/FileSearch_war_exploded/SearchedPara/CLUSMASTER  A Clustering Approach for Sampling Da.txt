 BILab (Telecom ParisTech &amp; EDF R&amp;D Lab)
The progressive advances in hardware and software tech-nologies have enabled the generation, transmission and capture of data in very high rates in a wide range of fields. Examples of such fields include finance, network monitor-ing, telecommunications, web clickstream, manufacturing, sensor networks, etc. The continuous arrival of data in mul-tiple, rapid, time-varying and possibly unbounded streams appears to yield fundamentally new research problems. As research in data streams processing keep progressing, the problem of how to simply and efficiently expose, share, and integrate data streams from various sources still remains a significant challenge [1]. Clustering techniques have been widely studied across several disciplines, but only a few of the techniques developed scale t o support clustering of very large data. Over the past few years, a number of clustering algorithms for data streams have been put forth [2]. Five different algorithms for clustering data streams are surveyed in [3].

Unlike classical clustering approaches existing in the literature on data streams which are directly applied to raw data represented by sensor measurements, we propose in this article a new clustering approach for sampling multiple source data streams which is applied to the SSE (Sum of Square Errors) matrix. The elements of this matrix rep-resent the error between the o riginal and the interpolated measurement curve of a sensor when its data is sampled at a specific rate. In sensor networks, the rate at which data is collected at each node affects the communication resources and the computational load at the central server. The proposed approach seeks to attribute the best sampling rate for each sensor and to guarantee that the volume of transferred data fits a predefined bandwidth.

The article is organized as follows. Sections II and III describe some existing work on data streams sampling and clustering. The formulation of the problem is presented in Section IV. Section V details the data streams sampling al-gorithm used in our approach. Section VI presents our clus-tering approach for sampling data streams. The experimental framework, the data streams analyzed, the implementation details and the result analysis are described in Section VII. Finally, concluding remarks and future work are discussed in Section VIII.

There are many applications where data is continuously produced by a large number of distributed sensors. Adap-tive sampling has been developed for these applications to manage limited resources. They aim at conserving network bandwidth and storage by filtering out data that may not be relevant in the current context. The data collection rate becomes dynamic and adaptable to the environment.
Most existing adaptive sampling techniques sample data from each source ( temporal sampling ). An adaptive sam-pling scheme which adjusts data collection rates in response to the content of the streams was proposed in [4]. A Kalman filter is used at each sensor to make predictions of future values based on those already seen. The Sampling Interval ( SI ) is adjusted based on the prediction error. If the needed sampling interval for a sensor exceeds the maximum that is allowed by a specified SI range, a new SI is requested to the server. The central server delivers new SI s according to available bandwidth, network congestion and streaming source priority.

In [5], the authors present a feedback control mechanism which makes the frequency of measurements in each sensor dynamic and adaptable. Sampled data are compared against a model representing the environment. An error value is calculated on the basis of the comparison. If the error value is greater than a predefined threshold, then a sensor node collects data at a higher s ampling rate; otherwise, the sampling rate is decreased. Sensor nodes are completely autonomous in adapting their sampling rate.

In [6], the authors present a method to prevent sensor nodes from sending redundant information; this is predicted by a sink node using an ARIMA prediction model. Energy efficiency is achieved by suppressing the transmission of those samples whose ARIMA based prediction values are within a predefined tolerance value with respect to their actual values. A similar approach is proposed in [7]. Their results show that reduced communication between sensors and the central server can be sufficient by using an appro-priate prediction model. A wide range of queries (including heavy hitters, wavelets and multi-dimensional histograms) can be answered by the central server using approximate sketches.

On the other hand, the authors of [8] use a spatial sam-pling technique called the  X  X ackcasting approach X . Back-casting operates by first activating only a small subset of sensor nodes which communicate their information to a fusion center. This provides an estimate of the environment being sensed, indicating that some sensors may not need to be activated to achieve a desired level of accuracy. The fusion center then backcasts information based on the estimate to the network and selectively activates additional sensors to obtain a target error level. In this approach, adaptive sampling can save energy by only activating a fraction of the available sensors. A similar approach has been developed in [9] to compute the average of data in a sensor network. A subset of sensors is activated and their data are used to make statistical inference on data from  X  X nactive X  sensors using a Bayesian approach. The algorithm developed is called infer and operates in two phases. During the first phase, a distributed algorithm decides which sensors should be active for the next time period based on their levels of energy (this is to increase the lifetime of the sensor network) and their historical data. The second phase occurs at the central server and uses Bayesian inference to predict the average data of all sensors. Even if authors were only interested in the average AVG to validate their approach, Bayesian inference has the advantage of being applicable to a wide range of aggregate queries.

In [10], the authors use Min-Wise sampling [11] to select uniformly and randomly a number r of sensors. The principle of this approach is as follows: given a hash function h applied to sensors ID, the central server selects the r sensors that have the minimum value computed by the hash function. This technique has the advantage of generating a random and uniform sample and its principle is very close to the Bernoulli sampling. Another technique for sampling sources with spatially distributed data was proposed in [12]. The authors use a uniform random sample of nodes in a sensor network to aggregate data from the sensors. Their approach is based on ancillary data such as geographic data to determine the number of nodes to select.

The main issue that one may ask is: what strategy (tem-poral or spatial) has to be used to summarize distributed data streams? It is obvious that the answer depends on the application and treatment on data. We will try to give an answer to this issue later in this article as part of the aggregation query processing (especially to estimate SUM) over all or part of the distributed data streams. We also propose a strategy to combine these two approaches, in order to build estimators of aggregate data streams of better quality than if we had made a purely temporal sampling or purely spatial sampling. In addition, we will discuss the possibility of dynamically updating the sample in order to take into account the evolution of data.

Clustering data streams is a well-studied problem in both theory and practice. One of the earliest and best known clus-tering algorithms for data streams is BIRCH [13]. BIRCH is a heuristic that computes a pre-clustering of the data into so-called clustering feature vectors (micro clustering) and then clusters this pre-clustering using an agglomerative bottom-up technique (macro clustering). BIRCH does not perform well because it uses the notion of radius or diameter to control the boundary of a cluster. STREAM [14] is the next main method which has been designed specially for data streams clustering. This method processes data streams in batches of points represented by weighted centroids which are recursively clustered until k clusters are formed. The main limitation of BIRCH and STREAM is that old data points are equally important as new data points.
CluStream [15] is based on BIRCH. It uses the clustering feature vectors to represent clusters and expands on this concept by adding temporal features. This method takes snapshots at different timestamp, favoring the most recent data. A snapshot correspond to q (where q depends on the memory available) micro-clusters stored at particular moments in the stream. New data points will be assigned to one of the micro-clusters in a previous snapshot if it falls within the maximum boundary of that micro-cluster. One weakness of the algorithm is the high number of parameters which depend on the nature of the stream and the arrival speed of elements.
StreamSamp [16] is based on CluStream and combines a memory-based reduction method and a time-based reduction module based on tilted windows. In StreamSamp, summaries with a constant size are maintained. The summaries cover time periods of varying sizes: shorter for the present and longer for the distant past. StreamSamp has the advantage of being fast on building the data streams summary. How-ever, its accuracy degrades over time because old elements increase in weight for a given sample size.

The aim of this article is to address the problem of storing infinite data streams from multiple sensors on disk with limited storage space.

The architecture considered here consists of a distributed computing environment, describing a collection of N re-mote sensors that feed a Data Stream Management System (DSMS). We assume that sensor measurements are generated regularly at the same rate for every sensor. We cut out all streams into sampling periods  X  also called temporal windows  X  of equal length. A typical sampling period corresponds to a length of one day. A set of measurements issued by a sensor during a sampling period is also referred as a curve from now on. A temporal window is composed of p measurements for each sensor (the number of measure-ments per sensor in the window is the same for all sensors). For each sensor, the number of measurements that have to be stored within a temporal window can vary from m (fixed parameter) to p . Let us define s as the maximum number of measurements which can be stored on the disk from the N sensors during a temporal window ( s&lt;N  X  p ).
The problem consists of finding the best  X  X olicy to sum-marize sensor curves within a time window which respects the constraints of the maximum number of data stored on disk (parameter s ) and the minimum number of data taken from a sensor (parameter m ). The goal is to answer to aggregate queries using the summary. Each curve r takes the value C r ( t ) at time t . In this article, we concentrate on SUM aggregate queries: where P ( N ) is a subset from { 1 , 2 ,...,r,...,N } .
The data collection model should preserve detailed in-formation as much as possible by reducing summarizing errors. Several criteria can be used to measure errors. In this article we use the Sum of Square Errors (SSE), which is also the square of the L 2 distance between the original curve C = { c 1 ,c 2 ,...,c p } and an estimation from the summarized curve  X  C = {  X  c 1 ,  X  c 2 ,...,  X  c p } . SSE is computed as follows:
Other error measures can be defined depending on the application. SSE is well-adapted to electric power consump-tion [17].

In this section, we present the different sampling meth-ods that are part of the algorithm MASTER (Method for Adaptive Spatio-Temporal summaRies).
 A. Temporal sampling
Temporal sampling is the process of keeping fewer mea-surements from a curve, preserving the underlying informa-tion as much as possible. Note that if no temporal sampling is done, then aggregating the whole population curves at each time t consists of summing all values C r ( t ) measured on each curve r in the following way:
From the observation of what is happening during sam-pling period T  X  1 , we determine a data collection model to apply to the N sensors for the next sampling period T . During a sampling period T , sensors send data to the DSMS. The engine computes on the fly SSE values for different assumptions of sampling rates between m and p values. It also summarizes data streams at the scheduled sampling rate and stores summarized data on a database. Then the application solves an optimization problem to find the best sampling rate for each sensor during sampling period T +1
Several sampling schemes can be considered to sum-marize data streams during a sampling period. Almost all methods of curve compression can be used depending on the handled data. For the needs of our application, we concentrate on one method: regular sampling. Other methods such as wavelet compression and curve segmentation have been experimented in [18].

In regular sampling, curves are sampled using a step j chosen between 1 and maxStep = p m ( x is the floor function). When j =1 , all measurements are selected, when j =2 every two measurement is selected and so on. This technique is described as regular because selected points are temporally equidistant, a jump j is made between two sampled measurements. For the estimation of the original curve from the sampled measurements, linear interpolation is used. Let c a and c b be two measurements of a same curve at different timestamps a and b , an intermediate measurement c (where a&lt;i&lt;b ) is obtained in the following way: At the end of each sampling period, different values of SSE X  X  are computed which correspond to different levels of summarization, indexed by j varying from 1 to maxStep . These SSE values are stored in a matrix W N  X  maxStep of N rows and maxStep columns ( N is the number of sensors connected to the server). Element w ij of the matrix corresponds to the SSE obtained when collecting data from sensor i with a summarizing level j .

Instead of distributing equally the summarizing levels between all sensors for sampling period T +1 ,an optimization is performed to assign different sampling rates to sensors from the SSE values computed during sampling period T . This problem can be formalized as an optimization problem minimizing the sum of SSE X  X  on sensors: subject to:
This is a problem of assignment of sampling rates to the sensor curves respecting the constraints above. A variable X ij =1 means that the sampling step j is assigned to the curve of index i . The second constraint maxStep j =1 X ij imposes only one value of j per curve. Lastly, the third constraint means that the number of data to be stored should not exceed the threshold s . To solve this problem, we used the simplex method applied to linear problems with real variables. The simplex method is coupled with the Branch-and-bound method to reach integer values [19]. Once the optimization problem is solved, the optimal sampling rates are sent to each sensor for regular sampling. B. Spatial sampling
Spatial sampling is the process of selecting an appropriate set of sensors so that the sample allows estimating unknown quantities (queries) of the population (the N sensors). This is similar to a standard sampling survey problem.
We consider a finite population (the N sensors). Each sensor can be identified by a label. Let { 1 , 2 ,...,r,...,N be the set of these labels. The aim of survey sampling is to study a variable of interest which is the curve in our context. The curve takes the value C r ( t ) for a sensor r at instant t . The purpose is more specifically to estimate a function of interest of the C r  X  X  for all or a subset of sensors. Note that we concentrate on SUM aggregate queries in this article.
There are many strategies of sampling in literature: simple random sampling, stratified sampling, sampling with unequal probabilities, balanced sampling, etc. (for details, see [20]). We use the simple random sampling in our framework. Let S be a sample containing n sensors,  X  r be the inclusion probability, i.e., the probability that sensor r is in the sample S ,  X  rq be the second-order inclusion probability, i.e., the probability that both sensors r and q belong to the sample S . We use the simple random sampling in our framework where the same inclusion probability  X  r is assigned to each sensor. An unbiased estimator often used is the Horvitz-Thompson estimator. We can calculate the SUM estimate of curves as follows:
The variance of this SUM can be estimated by: In the case of simple random sampling, the inclusion prob-
The confidence intervals with a confidence level of (1  X   X  are estimated as follows: where z 1  X   X / 2 is the (1  X   X / 2) -quantile of a standard normal random variable.
 C. MASTER algorithm
In both sampling techniques above (temporal sampling and spatial sampling), summarized estimates are used to answer the SUM aggregate queries. Interpolation in the first case, estimates affected by sampling error in the second case. We propose a method that combines these two approaches in order to reduce summarizing errors as much as possible. We call this  X  X ethod for Adaptive Spatio-Temporal sum-maRies X  or MASTER.

The principle of the spatio-temporal approach is repre-sented in Fig. 1 and is as follows: A survey technique is integrated into the temporal sampling approach to make up a spatio-temporal summary. As with traditional approaches, a survey plan is set up to select a sample with a size of n sensors from the whole population, i.e., the N sensors. Then, we apply the temporal sampling approach to collect mea-surements at variable granul arities in time for each sensor in the spatial sample and store them in a database. The sensors that are not part of the spatial sample are not observed. The curves in the temporal sample are reconstructed by linear interpolation. The remaining curves are estimated with the average curve of the sample. Algorithm 1 describes MASTER steps.
 Algorithm 1 MASTER Require: T , maxStep , s 1: Apply the spatial sampling to the whole population (the 2: Define sensor curves by reading all measurements sent 3: Compute SSE matrix W n  X  maxStep from curves of the n 4: Compute the best sampling rates for each sensor by 5: Apply the temporal sampling to the n active sensors on
Clustering techniques group individuals based only on information found in the data describing the individuals and their relationship. The goal is that the individuals in a cluster be similar to one another and different from the individuals in other clusters. The greater the difference between groups and the similarity (or homogeneity) within a group, the better or more distinct the clustering is.

In this section, we present CLUSMASTER (CLUStering on MASTER). The goal of our approach is to apply a clustering technique on the SSE matrix before computing the best sampling rates for each sensor in a network. This is done in order to reduce the execution time and the physical resources (e.g., CPU power) allocation required by the MASTER algorithm. As described in section V-A, each row of the SSE matrix W N  X  maxStep represents a sensor, each column represents a sampling rate in the interval [1 ,maxStep ] and each cell w ij represents the SSE value when the sampling rate j is applied to the sensor i .In our approach, we have analyzed two clustering strategies described as follows.
 A. Random clustering
In this strategy, all sensors are randomly assigned to k clusters. The sensors are equally distributed among the clusters. The best sample rates are then computed within each cluster by means of the linear programming solver (cf. Fig. 2). The total number of elements in each cluster is represented by n c ,where the allowed bandwidth for each cluster C is proportional to n c . In order to avoid unstable solutions introduced by the random nature of this strategy, the clustering process is repeated a predefined number of times. Notice that when k =1 , the solution found by this strategy corresponds to the one found by the MASTER algorithm with no clustering. B. Clustering based on a dissimilarity criterion
In this strategy, we apply a clustering method which minimizes a dissimilarity criterion to the SSE matrix. This criterion corresponds to a distance computed among all the sensors. In our case, we applied the Hierarchical As-cendant Clustering (HAC) algorithm with three different distances: Manhattan, Euclidean and Maximum, described in Table I. Once the clusters are defined, their representative (prototype) is computed by summing up the SSE of all individuals belonging to a same cluster. The prototype of acluster C is represented by a maxStep -tuple p c = ( p c ,...,p belonging to a same cluster in the SSE matrix are then replaced by the prototype of the corresponding cluster. The linear programming solver is then applied to the reduced SSE matrix containing k rows and maxStep columns (cf. Fig. 3). The sampling rate corresponding to a particular cluster is then assigned to each sensor belonging to the clus-ter. Unlike the random clustering strategy which finds the optimal solution for the samping rates within each cluster, the solution found by means of the clustering strategy based on a dissimilarity criterion is approximate. In this strategy, as individuals belonging to a same cluster have similar SSE values, it is expected that the same sampling rate will fit all the individuals in a cluster.
The parameters used in the experiments are specified as follows. The number of clusters k is in the range [1 , 50] maxStep is equal to 10 , the bandwidth is equal to 50% which means that the concentrator accepts half of total measurements at maximum, the number of repetitions of the random clustering is equal to 50 and parameter T is equal to 1, i.e., the time window has length equal to 1 day.
In our experiments, we applied three strategies described as follows: 1) MASTER algorithm with no clustering; 2) MASTER algorithm with random clustering; 3) MASTER algorithm with clustering based on a dis-
One of the advantages of clustering strategy 2 is that it can be solved in a framework that applies parallelized jobs, which can significantly reduce the execution time of this strategy. This characteristic was however not explored in our experiments. In strategy 3, we apply three different distances (cf. Table I) within raw and normalized data. For each strategy, we calculate the maximum, the minimum and the average execution time as well as the SSE on sampling period T and the SSE on sampling period T +1 .
 A. Data description
In our experiments, we anal yzed data streams produced by a 1-thousand sensor network from January 1 st 2009 to December 31 st 2009 at a rate of 1 measurement every 30 minutes, which corresponds to 48 measurements per day. Each sensor corresponds to a real electricity meter installed at a household by the French energy group (EDF). Each measurement represents an instantaneous power registered by the meter. For experimental purposes, all the measure-ments sent by the sensors are funnelled into a single input file which is then treated by the DSMS Esper from Es-perTech 1 (cf. Fig. 4). In real situations, the multi-source data streams can be directly read from the sensors by Esper. Esper is an open source event stream processing (ESP) and event correlation engine (CEP) providing a rich Event Processing Language (EPL) to express filtering, aggregation, and joins, possibly over sliding windows of multiple event streams. Esper can be easily embedded in an existing Java application or middleware to add real-time event-driven capabilities to existing platforms without paying high serialization cost or network latency for every message received and action triggered.

For the data streams analyzed, each event is represented by a unique identifier, a single measurement (power) and a timestamp. Events are encapsulated by Esper in a time window and then sent to our engine. Once the sampling rates are computed, samples for each sensor are stored in a PostgreSQL database.
 B. The class diagram
Fig. 5 shows the class diagram of our approach imple-mented in Java. The input data is treated by the Esper package that reads events from a .CSV (Comma Separated Values) file. The Esper package contains the esper.jar and esperio.jar java libraries which encapsulate the Esper func-tionalities to manipulate data streams. In our architecture and due to experimental purposes, input data come from a file containing 1 event per row. The DSMS Esper is able to read streams from other input devices, e.g., a network socket. The event structure is described in an .XML file which is read by an XML parser.

The main class of this diagram is named Application which reads parameters (MASTER pa rameters, clustering parameters, length of the temporal window, database connec-tion parameters, etc.) from the Config class and instantiates the CLUSMASTER class. The CLUSMASTER class en-compasses clustering and MASTER functionalities and uses for that two special Java libraries called LpSolve and JRI. LpSolve 2 is a free linear (integer) programming solver based on the revised simplex method and the Branch-and-bound method for integers. R 3 is a programming language and software environment for statistical computing and graphics. The R language has become a standard among statisticians and is widely used for statistical software development and data analysis. JRI 4 is a Java/R Interface, which allows run-ning R inside Java applications as a single thread. Basically, it loads R dynamic library into Java and provides a Java API to R functionalities.
 The CLUSMASTER class receives events from the DSMS Esper and converts them to an object of the Point class. An event corresponds to an instantaneous measurement sent by a sensor in the network. A set of measurements coming from a same sensor constitutes a curve. The inactive sensors, i.e. the ones which are not selected in the spatial sampling, receive special treatment implemented in the InactiveSensor class. Once the sampling rates are defined for each active sensor, samples are stored in a database by means of database connection functionalities implemented in the DbConnection class.
 C. Result Analysis
The experiments described in this section were carried out in a dual-core Sun machine, ultra24 model running Linux Fedora 11. In order to guarantee representative results, all the 1-thousand sensors are selected in the spatial sampling module of MASTER and thus activate in our experiments. According to the parameters used, the SSE matrix is com-puted at the end of the each day and has one thousand rows (one per sensor) and 10 columns (one per sampling rate).
Fig. 6 shows the execution time spent by each clustering strategy. Fig. 7 and Fig. 8 show the logarithm of SSE values computed on sampling period T (current day) and on sampling period T +1 (next day), respectively, by each clustering strategy according to the numer of clusters k  X  [1 , 50] . All the charts present the minimum, the average and the maximum values obtained by the three strategies to process a day of measurements. In the charts, the horizontal gray lines represent the minimum (min), the average (avg) and the maximum (max) values obtained by the strategy applying the MASTER algorithm with no clustering.
In Fig. 6, the average execution time (represented by the horizontal gray line named avg ) of strategy 1 is roughly equal to 300 seconds (5 minutes). The execution time of strategy 2 (represented by the red line) is equal to the one of strategy 1 when k =1 and tends to zero when the number of clusters increases. This is due to the fact that when k =1 , all the individuals are assigned to the same cluster and the linear programming solver is applied within a single cluster. In this case, strategy 2 has no advantages over strategy 1. When the number of clusters increases, the original SSE matrix is split into smaller SSE matrices. As a consequence, the linear programming solver deals with optimization problems of smaller magnitude, which requires shorter execution times. On the other hand, strategy 3 is very fast. Its execution time tends to zero no matter the number of clusters and the dissimilarity function applied (cf. the superimposed lines representing different distances applied within this strategy). This is due to the fact that the original SSE matrix containing N = 1000 rows is reduced to a matrix containing k rows (cf. Fig. 3), which represents a significant time execution.

In Fig. 7, the logarithmic average value obtained by strategy 1 is approximately equal to 17.5. The logarithmic SSE value obtained by the strategy 2 corresponds to the average value obtained by strategy 1 when k =1 ,and increase with the number of clusters. This is due to the fact that a same bandwidth is shared by a greater number of clusters which are composed by sensors randomly assigned. On the other hand, strategy 3 assigns similar sensors to a same cluster. This strategy achieves its worst logarithmic SSE value when k =1 , i.e., when all the individuals are assigned to a same cluster and have a same sampling rate. This strategy tends to find more homogeneous clusters when the number of clusters increases. Since the sensors assigned to a same cluster are similar, a same sampling rate tends to better fit smaller clusters. As a consequence, the SSE values decrease when the number of clusters increases. If the number of clusters is equal to the number of sensors ( k = N ), i.e., if each cluster has only one sensor, strategy 3 reaches the same SSE values obtained by strategy 1, because in this case the reduced SSE matrix is equal to the original SSE matrix.

Regarding the SSE on sampling period T +1 , computed when the sampling rates obtained from data on sampling period T are used to sample data on sampling period T +1 , the results obtained are those shown in Fig. 8. We can notice that the logarithmic SSE values obtained by the strategies on sampling period T +1 are quite similar to those obtained on sampling period T (Fig. 7). The main difference is that the SSE values obtained by the strategies on sampling period T +1 are greater then those on sampling period T .This is explained by the fact that the optimization module of the strategies are computed on sampling period T and have no information about the sensor measurements of sampling period T +1 .

Regarding the dissimilarity distance applied in the strategy 3, we did not notice a particular difference between the three distances applied (cf. Table I). Fig. 9 shows a graphical rep-resentation of the SSE matrix computed with the Euclidean distance within normalized data. Each curve in this chart represents a row of the original SSE matrix. Curves that belong to a same cluster are painted with a same color. Notice that similar curves are assigned to a same cluster. It is worth emphasizing an important characteristic of the SSE matrix related to the fact that SSE increases with the sampling rate. Notice that in Fig. 9 the SSE curves are all increasing and never cross each other. This is due to the fact that the SSE computed on a non-linear curve sampled with rate z is smaller than the SSE computed for this same curve sampled with rate z +1 . In other words, the greater the sampling rate is, the smaller the number of points sampled is. As a consequence, the interpolated curve is less accurate and the SSE values are greater.

In conclusion, we can say that clustering strategies 2 and 3 perform very well over the strategy 1, which applied the MASTER algorithm with no clustering. In the strategies with clustering, special attention should be paid to the choice of the number of clusters. In our experiments, 14 &lt;k&lt; 24 seams to be a good choice for strategy 2 and k&gt; 14 for strategy 3 since the trade-off between the execution time and SSE values is acceptable. If the goal of the application is to rapidly determine the best sampling rate for all sensors in a network, strategy 2 is suitable. If besides the sampling task, the application has to identify groups of similar individuals (in our study case, a cluster could represent a group of individuals with similar electrical consumption behaviour), strategy 3 should be applied.

In this article, we presented a new approach for com-bining clustering and sampling techniques on data streams. The proposed approach is generic and its parameters are independent of the nature of the data streams. The goal is to assign the best sampling rate to each sensor in a network in a rapid and efficient way by minimizing the sum of square errors (SSE). This optimization is dynamic and adapts continuously with the content of the stream. The proposed approach presents a good trade-off between the execution time and the SSE values obtained. Experiments have been done on real data describing measurements of electric power consumption generated by power meters installed in French households.

The analysis of results obt ained in the experiments carried out clearly testifies the power and benefits introduced by clustering into sampling techniques. Clustering strategies as the ones presented in this work are recommended for dealing with the problem of sampling and storing massive data streams generated by multiple sources. Future work may include: experiments on larger data sets generated by a larger number of sensors on a longer sampling period, extension of the approach to sensors producing multidimensional numeri-cal data, investigation and test of other clust ering techniques.
Concerning the electrical power and automation indus-tries, new challenges will arise from the development of the energy Web . The energy Web envisions a day when intelligent devices all across the value chain  X  generation, delivery, end use  X  communicate and interact to optimize the system. The energy Web refers to the integration of the utility electrical system, telecommunications system, and the energy market to optimize loads on the electrical network, reduce costs to consumers and utilities, facilitate the inte-gration of renewable resources, increase electrical system reliability and reduce environmental impacts of load growth. In a smart home equipped with intelligent electronic devices (e.g., lighting, temperature control, multi-media, security, window and door operations, etc.) emitting instantaneous measurements, the volume of data streams thus generated can reach colossal magnitudes. The proposal of efficient and fast sampling techniques able to deal with this new environment of sensor networks will certainly yield future research challenges.

This work is supported by the French national research agency  X  the ANR (Agence Nationale de la Recherche)  X  under grant n  X  ANR-07-MDCO-008 (the ANR MIDAS Project).

