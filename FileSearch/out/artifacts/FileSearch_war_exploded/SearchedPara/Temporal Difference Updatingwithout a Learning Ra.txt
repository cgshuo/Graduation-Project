 Witten [9]. The idea is as follows: The expected future discounted reward of a state s is, where the rewards r definition it follows that, Our task, at time t , is to compute an estimate V t have to base this estimate on is the current history of state t ransitions, s history of observed rewards, r r +  X V s heuristic for state s is known as TD (0) . One shortcoming of this method is that at each time step the va lue of only the last state s states as well. Formally, for any state s its eligibility trace is computed by, ence update is then, for all states s , This more powerful version of temporal different learning i s known as TD (  X  ) [7]. set the learning rate see [1].
 reinforcement learning problem. Section 7 ends the paper with a summary and some thoughts on future research directions. The empirical future discounted reward of a state s state s Formally, the empirical value of state s where the future rewards r v also on unknown future rewards. Note that if s state twice at different times m and n , this does not imply that v following the state visit may be different each time.
 Our goal is that for each state s the estimate V t future discounted reward V that s = s by some parameter  X   X  (0 , 1] . Formally, we want to minimise the loss function, For stationary environments we may simply set  X  = 1 a priori.
 each state and set to zero, where we could change V t Since v note that v way, Substituting this into Equation (5) and exchanging the orde r of the double sum, where E t the discounted reward with eligibility.
 have to replace with our current estimate of this value at tim e t , which is V t bootstrap our estimates. This gives us, For state s = s we obtain, an incremental update rule is more convenient. To derive thi s we make use of the relations, with N 0 By solving Equation (6) for R t Dividing through by N t +1 Making the first denominator the same as the second, then expa nding the numerator, After cancelling equal terms (keeping in mind that in every t erm with a Kronecker  X  may assume that x = y as the term is always zero otherwise), and factoring out E t V s = V Finally, by factoring out  X N t where the learning rate is given by, gibility traces (see Equation (2)), however the learning ra te  X  has now been replaced by  X  this problem.
 The first term in  X  follows: N t value estimate based on relatively many samples. In such a si tuation, the second term in  X  the learning rate so that V t +1 r and the learning rate is reduced in order to maintain the exis ting value of V We ran our algorithm 10 times on the above Markov chain and com puted the root mean squared Figure 1: 51 state Markov process averaged over 10 runs. The parameter a is the learning rate  X  . thus discounting old experience is not helpful.
 these tests appear in Figure 1.
 closer, we averaged over 300 runs. These results appear in Fi gure 2.
 tuning. To test on a Markov process with a more complex transition str ucture, we created a random 50 ran a brute force Monte Carlo simulation to compute the true d iscounted value of each state. HL (  X  ) and no manual learning tuning was required for these perform ance gains. Figure 3: Random 50 state Markov process. The parameter a is the learning rate  X  . changing, the lower we need to make  X  in order to more rapidly forget old observations. between the models of the environment every 5,000 steps. At e ach switch, we also changed the environment. For this experiment we set  X  = 0 . 9 .
 when we pushed  X  above its optimal value this caused poor performance in the p eriods following over 200 runs. The results of these tests appear in Figure 4.
 cycle. In any case, in this non-stationary situation HL (  X  ) again performed well. HL (  X  ) should be possible.
 to use an HL temporal difference update. In the presentation of this algorithm we have changed Algorithm 1 HLS(  X  ) Initialise Q ( s, a ) = 0 , N ( s, a ) = 1 and E ( s, a ) = 0 for all s , a
Initialise s and a repeat until end of run defined Q ( s, a ) := V to compute the  X  values, and replace  X  with  X  in the temporal difference update. task with an automatic transition from the goal state back to the start state. get accurate statistics.
 setting.
 we used with Sarsa(  X  ) above.
 that it achieved this level of performance without having to tune a learning rate. Figure 5: [Windy Gridworld] S marks the start state and G the goal state, at which the agent jumps back to S with a reward of 1. Markov chains or reinforcement learning, our new method pro duced superior results. Q reinforcement learning algorithms.
 Acknowledgements This research was funded by the Swiss NSF grant 200020-10761 6.

