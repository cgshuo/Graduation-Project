
Constrained pattern mining extracts patterns based on their individual merit. Usually this results in far more pat-terns than a human expert or a machine learning technique could make use of. Often different patterns or combinations of patterns cover a similar subset of the examples, thus be-ing redundant and not carrying any new information. To re-move the redundant information contained in such pattern sets, we propose a general heuristic approach for selecting a small subset of patterns.

We identify several selection techniques for use in this general algorithm and evaluate those on several data sets. The results show that the technique succeeds in severely re-ducing the number of patterns, while at the same time ap-parently retaining much of the original information. Addi-tionally the experiments show that reducing the pattern set indeed improves the quality of classification results. Both results show that the approach is very well suited for the goals we aim at. Pattern search is one of the main topics in data mining. In the last years different types of pattern languages were developed in order to be able to deal with the ever-present challenge of finding new and hopefully valuable representa-tions for all kind of data. Most algorithms developed handle the usually computationally intensive task in a decent way enabling us to extract millions of patterns from even small data sets. These patterns are then presented to the user or they are used as features such that each instance of the orig-inal database can be converted into a binary vector, each bit encoding the presence or absence of the pattern.

Due to the fact that interestingness (i.e. constraint sat-isfaction) of patterns is evaluated for each pattern individ-ually, the amount of patterns to be considered by a user is often too large. Furthermore, when presenting the binary vector data to a machine learning technique, an overabun-dance of features does not help in the learning task, possibly even  X  X onfusing X  the algorithm, leading to overfitting.
The aim of our work is to reduce the set of patterns re-turned by a data mining operation to a subset that is small enough to be inspectedby a human user. To be of maximal benefit to the user this set should show little redundancy while retaining as much information as possible encoded in the full pattern set. In our approach, the information of a pattern set is determined by the partition it induces on the examples, with all examples containing the same set of patterns belonging to one and the same block. Thus, in-formation is obtained from the composition of all patterns. This is in contrast to e.g. the notion of sets of closed [12] patterns, which only takes care that no two individual pat-terns induce the same partition. It does not take into account complementary information, i.e. patterns that are mutually exclusive. Additionally, especially on dense data sets the number of closed patterns is still significantly larger than anything humans can be reasonably expected to peruse.
Since the high amount of patterns leads to a very large search space of possible subsets, we develop a heuristic technique for solving the problem. In order to accommo-date different notions of how to traverse the set and how to select patterns, we keep the main algorithm rather general. Based on our intuition about redundancy between patterns we develop several selection measures and combine them with straight-forward ordering strategies. When evaluated on several data sets the techniques reduce the set of closed patterns severely.
 The rest of paper is structured as follows: in the next Section we introduce notions used throughout this work and describe the general formulation of our technique. In Sec-tion 3 we describe several instantiations of the general algo-rithm, motivating the different selection strategies. In Sec-tion 4 we perform experiments on several data sets, showing the effectiveness of the used selection techniques, and com-paring and discussing the resulting reduced pattern sets. In Section 5 we discuss related work and finally, we conclude in Section 6.
As stated above, the main goal of our work is present-ing both human user and machine learning technique with a set of patterns that is small enough to be easily processed. Small size is not a virtue in itself, however, if there is little information encoded in the pattern set. Thus it is important that much of the characteristics of the data described by the complete set of patterns is retained, and it is desirable that the redundancy between patterns in the set is low.

To achieve selection of patterns beyond that done in the mining operation, additional knowledge is required. The cheapest and often only form of background knowledge available is the database T which the patterns were ex-tracted from. By evaluating subsets of patterns jointly on this database, we exploit the knowledge the mining opera-tion itself did not use.

Hence, our goal is the following: Given a set S of pat-terns p i , the database T , and a redundancy measure  X  ,se-lect a subset S  X   X  S , such that S  X  satisfies the following requirements: 1. | S  X  | is small, such that a human expert could inspect it. 2. members of S  X  have low redundancy w.r.t. T accord-3. members of S  X  describe characteristics of T .

We will concretize these requirements in the next sec-tion. To give a formal description of the general technique, we introduce the following notions:
Let each pattern p be associated with a function p : T X  { true, f alse } . We define p ( t )= true if p matches t , and p ( t )= false otherwise. In general a pattern does not have values as such but can only be present or absent. Associ-ating a pattern with this boolean function does allow us to consider each pattern as a binary feature though.

A set of patterns S = { p 1 ,...,p n } is called a pattern set . Given a pattern set S we define an equivalence relation  X 
S on the set T of transactions as Thus, two transactions are considered to be equivalent un-der S if they share exactly the same patterns. Using the equivalence relation  X  S the partition or quotient set of over S is defined as with the equivalence classes also called blocks .

To get an intuition why partitions are central to our tech-nique, consider the following: For a machine learning algo-rithm a subset S  X  that induces the same partition as S will be of the same usefulness as the complete set since the sep-arability of instances is equally well possible. The actual syntactical composition and support of the patterns are not of interest in this case.

For a human user with elaborated knowledge about the domain the situation might be somewhat different, but by defining the total order &lt;  X  S  X  S in which to process pat-terns the user can control to some extend which patterns are considered for selection. Additionally, considering a sub-set of patterns that induces a partition on the entire data is preferable to browsing hundreds or even thousands of pat-terns or looking through the k most interesting which pos-sibly contain more or less the same information.
 Furthermore, we define a measure  X  as The measure is supposed to rate the value of adding p to S  X  The higher the value of  X  , the more valuable it would be adding p to S  X  .

To motivate the usage of an additional measure, consider that the minimal number of patterns needed to induce a par-tition is log 2 |T /  X  S  X  | . While this ideal number will hardly be reachable, it is still necessary to rate patterns regard-ing their contribution towards approaching it since this also minimizes redundancy between (combinations of) patterns.
Given the goals stated above and the intuition about meaningful pattern sets we have given in the preceding sec-tion, our aim is now to select a subset S  X   X  S which comes close to recovering the partition induced by S while being of low cardinality.

Since the number of patterns is rather high, the brute force approach of testing any possible subset S  X   X  S will quickly become infeasible. A possible solution to this lies in limiting the size of S  X  to a user-defined k [6]. The choice of this k is not straight-forward though, and furthermore not, as in the approach presented here, governed by the data.
Using the definitions given above we can now give a rather general heuristic algorithm to compute S  X  given S T , the order &lt; ,  X  , and some threshold t  X  [0 , 1]  X  R shown as Algorithm 1.
 Algorithm 1 The general algorithm 2: for i =2 to | S | do 4: if  X ( T , S  X  ,p i )  X  t then 6: return S  X 
The algorithm iterates over the patterns in S in the order given by &lt; . In each step the partition induced by S  X   X  is created and compared to the partition induced by S  X  .If there is no change, the pattern is rejected, thus reducing re-dundancy (subgoal 1). If there is change, the measure  X  is evaluated against the threshold and if the threshold is passed the pattern chosen.

To give some intuition about the process, consider Figure 1. The left-hand side shows a small snapshot of T . As can be clearly seen on the left-hand side, p 3  X  X  presence depends on the presence of both p 1 and p 2 , and p 4  X  X  presence on the presence of p 1 and absence of p 2 . The right-hand side shows the partition induced by the first three patterns on and shows that p 3  X  X  dependency is mirrored in the way T split into blocks.

Note that the rejection on a lack of change in the partition means that the S  X  created by this selection will induce the same partition as the whole S . Thus, the description of data characteristics that the original set allowed is maintained, satisfying subgoal 3.

Even though there might be cases where it is reasonable otherwise, we consider  X ( T ,  X  ,p i )=1 for all of our exper-iments. This means that the first pattern p 1  X  S according to &lt; is always in the reduced set S  X  . We will discuss other possibilities in the last section.

Obviously there are  X  apart from T  X  two major points influencing the result S  X  . First, there is the measure  X  which so far remains unspecified. Second, the order &lt; of the fea-tures in S may be important. We will discuss several possi-bilities for those two points in the next section.
To show the applicability of our general algorithm to the task of pattern subset selection we introduce several differ-ent instantiations. We start by describing three selection measures used. To give some more motivation for each of those instantiations, we will attempt to give some  X  X ean-ing X  to each selection measure used.

Figure 1. Partitions induced by patterns. The left table shows which patterns p i occur in the transactions t j . The right shows how the patterns split up the data set. Four binary patterns can induce at most 16 blocks. This combination of patterns and instances yields only four blocks, however. 3.1 Partition size quotient  X  Q
While rejection of patterns that don X  X  change the par-tition will effectively cut down on the number of patterns retained already, there is the possibility that adding a pattern will affect only a few blocks. While this may be acceptable in early steps of the selection process when not many patterns are used and only few blocks formed, in later steps this corresponds to only a small gain in new information. Let X  X  assume for instance that exactly one of the existing blocks is split into two sub-blocks when a new pattern is added. This means that the total number of blocks is raised by one. Depending on the number of already existing blocks, this e.g. corresponds to 33% for two blocks, but only 0.9% for 100 blocks.
 The crudest way of measuring this lies in defining the define a threshold on what we perceive to be an acceptable increase in the number of blocks and use it for additional pattern selection. The main advantage of this criterion is that it is easy to evaluate. A possible disadvantage might be that focusing solely on the number of blocks without considering which blocks are split and which instances are contained in the new sub-blocks is not enough. 3.2 Agglomerative clustering  X  C
To alleviate this, one can use an agglomerative clusterer which combines some of the new sub-blocks until the old number of blocks is reached. Let X  X  assume that as in the sec-tion before, the addition of a new pattern leads to a split of an existing block into two sub-blocks. These have a dissim-ilarity of 1 according to the Manhattan distance since they agree in all patterns except the new one. By the same argu-ment the parent block had a distance of at least 1 to the other blocks. Thus one of the sub-blocks will have a distance of 2 to all blocks except its sibling. Non-split blocks might have a smaller dissimilarity to non-sibling blocks than to siblings, depending on the effect of the new pattern.
An agglomerative clusterer will combine two blocks from the new partition into one block since then the old number of blocks is reached again. Given the effects de-scribed above, there will very likely be change that can be measured using the Rand index, affected by the new pat-tern over the entire partition, even unchanged blocks. For a bigger increase in the number of blocks this change can be expected to be even more pronounced.

The Rand index is defined as follows: assume two par-titions P , P . For each pair of instances t i ,t j , two decision variables exist -c ij which is set to 1 if the two instances end up in the same block in both P and P , 0 otherwise, and d ij =1 if the two instances are assigned to different blocks in both partitions, 0 otherwise. The Rand index is then: We define  X  C ( T , S  X  ,p i )=1  X  Rand ( T /  X   X  S , T / and set a threshold t that quantifies what we consider the minimal acceptable number of changes for a pattern to be chosen.

This technique will take longer to evaluate than the one shown before since the clustering process needs at least quadratic running time and for the Rand-index n ( n  X  1) 2 wise decisions have to be made. It does have the advantage of using information about the size and composition of the blocks and not only about their number though. 3.3 Inference of patterns  X  I
The first selection technique we showed strictly evalu-ates whether patterns can be described by a combination of others while the second one evaluates the effect of combin-ing instances that differ in only one pattern. A third op-tion is also possible in which a machine learning technique such as a rule-based learner is used to evaluate the possibil-ity of predicting the presence/absence of a pattern based on the presence of previously chosen patterns. While this will never lead to a perfect model 1 , a pattern whose presence or absence is correctly predicted on the majority of instances can be considered as not adding much information.
Given a pattern set S  X  = { p 1 ,...,p k } , a new pattern p k +1 and the database T , we identify each transaction t with its binary feature vector to induce a hypothesis h : X  X  X  0 , 1 } where X = {  X  X  X  f
S  X  ( t ) | t  X  X } and define the measure  X  I ( T , S  X  ,p 1
Note that in this case all instances are represented as fea-ture vectors including duplicates w.r.t. the feature vectors. This means that a feature having only marginal effect on large parts of the instance space will be predicted with high accuracy while a feature that e.g. splits the largest block in half will have far less accuracy, thus having a better chance of being chosen.
As we mentioned before, the second important issue in the instantiations of our general approach is the ordering relation used. When working with frequent itemsets two simple types of orderings are possible based on support and on length of the itemsets, respectively. In each case we can use two directions of ordering, either ascending or descend-ing. This leads to a total of four different orderings: support ascending ( s  X  ), support descending ( s  X  ), length ascending ( l ), and length descending ( l  X  ) evaluated in the experimen-tal section.
To evaluate the presented approach we used pattern sets from five UCI itemset mining tasks, harvested using an A
PRIORI implementation [1] with different minimum sup-port thresholds. We obtained closed pattern sets of a size as shown in Table 1.

Table 1 also reports the minimum and the maximum number of patterns needed to induce the same partition as is induced by the full set of patterns S . The low number is produced by either the s  X  or l  X  order, the large number by one of the other two.

For each of the data sets we evaluated two minimum support thresholds  X  . Due to a less than efficient imple-mentation, the subset selection for the 10% setting on the mushroom data set, which produced 16000+ closed pat-terns, didn X  X  finish in time and is not reported here. As Table 1 shows even setting  X  to quite large values 3 and restricting the result to closed patterns, removing quite some redun-dancy, the size of the pattern sets are still too large for use by a human expert. It can be observed that reducing the set its cardinality. to the patterns needed to recreate the partition gives a large reduction for the  X  X ight X  orders.

We used the three selection measures and four orders described in the preceding section. The measure  X  C ing clustering turned out to be the most most expensive in terms of computing power. The quotient measure  X  Q was rather fast, leaving the measure  X  I employing a learning algorithm in the middle. For each of the three techniques a threshold t has to be supplied which  X  although indirectly  X  determines the size of the resulting pattern set S  X  .We obtaining one reduced pattern set per threshold for each of the itemsets. The intervals between different thresholds thus become wider the tighter the thresholds are, since we work on the assumption that tightening the thresholds for relaxed values will have a larger effect.
 selection methods, with each curve corresponding to an or-der. Additionally a curve 2 | S These particular plots were derived on the voting-record set with  X  = 25% .The x -axis is scaled logarithmically to make the differences for the smallest pattern sets more visible since many methods converge there. As these plots compare the ability to recover a partition of equal informa-tiveness as the original pattern set to the size of the reduced set, points/curves closer to the upper left corner can be con-sidered better.

The first observation to be made is that the  X  X igh-support X  orderings ( s  X  and l  X  ) perform very similar to each other as do the  X  X ow-support X  orderings ( s  X  , l  X  ). Addition-ally the latter induce larger S  X  for the same size of the par-tition when compared to the former.

Second, the reduction in patterns is for most settings not linear to the decrease in blocks, meaning that our approach doesn X  X  need to sacrifice too much in information about the data set to improve comprehensibility by the user.
 Third, there are two very distinct differences between  X 
C , and  X  I and  X  Q . On the one hand, in the area where the thresholds are rather loose, leading to relatively large ,  X  I and  X  Q show a smooth curve that corresponds to the small increases in the threshold setting while  X  duces the cardinality of S  X  rapidly. It also induces a coarser (less blocks but of higher cardinality) partition doing so. On the other hand, once the threshold is raised above a certain value, large changes in the threshold have less effect on  X  than on the other two measures, resulting in a smooth curve for small sets for  X  C and more abrupt changes for  X  I and  X 
Q . Ultimately, the threshold of 0 . 4 leads to a reduction of | S  X  | to 2 , essentially the lowest reasonable cardinality, while  X  I and  X  Q produce larger sets at that threshold.

Regarding the comparison of  X  I to  X  Q , it should be noted that for the  X  X ow-support X  orderings  X  I shows a steeper descent for low thresholds which means that it is quicker in recovering the size of the partition.

The behavior for  X  Q is not surprising, since it mea-sures the ability of patterns to split existing blocks into sub-blocks. It is to be expected that raising the threshold in little steps will exclude only a few patterns compared to the set-ting before so that smooth curves are produced. In contrast, large steps will ratchet up the selectivity quite a bit, leading to larger drops in cardinality.

As explained before  X  I would e.g. reject a pattern that splits only one (or few) block(s), especially if they are small since then the pattern could be reliable inferred on the ma-jority of instances. On the other hand, patterns that would be rejected by  X  Q as not adding enough blocks might be accepted by  X  P if the  X  X ight X  blocks are split. This means that especially for relatively low thresholds less patterns are removed from consideration, thus allowing to quicker find near-optimal patterns regarding partitioning the data.
Finally, the main operational difference between  X  C and the other two measures lies (as mentioned in Section 3) in the fact that the effect of rolling back an  X  X ld X  decision af-fects acceptance of a new pattern. This means that two types of patterns will be accepted: those that split many blocks (as it should be) and those that evaluate the same for similar pattern combinations. Assuming a threshold of 0 . 01 and a pattern that splits a sin-gle block.  X   X  Q will very likely accept  X   X  I will accept if the pattern is not too uniform over the  X   X  C will accept if the pattern is not too diverse over the Uniformity over blocks is less likely than diversity. So  X  will potentially accept more patterns than  X  C .Also,  X  C will accept less patterns than  X  Q (at least if the increase in number of blocks required for acceptance by  X  Q is not large). This should explain why there is such a drop-off for small increases in the beginning. Later when large changes in the Rand are required, patterns are aided by the fact that roll-back of earlier decisions might help them in being ac-cepted. Based on these observations we perform the follow-ing comparisons:
The similarities of these two orders are not that surpris-ing, given that in pattern mining short (general) patterns usually match relatively many transactions, i.e. they have high support. This, however, does only hold to a certain degree 4 .

We are interested in how similar the S  X  induced by those two orderings are. This is evaluated for each method w.r.t. the orders in question. We use the Rand index as a simi-larity measure for any two pattern sets, which will decrease if there are a different number of blocks or instances are partitioned differently. Since a partition induced by a constructed using one order will not necessarily have a cor-responding partition of equal cardinality, we compare to the two closest partitions (one with higher, one with lower car-dinality) and consider the larger Rand index.

The partitions induced using the two orders are very sim-ilar for all three methods, especially for permissive thresh-olds reaching Rand values in excess of 0 . 95 . The similarity stays high for  X  I and  X  Q while  X  C derives rather different partitions for tight thresholds.
Obviously, the patterns selected from descending and as-cending orderings will not be the same. It is however possi-ble that patterns selected from one ordering can be inferred from the other pattern set. To evaluate this, we compare pattern sets resulting from descending vs. ascending order-ings while keeping all other variables fixed. Similarly to the Figure 2. Plots for the three measures on
Voting-record  X  =25 showing nicely the char-acteristics present in almost all data sets an-alyzed. The max in each plot indicates the maximum number of blocks that can be in-duced by the given number of patterns. inference selection step , we induce a model on T for each pattern of S  X  1 using the patterns of S  X  2 as features, and vice versa. The minimum accuracy for each pattern set is a quan-tifier of how well one pattern set can be inferred by another one.

Indeed, for S  X  inducing maximal partitions on the data set we observe very high accuracies regarding inference. When the S  X  induce partitions with fewer blocks, the in-ference accuracy decreases as well. However, S  X  obtained using a s  X  ordering can usually infer larger sets obtained us-ing a s  X  ordering. This observation suggests the following: to recover the complete partition with a compact (under-standable) S  X  , choose e.g. s  X  . To discover information not encoded in s  X  -sets, tighten the thresholds and use s  X  , since now s  X  -sets become manageable.
 Again, we also compared the partitions induced by the S  X  using the Rand index. Surprisingly,  X  C shows very sim-ilar behavior to the s  X  -l  X  comparison. The difference be-tween e.g. s  X  -and l  X  -induced partitions seems to be neg-ligible at least when compared to an s  X  -induced partition.  X  I is particularly stable in this comparison, never having a Rand value of less than 0 . 85 . This essentially mirrors the observation that these pattern sets can be inferred quite well from each other.
As we have seen, the size of reduced pattern sets is small enough that it would be possible for a human to inspect them. We have argued however that reducing the pattern set also helps machine learning algorithms that use the binary vector representation to learn a model. We are especially interested in whether the smallest sets (easiest to inspect for a human) give sensible accuracy results. To evaluate this we use C4.5 to induce models on the binary feature rep-resentation obtained by using the different S  X  and estimate their classification accuracy, via ten-fold cross-validation. We also learn models on the binary vector set constructed using S and on the original attribute-value representation.
An example for accuracies attainable with different se-lection measures and orderings is shown in Figure 3. The orderings from left to right are s  X  , s  X  , l  X  , l  X  . It can be seen that the  X  X igh support X  orderings are performing better than their respective  X  X ow support X -counterparts. While this does not hold of for all data sets it is a rather common trend.
We use a second figure (Figure 5) for comparing the accuracies of C4.5 models on four representations. These are the original attribute-value representation of the data, and binary vectors which are created using S , the best-performing S  X  selected by an instantiation of our approach, and miki s, respectively. At the top of the each bar represent-ing the best S  X  , a number denotes the size of the correspond-
Figure 3. Best cross-validated C4.5 accura-cies (all orderings for each measure) on Tic-TacToe (  X  =10 %)
Figure 4. A visualisation of S  X  for Tic-Tac-Toe  X  =5 %using s  X  ,  X  C , and a threshold of 0.4. ing pattern set. The most relevant result of this comparison is that usually neither the binary vector representation de-rived from the whole S , nor the one based on the S  X  that gives the maximal partition are best-suited for the machine learning algorithm. Instead, for all data sets, a reduced pat-tern set gives the best accuracy after cross-validation and pruning. This supports our assumption that too many fea-tures only lead to an over-fitting effect and don X  X  benefit the learner. The size of the corresponding pattern sets is so small that they should be easily interpretable by the user. An example for the tic-tac-toe data set is given in Figure 4.
The attribute-value representation still proves to be more expressible than the pattern representations for most cases, a problem that could probably be alleviated with the use of a more lenient support threshold. Furthermore, no single ordering or selection method does distinguish itself and se-lection thresholds vary between 0.03 and 0.4. Keep in mind however that we didn X  X  aim to maximize predictive accuracy and our illustrative instantiations are not meant to exhaust the entire issue. sentation, best S  X  ,and miki (2 patterns)
Finally, we also used the algorithms by Knobbe et al. to mine pattern teams on the sets of closed patterns. Due to the rather large running times of these techniques only pattern teams of size k =2 have been selected. The comparisons in this case focus on the similarity of the pattern teams to our reduced pattern sets ( Rand -index and inference) and on the effectiveness as features for classification purposes.
We compared the maximally informative k-itemsets ( miki ) obtained with algorithms by Knobbe et al. to reduced sets obtained by using  X  C with a rather strict threshold. In-terestingly the sets compared exhibit very similar behavior w.r.t. to all comparisons. First, all but one consist of two patterns. Their prediction qualities are the same, and they can both equally well infer the other set. High Rand values furthermore show that our approach induces very similar partitions. This is interesting insofar as miki s are mined us-ing a complete method maximizing mutual entropy, a mea-sure that rewards balanced partitions, while we employ a heuristic method.
There are two fields of research that our work shows similarities to: the selection of  X  X nformative X  subsets from mined patterns, and feature selection/feature construction for classification purposes.
Much work has been done on reducing the number of patterns that are returned to the user by a mining process. The approaches can roughly be differentiated into three dif-ferent categories: 5.1.1 Selecting subsets with ad-hoc properties The earliest and in a sense simplest approach to reducing sets of frequent patterns lies in selecting subsets of this set that allow for the reconstruction of the entire set. The ear-liest such technique reduces a set of frequent patterns to their (positive or negative) borders [10]. The information stored in the borders allows to quickly determine what the syntactic makeup of frequent pattern is. Support informa-tion would have to be found by rescanning the data again but borders are usually a lot smaller than the complete set. Subsets that allow for the reconstruction of both syntactic makeup and support information while at the same time de-creasing the size of the set of patterns to be considered in-clude the sets of closed, free , and non-derivable patterns [12, 2, 4].The main difference to our work lies in the fact that the underlying assumption of these solutions is that all found patterns are of interest to the user, an assumption that we do not make. Also, as can be seen in our experiments, even those property-restricted sets can be large and there is still a lot of redundant information present. 5.1.2 Summarizing frequent patterns On the other hand, to give the user a good idea of the infor-mation encoded in the patterns without focusing too much on the exact composition and support properties, it is pos-sible to summarize frequent patterns. To this end, frequent patterns (and association rules) have been clustered [9], usu-ally by defining a similarity measure involving the syntactic makeup and regions of the data covered. To the user then representatives of the found frequent pattern clusters are re-turned with the assumption that analysis of those represen-tatives will allow the user to gain information. The first step of the general technique we introduce does something similar in that for all patterns matching the exact same in-stances only one pattern will be returned. A difference here is that no patterns that encode the same information as a combination of patterns will be returned, which would prob-ably form their own cluster(s) in the existing approaches. Yet again, the working assumption of summarizing frequent patterns is that the user is interested in the information en-coded by all patterns. 5.1.3 Selecting informative patterns Finally, approaches that are most similar to our work are relatively new and attempt to select subsets of informative patterns with informativeness being defined w.r.t. some task at hand. This could for instance be lossless compression of the database using an MDL criterion [11], or removal of redundant information according to the joint entropy crite-rion [6]. The work of Siebes et al. (developed indepen-dently and a bit before our technique) follows very simi-lar ideas and intuitions about the meaning and use of pat-terns, and the na  X   X ve variant of their algorithms, given the right modifications, could be formulated as an instantiation of our general idea. A main difference is that we cannot ex-pect to find patterns that allow a lossless compression of the database, which on the other hand means that the reduced pattern sets of Siebes et al. are larger than ours. The ap-proach of Knobbe et al. is different in that their focus is very strongly on low-redundancy patterns. The upside of this that their pattern teams show a lot less redundancy than the sets we derive for loose threshold settings. The down-side is that searching the space of possible pattern teams is far larger which is why their approach needs an additional parameter, namely the size of the pattern teams (which is significantly below 10). As we have shown in the experi-mental session, we do, for strict settings of our thresholds arrive at pattern sets very similar to Knobbe X  X  pattern teams 5.2.1 Feature selection Approaches to feature selection involve for instance rele-vancy constraints in subgroup discovery [8]. We mention this approach in particular since the underlying idea is sim-ilar to ours, with the restriction to class-labeled data. Many other feature selection approaches also select relevant sub-sets of all features, with many techniques using heuristic methods. In general, measures such as an improvement of accuracy on some testing data could be used in our general algorithm. However, our approach is not limited to class-labeled data, but can easily be used unsupervised and the focus is not necessarily better classification accuracy but the removal of redundancy.

Another class of feature selection methods that is related to our technique are so-called  X  X rapper X  approaches. In these usually features are selected, handed to an evaluation algorithm and then on their inclusion decided. An example that seems very close to our work is [5] in that the wrap-per uses unsupervised learning instead of an induction algo-rithm. A main difference in such approaches lies in the fact that usually all features for inclusion are considered, mak-ing the search space again rather large. By using a heuristic approach, we can handle far more patterns than such an ap-proach could. 5.2.2 Feature construction A somewhat similar approach to this work is embodied by the K F OIL [7] system, as well as by T REE 2 [3]. The main principle of these approaches is that patterns are constructed while a classifier is built. This means that obviously every pattern used in the final classifier is dependent on the other ones and the information contained in them. Since those approaches proceed in such a way that they greedily con-struct the next best feature and the focus is again classifica-tion accuracy, there are also quite a few differences to our technique.
The focus of this work is the reduction of result sets of pattern mining operations to sets of what we called  X  X alu-able X  patterns -pattern sets with little redundancy among their members, capturing most of the characteristics of the underlying data, and being of a cardinality still accessible to humans. To achieve this task we introduced a general heuristic algorithm and showed several instantiations with selection criteria and orderings w.r.t. which the patterns are processed.

In the experimental evaluation on several UCI data sets we showed that we achieve our goal of significantly reduc-ing the sets of closed patterns mined on them. Given a well-chosen ordering (support descending or size ascend-ing), cardinality of reduced pattern sets is in most cases be-low or around 30 , a size that should still be interpretable by humans.

Additionally, while tightening the thresholds has the ef-fect that the cardinality of reduced sets decreases, the de-crease in number of blocks is less or equal to this. This means that the number of patterns for consideration can be reduced without losing too much information. Furthermore, reducing the number of patterns does not necessarily lead to a decrease in classification accuracy when compared to larger sets or the original representation. Quite contrary, on several occasions larger pattern sets do  X  X onfuse X  the ma-chine learner, leading to overfitting.

To summarize, we manage to achieve our three subgoals with different instantiations of our general algorithm. All three selection measures have their merits, with  X  C ing computationally most expensive but also showing very good reduction effects while the  X  I and  X  Q recover more of the original partitioning at the expense of less reduction in set cardinality.
 There is of course ample opportunity for future work. The flexibility of our approach allows for the definition of more exotic orderings -for instance could a human expert set a pattern (or a number of patterns) as seed(s) and de-fine a dynamic ordering that changes given the current pat-tern set (e.g. based on city block distance to the current blocks). Additionally, other measures are also possible such as the MDL criterion used in Siebes et al.  X  X  work or accu-racy based selection measures that mirror  X  X lassical X  feature selection.

So far our technique remains a post-processing step. And while the generality of the algorithm and the different mea-sures it allows make it hard to incorporate the entire selec-tion in a mining operation itself, at least the partition cardi-nality might be turned into a constraint that can be pushed into the mining step itself.
 Acknowledgments We thank Arno Knobbe for interest-ing discussions and providing the algorithm used for finding miki s as well as Luc De Raedt and Arno Siebes for helpful comments and discussion on the topic. We are also grate-ful for the help of Siegfried Nijssen and Ulrich R  X  uckert and the anonymous reviews which helped us improve this work. The authors were supported by the IQ project (European Union Project IST-FET FP6-516169) and by a doctoral re-search grant by the KU Leuven research fund.

