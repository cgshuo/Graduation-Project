 Recovering a large matrix from a small subset of its en-tries is a challenging problem arising in many real world applications, such as recommender system and image in-painting. These problems can be formulated as a general matrix completion problem. The Singular Value Threshold-ing (SVT) algorithm is a simple and efficient first-order ma-trix completion method to recover the missing values when the original data matrix is of low rank. SVT has been ap-plied successfully in many applications. However, SVT is computationally expensive when the size of the data matrix is large, which significantly limits its applicability. In this paper, we propose an Accelerated Singular Value Thresh-olding (ASVT) algorithm which improves the convergence rate from O ( 1 N )forSVTto O ( 1 N 2 ), where N is the num-ber of iterations during optimization. Specifically, the dual problem of the nuclear norm minimization problem is de-rived and an adaptive line search scheme is introduced to solve this dual problem. Consequently, the optimal solution of the primary problem can be readily obtained from that of the dual problem. We have conducted a series of experi-ments on a synthetic dataset, a distance matrix dataset and a large movie rating dataset. The experimental results have demonstrated the efficiency and effectiveness of the proposed algorithm.
 H.2.8 [ Database Management ]: Database Applications-Data Mining Algorithms Matrix Completion, Singular Value Thresholding, Nesterov X  X  Method, Adaptive Line Search Scheme
Estimating missing values from very limited information of an unknown matrix has received considerable attention recently. This problem occurs in both theoretical studies [4, 5, 7], and real world applications such as recommender sys-tem [13, 14, 22] and image/video analysis [17, 11]. Since the completion of arbitrary matrices is an ill-posed problem, it is usually assumed that the underlying matrix comes from a restricted class. One of the most natural assumption is that the matrix has a low-rank or approximately low-rank structure. Specifically, given the incomplete data matrix M  X  R m  X  n , the matrix completion problem can be formu-lated as follows: where  X  is the set of locations corresponding to the observed entries.

However, the rank minimization problem (1) is NP-hard in general due to the non-convexity and discontinuous na-ture of the rank function. The existing algorithms can not directly solve the rank minimization problem efficiently. It is known that the nuclear norm is the tightest convex lower bound of the rank function of matrices on the unit ball {
X  X  R m  X  n | X 2  X  1 } [20], where the spectral norm,  X  2 of a matrix is equal to the largest singular value of the ma-trix. Therefore, a widely used approach is to apply the nu-clear norm  X   X  (i.e., the summation of all the singular values) as a convex surrogate of the non-convex matrix rank function. Thus, the rank minimization problem can be ap-proximated by the nuclear norm minimization problem as its convex relaxation: Recent theoretical breakthrough in matrix completion [4, 5] shows that, under some general constraints, the solution of nuclear norm minimization problems is unique and, with a high probability, is equal to the solution of rank mini-mization problems if the number of observed entries is large enough.

Many algorithms have been proposed to solve nuclear norm minimization problems (2). Fazel [8] firstly shows that prob-lem (2) can be expressed as a Semi-Definite Programming (SDP) problem, which can be solved by conventional SDP solvers such as SDPT3 and SeDuMi [28, 23]. However, such solvers are usually based on interior point methods, and do not scale to large matrices. This limits the usage of the matrix completion technique in real world applications. Re-cently, to solve the rank minimization problem for large scale matrices, Toh et al. apply an accelerated proximal gradi-ent optimization technique (NNLS) [27] for solving nuclear regularized least squares problems. It has been shown theo-retically that NNLS can terminate within O ( 1  X  ) iterations with an -optimal solution. Keshavan et al. [12] consider the matrix completion problem by formulating it in a ma-trix factorization view. Their theoretical analysis shows that one could reconstruct a low-rank matrix by observing a set of entries of size at most a polylogarithmic factor larger than the intrinsic dimension of the variety of rank r matrices. The Singular Value Thresholding algorithm (SVT) [3] is a sim-ple and efficient algorithm for nuclear norm minimization problems proposed by Cai et al., which has been shown to achieve superior performance in practice. However, as a spe-cial case of gradient method, SVT has a global convergence rate of O ( 1 N ), where N is the number of iterations during optimization. This is too slow especially when dealing with large scale datasets.

In this paper, we propose a novel matrix completion algo-rithm called Accelerated Singular Value Thresholding (ASVT) for speeding up the standard SVT algorithm. Our basic idea is to obtain the solution of the nuclear norm minimization problem in SVT by solving its dual problem whose objec-tive function can be shown to be continuously differentiable with Lipschitz continuous gradient. Specifically, we exploit the relationship between the optimal solution of the primal problem and that of its dual problem, based on which, the optimal solution of the primary problem can be readily ob-tained from the optimal solution of the dual problem. We show that the dual problem can be efficiently solved by us-ing an adaptive line search algorithm with a convergence rate of O ( 1 N 2 ). Moreover, compared with the standard SVT algorithm, our approach can tune the step size adaptively for each iteration. This can further improve the efficiency of the proposed algorithm.

The rest of the paper is organized as follows. We pro-vide a brief review of the standard SVT algorithm in Sec-tion 2. In Section 3, we detail our proposed approach and provide some theoretical analysis. We propose an adaptive line search algorithm to solve the optimization problem in Section 4. Experimental results on both synthetic and real world datasets are presented in Section 5. Finally, we pro-vide some concluding remarks and suggestions for future work in Section 6.
 Notions: Let X =( x 1 ,  X  X  X  , x n )bean m  X  n matrix,  X   X  X  1 , ..., m } X { 1 , ..., n } denote the indices of the observed entries of X ,andlet X  c denote the indices of the missing entries. The Frobenius norm of X is defined as X 2 F =
 X  ( i,j ) X 2 ij .Let P  X  be the orthogonal projection operator onto the span of matrices vanishing outside of  X  so that the ( i, j )-th component of P  X  ( X )isequalto X ij when ( i, j )  X  and zero otherwise. Let X = U  X  V T be the singular value decomposition for X ,where X = diag (  X  i ) , 1  X  i  X  min { m, n } and  X  i is the i -th largest singular value of X . The  X  X hrinkage X  operator D  X  ( X ) is defined as [3]: where  X   X  = diag (max {  X  i  X   X , 0 } ), 1  X  i  X  min { m, n schitz gradient [19]. A continuous differentiable function f ( Y ) belongs to S 1 , 1  X ,L ( R m  X  n )forsome L  X   X   X  X,Y  X  R m  X  n we have both of the following:
The Singular Value Thresholding (SVT) algorithm solves the following problem: Cai et al. [3] give a theoretical analysis that when  X   X  X  X  the optimal solution of problem (5) converges to that of problem (2).
 With a given  X &gt; 0 and starting with Y 0  X  R m  X  n ,the SVT algorithm operates as follows until a stopping criterion is reached, where {  X  k } is a positive step size sequence. It has been shown that, when the step sequence obeys 0 &lt; inf  X  k  X  sup  X  k &lt; 2, the sequence obtained via (6) exactly converges to the unique solution of the problem in (5).

Actually the iteration scheme (6) is the linearized Breg-man iteration, which is a special instance of Uzawa X  X  algo-rithm [3]. Furthermore, by defining the Lagrangian function of problem (5) as where Y is the Lagrangian dual variable, we can derive its dual function as Cai et al. show that SVT indeed optimizes the dual function f ( Y ) via the gradient ascent method.

The SVT algorithm is shown to be an efficient algorithm for matrix completion, especially for large low rank matri-ces. However, SVT has a global convergence rate of O ( 1 N which is still too slow for real world applications such as recommender systems.
In this section we first examine the properties of the dual function f ( Y ). We then analyze the relationship between the optimal solution of the problem (5) and that of its dual problem. Based on these results, we show how to achieve the optimal solution of the problem (5) from its dual optimum directly.

As the nuclear norm  X   X  is not differentiable, it is difficult to optimize the dual function f ( Y ) directly. However, by using the Moreau-Yosida regularization technique [10], we can obtain some interesting and useful properties of the dual function f ( Y ).

In the SVT method, the shrinkage operator plays a critical role in the whole iteration scheme. We have the following result about the nuclear norm minimization problem (5). Theorem 1. [3] For each  X   X  0 and Y  X  R m  X  n , we have
Theorem 1 tells us that the shrinkage operator is the proxi-mal point mapping associated with the nuclear norm. Based on the properties of Moreau-Yosida regularization (see the Theorem 4.1.4 of [10]), we obtain the following results: Lemma 2. For any X,Y  X  R m  X  n , we have It follows that D  X  ( Y ) is globally Lipschitz continuous with modulus 1.

The main result of this section is summarized in the fol-lowing theorem:
Theorem 3.  X   X   X  0 , the dual function f ( Y ) in (8) is continuously differentiable with Lipschitz continuous gradi-entatmost 1 . Furthermore, when the dual optimal Y  X  of the problem (5) is obtained, the primal optimal X  X  of the problem (5) is given by:
Proof. where the first part of (19), i.e. g ( Y ), is the Moreau-Yosida Regularization of the nuclear norm  X   X  [10]. Using the well known properties of Moreau-Yosida Regularization [10] and Theorem 1, we get the following results Then the gradient of f ( Y ) can be obtained as follows: f ( Y )= g ( Y )+ P  X  ( M )  X  X   X  ( Y ) It follows that for any Y 1 ,Y 2  X  R m  X  n ,wehave where the second inequality follows from Lemma 2. There-fore, f ( Y ) is continuously differentiable with Lipschitz con-tinuous gradient at most 1.

When the dual optimal Y  X  is obtained, by using the result of (19), we can get where the third equality follows from Theorem 1.
Since f ( Y ) is the dual function of the objective function (5), f ( Y ) is concave. Define which is convex. Thus, the following holds for any Y 1 ,Y From (20) and (21), it is easy to see h ( Y ) belongs to the Furthermore, in view of the relationship between the primal and dual optimal solutions of problem (5), we can solve prob-lem (5) by firstly minimizing the objective function h ( Y ), i.e.,
The key step in our proposed algorithm is to solve the dual problem (26). In this section, we develop an efficient optimization algorithm to solve this problem. Because the objective function h ( Y ) is continuously differentiable with Lipschitz continuous gradient 1, in the following we propose to solve the smooth convex optimization problem (26) using the Nesterov X  X  method.

It has been shown that Nesterov X  X  method is a very power-ful optimization technique for class S 1 , 1  X ,L ( R m  X  n ever, how to choose the step size in each iteration is a criti-cal issue in the Nesterov X  X  method. To overcome this prob-lem, we propose an Accelerated Singular Value Threshold-ing (ASVT) method with an adaptive line search scheme to solve problem (26).

The Nesterov X  X  method attempts to find the optimal solu-tion of (26) by utilizing two sequence { Y k } and { S k } {
Y k } is the sequence of approximate solutions, and { S k } the sequence of searching points. The searching point S k the affine combination of Y k and Y k  X  1 as where  X  k is a tuning parameter. The approximate solution Y k +1 can be computed as a gradient step of S k as where 1 /L k is the step size. Starting from an initial point Y , we compute S k and Y k +1 according to (27) and (28), and arrive at the optimal solution Y  X  .

In the Nesterov X  X  method,  X  k and L k are two key param-eters. When they are set properly, the sequence { Y k } can converge to the optimal Y  X  at a certain convergence rate. The Nesterov X  X  constant scheme [19] and Nemirovski X  X  line search scheme [18] usually need to set  X  k and L k .How-ever, the Nesterov X  X  constant scheme assumes L k and  X  k to be constant, while in Nemirovski X  X  line search scheme, L k is required to monotonically increase, L k  X  L k +1 and  X  independent on L k [18], resulting in slow convergence.
In the following, we assume that  X   X  is the lower bound of  X  , and  X   X  is known in advance. This assumption is reasonable, since 0 is always a lower-bound for  X  in (4). With this as-sumption, we adopt an adaptive line search scheme proposed by Liu et al. [16] for the Nesterov X  X  method. This adaptive line search scheme is built upon the estimate sequence [19], which is defined as follows:
Definition 1. [19] A pair of sequences {  X  k ( Y ) } and { 0 } is called an estimate sequence of function h ( Y )ifthe following two conditions hold: 1. lim 2.  X  k ( Y )  X  (1  X   X  k ) h ( Y )+  X  k  X  0 ( Y ) ,  X  Y  X  R m  X  n The following theorem provides a systematic way for con-structing the estimate sequence:
Theorem 4. [19] Let us assume that: 1. h ( Y ) is smooth and strongly convex with Lipschitz gra-2.  X  0 ( Y ) is an arbitrary function on R m  X  n . 3. S k is an arbitrary searching sequence on R m  X  n . 4.  X  k satisfies:  X  k  X  (0 , 1) and  X  k =0  X  k =  X  . 5.  X  0 =1 .
 Then {  X  k ( Y ) , X  k } can be defined as follows: If we choose a simple quadratic function for  X  0 ( Y )as Algorithm 1 The Adaptive Line Search Scheme 1: Input:  X   X ,  X   X  1 =0 . 5 ,Y  X  1 = Y 0 ,L  X  1 = L 0 , X  2: Output: Y N 3: for k =0 , 1 , 2 ,  X  X  X  ,N do 4: while 1 do 5: compute  X  k  X  (0 , 1) as the root of L k  X  2 k =(1  X  6: compute S k = Y k +  X  k ( Y k  X  Y k  X  1 ) 9: goto Step 14 10: else 11: L k =2 L k 12: end if 13: end while 15: set  X  k +1 =(1  X   X  k )  X  k 16: end for then we can specify the estimation sequence defined in The-orem 4 as [19]: where the sequences  X  k ,V k and  X   X  k satisfy: V  X  k +1 =(1  X   X  k )  X  k +  X  k  X   X , (34)  X  The estimate sequence defined in Definition 1 has the fol-lowing important property:
Theorem 5. [19] Let {  X  k ( Y ) } and {  X  k  X  0 } be an esti-mate sequence. For any sequence { Y k } ,if we have Based on Theorem 5, we propose an Accelerated Singular Value Thresholding (ASVT) algorithm based on Nesterov X  X  method with an adaptive line search scheme. The complete procedure is summarized in Algorithm 1. In this scheme, the proper adaptive step size 1 /L k is designed to look for the approximate solution sequence { Y k } satisfying the condition (36). Then according to Theorem 5, the convergence rate of the solution sequence can be analyzed using the sequence {  X 
In Algorithm 1, the while loop from Step 4 to Step 13 is designed to choose a proper step size to satisfy step 8. As the Lipschitz gradient of h ( Y ) is 1, just like the Nemirovski X  X  line search scheme, L k is upper-bounded by 2, since the step 8 always holds when L k  X  1 [18]. Recall the fact that the iteration of L k in Nemirovski X  X  line search scheme is Iteration 20 40 60 80 Table 1: Relative error comparison between SVT and ASVT on solving the synthetic low rank ma-trix problem ( m =1 , 000 ,n = 500 , r =15 , p =0 . 7 and  X  =2 accelerate the convergence by 2-5 orders of magni-tude. required to monotonically increase, in step 14 we adopt a more flexible iteration scheme of L k [16] as where the parameter  X  is computed as due to the condition in Step 8. When  X  is too large, L k +1 becoming too small, which may slow down the convergence rate. Our experimental results show this particular choice of h (  X  ) works well.

Although the step size 1 L k does not monotonically decrease any more in our adaptive line search scheme, the proposed line search scheme preserves the convergence property, as summarized in the following theorem:
Theorem 6. [16] For Algorithm 1, we have  X  N  X  min and
Recall that h ( Y ) is continuously differentiable with Lips-chitz continuous gradient 1 and L k is upper bounded by 2. When we set r 0 larger than 2, ASVT can get a global con-vergence rate O ( 1 N 2 ) from Theorem 6, which is significantly better than the original SVT algorithm.
In this section, we evaluate the performance of ASVT in comparison with SVT on both synthetic and real world datasets.
We generate matrices M  X  R m  X  n of rank r by sampling two matrices of M L  X  R m  X  r and M R  X  R r  X  n ,eachhaving i.i.d. Gaussian entries, and setting M = M L M R . Suppose M
 X  is the observed part of M , and the set of observed in-dices  X  is sampled uniformly at random. Let p be the ratio between the observed entries and all m  X  n entries. Then different algorithms will be used to recover the missing en-tries from the partially observed information by solving the optimization problem in (5) with a given parameter  X  .As suggested in [3],  X  can be set to t Figure 1: Convergence rate of SVT and ASVT on synthetic data ( m =1 , 000 ,n = 500 , r =15 , p =0 . 7 and  X  =2 We evaluate the quality of the computed solution X of an algorithm by the relative reconstruction error defined by: which is a very commonly used criterion to evaluate the performance of a matrix completion algorithm [27]. For the parameter setting of SVT, we choose the constant step size as  X  =1 . 2 /p which is recommended in [3]. For ASVT we set the initial L and  X   X  as L = p/ 1 . 2 ,  X   X  =0 . 1.
Firstly we conduct a simulation study with the following parameters: m =1 , 000 ,n = 500, r = 15, p =0 . 7and  X  =2 recover the other 30% entries by running SVT and ASVT separately. Fig. 1 illustrates the fast convergence rate of ASVT compared with SVT. Table 1 reports the relative re-construction error of different methods after 20, 40, 60 and 80 iterations. We can observe that the convergence rate of ASVT is at least two orders of magnitude faster than that of SVT, which is consistent with our analysis.

To check how the performance of both algorithms changes with different settings, we test SVT and ASVT on the fol-lowing different low rank matrix completion problems: Figure 2: The largest 20 singular values of the dis-tance matrix data. Figure 3: The normalized cumulative sums of the singular values of the distance matrix data.
Table 2 reports the comparative results of randomly gen-erated matrix completion problems. We can observe that ASVT converges much faster than SVT in all cases. ASVT X  X  performance after 50 iterations surpasses SVT by several or-ders of magnitude. In average ASVT only needs 60% of the iterations required by SVT to achieve a given relative recon-struction error.
In this experiment, we compare the performance of SVT and ASVT on a real world distance matrix dataset [2]. We consider the problem of recovering the real world distance Figure 4: Convergence rates of SVT and ASVT on the real world distance matrix. matrix M of 312 cities in the United States and Canada. The element M ij of M measures the geodesic distance be-tween the city i and the city j . Suppose some locations of M are unknown, we can recover the missing information by using the known part of the pairwise distance matrix. As mentioned in [3], the squared Euclidean distance matrix is a low rank matrix. With geodesic distances, the distance ma-trix M can also be well approximated by low rank matrices (the top several singular values of M are actually dominant as shown in Fig. 2 and Fig. 3). It is thus reasonable to recover the missing information using matrix completion al-gorithms.

Suppose the recovered matrix after k iterations is X k .The rank of X k has been shown empirically to be nondecreasing in the SVT algorithm [3]; we observe that ASVT has the same property. Since the complete data matrix is known, as suggested in [3], we also use the relative reconstruction error X k  X  M F / M F to measure the effectiveness of SVT and ASVT after the k -th iteration.

The reconstruction error of SVT and ASVT after each iteration is shown in Fig. 4. To achieve the same level of ac-curacy (measured by the reconstruction error), ASVT needs much fewer iterations than SVT. Fig. 5 shows the change of the rank with the iteration of SVT and ASVT. To reach a certain rank, ASVT needs much fewer iterations than SVT too. The iterations and computational times needed to reach the i -th rank are shown in Table 3. Moreover, we can see that the computational costs in one iteration of SVT and ASVT are similar (time/number of iterations). Note that, the last column of Table 3 ( M  X  M r F / M F ) is the min-imum relative error that we can achieve, where M r is the best rank r approximation of M by computing the SVD of M .
We now focus on the application of the proposed algo-rithm on the recommendation problem. We show the re-sults of SVT and ASVT on the MovieLens data which is a widely used recommendation dataset [27, 29] and can be downloaded from [9]. The dataset is collected by the Grou-mn 121 61 8.18e-05 1.22e-07 mn 99 47 1.97e-05 3.70e-09 mn 77 30 1.96e-06 1.94e-13 mn 56 30 4.11e-08 2.81e-14 mn 94 62 1.04e-05 1.40e-07 mn 77 43 1.91e-06 8.85e-10 mn 64 30 2.36e-07 2.41e-14 mn 49 29 7.23e-09 3.56e-14 mn 44 29 9.29e-10 3.67e-14 mn 46 27 2.43e-09 1.61e-14 mn 53 28 1.85e-08 2.37e-14 mn 61 30 1.43e-07 3.72e-14 mn 95 44 1.43e-05 1.19e-09 mn 91 41 9.64e-06 3.42e-10 mn 66 31 3.64e-07 6.05e-14 mn 49 28 6.96e-09 2.04e-14 mn 46 27 2.11e-09 2.26e-14 mn 38 26 7.28e-11 2.68e-14 convergence by 2-7 orders of magnitude.
 minimum relative error that we can achieve, where M r is the best rank r approximation of M by computing the SVD of M . pLens Research Project at the University of Minnesota and contains 100,000 rating information from 943 users on 1,682 movies. The data has been cleaned up such that users who had less than 20 ratings were removed. So each user in the data has rated at least 20 movies. The ratings are from 1 (strongly unsatisfactory) to 5 (strongly satisfactory). In the recommendation situation, the data matrix is highly sparse (only about 6.3% entries are known). In order to test SVT and ASVT, we split the ratings into training and test sets. In our experiment, 80,000 ratings (80%) are randomly chosen to be the training set and the test set contains the remaining 20,000 ratings (20%).

Evaluation of recommendation algorithms has long been divided between accuracy metrics (e.g. precision/recall) and error metrics (notably, RMSE and MAE). The mathemati-cal convenience and fitness with formal optimization meth-ods have made error matrics like RMSE more popular [6]. Suppose all the existing rating locations of the test set are denoted as  X . Like many recent research papers [29, 1], we also take the Root Mean Squared Error (RMSE) to measure the effectiveness of an algorithm on solving recommendation problems: where X ij is the recovered value and M ij is the ground truth, and # |  X  | is exactly the number of test ratings, i.e. 20,000. So the setting here is m = 943 ,n =1,682 ,p = 80,000 / ( mn )  X  0 . 05 , and # |  X  | =20,000. We empirically choose  X  =10 4 . Both SVT and ASVT automatically re-cover the rating matrix by estimating the rank from a small to a large value.

As expected, the RMSE value decreases very fast with the iterations of both SVT and ASVT at first. And when the estimated rank is large enough, RMSE value becomes stable. The comparison result in terms of RMSE is shown in Fig. 6. It is clear that ASVT has a much faster convergence rate than SVT on the MovieLens dataset. Fig. 7 shows the change of the rank after each iteration of SVT and ASVT. Similar to Fig. 5, less time is needed for ASVT to reach a certain rank. More detailed numerical results are listed in Table 4. For the MovieLens dataset, it costs ASVT only about half of the time to get a similar RMSE value compared with SVT. Figure 5: Rank vs. number of iterations of SVT and ASVT on the real world distance matrix. Figure 6: Convergence rates of SVT and ASVT on MovieLens dataset. In this paper, we present an Accelerated Singular Value Thresholding method (ASVT) for estimating missing values for large scale matrix completion problems. The original SVT method solves the problem (5) with a global conver-gence rate O ( 1 N ). We show how to speed up the original SVT algorithm using the Nesterov X  X  method, which is an optimal first-order black-box method for the smooth convex optimization with a global convergence rate O ( 1 N 2 ). To fur-ther improve the efficiency, we adopt an adaptive line search scheme to tune the step size adaptively and in the meantime preserve the optimal convergence rate. We have conducted a series of experiments on synthetic and real world datasets. Experimental results show that ASVT is more efficient than the original SVT algorithm.

Recovering the missing values from limited information has been well studied for matrices. However, there is not much work on tensors, which are higher dimensional exten-Figure 7: Rank vs. number of iterations of SVT and ASVT on MovieLens dataset. sions of matrices. In many fields such as computer vision and biomedical signal progressing, it is more natural to rep-resent the data as a tensor. Based on the recent development of tensor techniques [26, 25, 24, 17, 21, 15], it is promising to extend our work from matrix completion to tensor com-pletion. This work was supported by the National Natural Science Foundation of China (Grant No: 61125203). [1] D. Agarwal, B.-C. Chen, and B. Long. Localized [2] J. Burkardt. Cities  X  city distance datasets. [3] J.F.Cai,E.J.Cand` es, and Z. Shen. A singular value [4] E. J. Cand` es and B. Recht. Exact matrix completion [5] E. J. Cand` es and T. Tao. The power of convex [6] P. Cremonesi, Y. Koren, and R. Turrin. Performance [7] A. Eriksson and A. van den Hengel. Efficient half of the time to get a similar RMSE value compared with SVT. [8] M. Fazel. Matrix rank minimization with applications. [9] GroupLens. Movielens. [10] J.-B. Hiriart-Urruty and C. Lemar  X  echal. Convex [11] H. Ji, C. Liu, Z. Shen, and Y. Xu. Robust video [12] R. H. Keshavan, A. Montanari, and S. Oh. Matrix [13] Y. Koren. Factorization meets the neighborhood: a [14] Y. Koren. Collaborative filtering with temporal [15] N. Li and B. Li. Tensor completion for on-board [16] J. Liu, J. Chen, and J. Ye. Large-scale sparse logistic [17] J. Liu, P. Musialski, P. Wonka, and J. Ye. Tensor [18] A. Nemirovski. Efficient Methods in Convex [19] Y. Nesterov. Introductory Lectures on Convex [20] B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed [21] B. R. Silvia Gandy and I. Yamada. Tensor completion [22] H. Steck. Training and testing of recommender [23] J. F. Sturm. Using sedumi 1.02, a matlab toolbox for [24] D. Tao, X. Li, X. Wu, W. Hu, and S. J. Maybank. [25] D. Tao, X. Li, X. Wu, and S. J. Maybank. General [26] D. Tao, M. Song, X. Li, J. Shen, J. Sun, X. Wu, [27] K.-C. Toh and S. Yun. An accelerated proximal [28] R. H. Tutuncu, K. C. Toh, and M. J. Todd. Sdpt3  X  a [29] Y. Zhang and J. Koren. Efficient bayesian hierarchical
