 The detection and improvement of low-quality information is a key concern in Web applications that are based on user-generated content; a popular example is the online encyclopedia Wikipe-dia. Existing research on quality assessment of user-generated content deals with the classification as to whether the content is high-quality or low-quality. This paper goes one step further: it targets the prediction of quality flaws , this way providing specific indications in which respects low-quality content needs improve-ment. The prediction is based on user-defined cleanup tags, which are commonly used in many Web applications to tag content that has some shortcomings. We apply this approach to the English Wikipedia, which is the largest and most popular user-generated knowledge source on the Web. We present an automatic mining ap-proach to identify the existing cleanup tags, which provides us with a training corpus of labeled Wikipedia articles. We argue that com-mon binary or multiclass classification approaches are ineffective for the prediction of quality flaws and hence cast quality flaw pre-diction as a one-class classification problem . We develop a quality flaw model and employ a dedicated machine learning approach to predict Wikipedia X  X  most important quality flaws. Since in the Wi-kipedia setting the acquisition of significant test data is intricate, we analyze the effects of a biased sample selection. In this regard we illustrate the classifier effectiveness as a function of the flaw distribution in order to cope with the unknown (real-world) flaw-specific class imbalances. The flaw prediction performance is eval-uated with 10 000 Wikipedia articles that have been tagged with the ten most frequent quality flaws: provided test data with little noise, four flaws can be detected with a precision close to 1.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; H.5.3 [ Information Interfaces and Presentation ]: Group and Organization Interfaces X  Evaluation/methodology User-generated Content Analysis, Information Quality, Wikipedia, Quality Flaw Prediction, One-class Classification
Web applications that are based on user-generated content come under criticism for containing low-quality information. This ap-plies also to Wikipedia, the largest and most popular user-generated knowledge source on the Web; Wikipedia contains articles from more than 280 languages, the English version contains more than 3.9 million articles, and wikipedia.org ranks among the top ten most visited Web sites. The community of Wikipedia authors is heterogeneous, including people with different levels of education, age, culture, language skills, and expertise. In contrast to printed encyclopedias, the contributions to Wikipedia are not reviewed by experts before publication. These factors make clear that the most important, but probably the most difficult challenge for Wikipedia pertains to the quality of its articles. Wikipedia founder Jimmy Wales announced in a recent interview:  X  X ur goal is to make Wi-kipedia as high-quality as possible. [Encyclop X dia] Britannica or better quality is the goal. X  [36] However, the size and the dynamic nature of Wikipedia render a comprehensive manual quality assur-ance infeasible. This is underlined by the fact that only a small number of articles are labeled as featured , i.e., are considered as well-written, comprehensive, well-researched, neutral, and stable.
The existing research on automatic quality assessment of Wiki-pedia articles targets the classification task  X  X s an article featured or not? X  Although the developed approaches perform nearly per-fect in distinguishing featured articles from non-featured ones, they provide virtually no support for quality assurance activities. The classification is based on meta features that correlate with featured articles in general, but cannot (and were not intended to) provide a rationale in which respects an article violates Wikipedia X  X  featured article criteria. This goal, however, is addressed in this paper where we try to predict the flaws of an article that need to be fixed to im-prove its quality. We exploit the fact that Wikipedia users who en-counter some flaw (but who are either not willing or who don X  X  have the knowledge to fix it) can tag the article with a so-called cleanup tag. The existing cleanup tags give us the set of quality flaws that have been identified so far by Wikipedia users. The tagged articles are used as a source of human-labeled data that is exploited by a machine learning approach to predict flaws of untagged articles.
Our contributions relate to the fields of user-generated content analyses, data mining, and machine learning and focus on the fol-lowing research questions: What cleanup tags exist in Wikipedia? Cleanup tags are realized by templates, which are special Wikipedia pages that can be in-cluded into other pages. The identification of templates that define
At the time of this writing, less then 0.1% of the English Wikipe-dia articles are tagged with the  X  X eatured X  label. cleanup tags is a non-trivial task since there is no dedicated quali-fier for cleanup tags and Wikipedia contains nearly 320 000 differ-ent templates. We hence implement an automated mining approach to extract the existing cleanup tags from Wikipedia. Altogether 388 cleanup tags are identified.
 How to model quality flaws? A large body of features X  X llegedly predicting article quality X  X as been proposed in previous work on automatic quality assessment in Wikipedia. We have compiled a comprehensive breakdown, implement more than 100 features from previous work, and introduce 13 new features that directly tar-get particular quality flaws. Moreover, we distinguish two model-ing paradigms: intensional and extensional. The former allows for an efficient and precise prediction of certain flaws based on rules, the latter resorts to the realm of machine learning.
 How to predict quality flaws? To the best of our knowledge, an algorithmic prediction of quality flaws in Wikipedia has not been operationalized before. We suggest to cast quality flaw prediction in Wikipedia as a one-class problem: Given a sample of articles that have been tagged with flaw f , decide whether or not an arti-cle suffers from f . We adapt a dedicated one-class classification machine learning approach to tackle this problem.
 How to assess classifier effectiveness? A representative sample of Wikipedia articles that have been tagged to not contain a particu-lar quality flaw is not available. To assess the effects of a biased sample selection our analysis is performed on both an optimistic test set, using featured articles as outliers, as well as a pessimistic test set, using random untagged articles as outliers. Given the op-timistic test set and a balanced class distribution, four flaws can be detected with a precision close to 1. Since the true flaw-specific class imbalances in Wikipedia are unknown, we illustrate the clas-sifiers X  precision values as functions of the class size ratio.
The paper is organized as follows. Section 2 discusses related work on quality assessment in Wikipedia. Section 3 describes our cleanup tag mining approach. Section 4 presents the quality flaw model. Section 5 gives a formal problem definition and describes the employed one-class classification approach. Section 6 presents the evaluation and discusses the results. Finally Section 7 con-cludes this paper and gives an outlook on future work.
From its debut in 2001 till this day Wikipedia is subject of on-going research in different academic disciplines. 2 This section sur-veys the research related to information quality, whereas the focus is on automatic quality assessment. We start with a discussion of the general concept of information quality.

Information quality is a multi-dimensional concept and com-bines criteria such as accuracy, reliability, and relevance. A good deal of the existing research focuses on the mapping between cri-teria sets and classification schemes [20], for which Madnick et al. [24] give a comprehensive overview. A widely accepted inter-pretation of information quality is the  X  X itness for use in a practical application X  [33]: the assessment of information quality requires the consideration of context and use case. In Wikipedia the context is well-defined, namely by the encyclopedic genre. It forms the ground for Wikipedia X  X  information quality ideal, which has been formalized X  X etter: made communicable and quantifiable X  X ithin the so-called featured article criteria 3 . That the relation between in-
Academic studies of Wikipedia: http://en.wikipedia.org/ wiki/Wikipedia:Wikipedia_in_academic_studies . Featured article criteria: http://en.wikipedia.org/wiki/ Wikipedia:Featured_article_criteria . formation quality and organizational outcome can be measured is reported by Slone [28]. It stands also to reason that incorporating information quality metrics into information retrieval approaches can significantly improve the search effectiveness of Web search environments [7, 26, 38, 39].

The machine-based assessment of information quality is becom-ing a topic of enormous interest. This fact is rooted, among oth-ers, in the increasing popularity of user-generated Web content [6] and the (unavoidable) divergence of the delivered content X  X  quality. Most of the prior research on automatic quality assessment deals with the identification of high-quality content, see for instance [2]. The relevant literature mentions a variety of approaches to auto-matically assess quality in Wikipedia. These approaches differ in their document model, i.e., the feature number, the feature com-plexity, and the rationale to quantify the quality of an article. We have compiled a comprehensive overview of the proposed article features, organized along the four dimensions content, structure, network, and edit history; see Table 3 in Appendix A.

Lih [21] models quality by the number of edits and the number of unique editors; the higher these values are the higher shall be an ar-ticle X  X  quality. However, an analysis whether the proposed metrics correlate with high-quality articles is missing. Stvilia et al. [29] use exploratory factor analysis to group 19 article features into 7 qual-ity metrics. By classifying 236 featured articles and 834 random articles under these metrics they achieve an F-measure of 0.91 for the featured set and 0.975 for the random set. Hu et al. [19] model the quality of an article via the mutual dependency between article quality and author authority. They perform several quality rank-ing experiments and show that their model is superior to a baseline model that relies on word counts. Wilkinson and Huberman [34] show that featured articles can be distinguished from non-featured articles by the number of edits and distinct editors. They also find that featured articles are characterized by a higher degree of coop-eration, which is quantified by the number of revisions of the par-ticular Wikipedia discussion pages. Blumenstock [8] shows that still a single word count feature can compete with sophisticated features when classifying 1 554 featured articles and 9 513 random articles. Dalip et al. [13] classify the articles along six abstract qual-ity schemes. Their comparison of different feature sets shows that textual features perform best. Lipka and Stein [22] employ charac-ter trigrams, originally applied for writing style analysis, to classify a balanced set of featured and non-featured articles. They achieve an F-measure value of 0.964 for featured articles. They also show that character trigrams are superior to part of speech trigrams, word counts, and bag of words models.

Although the mentioned approaches differ in their robustness and complexity, they perform nearly perfect in distinguishing fea-tured articles from non-featured ones or, stated generally, high-quality articles from low-quality articles. As already motivated, the practical support for Wikipedia X  X  quality assurance process is marginal. A first step towards automatic quality assurance in Wiki-pedia is the detection of quality flaws, which we proposed in previ-ous research [3, 4]. Here, we push this idea further and extend our previous work with (1) a comprehensive breakdown of prior work on quality assessment, (2) an in-depth discussion of the cleanup tag mining approach, (3) a description of the quality flaw model, and (4) a detailed analysis of the one-class problem. The cleanup tag mining approach has also been used in [5], were we analyzed the incidence and the extent of quality flaws in the English Wikipedia. There is also notable research that relates indirectly to quality in Wikipedia: trust and reliability of articles [12, 37], accuracy and formality [14, 16], author reputation [1, 32], and automatic vandal-ism detection [27]. Cleanup tags provide a means to tag flaws in Wikipedia articles. As shown in Figure 1, cleanup tags are used to inform readers and editors of specific problems with articles, sections, or certain text fragments. However, there is no silver bullet to compile a complete set of all cleanup tags. Cleanup tags are realized with templates, whereas templates in turn are special Wikipedia pages that can be included in other pages. Although templates can be separated from other pages by their namespace (the prefix  X  X emplate: X  in the page title), there is no dedicated qualifier to separate templates that are used to implement cleanup tags from other templates. A com-plete manual inspection is infeasible as Wikipedia contains about 320 000 different templates. To cope with this situation we devel-oped an extraction approach that exploits several sources within Wikipedia containing meta information about cleanup tags (Sec-tion 3.2). In this regard we also analyzed the frequency and nature of the extracted tags (Section 3.3). At first, we describe the data underlying our mining approach (Section 3.1).
To ensure reproducibility, the analyses in this paper are based on a snapshot instead of investigating Wikipedia up-to-the-minute. Wikipedia snapshots are provided by the Wikimedia Foundation in monthly intervals. Because of its size and popularity we consider the English language edition most appropriate, and we use the En-glish Wikipedia snapshot from January 15, 2011. 4 A Wikimedia snapshot comprises a complete copy of Wikipedia. The wikitext sources of all pages (and of all revisions) are available as a single XML file that is enriched by some meta information. In addition, several tables of the Wikipedia database are available in the form of SQL dumps, totaling about 40GB. In a preprocessing step, we create a local copy of the Wikipedia database by importing the SQL dumps into a MySQL database. Since we do not target a content analysis, a processing of the XML dumps is not necessary. The local copy of the Wikipedia database allows for efficient parsing and mining without causing traffic on the Wikimedia servers. Note that all of our analyses can be performed on the original Wikipedia database as well.
Wikimedia downloads: http://download.wikimedia.org .
We employ a two-step approach to compile the set of cleanup tags: (1) an initial set of cleanup tags is extracted from two meta sources within Wikipedia, and (2) the initial set is further refined by applying several filtering substeps.
 Step 1: Extraction The first meta source that we employ is the Wikipedia administration category Category:Cleanup_templates , which comprises templates that are used for tagging articles as re-quiring cleanup. The category also has several subcategories to further organize the cleanup tags by their usage, e.g., inline cleanup templates or cleanup templates for WikiProjects. The page titles of those templates linking to the category or some subcategory are ob-tained from the local Wikipedia database, which results in 272 dif-ferent cleanup tags. The second source is the Wikipedia meta page Wikipedia:Template_messages/Cleanup , which comprises a manu-ally maintained listing of cleanup templates that may be used to tag articles as needing cleanup. From a technical point of view, the page is a composition of several pages (transclusion principle). For each of these pages, the content of the revision from the snapshot time is retrieved using the MediaWiki API. 5 A total of 283 different cleanup tags are extracted from the wikitexts of the retrieved pages using regular expressions. Merging the findings from both sources gives 457 different cleanup tags.
 Step 2: Refinement A cleanup tag may have several alternative titles linking to it through redirects. For example, the tag Unref-erenced has the redirects Unref , Noreferences , and No refs among others. We resolve all redirects using the local Wikipedia database. Moreover, we discard particular subtemplates from the initial set of cleanup tags. Subtemplates are identified by a suffix in the page ti-tle, and they are used for testing purposes (suffixes  X /sandbox X  and  X /testcases X ) or provide a template description (suffix  X /doc X ). We also discard meta-templates, i.e., templates that are solely used ei-ther as building blocks inside other templates or to instantiate other templates with a particular parameterization. Meta-templates are derived from the Wikipedia categories Wikipedia_metatemplates and Wikipedia_substituted_templates . Altogether we collect a set of 388 cleanup tags.
 Discussion To evaluate our mining approach, we manually in-spected all documentation pages of the 388 cleanup tags. They give information about purpose, usage, and scope of a template, and our analysis reveals that all tags are indeed related to a par-ticular cleanup task. I.e., each of the 388 cleanup tags defines a particular quality flaw. Our mining approach does not guarantee completeness though, since the true set of cleanup tags is unknown in general. However, from a quantitative point of view we are con-fident that we identify the most common cleanup tags, and hence the most important quality flaws.
The Wikipedia snapshot comprises 3 557 468 articles, from which 979 299 (27.53%) are tagged with at least one quality flaw. Some articles are tagged with multiple flaws (multi-labeling). The number of flaws per article ranges from 1 to 17. However, the ma-jority (74.95%) of the tagged articles are tagged with exactly one flaw. The number of tagged articles is an underestimation of the true frequencies of the quality flaws, since due to the size and the few control mechanisms in Wikipedia it is more than likely that many flawed articles are still not identified. From the 388 cleanup tags 18 merely state that some cleanup is required at all but do not We use the API in favor of parsing the XML dumps. MediaWiki API: http://www.mediawiki.org/wiki/API . provide further information. Moreover, 307 cleanup tags refer to the whole article (e.g., Unreferenced in Figure 1), whereas 81 re-fer to particular claims (e.g., Citation needed ). In this paper, we target the prediction of specific flaws referring to the whole article, and hence we discard the 18 unspecific tags as well as the 81 inline tags. 6 Note in this respect that only 2,01% of the tagged articles are tagged with one of the 18 unspecific tags and that the majority (80.02%) of tagged articles are tagged with a flaw that refer to the whole article. 289 cleanup tags remain that define specific article flaws; the ten most frequent are listed in Table 1. 896 953 articles are tagged with the 289 quality flaws, out of which 76.41% are tagged with the ten flaws shown in Table 1.
The modeling of quality flaws can happen intensionally or ex-tensionally, depending on a flaw X  X  nature and the knowledge that is at our disposal. 7 An intensional model of a flaw f can be under-stood as a set of rules, which define, in a closed-class manner, the set of articles that contain f . An extensional model is given by a set of positive examples, and modeling means learning a classifier that discriminates positive instances (containing the flaw) from all other instances. Of course, the basis of both modeling paradigms are expressive features of Wikipedia articles.
To this day a large body of article features X  X llegedly quality predicting X  X as been proposed; we have compiled a comprehen-sive breakdown, see Table 3 in Appendix A. Within our implemen-tation all ticked features are employed, i.e., we capture the state-of-the-art with respect to feature expressiveness and information quality research. Some features are omitted since their implemen-tation effort exceeds the expected benefit by far. In addition we devise several new features, which are marked with an asterisk. The features are organized along four dimensions: content, struc-ture, network, and edit history. The source of information that is
Our prediction approach can be applied to inline flaws as well, by breaking the articles into paragraphs or sentences.
For special cases also a hybrid model is conceivable, where a fil-tering step (intensional) precedes a learning step (extensional). required for feature computation as well as the computational com-plexity differs for each dimension. Our model can be adjusted with respect to its transferability to other text documents than Wikipedia articles as well as to its computational complexity, by restricting to the features from a subset of the four dimensions.

Content features rely on the plain text of an article and are in-tended to quantify aspects like writing style and readability. Also, the new feature  X  X pecial word rate X , introduced to measure the pres-ence of certain predefined words, provides evidence of unwanted content and article neutrality. E.g., peacock words, such as leg-endary, great, and brilliant, may be an indicator of advertising or promotional content; similarly, the presence of sentiment-bearing words can be considered as an indicator of missing neutrality. The content features can be computed with a complexity of O ( | d | ) , where | d | denotes the length of an article d in characters.
Structure features are intended to quantify the organization of an article. We employ features which measure quantity, length, and nesting of sections as well as of subsections. Special attention is paid to the lead section, also called intro, as several flaws directly refer to it. Moreover, the usage of images, tables, files, and tem-plates is quantified, as well as the categories an article belongs to; the usage of lists is quantified for the first time. Other features quantify the usage of references, including citations and footnotes and shall target flaws related to an article X  X  verifiability. We in-troduce new features that check the presence of special sections that are either mandatory, such as  X  X eferences X , or that should be avoided, such as  X  X rivia X . Wikiprep is used to determine the num-ber of related pages, i.e., pages that are linked in special sections like  X  X ee also X  and  X  X urther reading X . 8 The computation of struc-ture features is governed by the complexity of parsing an article X  X  markup, which is in O ( | d | ) .

Network features quantify an article X  X  integration by means of hyperlinks. Here we distinguish the following types of outgoing links: These features count the number of outgoing links as well as their frequency relative to the article size. We introduce the feature of incoming links, where the origin has to be an article (i.e. links from disambiguation, redirect, and discussion pages are excluded). The in-link count targets the flaw Orphan , and the out-link counts target the flaw Wikify . The computation of network features is based on the link graph and is in O ( | d | X | D | ) , where D denotes the set of all Wikipedia articles.

Edit history features model article evolution, which pertains to the frequency and timing of revisions as well as to the community of editors. These features have been proven valuable to classify featured articles [13, 21, 29, 34]; they address aspects like stability, maturity, and cooperation. The computation of edit history features is in O ( | d | X  r d ) , where r d denotes the number of revisions of an article d .
The flaw descriptions in Table 1 show that three flaws from the set of the ten most frequent flaws, namely Unreferenced , Orphan , and Empty section can be modeled with rules based on the afore-mentioned features. http://sourceforge.net/apps/mediawiki/wikiprep .
An article suffers from the flaw Unreferenced if it does not cite any references or sources. Wikipedia provides different ways of citing sources, including inline citations, footnotes, and parenthet-ical referencing. 9 Here, we summarize all types of citations un-der the term  X  X eferences X . Using the structure features  X  X efer-ence count X  and  X  X eference sections count X  we define the predicate unreferenced ( d ) : unreferenced ( d ) = An evaluation on D  X  Unreferenced , the set of articles that have been tagged to be unreferenced, reveals that the unreferenced -predicate is fulfilled for 85.3% of the articles. We analyzed the remaining 14.7% and found that they actually provide references, and hence are mistagged. This observation shows a well-known problem in the Wikipedia community, and there is a WikiProject dedicated to cleanup mistagged unreferenced articles. 10 The fact that there is no such WikiProject for other quality flaws suggests that this problem is not considered to be serious for other flaws.

The Orphan flaw is well-defined: an article is called orphan if it has fewer than three incoming links. In this regard the following page types are not counted: disambiguation pages, redirects, soft redirects, discussion pages, and pages outside of the article name-space. 11 Using the network feature  X  X n-link count X  we define the predicate orphan ( d ) : An evaluation on D  X  Orphan reveals that the orphan -predicate is ful-filled for 98.4% of the articles.

An article suffers from the flaw Empty section if it has a section that does not contain content at all. Using the structure feature  X  X hort section length X  we define the predicate empty _ section ( d ) : empty _ section ( d ) = 1 , if short -section -length ( d ) = 0 An evaluation on D  X  Empty _ section reveals that the empty _ section -predicate is fulfilled for 99.1% of the articles.

The intensional modeling paradigm is very efficient since no training data is required and since the computation relies on few basic features. Moreover, as the above evaluations show, it is effec-tive at the same time. Note however, that if the definition of a flaw changes, an explicit model needs to be adapted as well.
The majority of quality flaws is defined informally and cannot be modeled by means of explicit rules (see Table 1); the knowledge is given in the form of examples instead. For an article d  X  D we model these flaws as a vector d , called document model. The dimensions of d quantify the features ticked in Table 3, and, for a set D of Wikipedia articles, D denotes the set of associated docu-ment models. By means of machine learning a mathematical deci-sion rule is computed from D that discriminates between elements from D  X  and D \ D  X  (see Figure 2).
Guidelines for citing sources: http://en.wikipedia.org/ wiki/Wikipedia:Citing_sources . WikiProject: http://en.wikipedia.org/wiki/Wikipedia: Mistagged_unreferenced_articles_cleanup .
Criteria for orphaned articles: http://en.wikipedia.org/ wiki/Wikipedia:Orphan#Criteria .
We argue that the prediction of quality flaws is essentially a one-class problem. This section gives a formal problem definition (Section 5.1) and devises a tailored one-class machine learning ap-proach to address the problem (Section 5.2).
Let D be the set of Wikipedia articles and let F be a set of quality flaws. A document d  X  D can contain up to | F | flaws, where, without loss of generality, the flaws in F are considered as being uncorrelated. A classifier c hence has to solve the following multi-labeling problem: 12 where 2 F denotes the power set of F . Basically, there are two strategies to tackle multi-labeling problems: 1. by multiclass classification, where an individual classifier is 2. by multiple binary classification, where a specific classifier Since the high number of classes under a multiclass classification strategy entails a very large number of training examples, the sec-ond strategy is favorable.

In most classification problems training data is available for all classes that can occur at prediction time, and hence it is appropriate to train a classifier c i with (positive) examples of the target class f and (negative) examples from the classes F \ f i . When spotting quality flaws, an unseen document can either belong to the target class f i or to some unknown class that was not available during training. I.e., the standard discrimination-based classification ap-proaches (binary or multiclass) are not applicable to learn a class-separating decision boundary: given a flaw f i , its target class is formed by those documents that contain (among others) flaw f but it is impossible to model the  X  X o-class X  with documents not containing f i . Even if many counterexamples were available, they could not be exploited to properly characterize the universe of pos-sible counterexamples. As a consequence, we model the classifica-tion c i ( d ) of an document d  X  D with respect to a quality flaw f as the following one-class classification problem: Decide whether or not d contains f i , whereas a sample of documents containing f is given.

As an additional illustration consider the flaw Refimprove , which is described in Table 1. An even large sample of articles that suf-fer from this flaw can be compiled without problems (89 686 arti-cles have been tagged with this flaw). However, it is impossible to compile a representative sample of articles that have a reasonable number of proper citations for verification. Although many articles with sufficient citations exist (e.g., featured articles), they cannot be considered as a representative sample . The fact that featured ar-ticles are not representative for typical Wikipedia articles becomes
Possibly existing correlations among the flaws in F will not affect the nature of the multi-labeling problem. clear when looking at Figure 3, which shows a sample of Wikipe-dia articles represented under the first two principle components. Figure 3 also shows that quality flaw prediction is a significantly harder problem than discriminating featured articles. Training a bi-nary classier using featured articles and flawed articles would lead to a biased classifier that is not able to predict flaws on the entire Wikipedia. Also, using random articles and flawed articles to train a binary classier is unacceptable because, as already mentioned, it is more than likely that many flawed articles are not yet identified. Stated another way, quality flaw prediction is a one-class problem.
Following Tax [31], three principles to construct a one-class clas-sifier can be distinguished: density estimation methods, boundary methods, and reconstruction methods. Here we resort to a one-class classification approach as proposed by Hempstalk et al. [17], which combines density estimation with class probability estima-tion. There are two reasons for using this approach: (1) Hempstalk et al. show that it is able to outperform state-of-the-art approaches, including a one-class SVM, and (2) it can be used with arbitrary density estimators and class probability estimators. Instead em-ploying an out-of-the-box classifier we apply dedicated density es-timation and class probability estimation techniques to address the problem defined above.

The idea is to use a reference distribution to model the probabil-ity P ( d | f 0 i ) of an artificial class f 0 i , and to generate (artificial) data governed by the distribution characteristic of f 0 i let P ( f i ) and P ( f i | d ) denote the a-priori probability and the class probability function respectively. According to Bayes X  theorem the class-conditional probability for f i is given as follows: P ( f i | d ) is estimated by a class probability estimator, i.e., a clas-sifier whose output is interpreted as probability. Since we are in a one-class situation we have to rely on the face value of P ( d | f More specifically, P ( d | f i ) cannot be used to determine a maxi-mum a-posterior (MAP) hypothesis among the f i  X  F . As a con-sequence, given P ( d | f i ) &lt;  X  with  X  = 0 . 5 , the hypothesis that d suffers from f i could be rejected. However, because of the approx-imative nature of P ( f i | d ) and P ( f i ) the estimation for P ( d | f is not a true probability, and the threshold  X  has to be chosen em-pirically. In practice, the threshold  X  is derived from a user-defined target rejection rate, trr , which is the rejection rate of the target class training data.

The one-class classifier is built as follows: at first a class with artificial examples is generated, whereas the feature values obey a Gaussian distribution with  X  = 0 and  X  2 = 1 . We employ the Gaussian distribution in favor of a more complex reference distri-bution to underline the robustness of the approach. The proportion of the generated data is 0 . 5 compared to the target class. As class probability estimators we apply bagged random forest classifiers with 1 000 decision trees and ten bagging iterations. A random for-est is a collection of decision trees where a voting over all trees is run in order to obtain a classification decision [18, 10]. The deci-sion trees of a forest differ with respect to their features. Here, each tree is build with a subset of log 2 ( | features | )+1 randomly chosen features, i.e., no tree minimization strategy is followed at training time. The learning algorithm stops if either all leaves contain only examples of one class or if no further splitting is possible. Each decision tree perfectly classifies the training data X  X ut, because of its low bias the obtained generalization capability is poor [35, 25]. However, the combination of several classifiers in a voting scheme reduces the variance and introduces a stronger bias. While the bias of a random forest results from several feature sets, the bias of the bagging approach results from the employment of several training sets, and it is considered as being even stronger [9].
We report on experiments to assess the effectiveness of our mod-eling and classification approach in detecting the ten most frequent quality flaws shown in Table 1. As already mentioned, 76.41% of the tagged Wikipedia articles suffer from these flaws (see Sec-tion 3.3). The evaluation treats the following issues: 1. Since a bias may not be ruled out when collecting outlier 2. Since users (Wikipedia editors) have diverse expectations re-3. Since the true flaw-specific class imbalances in Wikipedia Preprocessing We use the same data basis that underlies our cleanup tag mining approach, i.e., the English Wikipedia snapshot from January 15, 2011. The articles X  plain texts and wikitexts are extracted from the  X  X ages-articles X  dump, which is included in the Wikimedia snapshot and which comprises the current revisions of all articles in a single XML file of about 25GB size. The plain texts and the wikitexts form the basis to compute the content fea-tures and the structure features. Our local copy of the Wikipedia database, which is described in Section 3.1, is used to compute the network features. The computation of the history features is based on the  X  X ages-meta-history X  dump, which is included in the Wiki-media snapshot and which comprises the content of each revision in a single XML file of about 7.3TB size. The XML dumps are pro-cessed on an Apache Hadoop cluster using Google X  X  MapReduce.
Recall that no articles are available that have been tagged to not contain a quality flaw f i  X  F . Thus a classifier c i can be evaluated only with respect to its recall, whereas a recall of 1 can be achieved easily by classifying all examples into the target class of f order to evaluate c i with respect to its precision one needs a rep-resentative sample of examples from outside the target class, so-called outliers. As motivated above, in a one-class situation it is not possible to compile a representative sample, and a way out of the dilemma is the generation of uniformly distributed outlier ex-amples [31]. Here, we pursue two strategies to derive examples from outside the target class, which result in the following settings: 1. Optimistic Setting. Use of featured articles as outliers. This 2. Pessimistic Setting. Use of a random sample from D \ D
The above settings address two extremes: classification under laboratory conditions (overly optimistic) versus classification in the wild (overly pessimistic). The experiment design is owing to the facts that  X  X o-flaw features X  cannot be stated and that the number of false positives as well as the number of false negatives in the set D  X  of tagged articles are unknown. Experiment Design The evaluation is performed for the set F 0  X  F of the ten most frequent quality flaws. In the optimistic setting 1 000 outliers are randomly selected from the 3 128 featured articles in the snapshot. In the pessimistic setting 1 000 outliers are randomly selected for each flaw f i  X  F 0 from D \ D  X  i . We evaluate our approach under both settings by applying the following proce-dure: For each flaw f i  X  F 0 the one-class classifier c i with 1 000 articles randomly sampled from D  X  i and the respective 1 000 outliers, applying tenfold cross-validation. Within each run the classifier is trained with 900 articles from D  X  i , whereas test-ing is performed with the remaining 100 articles from D  X  outliers. Note that c i is trained exclusively with the examples of the respective target class, i.e., the articles in D  X  i . The training of c neither affected by the class distribution nor by the outlier selection strategy that is used in the respective setting.
 Operating Point Analysis For the major part of the relevant use cases precision is the determining measure of effectiveness; con-sider for instance a bot that autonomously tags flawed articles. The precision of the one-class classifier is controlled by the hyperpa-rameter  X  X arget rejection rate X . We empirically determine the op-timal operating point for each of the ten flaws under both the op-timistic and the pessimistic setting. Here, the optimal operating point corresponds to the target rejection rate of the maximum pre-cision classifier. Figure 4 illustrates the operating point analyses exemplary for the flaw Unreferenced : with increasing target rejec-tion rate the recall value decreases while the precision values in-crease. Observe that the recall is the same in both settings, since it
The hypothesis may hold in many cases but not always: the snap-shot comprises 13 featured articles that have been tagged with some flaw. We discarded these articles in our experiments. solely depends on the target class training data. For the flaw Un-referenced the optimal operating points under the optimistic and the pessimistic setting are at a target rejection rate of 0.1 and 0.35 respectively (with precision values of 0.99 and 0.63).

The precision of a one-class classifier cannot be adjusted arbi-trarily since the target rejection rate controls only the probability threshold  X  for the classification decision. For instance, a target rejection rate of 0.1 means that a  X  is chosen such that 10% of the target class training data will be rejected, which results in a classi-fier that performs with an almost stable recall of 0.9. Increasing the target rejection rate entails an increase of  X  . However, if  X  achieves its maximum no further examples can be rejected, and hence both the precision and the recall remain constant beyond a certain target rejection rate (which is 0.4 for the flaw Unreferenced , see Figure 4). Results Table 2 shows the performance values for each of the ten quality flaws. The values correspond to the performance at the respective optimal operating point. The performance is quantified as precision ( prec ) and recall ( rec ). We also report the area under ROC curves (AUC) [15], which is important to assess the tradeoff between specificity and sensitivity of a classifier: an AUC value of 0.5 means that all specificity-sensitivity-combinations are equiva-lent, which in turn means that the classifier is random guessing.
Under the optimistic setting four flaws can be detected with a nearly perfect precision. As expected, the precision values for the flaws Unreferenced , Orphan , and Empty section are very high; re-call that these flaws can also be modeled intensionally. For the flaw Notability even the achieved recall value is very high, which means that this flaw can be detected exceptionally well. As expected, the effectiveness of the one-class classifiers deteriorates under the pes-simistic setting. However, the classifiers still achieve reasonable precision values, and even in the noisy test set the flaw Orphan can be detected with a good precision. Notice that the expected per-formance in the wild lies in between the two extremes. For some flaws the effectiveness of the one-class classifiers is pretty low un-der both settings, including Original research . We explain this be-havior as follows: (1) Either the document model is inadequate to capture certain flaw characteristics, or (2) the hypothesis class of the one-class classification approach is too simple to capture the flaw distributions.
The performance values in Table 2 presume a balanced class dis-tribution, i.e., the one-class classifiers are evaluated with the same number of flawed articles and outliers. The real distribution of flaws in Wikipedia is unknown, and we hence report precision val-ues as a function of the class imbalance. Given the recall and the false positive rate ( fpr ) of a classifier for the balanced setting, its precision for a class size ratio of 1:n (flawed articles : flawless arti-cles) computes as follows: The false positive rate is the ratio between the detected negative examples and all negative examples, and hence it is independent from the class size ratio; the same argument applies to the recall. Figure 5 shows the precision values as a function of the flaw distri-bution under the optimistic setting.

We make two assumptions in order to estimate the actual fre-quency of a flaw f i : 1. each article in D  X  is tagged completely, i.e. with all flaws 2. the distribution of f i in D  X  is identical to the distribution of Based on these assumptions we estimate the actual frequency of a flaw f i by the ratio of articles in D  X  i and articles in D ble 2 lists the estimated flaw ratio for each of the ten most frequent flaws. For example, the ratio of the flaw Unreferenced is about 1:3 (273 230 : 979 299). In other words, about every fourth article is expected to contain this flaw.
 Figure 5 shows that the expected precision values for the flaws Unreferenced , Orphan , and Notability are still high. The flaw ra-tio of the flaw Unreferenced is 1:3, and thus the expected precision is close to that of the 1:1 ratio. The flaw Orphan can be detected with a precision of 1, i.e., the false positive rate is 0, and hence the prediction performance is independent of the class imbalance. Although the flaw ratio of the flaw Notability is 1:30, the expected precision is still about 0.9, which shows that the respective one-class classifier captures the characteristics of the flaw exceptionally well. The expected precision values for those flaws with a flaw ratio 1:n where n &gt; 40 are lower than 0.2. Aside from conceptual weak-nesses regarding the employed document model, the weak perfor-mance indicates also that the training set of the one-class classifiers may be too small.
We treat quality flaw prediction as a process where for each known flaw an expert is asked whether or not a given Wikipedia article suffers from it; the experts in turn are operationalized by one-class classifiers. The underlying document model combines several new features with the state-of-the-art quality assessment features. Our evaluation is based on a corpus comprising 10 000 human-labeled Wikipedia articles, compiled by utilizing cleanup tags. We report on precision values close to 1 for four out of ten important flaws X  X resuming an optimistic test set with little noise and a balanced flaw distribution. Even for a class size ratio of 1:16 three flaws can still be detected with a precision of about 0.9.
We are convinced that the presented or similar approaches will help to simplify Wikipedia X  X  quality assurance process by spotting weaknesses within articles without human interaction. We plan to operationalize our classification technology in the form of a Wi-kipedia bot that autonomously identifies and tags flawed articles. Our approach also supports the principle of intelligent task rout-ing [11], which addresses the automatic delegation of particular flaws to appropriate human editors. Though the proposed quality flaw prediction approach is evaluated in the Wikipedia context, it is also applicable to other user-generated Web applications where cleanup tags are used.

Our current research targets the development of knowledge-based predictors for individual quality flaws. Instead of resorting to a single document model, we develop a flaw-specific view that combines feature selection, expert rules, and multi-level filtering. In this respect, we analyze in detail which features prove essential for the prediction of a certain quality flaw and how effective the newly introduced features are. Moreover, instead of resorting to a single learning approach, we investigate the amenability of differ-ent one-class classification approaches with respect to the different flaws. We are also investigating whether a learning approach can benefit from the untagged articles, e.g., using partially supervised classification or PU-learning [23]. [1] B. Adler and L. de Alfaro. A content-driven reputation [2] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and [3] M. Anderka, B. Stein, and N. Lipka. Towards automatic [4] M. Anderka, B. Stein, and N. Lipka. Detection of text quality [5] M. Anderka and B. Stein. A breakdown of quality flaws in [6] R. Baeza-Yates. User generated content: how good is it? In [7] M. Bendersky, W. B. Croft, and Y. Diao. Quality-biased [8] J. Blumenstock. Size matters: word count as a measure of [9] L. Breiman. Bagging predictors. Machine Learning , [10] L. Breiman. Random forests. Machine Learning , 45(1):5 X 32, [11] D. Cosley, D. Frankowski, L. Terveen, and J. Riedl. Using [12] T. Cross. Puppy smoothies: improving the reliability of open, [13] D. Dalip, M. Gon X alves, M. Cristo, and P. Calado. Automatic [14] W. Emigh and S. Herring. Collaborative authoring on the [15] T. Fawcett. Roc graphs: Notes and practical considerations [16] J. Giles. Internet encyclopaedias go head to head. Nature , [17] K. Hempstalk, E. Frank, and I. Witten. One-class [18] T. K. Ho. Random decision forests. In Proc. of ICDAR X 95 , [19] M. Hu, E. Lim, A. Sun, H. Lauw, and B. Vuong. Measuring [20] Y. Lee, D. Strong, B. Kahn, and R. Wang. AIMQ: a [21] A. Lih. Wikipedia as participatory journalism: reliable [22] N. Lipka and B. Stein. Identifying featured articles in [23] B. Liu, Y. Dai, X. Li, W. S. Lee and P. Yu. Building text [24] S. Madnick, R. Wang, Y. Lee, and H. Zhu. Overview and [25] T. Mitchell. Machine Learning . McGraw-Hill Higher [26] A. Pirkola and T. Talvensaari. A topic-specific Web search [27] M. Potthast, B. Stein, and R. Gerling. Automatic vandalism [28] J. Slone. Information quality strategy: an empirical [29] B. Stvilia, M. Twidale, L. Smith, and L. Gasser. Assessing [30] B. Stvilia, M. Twidale, L. Smith, and L. Gasser. Information [31] D. Tax. One-Class Classification . PhD thesis, Delft [32] F. Vi X gas, M. Wattenberg, and K. Dave. Studying [33] R. Wang and D. Strong. Beyond accuracy: what data quality [34] D. Wilkinson and B. Huberman. Cooperation and quality in [35] D. Wilson and R. Randall. Bias and the probability of [36] The Wall Street Journal. Jimmy Wales on Wikipedia quality [37] H. Zeng, M. Alhossaini, L. Ding, R. Fikes, and [38] Y. Zhou and B. W. Croft. Document quality models for Web [39] X. Zhu and S. Gauch. Incorporating quality metrics in
A variety of features has been proposed in the literature on au-tomatic quality assessment of Wikipedia articles (the relevant lit-erature is reviewed in Section 2). Table 3 gives a comprehensive overview of the proposed features, organized along the four dimen-sions content, structure, network, and edit history (the dimensions are described in Section 4.1). The overview is intended as a re-source for other researcher, and we consider it as the first complete compilation of this kind.
