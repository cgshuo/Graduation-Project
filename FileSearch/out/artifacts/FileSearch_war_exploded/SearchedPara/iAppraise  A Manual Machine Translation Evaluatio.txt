 Evaluation is one of the difficult problems in Ma-chine Translation (MT). Despite its clear draw-backs, 1 human evaluation remains the most reliable method to evaluate MT systems and track the ad-vances in Machine Translation. Appraise is an open-source toolkit designed to facilitate the human eval-uation of machine translation (Federmann, 2012). It has been adopted as the preferred tool in the WMT evaluation campaigns (Bojar et al., 2013), and thus, it is currently used by dozens of researchers. According to the eye-mind hypothesis (Just and Carpenter, 1980) people cognitively process objects that are in front of their eyes. This has enabled researchers to analyze and understand how people perform certain tasks like reading (Rayner, 1998; Garrod, 2006; Harley, 2013). In recent times, eye-tracking has also been used in Machine Translation to identify and classify translation errors (Stymne et al., 2012), to evaluate the usability of automatic translations (Doherty and O X  X rien, 2014), and to im-prove the consistency of the human evaluation pro-cess (Guzm  X  an et al., 2015), etc. Furthermore, track-ing how evaluators consume MT output, can help to reduce human evaluation subjectivity, as we could use evidence of what people do (i.e. unbiased read-ing patterns) and not only what they say they think (i.e. user-biased evaluation scores). However, the main limitation for the adoption of eye-tracking re-search has been the steep learning curve that is asso-ciated with eye-tracking analysis and the high-cost of eye-tracking devices.

In this paper, we present iAppraise : an open-source framework that enables the use of eye-tracking for MT evaluation, and facilitates the repli-cation and dissemination of eye-tracking research in MT. First, it is designed to work with the increas-ingly popular, low-cost 2 eye-tracker eyeTribe . Sec-ondly, it provides a set of tools for extracting and ex-ploiting gaze features, which facilitate eye-tracking analysis. Lastly, it integrates fully with the Appraise toolkit, making it accessible to a larger audience.
Our setup allows to track eye-movements during the MT evaluation process. The data generated can be used to visualize a re-enactment of the evaluation session in real-time, thus providing useful qualita-tive insights on the evaluation; or to extract features for further quantitative analysis.
The applications for this toolkit are multiple. Us-ing reading patterns from evaluators could be a use-ful tool for MT evaluation: ( i ) to shed light into the evaluation process: e.g. the general reading be-havior that evaluators follow to complete their task; ( ii ) to understand which parts of a translation are more difficult for the annotator; and ( iii ) to develop automatic evaluation systems that use reading pat-terns to predict translation quality. In an effort car-ried using this framework, we proposed a model to predict the quality of the MT output. Our results showed that reading patterns obtained from the eye-movements of the evaluators can help to anticipate the evaluation scores to be given by them. We found that the features extracted from the eye-tracking data (discussed in Section 2.6) capture more than just the fluency of a translation. Details of findings are re-ported in (Sajjad et al., 2016). In this paper, we describe the overall architecture of iAppraise : the communication modules, the user interface, and the analysis package. Appraise (Federmann, 2012) is an open-source translation output. However, it also allows to col-lect human judgments on a number of annotation tasks (such as ranking, error classification, quality estimation and post-editing) and provides an envi-ronment to maintain and export the collected data. The toolkit is based on the Django web frame-work that supports database modeling and object-relational mapping, and uses Twitter X  X  Bootstrap as a template for the interface design. iAppraise consist of a series of modules that ex-tend Appraise to integrate eye-tracking from the eye-we briefly describe the architecture of the toolkit. 2.1 Overall Architecture In Figure 1 we present the overall architecture of our toolkit. First, the iAppraise Adapter communicates directly with the EyeTribe eye-tracker through its API and propagates the gaze events to the iAppraise UI (User Interface) .
The iAppraise UI module takes the gaze events, and translates their coordinates into local browser coordinates. It also converts all the textual material in the display into traceable objects, that can detect Gaze when a user is looking at them. Additionally, the module contains a view-task whose layout is op-timized for the recognition of gaze events.

When a traceable object in iAppraise UI detects that a user is looking at it, it stores this information, augmented with UI details from the gaze data. This data is later stored in iAppraise Model/DB at the end of each evaluation session. Finally, the iAppraise Analysis module is designed to extract useful eye-tracking features from the generated data. These can be used for modeling or analysis.
 2.2 iAppraise Communication Interfaces The EyeTribe eye-tracker, running at 30Hz or 60Hz, broadcasts gaze data through a TCP port (Eye-Tribe default port 6555) using JSON (JavaScript Ob-ject Notation) formatted messages. The iAppraise Adapter employs two sockets, one that listens to the eye-tracker, and the other to pass the data to the iAp-praise UI. 2.3 iAppraise User Interface To facilitate the usage of eye-tracking data for ma-chine translation evaluation, we added an evaluation and graphical elements, that have been optimized for the use of eye-tracking. The template has two main content regions: Reference and Translation . The task for this view requires to score the quality of a translation by comparing it to the provided ref-erence. The annotator is required to use a slider (see Figure 2) to provide a score. In return, he/she gets feedback in the form of stars, that reflect how close his/her score is to an optional gold-standard score. In principle, the stars are part of a gamification strat-egy used to keep the evaluator engaged. If the gold standard scores are not be available this option can be turned off. 2.4 iAppraise Model/DB Figure 2: iAppraise Eye-Tracking Evaluation task layout with feedback. From top to bottom: Refer-ence and Translation sentences, slider for scoring, and feedback in a form of stars.
 To handle data flow and gather information resulted from the eye-tracking and user interaction, a new data model was added to Appraise. This data model stores the data received from the iAppraise UI into a database. Table 1 shows the different attributes and the description of the fields for the data recorded during an eye-tracking task. Table 1: Description of attributes stored in the database. 2.5 Eyetacking Replay The eye-tracking data collected during the evalua-tion session can be visualized as a re-enactment or replay. This allows to analyze the evaluator during the task, or to perform some basic troubleshooting. The replay highlights the background of words in the sequence that they were observed (See Figure 3 for demonstration). 2.6 iAppraise Analysis The iAppraise Analysis module extracts useful features from the iAppraise DB that can be used to analyze the evaluation process or a train a prediction model. It consists of several auxiliary scripts that parse the data and extract features described below: Figure 3: iAppraise Eye-Tracking task replay; words gets highlighted in the sequence that they were observed.
 Jump features While reading text, the gaze of a person does not visit every single word, but ad-vances in jumps called saccades . These jumps can go forward ( progressions ) or backward ( regres-sions ). We classify the word-transitions according to the direction of the jump and distance between the start and end words. For subsequent words n , n + 1 , this would mean a forward jump of distance equal to 1 . All jumps with distance greater than 4 are sorted into a 5+ bucket. Additionally, we sepa-rate the features for reference and translation jumps. We also count the total number of jumps.
 Total jump distance We aggregate jump dis-evaluating a sentence. We count reference and trans-lation distance features separately. Such information is useful in analyzing the complexity and readability of the translation.
 Inter-region jumps While reading a translation, evaluators can jump between the translation and a reference to compare them. Intuitively, more jumps of this type could signify that the translation is harder to evaluate. Here we count the number of reference  X  translation transitions. Dwell time The amount of time a person fixates on a region is a crucial marker for processing difficulty in sentence comprehension (Clifton et al., 2007) and moderately correlates with the quality of a transla-tion (Doherty et al., 2010). We count the time spent by the reader on each particular word. We separate reference and translation features.
 Lexicalized features The features discussed above do not associate gaze movements with the words being read. We believe that this information can be critical to judge the overall difficulty of the reference sentence, and to evaluate which transla-tion fragments are problematic to the reader. To compute the lexicalized features, we extract streams of reference and translation lexical sequences based on the gaze jumps, and score them using a tri-gram language model. Let R i = r 1 , r 2 , . . . , r m be a sub-sequence of gaze movement over reference and there are R 1 , R 2 , . . . , R n sequences, the lex feature is computed as follows: The normalization factor | R i | is used to make the probabilities comparable. We also use un-normalized scores as additional feature. A similar set of features lex ( T ) is computed for the transla-tions. All features are normalized by the length of the sentence.

In a related effort, we used the above features to predict the quality scores given by an evaluator. More details on the model and how effective each of the features were, please refer to Sajjad et al. (2016). iAppraise demonstration will allow the users to ex-periment with the tool and the eye tracking device. The users will be able to perform an evaluation task, observe a replay of their own eye movements, and to export their gaze data. We will also demonstrate the basic functioning for additional tools and scripts. This includes using the exported data to extract fea-tures and information about the evaluation task.
The iAppraise server is available as an open-source project, and can also be downloaded as an already-configured virtual machine that can be de-ployed on any environment. In this paper, we presented iAppraise , a framework to provide eye-tracking capabilities to directly Ap-praise. Here we described the different components that make up the framework. The main goal of the framework is to provide a tool that lowers the entry-level bar to using eye-tracking in the MT commu-nity. iAppraise has several advantages: ( i ) it con-nects low-cost eye-trackers to an open-source MT analysis platform; and ( ii ) it provides a set of anal-ysis tools that allow the use of the gaze information effortlessly. We expect that in the future, more re-searchers will adopt iAppraise to explore the human consumption of text in other NLP tasks.

