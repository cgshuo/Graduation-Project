 In this pap er, we devise a metho d for the estimation of the true supp ort of itemsets on data streams, with the objectiv e to maximize one chosen criterion among f precision, recall g while ensuring a degradation as reduced as possible for the other criterion. We discuss the strengths, weaknesses and range of applicabilit y of this metho d that relies on con ven-tional uniform con vergence results, yet guaran tees statistical optimalit y from di eren t standp oints.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retriev al]: Information Searc h and Re-triev al; I.5.3 [Pattern Recognition]: Clustering General Terms: Algorithms.
 Keyw ords: Data stream mining.
A gro wing body of works arising from researc hers in Data bases and Data Mining deals with data arriving in the form of con tinuous poten tially in nite streams, i.e. an ordered sequence of item occurrences that arriv es in timely manner. Data streams have seen the emergence of crucial problems for databases that were previously not as pregnan t, suc h as the accurate retriev al of informations in a data ow that pre-vents its exact storage, and whose information may evolve through time. We consider items to be the unit information, and itemsets to be sets of items. An itemset is -frequent (frequen t for short) if it occurs in at least a fraction of the data stream, called its supp ort. An imp ortan t task is to build the set of the most frequen t items or itemsets en-coun tered. The sub ject of this pap er is to prop ose a tigh t and extensiv e study of the application of local uniform con-vergence results [4] to extend classical supp orts to statistic al supp orts.
There are two sources of error for the estimation of fre-quen t itemsets: it is possible that some itemsets observ ed as frequen t migh t in fact not be frequen t anymore from a longer observ ation of the data stream, even without a drift of the observ ation odds; on the other hand, some itemsets observ ed as not frequen t may well in fact be frequen t from a longer history of the data stream. The point is that it is statistically hard to nullify both sources of error from the
The way we use de nition 1 is simple. Consider that the user has xed both the theoretical supp ort 0 1, and a statistic al risk parameter 0 &lt; &lt; 1. Supp ose we can nd " suc h that: 8 ; S is an inf-( ; " )-co ver of X . Now, x 0 = + " , so that we keep S + " . We observ e 8 T 2 S n X ; S ( T ) ( T ) + " &lt; + " . Thus, we obtain 8 ; S + " X , whic h easily yields: 8 ; P = 1. Thus, there is no rst source of error, with high probabilit y. Symmetrically , supp ose we can nd " suc h that 8 ; S is a sup ( ; " ) cover of X , and x this time 0 = " , so that we keep S " . Because of the prop erty of S , we observ e 8 T 2 X ; S ( T ) X ( T ) " i.e. there is no second source of error with high probabilit y. Our problem is thus reduced to nding an accurate value of " suc h that S is a sup or inf ( ; " ) cover of X with high probabilit y. The follo wing Theorem gives a value " whic h yields with high probabilit y a sup ( ; " ) cover of X .
Theorem 1. 8 X; 8D ; 8 m &gt; 0 ; 8 0 1 ; 8 0 &lt; 1 , the following holds: 8 ; S is a sup ( ; " ) cover of X , for any " satisfying: " p (1 = (2 m )) ln( j X j = ) . Respectively, 8 ; S is an inf ( ; " ) cover of X , for any " satisfying: " p (1 = (2 m )) ln( j S n X j = ) .
 Theorem 1 says that nding (inf/sup) ( ; " ) covers is a fairly easy task 8 m . The follo wing argumen t sho ws that there are no signi can t better covers. Informally , we build a skewed distribution D on some very simple X , suc h that with probabilit y we "miss" the ( ; " )-co ver for some value of " sligh tly smaller than those of Theorem 1.
Theorem 2. 9 X; 9D ; 9 m &gt; 0 ; 9 0 1 ; 9 0 &lt; 1 such that the following holds: with probability , S is not a sup ( ; " ) cover of X , for any " satisfying: " c p (1 = (2 m )) ln( j X j = ) . Respectively, with probability , S is not an inf ( ; " ) cover of X , for any " satisfying: " c p (1 = (2 m )) ln( j S n X j = ) . Her e, c is some constant &lt; 1 .
 Theorem 2 says that the criterion whic h is not con trolled incurs a loss whic h is, in one sense, also statistically near-optimal; a simple argumen t sho ws that the value of this loss beha ves in a very reasonable manner.

We now shift to a discussion on the way our approac h be-haves when there is a distribution drift , i.e. when D changes through time. It turns out that our approac h can be tailored in a very simple way to estimating these changes in X . This simply consist in estimating S ( : ) on the basis of a moving window , wide enough to ensure m large enough, and reg-ularly sampling the data stream. All other parameters do not change . Figure 1 explains that, with this straigh tforw ard adaptation, the distribution drift is estimated with resp ect to the moving average of the distributions (thic k lines), and not with resp ect to the true distributions (regular line). We estimate for any itemset T the uctuations of a moving av-erage X ( T ) instead of X ( T ). With resp ect to this change, it is straigh tforw ard to sho w that the results still hold under any distribution drift, to keep maximal precision or recall with resp ect to the aver age drift. This smo othes the small local drifts, but keeps the signi can t variations of D within the detection range.

There only remains to upp erb ound j X j and j S n X j to compute empirically " for Theorem 1. Since j X j + j S n X j = m , we shall use afterw ards in the exp erimen ts the same up-perb ound, m , for both cardinals.
