 ORIGINAL PAPER Llu X s-Pere de las Heras  X  Oriol Ramos Terrades  X  Sergi Robles  X  Gemma S X nchez Abstract Recent results on structured learning methods have shown the impact of structural information in a wide range of pattern recognition tasks. In the field of document image analysis, there is a long experience on structural meth-ods for the analysis and information extraction of multiple types of documents. Yet, the lack of conveniently annotated and free access databases has not benefited the progress in some areas such as technical drawing understanding. In this paper, we present a floor plan database, named CVC-FP, that is annotated for the architectural objects and their structural relations. To construct this database, we have implemented a groundtruthing tool, the SGT tool, that allows to make specific this sort of information in a natural manner. This tool has been made for general purpose groundtruthing: It allows to define own object classes and properties, multiple labeling options are possible, grants the cooperative work, and provides user and version control. We finally have col-lected some of the recent work on floor plan interpretation and present a quantitative benchmark for this database. Both CVC-FP database and the SGT tool are freely released to the research community to ease comparisons between methods and boost reproducible research.
 1 Introduction Current advances on structured learning methods in many pattern recognition tasks have driven to the development of new methods encoding structural information. In the field of document image analysis, there is a long experience on struc-tural methods for information extraction and analysis of mul-tiple types of documents [ 21 , 29 , 37 ]. With all, these systems usually need conveniently annotated databases to extract and learnthestructuralinterrelationsbetweenobjects.Thelackof such available databases may constrain the research advances in some domains, which is for instance, the case of automatic floor plan understanding.

Indeed,automaticfloorplananalysisisahottopic.Despite having a main architectural design purpose, nowadays floor plans are spreading their usability into different areas. New tools help non-expert users to virtually create and modify their own house by simply drawing its floor plan on an on-line application, such as Autodesk Homestyler 1 and Floorplan-ner. 2 These tools can automatically generate the 3D view of a building to get an idea of how it would finally look like. More recently, Google has introduced more than 10,000 indoor floor plans in Google Maps Indoor to facilitate the mobile user navigation inside large buildings, usually airports, sta-tions, and malls [ 1 ]. In addition, state agents with large num-ber of properties may index floor plans by some structural information extracted from them, as individual room size of each building. This kind of indexing system would be of a great help when customers ask for specific requirements, like holding a conference or organizing musical shows.

These new applications combined with current architec-tural re-utilization of old designs in order to cut designing costs have dramatically incremented interest on automatic floor plan analysis systems. Thus, a wide research has been recently undergone in this topic: Floor plans structure has been extracted from hand-made images [ 8 , 28 ] and computer-generated scanned images [ 5 , 12 , 31 , 40 ]. 3D reconstruction from 2D printed documents has been studied in [ 19 , 30 , 33 ]. Moreover, the automatic analysis contributed to other spe-cific applications such as structural floor plan retrieval [ 6 ], and emergency evacuation simulation from complex build-ings [ 48 ].

Despite the great effort of the community, automatic floor plan interpretation is far from being solved. Floor plans con-tain multiple structural elements, symbols, color, and text. Moreover, there is not a common notation standard to draw floor plans, which lead the styles to highly vary from archi-tect to architect. With all, the task of creating a system able to cope with all possible notations is very challenging. Indeed, the vast majority of the recent systems only adapt well to a limited number of notations, since most of them are tested on own private collections, precluding the comparison to other systems.

Therefore, there is a current need of public collections that favor the research on floor plan understanding. These databases should preferably be representative for the exist-ing graphical notations on building modeling. Moreover, they must grant the extraction of structural interrelations between the architectural elements. This would let the systems to learn how these elements are structurally arranged and thus to trig-ger better interpretation less dependent on the graphical rep-resentation of the documents.

Yet, the creation of databases entails another difficulty: the image labeling. Even though it is a straightforward pro-cedure, the creation of groundtruth (GT) is, for the most part, tedious and slow. Thus, tools allowing complex GT genera-tion in an efficient way are highly required to speed up this procedure and make it as lighter as possible.

In order to give solution to the problems mentioned, this paper presents the following contributions: 1. We present a new database of real floor plans (CVC-FP) 2. A benchmark on wall segmentation and room detection
The CVC-FP database, the SGT tool, the complete bench-markresults,andtheevaluationscriptspresentedinthispaper are available at the CVC-FP web page. 3 Our intention is to ease and promote the researchers to test and compare their own interpretation methods.
We have organized the paper as follows. In Sect. 2 we review existing related databases and groundtruthing tools. Then, we start by introducing the SGT tool in Sect. 3 .This will allow us to explain in detail in Sect. 4 the structural con-tent and format of the groundtruth generated. Section 5 is devoted to present the images of the four datasets that con-form the CVC-FP database. We finally present benchmark in Sect. 6 and conclude the paper in Sect. 7 . 2 Related work In order to put our work into context, we briefly explain the existing databases related to floor plans analysis tasks. We subsequently overview the characteristics of the available annotation tools to generate GT in documents. 2.1 Floor plan databases Everyday, the amount of available datasets for research pur-poses is increasing thanks to the collaborative work of the community. Technical committees, research centers, and uni-versities are highly contributing by updating, maintaining, and sharing their resources [ 2 ]. Yet, we are still far away from having a wide range of representative benchmark datasets for the different scenarios in document analysis. Testing and comparing different approaches in distinct domains is limited to few well-known labeled collections. This fact sometimes can favor ad hoc systems that fit very well in the existing datasets over those ones that better fill in the large variability of the real world. For these reasons, new annotated datasets, well structured and detailed, that fill empty spaces in any research domain are always welcome.

In our area of interest, graphics recognition in documents, multiple available databases have been incorporated for the different sub-areas that it covers. These datasets can be cre-ated either by means of synthetic data generation or by real document annotation. On the one hand, synthetic databases consist of data generated by varying a predefined set of para-meters to model different degrees of distortion, noise, and degradation than real documents may suffer. The generation of these sort of collections tends to be much faster than the annotated ones. In return, the model has to be closed enough to the reality to allow strong conclusions when using them. On the other hand, the annotated databases of real docu-ments reflect the real variability of the world. However, col-lecting and manually groundtruthing the images can be very time demanding. This issue can be relaxed by semi-automatic annotation procedures [ 32 ].

One example of synthetic database is the GREC X 2003 [ 44 ]. It was conceived in the IAPR International Workshop on Graphics Recognition in 2003 to set up a common evalu-ation strategy for symbol recognition. This challenge dataset contains 50 cropped models from architectural and electri-cal documents. The primitives of these symbols are lines and arcs, which are subjected to different levels of noise, shape distortions, and linear transformations. Lately, the GREC X 2011 dataset [ 43 ] was created not only as an extension of GREC X 2003 in terms of recognition, but also included a symbol spotting contest in both architectural and electrical documents.

One of the most used databases for symbol recognition related tasks is the SESYD database [ 17 ]. It is a collection of labeled synthetic images. They include architectural and electrical documents for symbol spotting, recognition, and retrieval. Additional datasets for text-graphic separation and text segmentation are included. Regarding its floor plan col-lections, they are specifically generated for detection pur-poses, leaving aside the semantic assembly between symbols and the building structure.

The FPLAN-POLY database [ 38 ] is, to our best knowl-edge, the only available collection of annotated real floor plans. Nevertheless, it aims for symbol spotting tasks. It con-tains 38 symbol models in a collection consisting of 48 vec-torized images.

Although there is not any floor plan database for complete analysis purposes, on other structured drawings such as flow-chart diagrams, several work has been pursued on structural and semantic understanding. Thereby, the CLEF-IP initiative investigates information retrieval techniques on patent docu-ments. One of the goals of that challenge consists of extract-ing the structural information from patent flowcharts in order to be queried semantically a posteriori. This process entails not only the detection and recognition of the elements par-ticipating in the diagrams (nodes, text, arrows), but also the structural assembly between them and their semantic mean-ing [ 35 , 39 ]. 2.2 Groundtruthing tools In the document analysis domain, we can find a large set of tools developed for the generation of GT. We analyze them by describing their functionality and limitations.
Most of the existing groundtruthing tools for document analysis related tasks are oriented to deal with textual docu-ments. On the one hand, some of them address the evaluation of logical and physical layout methods, e.g., Aletheia [ 13 ], GEDI [ 18 ], TRUEVIZ [ 24 ], PinkPanther [ 47 ], and GiDoc. Here, entities are represented by rectangular or polygonal regions by both physical and logical information. Physical information usually belongs to textual regions, pictures , fig-ures , tables , etc., while logical information usually denote the semantic meaning of each physical entity in the doc-ument context, e.g., headers , title , footnote ,etc.Onthe other hand, some tools focus on performance evaluation at pixel level. These tools aim at a very accurate pixel anno-tation and include semi-automatic labeling tools to improve the groundtruthing efficiency. Examples of these tools are the multi-platform based on Java TM PixLabeler [ 41 ], and the very recent web-based tools WebGT [ 10 ] and APEP-te [ 26 ].

The specific focus of the previously cited tools hinders their usability on other document analysis tasks, e.g., graph-ics recognition. Some of them only allow to label rectangular segments [ 22 , 24 , 46 ]. Others delimit the definition of object categories into a small set of predefined classes [ 10 , 41 ]. Moreover,thedefinitionofobjectdependenciesusuallyrelies on hierarchical information [ 26 , 47 ] and limited structural concepts [ 13 , 18 ], e.g., reading order and relative location. Furthermore, to our best knowledge, only [ 13 ] has a multi-layer representation that permits the labeling of fully over-lapped objects.

Finally, it is worth to mention that the current tendency is to design multiuser tools that foster real-time groundtruthing cooperation either by version control [ 10 , 26 ]orfollowing crowdsourcing strategies [ 7 , 22 ]. Moreover, the vast majority of the recent tools use slight variations based on XML for GT specification,e.g., the PAGE format [ 36 ]. This fact permits to easily adapt the existing platforms to parse GT files generated by other applications. Yet, none of the existing web-based tools use the SVG format to naturally display the GT at the web browser interfaces. 3 The structural groundtruthing tool The SGT tool is thought to perform general purpose groundtruthing, not restricted only to one specific domain as most of the existing tools are. It grants full flexibility since the database owners can create, modify, and erase their own object classes. Additionally, it is possible to define and declare n-ary properties for the labeled objects. Thus, the groundtruth can be seen as an attributed graph repre-sentation where nodes are objects and edges are relations between them. In Fig. 1 we can see a scheme of the SGT tool architecture. The SGT tool is user-friendly, it allows two different labeling options (bounding box and polygo-nal), and the output is in the standard Scalable Vector Graph-ics (SVG). The tool is a cross-platform running on a web service, which enforces co-working without sacrificing secu-rity. It has been implemented in php5 and HTML5, and the collections are stored in a relational database like MySQL [ 3 ].

In this section, we overview the SGT tool. For a further detailed explanation we encourage the users to read the user guide , available in the project CVC-FP web page. 3.1 Classes and structural relations: definitions and labeling The SGT tool can be used in multiple domains since it allows the user to define their own object classes. For example, in the floor plan interpretation framework that we are interested in, we define object classes as Wa l l , Room , and Door . Contrar-ily, for symbol spotting we would rather define Bed-type1 , Bed-type2 , and Shower-bath , andfor textual document layout analysis Title , Legend , and Graphic . The classes are defined at dataset level in a Class Management window, where the user can define, modify, and delete their own classes. When a new class is created an example image of the object can be added into its definition, see Fig. 2 . This image is shown at labeling time to help unexperienced users in the cooperative groundtruthing task.

Object properties not only allow to define attributes for the different elements, but they also permit to declare structural and semantic dependences among multiple object instances. They are similarly defined and administered as classes at the Relation Management window. At definition time, the user can define the arity of the property: They can be specific for a single object or relating n . A brief description to help users can be also written in their definition, see Fig. 3 . Their labeling is done by selecting first the desired property and then by picking those labeled objects that participate in it. The SGT tool ensures that the arity declared agrees with the property definition.

SGT tool facilitates the user the labeling procedure with a clear interface (see Fig. 4 ). Objects can be labeled either by drawing their bounding box through selecting just two corners or by drawing their polygon through a sequence of clicks. Moreover, it allows to make local zooming to ease the labeling of tiny objects. In other GT tools, the visual-ization and selection of the desired objects can become a challenging task in crowded images with multiple overlap-ping objects. Since SGT tool uses a multilayer representation for each object category, the users can display or hide object annotations at their convenience. This functionality extrapo-lates to object properties. 3.2 Creation and version control of a database A registered user that uploads a collection of images to the SGT tool is its owner. Once uploaded, any registered user can participate in the groundtruthing task. They have only to select one image and start the annotation. Then, the tool will automatically avoid concurrent edition by control-ling the access to the in use documents. For each of them, the new GT version associated with its author is stored by the versioning control system. Thereby, the database owner can track and control the whole groundtruthing pro-cedure. 3.3 Input images and groundtruth SVG files Concerning the input documents, our application accepts the most common types of image formats: PNG, JPG, and TIFF. When an image is uploaded, it is stored by its file name and indexed locally in its database. The SGT tool has been implemented to support heavy files; it behaves smoothly with images around 20 mega pixels.

To make easier the exchange of classes and relations between databases, the SGT tool incorporates importing/ exporting tools. For a given image called X, the tool generates an extended version of a Scalable Vector Graphics (SVG) file. We have chosen SVG for formatting our GT mainly because of three reasons: It uses a well-structured format XML-based language, it is a recommendation of W3C, 5 which ensures evolution and maintenance and, finally, allows to describe 2-dimensional vectorial graphics that are displayable in most of the Internet browsers. It is worth to notice that, since the SGT tool is web-based, the user interface is displayed at the Internet browser. Therefore, the use of SVG format permits to adapt to different browser preferences while maintaining the same labeling visualization. Obviously, the tool also allows to import external SVG files to update the GT.

The format of an extended SVG file includes own meta-data information and is defined as follows. Firstly, the gen-eral information regarding the GT is specified. It includes the image dimensions (width and height) in pixels, the number of different instances labeled, the number of classes appearing in the document, and the name of all the classes that appear in the dataset. Secondly, it contains the list of the elements in the image. Each text-line describes one object by its label, its document-unique identity number, and its polygon com-posed by the extremity points selected by the user. Finally, the document describes the relations between the objects. Each relation is identified by its type and the identities of the elements involved. 4 The floor plan groundtruth In this section, we review in detail the GT for the CVC-FP database constructed using the SGT tool. This GT not only contains the location of the architectural elements, but also those structural relations that we have considered to be of the interest for floor plan analysis systems. In the defi-nition of this database we have taken into account several considerations. We have contacted a team of architects to address their needs in automatic interpretation applications. We experienced several cooperations with research and pri-vate companies aiming for different applications related to floor plan interpretation. We have considered other floor plan definitions in the literature that entail some sort of structural understanding, such is the case of [ 48 ] for evacuation build-ing simulation, and [ 45 ] for structural floor plan retrieval. Additionally, we have also been inspired by the relevance of the structural information for high-level understanding in graphical documents, e.g., flowchart interpretation in patent documents [ 35 ]. Still, this is our own definition of the GT and it will vary for other applications, images, and experts. Obviously, since the SGT tool is shared freely, the GT data can be modified or upgraded agreeing to every system and person requirements.

Nine people working on distinct areas of graphics recog-nition have participated in the generation of this GT. Thanks to the version and user control of the SGT tool, the creation of the GT has been parallelized for the complete collection of images. Once the annotation has been completed, one sin-gle person has checked the correctness and consistency of the data according to the definitions settled a priori. This task has been pursued to correct different subjective perceptions for the distinct users that have participated. Since the SGT tool is designed in a way that every category and relation can be displayed with independence to the rest, this process has been easily attended. We firstly review the convention followed in the object labeling and secondly the relations instantiation. 4.1 Element labels Letusexplainhowweperformedthelabelingofthestructural symbols. These are rooms, walls, doors, windows, parking doors, and separations. The labeling of each object has been pursued by selecting that polygon that maximizes the over-lapping of its area; this is by selecting each of the extremities of the object.  X  Walls work mainly to bear the structure of buildings, to  X  The labeling of doors , windows , and parking doors has  X  The labeling of rooms sometimes encloses ambiguity as  X  Separations are rectangular abstract elements that sepa-4.2 Structural relations Similar to the object properties in ontologies, the SGT tool permits the definition of relations between object instances. In other words, the SGT tool allows to define attributed graphs to enclose the mutual dependences among the labeled elements. In these graphs, the annotated elements are the nodes, whereas the contextual relations among the different objects are defined by attributed edges. This fact enriches the expressiveness our GT and allows systems to learn complex features and affinities between elements. We have defined 5 following relations:  X  Incident Twoelementsarecalledtobe incident whenthey  X  Surround Several walls doors , windows , parking  X  Neighborhood Two rooms are called to be neighbors  X  Access This relation put in correspondence two rooms  X  Surrounding perimeter It defines the exterior bound-5 The CVC-FP images Let us now introduce the images in the CVC-FP database. This is a collection of real floor plan documents compiled and groundtruthed during the last recent years. It all started with the SCANPLAN 6 project in 2008, and still today the Document Analysis Group of the Computer Vision Centre is working on these graphical documents in multiple domains, such as structural analysis, semantic reasoning, and sym-bol spotting and recognition. The dataset is composed of 122 scanned documents and a partially groundtruthed ver-sion was presented in [ 14 ]. Nevertheless, these documents have been shared much before to foster the research in floor plan analysis [ 4 , 5 , 31 ].

The four sets have completely different drawing styles, image qualities and resolutions, and incorporate different sort of information. This is not an arbitrary fact; We have created a heterogeneous dataset to foster the creation of robust tech-niques that are able to deal with different image scenarios and graphical notations. It is important to take into account that different architects and architectural studios usually have their own graphical conventions. Therefore, there is a need of constructing systems that are able to learn each specific notation to be able to generalize for the existing architec-tural conventions. In addition to that, the different amount of images in each dataset permits to test the effectiveness of the proposed methodologies either when there is a large or a small set of documents available for learning purposes. We subsequently overview the characteristics of each subset sep-arately, focusing on the structural information of the images, their symbolism, and the textual information. 5.1 Black dataset Thenameofthissubset,astherestdoes,referencesthegraph-ical modeling of the walls, a thick black line as it can be seen in Fig. 8 . It consists of 90 floor plan binary images of good quality. The size of these images is 2,480  X  3,508 or 3,508  X  2,480 pixels depending on the orientation of the building. These plans were conceived to sample the structural distrib-utions of the buildings to possible customers, so they do not contain an excessive amount of technical information.
In this dataset, building drawings are centered and well oriented with respect to the document and most of the archi-tectural lines are parallel to the horizontal and vertical axes. They model the ground floor of detached houses, usually including terraces, porches, and garages with cars. The draw-ing style is clear, with few elements crossing among them. Concerning the structural symbols, walls are mostly modeled by black lines of three different thicknesses whether they are main, interior, or exterior walls. Just in three plans, walls are modeled by parallel lines. Simple doors are drawn by a quarter of a circle arc, whereas building X  X  main doors have an additional rectangular base of the size of their incident walls. Moreover, toilet doors are represented by a quarter circle arc, and double doors by two consecutive arcs cen-tered in each of the wall limits and tangent in the center of the accessible area (see Fig. 8 b). The window models can highly vary, see Fig. 8 c. We can find full opened windows, partly opened windows, and sliding windows, all of them with different thicknesses. The last of the structural symbols that appear in some of these images are the stairs. They are modeled by consecutive parallel rectangles. In terms of non-structural symbols, the floor plans contain mostly symbols making reference to bath utilities. Different kind of sinks, toilets, shower baths, and bathtubs are the only ones repeated in all the images. In addition to that, occasionally in some images, we can find living room furniture and in buildings delighting of a terrace or a porch may include a garden table with four chairs.

Text can be found in these documents. Each floor plan has a title with big bold letters that it can be read  X  X lan du Rez de Chause X  X , in English  X  X lan of the ground floor X . As a subtitle we find the scale of the model (always 1/100, 1cm is 1m) and information about the architectural studio. In some plans, next to the title we can find information about the surface area of the dependencies, the building utile area, and the slope of the roof. Less frequently, information of the surface and the orientation of the windows is included in the subtitle. Moreover, each room encloses the text describing its functionality and area X  X n squared meters. Finally, each plan has two dimensions measuring in meters the rectangular surface of the building. They are located in the limits of the building perimeter. 5.2 Textured dataset This is the second floor plan dataset compiled by the authors. It consists of ten poor quality and grayscale images whose resolutions can vary from 1,098  X  905 pixels the smallest to 2,218  X  2,227 the largest, see Fig. 9 a. They are computer drawings of detached houses containing not only structural symbols but also furniture, several dimension quotes, and textual information.

Here walls are modeled by two parallel lines with a diag-onal line pattern in between for the exteriors, and a hetero-geneous gray-dotted pattern for the interiors. The notation of doors and stairs is exactly the same of Black Dataset. Contrar-ily, all the windows follow a rectangular pattern of different breadths, which can be seen in Fig. 9 b. In this dataset, ter-races are indicated by a repetitive pattern of squares. Regard-ing non-structural symbols, we mainly can find sofas, tables, and bath and kitchen utilities such as sinks, baths, and ovens. Furthermore, most of the buildings have a garage with the drawing of a car in it.

This dataset contains textual information, most of it belonging to numbers of dimension measurements. All the rooms are labeled with their name and their area X  X n squared meters. Some plans have also a big bold text at the bot-tom of the image that says  X  X ue en plan X , in English  X  X loor plan X  X  view X . Additionally, some extra structural information is written in barely readable text. 5.3 Textured2 dataset The Textured2 dataset is composed by 18 images of 7,383  X  5,671 pixels collected from a local architectural project in Barcelona. The singularity of this dataset is that the 18 floor plans belong to a single building of six floors. The first image, which is shown in Fig. 10 a, contains the drawings of two different floors: the one corresponding to the ground floor, and just beside the overlapping of the first, the second, and the third floors, which are identical. Similar to the first, the second image contains the plans of the basement and the 4th floor. The rest of the images contain the same drawings butnotexactlylocatedandwithdifferentsortsofinformation. The two first contain general structural information; this is for instance the habitable area of each floor, the area of living rooms, and the area of the sleeping rooms. The second couple of images contain the detailed architectural dimensions. The thirdcoupleincludestheinformationofthesurfacematerials: whether the ground is made of parked or marble and the walls are covered by either plastering or natural stone. The fourth pair of images shows the distribution of the dropped or suspended ceiling. The fifth shows the plumbing distribution, whereas the sixth displays the waste plumbing distribution. In the seventh the building X  X  electrical installation is detailed. The eighth shows the gas installation and finally, the ninth is for heating installation.

The walls are modeled similar to the Textured Dataset, this time with a higher frequency diagonal pattern between the two parallel lines. Doors are drawn by 90  X  arcs and windows follow the same model. Mostly all the utilities and furni-ture symbols are drawn in the first couple of images: sinks, toilets, bathtubs, ovens, beds, tables, and wardrobes. Mean-while the rest of the images enclose the different types of symbols agreeing to their architectural purpose, see Fig. 10 b. The types of suspended ceilings are represented by differ-ent textural patterns, and water, electrical, gas, and heating symbolism is specified in their respective legends. Mean-while, textual information is omnipresent in all the images. Firstly, a text-table situated at the bottom right corner of each image specifies the information regarding the architectural studio, the project, and the document. Secondly, each doc-ument except for those enclosing the architectural dimen-sions contains a legend detailing the semantic meaning of the symbol encountered in the plan. Finally, every document has specific text in key positions to help its interpretation. This text includes for instance room X  X  naming, dimensions, floor statements, walls X  height, and facade orientation. 5.4 Parallel dataset This last collection is composed by only four images and was added to perform wall segmentation on walls drawn by sim-ple parallel lines. They are extracted from Google Images using the keyword floor plan and are created by one single architectural studio to display the building distribution of two detached houses for sale. An instance of this dataset is shown in Fig. 11 .

The binary images are of good quality and high resolution (2,550  X  3,300 pixels). As mentioned, walls are modeled by simple parallel lines, doors by a 90  X  arcs, and windows fol-lowing a rectangular model. Some house utilities are drawn, as the usual from bath and kitchen. Moreover, since the build-ings delight of a laundry room, washing and drying machines symbols can be found. Text also appears in these images. Each room has its functionality written in its perimeter. In addition, two of them, those belonging to the ground floor, have a text-table with the characteristics of the different sur-face areas X  X n squared feet. 6 Experiments Several floor plan analysis strategies have already been applied on parts of CVC-FP while it was under construc-tion. They have mainly been centered on wall segmentation and room detection tasks. On the one hand, rooms define the structure of the buildings, so their detection has been the main objective of these systems. On the other hand, the cor-rect segmentation of walls usually has lead to better room detection in most of the floor plan interpretation techniques.
This section, which is slight extension of the evaluation sectionin[ 14 ], has been included in this paper to give com-pleteness to the database explanation. It is divided in two main parts: the wall segmentation task and the room detec-tion task . Both evaluations have been performed at pixel level. Since the GT is at polygon level, we have used a JAVA TM script to convert every SVG file into several binary images (PNG format) X  X ne for each object category. Addi-tionally, we have performed a 1-way analysis of variance (ANOVA) to each task in order to asses the significant differ-ence on the performance behavior for the different floorplan datasets [ 11 ]. This test allows to reject the hypothesis that the reference method behaves similarly on every dataset when the output p value is lower than a fixed significance value  X  (usually  X  = 0 . 01 or  X  = 0 . 05). The evaluation scripts, the JAVA TM software to convert the SVG files into PNG for-mat, and the evaluation results for every image are publicly accessible at the project website. 6.1 Wall segmentation task 6.1.1 Wall segmentation evaluation protocol We evaluate the performance on wall segmentation using the Jaccard Index (JI). The JI score is used in PASCAL VOC competitions for object segmentation [ 20 ], and it counts the mislabeled pixels in the image. In this evaluation, true posi-tives (TP), false positives (FP), and false negatives (FN) are calculated only on the black pixels of the original images binarized as Table 1 specifies. The reason is that only black pixels covey relevant information on walls. The JI is calcu-lated as follows: JI = TP 6.1.2 Base-line results on wall segmentation The first attempt to detect walls on the Black dataset was pre-sented in [ 31 ]. They firstly spot lines using the Hough trans-form on vectorized images. The lines of interest are those that are longer over a threshold or fit alignment heuristics. Then, wall hypothesis is generated by seeking parallel lines. The ones totally filled by a black texture in between are con-sidered as walls of the floor plan.

Very differently, a much more straightforward strategy by minding wall thickness with respect to the rest of the lines was used in [ 4  X  6 ]. They divide the image lines in three different layers regarding their thickness using mathematic morphology. Lines on the medium and large layer correspond to walls and those in the large layer are part of the building boundary.

Finally, the authors of this paper presented two differ-entiated methods for segmenting walls independently to the graphical notation. In [ 15 ], a bag-of-patches approach is able to learn the graphical modeling of walls from few anno-tated images. Contrarily in [ 14 ], some general structural knowledge for walls modeling is used to segment possible instances. After that, the walls graphical modeling is learned out of these positive instances to spot the rest of the unrecog-nized walls.

A comparison for the latest wall segmentation strategies is shown in Table 2 . At a glance, all the methods behave robustly well for the Black dataset. However, for the rest of the datasets, which contain more complex notations, their performance drops to 70%. This is also validated by the ANOVA test, which returns a p value p  X  0 &lt; X  and states that the performance of the method is significantly different regarding the dataset. 6.2 Room detection task 6.2.1 Room detection evaluation protocol We based the performance evaluation for room detection on the protocol of Phillips and Chhabra [ 34 ], which was first introduced in [ 31 ]. This protocol searches for the best align-ment between the rooms segmented and the ones in the GT and allows to report the exact and partial matches.
First of all we create a match _ score table where rows represent the rooms segmented by the system and columns are the rooms in the GT. Each table position ( i , j ) speci-fies the overlapping between the segmented room i and the groundtruthed room j . It is calculated as: match _ score ( i , j ) =
In the match _ score table, a one 2 one /exact match is given when the overlapping score in ( i , j ) overcomes an accep-tance threshold , and the rest of the row and column are below a rejection threshold . This means that the room segment i matches with groundtruth room j and does not match with any other. Both thresholds are set as in [ 5 , 6 , 31 ] to perform fair comparison: This is to 0.5 and 0.1, respectively. Then, the partial matches are calculated as it is described in [ 34 ] and they are divided into the following categories:  X  g _ one 2 many : A room in the groundtruth overlaps with  X  g _ many 2 one : More than one room in the groundtruth  X  d _ one 2 many : A detected room overlaps with more than  X  d _ many 2 one : More then one detected room overlaps
Finally, the detection rate (DR), the recognition accuracy (RA), and the one 2 oner at e are calculated as follows: DR = RA = one 2 one r at e = where N and M are the total number of groundtruth and detected rooms, respectively. 6.2.2 Base-line results on room detection Rooms in [ 31 ] are detected following a shape partitioning strategy. This top-down strategy progressively splits the geo-metric space into convex polygons using the method pro-posed in [ 27 ]. Room boundaries are potentially undetected walls, doors, or windows. A postprocessing step [ 23 ]is applied at the end to reduce oversegmentation issues.
In [ 4  X  6 ], room detection is divided into three dif-ferent steps: information segmentation, structural analy-sis, and semantic analysis. The first segmentation steps involve the separation between text and graphic informa-tion, and the segmentation of walls. In the structural analy-sis, they approximate the wall polygons out of the bor-der segments extracted by Suzuki and Be [ 42 ]. Then, the gaps between walls from potential doors, windows, and gates are closed by horizontal and vertical smear-ing. Finally, in the semantic analysis, doors, windows, and rooms are extracted from the images. Here, doors and windows X  X hich are represented by arcs X  X re detected using the SURF detector [ 9 ]. Rooms are closed white con-nected components within the dwelling boundary. They are eventually labeled by applying an OCR on the textual layer.

The authors in [ 14 ] divide the room detection into two sequential steps: (i) a statistical wall, window, and door seg-mentation and (ii) a structural wall and room recognition. In (i) a bag-of-patches approach based on [ 15 , 16 ] learns the graphical appearance and detects the potential areas where walls, doors, and windows are likely to be. Then, in (ii) it uses an own adaptation of the A* algorithm that searches well-aligned individuals in the vectorized original image. Finally, it recognizes the rooms of the building by finding closed regions using [ 25 ].
 We compare the existing room detection strategies in Table 3 . To do so, we have extended the results in [ 14 ] to the Textured2 and Parallel datasets. According to this table, the different strategies tend to perform better in terms of RA for the Black and Textured datasets. This is caused because in the Textured2 and Parallel datasets, the system [ 14 ] falls into overdetection X  X he one 2 one rate is lower and one 2 many count is much higher. This overdetection is mainly produced because the system relies on a simple heuristic based on relative orientation angles and distances to decide which walls, doors, and windows are correctly aligned. Moreover, in the Textured2 dataset, this overdetec-tion is aggravated by the fact that the images always con-tain two aligned flats of the building. The system enforces considering the two flats as one single building and thus tries to align possible rooms in the empty gap between the two drawings. This issue is shown in Fig. 12 a. Differ-ently, in the Parallel dataset, the oversegmentation of walls produces multiple possible room alignments, as it can be seen in Fig. 12 b. With all, the high scores on DR for all the methods demonstrate that almost all of the rooms are detected for all the datasets. This conclusion is also sup-ported by the ANOVA test. The p value obtained for the RA measure rejects the hypothesis that the method behav-ior is similar on every dataset. Contrarily, the p value for the DR measure is p = 0 . 026. Depending on the significance level considered, this hypothesis cannot be rejected. 7 Conclusion and future work Recent results on structured learning methods have shown the impact of structural information in the performance of a wide range of pattern recognition tasks. Yet, these techniques usually need conveniently annotated databases to learn the interrelation between the objects of interest. In this paper, we have presented the CVC-FP database. It is composed of real floor plan documents that are fully annotated for architectural symbolsandmakespecifictheirstructuralinterrelations.This sort of information will let floor plan analysis systems to learn directly from observable data how the elements are structurally arranged and thus to trigger better interpretation. We have additionally presented a quantitative benchmark on the CVC-FP for two different tasks: wall segmentation and room detection.

The groundtruthing tool used to generate this database, the so-called SGT tool, is a general purpose groundtruthing tool. It is a web-based service that permits to create own objects classes and relations using a very intuitive user interface. This tool fosters the collaboration by allowing standalone and multi-user handling, including user and version control. Thus, the SGT tool is suitable for the creation, the upgrade, and the maintenance of databases in domains where mak-ing specific additional structural information can be of great interest.

The CVC-FP database, the SGT tool, and the evaluation scripts are freely released to the research community to ease comparisons and boosting reproducible research.

Regardingfuturework,inashort-middletermweareplan-ning to upgrade the SGT tool to allow the arrangement of object classes and relations in a taxonomic way. The aim is to organize and facilitate complex labeling procedures and to foster the reutilization by defining formally the groundtruth domain. This taxonomy will be defined by either creating a class and relation hierarchy in the SGT tool, or upload-ing a formal ontology definition. The SGT tool will be able to export the GT in an ontological framework for further semantic reasoning, e.g., to check the groundtruth consis-tency according its definition. In a longer term, we plan to include this ontological functionality to the tool. Hence, the SGT tool will be able to help correct and make suggestions to the user at real time.
 References
