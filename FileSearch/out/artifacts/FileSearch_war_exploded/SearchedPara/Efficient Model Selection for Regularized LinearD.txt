 Classical Linear Discriminant Analysis (LDA) is not ap-plicable for small sample si ze problems due to the singu-larity of the scatter matrices involved. Regularized LDA (RLDA) provides a simple strategy to overcome the singu-larity problem by applying a regularization term, which is commonly estimated via cross-validation from a set of can-didates. However, cross-validation may be computationally prohibitive when the candidate set is large. An efficient al-gorithm for RLDA is presented that computes the optimal transformation of RLDA for a large set of parameter candi-dates, with approximately the same cost as running RLDA a small number of times. Thus it facilitates efficient model selection for RLDA.
 An intrinsic relationship between RLDA and Uncorrelated LDA (ULDA), which was recently proposed for dimension reduction and classification is presented. More specifically, RLDA is shown to approach ULDA when the regularization value tends to zero. That is, RLDA without any regular-ization is equivalent to ULDA. It can be further shown that ULDA maps all data points from the same class to a com-mon point, under a mild condition which has been shown to hold for many high-dimensional datasets. This leads to theoverfittingprobleminULDA,whichhasbeenobserved in several applications. The theoretical analysis presented provides further justification for the use of regularization in RLDA. Extensive experiments confirm the claimed the-Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00. oretical estimate of efficiency. Experiments also show that, for a properly chosen regularization parameter, RLDA per-forms favorably in classification, in comparison with ULDA, as well as other existing LDA-based algorithms and Support Vector Machines (SVM).
 Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications -Data Mining General Terms: Algorithms Keywords: Dimension reduction, Linear Discriminant Anal-ysis, regularization, model selection
Linear Discriminant Analysis (LDA) is a well-known clas-sification method that project s high-dimensional data onto a low-dimensional space where the data is reshaped to max-imize class separability [7, 9, 15]. The optimal projection or transformation in classical LDA is obtained by minimizing the within-class distance and maximizing the between-class distance simultaneously, thus achieving maximum discrimi-nation. Classical LDA involves three scatter matrices, i.e., the within-class, between-cla ss, and total scatter matrices. The total scatter matrix is a multiple of the sample covari-ance matrix and is required to be nonsingular. However, in many applications such as text mining, microarray data classification, and face recognition, all scatter matrices in question can be singular since the data points are in a very high-dimensional space and the sample size does not exceed this dimension. This is known as the singularity or under-sampled problem [17].

Regularized LDA (RLDA) provides an effective solution for the singularity problem. The idea is to add a constant  X  to the diagonal elements of the total scatter matrix, where  X &gt; 0isknownasthe regularization parameter . Regular-ization stabilizes the sample covariance matrix estimation and improves the classification performance of LDA. RLDA has applications in many areas, including face recognition [4, 18], microarray classification [11], medical image analy-sis [6], etc.

Choosing an appropriate regularization value is a critical issue in RLDA, as a large  X  may significantly disturb the information in the scatter matrix, while a small  X  may not be effective enough to solve the singularity problem. Cross-validation is commonly used to estimate the optimal  X  from a finite set,  X  = {  X  1 ,  X  X  X  , X  m } ,of m candidates. Selecting an optimal value for a parameter such as  X  is called model selection [15]. The computational cost of model selection for RLDA can be high, especially when m is large, since it requires expensive matrix computations for each  X  i  X   X . However, a large m is often desirable in practice to obtain a good  X  .
Besides regularized LDA, other methods have been brought to bear on such high-dimension al, small sample size prob-lems, including Penalized LDA (PLDA) [12], Diagonal LDA (DLDA) [5], Uncorrelated LDA (ULDA) [16], Orthogonal LDA (OLDA) [24], and PCA+LDA [2], where PCA stands for Principal Component Analysis. Many of these LDA-based methods have the same computational cost. We show an interesting relationship between RLDA and ULDA in this paper. This relationship imp lies that single-model RLDA (with m = 1) and ULDA are of the same time complexity. Thus our experimental studies on efficiency concentrate on the comparison between single-model RLDA and multiple-model RLDA (with m&gt; 1).

From the perspective of computing the discriminant score for classification, rather than feature extraction, Hastie et al. [14, 11] proposed an efficient algorithm for RLDA. However, these algorithms tend to be numerically unstable when the regularization parameter  X  is close to 0. Friedman [8] consid-ered a more general formulation of RLDA and proposed an efficient algorithm when leave-one-out cross-validation was applied. It did not address the high computational cost as-sociated with estimating the best regularization parameter from a large set of candidates, which has recently been ad-dressed in [25].

Regularization is the key to many other machine learn-ing methods such as Support Vector Machines (SVM) [22], spline fitting [23], Quadratic Discriminant Analysis (QDA) [8], etc. The tuning of the regularization parameter also consumes time in SVM training. Hastie et al. [13] proposed an algorithm for SVM, which fits the entire path of SVM so-lutions for every value of the regularization parameter, with essentially the same computational cost as fitting one SVM model. This dramatically reduces the computational cost of model selection in SVM training.
This paper aims to reduce the computational cost of the regularized LDA approach on high-dimensional, small sam-ple size problems. The primary contributions of this work include the following:
Besides the theoretical results that guarantee the efficiency of the proposed model selection algorithm for RLDA, experi-ments on computational efficiency confirm our theoretically-established bounds. Moreover, our experiments also show favorable performance of the algorithm in terms of classifi-cation, in comparison of several other LDA-based methods and SVM. In summary, we propose in this paper a new im-plementation of RLDA, which allows model selection to be optimized over a large set of candidates with low computa-tional effort.

The rest of the paper is organized as follows. An overview of classical LDA and regularized LDA is given in Section 2. The essential regularization property of Regularized LDA is presented in Section 3. Efficient RLDA algorithms are described in Section 4. Section 5 includes the experimental results. We conclude in Section 6.
We briefly review the classical LDA formulation and the regularized LDA formulation in this section.
Given a data matrix A  X  IR d  X  n , classical LDA computes a linear transformation G  X  IR d  X  that maps each column a of A ,for1  X  i  X  n ,inthe d -dimensional space to a vector y in the -dimensional space: G : a i  X  IR d  X  y i = G T a i IR ( &lt;d ).
 Let the data matrix A be partitioned into k classes as A =[ A 1 ,  X  X  X  ,A k ], where A i  X  IR d  X  n i ,and k i =1 discriminant analysis [9], three scatter matrices, i.e., within-class , between-class ,and total scatter matrices are defined as follows: where the centroid c ( i ) of the i -th class is defined as c centroid c is defined as c = 1 n Ae with e =(1 , 1 ,  X  X  X  , 1) IR n . It follows from the definition that S t = S b + S w . Define the matrices
Then the three scatter matrices: S w , S b ,and S t in Eqs. (1) X  (3) can be expressed as In the lower-dimensional space resulting from the linear trans-formation G , the scatter matrices S w ,and S b ,and S t be-come G T S w G , G T S b G ,and G T S t G , respectively. An opti-mal transformation G  X  in classical discriminant analysis can be computed by solving the following optimization problem [9]: whichcanbeshowntobeequivalentto using the following equality: S t = S b + S w .

The optimization problem in Eq. (9) is equivalent to find-ing x that satisfies S b x =  X S t x ,for  X  = 0 [9]. The solution can be obtained by applying an eigen-decomposition on the matrix S  X  1 t S b ,if S t is nonsingular. Note that there exist no more than k  X  1 eigenvectors corresponding to nonzero eigenvalues, since the rank of the matrix S b is bounded from above by k  X  1. Therefore, the reduced dimension of classical LDA is at most k  X  1.

Classical LDA is equivalent to maximum likelihood clas-sification assuming normal distribution for each class with the common covariance matrix. Although relying on heavy assumptions which are not true in many applications, LDA has been proved to be effective. This is mainly due to the fact that a simple, linear model is more robust against noise, and most likely will not overfit.
Classical discriminant analysis requires the total scatter matrix S t to be nonsingular, which may not hold for small sample size data. A simple way to deal with the singularity of S t is to apply regularization, by adding some constant value to the diagonal elements of S t as  X  S t = S t +  X I some  X &gt; 0, where I d is the identity matrix of size d .Since S is positive semi-definite, S t +  X I d is positive definite [10], and hence nonsingular.

An optimal transformation of RLDA can be computed by solving the following optimization problem: G  X  = arg max Similarly, the solution to Eq. (10) can be achieved by com-puting the eigen-decomposition of ( S t +  X I )  X  1 S b .Thecom-putation can be divided into two stages. First the Singular Value Decomposition (SVD) [10] of H t in Eq. (6) is com-puted: where  X  U  X  R d  X  d and  X  V  X  R n  X  n are orthonormal square matrices, and  X   X   X  R d  X  n is diagonal. It follows from Eq. (7) that Therefore of  X 
 X  l is large. However, we will show that  X   X  l can be replaced by a much smaller matrix whose size is the same as the rank of S t , a number much smaller than d for high-dimensional, small sample size data.

Next, let U b  X  b V T b be the SVD of where H b is defined in Eq. (5). Let Then from Eq. (7), we have
The optimal transformation G for RLDA consists of the first q columns of X ,where q =rank( S b ).

The time complexity of the above algorithm is O ( nd 2 ), which can be expensive for high-dimensional data. Espe-cially, when v -fold cross-validation ( v =5inourexperi-ments) is performed for choosing the best  X  ,theaboveal-gorithm needs to be repeated v times. It is often compu-tationally prohibitive, and thu s restricts the application of RLDA to data of small size.

Remark 2.1. Note that in the traditional RLDA formu-lation, S w is applied instead of S t as in Eq. (10). The reg-ularization value  X  is thus required to be positive and the algorithm is subject to numerical instability problems, when  X  is close to 0. The modified formulation for RLDA used in this paper overcomes this limitation, by showing that the limit of the solution to RLDA exists, when  X   X  0 ,andis equal to ULDA. More details can be found in Section 4.4. In this section, we present a key property of regularized LDA, establishing that regularizing the total scatter matrix S , in the context of LDA, is equivalent to regularizing the nonzero eigenvalues of S t . This property has significant im-plications in designing an efficient RLDA algorithm for small sample size problems.

The result of this section is motivated by the following fact: if the rank of H t is r ( r d ), the orthonormal matrices in the SVD decomposition have redundant columns. Only the first r columns of  X  U and  X  V corresponding to the first r rows and the first r columns in  X   X  where diagonal entries are nonzero play a role in the reconstruction of H t .Let U  X  IR d  X  r and V  X  IR d  X  r denote the first r columns of and  X  V , respectively. Let the square matrix  X  consist of the first r rows and the first r columns of  X   X . Then we have It is clear that However, our main result of this section shows that One of the basic tools used in our proof is the Sherman-Woodbury-Morrison formula [10]: Let P  X  IR d  X  d ,and Q, R IR d  X  n . Assuming that both the matrices P and ( I + R T P are nonsingular, we have
Proposition 3.1. Let the scatter matrices S t and S b be defined as above, and let U  X  V T be the skinny SVD of H t where U  X  IR d  X  r and V  X  IR n  X  r have orthonormal columns,  X   X  IR r  X  r is diagonal, and r = rank ( S t ) . We have where d is the dimension of data points, and U  X  is the or-thogonal complement of U .
 Proof. From H t = U  X  V T ,wehave Substituting P =  X I d ,and Q = R = U  X  into the Sherman-Woodbury-Morrison formula as in Eq. (12), we have
Note that  X ( I r +  X   X  1  X  2 )  X  1  X  is a diagonal matrix. Using the equality we can show that and thus
Note that U  X   X  IR d  X  ( d  X  r ) is the orthogonal complement of U ,thatis,[ U, U  X  ]  X  IR d  X  d is orthogonal. Using the fact that we have and the result follows by multiplying by S b on both sides.
The computation of the transformation G via the eigen-decomposition of the matrix in Eq. (13) may be sensitive to numerical disturbances as  X   X  0, due to the presence of  X  in the second term. Interestingly, it can be overcome using the result in the following lemma:
Lemma 3.1. Let U  X  , S t ,and S b be defined as above. Then, the null space of S t , denoted as Null ( S t ) is a subset of the null space, Null ( S b ) ,of S b . That is, Null ( S t )  X  Furthermore, U T  X  S b =0 .
 Proof. The proof directly follows from the fact that S t = S + S w ,andboth S b and S w are positive semi-definite.
With Proposition 3.1 and Lemma 3.1, we have the follow-ing main result of this section:
Theorem 3.1. Let S t , S b , U , V ,  X  , d ,and r be defined as in Proposition 3.1. Then for any  X &gt; 0 , the following equality holds: Theorem 3.1 implies that regularizing the total scatter S , in the context of LDA, is equivalent to regularizing the nonzero eigenvalues of S t . The eigenvectors of  X  S  X  1 be computed as follows:
Theorem 3.2. Let y be an eigenvector of  X  S  X  1 sponding to a nonzero eigenvalue  X  ,then y = Ux for some x ,where x is an eigenvector of ( X  2 +  X I r )  X  1 U T S b
Proof. Let y be an eigenvector of  X  S  X  1 to a nonzero eigenvalue  X  . Then for some x .
 Next, we show that x is an eigenvector of ( X  2 +  X I r )  X  1 Multiplying both sides of the following equation by U T : we have This completes the proof of the theorem.

Denote  X   X  s = X  2 +  X I r . In contrast to  X   X  l used in tradi-tional RLDA, as given in Eq. (11), the size of  X   X  s is typi-cally small for high-dimensional small sample size data. So is the size of  X   X   X  1 s U T S b U , an important matrix introduced in Theorem 3.2. It is also worth noting that computing U is independent of the regularization value  X  .Thus,Theo-rem 3.2 leads to a two-step computation of the eigenvectors of  X 
In the following, we describe an efficient way of computing the eigenvectors of  X   X   X  1 s U T S b U ,whichis since S b = H b H T b as in Eq. (7), where H b is defined in Eq. (5). Let U b  X  b V T b be the SVD of  X   X   X  1 / 2 s U have That is,  X   X   X  1 / 2 s U b diagonalizes the matrix  X   X   X  1 the columns of  X   X   X  1 / 2 s U b form the eigenvectors of The above computation is more efficient than directly ap-of  X 
 X   X  1 / 2 s U T H b is much smaller (i.e., r  X  k ).
In this section, we present efficient RLDA algorithms for both the single model, where |  X  | = 1 ( X  is the candidate set for regularization) and the multiple model, where |  X  | &gt; 1.
Given a fixed training dataset and a fixed regularization value  X  , our proposed single-model RLDA algorithm is sum-marized in Algorithm 1 below.
 The time complexity of this algorithm is dominated by Lines 2 and 3. For small sample size problems, the cost of Lines 4-6 is significantly smaller than the cost of Line 2. (More details will be given in Section 4.2.) Note that Lines 2 and 3 are independent of  X  . This observation is the key for the efficient multiple-model RLDA algorithm proposed next.

Let  X  = {  X  1 ,  X  X  X  , X  m } be the candidate set for the regular-ization  X  . In multiple-model RLDA, v -fold cross-validation is applied, where the data is divided into v subsets of (ap-proximately) equal size. All subsets are mutually exclusive, andinthe i -th fold, the i -th subset is held out for test and all other subsets are used for training. For each  X  j j =1 ,  X  X  X  ,m , we compute the cross-validation accuracy, Accu( j ), defined as the mean of the accuracies for all folds. The best regularization value  X  j  X  is the one with The pseudo-code for multiple-model RLDA is given in Al-gorithm 2 . Note that the k -nearest neighbor ( k = 1), called 1-NN, is used for classification as in [24].
Line 4 takes O ( n 2 d ) time for the SVD computation. Lines 5 and 6 take O ( drk )and O ( rdn ) time, respectively, for the matrix multiplication. For each choice  X  j ,Line9and10 take O ( rk 2 ) time for the eigen-decomposition and matrix multiplication. Line 11 takes O ( krn ) time for the matrix multiplication. The computation of the classification accu-racy by 1-NN in Line 12 takes O ( n 2 k ) time. Thus, the total time complexity, T ( m ), for estimating the best parameter is We can compare T ( m )with T (1), where m =1,andobtain For small sample size problems, where the number, k ,of classes is much smaller than the dimension d , i.e., k d , the overhead of estimating the optimal regularization value among a large set is small.

Uncorrelated LDA (ULDA) [24] was proposed for feature extraction on small sample size problems. One key property of ULDA is that the features in the transformed space are uncorrelated, thus ensuring minimum redundancy among the features in the reduced space. It was shown [24] that the transformation by ULDA consists of the first q eigen-vectors of S + t S b , where q =rank( S b ). Interestingly, we can show that the limit of RLDA when  X   X  0isequivalentto ULDA based on the following lemma:
Theorem 4.1. Let S t and S b be defined as above and  X &gt; 0 .Then Proof. The proof follows directly from Theorem 3.1.
Remark 4.1. Theorem 4.1 implies that the range of the parameter  X  in RLDA is [0 ,  X  ) . Note that for traditional RLDA algorithms, the range of  X  is [  X ,  X  ) for some positive  X  .When  X  is close to 0, these algorithms tend to have nu-merical instability problems, since they follow the optimiza-tion problem in Eq. (8), while the proposed RLDA algorithm follow the one in Eq. (9).

Theorem 4.1 implies that ULDA is a special case of RLDA when  X  = 0. With a properly chosen  X  through multiple-model RLDA in Section 4.2, RLDA is expected to outper-form ULDA, which is confirmed by the empirical results pre-sented in the next section. It was shown [24] that ULDA has the same computational cost as many other competi-tive LDA methods, including Orthogonal LDA. So in our experimental study on efficiency, we will concentrate on the comparison between the single-model RLDA and multiple-model RLDA.

One interesting property of ULDA [26] is that under a mild condition that the optimal transformation G lies in the null space of the within-class scatter matrix, that is, G T S w =0. Ithas been shown [26] that the condition in Eq. (15) holds for many high-dimensional data, including most datasets used in our studies in Section 5. We show in the following that if G
T S w = 0, then ULDA maps all points from the same class to a common vector.

Proposition 4.1. Let G be the transformation in ULDA, and let x be a data point from the i -th class. Assume G T class. That is, all data points from the i -th class are mapped to the common vector G T c ( i ) .

Proof. Since G T S w =0,wehave where H w is defined as in Eq. (4): where A i is the data matrix of the i -th class, and e ( i vector of all ones. It follows from Eq. (16) that G T H w Considering the i -th block of G T H w ,wehavethat Hence, G T x = G T c ( i ) ,foreachcolumn x in A i .Thiscom-pletes the proof of the proposition.
 Proposition 4.1 above shows that under a mild condition, ULDA maps all points from the same class to a common point. This leads to perfect separation between different classes, however, this may also lead to overfitting. The prob-lem may be worse especially when the data is noisy. Regu-larization applied in RLDA is thus expected to alleviate this problem, provided that a good regularization parameter can be estimated.
In this section, we experimentally evaluate the perfor-mance of RLDA. All of our experiments are performed on a P4 2.80GHz Linux machine with 1GB memory. As in [5], the data is randomly partitioned into a training set consist-ing of two-thirds of the whole set and a test set consisting of one-third of the whole set. 1-Nearest-Neighbor (1-NN) algorithm is applied for classification. To give a better es-timation of accuracy, the splitting is repeated 50 times and the resulting accuracies are averaged.
We use three types of data in our studies: text documents (Doc1 and Doc2), gene expression data (GCM and ALL), and face images (ORL and PIX). www.research.att.com /  X  lewis The statistics of the test data sets are summarized in table 1. In this experiment, we test the efficiency of multiple-model RLDA. Table 2 shows the computational time (in seconds) of RLDA on different m , i.e., the size of  X , ranging from 1 to 1024. It is clear that the cost of multiple-model RLDA grows slowly as m increases. For example, we can observe that T (1024) /T (1) on different datasets ranges from 1.35 to 4.42. Among the six datasets, the two image datasets have relatively larger increasing rate than the others. Note that the number of classes for the image datasets is relatively larger than the others, and so is the ratio k/d . The exper-imental results on efficiency evaluation are consistent with the theoretical estimation given in Section 4.2.
Inthisexperiment,weevaluateRLDAinclassification and compare it with other three LDA-based methods: ULDA, DLDA [5], and OLDA [24], as well as SVM 4 .Theresults are summarized in Table 3. We set m to be 1000 for all cases. We have found that using a small value of m usu-ally degrades the classification performance for our datasets. This confirms the effectiveness of using large candidate set to choose a good regularization value.

The results in Table 3 show that RLDA is competitive with other methods in classification. RLDA outperforms ULDA in most cases. For Doc1, GCM, and ORL, RLDA outperforms ULDA by a large margin. Interestingly, ULDA www.uk.research.att.com/facedatabase.html peipa.essex.ac.uk/ipa/pix/f aces/manchester/test-hard/
Linear SVM is used as it is shown to be comparable to non-linear SVM using kernels [3, 21] in most cases due to the high dimensionality of the data. The value of the regularization parameter in SVM is estimated through cross-validation. has a better performance than RLDA for Doc2. However, the difference is not significant. Recall from Section 4.4 that ULDA is equivalent to RLDA with  X  =0. Theexperimental results further confirm the effectiveness of choosing the best  X  from a large set of candidates.

OLDA appears to be very competitive with RLDA in many cases. However, there exist certain cases (such as the Doc1 dataset), where the difference is significant. It appears that RLDA, with a sufficiently large set of regularization candidates, is more robust to the diversity of training data than other LDA-based methods. Overall, RLDA is compet-itive with SVM.
An efficient algorithm for RLDA is proposed for small sample size problems. A key advantage of the proposed al-gorithm is that the optimal transformation of RLDA for a set of different regularization values can be computed with approximately the same cost as running the RLDA algo-rithm a very small number of times. Thus it dramatically reduces the computational cost for RLDA.
 We analyze the intrinsic relationship between RLDA and ULDA. More specifically, we show that RLDA without any regularization is equivalent to ULDA, while ULDA maps all data points from the same class to a common point, under a mild condition which has been shown to hold for many high-dimensional data. The theoretical analysis presented provides insights on the use of regularization in RLDA. Ex-periments on a variety of data show that RLDA is com-petitive with several other LDA-based methods and SVM, in terms of classification, which shows the effectiveness of regularization applied in RLDA.

Discriminant analysis can also be studied in the non-linear fashion, so-called kernel discriminant analysis [1, 18, 19]. It is desirable if the data has weak linear separability. One of the directions for future work is to extend the current work to the nonlinear case. [1] G. Baudat and F. Anouar. Generalized discriminant [2] P.N. Belhumeour, J.P. Hespanha, and D.J. Kriegman. [3] N. Cristianini and J.S. Taylor. Support Vector [4] D.Q. Dai and P.C. Yuen. Regularized discriminant [5] S. Dudoit, J. Fridlyand, and T. P. Speed. Comparison [6] M. Dundar, G. Fung, J. Bi, S. Sathyakama, and [7] R.A. Fisher. The use of multiple measurements in [8] J.H. Friedman. Regularized discriminant analysis. [9] K. Fukunaga. Introduction to Statistical Pattern [10] G. H. Golub and C. F. Van Loan. Matrix [11] Y. Guo, T. Hastie, and R. Tibshirani. Regularized [12] T. Hastie, A. Buja, and R. Tibshirani. Penalized [13] T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. The cies from fifty runs are reported.
 DLDA b OLDA b SVM b and Support Vector Machines, respectively. [14] T. Hastie and R. Tibshirani. Efficient quadratic [15] T. Hastie, R. Tibshirani, and J.H. Friedman. The [16] Z. Jin, J.Y. Yang, Z.S. Hu, and Z. Lou. Face [17] W.J. Krzanowski, P. Jonathan, W.V McCarthy, and [18] J. Lu, K.N. Plataniotis, and A.N. Venetsanopoulos. [19] S. Mika, G. R  X  atsch, J. Weston, B. Sch  X  olkopf, and [20] S. Ramaswamy et al. Multiclass cancer diagnosis using [21] S. Sch  X  olkopf and A. Smola. Learning with Kernels: [22] V.N. Vapnik. Statistical Learning Theory . Wiley, New [23] G. Wahba. Spline Models for Observational Data . [24] J. Ye. Characterization of a family of algorithms for [25] J. Ye and T. Wang. Regularized discriminant analysis [26] J. Ye and T. Xiong. Null space versus orthogonal [27] C.H. Yeang et al. Molecular classification of multiple [28] E.J. Yeoh et al. Classification, subtype discovery, and
