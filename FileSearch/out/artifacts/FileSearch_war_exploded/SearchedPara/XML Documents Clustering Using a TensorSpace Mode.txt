 Rapid growth of web technologies has witnessed a sudden surge in the num-ber of XML (eXtensible Markup Language) documents. For instance, English Wikipedia contains 3.1 million web documents in XML format; the ClueWeb dataset, used in Text Retrieval Conference (TREC) tracks, contains 503.9 million XML documents collected from the web in January and February 2009. The majority of existing XML document clustering methods utilize either the structure features [2] or the content feat ures present in the documents. Cluster-ing methods utilizing only the content features of the documents consider the documents as a  X  X ag of words X  or a Vector Space Model (VSM) and ignore the structure features [2]; clustering metho ds utilizing only the structure features of the documents represent each document a s a set of paths (sequences) or trees.
However, these methods, with their single-feature focus, tend to falsely group documents that are similar in for documents that are similar in both features. To correctly identify simila rity among documents, the clustering process should use both their structure and their content information. Approaches on clustering both the structure and the content features of the XML documents are limited. Approaches using the VSM often fail to scale for even small collections of a few hundred documents, and in some situations have resulted in poor accuracy [14]. VSM cannot model both structure a nd content features of XML documents effectively as the mapping between the str ucture and its corresponding content is lost. The content and structure features inherent in an XML document should be modeled in a way that the mapping between the content of the path or tree can be preserved and used in further analysis. In this paper we propose a novel method that represents the XML documents in a Tensor Space Model (TSM) and uses the TSM for clustering. In the TSM, storing the content corresponding to its structure helps to analyze the relat ionship between structure and content.
Unlike the VSM, which uses a vector to model, TSM is based on the multi-linear algebraic character level high-order tensors (generalization of matrices) [13]. Decomposition algorithms are used to analyze the relationships between various tensor orders (ways or modes). H owever, existing decomposition algo-rithms could be used to analyze small size and sparse TSMs. TSMs that are large and dense cannot be loaded into memory. Consequently, large datasets with tensor representation cannot be analyzed using these decomposition tech-niques. In this paper, we propose a randomized tensor decomposition technique that could upload the large size tensors into memory and decompose them with significant speedups. Experiments on a real-life dataset containing more than 50K documents show that the proposed method helps to improve the cluster quality through the enriched document representation of both structure and content information. The contributions of this paper can be summarized as: (1) a clustering method, XML document Clustering with TSM (XCT), that utilizes the tensor model to efficiently combine the content and structure features of XML documents; and (2) a new tensor decomposition algorithm, Progressive Tensor Creation and Decomposition (PTCD), for large sized tensors. Tensor Space Modeling (TSM) has been successfully used in representing and analyzing multi-way data in signal processing, web mining and many other fields [13]. Tensor clustering is a multi-way data analysis task which is currently gain-ing importance in the data mining community. The simplest tensor clustering scenario, co-clustering or bi-clustering, in which two orders are simultaneously clustered, is well establis hed [6]. Another recently pro posed approximation based Combination Tensor Clustering algorithm [7] clusters along each of the orders and then represents the cluster centers in the tensor. These co-clustering tech-niques capture only the 2-way relationships among the features and ignore the dependence of multiple orders in clustering: this may result in loss of information while grouping the objects.

Several decomposition algorithms, such as Higher Order SVD (HOSVD); CP, a higher-order analogue of Singular Value Decomposition (SVD) or Principal Component Analysis (PCA); Tucker and Multi-Slice Projection, have been re-viewed in detail in [5]. Incr emental Tensor Analysis (ITA) methods [8] have been proposed recently to detail with large data sets for efficiently decomposing sparse tensors ( density  X  0 . 001%). However, real-life XML documents represented in TSM are dense with about 127M non-entries with over 1M terms and these de-composition algorithms fail to scale. MET [9], a memory-efficient implementation of Tucker proposed to avoid the intermediate blow-up in tensor factorization, is shown in our results shows not to scale to our medium-sized and large-sized datasets. In MACH [13], a recently propo sed random decompos ition algorithm suitable for large dense datasets, the number of entries in the tensor is randomly reduced using Achlioptas-McSherry X  X  technique [1] to decrease the density of the dataset. However, as discussed in section 5, MACH often ignores smaller length documents and tends to group most of the smaller length documents in a single cluster in spite of differences in their structure and content. To remove this lack of decomposition algorithms suitable for very large-sized datasets, in this paper we propose a new decomposition algorithm, the Progressive Tensor Creation and Decomposition (PTCD) algorithm, that progressively unfolds a tensor into a matrix and applies SVD on this generated matrix. 3.1 Problem Definition and Preliminaries Let there be a collection of XML documents D = { D 1 ,D 2 ,...,D n } ,where D i is an XML document containing tags and data enclosed within those tags. The structure of D i can be defined as a list of tags showing the hierarchical relationships between them. The structure of D i is modeled as a rooted, ordered and node-labeled document tree, DT i =( N, n 0 ,E,f ), where (1) N is the set of nodes that correspond to tags in D i , with the node labels corresponding to tag names; (2) n 0 is the root node which does not have any edges entering in it; (3) E is the set of edges in DT i ;and(4) f is a mapping function f : E  X  N  X  N . Previous research has shown that, in a dataset, only the content constrained within the concise common or frequen t subtrees (Closed Frequent Induced -CFI ) can be used to group the documents, rather than the entire content of the XML documents [10]. Therefore the proposed XCT method generates these CFI subtrees to represent the common subt rees in the dataset and uses these CFI subtrees to extract the content of the documents corresponding to them. The process begins by identifying the s ubtrees that belong s to a document tree. A subtree CFI j  X  CFI is present in document tree DT i ,if CFI j preserves the same parent-child relationship as that of DT i . The document content( or structure-constrained content ) contained within the CFI j subtree in DT i ,noted as C ( D i ,CFI j ), is retrieved from the XML document D i , a collection of node values or terms. The node value of a node (or tag) of a CFI j , C ( N i )in D i is a vector of terms, { t 1 ,...,t k } that the node contains. The term t is obtained after stop-word removal and stemming.

The next step involves modeling the derived structure and content features of a tensor model. Firstly,the tensor notations and conventions used in this paper are akin to the notations used by previous works [5,8,13]. Let T X  R M 1  X  M 2  X  M 3  X  ...  X  M n be a tensor of n orders where M i is an order. In this work, we focus on the third-subscript ( i, j, k ) range from I,J,K in each order. Each element (or entry) of a tensor needs n indices to represent or reference its precise position in a tensor. For example, the element a ijk is an entry value at the i, j and k orders. Given the documents set D , its corresponding set of CFI subtrees and the set of terms for each CFI subtree, the collection of XML do cuments is now represented as a third-order tensor T X  R D  X  CFI  X  Terms . The tensor is populated with the num-ber of occurrences of the structure-constrained term Terms i that corresponds to the CFI j for document D k . Two optimization techniques are applied on the two orders, CFI and Terms , to reduce the size of the tensor. Fig. 1 provides an overview of the XCT method. It begins with mining the CFI subtrees using the PCITMinerConst algorithm and then identifying the constrained content within those CFI subtrees for a given document. Once the structure and content fea-tures are obtained for each document, the documents are represented in the TSM along with their structure and content f eatures. The next task is to decompose the created TSM to obtain factorized matrices. Lastly, the K -means algorithm is applied to one of the factorized matrices representing the left singular matrix for the  X  X ocument X  order U D and the clusters of documents are obtained. 3.2 Generation of Structure Features for TSM The Prefix-based Closed Induced Tree Miner (PCITMiner) algorithm [10] is modified to generate the length-constrained CFI subtrees from the document tree dataset DT . The length constrained CFI subtrees are used in this method for the following reasons: (1) Extracting all the CFI subtrees is computationally expensive for datasets with a high branching factor; (2) All CFI subtrees are not required while utilizing them in retrieving the content. In fact the long sized CFI subtrees become more specific and result i n retrieving distinct terms associated only with this tree. This may result in a higher number of clusters with uneven sizes. We call the modified algorithm the PCITMinerConst algorithm.

Fig. 1 illustrates the computationally expensive operation of checking whether the mined CFI exists in a given document tree due to the graph isomorphism problem. This step can be optimized by grouping similar subtrees based on their similarity and then retrieving the content corresponding only to the group of similar CFI . A large itemset algorithm for clustering transactional data has been modified to include subtrees, rather tha n items, to conduct the grouping of the CFI trees based on the similarity of the subtrees. The clusters of CFI subtrees, called Closed Frequent Induced Subtree Cluster ( CFISC ), become a tensor order for representing and analyzing XML documents. Let CFISC be a set of CFI subtrees given by { ( CFI 1 ,...,CFI q )( CFI r ,...,CFI s )( CFI t ,...,CFI u ) } . 3.3 Generation of Content Features for TSM CFISC is used to retrieve the structure-constrained content from the XML doc-uments. We now define the coverage of a CFISC j and its constrained content for the given document D i . Compared with the content features of an XML document, the structure-constrained content features include the node values corresponding only to the node labels of the set of CFI subtrees in CFISC j . Definition 1: Structure-Cons trained Content Features. These features of agiven CFISC j ,C ( D i ,CFISC j )ofanXMLdocument D i , are a collection of node values corresponding to the node labels in the CFISC j where CFISC j is a cluster of CFI subtrees corresponding to DT i . The node value of a node (or tag) of a CFISC j  X  CFISC,C ( N i ) , in D i is a vector of terms, { t 1 ,...,t k } that the node contains. The term t is obtained after using pre-processing techniques such as stop-word removal and stemming. Firstly, the CFI subtrees corresponding to the CFISC j = { CFI r ,...,CFI s } for a given document D i are flattened into their nodes { N 1 ,...,N m } X  N ,where N is the list of nodes in DT .Then the node values of { N 1 ,...,N m } are accumulated and their occurrences for a document D i are recorded.
 In large datasets, the number of terms in the structure-constrained content is very large with more than 1M terms and 127M tensor entries for INEX (Initia-tive for Evaluation of XML retrieval) 2009 Wikipedia even after pre-processing. To reduce this very large term space, we apply a Random Indexing (RI) tech-nique which has been favored by many researchers due to its simplicity and low computationally complexity [12]. In RI, each term in the original space is given a randomly generated index vector as sh own in Fig. 2. These index vectors are sparse in nature and have ternary values (0 , -1 and 1). Sparsity of the index vectors is controlled via a seed length that specifies the number of randomly selected non-zero features. We utilize Achlioptas X  X  proposed equation 1 [1] to generate distribution for creat-ing the random index vector for every term in the structure-constrained content of CFISC . For a given document D i , the index vectors of length l for all the terms corresponding to the given CFISC j are added. We illustrate this concept of RI on tensor using the Fig. 2, in which we consider a tensor= R 3  X  2  X  4 (in Fig. 2(a)) with 3 documents, 2 CFISC , 4 terms and 7 non-zero entries. The entries in the tensor correspond to the occurrences of a given term in the given CFISC for the document. Using that equation 1, the random index vectors of length 6 for the 4 terms are generated (see in Fig. 2(b). Let us consider document D 1 with three tensor entries a 121 =1, a 123 =1and a 124 = 1 corresponding to CFISC 1 and three terms Term 1 , Term 3 and Term 4 . The random vectors (from Fig. 2(b)) are added to these three terms in D 1 . The sparse representation of the resulting vector ( a 12: )for D 1 (given in Fig. 2(c)) conta ins two non-zero tensor entries a 123 =1and a 124 =  X  1. Fig. 2(d) shows the final reduced tensor T r in sparse representation containing 6 non-zero entries.This exa mple demonstrates how our technique can reduce the term space for such a small dataset.
It can be seen that the number of entries in T r , randomly reduced T ,is less in number than its original and maintains the shape of T as it retains the same similarity between D 2 and D 3 . The index vectors in RI are sparse; hence the vectors use less memory store and they are added faster. The randomly-reduced structure-co nstrained content of CFISC becomes another tensor mode for representing and analyzing XML documents. 3.4 The TSM Representation, De composition and Clustering Given the tensor T , the next task is to find the hidden relationships between the tensor orders. The tensor decomposition algorithms enable an overview of the relationships that can be further used in clustering. However, as already men-tioned, most of these decomposition algorithms cannot be applied on very large or dense tensor as the tensors cannot be loaded into memory [13]. To alleviate this problem, the tensors need to be built and unfolded or matricized incremen-tally. Fig. 3 shows the process of matricization or unfolding along the mode-1 of T which results in a matrix T (1) . This means that the mode-1 fibers (higher or-der analogue of rows and columns) are aligned to form a matrix. Essentially this means that the mode-1 fibers of T are mapped to the rows of matrix T (1) and the modes-2 and -3 are mapped to the columns of this matrix. We apply the pro-posed PTCD as shown in Fig. 4 to progressively build and then decompose the tensor using SVD. The motivation for this new tensor decomposition algorithm is that the computations by other decompositions store the fully formed ma-trices, which are dense and hence cannot s cale to very large sized tensors. But PTCD stores the sparse matrices generated progressively and enables further processing to be performed on the tensor. PTCD builds the tensor progressively by unfolding the tensor entries for the user-defined block size b toasparsema-trix T ( m ) where m  X  X  1 , 2 ,...,M } and M is the number of modes. Then this unfolded matrix, T ( m ) is used to update the final sparse matrix T ( m ) . After up-dating, all the tensor entries to the final matrix, T ( m ) is then decomposed to the user-defined number of required dimensions  X  using SVD.

Huang et al. [6] have theoretically proved that HOSVD on a tensor simultane-ously reduces the subspace and groups the values in each order. For the 3-order ten-sor T , the left singular matrix on the document order, U  X  ( D ) provides the clustering results on the data index direction; hence they are the cluster indicators for group-ing the documents. Consequently, we apply the K -means clustering algorithm on the U  X  ( D ) matrix to generate the required number of clusters of the documents. Experiments are conducted to evaluate the accuracy and scalability performance of XCT on the real-life datasets. 4.1 Datasets Three real-life XML datasets which have extreme characteristics, INEX 2009 Wikipedia documents collection (Wikipedia) 1 , INEX 2006 IEEE [4](IEEE) and ACM SIGMOD (ACM)[2,10], were used after a careful analysis of a number of datasets. The INEX 2009 document mining track used the Wikipedia dataset with semantically annotated tags to perform the clustering task. This dataset contains a very large number of docume nts with deeper structure and a high branching factor. It also supports multi-label categories in which one document can have more than one category. On the other hand, IEEE has single-labeled categories and contains more formatting tags and fewer semantic tags. Finally, the ACM is a small dataset that contains 140 XML documents correspond-ing to two DTDs, IndexTermsPage.dtd and OrdinaryIssuePage.dtd (with about 70 XML documents for each DTD), similar to the setup in XProj [2]. This dataset has been chosen in order to evaluate our method against other repre-sentations and decomposition algorithms which could work only on this kind of small datasets. 4.2 Experimental Design Experiments were conducted on the High Performance Computing system, with a RedHat Linux operating system, 16GB of RAM and a 3.4GHz 64bit Intel Xeon processor core. Experiments were conducted to evaluate the accuracy of cluster-ing results of XCT over other clustering techniques, decomposition techniques and representation. Previous research fo r XML documents clustering [2] has used the ACM to cluster the documents into two groups according to their structural similarity. To compare our work with this earlier research, we conducted our ex-periments not only with two cluster categories according to structural similarity but also on 5 categories using expert knowledge considering both the structure and the content features of XML documents. Due to the small number of terms in this dataset, the random indexing option for XCT has been disabled.
Following are the representation and the other existing algorithms used for comparing the outputs of the proposed XCT method.
 Structure Only (SO) Representation: An input matrix D  X  CFI is generated similar to XProj [2].
 Content Only (CO) Representation: The content of XML documents is represented in a matrix D  X  Terms with each matrix entry containing term frequency of terms in D . Structure and Content Representation (S+C) using VSM: The struc-ture and the content features for the doc uments are represented in a matrix by concatenating the CO and SO representations side by side.
 Clustering Using CP and Tucker: The left singular matrix resulting from applying CP or Tucker decomposition on the tensor is used as an input for k-means clustering.
 Clustering Using MACH: The MACH decomposition technique has been applied on the original tensor with random indexing. MACH randomly projects the original tensor to a reduced tensor with smaller percentage of entries (10% from the original tensor as specified in [13]) and then uses Tucker decomposition to decompose the reduced tensor. To compare with XCT, we apply k-means clustering on the left singular matrix to group the documents.
 Moreover, since the INEX dataset has been used by other researchers: we provide the results cited by other researchers [4,11] as well in our analysis. 4.3 Evaluation Measures The standard criterion of purity is used to determine the quality of clusters by measuring the extent to which each cluster contains documents primarily from one class. The macro and micro purity of the entire clustering solution is obtained as a weighted sum of the individual cluster purity. In general, the larger the value of purity,the better the clustering solution is.
 4.4 Empirical Analysis Accuracy of Clustering: Tables 2 and 3 provide the purity results of cluster-ing on the datasets using XCT, other representations and other decomposition algorithms. As can be seen from these thr ee tables, the proposed XCT method not only outperforms our benchmarks but also other INEX submissions in terms of the accuracy of their clustering solution. It should be noted that algorithms such as CP, Tucker could not scale ev en to the medium-sized dataset, IEEE and hence their results were not reported but the proposed PTCD was able to decompose even large dataset as shown in Tables 2 and 3. As the categories in IEEE was based on both the structure and the content we utilized this dataset for analyzing the sensitivity of the length constraint and min supp values on the purity. We conducted experiments by varying the length constraint( len )ofthe CFI subtrees from 3 to 10 for support thresholds from 10% to 30%. From Fig. 5 which indicates that with the increase in the length constraint the micro-purity and macro-purity values drops especially at 10% and 30% support threshold. Also, length constraint of over 7 shows a negative impact on the purity. With longer length patterns the content corresponding to the CFI subtrees becomes specific and hence results in less accura cy than the content corresponding to shorter subtrees. This shows the suitability of constraining the CFI subtrees as in PCITMinerConst.
 Time Complexity Analysis: The time complexity of XCT is composed of five major components, namely frequent mining for CFI subtrees, clustering of CFI subtrees, random indexing, matricization and decomposition in PTCD. It is given by O ( dsm )+ O ( drp )+ O ( tk X  )+ O ( dr X  )+ O ( dk  X  )where d represents the number of documents, s is the number of 1-Length CFI subtrees, m is the number of PCITMinerConst iterations, r is the number of structure-based clusters, p is the number of similarity computation iterations,  X  is the size of the random index vector, k and k are the non-zero entries per column in the tensor before random indexing and in the matricized tensor after random indexing respectively. The time complexity of PTCD is O ( dr X  )+ O ( dk  X  ), which includes the cost of matricization along the mode-n and the sparse SVD. Scalability Test: All the three datasets were used for this analysis with min supp at 10%, len at 5,  X  at 100 and the number of clusters chosen, 5, 18 and 100 respectively. Also we used 1000 documents each for IEEE and Wikipedia. The execution time of PCITMinerConst is less than a few 10 milliseconds and hence it has not been reported as it is not of much significance. We can see from Fig. 6 that both XCT and PTCD scale nearly linearly with the dataset size. The PTCD algorithm includes two main steps: (1)loading the tensor file into mem-ory by matricization, and (2) decompos ing the matrices using SVD. As it can be seen from Fig. 6 that minimal time is spent on decomposition and a large chunk of time on loading the tensor file into memory. Also, it is interesting to note the PTCD for ACM is greater in comparison to IEEE which indicates that random indexing option in XCT method helps to reduce the complexity of decomposition by reducing the term space. An interesting problem is how to choose the value  X  for the seed length in random indexing. The Johnson-Lindenstrauss result was used to get the bounds for  X  as given by  X  = we found that for Wikipedia dataset with =0.5,  X  is 433 but in the experiments,  X   X  100 was sufficient to obtain good accura cy similar to the results by [3]. In this paper, we have proposed a cluster ing method, XCT, for effectively com-bining both the structure and the content features in XML documents using TSM model. The experimental results clearly ascertain that XCT outperforms other existing approaches in improving accuracy. Also, our proposed decompo-sition algorithm PTCD has demonstrated that it has potential for decomposing tensors effectively and could scale for v ery large datasets. Our future work will focus on reducing the complexity of XCT and applying it on various types of other types of semi-structured documents.

