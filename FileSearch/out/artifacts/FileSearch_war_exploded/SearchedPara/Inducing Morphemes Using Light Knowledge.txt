 MICHAEL TEPPER and FEI XIA Department of Linguistics, University of Washington 1. INTRODUCTION Morphological analysis is useful for a variety of computational linguistics tasks. Morphological segmentation, a type of shallow analysis that indicates morphological boundaries, has resulted in improvements in speech recogni-tion error rates [Creutz 2006; Kurimo et al. 2006] as well as improvements in information retrieval [Kurimo et al. 2007] particularly for highly inflected lan-guages such as Finnish and Turkish. Morphological analysis may also benefit in areas where stemming has been successfully employed, such as statisti-cal word alignment [Corston-Oliver and Gamon 2004], because like stemming, morphological analysis can be used to identify stems.
 since the late 1970s, and typically produce stable and accurate analyzers. Of these, the finite state transduction approach has been particularly successful [Karttunen and Beesley 2005], and is quite popular. The main drawback to rule-based approaches is that they rely on a morphological lexicon, a resource that may be prohibitively expensive to acquire, or too difficult or time intensive to produce, in many situations.
 requires fewer resources. It can be done supervised, with an annotated corpus of previously moprhologically analyzed words, or unsupervised from a variety of resources. We focus on the class of unsupervised approaches that do not use prior knowledge of the morpheme lexicon, collectively known as Morphological Induction (MI) approaches. These approaches can be speedily adapted to new languages as long as training resources (often just a word list) exist. results than rule-based approaches for a number of reasons, including imper-fect/incomplete models and noisy training data. Another drawback is that al-lomorphic variation, or variation among morphs with the same meaning, is often not modeled by machine learning systems. For applications that attempt to retrieve meanings or a map between meanings, like information retrieval and machine translation, there is a benefit to knowing certain morphs are ac-tually representations of the same morpheme. For example, in Table I, the English morpheme / grab / has two regular variants: one, [ grabb ], occurring in a limited set of contexts such as before vowel-initial affixes (e.g., grabb+ing ), and another, [ grab ], occurring everywhere else. If one were to query  X  X and grabb ing, X  presumably one would be interested in a remark such as  X  X heir goal was to grab the land. X  Likewise, in Turkish there are two variants of nightingale X  X  wing), presumably one would also be interested in phrases like ferent morph. Finally, failing to account for allomorphic variation adds noise to the learning process, dividing up the frequency of a morpheme among several allomorphs, some of which may be frequ ent, others quite infrequent, creat-ing a scenario in which infrequent allomorphs of frequent morphemes may go unlearned.
 we present a hybrid approach that uses a small amount of linguistic knowledge in the form of orthographic rewrite rules to refine an existing morph segmen-tation and simultaneously learn a morpheme segmentation. In order to do this, we extend the Morfessor algorithm [Creutz and Lagus 2004, 2005] by adding segmentation analyses generated by orthographic rewrite rules along with a statistical framework to predict when analyses should be used as morphemes. This technique has resulted in improvements over the Morfessor baseline sys-tem and other state-of-the-art morphological induction systems when tested on the Morpho Challenge 2005 and 2007 data [Kurimo et al. 2006, 2007]. previous work on computational morphology including Morfessor Categories-MAP  X  our baseline system. In Section 3, we describe the methodology of the proposed approach and explain how rewrite rules are used in our system. The details of our system are provided in Section 4 and the experimental results are reported in Section 5. 2. BACKGROUND AND PREVIOUS WORK Before providing a brief review of the previous work on computational mor-phology, we will first summarize the linguistic concepts that are relevant to the discussion in the article. 2.1 Linguistic Preliminaries Morphology is the study of word struct ure and word formation in natural lan-guage. Similar to the way sentences are formed from words, words are formed from more basic units, called morphemes. In this section, we review mor-phemes, morphotactics (constraints that govern how morphemes form words), allomorphy (morpheme variation), and end with a short discussion of mor-pheme ambiguity. [2003] defines morphemes as  X  X he smallest unit in language to which a mean-ing may be assigned or, alternatively, as the minimal unit of grammatical analysis. X  This definition indicates the dual role of morphemes in language. There are morphemes that have semantic content (traditionally known as con-tent morphemes), such as English cat , boy ,and walk . And, there are mor-phemes that are associated with grammatical functions (known as function morphemes), such as the English plural morpheme -s and past-tense mor-pheme -ed .
 tent morpheme is a base. There are multi-morphemic bases, such as organ+ist or garden+er , and uni-morphemic bases such as organ and garden . Monomor-phemic bases are called roots. In computational linguistics literature, stem is often used interchangeably with base. Just note that stem has a more spe-cific denotation as the variant to which affixes attach, for example, the wak-in wak+ing and the stepp-in stepp+ing .

Function morphemes that attach to stems are called affixes. Depending on what information is carried, the role of an affix can be derivational or in-flectional. Derivational affixes modify the part of speech category, and/or the semantic content of the stem they attach to. Inflectional affixes, on the other hand, convey grammatical information such as number and case for nouns, and tense and aspect for verbs, and typically form paradigms, or sets of affixes for a stem class. For example, the English inflectional paradigm {  X , -ing , -ed , -s } attaches to the regular verb stem class, such as start and walk . Our system, which learns morphology from raw text input with no part of speech markup, currently does not make the distinction between inflectional and derivational affixes. 2.1.2 Morphotactics . Word structure is governed by morphotactics. A key element of morphotactics is morpheme order, which is typically fixed. For ex-ample, in English you can have over + extend + ed , but not  X  over + ed + extend .Are-lated concept is morphological selectivity, or the constraints that govern which morphemes can occur together. Such co nstraints are tied to morpheme fea-tures such as part of speech, phonology, and prosody. For example, prosodic properties allow English comparative suffixes -er and -est to combine with ad-jectival/adverbial stems that are either monosyllabic or disyllabic ending in -y (e.g. green+er , pretti+est ), but not with higher degree polysyllabic stems (e.g., redundant+er ). 2.1.3 Allomorphy . The mapping between morphemes and what actually occurs on the surface, that is, in actual written words, is not one-to-one. Whereas morphemes are abstract linguistic units, typically corresponding to one meaning or grammatical function, morphemes quite typically have several versions on the surface. Surface versio ns of morphemes are called morphs. The group of morphs that belong to a morpheme are allo morphs of that morpheme. The English plural morpheme and a subset of its allomorphs is presented in Figure 1.
 texts. For instance, in Figure 1, we can observe that there are some contexts that trigger the variant -es , such as occurring immediately following an s or tch . A complementary set of contexts triggers the variant -s . Irregular allomorphs, on the other hand, are lexically conditioned, occurring only with specific lexi-cal items. For example, the irregular plural suffix -i will only occur only with a small set of nouns borrowed from Latin, such as alumn+i . content or grammatical function is not necessarily one-to-one either, causing ambiguity. Morpheme ambiguities arise when a morpheme shares the form (i.e., the same regular allomorphs and triggering contexts) with one or more other morphemes. A prominent example of ambiguity in English is the plural and third-person singular present-tense morphemes, which share the same regular allomorphs, -s and -es . Global, or word-external, ambiguities arise because English noun and verb stems are often also ambiguous, and when inflected by -s the ambiguity remains. The inflected word boats , for exam-ple, could be a finite verb boat V+3sg ,asin  X  X he boats. X  or a plural noun boat N+PL ,asinthephrase  X  X is boats X  .

A model can try to capture the ambiguity mentioned above. In such a model, an ambiguous root or stem would be marked as belonging to multiple mor-phological categories, e.g. boat would become boat V and boat N ,andam-biguous inflectional affixes would map to multiple tags, for example -s would map to +3sg indicating third-person singular present tense, and +PL indicat-ing plural. A model can also be shallow and ignore such ambiguity; under a shallow model, boats might receive one analysis: boat+s .Suchamodelmay postulate morphemes, but refrain from analyzing their semantic and/or func-tional content. Our system is of the second type, postulating morphemes but not their categories or roles, because the input to our system is only a word list with frequency, and does not include part of speech tag information. 2.2 Computational Morphology Computational morphology is an applied discipline meant to implement a working model of the morphology of a given language. It has been used in NLP applications such as parsing, speech recognition, text-to-speech synthesis, in-formation retrieval, and machine translation. In what follows, we offer a short introduction to computational morphology, focusing on approaches relevant to the current work.
 coded morphological lexicon and rigo rous rules compiled into a finite-state transducer in order to relate surface words to analyses, and vice versa. Then we will move on to morphological induction (MI), which involves automatically learning morphology from limited resources, often just a word list. We will also briefly describe what has been done regarding capturing allomorphy in morphological induction, since th at is the goal of the current work. based approach which models morphology using finite-state transducers (FSTs). FSTs create a bidirectional map from one set of symbolic sequences (strings in this case) to another. When an FST traverses a valid input string, it produces a related output string. In FSM, FSTs map between the set of surface (actually occurring) words in a language and their underlying (mor-phemic) analyses, achieving morphological analysis, and back, achieving mor-phological generation.
 particular, FSM should: (1) Handle allomorphy. (2) Identify underlying morphemes. (3) Capture the morphotactics, that is, the morpheme ordering constraints. [Koskenniemi 1983], which can be thought of as filters between the up-per and lower tapes of an FST. These rules allow single-stage derivation from underlying to surface forms, without the need to independently order rules relative to each other.
 rules, also known as Sound Pattern of Eng lish-style (SPE-style) rules, were de-veloped by Chomsky and Halle [1968] for the purpose of representing phono-logical patterns, and were formalized for use with finite state technology in Kaplan and Kay [1981; 1994]. They have the general form: whenever the left context  X  and right context  X  are met. Rules may interact X  and may also be conditioned on changes made by other rules X  X o rule-ordering must be explicitly stated.
 from underlying analyses is generally straightforward, while analyzing sur-face forms frequently leads to more than one possibility. This ambiguity, when spurious, results in the overanalysis problem [Karttunen and Beesley 2001]. However, some surface forms may be genuinely ambiguous, with several possi-ble points of origin. Therefore, morphological analyzers must be able to recog-nize true ambiguity while discarding spurious analyses.
 ation, applying rules listed in the figure to the Turkish morpheme sequence k  X  opek+DAn , glossed as dog+FROM and meaning  X  X rom the dog, X  has one rendering as a morph sequence: k  X  opek+ten . However, given the morph se-quence k  X  opek+ten , there are four possible analyses: k  X  opek+DAn , k  X  opek+tAn , k  X  opek+Den ,and k  X  opek+ten , where only the first is legitimate. In the FSM approach, this problem is solved by the upper tape of the FST, which spells only valid morphemes (determined by a lexicon) in valid combinations (deter-mined by morphotactic constraints). In our example, the upper tape would lack -tAn , -Den ,and -ten as possible suffixes, thus only k  X  opek+DAn would be licensed.
 a set of modeling approaches that fall roughly into two categories: relation-based and segmentation-based.
 learn relations between word forms; for example, that funny , funnier ,and fun-niest are related by a common stem: funn-. A variety of techniques have been proposed to learn these sorts of relations automatically. There are alignment models, which use string and semantic similarity features to match inflected-form, root-form pairs like funnier, funny [Yarowsky and Wicentowski 2000]. There are also conflation set models, which attempt to learn sets of words that share the same stem using latent semantic analysis as a proxy for mean-ing, in combination with string-level features [Schone and Jurafsky 2001]. Then there are techniques that fall more loosely into the category, such as Goldsmith [2001]. Unlike the other approaches, Goldsmith X  X  Linguistica seeks to relate words by learning which share the same inflectional paradigms, as op-posed to how they cluster together according to semantic similarity and other features.

Segmentation-Based Approaches. The segmentation-based approaches at-tempt to segment words into morphs, without explicitly modeling how they are related. Some past approaches have used heuristics to find natural points of segmentation. For example, D  X  ejean [1998] proposes a technique that uses local peaks in character perplexity to predict morphological boundaries, which has origins in Harris [1951].

Most recent approaches use mathematical models trained by unsupervised learning. Minimal description length (MDL), initially a framework for data compression, has been widely used, first by de Marcken [1996] then later by Deligne and Bimbot [1997], Goldsmith [2001], and Creutz and Lagus [2002]. The goal of MDL is to minimize the des cription length of a corpus, which is defined as the sum of the model bit-length and the encoding bit-length of the corpus given the model. In morphological MDL, the units in the model are the best set of morphs from the corpus, and these are used to segment (encode) the words in the corpus in the least costly way [Goldsmith 2001; Creutz and Lagus 2002].
 (MAP) approach, the goal of which is to find a model to make the observed data most likely, as well as satisfy prior distributions on what models should contain. Many MAP approaches try to maximize the probability of the model against linguistically motivated priors [Deligne and Bimbot 1997; Snover and Brent 2001; Creutz and Lagus 2005]. MDL is similar to the MAP, and in prac-tice can be approximated as MAP estimation, using priors motivated by com-plexity theory.
 proaches, which search for the model to make the corpus most likely, but where the optimal model, that is,  X  X he best set of morphs, X  is not mathematically for-malized. These approaches often refine the model with heuristics, relying on categorical decisions to include one morph or another, based on linguistic prop-erties [Creutz and Lagus 2004], or other properties, such as frequency at a particular level of segmentation g ranularity [Peng and Schuurmans 2001]. igram model of morphology to produce the segmentation of the corpus given the model. Substrings in the model are proposed as morphs within a word based on their own likelihood, independent of phrase-, word-, and morph-contexts [de Marcken 1996; Peng and Schuurmans 2001; Creutz and Lagus 2002]. Other approaches, however, have more complex, more realistic mod-els of morphology. These models attempt to constrain segmentations in some linguistically plausible way, in order to produce more accurate analyses. For example, work by Creutz and Lagus [2004; 2005; 2006] constrains segmen-tations using morphotactics, assigning morphotactic categories (prefix, suffix, and stem) to a first-pass morph segmentation, and then training an HMM using those category assignments. Other more structured models include Goldsmith X  X  [2001] work which attempts to learn the best signatures (e.g., morph-level partial paradigms such as {  X , -ing , -ed , -s } for English regular verbs) and what words inflect with what signatures.
 ation, is the process by which a morphe me varies in particular contexts, as constrained by a grammar. To our knowledge, there is only a handful of work within MI attempting to integrate allomorphy into morpheme discovery. A notable approach is the Wordframe model developed by Wicentowski [2002], which performs weighted edits on root-forms, given context, as part of a larger similarity alignment model for discovering inflected-form, root-form pairs. by a template designed for simple inflectional morphologies, constrained to finding an optional affix on either side of a stem. Several authors have modi-fied Wicentowski X  X  template to represent the morphotactic properties of more complex languages. For example, Cheng and See [2006] add infixation and reduplication, boosting performance on Filipino. To the best of our knowl-edge, no template has been implemented for an agglutinative language such as Finnish or Turkish.
 allomorphic patterns, both root-internally and at points of affixation. A major drawback is that, so far, it does not account for affix allomorphy that involves affix-internal character replacement; such allomorphy is beyond the scope of point-of-affixation insertions and deletions. 2.3 The Baseline System We use the Morfessor Categories-MAP algorithm developed by Creutz and Lagus [2005; 2006] as both baseline and preprocessor for our system. In this approach, a first-pass segmentation is refined using an HMM designed to ac-count for the morphotactic behavior of morphs.
 described in Creutz and Lagus [2002]. This technique begins with a source text and attempts to learn a model (or codebook) to predict the optimal seg-mentation (or representation under the codebook) of a source text. Under the MDL approach, a segmentation is optimal when the sum of the costs of the representation and codebook is minimized. The approach is noisy and suffers from spurious segmentation. For instance, the English word strap may be im-properly segmented as s+trap because  X  trap  X  is a frequent morph that appears on its own as a word, and  X  s  X , as an affix, is modeled as just another frequent morph, with no indication as to where in a word it must attach. This approach treats words as bags of morphs, so morphs are frequently posited in impossible positions.
 prove the first-pass segmentation by adding a hidden layer to account for morph ordering constraints, in other words, to account for the structure that the MDL approach ignores. The Categories-MAP approach utilizes morpho-tactic category sequences made up of mor photactic tags like prefix, suffix, and stem, to predict how morphs such as -s typically behave. The sequences and segmentations are learned in an HMM mo del, which produces a more accurate segmentation than MDL, mainly because it predicts morphotactically plausi-ble sequences.
 pus and thus attempts to maximize the probability of the model given the corpus: bility of the model is calculated by the following equation, where M is the size of the lexicon, and s i is the i th morph in the lexicon: overall model probability. The factor M ! accounts for possible orderings of the morphs in the model. For the meaning distribution, a morph X  X  probability is made up of priors on morph length, frequency, and usage (in the form of left-and right-perplexity). For the form distribution, a morph X  X  probability is cal-culated as either the probability of its characters, or if it has any substructure, the probability of its submorphs.
 the j th word in the training data and W is the size of the training data, as shown in Equation (3). The probability P ( w | mo del ) is calculated as a first-order HMM, where s 1 ... s n is the surface segmentation of a word w and C 1 ... C n is its tag sequence, as shown in Equations (4) and (5): corpus. It begins by initializing a model using the first-pass segmentation. The model is then refined in phases. One phase involves splitting morphs apart, and another involves joining shorter morphs followed by longer morphs to-gether in a bottom-up strategy. The above-mentioned phases revise the morphs in the model and may either delete morphs, or modify them by adding sub-structure (submorph and tag sequences). Splitting and joining phases alter-nate with decoding phases and model-estimation phases. The output at the end of the procedure is a segmented corpus. 3. METHODOLOGY 3.1 The System Design The approaches mentioned in the previous section have some limitations. Finite state morphology can produce accurate analyses that are consistent with linguistic judgments, but in order to do so, it must have access to a lexicon containing the valid morphemes, as well as rules governing allomorphic and morphotactic properties. This is a seriou s practical drawback, given that creat-ing or procuring a large morphological l exicon to cover a language X  X  vocabulary is often quite expensive.
 resources, or none at all. MI learns morp hological structure from resources as simple as word lists. However, MI presents a drawback in terms of over-all quality, compared with FSM. MI models are never exact representations of underlying morphological phenomena, though modeling assumptions have improved over time. For example, as mentioned in Section 2.3, without a repre-sentation of morphotactic structure, MI models may produce spurious segmen-tations like s+trap . There are now models that take structure into account, as the Morfessor Categories approaches do. Another modeling difficulty is allo-morphy. In allomorphically naive systems, morphological variants such as -s and -es are learned separately as distinct units, despite the fact that they are variants of the same morpheme.
 supervised MI system, also referred to as the baseline approach, with the goal to handle allomorphy appropriately. Hand-written rewrite rules, which are approximations meant to capture regular orthographic variation, are used to express the relationship between morphs and underlying morphemes; the MI system then chooses the best segmentation of words into a sequence of under-lying morphemes.
 strictly rule-based FST approach, it does not require a morpheme lexicon. Also, the rules can overgenerate (i.e., posit some bad analyses), because the system is, in the end, choosing the most likely segmentation based on a statistical model. The main difference between our hybrid approach and the baseline approach is that, for a given word, our system will produce an underlying seg-mentation (i.e., a morpheme sequence) in addition to a surface segmentation (i.e., a morph sequence). To achieve that, we use rewrite rules at various stages of the system to generate the morphemes from the morphs in the surface segmentation. 3.2 Context-Sensitive Rewrite Rules Our rules capture regular, context-driven spelling-changes, such as the variation of the English plural morpheme, which is -es after sibilants and (sometimes) vowels, and -s elsewhere. 2 To capture spelling-changes, we use or-dered, SPE-style, context-sensitive rewrite rules. Like the rules introduced in Section 2.2.1, ours have the general form: they operate in the analysis direction. The rules have the following effect: whatever is matched by the surface element  X  at the focus position  X   X , be-tween the context of  X  and  X  , is replaced by the underlying element  X  .The elements (which are variables) may hold characters, character classes, charac-ter (class) sequences, as well as the empty character,  X  X  X . The rules can reverse insertions (  X  = X ),deletions(  X  =  X ), or substitutions, when triggered by the con-text. Context elements,  X  and  X  , will frequently contain  X + X  to indicate where a segment boundary must lie. One or the other context element may be absent when the conditioning environment is on just one side of the focus. 3 3.3 Underlying Mapping Function  X  As laid out in the previous sections, the goal of the current task is to discover underlying word structure, that is, to break words into morphemes. We can do this by specifying the spelling-change rules in a language, and then using those rules to analyze the segmentation. We define  X  ( s , R ) to be a function that takes a surface segmentation s and a set of rewrite rules R as input, and produces the set of possible underlying segmentations as output.
 to their own output. Instead, each rule applies once to each position (from left-to-right) over s , incorporating changes as it goes. The output is then fed to the next rule which applies to each (possibly modified) position, and the process continues until all rules have been applied. This enables multi-step analyses using rules designed specifically to apply to the outputs of other rules. overanalysis. That is, given a set of rewrite rules and a surface segmentation, the number of underlying segmentations can be exponential with respect to the number of rules. This can be observed in Figure 3, which shows segmentations for English citi+es and Turkish k  X  opek+ten ( from the dog ) and their respective underlying segmentations after applying rules. In each case there are two rules, which can either apply or not apply, leading to four possible underlying segmentations. In Section 4.2, we will show how, in our approach, we use a statistical model to select the best possible underlying segmentation from these possibilities. 3.4 Modifying  X  to Control Overanalysis Given the unsupervised nature of our approach, there is no lexicon to confirm which underlying segmentations are correct, nor are there lexical automata built to constrain analyses to valid ones as would be the case in pure FSM systems. For practical purposes we control the exponential blowup with the simplifying assumption that either all rules that can apply will apply, or none will. Figure 4 shows how this assumption works in practice. In the figure, segmentations where one rule applied but another did not, like the Turkish segmentation k  X  opek+Den , are not generated by  X  .
 segmentations produced by  X  contains only two members: u and u ,where u is the underlying segmentation when no rules applied so u = s ,and u is the segmentation when all rules that could have applied did apply. To distinguish u and u from other underlying segmentations, we refer to them as underlying analyses . The modified underlying mapping function  X  ( s , R ) is called by Steps 1, 2, 4, and 5 of our procedure, which we will explain in the next section. 4. PROCEDURE In this section, we describe the components of the system, starting with an overview, and then covering each step in some detail.
 aword w is segmented into a sequence of morphological segments generated by a morphotactic tag sequence. We use the same tags as Morfessor: { prefix, stem, suffix } . The primary difference is while Morfessor emits morphs s i ,our system emits morphemes u i . Morphemes may be surface morphs acting as mor-phemes, or candidates produced by the rules, as determined by the function  X  defined in Section 3.4. We alter training and HMM decoding in the Morfessor system to accommodate this difference in approach.
 pairs and a set of rewrite rules as input, and as output produces an underly-ing segmentation, tag sequence, and corresponding surface segmentation. The word list is preprocessed by Morfessor Categories-MAP. Tags and surface seg-mentations from preprocessing are fed to the Word-Resegmentation Stage (WR Stage), the goal of which is to find the maximum probability underlying seg-mentation and tag sequence. Decoding in the WR Stage also creates a surface segmentation corresponding to the underlying segmentation.
 from conservatively segmented input. The Split Stage is introduced to address this problem. The output of WR is fed to the Split Stage which breaks some segments in the segmentation into (the most probable sequence of) smaller segments.
 splits some segments without considering the greater context (i.e., the word) in which they occur. For that reason, segmentations produced by the Split Stage are fed back to the WR Stage. Here, morpheme segmentation is done at the word level once more, using a model reflecting a less redundant encoding. The procedure stops after the second pass of the WR Stage. In Steps 1, 2, 4, and 5,  X  is called to generate underlying analyses, as indicated by the small shaded boxes in Figure 5. 4.1 Preprocessing We use the Morfessor Categories-MAP algorithm developed by Creutz and Lagus [2005; 2006], which takes a frequency-annotated word list and produces a surface segmentation and tag sequence. For more detail on this procedure, please refer to Section 2.3. 4.2 Word-Resegmentation Stage The WR Stage has two steps: Given the initial segmentation produced by the preprocessing step, Step 1 estimates the probabilities associated with an HMM model. Step 2 involves decoding of each word, that is, finding its maximum probability tag and morpheme sequence. 4 Step 1: Estimate HMM Probabilities Transition probabilities P ( t i | t i  X  1 ) are estimated by maximum likelihood, given a tagged input surface segmentation.
 to generate underlying analyses u and u ; all (unique) morphemes u i found in { u , u } are used to estimate P ( u i | t i ). A morpheme u i can either be identical to its associated surface morph s i , or different when modified by the rewrite rules. refer to s as an allomorph of u i . The probability of u i given tag t i is calculated by summing over all allomorphs s of u i the probability that u i realizes s in the context of tag t i : in the input and output of  X  (for more on  X   X  X  output, see Figure 4). 5 Step 2: HMM Decoder Next we resegment the wordlist into underlying morphemes. u and tag sequence t , we maximize the probability of the following formula, where U w is the set of underlying segmentations of w . Equation (10) is carried out by a modified version of the Viterbi algorithm. The algorithm effectively explores all underlying segmentations u  X  U w derived from surface segmentations s  X  S w ,where S w is comprised of all surface span sequences in word w . Underlying analyses u and u output by  X  ( s , R ), and all ordered underlying sequences constructed by mixing u i  X  u and u i  X  u are considered part of U w and are explored by the algorithm. For example, given analyses u = citi + es and u = city + s , U w consists of these four segmentations: citi + es , citi + s , city + es ,and city + s , which are explored. Mixing allows the possibility of selecting an underlying segmentation distinct from u and u  X  one which has selected rule-modifications for some morphemes, but rejected them for others. The entire algorithm is available in Appendix A. 4.3 Split Stage Many times, segments in the model will have internal substructure and yet be too frequent to be split when considered at the WR Stage. For example, a word such as baking may never have been segmented into bak(e) + ing because the segment baking was too frequent on its own. We use the Split Stage to overcome this segmentation problem, encouraging splitting of segments into sub-segments. Given  X  , baking can be split into the morpheme sequence bake and ing . Therefore, we no longer need baking in the model lexicon. Ideally then, when probabilities are re-estimated after splitting, segments like baking will no longer be available, or will have significantly reduced likelihood. procedure described in Creutz and Lagus [2004]. The main differences from Morfessor are (1) segments in our system are morphemes, not morphs, (2) we allow more than one split per segment (Morfessor only allows binary split-ting), and (3) we introduce a typology parameter which determines whether certain types of segments are to be split. Following Creutz and Lagus [2004] we attempt to control spurious splits by first re-tagging (Step 3) to identify which morphemes are noise (fragmentary) and should not be used, and af-ter re-tagging, re-estimating HMM probabilities (Step 4; same as Step 1) and using the new probabilities to split segments (Step 5).
 Step 3: Re-tag Segmentation In Creutz and Lagus [2004], segments are re-tagged to identify which seg-ments are likely to be noise by estimating a distribution P ( CAT | u i )withthree true categories CAT = { prefix, stem, suffix } and one noise category. The probabilities of true categories are tied to characteristic features of the mor-phemes as well as to the value of certain cutoff parameters, the most important of which is b . Parameter b thresholds the probability of affixes. The prob-ability of the noise category is conversely related to the product of the true category probabilities, so when true categories become less probable, noise be-comes more probable. Following Creutz and Lagus [2004], we tune the amount of noise by adjusting the parameter b . For more on re-tagging and a more precise definition of parameter b , see Appendix B.
 Step 4: Estimate HMM Probabilities This is the same as Step 1, as discussed in Section 4.2.
 Step 5: Splitting Segments As in Creutz and Lagus [2004], each segment, tag pair in the input surface segmentation is examined to determine whether a split is warranted. For each segment that it is possible to split, the optimal split is chosen by performing HMM decoding on the segment, a process identical to Step 2 except that the decoder only considers the segmentations that do not violate the constraints mentioned below.
 with the tag  X  X oise X  are not allowed to be split; segments with the tag  X  X tem X  are split into the sequence: (prefix*+stem+suffix*); segments with affix tags (prefix or suffix) are split into segments with the same tag. We modify the approach slightly, making affix splitting an optional parameter that may be set according to typological properties of the language. If a language has rich suffixation, for example, we would hand-set this parameter to allow suffix-splitting. 5. EXPERIMENTS We evaluated our system by running it on the English and Turkish data used by the Morpho Challenge contests for 2005 and 2007 [Kurimo et al. 2006, 2007]. In this section, we first give an overview of the data and evaluation metrics. Next, we describe how the system parameters were chosen. We end the section with the experimental results and some analyses. 5.1 Data Morpho Challenge is part of the EU Network of Excellence PASCAL Chal-lenge Program, and beginning in 2007, has been organized in collaboration with CLEF. Morpho Challenge 2005 uses three languages: Finnish, English, and Turkish. Morpho Challenge 2007, a follow-up to the 2005 contest, adds German to the language list. We chose English and Turkish because we were familiar with the languages, which allowed us to create rewrite rules without much difficulty.
 for training, and two smaller evaluation (gold-standard) datasets for evalua-tion: a tiny dataset used for the development phase, and a larger one for final test. Table II shows the size of the data sets in terms of number of unique words. A more detailed description is available in the contest reports [Kurimo et al. 2006; 2007]. associated with word counts: the words were collected from a variety of sources, and the counts were the cumulative frequencies of the words in these sources. Some examples are shown in the first column of Table III. For Mor-pho Challenge 2007, the source text from which the words and frequencies were culled were also provided, but our experiments did not make use of it. are substantially different, reflecting shallower and deeper levels of analysis, respectively. Some examples are given in the second and the third columns of Table III.
 that is, words that have been separated into substrings known as morpholog-ical segments (morphs). A word like glasses would be segmented glass+es ,for instance.
 ses. The analyses consist of a lemma, derivational affixes, and inflectional tags. The lemma is tagged with its part of speech, for example, N = noun. Derivational affixes are tagged with a morphotactic position, for example, s = suffix, and inflectional affixes are often represented by tag alone, like PL = plural, which may abstract away from surface variation. For example, glasses has the analysis glass N+PL ,and cups had cup N+PL .Fromthese analyses we can easily identify that both words share the morpheme PL . 5.2 Evaluation Metrics For evaluation, the recent Morpho Challenge contests use F -measure-based evaluation metrics, which assess the quality of system output by calculating how many key features it shares with the gold standard. F -measure is the harmonic mean of precision and recall, as shown below: standard and the system output), divided by the number of hits plus the num-ber of insertions (components appearing only in the system output). Recall ,on the other hand, is the number of hits divided by the number of hits plus the number of deletions (components found only in the gold standard).
 tutes a hit, deletion, and insertion differs in the two contests, as discussed below. and has been used extensively to measure the effectiveness of recent mor-phological induction methods [Creutz and Lagus 2004; 2005; Kurimo et al. 2006; Cheng and See 2006; Demberg 2007]. It tracks the extent to which surface-segmentation boundaries match between the system output and a gold standard.
 the gold standard, and insertion and deletion are defined similarly. For exam-ple, suppose the word unlikable is segmented as un+likable by a system and as un+lik+able in the gold standard. There would be one hit, the boundary after  X  X n X  , one deletion, the boundary after  X  X ik X  , and no insertions. Therefore in this example precision would be 100% and recall 50%. We use this metric to measure the surface segmentation produced by our system. 5.2.2 F-Measure from Morpho Challenge 2007 . This metric replaced the 2005 Morpho Challenge evaluation metric, and has been used to evaluate con-testants starting with the 2007 contest [Kurimo et al. 2007]. It tracks the consistency of underlying morphemes shared across word pairs. For instance, there should be a morpheme shared between glasses and cups , even though the allomorphs that appear in those words, -s and -es , are different; submissions that do not have the correct morpheme co rrespondences between words are pe-nalized. We use this metric to measure the underlying segmentation produced by our system. Below we explain how pre cision and recall are calculated. system output. For each word in the selected sample and each morpheme in the word, a  X  X inked X  word (a word that shares that morpheme) is selected at random from the system output, forming a word pair. A hit is counted when a pair of linked words selected from the system output also share a morpheme in the gold standard. An insertion is counted when words are linked in the system output, but not in the gold standard. Finally, the precision score is calculated as hits divided by the sum (hits + insertions).
 the word cups as cup+s . The pair selected for the first morpheme might be cup +s , cup +holder , and for the second morpheme: cup+ s , spoon+ s . In the gold standard, these word pairs would be analyzed as: cup N +PL , cup N +holder N and cup N+ PL , spoon N+ PL . Although the gold-standard morphemes appear different from the system output, what matters is the gold-standard pairs share a common morpheme in each case, just like the system output. Therefore we have two hits and no insertions, yielding a precision of 100%.
 gold-standard analyses, which, though initially selected at random, is the same for all participants. For each word in the selection, and for each morpheme in the word, a linked word is selected from the gold standard, forming a pair. Analogously to the precision calculation, a hit is counted when a pair of linked words in the gold standard is also linked in the system output. A deletion is counted when words are linked in the gold standard, but not in the system output. The recall score is then calculated as hits divided by the sum (hits + deletions).
 glass N+PL , and given that, assume the following linked pairs are selected: glass N +PL, glass N +blow V+VBG and glass N+ PL ,cup N+ PL .Sup-pose, in the system output, the preceding pairs are rendered glass +es, glass +blowing ,and glass+es, cup+s respectively. The first pair is linked by a shared morpheme, yielding a hit, but the second pair is not, yielding a deletion. The recall score is then one out of two, or 50%. 5.3 Rewrite rules Rewrite rules and orthographic classes used in our system were culled from linguistic literature. We currently use 6 rules for English and 10 for Turkish. These rules are displayed in Appendices D and E.
 significant improvement even with a small set of imperfect rules. Therefore we include only the rules that will apply most generally; rules that apply only to exceptional cases in the lexicon are not encoded by rule. An example would be the Turkish consonant doubling rule, which affects only a small set of Arabic borrowings like s X r+Im  X  s X rr X m ,( secret+1Pos  X  my secret ). Writing a rule set involved taking generalizations from descriptive grammars about characters that undergo changes, and encoding those generalizations into rules. good sense for the approximate time it took for each language. The English rules took approximately a day to write. The Turkish rules, on the other hand, took longer. It took about a week to settle on a Turkish rule set. We found the difference in difficulty between the two languages to be due in part to the complexity of the allomorphic phenomena involved, and in part to the relative usefulness of the available linguistic literature.
 nomena than Turkish in the written lexicon, and those it does have are less complex (for instance, none involve nonlocal dependencies, such as Turkish vowel harmony). Regarding the latter point, the grammar we used for Eng-lish, Cambridge Grammar of English [Huddleston and Pullum 2001], has an orthography section with a thorough treatment of English orthographic con-ventions and spelling changes, which we were able to follow quite closely. We were unable to locate a Turkish grammar with a similar section, likely because Turkish orthography is much closer to its phonology, and many grammars con-flate the two to some degree. Because of this, rule writing had to be somewhat incremental for Turkish, as we had to make sure that each phenomenon we represented as a rule was actually an orthographic phenomenon, and not (or not just) a phonological one. 5.4 Parameters Our system has several tuned parameters. The parameters for preprocessing are tuned as suggested in Creutz and Lagus 2005. The main procedure has several numerical parameters involved in re-tagging similar to those in the preprocessing step: we set those parameters as suggested by Creutz and Lagus [2004] 6 , and tune parameter b , defined in Section 4.3 and Appendix B, on the development data. Here, and every subsequent time it is mentioned, b refers to the b used in the main procedure; it is distinct from the b used in preprocessing which was tuned separately to 200 for English and 375 for Turkish. In the following section, we show development results for b = 100, 300, and 500. iteration of the WR Stage, followed by the Split Stage to split some redundant segments, followed by an iteration of the WR Stage again. The whole procedure as run herein can be described as 1 WR + 1 Split + 1 WR. In preliminary experiments, running this entire procedure multiple times did not provide any further benefit.
 indicate whether segments tagged as prefixes, or suffixes, or both are allowed to be split. This parameter is introduced to distinguish morphologically rich languages (e.g., Turkish) from more impoverished languages (e.g., English). 5.5 Evaluation Results Our experiments make use of contest evaluation metrics for Morpho Challenge (MC) 2005 and 2007. When evaluating output of the main procedure, surface segmentations (morphs s i ) are evaluated by the 2005 metric, while underlying segmentations (morphemes u i ) are evaluated by the 2007 metric.
 final test, we present results of large-scale evaluations, which were con-ducted by Morpho Challenge contest organizers at the Helsinki University of Technology. 7 line. Also, in order to isolate the effect of the rewrite rules, we also ran our system with an empty rule set (the  X  X o-rules X  experiments). In effect, running without rules generates a surface-segmentation only, as the underlying and surface layers are the same. on the development data for English and Turkish: Baseline refers to the re-sults after the preprocessing step, WR to the results after the first pass of the WR Stage, and SPL:b = N to the results when the whole procedure is complete, that is, after the Split Stage (with parameter b=N ) and another iteration of the WR Stage.
 with which we evaluate the surface layer of our segmentations, the largest F -score improvement was observed for English (Figure 6(a)), 55.30% to 64.26%, an F -score gain of 8.96% over the baseline segmentation. The Turkish result also improves to a similar degree. For both languages, the bulk of the improve-ment is achieved only after the models have been refined by splitting. For the 2007-style evaluation, which we take on the underlying-layer of our segmenta-tions, the largest F -score improvement was observed for Turkish (Figure 6(b)), 31.37% to 54.86%, an F -score gain of more than 23% absolute.
 Stage and splitting result in consistent improvements in performance over the baseline. Without using allomorphic rules (no rules), the results may be neg-ative compared to the baseline (see Figure 6(b)), or mixed (Figures 6(a) and 7(b)). A representative scenario is the 2005-style measure on the Turkish re-sults (Figure 6(b)), which clearly shows segmentation improves reliably with rules, but not without. This indicates that segments in the model are improv-ing as a result of the rules, since the 2005-style evaluation considers surface morphs, not morphemes. In other words, the improvement is not a result of mapping to morphemes alone, but rather, having made available consistent units that are easier to learn.
 no rules scenarios as well (see Figure 7(a)). One explanation for this may be that our approach is derived from the Morfessor Categories-ML, whereas the baseline segmentation is produced by Categories-MAP. Creutz and Lagus [2005] found that ML segmentations have better coverage than MAP segmen-tations, and sometimes the increased coverage results in a better F -score. When we looked at recall and precision directly, we found this explanation to be quite plausible, as we observed consistently large boosts in recall for no rules scenarios.
 it comes to the contribution made by the rewrite rules. Notice that for the 2005 evaluation, the difference by which the best with rules scenario outperforms the best no rules scenario is 2.80 for English and 9.59 for Turkish. For the 2007 evaluation, this difference is 2.90 for English and 11.46 for Turkish. Thus we see that the gap between with rules and no rules is quite a bit smaller for English than for Turkish, particularly for the 2007-style evaluation. The sim-plest explanation for this is that English has less allomorphy than Turkish. In English there are some morphemes that have variant forms, and those that do have at most two variants. In Turkish, nearly all morphemes have some vari-ants, and can have up to twelve of them. In the with rules scenario, Turkish benefits much more from the amount of simplification the rules provide, while in English, the effect is more subtle. given in Table IV. We had three systems evaluated: the baseline used in our approach, the hybrid system in the with rules scenario, and the hybrid system inthenorulesscenario. 8 The results overall are quite positive. The hybrid system with rules managed to outperform the baseline as well as the more extensively tuned MC Morfessor MAP, across all test scenarios.
 Pitler 2006] and the Bernhard system for 2007 [Bernhard 2007]. The 2008 contest used the same evaluation and test data as 2007 and its top score was lower than 2007 because the top system in 2007 did not participate. Our hybrid system X  X  performance for the 2007 dataset was comparable to the 2007 and 2008 top systems. Its performance for the 2005 dataset was lower than the 2005 top system. There are several possible explanations for why we were not the top performer on English. Our splitting constraint for stems, which allows them to split into stems and chains of affixes, is suited for rich morphologies, and does not seem particularly well suited for English morphology. Also, as we have already observed for the development data, the allomorphic rules do not have as great an impact on improving induction in our system for English. Our rewrite-rules might also be impro ved by increasing their coverage over additional allomorphic phenomena, and by refining them so that they produce fewer spurious analyses.
 predicting affixes using transition probabilities between substrings [Keshava and Pitler 2006; Bernhard 2007], which are particularly reliable for English. Neither models allomorphy, and so both are complementary with our allomorphic-learning approach, suggesting that a better system may be pos-sible by combining the most beneficial aspects of their approaches with our approach.
 for all three years. 9 The gap between our system and the 2008 top system is smaller because that system, ParaMor+Morfessor, combines both ParaMor [Monson et al. 2008] and Morfessor MAP segmentations as options for each word. On its own ParaMor performs second best for that year, scoring 46.5% F -score, and our system beats this by 9.0% F-score. The ParaMor approach is also complementary to our strategy, and attempts to learn and cluster par-tial paradigms, so here again it may be possible to combine aspects of both approaches and acheive an even better score.
 the unsupervised approaches emphasizes the importance of handling allomor-phy for a highly inflected, highly allomorphic language such as Turkish. A Turkish suffix, for instance, may undergo multiple spelling rules, and can have as many as twelve variant forms. Knowing that these variants all come from the same morpheme makes a difference. 6. CONCLUSION Morphological analysis is important for a variety of NLP tasks. What kind of analyzer to use depends on many factors, including the application and avail-able resources. The FST approach can pro duce accurate analyses, but it re-quires a morphological lexicon to work, which is difficult to come by and often quite expensive to produce. The advantage of MI techniques, on the other hand, is the fact that they are data driven, requiring just a word list in the simplest case. MI approaches are typically quite portable and can be retrained with ease. The problem is they tend to produce noisy segmentations, especially for a morphologically rich language such as Turkish. a small amount of linguistic knowledge to augment a MI procedure, taking the idea of using rewrite rules to produce abstractions from the FST approach, and the idea to learn which abstractions are valid from the MI approach. Our experiments show that by adding even this small amount of knowledge, one can improve unsupervised segmentations significantly, particularly for com-plex languages such as Turkish. In MC 2007 test results, we get an improve-ment for the Turkish segmentation of nearly 22% against our baseline, and 2.5% against the state-of-the-art unsupervised approach.
 matically [Dasgupta and Ng 2007; Demberg 2007]. It is hoped that our work can inform these approaches, if only by showing what variation is possible, and what is relevant to particular languages. For example, variation in inflectional suffixes, driven by vowel harmony and other phenomena, should be captured for a language like Turkish.
 guages, as test data in those languages becomes available. More languages will help determine how extensible this approach is, particularly whether it might be adapted to languages whose morphology is more fusional than that of English or Turkish (e.g., Russian).
 learn those rules automatically from data. This might involve instantiat-ing variable-featured rule-templates using seed corpora containing aligned morphs and morphemes. By collecting rules automatically we will bypass the need of a human expert and therefore we can apply the same techniques to many languages. It is also possible that rule sets collected in such a manner will be more complete.
 APPENDIX A. MODIFIED VITERBI ALGORITHM During the Viterbi resegmentation step of our word-resegmentation procedure, we find the morpheme sequence  X  u and tag sequence  X  t that maximizes Equation (12) for a word w using a modified version of the Viterbi algorithm. Given a word, underlying morphemes are generated from all possible surface spans (i.e., substrings) by context-sensitive rewrite rules. The function  X  given in Section 3.4 produces underlying analyses, u and u , for each span. Each analysis is indexed to the span X  X  ending position and length.
 sequence over underlying morphemes produced from all surface spans, we are able to utilize a Viterbi procedure to perform an efficient search for each word. A.1 Goal of Viterbi Procedure The procedure X  X  goal is to find the most likely underlying morpheme and hid-den tag sequence for a word, given a model. The variable  X  ktl stores the proba-bility of the best path leading to each possible tag and morpheme combination in the model ending at position k ; L k isthelengthofasurfacespan s ( L k ), the right boundary of which lies at position k ,and C k is its tag. The underlying as defined in Section 3.4. A.2 Initialization The induction is initialized in Equation (14) by  X  t , transition probabilities from the start state to the first tag, t .
 A.3 Induction The induction proceeds by calculating  X  ktl according to Equation (15), for each position k from left to right in the word, with lengths equal to L k = l and tags t for morphemes ending at k .Theposition k and length L k uniquely determine where the right boundary of the previous morpheme ( k ) should be, distance L k from k : k = k the previous morpheme k ,aswellasitslength l ,arebothequalto0. In Equation (15),  X  k , t , l stores the probability of the best paths up to the previous morpheme with tag t , ending at position k with length L k = l , a [ t , t ] is the transition probability of going from state t to t and b t , u ( L ability for the best-scoring underlying morpheme u ( L k )given t , selected from u ( L k ) and u ( L k ) : A.4 Store Backtrace The backtrace  X  ktl stores the argument which maximizes Equation (15), and is given in Equation (16). It stores three essential pointers for both tags and morphemes: the best previous tag t =  X  t , the best previous morpheme length l =  X  l , and a binary valued variable  X  z , which indicates if the best previous morpheme is of type u or u . A.5 Termination and Path Readout (Backtracking) When k is equal to the last position of the word, we calculate the best final state and store the resulting underlying morpheme u ( L  X  k =  X  l )andtag  X  t .We use this to backtrack through the most probable tag and morpheme sequence. We read out the best tag sequence  X  t and morpheme sequence  X  u by iterating backward through the backtrace  X  ktl until we reach the beginning of the word: (1)  X  t ,  X  l ,  X  z =  X   X  k  X  t  X  l  X  k =  X  k  X   X  l , (2) store  X  t , u ( L  X  k =  X  l )in  X  t ,  X  u , (3) set  X  t =  X  t ,  X  k =  X  k ,  X  l =  X  l . goto (1).
 B. SPLIT STAGE DETAIL Here we offer more detail on the tagging phase used in the splitting procedure used in Creutz and Lagus [2004], and extended in our approach. It is im-portant for understanding parameter b , the one of the numerical parameters adjusted during tuning.
 B.1 Re-tagging Morphemes To identify noise morphemes, we use the same categories from Creutz and Lagus [2004] to estimate a distribution P ( CAT | u i ): and one noise category, CAT = NOI . This distribution is estimated from the underlying segmentation output by the WR Stage. Once again the vari-able u i refers to the underlying morpheme with index i in the underlying segmentation.
 segmentation. The probability of each morphological category is proportional to a characteristic function f . Since stems are typically longer than affixes, their characteristic function is morpheme-length: morphemes. For prefixes this occurs on the right, while for suffixes, on the left. The functions used to characterize this pro perty are right-and left-perplexity, respectively: between 0 and 1: Probability for each category is thus tied to the value of sigmoid parameters a,b,c, and d , the most important of which is the cutoff b , which thresholds the probability of affixes and is adjusted during tuning.
 true category sigmoids; when the sigmoids are adjusted down, noise becomes more probable: Then, the remainder of the probability mass is proportionally distributed to true categories according to the value of their sigmoid functions: Finally, all the morphemes in the segmentation are randomly re-tagged ac-cording to the distribution P ( CAT | u i ), and the new tags are sent to the next step.
 C. SAMPLE SEGMENTATIONS In this section we provide samples of the segmentations produced by the pro-cedure. The samples are provided in tables with columns for the results of preprocessing (Baseline), the first round of Word-Resegmentation Stage (WR Stage), and the Split Stage plus second round of WR Stage using two values of b (SPL:b = 300 and SPL:b = 500). For the segmentations produced using rules, the first line is the surface segmentation and the second is the underlying segmentation.
 C.1 English Sample Segmentations Table V shows a sample of segmentations for English when all rules are used. Manual segmentations have been provided for comparison. For each word in the table, the surface segmentation is listed above the underlying segmentation.
 segmentation and underlying segmentation are identical.
 C.2 Sample Segmentation for Turkish Table VII shows a sample of segmentations for Turkish when all rules are used. Manual segmentations have been provided for comparison. For each word in the table, the surface segmentation is listed above the underlying segmentation.
 segmentation and underlying segmentation are identical.
 D. REWRITE RULES FOR ENGLISH The set of rules used for English is listed in Table IX. Rules were derived from Huddleston and Pullum [2001] in combination with the author X  X  native intuitions about English spelling.
 D.1 Alternation Between -s  X  -es Huddleston and Pullum indicate that this alternation occurs in the English plural, as well as in the 3rd-person singular present-tense morphemes. Ac-cording to the authors the alternation can be described by analyzing the con-ditions in which -es occurs, while letting -s occur as the default alternate, that is, everywhere else.
 The first is after surface vowels. For instance, -es occurs after a y-final base that alternates with a surface-form ending in the vowel i ,asin try  X  tri+es . Also, it occurs after the vowel, o ,asin potato  X  potato+es. We capture this with Rule 1a, which says that any surface-vowel-final morph should be followed by an -es alternate.
 reason: it overgenerates badly. It leads to incorrect predictions for inflections of vowel-final stems like taxi  X  *taxi+es and papa  X  *papa+es . However, this type of overgeneration is not a problem for our system because its goal is only to learn morphemes from analyzing surface morphs, and not to generate the surface morphs accurately.
 occurs, but not as the result of morpheme alternation. This does happen, and will result in spurious analyses, many of which will be ignored, but some of which will persist. For example, an analysis like sees  X  *se+es should have a low probability, as se does not occur as a morph. However, bees  X  *be+es will persist, as the proposed stem, be , collides with a high probability morph. If such collisions occur frequently enough, this rule should be revised to try to avoid them.
 ending in a sibilant, or sibilant-final character. A sibilant-final character is a character that is pronounced as ending in a sibilant. The character x is an example; it is pronounced ks , with the final-sibilant s .Someexamples of this condition are fox  X  fox+es and dress  X  dress+es .Thisiscapturedby Rule 1b.
 D.2 Final e -deletion According to Huddleston and Pullum, a base-final e is usually dropped before suffixes that begin with a vowel. This rule is true with a few exceptions in the case of the so-called mute or silent e that follows a consonant, as in wake  X  wak+ing or pipe  X  pip+ed. We handle this variation with Rule 2, which says that a vowel-initial morph should trigger deletion of e (following a consonant) in its preceding morph.
 deleted, but that it is less common than e -deletion after consonants. There are examples like sue  X  su+ing and free  X  fre+ed, but not free  X  *fre+ing .Because e -deletion following vowels is more complex and less common, we chose not to write a rule for it.
 D.3 Final y -replacement In English, a base-final y is replaced by the character i before another morph is attached, as in pony  X  poni+es and cleanly  X  cleanli+ness . Rule 3 captures this by replacing the final y following a consonant with i before morphs that begin with any character. As Huddleston and Pullum point out, the y -replacement does not take place if y forms a diphthong with its preceding vowel as it does in stay and buy .Becausethe y undergoing variation could be a morpheme attached to another stem, as in paper+y  X  paper+i+ness , the rule allows an optional morpheme boundary between y and its preceding consonant.
 D.4 Consonant Doubling This phenomenon involves doubling a base-final consonant before attaching a vowel-or glide-initial suffix, as in bat  X  batt+ing or cat  X  catt+y. According to Huddleston and Pullum, the alternation does not occur with multi-character consonants (e.g., dress  X  *dresss+es ), consonants preceded by multi-character vowel symbols (e.g., seat  X  *seatt+ed ), consonants in a non-stressed syllable (e.g., offer  X  * offerr+ing ), nor does it occur at all for the set of consonants { h, w, y, and x } .
 too specific, only indicating the doubling of stops. The doubling phenom-enon also affects the other consonants (except the set mentioned explicitly above). The rules are incomplete because they were written at an early stage of the project; they have not been updated so as to maintain experimental consistency.
 E. REWRITE RULES FOR TURKISH The set of rules used for Turkish is listed in Table X. The Turkish rules were primarily derived from G  X  oksel and Kerslake [2005] with additional input from Lewis [1967].
 E.1 Vowel Harmony According to G  X  oksel and Kerslake [2005], vowel harmony is a process that primarily affects how suffix allomorphs are realized. Here, different vowels will obtain depending on which vowel precedes them, that is, the last vowel in the preceding base morph. There are two classes of vowel harmony in Turkish, A-type and I -type harmony. Suffixes in Turkish exhibit either one or the other, but not both. The use of a capital letter as a character implies that the  X  X efault variant X  is underspecified for one or more features; it always takes on some feature-values of characters in its immediate context.
 resented by the character A . It is unrounded (lips are not round) and non-high (tongue is low in the mouth), but is underspecified for frontness (how far for-ward the tongue is in the mouth).

Its frontness depends on the frontness of the vowel preceding it. This is captured by Rule 1a using the variable feature  X  F ,where  X  is instantiated to the frontness value of the preceding vowel; this determines whether it is realized as the nonfront (back) variant a or the front variant e. For example, the dative case suffix -A is realized as front -e in ev  X  ev+e ,( house  X  to the house ) or as back -a in bulut  X  bulut+a ( cloud  X  to the cloud ). It is high, while underspecified for both roundness and frontness. Similar to what was just described, I takes on roundness and frontness features of the vowel preceding it. This is captured i n Rule 1b using variable features where variables are instantiated by features of the preceding vowel. Because each feature is binary valued, there are four possibilities for a vowel undergoing I-type harmony.
 possessive suffix -I m: it can be realized as front and rounded - X  um after a front and rounded vowel, e.g. g  X  ul  X  g  X  ul+  X  um ( rose  X  my rose ), or as non-front and unrounded - X m after a non-front and non-round vowel, for example, at  X  at+ X m ( horse  X  my horse ).
 E.2 Suffix-Initial Consonant Voicing Alternation Suffix-initial consonant voicing alternation affects suffixes that start with voiced, voiceless consonant pairs like c ,  X  c , represented by C ,or d , t ,rep-resented by D . The pattern driving this alternation is simple: the voiced vari-ant obtains when the preceding character is voiced, and the voiceless variant when the preceding character is voice less. This pattern is encoded by Rule 2 in Table X.
 it will be voiced-initial -ci after a voiced character like z ,asin deniz  X  deniz+ci ( sea  X  mariner ).
 E.3 Suffix-Initial Consonant Insertion This phenomenon, which is also known as consonant deletion, involves conso-nants that appear or fail to appear as the initial character of a suffix, depend-ing on the preceding character and gra mmatical properties of the preceding suffix. This phenomenon involves the consonants y, n and s .
 -(s)I , which share the homographic allomorph -I in the context immediately following a consonant, but are distinct after a vowel: -yI and -sI respectively. verse a deletion by inserting a consonant is problematic: given a word like at- X  , one cannot know whether it came from at+yI ( horse +ACC ), or at+sI ( horse +GEN ) without referring to word-external context. Therefore, both possibili-ties would need to be generated by an analyzer.
 alterations, 10 our rules (see 3a-d in Table X) represent this type of variation as consonant-insertion instead, which is possible to reverse deterministically. The main drawback of this style of approach is that it creates ambiguous mor-phemes like -I , where there ought to be distinct morphemes: -(y)I and -(s)I . Section 4.4, positing ambiguous morphemes will likely result in lower precision scores. However, since a greater number of valid (even if ambiguously labeled) morphemes are often discovered this way , the recall score is likely to improve. E.4 Stem-Final Consonant Voicing Alternation This is a change induced in stems by adding suffixes. It involves a set of voiced, voiceless consonant pairs, similar to the set that undergoes voicing alternation in suffixes. Here, the voiced and voiceless variants alternate at the end of a stem. The alternation depends on phonological properties of the suffix that attaches to it. As in suffix-initial voicing alternation, the pattern behind the variation is simple. Word-finally, as well as before a consonant-initial suf-fix, the final consonant of a word will be voiceless. Before a vowel-initial suffix, it will be voiced. This is captured by Rules 4a-c.
 kanaD results in kanad+ X m ( my wing ), where the final character is the voiced d . As a word on its own, uninflected, it is rendered kanat . Upon attaching a consonant-initial suffix like locative -DE , it is also rendered with a final t ,as in kanat+ta ( on the wing ).
 nation. These are stems that end in a voiceless final-consonant that always remains voiceless, even in the presence of a vowel-initial suffix. An example would be sanat  X  sanat+ X m ( art  X  my art ). Such exceptions are not a problem for Rules 4a-c, however. Upon analyzing sanat+ X m , the conditions would not be met for any of these rules to fire. The refore, the only morpheme proposed for the stem would be the surface form, sanat , which would be correct.
