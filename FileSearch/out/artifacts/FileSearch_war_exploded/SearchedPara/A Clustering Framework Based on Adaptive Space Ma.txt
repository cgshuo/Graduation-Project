 With the explosion of documents on the Web, there has been increasing need for efficient and effective analysis methods to manage massive text collections. Docu-then been applied in different kinds of IR tasks [2], e.g. speeding up the information retrieval procedure [1], improving the precision or recall in information retrieval sys-tems [3], browsing a collection of documents [4], and organizing the search results for a given query [5]. 
However, the performance of clustering algorithms often suffers from the model misfit problem [7]. Most clustering algorithms are based on some underlying model assumptions. When real data fits the assumptions well, the performance of the cluster-ing algorithm would be reasonably good, otherwise not. Typically, there are two kinds of approaches to addressing the model misfit problem: adjusting algorithms X  strate-of approaches usually makes refinement on local regions where training errors occur, global characteristics should be reasonably considered because they may be directly related to the model misfit problem. The second kind of approaches tries to apply space transformation to alleviate the problem. However, since space transformation is usually proposed based on some assumptions without the consideration of algorithm's model, the performance improvement may be limited. In this paper, we present a framework in the way of space transformation. However, unlike the previous ap-proaches, our transformation is proposed with respect to the clustering algorithm X  X  model assumptions. 
The framework we propose is referred as M-R framework. It is a clustering frame-work based on adaptive space mapping and re scaling. Considering that real data dis-tribution usually may not fit the model assumptions of clustering algorithms very well, the model assumptions. Specifically, the M-R framework consists of two important steps as follows. 
Step 1: Space Mapping. Since the distribution features of data in high dimen-sional space are usually complicated and hard to analyze, we choose to map all docu-ments into a low dimensional coordinate which is constructed with respect to the cluster centers. In this way, the distribution statistics of each cluster could be analyzed on the corresponding dimension. 
Step 2: Rescaling. With those distribution statistics obtained in hand, we apply a rescaling operation to regularize the data distribution based on the model assumptions. In this way, we are able to make the data distribution more consistent with the model assumptions to help make better clustering decisions. 
By conducting these two steps iteratively along with the clustering algorithm, we are able to constantly improve the clustering performance. In our paper, we apply our M-R framework on the most popular clustering algorithm, i.e. k-means, to verify the effectiveness of our framework. The rest of this paper is organized as follows. Section 2 discusses related work and Section 3 proposes the M-R framework in detail. We apply our framework on lowed by some concluding remarks in the last section. Different clustering algorithms [6] take different point of views of data spaces. Hierarchi-cal clustering algorithms hold the assumption that any cluster is composed of smaller sub spherical regions with the same radius. Density-based algorithms and grid-based algo-small unit named grid, and make clustering decisions with these attributes. Actually, such ideal models more or less misfit real data distribution. Therefore, model misfit becomes a common problem in text mining. To address this problem, researchers make their efforts to figure out solutions in both algorithm layer and data presentation layer. 
In algorithm layer, most research approaches require training errors to refine the model, and thus are mainly proposed in supervised learning area. Wu et al. [7] retrain a sub-classifier using the training errors of each predicted class with the same learning efficient refinement strategy to enhance the performance of text classifiers by means field, little work has been done for solving the model misfit problem due to the lack of labels. Generally, this kind of approaches improves performance by achieving local refinements, but the global feature of real data distribution is usually ignored. 
As for approaches in the data representation layer, the basic idea is to apply space solved. Dumais [1] proposes LSI decompos ition to rectify the deficiency in VSM model that takes correlated terms as inde pendent dimensions. Kernel method [9] aims problem solving is much easier. Another novel and important approach in feature idea of spectral clustering is to model the whole dataset as a weighted graph, and aims to optimize some cut value (e.g. Normalized Cut [11], Ratio Cut [12], Min-Max Cut [13]). Since those criterions could be guaranteed global optimums via certain eigen-decompositions, spectral clustering algorithms often achieve better results than tradi-solving existing problems in the current f eature space. Most of them seldom account in the algorithm X  X  model assumptions during the mapping procedure. And the compu-tational and memory requirements are usually high. 3.1 Model Misfit in Clustering A simple but direct example about model misfit problem of clustering algorithm (e.g., k-means) is shown in Figure 1. There are two intrinsic clusters in the dataset that are label  X + X  distribute in a narrow elliptic region while points with label  X   X   X  distribute in assumption of k-means. Therefore, without knowledge of the distribution characteris-tics, k-means probably organizes all the points into two clusters which are marked with label  X + X  is assigned to the wrong cluster. 
How can we avoid such kind of mistakes caused by model misfit? According to the decision criterion of k-means, a point should be assigned to the nearest cluster. Take a point x in Figure 1 as an example. Assume the distance from x to the centroid of Clus-will be assigned to Cluster2 improperly. To avoid the mistake, we can make a trans-d1&lt;d2 , and thus point x will be assigned to Cluster1 more reasonably. An intuitive solution is to use Gaussian Mixture Model together with Expectation-Maximization [14] which would estimate the parameters of different distributions. However, the disadvantage is that GMM could not be directly applied in a sparse and high dimensional feature space and therefore high computational operation (i.e., LSI) as our proposed solution to address the model misfit problem, would not bring in any time consuming operations. 3.2 Rationale In our solution of M-R framework, we analyze the data distribution and transform it to better fit the model assumptions. However, the analysis work in a high dimensional impractical. To solve this problem, we first map all documents into a lower dimen-sional space which is suitable for distribution analysis. 
Suppose we have a dataset of n documents which contains k classes/clusters clusters are n 1 ,...,n i ,...,n k . Let multiple classes, a matrix W constructed with the best set of discriminant vectors can be obtained by maximizing the between-class scatter T B WSW and minimizing the within-class scatter T W WS W . This is equivalent to solve the criterion function de-fined as () TT BW J = WWSWWSW , where between-class scatter matrix and class scatter matrix . In our framework, we assume that the rescaling operation would tion, only the maximization of the between-class scatter will be considered. Thus, our criterion function is defined as The columns of W could be obtained by solving the eigenvectors of S B . Specifically, mmmm m m (1  X  i  X  k) . Therefore, we may directly use this set of direc-tions to construct a new coordinate as well. mmmm m m (1  X  i  X  k) as the directions of coordinate axes, This coor-dinate system is called M-R coordinate. The coordinate value of point x j in M-R coor-Although the choice of axes in M-R coordinate may not be optimal (better axes could be obtained by applying orthogonalization to those directions), there are proper fea-set of directions 1 ,... ,..., ik  X  X  X   X  mmmm m m (1  X  i  X  k) optimizes the criterion function distinguish corresponding cluster C i and other clusters. Suppose a partition containing two clusters, one of the clusters consists of documents in C i and the other one consists l dmm . And this direction is identical with i  X  mm because Therefore, for every cluster in the dataset, there is a corresponding axis which gives a axis could be applied according to the distribution statistics of the current cluster. 
We apply the rescaling operation by designing rescaling functions on all dimen-sions according to the distribu tion statistics of data. Assume the rescaling functions of could be either linear functions or nonlinear functions. Especially, if the adopted func-them into traditional distance measure, more simplified form of the rescaling opera-tion could be obtained. M-R Distance: If the adopted rescaling functions on all dimensions are linear func-Euclidian distance to form a new distance measure, referred as M-R distance: where i  X  (1  X  i  X  k) could be regarded as the rescaling coefficients of different axes. The that the coordinate values on different axes are more comparable. 
Considering that the rescaling operations ar e applied to regularize the data distribu-tion, an intuitive but effective solution is to choose a statistic that reflects the distribu-tion characteristics on the corresponding direc tions as the rescaling coefficient. Since the standard deviation is a parameter that reflects how a distribution spreads out, it is a reasonable choice for the rescaling coeffici ent. Thus, we can calculate the standard fore, Formula (3) could be represented as: where sponding directions are regula rized according to their standard deviations. As a result, the scales of different axes are more comparable and the distance measure is more reasonable. 
From another perspective, the rescaling operation could be regarded as a kind of space transformation. The feature space is transformed according to the rescaling coefficient, such that under the new scale the distribution of the dataset fits the model assumptions of the algorithm better. It is worth noting that the choice of 1 ii  X   X  = is of datasets to bring better clustering results. 3.3 The Framework Our solution is to execute the given clustering algorithm to generate a rough partition M-R coordinate could be constructed and rescaling coefficients could be calculated via statistical analysis. With more reasonable distance measure in the M-R coordinate, all documents could be re-clustered under the given clustering algorithm. The new conduct the M-R framework iteratively along with the clustering algorithm to con-stantly improve the clustering performance. And the optimization procedure is quite framework and the clustering partition will be improved simultaneously until the final result is obtained. For any clustering algorithm, the M-R framework works as bellow: To give a direct example, we apply our M-R framework on the traditional k-means to form the M-R k-means algorithm. For a dataset with n documents, the M-R k-means clustering algorithm works as follows. 
The parameter r that controls the initial k-means iteration time is a small integer to guarantee fast generation of the rough partition. Usually, r=2 or 3 . Obviously, the time complexity of generating the rough partition of k-means is ment account. In each iteration of M-R k-means, there are mainly three kinds of op-erations: adjusting the M-R coordinate, calculating the coordinate values of every document, and calculating the document-to-cluster distances while finding the nearest cluster. The time complexity of adjusting the M-R coordinate is O(n) which is mainly induced by updating centroids of all clusters. When calculating the coordinate values so the time complexity is O(kn) . With new coordinate values calculated, the dimen-sionality is reduced to k , i.e. the number of clusters. Comparing with the operations in the original space with tens of thousands of dimensions, the complexity of the docu-ment-to-cluster distance computation in the new coordinate could be omitted. There-fore, the time complexity of each iteration is O(n)+O(kn)=O(kn) . Assume that the the same as k-means. In this section, we describe the datasets and evaluation measures used in our experi-ments, analyze the model misfit problem on the real datasets and eventually conduct experiments to prove the effectiveness of our M-R framework. 5.1 The Datasets In our experiments, we use two corpora: RCV1-v2 [17] and 20Newsgroup [18]. 
RCV1-v2. The RCV1 dataset contains a corpus of more than 800,000 newswire stories in 103 classes from Reuters. From RCV1 we randomly select documents to construct a series of datasets (R1, R2, R3, R4 and R5) with cluster numbers vary from 7 to 15. And the document numbers vary from 1932 to 3330. 20NewsGroup. The 20Newsgroup (20NG) contains approximately 20,000 articles evenly divided into 20 Usenet newsgroups. From 20NG, we construct another series of datasets (N1, N2, N3, N4 and N5) by randomly select classes. The cluster numbers of these datasets vary from 6 to 15, and document numbers vary from 2112 to 3406. 
The overview of the 10 datasets is illustrated in Table 1. Both RCV1 series and 20NG series are designed to keep a large range on cluster numbers and document numbers. The reason is that we want to give a global view of performance comparison on datasets of different sizes. Stop words are removed, and simple feature selection is applied, e.g., words appear in less than three documents or more than 80% of the documents are automatically removed. Finally, the normalized VSM vector of every document is calculated. 5.2 Evaluation Measures For clustering, there are many different quality measures, among which the most commonly used ones are F-measure and entropy. F-measure combines the precision and reca ll metrics from information retrieval. For a given class in the dataset, the F-measure is determined by the most similar clus-finally averaged to form the total F-measure. 
Entropy is a measure that analyzes the homogeneity of all clusters (with the caveat The total entropy for a set of clusters is calculated as the sum of the entropy of each cluster weighted by the size of each cluster. 
Both F-measure and entropy are used to evaluate our experimental results. 5.3 Model Misfit Problem Verification As presented before, the M-R framework is proposed since the data distribution often misfit the model assumptions of the clustering algorithms. Here we conduct experi-sponding axes defined in the M-R coordinate. Therefore, standard deviation is also used to illustrate the model misfit problem in our experiment. As an example, we choose the datasets with most clusters and documents (R5 and N5) to verify the model misfit problem. For both datasets, we first construct the M-R coordinate according to the document labels and then calculate the standard devia-tions of clusters X  projections on corresponding axes. Those standard deviations are plotted in the form of column sections in Figure 4. As shown in this figure, for data-ID=11). And for N5, the values vary from 0.038 (class ID=8) to 0.077 (class ID=3). the misfit between data distribution and algorithm model. Therefore, it is improper to run clustering algorithms with original model assumptions while ignoring the irregu-lar nature of real data distributions. It is better to combine clustering algorithms with M-R framework which is capable of normalizing the irregular data. 5.4 Performance Improvement In this section, we conduct experiments to verify the performance of M-R framework. Three clustering algorithms are included in our experiments for performance com-parison. They are k-means, M-R k-means and spectral clustering with normalized cut (Ncut) [11]. K-means and M-R k-means are compared to verify the performance im-provement of M-R framework. Ncut is selected since it is one of the most successful clustering algorithms proposed by researchers in recent years. We can demonstrate the effectiveness of our M-R framework through the comparison between our approach and the state-of-the-art clustering method. 
The k-means and M-R k-means in our experiments are implemented with C++ lan-guage while the Ncut code is obtained from Spectral Clustering Toolbox provided by University of Washington 1 . For all algorithms in our experiments, i.e. k-means, Ncut and M-R k-means, the algorithm results are affected by the selection of initial points. run, k-means, Ncut and M-R k-means are launched with the same set of randomly runs for comparison. For experiments of every dataset, the cluster number of all algo-distance in k-means for comparison with our M-R distance. The k-means algorithm achieved. For Ncut, the convergence criterion is set to the same value with K-means. For M-R k-means we run 3 iterations of k-means to generate a rough partition to setup the M-R framework. M-R k-means iterates until no documents shift to other clusters or maximum iteration time (which is also set to 20) is achieved. The F-measure and entropy results of the experiments are shown in Table 2 and view of performance comparison. From the results we observe that: 1. Comparing with k-means, M-R k-mean s achieves overall improvement in all datasets on both F-measure and entropy scores. It proves the effectiveness of M-R framework. 2. As for comparison with Ncut, the result is quite interesting. When cluster num-ber is small, the performance of Ncut is superior to M-R k-means. However, with the growth of cluster numbers, M-R k-means is capable of getting comparable results with Ncut. In detail, the results of M-R k-means are comparable to or slightly weaker than Ncut on datasets of RCV1 series, and are comparable to or even better than Ncut on datasets of 20NG series when the cluster number is large. Here we give some brief would be very good. However, when k increases, the quality of new feature space would decrease. On the other hand, for our M-R k-means, the improvement compar-ing with k-means is quite significant and stable. As a result, for large datasets, M-R k-means would generate comparable results with Ncut. We omitted the execution time comparison in this section because we have proved that the time complexity of M-R k-means remains the same as k-means in Section 4. That is to say, M-R k-means is capable of executing as fast as k-means but generating better result. In this paper, we propose a novel clustering framework based on adaptive space map-ping and rescaling, referred as the M-R framework. The M-R framework maps all documents into a low dimensional coordinate which is constructed with respect to the cluster centers. In this way, the distribution statistics of each cluster could be analyzed on the corresponding dimension. A rescaling operation is then conducted with respect By conducting the framework iteratively along with the clustering algorithm, we are able to constantly improve the clustering performance. It is worth noting that M-R framework does not introduce any time consuming operation to the original algo-framework, traditional algorithm like k-means is capable of achieving comparable clustering performance with respect to the state-of-the-art methods. 
The distribution regularization idea introduced by M-R framework is novel and in-teresting, and is applicable to more text mining areas. In future work, we will focus on theoretical study of our M-R framework and try to apply the framework to supervised learning area like text classification. 
