 H.5.2 [ Information Interfaces and Presentation ]: User Interfaces  X  interaction styles, natural language. Algorithms, Experimenta tion, Human Factors Information Intent, Weak Labeling, Domestication In [4] we describe the Virtua l Information Officer (VIO), a system designed to determine us er intent from natural language messages and assist with task completion. VIO assists users with requests that specify a single form filling task and include all the information needed to execute it. There are four main functions that VIO performs in order to assi st the user: form ranking, form field analysis, entity resolution and user interface enhancement (Figure 1). Additionally, VIO logs values when the user submits the completed task. These activity logs are used to generate weak labels that drive the lear ning of the system. Form ranking is handled as classification pr oblem between all possible form targets to allow the system to suggest the most likely forms. Form field analysis is handled as a natural language text extraction problem with one trained model per field that appears on any form. Entity resolution is based on a learning reference resolution model that finds entities most likely to be mentioned in the message. The user interface enhancements take the results of the machine learning co mponents and use them to automatically fill the target fo rm with suggestions. Augmenting existing forms with suggestions can result in a confusing and error prone experience. In [3] we describe the extensive design of a suggestion interface attuned to a mixed-initiative dialog between the user and VIO. Traditional natural language processing techniques can determine user intent for a well understood domain. However, there are two major barriers to deploying such a system: large amounts of domain specific e ngineering and hand labeling training data for machine learning components. VIO is explicitly constructed to test the hypothesi s that NLP can be successfully used without extensive domain engineering and without hand labeling of corpora. VIO uses meta-data descriptions of the forms along with logs generated by users completing tasks. The system has no domain specific engineering and uses only knowledge that is contained in the meta-data, a dictionary of common names, or in the existing state of the database. The system starts out  X  X ntrained X  and learns as it observes users completing tasks. fields. In the McCullar example, the annotators successfully extracted his title, first name, last name, and organization. His record already had most of the information, and the system automatically suggested the new title. This leaves the task of updating the street address and room number to the user. In this example, the system correctly identified both the form and the record thus greatly reducing navigation time. Of the three fields to be updated, one was automatically filled. In the absence of suggestions, the inte rface works as a traditional form system allowing the user to naturally complete the task. Once the user finishes and presses commit, the system writes several values to its set of weak labels. It records the form that was used, the record that was updated, and the original and final values of every field on the form. Form ranking is driven by a k -way boosted decision tree classifier with a simple bag of words feature set to generate the ranked list. One classification mode l is trained per form and all the confidence weights are ranked and subjected to a threshold. VIO generates a set of anchored labels from the field information in the weak labels using the domestication algorithm. This algorithm attempts to locate similar strings in the source document and label them . Those labels are used to train conditional random field a nnotators that generate the extractions for the form field analysis step. VIO looks up each of the extractions in a dictionary containing existing database values. A naive bayes classifier learns the relative importance of each field and each match contributes to a per-entity score in our reference resolution algorithm. The entities are then sorted and subjected to a threshold to produce the ranked list of suggestions for the entity resolution step. In order to evaluate the system on a realistic workload, we acquired a corpus of e-mail fro m a departmental webmaster. These messages were anonymized and split into a training set of 195 messages and a test set of 39. The requests in each of the training messages were completed by selecting and completing the appropriate form from a set of website update forms. During this training period, we used an untrained VIO that made no suggestions and simply recorded weak labels. The system was then allowed to train its learned models and we measured its pe rformance in assisting with the remaining messages in the test set. The mean reciprocal rank of the correct form in the form ranking suggestions was 0.94. It was the first or second ranked suggestion for all but one of the test messages. Label domestication was evaluate d using hand generated labels as the standard. The algorithm had fairly uniform performance with an average F1 value of 0.95. Repeated values and embedded formatting requirement s of the form caused the majority of errors. Extraction performance for each field corresponded strongly to the number of domesticated labe ls in the training set. F1 performance values were around 0.86 for common fields with mixed results for the less common ones. For all recall values, 
