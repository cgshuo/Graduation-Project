
Many real world problems exhibit extremely skewed class membership. Some classes (the minority classes) are over-whelmed by the others (the majority classes) in terms of the example numbers. It is often the case that the minority classes are much more important than the majority classes. For example, minority classes can represent fraudulent bank-ing transactions or successful network intrusions -either can represent less than 0 . 01% of normal transactions or network traffic -yet detecting them can prove crucial. In medical diagnoses, some extremely severe diseases may have very few records; however, misdiagnosis of these diseases may have fatal consequences.

What is even more challenging is that such rare categories are often non-separable from the majority classes. For ex-ample, the guileful fraudulent people often try to camouflage their transactions within the normal transactions so that they can bypass the current fraud detection algorithms [5]. For most types of rare diseases, they have identified genetic origins, which involve only one or several genes or chro-mosomal abnormalities. Yet, the remaining vast number of genes and chromosomes behave normally [14].

That said, it is therefore a very important challenge to accurately classify such minority classes given that they are (1) highly skewed and (2) non-separable from the majority classes. In addition, if the minority classes can be character-ized by a concise representation, we may better understand the nature of the minority classes, and thus better identify examples from those classes. In this paper, we refer to this problem as rare category characterization, i.e., characterizing the minority classes for the purpose of understanding and correctly classifying those classes.

In this paper we exploit compactness of rare categories via a new algorithm we call RACH , which represents minority classes as hyperballs. Our key observation is as follows: although the minority classes are non-separable from the majority classes, they often exhibit compactness . That is, each minority class often forms a compact cluster. For example, the fraudulent people often make multiple similar transactions to maximize their profits [5]. For rare diseases, the patients with the same type of rare disease often share similar genes or chromosomal abnormalities [14].

In this paper, we propose RACH by exploring such compactness for rare category characterization. The core of RACH is to represent the minority classes with a hy-perball. We present the optimization framework as well as an effective algorithm to solve it. Furthermore, we show how RACH can be naturally kernelized. We also analyze the complexity of RACH . Finally, we justify the effectiveness of the proposed RACH by both theoretical analysis and empirical evaluations.

The rest of our paper is organized as follows. Related work is reviewed in Section II. In Section III, we propose the optimization framework to provide a compact representation for the minority class with justification, followed by the conversion of this framework to the convex optimization problem as well as its dual form. Then we introduce the RACH algorithm to solve the dual problem with performance guarantee in Section IV, and the kernelized RACH algo-rithm in Section V. Experimental results are presented in Section VI. Finally, we conclude the paper in Section VII.
In this section, we review the related work, which can be categorized into four parts: rare category detection, imbalanced classification, outlier detection and SVM-based algorithms.

Rare category detection is to find at least one example from each minority class with the help of a labeling oracle, minimizing the number of label requests. Up until now, researchers have developed several methods for rare category detection. For example, in [24], the authors assumed a mix-ture model to fit the data, and experimented with different hint selection methods, of which Interleaving performs the best; in [15], the authors studied functions with multiple out-put values, and used active sampling to identify an example for each of the possible output values; in [17], the authors developed a new method for detecting an example of each minority class via an unsupervised local-density-differential sampling strategy; in [11], the authors presented an active learning scheme that exploits the cluster structure in the data, which was proven to be effective in rare category detection; and in [28], to identify anomalies at different scales, the authors created a hierarchy by repeatedly applying mean shift with an increasing bandwidth on the data. Rare category detection can be followed by rare category characterization, i.e., after finding a few (limited) examples from the minority classes, the challenge becomes how to identify most or all rare examples in the unlabeled data.

Imbalanced classification has been well studied over the past decade, and several workshops and special issues have dedicated to this problem, such as AAAI X 2000 workshop on Learning from Imbalanced Data Sets [19], ICML X 2003 workshop on Learning from Imbalanced Data Sets [6], and ACM SIGKDD special issue on Learning from Imbalanced Data Sets [8]. Researchers have proposed many methods to address this problem, such as sampling methods [22][7][10], ensemble based methods [9][27], to name a few. These methods can be used for rare category characterization by returning the unlabeled examples that are classified as minority class examples. However, these methods might not take full advantage of the minority class properties (e.g., the compactness property). Notice that in [29], the proposed method exploits the local clustering within large classes; whereas our method is based on the compactness of the minority classes, i.e., small classes. In addition, we might not be able to obtain a compact representation for the minority classes with these methods.

Outlier Detection refers to the problem of finding pat-terns in data that do not conform to expected behaviors [4]. According to [4], the majority of outlier detection tech-niques can be categorized into classification based [2], near-est neighbor based [25], clustering based [30], information theoretic [18], spectral [13], and statistical techniques [1]. In general, outlier detection finds individual and isolated examples that differ from a given class in an unsupervised fashion. Typically, there is no way to characterize the outliers since they are often different from each other. On the other hand, in rare category characterization, we are given labeled examples from a certain minority class, and hope to find similar examples from the same minority class by charac-terizing the distribution of this class. Some work addresses the case where the outliers are clustered [23]. However, they still assume that the outliers are separable from the normal data points. Notice that the work proposed in [16] uses a hyperball-based representation for the normal class, or majority class. It assumes that the outliers are scattered and bear no-similarity among themselves. In contrast, our work uses a hyperball-based representation for the minority classes, and we assume that the rare examples from the same class are self-similar. We do not require the support region of the majority class to be a hyper-ball, i.e., we make no clustering or compactness assumption for the majority class.
SVM-based algorithms are also related to rare category characterization. Firstly, our proposed method is motivated by one-class SVM [26], which is used to estimate a subset of the input space such that the probability of finding an example outside the subset is small, given examples from a single class. However, it does not apply in our settings since we deal with data from multiple classes (one majority class and one minority class); and we have access to a set of labeled examples from both the majority and the minority classes. In addition, we will make use of the unlabeled data to further improve the performance of our method. Secondly, our method is also related to SVM-Perf, which is an implementation of the SVM formulation for optimizing multivariate performance measures [21]. SVM-Perf is optimizing an upper bound on the training loss regularized by the L 2 norm of the weight vector [21]. However, the compactness property of the minority classes is not exploited in SVM-Perf. In addition, with SVM-Perf, we can not obtain a concise representation for the minority classes.

In this section, we present our optimization framework, after we introduce the notation and the pre-processing step. A. Notation and Assumptions
Notation. For the sake of simplicity, we assume that there is only one majority class and one minority class in the data set. (Multiple majority and minority classes can be converted into several binary problems.) Throughout this paper, we will use bold lower-case letters to denote vectors, normal letters (lower-case or upper-case) to denote scalars, and calligraphic capital letters to denote sets. Let x 1 ,..., x n 1  X  R d the labeled examples from the majority class, which corre-spond to y i =1 ,i =1 ,...,n 1 ; let x n 1 +1 ,..., x n 1 denote the labeled examples from the minority class, which correspond to y j =  X  1 ,j = n 1 +1 ,...,n 1 + n 2 ; let x Here, n 1 , n 2 , and n denote the number of labeled examples from the majority class, the number of labeled examples from the minority class, and the total number of examples, both labeled and unlabeled. d is the dimensionality of the input space. Our goal is to identify a list of unlabeled examples which are believed to come from the minority class with high precision and recall.

Assumption. In many imbalanced problems, it is often the case that the rare examples from the same minority class are very close to each other, whereas the examples from the same majority class may be scattered in the feature space. This assumption is also used in [17][29][24], etc, when dealing with imbalanced data sets, either explicitly or implicitly. Furthermore, we also assume that the rare examples can be enclosed by a minimum-radius hyperball in the input space without including too many majority class examples. This seemingly rigorous assumption will become more flexible when we use the high-dimensional feature space instead of the input space via the kernel trick in Section V. With this assumption, we allow the support regions of the majority and minority classes to overlap with each other.
 B. Pre-processing: Filtering Algorithm 1 Filtering Process for Rare Category Character-ization 1: if n 2 &gt; 1 then 2: Estimate the center c of the hyperball by one-class 3: else 4: Set the center c = x n 1 +1 5: end if 6: for i = n 1 + n 2 +1 ,...,n do 7: Calculate the distance d i = x i  X  c 8: end for 10: n = n 1 + n 2 11: for i = n 1 + n 2 +1 ,...,n do 13: n = n +1 , x n = x i 14: end if 15: end for
In the unlabeled data, examples far from the hyperball may be safely classified as belonging to the majority class without materially affecting the performance of our classi-fier. Therefore, we first filter the unlabeled data to exclude such examples from the following optimization framework, and only focus on the examples that are close to the hyperball. The filtering process is described in Alg. 1. It takes both the labeled and the unlabeled examples as input, and outputs a set of unlabeled examples which are close to the hyperball. Here, n  X  n 1  X  n 2 is the number of unlabeled examples after the filtering process. The algorithm works as follows. It first estimates the center c of the hyperball using one-class SVM [26] or a single labeled example; then it estimates the proportion p of the rare examples in the unlabeled data using the labeled data; finally, it calculates the distance threshold D thre based on p , which is used to filter out the unlabeled examples far away from the hyperball. Notice that 3  X  D thre is actually used to filter the unlabeled data. This is to ensure that we do not miss any rare example. We should point out that the filtering process is orthogonal to the other parts of the proposed algorithm. In the remainder of this paper, unlabeled data (unlabeled examples) refer to the examples output by the filtering process.
 C. Problem Formulation
Now, we are ready to give the problem formulations for rare category characterization. We first give its original formulation and illustrate its intuitions. Then, we present its convex approximation together with its dual form.
Original Formulation. To find the center and radius of the minimum-radius hyperball, we construct the following optimization framework, which is inspired by one-class SVM [26].

Problem 1. min where R is the radius of the hyperball; c is the center of the hyperball; C 1 and C 2 are two positive constants that balance among the three terms in the objective function;  X  and  X  correspond to the non-negative slack variables for the labeled examples from the majority class and the unlabeled examples;  X  i and  X  k are the i th and k th component of  X   X  respectively.

In Problem 1, we minimize the squared radius of the hyperball and a weighted combination of the slack variables. Furthermore, we have three types of constraints with respect to the training data. The first type is for the labeled examples from the majority class, i.e., they should be outside the hyperball. Notice that these are not strict constraints, and the labeled examples from the majority class falling inside the hyperball correspond to positive slack variables  X  i .In this way, we allow the support regions of the majority and minority classes to overlap with each other. The second type is for the labeled examples from the minority class, i.e., they should be inside the hyperball. In contrast, these are strict constraints, and the hyperball should be large enough to enclose all the labeled rare examples. The last type is for the unlabeled examples, i.e., we want the hyperball to enclose as many unlabeled examples as possible. Different from the second type of constraints, these constraints are not strict, and the examples falling outside the hyperball correspond to positive slack variables  X  k . The intuition of this type of constraints is that after the filtering process, the unlabeled examples are all in the neighborhood of the minority class. The support region of the minority class should have a higher density compared with the rest of the neighborhood. Therefore, we want the hyperball to enclose as many unlabeled examples as possible.

Convex Approximation of Problem 1. Note that Problem 1 is difficult to solve due to the first type of constraints on the labeled examples from the majority class, which make this framework non-convex in the center c . To address this problem, we approximate these constraints based on first-order Taylor expansion around the current center  X  c , and have the following optimization problem, which is convex.
Problem 2. (Convex Approximation of Problem 1) min
Based on Problem 2, we find the solution to Problem 1 in an iterative way. To be specific, in each iteration step, we form Problem 2 based on the current estimate  X  c of the center, find the optimal R 2 , c ,  X  and  X  , and then update Problem 2 based on the new center c . We stop the iteration until the solution in two consecutive steps are very close to each other or the maximum number of iteration steps is reached.

Dual Problem for Problem 2. It is obvious that Problem 2 satisfies Slater X  X  condition [3]. Therefore, we solve this problem via the following dual problem.

Problem 3. (Dual Problem for Problem 2) where  X  is the vector of Lagrange multipliers,  X  i , i = 1 ,...,n 1 are associated with the constraints on the labeled examples from the majority class,  X  j , j = n 1 +1 ,...,n n 2 are associated with the constraints on the labeled exam-ples from the minority class, and  X  k , k = n 1 + n 2 +1 ,...,n are associated with the constraints on the unlabeled exam-ples. Furthermore, based on the KKT conditions of Problem 2, the center c of the hyperball can be calculated as follows.
Here, we present the proposed optimization algorithm to solve Problem 1. The basic idea is as follow: after an initialization step; we will recursively formulate Problem 2 using the current estimate  X  c for the center of the hyperball; and then solve Problem 2 in its dual form (Problem 3) by a projected subgradient method.
 A. Initialization Step
First, we need to initialize the center c of the hyperball and the Lagrange multipliers  X  in Problem 3, which is summarized in Alg. 2. It takes as input both the labeled and the unlabeled examples (after the filtering process), and outputs the initial estimates of the center c and the Lagrange multipliers  X  . In Step 1, it initializes the center c and the radius R of the hyperball using one-class SVM [26] if we have more than one labeled examples from the minority class; otherwise, it uses the only labeled rare example as the center c , and the smallest distance between this example and the nearest labeled example from the majority class as R .In Step 2, it initializes the Lagrange multipliers based on the KKT conditions of Problem 1. For a labeled example from the majority class, if its distance to the center c is bigger than R ,  X  i =0 ; if the distance is less than R ,  X  i = C and if the distance is equal to R ,weuse C 1 2 as the value for  X  i . For a labeled example from the minority class, if its distance to the center c is less than R ,  X  j =0 ; otherwise, we use C 1 + C 2 2 as the value for  X  j . For an unlabeled example, if its distance to the center c is less than R ,  X  k =0 ; if the distance is bigger than R ,  X  k = C 2 ; and if the distance is equal to R , we use C 2 2 as the value for  X  k .
 B. Projected Subgradient Method for Problem 3
Projected subgradient methods minimize a convex func-tion f (  X  ) subject to the constraint that  X   X  X  , where X a convex set, by generating the sequence {  X  ( t ) } via size, and X ( x )=argmin y { x  X  y : y  X  X } is the Euclidean projection of x onto X . To solve Problem 3, the gradient descent step is straight-forward. 1 Next, we will focus on the projection step, where X = {  X  :1+ n 1 i =1  X  n +1 ,...,n 1 + n 2 ;0  X   X  k  X  C 2 ,k = n 1 + n 2 +1 ,...,n Algorithm 2 Initialization for RACH Input: x 1 ,..., x n Output: initial estimates of c and  X  1: if n 2 &gt; 1 then 2: initialize the center c and the radius R of the hyperball 3: else 4: set c = x n 1 +1 , and set R as the smallest distance 5: end if 6: Initialize  X  as follows.

In the projection step, we consider the following optimiza-tion problem.

Problem 4. (Projection Step of Problem 3) where a i ( i =1 ,...,n ) denote a set of constants which are either 1 or -1; z is a constant; v can be seen as the updated vector for  X  based on gradient descent in each iteration step of the projected subgradient method, or  X  ( t )  X   X  t  X  (  X  i is the upper bound for  X  i . Without loss of generality, we assume that  X  i &gt; 0 , i =1 ,...,n . For this optimization problem, define S + = { i :1  X  i  X  n, a i =1 } , and S  X  = { i :1  X  i  X  n, a i =  X  1 } .

Before we give our optimization algorithm for Problem 4, we first give the following lemma, which is the key for solving Problem 4. 2 Lemma 1: Let  X  be the optimal solution to Problem 4. Let s and t be two indices such that s, t  X  S + or s, t  X  other hand, let s and t be two indices such that s ,t  X  S or s ,t  X  S  X  , and v s  X   X  s &lt;v t  X   X  t .If  X  s =  X  s  X  t must be  X  t as well.

L (  X  , X ,  X  ,  X  )= where  X  is a Lagrange multiplier associated with the equality constraint;  X  and  X  are two vectors of Lagrange multipliers associated with the inequality constraints whose elements are  X  i and  X  i respectively. Taking the partial derivative of L (  X  , X ,  X  ,  X  ) with respect to  X  and set it to 0, we get For the first half of Lemma 1, suppose that s, t  X  S + .If  X  s =0 and  X  t &gt; 0 ,wehave  X  s  X   X  0 . Therefore, v if  X  s =0 ,  X  t must be zero as well. Similar proof can be applied when s, t  X  S  X  . For the second half of Lemma 1, suppose that s ,t  X  S + .If  X  s =  X  s and  X  t &lt; X  t ,we have  X  s =0 ,  X  s  X  0 ,  X  t  X  0 and  X  t =0 . Therefore,  X   X  +  X  t &lt; 0 , which can not be satisfied simultaneously since v s, t  X  S  X  .

Besides the vector v , define the vector v such that its i th element v we can keep two lists: the first list sorts the elements of whose indices are in S + ( S  X  ) in an ascending order, and only a top portion of the list corresponds to 0 in  X  ; the second list sorts the elements of v whose indices are in S + ( S  X  ) in a descending order, and only a top portion of the list corresponds to the elements of  X  that reach their upper bounds. For the remaining indices in S + ( S  X  ), their corresponding elements in  X  are between 0 and the upper bound, and the Lagrange multipliers  X  i =  X  i =0 . Therefore, with respect to the value of  X  , we have the following lemma. Lemma 2: Let  X  be the optimal solution to Problem 4. to the elements in  X  that are equal to 0, equal to the upper bound, and between 0 and the upper bound respectively. S of S  X  which correspond to the elements in  X  that are equal to 0, equal to the upper bound, and between 0 and the upper bound respectively. S 1  X  S 2  X  S 3  X  = S  X  .  X  can be calculated as follows.  X  = k S  X  , S  X  j  X  S 2 + S 2  X  ,  X  j =  X  j ,  X  j =0 ,  X  j  X  0 ;  X  k  X  S 3 0 &lt; X  k &lt; X  k ,  X  k =  X  k =0 . Furthermore,  X  k  X  S 3 v  X   X  ;  X  k  X  S 3  X  ,  X  z = Solving this equation regarding  X  , we get Equation 3.
Based on Lemma 1 and Lemma 2, to solve Problem 4, we gradually increase the number of elements in S 1 + , S 2 + and S 2  X  , calculate  X  accordingly, and determine the value of  X  which has the smallest value of 1 2  X   X  v 2 2 . Alg. 3 gives the details for solving Problem 3 in RACH .InStep3of Alg. 3, we calculate the gradient of the objective function in Problem 3 at the current value of  X  and  X  c ; then in Step 4,  X  is updated via gradient ascent to obtain v . The remaining steps (Step 5-16) are for the projection step (i.e., for solving Problem 4): in Step 5, we calculate the vector v using both v and the upper bounds; to calculate the projection of v on X , in Step 7 to Step 15, we try different sizes for S 1 + , S S  X  and S the projection of v based on the distance between v and w , where w is calculated based on the current sets S 1 + , S S C. RACH for Problem 1
Algorithm. Now, we are ready to present the RACH algorithm (Alg. 4) to solve Problem 1. It is given the training data, the step size  X  , C 1 , C 2 , and the numbers of iteration steps N 1 , N 2 . (Note that N 2 is used in Alg. 3 in Step 3 of RACH .) The output is the unlabeled examples whose predicted class labels are  X  1 . RACH works as follows. It first initializes the center c and the Lagrange multipliers using Alg. 2; then it repeatedly forms Problem 3 based on the current estimate of the center c , and applies Alg. 3 to solve it, which is based on the projected subgradient method; after solving Problem 3, the center c is updated using Equation 1; finally, we classify the unlabeled examples based on their corresponding Lagrange multipliers  X  k . The last step can be justified as follows. In Problem 1, for the unlabeled instances x , k = n 1 + n 2 +1 ,...,n ,if x k  X  c &lt;R ,  X  k =0 , then y =  X  1 ;if x k  X  c = R , 0 &lt; X  k &lt;C 2 , then y k =  X  1 ;if x Concise Representation of the Minority Class. From Alg. 4, we can also compute the radius R of the hyber-ball, which is the maximum distance from the center c to x corresponding upper bounds. The resulting hyberball (fully described by the center c and the radius R ) provides a con-cise representation for the minority class. This representation can help us better understand the minority class. We can also use it to predict an unlabeled example as follow: if it is within the hyberball (i.e., its distance to the center less than R ), we classify it as a rare example; otherwise, it belongs to the majority class.
 Algorithm 3 Projected Subgradient Method for Problem 3 Input: x 1 ,..., x n ; step size  X  ; C 1 , C 2 ; N 2 ;  X  c Output:  X  1: Define S + = { n 1 +1 ,...,n } and S  X  = { 1 ,...,n 1 } 2: for step =1 to N 2 do 3: Calculate  X  as follows: l =1 ,...,n 1 :  X  + l = n 1 +1 ,...,n :  X  ( 4: Update  X  via gradient ascent to obtain: v =  X  +  X   X  5: Calculate v as follows: 6: Set D =  X  7: for I 1 =1 ,I 2 =1 ,I 3 =1 ,I 4 =1 to I 1 = | S + | + 8: Let S 1 +  X  S + denote the subset of indices in S + 9: If S 1 + S 2 + =  X  or S 2 + { n 1 +1 ,...,n 1 + n 2 } = 10: Let S 1  X   X  S  X  denote the subset of indices in S  X  11: If S 1  X  S 2  X  =  X  , continue; otherwise, S 3  X  = 13: Calculate w as follows: w i =0 ,i  X  S 1 + S 1  X  ; 14: If v  X  w &lt;D , set  X  = w and D = v  X  w . 15: end for 16: end for
Computational Complexity of RACH. It can be shown that the time complexity of RACH is O ( N 1 N 2 ( n  X  n 1 ) 2 ( n In practice, we can reduce the running time in the following three ways. First, we find that in our experiments RACH converges very quickly, often within a few tens of iterations. Second, in the applications that we are interested in, there are very few labeled examples from both the majority and the minority classes. A typical value for n 1 is a few tens, and a typical value for n 2 is less than 10. Finally, recall that in Section III, we have applied Alg. 1 to filter out the unlabeled examples which are far away from the minority class. After this operation, only a small portion of the unlabeled data (typically less than 10% ) is passed on to Alg. 4. Algorithm 4 RACH : Rare Category Characterization Input: x 1 ,..., x n ; step size  X  ; C 1 , C 2 ; N 1 , N 2 Output: unlabeled examples whose predicted class labels 1: Initialize the hyberball center c and the Lagrange mul-2: for step =1 to N 1 do 3: Update the Lagrange multipliers  X  by Alg. 3 based 4: Update the center c based on Equation 1 5: end for 6: for k = n 1 + n 2 +1 to n do 7: if  X  k &lt;C 2 then 8: set y k =  X  1 9: else 10: set y k =1 11: end if 12: end for In this section, we briefly introduce how to generalize RACH to the high-dimensional feature space induced by kernels. The major benefit of kernelizing the RACH algo-rithm is that, instead of enclosing the rare examples by a minimum-radius hyperball, we can now use more complex shapes, which make our algorithm more flexible and may lead to more accurate classification results.

Compared with the original Alg. 4 which is designed for the input space, in the kernelized RACH algorithm, we only need to make the following changes. First, instead of directly calculating the center c , we keep a set of coefficients  X  , i =1 ,...,n such that c = n i =1  X  i x i . Therefore, Step 1 of Alg. 4 generates a set of initial coefficients for c , and Step 4 updates the coefficients of c based on Equation 1. In this way, c  X  x = n i =1  X  i K ( x i , x ) , and kernel function. Next, notice that Alg. 4 and Alg. 3 are de-pendent on the examples only through the inner products or distances, which can be replaced by the kernel calculation.
In this section, we present some experimental results showing the effectiveness of RACH . A. Synthetic Data Set
Fig. 1(a) shows a synthetic data set where the majority class has 3000 examples drawn from a Gaussian distribution, and the 4 minority classes correspond to 4 different shapes with 84, 150, 267, and 280 examples respectively. In this figure, the green circles represent labeled examples from the minority classes, and the red pluses represent labeled examples from the majority class. Here we construct 4 binary problems (the majority class vs. each minority class). Fig. 1(b) shows the classification result where the green dots represent the rare examples, and the red dots represent the majority class examples. From this figure, we can see that almost all the rare examples have been correctly identified except for a few points on the boundary.
 B. Real Data Sets
We also did experiments on 7 real data sets, which are summarized in Table I. For each data set, we construct several binary problems consisting of one majority class and one minority class, and vary the percentage of labeled examples. For the sake of comparison, we also tested the following methods: (1) KNN (K-nearest neighbor); (2) Manifold-Ranking [31]; (3) Under-Sampling the majority class until the two classes are balanced and training with SVM; (4) TSVM [20] with different costs for the examples from different classes; (5) SVM-Perf [21]. We used the RBF kernel in RACH . All the parameters are tuned by cross validation. The comparison results in terms of the F-score (harmonic mean of precision and recall) of the minority class are shown in Fig. 2 to Fig. 8. Under each figure, the numbers outside the brackets are the class indices included in the binary problem, and the numbers inside the brackets are the number of examples from each class. Notice that in all the figures, the label percentage ranges from 5% to 25% . This is because in our applications, we are only interested in the cases where the percentage of labeled examples is small.
From these figures, we can see that in the majority of our experiments RACH outperformed all other methods, and in the remaining ones it was virtually indistinguishable from the top performer, such as Fig. 3(b) and Fig. 7(a). In particular, the performance of RACH is better than SVM-Perf in most cases, although the latter directly optimizes the F-score. This might be due to the fact that the objective function of SVM-Perf is only an upper bound of the training loss regularized by the L 2 norm of the weight vector. And also, SVM-Perf is designed for the purpose of a general classification problem; and it might ignore the skewness and the compactness properties of the minority class. On the other hand, the performance of the other methods varies a lot across the different data sets. For example, in Fig. 2(a), the performance of KNN is only worse than RACH ; whereas in Fig. 3(b), the performance of KNN is worse than TSVM, and as the percentage of labeled examples increases, KDD performs not as good as Under-Sampling.
In this paper, we proposed the RACH algorithm to address the problem of rare category characterization, which is used for understanding and correctly classifying the rare categories. We addressed the challenging case where the data set is highly skewed and the minority class is non-separable from the majority class. The basic idea is to enclose the rare examples with a minimum-radius hyperball by exploring the compactness property of the minority class. We formulate it as an optimization problem and present the effective optimization algorithm RACH .In RACH , we repeatedly (1) convert the original problem into a convex optimization problem, and (2) solve it in its dual form by a projected subgradient method. Furthermore, we generalize RACH to the high-dimensional feature space induced by kernels. In the majority of our experiments RACH outperformed all other methods, and in the remaining ones it was virtually indistinguishable from the top performer.

