 The growing aging population in the coming decades will result in many complications for families, society, and the government. According to the latest available statistics from 2009, the older population of the United States numbers 39 . 6 million (about 12 . 9% of the U.S. population), and this number is expected to grow to 19% of the population by 2030 [AOA 2011]. The increasing aging population will result in many new challenges and problems, such as the shortage of healthcare professionals and care facilities, an increase in age-related diseases, and rising healthcare costs. An estimated 9% of adults age 65 + and 50% of adults age 85 + need assistance with everyday activities due to physical and cognitive decline. For instance, Alzheimer X  X  disease is a major concern for the older population. An estimated 5 . 4 million Americans had Alzheimer X  X  disease in 2011, and by 2040, approximately 11 million people will be suffering from cognitive decline due to Alzheimer X  X  [Hebert et al. 2001].

As a result of the increasing aging population and its related mental and phys-ical complications, these is a rising interest in assisted-care and health-monitoring technologies based on ubiquitous computing and ambient intelligence in order to lower healthcare costs and to avoid unnecessary institutionalization. An example of assisted-care systems is a remote health-monitoring and intervention system. These systems are able to monitor and track the daily activities of older adults. Activities of daily living (ADL) [Reisberg et al. 2001] is a term that is used widely by geriatric specialists to refer to basic self-care activities that determine the independence of an older adult in daily life. The basic set of ADL activities include personal hygiene, dressing, self feed-ing, and taking medicine. The ability to perform ADLs independently and completely on a regular basis provides a measurement of the functional status of a person and the ability to live independently at home [McDowell and Newell 1996]. In a recent report by Rialle et al. [2008], family caregivers of Alzheimer disease X  X  patients were surveyed about assisted-care technologies. Most caregivers ranked activity identification and tracking at the top of their list of needs.

In response to this need, researchers are building remote ADL monitoring and inter-vention systems based on ambient intelligence technology. Not only can such systems identify and track daily activities, but they also can provide timely prompts if the resi-dent has difficulty completing a task or remembering the activity steps. Such systems allow the elderly and people with special needs to live more independently in the com-fort of their own home, while reducing the burden of caregivers by avoiding constant monitoring. The widespread use of such systems in the future will also significantly reduce healthcare costs by avoiding unnecessary institutionalization.

In this article, we use smart home technology [Rashidi and Cook 2009; Cook et al. 2003; Doctor et al. 2005; Abowd and Mynatt 2004; Helal et al. 2005] for monitoring the activities of older adults. A smart home can be considered as any regular home which has been augmented with various types of sensors, such as infrared motion sensors for tracking the movement of residents at home. Such sensors can provide rich context information about the environment and its inhabitants. Smart homes can be used for various purposes, such as security, energy conservation, home automation, and of course for assisted care of older adults [Gopalratnam and Cook 2007; Rashidi and Cook 2008; Yiping et al. 2006; Wren and Munguia-Tapia 2006].
 The main component of an ADL monitoring system is an activity recognition module. The activity recognition system recognizes resident X  X  activities from low-level sensor data. A number of supervised activity recognition methods have been proposed in the literature [Mozer et al. 1998; Brdiczka et al. 2005; Philipose et al. 2004; Zhao et al. 2010; Maurer et al. 2006; Liao et al. 2005; Inomata et al. 2009]. Supervised methods have the drawback that they require labeled data for training the recognition algorithm. Annotating human activity data from low-level sensors is a very time-consuming and laborious task and can limit the scalability of the system. Recently a few unsupervised methods have been proposed to tackle the data annotation problem [Gu et al. 2009; Pei et al. 2007; Huynh and Schiele 2006]. Though these methods address the data annotation problems, they consider a simplified version of the problem by ignoring the real-life properties of human activity, such as its sequential nature, the existence of disrupted patterns, and variations of the same pattern.

In this article, we propose an unsupervised method for discovering discontinuous and varied-order activity patterns in real-world settings. Our method is called COM, standing for C ontinuous, varied O rder, M ultithreshold activity discovery method. COM not only discovers discontinuous (disrupted) activity patterns and their variations, it also handles other real-world complications, such as vastly different frequencies in different regions of the home. COM is able to find a higher percentage of the frequent patterns, thus achieving a higher recognition rate. It eliminates the need for configuring most of the parameters by the user, resulting in a more automated approach. We also provide a pattern visualizer component to visualize the activity patterns and their variations. The visualization component helps the end users and caregivers to better identify the variations of the patterns and to detect the abnormal or suspicious cases. Relying on advances in sensor technology as well as in data mining and machine learn-ing fields, nowadays many researchers are working on smart environments [Cook and Das 2004]. Smart environments provide a way to respond to the needs of the residents in a context-aware manner. A smart environment is equipped with different types of sensors which allow the system to collect data on inhabitants activities and environ-mental situations. Some of the efforts for realizing such smart environments have been demonstrated in actual physical testbeds, such as our CASAS project [Rashidi and Cook 2009], MavHome project [Cook et al. 2003], Gator Tech Smart House [Helal et al. 2005], iDorm [Doctor et al. 2005], and Aware Home [Abowd and Mynatt 2004].
Activity data in a smart environment can be captured via different mediums depend-ing on the target task. Activity data in a smart home can be collected using ambient sensors, such as infrared motion sensors, to track the motion of residents around the home [Rashidi and Cook 2009]. Additional ambient sensors, such as temperature sen-sors, pressure sensors, contact switch sensors, water sensors, and smart powermeters, can provide other types of context information. For recognizing residents X  interaction with the key objects, some researchers have used object sensors, such as RFID tags [Tapia et al. 2004; Philipose et al. 2004]. Besides ambient sensors, surveillance cam-eras and other types of image capturing devices, such as a thermographic camera, have been used for activity recognition [Stauffer and Grimson 2000]. Our algorithms are designed to handle data provided from any of these classes of sensors. However, in our testbeds we utilize sensors that do not require frequent battery charging and do not raise privacy concerns among users [Hensel et al. 2006]. One of the most important components of an automatic monitoring system based on smart home technology is activity recognition . An activity recognition algorithm rec-ognizes resident activity patterns from low-level sensor data. It is usually given a sequence of sensor readings. In the training phase, each sensor event might be also accompanied by its corresponding activity label. Table I shows several labeled example sensor readings from our testbeds.

As depicted in Table I, a sensor event can be part of a labeled activity, for example, the first two sensor events. It also can have no activity labels, such as the third sensor event. The goal of an activity recognition algorithm is to predict the label of a sensor event or a sequence of sensor events, for example,  X  X ersonal Hygiene X  for the M 004  X  M 030 sequence.

The majority of activity recognition algorithms are supervised, that is, they rely on labeled data for training [Wadley et al. 2008]. Some researchers have published results of experiments in which the participants are required to manually note each activity they perform at the time they perform it [Liao et al. 2005; Tapia et al. 2004; Philipose et al. 2004]. In other cases, the experimenters told the participants which specific activities should be performed, so the correct activity labels were identified before the sensor data was even collected [Cook and Schmitter-Edgecombe 2009; Gu et al. 2009; Maurer et al. 2006]. Occasionally, the experimenter manually inspected the raw sensor data to annotate it with a corresponding activity label [Wren and Munguia-Tapia 2006]. Researchers have also used methods, such as experience sampling [Tapia et al. 2004], where subjects carry a personal digital assistant (PDA) as self-report devices.
The supervised activity recognition methods range from simple methods, such as naive Bayes [Brdiczka et al. 2005] based on sensor events independence assumption, to more recent and sophisticated methods, such as conditional random fields [Philipose et al. 2004; Zhao et al. 2010], which model the sensor events as probabilistic sequences. Other notable supervised methods include decision trees [Maurer et al. 2006], neural networks [Mozer et al. 1998], Markov models [Liao et al. 2005], and dynamic Bayes networks [Inomata et al. 2009]. Despite their prevalence, supervised methods do not scale up well in the real world. First, the assumption of consistent predefined activities does not hold in reality. Due to physical, mental, cultural, and lifestyle differences [Wray and Laird 2003], not all individuals perform the same set of tasks. Even for the same predefined activity, different individuals might perform it in vastly different ways, making the reliance on a list of predefined activities impractical due to the inter-subject variability. Thus data needs to be annotated for each individual person and for each task. Annotating activity data is a very time-consuming and laborious task. Therefore unsupervised approaches seem to be more suitable for activity recognition in a normal day-to-day setting. Figure 1 shows the differences between supervised and unsupervised methods more clearly.

Recently, a few unsupervised methods have been proposed to tackle the data annota-tion problem, such as the frequent sensor mining method [Gu et al. 2009], simultaneous frequent-periodic pattern mining method [Rashidi and Cook 2009], episode discovery [Heierman and Cook 2003], activity modeling based on low-dimensional Eigenspaces [Schiele 2006], multidimensional motif discovery [Vahdatpour et al. 2009; Zhao et al. 2010], mixed discriminative and generative methods [Huynh and Schiele 2006], prob-abilistic models [Barger et al. 2005; Dimitrov et al. 2010], and retrieving activities X  definitions using Web mining [Perkowitz et al. 2004; Palmes et al. 2010].

Though these methods address the data annotation problem, they consider a simpli-fied version of the problem by ignoring the real-world nature of data, such as its sequen-tial form, possible disruptions (e.g., a phone call in the middle of meal preparation), or variation of the same pattern. For example, Hayes et al. [2007] note that variation in the overall activity level at home is correlated with mild cognitive impairment. This highlights the fact that it is important for a caregiver to be able to monitor all the ac-tivities and their variations. Unlike conventional sequential mining methods [Agrawal and Srikant 1995; Zaki 2001; Pei et al. 2004; Yang et al. 2006; Chiu et al. 2004] which deal with transactional data, activity sequence patterns need to be discovered from a single stream of sensor data with no clear boundaries between consecutive activities. As a result, the length of the patterns can be highly variable. These problems turn the activity mining problem into a challenging problem, where sequence patterns can be disrupted by irrelevant events, can have many variations in terms of step ordering or duration, and their support might greatly vary in different areas of the space. Sequential pattern mining has been studied for more than a decade [Agrawal and Srikant 1995], and many methods have been proposed for finding sequential patterns in data [Agrawal and Srikant 1995; Pei et al. 2001; Wang and Han 2004; Masseglia et al. 1998]. All of these approaches assume that data is in a transactional format. However, input data in a smart environment is not usually in transactional format. Figure 2 depicts the difference between transaction data and sensor data. In the case of transaction data, each single transaction is associated with a set of items and is identified by a transaction ID, making it clearly separate from the next transaction. Sensor data, on the other hand, has no boundaries separating different activities from each other. Instead, it is just a single stream of sensor events with no clear boundaries between different activities.
 Approaches proposed by the sensor mining community [Papadimitriou et al. 2003; Loo et al. 2005] try to turn sensor data into a transactional dataset using techniques, such as the Apriori technique [Agrawal and Srikant 1995], to group frequent events together. Another method is to simply use fixed or varied clock ticks [Loo et al. 2005]. In our scenario, using such simple techniques does not allow us to deal with complex activity patterns which can be discontinuous, varied order, and of arbitrary length. In our solution, we group together the cooccurring events into varied-order discontinuous activity patterns.
 Our input data is a sequence of sensor events e in the form e = t , s , where t denotes a timestamp, and s denotes a sensor ID. An example showing several sensor events was depicted in Table I. Each sensor ID is associated with its room name (e.g., kitchen), which we will refer to as a location tag L . We define a pattern instance as a sequence of n sensor events e 1 , e 2 ,.. e n . A pattern itself represents the collection of all of its instances. An example of an activity pattern, such as meal preparation, may appear as M 005 , M 003 , M 001 , where M 005, M 003, and M 001 refer to sensors in the kitchen. One variation of such a pattern is M 003 , M 002 , M 001 .Notethatweusetheterms activity and pattern interchangeably. As previously mentioned, in contrast to the su-pervised approaches which reply on labeled data [van Kasteren et al. 2008; Chan et al. 2008; Vail et al. 2007; Tapia et al. 2004], we assume that our data is not annotated and the activity boundaries are not specified.

Our objective is to develop a method which can automatically discover real-life pat-terns of resident X  X  activities, even if the patterns are somehow discontinuous or have different event orders across their instances. For example, consider the  X  X eal prepara-tion X  activity. Most people will not perform this activity in exactly the same way each time, rather, some of the steps or their order might be changed (varied order pattern). In addition, the activity might be interrupted by irrelevant events, such as answering the phone (discontinuous pattern).

Our method improves upon our previous method called DVSM [Rashidi et al. 2011] which is able to find discontinuous and varied order patterns in the data. Simi-lar to previous unsupervised methods, DVSM works best when using data collected under controlled conditions and in artificial settings. In contrast, DVSM faces diffi-culty mining real-life data. For example, it has difficulty discovering activities where heterogeneous sensors are used (such as motion sensors in conjunction with contact switch sensors). Our new method is named COM, which stands for Continuous, varied Order, Multi Threshold activity discovery method. COM not only discovers discontin-uous patterns and their variations but is also able to better handle real-life data by dealing with different frequencies/sensors problem. It is able to find a higher percent-age of the frequent patterns, thus achieving a higher recognition accuracy rate. Also by pruning the irrelevant patterns based on mutual information, it only retains the most relevant variations of a pattern, thereby reducing the number of irrelevant variations. It eliminates the need for configuring most of the parameters by user, such as the per-centage of the top frequent sensor events which should be used to discover the activity patterns or the support threshold for frequent events or the number of activities. We also provide a pattern visualizer component to visualize the activity patterns and their variations in order to help the users/care-givers to better monitor the variations of the patterns and to help them detect the abnormal or suspicious cases.

The architecture of the system can be seen in Figure 3, including its mining, clus-tering, recognition, and visualization components. Note that after discovering activity patterns, we then cluster discovered patterns to provide a more concise and compact representation. The clusters are then used to track and recognize the resident X  X  activi-ties. In the following section, we will provide a more detailed description of each one of the mining, clustering, recognition, and visualization components. Our mining algorithm is able to discover activity patterns and their variations from sensor data, even if the patterns exhibit some sort of discontinuity. For example, as and { a , u , b } , despite the fact that the events are discontinuous and have varied orders. We denote a general pattern P as the pattern that comprises of all of its n variations, where each variation is denoted by P i . It should be noted that a continuous pattern is discovered by our algorithm as a special case of discontinuous patterns with a dis-continuity of zero. It also should be noted that our approach is different than frequent itemset mining, as we take into account the order of events. 3.1.1. Mining Method Details. First we create a reduced dataset D r from the input data D . The reduced dataset only contains frequent sensor events used for pattern construction. In the DVSM algorithm, the user has to specify what percentage of the top frequent events (  X  ) should be used. Then by using a global support threshold on the frequency of sensor events, the reduced dataset is created. Taking such an approach will result in ignoring the problem of rare sensors. In our studies, we found out that different regions of a home, as well as different types of sensors, exhibit different sensor frequencies. In our new solution, we do not require the user to identify  X  , rather we automatically compute several support thresholds for different regions of the home and for different types of sensors. We will explain this problem in more detail in subsequent sections.
After constructing D r , COM slides a window of size 2 across D r to find patterns of length 2. After this first iteration, the whole dataset does not need to be scanned again. Instead, COM extends the patterns discovered in the previous iteration by their prefix and suffix events. It then matches the extended pattern against the already-discovered patterns in the same iteration to check if the extended pattern is a variation of a previous pattern or if it is a new pattern [Rashidi and Cook 2009]. To facilitate comparisons, we save general patterns along with their discovered variations in a hash table.

At the end of each iteration, we prune infrequent, highly discontinuous or irrelevant variations of a general pattern. We also prune the infrequent, highly discontinuous or non-maximal general patterns. We identify a general pattern/variation to be interesting if it has a minimum compression value according to the Minimum Description Length (MDL) principle [Rissanen 1978]. The minimum description length principle advocates that the pattern which best describes a dataset is the one which maximally compresses the dataset by replacing instances of the pattern by pointers to the pattern definition. Figure 5 illustrates this compression concept in our algorithm better.

However, since we allow discontinuities to occur, each instance of the pattern needs to be encoded not only with a pointer to the pattern definition, but also with a continuity factor, . The discontinuity of a pattern instance is defined as the number of bits required to express the gaps in definition of the pattern. It can be defined in a top-down manner, starting from the component events of a pattern instance [Rashidi et al. 2011]. The continuity between each two events of a pattern instance is defined in terms of the average number of the infrequent events separating the two events. The more the separation between the two events, the less the continuity. For a pattern variation, the continuity is based on the average continuity of its instances. For a general pattern, the continuity is defined as the average continuity of its variations.

By including the continuity concept in the definition of the patterns, the compression value c for a general pattern P is defined as in Equation (1). Similarly Equation (2) defines the compression value c v for a variation P i . Here, L denotes description length according to MDL principle as the number of bits required to encode an entity. The length of compressed dataset using pattern definition is reflected in term L ( D r | P ).
We transform the compression values to be in the range of [0 .. 1] via the so-called softmax scaling technique [Pyle 2005], as in Equation (3).
Patterns with high compression values are flagged as  X  X nteresting X , while patterns with low compression values are discarded. In other words, we are looking for patterns which are longer, more frequent, and more continuous (less gaps within patterns). The same method is applied for pruning variations of a general pattern.

In addition, by computing the mutual information [Guyon and Elisseeff 2003] be-tween the general pattern and each of its sensor events, we are able to find the set of core sensors for each general pattern. Finding the set of core sensors allows us to prune the irrelevant variations of a pattern which do not contain the core sensors. Equation (4) computes the mutual information between the general pattern and each of its sensor events. Here, Pr ( x ) refers to the probability of a random variable x . At the end of each iteration, we also prune redundant non-maximal general pat-terns, that is, those patterns that are totally contained in another larger pattern. This multi-stage pruning process considerably reduces the number of discovered patterns. We continue extending the patterns by prefix and suffix until no more interesting pat-terns are found. This allows us to find variable length patterns. A post-processing step records attributes of the patterns, such as event duration and start time. 3.1.2. Multiple Support Thresholds. In mining real-life activity patterns, the frequency of sensor events can vary across different regions of the home or other space. If the such differences are not taken into account, then the patterns that occur in less frequently used areas of the space might not be discovered. For example, if the resident spends most of his/her time in the livingroom during the day and only goes to the bedroom for sleeping, then the sensors will be triggered more frequently in the living-room area than in the bedroom. Therefore, when looking for frequent patterns, the sensor events in the bedroom might be ignored, and consequently, the sleep pattern might not be discovered. The same problem happens with different types of sensors. Usually, motion sensors are triggered much more frequently than other types of sensors, such as cabinet sensors. This problem is known as the rare item problem in market basket analysis and is usually addressed by providing multiple minimum support values [Liu et al. 1999]. For example, a webpage might have a lower chance of being visited by visitors, but we still might be interested in finding click patterns in such pages.

In our solution, each different region of home is identified by a location tag L which represents the functional areas, such as bedroom, bathroom, etc. The sensors are also categorized depending on their type. In our experiments, we categorized the sensors into motion sensors and interaction-based sensors. The motion sensors are used to track the motion of a person around a home (e.g., infrared motion sensors). Interaction-based sensors, as we will call them key sensors , are the non-motion tracking sensors, such as cabinet sensors or RFID tags, on items. Based on observing and analyzing sensor frequencies in multiple smart homes, we found that a motion sensor might have a higher chance of being triggered than a key sensor in some regions. Hence we will derive separate minimum sensor supports for different sensor categories.

The type and the location tag of all sensors is passed on to COM as an initial configuration file. For each region and category, the minimum acceptable frequency is automatically derived as the average frequency for that specific region and category. More specifically, we denote the minimum acceptable frequency support of the key sensors for each region as  X  f k . It is computed as the average frequency of key sensors in that region. Similarly,  X  f m denotes the minimum acceptable frequency support for motion sensors in a specific region. It is computed as the average frequency of motion sensors in that specific region. Figure 6 shows a depiction of selected sensors and the minimum acceptable frequency supports for one of the smart homes used in our experiments. If a region contains only motion sensors and no key sensors, we denote its  X  f k as N/A. One can clearly see that the motion sensors are activated more frequently in the living room than in any other area of the home, such that the frequency support of motion sensors in the kitchen is half of the frequency support in the living room. Also, we can see that the motion sensors in the kitchen are activated three times more than the key sensors, such as cabinet doors or the refrigerator door. Of course, these results might be different in case of a different smart home and a different resident. For example, in a second smart home, we might observe that the sensors in the workspace are activated much more frequently than the sensors in the living room. Nonetheless, our algorithms allow us to automatically compute multiple support thresholds in different situations, thus tailoring our solution to different spaces. After discovering activity patterns, we cluster them to get an even more compressed representation. Though we group similar pattern variations in the mining stage, still their grouping is solely based on the structural similarity in terms of common sensor events. Two similar patterns with slightly different trajectories might activate different sets of sensors. So they will be considered as separate patterns even if they exhibit high similarity in start time, duration, and occurrence locations. To remedy this problem, we use a clustering algorithm. The clustering algorithm groups patterns in terms of their structure, start time, duration, and regional similarity. Clustering also addresses the problem of discovering too many similar patterns, where the sheer number of patterns makes it difficult to analyze the true underlying major salient ideas.

Our clustering algorithm is similar to conventional hierarchal agglomerative clus-tering techniques [Tan et al. 2005], although it does not form the complete hierarchy. The algorithm is depicted in Algorithm 1.

Agglomerative clustering techniques build a hierarchy from the individual elements by progressively merging clusters until all data ends up in one cluster. Here, we do not continue the hierarchal clustering up to the point of reaching a single cluster; rather, the clustering continues until the similarity between the two closest clusters drops below a threshold  X  . This gives us a set of clusters at the highest level of the hierarchy. After forming the clusters, the cluster centroids at the highest level are used to track and recognize resident X  X  activities. Using such a clustering method, the user no longer has to provide the number of clusters in advance. We use a group-average link method [Tan et al. 2005] to compute the proximity matrix based on the similarity measure defined in Equation (5).

In Equation (5),  X  t refers to the start time similarity (if the two activities happen at similar times, e.g., both around noon),  X  d refers to duration similarity (if the two activities have similar durations),  X  L refers to regional similarity (if the two activities happen in similar locations, e.g., both in the kitchen), and  X  S refers to structure sim-ilarity (if the two activities have similar structure in terms of sensors). We normalize  X  ( i , j ) to fall within the range [0 .. 1]. For simplicity, we have chosen the similarity values to have equal effects; however, it is possible to define  X  ( i , j ) as a weighted average.
In our model, start time is represented in terms of a mixture normal distribution, with means =  X  1 .. X  r . The use of multiple distributions allows us to better capture the variability in start time of activities. Two example start time mixture distributions can be seen in Figure 7 for the  X  X ating X  activity. We can observe that by using a normal mixture model, we are able to capture breakfast, lunch, and dinner times as the regular meals for the inhabitants. We represent start time  X  in an angular form measured in radians instead of a linear representation. This allows for time differences to be represented correctly (2:00 AM will be closer to 12:00 AM than to 5:00 AM). The similarity between the two start time distributions is calculated using Equation (6).
Including temporal information about activities can be quite crucial for the activity recognition step. For example, in order to distinguish between hand-washing activity and dish-washing activity in the same kitchen sink, one needs to rely upon tempo-ral information. The reason being that almost all contextual information, such as sensor activation, would be the same for the two activities. However, by taking into account the fact that dish-washing usually takes longer than hand-washing activity (re-flected in duration distributions), we are able to properly distinguish between these two activities. Likewise, start-time information can help us distinguish between activities, such as having dinner versus having lunch.

Duration mapping is calculated as in Equation (7). Duration is also represented in the form of a mixture normal distribution with means =  X  1 .. X  r .

The regional and structural similarities are calculated as in Equations (8) and (9) using the Jaccard similarity measure [Tan et al. 2005]. In Equation (8), E refers to the set of sensors for a pattern.
 The activity recognition algorithm in our model is based on using a hidden Markov model (HMM) [Rabiner 1990]. The HMM is constructed automatically from the cluster centroids. An HMM is a statistical model of a dynamic system, which models the system using a finite set of hidden states and observable states. Each of the states is associated with a multidimensional probability distribution. Transitions between hidden states are governed by transition probabilities. Transitions from hidden states to observable states are declared as emission probabilities. In our model, each hidden state corresponds to a discovered activity, while the observable states correspond to the fired sensor events (see Figure 8).

We can specify an HMM using three probability distributions: the distribution over initial states ={  X  k } ; the state transition probability distribution A ={ a kl } ,with a indicating the probability that the state l would generate observation x t = i . We can find the most likely sequence of hidden states given the observation in Equation (10) and by using the Viterbi algorithm [Viterbi 1967].
We compute the transition probabilities and the observation probabilities automati-cally from data during the data mining and clustering stages. We evaluated the performance of our COM algorithm using the data that was collected from two different smart apartments. The layout of the apartments including sensor placement and location tags are shown in Figure 9. We will refer to apartments in Figures 9(a) and 9(b) as Apartments 1 and 2. The data was collected during an ap-proximate three-month period. Each apartment is equipped with motion sensors and magnetic door sensors which monitor the open/closed status of doors and cabinets.
To be able to evaluate the results of our algorithms, each of the datasets was an-notated with ADL activities of interest for the corresponding resident and apartment. Those activities are typical activities used in the evaluation of assisted-living systems. A total of ten activities as noted for each apartment. Those activities included bathing, bed-toilet transition, eating, leave/enter home, meal preparation (cooking), personal hygiene, sleeping in bed, sleeping not in bed (relaxing), and taking medicine. The first and second datasets include 3,384 and 2,602 annotated activity instances, respectively.
All the activities are performed in a natural setting, that is, data is collected without any interruption in the daily life of residents. The resident in the first apartment is a 92 year-old man, while the resident in the second apartment is a female adult around 80 years old. Both residents live alone independently. All the sensor data is captured and stored in an SQL database using a publish/subscribe protocol middleware. To maintain privacy, we remove identifying information and encrypt collected data before it is transmitted over the network.
 We ran our algorithm for each one of the apartments. One major improvement of our COM algorithm is to use multiple support thresholds for different regions of homes, as well as for different types of sensors. A single support threshold might not pose a problem in scripted experiments where all activities are performed with the same frequency (e.g., 20 times, as in our DVSM experiments [Rashidi et al. 2011]). However, in real life, this assumption results in discarding many patterns. To show how using a single threshold affects the accuracy of pattern discovery, we performed a number of experiments, once using the COM algorithm and once using DVSM.

The data mining step was able to discover a considerable number of predefined activities of interest. In apartment 1, it discovered eight out of ten activities, including bathing, leave/enter home, meal preparation (cooking), personal hygiene, sleeping in bed, sleeping not in bed (relaxing), and taking medicine. In apartment 2, it was able to discover seven out of ten activities, including bathing, bed-toilet transition, eating, enter home, leave/enter home, meal preparation (cooking), personal hygiene, sleeping in bed, and sleeping not in bed (relaxing). Some of the patterns that have not been discovered are indeed quite difficult to spot and also in some cases less frequent. For example, the housekeeping activity happens every 2 X 4 weeks and is not associated with any specific sensor. Also, some of the similar patterns are merged together as they use the same set of sensors, such as eating and relaxing activities. It should be noted that some of the activities are discovered multiple times in the form of different patterns, as the activity might be performed in a different motion trajectory using different sensors. Figures 10(a) and 10(b) show the number of distinct discovered activities by both COM and DVSM algorithms in apartments 1 and 2. One can clearly see that COM is able to discover a higher number of distinct activities. These figures also show the amount of data for achieving a certain level of accuracy.

Using externally provided annotations to verify our results, we also computed the percentage of non-annotated discovered activities, that is, the percentage of activities that actually have no annotation but have been discovered by our algorithm. For apart-ment 1, the percentage of non-annotated activities with respect to the number of total distinct activities was 7 . 0%, and for apartment 2 it was 2 . 0%. The low percentage of discovered activities that are not annotated shows that most of the discovered pat-terns are indeed well aligned with those patterns identified by the human annotator as interesting in the first place.

Figures 11(b) and 12(b) show the total number of discovered patterns instances for both the COM and DVSM algorithms, as well as the total number of pruned instances. It can be seen that though sometimes COM generates more pattern instances and in general more patterns, it also prunes more pattern instances due to its improved pruning capabilities, while still discovering more distinct patterns (activities).
In the next step, we clustered the discovered activities together using our agglomer-ative clustering method. The similarity threshold  X  of 0 . 75 was found to be a suitable value based on several runs of our experiments. By using the externally provided la-bels, we were also able to measure the purity of the clustered patterns with respect to the consistency of their variations using Equation (11). Equation (11) is known in literature as Jaccard similarity is a form of simple matching coefficient and is used to determine the similarity between clusters and actual classes. Here | v 11 | refers to the number of variations that have the same label as their general pattern, and | v 01 | and | v 10 | refer to the number of variations that have a different label other than their general pattern X  X  label. We call this number the variation consistency.
The variation consistency for clustered patterns in apartment 1 was 0 . 90 in our experiments, while for apartment 2 it was 0 . 76. By closely looking at the data and examining it, it was revealed that the patterns in the second dataset are much more irregular. Therefore most similar patterns are combined together, such as taking med-ication and meal preparation, which usually happen at approximately the same time and the same location (in this case, the kitchen).

Next, using the discovered patterns, an HMM was automatically constructed to track and recognize the activities. Figures 11(c) and 12(c) show the recognition results for the two apartments. Tables II and III show the confusion matrices for all the activities. It can be seen from those tables that due to higher regularity in apartment 1, it is easier to track and recognize activities. It should be noted that some similar activities might be mistaken with one another, such as relaxing and eating, which happen at similar times and similar locations. In addition, some activities, such as the housekeeping activity, are not recognized because they were not discovered in the first place. As previously mentioned, housekeeping happens quite rarely compared to other activities; and even if it happens to be in the data, because of its erratic nature and the fact that it occurs all over the home, it is not very easy to discover. Therefore, in the activity recognition step, it is mistaken with other activities, such as cooking. However, one still can observe that despite the fact that the real-life activities can be somewhat hectic and irregular, still our algorithm is able to track and monitor a considerable number of the patterns.
The output of our preliminary algorithms are designed to be in XML format. Though an excellent choice for data representation and portability, XML might not be the best choice for a natural user-friendly representation, especially in the case of activity patterns with features such as temporal features and trajectory path. We have designed a preliminary visualizer to visualize the discovered activity patterns on the home map, as depicted in Figure 13(a). It also displays important statistical information, such as start time or duration. Users can go back and forth between patterns and their variations using navigation buttons. Also, it X  X  possible to see all the patterns or all the variations of a pattern at the same time. In this case, each pattern (or variation) will be visualized using a unique color code, as depicted in  X  X ap Guide X . Our visualization tool also generates graphic charts which highlight certain features of the data, as depicted in Figure 13(b). The charts include information such as activity occurrences at multiple temporal granularity and activity density in terms of color-coded number of sensor events per hour. Using such a simple visualizer allows users to not deal with the sensor information in a textual format, which might be confusing and hard to understand. Rather, it allows the users to see the patterns in a natural format and quickly diagnose relations between different variations of a pattern. Several discovered patterns and their variation are shown in Figure 14. In this article, we presented an automatic approach to activity discovery and monitoring for assisted living in a real-world setting. Most current approaches use supervised methods for activity recognition. However, due to the required effort and time for annotating activity datasets, supervised methods do not scale up well in practice. Annotating activity data imposes a burden on annotators and residents and often introduces a source of error in the process. Our model is able to automatically discover activity patterns and their variations, even if the patterns exhibit discontinuity or if the patterns X  frequencies exhibit difference across different regions in home.
Our sequence mining algorithm has the potential to be applied to a wide range of applications, such as Web click analysis or DNA sequence analysis. Ultimately, we hope that our model can be used in a fully functional system deployed in a real home. In the future, we plan to extend the system using an anomaly detection component to detect anomalies in observed data.

