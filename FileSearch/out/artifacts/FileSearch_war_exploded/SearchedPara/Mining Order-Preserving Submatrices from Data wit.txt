
Order-preserving submatrices (OPSM X  X ) have been shown useful in capturing concurrent patterns in data when the relative magnitudes of data items are more important than their absolute values. To cope with data noise, re-peated experiments are often conducted to collect multiple measurements. We propose and study a more robust version of OPSM, where each data item is represented by a set of values obtained from replicated experiments. We call the new problem OPSM-RM (OPSM with repeated measure-ments). We define OPSM-RM based on a number of practi-cal requirements. We discuss the computational challenges of OPSM-RM and propose a generic mining algorithm. We further propose a series of techniques to speed up two time-dominating components of the algorithm. We clearly show the effectiveness of our methods through a series of experi-ments conducted on real microarray data.
Among all data mining problems, Order-Preserving Sub-matrix (OPSM) has important applications particularly in the area of bioinformatics. The general OPSM problem ap-plies to a matrix of numerical data values. The objective is to discover a subset of attributes (columns) over which a subset of tuples (rows) exhibit a similar pattern of rises and falls in the tuples X  values. For example, when analyzing gene expression data from microarray experiments, genes (rows) with concurrent changes of mRNA expression levels across different time points (columns) may share the same cell-cycle related properties [11]. Due to the high level of noise in typical microarray data, it is usually more mean-ingful to compare the relative expression levels of differ-ent genes at different time points rather than their absolute values. Genes that exhibit simultaneous rises and falls of their expression values across different time points or ex-periments reveal interesting patterns and knowledge. As an example, Figure 1 shows the expression levels (y-axis) of two different sets of genes under four experimental condi-belong to different functional categories. From the figure we see that genes of the same group exhibit similar expres-sion patterns even though their absolute expression values under the same experiment vary. Figure 1. Concurrent expression patterns of two sets of genes from different functional categories The original OPSM problem was first proposed by Ben-Dor et al. [2].
 Definition 1 Given an n  X  m matrix (dataset) D , an order-preserving submatrix (OPSM) is a pair ( R, P ) , where R is a subset of the n rows (represented by a set of row ids) and is a permutation of a subset of the m columns (represented by a sequence of column ids) such that for each row in R , the data values are monotonically increasing with respect to
P , i.e., D iP j &lt; D iP
For example, Table 1 shows a dataset with 4 rows and 4 columns. The values of rows 2, 3 and 4 rise from a to b , so we assume that all values in a row are unique.

We say that a row supports a permutation if its values increase monotonically with respect to the permutation. In the above example, rows 2, 3 and 4 support the permutation  X  a, b  X  , but row 1 does not. For a fixed dataset, the rows that support a permutation can be unambiguously identified. In Table 1. A dataset without repeated measurements the following discussion, we will refer to an OPSM simply by its permutation, which will also be called a pattern .
An OPSM (and its corresponding pattern) is said to be frequent if the number of supporting rows is not less than a support threshold  X  [4]. Given a dataset, the OPSM mining problem is to identify all frequent OPSM X  X . In the gene expression context, these OPSM X  X  correspond to groups of genes that have similar activity patterns, which may suggest shared regulatory mechanisms and protein functions.
A drawback of the basic OPSM mining problem is that it is very sensitive to noisy data. In microarray experiments, each value in the dataset is a physical measurement that is subject to different kinds of errors. To combat errors, ex-periments are often repeated and multiple measured values (called replicates) are recorded. The replicates allow a bet-ter estimate of the actual physical quantity. Indeed, as the cost of microarray experiments has been dropping, research groups have been obtaining replicates to strike for higher data quality. For example, in the microarray dataset we use in our study, each experiment is repeated 4 times to produce 4 measurements of each data point. Studies have clearly shown the importance of having multiple replicates in im-proving data quality [8].
 Different replicates, however, may support different OPSM X  X . In our previous example, if the value of column a is slightly increased in row 3, say from 65 to 69, then row 3 will no longer support the pattern  X  a, b  X  , but will support  X  b, a  X  instead. As another example, Table 2 shows a dataset with two more replicates added per experiment. From this new dataset, we see that it is no longer clear whether row 3 supports the  X  a, b  X  pattern. For instance, while the replicates a , b Table 2. A dataset with repeated measurements
Our examples illustrate that the original OPSM defini-advantage of the additional information provided by data replicates. There is thus a strong motivation to revise the definition of OPSM to handle repeated measurements. Such a definition should satisfy the following requirements: (1) If a pattern is supported by all combinations of the repli-cates of a row, the row should contribute a high support to the pattern. For example, for row 3, the values of column are clearly smaller than those of column c . All 3  X  3 = 9 replicate combinations of b and c values ( b ..., ( b strongly support  X  b, c  X  . replicates, it is probably due to error. The replicate should not severely affect the support of a given pattern. For exam-ple, we see that row 2 generally supports the pattern  X  a, c  X  we ignore a pared to a tributed by row 2 should only be mildly reduced due to the presence of a (3) If the replicates largely disagree on their support of a pattern, the overall support should reflect the uncertainty. For example, in row 4, the values of b and c are mingled. Thus, row 4 should not strongly support  X  b, c  X  or  X  c, b  X 
The first two requirements can be satisfied by summariz-ing the replicates by robust statistics such as medians, and mining the resulting dataset using the original definition of OPSM. However, the third requirement cannot be satisfied by any single summarizing statistic. This is because under the original definition, a row can only either fully support or fully not support a pattern. The information of uncertainty is thus lost. To tackle this problem, we define a new OPSM problem based on the concept of fractional support : Definition 2 The fractional support s contributed by a row i is the number of replicate combina-tions of row i that support the pattern, divided by the total number of replicate combinations of the columns in P .
For example, for row 1, the pattern  X  a, b, d  X  is supported by 8 replicate combinations:  X  a  X  a  X  a combinations. The fractional support s therefore 8/27. We use sn numerator and the denominator of S our example, sn
The definition of fractional support satisfies all the three requirements we stated above. Firstly, if all replicate com-binations of a row support a certain pattern, the fractional support contributed will be 1. Secondly, if a replicate of a column j deviates from the others, the replicate can at the number of replicates of column j . This has small effects when the number of replicates r ( j ) is large. Finally, if only a fraction of the replicate combinations support a pattern, the resulting fractional support will be fuzzy (away from 0 and 1), which reflects the uncertainty.
The total fractional support of a pattern P (or simply the support of P ), is defined as the sum of all the fractional sup-ports of P contributed by all the rows: s ( P ) = P A pattern P is frequent if its support is not less than a given support threshold  X  . Our new OPSM mining prob-lem OPSM-RM (OPSM with repeated measurements) is to identify all frequent patterns in a data matrix with replicates.
From the definition of fractional support, we can observe the combinatorial nature of the OPSM-RM problem  X  the number of replicate combinations grows exponentially with respect to the pattern length. One of the objectives of this work is to derive efficient algorithms for mining OPSM-RM. By proving a number of interesting properties and the-orems, we propose pruning techniques that can significantly reduce mining time.
The conventional order-preserving submatrices (OPSM) mining problem was motivated and introduced in [2] to an-alyze gene expression data without repeated measurements. In [2], it was proved that the problem is NP hard. A greedy heuristic mining algorithm was proposed, which does not guarantee the return of all OPSM X  X  or the best OPSM X  X .
Since then, mining efficiency has been the main research issue. In [4], the monotonic and transitive properties of OPSM X  X  were proved. Based on the properties, a candidate set generation-and-test framework was proposed to mine all OPSM X  X . It makes use of a new data structure, the head-tail trees, for efficient candidate generation. The study re-ported in [5] concerned the high computational cost of min-ing OPSM X  X  from massive data. They defined the twig clus-ters , which are OPSM X  X  with large numbers of columns and naturally low supports. They proposed a KiWi framework to efficiently mine the twig clusters. None of the above stud-ies, however, handle data with repeated measurements.
The OP-clustering approach [9] generalizes the OPSM model by grouping attributes into equivalent classes. A depth-first search algorithm was proposed for mining all error-tolerated clusters. The model attempts to handle error in single expression values rather than exploiting extra in-formation obtained from repeated measurements. In [3], the problem of mining OPSM X  X  over multiple time points was considered. There are different experimental conditions in each time point, and a pattern is required to be consistent over the time points. An evolutionary algorithm was pro-posed to explore the search space.
In this section we discuss a straightforward algorithm for solving the OPSM-RM problem. We use an alternative rep-resentation of a matrix dataset that is more convenient for our discussion [5]. For each row, we sort all the values in ascending order, and record the resulting column names as a data sequence. For example, row 1 in Table 2 is represented by the data sequence  X  b, a, d, b, a, c, a, b, d, c, d, c  X  vantage of such a representation is that given a row i and a pattern P , the count sn subsequences in the data sequence that match P . For exam-ple, sn  X  b, a, d, b, a, c, a, b, d, c, d, c  X  that match the pattern In the following discussion, when we mention a row, we re-fer to the row X  X  sorted data sequence.
 Theorem 1 Let P a subsequence of P Proof. It is sufficient to show that the theorem is true for patterns whose lengths differ by 1, i.e., | P We can repeat the argument to prove the theorem for pat-P in column j . Each subsequence of row i that matches P column j replicate. Since there are only r ( j ) such repli-cates, at most r ( j ) such extensions are possible. Hence, sn i ( P 2 )  X  r ( j )  X  sn i ( P 1 ) number of possible replicate combinations is multiplied by a factor of r ( j ) , i.e., sd s The above monotonic property implies the following Apriori property: Corollary 1 Let P a subsequence of P Proof. If P s ( P 2 )  X  s i ( P 1 ) for all row i . So, s ( P 2 ) = P i P i s i ( P 1 ) = s ( P 1 ) &lt;  X 
The Apriori property ensures that an OPSM can be fre-quent only if all its subsequences (i.e., sub-patterns) are fre-quent. This suggests an iterative mining algorithm as shown in Figure 2.

As in frequent itemset mining [1], the algorithm itera-tively generates the set Cand terns, and verifies their supports. Those patterns that pass the support threshold are recorded in the set Freq are then used to generate the candidates of the next iteration.
We remark that in the original OPSM problem (without data replicates), all candidates are by definition frequent and thus support verification is not needed. This is due to the transitivity property: if a row supports both patterns  X  a, b, c  X  and  X  b, c, d  X  , the value at column a must be smaller than Algorithm OPSM-RM INPUT : raw data matrix D , support threshold  X  OUTPUT : the set of all frequent patterns 1: Transform D into sequence dataset D 0 2: Cand 3: k = 2 4: Freq 5: while Freq 6: k := k + 1 7: Cand 8: Freq 9: end while 10: return Freq However, when there are replicates, the fractional support of a pattern can be smaller than those of all its sub-patterns. For example, the sequence  X  b, a, d, b, a, c, a, b, d, c, d, c  X  a fractional support of 4 / 9 for  X  a, b  X  , 8 / 9 for  X  b, c  X  Support verification is thus necessary for OPSM-RM.
The efficiency of the algorithm depends on the two core functions generate and verify . For example, significant speed-up can be achieved if effective pruning techniques are applied so that generate produces a smaller set of candidate patterns. In the following we briefly describe the basic algo-rithms for implementing the generate and verify functions.
Generate . A convenient way to generate length-k can-didates from length-( k -1) frequent patterns is to use the head-tail trees. We briefly describe the data structure here. Readers are referred to [4] for details. Each length-( k frequent pattern P derives two length-( k -2) sub-patterns, called a head pattern P tained from P by removing the last symbol of P while P is obtained by removing the first symbol. For example, if patterns derived from all the length-( k -1) frequent patterns are collected and are stored as strings in a compressed data structure. For each head pattern P frequent patterns from which P In our implementation, we use a prefix tree [7] to store the head patterns. We call it the head tree. Similarly, tail pat-terns are collected and are stored in a tail tree.
To generate length-k candidates, the two trees are tra-versed in parallel to identify frequent patterns with com-mon sub-strings. For example, if both P P 2 =  X  b, c, d  X  string  X  b, c  X  will appear in both the head tree (due to and the tail tree (due to P retrieved. The two patterns are then joined to derive the candidate  X  a, b, c, d  X  .

Verify . Candidate patterns obtained from generate are stored as strings in another compressed data structure. Again, we use a prefix tree implementation. To count the candidates X  supports, we scan the dataset. For each row i we traverse the candidate tree and locate all candidate pat-terns that are subsequences of the data sequence of row i For each such candidate pattern P , we increment its sup-port s ( P ) by s
Support counting can be made more efficient by com-pressing data sequences using run-length encoding. Given a data sequence of a row, consecutive occurrences of the same column symbol are replaced by one single symbol and an occurrence count. For example, the data sequence vantage of compressing data sequences is that the com-pressed sequences are shorter (in our example, 7 symbols) than the originals (in our example, 12 symbols). The shorter data sequences allow more efficient subsequence match-ing in support counting. For example, the pattern  X  d, c, a  X  matches the above compressed data sequences two times (instead of 9 times against the uncompressed sequence): determine sn match and sum the results. In the above example, we have sn 2 (  X  d, c, a  X  ) = 3  X  2  X  1 + 3  X  1  X  1 = 9
From Theorem 1 we know that the support of a pattern contributed by a row cannot exceed the corresponding sup-ports of its sub-patterns. We can make use of this obser-vation to help deduce an upper bound to the support of a candidate pattern. If this upper bound is less than the sup-port threshold  X  , the candidate pattern can be pruned. Fewer candidates result in a faster verification and support count-ing process, and thus a more efficient mining algorithm.
In this section we discuss a simple bounding technique called MinBound. Recall that in candidate generation, a candidate pattern P is generated by joining two sub-patterns, say, P  X  a, b, c, d  X  is obtained by joining  X  a, b, c  X  and  X  b, c, d  X  that both P fractional supports given by each row of the dataset should have already been previously computed. If such supports are cached, we can determine an upper bound of s ( P ) by
For example, for row 1 in Table 2, s and s s exact value of s
Generally, the upper bounds derived by MinBound are not very tight. In this section we introduce head-tail arrays, a data structure that allows the calculation of the exact sup-port of candidate patterns. Although powerful, head-tail ar-rays are very memory demanding and are thus impractical. Fortunately, the arrays can also be used to derive fairly tight bounds for the support, in which case the memory require-ments can be substantially reduced by maintaining only cer-tain statistics. The details will be given in Section 6.
Recall that a length-k candidate pattern P is generated by two length-( k -1) frequent sub-patterns P correspond to the head (i.e., P P the fractional support s information we have previously obtained about P with respect to row i . To illustrate, let us use row 1 in Ta-ble 2 and P =  X  a, b, c, d  X  as a running example. Let be the data sequence of row 1. (Symbol indices are shown for ease of reference.) Also, we have P P 2 =  X  b, c, d  X  puted by constructing the following two auxiliary arrays.
The head array H concerns the head sub-pattern P contains r ( P [1]) entries (recall that r ( P [1]) is the number of replicates of column P [1] ). The l -th entry of the head array records the number of times P [2.. k -1] appear after the l -th occurrence of P [1] in S there are r ( P [1]) = r ( a ) = 3 replicates, so the head array head array thus record the number of times  X  b, c  X  occurs in S
The first entry is 5 because after the first a (position 2), there are 5 occurrences of  X  b, c  X  in S second entry is 2 because after the second a (position 5), there are 2 occurrences of  X  b, c  X  , at (8 , 10) and (8 , 12)
The tail array T concerns the tail sub-pattern P sists of sn records the number of times P [ k ] appears after the l -th oc-currence of P [2.. k -1] in S lexicographic order according to the positions of the occur-sn 1 (  X  b, c  X  ) = 8 order, the positions of these occurrences are: (1 , 6) , (1 , 10) (1 , 12) , (4 , 6) , (4 , 10) , (4 , 12) , (8 , 10) and (8 , 12) array thus has 8 entries, one for each occurrence of  X  b, c  X  For our example, P [ k ] = d . Each entry in the tail array thus records the number of d  X  X  that appear after the correspond-ing  X  b, c  X  in S
Since the first occurrence of  X  b, c  X  is (1,6) and there are 2 d  X  X  after that (at positions 9 and 11), the first entry of the tail array is 2. The other entries can be determined similarly.
By arranging the occurrences of  X  b, c  X  in lexicographic order, we ensure that all occurrences of  X  b, c  X  that appear after a certain position in S most entries of the tail array. This helps us in determining the number of occurrences of a pattern. For example, let us determine the number of  X  a, b, c, d  X  in S first a (position 2). From the head array, we know that there are 5  X  b, c  X   X  X  after the first a . Because of the lexicographic order, these 5  X  b, c  X   X  X  must be associated with the 5 right-most entries of the tail array. According to the tail array, there are 2, 1, 0, 1, and 0 d  X  X  after those 5  X  b, c  X   X  X  respec-tively. Therefore, there are 2 + 1 + 0 + 1 + 0 = 4  X  b, c, d  X  a and 1 after the third a . So in total there are 4 + 1 + 1 = 6 occurrences of  X  a, b, c, d  X  in S
We can generalize the above computation for any head array H and tail array T . We call the resulting sum the  X  X T-sum X , which can be expressed by the following formula:
Since sd tions, is given by Q | P | s ( P ) = sn i ( P ) /sd i ( P ) can be readily determined.
In Section 5 we show that given a length-k candidate pattern P and its generating sub-patterns P have constructed the head array H and the tail array then sn be computed by HT-sum. However, the tail array contains sn tial to the pattern X  X  length. It is thus impractical to construct or store all the tail arrays. In this section we show that it is possible to compute an upper bound of the HT-sum by stor-ing only 3 numbers without ever constructing the head and tail arrays. We call this bound the HTBound. Similar to the idea of MinBound, the HTBound allows us to prune candi-date patterns for a more efficient mining algorithm. We will show at the end of this section that HTBound is tighter than MinBound. To better illustrate the concepts, we continue with our running example considering the data sequence S the length-k candidate pattern P =  X  a, b, c, d  X  , its head sub-pattern P
To determine the HTBound of P , we need the follow-ing three values, all obtainable in previous iterations of the mining algorithm. (We show the corresponding values of our running example in parentheses.) We assume that the number of replicates for each column is stored as metadata, i.e., we know r ( j ) for all column j mer equals the number of entries in the head array, while no entry in the tail array can exceed the latter. In our example, r ( P [1]) = r ( a ) = 3, so H has 3 entries, and r ( P [ k ]) = 3, so no entry in T exceeds 3.

The above values thus constrain the sizes, sums and max-ima of H and T . For convenience, we call them the  X  X on-straint counts X . The following property, easily verifiable by the definition of head array, states another constraint on Property 1 The entries in the head array H are non-increasing (from left to right).

Our idea of upper bounding HT-sum( H , T ) is to show that there exists a pair of arrays H  X  and T  X  that can be ob-tained from H and T through a series of transformations. We will prove that (1) each transformation will not reduce the HT-sum and hence HT-sum ( H, T )  X  HT-sum ( H  X  , T  X  straint counts. Because of (2), H and T need not be materi-alized and stored. We will show a closed-form formula for sum( H , T ), in terms of the constraint counts. The transfor-mations are based on the following  X  X ush X  operations: Definition 3 A push-right operation on an array A from entry l to entry l 0 reduces A [ l ] by a positive value v increases A [ l 0 ] by v , where l &lt; l 0 .
 Definition 4 A push-left operation of an array A from entry l to entry l 0 reduces A [ l ] by one and increases A [ l 0 where l 0 &lt; l .

Essentially, the push operations push values towards the right and left of an array respectively. Here are two useful properties of the push operations: Lemma 1 With a fixed head array, each push-right opera-tion on the tail array T cannot reduce the HT-sum. Proof. A formal proof is given in the Appendix. In sum-mary, in the procedure of computing the HT-sum (Sec-tion 5), for each entry in the head array, a number of right-most entries of the tail array are summed. Since each push-right operation on T transfers a positive value from an entry to another entry on its right, the sum cannot be reduced. Lemma 2 If the tail array is non-increasing, each push-left operation on the head array cannot reduce the HT-sum. Proof. A formal proof is given in the Appendix. Here, we illustrate the proof by an example. Consider our example head array H = [5 , 2 , 2] . If we push-left on H from entry H [3] to H [2] by a value of 1, we get  X  H = [5 , 3 , 1] calculating the HT-sum, the entries H [2] = 2 and H [3] = 2 each requests the sum of the two rightmost entries in the entries  X  H [2] = 3 and  X  H [3] = 1 request the sum of the three rightmost entries in T (i.e., T [ t -2.. t ]) and the value of the rightmost entry in T (i.e., T [ t ] ), respectively. So the net difference HT-sum (  X  H, T )  X  HT-sum ( H, T ) = T [ t T [ t -1]. If T is non-increasing , T [ t -2]  X  T [ t -1] and thus HT-sum ( H, T )  X  HT-sum (  X  H, T ) .

Note that Lemma 2 is applicable only if the tail array is non-increasing. In our running example, however, T does not satisfy the non-increasing requirement. Fortunately, we can show that by applying a number of push-right opera-tions, we can transform T to a T 0 that is non-increasing. of push-left operations to transform H to an H  X  . Finally, we apply push-right operations to transform T 0 to a T  X  . In this transformation process, by Lemmas 1 and 2, we have HT-sum ( H, T )  X  HT-sum ( H, T 0 )  X  HT-sum ( H  X  , T 0 )  X  HT-sum ( H  X  , T  X  ) . To complete the discussion, we need to define the contents of T 0 , H  X  and T  X  , and to show that (1) T by transforming T via a number of push-right operations; (2) H  X  can be obtained from H via a number of push-left operations, each of which preserves the non-increasing property of the entries, and the content of H  X  so defined can be derived from the constraint counts; and (3) T  X  can and its content so defined can be derived from the constraint counts. To accomplish the above, we need to prove a few properties of T first.

Recall that T contains sn l -th entry of T records the number of P [ k ] that appears after the l -th occurrence of P [2.. k -1] in the data sequence our example, P [2.. k -1] =  X  b, c  X  and there are 8 occurrences of it in S if they correspond to the same occurrence of P [2]. In our example, P [2] = b . The three occurrences of b are posi-tions (1), (4) and (8). So we group the first 3 entries (which ilarly, the remaining entries in T are divided into two more groups. We note that each group forms a segment in the T array, called an interval . In our example, the intervals are: Given an interval I in T , we define the interval average of
I as the average of the entries in I . For example, the interval averages of the 3 intervals in our example T are 1, 1, and 0.5, respectively. Here is an important property of the interval averages: Lemma 3 The interval averages are non-increasing.
 Proof. A formal proof is given in the appendix. In sum-mary, consider any interval I and its immediate right neigh-bor interval I 0 . We can show that I must contain I 0 as its rightmost entries (e.g., the second interval ([2,1,0]) contains the third interval ([1,0]) at its right end). We can also show that if I contains additional entries (other than those of the average of these additional entries must be at least as large as the interval average of I 0 (e.g., the additional entry [2] in the second interval is larger than the third interval X  X  average, which is 0.5). Therefore, the interval average of must not be smaller than the interval average of I 0 . Hence, the interval averages are non-increasing.
 With the concept of intervals, we are ready to define T 0 : Definition 5 Array T 0 is the same as T in terms of its size, the number of intervals, and the length of each interval. For each interval I in T 0 , the value of each entry in I is equal to the average value of the corresponding interval in T . With our running example, we have, The following lemma states the desired properties of T 0 . from T via a number of push-right operations.
 Proof. (a): Within each interval, entries in T 0 share the same value, so they are non-increasing. Also, the entries in T averages are non-increasing. So, the entries in T 0 are non-increasing across intervals. (b): A formal proof is given in the appendix. In sum-mary, for each interval of T , we use push-right operations to obtain the corresponding interval of T 0 . Here we use our example to illustrate. The entries of the first interval of are non-increasing, therefore we can repeatedly move the excessive values from the leftmost entry to the next one by push-right operations, forming (1 , 2 , 0) and then (1 , 1 , 1) Next, we define H  X  . Recall that the l -th entry of records the number of times the pattern P [2.. k -1] occurs after the l -th P [1] in S sn value to the left as possible, subject to the constraint that no entry in H  X  exceeds sn the same size and sum. Let x be the number of entries in H , y = sn H In our example, x = 3, y = 8, and z = 9. H  X  is thus: Note that x , y , and z can be obtained from the constraint straint counts based on Equation 2 without materializing H Lemma 5 H  X  is obtainable from H by a number of push-left operations that preserve the non-increasing property. Proof. There are three types of entries in H  X  : (1) 0-valued entries, all rightmost; (2) max-valued entries, all leftmost; (3) zero or one remainder entry. For any entry j of H , we call it a donor, a receiver or a remainder entry if the j repeatedly perform the following: take the rightmost donor that is non-zero, and use a push-left operation to move one from it to the leftmost receiver that is not equal to the max-imum value y yet, or to the remainder entry if all receivers are already equal to y . After all the donors are made 0 by the above procedure, if there is a receiver that is still smaller than y by an amount w , we push w from the remainder entry to the receiver to obtain H  X  . It is easy to see that each op-eration preserves the non-increasing property of the array.
For our example, there is a donor H [3] , a receiver H [1] and a remainder entry H [2] . We first use two push-left oper-ations to move 2 from H [3] to H [1] to form (7 , 2 , 0) . Then since the receiver still has not reached the maximum value y = 8 , we use a push-left to move 1 from H [2] to H [1] to form (8 , 1 , 0) , which is equal to H  X  . from T 0 . Recall that T has sn sum of sn T evening among the x entries, with the reminder distributed to the rightmost entries of T  X  . That is, In our example, x = 8 and z = 7. T  X  is thus: counts alone.
 push-right operations.
 Proof. Since the entries in T 0 are non-increasing and those in
T  X  are non-decreasing, if T 0 [1] = T  X  [1] , then all corre-sponding entries in the two arrays are equal and no push-right operations are needed. Otherwise, T 0 [1] &gt; T  X  [1] we can move the difference T 0 [1]  X  T  X  [1] to T 0 [2] by a push-right operation. If we now ignore the first entry of each ar-ray, the same argument then applies to the second entry. We can repeat the process to equalize every pair of correspond-ing entries of the two arrays.

One can verify the following closed-form formula of HT-Finally, Note that the above computation only requires the constraint counts. Therefore HT-sum( H  X  , T  X  ) can be calculated with-out materializing any of H , H  X  , T , T 0 or T  X  . For our running example, h HT-sum ( H  X  , T  X  ) = 1  X  7+1  X  (0+1) = 8 . Our HTBound thus equals HT-sum ( H  X  , T  X  ) /sd the exact support is 6/81 and the MinBound is 7/27 = 21/81 (see Section 4). HTBound is thus much tighter than Min-Bound in this example. This is not mere coincidence. We can show that the HTBound is indeed theoretically guaran-teed to be better than the MinBound.
 Lemma 7 HTBound is always at least as tight as Min-Bound.
 Due to space limitation, readers are referred to the Ap-pendix for a proof of Lemma 7.
In this section we evaluate our methods using a real mi-croarray dataset that was also used in some previous stud-ies on mining data with repeated measurements [10, 12]. It is a subset of a dataset obtained from a study of gene re-sponse to the knockout of various genes in the galactose uti-lization (GAL) pathway of the yeast Saccharomyces cere-knockout experiments of 9 GAL genes and the wild type, growing in media with or without galactose, yielding a total of 2(9 + 1) = 20 experiments (columns). Each experiment has 4 replicates. There are 205 rows corresponding to genes that exhibit responses to the knockouts. The genes belong to four different classes according to their functional cate-gories. Figure 1 in Section 1 shows some example values of our dataset (only 1 replicate per column is shown).
We compare the performance of three methods: (1) Ba-sic , which applies the basic Apriori algorithm (see Figure 2) with data compression, (2) MinBound , which is the Basic method plus candidate pruning using MinBound, and (3) HTBound , which is the Basic method plus candidate prun-ing using HTBound. To test the scalability of the methods, we insert synthetic replicates, columns and rows to form larger datasets. To synthesize additional replicates, for each gene and each experiment, we follow standard practice to model the values by a Gaussian distribution with the mean and variance equal to the sample mean and variance of the 4 replicates. The expression values of new replicates are then sampled from the Gaussian. New columns are synthe-sized by randomly drawing an existing column, discarding the existing expression values, but keeping the fitted Gaus-sians and sampling new values from them. This way of construction mimics the addition of knockout experiments of genes in the same sub-paths of the original ones. Fi-nally, new rows are synthesized as in the synthesis of new columns, but with an existing row as template instead of a column. This way of construction mimics the addition of genes that have similar responses as some existing ones, due to co-occurrence in the same downstream pathways. In the experiments, the default support threshold is 20%.
We first compare the patterns mined under three differ-ent settings: (1) Apply the original OPSM method on only one set of replicates at a time. Since there are 4 replicates per column, the basic OPSM method is applied 4 times. We call these OPSM-i ( i = 1 .. 4 ). (2) Replace replicates by their averages and apply the basic OPSM method (OPSM-avg). (3) Consider all replicates and apply our approach (OPSM-RM). We use Fisher X  X  exact test [5] to compute the p-value of each pattern. Intuitively, a small p-value indi-cates that the genes that support the pattern are highly likely to belong to the same biological class. A pattern is said to be significant if its p-value is less than 0.01. In microar-ray data analysis, the classes are usually unknown and the mined patterns help identify genes that are biologically re-lated. Biologists need to perform costly small-scale exper-iments to verify the results. A successful mining algorithm should therefore return as few insignificant patterns as pos-sible, in order to minimize the cost.

From the mined patterns, we observe that OPSM-RM always returns fewer insignificant long patterns at various support thresholds. As an illustration, Table 3 shows the number of insignificant patterns of length 4 or more at 20% support threshold.
 Table 3. Number of insignificant long patterns
Our result shows that if we consider only one set of replicates (OPSM-i ), or only the averages of the replicates (OPSM-avg), many insignificant long patterns are returned. Some genes that may not be functionally related could sup-port the same long patterns due to noise in data. Since these insignificant patterns are not returned by OPSM-RM, our result shows that OPSM-RM is robust against data noise.
We now focus on the OPSM-RM model. First, we com-pare the efficiency of the three methods (Basic, MinBound, HTBound) by applying them to a dataset with 5,000 rows. We report the running time (Figure 3 left) and the number of unpruned candidates that need verification (Figure 3 right) pattern lengths k . For the graph on the right, we also show the actual number of frequent patterns mined for reference.
The figure shows that the two bounding techniques are very effective in speeding up the mining time by pruning infrequent candidates. The pruning effectiveness is most pronounced in iteration 4, in which the number of candi-dates is the highest. In addition, the pruning power of the HTBound is always stronger than MinBound, which is con-sistent with Lemma 7. From the figure, we also see that the number of unpruned candidates under HTBound is very Figure 3. Speed performance in different iterations close to the actual number of frequent patterns. In partic-ular, there are in total 17,900 candidates generated by the Basic method, among which 4,751 are frequent. There are therefore 13,149 infrequent candidates. Under HTBound, there are only 5,538 unpruned candidates, among which 5,538-4,751 = 787 are infrequent. So, HTBound has pruned (13149-787)/13149 = 94% of all infrequent candidates.
To study the effect of the support threshold, we repeat the comparisons at different thresholds. A dataset with 205 rows is used in this experiment. The results are shown in Figure 4. The left panel shows the running times, and the right panel shows the running times as percentages of the Basic method. Figure 4. Running time at various support thresh-olds
In general, the bounding methods significantly improve the efficiency of the mining algorithm, and HTBound is more effective than MinBound in all cases. We observe that at higher support thresholds, the two bounds are capable of pruning more candidates. Yet even at low thresholds, the bounds could still provide substantial performance gains.
Next, we study the scalability of the methods by varying the number of rows, columns, and replicates per column. The results are shown in Figures 5, 6 and 7, respectively.
The relative pruning power of the two methods as com-pared to the basic algorithm remains largely stable in all three sets of experiments. Also, the running time remains reasonable when there are many replicates, columns or rows, which demonstrates the practicality of our new def-inition of OPSM in analyzing large datasets.
Figure 6. Scalability w.r.t. number of columns
In this paper we have described the problem of high noise level to the mining of OPSM X  X , and discussed how it can be alleviated by exploiting repeated measurements. We have listed some practical requirements for OPSM-RM, and proposed a concrete definition that fulfills the require-ments. We have described a basic Apriori mining algo-rithm that utilizes a monotonic property of the definition. Its performance depends on the component functions gen-erate and verify . We have suggested a sequence compres-sion method for reducing the running time of verify . For generate , we have proposed two pruning methods based on the MinBound and the HTBound. The latter makes use of the head and tail arrays, which are useful both in construct-ing and proving the bound. We have performed experiments on real microarray data to demonstrate the effectiveness of the pruning methods, and the scalability of the algorithm.
We will continue our study on the head and tail arrays to see if it is possible to further improve the HTBound. We will also apply our technique to the analysis of other mi-croarray datasets, to look for new findings due to our new definition of OPSM.

Figure 7. Scalability w.r.t. number of replicates Lemma 1 Proof. Let H be a head array, and T with | T operation that moves a positive value v from the x -th entry of T 1 to the y -th entry, with x &lt; y . Then, = = =  X  0 Therefore the HT-sum is not reduced.
 Lemma 2 be a head array, and H where H one from the x -th entry of H y . Due to the push-left operation and the non-increasing property of head arrays, H 2 [ x ] = H 1 [ x ]  X  1 &lt; H 1 [ x ]  X  H 1 [ y ] &lt; H We have  X  | T 0 |  X  H 1 [ x ] + 1 &gt; | T 0 |  X  H 2 [ y ] + 1  X  T 0 [ | T 0 |  X  H 1 [ x ] + 1]  X  T 0 [ | T 0 |  X  H 2 [ y ] + 1]  X   X  T 0 [ | T 0 |  X  H 1 [ x ] + 1] + T 0 [ | T 0 |  X  H 2 where the third line is due to the non-increasing property of T = = = =  X  T 0 [ | T 0 |  X  H 1 [ x ] + 1] + T 0 [ | T 0 |  X  H 2 [ y ] + 1]  X  0 Therefore the HT-sum is not reduced.
 Lemma 3 Proof. Without loss of generality, let us compare the aver-ages of the first and second intervals. Each entry in the sec-ond interval corresponds to an occurrence of P [3.. k -1] after the second occurrence of P [2] , which is in turn after the first occurrence of P [2] . Therefore each entry in the second in-terval has a corresponding entry in the first interval with the same value. The first interval may contain additional en-tries, corresponding to occurrences of P [3.. k -1] where the P [3] is before the second occurrence of P [2] . Since the en-tries are in lexicographic order, these additional entries must be the leftmost entries of the first interval. Let us call the additional entries the leading group and the remaining ones the trailing group. We will prove that the average of the leading group is no smaller than that of the trailing group, which is sufficient to show that the average of the first inter-val is not smaller than that of the second interval. We prove this proposition by mathematical induction.

Base case : k -1=3. As discussed, the entries in the lead-ing group all have their P [ k -1]= P [3] before the second oc-currence of P [2] while the entries in the trailing group all have their P [ k -1] after it. Since the value of an entry equals ing group must be not smaller than every entry in the trailing group. The average of the leading group must therefore be not smaller than the average of the trailing group.
Inductive case : Now assume the proposition is true up to k -1= l , for some l  X  3 . For k -1= l +1, we transform the se-quence by keeping only elements after the first occurrence of
P [2] , and then remove all occurrences of P [2] in the re-sulting subsequence. Then each entry in the first interval of the original sequence corresponds to the number of oc-currences of P [ k ] after a P [3.. k -1] in this transformed se-quence. We again partition the transformed sequence into intervals by grouping entries that share the same occurrence of
P [3] together. If we can show the averages of these in-tervals are non-increasing, then certainly the average of the leading group, which is composed of the leftmost intervals, must be not smaller than the average of the trailing group, which is composed of the rightmost intervals. But this is ex-actly the inductive assumption. Therefore by mathematical induction, the proposition is true for all k  X  4 . Lemma 4(b) Proof. We will prove that for each interval of T , we can use push-right operations to obtain the corresponding interval of T Base case : k -1=3. As proved in the base case of Lemma 3, the entries in the interval are non-increasing in T . We repeat the following: take the leftmost entry in the interval that is larger than the average, and use a push-right operation to move the difference to the next entry. The re-which is the same as the corresponding interval in T 0 .
Inductive case : Now assume the proposition is true up to k -1= l , for some l  X  3 . For k -1= l +1, we first partition the entries in the interval of T into sub-intervals according to which P [3] they refer to. As proved in the inductive case of Lemma 3, the averages of these sub-intervals are non-increasing. We use push-right operations to make them all have the same average as follows: repeatedly we pick the leftmost sub-interval with an average larger than the average of the whole interval. Then we move the difference from the last entry of the sub-interval to the first entry of the next sub-equal to the average of the corresponding sub-intervals of T of the corresponding sub-interval of T .
 Lemma 7 Proof. For any pattern P [1.. k ] and each row i , MinBound is composed of two parts due to the head P [1.. k -1] and the tail P [2.. k ], with values s spectively. The part due to the head assumes the extreme case that each occurrence of the head is followed by r ( P [ k ]) occurrences of P [ k ] later in the sequence. It is interesting that this part of the bound, s from HT-sum( H, T  X  ), where H is the actual head array of P actual tail array of P , but every entry takes the maximum allowed value r ( P [ k ]) of the array: where the last line is due to the sum constraint of the head array. Normalizing the HT-sum by the number of replicate combinations, we get the part of MinBound due to the head:
Since the bounding tail array T  X  cannot contain any en-try larger than the maximum, HT-sum( H  X  , T  X  ) must not be larger than HT-sum( H, T  X  ):
Therefore the corresponding bound for s larger than that from the part of MinBound due to the head.
Similarly, the part of MinBound due to the tail assumes the extreme case that each occurrence of the tail is preceded by r ( P [1]) occurrences of P [1] earlier in the sequence. This part of the bound, s sum( H  X  , T ), where T is the actual tail array of P and is an array with the same number of entries as the actual head array of P , but every entry takes the maximum allowed value sn number of entries of T :
HT-sum ( H  X  , T ) = where the third line is due to the sum constraint of the tail array and the fourth line is due to the size constraint of the head array.
 Again, we can show that it is no better (smaller) than HT-sum( H  X  , T  X  ) : HT-sum ( H  X  , T  X  ) =
Combining the two parts of results, HTBound is always at least as tight as MinBound.
