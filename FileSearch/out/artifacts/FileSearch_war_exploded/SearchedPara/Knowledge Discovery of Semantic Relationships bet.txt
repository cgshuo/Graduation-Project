 We developed a model based on nonparametric Bayesian modeling for automatic discovery of semantic relationships between words taken from a corpus. It is aimed at discovering semantic knowl-edge about words in particular domains, which has become increas-ingly important with the growing use of text mining, information retrieval, and speech recognition. The subject-predicate structure is taken as a syntactic structure with the noun as the subject and the verb as the predicate. This structure is regarded as a graph structure. The generation of this graph can be modeled using the hierarchical Dirichlet process and the Pitman-Yor process. The probabilistic generative model we developed for this graph structure consists of subject-predicate structures extracted from a corpus. Evalua-tion of this model by measuring the performance of graph cluster-ing based on WordNet similarities demonstrated that it outperforms other baseline models.
 G.3 [ Probability and Statistics ]: Nonparametric statistics Algorithms Probabilistic Generative Model, N onparametric Bayes, Graph Clus-tering, Text Mining
Semantic knowledge about words for particular domains is in-creasingly important in text mining, information retrieval, speech recognition, and so on. We describe an unsupervised approach based on a probabilistic generative model to automatically discov-ering semantic relationships between words taken from a corpus. A probabilistic generative model is used for modeling the data genera-tion process. By focusing on the generation process, we can extract specific properties of the data such as a latent topic or document category. Moreover, our approach is based on the distributional hy-pothesis, which is the basis of statistical semantics and states that words that occur in the same context tend to have a similar mean-ing. This hypothesis is attracting a great deal of attention in the field of cognitive science [12].

We focused on the syntactic structures found in a sentence and used them as contextual information. In particular, the subject-predicate structure was taken as a syntactic structure with the noun as the subject and the verb as the predicate. That is, the observed data consisted of pairs of a subject (noun) and a predicate (verb) extracted from a corpus. The process for generating this data was modeled using a probabilistic generative model. The extracted pred-icates were regarded as the context of the subject and vice versa.
These extracted pairs of a subject and a predicate are regarded as a graph structure in which each vertex corresponds to a subject or a predicate and each edge means that the corresponding pair of a subject and a predicate is actually observed in the corpus. This graph structure is a disassortative graph structure in which the ver-tices have most of their connections outside their group. Figure 1 shows an example of a disassortative graph structure.

We extended the interpretation of the distributional hypothesis to  X  X ords that have links to the same vertices tend to have a similar meaning X . The purpose of our study was to develop a probabilistic generative model for this disassortative graph structure.
The probabilistic generative model proposed by Newman et al. [13] for a disassortative graph structure can be used to generate an assortative graph structure in which all the vertices are divided into groups such that the members of each group are mostly connected to other member of the same group, in a manner similar to a social network. We called this the  X  X ewman model X .

The Newman model assumes that the vertices in a graph are di-vided into classes or groups, and each class that corresponds to Figure 3: Example of Graph Clustering with Multiple Classes a latent variable is characterized by its links to particular vertices. This model is flexible, which enables researchers to analyze ob-served graph data without specifying the structure (disassortative or assortative) in advance. However, to extract semantic relationships between words, the user must first decide in advance the number of semantic classes. Moreover, each word is determined to belong to only one class that represents its semantic meaning or related concept. We illustrate the second problem in detail by using Figure 2.

Suppose there are three words,  X  X et X ,  X  X ake X , and  X  X ook X . This exclusive clustering generates classes such as { X  X et X ,  X  X ake X  X  { X  X ook X  X , { X  X et X  X { X  X ake X ,  X  X ook X  X , and { X  X et X ,  X  X ake X ,  X  X ook X  X . These results degrade clustering performance, and exclusive clustering is not appropriate for extracting semantic relationships between words. Therefore, we have to work out a method that allows over-lapping of semantic classes of a word (as illustrated in Figure 3). To overcome these problems, we developed a probabilistic generative model of a graph using the hierarchical Dirichlet process (HDP) [16] and the Pitman-Yor process (PYP) [14].

The remainder of this paper is organized as follows. Section 2 discusses related works. Section 3 describes the Newman model. Section 4 describes our proposed model. Sections 5 presents exper-imental results. Section 6 summarizes the key points and mentions future work.
The use of graph structures for lexical acquisition of words has received much attention. The relationships between graph struc-tures and human languages have been explained using power-law distributions like Zipf X  X  law [3]. Graph clustering algorithms are important for unsupervised lexical acquisition of words.
The Markov cluster (MCL) algorithm, a well known graph clus-tering method proposed by Van Dongen [17], consists of two steps, expansion and inflation, which are processed alternatively. A graph is subdivided into clusters without any overlap. The MCL algo-rithm has been applied to various domains including corpus lin-guistics. Dorow et al. [4, 5]) described an unsupervised algo-rithm that automatically discovers word senses from text by using a graph model representing words and the relationships between them. Sense clusters are iteratively updated by clustering a local graph of similar words around an ambiguous word. Gfeller et al. [8] used this algorithm to clean up a dictionary of synonyms that appeared to be ambiguous, incomplete, and inconsistent.
Relational learning, which has also received a great deal of atten-tion, is used, for example, to find social roles in social network data and to discover an ontology in a particular domain. The stochas-tic block model is a well-known model for relational learning in sociology. Kemp et al. [10] described the infinite relational model (IRM), which is a stochastic block model for entity-relational mod-eling. Entities are partitioned into clusters, and the number of clus-ters is estimated using the Dirichlet process(DP). They used it to automatically extract a biomedical ontology from the Unified Med-ical Language System. A vertex can be regarded as an entity, and a link between vertices can be regarded as the relationship between the vertices. It is straightforward to use relational models for mod-eling the graph generation process.
In this section, we describe the Newman model and the expan-sion of it by using the Dirichlet process (DP) [7]. vertices. is the number of classes. Suppose that the vertices fall into classes with probability ,where is the probability that a vertex is assigned to class .Vertex belongs to class , indicated by . Each class has a probability that a link from a particular vertex in class connects to vertex . A link from vertex to vertex is indicated by . Each vertex links to other vertices in accordance with .Thatis,vertex links to vertex in accordance with . The generation process for link is represented by where ,and is a multino-mial distribution.

The parameters , ,and are estimated using the EM algo-rithm: where is an adjacency matrix with elements if there is an edge from to ;otherwise .
As mentioned above, a problem with the Newman model is that one must decide the number of classes in advance. This problem can be solved by using DP [7][2]. We describe DP by adapting DP to Newman Model.
Suppose that is distributed in accordance with the Dirichlet distribution ; i.e., ,where is a parameter of the Dirichlet distribution. is a random probab ility measure over : where is the DP concentration parameter, and is the base measure, which is the Dirichlet distribution here. The generation process for link is represented by
The probability of given and adjacency matrix is formulated on the basis of Bayesian theory: where ,and . We can adapt a Gibbs sampler algorithm for estimating by using Eq. (9). where is the gamma function, and indicates if ; otherwise, it indicates . where is the number of vertices except vertex assigned to class ; i.e., .

This calculation can be described using a Chinese restaurant metaphor [1]. Consider a Chinese restaurant with an infinite number of ta-bles, each of which represents a different class. Each table has an infinite seating capacity. Customers enter the restaurant and seat themselves (each customer corresponds to a vertex). The first cus-tomer sits at the first available table, and each subsequent customer sits at an occupied table with a p robability proporti onal to the num-ber of customers already sitting there; i.e., , or at an unoccupied table with probability proportional to , which is estimated by auxiliary variable sampling [6].
Suppose that each vertex has multiple classes, and that links from a vertex are generated in accordance with its classes. This gener-ation process can be modeled using the hierarchical Dirichlet pro-cess (HDP) [16]. First, we describe the generation process by using the restaurant representation in Figure 4. Then, we formulate the generation process using HDP and PYP.
Each vertex is or has a restaurant. Each restaurant has tables, and each table is served a menu corresponding to a class. There are an infinite variety of menus. A menu lists specific dishes that repre-sent links to certain vertices. Vertex generates a link to vertex as follows. A customer enters restaurant , sits at a table with menu and orders dish . How a customer selects and enters a restaurant is described in Section 4.3. Figure 4 shows the restaurant represen-tation of the graph.

The model is formulated as follows. class assigned to vertex . is the probability that a link to vertex is generated from class . is the -dimension Dirichlet distri-bution, which is a probabilistic measure with ; i.e., . is the random probabilistic measure over class. is the random probabilistic measure over class given ver-tex , as shown in the following formulas. where and are the DP concentration parameters.

The following formulas represent the stick-breaking representa-tion of the generation process [15][16]. where is the Beta distribution, is the probability that class is generated in vertex ,and is the base probability of .
An observation is a link from vertex to vertex ; that is, a cus-tomer in restaurant orders dish . Weestimatethemenu(i.e., class), , being used by this customer. The indicates that the class of vertex is class when vertex generates a link to vertex . We use Gibbs sampling to infer the posterior distribution over latent class , similar to HDP [16]. is an index of menus (classes). is an index of tables.
 Let be the number of customers in restaurant sitting at table with menu .
 Let be the number of tables in restaurant serving menu . A dot denotes marginal counts; for example, is the number of customers sitting at table in restaurant , is the number of customers being served menu in restaurant (number of occur-rences of class in vertex ), and is the number of tables in restaurant . A customer eating dish on menu in restaurant is represented by . The number of customers eating dish on menu in all restaurants is represented by , which is the number of links to vertex for class .

The posterior probability over class is calculated using Bayes theorem. where  X  means the number of cus-tomers eating dish on menu with a customer eating in restau-rant removed from the calculation. The is sampled using where is obtained using functions AddCustomer and Re-moveCustomer (see Appendix A) and and are estimated by auxiliary variable sampling (see Appendix B).

We identify class of vertex linking to vertex as We cluster vertex in a graph by using this estimated class. Dif-ferent classes can be assigned to the same vertex depending on the vertices to which the vertex links. Therefore, a vertex are not al-ways be determined to belong to only one class; i.e., there is soft clustering. We model the selection of vertices that link to another vertex. That is, we select vertex from a probabilistic distribution over vertices and generate a link from to another vertex by using the process described in Section 4. In the restaurant representation, we model how a customer selects and enters a restaurant (vertex). The selection is related to vertex degree because the number of customers selecting a vertex is equal to the degree of the vertex. The modeling of vertex degree is useful for understanding the graph structure.

The probabilistic model of graph degree is based on PYP [14] because it generates a power-law distribution. We assume the ver-tex space size is .Let be a random probab ility measure over vertices; is the probability that vertex is selected to link to another vertex. We select a vertex from and link from the selected vertex to another vertex by using the process described in Section 4.1. is distributed in accordance with PYP: where $ $' is called the  X  discount parameter  X  , % ( $ is called the  X  concentration parameter  X  ,and &amp; denotes the uniform distribution over vertices, s.t. ( &amp; ).

The probability of the -th selection of vertex " , given that some selected vertices " ,isgivenby " " where is the number of the selection of in the previously selected " and ) is the number of newly selected vertices. In the restaurant representation, " indicates that the -th cus-tomer selects restaurant ,and " denotes the selections by the previous customers. The -th customer selects restaurant with probability or selects new restaurant with probability times a vertex (restaurant) has been selected, the more likely that vertex (restaurant) will be selected in the future. This is called the rich-get-richer property. Second, if $ is nearly one, the more times a new vertex (restaurant) is selected, the more likely a new vertex (restaurant) will be selected because ) is incremented. That is, the number of vertices (restaurants) with a small number of links (cus-tomers) increases. These two effects together produce a power-law distribution. In this way, we model the degree distribution of the graph using a power-law distribution. The $ and % are estimated using auxiliary variable sampling (Appendix C).
A graph is generated using the proposed model as follows. 1. Select restaurant (vertex) from # $ % &amp; . 2. Select a table with menu (class) from . 3. Select a dish (link) from menu .
 Figure 5 shows a graphical representation of the proposed model. The observed variables are vertex and its link . The other vari-ables are estimated using Gibbs sampling.

The Gibbs sampling in our model works as follows: 1. for //for each vertex 2. for if 3. RemoveCustomer( , ) 4. Sample using Eq. (22) 5. AddCustomer( , ) 6. end for 7. end for 8. Sample using Eq. (26) 9. Sample and 10. Sample $ and %
In an evaluation experiment, we investigated the syntactic struc-tures found in sentences. In particular, we took a subject-predicate structure as a syntactic structure with a noun as the subject and a verb as the predicate. The observed data were pairs of a subject (noun) and a predicate (verb) extracted from a corpus. The ex-traction was done using a head-driven phrase structure grammar (HPSG) parser called Enju 1 . We evaluated the proposed model by measuring its performance for graph clustering based on WordNet similarities. We compared the results of our model with those of the DP-Newman model, the infinite relational model (IRM), and the Markov cluster algorithm (MCL). http://www-tsujii.is.s.u-tokyo.ac.jp/enju
We used the Reuters corpus 2 and the LISA corpus 3 . LISA stands for "Library and Information Science Abstracts".

We considered two graph structures from Reuters.  X  X euters1 X  is a graph constructed from the pairs in the corpus with frequencies of over one. That is, if a pair was observed even once, we made an edge between the subject and predicate.  X  X euters2 X  is a graph constructed from the pairs in the corpus with frequencies of over two.  X  X ISA X  is a graph constructed from the pairs in the corpus with frequencies over one.

The number of links (extracted pairs) of a subject and a predicate were 33,054 for Reuters1 and 19,202 for Reuters2. The number of subjects was 4,438, and that of predicates was 2,583 for Reuters1. The number of subjects was 1,991, and that of predicates was 1,207 for Reuters2. The number of links (extracted pairs) of a subject and a predicate was 3,401 for LISA The number of subjects was 858, and that of predicates was 838 for LISA.
We used the WordNet similarity of words as used by Hagiwara et al. [9] to evaluate our model. WordNet similarity is based on the thesaurus tree structure in WordNet. The similarity between word sense * and is calculated as follows.

Assume that word * has synset * and word has synset .Let $ be the depth of node * from the root node. Let $ be the depth of node from the root node. Let $ be the maximum depth of common ancestors. The idea behind this similarity was derived from Lin X  X  method [11]. Figure 6 shows a calculation example of WordNet Similarity.
We evaluated how well each algorithm performed by measur-ing the quality of output clusters in terms of WordNet similarities. We used a label-prediction -based accuracy measure because it can be easily applied to both soft-and hard-clustering algorithms. For http://www.daviddlewis.com/resources/testcollections/reuters21578/ http://www.library.pitt.edu/articles/database_info/lisa.html Figure 6: Calculation Example of WordNet Similarity:  X  X ill X  and  X  X oast X  target word * , we predicted whether other words were similar or dissimilar to it. The set of target words, , was constructed us-ing pairs of nouns and verbs included in WordNet in a corpus. We selected a set of words for which the similarity to target word * was above a threshold and labeled those words .We also selected a set of words for which the similarity to the target word was below a threshold and labeled those words . The num-ber of words, -* , in each set for each target word * was equal. To predict these correct labels from the output of the algorithms, we created a similar word cluster , / * ,for * by merging all the clusters generated by the algorithm that contained * into a new cluster. If word " is in / * , the label of " is predicted to be ; otherwise, it is predicted to be . Therefore, the accuracy for word * is given by where  X  " is if " / * and the correct label of " is or " / * and the correct label of " is ;otherwise,itis .The total accuracy was calculated by taking the average accuracy for all words:
We set the threshold for WordNet similarity to . We calcu-lated and averaged the accuracy separately for three domains in ac-cordance with the degree distribution in the graph (as illustrated in Figure 7). Words in the target word set were ranked in accordance with the degrees of their corresponding vertices in the graph. We roughly separated the target word set into three domains: domain 1 is the set of words with high degrees, domain 2 is the set of words with medium-level degrees, and domain 3 is the set of words with low degrees. This separation was done to clarify the type of words for which each algorithm is good from the viewpoint of vertex de-grees (i.e., the number of adjacent words). As described in the next section, this separation enables clear discrimination among the al-gorithms which algorithms perform well for high-degree vertices and which ones perform well for low-degree ones. Table 1 lists the base information for each dataset with respect to the number of words and average degree in each domain. Domain1: 1% Figure 7: Three Domains in Accordance with Degree Distribu-tion We compared the results for our model with those for the DP-Newman model, the IRM, and the MCL algorithm. We also consid-ered frequency-weighted versions of the DP-Newman model and the proposed model. Since the DP-Newman model and the pro-posed model are based on a mixture of multinomial distributions, they can be easily modified to take integers greater than as the value of . Therefore, they can use the frequency of pairs of a subject and a predicate as a weight for links. If the value of curred times where the occurrences are treated individually and assigned different class variables. In restaurant representation, the frequency (weight) indicates the number of customers ordering the same dish. We assumed that such customers are individually seated to tables.  X  X P-Newman-f X  and  X  X roposed-f X  indicate frequency-weighted models.

Hereafter, we call MCL, IRM, and DP-Newman(-f) the  X  X ase-lines X .

A comparison with respect to accuracy is shown in Figures 8-13. The proposed algorithm performed especially well for domain-2 words and not particularly well for domain-1 words. In con-trast, the baselines generally performed best for domain-1 words although not as well as the proposed algorithm. Moreover, their performances did not differ drastically across the three domains.
We think that the MCL X  X  poor performance was due to the fact that it was a model proposed for extracting communitiy graphs, which are assortative graphs in general. Therefore, it might not work well for dissassortative graph structures like predicate-argument graphs used in our study.

The IRM performed especially poorly for domain 3. This is because the vertices in domain 3 tended to be categorized into a large cluster by IRM. DP-Newman-f performed a bit better than DP-Newman in some cases thanks to its use of frequency informa-tion.

The proposed model outperformed the baselines for domains 2 and 3 for all corpora and performed somewhat worse for domain 1. The vertices in domain 1 tended to have multiple classes be-cause domain 1 is the set of vertices with high degrees. In other words, many vertices in domain 1 tended to be categorized into the same class when the output of the proposed algorithm was used because it is a soft clustering algorithm. We solved this problem by introducing link weighting ( Proposed-f). Although Proposed-f seats customers ordering the same dish individually to tables, the customers sit in the almost same table in our observation. It means that one menu on a particular table had large influence on the selec-tion of the dish. It reduced the number of classes assigned to each link, resulting in improvement of accuracy in domain-1. One in-explicable finding is the lower performance of Proposed-f for verb clustering for the LISA corpus. In this clustering, the estimated number of classes with Proposed-f was smaller than with Proposed. This would explain the lower performance of Proposed.
 Finally, we turn to discount parameter $ , which was described in Section 4.3. Estimating $ helps us gain a better understanding of the obtained graph structures, especially the distribution of vertex degrees in a graph. We explained in Section 4.3 that if $ one, the distribution of vertex degrees in a graph becomes a power-law distribution.

For example, we estimated $ for a graph of nouns and $ for a graph of verbs in Reuters1. These mean that the distribution of vertex degrees in the graph for Reuters1 had a power-law distribution.

Moreover, we can calculate the probability that a new word (ver-tex) is selected using Eq. (29). For example, in Reuters1, the prob-ability for the new selection of a noun was 0.1333; in contrast, the probability fo r the new selection of a verb was 0.0823. This means that it is easy for a new noun to be selected (generated) compared with a new verb. This is a reasonable result given that new nouns are generally generated more frequently than verbs because nouns tend to indicate concrete entities and verbs tend to indicate more ab-stract concepts, and new concrete entities appear more frequently than new abstract concepts.
Finally, we present several examples of relationships between words extracted from the Reuters corpus. Table 2 shows examples of extracted relationships and their linking vertices with respect to estimated classes.

For example, for the link to  X  X and X  relationship, the observed vertices linking to  X  X and X  are seven words. Each word is catego-rized into one of two classes, 25 or 32. Class 25 indicates  X  X hysi-cally landing things X , and Class 32 indicates  X  X onceptually landing things X . The words  X  X light X ,  X  ball  X  ,  X  X pacecraft X , and  X  X irport X  are all related to  X  landing  X  . For the link to  X  X og X  relationship, the observed vertices linking to  X  X og X  are four words. The words in class 31,  X  X orry X  and  X  X ear X , have a similar meaning. Those in class 107,  X  X tock X  and  X  X erformance X , are seemingly unrelated. However, when they are used with  X  dog  X  ( X  X tock dog X  and  X  X er-formance dog X ),  X  X tock X  and  X  X erformance X  have meanings similar to  X  X orking X . 4
We investigated the syntactic structures found in sentences and used them as contextual information. In particular, we took the subject-predicate structure as a syntactic structure and constructed a disassortative graph structure. We also developed a probabilistic model for generating the graph structure. This model performed better than other graph clustering algorithms in terms of WordNet similarities. It can also be used in other domains where the data can be represented by a graph structure. Future work includes investi-gating such applications. This research was funded in part by a MEXT Grant-in-Aid for Scientific Research on Priority Areas  X  X -explosion X  in Japan. [1] D. Aldous. Exchangeab ility and related topics. Lecture Notes [2] Antoniak. Mixtures of dirichlet processes with applications [3] C. Biemann. A random text model for the generation of [4] B. Dorow and D. Widdows. Discovering corpus-specific [5] B. Dorow, D. Widdows, K. Ling, J.-P. Eckmann, D. Sergi, [6] Escobar and West. Bayesian density estimation and inference [7] Ferguson. A bayesian analysis of some nonparametric [8] D. Gfeller, J.-C. Chappelier, and P. D. L. Rios. Synonym [9] M. Hagiwara, Y. Ogawa, and K. Toyama. Selection of [10] C. Kemp, J. B. Tenenbaum, T. L. Griffiths, T. Yamada, and
The words  X  X tock X ,  X  X erformance X , and  X  X og X  do not have a subject-predicate structure. Their linking is probably due to a mis-take by the HPSG parser. [11] D. Lin. Automatic retrieval and clustering of similar words. [12] S. McDonald and M. Ramscar. Testing the distributional [13] M. E. J. Newman and E. A. Leicht. Mixture models and [14] J. Pitman and M. Yor. The two-parameter poisson-dirichlet [15] Sethuraman. A constructive definition of dirichlet priors. [16] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. [17] S. van Dongen. Graph clustering by flow simulation. PhD
AddCustomer( , ) adds a new customer using menu into restau-rant as follows.

RemoveCustomer( , ) removes a customer using menu from restaurant as follows.

The concentration parameters and of HDP can be estimated by auxiliary variable sampling [6, 16].

The is estimated by first sampling auxiliary variables * and 1 for each restaurant (vertex) given . where is the Bernoulli distribution. Next, is sampled given and . where is the gamma distribution, and and 2 are hyper parameters. and then sampling . where is the number of generated classes (kinds of menu), and
Discount parameter $ and concentration parameter % of PYP can be estimated by auxiliary variable sampling similar to that used by Escobar and West [6].

First, auxiliary variables 0 , * ,and 1 are sampled for each restaurant (vertex) .
Next, given the auxiliary variables, % and $ are sampled. % where , 2 , ,and 2 are hyper parameters. We set all hyper parameters to .

The posterior probability over class is calculated instead of Eq (22)-(25) as follows. We introduce that takes if customer eats dish in restaurant , and otherwise ; i.e., .
 The indicates that customer 0 eating dish in restaurant uses menu . where  X  means the number of customers eating dish on menu with customer 0 eating in restaurant removed from the calculation.
Table 2: Examples of extracted relationships between words
