 Topic models have been widely used for text analysis. Pre-vious topic models have enjoyed great success in mining the latent topic structure of text documents. With many ef-forts made on endowing the resulting document-topic distri-butions with different motivations, however, none of these models have paid any attention on the resulting topic-word distributions. Since topic-word distribution also plays an important role in the modeling performance, topic models which emphasize only the resulting document-topic repre-sentations but pay less attention to the topic-term distribu-tions are limited. In this paper, we propose the Orthogo-nalized Topic Model (OTM) which imposes an orthogonali-ty constraint on the topic-term distributions. We also pro-pose a novel model fitting algorithm based on the general-ized Expectation-Maximization algorithm and the Newthon-Raphson method. Quantitative evaluation of text classifi-cation demonstrates that OTM outperforms other baseline models and indicates the important role played by topic or-thogonalizing.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Indexing methods Probabilistic Text Modeling; Latent Semantic Analysis; Tex-t Classification
Topic modeling has attracted increasing attention and widely used for data analysis in many domains including text documents. By assuming that each document is mod-eled by a distribution over a certain number of topics, each of which is a distribution over words in the vocabulary, topic models can reveal the underlying latent semantic structure which facilitates further applications such as classification, clustering and retrieval. Traditional topic models, including Probabilistic Latent Semantic Analysis (PLSA) [7], Latent Dirichlet Allocation (LDA) [1] and many of their variations, most of which put certain constraints on the document-topic distributions based on specific considerations, have enjoyed impressive success in tasks [2, 9], etc.

Topic models usually contain two parts of parameters, which are documents-topic distributions and topic-word dis-tributions, and intuitively topic-word distributions also play an important role in the whole model. A model with bet-ter topic-word distributions can mine more diversified and reasonable topics, and therefore it may achieve better perfor-mance on many text mining tasks, like text classification and clustering. To the best of our knowledge, none of previous topic models pay any extra consideration on the topic-word distributions, such as orthogonalizing them, which is to be discussed in this paper.

Orthogonalizing techniques have been widely used in di-mension reduction models [6, 8]. So we hope adding the or-thogonality constraint to the topic models also can achieve performance improvement.

In this paper, we propose a new topic model, the Orthogo-nalized Topic Model (OTM), to focus on orthogonalizing the topic-word distributions. In order to address the importance of orthogonalized topics, we put a regularized factor measur-ing the degree of topic orthogonalities to the objective func-tion of PLSA. The OTM model is able to take advantage of statistical foundation of PLSA without losing orthogonal property of LSA. We present an efficient algorithm to solve the proposed regularized log-likelihood maximization prob-lem using the Expectation-Maximization(EM) algorithm [5] and the Newton-Raphson method.

To evaluate the proposed model, we apply it to two widely used text corpora on the classification task. The higher text classification accuracy based on OTM further stresses the important role played by topic orthogonalization.
Latent Semantic Analysis (LSA) [4], as one of the most useful tools for learning the latent concepts from text, has widely been used in the dimension reduction task. Specifi-cally, given a term-document matrix X 2 R N  X  M , where N is the number of words in the vocabulary and M is the number of documents in the corpus, by using singular value decom-positi on (SVD), LSA tries to find a low-rank construction of X such that where 2 R K  X  K is a diagonal matrix containing the K largest singular values of X . Orthogonal matrices U 2 R N  X  K ( U T U = I ) and V 2 R M  X  K ( V T V = I ) represent word and document embeddings onto the latent space.
Probabilistic Latent Semantic Analysis (PLSA) [7] defines a proper generative model based on a solid statistical foun-dation.
 Given a corpus that consists of M documents f d 1 ;d 2 ;:::;d with words from a vocabulary of N words f w 1 ;w 2 ;:::;w PLSA, by assuming that the occurrence of a word w in a particular document d is assigned with one of K latent top-ic variables f z 1 ;z 2 ;:::;z K g , defines the following generative process:
By summing all the latent variable z k , the joint probabil-ity of an observed pair ( d i ;w j ) can be computed as P ( d i ;w j ) = P ( d i ) P ( w j j d i ) = P ( d i ) So we can calculate the data log-likelihood as L = where n ( d i ;w j ) denotes the number of times word w j in document d i . Following the principles of maximum like-lihood estimation, we can estimate P ( w j j z k ) and P ( z
According to the above modeling, PLSA successfully en-dows its resulting document-topic and topic-word assign-ments a statistical meaning which LSA lacks; however, it discards the orthogonal properties of the resulting dimen-sions originally possessed by LSA.
In this section, we firstly introduce the measure to evalu-ate the degree to which topics are orthogonal to each other and then formalize our proposed model, named Orthogo-nalized Topic Model (OTM). We also present an efficient algorithm to solve the proposed optimization function using the Expectation Maximization (EM) algorithm [5].
In order to measure the degree to which topics are orthog-onal to each other without harming their probabilistic prop-erty, the orthogonal constraint in LSA which ensures that each column vector of the resulting dimension are strictly orthogonal to other column vectors, such as U T U = I and V
T V = I , can no longer be used. Instead, in this paper, we put a soft orthogonality constraint on the topic-word distri-butions in order to force topics as orthogonal to each other as possible. The constraint we proposed is to minimize the following orthogonalization factor: Minimizing this objective function will directly enforce the topic-word distributions of different topics to be orthogonal. One can conclude from the above factor that as O decreases to 0 all the topics are more likely to be orthogonal with other topics.
Preserving the statistical properties of the resulting topic-word distributions while ensuring that the resulting topic-word distributions are as orthogonal to others as possible, we formulate OTM as combining the statistical foundation of PLSA and the orthogonalized constraint mentioned above.
Formally, OTM tries to maximize the following objective function: where 0 is the regularization parameter. Note that if = 0 OTM boils down to PLSA and we assume &gt; 0 thereafter. When a model involves unobserved latent variables, the EM algorithm is the standard procedure for maximum like-lihood estimation. EM alternates between two steps: (a) an Expectation(E) step where posterior probabilities of the latent variables are computed given the current estimates of the parameters, (b) a maximization(M) step where parame-ters are updated by maximizing the expected complete data log-likelihood given the posterior probabilities computed in the E-step. The parameters of OTM are P ( z k j d i ) ;P ( w Thus, MK + NK parameters are to be estimated, which is the same as PLSA.

E-step: The E-step of OTM is exactly the same as that of PLSA. The posterior probabilities for the latent variables P ( z k j d i ;w j ) can be computed using applying Bayes X  formu-la.

M-st ep: In the M-step of OTM, we improve the expected value of the complete data log-likelihood which is with the constraints  X 
The M-ste p re-estimation for P ( z k j d i ) is exactly the same as those of PLSA because the orthogonalized term of OTM does not include P ( z k j d i ).

No w we turn to the re-estimation part for P ( w j j z k ). Maxi-mization of Q with respect to P ( w j j z k ) leads to the following set of equations: = = 0 where k 0 is the Lagrange multiplier for topic k . So we get Let ting Due to the coupling between the summation in the denomi-nator and the summation outside the fraction, it is not easy to give an analytic solution for the P ( w j j z k ) in Eq.(10). Thus we turn to find a numerical solution. We apply Newton-Raphson method to calculate k from (11) as below: wh ere k is the value of k in the n th iteration. We initialize k with a random real number 0 k and get the root of the function f k , or the numerical solution of k by updating the value of k iteratively until a sufficiently accurate val-ue is reached. By noting that k ,  X  it is easy to prove that the left hand side of Eq.(11) is a monotone function for the parameter k . This indicates that there is only one accurate numerical solution of k , with nothing to do with the initial number 0 k . By using continues to calculate the EM step cycles, until the value of objective function (5) converges.
To evaluate the effectiveness of the proposed OTM model, we apply it on two widely used text corpora, Yahoo! News K-series and Reuters 21578 corpora. We will report the quantitative evaluation of document classification based on the topics extracted from various topic models.
Yahoo! News K-series is a collection consisting of 2,340 news articles belonging to 20 different categories. The num-bers of articles per categories are almost the same. We get the preprocessed version with a vocabulary size of 8104 from D. L. Boley X  X  page 1 . The Reuters-21578 dataset is a corpus of news stories made available by Reuters, Ltd. The prepro-cessed version is downloaded from R. F. Corr X  e a X  X  web page which consists of 9,821 documents and 5,180 distinct words. Table 1 shows the statistical details of the two datasets. All experiments are conducted on a computer with 2.6GHz Pentium Dual Core CPU and 2 GB physical memory.
We evaluate the performance of OTM on the tasks of doc-ument classification using the method similar to [9]. To ad-dress real-world problems in a semi-supervised setting, We first using OTM to generate the document-topic and word-topic distributions from the whole dataset, and randomly se-lect a small number of documents (one of 10,20,and 40) from each category as labeled for training and use the remaining documents for test. The selected training data number from each category are the same. Then a linear kernel support vector machine (SVM) [3] is trained on the document-topic representations of the training set and make prediction on the test set. This means that we use the rank-reduced rep-resentations of each document (the document-topic distri-butions) as the input space of SVM, and the output space is the categories of documents.

Beside the LSA and PLSA, we also provide two more base-lines:
LapPLSA is a topic model based on PLSA with the modi-fication on the document-topic distribution. It assumes that the document space is a manifold, either linear or nonlin-ear. By adding a term related to document-topic distribu-tion as regularization to the objective function of PLSA, this model achieves high performance on the text clustering task. We choose LapPLSA as a representation which add constraints on the resulting document-topic distribution to compare with our OTM which endows the resulting topic-words distribution. We use the source code downloaded from D. Cai X  X  webpage 3 .

For LSA, PLSA, LapPLSA, the dimension reduction re-sults of each document are set as the input for SVM, similar to OTM. For each model, we vary the number of latent top-ics K from 10 to 100. We report the average of classification accuracies after 10 test runs.

Figures 1 and 2 demonstrate the classification performance of OTM and other baseline models. On both text sets, OTM outperforms LSA, PLSA, LapPLSA in terms of classification accuracies due to the orthogonality of the topics. This indi-cates that the OTM model, which combines the statistical foundation of PLSA and the orthogonalized constraint, im-proves topic representation of documents to a certain degree. htt p://www-users.cs.umn.edu/~boley/ftp/PDDPdata/ https://sites.google.com/site/renatocorrea02/ http://www.zjucadcg.cn/dengcai/LapPLSA/index.html training" is per category. per category.
In this paper, we propose an Orthogonalized Topic Model (OTM) which aims to increase the resulting topic diversity by putting a constraint on topic-word distributions to or-thogonalize the topics. The consideration to constrain the topic-word distribution has not been made in previous work-s. We formulate the model as a maximization problem and fit the model efficiently using the EM algorithm and the Newton-Raphson method. We conduct experiments on two real world text corpora. The application of topic models to text classification verifies the effectiveness of the proposed model.

In future, we plan to add the orthogonality constraint into other topic modeling algorithms such as Latent Dirichlet Allocation(LDA) to improve their performance. In another direction, we will use some other measure to express the orthogonality and obtain faster optimization process. [1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [2] D. Cai, Q. Mei, J. Han, and C. Zhai. Modeling hidden [3] C. Cortes and V. Vapnik. Support-vector networks. In [4] S. Deerwester, S. Dumais, T. Landauer, G. Furnas, and [5] A. P. Dempster, N. M. Laird, and D. B. Rubin. [6] I. S. Dhillon and D. S. Modha. Concept decompositions [7] T. Hofmann. Probabilistic latent semantic indexing. In [8] P. O. Hoyer and P. Dayan. Non-negative matrix [9] S. Huh and S. E. Fienberg. Discriminative topic
