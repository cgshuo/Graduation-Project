 In addition to the news content, most web news pages also contain navigation panels, advertisements, related news links etc. These non-news items not only exist outsi de the news region, but are also present in the news content region. Effectively extracting the news content and filtering the noise have important effects on the follow-up activities of content management and analysis. Our extensive case studies have indicated that there exists potential relevance between web content lay outs and their tag paths. Based on this observation, we design two tag path features to measure the importance of nodes: T ext to tag P ath R atio (TPR) and E xtended T ext to tag P ath R atio (ETPR), and describe the calculation process of TPR by traversing the parsing tree of a web news page. In this paper, we pr esent Content Extraction via Path Ratios (CEPR) -a fast, accurate and general on-line method for distinguishing news content from non-news content by the TPR/ETPR histogram effectively. In order to improve the ability of CEPR in extracting short texts, we propose a Gaussian smoothing method weighted by a tag path edit distance. This approach can enhance the importance of internal-link nodes but ignore noise nodes existing in news content. Experimental results on the CleanEval datasets and we b news pages randomly selected from well-known websites show that CEPR can extract across multi-resources, multi-styles, and multi-languages. The average F and average score with CEPR is 8.69% and 14.25% higher than CETR, which demonstrates better web news extraction performance than most existing methods. H.3.3 [ Information Search and Retrieval ]: Information filtering; H.3.1 [ Content Analysis and Indexing ]: Abstracting methods Algorithms, Experimentation Content extraction, web news, text to tag path ratio, weighted Gaussian smoothing The Web has become a platform for content producing and consuming. According to Pew Internet &amp; American Life tracking surveys, reading news is one of the most popular behaviors of Internet users. An investigati on in November 2005 showed that over 46% Internet users read web news on a typical day 1 . Besides, corresponding web news sites to follow this trend. As reading web news is the fastest way to ac quire rich information, web news owns a huge user community. Ta ble 1 shows the top 5 popular news websites and their estimat ed numbers of unique monthly visitors as of May 1, 2013 2 . 
Table 1. The top 5 most popular news websites and their Popular News Website Unique Monthly Visitors Yahoo! News 110 million Google News 65 million New York Times 59.5 million Besides news content, a typical web news page contains title banners, advertisements, related links, copyrights and disclaimer notices. These additional non-news items, which are also known as noise, are responsible for roughly 40-50% of content on news websites [1]. Therefore, many web applications need to clean up news web pages. For instance, w ith highly-cleaned news inputted, the quality of web news summaries will be improved [2]; pocket-sized devices with small screens like mobile phones or PADs will achieve a better effect of user experience. In addition, the noise costs more data storage space and more computing time in web information retrieval, web conten t management and analysis, and reduces the quality of service at the same time. Our goal is to extract news content and filter non-news noise from web pages. This goal does not seem very complicated, but extracting news content from millions of websites without consistent news publication sta ndards is really a non-trivial problem. Massive and heterogeneous web news brings a data management challenge to handcrafted or rule-based learning techniques. These techniques are suitable for building wrappers for specific websites, but they usually become invalid in extracting news from massive and heterogeneous web pages in an open environment. In addition, most vision-based or template-based wrappers have failed to keep up with the changes of websites X  layouts or structures. It is difficult for traditional models http://www.pewinternet.org/~/me dia/Files/Reports/2005/PIP_Sea rchData_1105.pdf.pdf http://www.ebizmba.com/articles/ news-websites, May 1, 2013. and techniques to distinguish non-news items from content segments. Let X  X  take a news article from the New York Times as an example, which appeared on October 23, 2012, as shown in Figure 1. This is a typical web ne ws page. The content in the solid frame is web news, and the content in the dotted boxes is noise. Obviously, the banners, related li nks and advertisements occupy over one half of the page. Besides, we notice that the news in the solid frame also contains two non-news areas. In this paper, we propose a highly effective content extraction algorithm for distinguishing news content from non-news content effectively. Our news extraction technique is based on the potential relevance between web content layouts and their tag paths on the parsing trees, which is summarized as follows: on a typical web news page, news se gments commonly have similar tag paths and contain long text and more punctuations, whereas the noise items usually have sim ilar tag paths and contain brief text and less punctuations. By this intuition, we design two features based on the tag paths for measuring the importance of te xt nodes: a Text to tag Path Ratio (TPR) and an Extended Text to tag Path Ratio (ETPR). Once an HTML document is parsed into a DOM tree, we text to tag path ratio implies the node is more likely to represent a news segment. In case of a high value of link text, we extend the Text to tag Path Ratio to the Extended Text to tag Path Ratio by adding the punctuation informa tion and some statistical information. In order to improve the ability in extracting short text, a Gaussian smoothing method weighted by a tag path edit distance is designed. Empirical results will show that this is a fast, accurate and general content extraction approach that outperforms existing content extraction methods on large data sets. The difference between our approach and existing methods is that we do not assume there are any particular HTML tags, any specific structures, or any common structures between web news pages. We seek to make a full use of tag paths on the parsing trees of web pages to explore an accurate online web news extraction method. This paper makes the following main contributions: extraction feature based on tag paths; (2) methods for extracting news content from web pages: CEPR-TPR, CEPR-ETPR and CEPR-SETPR; (3) a Gaussian smoothing technique weighted by a tag path edit distance, which resolves shor t text extraction and non-news filtering problems effectively; and heterogeneous and dynami c web news extraction. The paper is organized as follows . After briefly reviewing related work in Section 2, we propose two different definitions for text to tag path ratios in Section 3. Next, we introduce the content extraction threshold method CEPR based on the TPR feature and a Gaussian smoothing method weighted by a tag path edit distance in Section 4. Then, in Section 5, we describe our evaluation criteria and parameter settings, compare the performance of our method with CETR, a state-of-the-art method, and discuss the empirical results. Finally, conclusions and future work can be found in Section 6. The research on web content extraction could be traced back to the integration investigation of heterogeneous structured data and semi-structured data, and was also promoted by the Automatic Content Extraction (ACE) evaluation conference, which was organized by the National Institu te of Standards and Technology (NIST). The evaluation conference was held several times from 2000 to 2007 [3]. During the past years, with the development of search engines, news aggregation, web mining and web intelligence, many research efforts have been made on extracting titles, content or other attributes from web pages. Perhaps the most simplistic and straightforward approaches are seen in handcrafted W eb I nformation E xtraction (WIE) systems which were built by traditional programming languages like Java, Perl or specialized languages desi gned by users. Handcrafted WIE systems include Minerva [4], Web-OQL [5], W4F [6], XWRAP [7] etc. All of them require users to have sophisticated programming skills, be familiar w ith data sources and results X  output formats, and understand the meanings of data items and extraction rules. The biggest advantage of these methods is they can solve extraction problems in particular fields. Obviously, the disadvantages lie in the low au tomation, high construction cost, and poor extensibility. Furthermore, the changes in data sources, output formats or semantics of data items makes such systems in need of continuous manual maintenance. In view of the above problems of manual extraction systems, a growing trend is to promote the automation and accuracy of a WIE system. With the help of supervised, semi-supervised and unsupervised learning techniques, the extraction rules can be configured automatically. In addition, the extraction accuracy could be improved and the labeli ng costs or scales could be reduced at the same time. Supervised WIE systems like WHISK [8], DEByE [9], SoftMealy [10], SRV [11] and PPWIE [12, 2, 13], get extraction rules by labeling training documents manually or with tools. Such systems do not require users to have programming skills or well understand the extraction tasks. These systems can achieve high extraction accuracy, and have a good extensibility. While the disadvantage of these systems lies in the fact that the professional annotation requirement and intensive workload make general users disqualified for the extraction tasks. Semi-supervised WIE systems like IEPAD [14], OLERA [15] and Thresher [16], learn extraction rules by roughly annotating a training corpus or selecting target patterns. Compared to supervised WIE systems, these techniques somewhat reduce the annotating work. However, due to the embedded domain knowledge, these systems are mainly used for extracting data records, and the shortcomings lie in lacks of extensibility. Unsupervised WIE systems such as RoadRunner [17], EXALG [18], DeLa [19], DEPTA [20] and NET [21], do not ask for a training corpus. These systems assume that web pages are generated by templates firstly. Then, how to detect the original templates from those pages and how to get the embedded content are discussed further. The advantage of these systems is the light workload. But these systems turn out to be limited in practical applications because of the above assumption or the missing data. Besides, lots of wrappers need to be built for thousands of websites. Any update of the page s X  templates may result in the wrappers X  failure, which leads to high production and maintenance consumption. A different approach is vision ba sed algorithms [22, 23, 24] in which documents are segmented in to visual block trees. In the Vision Page Segmentation technique introduced by Cai et al. [22] , DOM trees also need to be built for computing visual features. Generally speaking, compared to other methods, vision-based methods occupy more computing resources. In 2007, Zheng et al. presented a similar method which treats each node in the DOM composite feature derived from a series of visual features to evaluate its importance. W ith machine learning methods, wrappers could be built for every news site. The problem is that the training sets used in these methods were gene rated by manual annotationa, which is a time-c onsuming process. Wang et al. introduced some novel visual feat ures and combined them with machine learning methods to set up a training model only based on one website X  X  training sets [ 24]. The experimental results tested on 12 websites have demonstrat ed that these visual features have good generalization performance. Current web information extr action techniques meet great challenges in an open environment. For example, in the CleanEval task [25], only a few pages are available from the same site. Assumptions about specific structures (such as, contains &lt;table&gt;), the adjacency of main content, prescient formats or sharing common templates, make current methods fail to extract in an open field. Thus, a more general approach is required urgently. There are some approaches that can extract news content real-time and online. Real-time means neither pretreatment nor precognition for page structures, and online means adapting any web pages perfectly. One of such methods is CCB [26], which describes the documents X  characters by CCV (Content Code Vector). Each character corres ponds to a symbol relying on its original attribute. Then content code ratios (CCR) were calculated from the CCV basis, and then were judged to be content meaningful or not. Another online method is CETR [27], which uses an HTML document X  X  tag ra tios for extracting news content from diverse web pages. This a pproach computes tag ratios on a line-by-line basis and clusters the resulting values into content and noise areas. As 1-dimensi onal histograms do not contain ordered sequence information, 2-dimensional tag ratios were constructed subsequently. Experime nts have shown that CETR is laconic and efficient, and can extract across multi-languages. However, the online methods me ntioned above do not pay much each web page. Many efforts have demonstrated that there are potential relationships between the tag path structure and the content of a web page. XWrap [7] introduced by Liu et al. in 2000 locates the extracted objects mainly relying on the tag paths. Without a specific learning algorithm or extraction rules, but requiring the user X  X  understanding of the HTML parsing trees, XWrap could extract meaningful content accurately. To solve the auto-acquisition problem of path patterns in the Content Extraction Model [2, 13], Wu et al. proposed an extraction algorithm -MPM based on the path patterns mining [12]. A visual labeling tool was designed to annot ate the training sets by adding (attribute, value) pairs in the nodes of the parsing trees, and a distinguishing path pattern mining method was proposed according to the path of positive nodes or negative nodes annotated before. The experimental results showed that if appropriate parameters are sel ected then MPM can achieve good performance. Vertex [28], a web information extraction system developed by Yahoo!, uses lear ned extraction rules based on XPath for extracting structured da ta records from template pages. This technique has already been used in practical applications and extracted above 250 million records from more than 200 websites. From Figure 1, it was found that the news segments on the web page are usually included in one part and have the same layout, while the noise often has many parts and each part is highly formatted. Analyzing that structure further, we find that news segments often have similar tag paths. Based on this observation, we describe DOM trees and tag path s first and then define a text to tag path ratio to distingui sh news nodes from noise nodes. The Document Object Model (DOM) is a standardized interface designed by W3C for accessing HTML and XML documents. Each HTML page corresponds to a DOM tree where tags are internal nodes and the detailed text is on leaf nodes. Thus we can use DOM trees to process HTML documents. E
XAMPLE 1. A brief segment of HT ML codes from Figure 1 . Example 1 shows a segment of HTML codes from Figure1, and Figure 2 shows the DOM tree of Example 1. For convenience of description, this paper adopts the labeled ordered tree model to formalize an HTML page. According to the extraction demands, we extend th e labeled ordered tree model, and meanwhile, ensure all operations could be operated on the DOM tree conveniently. If no confusion in the context, we treat the extended labeled ordered tree and the DOM tree as the same concept. Definition 1 (Extended Labeled Ordered Tree): L = { l ...} is a set of finite alphabetic elements, and each l i HTML language. The extended labeled ordered tree on L is defined as a 7-tuple expression T = ( V , E , v 0 this expression, V is a set of nodes, E is a set of edges, v is the sibling relationship set of nodes, and G = ( V , E , v ordered tree with v 0 as the root. Mapping l : V  X  L is a label of node v . The mapping c ( X ) in an extended labeled ordered tree could be defined in different ways based on the actual applications. In this paper, we define it as the text node. Definition 2 (Tag Path): Given an extended labeled ordered tree T , whose root node is v 0 .  X  v  X  V , v 0 v 1 ... v of tree T from v 0 to v k , parent( v i ) = v i-1 (1  X  i  X  k ), v l ( v 1 )... l ( v k ) is called the tag path of node v , denoted as path( v ). To extract content from a web page, we take advantage of typical text features related to the DOM tree to distinguish content segments from noise parts. Plenty of case studies have indicated that on a typical web news page: (1) Content nodes commonly ha ve similar tag paths; (2) Noise nodes also have similar tag paths; (3) Content nodes often contain more text; (4) Noise nodes often contain less text; and (5) All the text nodes are leaf nodes. Once an HTML document has been parsed to a tree T , the tag path of each text node (leaf node) can be figured out and the text feature, e.g. number of text characters, of each node also can be calculated. Then such statistical in formation can be integrated into the tag path feature. pathNum : the occurring times of a tag path in the tree T . txtNum : number of all characters in the node. Furthermore, we can compute the ratio of the number of characters to the occurring times per tag path. Now, the Text to tag Path Ratio (TPR) can be define d, as the basis of our method. the text length of v . Suppose p is a tag path of a tree T , and accNodes ( p )={ 1 } is the set of accessible nodes collocated for p . The occurring is )( paccNodes . Definition 3 (Text to Tag Path Ratio) : Suppose p is a tag path. txtNum to its pathNum . TPR ( p )= () TPR ( p ) is a measure of each tag path in the parsing tree of a web page, and it is also a measure of each node in the parsing tree, because each node corresponds to a unique tag path. It assigns high values for paths that contai n long text and low values for other paths. It is useful to de termine whether a node of a parsing tree is meaningful or not. Clearly, content nodes in a parsing tree will be assigned relatively high TPR values. Algorithm CalculateTPRs INPUT: a tree T .

OUTPUT: a tag path set P . 1: P  X   X  ; 2: CountTxtPathNums( root ( T ), P ); 3: for each pObj in P do 4: pObj . TPR  X  pObj . txtNum / pObj . pathNum ; 5: return P . Procedure CountTxtPathNums( v , P ) Parameters: a node v in the tree T , a tag path set P .
 Function: Computing the characters of each text node and the number of paths in tree T rooted on v. 1: if ( v is a text node) { 2: Get a pObj from P with the key path ( v ); 3: if ( pObj == null) { 4: Create a tag path object pObj ; 5: pObj . path  X  path ( v ); 6: pObj . txtNum  X  length ( c ( v )); 7: pObj . pathNum  X  1; 8: P  X  P  X  { pObj }; 9: } else { 10: pObj . txtNum  X  pObj . txtNum + length ( c ( v )); 11: p . pathNum  X  p . pathNum + 1; 12: } 13: } else { 14: for each v' in v . children do 15: CountTxtPathNums( v' , P ); 16: } Computing text to tag path ratios is a recursive task, as shown by Aalgorithm CalculateTPRs in Figure 3. BBC Episode Examines Its... The main activity of Algorithm CaluateTPRs is the traversal of a number of characters for each text node is stored, along with the number of tag paths and TPR values, it needs an auxiliary space of O ( m ), where m is the number of unique tag paths in T . Before performing the computation, script , comment and style tags are removed from the parsing tree because such information is not visible hence should not be included in the calculation. In addition, the entire non-text nodes are also ignored because they are not among our extraction objects. Based on the definition of TPR, Example 2 below shows the text to tag path ratio for each tag path of Example 1. E XAMPLE 2 . The text to tag path ratios for four text nodes in Example 1 are computed as follows. Figure 5 shows the TPR-histogram ordering by preorder traversal text nodes sharing a common TPR value are close to each other, which suggests that content nodes are gathering together and so are noise nodes. Intuitively, we can identify the high value portion as the web page X  X  news cont ent and the others as noise. 
Figure 5. TPR of each node from the web news in Figure 1 With further studies, we have found that most news content on web pages contains more punctua tions while noise parts don X  X . The example web page in Figure 1 supports this observation too. Besides, the content lengths and the numbers of punctuations in text nodes may differ vastly, they don X  X  change much in noise information per tag path is calculated as follows. puncNum : the number of punctuations in each text node. txtNumStd : the standard deviation of text lengths of nodes that share one common tag path. puncNumStd : the standard deviation of punctuation numbers of nodes that share one common tag path. According to these three elements of statistical information together with pathNum and txtNum , we now redefine the text to ratio defined before, we name it as an extended text to tag path ratio. Unless otherwise specified, we use text to tag path ratios to refer to these two definitions. Definition 4 (Extended Te xt to Tag Path Ratio) : Suppose p is a ratio (ETPR) is given as follows. ETPR ( p )= TPR ( p )* () where the first factor of Equation 2 is the measure of TPR ( p ), and the second factor of the equation is a measure of the proportion of average punctuations in p , which assigns high values to content parts and low values to noise parts. The third factor,  X  standard deviation of text lengths of nodes accessed by p . A webpage X  X  content segments often have a high standard deviation because the text lengths of content nodes often differ vastly. Conversely, noise segments have low values. The fourth factor,  X  , is the standard deviation of punctuation numbers of nodes and noise segments have low values. To sum up, we can expects that ETPR ( p ) assigns high values to content nodes and low values to noise nodes. Figure 6 shows the ETPR-histogram ordering by preorder traversal of the text nodes for the page in Figure 1. It is similar to Figure 5, the tag paths having a common ETPR value gather together, which indicates that content nodes gather together and noise nodes also gather together. Compared to Figure 5, ETPR is better suited for identifying web news because of the more obvious difference between secti ons. The difference between the two calculation methods is discu ssed in detail in Section 5. 
Figure 6. ETPR of each node from the web news in Figure 1 In this section we describe the technique designed to extract the news content with TPR-histograms. The main idea of this approach is to determine a threshold  X  that divides nodes into content and noise sections. The idea of content extraction based on the text to tag path ratio is straightforward. Suppose v is a text node in a DOM tree T . With a otherwise, v is a noise node. The problem then becomes a matter of finding the best value of  X  and a way to extract the content nodes completely. Section 5.2 shows the method to set  X  . After the ETPR-histogram is calcu lated, a smoothing pass is made on the histogram. Many important content nodes might be lost without smoothing. Different from other methods, short text nodes often have high values in our method because the nodes accessed by one tag path have a common value. Therefore, our method aims to smooth the text to tag path histogram. According to the analysis of example pages, it was found that specially formatted short texts which are meaningful like internal-links often come with lower values. W ithout smoothing, such important information might be lost. To solve the above problem, we propose a weighted Gaussian smoothing technique by making use the similarity of tag paths. Standard Gaussian smoothing algor ithms are generally applied to processing images or handling smoothing problems with continuous attributes, which do not suit for our purpose. Therefore the algorithm used in this approach was re-implemented as a discrete function operating in a single dimension. Equation 3 shows the construction of a Gaussian kernel k with a radius of r , giving a total window size of 2 r +1. Next, Equation 4 shows that k is normalized to form k  X  form a smoothed histogram H  X  as shown in Equation 5. Thereinto, In Equation 6, dist( p i , p j ) is an edit distance of strings p [29].  X  is a smoothing parameter and w ij is a value to measure the contribution of smoothing data. In Equation 6, along with the increase of  X  , the smoothing contribution of nodes with long edit distances will decrease. Because the edit distance of content nodes X  tag paths is commonly lowe r than that of noise nodes, w could improve the values of inte rnal links and short text nodes, and meanwhile reduce the values of noise nodes that are mixed with content nodes such as noise link nodes. distinguishing content segm ents from noise segments. H  X  has a lower variance because outlying peaks and valleys are smoothed. Similarly, the important short texts, such as internal-links, that may occupy a single low-ETPR node among many high-ETPR nodes, are smoothed to a higher value than the threshold. Therefore smoothing increases the cohesiveness within segments and also the differences between segments. Figure 7. Smoothed ETPR of each node from the web news in Combining threshold setting with Gaussian smoothing weighted by the tag path edit distance together, we give our content extraction algorithm with the TPR/ETPR basis in Figure 8. Algorithm CEPR INPUT: an HTML web page wp , a parameter  X  .
 OUTPUT: content. 1: parsing wp into a tree T ; 2: P  X  CalculateETPRs( T ); 3: H  X  GetHistogram( root ( T ), P ); 4: H  X  X  X  Smoothing( H ); 5:  X   X   X  X  ( H  X  ); 6: content  X  ""; 7: for i = 0 to H  X  .size -1 do 8: if ( H  X  [ i ]. TPR  X   X  ) 9: content  X  content + c ( H  X  [ i ]. node ); 10: output content . CEPR is implemented in Java, and uses JTidy and HTMLParser components for cleaning and parsi ng web pages. Incorrect tags, wrong HTML coding and other erro rs that exist on web pages often make HTMLParser fail to parse. We use JTidy to fix these errors and translate them into standard XHTML documents. After parsing a webpage into a DOM tree, we calculate the text to tag path ratios by ignoring empty text nodes. In the extraction process, we remove the HTML tags, redundant blanks and empty lines from the content. To distinguish the extraction performance of TPR and ETPR from smoothed ETPR, we name them as CEPR-TPR, CEPR-ETPR and CEPR-SETPR respectively, and th ree methods are collectively referred to as the CEPR method. In this section we describe experiments on real world data from various web sites to demonstrate the effectiveness of our methods. Data sets used in our experime nts and evaluation measures are described first. We then compare with the CETR method and discuss the experimental resu lts. The configuration of our experiments is as follows: Inte l(R) Core(TM) 2 Duo CPU E7500 @ 2.93GHZ, 2.93GHZ, 2GB RAM, Windows XP, and JDK 1.6 are chosen as the development platform. The data used in our experiment s was taken from one source: the web news from CETR [27], which c onsists of two kinds of data sets. CleanEval: The CleanEval project was started by the ACL X  X  SIGWAC and initially took place as a competition during the summer of 2007. Besides extracti ng content, it also asked for participants to mark the struct ure of the web pages, such as identifying lists, paragraphs and h eaders. In this paper, we only consider content extraction. The corpus includes four divisions: a development set and an evalua tion set in both English and Chinese languages which are all annotated manually. Many of these pages use various styles and structures. There are 723 English news items and 714 Chinese news items in this data set. News: This data set includes news from 7 different English news websites: Tribune, Freep, NY Post , Suntimes, Techweb, BBC and NYTimes. Each website contains 50 pages chosen randomly. Standard metrics are used to evaluate and compare the performance of different approaches . In this paper the extraction results and the evaluation sets are regarded as word sequences in in the extraction result, S e be the set of words/characters from e , l be the text in the gold standard, and S l words/characters from l in the extraction result. This paper uses two kinds of standard metrics to evaluate the quality of web news extraction as follows. (1) Precision, Recall and F-score The precision (P), recall (R), and F -score are defined as follows. F is a composite indicator to ev aluate the extraction performance. (2) Score based on the common subsequence The score of the extraction result is defined as follows. This scoring method is transformed from the metric used by the CleanEval competition, where LCS ( e , l ) is the longest common subsequence between text e and text l . In the CleanEval competition, the scoring method was based on the Levenshtein Distance from the output of an extraction algorithm to the gold standard text, while the Levenshtein distance is relatively expensive to compute, taking O (| e | X | l |) time. When the text size is large, the scoring method of CleanEval takes a long time to return results, or ev en an  X  X ut of memory X  error. Therefore, we implement the ev aluation function using Equation 8 instead. Equation 7 gives the evaluation criteria used in CETR, which is our comparison algorithm. The basis of Equation 7 is  X  X ommon words matching X . So Equation 7 is a metric at the word level and the evaluation results mainly depend on the length of text. Compared with the criteria in Equation 7, the score in Equation 8 at the subsequence level is an  X  X xtraction matching X . We use all criteria in both Equations 7 and 8 as standard metrics to evaluate and compare the performance of our approach and CETR. After one webpage is parsed into a tree, we can calculate a TPR or ETPR histogram easily. Content nodes are often with high values and noise nodes have low values. But how to make a good use of high values from low values? In other words, how to distinguish content nodes from noise nodes? In this paper, we use a threshold technique. Accordi ng to the above definitions of threshold  X  , there might be a corresponding increase of precision considered to be a key issue. Figure 9. Precision , Recall and F-score tradeoff for NY Post as Common threshold selection met hods include the middle value, the average value and the standa rd deviation. However, from ETPR histograms, we can figure out that the number of lower values in ETPR is far more than the higher ones. The middle value might be affected by lo w values, and the average value might be affected by high valu es, so we choose the standard threshold parameter. Figure 9 shows the tradeoff of precision , recall and F -score along with  X  when extracting the source of the NY Post corpora. We can see that with the  X  increasing, the recall exists noise in the gold sets provided by CETR. For the NY Post corpora, a good tradeoff might be  X  =0.8. It is hard to find a good experiments, the empirical values of  X  we set for CEPR-TPR, CEPR-ETPR and CEPR-SETPR are 1. 7, 0.7 and 0.8 respectively. The smoothing parameter  X  is used for adjusting the contribution of the smoothing data in nodes that are closed to the current node. From Equation 6, the larger  X  is, the lager negative smoothing too low, the smoothing effect of nodes with large tag path edit edit distances. When  X  is set as 0, 1, 2 and 3, the extracting performances of CEPR-SETPR in the Techweb corpora are shown in Figure 10, while the window size (2 r +1) is 3 and the threshold parameter  X  is 0.8. The results shown in Figure 10 demonstrate that the smoothed extractions outperform unsmoothed extraction. During the smoothing process, with the increase of  X  , the average P increases but average R decreases, and the highest average F (92.60%) is obtained when  X  is set as 1. 
Figure 10. The extraction performance of CEPR-SETPR for In our experiments, the empirical value of  X  for CEPR-SETPR is set as 1. Table 2 shows the results of the content extraction, with CEPR-TPR (  X  =1.7), CEPR-ETPR (  X  =0.7) and CEPR-SETPR ( 2r+ 1=3,  X  =1,  X  =0.8) when given the task of extracting contents from the CleanEval, NYTimes, BBC and other web news sites. The highest values for each data source are in bold. With the CEPR-TPR method we observe high recalls but low precisions in some corpora such as Freep, Techweb, and Reuters. In those corpora, there are lots of hyperlinks and comments, which have high TPR values. The CEPR-TPR identifies them as content nodes and extracts the co rresponding texts. The results explain that when the text lengths of noise nodes are close to that of content nodes, the noise segments affect the extraction performance of CETR-TPR. Table 2 clearly shows that CE PR-ETPR outperforms CEPR-TPR. The extraction precision and recall of CEPR-ETPR balance well on the broader corpora including Freep, Techweb and Reuters on which CEPR-TPR performs poorly. But this method achieves lower recalls compared to CEPR-TPR in many domains. This is because the addition of punctua tion information and standard deviations are not conductive to extract titles. In one word, CEPR-ETPR performs better than CEPR-TPR, but often misses the title extraction. Meanwhile, Table 2 indicates that CEPR-SETPR outperforms CEPR-TPR and CEPR-ETPR on many data sets. Compared to CEPR-ETPR, the extraction recall is improved. This is because the title nodes and the internal-link nodes, which are missed by CEPR-ETPR but often have important meanings, are extracted by the aid of the weighted Gaussian smoothing process. The results demonstrate the effectiveness of smoothing. In order to show the effectiveness of our methods, we compare the above performance with the C ETR approach in Table 2. The cluster number in CETR is 2, which performs better than other parameters. CleanEval-P R F S core CETR is a web extraction algorithm by computing tag ratios. It histogram and extracts the news content based on the clustering results. Due to the ordered sequence information not being considered in 1-dimensional tag ratio histograms, it applies a smoothing pass to the histograms a nd then constructs a 2D model to improve the performance. It is noteworthy that CETR is similar to our approach but ignores the structural information of web pages. As the parsing tree could re flect structural features, CEPR is more interpretative than CETR too. Table 2 presents the comparis on results between CEPR-TPR, CEPR-ETPR, CEPR-SETPR and CETR on each source. Clearly, no matter in precision , recall , F or score , CEPR-TPR, CEPR-ETPR and CEPR-SETPR all perform better than CEPR. CEPR-SETPR outperforms CETR by an average 8.69% of F and an average 14.25% of score , especially on Freep, Techweb and Reuters, on which the average F CEPR-SETPR achieved is higher than CETR by 10.94%, 9.41% and 40.84% respectively. But on NYTimes, CEPR-SETPR performs poor ly. This is because the gold standard of NYTimes treats recommended links embedded in news content as content, but CEPR-SETPR counts them as noise, which results in a decreased recall . The empirical results above demonstr ate that CEPR is an effective and robust content extraction algorithm, which performs better than CETR. The experimental results in Table 2 show the significant difference between the precision and the recall that CEPR-TPR has achieved. For example, for Freep, Techweb and Reuters, the the fact that these pages contain many hyperlinks with long texts and the TPR values are higher than the threshold. Then they are be extracted by mistake. Analyzing the results in Table 2, we find that the precision CEPR-ETPR has achieved is higher than CEPR-TPR, but the recall is lower. The added statistical information like the number of punctuations enhances the differentiation between content segments and noise segments, and decreases the values of hyperlink nodes which CEPR-TPR could not handle. But the title information is missed at the same time. In a typical web page, there often exists only one title wh ich contains few punctuations. For example, the title path shown in Figure 2 is &lt;div.div.div.h1&gt;. This path only collects one node, w hose standard deviation of text lengths and punctuation num bers are both 0, thus  X  the ETPR value is 0, which means CEPR-ETPR does not extract the information, and then the recall is decreased. E
XAMPLE 3. The ETPR values for the f our nodes in Figure 2 are computed as follows . Comparing CEPR-ETPR with CEPR -SETPR, we find that CEPR-SETPR achieves both higher recall and precision . Based on the path similarity, the smoothing method extracts the content nodes with low ETPR values. For instance, Example 3 shows the ETPR value of each node presented in Figure 2. The ETPR values of node #2 and node #4 are both 0, which must be lower than the threshold, wh ile these nodes are content nodes. and take node #3 as the node be ing smoothed. Af ter smoothing, the ETPR value of node #3 is 207768.206, which has been improved significantly. In the same way, we can improve the ETPR value of title paths to enhance the recall score. Most web page extraction methods obtain a webpage X  X  title through extracting the &lt;title&gt; tag. However the content in the &lt;title&gt; tag might not really be the title displayed on the web page, which can contain several noise words. By contrast, the CEPR method has a significant advantage compared to other methods in title extraction. In addition, CEPR is a real-time and online method, which requires no training to be done, no models to be learned, and no assumptions on the structure of a webpage; all it requires is a web news page as input, and then cleaned content is returned. Furthermore, CEPR can also ex tract across web languages due to TPR, the basis of the method, concerning no characteristics of webpage languages. This paper presented a novel onlin e approach for extracting news content from web pages in an open environment using TPR features, based on the observati on that news nodes belonged to the same segment have similar path s. As noise segments have less punctuations and smaller text le ngths than news content, we extended the TPR feature to the ETPR feature, which is more effective than the former for extracting web news. In order to extract news content completely, we designed a Gaussian smoothing method weighted by a ta g path edit distance, which has Experimental results have shown that when compared to CETR, which is one of leading online content extraction methods, our CEPR method performs better on average. In addition to its effectiveness, the greatest strength of our CEPR method over other methods is the simplicity of its concept and implementation. CEPR depends only on the parsing tree without any assumption about web page layouts or structures, which makes it an effective domain-i ndependent and site-independent method. This approach is suita ble for handling web-scale news extraction because it is fully automatic and does not need any additional domain information othe r than the web news pages. Lastly, CEPR provides an accurate method that can extract across multi-resources, multi-styles, and multi-languages pages. The explosive growth of the Internet has produced a huge number of web new pages, and th e web news influence continues to grow. Thus, automatic news content extraction remains a popular technique for discovering useful in formation. With this in mind, there are several research issues to be further explored. feature, which is short of diversit y. There might exist other tag path features to be integrated for a be tter diversity of web news pages. Another area for further investig ation is the threshold technique used in CEPR for determining whether the content is meaningful or not. Selecting a best threshold appropr iate for all web news fields is really a tedious and unreasonable task. We do not claim that the threshold technique is optimal, and there may be other methods such as clustering that could bring better results. This work is supported by the National High Technology Research and Development Program of China (863 Program) under award 2012AA011005, the National 973 Program of China under award 2013CB329604, and the National Natural Science Foundation of China (NSFC) under awards 61273297, 61229301, 61273292 and 61202227. [1] Gibson, D., Punera, K. and To mkins, A. 2005. The volume and [2] Wu, X., Wu, G.-Q., Xie, F., Zhu, Z., Hu, X.-G., Lu, H., and Li, H. [3] Doddington, G., Mitchell, A., Przybocki, M., Ramshaw, L., [4] Crescenzi, V. and Mecca, G. 1998. Grammars have exceptions. [5] Arocena, G.O. and Mendelzon, A.O. 1998. WebOQL: [6] Sahuguet, A. and Azavant, F. 2001. Building intelligent web [7] Liu, L., Pu, C., and Han, W. 2000. XWRAP: An XML-enabled [8] Soderland, S. 1999. Learning info rmation extraction rules for semi-[9] Laender, A.H.F., Ribeiro-Neto, B ., and Silva, A.S. 2002. DEByE -[10] Hsu, C.N. and Dung, M.T. 1998. Generating finite-state [11] Freitag, D. 1998. Information ex traction from HTML: Application [12] Wu, G. and Wu, X. 2012. Extrac ting Web News Using Tag Path [13] Wu, X., Xie, F., Wu, G.-Q., a nd Ding, W. 2011. Personalized [14] Chang, C.H. and Lui, S.C. 2001. IEPAD: Information extraction [15] Chang, C.H. and Kuo, S.C. 2004. OLERA: A semi-supervised [16] Hogue, A. and Karger, D. 2005. Thresher: automating the [17] Crescenzi, V., Mecca, G., and Me rialdo, P. 2001. RoadRunner: [18] Arasu, A. and Garcia-Molina, H. 2003. Extracting structured data [19] Wang, J. and Lochovsky, F.H. 2003. Data extraction and label [20] Zhai, Y. and Liu, B. 2005. Web data extraction based on partial [21] Liu, B. and Zhai, Y. 2005. NET -A system for extracting web data [22] Cai, D., He, X., Wen, J.R., and Ma, W.Y. 2004. Block-level link [23] Zheng, S., Song, R., and Wen, J.R. 2007. Template-independent [24] Wang, J., Chen, C., Wang, C., Pei, J., Bu, J., Guan, Z., and Zhang, [25] Baroni, M., Chantree, F., Kilgarriff, A., and Sharoff, S. 2008. [26] Gottron, T. 2008. Content code blurring: a new approach to [27] Weninger, T., Hsu, W.H., and Han, J. 2010. CETR: content [28] Gulhane, P., Madaan, A., Mehta, R., Ramamirtham, J., Rastogi, R., [29] Levenshtein V.I. 1966. Binary codes capable of correcting 
