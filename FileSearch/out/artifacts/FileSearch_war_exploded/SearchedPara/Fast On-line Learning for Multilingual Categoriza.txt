 Multiview learning has been shown to be a natural and effi-cient framework for supervised or semi-supervised learning of multilingual document categorizers. The state-of-the-art co-regularization approach relies on alternate minimizations of a combination of language-specific categorization errors and a disagreement between the outputs of the monolin-gual text categorizers. This is typically solved by repeatedly training categorizers on each language with the appropriate regularizer. We extend and improve this approach by intro-ducing an on-line learning scheme, where language-specific updates are interleaved in order to iteratively optimize the global cost in one pass. Our experimental results show that this produces similar performance as the batch approach, at a fraction of the computational cost.
 I.2.7 [ Artificial Intelligence ]: Natural Lang. Processing Multilingual text categorisation, on-line learning
Large annotated multilingual corpora are massively pro-duced by many national or supra-national initiatives and are now available for various purposes, e.g. machine translation [4], semantic web [6] or classification [1]. For the latter, multiview learning was shown to be a natural and efficient framework for supervised or semi-supervised learning of mul-tilingual document categorizers [2].

We propose a new online learning algorithm for multi-lingual document categorization that is as efficient as, and much faster than, state-of-the-art multilingual categoriza-tion algorithms. Our approach operates by learning two language-specific categorizers, iteratively adjusting their pa-rameters on the basis of the prediction error on each lan-guage and the disagreement between the predictions on ei-ther language. The main difference with previous co-clas-sification work is that we leverage the structure of percep-tron updates in order to learn both categorizers simulta-neously instead of alternatingly. Experiments carried out on Reuters RCV1/RCV2 multilingual documents show that our approach performs at least as well as state-of-the art strategies, while being consistently and significantly faster.
We consider the representation of a bilingual document as a pair of two vectors x def = ( x 1 ,x 2 ), where each vector provides the representation of the same document in a given vectorial space X v ,v  X  X  1 , 2 } associated to each language. Our goal is to learn languages-specific categorizers h 1 ( and h 2 ( x 2 ) that minimize prediction error over new docu-ments in either language. Original work [1] solved this by minimizing a global loss on a training set S = { ( x i ,y using either logistic regression or Boosting. These are opti-mized in turn on each language by minimizing the misclas-sification loss for that language and the disagreement, and alternating between languages until convergence[1, 2].
When many examples are available, online learning has been shown to provide an efficient way to train models. In-stead of optimizing the model over the entire training set, online learning randomly picks one example and adjusts the model based on that example alone. Although it doesn X  X  directly optimize the full cost function, it can handle large datasets very efficiently[3].

We consider two linear binary categorizers with param-eters w v  X  X  v ,v  X  X  1 , 2 } associated to each of the two languages. Each may be trained independently by on-line learning in the co-classification framework outlined above. However, we propose to jointly train both models in the same stochastic online learning process. At each iteration t , a multilingual document ( x t ,y t ) is picked randomly from the training set and presented to both model. If it is mis-is updated using the following rule: where w v stands for the categorizer on the other language, and  X  is the learning rate, while  X  weighs the influence of the disagreement term (part of D in eq. 1). Following [1], we set the disagreement function D to the Kullback-Leibler diver-gence between the outputs of the categorizers on each view, h v and h v , mapped to [0; 1] with a sigmoid transformation (  X  ( x )=1 / (1 + e  X  x )). We then get: The algorithm is summarized as follows:
Algorithm: Online co-classification repeat until Convergence of global loss ;
We illustrate our method on data from a large extract of the RCV1 corpus [5], processed and made freely available for multiview multilingual learning experiments [2]. 1
We used the entirety of the English and French documents (
N = 111 , 740 documents), comprised of both originals and machine-translated Reuters newswire stories covering 6 cat-egories: C15, CCAT, E21, ECAT, GCAT and M21. In these experiments, we randomly sampled 20 different training sets of 10,000 documents from the full corpus, each with a cor-responding (non-overlapping) test set of 90,000 documents. All results are averaged over these 20 samples. For each cat-egory, we measured the performance using the F-score, and the training time (in seconds), excluding data load.
Table 1 shows the experimental results obtained using the state-of-the-art batch algorithm as well as using online ap-proaches with interleaved updates. As observed in the orig-inal work of [1], the performance achieved by the English and French categorizers are very similar, within one point in F-score. This is due to the fact that we minimize the dis-agreement between the categorizers, therefore biasing them to provide the same categorization (and therefore reach sim-ilar scores) in either languages. We see also see that the F-scores are very close for both algorithms, apart from ECAT where the online version does about 1 point better. This is expected as both algorithms learn similar, linear mod-els minimizing the same cost using similar updates. We do not expect large differences in performance, we expect large differences in training time [3]. Here, the online approach provides a very clear and consistent speedup (rightmost col-umn). It is about three times faster, completing training within 11 to 27 seconds depending on the category, while the original batch approach requires 34-71 seconds.
In a typical scenario, convergence is achieved by the on-line approach (at the chosen convergence threshold) using 7 to 10 epochs over the entire training set. By contrast, the batch algorithm performed as little as 2 (but usually more) alternate otimizations of the models in each view, each com-posed of several epochs on the training set. This therefore multiplies the overall training time. The following figure shows that the speedup of the online vs. batch algorithm actually increases as the training set size increases. http://multilingreuters.iit.nrc.ca Table 1: Online vs. batch results (F-score and time). Cat. batch online batch online bat onl  X  C15 79.9 79.8 78.8 78.7 51 17 3.0 CCAT 70.0 70.0 69.1 69.0 65 27 2.4 E21 72.8 72.7 71.9 72.0 56 17 3.3 ECAT 68.7 69.6 67.8 68.9 71 27 2.7 GCAT 78.1 78.1 77.1 77.0 53 17 3.2
M11 88.5 88.3 87.5 87.6 34 11 3.0
We propose a novel online algorithm that builds on the co-classification work of [1], but leverages the structure of online perceptron learning in order to simultaneously update the models on both views at each example presentation, yield-ing a clear and consistent speedup while providing similar performance.

This provides a natural way to learn multiple categorizers in a multiview learning framework, without the need to re-sort to alternating optimizations of regularized view-specific losses as in the original co-classification approach. In addi-tion, although we have not yet obtained evidence for that, we expect that it will scale up better to more than 2 lan-guages, because all models can be updated simulatneously at each iteration, instead of alternating view-specific opti-mizations. We expect that further work will shed light on the scalability to larger corpora and more languages. [1] M.-R. Amini and C. Goutte. A co-classification [2] M. R. Amini, C. Goutte, and N. Usunier. Combining [3] L. Bottou and Y. LeCun. Large scale online learning. In [4] A. Eisele and Y. Chen. MultiUN: A multilingual corpus [5] D. D. Lewis, Y. Yang, T. Rose, and F. Li. A new [6] B. Pouliquen, R. Steinberger, and C. Ignat. Automatic
