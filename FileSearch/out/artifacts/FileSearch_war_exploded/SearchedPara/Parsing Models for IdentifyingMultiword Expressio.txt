 Stanford University Stanford University Stanford University
Multiword expressions lie at the syntax/semantics interface and have motivated alternative theories of syntax like Construction Grammar. Until now, however, syntactic analysis and multiword expression identification have been modeled separately in natural language process-ing. We develop two structured prediction models for joint parsing and multiword expression grammars, a formalism that can store larger syntactic fragments. Our experiments show that both models can identify multiword expressions with much higher accuracy than a state-of-the-art system based on word co-occurrence statistics.
 sions. Relative to English, they also have richer morphology, which induces lexical sparsity for the context-free parsing model. Morphological analyses are automatically transformed into rich feature tags that are scored jointly with lexical items. This technique, which we call a factored lexicon, improves both standard parsing and multiword expression identification accuracy. 1. Introduction
Multiword expressions are groups of words which, taken together, can have un-predictable semantics. For example, the expression part of speech refers not to some altered in some ways X  part of speeches , part of speaking , type of speech  X  X hen the idiomatic meaning is lost. Other modifications, however, are permitted, as in the plural parts of speech . These characteristics make multiword expressions (MWEs) difficult to knowledge has been shown to improve task accuracy for a range of NLP applications including dependency parsing (Nivre and Nilsson 2004), supertagging (Blunsom and
Baldwin 2006), sentence generation (Hogan et al. 2007), machine translation (Carpuat and Diab 2010), and shallow parsing (Korkontzelos and Manandhar 2010).
 is then trained to render a MWE/non-MWE decision. All entries into the 2008 MWE Shared Task (Evert 2008) utilized variants of this technique.
 pose that a corpus contains more occurrences of part of speech than parts of speech . Surface statistics may erroneously predict that only the former is an MWE and the latter is not.
More worrisome is that the statistics for the two n -grams are separate, thus missing an obvious generalization.
 over arbitrary-length multiword expressions. This approach has not been previously demonstrated. To show its effectiveness, we build two parsing models for MWE iden-tification. The first model is based on a context-free grammar (CFG) with manual rule refinements (Klein and Manning 2003). This parser also includes a novel lexical model X  X he factored lexicon  X  X hat incorporates morphological features. The second model is based on tree substitution grammar (TSG), a formalism with greater strong generative capacity that can store larger structural tree fragments, some of which are lexicalized.
  X  X rabic X ) and French, two morphologically rich languages (MRLs). The lexical sparsity (in finite corpora) induced by rich morphology poses a particular challenge for n -gram classification. Relative to English, French has a richer array of morphological features X  such as grammatical gender and verbal conjugation for aspect and voice. Arabic also has richer morphology including gender and dual number. It has pervasive verb-initial matrix clauses, although preposed subjects are also possible. For languages like these it is well known that constituency parsing models designed for English often do not generalize well. Therefore, we focus on the interplay among language, annotation choices, and parsing model design for each language (Levy and Manning 2003; K X bler 2005, inter alia), although our methods are ultimately very general.
 stituents or even contiguous strings (O X  X rady 1998). Utterances such as All hell seemed to break loose and The cat got Mary X  X  tongue are clearly idiomatic, yet the idiomatic elements are discontiguous. Our models cannot identify these MWEs, but then again, neither can n -gram classification. Nonetheless, many common MWE types like nominal compounds are contiguous and often correspond to constituent boundaries.

The idiomatic meaning  X  X yntactic category X  does not derive from any of the component 196 words. This non-compositionality affects the syntactic environment of the compound as shown by the addition of an attributive adjective: (1) a. Noun is a part of speech. (2) a. Liquidity is a part of growth. In Example (1a) the copula predicate part of speech as a whole describes Noun .In
Examples (1b) and (1c) big clearly modifies only part and the idiomatic meaning is lost. The attributive adjective cannot probe arbitrarily into the non-compositional com-pound. In contrast, Example (2) contains parallel data without idiomatic semantics.
The conventional syntactic analysis of Example (2a) is identical to that of Example (1a) except for the lexical items, yet part of growth is not idiomatic. Consequently, many pre-modifiers are appropriate for part , which is semantically vacuous. In Example (2b), big clearly modifies part ,and of growth is just an optional PP complement, as shown by Example (2c), which is still grammatical.
 (2a). Figure 1a shows a Penn Treebank (PTB) (Marcus, Marcinkiewicz, and Santorini 1993) parse of Example (1a), and Figure 1b shows the parse of a paraphrase. The phrasal compound part of speech functions syntactically like a single-word nominal like category , and indeed Noun is a big category is grammatical. Single-word para-phrasability is a common, though not mandatory, characteristic of MWEs (Baldwin flat. This representation explicitly models the idiomatic semantics of the compound cation becomes a by-product of parsing as we can trivially extract MWE spans from full parses.
 representation. With this representation, the TSG model yields the best MWE iden-even though its parsing results lag state-of-the-art probabilistic CFG (PCFG)-based tored lexicon model with gold morphological annotations achieves the best MWE results for French (87.3% F1) and competitive results for Arabic (78.2% F1). For both languages the factored lexicon model also approaches state-of-the-art basic parsing accuracy.
 types in Arabic and French (Section 2). We then describe two constituency parsing supervised and can be trained on existing linguistic resources (Section 5). We evaluate the models for both basic parsing and MWE identification (Section 6). Finally, we compare our results with a state-of-the-art n -gram classification system (Section 7) and to prior work (Section 8). 2. Multiword Expressions in Arabic and French
In this section we provide a general definition and taxonomy of MWEs. Then we discuss types of MWEs in Arabic and French. 2.1 Definition of Multiword Expressions
MWEs, a known nuisance for both linguistics and NLP, blur the lines between syntax and semantics. Jackendoff (1997, page 156) comments that MWEs  X  X re hardly a marginal part of our use of language, X  and estimates that a native speaker knows at least as many
MWEs as single words. A linguistically adequate representation for MWEs remains an active area of research, however. Baldwin and Kim (2010) define MWEs as follows: Definition 1
Multiword expressions are lexical items that: (a) can be decomposed into multi-ple lexemes; and (b) display lexical, syntactic, semantic, pragmatic, and/or statistical idiomaticity.
 198
MWEs fall into four broad categories (Sag et al. 2002): idiomaticity across all four MWE classes. However, to our knowledge, we are the first to explicitly tune parsers for MWE identification. 2.2 Arabic MWEs
The most recent and most relevant work on Arabic MWEs was by Ashraf (2012), who analyzed an 83-million-word Arabic corpus. He developed an empirical taxonomy of by the projection of the purported syntactic head of the MWE. MWEs are further subcategorized by observed POS sequences. For some of these classes, the syntactic distinctions are debatable. For example, in the verb-object idiom
Daraba c Sfuurayn bi-Hajar ( X  X e killed two birds with one stone X ) verb ( X  X e killed X ), yet Ashraf (2012) classifies the phrase as a verbal idiom.
Nominal idioms (MWN) consist of proper nouns (Example 3a), noun compounds (Example 3b), and construct NPs (Example 3c). MWNs typically correspond to NP bracketings: (3) a. N N:
Prepositional idioms (MWP) include PPs that are commonly used as discourse con-nectives (Example 4a), function like adverbials in English (Example 4b), or have been institutionalized (Example 4c). These MWEs are distinguished by a prepositional syntactic head: (4) a. P D+N:
Adjectival idioms (MWA) are typically the so-called  X  X alse X  iDaafa constructs in which the first term is an adjective that acts as a modifier of some other noun. These constructs often correspond to a hyphenated modifier in English such as Examples (5a) and (5b). Less frequent are coordinated adjectives that have been institutionalized such as
Examples (5c) and (5d): (5) a. A D+N:
These idiom types usually do not cross constituent boundaries, so constituency parsers are well suited for modeling them. The other three classes of Ashraf (2012) X  X erb-subject, verbal, and adverbial X  X end to cross constituent boundaries, so they are dif-ficult to represent in a PTB-style treebank. Dependency representations may be more appropriate for these idiom classes. 2.3 French MWEs
In French, there is a lexicographic tradition of compiling MWE lists. For example, Gross (1986) shows that whereas French dictionaries contain about 1,500 single-word adverbs there are over 5,000 multiword adverbs. MWEs occur in every part of speech (POS) ( X  X holly X )).
 linguistic theory known as Lexicon-Grammar. In this theory, MWEs are classified terms of the sequence of the POS tags of the words that constitute the MWE (e.g.,  X  X  ready X )) (Gross 1986). In other words, MWEs are represented by a flat structure. The
Lexicon-Grammar distinguishes between units that are fixed and have to appear as is variation such as admitting the insertion of an adverb or adjective, or the variation of 200 one of the words in the expression (e.g., a possessive as in from the top of one X  X  hat ). It also notes whether the MWE displays some selectional preferences (e.g., it has to be preceded by a verb or by an adjective).

Nominal idioms (MWN) consist of proper nouns (Example (6a)), foreign common nouns (6b), and common nouns. The common nouns appear in several syntacti-cally regular sequences of POS tags (Example (7)). Multiword nouns allow inflection (singular vs. plural) but no insertion: (6) a. London Sunday Times, Los Angeles (7) a. N A: corps m X dical ( X  X edical staff X ), dette publique ( X  X ublic debt X )
Adjectival idioms (MWA) appear with different POS sequences (Example (8)). They include numbers like vingt et uni X me ( X 21st X ). Some MWAs allow internal variation. For example, some adverbs or adjectives can be added to both examples in (8b) (  X  tr X s haut risque , de toute derni X re minute ): (8) a. P N: d X  X ntan [from before] ( X  X ld X ), en question ( X  X nder discussion X )
Verbal idioms (MWV) allow number and tense inflections (Example (9)). Some MWVs containing a noun or an adjective allow the insertion of a modifier (e.g., donner grande venes between the main verb and its complement, the two parts of the MWV may be marked discontinuously (e.g., [ MWV [ V prennent]] [ ADV ( X  X lready take into account X )): (9) a. V N: avoir lieu ( X  X ake place X ), donner satisfaction ( X  X ive satisfaction X )
Both Gross (1986) and Ashraf (2012) classify MWEs according to global syntactic role and internal POS sequence. In a constituency tree, these two features can be modeled by a span over the MWE composed of a phrasal label indicating the MWE type and pre-terminal labels indicating the internal POS sequence. MWE identification then becomes a trivial process of extracting such subtrees from full parses. 3. Context-Free Parsing Model: Stanford Parser tuned for MWE identification. The algorithmic details of the parsing models may seem removed from multiword expressions, but this is by design. MWEs are encoded in representation rather than trying to model semantic phenomena directly.
 outputs of a manually refined PCFG with an arc-factored dependency parser. Adapting the Stanford parser to a new language requires: (1) feature engineering for the PCFG development of an unknown word model. 3 factored lexicon . The factored lexicon incorporates morphological information that is predicted by a separate morphological analyzer. 3.1 Grammar Development
Grammar features consist of manual splits of labels in the training data (e.g., marking base NPs with the rich label  X  X P-base X ). These features were tuned on a development splitting) have only empirical justification.

French Grammar Features. Table 2 lists the category splits used in our grammar. Most of the features are POS splits as many phrasal tag splits did not improve accuracy. This result may be due to the flat annotation scheme of the FTB. 202 complement (VPinf), but some prepositions will uniquely appear in one context and not the other (e.g., sur ( X  X n X ) will only occur in a PP environment). The tagPA provides this kind of distribution. We also split punctuation tags ( splitPUNC ) into equivalence classes similar to those present in the PTB.
 prepositions which introduce PPs modifying a noun (NP). Marking other kinds of prepositional modifiers (e.g., verb) did not help. The feature markDe the preposition de and its variants ( du , des , d X  ), which are very frequent and appear in many contexts. under S nodes ( MWADVtype1 ), and those with POS sequences that occur more than 500 times ( X  X  N X   X  en jeu ,  X peine ,or X  X DN X  dans l X  X mm X diat ,  X  l X  X nverse )( MWADVtype2 ).
Similarly, we mark MWNs that occur more than 600 times (e.g.,  X  X  P N X  and  X  X  N X ) ( MWNtype1 and MWNtype2 ).
 Arabic Grammar Features. The Arabic grammar features come from Green and additional feature, markMWEPOS , which marks POS tags dominated by MWE phrasal categories. 3.2 Head-Finding Rules
For Arabic, we use the head-finding rules from Green and Manning (2010). For French, we use the head-finding rules of Dybro-Johansen (2004), which yielded an approxi-mately 1% development set improvement over those of Arun (2004). 3.3 Unknown Word Models
For both languages, we create simple unknown word models that substitute word signatures for rare and unseen word types. The signatures are generated according to the features in Table 3. For tag t and signature s , the signature parameters p ( t estimated after collecting counts for 50% of the training data. Then p ( s via Bayes rule with a flat Dirichlet prior.
 3.4 Factored Lexicon with Morphological Features
We will apply our models to Arabic and French, yet we have not dealt with the lexical sparsity induced by rich morphology (see Table 5 for a comparison to English). One way to combat sparsity is to parse a factored representation of the terminals, where factors might be the word form, the lemma, or grammatical features such as gender, number, and person (  X  features) (Bilmes and Kirchoff 2003; Koehn and Hoang 2007, inter alia).
 tag p ( w | t ) from word/tag pairs observed in the training set. Additionally, the lexicon includes parameter estimates p ( t | s ) for unknown word signatures s produced by the unknown word models (see Section 3.3). At parsing time, the lexicon scores each input unsmoothed and smoothed parameter estimates:
We then compute the desired parameter p ( w | t )as We found that  X  = 1 . 0and  X  = 100 worked well on both development sets.
 which is a string describing various grammatical features (e.g., tense, voice, definite-ness). Instead of generating terminals alone, we generate the word and morphological analysis using a simple product: training data for each language, we tend to get sharper parameter estimates, namely, we usually estimate p ( t | m ) directly as in Equation (1). Moreover, at test time, even if the word type w is unknown, the associated morphological analysis m is almost always known, providing additional evidence for tagging.
 improve accuracy. We thus excluded the lemma factor from our experiments.
 clearly redundant. Consider, however, the case of the Arabic triliteral in unvocalized text, can be either a verb meaning  X  X e killed X  or a nominal meaning  X  X urder, killing. X  If zero probability because nominals never carry tense in the training data. 204 4. Fragment Parsing Model: Dirichlet Process Tree Substitution Grammars
For our task, a shortcoming of CFG-based grammars is that they do not explicitly capture idiomatic usage. For example, consider the two utterances: (10) a. He kicked the bucket. Unless horizontal markovization is applied, PCFGs generate words independently.
Consequently, no phrasal rule parameter in the model differentiates between Exam-ples (10a) and (10b). Recall, however, that in our representation, Example (10a) should receive a flat analysis as MWV, whereas Example (10b) should have a conventional analysis of the transitive verb kicked and its two arguments.
 (Joshi and Schabes 1997). TSGs can store lexicalized tree fragments as rules. Conse-quently, if we have seen [ MWV kicked the bucket ] several times before, we can store that whole lexicalized fragment in the grammar.

Goldwater, and Blunsom (2009) in which tree fragments are drawn from a Dirichlet process (DP) prior. 4 The DP-TSG can be viewed as a data-oriented parsing (DOP) model (Scha 1990; Bod 1992) with Bayesian parameter estimation. A PTSG is a 5-tuple
V ,  X  , R ,  X  ,  X  where c  X  V are non-terminals; t  X   X  are terminals; e trees ; 5  X  X  X  V is a unique start symbol; and  X  c , e fragment. A PTSG derivation is created by successively applying the substitution operator to the leftmost frontier node (denoted by c + (denoted by c  X  ).
 problem. We have a treebank T that we segment into the set R , a process that is modeled with Bayes X  rule:
Because the tree fragments completely specify each tree, p ( T work is performed by the prior over the set of elementary trees.
 rooted at non-terminal c according to:  X   X  n where x n = x ( x + 1) ... ( x + n  X  1) is the rising factorial.

Base Distribution. The base distribution P 0 is the same maximum likelihood PCFG used in the Stanford parser. 6 After applying the manual grammar features, we perform sim-ple right binarization, collapse unary rules, and replace rare words with their signatures (Petrov et al. 2006).

P , the probability of generating a tree fragment A +  X  terminals is Unlike Cohn, Goldwater, and Blunsom (2009), we penalize lexical insertion: where p ( t ) is equal to the MLE unigram probability of t in the treebank. Lexicalizing a rule makes it very specific, so we generally want to avoid lexicalization with rare words. Empirically, we found that this penalty reduces overfitting.
 Type-Based Inference Algorithm. To learn the parameters  X  we use the collapsed, block
Gibbs sampler of Liang, Jordan, and Klein (2010). We sample binary variables b associated with each sampling site s in the treebank. The key idea is to select a block 206 sites in S are exchangeable, we can set b S randomly if we know m , the number of sites reader to Liang, Jordan, and Klein (2010) for further details.
 binomial-Beta conjugacy. We also infer the DP concentration parameters  X  auxiliary variable procedure of West (1995).

Decoding. To decode, we first create a maximum a posterior (MAP) grammar in which tree fragments have fixed estimates according to a single sample from the DP-TSG:
This MAP grammar has an infinite rule set, however, because elementary trees with zero count in n have some residual probability under P 0 . We discard all zero-count trees except for the zero-count CFG rules in P 0 . Scores for these rules follow from Equation (9) inference using dynamic programming (Cohn, Goldwater, and Blunsom 2009).
 we can form a CFG of the derivation sequences and use a synchronous CFG to translate the most probable CFG parse to its TSG derivation. Consider a unique tree fragment e rooted at c j with frontier  X  , which is a sequence of terminals and non-terminals. We encode this fragment as an SCFG rule of the form where c k , c l , ... is a finite-length sequence of the non-terminal frontier nodes in  X  .
TSG substitution operator applies to the leftmost frontier node, the best TSG parse can be deterministically recovered from the sequence of indices. optimized SCFG decoders for machine translation. We use cdec (Dyer et al. 2010) to find the Viterbi derivation for each input string. 5. Training Data and Morphological Analyzers
We have described two supervised parsing models for Arabic and French. Now we show how to construct MWE-aware training resources for them.
 (Maamouri et al. 2004) and the French Treebank (FTB) (Abeill X , Cl X ment, and Kinyon 2003). Prior to parsing, both treebanks require significant pre-processing, which we perform automatically. 8 Because parsing evaluation metrics are sensitive to the terminal/non-terminal ratio (Rehbein and van Genabith 2007), we only remove non-terminal labels in the case of unary rewrites of the same category (e.g., NP the PTB. Appendix C compares the annotation consistency of the ATB, FTB, and WSJ. 5.1 Arabic Treebank
We work with parts 1 X 3 (newswire) of the ATB, 9 which contain documents from three different news agencies. In addition to phrase structure markers, each syntactic tree also contains per-token morphological analyses. 208 MWN noun 6,975 91.6% 9,680 49.7% MWP prep. 623 8.18% 3,526 18.1% MWA adj. 18 0.24% 324 1.66% MWPRO pron.  X   X  266 1.37% MWC conj.  X   X  814 4.18% MWADV adverb  X   X  3,852 19.8% MWV verb  X   X  585 3.01% MWD det.  X   X  328 1.69% MWCL clitic  X   X  59 0.30% MWET foreign  X   X  24 0.12% MWI interj.  X   X  4 0.02% Total 7,616 19,462 Tokenization/Segmentation. We retained the default ATB clitic segmentation scheme.
Morphological Analysis. The ATB contains gold per-token morphological analyses, but no lemmas.

Tag Sets. We used the POS tag set described by Kulick, Gabbard, and Marcus (2006). We previously showed that the  X  X ulick X  tag set is very effective for basic Arabic parsing (Green and Manning 2010).
 MWE Tagging. The ATB does not mark MWEs. Therefore, we merged an existing Arabic
MWE list (Attia et al. 2010b) with the constituency trees. list that was bracketed in the treebank, we flattened the structure over the MWE span and added a non-terminal label according to the MWE type (Table 6). We ignored MWE strings that crossed constituent boundaries.

Orthographic Normalization. Orthographic normalization has a significant impact on parsing accuracy. We remove all diacritics, instances of taTwiil , We also applied alif normalization 12 and mapped punctuation and numbers to their Latin equivalents.

Corpus Split. We divided the ATB into training/development/test sections according to the split prepared by Mona Diab for the 2005 Johns Hopkins workshop on parsing
Arabic dialects (Rambow et al. 2005). 13 5.2 French Treebank
The FTB 14 contains phrase structure trees with morphological analyses and lemmas. In addition, the FTB explicitly annotates MWEs. POS tags for MWEs are given not only at the MWE level, but also internally: Most tokens that constitute an MWE also have a POS tag. Our FTB pre-processing is largely consistent with Lexicon-Grammar, which defines MWE categories based on the global POS.

Tokenization/Segmentation. We changed the default tokenization for numbers by fusing adjacent digit tokens. For example, 500 000 is tagged as an MWE composed of two words 500 and 000 . We made this 500000 and removed the MWE POS. We also merged numbers like  X 17,9 X .

Morphological Analysis. The FTB provides both gold morphological analyses and lemmas for 86.6% of the tokens. The remaining tokens lack morphological analyses, and in many cases basic parts of speech. We restored the basic parts of speech by assigning each token its most frequent POS tag elsewhere in the treebank. 15 This technique was too coarse for missing morphological analyses, which we left empty.

Tag Sets. We transformed the raw POS tags to the CC tag set (Crabb X  and Candito 2008), which is now the standard tag set in the French parsing literature. The CC tag set includes WH markers and verbal mood information.

MWE Tagging. We added the 11 MWE labels shown in Table 6. We mark MWEs with prefix, and the preterminals are the internal POS tags for each terminal. The resulting
POS sequences are not always unique to MWEs: They appear in abundance elsewhere in the corpus. Some MWEs contain normally ungrammatical POS sequences, however (e.g., adverb  X lavavite ( X  X n a hurry X ): P D V ADV [at the goes quick]), and some words appear only as part of an MWE, such as insu in  X  l X  X nsu de ( X  X o the ignorance of X ). We also found that 36 MWE spans still lacked a global POS. To restore these labels, we assigned the most frequent label for that internal POS sequence elsewhere in the corpus.
Corpus Split. We used the 80/10/10 split described by Crabb X  and Candito (2008). They used a previous release of the treebank with 12,531 trees. Subsequently, 3,391 trees were added to the FTB. We appended these extra trees to the training set, thus preserving the original development and test sets. 5.3 Morphological Analysis for Arabic and French The factored lexicon requires predicted per-token morphological analyses at test time.
We used separately trained, language-specific tools to obtain these analyses (Table 7). 210 Arabic. The morphological analyses in the ATB are human-selected outputs of the
Standard Arabic Morphological Analyzer (SAMA), 16 a deterministic system that relies on manually compiled linguistic dictionaries. The latest version of SAMA has complete lexical coverage of the ATB, thus it does not encounter unseen word types at test time.
To rank the output of SAMA, we use MADA (Habash and Rambow 2005), makes predictions based on an ensemble of support vector machine (SVM) classifiers.
French. The FTB includes morphological analyses for gender, number, person, tense, type of pronouns (relative, reflexive, interrogative), type of adverbs (relative or inter-rogative), and type of nouns (proper vs. common noun). Morfette (Chrupala, Dinu, and van Genabith 2008) has been used in previous FTB parsing experiments (Candito and Seddah 2010; Seddah et al. 2010) to predict these features in addition to lemmas.
Morfette is a discriminative sequence classifier that relies on lexical and greedy left con-text features. Because Morfette lacks a morphological generator like SAMA, however, it is effectively a tagger that must predict a very large tag set. We trained Morfette on our split of the FTB and evaluated accuracy on the development set: 88.3% (full morpho-logical tagging); 95.0% (lemmatization); and 86.5% (full tagging and lemmatization). 6. Experiments
For each language, we ran two experiments: standard parsing and MWE identifica-tion. The evaluation included the Stanford, Stanford + factored lexicon, and DP-TSG models.
 does not contain the raw source documents, so we could not start from raw text for both languages. We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010).
 between the two languages. The morphological analyses were obtained with signifi-cantly different tools: in Arabic, we had a morphological generator/ranker (MADA), whereas for French we had only a discriminative classifier (Morfette). Consequently,
French analysis quality was lower (Section 5.3). 6.1 Standard Parsing Experiments
Baselines. We included two parsing baselines: a parent-annotated PCFG (PAPCFG) and a PCFG with the grammar features in the Stanford parser (SplitPCFG). The PAPCFG is the standard baseline for TSG models (Cohn, Goldwater, and Blunsom 2009).

Berkeley Parser. We previously showed optimal Berkeley parser (Petrov et al. 2006) pa-rameterizations for both the Arabic (Green and Manning 2010) and French (Green et al. 2011) data sets. 19 For Arabic, our pre-processing and parameter settings significantly increased the best-published Berkeley ATB baseline. Others had used the Berkeley parser for French, but on an older revision of the FTB. To our knowledge, we are the first to use the Berkeley parser for MWE identification.

Factored Lexicon Features. We selected features for the factored lexicon on the develop-ment sets. For Arabic, we used gender, number, tense, mood, and definiteness. For
French, we used the grammatical and syntactic features in the CC tag set in addition to grammatical number. For the experiments in which we evaluated with predicted morphological analyses, we also trained the parser on predicted analyses.

Evaluation Metrics. We report three evaluation metrics. Evalb is the standard labeled precision/recall metric. 20 Leaf Ancestor measures the cost of transforming guess trees banks like the FTB (Rehbein and van Genabith 2007). The Leaf Ancestor score ranges from 0 to 1 (higher is better). We report micro-averaged (Corpus) and macro-averaged (Sent.) scores. Finally, EX% is the percentage of perfectly parsed sentences according to Evalb.

Sentence Lengths. We report results for sentences of lengths accounts for similar proportions of the ATB and FTB. The DP-TSG grammar extractor produces very large grammars for Arabic, 21 and we found that the grammar constant was too large for parsing all sentences. For example, the ATB development set contains a sentence that is 268 tokens long. 212
Results. Tables 8 and 9 show Arabic and French parsing results, respectively. For both languages, the Berkeley parser produces the best results in terms of Evalb F1. The gold factored lexicon setting compares favorably in terms of exact match. 6.2 MWE Identification Experiments
The predominant approach to MWE identification is the combination of lexical associa-language-independent package that implements this approach for higher order n -grams is mwetoolkit (Ramisch, Villavicencio, and Boitet 2010). mwetoolkit Baseline. We configured mwetoolkit with the four standard lexical features: the maximum likelihood estimator, Dice X  X  coefficient, pointwise mutual information, and Student X  X  t -score. We also included POS tags predicted by the Stanford tagger (Toutanova et al. 2003). We filtered the training instances by removing unigrams and non-MWE n -grams that occurred only once. For each resulting n -gram, we created real-valued feature vectors and trained a binary SVM classifier with Weka (Hall et al. 2009) with an RBF kernel. See Appendix D for further configuration details.

Results. Because our parsers mark MWEs as labeled spans, MWE identification is a by-product of parsing. Our evaluation metric is category-level Evalb for the MWE non-a weighted average for all categories. Table 12 shows aggregate MWE identification results. All parsing models X  X ven the baselines X  X xceed mwetoolkit by a wide margin. 7. Discussion 7.1 MWE Identification Results
The main contribution of this article is Table 12, which summarizes MWE identification results. For both languages, our parsing models yield substantial improvements over the n -gram classification method represented by mwetoolkit . The best improvements come from different models: The DP-TSG model achieves 66.9% F1 absolute improve-ment for Arabic and the Stanford+FactLex* achieves 50.0% F1 absolute improvement for French.
 ences in the ordering of the models. The Arabic MWE list consists mainly of named entities and nominal compounds, hence the high concentration of MWN types in the 214  X  pre-processed ATB (Table 10). Consequently, this particular Arabic MWE identification experiment is similar to joint parsing and named entity recognition (NER) (Finkel and
Manning 2009). The DP-TSG is effective at memorizing the entities and re-using them at test time. It would be instructive to compare the DP-TSG to the discriminative model of Finkel and Manning (2009), which currently represents the state-of-the-art for joint parsing and NER.
 rules. One explanation for this result could be the CC tag set, which was explicitly tuned for the Berkeley parser. The CC tag set improved Berkeley MWE identification accuracy by 1.8% F1 and basic parsing accuracy by 1.2% F1 over the previous version of our work (Green et al. 2011), in which we used the basic FTB tag set. However, this tag set yielded only 0.2% F1 and 1.1% F1 improvements, respectively, for the DP-TSG.
 metries. For French, the extraordinary result with gold analyses (Stanford + FactLex*) is partly due to annotation errors. Gold morphological analyses are missing for many of the MWE tokens in the FTB. The factored lexicon thus learns that when a token has no morphology, it is usually part of an MWE. In the automatic setting (Stanford + FactLex), however, Morfette tends to assign morphology to the MWE tokens because it has no semantic knowledge. Consequently, the morphological predictions are less consistent, and the parsing model falls back to the baseline Stanford result. Certainly more consistent FTB annotations would help Morfette, which we found to be significantly less accurate on our version of the FTB than MADA on the ATB (see Habash and Rambow 2005). Another remedy would be to incorporate MWE knowledge into the lexical analyzer, a strategy that Constant, Sigogne, and Watrin (2012) recently found to be very effective.
 a 0.7% F1 improvement over Stanford along with a significant improvement in exact match (EX%). In the automatic setting, a 0.3% F1 improvement is maintained for MWE identification. One direction for improvement might be the POS tag set. The  X  X ulick X  tag set encodes some morphological information (e.g., number, definiteness), so the factored lexicon can be redundant. Eliminating this overlap might improve accuracy. 7.2 Interpretability of DP-TSG MWE Rules
Arabic. Table 13 lists a sample of the TSG rules learned by the DP-TSG model. Fixed in the grammar. The model also generalizes over nominal compounds with rules level , Soviet-made ). Memoryless binarization permits the grammar to capture rules like
MWA  X  A MWA, which permits generation of a previously unseen multiword ad-jectives. Some of these recursive rules are lexicalized, as in the multiword preposition rule MWP  X  MWP.
 French. We find that the DP-TSG model also learns useful generalizations over French MWEs. A sample of the rules is given in Table 14. Some specific sequences like  X  X 
MWEs, for example, coup de pied ( X  X ick X ), coup de coeur , coup de foudre ( X  X ove at first ( X  X ive no peace X ), perdre de vue [lose from sight] ( X  X orget X ), prendre de vitesse [take from also generalizes over very frequent sequences:  X  X n N de X  occurs in many multiword faveur de , en raison de , en fonction de ). The TSG grammar thus provides a categorization of MWEs consistent with the Lexicon-Grammar. It also learns verbal phrases which contain discontiguous MWVs due to the insertion of an adverb or negation such as  X  X  VN [ MWV va] [ MWADV d X  X illeurs] [ MWV bon train]] X  [go indeed well],  X  X  [ ADV jamais] [ MWV  X t X  question d X  X ] X  [has never been in question].
 216 7.3 Basic Parsing Results
The relative rankings of the different models are the same for Arabic and French (Berkeley &gt; Stanford parser &gt; DP-TSG &gt; PAPCFG), and these rankings correspond to those observed for English (Cohn, Blunsom, and Goldwater 2010). Although statistical statements cannot be made about the difficulty of parsing the two languages by com-paring raw evaluation figures, we can compare the differences between PAPCFG and the best model for each language. From this perspective, manual rule splitting in the
Stanford parser is apparently more effective for the ATB than for the FTB. Differences in annotation styles may account for this discrepancy. Consider the unbinarized treebanks. The ATB training set has 8,937 unique non-unary rule types with mean branching factor
M = 2.41 and sample standard deviation SD = 0.984. The FTB has a flat annotation style, which leads to more rule types (16,159) with a higher branching factor ( M = 2.87, SD = 1.51).
 tion that motivated memoryless binarization in both the Berkeley parser (Petrov et al. 2006, page 434) and the DP-TSG. The Berkeley parser results also seem to support the observation that rule refinement is less effective for the FTB. Automatic rule refinement results in a 16.1% F1 absolute improvement over PAPCFG for Arabic, but only 10.0% F1 for French.
 counts are also sparser. In addition, we found that the FTB has lower inner-annotator agreement (IAA) than the ATB (Appendix C), which also negatively affects super-Genabith 2007). To counteract that bias, we also included a Leaf Ancestor evaluation.
Nonetheless, even Leaf Ancestor showed that, with respect to PAPCFG, the best Arabic model improved nearly twice as much as the best French model.
 crucial difference between the two models is the decoding objective. The Berkeley parser maximizes the expected rule count (max-rule-sum) (Petrov and Klein 2007), an objective that Cohn, Blunsom, and Goldwater (2010) demonstrated could improve the DP-TSG by 2.0% F1 over Viterbi for English with no changes to the grammar . We decoded with
Viterbi, so our results are likely a lower bound relative to what could be achieved with objectives that correlate with labeled recall. Because MWE identification is a by-product of parsing, we expect that MWE identification accuracy would also improve. improvement must come from relaxing independencies in the grammar rules (by sav-for PCFGs (Johnson 1998, page 614). We observe an 8.5% F1 absolute improvement for Arabic, but just 3.8% F1 for French. Nonetheless, we chose this model precisely for its greater strong generative capacity, which we hypothesized would improve MWE identification accuracy. The MWE identification results seem to bear out this hypothesis. 8. Related Work
This section contains three parts. First, we review work on MWEs in linguistics and relate it to parallel developments in NLP. Second, we describe other syntax-based MWE identification methods. Finally, we enumerate related experiments on Arabic and
French. 8.1 Analysis of MWEs in Linguistics and NLP
An underlying assumption of mainline generative grammatical theory is that words are the basic units of syntax (Chomsky 1957). Lexical insertion is the process by which words enter into phrase structure, thus lexical insertion rules have the form [N car, apple ], and so on. This assumption, however, was questioned not long after it was proposed, as early work on idiomatic constructions like kick the bucket  X  X hich functions like a multiword verb in syntax X  X eemed to indicate a conflict (Katz and Postal 1963;
Chafe 1968). Chomsky (1981) briefly engaged kick the bucket in a footnote, but idioms remained a peripheral issue in mainline generative theory.
 ate given their pervasiveness cross-linguistically. In their classic work on the English construction let alone, Fillmore, Kay, and O X  X onnor (1988) argued that the basic units of grammar are not Chomskyan rules but constructions , or triples of phonological, syntac-tic, and conceptual structures. The subsequent development of Construction Grammar (Fillmore, Kay, and O X  X onnor 1988; Goldberg 1995) maintained the central role of which includes multiword expressions in the lexicon.
 conceptualized an alternate model of parsing in which new utterances are built from previously observed language fragments. In his model, which became known as data-oriented parsing (DOP) (Bod 1992),  X  X diomaticity is the rule rather than the exception X  (Scha 1990, page 13). Most DOP work, however, has focused on parameter estimation issues with a view to improving overall parsing performance rather than explicit mod-eling of idioms.
 it is curious that most NLP work on MWE identification has not utilized syntax. More-over, the words-with-spaces idea, which Sag et al. (2002) dismissed as unattractive on both theoretical and computational grounds, 22 has continued to appear in NLP evaluations such as dependency parsing (Nivre and Nilsson 2004), constituency parsing (Arun and
Keller 2005), and shallow parsing (Korkontzelos and Manandhar 2010). In all cases, the conclusion was drawn that pre-grouping MWEs improves task accuracy. Because the the experiments were not strictly comparable. Moreover, gold pre-grouping was usually assumed, as was the case in most FTB parsing evaluations after Arun and Keller (2005). intensifies the sparsity problem in the lexicon; and (2) it is not robust to morphological and syntactic processes such as inflection and phrasal expansion. 8.2 Syntactic Methods for MWE Identification
There is a voluminous literature on MWE identification, so we focus on syntax-based methods. The classic statistical approach to MWE identification, Xtract (Smadja 1993), used an incremental parser in the third stage of its pipeline to identify predicate-argument relationships. Lin (1999) applied information-theoretic measures to automatically extracted dependency relationships to find MWEs. To our knowledge, 218 expression identification. No empirical results were provided, however, and the MWE-augmented scoring function for the output of his symbolic parser was left to future research. Recently, Seretan (2011) used a symbolic parser for collocation extraction. Col-locations are two-word MWEs. In contrast, our models handle arbitrary length MWEs. of statistical parsing. Nivre and Nilsson (2004) converted a Swedish corpus into two versions: one in which MWEs were left as tokens, and one in which they were grouped ( words-with-spaces ). They parsed both versions with a transition-based parser, showing that the words-with-spaces version gave an improvement over the baseline.
Cafferkey (2008) also investigated the words-with-spaces idea along with imposing chart constraints for pre-bracketed spans. He annotated the PTB using external MWE lists and an NER system, but his technique did not improve two different constituency pre-grouping changes the number of evaluation units (dependency arcs or bracketed spans), thus the results are not strictly comparable. From an application perspective, pre-grouping assumes high accuracy identification, which may not be available for all languages.
 prove parsing via MWE information. In contrast, we tune statistical parsers for MWE identification. 8.3 Related Experiments on Arabic and French
Arabic Statistical Constituency Parsing. Kulick, Gabbard, and Marcus (2006) were the first to parse the sections of the ATB used in this article. They adapted the Bikel parser (Bikel 2004) and improved accuracy primarily through punctuation equivalence classing and the Kulick tag set. The ATB was subsequently revised (Maamouri, Bies, and Kulick 2008), and Maamouri, Bies, and Kulick (2009) produced the first results on the revision for our split of the revised corpus. They only reported development set results with gold POS tags, however. Petrov (2009) adapted the Berkeley parser to the ATB, and we later provided a parameterization that dramatically improved his baseline (Green and
Manning 2010). We also adapted the Stanford parser to the ATB, and provided the first word model for the Berkeley parser based on signatures, much like those in Table 3. More recently, Huang and Harper (2011) presented a discriminative lexical model for Arabic that can encode arbitrary local lexical features.
 fication. Attia (2006) demonstrated a method for integrating MWE knowledge into a lexical-functional grammar, but gave no experimental results. Siham Boulaknadel and Aboutajdine (2008) evaluated several lexical association measures in isolation for lingual projection methods (using Wikipedia and English Wordnet) with standard n -gram classification methods.

French Statistical Constituency Parsing. Abeill X  (1988) and Abeill X  and Schabes (1989) identified the linguistic and computational attractiveness of lexicalized grammars for modeling non-compositional constructions in French well before DOP. They developed a small tree adjoining grammar (TAG) of 1,200 elementary trees and 4,000 lexical items that included MWEs. Recent statistical parsing work on French has included stochas-adjunction operation. 23 Both Seddah, Candito, and Crabb X  (2009) and Seddah (2010) showed that STIGs underperform CFG-based parsers on the FTB. In their experiments, MWEs were grouped. Appendix B describes additional prior work on CFG-based FTB parsing.

French MWE Identification. Statistical French MWE identification has only been investi-gated recently. We previously reported the first results on the FTB using a parser for
MWE identification (Green et al. 2011). Contemporaneously, Watrin and Francois (2011) applied n -gram methods to a French corpus of multiword adverbs (Laporte, Nakamura, and Voyatzi 2008). Constant and Tellier (2012) used a linear chain conditional random fields model (CRF) for joint POS tagging and MWE identification. They incorporated external linguistic resources as features, but reported results for a much older version of the FTB. Subsequently, Constant, Sigogne, and Watrin (2012) integrated the CRF model into the Berkeley parser and evaluated on the pre-processed FTB used in this article.
Their best model (with external lexicon features) achieved 77.8% F1. 9. Conclusion
In this article, we showed that parsing models are very effective for identifying arbitrary-length, contiguous MWEs. We achieved a 66.9% F1 absolute improvement tion methods. All parsing models discussed in the paper improve MWE identification over n -gram methods, but the best improvements come from different models. Unlike n -gram classification methods, parsers provide syntactic subcategorization and do not any language for which the following linguistic resources exist: a syntactic treebank, an MWE list, and a morphological analyzer.
 semantics. This connection has been debated in linguistics, yet overlooked in statistical
NLP until now. Although empirical task evaluations do not always reinforce linguistic theories, our results suggest that syntactic context can help identify idiomatic language, as posited by some modern grammar theories.
 the lexical insertion model that helps combat lexical sparsity in morphologically rich languages. In the gold setting, the factored lexicon yielded improvements over the basic lexicon for both standard parsing and MWE identification. Results were lower in the automatic setting, suggesting that it might be helpful to optimize the morphological analyzers for specific features included in downstream tasks like parsing. We evaluated on in-domain data, but we expect that the factored lexicon would be even more useful on out-of-domain text with higher out-of-vocabulary rates.
 never been demonstrated with experiments like ours. Although the DP-TSG, which is a relatively new parsing model, still lags other parsers in terms of overall labeling 220 tification. Because we modified the syntactic representation rather than the model formulation, general improvements to this parsing model should yield improvements in MWE identification accuracy.
 Appendix A: Additional French MWEs This appendix describes the other French MWE categories annotated in the FTB.
Adverbial idioms (MWADV) often start with a preposition (Example (11)) but can have very different part-of-speech sequences: (11) a. P N: du coup ( X  X o X ), sans doute ( X  X oubtless X )
Foreign words (MWET) include English nominal idioms, such as cash flow and success story , which are less integrated in French than words such as T-shirt . Expressions such as Just do it or struggle for life also fall in this category.

Prepositional idioms (MWP) are mostly fixed (Example (12)), but some permit minimal variation such as de vs. des or  X  vs. au : (12) a. P N P: en d X pit de ( X  X espite X ),  X  hauteur de ( X  X t the height of X )
Pronominal idioms (MWPRO) consist of demonstrative pronouns ( celui-ci  X  X his one X , celui-l X   X  X hat one X ) and reflexive pronouns ( lui-m X me  X  X imself X ), which vary in gender and number, as well as a few indefinite pronouns which allow gender inflection ( d X  X ucuns and some which are fixed ( d X  X utres  X  X thers X , la plupart  X  X ost X , tout un chacun  X  X veryone X , tout le monde  X  X verybody X ).

Multiword determiners (MWD) consist of expressions such as bien des ( X  X  lot of X ) and vs. la plupart des  X  X ost of X ). Numbers which act as determiners in the sentence ( class X es en vingt-huit cat X gories  X  X ategorized in twenty-eight categories X ) are also classified as MWD.
Multiword conjunctions (MWC) are a fixed class: (13) a. C C: parce que ( X  X ecause X )
Multiword interjections (MWI) are a small category with expressions such as mille sabords ( X  X listering barnacles X ) and au secours ( X  X elp X ).
 Appendix B: Comparison to Prior FTB Pre-Processing Our FTB pre-processing is automatic, unlike all previous methods.
 A sentence treebank that differed principally in their treatment of MWEs: (1) C which MWE tokens were grouped ( en moyenne  X  en_moyenne ); and (2) E
MWEs were marked with a flat structure. For both representations, they also gave results in which coordinated phrase structures were flattened. In the published exper-iments, they mistakenly removed half of the corpus, believing that the multi-terminal (per POS tag) annotations of MWEs were XML errors (Schluter and van Genabith 2007).
MFT . (Schluter and van Genabith 2007) Manual revision to 3,800 sentences. Major changes included coordination raising, an expanded POS tag set, and the correction of annotation errors. Like A RUN -C ONT , MFT contains concatenated MWEs.

FTB-UC . (Candito and Crabb X  2009) A version of the functionally annotated section that makes a distinction between MWEs that are  X  X yntactically regular X  and those that are not. Syntactically regular MWEs were given internal structure, whereas all other MWEs were grouped. For example, nouns followed by adjectives, such as loi agraire ( X  X and law X ) or Union mon X taire et  X conomique ( X  X onetary and economic Union X ) were considered syn-tactically regular. They are MWEs because the choice of adjective is arbitrary ( loi agraire and not  X  loi agricole , similarly to ( X  X oal black X ) but not ( but their syntactic structure is not intrinsic to MWEs. In such cases, FTB-UC gives the
MWE a conventional analysis of an NP with internal structure. Such analysis is indeed sufficient to recover the meaning of these semantically compositional MWEs that are extremely productive. FTB-UC loses information about MWEs with non-compositional semantics, however.
 grouping. Candito, Crabb X , and Denis (2010) were the first to acknowledge and address this issue, but they still used FTB-UC (with some pre-grouped MWEs). Because the syntax and definition of MWEs is a contentious issue, we take a more agnostic view X  which is consistent with that of the FTB annotators X  X nd leave them ungrouped. This permits a data-oriented approach to MWE identification that is more robust to changes to the status of specific MWE instances. 222 experiments are not comparable: The data split and pre-processing were different, and he grouped MWEs.
 Appendix C: Annotation Consistency of Treebanks
Differences in annotation quality among corpora complicate cross-lingual experimental comparisons. To control for this variable, we performed an annotation consistency evaluation on the PTB, ATB, and FTB. The conventional wisdom has it that the PTB has comparatively high inter-annotator agreement (IAA). In the initial release of the ATB,
IAA was inferior to other LDC treebanks, although in subsequent revisions, IAA was quantifiably improved (Maamouri, Bies, and Kulick 2008). The FTB also had significant annotation errors upon release (Arun and Keller 2005), but it, too, has been revised. corpus position i with label l . We consider all substrings in the corpus. If s is bracketed |
L | &gt; 1, then s is a variation nucleus. 24 man evaluation is one way to distinguish between the two cases. Following Dickinson (2005), we sampled 100 variation nuclei from each corpus and evaluated each sample for the presence of an annotation error. To control for the number of corpus positions included in each treebank sample, we used frequency-matched stratified sampling with bin sizes of 2, 3, 4, 10, 50, and 500.
 English speaker for the WSJ (the third author). 25 evaluation, which supports the anecdotal consistency ranking of the three treebanks.
The FTB averages more than one variation nucleus per sentence and has twice the token-level error rate of the other two treebanks.
 Appendix D: mwetoolkit Configuration
We configured mwetoolkit 27 with the four standard lexical features: the maximum likelihood estimator, Dice X  X  coefficient, pointwise mutual information, and Student X  X  t -score. We added the POS sequence for each n -gram as a single feature. We removed the Web counts features since the parsers do not use auxiliary data.
 once. To further balance the proportion of MWEs, we trained on all valid MWEs plus 10x randomly selected non-MWE n -grams. This proportion matches the fraction of MWE/non-MWE tokens in the FTB. As we generated a random training set, we reported the average of three independent training runs.
 classifier with Weka (Hall et al. 2009). Although mwetoolkit defaults to a linear kernel, we achieved higher accuracy on the development set with an RBF kernel.
 mwetoolkit . Ramisch, Villavicencio, and Boitet (2010) experimented with the Genia corpus, which contains 18k English sentences and 490k tokens, similar to the FTB. Their test set had 895 sentences, fewer than ours. They reported 30.6% F1 for their task against an Xtract baseline, which only obtained 7.3% F1. Their best result compares favorably (in magnitude) to our mwetoolkit baselines for French and Arabic.
 Acknowledgments References 224 226
