 q }, an group nearest neighbor (GNN) query [1-8] returns a data object in dataset P with the smallest sum of distances to all data objects in Q . Figure 1(a) shows an example of p
However, GNN query only finish the computation from the user X  X  perspective. In this paper, we study reverse group nearest neighbor (RGNN) query which consider the GNN query from company X  X  perspective. Given a dataset P and a query object q , an RGNN query aims to find multiple subsets from P such that each subset G has q as its GNN, and | G |= m ( m  X  2), where m is a specified integer. When m =1, an RGNN query is objects. Since { p 1 , p 3 } has q as its GNN, it is a result of RGNN of q . 
Unlike GNN query which only returns a data object as the answer, RGNN query might retrieve multiple subsets as the answers. To enhance user decision-making, we may set up a parameter k to limit the number of the subsets using the aggregate distance as the monotonic sorting function. This is a general RGNN query. We call it reverse top-k group nearest neighbor (R k GNN) query. Consider the example in Figure 1(b), R k GNN will return { p 1 , p 3 } and { p 2 , p 3 } as the answers when k =2. 
R k GNN query has many applications in business analytic and decision support systems. Consider the example: a big company plans to open m branches according to the location of headquarters (denotes as the query object q ). The candidate locations of branches can be regarded as data objects in a database. Generally, the manager hopes that the branches have q as their GNN. In other words, these branches have less sum distances to q , and there is not another branch which has a smaller sum distances to the products) from n alternative products picked up according to a specified prototype q , the corporation generally hopes that the sum distance from q is less for the group of products; meanwhile the new products should have a diversified design so that they can satisfy the requirement of different customer. The diversity can be measured by the sum distance from another product. Actually, the principle of design embodies the proximity with q and diversity from another object for a group of objects. 
In this paper, we introduce two efficient algorithms, i.e., Lazy RkGNN (LR k GNN) and Spatial pruning LRkGNN (SLR k GNN) on datasets indexed by R-tree using Euclidean distance function, by several pruning heuristics, which reduce a large of subsets, the early stopping technologies which avoid searching the whole data space and the method of lazily outputting candidate subsets based sorting mechanism. Specifically, our main contributi ons are summarized as follows. (i) We formalize the R k GNN query, an interesting variant of GNN query. (ii) We develop two algorithms, viz. LR k GNN and SLR k GNN, for efficiently answering reverse top-k group nearest neighbor queries. (iii) We demonstrate the effectiveness of our proposed heuristics and the performance of our proposed algorithms by extensive experiments. Table 1 contains the primary symbols used in our description. Definition 1 (Reverse top-k Group Nearest Neighbor, R k GNN). Given a dataset P , a query object q , and two (user specified) parameters m and k , an R k GNN query finds k subsets, which have the least aggregate distances , such that each subset contains m data objects from P and has q as its group nearest neighbor . 
A straightforward approach is to enumerate the combination (or say subset G ) of the data objects, and compute the GNNs of each such combination. However, this approach is not efficient. 3.1 Lazy R k GNN Algorithm In this subsection, we describe the L azy R everse top-k G roup N earest N eighbor shrink the search space based lazy techniques , and employs some effective pruning heuristics to discard candidate subsets. Lazy Output and Advanced Sorting Technique minimum distance between current entry e and the query object q as the sorting key . among H when an entry e is removed from the auxiliary heap H . If the candidate subset G only contains data objects, it is processed by invoking MBM algorithm in [1] to judge be processed later until the intermediate entry in G is expanded. 
Unlike basic R k GNN algorithm, LR k GNN algorithm does not immediately enumerate the combinations for each time popped object until there are multiple data objects. LR k GNN uses two min-heaps, H t and H a , to save the data objects popped from H satisfying (1), and all entries among H before enumerating, respectively. Then, LR k GNN generates all combinations so far using the data objects or entries of H a . 
Owing to that LR k GNN only need to retrieve k final results, LR k GNN uses the following inequality in (1) as the triggered conditions of enumerated combinations: where | H t | represents the number of data objects in H t . The inequality in (1) guarantees that the algorithm generates at least k combinations; meanwhile it avoids to expand too many intermediate entries. 
In fact, there still are too many entries in auxiliary heap H because a leaf node often includes more than one hundred data objects. Thus, the number of combinations is still number of evaluating candidate subsets, LR k GNN progressively outputs some subsets from total subsets and process th em according to their aggregate distances. 
How to progressively generate some ordered subsets? Clearly, this problem is not trivial when the size of H t is large. Our main idea is repeatedly applying two operations: decomposing and re-sorting. First, we sort the n entries in ascending order according C Next, the two parts of subsets are merged and form a sorting list of subsets in ascending order according to adist ( q , G ). Repeating the procedure until the aggregate distance of best_kdist . Our experiments show that only about 0.1% subsets need to be processed for each enumerating. distances from q are 1, 2, 3, and 3.5, respectively. LR k GNN decomposes set C 4 2 into { o 2 , o 1 }) and o 3 o output. Finally, { o 3 , o 2 }, { o 4 , o 2 }, and { o 4 , o 3 } are outputted as results. Sorting and Threshold Method Obviously, two key factors ultimately determine the actual performance of LR k GNN: how to progressively sort for all subsets formed by the entries among H , which might severely influence the number of subsets to be evaluated, and the strategy of choosing the stop point, which greatly shrink the search space. 
Now, let X  X  consider the second factor now. By Example 1, we know that LR k GNN progressively processes three parts of ordered subsets. Therefore, we use the variable best_hdist to save the minimum aggregate distance of each part of subsets. Moreover, best_hdist is 3 when LR k GNN deals with the first part of subsets in Example 1. However, best_hdist becomes 4 when LR k GNN processed the second part of subsets. In fact, LR k GNN can terminate the search using the following Theorem 1 and Corollary 1. Theorem 1. R k GNN can stop the search if best_hdist and best_rfndist are both larger than best_kdist , namely, best_hdist  X  best_kdist and best_rfndist  X  best_kdist . Proof. Since best_rfndist is larger than best_kdist , there is not any combination which has a lower aggregate distance than best_kdist so far. Therefore, any combination best_kdist , there is not any new combination which has a less aggregate distance than Corollary 1. The search of R k GNN can be stopped if best_hdist is larger than best_kdist , namely, best_hdist  X  best_kdist , when the set of G rfn is empty. In practice, LR k GNN can also stop enumerating procedure by following Lemma 1. G )  X  best_kdist . Proof. Since all enumerated subsets are arrang ed in ascending order according to their distance. Thus, there is not another subset which has a larger aggregate distance than algorithm save much unnecessary cost of computation. Pruning Heuristics Next, we introduce some pruning heuristics to discard many candidate subsets by Lemma 2 and Lemma 3. Lemma 2. Assume that a node e contains at least m +1 data objects, and mindist ( q , e ) cannot be the result of R k GNN query if maxdist ( e )  X  mindist ( q , e ). p  X  among e which is not contained in G . Obviously, the sum of distances between p  X  and a general assumption, therefore, we have that Lemma 2 is correct. Consider the example in Fig. 2(a) where the minimum bounding rectangle (MBR) of Lemma 2 can prune many combinations, it might be too restrictive to prune any combination when the MBR of the node is too large. At this point, we can still utilize the following Lemma 3 to prune the current subset G . cannot be the result of R k GNN query. Proof. Assume that there is another point p  X  among e which is not contained in G . Since maxdist ( e ) represents the maximum distance between any two data objects among e , Lemma 3 is correct. Query Processing In this subsection, we describe the query procedure of LR k GNN algorithm. LR k GNN 5~7). The lazy technique reflects on lines 9~18. Line 12 progressively generates candidate subsets in ascending order of adist ( q , G ). Line 13 discards candidate subsets by Lemma 2 and Lemma 3. Line 14 guarantees that LR k GNN only need to evaluate a few of subsets when progressively output subsets. LR k GNN processes current subset G , where the procedure UpdateRlt is used to update the query results and current best aggregate distance best_kdist (Lines 15~17). The intermediate entry is expanded by lines 20~21. Line 22 employs Lemma 2 and Lemma 3 to mark the false alarms . LR k GNN makes use of lines 23~29 to expand the subsets which contains the current entry e using a similar procedure of lines 15~17. 3.2 Spatial Pruning LR k GNN Algorithm Since LR k GNN need to invoke the MBM algor ithm for each candidate subset, we present, in this subsection, an enhanced algorithm, called Spatial pruning Reverse top-k Group Nearest Neighbor algorithm (SLR k GNN), which takes advantage of spatial invoke MBM algorithm to compute GNN. In the sequel, we illustrate the intuition of our spatial pruning method by Figure 3. contains in the dataset P . In particular, we denotes the pruning region (circle) of object p  X  PR ( q , p 2 )...  X  PR ( q , p m ). By analyzing the cases in Fig. 3, we immediately obtain the following Lemma 4. Lemma 4. ( RGNN Spatial Pruning ) Given a candidate subset G and a query object q . If result of RGNN query of q ; if there does not exist any objects contained in  X  PR ( q , G ), G is the result of RGNN query of q ; otherwise, there exists at least an object contained among R to judge whether or not G has q as its GNN. 
The importance of Lemma 4 is to enable R k GNN algorithm to shrink the search GNN. In practice, Lemma 4 saves much cost in distance computation and reduces many I/O accesses. In this section, we experimentally evaluate the effectiveness of our proposed pruning heuristics and the performance of our developed algorithms for R k GNN query. 
We use two real datasets, namely, PP and NE , from www.rtreepotral.org. We also create Independent ( IN ) and Correlated ( CO ) datasets with dimensionality dim = 2 and cardinality N in the range [20K, 100K] ( PP is in the range [3K, 15K]). All datasets is normalized to range [0, 1]. Each dataset is indexed by an R-tree, with a page size of 4096 bytes. each experiment, only one factor varies, whereas the others are fixed to their default values ( k =15, m =3, N =60K). The wall clock time (i.e., the sum of I/O cost and CPU time, where the I/O cost is computed by charging 10ms for each page access, as with performance of early stopping R k GNN computation, the number of valid candidate subsets for GNN query ( NVS ) which reports the capability of pruning heuristics (i.e., Lemma 1, Lemma 2, and Lemma 3, etc.), are used as the major performance metrics. Each reported value in the following diagrams in the average of 50 queries. (1) The effectiveness of pruning techniques 
This set of experiments aims at verifying the effectiveness of our proposed pruning heuristics. We vary k from 5 to 25, with m fixed at 3 in Figure 4, using four datasets, candidate subsets (or say combinations), which validates their usefulness because NES and NVS both have a small value. In fact, a small value of NES verifies the efficiency of Theorem 1 and Corollary 1; meanwhile a small value of NVS confirms the efficiency of our proposed Lemma 1~Lemma 3. Although computing R k GNN is time consuming, our algorithm still obtains a higher performance owing to the adoption of early stopping technology which dramatically shrinks the search space. Similarly, Figure 5 and Figure using real datasets and synthetic datasets, respectively. The diagrams confirm the observations and corresponding explanations of Figure 4. (2) The results on R k GNN queries 
The second set of experiments studies the performance of our proposed algorithms in answering R k GNN queries. First, we explore the impact of k on algorithms, and the results are shown in Figure 7. Clearly, SLR k GNN outperforms LR k GNN in all cases Furthermore, as k grows, the cost of LR k GNN and SLR k GNN slightly increase. 
Then, we evaluate the effect of N on the algorithms. Figure 8 depicts the cost as a function of N . SLR k GNN clearly gains a better performance than LR k GNN. When N increases, the cost of the algorithms does not always increase. This is because, on the there is a big chance of finding some subsets which have q as their GNN in a big dataset than in a small dataset. (except for N = 9K on dataset PP ). Figure 9 shows the performance of algorithms as a function of m . Again, SLR k GNN outperforms LR k GNN in all cases. Moreover, although the number of candidate subsets dramatically increase as m increases, the cost of algorithms does not always grow. This is because on the one hand, Theorem 1, Corollary 1 and Lemma 1 prune most of the candidate subsets, and on the other hand, a big m brings more candidate subsets which have q as their GNN object. This paper firstly introduces and solves a new form of nearest neighbor queries, namely, reverse top-k group nearest neighbor (R k GNN) retrieval. We develop two efficient algorithms, LR k GNN and S LR k GNN, using sorting and threshold technique, and some smart pruning heuristics to prune away most of unqualified candidate R k GNN queries by using the reuse technique. Another interesting direction for future constrained R k GNN, the R k GNN in metric spaces, etc. Acknowledgements. This work was supported in part by NSFC 61003049, ZJNSF LY12F02047 and LY12F02019, the Science and Technology Plan Fund of Jiaxing Municipal Bureau under Grant 2011AY1005, the Fundamental Research Funds for the Central Universities under Grant 2012QNA5018, the Key Project of Zhejiang University Excellent Young Teacher Fund (Zijin Plan). 
