 If there were a general-purpose semantic representation of text, what would it look like? It has been remarked (Bar-Haim et al. 2007 ) that while there exist several widely used formats for a general-purpose syntactic representation, it is far less clear what a general semantic representation should be. Also, while there is a single task of syntactic parsing, for semantics there is a large number of separate tasks that could furnish components for an overall semantic analysis, including word sense analysis (Klapaftis and Manandhar, this volume, McCarthy et al., this volume) semantic role labeling (Ruppenhofer et al., this volume), coreference (Ma ` rquez et al., this volume), transformation to logical form, analysis of modals and negation, relations between words, and many others. Compounding the problem, different applications, such as machine translation (McCarthy et al., this volume), sentiment analysis (Wu and Jin, this volume), and keyphrase extraction (Kim et al., this volume) will require semantic analyses that differ in their depth and their focus. This variety of tasks and applications is reflected in SemEval, the Semantic Evaluation workshop series. The origins of the series are described by (Agirre et al. 2009 ). SemEval consists of shared tasks proposed by the community as they become relevant in the field. Because of this community-driven approach, SemEval directly reflects the wide variety of semantic tasks. SemEval does not focus solely on either general linguistic phenomena or application-specific tasks, but welcomes, and features, both. Also, individual tasks are independent, such that researchers do not necessarily need to integrate approaches for different semantic phenomena into a single, overall system. On the other hand, if and when tasks come up that require such an integration, then that integration takes place. In this way, SemEval fits the needs of the semantic analysis community. An additional important function of SemEval is to provide an archive of the datasets produced for the shared tasks, such that researchers can compare their performance on the same data even years later. 1 SemEval-2 This special issue on semantic evaluation collects articles connected to SemEval-2, which took place in Uppsala, Sweden, in July 2010. It consists of extended versions of both task descriptions and system descriptions for tasks at SemEval-2.
 Table 1 shows a complete list of the shared tasks that took place at SemEval-2. The second column lists the task titles, and the third column gives a rough classification of tasks by their topics (created by us). The list of topics clearly reflects the mixture of general and application-specific tasks. Tasks 5, 15, 17 and 18 address semantic problems in specific applications, while tasks like 1, 3, 10, and 13 explore more general language phenomena. Another theme for this SemEval that we can see in the task list is multilinguality and cross-linguality in different forms, in particular in tasks 1, 2, and 3.
 2 Articles in this special issue The papers in this special issue take up these two themes, along with two further themes: the questions of appropriate annotation schemes and evaluation techniques.
The papers by Klapaftis and Manandhar and by McCarthy et al. are related to word sense, the oldest topic within Senseval/SemEval challenges, but neither of the two paper works with a fixed dictionary. Klapaftis and Manandhar report on the SemEval word sense induction task (task 14). They argue that using an induced word sense inventory, rather than a manually created dictionary, is advantageous when adaptation to a given domain or application is important. They propose a new, more in-depth evaluation: In addition to unsupervised (cluster comparison) and supervised evaluation (disambiguation of new data), they study system performance relative to relative sense frequencies of a lemma, arguing that lemmas with a more prevalent frequent sense are harder to analyze.

McCarthy et al. discuss the SemEval cross-lingual lexical substitution task (task 2), a cross-lingual extension of the earlier English lexical substitution task (McCarthy et al. 2007 ). In this task, participating systems produce a Spanish translation for a single target word within an English sentence. This task is particularly relevant for machine translation, but focuses on lexical semantics rather than complete translation systems. In an in-depth evaluation of systems participat-ing in the task, McCarthy et al. show which systems are similar and which are complementary in their behavior, and can be combined into an ensemble that performs better than any individual system.

The article by Ma ` rquez et al. takes up the themes of cross-linguality and evaluation. They follow up on the evaluation of coreference resolution systems in SemEval (task 1), but perform a more focused comparison across three languages, focusing on the different evaluation conditions. What emerges is a complex interplay of evaluation conditions, phenomena, and application requirements. The conclusion that the article draws is that a combination of evaluation measures is necessary for making an informed decision between systems. Another conclusion of this article is that the underlying philosophy of a coreference system is not as important as the choice of the right parameters and training conditions.

Ruppenhofer et al. discuss SemEval task 10, semantic role labeling beyond the sentence level, focusing particularly on the theme of annotation schemes. They focus on the challenging problem of annotating null instantiations , semantic role fillers that are not instantiated locally but inferrable from context, in a full-text annotation setting. Interesting problems can be found in all sub-tasks: the decision of whether a semantic role should be treated as null-instantiated in the first place, and in what way, and the decision of which preceding phrase should be considered as a filler.

Stro  X  tgen and Gertz present a system for automatic temporal analysis (SemEval task 13). The themes in this article are cross-lingual evaluation, and annotation standards and corpora. In addition to presenting their state-of-the-art system, which is completely rule-based, the authors argue that the domains of news and narrative text differ in what information is relevant for resolving temporal expressions, and demonstrate that it is beneficial to use different resolution techniques for the two genres.

The articles by Wu and Jin and by Kim et al. focus on specific applications. Wu and Jin take up the problem of word sense again, but with an application-specific twist: They describe the task of disambiguating adjectives that can convey both positive and negative sentiment (SemEval task 18), such as  X  X  X ow X  X , which is positive when it is used to describe  X  X  X ost X  X  but less so when describing  X  X  X alary X  X . The good news is that the best performance for this task is close to 100% in all cases.
Kim et al. discuss the automatic extraction of keyphrases from scientific articles (SemEval task 5), an information extraction task. This, the authors find, is a highly challenging task, both for systems and for humans, at least when exact match with gold keyphrases is required. Interestingly, the most successful systems comprise both supervised and unsupervised approaches.

The article of Yuret et al. is linked to the application of textual entailment, but with a novel twist. SemEval task 12, which they describe, introduces the notion of  X  X  X yntactic entailment X  X , textual entailment problems that can be solved using grammatical knowledge alone. The data for this task was created by turning potential syntactic dependencies into entailments, such that untrained annotators could make syntactic judgments by deciding on entailment. The evaluation in this task can thus mostly be viewed as an evaluation of the underlying parsers. In the current article, the authors perform an even more direct comparison of parsers by plugging them into the exact same textual entailment framework. 3 Conclusion This special issue presents a selection of the tasks from SemEval-2, describing the motivation, the guidelines used to create the data and resources, the participant systems, and the result analysis. Looking at the evolution of SensEval/SemEval, we can see, over the years, a broadening of the perspective on what is semantic processing. The computational linguistics community is becoming more and more interested in exploring computational approaches to many tasks at the core of language understanding. We hope that this trend will continue with the same and even more energy.
 References
