 from a metric space as in [10]. training set is an i.i.d sample x Schoenberg (see for example [4]).
 X X bution has density p with respect to a known measure, in fact X otherwise a challenging question for a general distributio n. Intuitively, X region where the distribution is concentrated, that is  X  ( X the measure is not well defined since no topology is given.
 proach to model X second idea is to use the kernel to construct a function F one is exactly the support X support X only separate certain sets. More precisely, our contributi on is two-fold. any probability distribution).
 development can be found in the supplementary material. section are reported in the supplementary material. kernel Hilbert space H on X with a reproducing kernel K : X  X  X  X  C [2]. The scalar product K x  X  H denote by H some preliminary definitions and results.
 Definition 1. A subset C  X  X is separated by H , if, for any x function f depends on x of separating property is given.
 We need some further notation. For any set C , let P the closure of the linear space generated by { K Moreover let F Proposition 1. For any subset C  X  X , the following facts are equivalent If one of the above conditions is satisfied, then K ( x, x ) 6 = 0  X  x /  X  C. this implies that K by to be well defined. Then ( X, d solve this problem.
 Proposition 2. Assume that K to measurable.
 kernel Hilbert spaces which are able to separate the largest family of subsets of X . kernel K such that K C  X  X which are closed with respect to the metric (2) .
 pletely regular if, for any closed subset C and any point x such that f ( x function d the metrics d of H can be proved by showing that a suitable family of bump 2 functions is contained in H . Corollary 1. Let X be a separable metric space with respect to a metric d K is a continuous function with respect to d is closed with respect to d x 0 /  X  C with the normalized kernel K  X  ( x, t ) = K ( x, t ) d  X  N . proposed regularized spectral algorithms.
 assumption.
 K defines a completely regular and separable RKHS H .
 We endow X with the metric d measurable by Prop. 2. Then we can define the support X all the closed sets C  X  X , such that  X  ( C ) = 1 . Clearly X last property depends on the separability of X , hence of H ). Summarizing the key result in the previous section, under th e above assumptions, X set of the function F where P 3.1 A New Characterization of the Support The key observation towards defining a learning algorithm to estimate X can be expressed in terms of the integral operator defined by t he kernel K . To see this, for all x  X  X , let K Moreover, let T : H X  X  be the linear operator defined as simply the integral operator with kernel K with domain and range in H . Then, one can easily see that the null space of T is precisely ( I  X  P where T  X  is the pseudo-inverse of T (see for example [9]). Hence Observe that in general K function with  X  (0) = 0 , then spectral theory gives that P the latter can be estimated from data. 3.2 Spectral Regularization Algorithms Finally, in this section, we describe how to construct an est imator F empirical version of the integral operator associated to K which is simply defined by The latter operator is an unbiased estimator of T . Indeed, since K inequalities for random variables in Hilbert spaces to prov e that general T  X  In this paper we study a spectral filtering approach which rep laces T  X  obtained filtering out the components corresponding to the small eigenvalues of T is defined by spectral calculus. More precisely if T T , then g the theory of inverse problems [9]. Intuitively, g as  X  decreases. More formally these properties are captured by t he following set of conditions. Assumption 2. For  X   X  [0 , 1] , let r g ization g constraints, will focus mostly on Tikhonov regularization in the following. For a chosen filter, the regularized empirical estimator of F One can see that that the computation of F introduce the sampling operator S points. The adjoint S  X  T Then it is easy to see that g where k problem. performance measure to compare F of a reference measure (usually the Lebesgue measure) and the assumption that  X  is absolutely continuous with respect to .
 mator F a suitable sequence  X  Theorem 1. Let F so that then for every compact subset C of X error depends on the gap between the M -th and the M + 1 -th eigenvalue of T [1], where M -th of the sample error. Third, we note that the uniform converge nce of F does not imply the convergence of the level sets of F an effective decision rule, an off-set parameter  X  X n = { x  X  X | F n ( x )  X  1  X   X  n } show that for a suitable choice of  X  Theorem 2. If the sequence (  X  then, for any compact subset C .
 and 2 also hold by choosing C = X . Second, note that the choice of  X  convergence of F learning rates and finite sample bound is a key question that w e will tackle in future work. Tikhonov regularization defines an estimator F as belonging to the support if F have to predict the value of F solution (note that the cost of the eigen-decomposition of K estimator and one-class SVM (1CSVM )as implemented by [6]. F or the Parzen window estimator sponding regularization parameter (Left) and a detail of th e first 50 eigenvalues (Right). best result, but spectral algorithm still provides a compet itive performance. Center, CBCL Right. tasks. and to a more extensive experimental analysis.
 [2] N. Aronszajn. Theory of reproducing kernels. Trans. Amer. Math. Soc. , 68:337 X 404, 1950. [6] S. Canu, Y. Grandvalet, V. Guigue, and A. Rakotomamonjy. Svm and kernel methods matlab [11] H. Hoffmann. Kernel pca for novelty detection. Pattern Recogn. , 40(3):863 X 874, 2007.
