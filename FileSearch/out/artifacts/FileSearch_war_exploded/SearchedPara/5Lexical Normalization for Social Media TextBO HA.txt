 Microblogging services, such as Twitter, 1 are highly attractive for information extrac-tion and text mining purposes, as they offer large volumes of real-time data. According to Twitter [2011], 2, 65, and 200 million messages were posted per day in 2009, 2010, and the first half of 2011, respectively, and the number is still growing. Messages from Twitter have shown to have utility in applications such as disaster detection [Sakaki et al. 2010], sentiment analysis [Jiang et al. 2011; Gonz  X  alez-Ib  X  a  X  nez et al. 2011], and event discovery [Weng and Lee 2011; Benson et al. 2011]. The quality of messages varies significantly, however, ranging from high-quality newswire-like text to mean-ingless strings. Typos, ad hoc abbreviations, phonetic substitutions, ungrammatical structures, and emoticons abound in short text messages, causing grief for text pro-cessing tools [Sproat et al. 2001; Ritter et al. 2010]. For instance, presented with the input u must be talkin bout the paper but I was thinkin movies ( X  X ou must be talking about the paper but I was thinking movies X ), 2 the Stanford parser [Klein and Manning 2003; de Marneffe et al. 2006] analyses bout the paper and thinkin movies as a clause and noun phrase, respectively, rather than a prepositional phrase and verb phrase. One way to minimize the performance drop of current tools and make full use of Twitter data is to retrain text processing tools on this new domain [Gimpel et al. 2011; Liu et al. 2011; Ritter et al. 2011]. An alternative approach is to preprocess messages to produce a more standard rendering of these lexical variants. For example, se u 2morw!!! would be normalized to see you tomorrow! The normalization approach is especially attractive as a preprocessing step for applications which rely on keyword match or word frequency statistics, such as topic trend analysis. For example, earthqu , eathquake ,and earthquakeee  X  X ll attested in a Twitter corpus X  X ave the standard form earthquake ; by normalizing these types to their standard form, better coverage can be achieved for keyword-based methods, and better word frequency estimates can be obtained. We will collectively refer to individual instances of typos, ad hoc abbreviations, unconventional spellings, phonetic substitutions, and other causes of lexical deviation as  X  X exical variants X .

The normalization task is challenging. It has similarities with spell checking [Peter-son 1980], but differs in that lexical variants in text messages are often intentionally generated, whether due to the desire to save characters/keystrokes, for social identity, or due to convention in this text subgenre. We propose to go beyond spell checkers, in performing deabbreviation when appropriate, and recovering the canonical word form of commonplace shorthands like b4  X  X efore X , which tend to be considered beyond the remit of spell checking [Aw et al. 2006]. The free writing style of text messages makes the task even more complex, for example, with word lengthening such as goooood being commonplace for emphasis. In addition, the detection of lexical variants X  X hat is, distinguishing lexical variants from out-of-vocabulary words that do not require normalization X  X s rather difficult, due at least in part to the short length of, and noise encountered in, Twitter messages.
 In this article, we focus on the task of lexical normalization of English Twitter and SMS messages, in which Out-Of-Vocabulary (OOV) tokens are normalized to their In-Vocabulary (IV) standard form, that is, a standard form that is in a dictionary. We further restrict the task to be a one-to-one normalization in which one OOV token is normalized to one IV word.

The remainder of this article is organized as follows. After reviewing relevant related work in Section 2, we offer a more formal definition of the task of lexical normalization in Section 3. In Section 4 we analyze a number of linguistic properties of SMS and Twitter messages to gain some insight into the task of lexical normalization. We then propose a context-sensitive method for lexical normalization, and further present a manually annotated lexical normalization dataset based on Twitter data. We compare the performance of our proposed normalization method to a number of baselines and benchmark approaches. In Section 5 we consider the task of lexical variant detection, an often overlooked aspect of lexical normalization. We propose and evaluate a token-based method for this task. Our findings indicate that poor performance on lexical variant detection leads to poor performance in real-world normalization tasks. In response to this finding, in Section 6 we propose a purely type-based dictionary-lookup approach to normalization focusing on context-insensitive lexical variants. It combines the detec-tion and normalization of lexical variants into a single step. In Section 7 we then show that this new approach achieves state-of-the-art performance. We consider the impact of lexical normalization on downstream processing in Section 8. In experiments on part-of-speech tagging, we find some evidence that prenormalization of the input text can lead to performance improvements. Finally, we offer some concluding remarks in Section 9.

This article is a revised and extended version of Han and Baldwin [2011] and Han et al. [2012]. This article further includes the following additional material, not included in either of those papers: (1) experiments on the impact of normalization in an applied setting (Section 8); (2) baselines for context-sensitive lexical normalization based on a Web-based language model and a spell checker (Section 4.4.1), and a discussion of the limitations of such approaches; (3) an analysis of the impact of lexical variants on context-sensitive normalization (Section 7.2.4). The noisy channel model [Shannon 1948] has traditionally been the primary approach to tackling text normalization. Given a token t , lexical normalization is the task of IV word. Standardly in lexical normalization, t is assumed to be an OOV token, relative to a fixed dictionary. In practice, not all OOV tokens should be normalized; that is, only lexical variants (e.g., tmrw  X  X omorrow X ) should be normalized and tokens that are OOV but otherwise not lexical variants (e.g., iPad  X  X Pad X ) should be unchanged. Most work in this area focuses only on the normalization task itself, oftentimes assuming that the task of lexical variant detection has already been completed.

Various approaches have been proposed to estimate the error model, P ( t | s ). For example, in work on spell checking, Brill and Moore [2000] improve on a standard edit-distance approach by considering multi-character edit operations; Toutanova and Moore [2002] build on this by incorporating phonological information. Li et al. [2006] utilize distributional similarity [Lin 1998] to correct misspelled search queries.
In text message normalization, Choudhury et al. [2007] model the letter transforma-tions and emissions using a hidden Markov model [Rabiner 1989]. Cook and Stevenson [2009] and Xue et al. [2011] propose multiple simple error models, each of which cap-tures a particular way in which lexical variants are formed, such as phonetic spelling (e.g., epik  X  X pic X ) or clipping (e.g., walkin  X  X alking X ). Nevertheless, optimally weighting the various error models in these approaches is challenging.
 Without precategorizing lexical variants into different types, Liu et al. [2011a] collect Google search snippets from carefully designed queries from which they then extract noisy lexical variant-standard form pairs. These pairs are used to train a conditional random field [Lafferty et al. 2001] to estimate P ( t | s ) at the character level. One short-coming of querying a search engine to obtain training pairs is it tends to be costly in terms of time and bandwidth. Here we exploit microblog data directly to derive (lexical variant , standard form) pairs, instead of relying on external resources. In more recent work, Liu et al. [2012] endeavor to improve the accuracy of top-n normalization candidates by integrating human cognitive inference, character-level transformations, and spell checking in their normalization model. The encouraging results shift the focus to reranking and promoting the correct normalization to the top-1 position. However, like much previous work on lexical normalization, this work assumes perfect lexical variant detection.

Aw et al. [2006] and Kaufmann and Kalita [2010] consider normalization as a ma-chine translation task using off-the-shelf tools. Essentially, this is also a noisy channel model, which regards lexical variants and standard forms as a source language and a target language, and translates from one to the other. The advantage of these methods is they do not assume that lexical variants have been preidentified; however, these methods do rely on large quantities of labeled training data, which is not available for microblogs. In this article, we extensively discuss lexical normalization methods for short text message, and compare our proposed methods with benchmarks using different evaluation metrics. Popular microblogs such as Twitter attract users from diverse language backgrounds, and are therefore highly multilingual. Preliminary language identification shows that English tweets account for less than 50% of the overall data. In this research, we focus exclusively on the English lexical normalization task, leaving the more general task of multilingual lexical normalization for future work.

We define the lexical normalization task as follows.  X  X nly OOV words are considered for normalization.  X  X ormalization must be to a single-token word, meaning that we would normalize smokin to smoking , but not imo to in my opinion ; a side-effect of this is to permit lower-register contractions such as gonna as the canonical form of gunna (given that going to is out of scope as a normalization candidate, on the grounds of being multitoken, and assuming that gonna is in-vocabulary).
 An immediate implication of our task definition is that lexical variants which happen to coincide with an IV word (e.g., can X  X  spelled as cant ) are outside the scope of this research. We also consider deabbreviation of acronyms and initialisms (e.g., imo  X  X n my opinion X ) to largely fall outside the scope of text normalization, as such abbreviated forms can be formed freely in standard English. Note that single-word abbreviations such as govt  X  X overnment X  are very much within the scope of lexical normalization, as they are OOV and correspond to a single token in their standard lexical form.
Given this definition, a necessary preprocessing step for lexical normalization is the identification of candidate tokens for normalization. Here we examine all tokens that consist of alphanumeric characters, and categorize them into IVs and OOVs, relative to a dictionary, and take the OOVs as candidates for lexical normalization. Note, however, that the OOVs will include lexical variants, but will also include other word types, such as neologisms and proper nouns, that happen to not be listed in the dictionary being used. One challenge for lexical normalization is therefore to distinguish between OOV words that should not be normalized (such as hopeable and WikiLeaks ,which are not included in the dictionary we use in our experiments) and lexical variants requiring normalization such as typos (e.g., earthquak  X  X arthquake X ), register-specific single-word abbreviations (e.g., lv  X  X ove X ), and phonetic substitutions (e.g., 2morrow  X  X omorrow X ). Note that many previous approaches to normalization X  X iscussed in Section 2 X  X ave made the simplifying assumption that lexical variants have already been identified. In this article, we begin by considering lexical normalization making this assumption in Section 4.2. We then address the issue of identifying lexical variants from amongst OOVs in Section 5, and show that this task is challenging, and that poor performance at this task negatively impacts overall normalization performance. In Section 6 we then propose a new dictionary-based approach to normalization that avoids such problems associated with lexical variant identification.

Throughout this article, we use the GNU aspell dictionary (v6.06) 3 to determine whether a token is OOV. In tokenizing the text, Twitter mentions (e.g., @twitter )hash-tags (e.g., #twitter ) and URLs (e.g., twitter.com ) are excluded from consideration for normalization, but left in situ for context modeling purposes. Dictionary lookup of Internet slang is performed relative to a dictionary of 5021 items collected from the Internet. 4 The language filtering of Twitter to automatically identify English tweets was based on the language identification method of Baldwin and Lui [2010], using the EuroGOV dataset as training data, a mixed unigram/bigram/trigram byte feature representation, and a skew divergence nearest prototype classifier. To get a sense of the relative need for lexical normalization, we perform an analysis of the distribution of OOV words in different text types. In particular, we calculate the proportion of OOV tokens per message (or sentence, in the case of edited text), bin the messages according to OOV token proportion, and plot the probability mass contained in each bin for a given text type. The three corpora we compare are the New York Times (NYT), 5 SMS, 6 and Twitter. 7 The results are presented in Figure 1.

Both SMS and Twitter have a relatively flat distribution, with Twitter having a particularly long tail: around 15% of tweets have 50% or more OOV tokens. Therefore, many OOV words in SMS and Twitter co-occur, and this makes context modeling difficult. In contrast, NYT shows a more Zipfian distribution, despite the large number of proper nouns it contains.

While this analysis confirms that Twitter and SMS are similar in being heavily laden with OOV tokens, it does not shed any light on the relative similarity in the makeup of OOV tokens in each case. To further analyze the two data sources, we extracted two lists of OOV terms X  X hose found exclusively in SMS, and those found only in Twitter X  and sorted each list by frequency. Manual analysis of high-frequency items in each list revealed that OOV words found only in SMS were largely personal names, while the Twitter-specific set, on the other hand, contained a more heterogeneous collection of OOVs including more lexical variants. Despite the difference in size of these datasets, this finding suggests that Twitter is a richer source of lexical variants, and hence a noisier data source, and that text normalization for Twitter needs to be more nuanced than for SMS.
To further analyze the lexical variants in Twitter, we randomly selected 449 tweets and manually analyzed the sources of lexical variation, to determine the phenomena that lexical normalization needs to deal with. We identified 254 token instances of lex-ical variants, and broke them down into categories, as listed in Table I.  X  X etter X  refers to instances where letters are missing or there are extraneous letters, but the lexical correspondence to the target word form is trivially accessible (e.g., shuld  X  X hould X ).  X  X umber Substitution X  refers to instances of letter-number substitution, where num-bers have been substituted for phonetically similar sequences of letters (e.g., 4  X  X or X ).  X  X etter&amp;Number X  refers to instances which have both extra/missing letters and num-ber substitution (e.g., b4  X  X efore X ).  X  X lang X  refers to instances of Internet slang (e.g., lol  X  X augh out loud X ), as found in a slang dictionary (see Section 3).  X  X ther X  is the remainder of the instances, which is predominantly made up of occurrences of spaces having been deleted between words (e.g., sucha  X  X uch a X ). 8 If a given instance belongs to multiple error categories (e.g.,  X  X etter&amp;Number X  and it is also found in a slang dic-tionary), we classify it into the higher-occurring category in Table I. Acknowledging other categorization methods based on lexical variant format process [Thurlow 2003; Cook and Stevenson 2009; Xue et al. 2011], our classification is shaped to coordinate the downstream normalization.
 From Table I, it is clear that  X  X etter X  accounts for the majority of lexical variants in Twitter, and that most lexical variants are based on morphophonemic variations. This empirical finding assists in shaping our strategy for lexical normalization. The proposed lexical normalization strategy involves two general steps: (1) confusion set generation, where we identify IV normalization candidates for a given lexical vari-ant type; (2) candidate selection, where we select the best standard form of the given lexical variant from the candidates generated in (1). 4.2.1. Confusion Set Generation. In generation of possible normalization candidates, the following steps are utilized. First, inspired by Kaufmann and Kalita [2010], any rep-etitions of more than 3 letters are reduced back to 3 letters (e.g., cooool is reduced to coool ). Second, IV words within a threshold of T c in terms of character edit distance of a given OOV word are considered, a heuristic widely used in spell checkers. Third, the double metaphone algorithm [Philips 2000] is used to decode the pronunciation of all IV words; IV words within an edit distance of T p of a given OOV word, under phonemic transcription, are included in the confusion set. This allows us to capture OOV words such as earthquick  X  X arthquake X . In Table II, we list the recall and average size of the confusion set generated by the final two strategies with different threshold settings, based on our evaluation dataset (see Section 4.3).

The recall for lexical edit distance with T c  X  2 is moderately high, but it is unable to detect the correct candidate for about one quarter of words. The combination of the lexical and phonemic strategies with T c  X  2  X  T p  X  2 is more impressive, but the number of candidates has also soared. Note that increasing the edit distance further in both cases leads to an explosion in the average number of candidates, and causes expensive computational cost. Furthermore, a smaller confusion set is easier for the downstream candidate selection as well. Thankfully, T c  X  2  X  T p  X  1 leads to an extra increment in recall to 88.8%, with only a slight increase in the average number of candidates. Based on these results, we use T c  X  2  X  T p  X  1 as the basis for confusion set generation.

In addition to generating the confusion set, we rank the candidates based on a trigram language model trained over 1.5GB of clean Twitter data, that is, tweets which consist of all IV words: despite the prevalence of OOV words in Twitter, the sheer volume of the data means that it is relatively easy to collect large amounts of all-IV messages. To train the language model, we used SRILM [Stolcke 2002] with the -&lt;unk&gt; option. We truncate the ranking to the top 10% of candidates in the experiments, the recall drops back to 84% with a 90% reduction in candidates. The confusion set generation process is summarized in Algorithm 1.

Examples of lexical variants where we are unable to generate the standard lexical form are clippings such as fav  X  X avorite X  and convo  X  X onversation X . 4.2.2. Candidate Selection. We select the most likely candidate from the previously generated confusion set as the basis of normalization using both lexical string similarity and contextual information. The information is combined linearly in line with previous work [Wong et al. 2006; Cook and Stevenson 2009].

Lexical edit distance, phonemic edit distance, prefix substring, suffix sub-string, and the Longest Common Subsequence (LCS) are exploited to capture morphophonemic similarity. Both lexical and phonemic Edit Distance (ED) are nonlin-early transformed to 1 exp ( ED ) so that smaller numbers correspond to higher similarity, as with the subsequence-based methods.

The prefix and suffix features are intended to capture the fact that leading and trailing characters are frequently dropped from words, for example, in cases such as gainst  X  X gainst X  and talkin  X  X alking X . We calculate the ratio of the LCS over the maximum string length between lexical variant and the candidate, since the lexical variant can be either longer or shorter than (or the same size as) the standard form. For example, mve can represent either me or move , depending on context. We normalize these ratios so that the sum over candidates for each measure is made to be 1, following Cook and Stevenson [2009].

For context inference, we employ both language-model-and dependency-based fre-quency features. Ranking by language model score is intuitively appealing for candi-date selection, but our trigram model is trained only on clean Twitter data and lexical variants often don X  X  have sufficient context for the language model to operate effec-tively, as in bt  X  X ut X  in say 2 sum1 bt nt gonna say  X  X ay to someone but not going to say X . To consolidate the context modeling, we obtain dependency features that are not restricted by contiguity.

First, we use the Stanford parser [Klein and Manning 2003; de Marneffe et al. 2006] to extract dependencies from the NYT corpus (see Section 4.1). For example, from a sentence such as One obvious difference is the way they look , we would ex-tract dependencies such as rcmod(way-6,look-8) and nsubj(look-8,they-7) .Wethen transform the dependencies into dependency features for each OOV word. Assuming that way were an OOV word, for example, we would extract dependencies of the form (look,way,+2) , indicating that look occurs 2 words after way . We choose dependencies to represent context because they are an effective way of capturing key relationships between words, and similar features can easily be extracted from tweets. Note that we don X  X  record the dependency type here, because we have no intention of dependency parsing text messages, due to their noisiness and the volume of the data. The counts of dependency forms are combined together to derive a confidence score, and the scored dependencies are stored in a dependency bank. 9
Although text messages are of a different genre to edited newswire text, we assume that words in the two genres participate in similar dependencies based on the common goal of getting across the message effectively. The dependency features can be used in noisy contexts and are robust to the effects of other lexical variants, as they do not rely on contiguity. For example, uz  X  X se X  in idid#ttuzmeandyu , dependencies can capture relationships like aux(use-4, do-2) , which is beyond the capabilities of the language model due to the hashtag being treated as a correct OOV word. The aim of our experiments is to compare the effectiveness of different methodologies over two datasets of short messages: (1) an SMS corpus [Choudhury et al. 2007]; and (2) a novel Twitter dataset developed as part of this research, based on a random sampling of 549 English tweets. The English tweets were annotated by three indepen-dent annotators. All OOV words were automatically preidentified, and the annotators were requested to determine: (a) whether each OOV word was a lexical variant or not; and (b) in the case of tokens judged as lexical variants, what the standard form was, subject to the task definition outlined in Section 3. The total number of lexical variants contained in the SMS and Twitter datasets were 3849 and 1184, respectively. 10
As discussed in Sections 2 and 3, much previous work on SMS data has assumed perfect lexical variant detection and focused only on the identification of standard forms. Here we also assume perfect detection of lexical variants in order to compare our proposed approach to previous methods. We consider token-level precision, recall, and F-score (  X  = 1), and also evaluate using BLEU [Papineni et al. 2002] over the normalized form of each message. We consider the latter measure because SMT-based approaches to normalization (which we compare our proposed method against) can lead to perturbations of the token stream, vexing evaluation using standard precision, recall, and F-score.
 4.4.1. Baselines. We compare our proposed approach to normalization to some off-the-shelf tools and simple methods. As the first baseline, we use the Ispell spell checker to correct lexical variants. 11 We also consider a Web-based language modeling approach to normalization. For a given lexical variant, we first use our method for candidate set generation (Section 4.2.1) to identify plausible normalization candidates. We then identify the lexical variant X  X  left and right context tokens, and use the Web 1T 5-gram corpus [Brants and Franz 2006] to determine the most frequent 3-gram (one word to each of the left and right of the lexical variant) or 5-gram (two words to each of the left and right). Lexical normalization takes the form of simply identifying the center word of the highest-frequency n -gram which matches the left/right context, and where the center word is a member of the lexical variant X  X  candidate set. Finally, we also consider a simple dictionary lookup method using the Internet slang dictionary (Section 3) where we substitute any usage of an OOV having an entry in the dictionary by its listed standard form. 4.4.2. Benchmarks. We further compare our proposed method against previous meth-ods, which we take as benchmarks. We reimplemented the state-of-art noisy channel model of Cook and Stevenson [2009] and the SMT approach of Aw et al. [2006], widely used in SMS normalization. We implement the SMT approach in Moses [Koehn et al. 2007], with synthetic training and tuning data of 90,000 and 1000 sentence pairs, respectively. This data is randomly sampled from the 1.5GB of clean Twitter data, and errors are generated according to the distribution of the SMS corpus. The 10-fold cross-validated BLEU score over this data is 0.81. In Table III, we compare our method ( DWC ) with the baselines and benchmarks dis-cussed before. Additionally, we determine the relative effectiveness of the component methods of our approach, namely Dictionary Lookup ( DL ), Word Similarity ( WS ), Context Support ( CS ), and combined Word similarity and Context support ( WC ). From Table III, we see that the general performance of our proposed method over Twitter is better than that over the SMS dataset. To better understand this, we ex-amined the annotations in the SMS corpus, and found them to be less conservative than ours, due to the different task specification. In our annotations, the annotators were instructed to only normalize lexical variants if they had high confidence of how to normalize, as with talkin  X  X alking X . For lexical variants where they couldn X  X  be certain of the standard form, the tokens were left untouched. However, in the SMS corpus, annotations such as sammis is mistakenly recognized as a variant of  X  X ame X , which actually represents a person name in the context. This leads to a performance drop for most of methods over the SMS corpus.

Among all the baselines in Table III, the Spell Checker ( SC ) performs marginally better than the language-model-based approaches ( LM3 and LM5 ), but is inferior to the dictionary lookup method, and receives the lowest BLEU score of all methods over the SMS dataset.

Both Web n -gram approaches are relatively ineffective at lexical normalization. The primary reason for this can be attributed to the simplicity of the context modeling. Comparing the different-order language models, it is evident that longer n -grams (i.e., more highly specified context information) support normalization with higher precision. Nevertheless, lexical context in Twitter data is noisy: many OOV words are surrounded by mentions of other Twitter users, hashtags, URLs, and other OOV words, which are uncommon in other text genres. In the Web n -gram approach, OOV words are mapped to the &lt;UNK&gt; flag in the Web 1T corpus construction process, leading to a loss of context information. Even the relaxed context constraints of the trigram method suffer from data sparseness, as indicated by the low recall. In fact, due to the temporal mismatch between the Web n -gram corpus (harvested in 2006) and the Twitter data (harvested in late 2010), lexical variant contexts are often missing in the Web n -gram data, limiting the performance of the Web n -gram model for normalization. Without the candidate filtering based on confusion sets, we observed that the Web n -gram approach generated fluent-sounding normalization candidates (e.g., back, over, in, soon, home ,and events )for tomoroe in coming tomoroe ( X  X oming tomorrow X ) but which lack semantic felicity with the original OOV word. This demonstrates the importance of candidate filtering as proposed.

The dictionary lookup method ( X  X L X ) unsurprisingly achieves the best precision, but the recall on Twitter is not competitive. Twitter normalization clearly cannot be tackled with such a small-scale dictionary lookup approach, although it is an effective prepro-cessing strategy when combined with other wider-coverage normalization approaches. Nevertheless, because of the very high precision of the dictionary lookup method, we will reconsider such an approach, but on a much larger scale, in Section 6.
The Noisy Channel method of Cook and Stevenson [2009] (NC) shares similar fea-tures with our Word Similarity method ( WS ). However, when Word similarity and Context support are combined ( WC ), our method outperforms NC by about 7% and 12% in F-score over the SMS and Twitter datasets, respectively. This can be explained as follows. First, NC is type based, so all token instances of a given lexical variant will have the same normalization. However, the same lexical variant can correspond to different IV words, depending on context, for example, hw  X  X ow X  in so hw many time remaining so I can calculate it? versus hw  X  X omework X  in I need to finish my hw first . Our word similarity method does not make the assumption that each lexical variant has a unique standard form. Second, NC was developed specifically for SMS normal-ization, based on observations about how lexical variants are typically formed in text messages, for example, clipping is quite frequent in SMS. In Twitter, word lengthening for emphasis, such as moviiie  X  X ovie X , is common, but this is not the case for text messages; NC therefore performs poorly on such lexical variants.

The SMT approach is relatively stable on the two datasets, but performs well below our method. This is due to the limitations of the training data: we obtain the lexical variants and their standard forms from the SMS corpus, but the lexical variants in the SMS corpus are not sufficient to cover those in the Twitter data (and we don X  X  have sufficient Twitter data to train the SMT method directly). Thus, novel lexical variants are not recognized and are therefore not normalized. This shows the shortcoming of supervised data-driven approaches that require annotated data to cover all possibilities of lexical variants in Twitter.

Of the component methods proposed in this research, Word Similarity ( WS ) achieves higher precision and recall than Context Support ( CS ), signifying that many of the lexical variants emanate from morphophonemic variations. However, when combined with context support, the performance improves over word similarity at a level of sta-tistical significance (based on randomized estimation, p &lt; 0 . 05: Yeh [2000]), indicating the complementarity of the two methods, especially on Twitter data. The best F-score is achieved when combining Dictionary lookup, Word similarity, and Context support ( DWC ), in which lexical variants are first looked up in the slang dictionary, and only if no match is found do we apply our normalization method.

As is common in research on text normalization [Gouws et al. 2011; Liu et al. 2012], throughout this section we have assumed perfect detection of lexical variants. This is, of course, not practical for real-world applications, and in the following section we consider the task of identifying lexical variants. A real-world end-to-end normalization solution must be able to identify which tokens are lexical variants and require normalization. In this section, we explore a context-fitness-based approach for lexical variant detection. The task is to determine whether a given OOV word in context is a lexical variant or not, relative to its confusion set. To the best of our knowledge, we are the first to target the task of lexical variant detection in the context of short text messages, although related work exists for text with lower relative occurrences of OOV words [Izumi et al. 2003; Sun et al. 2007]. Due to the noisiness of the data, it is impractical to use full-blown syntactic or semantic features. The most direct source of evidence is IV words around an OOV word. Inspired by work on labeled sequential pattern extraction [Sun et al. 2007], we exploit dependency-based features generated in Section 4.2.1.

To judge context fitness, we first train a linear kernel SVM classifier [Fan et al. 2008] on clean Twitter data, that is, the subset of Twitter messages without OOV words (discussed in Section 4.2.1). Each target word is represented by a vector with dimensions corresponding to the IV words within a context window of three words to either side of the target, together with their relative positions in the form of (word1,word2,position) tuples, and with the feature value for a particular dimension set to the score for the corresponding tuple in the dependency bank. These vectors form the positive training exemplars. Negative exemplars are automatically constructed by replacing target words with highly ranked candidates from their confusion set. For example, we extract a positive instance for the target word book with a dependency feature corresponding to the the tuple (book,hotel,  X  2 ) . A highly ranked confusion of book is hook . We therefore form a negative instance for hook with a feature for the tuple (hook,hotel,  X  2 ) . In training, it is possible for the exact same feature vector to occur as both positive and negative exemplars. To prevent the positive exemplars from becoming contaminated through the automatic negative-instance generation, we remove all negative instances in such cases. The (word1,word2,position) features are sparse and sometimes lead to conservative results in lexical variant detection. That is, without valid features, the SVM classifier tends to label uncertain cases as correct (i.e., not requiring normalization) rather than as lexical variants. This is arguably the right approach to normalization, in choosing to under-rather than overnormalize in cases of uncertainty. This artificially generated data is not perfect; however, this approach is appealing because the classifier does not require any manually annotated data, as all training exemplars are constructed automatically.

To predict whether a given OOV word is a lexical variant, we form a feature vector as earlier for each of its confusion candidates. If the number of the OOV X  X  candidates predicted to be positive by the model is greater than a threshold t d , we consider the OOV to be a lexical variant; otherwise, the OOV is not deemed to be a lexical variant. We experiment with varying settings of t d  X  X  1 , 2 ,..., 10 } . Note that in an end-to-end normalization system, for an OOV predicted to be a lexical variant, we would pass all its confusion candidates (not just those classified positively) to the candidate selection step; however, the focus of this section is only on the lexical variant detection task.

As the context for a target word often contains OOV words which don X  X  occur in the dependency bank, we expand the dependency features to include context tokens up to a phonemic edit distance of 1 from context tokens in the dependency bank. In this way, dependency-based features tolerate the noisy context word, for example, given a lexical variant seee , its confusion candidate  X  X ee X  can form (see, film, +2) in film to seeee , but not (see, flm, +2) . If we tolerate the context word variations assuming flm is  X  X ilm X , (see, flm, +2) would be also counted as (see, film, +2) .

However, expanded dependency features may introduce noise, and we therefore in-troduce expanded dependency weights w d  X  X  0 . 0 , 0 . 5 , 1 . 0 } to ameliorate the effects of noise: a weight of w d = 0 . 0 means no expansion, while 1.0 means expanded dependen-cies are indistinguishable from nonexpanded (strict match) dependencies.

We test the impact of the w d and t d values on lexical variant detection effectiveness for Twitter messages, based on dependencies from either the NYT, or the Spinn3r blog corpus (Blog: Burton et al. [2009]), a large corpus of blogs which we also processed and parsed like Twitter data. The results for precision, recall, and F-score are presented in Figure 2.

Some conclusions can be drawn from the graphs. First, higher detection threshold values ( t d ) give better precision but lower recall. Generally, as t d is raised from 1 to 10, the precision improves slightly but recall drops dramatically, with the net effect that the F-score decreases monotonically. Thus, we use a smaller threshold, that is, t d = 1. Second, there are differences between the two corpora, with dependencies from the Blog corpus producing slightly lower precision but higher recall, compared with the NYT corpus. The lower precision for the Blog corpus appears to be due to the text not being as clean as NYT, introducing parser errors. Nevertheless, the difference between the two corpora with the best F-score is slight (when t d = 1and w d = 0 . 5onBlog corpus). The lexical variant proportion among all OOV words in the Twitter dataset is 55%. Overall, the best F-score is 71.2%, with a precision of 61.1% and recall of 85.3%. Clearly there is significant room for improvements in these results. One quick solution is to find as many named entities as possible to filter out the nonlexical variant OOV words. Owning to the extensive editing, sheer volume of data and up-to-date content, we choose Wikipedia article titles as a source of nonlexical variants which contains many named entities. However, the results in preliminary experiments with this data source do not lead to any improvement. By analyzing the results, we found that terms from Wikipedia article titles are inappropriate for our task because they include many lexical variants such as u and hw which cause a decrease in recall.

We further consider the performance of a lexical variant detection method based on the Internet slang dictionary. In particular, if an OOV type has an entry in this dictionary, we consider all token instances of that type to be lexical variants; if a type is not in this dictionary, instances of that type are considered to be nonlexical variant OOVs. This very simple method achieves precision, recall, and F-score of 95.2%, 45.3%, and 61.4%, respectively. Although the performance of this dictionary-based method is substantially below that of our best-performing method, we are encouraged by the very high precision of this method, particularly because of the previously noted importance of not overnormalizing.

The challenges of lexical variant detection, along with the high precision of dictionary-based methods at both lexical normalization and lexical variant detection, lead us to consider an alternative dictionary-based approach to normalization. As discussed in Section 4.5, dictionary lookup approaches to normalization have been shown to have high precision but low recall. Frequent (lexical variant , standard form) pairs such as ( u , you ) are typically included in the dictionaries used by such methods, while less frequent items such as ( g0tta , gotta ) are generally omitted. Because of the degree of lexical creativity and large number of nonstandard forms observed on Twitter, a wide-coverage normalization dictionary would be expensive to construct manually. Based on the assumption that lexical variants occur in similar contexts to their stan-dard forms, however, it should be possible to automatically construct a normalization dictionary with wider coverage than is currently available.

Dictionary lookup is a type-based approach to normalization, that is, every token instance of a given type will always be normalized in the same way. However, lexical variants can be ambiguous, for example, y corresponds to  X  X ou X  in yeah, y r right! LOL but  X  X hy X  in AM CONFUSED!!! y you did that? . Nevertheless, the relative occurrence of ambiguous lexical variants is small [Liu et al. 2011], and it has been observed that while shorter variants such as y are often ambiguous, longer variants tend to be unambiguous. For example bthday and 4eva are unlikely to have standard forms other than  X  X irthday X  and  X  X orever X , respectively. Therefore, the normalization lexicons we produce will only contain entries for OOVs with character length greater than a specified threshold, which are likely to have an unambiguous standard form.
Recently, Gouws et al. [2011] produced a small normalization lexicon based on dis-tributional similarity and string similarity [Lodhi et al. 2002]. Our method adopts a similar strategy using distributional/string similarity, but instead of constructing a small lexicon for preprocessing, we build a much wider-coverage normalization dictio-nary and opt for a fully lexicon-based end-to-end normalization approach. In contrast with the normalization dictionaries which focus on very frequent lexical variants, we focus on moderate-frequency lexical variants of a minimum character length, which tend to have unambiguous standard forms; our intention is to produce normalization lexicons that are complementary to those currently available. Furthermore, we investi-gate the impact of a variety of contextual and string similarity measures on the quality of the resulting lexicons. In summary, our dictionary-based normalization approach is a lightweight end-to-end method which performs both lexical variant detection and normalization, and thus is suitable for practical online preprocessing, despite its simplicity. Our method for constructing a normalization dictionary is as follows: Input : Tokenized English tweets (1) Extract (OOV , IV) pairs based on distributional similarity. (2) Rerank the extracted pairs by string similarity.
 Output : A list of (OOV , IV) pairs ordered by string similarity; select the top-n pairs for inclusion in the normalization lexicon.

In step 1, we leverage large volumes of Twitter data to identify the most distri-butionally similar IV type for each OOV type. The result of this process is a set of (OOV , IV) pairs, ranked by distributional similarity. The extracted pairs will include (lexical variant , standard form) pairs, such as ( tmrw , tomorrow ), but will also contain false positives such as ( Tusday , Sunday ) X  Tusday is a lexical variant, but its standard form is not  X  X unday X  X  X nd ( Youtube , web ) X  Youtube is an OOV named entity, not a lexical variant. Nevertheless, lexical variants are typically formed from their standard forms through regular processes [Thurlow 2003; Cook and Stevenson 2009; Xue et al. 2011] X  X or example, the omission of characters X  X nd from this perspective, Sunday and Web are not plausible standard forms for Tusday and Youtube , respectively. In step 2, we therefore capture this intuition in reranking the extracted pairs by string similarity. The top-n items in this reranked list then form the normalization lexicon, which is based only on development data.
 Although computationally expensive to build, this dictionary can be created offline. Once built, it then offers a very fast approach to normalization.

We can only reliably compute distributional similarity for types that are moderately frequent in a corpus. Nevertheless, many lexical variants are sufficiently frequent to be able to compute distributional similarity, and can potentially make their way into our normalization lexicon. This approach is not suitable for normalizing low-frequency lexical variants, nor is it suitable for shorter lexical variant types which X  X s discussed in Section 6 X  X re more likely to have an ambiguous standard form. Nevertheless, previously proposed normalization methods that can handle such phenomena also rely in part on a normalization lexicon. The normalization lexicons we create can therefore be easily integrated with previous approaches to form hybrid normalization systems. Our objective is to extract contextually similar (OOV , IV) pairs from a large-scale collection of microblog data. Fundamentally, the surrounding words define the primary context, but there are different ways of representing context and different similarity measures we can use, which may influence the quality of generated normalization pairs.

Intuitively, distributional similarity measures the context proximity of two words in a corpus, as follows: (1) represent a word X  X  context by its surrounding words in a (large) feature vector. Each entry in the vector represents a particular word, usually in the form of a word frequency. (2) calculate the similarity between two context vectors based on some distance/similarity measure. For instance, tmrw and tomorrow in the following tweets share a number of context words in the vector, like see , you ,and school . This suggests they are distributionally similar.  X  X  don X  X  wanna go to school tmrw  X  X kay off to work now . paipai . see you guys tmrw (:  X  X o school tomorrow or Tuesday woot!!!  X  X h i can X  X  wait to see you tomorrow In representing the context, we experimentally explore the following factors: (1) context window size (from 1 to 3 tokens on both sides); (2) n -gram order of the context tokens (unigram, bigram, trigram); (3) whether context words are indexed for relative position or not; and (4) whether we use all context tokens, or only IV words. Because high-accuracy linguistic processing tools for Twitter are still under exploration [Liu et al. 2011; Gimpel et al. 2011; Ritter et al. 2011; Foster et al. 2011], we do not consider richer representations of context, for example, incorporating information about part-of-speech tags or syntax. We also experiment with a number of simple but widely used geometric-and information-theoretic distance/similarity measures. In particular, we use Kullback X  X eibler (KL) divergence [Kullback and Leibler 1951], Jensen X  X hannon (JS) divergence [Lin 1991], Euclidean distance, and cosine distance.

We use a corpus of 10 million English tweets to do parameter tuning over, and a larger corpus of tweets in the final candidate ranking. All tweets were collected from September 2010 to January 2011 via the Twitter API. 12 From the raw data we extract English tweets using an improved language identification tool [Lui and Baldwin 2011], 13 and then apply a simplified Twitter tokenizer (adapted from O X  X onnor et al. [2010]). We use the Aspell dictionary (v6.06) 14 to determine whether a word is IV, and only include in our normalization dictionary OOV tokens with at least 64 occurrences in the corpus and character length  X  4, both of which were determined through empirical observation. For each OOV word type in the corpus, we select the most similar IV type to form (OOV , IV) pairs. To further narrow the search space, we only consider IV words which are morphophonemically similar to the OOV type, based on parameter tuning from Section 4.2.1 over the top-30% of most frequent IV words in the confusion set.
In order to evaluate the generated pairs, we randomly selected 1000 OOV words from the 10-million tweet corpus. We set up an annotation task on Amazon Mechanical Turk, 15 presenting five independent annotators with each word type (with no context) and asking for corrections where appropriate. For instance, given tmrw , the annotators would likely identify it as a nonstandard variant of  X  X omorrow X . For correct OOV words like Wikileaks , on the other hand, we would expect them to leave the word unchanged. If 3 or more of the 5 annotators make the same suggestion (in the form of either a canonical spelling or leaving the word unchanged), we include this in our gold standard for evaluation. In total, this resulted in 351 lexical variants and 282 correct OOV words, accounting for 63.3% of the 1000 OOV words. These 633 OOV words were used as (OOV , IV) pairs for parameter tuning. The remainder of the 1000 OOV words were ignored on the grounds that there was not sufficient consensus amongst the annotators. 16
Contextually similar pair generation aims to include as many correct normalization pairs as possible. We evaluate the quality of the normalization pairs using Cumulative Gain (CG).
 the frequency of oo v i to indicate its relative importance; for example, ( thinkin , thinking ) has a higher weight than ( g0tta , gotta ) because thinkin is more frequent than g0tta in our corpus. In this evaluation we don X  X  consider the position of normalization pairs, and nor do we penalize incorrect pairs. Instead, we push distinguishing between correct and incorrect pairs into the downstream reranking step in which we incorporate string similarity information.

Given the development data and CG, we run an exhaustive search of parameter combinations over our development corpus. The five best parameter combinations are shown in Table IV. We notice the CG is almost identical for the top combinations. As a context window size of 3 incurs a heavy processing and memory overhead over a size of 2, we use the 3rd-best parameter combination for subsequent experiments, namely: context window of  X  2 tokens, token bigrams, positional index, and KL divergence as our distance measure.
To better understand the sensitivity of the method to each parameter, we perform a post hoc parameter analysis relative to a default setting (as underlined in Table V), altering one parameter at a time. The results in Table V show that bigrams outperform other n -gram orders by a large margin (note that the evaluation is based on a log scale), and information-theoretic measures are superior to the geometric measures. Further-more, it also indicates using the positional indexing better captures context. However, there is little to distinguish context modeling with just IV words or all tokens. Simi-larly, the context window size has relatively little impact on the overall performance, supporting our earlier observation from Table IV. Once the contextually similar (OOV , IV) pairs are generated using the selected pa-rameters in Section 6.3, we further rerank this set of pairs in an attempt to boost morphophonemically similar pairs like ( bananaz , bananas ), and penalize noisy pairs like ( paninis , beans ).

Instead of using the small 10-million tweet corpus, from this step onwards, we use a larger corpus of 80 million English tweets (collected over the same period as the development corpus) to develop a larger-scale normalization dictionary. This is because once pairs are generated, reranking based on string comparison is much faster. We only include in the dictionary OOV words with a token frequency &gt; 15 to include more OOV types than in Section 6.3, and again apply a minimum length cutoff of 4 characters.
To measure how well our reranking method promotes correct pairs and demotes incorrect pairs (including both OOV words that should not be normalized, e.g., ( Youtube , web ), and incorrect normalizations for lexical variants, e.g., ( bcuz , cause )), we modify our evaluation metric from Section 6.3 to evaluate the ranking at different points, using Discounted Cumulative Gain (DCG@ N : Jarvelin and Kekalainen [2002]). We have where rel i again represents the frequency of the OOV, but it can be a gain (a positive number) or loss (a negative number), depending on whether the i th pair is correct or incorrect. Because we also expect correct pairs to be ranked higher than incorrect pairs, DCG@ N takes both factors into account.

Given the generated pairs and the evaluation metric, we first consider three base-lines: no reranking (i.e., the final ranking is that of the contextual similarity scores), and rerankings of the pairs based on the frequencies of the OOVs in the Twitter corpus, and the IV unigram frequencies in the Google Web 1T corpus [Brants and Franz 2006] to get less noisy frequency estimates. We also compared a variety of rerankings based on a number of string similarity measures that have been previously considered in normalization work (reviewed in Section 2). We experiment with a series of string sim-ilarity measures: standard edit distance [Levenshtein 1966], which is the minimum number of character-level insertions/deletions/substitutions to transform one string to another (from a lexical variant to its normalization in this article); edit distance over double metaphone codes (phonetic edit distance: [Philips 2000]); longest common subsequence ratio over the consonant edit distance of the paired words (hereafter, de-noted as consonant edit distance: [Contractor et al. 2010]); a string subsequence kernel [Lodhi et al. 2002], which measures common character subsequences of length n be-tween (OOV , IV) pairs. Because it is computationally expensive to calculate similarity for larger n , we choose n = 2, following Gouws et al. [2011].

In Figure 3, we present the DCG@ N results for each of our ranking methods at different rank cutoffs. Ranking by OOV frequency is motivated by the assumption that lexical variants are frequently used by social media users. This is confirmed by our findings that lexical pairs like ( goin , going )and( nite , night ) are at the top of the ranking. However, many proper nouns and named entities are also used frequently and ranked at the top, mixed with lexical variants like ( Facebook , speech ) and ( Youtube , web ). In ranking by IV word frequency, we assume the lexical variants are usually derived from frequently used IV equivalents, for example, ( abou , about ). However, many less frequent lexical variant types have high-frequency (IV) normal-izations. For instance, the highest-frequency IV word the has more than 40 OOV lexical variants, such as tthe and thhe . These less-frequent types occupy the top positions, reducing the cumulative gain. Compared with these two baselines, ranking by default contextual similarity scores delivers promising results. It successfully ranks many more intuitive normalization pairs at the top, such as ( 2day , today )and ( wknd , weekend ), but also ranks some incorrect pairs highly, such as ( needa , gotta ). The string-similarity-based methods perform better than our baselines in general. Through manual analysis, we found that standard edit distance ranking is fairly ac-curate for lexical variants with low edit distance to their standard forms, for example, ( thinkin , thinking ). Because this method is based solely on the number of character edits, it fails to identify heavily altered variants like ( tmrw , tomorrow ). Consonant edit distance favors pairs with longer common subsequences, and therefore places many longer words at the top of the ranking. Edit distance over double metaphone codes (phonetic edit distance) performs particularly well for lexical variants that include character repetitions X  X ommonly used for emphasis on Twitter X  X ecause such repeti-tions do not typically alter the phonetic codes. Compared with the other methods, the string subsequence kernel delivers encouraging results. As N (the lexicon size cutoff) increases, the performance drops more slowly than the other methods. Although this method fails to rank heavily altered variants such as ( 4get , forget ) highly, it typically works well for longer words. Given that we focus on longer OOVs (specifically those longer than 4 characters), this ultimately isn X  X  a great handicap. Given the reranked pairs from Section 6.4, here we apply them to a token-level nor-malization task, once again using the normalization dataset from Section 4.3. We use the same standard evaluation metrics of precision (P), recall (R), and F-score (F) as detailed in Section 4.3. In addition, we also consider the False Alarm Rate (FA) and Word Error Rate (WER), as shown shortly. FA measures the negative effects of applying normalization: a good approach to normalization should not (incorrectly) normalize tokens that are already in their standard form and do not require normalization. 17 WER, like F-score, shows the overall benefits of normalization, but unlike F-score, measures how many token-level edits are required for the output to be the same as the ground-truth data. In general, dictionaries with a high F-score/low WER and low FA are preferable.
 We select the three best reranking methods, and best cutoff N for each method, based on the highest DCG@ N value for a given method over the development data, as presented in Figure 3. Namely, they are string subsequence kernel (S-dict, N = 40,000), double metaphone edit distance (DM-dict, N = 10,000), and default contextual similarity without reranking (C-dict, N = 10,000). 18
We evaluate each of the learned dictionaries in Table VI. We also compare each dictionary with the performance of the manually constructed Internet slang dictionary (HB-dict) used in Section 4.5, the small automatically derived dictionary of Gouws et al. [2011] (GHM-dict), and combinations of the different dictionaries. In addition, the contribution of these dictionaries in hybrid normalization approaches is presented, in which we first normalize OOVs using a given dictionary (combined or otherwise), and then apply the normalization method of Gouws et al. [2011] based on consonant edit distance (GHM-norm), or the approach of Han and Baldwin [2011] based on the summation of many unsupervised approaches (HB-norm), to the remaining OOVs. Results are shown in Table VI, and discussed next. 7.2.1. Individual Dictionaries. Overall, the individual dictionaries derived by the rerank-ing methods (DM-dict, S-dict) perform better than those based on contextual similarity (C-dict) in terms of precision and false alarm rate, indicating the importance of rerank-ing. Even though C-dict delivers higher recall X  X ndicating that many lexical variants are correctly normalized X  X his is offset by its high false alarm rate, which is par-ticularly undesirable in normalization. Because S-dict has better performance than DM-dict in terms of both F-score and WER, and a much lower false alarm rate than C-dict, subsequent results are presented using S-dict only.
 Both HB-dict and GHM-dict achieve better than 90% precision with moderate recall. Compared to these methods, S-dict is not competitive in terms of either precision or recall. This result seems rather discouraging. However, considering that S-dict is an automatically constructed dictionary targeting lexical variants of varying frequency, it is not surprising that the precision is worse than that of HB-dict X  X hich is manually constructed X  X nd GHM-dict X  X hich includes entries only for more frequent OOVs for which distributional similarity is more accurate. Additionally, the recall of S-dict is hampered by the restriction on lexical variant token length of 4 characters. 7.2.2. Combined Dictionaries. Next we look to combining HB-dict, GHM-dict, and S-dict. In combining the dictionaries, a given OOV word can be listed with dif-ferent standard forms in different dictionaries. In such cases we use the following preferences for dictionaries X  X otivated by our confidence in the normalization pairs of the dictionaries X  X o resolve conflicts: HB-dict &gt; GHM-dict &gt; S-dict.

When we combine dictionaries in the second section of Table VI, we find that they con-tain complementary information: in each case the recall and F-score are higher for the combined dictionary than any of the individual dictionaries. The combination of HB-dict + GHM-dict produces only a small improvement in terms of F-score over HB-dict (the better-performing dictionary) suggesting that, as claimed, HB-dict and GHM-dict share many frequent normalization pairs. HB-dict + S-dict and GHM-dict + S-dict, on the other hand, improve substantially over HB-dict and GHM-dict, respectively, indi-cating that S-dict contains markedly different entries to both HB-dict and GHM-dict. The best F-score and WER are obtained using the combination of all three dictionaries, HB-dict + GHM-dict + S-dict. Furthermore, the difference between the results using HB-dict + GHM-dict + S-dict and HB-dict + GHM-dict is statistically significant ( p &lt; 0 . 01), based on the computationally intensive Monte Carlo method of Yeh [2000], demon-strating the contribution of S-dict. 7.2.3. Hybrid Approaches. So far, we have discussed using approaches using context-sensitive normalization (Section 4.2) and dictionary-based type normalization (Sec-tion 6). The methods of Gouws et al. [2011] (i.e., GHM-dict + GHM-norm) and our proposed token-based hybrid approach (i.e., HB-dict + HB-norm) have lower precision and higher false alarm rates than the dictionary-based approaches; this is largely caused by lexical variant detection errors. When doing such comparisons, we report results that do not assume perfect detection of lexical variants, unlike the original pub-lished results in each case. Using all dictionaries in combination with these methods X  HB-dict + GHM-dict + S-dict + GHM-norm and HB-dict + GHM-dict + S-dict + HB-norm X  gives some improvements, but the false alarm rates remain high. A larger dictionary helps in improving the F-score and reducing the WER. 7.2.4. Impact of Context. As mentioned in Section 4.5, the disappointing performance of context features is partially attributable to noisy contexts, as neighboring lexical variants mutually reduce the usable context of each other. To counter this effect, we apply context-sensitive token-based normalization on the basis of the already partially normalized text (through our best dictionary) and compare its performance with token-based normalization using the original unnormalized text, as shown in the last two rows of Table VI. This quantifies the relative impact of dictionary-based prenormalization on context-sensitive normalization.
 The results indicate that partial prenormalization has only a very slight effect. Analysis of the two methods led to the finding that only 45 tokens were altered by the context-sensitive normalization. That is, most lexical variants are already normalized by the lexicon in prenormalization, and it is not surprising that the context-sensitive lexical normalization step had little impact.

We further analyzed the 45 instances which the context-sensitive normalization modified, and found that cleaned text does indeed help in context-sensitive normaliza-tion, as shown in Table VII. When presented with the noisy context sorryy , the lexical variant im is incorrectly normalized to him , however, when the context is cleaned X  that is, sorryy is restored to sorry  X  im is correctly normalized to ( X  X  X  X  X ), as both the language-model-based and dependency-based context feature strongly support the us-age of  X  X  X  X  sorry X . Other encouraging cases where prenormalization with the dictionary aids context-sensitive normalization are shown in Table VII. In the bulk of cases, how-ever, the updated context led to a different, but still incorrect, normalization candidate, as compared to the simple case of no prenormalization. Clearly, therefore, more work can be done at the interface between the two methods.

Despite the limitations of a pure dictionary-based approach to normalization X  discussed in Section 6.2 X  X he current best practical approach to normalization is to use a lexicon, combining hand-built and automatically learned normalization dictionaries. We first manually analyze the errors in the combined dictionary (HB-dict + GHM-dict + S-dict) and give examples of each error type in Table VIII. The most frequent word errors are caused by slight morphological variations, including plural forms (a), negations (b), possessive cases (c), and OOVs that are correct and do not require nor-malization (d). In addition, we also notice some missing annotations where lexical variants are skipped by human annotations but captured by our method (e). Ambigu-ity (f) definitely exists in longer OOVs, however, these cases do not appear to have a strong negative impact on the normalization performance. An example of a remaining miscellaneous error is bday  X  X irthday X , which is misnormalized as day .

To further study the influence of OOV word length relative to the normalization performance, we conduct a fine-grained analysis of the performance of the derived dictionary (S-dict) in Table IX, broken down across different OOV word lengths. The results generally support our hypothezis that our method works better for longer OOV words. The derived dictionary is much more reliable for longer tokens (length 5, 6, and 7 characters) in terms of precision and false alarm. Although the recall is relatively modest, in the future we intend to improve recall by mining more normalization pairs from larger collections of microblog data.
 Having proposed a number of approaches to lexical normalization, and evaluating those methods directly, we now evaluate the impact of normalization in an applied setting. When NLP tools trained on more conventional text are applied to social media, their performance is hampered in part due to the presence of lexical variants (as discussed in Section 1). We therefore hypothesize that the performance of such tools might improve if lexical normalization is applied after tokenization, and before subsequent processing.
In this section we test the aforesaid hypothesis on a Twitter Part-Of-Speech (POS) tagging task. We choose POS tagging for the following reasons: (1) the impact of lexical normalization is readily observed, as it is easy to compare the POS tags for the original and normalized texts; (2) off-the-shelf part-of-speech taggers are available for both more conventional text [Toutanova et al. 2003] and social media [Gimpel et al. 2011]; (3) a human-annotated Twitter POS tagging dataset is publicly available [Gimpel et al. 2011].

The part-of-speech tagging dataset of Gimpel et al. [2011] consists of 1827 tokenized and annotated messages from Twitter. 19 500 messages X  X eferred to as the test set X  are held out for test purposes, with the rest of the data being used for training and development, as described in Gimpel et al. [2011]. For each message in the test set, we apply the best-performing dictionary-based normalization method from Section 7.2, namely HB-dict + GHM-dict + S-dict. However, when substituting words, we also con-sider information about the case of the original tokens, as this information is known to be important for Twitter POS tagging [Gimpel et al. 2011]. Specifically, the case of the first and last characters of the normalized word form are set to the case of the first and last characters of the original token, respectively. All the other characters of the normalized form are set to the case of the middle character of the original token. For example, Todei , WKEND ,and tmrw are normalized as  X  X oday X ,  X  X EEKEND X , and  X  X omorrow X , respectively.

We compare the performance of the Twitter-specific POS tagger ( X  X OS twitter  X ) to that of a standard off-the-shelf tool, the Stanford POS tagger ( X  X OS Stanford  X : Toutanova et al. [2003]). However, these taggers use different tagsets: POS twitter uses a much more coarse-grained tagset than the Penn Treebank POS tagset that is used by POS Stanford . We are interested in the performance of a conventional off-the-shelf tool, and therefore do not retrain POS Stanford on the POS-annotated tweets. Instead, we manually devised a lossy mapping from the fine-grained POS Stanford tagset to that of POS twitter .Inthis mapping, finer-grained tags unique to POS Stanford (e.g., VBP and VBN ) are mapped to coarser-grained POS twitter tags (e.g., V ). 20
We apply POS twitter and POS Stanford to the test set, both with and without first applying normalization. We use accuracy to measure performance, excluding tokens in the gold standard with a Twitter-specific POS tag from the calculation (as there is no way of reliably mapping onto them from the Penn POS tagset). Results are shown in Table X. First, we compare the performance of POS Stanford on the original tweets to its performance on the normalized tweets. The accuracy on normalized text is 1.6 percentage points higher than that on the original text. In total, 108 more tokens are correctly tagged when lexical normalization is used. We observe that most of this improvement is for nouns and verbs. Although the improvement in performance is small, it is statistically significant ( p &lt; 0 . 01) using the method of Yeh [2000]. Furthermore, only 275 tokens in the test set are normalized (i.e., the remaining tokens are unchanged through normalization). It could be the case that more normalization would lead to a greater improvement in POS tagging performance for noisier text containing more lexical variants.

We further consider the impact of normalization on POS twitter , the Twitter-specific tagger. In this case the performance on prenormalized tweets drops slightly over that for the original messages, indicating that normalizing the input hurts the performance of POS twitter . This is somewhat expected because some features used by POS twitter are derived from noisy tokens: when the input is normalized, some of these features are not present, for example, features capturing cooccurrence with lexical variants.
In summary, these preliminary comparisons show the influence of normalization on the task of POS tagging for Twitter. In terms of cost, using a conventional off-the-shelf tool (e.g., POS Stanford ) with normalization is the least expensive option, and would ob-viate the need for the development of a Twitter-specific tool such as POS twitter .Not surprisingly, building a tool specific to the target domain yields the best performance. However, this comes at the substantial overhead of developing a specific tagset, man-ually annotating training data, and developing the tagger itself. In this article, we have proposed the task of normalizing lexical variants to their canonical forms for short text messages in social media, for example, Twitter and SMS data. We first analyze the in-domain OOV word types and distributions, and propose a detection-and-normalization approach using both contextual and string similarity information. The proposed method beats other benchmark methods in token-based normalization, however, most of these conventional methods suffer from the poor per-formance of lexical variant detection, which makes them less practical in real normal-ization. Encouraged by the dictionary-based normalization performance, we moved on to use contextual/string similarity information to build a type-based normaliza-tion lexicon with particular focus on context-insensitive lexical variants. Although the proposed type-based method has the limitation that it cannot capture context or dis-ambiguate different usages of the same token, in empirical evaluation, we showed it to achieve state-of-the-art accuracy, with broader coverage than existing dictionaries and reasonable precision. Furthermore, this dictionary-lookup approach combines the de-tection and normalization of lexical variants into a simple, lightweight solution which is suitable for processing high-volume microblog feeds.

Given the encouraging results of the dictionary-based method, we primarily intend to improve the quantity and quality of the mined lexicon in future work, for example, (1) enriching our dictionary entries by leveraging the constantly growing volume of microblog data; and (2) exploring alternative ways to combine distributional and string similarity for better-quality lexical candidate generation. Ultimately, context-sensitive normalization is much more powerful than the lexicon-based approach, and we also plan to explore context-sensitive methods for token-based normalization given different potential normalizations. We only consider one-to-one token-level normalization in this article, and plan to further expand the scope of the task to include many-to-many token normalization.

