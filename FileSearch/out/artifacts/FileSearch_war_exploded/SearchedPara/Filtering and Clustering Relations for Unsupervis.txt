 Information Extraction has recently been extended to new areas by loosening the constraints on the strict definition of the extracted information and allowing to design more open information extraction systems. In this new domain of unsupervised information extraction, we focus on the task of extracting and characterizing a priori unknown relations between a given set of entity types. One of the challenges of this task is to deal with the large amount of candidate relations when extracting them from a large corpus.

We propose in this paper an approach for the filtering of such candidate relations based on heuristics and machine learning models. More precisely, we show that the best model for achieving this task is a Conditional Random Field model according to evaluations performed on a manually an-notated corpus of about one thousand relations. We also tackle the problem of identifying semantically similar rela-tions by clustering large sets of them. Such clustering is achieved by combining a classical clustering algorithm and a method for the efficient identification of highly similar re-lation pairs. Finally, we evaluate the impact of our filtering of relations on this semantic clustering with both internal measures and external measures. Results show that the fil-tering procedure doubles the recall of the clustering while keeping the same precision.
 H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval X  clustering, information filtering, selection process Algorithms, Experimentation Unsupervised information extraction, filtering, machine learn-ing, clustering
Traditionally, Information Extraction was considered from the viewpoint of the MUC (Message Understanding Confer-ences) paradigm [11]. According to this view, its objective is to extract pieces of information from texts for filling with a fixed role a predefined template. More recently, new forms of information extraction have been developed under the gen-eral idea of having more flexible ways to specify the infor-mation to extract from texts. Such information is generally defined as a configuration of relations between entities 1 each relation being defined either as a handcrafted model (frequently a set of rules) or by a set of examples of rela-tions in context that are used to train a statistical model. For an event such as an earthquake for instance, the extrac-tion typically focuses on its location, its date, its magnitude and the damages it has caused and relies on the relations between these pieces of information and the mentions of the event [17]. This approach is globally a supervised or goal-driven approach. Weak forms of supervision have also been developed in this field. Work based on bootstrapping where, following [16], relations are first specified by a small number of examples or linguistic patterns [1], falls into this category. More recently, work related to the notion of distant supervi-sion [23], in which relation examples are limited to pairs of entities without any linguistic form for relations, is also an example of such trend.

A reverse approach, called unsupervised information ex-traction , has also been explored during these last years. It aims at finding in texts relations between target entities or types of entities without any a priori knowledge concerning the type of the extracted relations. Furthermore, these re-lations can be clustered according to their similarity to be structured into meaningful sets. Work in this area can be considered according to three main viewpoints. The first one regards the unsupervised extraction of relations as a means for learning knowledge. This view has been developed both for learning  X  X eneral world knowledge X  through the concept of Open Information Extraction [2] applied for large-scale knowledge acquisition from the Web in [3] and in more re-
Configuration that is often restricted to one relation. stricted domains, as the biomedical domain, where such re-lation extraction is used for adding new types of relations between entities in an already existing ontology [6].
The two other viewpoints are more directly related to In-formation Extraction. The main one tackles the problem of making it possible for users to specify their information needs in a more open and flexible way. The On-demand information extraction approach [26], relying on [14] and extended by the notion of Preemptive Information Extrac-tion [27], aims at inducing a kind of template from a set of documents that are typically retrieved by a search engine from queries that are representative of the information to extract. The same perspective can be found in [19] and, with a specific emphasis put on relation clustering, in [25].
Finally, the last viewpoint, less represented than the two others, considers unsupervised information extraction as a source of improvement for supervised information extrac-tion. The supervised approach frequently depends on man-ually annotated corpora. As the task is complex, the anno-tation cost is high and these corpora are generally not very large. In this context, the results of an unsupervised ap-proach can be used to extend the coverage of models learned from an annotated corpus. This idea is more specifically de-veloped in [4] and is also present in [10].

Following the second viewpoint above, our work contributes to the definition of a more flexible information extraction scheme. Within this context, it tackles more particularly the problem of the filtering of extracted relations. The ob-jective of such information extraction process is to discover new types of relations between entities by clustering rela-tions, which requires as less noise as possible among the extracted relations. Hence, we first focus on the filtering procedure, whose objective is to determine whether a rela-tion exists between two named entities in a sentence without any a priori knowledge about its type. Then, we evaluate the impact of such filtering on the clustering of large sets of relations and show that it leads to double its recall with the same precision.
The work we present in this article takes place in a larger context whose global objective is to develop an unsuper-vised information extraction process for addressing technol-ogy watch issues such as  X  X racking all events involving com-panies X and Y X . This process is based on the extraction of relations defined initially by the co-occurrence of two named entities in a sentence, similarly to most of the works cited in the previous section 2 . The main idea behind these restric-tions is to focus first on simple cases to counterbalance the difficulties raised by the unsupervised nature of the global approach.  X  X imple cases X  means here relations whose argu-
In some works such as [2], entities in relation are extended to any noun phrases. ments are rather easy to identify and relations whose linguis-tic expression is small enough to be easily delimited and to avoid coreference phenomena concerning their arguments.
More formally, candidate relations extracted from texts are characterized by two different kinds of information: Figure 1 gives an example of relation with its constituents. It should be noted that such relation has a semi-structured form as one part of its definition  X  the pair of entities  X  is defined with elements coming from an already existing ontology while its other part only appears under a linguistic form.

The unsupervised information extraction process based on this notion of relation is defined by the following sequence of tasks:
The linguistic preprocessing of documents aims at extract-ing the defining elements of relations. Hence, it includes named entity recognition for the target types of entities but also part-of-speech tagging and lemmatization for normaliz-ing the three parts of the linguistic description of relations. It is achieved by the OpenNLP tools 3 .

The step of candidate relation extraction involves very limited constraints: all pairs of named entities with the tar-get types are extracted provided that the two entities appear in the same sentence with at least one verb between them. Table 1 illustrates the volume of the extracted candidate relations from a subpart of the AQUAINT-2 corpus con-taining news articles from 18 months of the newspaper New http://opennlp.sourceforge.net York Times . All our experiments in this article about rela-tion filtering in section 3 and relation clustering in section 4 are based on this corpus and focus on relations involving persons ( per ), organizations ( org ) or locations ( loc
As a consequence of our relation extraction strategy, a sig-nificant number of candidates do not contain a true relation between their entities. This basic method, which can lead to good results in specific domains ([9] shows that 79% of extracted candidates using this heuristic are true relations in the biomedical domain), does not seem to be selective enough for open domain. Therefore, we added to this sim-ple strategy a filtering procedure designed to determine the existence of a relation between two entities in a sentence.
We first defined more discriminative criteria to filter out sentences that do not contain a true relation between a pair of entities. In this perspective, three heuristics were tested:
The application of these three heuristics to relation ex-traction globally reduced the volume of candidate relations by about 50%. Table 2 presents in more details the filtering ratio of each relation type for a sample of 8,000 relations for each type. For each pair of entity types, the second column shows the numbers of relations filtered and kept using all the heuristics, as well as the ratio of kept relations. The three following columns give the number of filtered relations for each individual heuristic, considering that one relation may be filtered by more than one heuristic. The distance limit has obviously an important filtering effect but the only-one-verb limitation has an equally significant impact.

These filtering ratios only give quantitative information about the reduction of relation candidates. In order to eval-uate the efficiency of this heuristic filtering in a reliable and feasible way, a subset of 50 randomly selected relations of each type were manually annotated to verify their validity with a Web interface (presented in Figure 2) generated by applying XSLT transformations to the XML representation of relations.

Relation type Filtered Kept true false true false
The results of this annotation, presented in Table 3, show that a very high percentage of filtered relations are indeed false ones, which confirms the relevance of our filtering crite-ria. These results also show that the ratios of false relations after filtering remain important, especially those with loc as first named entity type. This phenomenon can be ex-plained by the fact that, in a true relation, the first entity should have an agent role in the sentence whereas location names often occur at the beginning of sentences in adverbial phrases. These cases could be detected using a deeper syn-tactic analysis but such analysis is too costly for the amount of data we process. Considering this observation, relations between entities involving a loc as first entity are excluded from the following steps.
The results of Table 3 demonstrate the utility of our filter-ing heuristics. However, it also indicates that these heuris-tics are not sufficient to reach a high proportion of correct relations ( i.e. high enough for the following steps of the unsupervised information extraction process). We present in this section an additional filtering method using statisti-cal machine learning models. Training and test corpora for these models were built manually by annotating relations with the same interface as in Figure 2. More precisely, 200 relations for each of the 6 pairs of our target entity types were randomly selected and annotated. The annotation dis-tinguished correct relations ( true ), incorrect relations due to a named entity recognition error ( NEerr ), and incor-rect relations due to the absence of any effective relation ( false ). An additional distinction between the nature of the relations was also made (between attributive relations and event-related relations) but it was not exploited in this work. The results of this annotation are presented in Table 4.
Relation Type true NE errors false
The figures of Table 4 show that about 20% incorrect re-lations are caused by named entity errors. These relations were removed from the corpus to avoid introducing too much noise in the training data. The remaining corpus was com-posed of 964 relations, 531 of which were true and 433 were false. The resulting set of relation instances is well-balanced enough to avoid problems related to the training of classifiers with unbalanced data sets.
We first tested several models based on non-structured lo-cal features. Classically, we trained a Naive Bayes classifier, a Maximum Entropy classifier (MaxEnt), a Decision Tree and a Support Vector Machine classifier (SVM). The first three models were implemented using the tools provided by MALLET [22] while the last model was implemented with
The same set of features was used to train these four dif-ferent classifiers:
As [4], we also tested a classifier based on the sequential tagging of each word in a sentence. Our representation of this sequential model is illustrated by Figure 3.
 Figure 3: Sequential representation of sentence an-notation
More precisely, each word is tagged with one of the four labels below, following the BIO encoding model introduced by [24]:
After their tagging, sentences containing true relations are labeled as the first configuration of Figure 3 (with a variable number of I-REL depending on the expression of the re-lation) whereas false relation sentences are labeled as the second one. In practice, a well-trained classifier should not generate configurations other than the two presented in Fig-ure 3: for instance (O  X  NE  X  B-REL  X  O  X  O  X  NE  X  O) is not possible since B-REL is always followed by at least one I-REL in the training corpus.
 This approach was implemented by a linear Conditional Random Fields (CRF) model, using the Wapiti [20] tool, and trained with the following set of features for each word of the sentence:
Considering the relatively small size of the annotated cor-pus, we used a 10-fold cross-validation to evaluate these dif-ferent classifiers: the corpus was split into 10 equal parts, 9 of which being used for training and 1 for testing; the procedure is repeated 10 times so that each part is used for training and for test at least once. The results of Table 5 rep-resent the average values on the 10 iterations of the standard measures of Accuracy , Precision , Recall and F1-measure .
Table 5 first shows that the SVM classifier obtains the best performance among the non-sequential models, which is a result generally obtained by similar works about rela-tion extraction. It also shows that the CRF classifier slightly outperforms the SVM classifier, which confirms the advan-tage of the sequential model. Moreover, we note a balance between precision and recall for all types of classifiers. Com-pared with the results of [4] on the same subject (see the last line of the table), our results exhibit a better F1-measure, with a much better recall and a slightly lower precision. However, the work in [4] relies on more general entities than only named entities, which makes the task harder. As we have not enough room here for detailing the various rela-tion representations and the different sets of features we have tested, we only present those achieving the best per-formance. However, it is interesting to notice, concerning the entities of relations, that removing the entity type as a feature and replacing it by a generic  X  X E X  tag to mark the presence of a named entity only causes a slight decrease of the performance of the CRF classifier, with a F1-measure equal to 0.768. This indicates a promising extensibility of our classifier to other named entity types. Finally, as a con-sequence of the global results of this evaluation, the CRF model was adopted for the relation filtering part of our unsu-pervised information extraction method in the experiments of section 4.
The extraction of relations is composed of three successive steps:
Moreover, we observed the presence of a certain number of identical relations coming either from articles about the same subject or from very formatted expressions. Hence, we completed the filtering procedure with a final deduplica-tion step for discarding these redundant relations. As there exists a superior boundary for the values of the similarity measure between relations, the implementation of this fi-nal step was based on the identification of pairs of relations with this maximal similarity value, which was done by rely-ing on the same approach as for the clustering of relations in section 4 for evaluating the similarities between relations. For relations having this maximal similarity value, only one representative element was kept. We put this deduplication operation as the last step of the relation extraction process for two reasons: first, this procedure is more costly than the other filtering operations; second, it relies on the evaluation of relation similarity performed for relation clustering.
Table 6 shows detailed information for each step of rela-tion filtering, starting from all candidate relations of Table 1. We can note that this filtering put aside a large number of the initially extracted relations but we have estimated that only 19.9% of these discarded relations result from erroneous decisions, with a global recall of the filtering procedure es-timated to 0.553. Finally, the remaining volume is a priori sufficient for the next steps of our unsupervised information -per per -loc per -org per -per extraction process. Furthermore, as in [4], the context of our work is the processing of large text collections characterized by informational redundancy for which high-precision results are preferred to avoid too much noise.
The objective of our work is to cluster similar relations in order to offer users a better view of existing relations between entities as in many articles in the domain of unsu-pervised information extraction [27, 25]. We have chosen for this clustering an approach similar to [14]: we only consider one level of clustering for gathering relations that share the same meaning (semantic clustering). The definition of this similarity of meaning is relatively loose: it does not only represent a strict notion of paraphrase or implication but is more related to the notion of information redundancy used in automated summarization.

The clustering method relies on two elements: a simi-larity measure between relations and a clustering algorithm that uses the pairwise similarities between relations. For the similarity measure, we chose the widely used cosine measure and applied it to a bag-of-words representation of relations. More precisely in our case, we only used the Cmid part of each relation in order to focus on its core meaning rather than on its context. The choice of the cosine measure was also justified by the results of preliminary experiments show-ing its superiority over the edit distance for a similar task.
Clustering algorithms often rely on a similarity matrix which can be costly to compute, in particular for larges sets of relations like the one we want to process (several tens of thousands of relations), since the number of similarities is quadratic with respect to the number of relations. Al-gorithms such as k-means are a little less costly since they only consider the similarities between the points and the centroids of current clusters but they require to fixed a pri-ori the number of classes, which is hard to evaluate in our case (optimizing this number is possible but can lead again to a problem of complexity). We tackled this issue by using the All Pairs Similarity Search algorithm (APSS) [5] that allows to compute efficiently a similarity measure such as the cosine measure for all pairs of elements whose similarity value is above a given threshold. The efficiency of this algo-rithm relies on a series of optimizations in the indexing of the elements to compare that exploit the fixed threshold and the sparsity of the input vectors for reducing the number of comparisons to perform. In our experiments, this thresh-old was based on observations from the Microsoft Research Paraphrase Corpus [8]. This corpus contains a set of sen-tence pairs associated with an assessment indicating if they are paraphrases or not. We computed the cosine measure for all pairs of paraphrase sentences and chose to fix our threshold value to 0.45, which covers 3/4 of the similarity values between these sentences.

We then used the Markov Clustering algorithm [29] to create the final clusters of relations from the similarity ma-trix computed by the APSS algorithm. More precisely, this matrix, which is rather sparse, is directly transformed into a similarity graph by associating each relation with a node and each non-zero similarity with a weighted edge between two nodes. The Markov Clustering algorithm performs the partitioning of a graph by the means of a series of random walks on the graph. This algorithm converges quite fast in practice, which allows to deal with large graphs, and does not depend on a fixed number of clusters: its only parame-ter, inflation , controls the granularity of the clusters. In our experiments, we adopted the default inflation value of the MCL implementation 4 .
We present in this section a quantitative evaluation of the relation clustering. Evaluation of clustering is a hard task because no gold-standard partitioning of the set of el-ements is available: such a reference would be too costly to build, considering we have tens of thousands of relations. The usual approach in this case is to evaluate the quality of the results obtained by manually looking at a particular set of clustering results. A major drawback of this kind of eval-uation is that it relies on a specific clustering configuration and would require to perform the complete evaluation pro-cess again if the clustering technique changes. We wanted to have a more reproducible evaluation framework in order to compare clustering results with and without filtering and possibly with different filtering techniques.

We then propose to perform two evaluations of the clus-tering: the first one is an evaluation using internal criteria; the second one using external criteria, but only on a par-tial reference. On one hand, internal criteria for clustering evaluation allows to establish to which extent the clusters obtained correspond to the similarity measures between the relations [12]. More precisely, we use the internal criteria to test the hypothesis that the similarities in the relation space after filtering have a better distribution that the ones before filtering, then leading to a better clustering. On the other hand, external criteria allow to better take into account an actual evaluation of whether two relations in the same clus-ter belong to the same semantic relation. Since we do not have the possibility to create a gold-standard for the whole set of relations, we decided to create reference data for a selected subset of relations and evaluate how these relations are distributed among the different clusters.
Among various internal measures for clustering evalua-http://micans.org/mcl tion, we chose a measure of expected density , which is eval-uated in [28] as the one having the best correlation with F-measure for documents clustering (the more usual mea-sure of the Dunn index is said to be less stable).

Given a weighted graph ( V,E,w ) with a node set V , an edge set E and a weight function w , the density  X  of the graph is defined by: with w ( G ) = | V | + P e  X  E w ( e ) and the weight function w defined by the relation similarity in our case.

Expected density can be computed by local and global graph density of clustering. For a set of result clusters C = { C i } with C i = ( V i ,E i ,w ), the expected density is defined by: where | V i | | V | intends to balance the difference of size of clus-ters. For taking into account the considerable difference of the collection size due to the filtering phase, we defined an expected density measure that is less dependent on the cor-pus size by loosening the exponential factor | V i | , which is connected to the size of each cluster. Therefore, we used the following definition of expected density: A higher value of the measure  X  0 implies a better clustering quality.

We also considered the Connectivity measure [13], another internal measure. Connectivity evaluates how many nearest neighbors are not clustered together. This measure is of particular interest for us since it is based on the same sim-ilarity graph that we are using for the clustering method. The connectivity measure is defined by: where p denotes how many neighbors are taken into account, nn i ( j ) is the j th nearest neighbor of i and x i,nn i ( j ) to 0 if i and nn i ( j ) are in the same cluster and equals to 1 otherwise.

As shown by its formal definition, connectivity also de-pends on corpus size. To avoid such dependence, we selected randomly a subset of the total corpus (5,000 relations were used for evaluation in our experiments). This measure is inverse compared to the expected density: a lower connec-tivity value indicates a better clustering.

Results of expected density measure and connectivity mea-sure are presented in Table 7. The results with these two internal measures show that the filtering phase generally im-proves the clustering processing. Better clusters are gener-ated from the filtered relations using the same clustering method. The two entity pairs which do not follow the same tendency are, for the expected density, org  X  loc and per  X  loc . Since both share the same entity type location , this observation probably indicates a special behavior of these entities. Actually, as we stated in section 3, location entities are often included in adverbial phrases. When such a case happens, there is no real relation between the location entity and the other entity although, with the currently used simi-larity measure, phrases with similar location adverbials can be clustered together and obtain a good clustering score.
The first results with internal measures demonstrate the interest of the filtering procedure. Then, we have tried to confirm this interest using external measures by comparing the clustering results with reference clusters. A partial ref-erence has been built in three steps: 1. indexing of all relation candidates with a search engine; 2. querying of this index iteratively to locate interesting 3. creation of relation clusters manually from the results
More precisely, we first indexed the extracted relations with the search engine Lucene 5 , using distinct fields for the text, the named entities and the entity types. This allows us to search relations with queries specifically targeting their first or second named entity ( E1 , E2 ), the types of these entities ( T1 , T2 ) or the linguistic constituents of relations Cmid , Cpos or Cpre (see relation example in Figure 1). We used this possibility by querying the index with various com-binations of fields, in a first step to explore potential tar-get relations between given named entities and entity types ( e.g. with queries such as E1=Bush,T2= loc ), and then in a second step, to explore different named entities with target relations ( e.g. with queries such as T1= per , Cmid con-tains  X  X isit X  ). After several iterations of these two steps, we obtained a set of relations mixing a large diversity of rela-tions together with a significant number of similar relations. Based on these relations, we built manual clusters with a specific Web-based annotation tool.

Currently, our gold-standard reference concentrates on the relation type per  X  loc and contains 17 clusters with 253 relations, including relations such as, come from , be going to , have a speech in , like , etc. We present below some examples for the relation grow up in :
External measures like Purity , Normalized Mutual Infor-mation and Rand Index are well discussed in the literature ( e.g. [21]). Given reference clusters with N relations, Rand Index is defined to check how all N ( N  X  1) / 2 pairs of rela-tions are grouped. A clustering method should assign similar http://lucene.apache.org TP FP FN TN purity MI NMI relations to the same cluster and separate dissimilar ones. Hence, there are four kinds of decisions. First, a true posi-tive (TP) decision assigns two similar relations to the same cluster while a true negative (TN) one assigns two dissim-ilar relations to different clusters. TP and TN are both correct decisions. On the other hand, there are two incor-rect decisions: false positive (FP) decisions, which assign two dissimilar relations to the same cluster, and false neg-ative (FN) decisions, which assigns two similar relations to different clusters. The Rand Index measures the clustering accuracy, which is defined by: F-measure can be defined in the same time, relying on the precision P and recall R :
We have computed the distribution of reference relations in clustering results for both the pre-filtering phase and the post-filtering phase. The results are presented in Table 8. These results show that the filtering procedure almost dou-bles the recall measure, from 0.1271 to 0.2211, while the pre-cision is kept around 0.53. We can also see directly from this table that many more pairs of truly similar relations (TP) are found by the clustering method on the post-filtering cor-pus than on the pre-filtering corpus.

Rather than examining all pairs of relations, clustering quality can be measured directly at a cluster level. Purity and Normalized Mutual Information (NMI) are usually used for this purpose. A prerequisite of this approach is to assign each result cluster to the class (a reference cluster) with which it shares the largest number of relations. Purity is defined by: where  X  = { w 1 ,w 2 ,...w K } is the set of result clusters and C = { c 1 ,c 2 ,...,c J } is the set of reference clusters.
Purity has a bias when the number of clusters is large: it is equal to 1 when each relation forms its own cluster. Normalized mutual information makes a trade-off between the number of clusters and their quality. It is defined by: MI ( X  , C ) is the mutual information between  X  and with the definition: where H ( X ) and H ( C ) are respectively entropies of  X  and C , defined as: where P ( w k ), P ( c j ) and P ( w k  X  c j ) are respectively the probabilities of a relation being in a result cluster w k reference cluster c j and in the intersection of the two. The probabilities are computed directly by counting the cardi-nalities of the clusters.

In the same way as for the Rand Index measure, we com-puted these measures for the result clusters obtained using relations extracted with or without the filtering phase. Eval-uation results for Purity and NMI are illustrated in Table 8. The results show that both Purity and Normalized Mutual Information are improved by the filtering. In particular, the augmentation of Purity confirms the recall improvement ob-served with the Rand Index measure.
One of the specificities of the work we have presented in this article is to associate two types of work in the field of unsupervised information extraction, relation filtering and relation clustering, and to study the consequences of this as-sociation. Concerning relation filtering, our work is close to the work described in [4], with two main differences. First, relation arguments are restricted to named entities in our case, whereas such argument has more general form in [4] and can be any base noun phrase 6 . Second, [4] does not rely on manually annotated examples as we do but exploits examples built automatically from the successful parses of a
Base noun phrases refer to noun phrases that do not con-tain nested noun phrases or modifiers such as prepositional phrases. syntactic parser by applying a small set of heuristics. The possible impact of these two differences is not easy to pre-dict as they tend to diverge in terms of effects. The first dif-ference makes relation extraction more difficult in the case of [4] as it enlarges the set of possible relations to cover. However, restricting the possible relations to a subset of the successful parses of a syntactic parser clearly favors relations with a simple syntactic form whereas the limits of our refer-ence relations are only set by a human annotation. Finally, the results of [4], a high precision but a low recall, can be explained as follows: because of the kind of entities it fo-cuses on, [4] considers a large set of possible relations but the classifier it has developed is actually able to takes into account only a small subset of them because of the way it is trained. Our approach implements a more balanced choice between the set of relations we want to take into account and the set of relations we actually model, which globally leads to higher results. From a practical viewpoint, building the training examples automatically from the results of a syntactic parser as [4] did is of course an interesting choice to have a large training set without the cost of a human annotation but of course, this method heavily depends on both the availability and the quality of such parser in the target context (language, domain or type of texts).

Concerning relation clustering, the comparison with ex-isting work raises two main issues. The first one is the scal-ability of the clustering process. Clustering algorithms fre-quently start from a similarity matrix that can be difficult to compute when the number of items to cluster is large. One way to overcome this difficult is to fix or to evaluate a priori the number of clusters to build. For instance, [30] sets arbitrarily the number of clusters according to the doc-ument set, [25] tests different values whereas [10] uses the Akaike Information Criterion to evaluate this number in one of its experiments. This problem is also bypassed in some works by limiting the number of relations to cluster, either directly or through the initial number of documents. [14] for instance only considers relations with at least 30 occurrences whereas experiments in [25] are limited to 4,000 relation oc-currences and those in [30] to 526 Wikipedia documents. In our case, this issue is tackled by associating a filtering method for discarding explicitly false relations and the use of the APSS algorithm for evaluating efficiently the similar-ity of the remaining relations. This combination makes the use of a large spectrum of clustering algorithms possible.
The second important issue concerning relation clustering is the evaluation of its results. As mentioned in section 4.2, a direct evaluation of the built clusters and their content by human annotators as it was performed in [14] or in [30] cannot be achieved very often because of its cost. In partic-ular, it does not fit the constraints resulting from the tuning of a system. It is why we have adapted and applied mea-sures for the internal evaluation of clustering to the context of unsupervised information extraction, which was not done before to our knowledge. These measures were more specif-ically used for testing the impact of relation filtering and their conclusions appear as coherent with those of external measures, as illustrated in section 4.2.2. For our external evaluation, we have chosen as [25] to select a sample of re-lations and to cluster them manually to build a reference. More precisely, because of the very large number of relations we have, this selection was guided in our case by a search engine. Finally, the evaluation consists in determining to what extent relations that are part of a reference cluster are found in the same cluster in the evaluated clustering. Following [15], [10] adopts the same principle but uses as reference the relations annotated in a corpus in the context of a supervised information extraction task, more precisely, the Relation Mention Detection task of the ACE (Automatic Content Extraction) evaluation [7].
In this paper, we have presented a work on relation filter-ing for unsupervised information extraction whose purpose is to determine if two entities occurring in the same sen-tence are linked by a relation without a priori knowledge about the relation type. This filtering is performed using both heuristic and machine learning techniques. Heuristic filtering is first used to remove the simple cases whereas ma-chine learning techniques are used for more ambiguous cases. Evaluation of machine learning techniques shows that best results are obtained with CRF, compared with results ob-tained with SVM, MaxEnt or NaiveBayes classifiers. Our best performances are quite balanced between precision and recall and are better than the results reported in [4] (but their study is not limited to named entities, which makes the task more difficult). Applied to unsupervised informa-tion extraction, we have also showed, through an evaluation of relation clustering with both internal and external crite-ria, that this filtering is useful for a semantic clustering of the extracted relations.

The most direct perspectives of this work are about the clustering of relations. We will improve the external eval-uation of the clustering using a larger set of annotated ex-amples and try to have more evidences on the interest of such partial external evaluation by computing its correla-tion with a completely annotated reference. We also plan to use a more sophisticated clustering including two levels of clustering: a semantic clustering and a thematic cluster-ing. Finally, we also consider applying this filtering process to improve a system for knowledge base population based on distant supervision by filtering out the candidate rela-tions extracted from a corpus for learning linguistic relation patterns.
This work was partly supported by the ANR FILTRAR-S project and the FP7 Virtuoso project. [1] E. Agichtein and L. Gravano. Snowball: Extracting [2] M. Banko, M. J. Cafarella, S. Soderland, [3] M. Banko and O. Etzioni. Strategies for Lifelong [4] M. Banko and O. Etzioni. The Tradeoffs Between [5] R. J. Bayardo, Y. Ma, and R. Srikant. Scaling Up All [6] M. Ciaramita, A. Gangemi, E. Ratsch, J. Saric, and [7] G. Doddington, A. Mitchell, M. Przybocki, [8] B. Dolan, C. Quirk, and C. Brockett. Unsupervised [9] M. Embarek and O. Ferret. Learning patterns for [10] E. Gonz  X alez and J. Turmo. Unsupervised Relation [11] R. Grishman and B. Sundheim. Design of the MUC6 [12] M. Halkidi, Y. Batistakis, and M. Vazirgiannis. [13] J. Handl, J. Knowles, and D. B. Kell. Computational [14] T. Hasegawa, S. Sekine, and R. Grishman. Discovering [15] H. Hassan, A. Hassan, and O. Emam. Unsupervised [16] M. Hearst. Automatic Aquisition of Hyponyms from [17] L. Jean-Louis, R. Besan  X con, and O. Ferret. Using [18] T. Joachims. Making large-scale SVM learning [19] H. H. Kathrin Eichler and G. Neumann. Unsupervised [20] T. Lavergne, O. Capp  X e, and F. Yvon. Practical Very [21] C. D. Manning, P. Raghavan, and H. Sch  X  utze. [22] A. K. McCallum. MALLET: A Machine Learning for [23] M. Mintz, S. Bills, R. Snow, and D. Jurafsky. Distant [24] L. Ramshaw and M. Marcus. Text Chunking Using [25] B. Rosenfeld and R. Feldman. Clustering for [26] S. Sekine. On-Demand Information Extraction. In 21 st [27] Y. Shinyama and S. Sekine. Preemptive Information [28] B. Stein, Sven, and F. Wi X brock. On Cluster Validity [29] S. van Dongen. Graph Clustering by Flow Simulation . [30] Y. Yan, N. Okazaki, Y. Matsuo, Z. Yang, and
