 1. Introduction
Cross-language Information Retrieval (CLIR) is the problem of finding documents that are expressed in a language differ-uments containing terms in one language (which we consistently refer to as f ) based on query terms in some other language (which we consistently refer to as e ). A broad range of approaches to CLIR involve some sort of direct mapping between terms in each language, either from e to f (query translation) or from f to e (document translation). In this article we argue that these are both ways of asking the more general question  X  X  X o terms e and f have the same meeting? X  X  Moreover, we argue tion that we ask when performing monolingual retrieval. We therefore derive a  X  X  X eaning matching X  X  framework, first intro-duced in ( Wang &amp; Oard, 2006 ), but presented here in greater detail.

Instantiating such a model requires that we be specific about what we mean by a  X  X  X erm X  X . In monolingual retrieval we might treat each distinct word as a term, or we might group words with similar meanings (e.g., we might choose to index all words that share a common stem as the same term). But in CLIR there is no escaping the fact that synonymy is central to what we are doing when we seek to match words that have the same meaning. In this article we show through experiments that by modeling synonymy in both languages we can improve efficiency at no cost (and indeed perhaps with some improve-lections on which we had previously observed this result ( Wang, 2005; Wang &amp; Oard, 2006 ).

When many possible translations are known for a term, a fundamental question is how we should select which transla-tions to use. In our earlier work, we had learned translation probabilities from parallel text and then used however many translations were needed to reach a preset threshold for the Cumulative Distribution Function (CDF) ( Wang &amp; Oard, 2006 ). In this article we extend that work by comparing a CDF threshold to two alternatives: (1) a threshold on the  X 
Probability Mass Function (PMF) and (2) a fixed threshold on the number of translations. The results show that thresholding the CDF or the PMF are good choices.

The remainder of this article is organized as follows. Section 2 reviews the salient prior work on CLIR. Section 3 then intro-duces our  X  X  X eaning matching X  X  model and explains how some specific earlier CLIR techniques can be viewed as restricted variants of that general model. Section 4 presents new experiment results that demonstrate its utility and that explore which aspects of the model are responsible for the observed improvements in retrieval effectiveness. Section 5 concludes the article with a summary of our findings and a discussion of issues that could be productively explored in future work. 2. Background
Our meaning matching model brings together three key ideas that have previously been shown to work well in more re-stricted contexts. In this section we focus first on prior work on combining evidence from different document-language terms to estimate useful weights for query terms in individual documents. We then trace the evolution of the idea that nei-of which translations to use. 2.1. Estimating query term weights
A broad class of information retrieval models can be thought of as computing a weight for each query term in each doc-ument and then combining those query term weights in some way to compute an overall score for each document. This is the so-called  X  X  X ag of words X  X  model. Notable examples are the vector space model, the Okapi BM25 measure, and some language models.

In early work on CLIR a common approach was to replace each query term with the translations found in a bilingual term list. When only one translation is known, this works as well as anything. But when different numbers of translations are known for different terms this approach suffers from an unhelpful imbalance (because common terms often have many translations, but little discriminating power). Fundamentally this approach is flawed because it fails to structurally distin-guish between different query terms (which provide one type of evidence) and different translations for the same query term (which provide a different type of evidence).

Pirkola (1998) was the first to articulate what has become the canonical solution to this problem. Pirkola X  X  method esti-mates term specificity in essentially the same way as is done when stemming is employed in same-language retrieval (i.e., any document term that can be mapped to the query term is counted). This has the effect of reducing the term weights for query terms that have at least one translation that is a common term in the document language, which empirically turns out to be a reasonable choice. The year 1998 was also when Nie, Isabelle, Plamondon, and Foster (1998) and McCarley and Rou-kos (1998) were the first to try using learned translation probabilities rather than translations found in a dictionary. They, and most researchers since, learned translation probabilities from parallel (i.e., translation-equivalent) texts using tech-niques that were originally developed for statistical machine translation ( Knight, 1999 ).

The next year, Hiemstra and de Jong (1999) put these two ideas together, suggesting (but not testing) the idea of using translation probabilities as weights on the counts of the known translations (rather than on the Inverse Document Frequency both was that evidence combination across translations should be done before evidence combination across query terms. Xu and Weischedel (2000) were the first to actually run experiments using an elegant variant of this approach in which the
Term Frequency (TF) of term e , tf ( e ), was estimated in the manner that Hiemstra and de Jong (1999) had suggested, but the Collection Frequency (CF) of the term, cf ( e ), which served a role similar to Hiemstra X  X  document frequency, was com-puted using a separate query-language corpus rather than being estimated through the translation mapping from the doc-ument collection being searched.

Hiemstra and de Jong (1999) and Xu and Weischedel (2000) developed their ideas in the context of language models. It remained for Darwish and Oard (2003) to apply similar ideas to a vector space model. The key turned out to be a computa-tional simplification to Pirkola X  X  method that had been introduced by Kwok (2000) in which the number of documents con-taining each translation was summed to produce an upper bound on the number of documents that could contain at least one of those translations. Darwish and Oard (2003) showed this bound to be very tight (as measured by the extrinsic effect on Mean Average Precision (MAP)), and from there the extension to using translation probabilities as weights on term counts was straightforward.

Statistical translation models for machine translation are typically trained on strings that represent one or more consec-utive tokens, but for information retrieval some way of conflating terms with similar meanings can help to alleviate sparsity without adversely affecting retrieval effectiveness. For example, Fraser, Xu, and Weischedel (2002) trained an Arabic-English translation model on stems (more properly, on the results of what it called  X  X  X ight stemming X  X  for Arabic). Our experiments with aggregation draw on a generalization of this idea.

The idea of using learned translation probabilities as term weights resulted in somewhat of a paradigm shift in CLIR. Earlier  X  X  X ictionary-based X  X  techniques had rarely yielded MAP values much above 80% of that achieved by a comparable monolingual system. But with translation probabilities available we started seeing routine reports of 100% or more. For example, Xu and
Weischedel (2000) reported retrieval results that were 118% of monolingual MAP (when compared using automatically seg-mented Chinese terms), suggesting that (in the case of their experiments) if you wanted to search Chinese you might actually be better off formulating your queries in English! 2.2. Bidirectional translation
Throughout these developments, the practice regarding whether to translate f to e or e to f remained somewhat incon-sistent. Nie et al. (1998) (and later Darwish &amp; Oard, 2003 ) thought of the problem as query translation, while McCarley and Roukos (1998), Hiemstra and de Jong (1999) and Xu and Weischedel (2000) thought of it as document translation. In reality, of course, nothing was being  X  X  X ranslated. X  X  Rather, counts were being mapped.

Indeed, the implications of choosing a direction were not completely clear at that time. We can now identify three quite different things that have historically been treated monolithically when  X  X  X uery translation X  X  or  X  X  X ocument translation X  X  is mentioned: (1) whether the processing is done at query time or at indexing time, (2) which direction is assumed when learn-ing the word alignments from which translation probabilities were estimated (which matters only because widely used effi-cient alignment techniques are asymmetric), and (3) which direction is assumed when the translation probabilities are normalized. We now recognize these as separable issues, and when effectiveness is our focus it is clear that the latter two should command our attention. Whether computation is done at query time or at indexing time is, of course, an impor-tant implementation issue, but if translation probabilities do not change the results will be the same either way.
McCarley (1999) was the first to explore the possibility of using both directions. He did this by building two ranked lists, bining the two ranked lists yielded better MAP than when either approach was used alone. Similar improvements have since been reported by others using variants of that technique ( Braschler, 2004; Kang, Na, &amp; Lee, 2004 ).
Boughanem, Chrisment, and Nassr (2001) tried one way of pushing this insight inside the retrieval system, simply filter-ing out potentially problematic translations that were attested in only one direction. They did so without considering trans-lation probabilities, however, working instead with bilingual dictionaries. On that same day, Nie and Simard (2001) introduced a generalization of that approach in which translation probabilities for each direction could be interpreted to as partially attesting the translation pair. The product of those probabilities was (after renormalization) therefore used in tive approach, although the experiments in Nie and Simard (2001) on a different test collection (and with some differences in implementation details) were not as promising. As we show in Section 4.1.3 , the relative effectiveness of bidirectional and unidirectional translation does indeed vary between test collections, but aggregation can help to mitigate that effect and, regardless, bidirectional translation offers very substantial efficiency advantages. 2.3. Translation selection
One challenge introduced by learned translation probabilities is that there can be a very long tail on the distribution (be-cause techniques that rely on automated alignment might in principle try to align any term in one language with any term in work has sought to exploit context to inform the choice. For example, Federico and Bertoldi (2002) used an order-indepen-dent bigram language model to make choices in a way that would prefer translated words that are often seen together. By relaxing the term independence assumption that is at the heart of all bag-of-words models, these techniques seek to improve retrieval effectiveness, but at some cost in efficiency. In this article, we have chosen to focus on techniques that preserve term independence, all of which are based on simply choosing the most likely translations. The key question, then, is how far down that list to go.

Perhaps the simplest alternative is to select some fixed number of translations. For example, Davis and Dunning (1995) used 100 translations, Xu and Weischedel (2000) (observing that using large numbers of translations has adverse implica-cases in which a preference order among translations is known, but reliable translation probabilities are not available (as is the case for the order in which translations are listed in some bilingual dictionaries).

Because the translation probability distribution is sharper for some terms than others, it is attractive to consider alterna-tive approaches that can make use of that information. Two straightforward ways have been tried: Xu and Weischedel (2000) used a threshold on the Probability Mass Function (PMF), while Darwish and Oard (2003) used a threshold on the
Cumulative Distribution Function (CDF). We are not aware of comparisons between these techniques, a situation we rectify in Sections 4.1.3 and 4.2.3 .

Another approach is to look holistically at the translation model rather than at just the translations of any one term, view-pairs) in a way that maximizes some function for the overall translation model between all term pairs. Kraaij, Nie, and Sim-ard (2003) reports that this approach (using an entropy function) yields results that are competitive with using a fixed PMF threshold that is the same for all terms. Our results suggest that the PMF threshold is indeed a suitable reference. Future work to compare effectiveness, efficiency and robustness of approaches based on entropy maximization with those based on a PMF threshold clearly seems called for, although we do not add to the literature on that question in this article. 3. Matching meaning
In this section, we rederive our overarching framework for matching meanings between queries and documents, present-ing a set of computational implementations that incorporate evidence from translation probabilities in different ways. 3.1. IR as matching meaning
The basic assumption underlying meaning matching is that some hidden shared meaning space exists for terms in differ-ent languages. Meaning matching across languages can thus be achieved by mapping the meanings of individual terms into that meaning space, using it as a  X  X  X ridge X  X  between terms in different languages. Homography and polysemy (i.e., terms that have multiple distant or close meanings) result in the possibility of several such  X  X  X ridges X  X  between the same pair of terms.
This way of looking at the problem suggests that the probability that two terms share the same meaning can be computed as the summation over some  X  X  X eaning space X  X  of the probabilities that both terms share each specific meaning. for a query term e in Language E , we assume that each document-language term f meaning of e that was intended by the searcher with some probability p ( e tion in our basic notation. For a term in Language F that does not share any meaning with e , the meaning matching proba-bility between that term and e will be 0. Any uncertainty about the meaning of e is reflected in these probabilities, the computation of which is described below. If we see a term f d , we can treat this as having seen query term e occurring p ( e of the total  X  X  X ccurrence X  X  of query term e will be p ( e document side and considering all the terms in document d
Turning our attention to the df , if document d k contains a term f each unique term that might share a common meaning with e . We then assume that terms are used independently in dif-ferent documents and estimate the df of query term e in the collection as:
Because we are interested only in relative scores when ranking documents, we can (and do) perform document length normalization using the document-language terms rather than the mapping of those terms to the query language.
Eqs. (1) and (2) show how the meaning matching probability between a query term and a document term can be incor-porated into the computation of term weight. The remaining question then becomes how the meaning matching probability p ( e M f ) can be modeled and computed for any given pair of query term e and document term f . 3.2. Matching abstract term meanings
Given a shared meaning space, matching term meaning involves mapping terms in different languages into this shared meaning space. Fig. 1 illustrates this idea for a case in which two terms in the query language E and three terms in the doc-ument language F share subsets of four different meanings. At this point we treat  X  X  X eaning X  X  as an abstract concept; a com-putational model of meaning is introduced in the next section. In our example, term e and only if e 2 and f 2 both express meaning m 2 or e 2 and f of meaning for e 2 is independent of the author X  X  choice of meaning for f events. Generalizing to any pair of terms e and f :
Applying Bayes X  rule, we get:
Assume, given a meaning, that seeing a term in one language is conditionally independent of seeing another term in the other language (i.e., p (( e , f ) j m i )= p ( e j m i ) p ( f j m
Furthermore, assuming seeing a term in one language is (unconditionally) independent of seeing another term in the other language (i.e., p ( e , f )= p ( e ) p ( f )), Eq. (5) then becomes:
Lastly, if we make the somewhat dubious but very useful assumption that every possible shared meaning has an equal chance of being expressed, p ( m i ) then becomes a constant. Therefore: where p ( e M f ): the probability that term e and term f have the same meaning. p ( m i j e ): the probability that term e has meaning m i p ( m i j f ): the probability that term f has meaning m i
For example (see Fig. 1 ), if all possible meanings of every term were equally likely, then p p and term f 2 will be: p  X  e 2 $ f 2  X / p 22 p 0 22  X  p 23 3.3. Using synsets to represent meaning
We use  X  X  X ynsets, X  X  sets of synonymous terms as a straightforward computational model of meaning. To make this explicit, we denote a synset s i for each meaning m i in the shared meaning space, so the meaning matching model described in Eq. (7) simply becomes:
Our problem is now reduced to two subproblems: (1) creating synsets s term mapping to any specific synset p ( s i j e ) and p ( s synonyms in both languages. One way to develop such multilingual synsets is as follows: Cross-language synset alignments are available from some sources, most notably lexical resources such as EuroWordNet.
However, mapping unrestricted text into WordNet is well known to be error prone ( Voorhees, 1993 ). Our early experiments with EuroWordNet proved to be disappointing ( Wang, 2005 ), so for the experiments in this article we instead adopt the sta-tistical technique for discovering same-language synonymy that we first used in Wang and Oard (2006) .

Previous work on word sense disambiguation suggests that translation usage can provide a useful basis for identifying guage F can translate to a term e i in language E , which can further back-translate to some term f be a synonym of f . Furthermore, the more terms e i exist as bridges between f and f f is a synonym of f . Formalizing this notion: where p ( f j 2 s f ) is the probability of f j being a synonym of f (i.e., in a synset s translation model from Language F to Language E , and p ( f to Language F . Probability values generated in this way are usually sharply skewed, with only translations that are strongly attested in both directions retaining much probability mass, so any relatively small threshold on the result of Eq. (9) would suffice to suppress unlikely synonyms. We somewhat arbitrarily chose a threshold of 0.1 and have used that value consis-2006 )). Candidate synonyms with a normalized probability larger than 0.1 are therefore retained and, along with f , form syn-set s f . The same term can appear in multiple synsets with this method; that fact has consequences for meaning matching, as we describe below.

As an example, applying Eq. (9) using the statistical translation probabilities described later in Section 4.2.1 , we automat-are often not actually synonyms in the usual sense, but they do capture useful relationships (e.g., the Holzmann construction company was financially rescued, as was the hedge fund LTCM), and drawing on related terms in information retrieval appli-lated terms. 3.4. From statistical translation to word-to-synset mapping Because some translation f i of term e may appear in multiple synsets, we need some way of deciding how p ( e translation probability evenly across each synset in which a translation appears, assuming a uniform distribution. For exam-ple, since translation f 1 appears in synsets s 1 and s 2
Fig. 2 b illustrates an alternative in which each translation f set that would yield the greatest aggregate probability, as follows:
Method (b) is minimalist in the sense that it seeks to minimize the number of synsets. Moreover, Method (b) does this by rewarding mutually reinforcing evidence: when we have high confidence that e can properly be translated to some synonym of f , that might quite reasonably raise our confidence in f j we chose method (b) for the experiments reported in this article.

The two word-to-synset mappings in Fig. 3 illustrate the effect of applying Method (b) to the corresponding pre-aggre- X  X  X auvetage X  X  is assigned to a single synset, which inherits the sum of the translation probabilities of its members.
At this point, the most natural thing to do would be to index each synset as a term. Doing that would add some imple-mentation complexity, however, since rescue and saving are together in a synset when translating the French term  X  X  X auve-tage, X  X  but they might wind up in different synsets when translating some other French term. To avoid that complexity, for our experiments we instead constructed ersatz word-to-word translation probabilities by distributing the full translation probability for each synset to each term in that synset and then renormalizing it. The results are shown in the penultimate row in Fig. 3 .
 3.5. Variants of the meaning matching model
Aggregation and bidirectionality are distinguishing characteristics of our full meaning matching model, but restricted variants of the model are also possible. In this section we introduce variants of the basic model, roughly in increasing order of complexity. See Table 1 for a summary and Fig. 3 for a worked example.

Probabilistic Structured Queries (PSQ): one of the simplest variants, using only translation probabilities learned and nor-malized in the query translation direction ( Darwish &amp; Oard, 2003 ).

Probabilistic Document Translation (PDT): an equally simple variant, using only translation probabilities learned and nor-malized in the document translation direction.

Individual Meaning Matching (IMM): translation probabilities for both directions are used without synsets by multiplying the probabilities for PSQ and PDT. Since the result of multiplying probabilities is no longer normalized we renormalize in the query translation direction (so that the sum over each translation f of a query term e is 1). IMM can be thought of as a variant of DAMM (explained below) in which each term encodes a unique meaning.

Aggregated Probabilistic Structured Queries (APSQ): translation probabilities in the query translation direction are aggre-gated into synsets, replicated, and renormalized as described above.

Aggregated Probabilistic Document Translation (APDT): translation probabilities in the document translation direction are aggregated into synsets, replicated, and renormalized as described above.

Derived Aggregated Meaning Matching (DAMM): translation probabilities are used with synsets for both directions by mul-tiplying the APSQ and APDT probabilities and then renormalizing the result in the query translation direction.
Partially Aggregated Meaning Matching (PAMM): a midpoint between IMM and DAMM, translation probabilities in both directions are used, but with aggregation applied only to one of those directions (to the query translation direction for
PAMM-F and the document translation direction for PAMM-E). Specifically, for PAMM-F we multiply APSQ and PDT prob-abilities, for PAMM-E we multiply PSQ and APDT probabilities; in both cases we then renormalize in the query translation direction. For simplicity, PAMM-F and PAMM-E are not shown in Fig. 3 . 3.6. Renormalization
Two meaning matching techniques (PSQ and APSQ) are normalized by construction in the query translation direction; two others (PDT and APDT) are normalized in the document translation direction. For the others, probability mass is lost when we multiply and we therefore need to choose a renormalization direction. As specified above, we consistently choose ument Frequency ( DF ) is really a fact about a query term (helping us to weight that term appropriately with respect to other terms in the same query), while Term Frequency ( TF ) is a fact about a term in a document. This creates some tension, with the query translation direction seeming to be most appropriate for using DF evidence to weight the relative specificity of query terms and the document translation direction seeming to be most appropriate for estimating TF in the query language from the observed TF  X  X  in the document language.

To see why this is so, consider first the DF . The question we want to ask is how many documents we believe each query term (effectively) occurs in. For any one query term, that answer will depend on which translation(s) we believe to be appro-priate. If query term e can be translated to document language terms f be reasonable to estimate the DF of e as the expectation over that distribution of the DF of f by normalizing so that P f make less sense, since it could result in DF estimates that exceed the number of documents in the collection.
Now consider instead the TF calculation. The question we want to ask in this case is how many times a query term (effec-tively) occurred in each document. If we find term f in some document, and if f can be translated as either e half the occurrences of f to e 1 . This is achieved by normalizing so that
TF  X  e ; d k  X  X  P f result in TF estimates for different query terms that sum to more terms than are actually present in the document.
Our early experience with mismanaging DF effects ( Oard &amp; Wang, 1999 ) and the success of the DF handling in Pirkola X  X  structured queries ( Pirkola, 1998 ) have led us to favor reasonable DF calculations when forced to choose. When probability mass is lost (as it is in IMM, DAMM, PAMM-E, and PAMM-F), we therefore normalize so that translation direction). This choice maximizes the comparability between those techniques and PSQ and APSQ, which are nor-malized in that same direction by construction. We do, however, still gain some insight into the other normalization direc-tion from our PDT and APDT experiments (see Section 4 below). 4. Experiments
In our earlier conference paper ( Wang &amp; Oard, 2006 ), we reported on two sets of experiments, one using English queries and French news text, and the second using English queries and Chinese news text. A third set of experiments, again with
Mean Average Precision (MAP) obtained in those experiments for each Meaning Matching (MM) variant. In each experiment, we swept a CDF threshold to find the peak MAP (usually at a CDF of 0.9 or 0.99).

Several conclusions are evident from these results. First, at the peak CDF threshold DAMM is clearly a good choice, some-times equaled but never bettered. Second, PSQ and APSQ are at the other end of the spectrum, always statistically signifi-cantly below DAMM. The results for IMM, PDT and APDT are more equivocal, with each doing better than the other two in one of the three cases. PAMM-E and PAMM-F turned out to be statistically indistinguishable from DAMM, but perhaps not worthy of as much attention since they occupy a middle ground between IMM and DAMM both in the way they are con-More broadly, we can conclude that there is clear evidence that bidirectional translation is generally helpful (comparing
DAMM to APDT and APSQ, comparing PAMM-F to APDT and PSQ, comparing PAMM-E to APSQ and PDT, and comparing IMM to PSQ and PDT), but not always (PDT yields better MAP than IMM one time out of three, for example). We can also conclude that aggregation results in additional improvement when bidirectional translation is used (comparing DAMM, PAMM-E and
PAMM-F to IMM), but that the same effect is not present with unidirectional translation (with APDT below PDT in two cases out of three, and APSQ always below PSQ).

Notably, the three collections on which these experiments were run are relatively small, and all include only news. In this section we therefore extend our earlier work in two important ways. We first present a new set of experiments with a sub-stantially larger test collection than we have used to date. That is followed by another new set of experiments for two con-tent types other than news, using French queries to search English conversational speech or to search English metadata that was manually associated with that speech. Finally, we look across the results that we have obtained to date to identify com-monalities (which help characterize the strengths and weaknesses of our meaning matching model) and differences (which help characterize dependencies on the nature of specific test collections). 4.1. New Chinese experiments CLIR results from our previous Chinese experiments in Wang (2005) and Wang and Oard (2006) were quite good, with
DAMM achieving 98% and 128% of monolingual MAP (see Table 2 ). Many CLIR settings are more challenging, however, so we chose for our third set of English-Chinese experiments a substantially larger English-Chinese test collection from NTCIR-5, for which the best NTCIR-5 system had achieved only 62% of monolingual MAP ( Kishida et al., 2005 ). 4.1.1. Training statistical translation models For comparability, we re-used the statistical translation models that we had built for our previous experiments with the 2005) the word alignments from which others in our group were at the time building state-of-the-art hierarchical phrase-the Foreign Broadcast Information Service (FBIS), Hong Kong News, Hong Kong Laws, the United Nations, and Sinorama. All were written using simplified Chinese characters. A modified version of the Linguistic Data Consortium (LDC) Chinese segmenter was used to segment the Chinese side of the corpus. After removing implausible sentence alignments by eliminating sentence pairs
A CDF threshold of 0.99 was applied to the model for each direction before they were used to derive the eight meaning matching variants described in Section 3 . 4.1.2. Preprocessing the test collection The NTCIR-5 English-to-Chinese CLIR test collection (formally, CIRB040r), contains 901,446 documents from United Daily
News, United Express, Ming Hseng News, and Economic Daily News. All of the documents were written using traditional Chi-nese characters. Relevance judgments for total of 50 topics are available. These 50 topics were originally authored in Chinese (using traditional characters), Korean or Japanese (18, 18 and 14 topics, respectively) and then manually translated into Eng-lish, and then translated from English into each of the two other languages. For our study, the English version of each topic was used as a basis for forming the corresponding CLIR query; the Chinese version was used as a basis for forming the cor-responding monolingual query. Specifically, we used the TITLE field from each topic to form its query. Four degrees of rel-and  X  X  X rrelevant X  X  as not relevant; in NTCIR this choice is called rigid relevance.

With our translation models set up for simplified Chinese characters and the documents and queries written using tra-ditional Chinese characters, some approach to character conversion was required. We elected to leave the queries and doc-uments in traditional characters and to convert the translation lexicons (i.e., the Chinese sides of the indexes into the two translation probability matrices) from simplified Chinese characters to traditional Chinese characters. Because the LDC seg-menter is lexicon driven and can only generate words in its lexicon, it suffices for our purposes to convert the LDC seg-menter X  X  lexicon from simplified to traditional characters. We used an online character conversion tool Spot checks indicated the results to be generally reasonable in our opinion, however.

For document processing, we first converted all documents from BIG5 (their original encoding) to UTF8 (which we used consistently when processing Chinese). We then ran our modified LDC segmenter to identify the terms to be indexed. The
TITLE field of each topic was first converted to UTF8 and then segmented in the same way. The retrieval system used for our experiments, the Perl Search Engine (PSE), is a local Perl implementation of the Okapi BM25 ranking function ( Robertson &amp; Sparck-Jones, 1997 ) with provisions for flexible CLIR experiments in a meaning matching framework. For the Okapi parameter settings, we used k 1 = 1.2, b = 0.75, and k 3 = 7, as is common. To guard against incorrect character handling for multi-byte characters by PSE, we rendered each segmented Chinese word (in the documents, in the index to the translation probability tables, and in the queries) as a space-delimited hexadecimal token using ASCII characters. 4.1.3. Retrieval effectiveness results
To establish a monolingual baseline for comparison, we first used TITLE queries built from the Chinese topics to perform a monolingual search. The MAP for our monolingual baseline was 0.3077 (which compares favorably to the median MAP for title queries with Chinese documents at NTCIR-5, 0.3069, but which is well below the maximum reported MAP of 0.5047, obtained using overlapping character n-grams rather than word segmentation). We then performed CLIR using each MM var-iant, sweeping a CDF threshold from 0 to 0.9 in steps of 0.1 and then further incrementing the threshold to 0.99 and (for variants for which MAP values did not decrease by a CDF of 0.99) to 0.999. A CDF threshold of 0 selects only the most prob-able translation, whereas a CDF threshold of 1 would select all possible translations.

Fig. 4 shows the MAP values relative to the monolingual baseline for each MM variant at a set of CDF thresholds selected between 0 and 1. The peak MAP values are between 50% and 73% of the monolingual baseline for all MM variants; all are statistically significantly below the monolingual baseline (by a Wilcoxon signed rank test for paired samples at p &lt; 0.05). For the most part the eight results are statistically indistinguishable, although APSQ is statistically significantly below
PDT, DAMM, APDT and PAMM-F at each variant X  X  peak MAP. For comparison, the best official English-to-Chinese CLIR runs under comparable conditions achieved 62% of the same team X  X  monolingual baseline ( Kishida et al., 2005; Kwok, Choi, Dinstl, &amp; Deng, 2005 ). All four bidirectional MM variants (DAMM, PAMM-E, PAMM-F, and IMM) achieved their peak MAP at a CDF of 0.99, consistent with the optimal CDF threshold learned in our earlier experiments ( Wang, 2005; Wang &amp; Oard, 2006 ).
Overall, adding aggregation on the document-language (Chinese) side to bidirectional translation seems to help, as indi-cated by the substantial increase in peak MAP from IMM to PAMM-F and from PAMM-E to DAMM. By contrast, adding aggre-gation on the query-language (English) side to bidirectional translation did not help, as shown by the decrease of the best
MAP from IMM to PAMM-E and from PAMM-F to DAMM. Comparing PDT with APDT and PSQ with APSQ indicates that applying aggregation with unidirectional translation hurts CLIR effectiveness (at peak thresholds), which is consistent with our previous results on other collections. Surprisingly, PDT yielded substantially (nearly 10%) better MAP than DAMM (although the difference is not statistically significant). As explained below, this seems to be largely due to the fact that
PDT does better at retaining some correct (but rare) translations of some important English terms. 4.1.4. Retrieval efficiency results
One fact about CLIR that is not remarked on as often as it should be is that increasing the number of translations for a term adversely affects efficiency. If translation is performed at indexing time, the number of disk operations (which dominates the performed at query time, then the number of disk operations rises with the number of unique terms for which the postings file must be retrieved. Moreover, when some translations are common (i.e., frequently used) terms in the document collec-tion, the postings files can become quite large. As a result, builders of operational systems must balance considerations of effectiveness and efficiency. 4
Fig. 5 shows the effectiveness (vertical axis) vs. efficiency (horizontal axis) tradeoff for four MM variants and three ways of choosing how many translations to include. Fig. 5a was created from the same data as Fig. 4 , sweeping a CDF threshold, threshold value. Results for FAMM-F and FAMM-E (not shown) are similar to those for IMM; APSQ and APDT are not included because each yields lower effectiveness than its unaggregated counterpart (PSQ and PDT, respectively).

Three points are immediately apparent from inspection of the figure. First, PSQ seems to be a good choice when only the threshold that yields an average of three translations DAMM becomes the better choice. This comports well with our intu-ition, since we would expect that synonymy might initially adversely impact precision, but that our greedy aggregation method X  X  ability to leverage reinforcement could give it a recall advantage as additional translations are added. Third, although PDT does eventually achieve better MAP than DAMM, the consequences for efficiency are very substantial, with
PDT first yielding better MAP than DAMM somewhere beyond an average of 40 translations per query term (and, not shown, peaking at an average of 100 translations per query term).

One notable aspect of the PDT results is that, unlike the other cases, the PDT results begin at an average of 8 translations per query term. For DAMM, IMM and PSQ, a CDF threshold of 0 selects only the one most likely translation for each query term. For PDT, by contrast, a CDF threshold of 0 selects only the most likely translation for each document term. Because of the relative lack of morphological variation in Chinese, and because our Chinese segmentation cannot generate words that query terms have 8 or more Chinese translations for PDT when only the most likely English translation of each Chinese doc-ument term is selected. The most extreme of these is the term  X  X  X ime X  X  in the query  X  X  X ime warner american online aol merger impact X  X , which has 67 different Chinese translations with PDT at a CDF threshold of 0. Of course, PDT yield no Chinese translations at all with that threshold for 35 English terms across the 50 queries, notably including  X  X  X arner X  X . Thus we
IMM and DAMM) avoids both problems, and thus is the better choice when seeking to optimize retrieval effectiveness without using very many translations.

Fig. 5b shows comparative results for sweeping a PMF threshold. As with the CDF threshold, PSQ is a good choice when 1 translation per query term is desired, DAMM is the better choice by 3 translations per query term (peaking around 5), and
PDT becomes better somewhere much further out (in this case, somewhere after 20, peaking at an average of 57 transla-tions per query term). Notably, PDT exhibits a markedly better effectiveness-efficiency tradeoff with a PMF threshold than with a CDF threshold (PSQ shows the opposite effect; IMM and DAMM are for the most part unaffected).

As Fig. 5c shows, DAMM, IMM and PSQ are adversely affected when a fixed top-n threshold is applied to the number of translations, both because of lower peaks (than achieved by the same technique with a CDF threshold) and because of shar-per peaks (thus making actual results in operational settings more sensitive to parameter tuning). PDT, by contrast, does about as well with a top-n threshold as with a PMF threshold (rising about as rapidly to peak at 73% of monolingual
MAP, compared with 75% for a PMF threshold), and using of a constant number of translations may have some modest ben-efits for storage management.

It is important to recognize that the differences we are seeing here are not statistically significant, so it is the broader trends on which we must focus. From these results we can reasonably conclude that CDF and PMF thresholds are both good choices over a broad range of effectiveness-efficiency tradeoffs, and that a PMF or a top-n threshold may be a reasonable the information provided by the probability distributions can provide a useful degree of query-specific tuning when the choice involves relatively common events, but that empirical statistics for uncommon events (which is what we must work with when the number of translations becomes very large) are generally not as reliable. 4.1.5. Query-specific analysis
Taking the mean of the AP for the 50 queries in our test collection is useful when seeking to characterize expected retrie-val effectiveness for the as-yet-unseen 51st query that what we really care about, but as we saw with  X  X  X ime warner X  X  when we are seeking to understand why one technique works better than another it can also be useful to look at what happens in specific cases. For the analysis in this section we chose the run with the peak MAP (obtained by sweeping a CDF threshold) for each of two techniques, which we generically refer to as Run A and Run B. We remove any topics for which neither Run A nor Run B achieved an AP of 0.2 or better (in order to avoid focusing on small differences between bad and worse), we then compute a relative AP difference (rAPd), defined as ( AP A into three groups: (1) the rAPd is markedly better (at least +20%), (2) the rAPd is markedly worse (at least 20%), and the remainder (which we normally ignore) in which the AP is little changed.
 As Fig. 7 a shows, there were 10 topics for which DAMM was markedly better than PSQ and 5 topics (not shown) for which
DAMM was markedly worse. Table 3 shows query terms from several queries that (in our opinion) have substantial proba-bility mass assigned to incorrect translations (for space reasons, translations with very low probabilities are not shown). As expected, DAMM (the upper row of translations for each term) often produces sharper distributions that emphasize better 3 a are simply words that co-occur frequently with correct translations. Nonetheless, many of the additional translations to which PSQ (the lower row) assigns substantial probability mass are both incorrect and (in our opinion) unlikely to be helpful.
We have indicated those that we feel could be useful using bold, and those that we expect would adversely affect retrieval effectiveness using italics.

For three of the five cases in which DAMM was markedly below PSQ (Queries 23, 47, and 7 in Table 3 b), DAMM seems to be overtuned to the domain of the parallel text collection. For example, it may be the case that many of the sentences in the parallel text collection that contain the word  X  X  X eneral X  X  are talking about attorneys general, military generals, or the U.N. sense needed for  X  X  X orean general election X  X  in Query 47. In future work we might be able to mitigate this overtuning to some using broader context in some way (e.g., with phrase translation models).

With a similar analysis (not shown), we found that PDT did markedly better than IMM for 10 queries and markedly worse mous number of translations. Examining only the most likely translations for each query term in the 10 queries for which
PDT achieved a marked improvement, we found just two English query terms for which PDT assigned a much higher prob-ability to a good translation than IMM did:  X  X  X peration X  X  in Query 36 ( X  X  X emote operation robot X  X ), and  X  X  X lass X  X  in Query 24 ( X  X  X conomy class syndrome flight X  X ). This paucity of evidence suggests that other as-yet-uncharacterized effects must be responsible for the majority of the benefit that we see from PDT at high CDF thresholds.

Fig. 7 b shows the per-topic AP for the 22 queries in which the monolingual condition (i.e., Chinese queries) yielded a markedly higher AP than DAMM (there were also 8 cases in which DAMM was markedly better than the monolingual base-line). In aggregate, the MAP over those 22 queries is only 34% of the monolingual baseline, so those 22 queries account for almost all of the observed MAP difference between the monolingual baseline and DAMM over the full 50-query set. For each of those 22 queries we inspected the DAMM translations for each query term and identified the following factors that in our opinion had likely degraded CLIR effectiveness:
Incompatible tokenization . Alternative ways of tokenizing text are sometimes plausible, and this effect is particularly nota-ble for languages such as Chinese in which word boundaries are not marked when writing. For example, both  X  X  X im Dae Jun X  X  and  X  X  X im Jong Il X  X  in Query 3 correspond to three-character person names on the Chinese side of the parallel corpus.
These names were both correctly segmented as three-character terms. On the English side, each space-delimited word was tokenized, also correctly, resulting in three terms in each case (one for each token in the name). This created some problems because  X  X  X un X  X  is a common abbreviation in English that was aligned with the Chinese term for the month  X  X  X une X  X  more often than with the Chinese term for  X  X  X im Dae Jun X  X  and because  X  X  X l X  X  was more often aligned with the Chinese term optimal for this query. Similar problems arose in Queries 7, 11, 14, and 38.
 cause problems. For this Chinese test collection, all four cases involved proper names:  X  X  X ursk, X  X   X  X  X reenspan, X  X   X  X  X ennis X  X  and  X  X  X ito X  X  in Queries 6, 42, 30 and 30, respectively.

English vocabulary gaps : Some words not covered by the lexicon because they are not present sufficiently often in the par-allel corpus. In all three cases these were proper names  X  X  X ubka, X  X   X  X  X aru, X  X  and  X  X  X loveyou X  X  (the name of a virus) in Que-ries 23, 8, and 37, respectively. The effect on these queries was rather severe (all are on the left side of the Fig. 7 b, which is ordered by decreasing relative advantage of monolingual).

Domain differences . All of the other English query terms have one or more translations, but in some cases one or more appropriate Chinese translations of an English query term are simply not common enough in the parallel text to result in the right translations receiving much probability mass. Table 4 shows the queries containing these words and their translations. In our opinion, very few of the translations that are receiving probability mass from DAMM in these cases would be helpful.

Nearly half (10 of 22) of the queries in Fig. 7 b contain names of persons, and most of the rest (7 of 22) contain proper names of objects or organizations. Together, these queries that were difficult for DAMM account for most of the queries in the 50-query NTCIR-5 Chinese test collection that contain proper names (10 of 13 person names, 17 of 24 total proper names). For comparison, only 4 of the 25 topics in the TREC-9 Chinese collection contain organization or location names, include only 2 topics that contain organization or location names, and none that contains person names. We often think of the purpose of replicating experiments on a different test collection as seeing what will happen with a new set of documents.
As these statistics clearly indicate, however, seeing what happens with queries that are constructed in a different way can be equally important.

We can also use Fig. 7 b to see whether topic-specific CDF thresholds might yield further improvement. Although it is not clear how topic-specific thresholds would best be chosen, it is straightforward to bound the potential improvement by using post hoc optimal topic-specific thresholds as an oracle. Focusing on the same 22 queries on which the monolingual baseline obtained a markedly higher AP than DAMM, our oracle found that higher AP could be obtained at some CDF threshold below 0.99 for 19 of those 22 queries and that a CDF threshold above 0.99 was optimal for the remaining 3. In other words, although a CDF threshold of 0.99 was optimal when averaging over all 50 queries, it was not optimal for any of the 22 queries that we have identified as having the greatest potential for improvement! As the few middle (gray) bars in Fig. 7 b indicate, the mag-nitude of the potential gain in AP is in most cases very small. For Queries 7 and 41 the relative improvement in DAMM is quite large, however (even slightly exceeding monolingual MAP for topic 41). On average over all 50 topics, AP from DAMM rises from 66% of monolingual AP with a constant (0.99) threshold to 72% of monolingual AP with oracle topic-specific thresholds. From this we conclude that although CDF (and PMF) thresholds provide some degree of topic-specific behavior, further work on topic-tuned thresholds might be worthwhile.

Turning our attention to the 8 queries for which DAMM achieved a markedly higher AP than the monolingual condition (not shown) we see that as we would expect the right translations are being used. Specifically, for every English term in all eight of those queries the corresponding term from the Chinese query appeared among the DAMM translations, often with the highest translation probability. The additional benefit comes from the frequent presence of synonyms among the Chinese translations that (in our opinion) would provide a useful query expansion effect. In some sense this makes the MAP obtained using unexpanded monolingual queries an artificially low point of comparison, but we prefer an unexpanded monolingual reference because monolingual query expansion would introduce an additional source of variance (potentially harming some queries and helping others).

We expected that our decision in these experiments to threshold the two translation probability tables at a CDF of 0.99 before the DAMM computation would result in less sensitivity to specific threshold choices near the optimal value of 0.99. To check this, we looked across all 50 queries, finding only 12 that achieved their maximum AP at a CDF threshold at or above 0.99. Omitting 3 that yielded very low DAMM AP (peaking at 0.0003) and 1 that yielded very low monolingual AP (0.0002),
Fig. 8 shows the ratio of DAMM AP to monolingual AP for the remaining 8 queries. Except for Query 41, the AP of these eight queries is generally quite stable above a CDF threshold of 0.9. We therefore would not expect to see further improvements from exploring the region between 0.9 and 1.0 with greater granularity.

These English X  X hinese experiments confirm what we have seen before: DAMM is generally a good choice. Importantly, we have now seen that on a larger test collection (although we should note that the NTCIR-5 collection that we used is still far smaller than the Web-scale test collections that are now commonly used in monolingual experiments). Among our new contributions are detailed analysis of effectiveness-efficiency tradeoffs, better understanding of why DAMM, IMM, PSQ and
PDF behave as they do, and obtaining some indication that some further gain might be obtained from query-specific CDF thresholds. Along the way we have seen that the same CDF threshold (0.99) is optimal across for DAMM across several test vidual topics at higher threshold values. Our analysis also illuminates some issues that are of broader interest in CLIR re-search generally, most notably consequential differences in the prevalence of named entities among the queries when the two languages use different character sets. 4.2. New French experiments
Our Chinese experiments have told us what happens with larger test collections, but we do not yet know what we will see when we look beyond news to other types of content. In this section we therefore report on another new set of experiments, this time with the Cross-language Speech Retrieval (CL-SR) collection of the 2006 Cross-language Evaluation Forum (CLEF).
Queries written in French were used to retrieve manually partitioned segments of English interviews. The collection includes two parallel representations of the same content, one which we call the  X  X  X anual X  X  condition that was prepared entirely by subject matter experts, and the other that we call  X  X  X utomatic X  X  condition that was produced with an Automatic Speech Rec-ognition (ASR) system that had been tuned (on held-out data) to optimize recognition accuracy for the accented, elderly, do-main-specific speech content of the interviews (which were conducted with survivors of, witnesses to, or rescuers during the
Holocaust). 4.2.1. Training statistical translation models For comparability with our earlier work, we used the same models for the French X  X nglish language pair as in ( Wang &amp; Oard, 2006 ), although in this case the query and document languages are reversed (in our earlier experiment, we had used English queries and French documents). Recapping briefly, we derived word-to-word translation models from the European
Parliament Proceedings Parallel Corpus, known as Europarl Corpus ( Koehn, 2005 ),
Before word alignment we stripped accents from the French documents and we filtered implausible sentence alignments by eliminating sentence pairs that had a token ratio either smaller than 0.2 or larger than 5. GIZA++ was then run twice on the remaining 672,247 sentence pairs, first with English as the source language and subsequently with French as the source lan-guage. When training translation models, we started with five Hidden Markov Model (HMM) iterations, followed by ten IBM
English words to French words and the other from French words to English words. In contrast with our Chinese experiments, all nonzero values produced by GIZA++ were retained in each table. 4.2.2. Test collection
The  X  X  X ocuments X  X  in the CLEF 2006 CL-SR test collection correspond to 8,104 manually partitioned intervals (called  X  X  X eg-ments, X  X  which average about 4 min duration) from 272 interviews that were collected and indexed by the Survivors of the
Shoah Visual History Foundation (now the University of Southern California Shoah Foundation Institute for Visual History and Education) ( Oard et al., 2004; Oard et al., 2007 ). Three types of metadata were created for each segment as a part of the indexing process: the names of mentioned people were recorded (regardless of whether the person was actually men-tioned by name), some thesaurus keywords were assigned, and a somewhat stylized three-sentence summary was written that focused on providing  X  X  X ho, what, when, where X  X  information (and that was originally intended to be displayed with search results to support segment-level selection). We used the terms in these fields (formally, the NAME, MANUALKEY-
WORD and SUMMARY fields of the CLEF 2006 CL-SR collection, respectively) for the document representation that we call  X  X  X anual X  X  in this article. No distinction was made between the fields for this purpose; all were tokenized in the same way, and indexed together.

For our  X  X  X utomatic X  X  representation, ASR was used to generate a (potentially erroneous) one-best word transcript of what had been said in the interview. The ASR process (with a 38% measured word error rate on held out data) was optimized for the interviewee rather than the interviewer (by automatically detecting and then consistently using only the interviewee X  X  microphone) and was trained using 200 hours of in-domain held out data along with other standard ASR training resources effort to automatically capture some knowledge from the manual indexing process, these words were used as the feature set for two k-Nearest-Neighbor (kNN) classifiers trained (using cross-validation) to approximate human assignment of thesau-rus terms. The resulting kNN keywords were contained in the AUTOKEYWORD2004A1 field and the AUTOKEYWORD2004A2 field, respectively. A second ASR transcript (with a measured word error rate of 25% on held out data) that had been trained similarly to the first system, but using a later generation of ASR models, was also used to create the ASRTEXT2006A1 field (although that system was not used by the classifiers that automatically assigned thesaurus terms). We tokenized these four fields consistently and indexed the resulting terms together.
 The Porter stemmer was used to stem the English collections and the English side of the translation probability matrix.
The 33 French evaluation topics in the CLEF CL-SR 2006 test collection were created initially in English and then translated into French by bilingual speakers. We used the TITLE field (2 X 3 words) and the DESCRIPTION field (typically, one sentence) together as unstructured (i.e., bag of words) queries. Binary relevance judgments (relevant or not relevant) for segments that have previously been judged by subject matter experts are distributed with the test collection. The Perl Search Engine (PSE) with the same settings as in our NTCIR-5 experiments was used for retrieval. 4.2.3. Results
To facilitate cross-collection comparisons, we again report the fraction of monolingual MAP achieved by each CLIR system (monolingual MAP is 0.0466 for the  X  X  X utomatic X  X  condition and 0.2300 for the  X  X  X anual X  X  condition, respectively). shows the MAP of each MM variant at the best CDF threshold for the  X  X  X utomatic X  X  and  X  X  X anual X  X  conditions. For comparison, we also include in the table the results reported in Section 4.1 . For the automatic condition, the MAP of these MM variants ranges between 72% and 85% of monolingual MAP, all at a CDF threshold of 0.99. Wilcoxon signed rank tests for paired samples nique is statistically indistinguishable from the monolingual baseline. For the manual condition, the best MAP of each MM var-iant ranges between 71% and 80% of monolingual MAP, all except PSQ at a CDF threshold of 0.99 (PSQ peaked at 0.9). No significantly below the monolingual baseline. The relatively small number of queries (33) may have contributed to this failure have shown that relative effectiveness comparisons become more stable as more queries are added, and that more than 40 que-ries are typically needed to reliably make relatively fine-grained comparisons.
 Fig. 9 shows the effectiveness-efficiency tradeoff for sweeping CDF, PMF, and top-n thresholds, focusing again on DAMM,
PMF threshold seems to be a good choice for DAMM, both because DAMM seems to peak at a somewhat higher MAP and because that peak is more robust to differences in the resulting average number of translations. A top-n threshold seems like a good choice for PDT, which in this case starts outperforming the best DAMM results (DAMM with a PMF threshold) at an average of around 15 translations per query term in the  X  X  X anual X  X  condition and around 45 translations per query term in the  X  X  X utomatic X  X  condition, respectively. In contrast with the pattern seen for Chinese, when only one translation is used IMM now looks to be a better choice than PSQ for both the manual and the automatic conditions. For Chinese, IMM had not been far below PSQ for top-1 translation, so looking back over both sets of experiments, we can therefore recommend IMM (or perhaps PSQ) for one-best translation, DAMM when it is possible to use 3 X 5 translations, and PDT when efficiency is not a limiting factor.
 4.2.4. Query-specific analysis
As in our Chinese results above, we identified the topics whose peak DAMM Average Precision (AP) in the manual con-dition was substantially lower than the corresponding monolingual AP for the same topic, and then looked for factors that might explain the difference. There were four such topics, from which we identified the following factors:
Normalization error: Topic 3005 (English: Death marches; Experiences on the death marches conducted by the SS to evacuate the concentration camps as the allied armies approached. French: Les marches de la mort; Exp X riences concernant ces marches
According to our meaning matching lexicon, the French word  X  X  X arches X  X  has only one English synonym  X  X  X arkets X  X  (with a probability of 1). Clearly this is wrong, since a better translation would be  X  X  X arches. X  X  This error was almost certainly caused by stripping French accents before training the statistical translation models, since the French word  X  X  X arch X  X  X  can properly be translated as  X  X  X arket X  X .

Incompatible tokenization: Topic 1325 (English: Liberators of Concentration Camps;African X  X merican Liberators of Concentra-tion Camps. French: Les lib X rateurs des Camps de Concentration; La lib X ration des camps de concentration par les afro-am X -ricains. ): The English term  X  X  X frican X  X merican X  X  was expressed as  X  X  X fro-am X ricans X  X  in the French query. After tokenization this became  X  X  X fro X  X  and  X  X  X merican X  X  which wound up in different synonym sets in English, yielding poor results for DAMM.

Differing transliteration conventions: Topic 3013 (English: Yeshiva in Poland; Accounts regarding Poland X  X  Pre-war Yeshiva and its influence on its graduates and their descendants. French: Y X shiva en Pologne; R X cits  X voquant la Y X shiva de la Pologne d X  X vant-guerre, ainsi que son influence sur ses dipl X m X s et leurs descendants. ): Our meaning matching lexicon mapped
French word  X  X  X  X shiva X  X  to the English word  X  X  X eshiwa X  X  with a probability of 1, presumably because  X  X  X eshiva X  X  rather than  X  X  X eshiwa X  X  was used by convention in the European Parliament parallel corpus from which we learned our translation model.
 American DP camps, especially involving US Jewish chaplains. French: Activit X s au sein des camps de d X port X s am X ricains; aum X neries juives am X ricaines. ): The French word  X  X  X  X port X s X  X  was (correctly) translated by DAMM to the English stem  X  X  X eport, X  X  in part because the term  X  X  X P X  X  does not occur on the English side of the parallel text collection. However, the interviewers and interviewees consistently spoke of  X  X  X P camps X  X  in this context, and used  X  X  X eport X  X  and  X  X  X epartation X  X  mostly in other contexts. As a result, the English query did quite well with the manually created metadata (reliably find-ing the term  X  X  X P X  X ), while the French query yielded poor results.

That these are similar to the types of errors seen in Chinese suggests that error patterns do vary from one test collection to saw with the names in the NTCIR-5 Chinese test collection) or the nature of the languages involved (e.g., as we have seen with the differing transliterating conventions in both experiments). Moreover, as we would expect, none of these error types are specific to meaning matching; all are well known in CLIR research generally. From this we can conclude that our imple-mentations of bidirectional translation and synonymy have performed as we would expect given the design decisions that we have made. 4.3. Comparing six experiments
Table 6 compares the results from the six experiments that we have conducted to date, including our three previous experiments from Table 2 ( Wang, 2005; Wang &amp; Oard, 2006 ) and our three new experiments from Table 5 . each of the three experiments involving Chinese experiments.

The rightmost column in Table 6 provides an easy way of discerning general trends, so Table 6 is sorted by those mac-roaveraged values. Averaged over the six experiments, DAMM and PDT yield quite similar results at peak, with none of the observed differences in individual experiments being statistically significant; the choice between these two techniques thus turns more on efficiency (where DAMM has a decided advantage) than effectiveness. Other meaning matching methods yield reasonable results on both languages, with the exception of APSQ which does very poorly on all three Chinese test col-niques. Differences in specific cases should of course be interpreted with caution, but the consistent pattern of improvement over a broad range of conditions (two quite different language pairs, two types of sources (news and conversations), six col-lections, and over 300 topics) gives credibility to a broad conclusion that the choice to be made is between DAMM and PDT. 5. Conclusion
Excellent results have been reported for meaning matching in prior work ( Wang, 2005; Wang &amp; Oard, 2006 ), but the eval-uation framework that we use for information retrieval experimentation relies on replication to achieve high confidence in tant (and far more time consuming) to replicate across collections. Indeed, our experiments in this paper showed that PDT can be quite effective at peak, which had not been apparent in our earlier work. average number of translations per query term provides a useful proxy. Finally, we have looked in some detail at what is hap-we are seeing are fundamental to the methods we have tested and which are incidental to the way we have implemented our experiments.

As for what we might grandly call  X  X  X heory, X  X  we have presented the derivation of meaning matching in greater detail, thus highlighting the key role of normalization. This led to a new discussion of the tension between normalizing in the document translation direction, which we see as favorable for principled mapping of TF and normalizing in the query translation direc-tion, which we see as favorable for principled mapping of DF . For the experiments in this paper we simply chose one direc-tion and used it consistently (although in some side experiments we did confirm that choosing the other direction was not below.

Our experiments have led us to formulate some guidelines for practice as well. The use of top-1 translation is today quite common (often implemented by simply using Google Translate), and our results suggest that under those conditions trans-lating the queries is a better choice than translating the documents because that choice ensures that every query term will have an opportunity to influence the results. This comports well with present practice, which (at least in experimental set-tings) favors query translation. When access to the internals of the translation system is possible, our more complex DAMM than one-best translation, although at the expense of somewhat greater disk activity (because of using 3 X 5 times as many translations per query term). When efficiency is not a factor, we have also now seen cases in which the probability-weighted translation of document term counts that has been widely used with language modeling techniques for information retrieval minated a tradeoff that must be considered by designers of operational systems. Future work on effectiveness X  X fficiency tradeoffs for specific settings should, of course, also consider the amortized costs of constructing the translation model.
Our analysis results may also be of broader interest to CLIR researchers. For example, our observations on the relative predominance of named entities in different Chinese test collections may help others to identify which test collections are most suitable to the research questions they wish to explore. We also saw some evidence of the domain difference effects between the parallel texts on which our translation models were trained and the texts in our information retrieval test col-lections. Although this is a well known effect among machine translation researchers, we believe that it is an understudied issue in cross-language information retrieval research. Some previous studies have demonstrated substantial effects from lexicon size on retrieval effectiveness (e.g., McNamee, Mayfield, &amp; Nicholas, 2009; Demner-Fushman &amp; Oard, 2003 ), but we are not aware of comparable studies in which the effect of parallel corpus domain has been well characterized in CLIR experiments. There is now a considerable amount of work underway on what is broadly called  X  X  X omain adaptation X  X  in which the statistics learned from corpora that are not quite  X  X  X lose enough X  X  are adjusted in some way to make them  X  X  X loser X  X  X  X s that work matures, it may be possible to begin to apply the resulting techniques in a CLIR setting ( Daum X , 2007 ).
The most important new research question that we now need to grapple with is how best to address normalization. The issue arises from the assumption in our derivation of meaning matching that every possible shared meaning has an equal expressed than others. One approach to avoiding the normalization issue would be to find some reasonable way of estimat-
DF . The competing advantages of DAMM and PDT (at least on some test collections) suggest that these could be productive lines of inquiry.

The other place where normalization arises in our work is in our implementation of aggregation. The whole point of aggregation is to treat equivalent translations equivalently, and our term-oriented normalization indeed accomplishes that while permitting us to retain a term-oriented architecture. But the cost of that design is some reduction in fidelity when compared to what we actually wish to model, which is that sets of translations are (for our purposes) interchangeable. Mod-eling that situation more faithfully would require an architecture in which we index synonym sets that have potentially overlapping elements. That can be done, but at some cost in implementation complexity that would have added complexity to the comparisons between techniques that we have been able to make using a simpler term-oriented model.
Although not a focus of our work in this article, one clear implication of the meaning matching framework is an interac-tion between segmentation granularity, translation ambiguity, and recall-preserving generality that has not yet been well ular, it is well known that it is more effective to index both phrases and their component words than it would be to index phrases alone. Our present meaning matching framework assumes, however, that we have some specific tokenization pro-cess for the queries and for the documents. Future work on integrating evidence from multiple plausible tokenizations might be productive. The idea of bidirectional translation is also not unique to CLIR. Machine translation researchers leverage a comparable idea ( X  X  X lignment by agreement X  X ), which is now available as a replacement for GIZA++ in the Berkeley Aligner ( Liang, Taskar, &amp; Klein, 2006 ). Comparison of our implementations of IMM and DAMM with variants based on Berkeley align-ment results would be a logical first next step towards understanding the potential of these alignments in CLIR applications.
Of course, there are many other ways in which our work could be extended. For example, we chose not to use blind rel-our experiment designs straightforward enough for the analysis that we wished to perform, but in future work all of those techniques might productively be tried.

There are also many sources of evidence for synonymy that might be explored, including traditional sources such as stem-ming, WordNets and thesauri, and more recent developments such as learning term relationships from distributional statis-tics in a monolingual corpus, or extracting emergent term relationships from crowdsourced resources such as Wikipedia. In
Wu, 2008 ). It is also worth noting that for implementation convenience we leveraged synonymy after translation probabil-ities had been generated, but architectures in which putative synonyms are conflated before the translation probabilities are learned also deserve study. Our greedy approach to conflation might also be improved upon.
 Notably, we have to date relied on just one kind of evidence (term counts) and just one way of using that evidence (Okapi
BM25 term weights), but of course real applications can and should rely on a broader range of evidence. Indeed, experience with learning to rank for Web search has shown that non-content evidence (e.g., associating queries and clicks, learning ness. Investigating the relative contribution of meaning matching in such settings should therefore command the attention of those researchers who have access to more comprehensive feature sets.

Finally, the way in which we have modeled uncertainty in meaning matching would seem to have a clear applicability to other cases such as speech retrieval and retrieval of scanned documents based on Optical Character Recognition (OCR); in both settings, uncertainty naturally arises in the query-document matching process. The generative techniques that have that could likely be mitigated by modeling uncertainty as a translation process (even when the query and document lan-meanings. CLIR evolved in its early days through a stage in which the translation and retrieval components were seen as modules to be coupled. It was only when information researchers who were working with language models turned their attention to CLIR that closer integration began to be explored. We have now formalized that closer integration in a way that extends the technique to include ranking with Okapi BM25 weights, and further extension to other ranking techniques would be quite straightforward. But the story does not end there X  X he key idea is that representations of uncertainty have a natural place in the retrieval process, and with that in mind we are well positioned to think broadly about how best these techniques can be further extended, and applied in new ways.
 Acknowledgments
The authors thank Vedat Diker, Jimmy Lin, Jim Mayfield, Philip Resnik, Dagobert Soergel and the anonymous reviewers for their valuable comments. This work was supported in part by DARPA contracts N661010028910 (TIDES) and HR0011-06-2-0001 (GALE).
 References
