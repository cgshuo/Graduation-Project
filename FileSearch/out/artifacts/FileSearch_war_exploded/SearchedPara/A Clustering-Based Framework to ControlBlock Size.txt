 Entity resolution (ER) is a common data cleaning task that involves determining which records from one or more data sets refer to the same real-world entities. Because a pairwise comparison of all records scales quadratically with the num-ber of records in the data sets to be matched, it is common to use blocking or indexing techniques to reduce the num-ber of comparisons required. These techniques split the data sets into blocks and only records within blocks are compared with each other. Most existing blocking techniques do not provide control over the size of the generated blocks, despite this control being important in many practical applications of ER, such as privacy-preserving record linkage and real-time ER. We propose two novel hierarchical clustering ap-proaches which can generate blocks within a specified size range, and we present a penalty function which allows con-trol of the trade-off between block quality and block size in the clustering process. We evaluate our techniques on three real-world data sets and compare them against three base-line approaches. The results show our proposed techniques perform well on the measures of pairs completeness and re-duction ratio compared to the baseline approaches, while also satisfying the block size restrictions.
 H.2.8 [ Database management ]: Database applications X  Data mining ; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Clustering Blocking, indexing, data cleaning, record linkage.
Entity resolution (ER) is a common data cleaning and pre-processing task that aims to determine which records in one or more data sets refer to the same real-world en-tities [7]. It has numerous applications, including match-ing customer records following a corporate merger, detecting persons of interest for national security, or linking medical records from different health organisations. In many cases ER is performed in static mode where all matching decisions are conducted at once. However, it can also be performed in real-time, where the task involves finding the most similar record(s) to a given query record [18].

In both static and real-time ER, a key step is to compare the similarities of record pairs [2]. However, comparing all records in a data set, or between multiple data sets, scales quadratically with the number of records in the data sets and can be computationally infeasible if the data sets are large. As a result, blocking or indexing techniques are commonly used to limit the number of comparisons so that only record pairs that have a high likelihood of referring to the same real-world entity are compared [2]. This is done by dividing the data set(s) into (possibly overlapping) blocks and only performing comparisons between records in the same block.
In this paper we study the problem of how to control block sizes when generating blocks for ER. Our study is motivated by the observation that there are various application areas where maximum and minimum block sizes are important.
In this paper, we propose two recursive clustering ap-proaches for generating blocks within a specified size range. The idea behind our approaches is to use an initial blocking key to split a data set into individual blocks. If some of the blocks are too small we merge them, and if blocks are too large we use a second blocking key to split them. We merge any resulting small blocks, split any that are still too large using a third blocking key, and so on. Our two approaches differ in how we perform the clustering during the merge steps and as a result, give different distributions of block sizes, as we will present in Section 4.
 Motivating Example. Throughout the rest of this paper we make use of the example data set in Table 1 to help illustrate this process. Figure 1 shows the algorithm flow of our approaches on this small data set.
 Contributions. In this paper, we develop a novel blocking framework based on recursive agglomerative clustering to produce blocks in a specified size range. We then propose a novel penalty function which allows us to relax the hard block size restrictions and gives us control over the block generation process by selecting a trade-off between block size and block quality. We have conducted experiments on t hree real-world data sets and the results show our proposed approaches perform well on measures of block quality (both pairs completeness and reduction ratio [2]) in comparison to three baseline approaches, and can effectively generate blocks within the specified size range.
 Outline. We next discuss recent literature relating to iter-ative blocking and clustering methods. In Section 3 we de-scribe the notation we use and formally define our problem. In Sections 4 and 5 we describe our blocking approaches and a penalty function which allows a trade-off between block size and block quality. In Section 6 we conduct an experi-mental evaluation of our approaches and we finish with con-clusions and directions for future work in Section 7.
Blocking (also called indexing ) for ER is an area of active research and several recent surveys have been conducted [3, 7, 15]. In the following we briefly describe some key prior research that relates to our work, in particular, the blocking techniques that adopt an iterative approach or that aim to control the size of blocks.
 Several iterative blocking techniques have been studied for ER in recent years [5, 18, 23]. Whang et al. [23] proposed an iterative blocking process in order to perform ER. Rather than processing each block individually, the approach prop-agates the results from processed blocks (i.e. where records have been compared) to inform decisions in subsequent blocks. On ce two records are determined as a match, they are merged, and the resulting new record is propagated into other blocks where the combination of attributes may cause previously undetected matches to be found. The results of previous comparisons are stored so that comparisons are not repeated unnecessarily. However, these techniques give no control over the size of the blocks that are produced.

Das Sarma et al. [5] also developed an iterative blocking approach that combines splitting and merging to efficiently block large-scale data sets for ER. The work makes use of labelled training examples to generate blocking schemas in an automated fashion. The authors performed a post-processing step of merging small canopies (blocks) to in-crease recall based on a heuristic of minimising combined size and maximising the number of matches. While this technique gives some control of the block sizes, it does not enforce hard size contraints and also requires labelled train-ing examples, whereas our approaches are unsupervised.
Ramadan et al. [18] modified the sorted neighbourhood approach [2] for real-time ER to allow for updating a block-ing key value tree in response to a query record. The au-thors examined an adaptive window-size approach to vary the number of candidate records returned for comparison based on either a similarity or a size threshold. The sim-ilarity between neighbouring nodes in a tree can be pre-calculated to reduce query times. This approach does not enforce minimum and maximum size constraints nor does it generate individual blocks which makes it unsuitable for applications such as privacy-preserving record linkage.
Zhu et al. [25] examined the clustering problem under size constraints, although not in the context of ER. They proposed an approach to produce clusters of a certain size, which can also be relaxed to a size range. Nevertheless, the authors only tested their approach on small data sets that have less than one thousand records or no more than three clusters. Their approach also requires computing the com-plete similarity between all pairs of records in a data set, which limits its usefulness for blocking in ER tasks where the aim is specifically to avoid this complete pairwise com-parison. Work by Ganganath et al. [10], Malinen and Fr  X  anti [16] and Rebollo-Monedero et al. [19] have the same lim-itations. In contrast to their work, our approach aims to support larger data sets, and does not require a complete pairwise comparison of all records.
We assume that a data set R consists of records, each of which is associated with a set A of attributes. The value of an attribute a  X  A in a record r  X  R is denoted as r.a .
To split a set of records R x  X  R into blocks we make use of one or more blocking keys. A blocking key , k i,j = h a is a pair consisting of an attribute a i  X  A and a function f The function f j takes as input an attribute value and returns another value, such as a phonetic encoding, a substring, or a reversed string. For a given blocking key k i,j = h a i , f generate a blocking key value (BKV) for record r y  X  R x by applying function f j to r y .a i , denoted v i,j,y = f j example, possible functions include the first two characters ( F2 ), exact value ( Ext ) and a Soundex encoding ( Sdx ) [7].
To illustrate this using the example in Table 1, we con-sider the following blocking keys: the first two characters of the F irstN ame attribute h F N , F2 i , a Soundex encoding of the Surname attribute h SN , Sdx i and the exact value of the P ostcode attribute h P C , Ext i . The BKV of h F N , F2 i applied to r 1 is  X  X o X  (the first two characters of  X  X ohn X ), the BKV of h SN , Sdx i applied to r 1 is  X  X 530 X  (the Soundex en-coding of  X  X mith X ) and the BKV of h P C , Ext i applied to r is  X 2000 X .

To split a set of records R x into blocks we use a blocking key k i,j to generate a BKV v i,j,y for each r y  X  R x and we create one block for each unique BKV generated. We insert each record into the block corresponding to its BKV. This means two records r y , r z  X  R x will be inserted into the same block if and only if they generate the same BKV using blocking key k i,j , i.e. f j ( r y .a i ) = f j ( r our approaches we also need to merge blocks. This results in a single block being associated with multiple BKVs. We denote the set of BKVs associated with block b i as V ( b
Based on a pre-defined list of blocking keys K = h k i,j , k . . . i , we aim to adaptively split R into to a set of blocks B by using the BKVs generated by one or more blocking keys in K . However, we also want to control the size of the blocks we produce. The size of a block b , denoted as | b | , is the num-ber of records in the block. To control the size of blocks, we use two size parameters: s min and s max with s min  X  s max to specify the minimum and maximum block sizes that are permitted, respectively.
 Problem statement. Given a data set R , two size pa-rameters s min and s max , and a list of blocking keys K = h k i,j , k l,m , . . . i , the problem we study is to use K to parti-tion the records in R into a set B of non-overlapping blocks such that for each b  X  B , s min  X  | b |  X  s max , and within each block the number of true matches is maximised and the number of true non-matches is minimised.

In practice, s min and s max can be set in accordance with operational requirements such as computational limitations, real-time efficiency requirements, or privacy-preserving re-quirements. As is common with other blocking techniques, we can also improve the robustness of our approaches by running them multiple times with different lists of blocking keys for a single ER task [3]. This reduces the impact that a single typographical error or incorrect value has on the ER process [2]. In future work we intend to further investi-gate the impact of different blocking keys and whether the optimal list of keys can be discovered automically.
We propose two recursive clustering approaches for gener-ating blocks within a specified size range. The idea behind our approaches was illustrated in Figure 1. We iteratively split and merge blocks until the size parameters s min and s max are satisfied. The first approach processes blocks in order of decreasing block similarity (i.e., similarity-based), and the second approach in order of increasing block size (i.e., size-based). In Section 4.1 we briefly describe the way we calculate the similarity between BKVs as well as between blocks (clusters) during clustering, then in Sections 4.2 and 4.3 we describe our two approaches, and in Section 4.4 we discuss their respective advantages and disadvantages. During clustering we aim to merge blocks with similar BKVs together, since this is more likely to bring true matches together into the same block. This requires a way of mea-s uring the similarity between two BKVs. In addition, once blocks are merged, each block can be associated with multi-ple BKVs as shown in Figure 1, so we also require a way of combining the pairwise similarities between BKVs into an overall block similarity measure.

To calculate the similarity between two BKVs v 1 and v , denoted as  X  ( v 1 , v 2 ) we use traditional string compar-ison functions such as Jaro-Winkler or Jaccard similarity [2]. However, this does not always give good results for blocking keys using functions such as Soundex encodings or the first two characters of an attribute. For example, none of the above similarity functions give a good indication of the similarity between the Soundex codes  X  X 530 X  and  X  X 550. X  In order to obtain a better similarity measure, we use the original unencoded attribute values and apply a traditional string comparison function on them instead. For the above example we take the values that encode to  X  X 530 X  (such as  X  X mith X  and  X  X mythe X ) and compute their similarity with val-ues that encode to  X  X 550 X  (such as  X  X imon X  and  X  X imeon X ). If possible, we calculate all pairwise combinations of all values in a data set with these encodings to get a weighted average similarity between pairs of Soundex codes.

However, if the full pairwise calculation is computation-ally infeasible, we randomly sample a selection of original values for each code and take the average similarity between these. In practice we found that even small sample sizes still produced results that were nearly identical to those of the complete calculation. We discuss this further in Section 6.
To combine the pairwise similarity between BKVs into an overall block similarity measure, denoted as  X  ( b 1 , b we make use of three traditional approaches [24]: (1) sin-gle link  X  ( b 1 , b 2 ) = max( T ), (2) average link  X  ( b mean( T ) and (3) complete link  X  ( b 1 , b 2 ) = min( T ), where T = {  X  ( v 1 , v 2 ) : v 1  X  V ( b 1 ) and v 2  X  V ( b 2
The similarity-based blocking approach is described in Al-gorithm 1. To begin, we set n = 1 and take the set of records as R . We generate a set B of blocks using the n th blocking key in K (line 1). One block is created for each unique BKV. Next, B is partitioned into three disjoint sets B  X  , B  X  and B + , with b i  X  B  X  if | b i | &lt; s min , b s min  X | b i | X  s max and b i  X  B + if | b i | &gt; s max (line 2). We place each pair of blocks in B  X   X  B  X  into a priority queue Q , in order of their decreasing block similarity (lines 4-7). We retrieve from Q the pair of blocks ( b i , b j ) with maxi-mum  X  ( b i , b j ) (line 9). We merge b i and b j into b V ( b ij ) = V ( b i )  X  V ( b j ). We then calculate  X  ( b all b k s.t. | b k | + | b ij |  X  s max and reinsert these new pairs of blocks into Q (line 13). We then proceed with the pair of blocks with the second highest block similarity (loop back to line 9), and continue this process until no more merges are possible. For each b i  X  B + (i.e. blocks that are too large, | b | &gt; s max ) we call the algorithm recursively with b i new set of records and using the next blocking key in K to generate new BKVs (lines 16-18).

Figure 1 illustrates this process applied to the example data set in Table 1 with K = hh F N , F2 i , h SN , Sdx ii and s min = 2 and s max = 3. We start by splitting the records into blocks using the first blocking key h F N , F2 i (the first two characters of F irstN ame ). The blocks that have a size smaller than 2 ( s min ) are clustered and merged. Any blocks with size greater than 3 ( s max ) are split using the second blocking key h SN , Sdx i (the Soundex encoding of Surname ). Then, in a second merging phase, blocks that are smaller than size 2 ( s min ) are again clustered. In this case this finishes the algorithm since all blocks are now in the correct size range. However, if there were still blocks with size greater than 3 they would be split using a third blocking key, for example h P C , Ext i , any resulting small blocks would again be clustered and merged, and so forth. This continues until no blocks remain with size greater than 3 or we run out of blocking keys in K .

The main drawback of the similarity-based approach is the need to calculate  X  ( b i , b j ) for each pair of blocks in B  X   X  B  X  and store them in Q . In addition, as blocks are merged, the block similarity needs to be calculated between the new block and all remaining blocks. This reduces the scalability o f the approach and also leads to high memory overhead since Q can become large, O ( | B | 2 ). Next we present an alternative approach with better scalability that removes the need to store all pairwise combinations of blocks in memory.
The size-based blocking approach is described in Algo-rithm 2. The initial setup for this approach is identical to that of the similarity-based blocking approach. However, in the size-based case the priority queue Q contains individ-ual blocks, which are ordered by increasing block size (line 5). This is an important distinction since it significantly re-duces the size of Q from O ( | B | 2 ) to O ( | B | ). In the main loop of the algorithm (lines 6-13) we remove the smallest block b i from Q (line 7), determine the block b j such that | b | + | b j | X  s max and  X  1 ( b i , b j ) is maximised (line 8). Es-sentially we find the most similar block to b i such that their combined size would be less than s max . We merge b i and b j into b ij (line 9) and if | b ij | X  s min , we reinsert b Q . We then proceed to the next smallest block (loop back to line 6) and continue this process until no blocks remain with size less than s min . As with the similarity-based approach, for each block in B + the algorithm is called recursively with n = n + 1 and using the next blocking key in K .
We now discuss the characteristics of the two approaches and present their computational complexities. Depending on the settings of s min and s max , it is possible that our approaches may generate some blocks that are outside the desired size range. For example, if s min = 0 . 8  X  s max blocks may have a size in the range 0 . 5  X  s max to 0 . 8  X  s Merging any two of these blocks would result in a block size greater than s max , so none of them end up being merged. However, if s min and s max satisfy s max  X  2  X  s min then we are guaranteed that at most one block at the end will be smaller than s min because if two blocks were left, they could be merged as their combined size would still be below s max
If blocks are left at the end of either algorithm which are larger than s max , then there must exist some unique combination of BKVs that occurs more frequently than s max and our only option is to add another blocking key to K .
The similarity-based blocking approach ensures that pairs of blocks with high block similarity are merged together. In practice, the approach often creates many blocks that are close in size to s max which makes it effective for load bal-ancing in parellel ER applications [14]. However, if there is a block left at the end which is too small, it may be quite small in comparison to s min , which may make this approach less suitable in applications where s min is important. The run-ning time of the similarity-based approach is also typically longer than that of the size-based approach.

In practice, if s max  X  2  X  s min , the size-based approach tends to produce blocks that are more evenly distributed within the size range, with potentially a single block that is too small. Since the merging is done iteratively from small-est to largest, if there is a block that is smaller than s size is typically close to s min , although this closeness is not mathematically guaranteed. This means that for situations where minimum block size is important the size-based ap-proach is a good candidate. However, the size-based block-ing approach is not as successful when there are multiple large blocks with different BKVs from values that are quite similar. For example, depending on the blocking keys used, the first names  X  X ohn X  and  X  X ohnathon X  may generate differ-ent BKVs but we would prefer to combine them into the same block. However, because blocks are processed in order of size and both blocks may be quite large, neither block will be considered until late in the merging process. As a result, by the time they are compared one of them may have already grown too large (due to other merges) for them to be merged. This situation can be partially overcome by the penalty function detailed in the next section.

The selection of the blocking keys in K is important for both approaches and has a significant effect on the running time and the blocking quality. At present we rely on domain expertise to select the blocking keys, taking into account such factors as completeness, size of the domain, distribu-tion of values and general data quality. As part of our future work we intend to investigate methods for automatically se-lecting blocking keys, such as those developed by Kejriwal and Miranker [13] and Ramadan and Christen [17].

In the worst case, the time complexity of the similarity-based approach is O ( | R | 3 log( | R | )), while the size-based ap-proach is O ( | R | 3 ). For the similarity-based approach, Q can contain O ( | R | 2 ) blocks (line 7) and during the loop (lines 8 -15) we have to perform O ( | R | ) insertions into Q of time complexity O (log( | R | )) (line 13). For the size-based approach the size of Q is at most O ( | R | ) (line 5) but calcu-we end up with an overall complexity of O ( | R | 3 ).
In practice the similarity-based approach is significantly slower than the size-based approach. In addition the run-ning time of both approaches is much more dependent on the number of unique BKVs generated by the blocking keys in K rather than the size of R . This is because we create one block for each BKV during clustering so the running time of the similarity-based approach becomes O ( | B | 3 log ( | B | )) and the size-based approach becomes O ( | B | 3 ). In the worst case, each record generates a unique BKV and we end up with the asymptotic complexity above. Phonetic encodings such as Soundex and Double Metaphone [7], which have hard limits on the maximum number of unique BKVs they can create, can be particularly effective in this regard. Similarly, select-ing just the first one or two characters from an attribute also restricts the maximum number of blocks that can be created at each step. In addition, some optimisation techniques such as pre-calculating and caching similarity values can be per-formed to improve the efficiency of both techniques.
If two blocks have similar BKVs, then it may be prefer-able to merge them even if they are large, and use the next blocking key in K to split them and enforce the size restric-tions. We now present a penalty function that replaces the hard size restrictions ( s min and s max ) on merging blocks in our approaches with a sliding scale, that combines block size and block similarity to determine whether or not to merge two blocks. The penalty function  X  is as follows:
 X  ( b i , b j ) = 1  X   X   X  Two blocks b i and b j will be merged if they satisfy the inequality  X  ( b i , b j )  X   X  ( b i , b j ). As the combined block size gets larger, the similarity threshold required for merging also increases, and vice versa.

The penalty function involves two parameters,  X  and  X  , which together with s scale (related to s min and s max ), pro-duce the desired merging behaviour.
We next provide the idea behind the penalty function with reference to the examples in Figure 2. In each example s scale is set to 1,000 (the vertical dashed line). Consider the case where  X  = 2 and  X  = 0 represented by the curved dashed line from (0 , 0) to the top right corner in each example. Be-fore merging two blocks b i and b j where | b i | + | b j (i.e. | b i | + | b j | = 1,000), the similarity between the blocks must be at least 1  X  1 2 1 = 0 . 5. Before merging two blocks with a combined size of 2  X  s scale , the similarity must be at least 1  X  1 2 2 = 0 . 75. A size of 3  X  s scale requires similarity greater than 0 . 875, and so on.

The value of  X  determines the rate at which the required similarity approaches 1 . 0, with higher values approaching more quickly than lower values as shown in Figure 2(a). Changing the value of  X  has the effect of moving the curve to the left or right as shown in Figure 2(b). For example,  X  =  X  1 and  X  = 2 set a minimum similarity for merging to be 1  X  1 2 1 = 0 . 5. If  X  = 1 and  X  = 2, then blocks will be merged regardless of similarity until the combined size technique uses a window of constant size. is at least 1,000 (equal to s scale ). By combining different values of  X  and  X  we can obtain a wide variety of merging conditions as shown in Figure 2(c).

We now explain how best to choose the values of  X  ,  X  and s scale in order to achieve the desired merging behaviour. If minimum block size is not critical, the default we use on a data set is s scale = 0 . 5  X  s max ,  X  = 2 and  X  = 0. This sets a similarity threshold of 0 . 75 to merge blocks with combined size greater than s max and prevents blocks with very low similarity from being merged regardless of size. If minimum block size is important, then the default parameters we use are s scale = s min ,  X  = (2  X  s max ) / ( s max  X  s min ) and  X  = 1. This causes blocks to be merged regardless of similarity up to a combined size of s min , and sets a similarity threshold of 0 . 75 to merge blocks with a combined size larger than s max . In both cases, with some knowledge of the data, the value of  X  can be scaled to increase or decrease the similarity threshold of 0 . 75 as desired.

To incorporate the penalty function, both Algorithm 1 and 2 have to be slightly modified. In Algorithm 1, we replace the size restrictions on b i and b j in lines 4 -6 with the penalty function condition, and the same for b k and b in lines 11 and 12. In Algorithm 2, all blocks are inserted into Q in line 5, not just blocks with size less than s min Similarly b ij is always reinserted into Q in line 11, regardless of size. Additionally, in line 8, we replace the size restriction on b k with the penalty function condition on b i and b k We have evaluated our approaches on three data sets. (1) Cora: This is a public bibliographic data set of scientific papers that has previously been used to evaluate ER tech-niques [20]. This data set contains 1,295 records and truth data is available. (2) UKCD: This data set consists of cen-sus data for the years 1851 to 1901 in 10 year intervals for the town of Rawtenstall and surrounds in the United King-dom. It contains approximately 150,000 individual records of 32,000 households. A portion of this data (nearly 5,000 records) has been manually linked by domain experts. Fu et al. [9] have used this data set for household based group linkage where the task is to link households across time. (3) NCVR: This data set consists of voter registration data for million records consisting of the full name, address, age and other personal information of voters registered in the state. For most of our experiments we make use of a subset of this data set containing 447,898 records, named NCVR-450. We use the full data set to test the scalability of our approaches.
To evaluate our approaches we compared performance with standard blocking [8], Soundex encoding [7], and sorted neighbourhood based indexing [11]. For evaluation mea-sures we used pairs completeness and reduction ratio [2] and a combination of the two measures similar to F-Measure: Pairs Completeness (PC) = s M n where n M , n N , s M , s N correspond to the total number of matched pairs, the total number of non-matched pairs, the number of true matched candidate record pairs and the num-ber of true non-matched candidate pairs, respectively. s mi n -s max 20 -50 20 -100 50 -100 Similarity-based s mi n -s max 50 -100 100 -200 500 -1,000 Similarity-based s mi n -s max 500 -1,000 2,500 -5,000 5,000 -10,000 Similarity-based link, and complete link. The best value(s) in each row is shown in bold.
We do not explicitly model block quality. However, since merging blocks can only improve improve PC, we merge blocks until s max is reached, regardless of block quality. If higher quality blocks are preferred over larger blocks, this can be achieved by using the penalty function, where a min-imum similarity threshold will prevent blocks with a low likelihood of containing true matches from being merged, regardless of block size.

All our experiments were performed on a server with 6-core 64-bit Intel Xeon 2.4 GHz CPUs, 128 GBytes of mem-ory and running Ubuntu 14.04. All programs were written in Python 3. For similarity functions we used Jaro-Winkler for single proper name attributes (i.e. first name or last name) and q-gram based Jaccard similarity for other string attributes with q = 2 [2].
 Each of our experiments uses a single list of blocking keys. As with many blocking techniques, the overall results could be improved by combining the blocks generated from mul-tiple lists of blocking keys, with a corresponding reduction in s max so as to maintain any efficiency requirements. In future work we plan to investigate the automatic selection of blocking keys to reduce the need for domain expertise.
The experimental results on the Cora, UKCD, and NCVR-450 data sets are shown in Figure 3(a) -3(c). For Cora we set s min = 50, s max = 100 and K = hh T itle , Ext i , h Author , Ext ii . For UKCD we set s min = 500, s max = 1 , 000 and K = hh Surname , Ext i , h F irst N ame , Ext i , h Birth P arish , Ext ii . For NCVR-450 we set s min = 500, s max = 1 , 000 and K = hh Surname , F2 i , h F irst N ame , F2 ii .
 On all three data sets, we achieve equal or better F-Measure values than the three baseline approaches. This indicates that our approaches achieve comparable blocking quality to other common blocking techniques. However, the main focus of our approaches was to satisfy the block size restrictions while achieving high quality blocking. We also show the distribution of block sizes generated by our ap-proaches in Figure 3(d) -3(f). As can be seen from the results, both our approaches produce blocks in the required size range, 500 -1,000 records for UKCD and NCVR-450, and 50 -100 records for Cora. While the size-based approach tends to distribute the block sizes throughout the interval [ s min , s max ], the similarity-based approach tends to generate the majority of blocks with size close to s max . This means it creates fewer blocks overall and makes it appropriate for parallel ER applications.

We tested different parameter settings for our approaches to examine how sensitive they are to changing s min , s max and the block similarity measure  X  , and the results are shown in Table 2. In most cases, the choice of block similarity mea-sure  X  has minimal effect on the results. However, complete link did not work well with the size-based approach, partic-ularly on the Cora data set. Changing s min and s max affects the trade-off between PC and RR as expected.

We tested the penalty function and the results are shown in Figure 4. For Cora we set s scale = 50 and s max = 100, and for UKCD and NCVR-450 we set s scale = 500 and s max = 1 , 000. When  X  = 0 (no minimum block size or minimum similarity threshold), the penalty function gener-ally achieves the best combination of PC and RR values, the exception being for low values of  X  where the similarity threshold is very low, even for large blocks which results in poor RR values. High values of  X  and negative values of  X  mean the similarity threshold to perform any merging is high. This essentially negates the clustering steps of the al-gorithms, which results in poor PC values for data sets with lower data quality. High values of  X  in combination with positive values of  X  produce generally balanced blocks. We note that for the UKCD data set, setting  X  = 1 . 1 performs very poorly. It repeatedly merges many blocks in each iter-ation of the algorithm and either runs out of blocking keys (resulting in poor RR values), or has to use attributes that have poor data quality (resulting in poor PC values). For the NCVR-450 data set, the penalty function produces very similar results regardless of the settings for  X  and  X  . The merging of blocks has less impact on the NCVR-450 data set, since it is relatively clean so merges do not increase PC values substantially, and also large enough that it requires many merges to reduce RR values significantly.

We also tested the scalability of our approaches using sub-sets of different sizes of the entire NCVR data set. We set s min = 500, s max = 1 , 000 and K = hh Surname , F2 i , h F irst N ame , F2 ii and the results are shown in Figure 5(a). As can be seen, even though the asymptotic complexity of each approach is cubic or worse, because functions such as F2 or Sdx generate a limited number of BKVs the scalability is still nearly linear in practice. However, in the future we plan to optimise both approaches to improve their scalability.
We compared the total number of candidate pairs gener-ated as well as the largest block generated by the different approaches and the results are shown in Figure 5(b) and Figure 5(c). Controlling the maximum block size ensures that the total number of candidate pairs increases linearly with the size of the data set which means that once the data set becomes large, our techniques generate fewer candidate pairs than the traditional and Soundex based approaches. As a result, even though our approaches increase the time required for blocking compared to the baseline approachs, in general this will be more than made up for by a reduction i n the time required to perform the matching.

In addition, the worst case block size is also controlled by our approaches. This means that if the blocking is be-ing performed as a pre-processing step for an ER technique with scalability worse than quadratic, such as Markov logic networks [20], or privacy-preserving record linkage [22], then the time saving will be even greater than that indicated by the reduction in the number of candidate pairs. Controlling the worst-case block size means that our techniques are suit-able for real-time ER, where operational requirements limit the number of comparisons that can be performed [18].
Finally, we investigated the impact of the sample size in the similarity calculations on the NCVR-450 data set us-ing Soundex encodings. Even with a sample size of 1, the clustering still produced similar results to the complete cal-culation and the reduction in F-Measure was less than 0 . 1% in all cases. As a result, we conclude that the sample size does not significantly affect the performance.
In this paper we have developed two novel recursive clus-tering approaches which can generate blocks for ER within a given size range. We have also proposed a penalty function which allows us to control the trade-off between block size and block quality, and fine tune either approach. We have evaluated our approaches on three data sets. Our experi-mental results show that both our techniques perform well in comparison to the baseline approaches and create blocks in the required size range.

In the future, we intend to extend the current work in several directions. First, we hope to investigate the pos-sibility of automatically selecting the blocking keys using techniques similar to Kejriwal and Miranker [13]. We also aim to investigate optimisations to the algorithms and the use of different clustering techniques, to improve the quality of the results and the scalability of our approaches. This work was partially funded by the Australian Research Council, Veda, and Funnelback Pty. Ltd., under Linkage Project LP100200079. [1] I. Bhattacharya and L. Getoor. Collective entity [2] P. Christen. Data matching: concepts and techniques [3] P. Christen. A survey of indexing techniques for [4] P. Christen. Preparation of a real temporal voter data [5] A. Das Sarma, A. Jain, A. Machanavajjhala, and [6] X. Dong, A. Halevy, and J. Madhavan. Reference [7] A. K. Elmagarmid, P. G. Ipeirotis, and V. S. Verykios. [8] I. P. Fellegi and A. B. Sunter. A theory for record [9] Z. Fu, P. Christen, and J. Zhou. A graph matching [10] N. Ganganath, C.-T. Cheng, and C. Tse. Data [11] M. A. Hernandez and S. J. Stolfo. Real-world data is [12] D. Kalashnikov and S. Mehrotra. Domain-independent [13] M. Kejriwal and D. P. Miranker. An unsupervised [14] H. Kim and D. Lee. Parallel linkage. In ACM CIKM , [15] H. K  X  opcke and E. Rahm. Frameworks for entity [16] M. Malinen and P. Fr  X  anti. Balanced k-means for [17] B. Ramadan and P. Christen. Unsupervised blocking [18] B. Ramadan, P. Christen, and H. Liang. Dynamic [19] D. Rebollo-Monedero, M. Sol  X e, J. Nin, and J. Forn  X e. A [20] P. Singla and P. Domingos. Entity resolution with [21] D. Vatsalan and P. Christen. Sorted nearest [22] D. Vatsalan, P. Christen, and V. S. Verykios. A [23] S. E. Whang, D. Menestrina, G. Koutrika, [24] R. Xu and I. Wunsch, D. Survey of clustering [25] S. Zhu, D. Wang, and T. Li. Data clustering with size
