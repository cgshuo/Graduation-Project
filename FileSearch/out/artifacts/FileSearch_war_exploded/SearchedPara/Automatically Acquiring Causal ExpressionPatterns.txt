 RYUICHIRO HIGASHINAKA and HIDEKI ISOZAKI NTT Communication Science Laboratories, NTT Corporation 1. INTRODUCTION Following the trend of non-factoid QA, we are seeing the emer gence of work on why-QA; i.e., answering generic  X  X hy X? X  questions [Verber ne 2006]. However, since why-QA is an inherently difficult problem, there have o nly been a small number of fully implemented systems dedicated to solving it . Recently, the NTCIR-6 1 Question Answering Challenge (QAC-4) was held to encourage the research and development of Japanese nonfactoid QA systems , attracting 14 systems from eight groups.
 causes as answer candidates. The most common approach for th is is to use hand-crafted patterns that comprise typical cue phrases or POS-tag sequences related to causality. For example, Fukumoto et al. [2007] ut ilizes a number of hand-crafted patterns to extract cause-bearing passage s. Such patterns in-clude tame and node , which are typical cue words in Japanese corresponding to  X  X ecause X  in English, as well as words and phrases expressin g causality, such as genin (cause) and shiin (cause of death). In fact, almost all systems pre-sented at QAC-4 relied on such hand-crafted patterns for ans wer extraction. matically extracted from corpora. We considered this appro ach to be necessary because, as noted in Inui and Okumura [2005], causes are expr essed in var-ious forms, which makes it difficult to cover all causal expre ssions by hand. Hand-crafting is also very costly if we want to increase the c overage of causal expressions.
 automatically acquires causal expressions from relation-annotated corpora to derive causal expression patterns in order to improve why-Q A. We also utilize a machine learning technique to train an answer-candidate r anker on the ba-sis of the features created from the causal expression patte rns together with other possible features related to causality so that the QA p erformance can be maximized with regard to a corpus of why-questions and answe rs.
 why-QA, and Section 3 describes our approach. Section 4 desc ribes the ac-tual procedure for automatically acquiring causal express ion patterns from relation-annotated corpora. Section 5 describes the imple mentation of our ap-proach, and Section 6 presents evaluation results. Section 7 presents a detailed analysis of the results by examining instances of successfu l and unsuccessful cases. Section 8 summarizes and mentions future work. 2. PREVIOUS WORK Although systems that can answer why-questions are emergin g, they tend to have limitations in that they can answer questions only with causal verbs [Girju 2003], in specific domains Khoo et al. [2000], or cover ed by a specific knowledge base [Curtis et al. 2005]. Recently, Verberne [20 06, 2007a] has been intensively working on why-QA based on the Rhetorical S tructure Theory (RST) [Mann and Thompson 1988]. However, her approach requi res manually annotated corpora with RST relations.
 tions, we only find a small number of such systems even if we inc lude those at QAC-4. Since why-QA would be a challenging task when tackled straightfor-wardly, requiring common-sense knowledge and semantic int erpretation of questions and answer candidates, current systems place hig her priority on achievability and therefore use hand-crafted patterns and heuristics to extract causal expressions as answer candidates and use convention al sentence sim-ilarity metrics for answer candidate evaluation [Fukumoto 2007; Mori et al. 2007; Shima and Mitamura 2007]. We argue in this article that this hand-crafting effort can be reduced using automatic methods.
 tect causal expressions. In the CoNLL-2005 shared task (SRL for English), the best system found causal adjuncts with a reasonable accurac y of 65% [M ` arquez et al. 2005]. However, when we analyzed the data, we found tha t more than half of the causal adjuncts contain explicit cues such as  X  because . X  Since causes are expressed by a wide variety of linguistic phenomena, not just explicit cues [Inui and Okumura 2005], further verification is needed befo re SRL can be safely used for why-QA.
 questions are observed in many FAQ sites, such sites have bee n regarded as valuable resources for the development of non-factoid QA sy stems. Examples include Burke et al. [1997], who used FAQ corpora to analyze q uestions to achieve accurate question-type matching; Soricut and Bril l [2006], who used them to train statistical models for answer evaluation and f ormulation; and Mizuno et al. [2007], who used them to train classifiers of que stion and answer-types. However, they do not focus on why-questions and do not use any causal knowledge, which is considered to be useful for explicit why -questions [Soricut and Brill 2006]. 3. APPROACH We propose automatically acquiring causal expression patt erns in order to re-duce the hand-crafting effort that is currently necessary. We first collect causal expressions from corpora and convert them into causal expre ssion patterns. We use these patterns to create features to represent answer candidates. The features are then used to train an answer candidate ranker th at maximizes the QA performance with regards to a corpus of why-questions and answers. We also enumerate possible features whose incorporation in th e training improves the QA performance.
 in [Verberne 2007b; Verberne et al. 2007], we consider the ta sk of why-QA to be a sentence/paragraph extraction task. Although pinpoin ting exact answers may be desirable, there has not been an agreed-upon answerin g unit for why-QA. In definition QA, which is also a type of non-factoid QA, co ncise phrases or information nuggets are extracted as answers [Dang and Lin 2 007]; however, they may not be suitable because causes may be expressed diff erently from definitions. Therefore, in this study, we start with sentenc es/paragraphs, both of which are basic units in natural language. We also assume t hat a document retrieval module of a system returns top-N documents for a qu estion on the basis of conventional information retrieval (IR) related m etrics and regard all sentences/paragraphs extracted from them as answer candid ates. Hence, the task becomes the ranking of given sentences/paragraphs.
 swer, the candidate should (1) have an expression indicatin g a cause, (2) be similar to the question in content, and (3) some causal relat ion should be ob-served between the candidate and the question. For example, an answer candi-date  X  X  was arrested for fraud X  is likely to be a correct answe r to the question  X  X hy was X arrested? X  because  X  X or fraud X  expresses a cause, the question and the answer are both about the same event (X being arrested ), and  X  X raud X  and  X  X rrest X  indicate a causal relation between the questio n and the candi-date. Condition (3) would be especially useful when the cand idates do not have obvious cues or topically similar words/phrases to the question; it may be worthwhile to rely on some prior causal knowledge to select o ne over others. Although current working systems [Fukumoto 2007; Mori et al . 2007] do not explicitly state these conditions, they can be regarded as u sing hand-crafted patterns for (1) and (3). 2 Lexical similarity metrics, such as cosine similarity and n-gram overlaps, are generally used for (2).
 larity, and causal relation features that encode how it comp lies with the three conditions. Here, the causal expression features are those based on the causal expression patterns we aim to acquire automatically. For th e other two types of features, we turn to the existing similarity metrics and d ictionaries to de-rive features that would be useful for why-QA. To train a rank er, we create a corpus of why-questions and answers and adopt one of the mach ine learning algorithms for ranking. The following sections describe ho w we extract causal expression patterns from corpora, the three types of featur es, the corpus cre-ation, and the ranker. 3.1 Extracting Causal Expression Patterns from Corpora With the increasing attention paid to SRL, we currently have several corpora, such as PropBank [Palmer 2005] and FrameNet [Baker et al. 199 8], that are tagged with semantic relations including a causal relation . We came up with two ways to automatically derive causal expression pattern s from such corpora (see Section 4 for the instances of the patterns we derived).
 spans are guaranteed to be expressing a cause, they provide g ood instances of causal expressions. By abstracting such causal expressi ons, we can create causal expression patterns. For example, if we have a causal expression  X  X or the suspicion of fraud X  in a corpus, we can create a causal exp ression pattern such as [for the NN of NN] by converting it into a POS-tag sequence. Note that, in this example, the prepositions and the definite arti cle were preserved in order to avoid making the pattern too generic.
 niques. In the relation-annotated corpora, there are sente nces that are an-notated with a causal relation as well as those that are not. W e can use the patterns that frequently occur in the sentences annotated w ith a causal rela-tion as our causal expression patterns. For example, if a POS -tag sequence such as [for the NN of NN] occurs frequently in sentences annotated with a causal relation to a significant degree, we can accept it as a c ausal expression pattern. This process corresponds to pattern mining, and, f or this purpose, we could apply one of the existing pattern mining methods. Comp ared to the pat-terns based on abstracted text spans, the resulting pattern s of this approach could offer better precision because counter examples (sen tences without a causal relation) are taken into account in the mining proces s. 3.2 Features terns, we create n binary features from n causal expression patterns with each feature representing whether an answer candidate matches e ach pattern. In addition, some why-QA systems may already possess some good hand-crafted patterns for the detection of causal expressions. Since the re is no reason not to use them if we know they are useful for why-QA, we can create a b inary fea-ture indicating whether an answer candidate matches existi ng hand-crafted patterns. candidate share many words, it is likely that they are about t he same content. From this assumption, we create a feature that encodes the le xical similarity of an answer candidate to the question. To calculate its value, existing sentence similarity metrics, such as cosine similarity or n-gram ove rlaps, can be used. uments as answer candidates because the existence of query t erms in the answer candidates does not necessarily mean that they are so lely eligible as answers. We want to include as our answer candidates sentenc es/paragraphs that may have implicit references to the content of the quest ion by, for ex-ample, reference/anaphoric expressions. Therefore, we al so need to be able to calculate the content similarity between a question and an a nswer candidate even if they do not share the same words.
 likely to be similar in content. To express this case as a feat ure, we can use the similarity of the question and the document in which the answ er candidate is found. Since the documents from which we extract answer cand idates typically have scores output by an IR engine that encode their relevanc e to the question, we can use this score or simply the rank of the retrieved docum ent as a feature. same content with different expressions. The simplest case is when synonyms are used to describe the same content; e.g., when  X  X rrest X  is used instead of  X  X pprehend. X  For such cases, we can exploit existing thesau ri. We can create a feature encoding whether synonyms of words in the question are found in the answer candidate. We could also use the value of semantic similarity and relatedness measures [Pedersen et al. 2004] or the existenc e of hypernym or hyponym relations as features. mantic relation between concepts is indicated. For example , the EDR concept dictionary 3 shows whether a causal relation holds between two concepts; e.g., between  X  X urder X  and  X  X rrest. X  Using such dictionaries, we can create pairs of expressions, one expression in a pair indicating a cause and the other its effect. If we find an expression for a cause in the answer candidate and that for an effect in the question, it is likely that they hold a causal re lation. Therefore, we can create a feature encoding whether this is the case. In c ases where such semantic lexicons are not available, they may be automatica lly constructed, although with noise, using causal mining techniques such as Marcu and Echihabi [2002], Girju [2003], and Chang and Choi [2004]. 3.3 Creating a QA Corpus For ranker training, we need a corpus of why-questions and an swers. Because we regard the task of why-QA as a ranking of given sentences/p aragraphs, it is best to prepare the corpus in the same setting. Therefore, we use the following procedure to create the corpus: (a) create a question, (b) us e an IR engine to retrieve documents for the question, (c) select among all se ntences/paragraphs in the retrieved documents those that contain the answer to t he question, and (d) store the question and a set of selected sentences/parag raphs with their document IDs as answers. 3.4 Training a Ranker Having created the QA corpus, we can apply existing machine l earning algo-rithms for ranking, such as RankBoost [Freund et al. 2003] or Ranking SVM [Joachims 2002], so that the selected sentences/paragraph s are preferred to nonselected ones on the basis of their features. Good rankin g would result in good Mean Reciprocal Rank (MRR), which is one of the most comm only used measures in QA. 4. ACQUIRING CAUSAL EXPRESSION PATTERNS We used the EDR corpus as our relation-annotated corpus. The EDR corpus is a part of the EDR dictionary, which is a suite of corpora and dictionaries and includes the EDR corpus, the EDR concept dictionary (hie rarchy and re-lation of word senses), and the EDR Japanese word dictionary (sense to word mappings). As far as we know, it is one of the most commonly use d Japanese corpus annotated with semantic relations. We first briefly de scribe the EDR corpus and then describe how we derived our causal expressio n patterns. 4.1 The EDR Corpus The EDR corpus is a collection of independent Japanese sente nces taken from various sources, such as newspaper articles, magazines, an d dictionary glosses. The corpus has a semantic representation for each sentence a nd this informa-tion can be used as a relation annotation. For example, the ED R corpus has the following sentence: 4 (1) Obon no kiseikyaku wo hakobu koukuubin de pus has for this sentence. The semantic representation has a tree structure. The leaf nodes are composed of words that have corresponding concept IDs in the EDR concept dictionary; that is, words that do not bear co ncept IDs, such as functional words, do not appear in the tree structure. Nod es headed by object , cause , and modifier indicate that they hold object, causal, and modi-fying relations to the main nodes in their siblings. For example, sora (sky) and koukuubin (aircrafts) have object and cause relations to konzatsu (crowd), meaning that the sky is crowded by reason of aircrafts. 4.2 Patterns by Abstracting Text Spans From the semantic representation, it is possible to identif y the text spans cor-responding to the node that has a causal ( cause ) relation. To find the text span for the cause node in Figure 1, we first extract word positions under that no de; namely 1, 3 X 4, 6, and 8. By taking its minimum and maximum word posi-tions, we can obtain a text span by concatenating words 1 X 8; n amely, Obon no kiseikyaku wo hakobu koukuubin . Although text spans created this way may be sufficient, considering that bunsetsu 5 is a commonly used linguistic unit in Japanese language, we expand each end of the span to its bunse tsu bound-aries. In this example, there is no need to expand the left end of the span since it is already at the beginning of the sentence, but there is a b unsetsu boundary one word after koukuubin ; between de and konzatsu . Therefore, we expand the text span to include de to obtain Obon no kiseikyaku wo hakobu koukuubin de as our desired text span. To detect bunsetsu boundaries, we u sed Cabocha, 6 a Japanese dependency analyzer relation although the structure of the EDR corpus is slightl y different from those of PropBank and FrameNet. Out of all 207,802 sentences in the EDR corpus, there are 8,379 sentences annotated with a causal re lation. Since 82 sentences required elements outside them to complete words under a causal relation node due to anaphora and ellipsis, we used the remai ning 8,297 sen-tences to obtain the causal text spans. From the 8,297 senten ces, we found 8,774 text spans, with each sentence producing one or more te xt spans. lar to the previous pattern-based approaches on QA [Ravicha ndran and Hovy 2002; Cui et al. 2007], we abstracted them by leaving only the functional words (auxiliary verbs and case, aspect, tense markers) and repla cing others with wild-cards  X * X . We chose this abstraction because function al words indicate important grammatical functions in Japanese and because in cluding content words such as nouns and verbs could jeopardize the generalit y of the patterns when considering the relatively small number of text spans f ound in the cor-pus. By this abstraction, we obtain a pattern [no * wo * de] fro m Obon no kiseikyaku wo hakobu koukuubin de . 7 We also used Cabocha to perform this abstraction. From the 8,774 text spans, we obtained 402 dist inct causal ex-pression patterns after filtering out those that occurred on ly once. We call the patterns acquired by abstracting text spans the ATS (abstra cted text span) patterns.
 cause it mainly utilizes the characteristics of Japanese, i n which tenses, as-pects, and modalities are expressed mainly with functional affixes, we believe we can reasonably perform a similar abstraction for other la nguages, for ex-ample, by making use of verbal inflections and auxiliary verb s in the case of English.
 that the patterns that have de (by) are very frequent. Although we can also observe many typical causal cue words such as niyoru , niyotte , kara , and tame (all of which correspond to  X  X ecause X  in English), it is inte resting that patterns that do not contain such cue words are also frequent; e.g., [ ni (-DAT)], [ no (-GEN) * ni (-DAT)] and [* wa (-TOPIC)], reconfirming the variety of causal expressions as pointed out in Inui and Okumura [2005] and als o indicating possible insufficiency of hand-crafting approaches. 4.3 Mining Syntactic Patterns by BACT Since syntactic patterns have been found useful for extract ing causal expres-sions [Khoo et al. 2000], we decided to mine syntactic patter ns. We mine such patterns by finding syntactic structures that dominantly oc cur in the causally annotated sentences in the EDR corpus. There are 8,379 sente nces that have causal relations and 199,423 sentences that do not.
 a machine learning algorithm based on a tree-mining techniq ue. BACT mines, from positively or negatively labeled trees, subtrees that are useful for the clas-sification of the trees using a boosting-based algorithm [Br eiman 1999]. Since a syntactic structure has a tree representation, BACT can be directly applied. BACT has been used to mine syntactic patterns useful for text classification [Kudo and Matsumoto 2004] and anaphora resolution [Iida et a l. 2006], in which syntactic information plays an important role.
 tion to the syntactic structures to improve generality. The abstraction was performed in the following three steps (See Figure 2 for an ex ample of how we make a syntactic structure for  X  X  was arrested for fraud X ).
 ing them as positive or negative on the basis of whether they h ave a causal relation, we processed the structures with BACT, which mine d useful sub-trees and produced 669 syntactic patterns. We call these pat terns the BACT patterns.
 Note that the patterns are to be read from right to left follow ing the structure of our dependency tree (See Figure 2).
 in the top 10 together with many other patterns having POS-ta gs and seman-tic categories. The pattern with the heaviest weight has N-1 398 [doubt] prob-ably because of the many crime-related sentences in the EDR c orpus, which is mainly composed of newspaper articles. We also see semant ic categories such as N-1265 [wonder/astonishment], N-2115 [shaking] (e .g., earthquakes), N-2419 [types of illness], and N-2558 [activity] that we can intuitively recog-nize as sources of causes. Surprisingly, the Japanese punct uation mark (  X  ) was found to be a useful indication of a cause (See pattern # 9). 8 This is probably because English-style punctuations were used in the senten ces from dictionary glosses where causes are less likely to be expressed. 5. IMPLEMENTATION We created a Japanese why-QA system that implements our appr oach. The system is called NAZEQA ( X  X aze X  means  X  X hy X  in Japanese). Th e system was built by extending our factoid QA system, SAIQA [Isozaki 200 4, 2005]. The system works as follows: (1) The question is analyzed by a rule-based question analys is component to (2) Using the disjunction of query terms as a query, the docum ent retrieval (3) The feature extraction component produces, for each ans wer candidate, (4) The SVM ranker trained by a QA corpus ranks the answer cand idates on (5) The top-N answer candidates are presented to the user as a nswers. In the following sections, we show our list of features and de scribe the QA corpus and ranker. 5.1 Features Causal Expression Features:  X  X UTO-ATS-Causal Expression Features: We have 402 ATS patt erns.
  X  X UTO-BACT-Causal Expression Features: We have 669 BACT pa tterns.  X  X AN-Causal Expression Feature: We emulate the manually cr eated pat-Content Similarity Features:  X  X uestion-Candidate Cosine Similarity Feature: We use the cosine similar- X  X uestion-Document Relevance Feature: We use, as a feature , the inverse  X  X ynonym Pair Feature: This is a binary feature that indicat es whether Causal Relation Feature:  X  X ause-Effect Pair Feature: This is a binary feature that in dicates whether 5.2 WHYQA Collection Since QAC-4 does not provide official answer sets and their qu estions include only a small number of why-questions, we created a corpus of w hy-questions and answers on our own.
 ated questions from articles randomly extracted from Maini chi newspaper arti-cles (1998 X 2001). Then, for each question, she created sent ence-level answers by selecting the sentences that she considered to fully incl ude the answer from a list of sentences from top-20 documents returned from the t ext retrieval en-gine with the question as input. Paragraph-level answers we re automatically created from the sentence-level answers by selecting the pa ragraphs contain-ing the answer sentences.
 existing declarative sentences into interrogatives. It to ok approximately five months to create 1,000 question-and-answer sets (called th e WHYQA collec-tion). All questions in the collection are guaranteed to hav e answers. Figure 3 shows a sample question and its answer sentences in the colle ction. 5.3 Training a Ranker by Ranking SVM We trained ranking models using the idea of ranking SVM [Joac hims 2002]. The ranking SVM learns ranking by utilizing the preferences in the pairs of training samples. Suppose that an answer candidate x i is ranked higher than making it possible for the SVM to obtain w from training data { +1 , x i  X  x j } . In the ranking phrase, input samples are simply ranked by the ir scores w x . Although SVM-light 9 is a widely used implementation for the ranking SVM, we implemented the equivalent using Pegasos [Shalev-Shwar tz et al. 2007], which is an efficient linear kernel SVM training algorithm.
 level answers as well as nonanswers from the WHYQA collectio n. Here, nonan-swers are the sentences/paragraphs that were not selected a s answers by the analyst in the top-20 documents as described in Section 5.2. When we regard sentences as answers, there are 4,849 answers and 521,177 no nanswers in the WHYQA collection. In the case of paragraphs, there are 4,371 answers and 261,215 nonanswers.
 feature sets; namely, NOAC, ATS, BACT, and ALL. The NOAC feat ure set does not use any features related to the automatically acquired c ausal expression patterns; namely, it uses the MAN-Causal Expression, Quest ion-Candidate Cosine Similarity, Question-Document Relevance, Synonym Pair, and Cause-Effect Pair features. The ATS feature set uses all features w ithout AUTO-BACT-Causal Expression features and the BACT feature set us es all features without AUTO-ATS-Causal Expression features. The ALL feat ure set uses all features. The NOAC, ATS, BACT, and ALL feature sets comprise 5, 407 (402 + 5), 674 (669 + 5), and 1076 (402 + 669 + 5) features, respective ly. x  X  for all pairs of answers x + and nonanswers x  X  for each question in the WHYQA collection. Finally, using the difference vectors fo r all questions as training data, we trained our ranking models using Pegasos. 6. EVALUATION For evaluation, we compared the proposed system (NAZEQA) wi th three base-lines. We created three versions of NAZEQA (NAZEQA-ATS, NAZ EQA-BACT, and NAZEQA-ALL) depending on the feature set used to train th e ranker (the ranking model). The aim of having these three versions is to e xamine the ef-fects of each type of automatically extracted causal expres sion patterns and also to examine whether the combination of the two types of pa tterns can lead to improvement.
 similarity between an answer candidate and a question based on frequency vectors of their content words. The aim of having this baseli ne is to see how the system performs without any use of causal knowledge. Bas eline-2 (FK) uses hand-crafted patterns described in Fukumoto [2007] to narrow down the answer candidates to those having explicit causal expressi ons, which are then ranked by the cosine similarity to the question. Baseline-3 (NOAC) uses the ranking models trained by using the NOAC feature set. Since N OAC does not utilize the automatically extracted causal expression patterns, we do not regard it as a version of NAZEQA. The aim of having this baseli ne is to see how the system optimizes the ranking performance by the ranking SVM without the automatically extracted causal expression patterns.
 to obtain the top-20 documents from Mainichi newspaper arti cles (1998 X 2001) and ranked the sentences or paragraphs in these documents. 6.1 QA Performance in MRR and Coverage We made each system output the top-1, -5, -10, and -20 answer s entences and paragraphs for all 1,000 questions in the WHYQA collecti on. We used the MRR and coverage as the evaluation metrics. Coverage means the rate of questions that can be answered by at least one of the top-N a nswer candi-dates. Tables III and IV show the MRRs and coverage for the bas elines and NAZEQA. A ten-fold cross validation was used for the evaluat ion of NAZEQA and NOAC; that is, we first split the WHYQA collection into ten sets and then trained ranking models using nine of the ten sets and evaluat ed the perfor-mance using the remaining set; this was repeated ten times in a round-robin fashion.
 NAZEQA-ALL are better in all comparisons to the baselines. A statistical test (a sign test that compares the number of times one system plac es the correct answer before the other) showed that they are significantly b etter ( p &lt; 0 . 01) than all baselines for all top-Ns in the sentence and paragra ph-levels. In addi-tion, NOAC, which does not use the automatically extracted c ausal expression patterns, performs significantly worse than the NAZEQA syst ems, which do use them, showing the effectiveness of the automatically ac quired causal ex-pression patterns.
 performs better in all cases, showing the effectiveness of m ining causal ex-pression patterns. We see no remarkable difference between NAZEQA-BACT and NAZEQA-ALL. They are mostly tied in the sentence-level a nd NAZEQA-BACT leads slightly in the paragraph-level. It seems that us ing the ATS patterns in addition to the BACT patterns does not contribut e greatly to the QA performance. Our analysis revealed that this limited performance of NAZEQA-BACT comes from its inability to assign appropriate scores to an-swer candidates with multiple pattern matches, because, in our current imple-mentation, combinations of the patterns are not taken into a ccount due to the limited expressiveness of the linear kernel used in our trai ning of the ranking models using the ranking SVM. Polynomial kernels might solv e this problem. The coverage is also high for NAZEQA, making it possible to fin d correct an-swers within the top-10 sentences and top-5 paragraphs for m ore than 50% of the questions.
 NAZEQA systems when we evaluated them using the why-questio ns in the 100 questions of the QAC-4 formal run. We identified 33 questi ons as why-questions; namely, Q2, 5 X 8, 10, 12, 15, 26, 29 X 31, 35, 37, 40 X  41, 51, 53, 61 X 62, 64 X 65, 68, 70 X 71, 73, 79, 87, 90, 92, 96, and 98 X 99. We chose th em as why-questions because two independent labelers, who are not the authors, agreed that they are asking for causes. The agreement ratio (Cohen X  s  X  ) was high with 0.91. The answers for the 33 questions were created in th e same manner as the WHYQA collection by the same analyst. The ranking mode ls used by NOAC, NAZEQA-ATS, NAZEQA-BACT, and NAZEQA-ALL are those tr ained using the entire WHYQA collection.
 and the NAZEQA systems due to the small number of questions; h owever, in terms of figures, NAZEQA-ATS shows the best performance in th e sentence-level and FK performs best in the paragraph-level. It is also noticeable that NAZEQA-BACT does not perform as well as it does for the WHYQA c ollec-tion. Since FK and NAZEQA-ATS, which rely mainly on surface p atterns, show good performance compared to NAZEQA-BACT, which utili zes syntac-tic and semantic information, we suspect that the 33 questio ns in QAC-4 were those that can be answered by focusing on their surface expre ssions, espe-cially causal cue words, rather than their syntactic struct ures or semantic in-formation. This does not mean that NAZEQA-BACT cannot ident ify causal cue words. Remember that our automatically mined causal expres sion patterns in-clude many such cue words (Table II). Our brief analysis of NA ZEQA-BACT X  X  answers revealed that, for the particular questions of QAC-4, NAZEQA-BACT finds many irrelevant matches with its complex patterns to co me up with an-swers, while the correct answer can be simply obtained by usi ng a few explicit causal cue words.
 for all questions in the WHYQA collection for the baselines a nd the NAZEQA systems. The distribution of COS is almost uniform, indicat ing that lexical similarity cannot be directly translated into causality. N OAC shows a similar tendency due to its heavy reliance on the content similarity features. The fig-ure also shows that the NAZEQA systems consistently outperf orm FK. Among the NAZEQA systems, NAZEQA-BACT leads slightly in the numbe r of top-1 answers. 6.2 Impact of the Features It is interesting to know how each type of feature contribute s to the QA performance. Table VII shows how the performance of NAZEQA-ATS and NAZEQA-BACT in MRR (top 5) changes when one type of feature is excluded in the ranker training.
 AUTO-BACT-Causal Expression features in NAZEQA X  X  compari son to NOAC in the previous section. Note that the performance without t hese features is the same as that of NOAC (see Table III). In addition, we see significant drops in performance when we remove the Question-Candidate Cosine Simi-larity and Document-Question Relevance features, showing the effectiveness of lexical and topic similarity.
 tribute much to the performance. One of the reasons for the sm all contribution of the MAN-Causal Expression feature may be that the manual p atterns used to create this feature overlap greatly with the automatical ly collected causal expression patterns, lowering the impact of the MAN-Causal Expression feature.
 way the answers were created in the creation of the WHYQA coll ection. Since the answer candidates from which the expert chose the answer s were those retrieved by a text retrieval engine that uses lexical simil arity to retrieve rel-evant documents, it is possible that the answers that contai n synonyms had already been filtered out in the beginning, making the Synony m Pair feature less effective.
 is because, although the performance does not change when th is feature is removed from NAZEQA-BACT, the performance seems to degra de in the sentence-level and vice versa in the paragraph-level when i t is removed from NAZEQA-ATS. Our interpretation is that, although the Cause -Effect Pair fea-ture is generally effective, overfitting to the training dat a occurred in the sentence-level, namely, the comparative impact of the exis tence of a cause-effect pair is likely to be bigger for sentences than for para graphs considering their short length.
 Pair feature when it is removed from NAZEQA-BACT, we also nee d to ver-ify the quality of our cause-effect word pairs because we bli ndly expanded concepts holding a causal relation into corresponding word s in creating the pairs; when the concepts have broad senses, their lexicaliz ations may not nec-essarily hold a causal relation. For example, we have  X  satsujin (murder) X  and  X  taiho suru (arrest) X  as a cause-effect word pair, but we also have  X  keru (kick) X  and  X  taiho suru (arrest). X  Here, taiho suru corresponds to a concept ID 3ce77a (an act of seizing a person who breaks the law; arrest) and keru 3cf10d (to refuse one X  X  request). Although keru is one lexicalization of a refusal (e.g.,  X  hito no iken wo keru (kick one X  X  opinion) X ), the word itself has other meanings (e.g., kicking a ball) and would not necessarily lead to an ar rest. weights given to the features by the ranking SVM. Table VIII s hows the weights of the top-10 features. We also include in the table t he weights of the MAN-Causal Expression and Cause-Effect Pair features s o that the role of all types of features in our approach can be seen. The analy zed model was the one trained trained with all 1,000 questions in the WH YQA collection with paragraphs as answers using the ATS feature set. Just as indicated in Table VII, the Question-Candidate Cosine Similarity featu re plays the key role, followed by the Document-Question Relevance feature and the ATS patterns.
 ture set (see Table IX). Here, the analyzed model was the one t rained with all 1,000 questions in the WHYQA collection with sentences a s answers. It is noticeable that many semantic categories, such as N-2329 [p ollution], N-2522 [confusion], N-1246 [hunger and thirst], and N-2419 [types of illness], are in-cluded in the table. We also have semantic categories, such a s N-1702 [invita-tion] (e.g., invoking of events), N-1301 [detest/dislike] , N-1321 [respect/value/a high regard], and N-2265 [increase] in the top 20. Since they represent events that are likely to generate an effect, it strongly suggests t he importance of having some prior knowledge about sources of causes for bett er why-QA. patterns that have semantic categories together with funct ional words, such as BACT-Exp.[ de (by) General-Noun N-2419 [types of illness]], BACT-Exp.[V erb N-1259 [pain/hardship]  X  ni (-DAT)], and BACT-Exp.[ ga (-NOM) General-Noun N-2518 [situation/prospects]]. We consider that these sem antic categories are used to disambiguate the usage of the functional words. For e xample, de is known to have more than ten usages (e.g., by, for, with, at, et c.) [Ishiwata 1999; Kiyota and Kurohashi 2001], and this ambiguity makes i t difficult to decide when it is used for a causal relation. The BACT pattern s seem to use the semantic categories around functional words to distinguis h contexts in which de can be used as a causal cue. 6.3 Effects of Training Data Size on QA Performance It may be useful to know how much training data is needed to tra in a ranker. We therefore fixed the test set to Q1 X  X 100 in the WHYQA collect ion and trained rankers with nine different sizes of training data ( 100 X 900) created from Q101 X  { Q200 Q1000 } . Figure 5 shows the learning curve. We used the BACT feature set for the training, and sentences were use d as answers. Naturally, the performance improves as we increase the data . However, the performance gains begin to decrease when the training data s ize exceeds 500, possibly indicating a limitation of our current implementa tion. 7. ANALYSIS OF ANSWERABLE QUESTIONS Although it has been shown that the NAZEQA systems consisten tly outper-form the baselines for the 1,000 questions in the WHYQA colle ction, when we evaluated them using the 33 why-questions of QAC-4 in the par agraph-level answers, FK showed better performance than the NAZEQA syste ms. Although this difference was not statistically significant, examini ng these cases closely could lead to further improvement of our approach.
 the top-1 sentence-level answers by FK, NAZEQA-ATS, and NAZ EQA-BACT. Q7 was answered correctly by all systems, indicating that it was probably the easiest why-question in QAC-4. In our analysis, we look in de tail at Q2 where only FK succeeds and Q92 where only NAZEQA-BACT succeeds.
 ATS, and NAZEQA-BACT for Q2. Contributions of the features i dentified by the ranking SVM are also shown for NAZEQA-ATS and NAZEQA-BAC T. In this question, FK used the relatively high cosine similarit y of 0.424 and the existence of a cue word tame to come up with the correct answer, whereas NAZEQA-ATS and NAZEQA-BACT chose answer candidates with a s imilar level of cosine similarity, but with the many matching ATS or BACT pat-terns. Since many automatically derived patterns are obser ved dominantly as their contributing factors, we consider that the weights for them might have been over-tuned to the corpus, causing an adverse effec t. For example,  X   X , the Japanese punctuation mark, shows a significant contri bution. This is mainly because the headlines of newspaper articles, whic h do not end with the Japanese punctuation mark, were seldom selected as answ ers in the QA corpus, showing heavy reliance on the training data. It is po ssible that the 1,000 questions may not be enough for training ranking model s or that more abstraction of the patterns, such as generalizing over styl istic variations, may be necessary to suppress this overfitting.
 NAZEQA-BACT for Q92. Here, although the cosine similarity i s low with 0.137, NAZEQA-BACT came up with the correct answer on the bas is of many matches with the BACT patterns, including the one with a sema ntic category corresponding to a cause (N-2450 [cause]). This semantic ca tegory came from a word  X  genin (cause) X  in the candidate that strongly expresses a cause. A l-though FK uses a list of causal words for answer extraction an d the list con-tains  X  genin , X  it could not come up with this candidate because the patter ns of FK select candidates where such cue words appear only afte r specific func-tional words such as ni, ga, wo, toiu , and toitta . By restricting the context of causal words, FK aims to extract answer candidates with high precision, but seemingly at the cost of recall. This reconfirms the difficult y of cyclopaedically covering all causal expressions by hand.
 answered by the top-5 sentence and paragraph-level answers by FK, NAZEQA-ATS, and NAZEQA-BACT for all questions in the WHYQA collecti on. From the good number of questions that can only be answered by NAZE QA-ATS and NAZEQA-BACT, the effectiveness of our approach can be se en. It is also interesting to see a big overlap between NAZEQA-ATS and NAZE QA-BACT compared to their small overlaps with FK, showing that NAZEQ A-ATS and NAZEQA-BACT have similar ranking models and that such model s greatly differ from a ranking process conceived and implemented by h umans. 8. SUMMARY AND FUTURE WORK This article described our approach for why-QA, which we ini tially intro-duced at QAC-4. We automatically obtained causal expressio n patterns from relation-annotated corpora by abstracting text spans that are annotated with a causal relation and also by mining syntactic patterns that are useful in dis-tinguishing sentences annotated with a causal relation fro m those annotated with other relations.
 candidates, and used these features together with other pos sible features re-lated to causality to train an answer-candidate ranker that maximizes the QA performance with regards to the corpus of why-questions and answers. performs baselines with a MRR (top 5) of 0.223 when sentences are used as answers and with a MRR (top 5) of 0.326 when paragraphs are use d as an-swers, making it presumably the best-performing fully impl emented why-QA system. The usefulness of the automatically acquired causa l expression pat-terns was also verified.
 tigate other possible features that may be useful for why-QA . For example, lexical similarity can be more accurately calculated by inc orporating IDF val-ues of the words as well as N-gram overlaps. We also need to inv estigate the quality of our synonyms and cause-effect word pairs because their usefulness was found to be limited in our analyses. In this work, we focus ed only on the  X  cause  X  relation in the EDR corpus to obtain causal expressions. Ho w-ever, there are other relations, such as  X  purpose  X , that may also be related to causality [Verberne 2006].
 worth verifying it by creating an English version of NAZEQA b ased on causal expression patterns that can be derived from PropBank and Fr ameNet. Fi-nally, we are planning to make public some of the WHYQA collec tion at the authors X  Web page so that various why-QA systems can be compa red. We thank Jun Suzuki, Kohji Dohsaka, Masaaki Nagata, and all m embers of the Knowledge Processing Research Group for helpful commen ts and discus-sions. We also thank the anonymous reviewers for their valua ble comments and suggestions.

