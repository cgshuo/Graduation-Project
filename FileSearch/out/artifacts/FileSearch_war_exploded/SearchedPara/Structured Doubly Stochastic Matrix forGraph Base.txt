 As one of the most significant machine learning topics, clustering has been extensively employed in various kinds of area. Its preva-lent application in scientific research as well as industrial practice has drawn high attention in this day and age. A multitude of clus-tering methods have been developed, among which the graph based clustering method using the affinity matrix has been laid great em-phasis on. Recent research work used the doubly stochastic ma-trix to normalize the input affinity matrix and enhance the graph based clustering models. Although the doubly stochastic matrix can improve the clustering performance, the clustering structure in the doubly stochastic matrix is not clear as expected. Thus, post-processing step is required to extract the final clustering results, which may not be optimal. To address this problem, in this pa-per, we propose a novel convex model to learn the structured dou-bly stochastic matrix by imposing low-rank constraint on the graph Laplacian matrix. Our new structured doubly stochastic matrix can explicitly uncover the clustering structure and encode the probabil-ities of pair-wise data points to be connected, such that the clus-tering results are enhanced. An efficient optimization algorithm is derived to solve our new objective. Also, we provide theoretical discussions that when the input differs, our method possesses in-teresting connections with K -means and spectral graph cut models respectively. We conduct experiments on both synthetic and bench-mark datasets to validate the performance of our proposed method. The empirical results demonstrate that our model provides an ap-proach to better solving the K -mean clustering problem. By using the cluster indicator provided by our model as initialization, K -means converges to a smaller objective function value with better clustering performance. Moreover, we compare the clustering per-formance of our model with spectral clustering and related double stochastic model. On all datasets, our method performs equally or better than the related methods.  X 
To whom all correspondence should be addressed. This work was partially supported by US NSF-IIS 1117965, NSF-IIS 1302675, NSF-IIS 1344152, NSF-DBI 1356628, NIH R01 AG049371.
  X 
Theory of computation  X  Unsupervised learning and cluster-ing; Doubly Stochastic Matrix; Graph Laplacian; K -means Clustering; Spectral Clustering
Graph based learning is one of the central research topics in ma-chine learning. These models use similarity graph as input and per-form learning tasks over the graph, such as spectral clustering [17], manifold based dimensional reduction [2, 12], graph-based semi-supervised learning [30, 31], etc . Because the input data graph is often associated to the affinity matrix (the pair-wise similarities be-tween data points), graph based learning methods show promising performance and have been introduced to many applications in data mining [20, 19, 18, 1, 6]. The graph-based learning methods de-pend on the input affinity matrix, thus the quality of the input data graph is crucial in achieving the final superior learning solutions.
To enhance the learning results, graph based learning methods often need pre-processing on the affinity matrix. The doubly stochas-tic matrix (also called bistochastic matrix) was utilized to normalize the affinity matrix and showed promising clustering results [29]. A doubly stochastic matrix S  X  n  X  n is a square matrix and all ele-ments in S satisfy:
Some previous works have been proposed to learn the best dou-bly stochastic approximation to a given affinity matrix [29, 26, 11]. Imposing the double stochastic constraints can properly normal-ize the affinity matrix such that the data graph is more suitable for clustering tasks. However, even with using the doubly stochastic matrix, the final clustering structure is still not obvious in the data graph. The graph based clustering methods often use K -means al-gorithm to post-process the clustering results to get the clustering indicators, thus the final clustering results are dependent on and sensitive to the initializations.

To address these challenges, in this paper, we propose a novel model to learn the structured doubly stochastic matrix, which en-codes the probability of each pair of data points to be connected. We explicitly constrain the rank of the graph Laplacian and guar-antee the number of connected components in the graph to be ex-actly k , i.e. the number of clusters. Thus, the clustering results can be directly obtained without the post-processing step, such that the clustering results are superior and stable. To solve the proposed new objective, we introduce an efficient optimization algorithm. Meanwhile, we also provide theoretical analysis on the connection between our method and K -means clustering as well as spectral clustering, respectively.

We conduct extensive experiments on both synthetic and bench-mark datasets to evaluate the performance of our method. We find that our method provides a good initialization for K -means clus-tering such that a smaller objective function value can be achieved for the K -means problem. Also, we compare the clustering per-formance of our model with other methods. On all datasets, our method performs equally or better than related methods.
Notations: Throughout this paper, matrices are all written as uppercase letters while vectors as bold lower case letters. For a matrix M  X  d  X  n , its i -th row, j -th column and ij -th element are denoted by m i , m j and m ij respectively. The Frobenius norm of M is defined as M F = known as the nuclear norm) is defined as M  X  = where  X  i is the i -th singular value of M . For a vector v when p =0 , its p -norm v p is defined as v p =( Specially, 1 represents a vector whose elements are 1 consistently and I stands for the identity matrix.
To achieve good clustering results, doubly stochastic matrix is usually utilized to approximate the given affinity matrix such that the clustering structure in data graph can be maintained. However, final clustering structures are still not obvious in the data graph and post-processing step has to be employed, which leads the clustering results to be not optimal. To address this problem and learn a more powerful doubly stochastic matrix, we propose a new structured doubly stochastic model to capture clustering structure and encode probabilistic data similarity.
The ideal clustering structure of a graph with n data points is to have exactly k connected components, where k is the number of clusters. If we reshuffle the n data points in the similarity ma-trix of the ideal graph such that points in the same connected com-ponents are arranged together, then the k connected components form k blocks ranging along the diagonal of the similarity ma-trix. With such ideal structure we can immediately obtain clus-tering indicators without post-processing steps. Thus, we propose a novel method to learn the new structured doubly stochastic ma-trix which has such ideal clustering structure with exactly k blocks. From [15, 8], we know that each connected component in the graph W corresponds to an eigenvalue 0 in the Laplacian matrix L D
W  X  W . D W is the degree matrix of graph W defined as D W Diag ( agonal elements formed by the entries of vector a .If W has exactly k blocks, then k eigenvalues of L W are zeros, i.e. the rank of L is equal to n  X  k .

Therefore, given the affinity matrix W , we propose to learn a new structured doubly stochastic matrix M  X  n  X  n such that its Laplacian matrix L M = D M  X  M is restricted to be rank ( L n  X  k . With this constraint, the learnt M has the clustering block structure along the diagonal with proper permutation, based on which we can directly partition the data points into k clusters.
Moreover, to make M encode the probability of each pair of data points to be connected in the graph, we normalize M 1 = 1 .Asa result, the entire probability for a point to connect with others is 1. Consequently, we have D M = I . Meanwhile, since M represents the probability of each pair of data points to be connected, we nat-urally expect the learnt M is symmetric and nonnegative. These constraints validate the doubly stochastic property of matrix M .
Under our new constraints, we learn a structured doubly stochas-tic matrix M to best approximate the affinity matrix W by solving:
Theoretically, in the ideal case the probability of a certain point to correlate with points in the same cluster should be the same. That is, suppose there are n i points in the i -th cluster, for any two points p s and p n in the i -th cluster, the probability of p to be connected is m sn = 1 n i . Consistently, for point p m according to Lemma 1 a large enough parameter r forces the ele-ments in each block of matrix M to be the same.

L EMMA 1. In the following problem: if the value of r tends to be infinity, the matrix M is block diagonal with elements in each block to be the same. The number of blocks in matrix M is k .

Proof :If r tends to infinity, the following optimization problem is equivalent to
According to the previous discussion, with proper rearrangement of rows and columns of M , the constraint rank ( L M )= n require the structure of M to be k blocks diagonally arranged like:  X   X   X   X   X 
Meanwhile, let X  X  we denote a random row (or column) of one block in M as m T ( or m ) . Since when all m i are of the same value makes the left and right sides be equal. Thus, the solution to minimize Problem (4) is that M is a block diagonal matrix with elements in each block to be the same.
In addition, to highlight the constraint that M has exactly k blocks, we add another constraint as Tr ( M )= k and have:
The constraint rank ( L M )= n  X  k makes Problem (5) non-convex. Therefore, we use the trace norm of L M as a relaxation form of rank ( L M ) . Our final objective is to solve:
It is not trivial to solve our new objective J opt in Eq. (6). We will propose a novel algorithm to solve the new objective. Before that, we first show the interesting connection between our model and spectral clustering.
T HEOREM 1. In Problem (5), if r  X  X  X  and W is doubly stochastic, then Problem (5) is equivalent to spectral clustering. Proof : As illustrated in [25], given a graph G with n points and its affinity matrix W  X  n  X  n , the definition of graph cut is: where k is the number of clusters, C l is the l -th cluster and is the complement subset of cluster C l in graph G , s ( M,N )=
For Ratio Cut, the function  X  ( C l ) in Eq. (7) is defined as:
We introduce an indicator vector q l  X  n ( l =1 , 2 ,  X  X  X  G belongs to the l -th cluster, and 0 otherwise.

With the indicator vector q l ,wehave:
Along with Eq. (8), we can rewrite the cut function of Ratio Cut in Eq. (7) as:
Define matrix G  X  n  X  k such that g l = q l . We introduce an indicator matrix F :
Assume x i  X  C i and x j  X  C j . For the F matrix in Eq. (10), we can observe that if C i = C j , then f ij = 1  X  n i ; otherwise f where n i denotes the number of data points in the i -th cluster. Thus, the cut function (9) can be written as:
If W is doubly stochastic, then D W = I . In this case, Nor-malized Cut is equivalent to Ratio Cut and tackles the following problem:
Let M = FF T , the Problem (12) becomes:
We can directly find that m ij = 1 n i when C i = C j ; while m ij =0 when C i = C j , where x i  X  C i and x j  X  C j .

Thus, the spectral clustering problem is to find a matrix M min-imizing Problem (13). From the definition of M , we can obtain some properties of M that M  X  0 , M T = M and M 1 = 1 , thus it is doubly stochastic. As for Tr ( M ) ,wehave:
In practice, matrix W may not always be doubly stochastic. Given affinity matrix W 0 , we can learn a doubly stochastic matrix W as the initialization by solving:
The above problem is the same as the problem solved in [29] and also a special case of our proposed objective in Eq. (6) when both parameter  X  and r are set as 0.

Previous method in [29] can only learn a doubly stochastic ma-trix without clear clustering structure. After adding the regulariza-tion terms, our new objective can achieve a better doubly stochastic matrix with clear clustering structure to improve the clustering re-sults.
We use the Augmented Lagrange Multiplier (ALM) optimization strategy [4] to solve our new objective J opt in Eq. (6). Here we introduce a slack variable L that L = I  X  M , then Problem (6) can be rewritten as: and Problem (16) is equivalent to: where  X   X  n  X  n is the Lagrange multiplier and  X  is the penalty parameter for Eq. (17).

Compared with Problem (6), Problem (17) is easier to solve since the trace norm term  X  L  X  is now independent to M . We intro-duce an efficient alternating algorithm to tackle Problem (17).
The first step is fixing L and solving M , thus Problem (17) becomes: Let the Problem (18) can be rewritten as:
Since the constraint on Tr ( M ) is only concerned with the diag-onal elements, Problem (19) can be divided into two subproblems: and where m = diag ( M ) and diag ( M ) denotes a vector formed by the diagonal elements of M .

Our strategy is to solve two subproblems, Problem (20) and Prob-lem (21) alternately, and let their solutions project mutually. In each iteration, we solve Problem (20) first and let its solution M the T matrix in Problem (21), afterwards we solve Problem (21) and let its solution M 2 play the role of matrix T in Problem (20). We solve these two problems alternately and iteratively until M converges.

According to Von Neumann X  X  successive projection lemma [16], this mutual projection strategy we use will converge to the cross of two subspaces formed by Problems (20) and (21). The lemma theoretically ensures that the solution of the alternate projection strategy ultimately converges onto the global optimal solution of Problem (19).
 According to Lemma 2 in Appendix A, the optimal solution of Problem (20) is as follows: As far as Problem (21) is concerned, firstly we let matrix T in Problem (21) equal to the solution of M to Problem (20) (shown in Eq. (22)). Then according to Lemma 3 in Appendix B, the optimal solution of Problem (21) is:
Alternately we solve Problems (20) and (21) till M converges onto its global optimal solution.

The second step is fixing M and solving L , then Problem (17) becomes: Let N =( I  X  M + 1  X   X ) , Problem (24) becomes According to [5], the solution of Problem (25) is: where the singular value decomposition of N is N = U  X  V T Diag ((  X  i  X   X   X  ) + ) is a diagonal matrix with i -th diagonal element
Our algorithm to solve the new objective is summarized in Al-gorithm 1.
 Convergence and Complexity Analysis: The convergence of ALM algorithm was proved and discussed in previous papers. Please Algorithm 1 Proposed Algorithm Input: The given affinity matrix W  X  n  X  n ; The number of clusters k ; Output: The learnt similarity matrix M  X  ; Initialization:
Let the count number of iteration t =0 . Randomly initialize matrix L (0)  X  n  X  n and set the Lagrange multiplier matrix
 X  (0) = 0  X  n  X  n . Set the penalty parameter  X  (0) =0 . 1 , and the increment step parameter  X &gt; 1 ; Preprocessing:
Solve Problem (15) to pre-process W and get a doubly stochastic matrix M (0) . Let W = M (0) . while not converge do end while
Return: M  X  ; refer to the literature therein [3, 22]. Because our new objective is convex, our algorithm converges to the global optimum.

In Algorithm 1, the slowest step is Step 2 for updating L .It requires O ( n 3 ) time to implement the singular vector decomposi-tion, where n is the number of samples in the dataset. This time complexity is comparable to that of spectral clustering.
Here in this section, we will further discuss the connection be-tween our model and the K -means clustering problem.
 T HEOREM 2. In Problem (5), if r  X  X  X  and W = X T X , then Problem (5) is equivalent to K -means clustering.
 Proof : Given a set of data points X =[ x 1 , x 2 , ..., x the K -means clustering problem is meant to partition X into k ( 1  X  k  X  n ) clusters C = { C 1 ,C 2 , ..., C k } such that the sum of the within cluster variance is minimized [13]. That is to say, the objective function of the K -means problem is: where  X  i is the mean of data points belonging to C i . If we introduce two matrix U  X  d  X  k and G  X  n  X  k , where U =[  X  1 , X  2 , ...,  X  k ] and G indicates the clustering indices , then Eq. (27) can be reformulated as:
Since the solution of U w.r.t. X and G is U = XG ( G T G ) we have Tr ( GU T UG T )= Tr ( X T UG T ) , thus Eq. (28) can be written as: where F = G ( G T G )  X  1 2 .
 Note that the Tr ( FF T FF T )= Tr ( FF T )= Tr ( G ( G T = k , so Problem (29) is equivalent to:
Let M = FF T , then Problem (30) can be rewritten as: where M  X  X  indicates some constraints on the M matrix. So the K -means clustering problem is to find a matrix M meeting some requirements such that M can minimize Problem (31).
 Let X  X  take further observe the properties of M .

From the definition of M , where M = FF T , we can directly find that m ij = 1 n i ,if x i and x j belongs to the same cluster; and m ij =0 otherwise. Also, it X  X  apparent that M  X  0 , M T = M and M 1 = 1 , that is to say, M is doubly stochastic. Moreover, we have Tr ( M )= Tr ( FF T )= k .
Our structured doubly stochastic model (SDS) can uncover the clustering structure and directly provide the clustering results, thus the clustering performance using doubly stochastic matrix can be enhanced. In this section we evaluate the clustering performance of our method on both synthetic and benchmark datasets, and compare them to the related doubly stochastic model and spectral clustering methods.

Moreover, according to the discussion in the previous section, our model possesses interesting connection with K -means cluster-ing, we also conduct experiments to test whether our model pro-vides an approach to better solving the K -means clustering prob-lem.
In this subsection, we conduct clustering experiments on our method and several related method. Our goal is to test whether the structured doubly stochastic matrix learned in our model is benefi-cial to improve the clustering performance under different circum-stances.
To evaluate the clustering performance of our method, we com-pare with spectral clustering, i.e, Ratio Cut and Normalized Cut, as well as the doubly stochastic normalization (DSN) method [29]. All comparing methods require an affinity matrix as the input. We construct the input affinity matrix with the self-tune Gaussian method [7], where the number of neighbors is set to be 5 and the value of  X  is self-tuned. Moreover, we let the input matrix W of our method to be initialized as shown in Eq. (15) such that W is doubly stochastic. In the experiment, we set the number of clus-ters to be the ground truth in each dataset. In our method, we set parameter  X  = 0.1,  X  = 1.1 and r to be tuned in the range of { 10 0 , 10 0 . 5 , ..., 10 5 } .

For all methods requiring K -means as the post-processing step, including Ratio Cut, Normalized Cut and DSN, we give them the same 100 random initializations and compute their respective best initialization vector w.r.t K -means objective function value. Since their performance is unstable with different initialization, we only report their respective best results in the 100 times repetition. For DSN method, we set the number of iteration as 3000 so as to get a good doubly stochastic matrix for clustering.
 All experiments are conducted on a Windows system with Intel Core i7-3770 Processor (8M Cache, 3.40 GHz).

The evaluation of different methods is based on two clustering metrics: accuracy and NMI (Normalized Mutual Information).
Accuracy is the percentage of the correctly assigned labels. NMI is short for the normalized mutual information. Let L denote the real label vector in a certain dataset, while L denotes the predicted one, then where I ( L, L ) is the mutual information between L and L : and H ( L ) is the entropy of L :
First of all, we conduct clustering experiments on the synthetic data as a sanitary check. The synthetic dataset is a 100  X  trix with four 25  X  25 block matrices diagonally arranged. The data within each block denotes the probability of two corresponding points from one same cluster to be connected; while the data out-side all the blocks denotes the probability of pair-wise data points from different clusters to be connected, i.e., noise (which should be 0 in the ideal clustering data). The probability values within each block are randomly generated in the range of (0 , 1) ; while the noise data is randomly generated in the range of (0 ,c ) , where c is set to be 0.5 and 0.6 respectively. What X  X  more, to make this clustering task more challenging, we randomly pick out 25 noise data and set their value to be 1.

Fig. 1 shows the original random matrix and corresponding clus-tering results of SDS. We can notice that our model performs well in this task. In our approach, we successfully learn a structured doubly stochastic matrix with explicit block structure, which di-vides the data into exactly four clusters. After adding high-level disturbance in the random data, our method still effectively recov-ers the clustering structure, which indicates the robustness of our model.

When the noise ratio is 0.5, our method works out an almost perfect structured doubly stochastic matrix with four clear blocks. As the noise increases, the block structure in the original data blurs, but our model is still able to detect the intrinsic cluster structure from the data.
We evaluated the proposed double stochastic method on 7 bench-mark datasets: AR [14], FERET [21], Yale [9], ORL [23], Carcino-(a) Original Graph, noise = 0.5 (c) Original Graph, noise = 0.6 Figure 1: Illustration of our clustering results on the block di-agonal synthetic data with different settings of noise. On the left column shows the graph structure of the original data gen-erated in the experiment. Figures on the right denote the struc-ture of the doubly stochastic matrix obtained in our SDS model. Table 1: Descriptions of benchmark datasets used in our exper-iments.
 Datasets Number of Instances Dimensions Classes Carcinomas 174 9182 11 SRBCTML 83 2308 4 mas [24, 28], SRBCTML [10] and LEUML 1 . The detailed descrip-tion of these datasets is summarized in Table 1.

The clustering performance comparison is summarized in Table 2. Results in Table 2 suggest that our method works very well on real benchmark datasets. Our SDS method maintains a high po-tential to outperform other methods on these distinct datasets. The theoretical proof in the methodology section indicates the connec-tion between our our model and spectral clustering problem, while the experimental results here verify SDS X  X  better clustering perfor-mance. This suggests SDS has the ability to better solve the spec-tral clustering problem. Compared with the up-to-date method, our method gains an obvious advantage over DSN. DSN only holds constraints on the doubly stochastic property of the learned graph but not its cluster structure, thus cannot get the optimal clustering results. On the contrary, our model learns a novel doubly stochas-tic matrix with explicit block structure, which performs better in clustering.
To further analyze the clustering performance, we draw the graph learned from different methods and compare their structure. We compare the graphs represented by the doubly stochastic matrix learned in DSN and SDS, respectively, and also, we display the http://www2.stat.duke.edu/courses/Spring01/sta293b/datasets.html graph constructed via self-tune Gaussian method, which is the input for spectral clustering. To explicitly view the graph structure, here we use the two datasets with relatively small number of classes and samples as the example, i.e., LEUML and SRBCTML. We present the graphs in Fig. 2. In each graph, row and columns are reshuf-fled such that samples from the same cluster are put together, which makes the cluster structure more clear to observe in the graph. For LEUML and SRBCTML, the r value we set for SDS is 10 0 and 10 0 . 5 , respectively. We can notice that the graph learned by SDS maintains the most clear block structure, and elements in each clus-ter tend to have similar value. These observations coincide with our theoretical analysis. Especially in the LEUML dataset, the doubly stochastic matrix learned by DSN is quite noisy, which leads to bad clustering performance shown in Table 2. Whereas, due to the low-rank constraint on the graph Laplacian matrix, the doubly stochas-tic matrix learned in our SDS model has more clear block structure, which accounts for the better clustering results obtained from SDS.
In the K -means clustering problem, a  X  X etter" solution signi-fies a smaller objective function value as well as a higher cluster-ing accuracy. Since K -means problem is non-convex, the quality of initialization is crucial in performing K -means clustering. In this subsection, we conduct experiments on both synthetic and real benchmark datasets to demonstrate the contribution of our method in better solving the K -means problem.
The experimental settings are similar to the settings in the clus-tering experiments. The different part is that in this section, we assign the clustering indicator obtained in our method as an initial-ization for K -means and see if the K -means clustering problem can be better solved with our initialization. For our method, we use W = X T X as the input matrix.

Still, the number of clusters is set to be the ground truth in each dataset. When implementing K -means clustering, unless specified otherwise, the following settings are adopted: we use 100 random initializations and record the average as well as best result w.r.t. K -means objective function value in the 100 times repetition. The evaluation is based on three metrics: accuracy, NMI and the K -means objective function value.
In the synthetic experiment, our toy data is a randomly generated multi-cluster matrix. Data points in each cluster are sampled i.i.d. from the Gaussian distribution N (0 , 1) . In our experiment, we set the number of clusters to be 100, number of samples to be 1000, while the dimensionality to be {2, 50, 1000} respectively. Our goal is to partition these clusters apart with K -means method. In the beginning we run K-Means for 10000 times and record the min-imum K-means objective value and the corresponding clustering accuracy. Then we run our method once by setting the input matrix to be W = X T X and use the obtained clustering results as an ini-tialization index vector for K-means and compute the same metrics. Comparison results are summarized in Table 3, which indicates ap-parent superiority of our method over K -means. It shows that even after 10000 times run, the minimum K -means objective value and clustering accuracy obtained by K -means are still far behind the result obtained by our method with just one run. This verifies that our method is able to better solve the K -means problem.
Still, we evaluate our model on the 7 benchmark datasets shown
K -means Accuracy (min_obj) K -means Accuracy with SDS Initialization in Table 1. We summarize the K -means performance comparison of K -means clustering and our method in Table 4. From Table 4, we can notice that our method improves the performance of K -means on real benchmark datasets. On all dataset, our SDS method performs equally or even better than K -means clustering. These re-sults demonstrates that our model makes a good way to better solve the K -means clustering problem. By adopting the cluster indicator learned in our model as the initialization, not only is the K -means objective function value reduced, but the clustering performance is also boosted to a large extent.
In this subsection, we analyze the influence of parameter r in Eq. (6) to the convergence of our algorithm. To save space, we just take two datasets, Carcinomas and ORL, as an example. We apply our method to these benchmark datasets with three different r values ( i.e., 10 , 10 3 and 10 5 ) and record the objective value of our model in each iteration.

The convergence results are presented in Fig. 3. We can notice that no matter what the r value is, our model always converges within about 80 iterations, which indicates the fast convergence of our algorithm.
In this paper, we proposed a novel structured doubly stochastic model with rank constraint on the graph Laplacian matrix. The doubly stochastic matrix learned in our model possesses explicit clustering structure, from which we can immediately partition data points into k connected components, where k is the number of clus-ters. The doubly stochastic property guarantees the effectiveness of the learnt similarity matrix while the rank constraint on the graph Laplacian matrix enhances the clustering ability. The quality of the learnt graph was verified by extensive experimental results, which suggested the feasibility of our model. What X  X  more, we theoreti-cally and empirically proved that our method made its own contri-bution in better solving the K -means and spectral clustering prob-lem.
 L EMMA 2. The following gives the global optimal solution to Problem (20):
Proof : With the Lagrangian function, Problem (20) can be rewrit-ten as: min where  X   X  n  X  n and  X   X  n are Lagrange multipliers.
 Taking derivative w.r.t. M and set it to 0, we have: Compute transpose on both sides of Eq. (36), we have: Subtracting Eq. (36) from Eq. (37), we get:
Combining Eq. (36) with Eq. (38), we further get:
Multiply the vector 1 on both sides of Eq. (39), we have:
Since  X  T 1 is a number, it is apparent that (  X  T 1 ) T =
From Eq. (41) we can obtain the solution of  X  as follows:
To enhance the computing speed, we compute the inverse term in Eq. (42) by means of the Woodbury formula [27]:
Thus the inverse term in Eq. (42) can be rewritten as:
From Eq. (44), we can rewrite Eq. (42) as follows:
Plugging the solution of  X  in Eq. (45) to Eq. (39), we get: 2 M = T + T T +  X  1 T + 1  X  T Let K = T + T T 2 , then we can rewrite the above equation as: which is the global optimal solution of Problem (20). Obviously, M is symmetric and which means that the solution of M in Eq. (46) meets the require-ments of Problem (20).
 Specially, when T is symmetric, the solution of M is: L EMMA 3. The following gives the global optimal solution to Problem (21): where m = diag ( M ) .

Proof: Require M = T + , then Problem (21) could be rewritten as follows: where m = diag ( S ) and t = diag ( T ) .
Similarly, we can use the Lagrangian function to solve Problem (49) and define: where  X   X  n and  X   X  n are Lagrange multipliers.

Taking derivative w.r.t. m and set it to 0, then we can solve m in Problem (51) as follows: [1] R. Angelova and G. Weikum. Graph-based text [2] M. Belkin and P. Niyogi. Laplacian eigenmaps for [3] D. P. Bertsekas. Constrained optimization and lagrange [4] D. P. Bertsekas et al. Augmented lagrangian and [5] J.-F. Cai, E. J. Cand X s, and Z. Shen. A singular value [6] A. Celikyilmaz, M. Thint, and Z. Huang. A graph-based [7] W.-Y. Chen, Y. Song, H. Bai, C.-J. Lin, and E. Y. Chang. [8] F. R. K. Chung. Spectral Graph Theory . CBMS Regional [9] A. Georghiades, P. Belhumeur, and D. Kriegman. From few [10] J. Khan, J. S. Wei, M. Ringner, L. H. Saal, M. Ladanyi, [11] D. Luo, C. Ding, and H. Huang. Forging The Graphs: A Low [12] D. Luo, C. Ding, F. Nie, and H. Huang. Cauchy graph [13] J. MacQueen et al. Some methods for classification and [14] A. Martinez and R. Benavente. The ar face database. [15] B. Mohar. The laplacian spectrum of graphs. In Graph [16] J. V. Neumann. Functional Operators , volume 2. 1950. [17] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering: [18] F. Nie, H. Wang, H. Huang, and C. Ding. Unsupervised and [19] F. Nie, X. Wang, and H. Huang. Clustering and projected [20] F. Nie, X. Wang, M. Jordan, and H. Huang. The constrained [21] P. Phillips, H. Wechsler, J. Huang, and P. Rauss. The FERET [22] M. J. D. Powell. A method for nonlinear constraints in [23] F. S. Samaria and A. C. Harter. Parameterisation of a [24] A. I. Su, J. B. Welsh, L. M. Sapinoso, S. G. Kern, [25] U. Von Luxburg. A tutorial on spectral clustering. Statistics [26] F. Wang, P. Li, and A. Konig. Learning a bi-stochastic data [27] M. A. Woodbury. Inverting modified matrices . 1950. [28] K. Yang, Z. Cai, J. Li, and G. Lin. A stable gene selection in [29] R. Zass and A. Shashua. Doubly stochastic normalization for [30] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and [31] X. Zhu, Z. Ghahramani, and J. D. Lafferty. Semi-supervised (a) Graph for Normalized Cut in LEUML (d) Graph for Normalized Cut in SRBCTML in ORL
