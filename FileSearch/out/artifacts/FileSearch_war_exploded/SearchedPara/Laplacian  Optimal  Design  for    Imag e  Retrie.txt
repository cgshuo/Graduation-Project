 Relevance feedback is a powerful technique to enhance Content-Based Image Retrieval (CBIR) performance. It solicits the user X  X  relevance judgments on the retrieved images returned by the CBIR systems. The user X  X  labeling is then used to learn a classifier to distinguish between relevant and irrele-vant images. However, the top returned images may not be the most informative ones. The challenge is thus to deter-mine which unlabeled images would be the most informative (i.e., improve the classifier the most) if they were labeled and used as training samples. In this paper, we propose a novel active learning algorithm, called Laplacian Opti-mal Design (LOD), for relevance feedback image retrieval. Our algorithm is based on a regression model which mini-mizes the least square error on the measured (or, labeled) images and simultaneously preserves the local geometrical structure of the image space. Specifically, we assume that if two images are sufficiently close to each other, then their measurements (or, labels) are close as well. By constructing a nearest neighbor graph, the geometrical structure of the image space can be described by the graph Laplacian. We discuss how results from the field of optimal experimental design may be used to guide our selection of a subset of im-ages, which gives us the most amount of information. Exper-imental results on Corel database suggest that the proposed approach achieves higher precision in relevance feedback im-age retrieval.
 H.3.3 [ Information storage and retrieval ]: Information search and retrieval X  Relevance feedback ; G.3 [ Mathematics of Computing ]: Probability and Statistics X  Experimental design Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00. Algorithms, Performance, Theory Image retrieval, active learning, relevance feedback
In many machine learning and information retrieval tasks, there is no shortage of unlabeled data but labels are expen-sive. The challenge is thus to determine which unlabeled samples would be the most informative (i.e., improve the classifier the most) if they were labeled and used as training samples. This problem is typically called active learning [4]. Here the task is to minimize an overall cost, which depends both on the classifier accuracy and the cost of data collec-tion. Many real world applications can be casted into active learning framework. Particularly, we consider the problem of relevance feedback driven Content-Based Image Retrieval (CBIR) [13].

Content-Based Image Retrieval has attracted substantial interests in the last decade [13]. It is motivated by the fast growth of digital image databases which, in turn, require efficient search schemes. Rather than describe an image us-ing text, in these systems an image query is described using one or more example images. The low level visual features (color, texture, shape, etc.) are automatically extracted to represent the images. However, the low level features may not accurately characterize the high level semantic concepts. To narrow down the semantic gap, relevance feedback is in-troduced into CBIR [12].

In many of the current relevance feedback driven CBIR systems, the user is required to provide his/her relevance judgments on the top images returned by the system. The labeled images are then used to train a classifier to separate images that match the query concept from those that do not. However, in general the top returned images may not be the most informative ones. In the worst case, all the top images labeled by the user may be positive and thus the standard classification techniques can not be applied due to the lack of negative examples. Unlike the standard classification problems where the labeled samples are pre-given, in relevance feedback image retrieval the system can actively select the images to label. Thus active learning can be naturally introduced into image retrieval.
 Despite many existing active learning techniques, Support Vector Machine (SVM) active learning [14] and regression based active learning [1] have received the most interests. Based on the observation that the closer to the SVM bound-ary an image is, the less reliable its classification is, SVM active learning selects those unlabeled images closest to the boundary to solicit user feedback so as to achieve maximal refinement on the hyperplane between the two classes. The major disadvantage of SVM active learning is that the esti-mated boundary may not be accurate enough. Moreover, it may not be applied at the beginning of the retrieval when there is no labeled images. Some other SVM based active learning algorithms can be found in [7], [9].

In statistics, the problem of selecting samples to label is typically referred to as experimental design . The sample x is referred to as experiment , and its label y is referred to as measurement . The study of optimal experimental de-sign (OED) [1] is concerned with the design of experiments that are expected to minimize variances of a parameterized model. The intent of optimal experimental design is usu-ally to maximize confidence in a given model, minimize pa-rameter variances for system identification, or minimize the model X  X  output variance. Classical experimental design ap-proaches include A-Optimal Design , D-Optimal Design ,and E-Optimal Design . All of these approaches are based on a least squares regression model. Comparing to SVM based active learning algorithms, experimental design approaches are much more efficient in computation. However, this kind of approaches takes only measured (or, labeled) data into account in their objective function, while the unmeasured (or, unlabeled) data is ignored.

Benefit from recent progresses on optimal experimental design and semi-supervised learning, in this paper we pro-pose a novel active learning algorithm for image retrieval, called Laplacian Optimal Design (LOD). Unlike tradi-tional experimental design methods whose loss functions are only defined on the measured points, the loss function of our proposed LOD algorithm is defined on both measured and unmeasured points. Specifically, we introduce a locality preserving regularizer into the standard least-square-error based loss function. The new loss function aims to find a classifier which is locally as smooth as possible. In other words, if two points are sufficiently close to each other in the input space, then they are expected to share the same label. Once the loss function is defined, we can select the most informative data points which are presented to the user for labeling. It would be important to note that the most informative images may not be the top returned images.
The rest of the paper is organized as follows. In Section 2, we provide a brief description of the related work. Our proposed Laplacian Optimal Design algorithm is introduced in Section 3. In Section 4, we compare our algorithm with the state-or-the-art algorithms and present the experimental results on image retrieval. Finally, we provide some conclud-ing remarks and suggestions for future work in Section 5.
Since our proposed algorithm is based on regression frame-work. The most related work is optimal experimental design [1], including A-Optimal Design , D-Optimal Design ,and E-Optimal Design . In this Section, we give a brief description of these approaches. The generic problem of active learning is the following. Given a set of points A = { x 1 , x 2 ,  X  X  X  , x m } in R d subset B = { z 1 , z 2 ,  X  X  X  , z k } X  X  which contains the most in-formative points. In other words, the points z i ( i =1 , can improve the classifier the most if they are labeled and used as training points.
We consider a linear regression model where y is the observation , x is the independent variable , w is the weight vector and is an unknown error with zero mean. Different observations have errors that are indepen-dent, but with equal variances  X  2 . We define f ( x )= w to be the learner X  X  output given input x and the weight vector w . Suppose we have a set of labeled sample points ( z 1 ,y 1 ) ,  X  X  X  , ( z k ,y k ), where y i is the label of z maximum likelihood estimate for the weight vector,  X  w ,is that which minimizes the sum squared error The estimate  X  w gives us an estimate of the output at a novel input:  X  y =  X  w T x .

By Gauss-Markov theorem, we know that  X  w  X  w has a zero mean and a covariance matrix given by  X  2 H  X  1 sse ,where H sse is the Hessian of J sse ( w ) where Z =( z 1 , z 2 ,  X  X  X  , z k ).

The three most common scalar measures of the size of the parameter covariance matrix in optimal experimental design are: Since the computation of the determinant and eigenvalues of a matrix is much more expensive than the computation of matrix trace, A-optimal design is more efficient than the other two. Some recent work on experimental design can be found in [6], [16].
Since the covariance matrix H sse used in traditional ap-proaches is only dependent on the measured samples, i.e. z  X  X , these approaches fail to evaluate the expected errors on the unmeasured samples. In this Section, we introduce a novel active learning algorithm called Laplacian Optimal Design (LOD) which makes efficient use of both measured (labeled) and unmeasured (unlabeled) samples.
In many machine learning problems, it is natural to as-sume that if two points x i , x j are sufficiently close to each other, then their measurements ( f ( x i ), f ( x j )) are close as well. Let S be a similarity matrix. Thus, a new loss function which respects the geometrical structure of the data space can be defined as follows:
J 0 ( w )= where y i is the measurement (or, label) of z i .Notethat, the loss function (3) is essentially the same as the one used in Laplacian Regularized Regression (LRR, [2]). However, LRR is a passive learning algorithm where the training data is given. In this paper, we are focused on how to select the most informative data for training. The loss function with our choice of symmetric weights S ij ( S ij = S ji ) incurs a heavy penalty if neighboring points x i and x j are mapped far apart. Therefore, minimizing J 0 ( w ) is an attempt to ensure that if x i and x j are close then f ( x i )and f ( x close as well. There are many choices of the similarity matrix S . A simple definition is as follows: S Let D be a diagonal matrix, D ii = j S ij ,and L = D  X  S . The matrix L is called graph Laplacian in spectral graph theory [3]. Let y =( y 1 ,  X  X  X  ,y k ) T and X =( x 1 ,  X  X  X  Following some simple algebraic steps, we see that: The Hessian of J 0 ( w ) can be computed as follows: In some cases, the matrix ZZ T +  X XLX T is singular (e.g. if m&lt;d ). Thus, there is no stable solution to the optimization problem Eq. (3). A common way to deal with this ill-posed problem is to introduce a Tikhonov regularizer into our loss function: The Hessian of the new loss function is given by: where I is an identity matrix and  X  =  X  1 XLX T +  X  2 I . Clearly, H is of full rank. Requiring that the gradient of J ( w )withrespectto w vanish gives the optimal estimate  X  w : The following proposition states the bias and variance prop-erties of the estimator for the coefficient vector w .
Proposition 3.1. E (  X  w  X  w )=  X  H  X  1  X  w ,Cov (  X  w )=  X  Proof. Since y = Z T w + and E ( ) = 0, it follows that Notice Cov( y )=  X  2 I , the covariance matrix of  X  w has the expression: Therefore mean squared error matrix for the coefficients w is For any x , let  X  y =  X  w T x be its predicted observation. The expected squared prediction error is = E ( + w T x  X   X  w T x ) 2 =  X  2 + x T [ E ( w  X   X  w )( w  X   X  w ) T ] x Clearly the expected square prediction error depends on the explanatory variable x , therefore average expected square predictive error over the complete data set A is = 1 = 1 Since Our Laplacian optimality criterion is thus formulated by minimizing the trace of X T H  X  1 X .
 Definition 1. Laplacian Optimal Design where z 1 ,  X  X  X  , z k are selected from { x 1 ,  X  X  X  , x m Canonical experimental design approaches (e.g. A-Optimal Design, D-Optimal Design, and E-Optimal) only consider linear functions. They fail to discover the intrinsic geometry in the data when the data space is highly nonlinear. In this section, we describe how to perform Laplacian Experimen-tal Design in Reproducing Kernel Hilbert Space (RKHS) which gives rise to Kernel Laplacian Experimental Design (KLOD).

For given data points x 1 ,  X  X  X  , x m  X  X  with a positive definite mercer kernel K : X X X X  R , there exists a unique RKHS H K of real valued functions on X .Let K t ( s )bethe function of s obtained by fixing t and letting K t ( s ) . H
K consists of all finite linear combinations of the form t become dense in X .Wehave K s ,K t H K = K ( s, t ).
Consider the optimization problem (5) in RKHS. Thus, we seek a function f  X  X  K such that the following objective function is minimized: min We have the following proposition.

Proposition 4.1. Let H = { m asubspaceof H K , the solution to the problem (12) is in H Proof. Let H  X  be the orthogonal complement of H , i.e. H
K = H X  X   X  . Thus, for any function f  X  X  K ,ithas orthogonal decomposition as follows: Now, let X  X  evaluate f at x i : Notice that K x i  X  X  while f H  X   X  X   X  . This implies that f This completes the proof.
 Proposition 4.1 tells us the minimizer of problem (12) admits details.
 Let  X  : R d  X  X  be a feature map from the input space the data matrix in RKHS, X =(  X  ( x 1 ) , X  ( x 2 ) ,  X  X  X  , X  ( x Similarly, we define Z =(  X  ( z 1 ) , X  ( z 2 ) ,  X  X  X  , X  ( z the optimization problem in RKHS can be written as follows: Since the mapping function  X  is generally unknown, there is no direct way to solve problem (13). In the following, we apply kernel tricks to solve this optimization problem. Let X  X  1 be the Moore-Penrose inverse (also known as pseudo inverse) of X .Thus,wehave: = X T X ZZ T X +  X  1 X L X T X +  X  2 X  X  1 ( X T )  X  1 X T = X T X X T ZZ T X +  X  1 X T X L X T X +  X  2 X T X  X  1 X T where K XX is a m  X  m matrix ( K XX ,ij = K ( x i , x j )), K is a m  X  k matrix ( K XZ ,ij = K ( x i , z j )), and K ZX matrix ( K ZX ,ij = K ( z i , x j )). Thus, the Kernel Laplacian Optimal Design can be defined as follows: Definition 2. Kernel Laplacian Optimal Design
In this subsection, we discuss how to solve the optimiza-tion problems (11) and (14). Particularly, if we select a linear kernel for KLOD, then it reduces to LOD. Therefore, we will focus on problem (14) in the following.
 It can be shown that the optimization problem (14) is NP-hard. In this subsection, we develop a simple sequential greedy approach to solve (14). Suppose n points have been selected, denoted by a matrix Z n =( z 1 ,  X  X  X  , z n ). The ( n + 1)-th point z n +1 can be selected by solving the following optimization problem: as follows: Thus, we have: We define: A is only dependent on X and Z n .Thus,the( n +1)-th point z n +1 is given by: z Each time we select a new point z n +1 , the matrix A is up-dated by: If the kernel function is chosen as inner product K ( x , y )= x , y ,then H K is a linear functional space and the algo-rithm reduces to LOD.
In this section, we describe how to apply Laplacian Op-timal Design to CBIR. We begin with a brief description of image representation using low level visual features. Low-level image representation is a crucial problem in CBIR. General visual features includes color, texture, shape, etc. Color and texture features are the most extensively used visual features in CBIR. Compared with color and texture features, shape features are usually described after images have been segmented into regions or objects. Since robust and accurate image segmentation is difficult to achieve, the use of shape features for image retrieval has been limited to special applications where objects or regions are readily available.

In this work, We combine 64-dimensional color histogram and 64-dimensional Color Texture Moment (CTM, [15]) to represent the images. The color histogram is calculated us-ing 4  X  4  X  4 bins in HSV space. The Color Texture Mo-ment is proposed by Yu et al. [15], which integrates the color and texture characteristics of the image in a compact form. CTM adopts local Fourier transform as a texture rep-resentation scheme and derives eight characteristic maps to describe different aspects of co-occurrence relations of im-age pixels in each channel of the (SVcosH, SVsinH, V) color space. Then CTM calculates the first and second moment of these maps as a representation of the natural color image pixel distribution. Please see [15] for details.
Relevance feedback is one of the most important tech-niques to narrow down the gap between low level visual features and high level semantic concepts [12]. Tradition-ally, the user X  X  relevance feedbacks are used to update the query vector or adjust the weighting of different dimensions. This process can be viewed as an on-line learning process in which the image retrieval system acts as a learner and the user acts as a teacher. They typical retrieval process is out-lined as follows: 1. The user submits a query image example to the sys-2. The system selects some images from the database and 3. The system uses the user X  X  provided information to re-Our Laplacian Optimal Design algorithm is applied in the second step for selecting the most informative images. Once we get the labels for the images selected by LOD, we apply Laplacian Regularized Regression (LRR, [2]) to solve the optimization problem (3) and build the classifier. The clas-sifier is then used to re-rank the images in database. Note that, in order to reduce the computational complexity, we do not use all the unlabeled images in the database but only those within top 500 returns of previous iteration.
In this section, we evaluate the performance of our pro-posed algorithm on a large image database. To demonstrate the effectiveness of our proposed LOD algorithm, we com-pare it with Laplacian Regularized Regression (LRR, [2]), Support Vector Machine (SVM), Support Vector Machine Active Learning (SVM active ) [14], and A-Optimal Design (AOD). Both SVM active , AOD, and LOD are active learn-ing algorithms, while LRR and SVM are standard classi-fication algorithms. SVM only makes use of the labeled images, while LRR is a semi-supervised learning algorithm which makes use of both labeled and unlabeled images. For SVM active , AOD, and LOD, 10 training images are selected by the algorithms themselves at each iteration. While for LRR and SVM, we use the top 10 images as training data. It would be important to note that SVM active is based on the ordinary SVM, LOD is based on LRR, and AOD is based on the ordinary regression. The parameters  X  1 and  X  2 in our LOD algorithm are empirically set to be 0.001 and 0.00001. For both LRR and LOD algorithms, we use the same graph structure (see Eq. 4) and set the value of p (number of near-est neighbors) to be 5. We begin with a simple synthetic example to give some intuition about how LOD works.
A simple synthetic example is given in Figure 1. The data set contains two circles. Eight points are selected by AOD and LOD. As can be seen, all the points selected by AOD are from the big circle, while LOD selects four points from the big circle and four from the small circle. The numbers beside the selected points denote their orders to be selected. Clearly, the points selected by our LOD algorithm can better represent the original data set. We did not compare our algorithm with SVM active because SVM active can not be applied in this case due to the lack of the labeled points.
The image database we used consists of 7,900 images of 79 semantic categories, from COREL data set. It is a large and heterogeneous image set. Each image is represented as a 128-dimensional vector as described in Section 5.1. Figure 2 shows some sample images.

To exhibit the advantages of using our algorithm, we need a reliable way of evaluating the retrieval performance and the comparisons with other algorithms. We list different aspects of the experimental design below.
We use precision-scope curve and precision rate [10] to evaluate the effectiveness of the image retrieval algorithms. Thescopeisspecifiedbythenumber( N ) of top-ranked im-ages presented to the user. The precision is the ratio of the number of relevant images presented to the user to the scope N . The precision-scope curve describes the precision with various scopes and thus gives an overall performance evaluation of the algorithms. On the other hand, the pre-cision rate emphasizes the precision at a particular value of scope. In general, it is appropriate to present 20 images on a screen. Putting more images on a screen may affect the quality of the presented images. Therefore, the precision at top 20 ( N = 20) is especially important.

In real world image retrieval systems, the query image is usually not in the image database. To simulate such environ-ment, we use five-fold cross validation to evaluate the algo-rithms. More precisely, we divide the whole image database into five subsets with equal size. Thus, there are 20 images per category in each subset. At each run of cross validation, one subset is selected as the query set, and the other four subsets are used as the database for retrieval. The precision-scope curve and precision rate are computed by averaging the results from the five-fold cross validation.
We designed an automatic feedback scheme to model the retrieval process. For each submitted query, our system re-trieves and ranks the images in the database. 10 images were selected from the database for user labeling and the label information is used by the system for re-ranking. Note that, the images which have been selected at previous iter-ations are excluded from later selections. For each query, the automatic relevance feedback mechanism is performed for four iterations.

It is important to note that the automatic relevance feed-back scheme used here is different from the ones described in [8], [11]. In [8], [11], the top four relevant and irrelevant images were selected as the feedback images. However, this may not be practical. In real world image retrieval systems, it is possible that most of the top-ranked images are relevant (or, irrelevant). Thus, it is difficult for the user to find both four relevant and irrelevant images. It is more reasonable for the users to provide feedback information only on the 10 images selected by the system.
In real world, it is not practical to require the user to provide many rounds of feedbacks. The retrieval perfor-mance after the first two rounds of feedbacks (especially the first round) is more important. Figure 3 shows the average precision-scope curves of the different algorithms for the first two feedback iterations. At the beginning of retrieval, the Euclidean distances in the original 128-dimensional space are used to rank the images in database. After the user provides relevance feedbacks, the LRR, SVM, SVM active , AOD, and LOD algorithms are then applied to re-rank the images. In order to reduce the time complexity of active learning algorithms, we didn X  X  select the most informative images from the whole database but from the top 500 im-ages. For LRR and SVM, the user is required to label the top 10 images. For SVM active , AOD, and LOD, the user is required to label 10 most informative images selected by these algorithms. Note that, SVM active can only be ap-consistently outperforms the other four algorithms. plied when the classifier is already built. Therefore, it can not be applied at the first round and we use the standard SVM to build the initial classifier. As can be seen, our LOD algorithm outperforms the other four algorithms on the en-tire scope. Also, the LRR algorithm performs better than SVM. This is because that the LRR algorithm makes effi-cient use of the unlabeled images by incorporating a locality preserving regularizer into the ordinary regression objective function. The AOD algorithm performs the worst. As the scope gets larger, the performance difference between these algorithms gets smaller.

By iteratively adding the user X  X  feedbacks, the correspond-ing precision results (at top 10, top 20, and top 30) of the five algorithms are respectively shown in Figure 4. As can be seen, our LOD algorithm performs the best in all the cases and the LRR algorithm performs the second best. Both of these two algorithms make use of the unlabeled images. This shows that the unlabeled images are helpful for discovering the intrinsic geometrical structure of the image space and therefore enhance the retrieval performance. In real world, the user may not be willing to provide too many relevance feedbacks. Therefore, the retrieval performance at the first two rounds are especially important. As can be seen, our LOD algorithm achieves 6.8% performance improvement for top 10 results, 5.2% for top 20 results, and 4.1% for top 30 results, comparing to the second best algorithm (LRR) after the first two rounds of relevance feedbacks.
Several experiments on Corel database have been system-atically performed. We would like to highlight several inter-esting points: 1. It is clear that the use of active learning is beneficial 2. In many real world applications like relevance feed-3. The relevance feedback technique is crucial to image
This paper describes a novel active learning algorithm, called Laplacian Optimal Design, to enable more effective relevance feedback image retrieval. Our algorithm is based on an objective function which simultaneously minimizes the empirical error and preserves the local geometrical structure of the data space. Using techniques from experimental de-sign, our algorithm finds the most informative images to label. These labeled images and the unlabeled images in the database are used to learn a classifier. The experimen-tal results on Corel database show that both active learning and semi-supervised learning can significantly improve the retrieval performance.

In this paper, we consider the image retrieval problem on a small, static, and closed-domain image data. A much more challenging domain is the World Wide Web (WWW). For Web image search, it is possible to collect a large amount of user click information. This information can be naturally used to construct the affinity graph in our algorithm. How-ever, the computational complexity in Web scenario may be-come a crucial issue. Also, although our primary interest in this paper is focused on relevance feedback image retrieval, our results may also be of interest to researchers in patten recognition and machine learning, especially when a large amount of data is available but only a limited samples can be labeled. [1] A. C. Atkinson and A. N. Donev. Optimum [2] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold [3] F. R. K. Chung. Spectral Graph Theory ,volume92of [4] D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active [5] A. Dong and B. Bhanu. A new semi-supervised em [6] P. Flaherty, M. I. Jordan, and A. P. Arkin. Robust [7] K.-S. Goh, E. Y. Chang, and W.-C. Lai. Multimodal [8] X. He. Incremental semi-supervised subspace learning [9] S. C. Hoi and M. R. Lyu. A semi-supervised active [10] D. P. Huijsmans and N. Sebe. How to complete [11] Y.-Y. Lin, T.-L. Liu, and H.-T. Chen. Semantic [12] Y. Rui, T. S. Huang, M. Ortega, and S. Mehrotra. [13] A. W. Smeulders, M. Worring, S. Santini, A. Gupta, [14] S. Tong and E. Chang. Support vector machine active [15] H. Yu, M. Li, H.-J. Zhang, and J. Feng. Color texture [16] K. Yu, J. Bi, and V. Tresp. Active learning via
