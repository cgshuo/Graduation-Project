 Opinion mining became an importa nt topic of study in recent years due to its wide range of applications. There are also many companies offering opinion mining services. One problem that has not been studied so far is th e assignment of entities that have been talked about in each sentence. Let us use forum discussions about products as an example to make the problem concrete. In a typical discussion post, the author may give opinions on multiple products and also compare them. Th e issue is how to detect what products have been talked about in each sentence. If the sentence contains the product names, they need to be identified. We call explicitly mentioned in the senten ce but are implied due to the use of pronouns and language conventions, we need to infer the products. We call this problem entity assignment . These problems are important because without knowing what products each sentence talks about the opinion mined from the sentence is of little use. In this paper, we study these problems and propose two effective methods to solve the probl ems. Entity discovery is based on pattern discovery and entity assignment is based on mining of comparative sentences. Experiment al results using a large number of forum posts demonstrate the eff ectiveness of the technique. Our system has also been successfully tested in a commercial setting. H.3.3 [ Information Storage and Retrieval ]: Information Search Intelligence ]: Natural Language Processing  X  text analysis . Algorithms, Experimentation. Entity Discovery, Sentiment Analysis It is now well recognized that th e user-generated content (e.g., product reviews, forum discussions and blogs) contains valuable consumer opinions that can be exploited for many applications. There are already many companies that provide opinion mining services. In this paper, we report a system that solves an important problem in these applications. It identifies what entities (e.g., products) each sentence talks about. Most opinion mining researches are based on product reviews [2, 4, 10, 19, 20, 23] because a review usually focuses on a specific product or entity and contains little irrelevant information. However, in forum discussions a nd blogs, the situation is very different, where the authors often ta lk about multiple entities (e.g., products), and compare them. This raises two important issues: (1) how to discover the entities that are talked about in a sentence and (2) how to assign entities to each sentence because in many sentences entity names are not explicitly mentioned, but are implied. We term the first problem entity discovery and the second problem entity assignment . Without knowing the entities that a sentence talks about, any opinion mined from the sentence expresses a negative opinion about something, but it cannot determine on what product, then the opinion is meaningless. The first problem is basically the named entity recognition (NER) problem. However, traditional NE R methods do not work well because of the ungrammatical nature of the forum posts, over-capitalization and under capitalizat ion. Over-capitalization means that the user may capitalize every word in the sentence, and under-capitalization means that the first letters of many entity names are not capitalized. These cause serious problems for existing entity recognition programs as we will see in Section 5. We propose a technique to solve the problem based on sequential pattern mining and natural language processing (NLP). The second problem is similar to pronoun resolution [3, 25] in NLP, which identifies what each pronoun in a sentence refers to. Pronoun resolution is still a major ch allenge. The accuracy of the current state-of-the-art systems is only about 60-70% on well-formed sentences such as those in news articles [3, 25]. However, this accuracy cannot be used for applications. In addition, for the user-generated content, the problem is harder due to ungrammatical sentences, and missing or wrong punctuations. To solve this problem, our proposed method will not rely on pronoun resolution because our task is also different. Many sentences do not have pronouns, but we still need to know which entities these sentences talk about . For example, sentences (3) and (5) of the discussion post in Example 1 below have no pronoun or any other reference to resolve. The question is how to discover the entity that sentences (3) and (5 ) talk about in Example 1. Example 1 :  X (1) I bought Camera-A yesterday. (2) I took some pictures in the evening in my living room. (3) The images are very clear. (4) They are definitely better than those from my old Camera-B. (5) The battery is very good too. X  A simple approach to assigning entities that are talked about in each sentence is the following: The algorithm sequentially processes each sentence. Whenever an entity name is encountered in a sentence, it is assumed that the sentence talks about that entity. It is also assumed that th e subsequent sentences talk about that entity as well until a new entity name appears. Then the new entity is the one talked about in its sentence. The subsequent sentences also talk a bout the new entity, and so on. This simple strategy works reasonably well in practice. However, it breaks down when a comparative sentence is encountered. Let us use this strategy to process the sentences in Example 1. Clearly, sentences (1)  X  (3) talk about Camera-A because Camera-A is encountered in sentence (1). Sentences (2) and (3) contain no new product. Sentence (4) is a co mparative sentence, which also introduced Camera-B. Now, the above strategy is not applicable because although sentence (4) only mentions Camera-B, it actually talks about both cameras. What is more serious is that the simple strategy also infers that sentence (5) talks about Camera-B, which is clearly wrong. For us human beings, we can see that sentence (5) talks a bout Camera-A. The question is how to solve the problem using an automated approach. One may say that the sentence after the comparative sentence should talk about the product mentioned before. Unfortunately, this is not right either. For example, we change Example 1 to: Example 2 :  X (1) I bought Camera-A yesterday. (2) I took a few pictures in the evening in my living room. (3) The images were very clear. (4) They were definitely better than those from my old Camera-B. (5) The pictures of that camera were blurring for night shots, but for day shots it was ok X  Example 2 is the same as example 1 except the last sentence. Obviously, sentence (5) here ta lks about Camera-B. Then, the above simple algorithm does not work. An important finding of this work is that the implicit entity assignment problem is mainly caused by comparative sentences as Example 1 shows. Based on our data sets, if we simply infer that the implicit entity is the previously mentioned entity, 20% of them are wrong and 98% of these errors are caused by comparisons. This paper proposes a technique to solve the problem. The method does not rely on pronouns, which has two advantages. First, there is no need to solve the difficult problem of pronoun resolution in NLP. Second, sentences that do not use pronouns can be handled. Sentiment consistency : To introduce the proposed technique, let us make an important observatio n about the two examples above. The comparative sentences (4) in both examples say that Camera-A is superior to Camera-B. The next sentence, sentence (5) in Example 1, expresses a positive sentiment. Intuitively, sentence (5) in Example 1 should refer to the superior product. Similarly, sentence (5) in Example 2, whic h expresses a negative sentiment in its first clause, should refer to the inferior product. We call this phenomenon sentiment consistency , which says that consecutive sentiment expressions should be c onsistent with each other. It would be ambiguous if this consis tency is not obser ved in writing. It turns out that this observation is quite general, which suggests a novel approach to solving the pr oblem. That is, opinion mining can be employed. Two tasks are necessary: (1) for a comparative sentence, we need to identify which entity is superior, and (2) for the subsequent sentence, we need to determine whether its first clause (sentence 5 of Example 2) is positive, negative, or neutral. The opinion mining technique in [4] is adapted to solve the problem. In the process, we al so made two contributions to opinion mining. First, we discove red that the opinion mining method can also be adapted to analyze comparative sentences. Thus, a separate algorithm is not needed although the two types of sentences look quite different (e.g ., sentences (4) and (5) in the examples). Second, we define a rule specification language to make the rule-based opinion mining technique in [4] more convenient. Experimental result s based on 753 forums posts from 63 threads with 4385 sentences s how that our technique is effective. Our system has also been successfully tested in a commercial setting. Related works about entity discove ry is mainly in the field of named entity recognition (NER). NER aims to identify entities such as names of persons, organi zations and locations in natural language text. Most NER met hods are based on supervised learning [22]. Recent strategies also exploit the domain structure in the training data to improve the performance [11]. [12] studied extraction of entities and other items from comparative sentences based on comparative sentence structures. On product extraction, [6] used hand-crafting rules to extract product entities and exploited comparative sentences for product comparison analysis. Our work is more general and not focused on comparative sentences. Our work is also different from the classic NER as we are only interested in the product type of entities. [22] gave good surveys of existing NER and general information extraction algorithms. Conditional random fiel ds (CRF) [16] have been shown to perform the best so far. We will show that the proposed method outperforms CRF dramatically for our task. Although the problem of assigning ent ities referred to in sentences is important in practice for appli cations, we are not aware of any focused study of this problem. Related works are mainly in the area of pronoun resolution or co-reference resolution in NLP. Pronoun or coreference resolution has been extensively studied in natural language processing [3, 18, 25]. However, it is still a major challenge. As discussed in the introduction, our task is in fact quite different because ma ny sentences do not have pronouns, but we still need to know wh ich entities they discuss. Although the objective of the pr oposed problem is not opinion mining, some of the opinion mi ning techniques are applied to solve the problem. The most widely studied opinion mining topic is sentiment classification of documents and sentences, i.e., classifying them as expressing positive, negative, or neutral opinions [2, 19, 23]. However, our wo rk needs to study clauses. In the same sentence, one clause may be positive but another clause may be negative, e.g.,  X  The photo quality is great, but the battery sucks  X , which contains two opinions. We have made use of some me thods in feature-based opinion mining [4, 17, 20] in our work. Feature-based opinion mining means to find opinions expresse d on individual product features. For example, in the above exampl e,  X  X hoto quality X  and  X  X attery X  are product features. The opinion on  X  X hoto quality X  is positive and the opinion on  X  X attery X  is negative. Existing techniques exploit opinion words for the task. Opinion words are words that express desired or undesired states . Positive words express desired states, e.g.,  X  X reat X  and  X  X ood X . Negative words express undesired states, e.g.,  X  X ad X  and  X  X oor X . Identifying opinion words have compiled. Apart from individual words, there are also many opinion phrases, e.g.,  X  X ost someone an arm and a leg X . We adapted the method in [4, 10] for our purpose as it can be easily changed for opinion analysis of each clause. We also made two contributions to opinion mini ng. First, [4] hard-codes all opinion phrases in the system, wh ich is undesirable because any addition/deletion of phrases will involve changing the program code. A specification language is then proposed to allow the user to add/delete phrases without touching the underlying program. Second, we show that our adap ted opinion mining method can be easily extended to deal with comparative sentences, which is important for our task of entity assignment. On the study of comparative sentences, [1, 12] proposed some methods to study comparative and superlative senten ces. However, they do not determine superior entities expressed in comparative sentences, which is what we need. [7] studied opinions in comparative sentences. However, it needs a large volume of external information, i.e., pros and cons in product reviews. Our method is much simpler, and we only use the method to solve the entity assignment problem, which is the focus of this paper. The basic information unit of forums, blogs and discussion boards consists of a start post and a list of follow-up posts or replies . This basic information unit is often called a thread . A thread t thus can be modeled as a sequence of posts, &lt; p 1 , p 2 , ..., p post. Each post consists of a sequence of sentences, &lt; s s &gt;. Each sentence s i describes something on a subset of entities  X  be a person, a product, an organizati on, an event, etc. If an entity name is explicitly mentioned in sentence s i , we say that the entity is an explicit entity in s i . If the entity is not e xplicitly mentioned in s but it is implied, we say that the entity is an implicit entity . For example, Camera-A in the first sentence below is an explicit entity. Camera-A is an implicit entity in sentence 2 as it is not explicitly mentioned there, but it is implied.  X  Camera-A looks really pretty. The battery lasts very long  X . Most sentences talk about a si ngle entity, i.e., the size of  X  is usually 1. If a sentence involves mu ltiple entities (explicit and/or implicit), it is usually a comparative sentence, e.g.,  X  Camera-A is better than Camera-B  X . A related type of sentences is the superlative sentences, e.g.,  X  Camera-A is the best . X  This is a simplified model of the real word. For example, it does not cover irrelevant sentences, wh ich are usually rare. It does not cover quotes (from previous posts) in the reply. However, quotes are easy to handle because they are usually in a different format. Problem statement : Given a set of threads T in a particular domain, two tasks are performed in this paper: 1. Entity discovery : discover the set of entities E discussed in the 2. Entity assignment : assign the entities in E that each sentence s The main idea for entity discovery is to discover linguistic patterns and then use the patter ns to extract entity names. However, traditional methods need a large number of manually labeled training examples, and la beling is very time consuming. For a different domain, the labeling process may need to be repeated. This section proposes an automated pattern discovery method for the task, which is thus unsupervised. The basic idea of the algorithm is that the user starts with a few seed entities. The system bootstraps from them to find more entities in a set of documents (or posts). The algorithm is thus iterative. Sequential pattern mining [8] is employed at each iteration to find more entities ba sed on already found entities. The iterative process ends when no new entity names are found. Pruning methods are also propos ed to remove those unlikely entities. Given a set of seed entities E = { e algorithm consists of the following iterative steps: Step 1  X  data preparation for sequential pattern mining: This step perform two tasks, it first finds all sentences that contain anyone of the seed entities, e 1 , e 2 , ..., e generate a sequence for each occurrence of e i for pattern mining. In order to focus patterns on entities and not generate too many patterns, we use only a window of 5 words before each entity name and 5 words after each entity name. Each word of a seed entity name is replac ed with a generic na me  X  X NTITYXYZ X . The purpose of using this generic word is to ensure that general patterns about any entities are found. Note that each entity name may consist of more than one word. The part-of-speech (POS) tag of each word is also used. In the final sequence each element of the sequence is a pair, POS tag of the word and the word. Example 3: We have the following sentence with POS tags The window is (n95 has been re placed with ENTITYXYZ): The resulting sequence is: &lt;{JJ, mad}{NN, everyone}{NN, doesnt}{VBP, have}{DT, a}{CD, ENTITYXYZ}{NN, phone}{ NN, fetish} {JJ, ducky}&gt; Step 2  X  Sequential pattern mining: Given the set of sequences generated from step 1, a sequen tial pattern mining algorithm is applied to generate sequential pa tterns [8]. We use 0.01 as the minimum support. We also require that each pattern must contain {POStag, ENTITYXYZ} a nd its length to be gr eater than or equal to 2 for obvious reasons. An example pattern is: Here  X  X N X ,  X  X T X ,  X  X NP X  are PO S tags which can match any words with that tag, and  X  X s X  is a concrete word which can only match this particular word. Step 3  X  Pattern matching to extract candidate entities : For each sentence in the test dataset, the system matches the generated patterns to extract a set of candidate entities. The patterns are sorted based on their supports. In order not to generate too many spurious candidates, the matching process in a sentence terminates after 5 patterns have been matc hed. We tried several numbers and find that 5 is a good number with respect to results and efficiency. Example 4: We have the follow sentence with POS tags attached: The pattern, &lt;{DT}, {NNP, EN TITYXYZ}, {CD}&gt;, will match the sentence segment, a/DT Nokia/NNP 7390/CD, to produce the candidate entity:  X  X okia X . The pattern, &lt;{DT}, {NNP}, {CD, ENTITYXYZ}, {IN}&gt;, will match the sentence segment, a/DT Nokia/NNP 7390/CD at/IN, to produce the candidate entity: 7390 Step 4  X  Candidate pruning : The above pattern matching may extract many wrong entities. A pruning method based on POS check is proposed. It remedies some errors made by the POS tagger system. Since an entity is always associated with a POS tag in our patterns, this method checks in the dataset to see whether the POS tag is the most frequent one for this candidate. If it is not, the candidate entity is eliminated (a possible POS tagging error). Example 5: Given the sentence: The pattern, &lt;{IN}{DT}{CD, ENTITYXYZ}&gt;, matches the sentence segment: to produce the candidate entity: acc essories, which is incorrect. But when the algorithm goes over the sentences in the dataset again, it found that  X  X ccessori es X  appear as  X  X NS X  more often than as  X  X D X . This candidate is deleted. The algorithm so far is generic and applicable to any domain because no assumption was made. The step below is more applicable to manufactured products (which are our main area of applications), which have brands and models. It should not be used for non-manufactured products. This step makes the assumption that a model name has a digit in it. In the experimental section we will show their results separately. Step 5  X  Pruning using brand and model relation and syntactic patterns . For most manufactured products, brands and models often appear together, e.g.,  X  X oto Razr V3 X . Here we need to use the above digit assumption. Thus, based on the entities that were found so far (step 4), this step tries to prune entities by using the pattern &lt;Brand Model&gt;. The first task is to discover relationships from the entities discovered so far. This is simple as the example below shows. Example 6: We have the following sentence: In this sentence, if both  X  X okia X  and  X  X 95 X  are in the entity list,  X  X okia X  is considered as &lt;Brand&gt;, and  X  X 95 X  is considered a &lt;Model&gt;. Then using some syntactic patterns can help find competing brands and models. The syntactic patterns exploit conjunctions and comparisons in sentences. We use C to denote a discovered entity and CN as a competitor. The following eight patterns are used: The second task is to remove those entities discover in step 4 that appear with a candidate in the syntactic patterns. We now present the entity assi gnment algorithm, which depends on opinion mining of comparativ e sentences. Below we first introduce the concepts of compara tives and superlatives and then discuss their impacts on the proble m. Based on the discussion, the algorithm is naturally derived. Comparative and superlative sentences are instrumental to our task. We define them here based on [12]. Comparative sentences express similarity and differences of more than one entity. There are three main types of comparatives, 1) Non-equal gradable :  X  X  reater or less than X  that expresses a 2) Equative :  X  equal to X  that states two entities as equal with 3) Non-gradable: Comparing two or more entities, but do not A superlative sentence expresses a relation of the type  X  greater or less than all others  X , i.e., it ranks one entity over all other entities. For example, the sentence,  X  Camera-X X  X  battery life is the longest  X , expresses a superlative relation. he/she will continue with the ent ity. If he/she wants to introduce a new entity e , he/she has to state the name of the entity explicitly in a sentence s 0 , which can be (1) a normal, (2) a comparative or (3) a superlative sentence. The questi on is what happens to the next entity, or s 1 is a comparative sentence and it does not mention e . For (1), when s 0 is a normal sentence, if s 1 is a normal sentence, it should talk about e . If s 1 is a comparative sentence, it should compare e with a new entity, whic h should be explicitly mentioned. For (2), when s 0 is a comparative sentence, if s normal sentence, there are a few cases: s is non-equal gradable : If s 1 has no entity name and it expresses a positive (respectively negative) se ntiment, it should talk about the superior (or inferior) entity to satisfy sentiment consistency . s is equative : In this case, it is unclear which entity is referred to in s 1 . We assume that it is the previous entity before s s is non-gradable : In this case, it is also unclear which entity is about before s 0 . For (3), when s 0 is a superlative sentence, if s sentence, it refers to the superlative entity in s (3), if s 1 is a comparative sentence, the entities in s Based on the above discussion, a natural algorithm emerges, which is given in Figure 1. It fo llows the simple method given in Section 1 but with special handlings to comparative sentences as discussed above. The input is a pos t, and the output is a set of entities discussed in each sentence. Note that the algorithm is simplified for presentation clarity. In our implemented system, the start post and quotes in replies are also considered as entities may be inherited from them. Comparative sentences here cover superlative sentences that contain more than one entity. For a superlative sentence with only a single entity, it is treated as a normal sentence. The notations used in the algorithm are: s .Entity: It stores the names of th e entities discussed in sentence s s opinion (): It is the opinion mining function that analyzes a non-compOpinion (): It is the opinion mining function that finds for each sentence s i in sequence in a post do 1 If s i is not a comparative sentence then 2 if s i contains an explicit entity then 3 s i .Entity  X  the explicit entity of the sentence s 4 else // s i does not contain an explicit entity 5 if s i -1 is not a comparative sentence then 6 s i .Entity  X  s i-1 .Entity 7 elseif a superior entity and an inferior entity were 8 opinion ( s i ); // opinion mining 9 if s i  X  X  first clause is a positive clause then 10 s i .Entity  X  s i -1 .superiorEntity 11 elseif s i  X  X  first clause is a negative clause then 12 s i .Entity  X  s i -1 .inferiorEntity 13 else s i .Entity  X  s i-1 .superiorEntity 14 else s i .Entity  X  s f .Entity, entities of the last sentence 15 else // s i is a comparative sentence 16 if no entity is mentioned in s i then 17 s i .Entity  X  s i-1 .Entity 18 else s i .Entity  X  { s i-1 .Entity}  X  {entities in s 19  X  s i .inferiorEntity, s i .superiorEntity  X   X  compOpinion ( s Figure 1 : The overall algorithm for entity assignment. We now present the opinion mining method used in the algorithm (i.e., opinion ( s i )), which is based on the method in [4]. Interestingly, we will also show in Section 6.3 that comparative sentences can be analyzed in a similar way (i.e., compOpinion ()). The main idea of the approach is to use opinion indicators to decide the orientations of opinions expressed on entity features. Orientations of opinions mean wh ether the opinions are positive, negative or neutral. There are three main opinion indicators that are used in opinion mining, i.e., opinion words and phrases, negations, and but-clauses. Th ey are discussed below. Opinion words and phrases : In most cases, opinions in sentences are e xpressed with opinion words , e.g.,  X  great X  ,  X  good  X ,  X  bad  X , and  X  X oor X  . Researchers have compiled sets of such words. Such lists are collectively called the opinion lexicon . In this work, we obtained the list from [4] with some additions. Apart from individual words, there are opinion phrases and idioms , e.g.,  X  X ost someone an arm a nd a leg X . Furthermore, some phrases may involve opinion words, but the whole phrases have no opinion or their opinions depend on contexts. For example, the phrase  X  X  good deal of X  does not ha ve an opinion although it has the positive opinion word  X  X reat X . Such phrases are called non-opinion phrases involving sentiment words . While most adjectives/adverbs have explicit positive or negative orientations, there are also many words whose orientations depend on contexts in which they appear. For example, the word  X  X ong X  in the following two sentences has completely different orientations, one positive and one negative:  X  The battery of this camera lasts long  X  and  X  This program takes a long time to run.  X  A method will be described in Section 6.3 to deal with this. Negations: opinion words and phrases fo rm the basis of opinions in a sentence. Negations reverse their orientations. Apart from  X  X ot X , many other words and phrases can be used to express negations. Furthermore,  X  X ot X  ma y not express negation in some cases, e.g., in  X  X ot only ... but also X . Such phrases are called non-negations involving negation words . But-clauses :  X  X ut X  means contrary. For example, the sentence,  X  The picture quality is great, but not the battery life  X  expresses a positive opinion on  X  X icture quality X  but a negative opinion on  X  X attery life X . The following rule states the effect of  X  X ut X : The orientation before  X  X ut X  is opposite to that after  X  X ut X  . Apart from the word  X  X ut X , many other words and phrases behave similarly, e.g.,  X  X hough X  and  X  X x cept that X . Similar to opinions and negations, not every  X  X ut X  ch anges opinion direction. For example,  X  X ut X  in the pattern  X  X ot only ... but also X  does not. Such phrases are called non-but phrases involving  X  X ut X  . With a large number of indicato rs, one can hard-code them in a system, which is, however, very undesirable because whenever a new word or phrase is encountered the program needs to be changed, which is time consuming. In [4], all phrases are hard-coded in the system. In this work, we propose a specification language to enable the user to specify indicators, which are (1) opinion words and phrases , (2) negation words and phrases , (3) but-like words and phrases , (4) non-opinion phrases involving sentiment words , (5) non-negation phrases involving negation words , and (6) non-but phrases involving but-like words . The system then automatically uses th e indicators for opinion mining (Section 6.3). All the opinion indicators are compiled manually. Two types of specifications are used: one for individual words and one for phrases. The reason for th e separation is that individual words express their default meani ngs, but their meanings may be changed by phrases, i.e., overwriti ng the defaults to express the indicators (4), (5) and (6). Specification of Individual Words: The grammar of the language for expressing individual words, which include opinion words, negation words and but-like words, is given below: The specification consists of a set of rules, i.e., each indicator word is represented as a rule. Each rule consists of two parts, an item on the left and an action on the right. The &lt;item&gt; is either an individual word or a word attached with a type, which may be anyone of the part-of-speech (POS) tags. &lt;action&gt; may be anyone of the five symbols, Po (positive), Ne (negative), Neu (neutral), Ng (negation) and But (but-like word). For example, if we want to express that  X  X ike X  expresses a positive opinion when it is a verb, we can use: Given a sentence, the system applies each rule by matching the word together with its type in the sentence and then associates the action symbol to the matched word. Specification for Phrases : The grammar is given below . The specification again consists of a set of rules. Each rule has pattern has a target word , indicated by [T], to which the action is applied. The idea is that the left-hand-side of the rule is first matched in the sentence and then the action of the rule is applied to the target in the sentence. See an example in Section 5.3. Some main concepts used in the grammar are: IndicatorSym: These are indicator symbols, Po, Ne, Neu, Ng and But, from individual indicator wo rds discussed above. A  X  X ype X  may also be attached, specifying the POS tag of the word. Word : It can be any word with an optional type. Distance : It indicates the number of words (or gap) that can appear between two non-adjacent items in the phrase.  X - X  means from  X  X um X  to  X  X um X  (num is an integer number). Target : It is the core item of th e phrase, indicating which word the rule is applied to. Some additional notes about the gra mmar:  X + X  is the separator,  X / X  means  X  X r X  and  X !num + !&lt;item&gt; X  means that within &lt;num&gt; words gap, &lt;item&gt; does not appear. The action on the right states that the action symbol should be associated with the target. The action symbol can be any of the outcomes or their negations, i.e., PO (positive), NE (negative), NEU (neutral), NG (negation), and BUT (but-like).  X ! X  means  X  X ot X . These action symbols cannot appear on left-hand-side, which prevents looping. Some remarks about th e language are: The ordering of rules is significant. When the first rule fo r a target word is matched and applied, the rest will not be tried. Choosing the right target is also important in the situation where a phrase overwrites the default meaning of a word. The target should be the word in question. For However, the phrase  X  X  great d eal of X  overwrites the orientation of  X  X reat X  because  X  X  great deal of X  has no opinion. In this case, the rule should be  X  X  great[T] + deal + of =&gt; NEU X  as the opinion of  X  X reat X  is nullified by the phras e. If we use  X  X  great deal[T] of =&gt; NEU X ,  X  X reat X  will still be treated as positive. We now describe opinion mining . We use the following running example sentence to show the working of each step: Step 1  X  Part-of-speech tagging: The tags are used for matching &lt;type&gt; X  X  in the rules. Step 2  X  Applying indicator word rules : All opinion words, negation words and but-like words in the sentence are discovered in this step. For our example, after this step, we obtain 
The picture quality is not [Ng] good [Po] , reaction is too slow [Neu] , but [But] the battery life is long [Neu].
 All the bold attachments are added in this step. The POS tags are omitted to improve readability. Step 3 -Applying phrase rules : This step identifies all phrases in the sentence and performs the actions specified in the rules. After this step, our running example sentence becomes: 
The picture quality is not[Ng] good[Po], reaction is too slow [NE] , but[But] the battery life is long[Neu]. The orientation of  X  X low X  is revised to negative ([NE]) due to the rule:  X  X oo + Neu[JJ][T] =&gt; NE X . Step 4 -Handling negations : A negation in a sentence reverses the orientation of an opinion. For ne utral, it is turned to negative. After negation handling, our running example sentence becomes ( X  X ood X  is now turned to negative from positive): 
The picture quality is not[Ng] good [Negative] , reaction is too slow[NE], but[But] the battery life is long[Neu]. Step 5 -Aggregating opinions: This step first finds but-symbols, which indicate opinion changes. The opinions on the two sides of a but-symbol are opposite to each other. Opinion aggregation : All opinion indicators in the first clause of the sentence are aggregated to arrive at the final opinion. The algorithm simply sums up all indicators. A positive (or negative) then the clause is positive. If the su m is less than 0, then the clause is negative and neutral otherwise. Handling context-dependent opinions : For those sentences that the above process cannot determ ine their orientations, the algorithm checks if it can detect context dependent opinions as in [9], which uses several rules. Only the conjunction rule is used in this work (the others are inaccurate). For example, in  X  The battery life is long  X , it is unclear whether  X  X ong X  means positive or negative. The method tries to se e whether any other person said that  X  X ong X  is positive (or negative). If another person wrote  X  this camera takes great pictures and has a long battery life  X . From this sentence, we can infer that  X  X ong X  is positive for  X  X attery life X  because it is conjoined with the positive word  X  X reat X . This is the conjunction rule , which says that a sentence only expresses one opinion, unless there is a but-lik e word changing the direction. As we mentioned earlier, the opi nion mining method above can be adapted to find superior and inferior entities in comparative sentences. This is due to the fact that positive and negative opinion words have their corresponding comparative and superlative forms indicating s uperior and inferior states respectively. For example, the positive opinion word,  X  X ood X , has its comparative and superlative forms,  X  X etter X  and  X  X est X , which indicate superior (and inferior) entities. In English, comparatives and superlatives are special forms of adjectives and adverbs. In general, comparatives are formed by adding the suffix  X  -er  X  and superlatives are formed by adding the Adjectives and adverbs with two syllables or more and not ending in y do not form comparatives or superlatives this way. Instead,  X  X ore X ,  X  X ost X ,  X  X ess X  and  X  X east X  are used before such words, e.g.,  X  X ore interesting X  and  X  most awful X . These two types are called regular comparatives and superlatives . English also has irregular comparatives and superlatives that do not follow the above rules. These are,  X  X ore X ,  X  X ost X ,  X  X ess X ,  X  X east X ,  X  X etter X ,  X  X est X ,  X  X orse X ,  X  X orst X ,  X  X urther/farther X  and  X  X urthest/farthest X . In order to use the opinion mi ning method mentioned above to opinion adjectives and adverbs to their comparative and superlative forms, which is done automatically by using English grammar rules and WordNet. Due to space limitations, we will not discuss the conversion in detail here as it is fairly straightforward. We then regard the comparatives and superlatives as positive and comparatives,  X  X etter X  and  X  X est X  are treated as positive, and  X  X orse X  and  X  X orst X  are treated as negative.  X  X ore X ,  X  X ost X ,  X  X ess X , and  X  X east X  require special handling. They are considered together with opinion words using the following 4 rules: Rule 1 says that  X  X ore/most X  and a positive (Pos) opinion word together mean positive, e.g.,  X  X ore beautiful X . Other rules have similar meanings. Non-standard words : Apart from the above comparatives and superlatives, many other words can also express comparisons, e.g.,  X  X in X ,  X  X refer X ,  X  X uperior X  and  X  X nferior X . For example, the Camera-Y  X , expresses a comparison indicating that Camera-X is preferred with regard to  X  battery life  X . These words are treated as positive or negative based on their meanings. Identify comparative and superlative sentences : Before we can identify superior entities from comparative sentences, we need to identify such sentences. [12] proposed a pattern mining approach to identifying comparative and superlative sentences. In this work, we did not focus on this task. Only several heuristic rules are designed to identify such sentences, which perform quite well. Clearly, comparative and superlative sentences are signaled by various keywords. We use a list of 67 keywords (obtained from [12]), which includes 4 part-of-speech tags, i.e., JJR (comparative adjective), RBR (comparative adverb ), JJS (superlative adjective) and RBS (superlative adverb). Our heuristics rules are as follows (if a sentence matches anyone of the rules, it is considered a comparative or a superlative sentence): where compkey is a comparative keyword, prodname is a product name and superkey is a superlative keyword. Discover superior entities : Finally, as mentioned earlier, the above opinion mining method can be used to discover superior entities. Since a gradable comparative sentence typically has entities on the two sides of the comparative keyword, i.e.,  X  Camera-X is better than Camera-Y  X . Based on opinion mining, if the sentence is positive, then the entities before the comparative keyword is superior and otherwise they are inferior (with the negation considered). Superlative sentences can be handled in a similar way. Note that equative and non-gradable comparisons do not express preferences. This section evaluates the proposed techniques for the two tasks, entity discovery and entity assignm ent. Below, we first describe our datasets and then presen t the experimental results. The experiment data collections are crawled from two forums, HowardForums and AVSforums. HowardForums is a message board dedicated to mobile phones while AVSforum is a message board dedicated to Home Theater and the products used. Our data from AVSforum are discussions about Plasma and LCD TVs, Projectors and DVD player s. Table 1 shows the characteristics of the two data sets. Altogether, we downloaded 64 threads, which contain 753 individual posts with 1072 comparative and superlative sentences. The total number of sentences is 4385. All the sentences and product names we re annotated by two graduate students based on consensus. We now present the experimental results for both tasks. Table 1 : Characteristics of the two data sets (comparative sentences including superlative sentences) The results of entity discovery are given first. Our method is called EI. It is compared with the NET system [27] from University of Illinois at Urbana Champion, and the Conditional Random Fields method (CRF) [16]. NET is a Named Entity Tagger, which can be used in our case as product names are named entities. The CRF system that we use is from Sunita Sarawagi [26]. Table 2 shows the results. Note that the NET system does not need training. The training data for CRF is the data obtained from step 2 of our algorithm. Recall that the data from step 2 is auto matically generated. The entities in those sentences are regarded as pos itive data and all the other words whole set for all the systems. Using the whole set as the test data is reasonable because our system does not use any manually labeled training data. Only a set of seed entities is supplied. The training data is automatically generated. In Table 2, we also compare EI when the first 3 steps (EI (1-3)), the first 4 steps (EI (1-4)) and all 5 steps are used (EI (1-5)). Using the first 3 steps basically means that the system only uses pattern mining for extraction. Pruning is step 4 was quite effective. As expected EI(1-4) produces high recalls but low precisions. However, EI(1-4) X  X  F scores are already dramatically higher than those of CRF and NET. For NET, we only used its results for organization entities. For other types of entities, the results are much worse. From Table 2, we also see that the additional step of EI improve the result further (EI(1-5)). Compared to EI(1-4), the precision increases dramatically with a small drop in recall, but the overall F scores are much higher. In practice, step 5 (which makes the digit assumption) is not needed because the high recall is the those non-entities fairly easily. Recall that our system uses some seeds to start the process. The question is how the number of seeds affects the final results. We performed a set of experiments by varying the number of seeds to see their effects. Figure 3 gives the results of 5, 10, 15 and 20 seeds. Clearly, when the number of seeds is small the precision is higher, but the recall is very low. With mo re seeds, we get more balanced better, it defeats the purpose of method which requires little user knowledge. Our experimental results reported in Table 2 are based on 15 seeds. All results are the averages of 10 random runs with randomly selected seeds. Figure 3 : Results of different seeds for entity discovery (Average of the two datasets) Table 3 gives the experimental results for entity assignment, which includes the results of two baseline methods. We use ED to denote the proposed technique. Tw o sets of experiments were conducted. The first set is denoted by  X  X ext Sentences X  in Table 3.  X  X ext Sentences X  means that only the comparative sentences and their subsequent sentences are considered. This set of experiments thus shows how effect ive the ED technique is in its intended task. The second set of experiments is denoted by  X  X ll Sentences X , which considers all se ntences. It shows how the ED method affects the overall impl icit entity assignment task. Column 1 (baseline1-next sentences): Baseline1 works as follows: Column 2 (baseline2-next sentences): In the Baseline2 method, if Column 3 (ED (k-com)  X  next sent ences): It gives the result of Column 4 (ED (unk-com)  X  next sent ences): It gives the result of Columns 5-8 (all sentences): These results correspond to those in Columns 9-11: They give the precision, recall and F-score of each In summary, the experimental results clearly demonstrated the effectiveness of the ED method. This paper presented a practical system that deals with two related problems in applications of opini on mining, i.e., mining entities discussed in a set of posts and assigning entities to each sentence. These are so important that w ithout solving them, any opinion discovered from the user-generated content is of limited use. We proposed a pattern-based method to deal with the first problem. To solve the second problem, we fi rst showed that the problem is mainly caused by comparative sentences. We then showed that the problem can be dealt with to a large extent by opinion mining on both the comparative sentences and the subsequent sentences. In the process, we also advanced the state-of-the X  X rt of opinion mining, especially in the analysis of comparative sentences. Our experimental results show th at the proposed techniques are effective. In our future work, we will further improve the accuracy of the system. [1] Bos, J., and Nissim, M. An Empirical Approach to the [2] Dave, D., Lawrence, A. , and Pennock, D. Mining the Peanut [3] Denis, P., and Baldridge, J, A Ranking Approach to Pronoun [4] Ding, X., Liu, B., and Yu, P. A Holistic Lexicon-Based [5] Esuli, A., and Sebastiani, F. Determining Term Subjectivity [6] Feldman R., Fresko M., Goldenberg J., Netzer O., and Ungar [7] Ganapathibhotla M., and Li u B. Mining opinions in [8] Han, J., and Kamber M. Data Mining: Concepts and [9] Hatzivassiloglou, V., and McKeown, K. Predicting the [10] Hu. M, and Liu, B. Mining and summarizing customer [11] Jiang, J., and Zhai C. Exploiting Domain Structure for [12] Jindal, N., and Liu, B. Mini ng Comparative Sentences and [13] Kaji, N., and Kitsuregawa, M. Building Lexicon for [14] Kanayama, H., and Na sukawa, T. Fully automatic lexicon [15] Kim, S., and Hovy, E. Dete rmining the Sentiment of [16] Lafferty J., McCallum A., a nd Pereira F.: Conditional [17] Mei, Q., Ling, X., Wondra, W., Su, H., and Zhai, C. Topic [18] Ng, V. Supervised ranking for pronoun resolution: Some [19] Pang, B., Lee, L., and Vait hyanathan, S. Thumbs up? [20] Popescu, A.-M., and Etzioni, O. Extracting Product Features [21] Riloff, E., and Wiebe, J. Learning extraction patterns for [22] Sarawagi, S. Information Extrac tion (Survey). Forthcoming. [23] Turney, P. Thumbs Up or Thumbs Down? Semantic [24] Wiebe, J., and Riloff, E. Creating Subjective and Objective [25] Yang, X, Su, J., and Tan, C.L. Improving Pronoun [26] http://crf.sourceforge.net/ [27] http://l2r.cs.uiuc.edu/~cogc omp/asoftware.php?skey=NE 
