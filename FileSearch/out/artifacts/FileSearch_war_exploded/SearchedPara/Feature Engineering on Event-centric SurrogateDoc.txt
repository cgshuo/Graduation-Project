 We investigate the task of re-ranking search results based on query log information. Prior work has considered this problem as either the task of learning document rankings of using features based on user behavior, or as the task of enhancing documents and queries using log data. Our contribution combines both. We distill log in-formation into event-centric surrogate documents (ESDs), and ex-tract features from these ESDs to be used in a learned ranking func-tion. Our experiments on a legal corpus demonstrate that features engineered on surrogate documents lead to improved rankings, in particular when the origi nal ranking is of poor qua lity. H.3.3 [ Information Search and Retrieval ]: Information filtering; Retrieval models; Search process Algorithms, Experimentation, Performance
Search engines are now part of daily life. Users routinely use them to find relevant information on the Web, and search engines leverage user interactions to enhance user experience and their bot-tom line. Prior research has shown that effectively extracting and using information from query logs can prove invaluable to improv-ing search results. Query logs can be used to modify how systems represent documents, rank results, and/or handle queries.
There are three main approaches to research using query logs: document-centric, ranking-centric, and query-centric. Document-centric approaches enrich or replace documents with surrogate doc-uments comprised of selected queries from logs. Recent approaches create surrogate documents by associating query text to documents viewed by users. Bilenko et al. [2] associate queries with docu-ments viewed via either search or browsing, and show that com-plete post-search viewing and browsing trails outperform other al-ternatives. Xue et al. [11] build surrogate documents from user queries, and then linearly combine the "query to surrogate docu-ment" similarity and the "query to original document" similarity into the final similarity score used for ranking.

Document-centric methods [2, 11] tend to handle surrogate doc-uments by running searches against them as if they were regular search documents. But they are not real documents and may con-tain a lot of noise. Simply treating a surrogate document as a regu-lar document does not leverage its full potential.

Ranking-centric approaches focus on transforming user interac-tions into features representing documents, and learning a ranking function using models such as SVMs or neural networks [1]. Some approaches focus on user behavior as implicit relevance judgments to replace manual annotations [5]. Joachims et al. [7] explore several strategies for generating relative feedback from clicks and show that implicit feedback corresponds with explicit judgements. Agichtein et al. [1] derive features based on various user actions and use a neural network to learn a ranking function.

Ranking-centric methods [1, 5, 7] are able to reduce the noise present in query logs, and thus improve search results. However, they may lack coverage. The features used in these methods are specific to a given query, and rely only on documents associated with that query in prior user interactions. Given the fact that more than 50% of queries submitted to search engines are new queries [9], these features can be quite sparse. Even when we take similar queries into account, the click-through could be quite low.
Lastly, query-centric approaches extend query expansion and query modification research by using information from query logs [4, 8] .
We propose a novel approach that combines document represen-tation using query logs and document ranking to improve search results. We construct surrogate documents using not only queries, but also by representing user interactions such as clicked or printed documents. Instead of relying solely on similarity between queries and surrogate documents as other methods do, we develop various features based on our event-centric surrogate documents. Further-more, we leverage the research on ranking-centric methods and use our features as input to a ranking SVM [6] rather than an ad-hoc combination of similarity scores.

Our experimental results are very promising. We show that our re-ranking method, based on simple and efficient features, signifi-cantly improves on the original search results. More importantly, we are able to greatly improve ranking when the original search performs poorly, while maintaining good performance when it per-forms well. In summary, our approach not only avoids the coverage issue of click-through based methods, but also reduces the noise is-sue of traditional surrogate document-based methods, improving search results significantly.
The information in query logs is usually quite noisy. By adding event information to surrogate documents, we are able to tackle the noise issue and extract more focused features from these event-centric surrogate documents.

Specifically, we create an event-centric surrogate document (ESD) for documents that appear in the query logs. We refer to the doc-ument appearing in query logs as the real document. An ESD is built by collecting all related queries as well as the corresponding events and their frequency. In effect an ESD is an aggregate of queries, events and counts across sessions where the real document appears. We say a query is related to a document if the query and the document appear in the same session. In addition to stemming, queries are normalized for space, punctuation and for syntax. Stop-words are also removed.

Table 1 provides an example of an event-centric surrogate docu-ment. The ESD is organized by queries and their associated events: each query is followed by one or more event types, as well as the count for each event type. For example, the table shows the real document is viewed twice but printed once when it appears in ses-sions with query q 4 . A surrogate document can consist of one to hundreds (even thousands) of queries. Although the content of an ESD is usually quite different from the real document, our research showssuchESDstobeveryuseful.
 Table 1: An example event-centric su rrogate document (ESD); note
ESDs differ from traditional surrogate documents in that they capture user behaviors as well as queries. That enriched represen-tation allows us to reduce the impact of noise by picking only rele-vant queries and assigning different weights to specific events.
Instead of using surrogate documents as replacements or addi-tional information to real documents, we extract various features that allow us to take full advantage of ESDs: 1)We utilize a subset of the queries in an ESD that is closely related to the user query; 2) We utilize the events associated with the selected queries; and 3) We utilize implicit relationships between documents.

Earlier, we pointed out that spar sity is a problem when features used for ranking rely on query X  X ocument pairs. To address sparsity, we introduce features not only at the query level, but also on query term level. There again, we will take full advantage of ESDs and incorporate events in the computation of features.
Given a user query q u and an ESD d , Q ud is the subset of queries in the ESD d related to q u . Then, for each query q i in Q compute a query X  X ocument feature f ( q i ,d ) by aggregating event-based feature values h ( e j ,d ) for individual events e Each query q i contributes to the final feature  X  ( q u ,d ) with a weight g ( q i ,q u ) :
Depending on how we define Q ud , f ,and g , we can generate multiple variations of these features.
 Currently, our selection of related queries is based on lexical sim-ilarity. We explored the following definitions for Q ud : 1) Exact match: Q ud includes a single query q u after normalization; 2) Top K: Q ud includes the top K similar queries based on the similarity between the query q u and candidate queries q i in the ESD d ;3) Threshold T: Q ud includes candidate queries q i when their similar-ity to the user query q u passes a predefined threshold.
To compute similarity, we rely on the vector space model and the cosine metric. We represent both query vectors solely in terms of term frequency ( tf ). We choose to ignore the inverse document frequency component idf as we believe that a term that appears in many queries should not be penalized.
 Event-based features are aggregated across individual event fea-tures: e j can be: 1) simple events such as views, prints, bookmarks, following an hyperlink, etc. 2) complex events, that is the com-bination of simple events on the same document in a session; for example a view followed by a print; a view followed by a naviga-tion and a print, etc. h ( ., . ) can be 1) raw frequency, the frequency of the event for query q i in surrogate document d ; 2) normalized frequency, the raw frequency normalized using a log function.
 The idea behind the weighting function is that queries more simi-lar to the user query may contribute more to the final feature value than queries that are less similar. We considered the following three weights: 1) Equal weight, which introduces no preference for sim-ilar queries; 2) Similarity: the similarity score sim ( q q and the user query q u ; and 3) Normalized similarity: the log of the similarity score above: log( sim ( q i ,q u )+1) The features above focus on individual documents. We now take into account relationships between documents by selecting a group of documents (anchor documents) and comparing each document to be ranked with these anchor documents. Specifically, we com-pute the number of queries in common between the ESD of the document to be ranked and the ESDs of the anchor documents. Our assumption is that documents sharing many queries with the anchor documents are more likely to be relevant to the user query q . Currently, we use the top N search results from the base search engine as the anchor documents.
In order to address sparsity, we use term-based similarity be-tween user queries and surrogate documents as additional features for ranking. A key attribute of our term-based features is that we incorporate event types as a weighting function of ESD terms. This allows us to build a bridge between query terms and various user behaviors. This property is increasingly important as the type of events tracked by search engines grows. In our experiments, we rely on the similarity introduced in the pre-vious section. We represent ESD vectors in terms of weighted term frequency ( tf ), and give more weight to terms associated with events that require more engagement from users. For example, terms associated with print events contribute more to the similar-ity score than terms associated only with view events. We assigned these weights heuristically in the experiments.
 Using the exact query-document similarity as defined above is prob-lematic as a feature, as the feature value will be zero for the many ESDs that do not include any user query terms. To address this term mismatch, we use query expansion. Following Billerbeck et al [3], we compute the Term Selection Value (TSV) from a group of anchor documents (ESDs). Rather than selecting K terms, we select a variable number of terms corresponding to the top K TSV values. The query expansion feature corresponds to the similarity score between the expanded query  X  q u and the ESD.
 Our third term-based feature makes indirect use of the user query q via anchor documents, and is the counterpart of the query shar-ing feature presented above.

We select a set of anchor documents D s , typically the highest ranked results by the base search engine. We then compute the av-erage similarity between the ESD d and the ESDs in the set D avoid over-crediting anchor documents by down-weighting their contribution in the average if the ESD d is part of set D the cosine similarity metric used in prior sections. In particular, we allow events to weigh in the similarity.

We assume that the base search engine returns a number of rele-vant documents to the user query q u in top positions. The document-document similarity feature assumes that the more similar a docu-ment is to those in top positions, the more relevant that document is to the user query q u . An advantage of this feature is that there is no parameter tuning beyond the selection of anchor documents.
We evaluate our re-ranking approach, combining the output of a search engine and features derived from query logs. We use WIN [10] as our baseline search engine on a collection of legal docu-ments, and we learn a new ranking using ranking SVMs [6].
We assembled a set of 270 queries created by attorney editors as well as partial relevance judgments for each query. We use that col-lection for both training and testing using 5-fold cross-validation. We used 5 levels of relevance, from A (most relevant) to F (marginal relevance at best). The average number of relevant (A and B) doc-uments per query is 9.

We used WIN to return the best 100 documents for each query, and then re-ranked those 100 documents using our surrogate-based features. We used 6 months of query log data to create the ESDs and compute their associated features.

Our primary interest was to determine the contribution of the query logs when the base search fails to return good results. We define a result (and its associated query) as good if WIN returns more than 2 A or B documents in the top 5 positions; otherwise, we consider it as bad . In our collection, there are 149 bad queries (referred to as BadQs ) and 121 good queries (henceforth GoodQs ). We report our results on the two sets separately.
We first evaluate the strength of individual features. For this evaluation, we focus on precision of strongly relevant (A and B) documents at rank 5. Table 3 summarizes individual performance of the features explained in Table 2. We omit other variations of query-based features as the results were of lesser quality.
By definition, the WIN rank works well on good queries and poorly on bad queries. Except for the term-based feature sim ( q all log-based features have better precision at 5 than the original search on bad queries. However none of the log-based features per-form better than WIN on good queries.
 Table 2: Feature descriptions for Table 3. The number of anchor doc-Table 3: Performance of individual features. Here coverage is the per-
As shown in Table 3, the coverage of most of the query-based features is low. As the threshold decreases, coverage in terms of queries and documents grows to the detriment of precision for both good and bad queries. In comparison with query-based features, term-based features have higher coverage as it is more common for documents to share terms than full queries. Surrogate docu-ments, as well as ESDs, are not good replacements for real doc-uments for ranking purpose as s hown by the poor quality of the query-document similarity approach, sim ( q u ,d ) . Query expansion improves precision at 5 dramatically compared to query-document similarity. We believe that adding terms from ESDs helps bridge the gap between a particular query and ESDs. The features sim (  X  q, d ) and sim ( d, D s ) may be complementary: although they have the same precision at 5, they rank different documents to the top.
After analyzing the strength of individual features, we represent each document with these features and train a ranking SVM to re-rank the search results. Our method, denoted as SUR, uses all the features in Table 2 except Threshold=0.4 and sim ( q u ,d ) . All fea-tures are normalized between 0 and 1 by dividing the value by the maximum value per query. The baseline method is to combine the ranks of both the WIN search and sim ( q, d ) (the similarity score between q and ESD), where the weight is learnt through SVM.
Figure 1 and Table 4 compare precision and NDCG among the baseline, WIN and SUR. The baseline performs much worse than WIN and SUR for both good and bad queries. For good queries, WIN already has high precision and NDCG; SUR is only slightly better (the difference is not significant). For bad queries (those where WIN fails), our proposed method improves results signifi-cantly at high rank. At ranks 1 and 5, precision of the re-ranking approach is 10% better than WIN, while at rank 10, precision is 4% better. The difference at ranks 1 and 5 between WIN and SUR is significant (We used a T-test with p =0 . 005 .) for bad queries. Figure 1: Baseline vs WIN vs SUR in terms of precision for: (a) good
Lastly, we look at the number of A and B documents returned in the top 5, 10 and 30 results respectively. Cumulative counts are plotted in Figure 2. The re-ranking approach SUR performs better than the baseline WIN for both good and bad queries. In particular, for bad queries, the number of A/B documents increases 38 . 2% , 13 . 6% ,and 6 . 4% in the top 5, top 10, and top 30 respectively. Figure 2: WIN vs SUR in terms of the number of highly relevant
Keyword-based search engines typically match query terms with document terms, and fail to rank relevant documents high in the re-sult list when a query term does not appear in a relevant document. Our approach using ESDs leverages how users have described doc-uments (via their queries and actions) and can place such docu-ments at the top of the result list. For example, one of the bad queries in our test set is "weight condition employment". One of the associated key documents, ranked low by the baseline system, does not contain the word condition. However, past sessions have associated queries similar to the test query to that document:  X  X m-ployer weight discrimination",  X  X eight limit employer", etc. Using any of these queries, users were able to find the relevant document, either from search result or navigating from a search result to a doc-ument out of the search results. Since we use surrogate documents as well as specific events in our re-ranking, the SUR algorithm is able to place that key case in the top re-ranking results.
The results in Table 3 also show that the query-document sim-ilarity feature sim ( q u ,d ) , representative of typical surrogate doc-ument approaches, is our worst performing feature. The baseline reranking result based on that feature also performs poorly. Our approach overcomes these issues by integrating query-based and event-centric features as well as term-based features: it attenuates noise by using the subset of queries that are most similar to the user query and their associated events.

Query sharing and document-document similarity sim ( d, D anchor features around the top-ranked search results. When the results are relevant, we expect these two features to perform well. When the top-ranked search results contain few relevant documents, there is a risk that the features will degrade over the ranking of the baseline system. However, our experiments show that these two features are very hel pful for reranking.
We propose a novel approach that combines event-centric sur-rogate documents from user logs with feature engineering for re-ranking. In particular, we introduce a flexible framework where query-based features can be engineered along three dimensions: finding similar queries, representing events, and weighting individ-ual contributions for queries and events. Our experiments instanti-ate the framework with very simple features; yet the results are very encouraging as our proposed appr oach improves search quality. On particular, when the original search engine fails to put relevant doc-uments at the top of the search results, our re-ranking approach is able to put more relevant documents at the top of the list.
We would like to thank James Shaw and Hiroko Bretz for assist-ing in the query-log data preparation, and Tonya Custis and Khalid Al-Kofahi for providing helpful feedback on our work.
