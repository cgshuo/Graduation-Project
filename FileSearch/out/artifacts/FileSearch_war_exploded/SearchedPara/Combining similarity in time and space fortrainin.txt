
Vilnius University, Vilnius, Lithuania Eindhoven University of Technology, Eindhoven, The Netherlands 1. Introduction
Concept drift challenges building supervised learning models for sequential data. The data distribution might change over time due to, for example, changes in user interests (recommender systems), external unobserved variables (bankruptcy pr ediction) or adversary activities (fraud detection). Thus adaptive learning models are required.

In supervised learning adaptivity can be achieved either by designing speci fi c base learners (e.g. [1]) or by manipulating training set over time in instance or feature space, or both. Manipulating training set includes instance selection (e.g. training windows [2], selective sampling [3]), instance weighting [4] and dynamic feature selection [5]. Training set manipulation strategies are wrapper approaches in a sense that they can be used for online learning plugging in different types of base classi fi ers.
Sequential instance selection (training windows) is typically used at sudden concept drift. Training window strategies select the nearest neighbors in time to form a training set. Selective sampling in space the feature space to the target instance are selected to form a training set.

In this study we present a concept of combining distances in time and space for training set selection under concept drift. A combined view to instance selection is required due to complex nature of the real data. Therefore, a uni fi ed view to the training sample formation is proposed which is fl exible with respect to the actual changes.

Preliminary results were presented in a short conference paper [6]. That study was delimited to the fi xed proportion of the distance in time and space.
Using time and space similarity concept we develop a method for classi fi er training, especially relevant when the expected drift is gradual. Training set selection is based on similarity to the target instance. Distances in space and in time are linearly combined. The method determines an optimal training set size online at every time step using cross validation. It is used as a wrapper approach, that means different base classi fi ers can be plugged in.
 The proposed method shows the best accuracy in the peer group on the real and arti fi cial drifting data. The method complexity is reasonable for the fi eld applications. The method is expected to demonstrate a competitive advantage under gradual drift scenarios in small and moderate size data sequences.
The paper is organized as follows. We start by a motivation for combining time and space similarity for training set seletion in Section 2, followed by fi xing the framework and basic assumptions. Next we introduce and illustrate the concept of distance in time and space in Section 3. The following Section 4 outlines particularly related work and maps the pr oposed concept within the related work. In Section 5 we present the proposed methods. Section 6 gives experimental setup and the results. Sections 8 and 9 discuss the results and conclude. 2. Problem set-up
In this section we present a motivation for combining similarity in time and space for training set formation under concept drift and fi x the set-up and basic assumptions for this study. 2.1. Motivation
Assume an online recommender system where a user reads online news. When she became more interested in real estate, market news were appearing more and more often as the most interesting topic. At the same time she was still interested in meat prices in New Zealand, but the relative interest was declining. Thus the relevance of a given document to the reader X  X  interests depended on the age of the document (distance in time) and the content (distance in space).

In order to build a classi fi er, which would react to a gradual concept drift, we aim to select the most relevant historical instances to form a training set. In the online news example, for each incoming document (unlabeled) we would look for similar documents within the historical stock.

Similarity between two objects in instance based learning [7] is de fi ned as a function of distance in a snapshot of Electricity data [8], which is provided in Fig. 1.

We plot the distance of the historical instances to the target instance over time against the distance in space (here we use the Euclidean distance in the feature space). The target instance is the very last in time (denoted  X  X ow X ), and its distance to itself in space is 0. The older instances are generally further from the target instance (declining slope along with x axis), which indicates the relevance of similarity in time. Recent instances are closer to the target instance. Moreover, there are notable recurrences in space, indicated by circles. This advocates that sequential sampling (window) might miss relevant training instances. Thus both distances in space and in time are to be taken into consideration. 2.2. Set-up and basic assumptions
Assume online classi fi cation task. One data instance X  X  p is received at a time, the corresponding instance X t +1 . It is allowed to retrain a classi fi er at every time step if needed.
Any selected or all the historical labeled data X 1 ,..., X t with corresponding known labels y 1 ,..., y t true label can be received, we can add X t +1 to the training data and proceed with the decision making for a target instance X t +2 .

Consider a gradual drift scenario, illustrated in Fig. 2. Up to time t 1 data generating source S I is active. Note, that the source is not the same as class label. Each source can generate an instance from either of the classes. A source can be considered to be a distribution. From time t 2 +1 on the source S instance comes from either one or the other source with a prior probability. The probability of sampling from S II increases with time. A designer does not know when the sources switch.

The task is to assign a class label to an instance X t +1 . It is expected that a concept drift might have We aim to select a training set, consisting of the instances, which are similar to the target instance. Similarity is a share of commonality. A detailed discussion on similarity concept can be found in [9]. In the next section we de fi ne similarity as a function of distances in time and space. 3. Similarity in time and space for training set selection
In this section we introduce and explore the concept of combining distances in time and space for training set selection to achieve adaptive learning. First we de fi ne how to measure similarity and then look how to use it for training set selection. 3.1. The concept of similarity in time and space
Let the similarity in time and space between the target instance X j and a historical instance X i be a distance function instances in time. The smaller the distance, the more similar are the instances.

Distance in time between the instances X i and X j in case of equally spaced time intervals is de fi ned as a function Different distance function can be chosen based on the domain knowledge and visual inspection of the In this study we use a linear distance, which is the least complex option d ( T ) ij = | i  X  j | .
Time intervals can be unequally spaced, e.g. stock prices are recorded only on weekdays, thus there is where T is the function mapping indexes to actual time values.

Distance in space can have a number of alternative metrics (e.g. Cityblock, Euclidean distances), a discussion of the most common metrics can be found in [10,11]. A distance metric of designer X  X  choice can be used.

We use two terms, similarity and distance , which are inversely related. The larger is distance, the smaller is similarity. We use the term similarity when referring to a general concept and the term distance when referring to an actual metric. 3.2. Combining distances in time and space
The form of a combination function D depends on the expectations of a designer related to the data at hand. The choice of time and space proportions directly depends on the observed change types and target instances well.

Let us look at the two boundary cases. If a designer selects training set only based on the distance in time, that is a training window strategy (see Fig. 3(a)). The most recent instances are selected as a training set in a sequential order. Another boundary case is to disregard time and select training set only based on the distance in space (see Fig. 3(b)).

In Fig. 4(a) we illustrate a linear combination of the distances in time and space, which we present in might be considered if an emphasis of the boundary instances is to be made.
 The linearly combined distance between the instances X i and X j is on the domain knowledge or visual inspection of the data or they can be trainable on a validation set or online.

For interpretation we normalize the proportions of time and space in the distance function d ( S ) and d ( T ) . We scale the values of each feature in X to the interval [0 , 1 the proportion can be regulated by the weights  X  1 and  X  2 . However, this way time and space distances become comparable across different datasets.

For training set selection under concept drift we are interested in relative distances (ranking). Thus, for simplicity,  X  1 and  X  2 can be replaced by A =  X  2  X  ranks of distances between the historical instances and the target instance X t +1 . Thus, we can simplify Eq. (3) to 3.3. Training set size
We de fi ned the distance in time and space D  X  , intended to be used for ranking the historical da-ta ( X 1 ,..., X t ) according to the distance to the target instance X t +1 . Another important choice in constructing a training set is how many from the most similar instances to include into the training set.
The training set size is speci fi ed applying a threshold to the distance measure. After the distance measure D  X  is fi xed, the training set size can be decided by moving the decision threshold, as shown in Fig. 5(a). Note, that here the slope is fi xed. It indicates the proportions of time and space distances in the fi nal distance measure, as described in Eq. (3).
 The threshold principle is the same as in variable window size selection, the case is illustrated in Fig. 5(b).

Thus, having an unlabeled target instance X t +1 ,for i =1 ,...,t the instance X i is selected into a by a designer or trainable based on a validation set. 4. Positioning within the related work In this section we present the related work and its relation to our approach.

We contribute to the fi eld by generalizing training set selection using time and space similarity. To our best knowledge the representation unifying windowing and instance selection under concept drift has not been formulated before. There are related techniques which are implicitly using instance forgetting when employing instance selection strategy, e.g. [12], which is mainly to overcome computational challenges in data streams.

Our approach integrates windowing techniques and instance selection techniques under uni fi ed frame-work of systematic training set selection. Moreover, it extends the existing approaches to a combination of both windowing and instance selection.

The issue of systematic training set selection in space under concept drift has been brought up in [3, plug-and-play algorithm. The blocks (intervals) of training data can be picked using moving window based templates.

The following two approaches use space based training set selection techniques. Tsymbal et al. [15] use an ensemble, where the competence of the base classi fi ers is determined by cross validation on the nearest neighbors of the target instance. However, they use training windows to build the individual prototypes. Since the main focus is on reoccurring concepts, time similarity is not integrated there. Valizadegan and Tan [16] use intelligent training set selection procedure after a change is detected. They aim to acquire more samples from the regions where classi fi cation is unreliable. They call the strategy differed-boosting and deferred active approach, where deferred means that resampling is triggered by a change detection.

The above approaches limit the history in time from which the instances can be selected. That is an implicit assumption in data stream mining, where the data streams in principle are endless. The approaches overviewed here have a clear cut in history, without incorporating time features into instance selection procedure.
 Beringer and Hullermeier [3] organize training data into prototype clusters, referred to as case bases. In contrast to the peer works, they exclude the instances which are too similar to the ones already present in the training set. The y explicitly address relev ance in time and space, as well as consistency. The major difference in our and their approach is in the future assumption. They assume continuous concept (and call it consistency). Track the concept itself and drop out inconsistent data. The approach would be unfavorable to reoccurring concepts and robust to noise. In our approach we determine the concept for a target instance without tracking the change. This way relevant training set might be found as well in case of reoccurring concepts and even in case of noise.

Lazarescu and Verkantesh [14] use time and space dimensions to determine the relevance of a given historical instance. The idea is closely related to the work by Beringer and Hullermeier [3] and has the same limitations regarding following the current concept. The former work was presented four years later than the latter.

Adaptive nearest neighbor classi fi cation [12,18] is related to our approach. Ueno et al. [18] focus on the computational complexity issues in streaming kNN application, not on training set selection directly. Their idea is to introduce an order in which the comparison of the distances between the instances is processed, that is likely to give more accurate results than random order, if the comparison is stopped before the end of the historical data is reached. Law and Zaniolo [12] use exponential weighting of the instances in time. They use the grid to divide the space into neighborhood region and adapt only the grids, where the newly arrived instances belong. Building the neighborhood can be viewed as instance selection in space, but their approach is kNN classi fi er speci fi c. The griding mechanism is explicitly oriented towards forming a single class cells, while generalization to different base classi fi ers would require an opposite strategy.

Finally, Black and Hickey [19] use the idea of augmenting the feature space by adding a time stamp feature. Then they use training window approaches, thus they do not employ space based selection. In principle the time feature can be integrated with space based instance selection. Augmenting the feature space and then measuring distances in the new space can be more fl exible with respect to base classi fi er can be organized. We choose the explicit combination of distances in time and space for training set selection mainly because it can be easier to control and interpret.

We propose an approach for training set selection based on similarity to target instance. There are phase. However, the needed classi fi er might not be present among the ensemble members. From similarity in space perspective our approach is related to a lazy learning [20], but the main difference is that the latter does not construct explicit generalization and makes classi fi cation based on direct comparisonof the target and training instances. Our approachcan be used as a wrapper with different base to generalize to reoccurring concepts. 5. FISH method family
To support our approach of combining distance in time and space, we propose a family of methods called FISH (uni F ied I nstance S election algorit H m), which incorporates the ideas presented above. Training instances are systematically selected at each time step. The methods can be used with different base classi fi ers.

The family includes three modi fi cations: FISH1, FISH2 and FISH3. In FISH1 the size of a training set is fi xed and set in advance, the extension FISH2 operates using variable training set size. In FISH2 in advance as a design choice. We present a modi fi cation of FISH3, where this proportion is trainable online.

We consider FISH2 to be the central in the family. We believe that in many cases the optimal instance, using an of fl ine validation set). On the other hand, the drifts might take non uniform speeds, thus online adjustable training set size is relevant.
 Next we give pseudo code and explain the details and intuitio n for each of the thr ee FISH methods. 5.1. FISH1
We start with presenting FISH1 in Fig. 6. The pseudo code includes the steps for training set selection for decision making at time t + 1.

The method ranks the historical instances (without their labels) according to their distance to the target instance and picks N the most similar instances to form a training set. Since the size of the training set selected from each class, altogether forming a training set of size N . 5.2. FISH2
FISH1 uses fi xed training set size N . FISH2 is an extension, where the training set size is learnable online. To implement a variable training size we incorporated the ideas inspired by two windowing methods [21] (KLI) and [15] (TSY). FISH2 is presented in Fig. 7.

We start by calculating the distances in time and space between the target instance X t +1 and every historical instance in X H as in Eq. (4). The distances to each historical instance are ranked based on the distance.

Next we decide how many of the most similar training instances to pick using cross validation. For We select the training size N  X  , which has given the best accuracy on the validation set. The method works similarly to windowing in [21] (KLI). They use sequential instances in time to form the windows. We employ a combined distance metric in time and space.

Leave-one-out cross validation needs to be employed. It means that we repeat the validation process k times for every training set size N being checked. Each time we leave out one validation instance from best accuracy, because in that case training set would be equal to the validation set.

The outcome of the method is a set of N  X  indexes I t = { z 1 ,...,zN  X  } . They indicate the historical instances to be picked as a training set X T t =( X z 1 ,..., X zN ) . Using the original instances X T t a in various types of a base classi fi er L . 5.3. FISH3 FISH3 is a extension of FISH2. FISH2 uses a pre fi xed proportion A of distances in time and space. FISH3 can learn the proportion online using an additional loop of cross validation. FISH3 is presented in Fig. 8. Instead of fi xing the proportion between time and space distances in D in Eq. (3) we try a number of options and pick the learner which is the most accurate on the validation set, the same principle as in FISH2. 6. Experimental evaluation
In order to verify the properties of FISH methods we carry out extensive numerical experiments. The main goal is to illustrate the advantage of combining distances in time and space as compared to using only time or only space criterion. We implement two peer methods and run them in parallel to FISH on different base classi fi ers and two alternative distance in space measures. 6.1. Datasets We use six data sets with potential gradual drift. Three datasets are real (Luxembourg, Ozone, Electricity), three other are real with an arti fi cially introduced drift (German, Vote2, Iono2). The All the datasets imply binary classi fi cation task. The characteristics of the datasets are summarized in Table 1.

We constructed 1 Luxembourg dataset using social survey data from [22 X 24]. 2 Each instance is a data [8] characterizes electricity demand in Australia, the task is to predict electricity market price.
German credit data [25] consists of individual credit application records, the task is to predict bankrupt-cy. We introduced arti fi cial drift in German credit data by hiding the age feature. We do not introduce learning process, this can be treated as the drift evidence.

In Fig. 9 we visualize all the datasets on time against distance in space axes. Note that the distances to one data point are visualized, which is rather a snapshot than a representation of the whole dataset. For illustration we u se cosine dist ances in space. 6.2. Experimental scenario
We perform three series of experiments related to FISH1, FISH2 and FISH3 correspondingly. 6.2.1. FISH1 and fi xed training set size
First, we run controlled FISH1 experiments. We vary the proportion of time and space A =  X  2  X  set size N .Theextreme  X  1 = 0 corresponds to a fi xed training window. Contrary,  X  2 = 0 corresponds only the distance in space.

We include a baseline ALL, which is using all the historical data as the training set. Thus it does not past data. If the data happens to be stationary, ALL should be the most accurate.

The pseudo code for ALL is provided in Appendix 9. 6.2.2. FISH2 and variable training set size
We present FISH2 as a fl agman in the FISH family and perform an extensive experimental evaluation (NMC), non-parametric k Nearest Neighbors classi fi er (kNN) (for which we take k = 7), Parzen Window cosine (the details will follow in the next section).

To support the viability of FISH2, we implement and run two peer methods for training set selection under concept drift: Klinkenberg and Joachims [21] (KLI) and Tsymbal et al. [15] (TSY). KLI method tries out a set of different training windows and selects the one showing the best accuracy on the validation data. The most recent training data is chosen as a validation set. TSY method builds a number performance on a validation set. Contrary to a time based selection, used in KLI, the latter method employs distance in space (to the target instance) criterion to select a validation set. Both methods use windows to form the individual classi fi ers. In contrast, FISH builds individual classi fi ers using systematic instance selection based combining distance in time and space. The summary of KLI and TSY with the options and interpretations chosen are presented in Appendix A.

The motivation for choosing this peer group is to observe the effect of integrated instance selection (time and space) which is done in FISH. The chosen methods are able to determine training set size using cross validation, they use no explicit change detection and are base classi fi er independent and do not require complex parametrization. We also include a baseline ALL, which uses all the historical data. If the data happens to be stationary, ALL should be the most accurate. 6.2.3. FISH3 and variable time and space ratio
Finally, we compare the performances of FISH1, FISH2 and FISH3 to see what bene fi ts in accuracy are brought by online parameter selection at a cost of increased computational complexity (as compared to FISH1 and FISH2).

We also analyze the progress of the training set size and the proportions of time and space in the distance function over time. 6.3. Implementation details
For FISH, FISH2, FISH3 and TSY we use the Euclidean distance in space where x ( i ) j is the i th feature of the instance X j and p is the dimensionality.

We also test FISH2 using an alternative cosine distance (inverse similarity) in space The features are scaled to the interval [0 , 1] before calculating the distance in space. We use linear calculating the proportion  X  1 :  X  2 .

We use the following setting for the methods.  X  For FISH1 and TSY we use training set size N = 40, FISH2, FISH3 and KLI have adaptable set  X  KLI and TSY operate in batch mode, we use batch size 15 for both.  X  For TSY we set maximum ensemble size to 7, and use 7 nearest neighbors for error estimation.  X  The fi xed weights proportions of time and space in the distance function for FISH1 and FISH2 are  X  For FISH2 and FISH3 we took training set sizes for cross validation with a step 5 to speed up the  X  If there are too few instances from one class in a formed sample, the label is assigned according to For Ozone, Elec and LU data backward search for FISH2, FISH3, KLI and TSY was limited to 1000 instances to reduce the complexity of the experiment. For testing with the decision tree using Elec and Ozone data we subsampled taking every 5th instance to speed up the experiments.
 7. Evaluation
We evaluate FISH2 performance based on the testing error and complexity . We analyze the progress of the experiments to draw qualitative conclusions.

To evaluate the accuracy, we calculate the ranks of the peer methods. The best method for a given data set is ranked 1, the worst method is ranked 4. The ranks for each data set sum up to 10. An average rank over all the datasets is calculated for each classi fi er and used as performance measure. for real datasets we used the McNemar [27] paired test, which does not require assumption about i.i.d. origin of the data.

To evaluate the applicability, we calculate the worst case and the average complexity of the six peer methods. We count the number of data passes required to make a classi fi cation decision for one observation at time t . The results (approximations) are presented in Table 2. Granularity g is a step of the time and space proportion. 3 We also present the parameters that need to be pre fi xed in advance for each method.
 The run time of FISH2 is reasonable for sequential data, for all fi ve methods it takes up to 1 min for NMC, kNN and PWC and for the decision tree it is  X  5 times longer to cast a classi fi cation decision for one time observation on a 1.46 GHz PC, 1GB RAM. For implementation MATLAB 7.5 is used.

Finance, biomedical applications are the domains where the data is scarce, imbalanced, while concept drift is very relevant. For example, in bankruptcy prediction an observation might be received once per day or even per week, while the model needs to be constantly updated and economic cycles imply concept drift. In supermarket stock management, stock quantity needs to be predicted once per week, thus only 52 observations are received per year. In such application cases even one hour of the method run for the decision would not be an issue. 8. Results and discussion
In this section we present and discuss experimental results. 8.1. FISH1 results
We run controlled experiments with FISH1 varying the proportion of time and space contribution in the distance measure. By controlled we mean that we fi x the setting except one parameter, which is the proportion of time and space in the distance function. We investigate the effect of the proportion to the testing accuracy on the six real dataset. We allow the proportion  X  1 :  X  2 change from 0:1 to 1:0 with a step 0.01.

We use NMC as the base classi fi er to simplify the setup as much as possible to analyze the effect of the proportions of time and space in the distance function to the testing accuracy.

The testing results for each of the six datasets are provided in Fig. 10. We plot the testing accuracy against the proportion of time and space in the distance function.  X  1 = 0 means that only the distance in time is used, which corresponds to a training window of a fi xed size.  X  1 = 1 (implies  X  2 = 0) means that only the distance in space is used. All the values in between indicate different proportions of time and space in the distance function in training set selection.

In Table 3 we provide the numerical results using a step 0.1 for  X  1 values. Although the primary purpose of the experiment is to analyze the relation between the proportions of time and space in the distance function and accuracy, we also indicate statistical signi fi cance of the difference between FISH1 and the baseline ALL using McNemar test.

The results both in Fig. 10 and in the table show that already using a primitive fi xed size N technique the best accuracy is achieved in combination of distances in time and space. The minimum error is heavily shifted towards distance in space in most of the datasets ( X  X zon X ,  X  X red X ,  X  X ono X  and  X  X U X ), which is explainable by the data origin. The datasets were picked expecting heterogeneous structure in space and also to have a temporal order.  X  X U X  data has it X  X  minimum testing error at the very extreme distance in space proportion (time proportion is 0), which we could observe in the data plot in Fig. 9. Visually, the time order in  X  X U X  data is weak. On the contrary,  X  X lec X  data has visually strong time order and that correlates with the testing results. One can observe in Fig. 10 that the minimum testing error for  X  X lec X  is close to the time proportion extreme. It suggests the training based on windowing would be preferable. Change detection technique, which is in principle variable sized windowing, was applied by Gama et al. [28], who introduced this data set for concept drift problems.

The dashed lines in Fig. 10 indicate the baseline testing error, which is achieved by using full history as a training set (ALL). The primitive FISH1 using a fi xed training size already outperforms ALL in fi ve out of six datasets. Absolute error in  X  X zon X  is so high since the classes are heavily unbalanced (major class makes 94%). In  X  X red X  FISH1 is worse than ALL in all the time and space proportions. It suggests that either the data is stationary, or the fi xed size of the training set is very much non optimal. The training set size was chosen to be equal for all the data sets to keep the settings uniform and the results comparable. Next we look at the results when the training size is learnable online. 8.2. FISH2 results
We test FISH2 along with two peer methods KLI and TSY as well as the baseline ALL. We test using allinallwerun4  X  2  X  6 = 48 experiments for each of the methods. The results are provided in Tables 4 and 5. We use McNemar paired test to estimate the statistical signi fi cance of the difference between FISH2 and the peers.

The fi ve methods were ranked as presented in Section 7 with respect to each data set, and then the ranks were averaged (last column in Table 4).
 FISH2 has the best rank by a large margin with NMC and tree classi fi ers, for kNN and PWC either FISH2 or ALL prevails, depending on the distance measure. The fi nal scores averaged over all four base classi fi ers and two alternative distance measures are: 1.68 for FISH2, 2.83 for KLI, 3.07 for TSY and 2.42 for ALL.
Using kNN, PWC and tree as a base classi fi er, ALL method outperform TSY and KLI according to the responsive methods and increasing complexity, as simple retraining (ALL) would do well. The results in favor of FISH2 are signi fi cant in about half of the cases. Some of the results indicate no statistical difference, however, it should be taken into account that the datasets are not large and the test is non parametric.

FISH2 method is designedto work where conceptdrift is not clearly expressed. These are the situations of gradual drift, reoccurring concepts. ALL method outperforms all the drift responsive methods but not FISH2 with kNN as a base classi fi er.

Windowing methods work well on Elec data, because the drifts in this data are more sudden. Elec data shows the biggest need for concept drift adaptive methods, because for these datasets ALL method performs relatively the worst from the peer group.

The credits for FISH2 performance in the peer group shall be given to similarity based training set selection. KLI addresses only similarity in time (training window). TSY uses only similarity in time for of distance in time and space already in classi fi er building phase.

It is interesting to look why FISH2 outperforms KLI and TSY in terms of accuracy even on the datasets demanding training windows. This is because FISH2 uses adaptive validation set as compared to KLI and variable training set size as compared to TSY.

In FISH2 experiments we fi xed equal proportion of time and space in the distance function  X  1 :  X  2 = 1:1 , in order to have uniform comparable setups for all the datasets. In the next section we look if FISH2 results can be improved by allowing the time and space proportion to be learnable online. 8.3. FISH3 results
FISH3 implements variable training set size and variable proportions of time and space in the distance function, both are learnable online. Recall, that we had different time and space proportions in FISH1 experiments. However, in FISH1 the proportion is fi xed for all the experiment. In FISH3 we have a variable proportion for every time step and it is learnable online.

In Table 6 we compare the accuraci es of the three FISH methods usin g simple settings: NMC classi fi er and Euclidean distance in space. We use the same fi xed proportion of time and space as before for both FISH1 and FISH2 (  X  1 :  X  2 =1:1 ).

FISH3 has the best accuracy in all cases except for Ozone data, which is very highly imbalanced. We include a baseline ALL to verify if our concept drift responsive methods make sense. The differences between ALL and FISH accuracies are statistically signi fi cant everywhere except in Credit data. It might be argued that improvement in accuracy shown by FISH3 as compared to FISH2 is marginal. (Luxembourg, Ozone and Electricity) which are more than twice longer than the remaining ones, besides they have a natural temporal order, while the remaining three have assumed temporal order.
Let us look at the time and space proportion. In Table 6 we provide averaged space proportion (mean  X  ). Luxembourg and Ozone datasets are inclined towards distance in space, while Vote and Electricity data shows preference to distance in time. That is not fully consistent with the observations in FISH1 experiments, Section 8.1. Note that in FISH1 experiments the time and space proportion was fi xed for all the run on a dataset, while here we allowed the proportion to vary every time step. In Fig. 11 we plot the progress of the time and space proportion for all six datasets. The line is smoothed using a moving average of 5 to emphasize the tendencies against individual peaks.

It can be concluded that if the domain allows increased complexity variable training set size and variable time and space proportions are worth applying to gradually drifting datasets.

The FISH family of methods should be regarded as an extension to existing techniques. It emphasizes that time and space relations are not discrete, but can be viewed in a continuous space.
Tsymbal et al. [15] method is presented in Fig. 14. 9. Conclusion
We formulated a concept of similarity in time and space in for adaptive training set selection. It leads to a range of training set selection strategies from based on training window based to instance selection in space.

Based on the formulated concept we developed a family of methods for training set selection under concept drift. FISH1 uses a preset proportion of time and space in the distance function and preset training set size. FISH2 learns the training set size online at every time step using cross validation on the historical data. FISH3 learns online both the training set size and the proportions of time and space in the distance function.

With FISH1 we demonstrate that for a gradually drifting data combination of distances in time and space can lead to a better classi fi cation accuracy than using a single technique.

FISH2 shows the best accuracy in the peer group on th e datasets exhibiting gra dual drifts and a mixture of several concepts. The method complexity is reasonable for fi eld applications.
 FISH3 demonstrates that the proportion of time and space in the distance function is learnable online.
We show the advantages of combination of the distances in time and space. Combining time and space for training instance selection contributes to improvement of classi fi er generalization performance under gradual concept drift, since this way heterogeneous nature of the drifting data can be captured. Appendix Peer methods
In this Appendix we present pseudo codes and the settings used for the peer methods, which we implemented and used in experimental evaluation.
 In Fig. 12 we provide a pseudo code for the incremental growing window, which is used as a baseline. Klinkenberg and Rentz [29] method is presented in Fig. 13. The original work used Support Vector Machines (SVM) as the base classi fi er. We use the method with different base classi fi ers. References
