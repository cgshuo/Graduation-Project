 Here ` learning.
 dif ferent lines.
 number of alternations in the sequence of output bits b where w with a weak learner that exhausti vely considers all 2 n possible literals x The algorithm' s performance can be bounded in terms of the L ples. Recall that the L The L concentrated on a constant number of elements of the domain then the L if the probability mass is spread uniformly over a domain of size N then the L f rele vant variables to f (i.e. if the rele vant variables are x on an input ( z minimal L probability 1 constructs a hypothesis h that is -accur ate with respect to D . 2 respect to D .
 the form sgn ( P n P decision lists.
 boosting is exploited for attrib ute efcient learning. We sometimes refer to E stages and at each stage t maintains a distrib ution D rithm is given a weak hypothesis h to construct the next distrib ution D constructs a nal hypothesis h based on the weak hypotheses h Let D and any distrib ution D -weak hypotheses at each stage, D will denote such a smooth boosting algorithm.
 relati ve to U but which still have kD k which puts weight 1 2 the L to the uniform distrib ution. The total variation distance between two probability distrib utions D d of the distrib utions are disjoint. The follo wing is immediate: Lemma 1 For any two distrib utions D 1 P x 2 X min fD 1 ( x ) ; D 2 ( x ) g : terms of the ratio kD k we have d By Mark ov's inequality , By Lemma 1, we have inequality uses (1). Using the denition of M and solving for d Let f be any decision list that depends on k variables: where each ` essentially equi valent claims): It is easy to see that for any x ed c exactly once. No w we can pro ve: Lemma 4 Let f be any decision list of length k over the n Boolean variables x by the k rele vant variables of f: Suppose that d hypothesis h 2 f x under any distrib ution D some weak hypothesis h from f x E w.l.o.g. that &gt; 4 second concerns x so over the 2 k odd inte gers in the interv al [ 2 k ; 2 k ] , as c 2 k 1 , and so on.
 Let S denote the set of those x 2 f 1 ; 1 g k that satisfy j L ( x ) j 2 inte ger such that 2 j 1 Pr
U k [ j L ( x ) j &gt; 4 2 k ] 1 = 2 have d Recalling that each j c satisfy E is complete. additional technical complications the basic idea is as in the pre vious section. We will use the follo wing fact due to H astad: tion that depends on all k variables x for f whic h is suc h that (assuming the weights w Theorem 2 in [28 ].
 ther e is some weak hypothesis h 2 f x Pr oof sketch: We may assume that f ( x ) = sgn ( L ( x )) where L ( x ) = w w ; : : : ; w k as described in Fact 3.
 some h 2 f x so the lemma holds if ` k: Thus we henceforth assume that ` &lt; k: It remains only to sho w that once we have this, follo wing (3) we get E and now since each j w Case I: For all 1 i ` we have w 2 Let := r 2 P k k w k to denote q P k i =1 w 2 i ). This bound directly gives us that Pr 1 = (( k + 1)( ` + 1)!) by Fact 3, we have established (4) in Case I. Case II: For some value J ` we have w 2 4) we have Thus for each z 2 f 1 ; 1 g J 1 we have Pr This immediately yields Pr w and J `: when MadaBoost simulates the distrib ution D are used to obtain an empirical estimate of E (Here is an upper bound on the adv antage E observ ed empirical estimate. The algorithm is run for T = O ( 1 h after at most T = O ( 1 the best weak hypothesis h 2 f x consequently we will have Thus, by Lemma 2 each distrib ution D the required adv antage. decision lists with respect to dif fuse distrib utions? Is it critical? list was determined by picking ` from among the possibilities.
 each of these in detail. 1 d 30000 =m e times. The average test-set error is reported. a smooth boosting algorithm. The average smoothnesses are given in Table 2.
