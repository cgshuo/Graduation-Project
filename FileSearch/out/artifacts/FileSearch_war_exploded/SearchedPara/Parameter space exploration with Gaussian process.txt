 Robert B. Gramacy rbgramacy@ams.ucsc.edu HerbertK.H.Lee herbie@ams.ucsc.edu Baskin Engineering: 1156 High St. Santa Cruz, CA 95064 William G. Macready wgm@email.arc.nasa.gov University Affiliated Research Center As computing power has advanced so too has the fi-delity of computer simulation. However, this fidelity often comes at great computational expense. Com-putational fluid dynamics simulations in which fluid flow phenomena are modeled are an excellent example  X  fluid flows over complex surfaces may be modeled accurately, but only at the c ost of supercomputer re-sources.
 A simulation model defines a mapping (perhaps non-deterministic) from parameters describing the input to one or more output responses. Without an analytic representation of this mapping, the simulation must be run for many different inputs in order to build up an understanding of the simulation X  X  possible outcomes. Computational expense and/or high dimensional in-put usually prohibits a naive approach to the mapping of the response surface. A computationally inexpen-sive approximation to the simulation (O X  X agan et al., 1999) with active learning is one possible remedy. If the approximation is a good match to the simulation, then samples may be drawn in regions of the input space where the output response is changing signifi-cantly. For models which return both predictions and associated confidences, regions can be identified where the model is unsure of the response.
 We focus on Gaussian processes (GPs) as a suitable approximation for a number of reasons. GPs are con-ceptually straightforward, easily accommodate prior knowledge in the form of covariance functions, and return a confidence around predictions. In spite of these benefits there are three important difficulties in applying standard GPs in our setting. Firstly, infer-ence on the GP scales poorly with the number of data points; typically requiring O ( N 3 ) time, where N is the number of data points. Secondly, GP models are usually stationary as the same covariance structure is used throughout the entire input space. In the appli-cations we have in mind, where subsonic flow is quite different than supersonic flow, this limitation is unac-ceptable. Thirdly, the error (standard deviation) as-sociated with a predicted response under a GP model does not directly depend on any of the previously ob-served output responses. Instead, it depends only upon the previously sampled input settings { x i } N i =1 and the covariance matrix C ( x i , x j ). All of these shortcomings may be addressed by partitioning the in-put space into regions, and fitting separate GPs within each region. Partitioning allows for non-stationary be-havior, and can ameliorate some of the computational demands (by fitting models to less data). Finally, a fully Bayesian interpretation yields uncertainty mea-sures for predictive inference which can help direct fu-ture sampling.
 This work draws upon two successful approaches. The use of trees and recursive partitioning for achieving non-stationarity has a long history (Breiman et al., 1984; Denison et al., 1998) X  the Bayesian applica-tion of which is well worked out X  and the use of GPs for active learning has also seen recent success (Seo et al., 2000). Alternative approaches for non-stationary modeling include mixtures of GPs (Tresp, 2001) and infinite mixtures of GPs (Rasmussen &amp; Ghahramani, 2002).
 This paper is structured as follows. We define the problem and review the necessary background in Sec-tion 2. Section 3 provides details on the use of Bayesian treed GP models including inference and pre-diction. Section 4 consider s how the treed GP model is used to adaptively select input parameters. Section 5 presents results on real and simulated data, and pro-vides a comparison of the proposed adaptive sampling scheme to two others used in the literature. We model the simulation output as (Sacks et al., 1989) where Y is the (possibly multivariate) output of the computer model, x is a particular (multivariate) input value,  X  are linear trend coefficients, and W ( x )isa zero mean random proces s with covariance C ( x , x )=  X  K ( x , x ), and K is a correlation matrix. Stationary Gaussian processes (Sacks et al., 1989) are a popular example of a model that fits this description. As discussed in the introduction, we require more flexi-bility than offered by a stationary GP. To achieve non-stationarity we turn to binary trees, using them to partition the input space, and then fit a GP to each partition. This approach bears some similarity to the models of Kim et al. (2002), who fit separate GPs in each element of a Voronoi t essellation. Our approach is better geared toward problems with a smaller num-ber of distinct partitions, leading to a simpler overall model. Using a Voronoi tessellation allows an intri-cate partitioning of the space, but has the trade-off of added complexity and can produce a final model that is difficult to interpret. The complexities of this added flexibility are not warranted in our application. 2.1. Stationary Gaussian Processes GPs are a popular kernel-based method for regression and classification. Though the method can be traced back to Kriging (Matheron, 1963), it is only recently that they have been broadly applied in machine learn-ing. Consider a training set D = { x i , t i } N i =1 of m dimensional input parameters and m Y -dimensional simulation outputs. We indicate the collection of in-puts as the N  X  m X matrix X whose i th row is x i . A GP (Seo et al., 2000) is a collection of random vari-ables Y ( x ) indexed by x having a jointly Gaussian distribution for any subset of indices. It is specified by a mean  X   X   X  ( x )= E Y ( x ) and correlation function a set of observations D , the resulting density over out-puts at a new point x is easily found to be Gaussian with variance  X  2  X  y ( x )=  X  2 [ K ( x , x )  X  k ( x ) K  X  1 For simplicity we assume that the output is scalar (i.e., we are modeling each output response independently and so m Y = 1) so that the image of the covariance function is a scalar. To keep this equation simple, the linear term from (1) has b een omitted here. We de-fine k ( x )tobethe N -vector whose i th component is K ( x , x i ), K to be the N  X  N matrix with i, j element K ( x i , x j ), and t to be the N -vector of observations with i component t i . It is important to note that the uncertainty,  X  2  X  y ( x ), associated with the prediction has no direct dependence on the observed simulation out-puts t . Typically, the covariance function depends on hyperparameters which are d etermined either by max-imizing the likelihood of D or integrating over them. 2.2. Bayesian Treed Models A tree model partitions the input space and infers a separate model within each partition. Partitioning is often done by making binary splits on the value of a single variable (e.g., speed &gt; 0 . 8) so that partition boundaries are parallel to coordinate axes. Partition-ing is recursive, so each new partition is a sub-partition of a previous one. For example, a first partition may divide the space in half by whether the first variable is above or below its midpoint. The second partition will then divide only the space below (or above) the mid-point of the first variable, so that there are now three partitions (not four). Since variables may be revisited, there is no loss of generalit y by using binary splits as multiple splits on the same variable will be equivalent to a non-binary split. These sorts of models are of-ten referred to as Classification and Regression Trees (CART) (Breiman et al., 1984). CART has become popular because of its ease of use, clear interpretation, and ability to provide a good fit in many cases. The Bayesian approach is straightforward to apply to tree models (Chipman et al., 1998; Denison et al., 1998). Key is the specification of a meaningful prior for the size of the tree. Here we follow Chipman et. al who specify the prior through a tree-generating pro-cess. Starting with a null tree (all data in a single partition), the tree T is probabilistically split recur-sively, with each partition  X  being split with probabil-ity p split (  X , T )= a (1 + q  X  )  X  b where q  X  is the depth of  X  in T and a and b are parameters chosen to give an appropriate size and spread to the distribution of trees. More details are available in Chipman et. al (1998). We expect a relatively small number of par-titions, and choose these parameters accordingly. As part of the process prior, we further require that each new region have at least five data points, since the parameters of a GP cannot be effectively estimated if there are too few points in a partition. We begin by defining the model conditional on a par-ticular tree, and later discus s integrating over possible trees. 3.1. Hierarchical Model Atree T recursively partitions the input space into into R non-overlapping regions: { r  X  } R  X  =1 .Eachregion r  X  contains data D  X  = servations. Each split in the tree is based on a selected dimension u j  X  X  1 ,...,m X } and an associated split criterion s j , so that one of the resulting sub-partitions consist of those observations in { X  X  , t  X  } with the u parameter less than s j , and the other contains those observations greater than or equal to s j .
 Given a tree T , we fit a stationary GP with linear trend (1) independently within each region. The n  X   X  n  X  covariance matrix for the process in the  X  th region is denoted K  X  and the linear trend coefficients are  X  . We denote the full set of coefficients across all re-gions as  X  =[  X  1 ,...,  X  R ] (and similarly for all other region-specific parameters). The hierarchical genera-tive model we use is: 1 with F  X  =( 1 , X  X  ), and W is a ( m X +1)  X  ( m X +1) ma-trix. N , IG ,and W are the Normal, Inverse-Gamma, and Wishart distribution, respectively. The GP corre-lation structure for each partition, K  X  ,ischosenfrom an isotropic power family with a fixed power p 0 , but unknown range d  X  and nugget g  X  parameters: where  X   X  ,  X  is the Kronecker delta function. For nota-tional convenience we continue to refer K as a corre-lation matrix, even though with the nugget term, g , in K (  X  ,  X  )ofEq. (2)itisnolongertechnicallyacorre-lation matrix. Hierarchical mixture-priors on d and g can express our prior belief that the global covariance structure is non-stationary. Below, we shall refer to parameters to such hierarchical priors as  X  . Finally, constants  X  , B , V , X , X  0 ,q 0 ,p 0 are treated as known. 3.2. Prediction Prediction under the above GP model is straightfor-ward (Hjort &amp; Omre, 1994). The predicted value of y at x is normally distributed with mean and variance where C  X  1  X  =( K  X  + F  X  WF  X  )  X  1 , q  X  ( x )= k  X  ( x )+ F  X  W  X  f ( x ), f ( x )=(1 , x ),  X  ( x , y )= K  X  ( x , y )+ f ( x ) Wf ( y ), and k  X  ( x )isa n  X   X  vector with k  X ,j K  X  ( x , x j ), for all x j 3.3. Estimating the model parameters The data D  X  = { X , t }  X  are used to estimate the pa-rameters  X   X   X {  X   X  , X  2  X  ,d  X  ,g  X  } ,for  X  =1 ,...,R .Pa-rameters to the hierarchical priors (  X  0 = { W , X  0 ,  X  } depend only on {  X   X  } R  X  =1 . Conditional on the tree T we write the full set of parameters as  X  =  X  0  X  R  X  =1  X  Samples from the posterior distribution of  X  are gath-ered using Markov chain Monte Carlo (MCMC) (Gel-man et al., 1995).  X  , X  2  X  ,  X  0 , W are updated with Gibbs steps. The other parameters require Metropolis Hastings (MH) steps. It is advantageous for mixing to analytically in-tegrate out  X  and  X  2 to get a marginal posterior when updating d  X  and g  X  . 3.4. Tree Structure Integrating out dependence on the tree structure T is accomplished by reversibl e-jump MCMC (RJ-MCMC) (Richardson &amp; Green, 1997). We implement the tree operations grow, prune, change ,and swap similar to those in Chipman et al. (1998). Tree proposals can change the size of the parameter space (  X  ). To keep things simple, proposals for new parameters X  via an increase in the number of partitions R  X  X redrawn from their priors, thus eliminating the Jacobian term usually present in RJ-MCMC. New splits are chosen uniformly from the set of marginalized input locations X .
 Swap and change tree operations are straightforward because the number of partitions (and thus parame-ters) stays the same. In a change operation we propose moving an existing split-point { u, s } , to either the next greater or lesser value of s ( s + or s  X  )alongthe u th dimension of (marginalized) locations from X .This is accomplished by sampling s uniformly from the set { u the split-point { u, s } are held fixed. Uniform propos-als and priors on split-poi nts cause the MH acceptance ratio for change to reduce to a simple likelihood ratio. The swap operation is similar, however we slightly aug-ment the one described in Chipman et al. (1998). Swaps proposed on parent-child internal nodes which split on the same variable are always rejected because a child region below both parents becomes empty after the operation. However, if instead a rotate operation from Binary Search Trees (BSTs) is performed, the the proposal will almost al ways accept. Rotations are a way of adjusting the configuration (and thus height) of a BST without violating the BST property. Ro-tations encourage better mixing of the Markov chain by providing a more dynamic set of candidate nodes for pruning, thereby helping it escape local minima. Since the partitions at the leaves remain unchanged, the likelihood ratio of a proposed rotate is always 1. The only  X  X ctive X  part of the MH acceptance ratio is the prior on tree T , preferring trees of minimal depth. Grow and prune operations are more complex because they add or remove partitions, and thus cause a change in the dimension of the parameter space. The first step for either operation is to select a leaf node (for grow ), or the parent of a pair of leaf nodes (for prune ). We choose the node uniformly from the set of legal candi-dates. When a new region r is added, new parameters { d, g } r must be proposed, and when a region is taken away the parameters must be absorbed by the par-ent region, or discarded. When evaluating the MH acceptance ratio for either o peration we marginalize over the {  X  , X  2 } r parameters. One of the newly grown children is selected (uniformly) to receive the d and g parameters of its parent. To ensure that the resulting Markov chain is ergodic and reversible, the other new sibling draws its d and g parameters from their pri-ors. Symmetrically, prune operations randomly select parameters d and g for the consolidated node from one of the children being absorbed. If the grow or prune operation is accepted,  X  2 r can next be drawn from its marginal posterior (with  X  r integrated out) after which draws for  X  r and the other parameters for the r th region can then proceed as usual. Having described the predictive algorithm used to model P ( y | x ), we now consider how to choose new sampling locations based on this distribution. Two criteria have been previously proposed. The simplest choice is to maximize the information gained about model parameters {  X  , T} by selecting the  X  x which has the greatest standard deviation in predicted output (Mackay, 1992). Given its simplicity this is the method we explore here. An alternative measure is to select  X  x minimizing the resulting expected squared error over the input space (Cohn, 1994). A comparison between these two methods using standard GPs appears in Seo et al. (2000). In the results described below we use the difference between the 95% and 5% quantiles of the predicted output value as the measure of uncertainty X  an MCMC scaled-approximate standard deviation. A comparison of our methods to the algorithms described by Seo et al. is included in Section 5.3.
 To further improve our adaptive sampling we shall ex-ploit Latin hypercube (LH) designs (Box et al., 1978). LHs systematically choose points that are spread out, taking on values throughout the region, but in differ-ent combinations across dim ensions, thereby obtaining nearly full coverage with fewer points than a full grid-ding. To create a LH (McKay et al., 1979) with n samples in a m X -dimensional space, one starts with an n m X grid over the search space. For each row in the first dimension of the grid, a row in each other di-mension is chosen randomly without replacement, so that exactly one sample point appears in each row for each dimension. Within these chosen grid cells, the actual sample point is typically chosen randomly. In one dimension a LH design is equivalent to a complete grid, but as the number of dimensions grows, the num-ber of points in the LH design stays constant, and the computational savings grow exponentially with m X . In this section we demonstrate an adaptive sampling scheme based on the Bayesian treed GP model of Section 3. Given N previous samples and their re-sponses we use the model and its predictive quan-tiles to select a new location at which to request a response. For all experiments herein this is accom-plished as follows: 15,000 MCMC rounds are per-formed, in which the parameters  X  |T are updated. Ev-ery fourth round we also update the tree structure ( T ) by drawing probabilistically from the discrete distribu-tion { 2 / 5 , 1 / 5 , 1 / 5 , 1 / 5 } , and attempting a grow, prune, swap } operation accordingly. The first 5,000 of the 15,000 rounds are treated as burn-in, af-ter which predictions are made using the parameters sampled during the remaining 10,000 rounds. Suppose that there are currently N locations x i for which we have a response t i . 2 An initial LH sample of size N 0 is used to get things started. At the beginning of the MCMC rounds we lay down a LH sample of N new locations on which to predict. Quantiles (95th and 5th) are computed at each of the N predictive loca-tions. Based on their differe nce, one is selected. Every third adaptive sample is chosen probabilistically, treat-ing the quantiles as a discrete distribution, while the rest are chosen by taking the maximum. Probabilistic samples are taken for robustness, as the maximum is only the optimal choice when the model is specified completely correctly. Finally, a response is elicited at the chosen input location, and the pair is then added into the data. The process is repeated, the model re-fit, and another adaptive sample is chosen from a new set of N LH samples. 5.1. Synthetic Data 1-d Sinusoidal dataset: Our first example is a simulated dataset on the input space [0 , 60]. The true response is (Higdon, 2002): t ( x )= sin where  X  is the step function defined by  X  ( x )=1if x&gt; 0and  X  ( x ) = 0 otherwise. Zero mean Gaussian noise with sd = 0 . 1 is added to the response. This dataset typifies the type of non-stationary response surface that our model was designed to exploit. Figure 1 shows the model after N 0 =20initialLH samples and 80 adaptive samples (with N = 200) were drawn iteratively (as outlined above). The top graphs show a snap-shot of the predictive mean and predic-tive 5% and 95% quantiles after the 80th adaptive sample, illustrating that of 80 adaptive samples (dots) only about 25% are in the flat region. The quantile-based error differences (shown in middle graph) are indeed lower in this region. These errors also point to high model uncertainty around the split-point be-tween the two regions, and consequently this part of the space has the highest concentration of adaptive samples. The bottom graph illustrates how the model adapts over time, showing that the mean standard er-ror (MSE) decreases steadily as samples are added, despite the fact that very few are added in the flat re-gion. Moreover, it shows that adaptive sampling yields significantly lower MSE than using the same Bayesian treed GP model with 100 and even 200 LH samples. This corresponds to a facto r of two decrease in the to-tal number of samples co mpared to LH sampling. As might be expected, there were on average two parti-tions constructed in each ro und with quite small vari-ance.
 Detailed analysis of the learned models clearly illus-trates another difference be tween sampling adaptively and using a LH design. The predictive surface based entirely on LH samples does not achieve the same ac-curacy (or resolution) as the surface which is based on the same number adaptive samples as it fails to discover the secondary structure contributed by the cosine term in (5). We also note that the stationary model (which does not partition the input space) fits the sinusoidal region about as well as the non-adaptive, non-stationary model, however the fit of the station-ary model in the flat region is poor because it assumes a homogeneous correlation structure.
 Our implementation in C/C++ using the ATLAS library running on an Intel Xeon at 2 GHz required 10 sec-onds for sample 21 (first adaptive) and 65 seconds for sample 100 (last). 2-d Exponential dataset: Next we show results for a two-dimensional input space in [  X  2 , 6]  X  [  X  2 , 6]. The true response is given by As before, a small amount of Gaussian noise (with sd = 0 . 001) is added. There are N 0 =20initialLHsamples and an additional 60 adaptively selected samples (from N =2 N ). Note that besides its dimensionality, a key difference between this data set and the last is that it is not defined using step functions; this smooth function does not have any artificial breaks between regions. Figure 2 shows results which are similar to those for the 1-d sinusoidal data set. After the 60th adaptive sample, the top-left graph shows the fitted response generated by the sample shown at the top-right ,most of which were, by design, drawn in [  X  2 , 2]  X  [  X  2 , 2] as the corresponding quantile based errors were largest here (not shown). The bottom graph compares adap-tive sampling with LH sampling by MSE. The two horizontal lines in this graph represent the MSE of Bayesian treed GP models fit on 80 and 160 LH sam-ples. Notice that the adaptive sampler beats out the LH design using half as many samples. Drawing the first and last adaptive samples took 10 and 45 seconds, respectively. 5.2. 3-d CFD data The third dataset is the motivating example for this work  X  the output from computational fluid dynamics simulations of a proposed reusable NASA launch vehi-cle called the Langley-Glide-Back Booster. The sim-ulations involved the integration of the inviscid Euler equations over a mesh of 1.4 million cells (0.8 million cells were used for the supersonic cases). Each run of the Euler solver for a given set of pa-rameters takes on the order of 5-20 hours on a high end workstation. Three input parameters were varied over (side slip angle, Mach number, and angle of at-tack) and for each setting of the input parameters six outputs were monitored. Using an interface to launch many jobs on many machines a total of around 3000 input configurations were tested. A more detailed de-scription of this system and its results can be found in (Rogers et al., 2003). The left side of Figure 3 shows one of the six outputs (lift) plotted as a function of speed (Mach) and angle of attack (alpha). The third input (side slip angle) is fixed at zero. Much of the space has a linear response, however it is highly non-linear near Mach one. We want to automatically sample points more frequently in this region. A fitted surfaced based upon 750 total samples, N 0 = 200 randomly selected initial subset for  X  X eppering X  and 550 chosen adaptively, is shown on the right side of Figure 3. Adaptively sampled con-figurations ( N = N random subsample) are shown in the top panel of Figure 4.
 Our Bayesian treed GP model has the desired behav-ior and focuses most of the adaptive sampling on the Mach 1 region. Visually, there is little difference be-tween the true surface (left) and the estimated surface (right) shown in Figure 3. However, using a Bayesian treed GP model with adaptive sampling requires fewer than 1 / 4 as many samples compared to a simple grid-ding, saving thousands of hours of computing time. The first and last adaptive sample took 30 seconds and 10 minutes, respectively, which is fast relative to one execution of the Euler solver. 5.3. A comparator As mentioned briefly in Section 4 our adaptive sam-pling scheme is motivated by an approach described by Seo et. al.. In their paper they outline two al-gorithms based on the predictive variances (4): ALM and ALC. ALM chooses the next adaptive sample by finding the input location  X  x  X   X  X which maximizes (4). ALC chooses  X  x to maximize the expected reduction in predictive variance at set of locations  X  X . Theoretically  X  X could be an open subset of R d , although in practice it is taken to be a finite lattice or regular grid, cho-sen apriori . Section 5 describes a sampling algorithm similar to ALM, however we note two key differences. The first is the aforementioned randomization of ev-ery third adaptive sample for model robustness. The second is that Seo et. al. assume knowledge of the cor-rect covariance structure ( and parameters) at the start of the experiment  X  a significant assumption that we do not make. Furthermore they do not update their model in light of new responses. Figure 5 demonstrates the advantage of adaptive sam-pling using Gaussian Pro cess trees by comparing the MSE of samples chosen from a regular grid to those of ALM and ALC. ALM and ALC have the luxury of not having to learn the correct model and so fare better for the first adaptive samples. However, since our model can partition the space and adapt its parameters to ac-count for the responses of sampled data, it eventually achieves lower error, with fewer total samples. By building a non-stationary surrogate model we can sample adaptively and thus select smaller designs with a corresponding savings in overall computing time and a reduction in predictive variance. We have seen that savings grows as the dimension of the problem grows, allowing us to tackle problems that may not be feasible using a traditional parameter sweep, or LH sampling. One of the next key steps is to be able systematically request more than one new sample at a time, in order to take full advantage of parallel computing resources. In a multiple-processor environment, simulator runs will finish at different times, and the main controller needs to be prepared with the next sampling point so that processors are not sitting idle. Additionally, other criteria for choosing the adaptive samples will be explored in the hopes of developing even more efficient sampling.
 This work was partially supported by research sub-award 08008-002-011-000 from the Universities Space Research Association and NASA. We would like to thank the area chair and three anonymous referees for their helpful suggestions.
 Box, G. E. P., Hunter, W. G., &amp; Hunter, J. S. (1978). Statistics for experimenters . New York: Wiley. Breiman, L., Friedman, J. H., Olshen, R., &amp; Stone, C. (1984). Classification and regression trees .Belmont, CA: Wadsworth.
 Chipman, H., George, E., &amp; McCulloch, R. (1998). Bayesian CART model search (with discussion).
Journal of the American Statistical Association , 93 , 935 X 960.
 Cohn, D. (1994). Neural network exploration using op-timal experim ental design. Advances in Neural In-formation Processing Systems (pp. 679 X 686). Mor-gan Kaufmann Publishers.
 Denison, D., Mallick, B., &amp; Smith, A. (1998). A
Bayesian CART algorithm. Biometrika , 85 , 363 X  377.
 Gelman, A., Carlin, J. B., Stern, H. S., &amp; Rubin, D. B. (1995). Bayesian data analysis . London: Chapman and Hall.
 Higdon, D. (2002). Space and space-time modeling us-ing process convolutions. Quantitative Methods for Current Environmental Issues (pp. 37 X 56). London: Springer-Verlag.
 Hjort, N. L., &amp; Omre, H. (1994). Topics in spatial statistics. Scandinavian Journal of Statistics , 21 , 289 X 357.
 Kim, H.-M., Mallick, B. K. , &amp; Holmes, C. C. (2002).
Analyzing non-stationary spatial data using piece-wise Gaussian processes (Technical Report). Texas A&amp;M University  X  Corpus Christi.
 Mackay, D. (1992). Information-based objective func-tions for active data selection. Neural Computation , 4 , 589 X 603.
 Matheron, G. (1963). Principles of geostatistics. Eco-nomic Geology , 58 , 1246 X 1266.
 McKay, M. D., Conover, W. J., &amp; Beckman, R. J. (1979). A comparison of three methods for selecting values of input variables in the analysis of output from a computer code. Technometrics , 21 , 239 X 245. O X  X agan, A., Kennedy, M. C., &amp; Oakley, J. E. (1999).
Uncertainty analysis and other inference tools for complex computer codes. Bayesian Statistics 6 (pp. 503 X 524). Oxford University Press.
 Rasmussen, C. E., &amp; Ghah ramani, Z. (2002). Infinite mixtures of Gaussian process experts. Advances in Neural Information Processing Systems . MIT Press. Richardson, S., &amp; Green, P. J. (1997). On Bayesian analysis of mixtures with an unknown number of components. Journal of the Royal Statistical Soci-ety, Series B, Methodological , 59 , 731 X 758. Rogers, S. E., Aftosmis, M. J., Pandya, S. A., N. M. Chaderjian, E. T. T., &amp; Ahmad, J. U. (2003).
Automated cfd parameter studies on distributed parallel computers. 16th AIAA Computational Fluid Dynamics Conference . AIAA Paper 2003-4229.
 Sacks, J., Welch, W. J., Mitchell, T. J., &amp; Wynn,
H. P. (1989). Design and analysis of computer ex-periments. Statistical Science , 4 , 409 X 435. Seo, S., Wallat, M., Graepel, T., &amp; Obermayer, K. (2000). Gaussian process regression: Active data selection and test point rejection. Proceedings of the International Joint Conference on Neural Networks IJCNN 2000 (pp. 241 X 246). IEEE.
 Tresp, V. (2001). Mixtures of gaussian processes. Ad-vances in Neural Information Processing Systems 13
