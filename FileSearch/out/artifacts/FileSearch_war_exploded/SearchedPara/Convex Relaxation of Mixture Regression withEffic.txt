 Regression is a foundational problem in machine learning and statistics. In practice, however, data is often better modeled by a mixture of regressors, as demonstrated by the prominence of mixture regression in a number of application areas. Gaffney and Smyth [1], for example, use mixture regres-in video sequences as a function of time. Each trajectory is believed to have been generated from one of a number of components, where each component is associated with a regression model. Finney et al. [2] have employed an identical mixture regression model in the context of planning: regression functions are strategies for a given planning problem. Elsewhere, the mixture of regressors model the training set used for modeling does not match the distribution of the test set in which the model will be used. Storkey and Sugiyama [3] model the covariate shift process in a mixture regression setting by assuming a shift in the mixing proportions of the components.
 functions whose values correspond to the mean of response variables, under the assumption that be easily tackled if it is known to which component each response variable belongs (yielding k independent regression problems). However in general the component of a given observation is not known and is modeled as a latent variable. A commonly adopted approach for maximum-likelihood estimation with latent variables (in this case, component membership for each response variable) is Expectation-Maximization (EM) [4]. Essentially, EM iterates inference over the hidden variables and parameter estimation of the resulting decoupled models until a local optimum is reached. We are not aware of any approach to maximum likelihood estimation of a mixture of regression models that is not based on the non-convex marginal likelihood objective of EM.
 In this paper we present a convex relaxation of maximum a posteriori estimation of a mixture of re-gression models. Recently, convex relaxations have gained considerable attention in machine learn-a semidefinite program. To achieve a scalable approach, however, we propose two reformulations that admit fast algorithms. The first is a max-min optimization problem which can be solved by iter-ations of quasi-Newton steps and eigenvector computations. The second is a min-min optimization problem solvable by iterations of closed-form solutions. We present experimental results comparing our methods against EM, both in synthetic problems and real computer vision problems, and show some benefits of a convex approach over a local solution method. Related work Goldfeld and Quandt [7] introduced a mixture regression model with two components called switching regressions . The problem is re-cast into a single composite regression equation by introducing a switching variable. A consistent estimator is then produced by a continuous relaxation of this switching variable. An EM algorithm for switching regressions was first presented by Hos-a minimum. A non-probabilistic algorithm similar to k -means was proposed. Subsequently, the general k -partition case employing EM was developed (c.f. [10, 11, 1]) and extended to various situations including the use of variable length trajectory data and to non-parametric regression mod-els. In the extreme, each individual could have its specific regression model but coupled at higher level with a mixture on regression parameters [12]. An EM algorithm is again employed to handle hidden data, in this case group membership of parameters. The Hierarchical Mixtures of Experts [13] model also shares some similarity to mixture regression in that gating networks which contain mixtures of generalized linear models are defined. In principle, our algorithmic advances can be applied to many of these formulations. Notation In the following we use the uppercase letters ( X,  X  ,  X ) to denote matrices and the low-ercase letters ( x,y,w, X , X ,c ) to denote vectors. We use t to denote the sample size, n to denote the dimensionality of the data and k to denote the number of mixture components.  X ( a ) denotes a of matrix A . Finally, we let 1 denote the vector of all ones, use to denote Hadamard (component-wise) matrix product, and use  X  to denote Kronecker product.
 We are given a matrix of regressors X  X  R t  X  n and a vector of regressands y  X  R t  X  1 where the re-sponse variable y is generated by a mixture of k components, but we do not know which component to denote the hidden assignment of mixture labels to each observation:  X  ij = 1 iff observation i for y i on a feature representation  X  i =  X  i  X  x i , under i.i.d. sampling where w  X  R ( n  X  k )  X  1 is the vector of stacked parameter vectors of the components. We therefore have the likelihood prior on w for capacity control. Also, one may want to constrain the size of the largest mixture component. For that purpose one could constrain the solutions  X  such that max( diag ( X  T  X ))  X   X t , proportion of the largest component). Combining these assumptions and adopting matrix notation we obtain the optimization problem: minimize the negative log-posterior of the entire sample Here  X  is the matrix whose rows are the vectors  X  i =  X  i  X  x i . Since X is observed, note that the optimization only runs over  X  in  X  . The constraint max( diag ( X  T  X ))  X   X t may also be added. Eliminating constant terms, our final task will be to solve Although marginally convex on w , this objective is not jointly convex on w and  X  (and involves non-convex constraints on  X  owing to its discreteness). The lack of joint convexity makes the opti-such as EM. Instead, in the following we develop a convex relaxation for problem (5). To obtain a convex relaxation we proceed in three steps. First, we dualize the first term in (5). and therefore A ( X  w ) = max c c T  X  w  X  1 2  X  2 c T c .
 therefore by definition of Fenchel duality A ( X  w ) = max c c T  X  w  X  1 2  X  2 c T c . A second Lemma is required to further establish the relaxation: Lemma 2 The following set inclusion holds and  X  1 = 1 together imply that  X  has a single 1 per row (and the rest are zeros). In particular diagonal matrix and therefore its diagonal elements are the eigenvalues of  X  X  T and in particular max( diag ( X  T  X ))  X   X t means that the largest possible eigenvalue of  X  X  T is  X t , which implies Therefore  X  X  T is also a member of the second set.
 The above two lemmas allow us to state our first main result below.
 Theorem 3 The following convex optimization problem is a relaxation of (5) only in the sense that domain (6) is replaced by domain (7). Proof We first use Lemma 1 in order to rewrite the objective (5) and obtain Second, using the distributivity of the (max , +) semi-ring, the max c can be pulled out and we then use Sion X  X  minimax theorem [14], which allows us to interchange max c with min w and we can solve for w first, obtaining Substituting (11) in the objective of (10) results in We now note the critical fact that  X  only shows up in the expression  X  X  T which, from the definition  X  i =  X  i  X  x i , is seen to be equivalent to  X  X  T XX T . Therefore the minimization over  X  effectively takes place over  X  X  T (since X is observed), and we have that (12) can be rewritten as Lemma 2, we obtain the claimed semidefinite relaxation. By upper bounding the inner maximization in (8) and applying a Schur complement, problem (8) can be re-expressed as a semidefinite program. Unfortunately, such a formulation is computationally relaxation. The basis of our development is the following classical result.
 Theorem 4 ([15]) Let V  X  R t  X  t ,V = V T have eigenvalues  X  1  X   X  2  X   X  X  X   X   X  t . Let P be the matrix whose columns are the normalized eigenvectors of V, i.e. P T V P =  X ((  X  1 ,..., X  t )) . Let q  X  X  1 ,...,t } and P q be the matrix comprised by the top q eigenvectors of P . Then Proof See [15] for a proof of a slightly more general result (Theorem 3 . 4 ).
 We will now show how the optimization on M for problem (8) can be cast in the terms of Theorem 4. This will turn out to be critical for the efficiency of the optimization procedure, since Theorem interior-point methods used for semidefinite programming ( O ( t 6 ) ).
 Proposition 5 Define  X  y := y  X  2 . The following optimization problem is equivalent to optimization problem (8).
 Proof By Sion X  X  minimax theorem [14], min M and max c in (8) can be interchanged which, by distributivity of the (min , +) semi-ring, is equivalent to Now, define K := XX T . The objective of the minimization in (18) can then be written as Finally, by writing min M  X  f ( M ) as  X  max M f ( M ) , we obtain the claim.
 We can now exploit the result in Theorem 4 for the purpose of our optimization problem. Proposition 6 Let q = { u : u = max { 1 ,...,t } ,u  X   X   X  1 } . The following optimization problem is equivalent to optimization problem (16). Algorithm 1 1: Input:  X  ,  X  ,  X  , XX T 2: Output: ( c  X  ,M  X  ) 3: Initialize c = 0 4: repeat 5: Solve for maximum value in inner maximization of (22) using (14) 6: Solve outer maximization in (22) using nonsmooth BFGS [16], obtain new c 7: until c has converged ( c = c  X  ) 8: At c  X  , solve for the maximizer(s) P q in the inner maximization of (22) using (15) 9: if P q is unique then 10: return M  X  = P q P T q break 11: else 12: Assemble top l eigenvectors in P l 13: Solve (24) 14: return M  X  = P l  X (  X   X  ) P T l 15: end if then I &lt;  X  M &lt; 0 since q  X   X   X  1 . We then have tr  X  M = q . The result follows. And finally we have the second main result Theorem 7 Optimization problem (22) is equivalent to optimization problem (8).
 Proof The equivalence follows directly from Propositions 5 and 6.
 (8), which demands O ( t 6 ) operations, we instead solve (22), which has as inner optimization a max eigenvalue problem, demanding only O ( t 3 ) operations. In the next section we describe an algorithm to jointly optimize for M and c in (22), which will essentially consist of alternating the efficient spectral solution over M with a subgradient optimization over c . 4.1 Max-Min Algorithm Algorithm 1 describes how we solve optimization problem (22). The idea of the algorithm is the pursue a fast subgradient ascent algorithm (e.g. such as nonsmooth BFGS [16]). So at each step we solve the eigenvalue problem and recompute a subgradient, until convergence to c  X  . We then need to recover M  X  such that ( c  X  ,M  X  ) is a saddle point (note that problem (22) is concave in c we are done and the labeling solution of mixture membership is M  X  (subject to roundoff). If P q is not unique, then we have multiplicity of eigenvalues and we need to proceed as follows. Define P eigenvalue which is equal to the eigenvalue of some of the previous q eigenvectors. We then have enforce that we are at the optimal c ( c  X  ), i.e. we must have Such condition can be pursued by minimizing the above norm, which gives a quadratic program optimizer of (24). The optimal value of (24) should be very close to zero (since it X  X  the norm of the derivative at point c  X  ). The pseudocode for the algorithm appears in Algorithm 1. Algorithm 2 1: Input:  X  ,  X  ,  X  , XX T 2: Output: ( c  X  ,M  X  ) 3: Initialize M =  X ((1 / (  X t ) ,..., 1 / (  X t ))) 4: repeat 5: Solve for minimum value in inner minimization of (25), obtain A 6: Solve outer minimization in (25) given SVD of A using Theorem 4.1 of [18], obtain new M 7: until M has converged ( M = M  X  ) ing [17, 18] has developed an alternate strategy for bypassing general semidefinite programming. Specifically, work in this area lead to convex optimization problems expressed jointly over two ma-trix variables where each step is an alternating min-min descent that can be executed in closed-form or by a very fast algorithm. Although it is not immediately apparent that this algorithmic strategy is applicable to the problem at hand, with some further reformulation of (8) we discover that in fact the same min-min algorithmic approach can be applied to our mixture of regression problem. Theorem 8 The following optimization problem is equivalent to optimization problem (8).
 Proof = min = min those two variables into (28) proves the claim. 5.1 Min-Min Algorithm The problem (25) is jointly convex in A and M [14] and Algorithm 2 describes how to solve it. It is important to note that although each iteration in Algorithm 2 is efficient, many iterations are experiments that the concave-convex max-min approach in Algorithm 1 is more efficient simply because it has the same iteration cost but exploits a quasi-Newton descent in the outer optimization, which converges faster.
 Remark 9 In practice, similarly to [17], a regularizer on M is added to avoid singularity, resulting in the following regularized objective function, The problem is still jointly convex in M and A . Our primary objective in formulating this convex approach to mixture regression is to tackle a dif-ficult problem in video analysis (see below). However, to initially evaluate the different approaches y assumed to be generated from a mixture of 5 components. We compared the quality of the relax-ation in (22) to EM. Max-min algorithm is used in this experiment. For EM, 100 random restarts was used to help avoid poor local optima. The experiment is repeated 10 times. The error rates are of the recovered membership for one of the runs is given in Figure 1. This demonstrates that the relaxation can retain much of the structure of the problem. 6.1 Vision Experiment In a dynamic scene, various static and moving objects are viewed by a possibly moving observer. For example, consider a moving, hand-held camera filming a scene of several cars driving down the road. Each car has a separate motion, and even the static objects, such as trees, appear to move in the video due to the self-motion of the camera. The task of segmenting each object according to its motion, estimating the parameters of each motion, and recovering the structure of the scene is known as the multibody structure and motion problem . This is a missing variable problem. If the motions have been segmented correctly, it is easy to estimate the parameters of each motion. Naturally, models employing EM have been proposed to tackle such problems (c.f. [19, 20]). From epipolar geometry, given a pair of corresponding points p i and q i from two images ( p i ,q i  X  R 3  X  1 ), we have the epipolar equation q T mation about the translation and rotation relative to the scene points between the positions of the camera where the two images were captured, as well as the camera calibration parameters such as its focal length. In a static scene, where only the camera is moving, there is only one fundamental matrix, which arises from the camera self-motion. However, if some of the scene points are moving independently under multiple different motions, there are several fundamental matrices. If there are k motion groups, the epipolar equation can be expressed in term of the multibody fundamental Generalized PCA [21]. An alternative approach, which we follow here, is by Li [22], who casts the variable  X  ij = 1 when image point i belongs to motion group j , and zero otherwise. Furthermore, recovered easily if the indicator variable  X  ij is known.
 We are interested in assessing the effectiveness of EM-based and convex relaxation-based methods for this multibody structure and motion problem. We used the Hopkins 155 dataset [23]. The exper-imental results are summarized in Table 1. All hyperparameters (EM:  X  and  X  ; Convex relaxation:  X  ,  X  , and  X  ) were tuned and the best performances for each learning algorithm are reported. The EM algorithm was run with 100 random restarts to help avoid poor local optima. In terms of com-putation time, the max-min runs comparably to the EM algorithm, while min-min runs in the order of 3 to 4 times slower. As an illustration, on a Pentium 4 3 . 6 GHz machine, the elapsed time (in seconds) for two cranes dataset is 16 . 880 , 23 . 536 , and 60 . 003 for EM, max-min and min-min, respectively. Rounding for the convex versions was done by k-means, which introduces some dif-ferences in the final results for both algorithms. Noticeably, both max-min and min-min outperform the EM algorithm. Visualizations of the motion segmentation on two cranes , three cars , and cars2 07 datasets are given in Figure 2 (for kanatani2 and articulated please refer to Appendix). The mixture regression problem is pervasive in many applications and known approaches for param-eter estimation rely on variants of EM, which naturally have issues with local minima. In this paper we introduced a semidefinite relaxation for the mixture regression problem, thus obtaining a con-vex formulation which does not suffer from local minima. In addition we showed how to avoid the use of expensive interior-point methods typically needed to solve semidefinite programs. This was achieved by introducing two reformulations amenable to the use of faster algorithms. Experimental results with synthetic data as well as with real computer vision data suggest the proposed methods can substantially improve on EM while one of the methods in addition has comparable runtimes. Figure 1: Recovered membership on synthetic data with EM and convex relaxation. 30 data points Figure 2: Resulting motion segmentations produced by the various techniques on the Hopkins 155 dataset. 2(a)-2(d): two cranes , 2(e)-2(h): three cars , and 2(i)-2(l): cars2 07 . In two cranes (first row), EM produces more segmentation errors at the left crane. In three cars (second row), the max-min method gives the least segmentation error (at the front side of the middle car) and EM produces more segmentation errors at the front side of the left car. The contrast of EM and convex methods is apparent for cars2 07 (third row): the convex methods segment correctly the static grass field object, while EM makes mistakes. Further, the min-min method can almost perfectly segment the car in the middle of the scene from the static tree background. [1] S. Gaffney and P. Smyth. Trajectory clustering with mixtures of regression models. In ACM [2] S. Finney, L. Kaelbling, and T. Lozano-Perez. Predicting partial paths from planning problem [4] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data [6] Y. Guo and D. Schuurmans. Convex relaxations for latent variable training. In Platt et al., [7] S. M. Goldfeld and R.E. Quandt. Nonlinear methods in econometrics. Amsterdam: North-[8] D. W. Hosmer. Maximum likelihood estimates of the parameters of a mixture of two regression [9] H. Sp  X  ath. Algorithm 39: clusterwise linear regression. Computing , 22:367 X 373, 1979. [10] W.S. DeSarbo and W.L. Cron. A maximum likelihood methodology for clusterwise linear [11] P.N. Jones and G.J. McLachlan. Fitting finite mixtures models in a regression context. Austral. [12] S. Gaffney and P. Smyth. Curve clustering with random effects regression mixtures. In AIS-[13] M.I. Jordan and R.A. Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural [14] S. Boyd and L. Vandenberghe. Convex Optimization . Cambridge University Press, 2004. [15] M. Overton and R. Womersley. Optimality conditions and duality theory for minimizing sums [16] J. Yu, S.V.N. Vishwanathan, S. G  X  unter, and N. Schraudolph. A quasi-Newton approach to [17] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learn-[18] J. Chen, L. Tang, J. Liu, and J. Ye. A convex formulation for learning shared structures from [19] N.Vasconcelos and A. Lippman. Empirical bayesian em-based motion segmentation. In CVPR , [20] P. Torr. Geometric motion segmentation and model selection. Philosophical Trans. of the [21] R. Vidal, Y. Ma, S. Soatto, and S. Sastry. Two-view multibody structure from motion. IJCV , [22] H. Li. Two-view motion segmentation from linear programming relaxation. In CVPR , 2007. [23] http://www.vision.jhu.edu/data/hopkins155/ .
