 We present the research, and product development and deployment, of Voice Analyzer X  by Jobaline Inc. This is a patent pending technology that analyzes voice data and predicts human Human voice characteristics, such as tone, complement the verbal communication. In several cont exts of communication,  X  X ow X  paper provides an overview of our deployed system, the raw data, the data processing steps, and the prediction algorithms we experimented with. A case study is included where, given a voice clip, our model predicts the degree in which a listener will find the voice  X  X ngaging X . Our prediction results were verified through independent market research with 75% in agreement on how an average listener would feel. One application of Jobaline Voice Analyzer technology is for assisti ng companies to hire workers in the service industry where cust omers X  emotional response to workers X  voice may affect the service outcome. Jobaline Voice Analyzer is deployed in production as a product offer to our clients to help them identify workers who will better engage with their customers. We will also share some discoveries and lessons learned. H.2.8 [ Database Management ]: Database Applications -data mining ; H.5.2 [ Information Interfaces and Presentation ]: User Interfaces -voice I/O ; I.5.2 [ Pattern Recognition ]: Design Methodology -pattern analysis . Algorithms, Design, Experime ntation, Human Factors. Voice Analysis, Predictive Modeling, Speech Signal Processing, Deployed System This paper presents Jobaline Vo ice Analyzer, a deployed system that analyzes human voice data and predicts listener emotions elicited by the paralinguistic elements of the voice [9]. The motivation for developing this technology at Jobaline is to automate the screening process of hiring workers for service industries. Jobaline is the lead ing mobile and bilingual hourly job matching marketplace and training network. Hourly job workers represent approximately two-thirds of the U.S. labor force and more than 50 million people are hired every year in this job category. The hiring of these hour ly job workers is typically done through a manual process conducted by human resource personnel to match workers with jobs. Many hourly jobs can not be filled in time even though employers spend nearly one billion hours pre-screening job applicants. To overcome this extreme low efficiency, Jobaline automates th e pre-screening and matching of applicants to jobs. The auto mation includes automated phone interviews whereby applicants record their answers to a set of interview prompts. At this writing, Jobaline has processed over 700,000 job applications in either English or Spanish with millions of voice clips recorded by job candidates of different cultures, education levels, genders and ages. We resort to the power of data science and predictive modeling to analyze these voice clips to further facilitate the automation of recruiting processes. If we can identify applicants whose voices can elicit positive emotions in a listener, thanks to certain characteristic s possessed by the voice, in real-time during the automated interview, our system can rank them higher in the job matching process. This in turn optimizes for best matching between workers and jobs. We have thus researched and developed a voice data analysis system and built data mining m odels that can predict human emotions elicited by paralinguistic elements of the voice interview records. Our system utilizes only the paralinguistic elements contents (what is being said). This has multiple benefits, including privacy preserving and fair hiring across cultural backgrounds. Our main contributions are: 1) the framing of the problem of predicting listener emotion respons e to voice by paralinguistic elements; 2) a set of system modules that process and extract features representing voice cues; and 3) a set of data mining models and analyses that derive insights and make predictions about listener emotions in response to voice clips. The prediction results were verified by independent market research, and the system modules and predicti on models are deployed into production. In particular, our prediction models quantify, for any given voice clip, the likelihood that a listener may associate certain emotions such as  X  X ngaging X . The utilization of only paralinguistic (i.e., non-lexical) elements enables us to predict listener response to  X  X ow things are said X  regardless of what is being said. One exemplary application of this technology is in the services industry, where hiring workers who can connect with their customers and keep their customers engaged is critically important. Some examples of these jobs include telemarketers, retail store clerks, frontline employees at a quick serve restaurant, or front desk associates at a hotel. By analyzing job applicants X  voices with Jobaline Voice Analyzer, companies can predict which job candidates will likely perform better in sales and marketing or customer-facing positions at restaurants, hotels, or call centers. This provides an objective input into recruiters X  hiring decision process. This paper is organized as follo ws: Section 2 provides some basic definitions and background knowle dge about voice data, and a high level overview of the state of the art of speech affect analysis. Section 3 presents our voice analyzer system: from the raw data, to feature extraction, to modeling, and deployment. Section 4 presents a case study where we built models which, given a voice clip, predict the likelihood of an emotion of  X  X eing engaged X  generated in the listener. We also present validation of our prediction results through third-party market research. In Section 5, we share some discoveries from analyzing job applicants X  voices and le ssons learned. And lastly, in Section 6, we conclude with business benef its and outline next steps in our research and development. For our goal of predicting emotion responses to voices, we envision a framework for research and development to encompass the below elements:  X  an  X  X motion taxonomy X  that includes all the emotion  X  a general understanding of voice data that could provide  X  a machine learning and data science experimentation Among the many emotion categories and definitions researched and employed by the research fields, we found the framework articulated by  X  X EELTRACE X  [2], screenshot in Figure 1, is most compelling for our purpose in the sense that it is more inclusive of all emotions we want to predict and it describes various emotions by finite quantifiable dimensions (see Figure 1). Whereas other research papers cover only a sma ll set of  X  X trong emotions X , such as  X  X ear X ,  X  X appy X , or  X  X ngry X . The finite dimensions such as mapping emotion characteristics to and measure them by known voice paralinguistic features. Combining emotion research with data science and engineering is a relatively new research and development area [10]. Our focus of predicting listener emotion respons e to voices would need to employ domain knowledge related to the processing and analysis of human voice data, as well as machine learning and data mining methodologies and techniques. Th e remainder of this section presents a brief introduction of affects associated with the human classifying emotions associated with voices (Section 2.2), and an overview of paralinguistic features and their use in existing works (Section 2.3). Attention to the affects of human voice probably existed as long as human existed. Systematic study of affects of human voice dated back centuries ago; Kr eiman et al [6] provides a comprehensive survey on the studies of perception of voice quality. Research and studies on the affects in and from human voice can be found in s ubject areas such as theories of emotions for Information Retrieval [8], Affective Computing (e.g., MIT Media Lab Affective Computing Group), and Speech Communication (e.g., International Speech Communication Association) [14, 16, 17]. Table 1 lists some attributes associated with human voice that have been the subjects of research. Table 1. Example Attributes Associated with Human Voice Categories of Attributes speaker trait characteristics that speaker state attributes of speaker speaker acoustic behavior acoustic affect elicited listener emotion There is a rich repertoire of scientific research in speech signal processing and analysis that make classifications of affects based on human voice data. The majority of these research works can be intrinsically possessed by the speaker. These personality traits can be independent of, or in relation to, when the speech was made. The other set of goals focuses on recognizing the presence of and classifying the types of emotions carried within a speech clip or the context out of which the speech clips arise. Table 2 lists some previous research works that aimed at classifying speaker traits or presence of emotions in speech. Table 2. Classification of Spe aker Traits or Speaker Emotions. Speaker Type
Parkinson X  X  disease patient 
Actor portray emotion presence of emotion in 
Consumer phone recording 
Medical call center dialog Jobaline Voice Analyzer is particularly focused on predicting the elicited emotions based on parali nguistic features of voice clips. We have not come across research works aimed at predicting/classifying listener emotions elicited by voices. The modeling of the anatomy and physiology of human voice production have been studied in the domain of speech signal processing with established paradi gm and methodologies [13]. We list some definitions related to analyzing paralinguistic features in Table 3. Converting audio signals into data features typically involves:  X  Sampling in time domain and frequency domain  X  Extracting FFT and energy in frequency domain and  X  Extracting feature vectors in the time and frequency domain Generally, frequency and energy variation over time are the major cues used to analyze emotions in audio samples. A number of methods, from mathematical transformations to slicing audio samples into shorter snippets, turn direct acoustic signals into pitch contours, from which statistics can be extracted and analyzed for correlations with emotions. Concept Definition Data amplitude measurement of the formants the resonance perceived pitch fundamental frequency Existing research on detecting speaker emotions from voice clips has demonstrated that speaker emo tions, and the intensity of those emotions, can be recognized from acoustic data with varying accuracy depending on the types of emotions. Pitch has been used as one major cue for emotion recognition. Intensity and speech rate have also been used. Typical transformations include turning a voice clip into pitch contours and measuring vari ous statistics such as max, min, standard devi ation, and time-window averages, on the whole clip or on snippets of the clip. Some features that have proven effective for recognizing speaker emotions include:  X  Fundamental frequency and its st atistics such as min, max,  X  Pitch contour  X  Speech signal amplitude  X  Frequency spectrum energy distribution  X  Durations: proportion of pauses, duration of syllables, In addition, some research also demonstrated associations between voice features and emo tions expressed in voices. For example, researchers found that the presence of anger in voice clips was associated with a rise in fundamental frequency and amplitude, whereas despondency was associated with a decreased syllable rate [5]. Researchers have experimented with novel acoustic features and have fo und that they outrank  X  X lassic X  features has also been demonstrated effective in assisting other features to further disambiguate affects. The list of features presented at INTERSPEECH competitions, which focused on recognizing speaker emotion and likability through paralinguistic features [ 15, 16] provided a starting point for our feature space construction for our voice analyzer. Intuitively, our common day experience tells us that predicting emotions that might be elicited when we hear a voice clip is a feasible task. For instance, when we hear a voice, we can tell whether the voice makes us feel  X  X eing engaged X ,  X  X t ease X , or  X  X oothed X . Figure 2 shows the spectrograms of two sample voice clips from job applicants responding to the interview prompt,  X  Greet me as if I am a customer  X . One can clearly notice the energy level difference between the two voice clips. The spectrogram on the top (Figure 2(a)) is from a voice clip that would make listeners feel much less engaging than the clips depicted on the bottom (Figure 2(b)). At Jobaline, we focused our stu dy on the types and instances of paralinguistic features and their e ffectiveness in classifying voice recordings. We will describe our system in steps and provide descriptions about the components of our system. Our task is to analyze and model speech affect based on paralinguistic correlates in acoustic data. The desired outcome is a system capable of predicting listeners X  emotion response to voice building blocks of our system are drawn in Figure 3. The resulting Jobaline Voice Analyzer accomplishes the tasks through the following steps: 1. Record and sample raw voice clips 2. Extract audio features that represent voice cues 3. Construct data feature spa ce suitable for applying data 4. Build models using various algorithms for unsupervised 5. Engineer scalable data pro cessing pipeline s that process Our raw data are voice clips from job applicants recorded as audio signals in wave format. Jobaline has served more than one-half million job applicants, where each job applicant may record voice clips as responses to various inte rview prompts. Typical interview prompts are:  X  Greet me as if I am your customer.  X  How would you describe excellent customer service?  X  Tell me about a time you handled an angry customer.  X  Tell me about a time when you were able to diffuse an  X  Briefly explain your experienc e on this type of job. The metadata associated with voice clips include job categories for which the applicants were ap plying and the interview prompts employers. The metadata were used to filter voice clips for building models according to groups of interview prompts. The content of metadata is not used for modeling because we wanted our models to be purely based on voice data. Our preprocessing tasks involves: 1. Converting audio signal into data in time domain and 2. Filtering out voice clips that are unfit for modeling and Our audio signal processing component transforms voice clips in wave format to the following data elements:  X  Short-term Fast Fourier Transform per frame  X  Energy measures in frequency domain per frame  X  Linear Prediction Coefficient in frequency domain per frame file. Our feature space for modeling is subsequently built based on these data elements (more in Section 3.4). Voice clips that are not suited for modeling or scoring are those that:  X  Are too short for any data elements to be meaningful  X  Contain too much background noise, leading/trailing noise, Since our voice clips are free-form speech recorded from job applicants, they do not have a uniform length when they arrive at our system. Figure 4 shows the distribution of voice clip length. We discard voice clips shorter than two seconds because they do not provide enough evidence of qu alifications for employers to screen for further information. We experimented with feature construction based on the following dimensions and combinations:  X  Signal measurements such as energy and amplitude  X  Statistics such as min, max, mean, and standard deviation on  X  Measurement window in time domain: different time size  X  Measurement window in frequency domain: all frequencies,  X  Distance metrics: dynamic time warping and Euclidean  X  Algorithms: hierarchical clustering, k-means and complete Combining our explorative analysis with learnings from existing researchers in speech signal processing and speaker emotion constructed feature space using various statisti cal measures of energy attributes. We analyzed feat ure selection against clustering algorithms to determine the effectiveness of the features and to guide us in the final selection of features. Our modeling goal is to be able to construct prediction models that, given a voice clip, predict li steners X  emotion response to it. In modeling listener emotional res ponse to voice c lips, we cannot expect the availability of the absolute ground truth as we have yet to have a mathematical formula tion of mappings from voice to emotion. We can jump start with the conventional approach of collecting training data through human labeling (labeled data), however we also note that the labelers X  sense of emotion may vary and thus could potentially intro duce hidden bias in the labeled data. When given a set of voice clips without prior knowledge of quantified ground truth or definitive verified examples that represent ground truth, we firs t utilize unsupervised learning techniques to help us find patte rns in unlabeled data. We then constructed training data sets based on evaluation of clustering results combined with independent manual labeling of individual voice clips. The labeled data are then fed to supervised learning routines to build the prediction models. We ran clustering analysis on the paralinguistic features of the voice clips (as listed in Secti on 3.4). We experimented with:  X  Various clustering algorithms such as hierarchical clustering  X  Various distance metrics employed within the clustering The clustering results were evaluated based on:  X  Cluster quality measurements such as compactness, good  X  Manual validation, including visual inspection of the clusters We ran cluster quality measurements on multiple clustering algorithms and multiple choices for the number of clusters to determine which clustering algorithms yield the best results and the number of clusters that are most appropriate for the data. Figure 5 gives an example of those results, which indicates that hierarchical clustering with five clusters gives the best clusters in this particular example. Figure 5. Cluster quality measurement by number of clusters. We visualized the clustering results by displaying the centroids of clusters by average energy in frequency domain across voice clips run. We listened to the voice clips that were clustered together to evaluate whether the voice clips from the same cluster did indeed generate similar listene r effect. Based on ou r listening test, we detected reasonable similarities within clusters and dissimilarities between clusters, and they also mapped to a scaled positive vs. negative response. For the example shown in Figure 6, we noted that two distinct clusters emerged: one for highly energetic voice clips and one for very low energy voice clips. This indicates that the voice clips and the corresponding extracted feature data have reasonable differentiating power to map the voice clips into clusters that might correlate with listener resp onses. We combine clustering results with human labeling to produce labeled data for training prediction models. In building our prediction models, we experimented with various supervised learning algorithms, including:  X  Logistic regression  X  Support Vector Machine  X  Random Forest where the training data is a combination of clustering results and human labeling. We also experi mented with models to predict binary outcome (positive vs. nega tive) and numerical scores for further classifying listener emotions. Our experiments show that the pr edictive models built on top of clustering insights and iterative feedback from listeners produces meaningful results. We will provide detailed accuracy analysis in Section 4 when we present our prediction model for emotion response of  X  X eeling engaged X . In addition to prediction models using the convention methods like SVM and Random Forest, we also conducted experiments with an unsupervised and semi-supervised learning algorithm, called Kodama, which performs f eature extraction from noisy and high-dimensional data [1]. The output of Kodama includes a dissimilarity matrix from which we can perform clustering and classification. In Figure 7, we project voice clips onto two-dimensional space from the multidimensional scaling of the Kodama dissimilarity matrix. Th e voice clips X  emotion response label is overlaid with color ( X  X  urquoise X  for positive response, and  X  X ed X  for negative response). We can see a fairly good separation of the two types of responses in tr aining data. We use experiments like this to validate the direction of research investigations, and to lead to the final selection of training data and corresponding features to be used in model training. The Jobaline Voice Analyzer system is a production system deployed in a cloud infrastructure. The functional components as described in Figure 3 are all de ployed to production: voice data arrive at our system after a job applicant records their answers to interview questions; a prediction score is produced by the system; and the result is stored in a data base for downstream consumption, such as ranking of applicants. The main architecture consists of: Hadoop clusters; audio signal preprocessing components developed in MATLAB and then converted to Java running on the clusters; various model training and prediction routines written in R. 
Figure 7. Overlay emotion label on Kodama dissimilarity. Figure 8. UI screen shots of how voice analysis is surfaced. Figure 8 provides two screen shots of Voice Analyzer results that are surfaced to the recruiter UI as in the deployed system. Figure 8(a) shows the UI where a recruiter as a Jobaline user can access the voice recordings left by the job applicants as answers to their interview questions. Once a voice clip went through our system, a notice and flag will show on the UI, as shown in lower right of elicit a positive response from cu stomers for the types of jobs the candidate is applying for. The blur in Figure 8(b) is added for the purpose of this paper to mask ou t the candidate X  X  personal data. At the time of this submission (February 2015), Jobaline had more than 700,000 job applicants on file, processed more than two million voice clips, and Jobaline Voice Analyzer was already available as product off ering to clients. Our work has been focused on the paralinguistic aspects of the voice data. We do not analyze the lexical contents of the voice clips. The result of Voice Analyzer is one additional data point to the recruiting process, not a hiring decision. The use of lexical content of the candidate X  X  answers to the interview questions in the recruiting process is left to the human recruiter. We present a case study for predicting listener emotion responses to voices for the purpose of assisting employers in screening job applicants. The business goal is to enable better matching of workers to jobs that require interaction with customers and keeping them engaged. Examples of such jobs are telemarketer, retail store clerk, frontline employee at a quick serve restaurant, or a front desk associate at a hotel. We incorporate the thinking fro m the emotion presentation framework by Cowei et al [2] (Figure 1). Instead of taking any specific categorization and definition of one particular emotion (such as  X  X appy X ) and building a m odel for it, we chose to start by predicting a positive response vs . negative response. A positive response could be one or multip le perceptions of a  X  X leasant voice X ,  X  X akes me feel good X ,  X  X ares about me X ,  X  X akes me feel comfortable X , or  X  X ak es me feel engaged X . While being novel in predicting listener emotion responses, we take the learnings from classify ing speaker traits and speaker states by previous research work s as guidance. In particular, the identifying the presence of and further classifying speaker emotions helps us decide what f eatures to use to construct our feature spaces. We used the methodologies and system components described in Section 3 to obtain the predictio n model and deployed the model to production. The production system was built and upgraded in two versions by the time of this writing. We experimented with SVM for version 1 and Random Forest for version 2. The collection of voice clips that answer the interview prompt of  X  Greet me as if I am a customer  X  are used for building the models combination of unsupervised lea rning and human manual labeling and inspection. We have achieved accuracy in the range of 76% to 90% as measured by cross-validation for binary classification (positive vs. negative on  X  X eeling engaged X ,  X  X  nergetic X , etc.). Here we report one model built with random forest algorithm, it has an AUC value of 0.918, the ROC for binary classification performance as shown in Figure 9, and some ot her performance measures as listed below: The downstream applications of th e prediction scores need to be based on a set of well thought out principles. As we iteratively build out our prediction engines for voice elicited emotions, our system is comprised of models built with different training datasets, different predictive modeling algorithms, and different tuning parameters. The prediction scores will have different meaning and implications. Figure 10 shows the spread of the predicted scores on 1000 randomly selected, unseen voice clips, by a support vector regression mo del (Figure 10(a)), and a random forest classification model (Fig ure 10(b)). The two sets of scores are directionally parallel, but have to be treated differently, as the model in Figure 10(a) predicts the engaging score and the model in Figure 10(b) produces the probab ility of positive response to a voice clip. Each voice clip is very rich in what it expresses; listener response to voices is complex depending on the listener X  X  context such as cultural and demographical factors. Thus, even within the same model, say the random forest model (Figure 10(b)), when applying the scores in downstream applications such as ranking, a more reasonable way to use the pre diction scores is to bucketize voice clips directionally, accordingly. As we will continue to improve on all aspects of our efforts, such as feature extraction, feature transformation, combination of training datasets, modeling algorithms, and their parameters, the absolute scored values will undergo changes until a global scaled normalization is found satisfactory. We can also observe important differences in model behavior from Figure 10. The SVM model in Figure 10(a) tends to be more biased towards predicting positive response than the random forest model. One reason for this behavior is that the training dataset was imbalanced due to our limited labeled training data at the earlier stage of our engine development. We moved our production system to the version 2 model that is random forest based with more balanced training dataset. (b) Scores from v2 model of random forest classification. 
Figure 10. Sorted p rediction scores by two models for 1000 To get third party opinion on our production models, we used market research with a represen tative U.S. demographic sample (by geographic region, income, ethnicity, gender and age) to manually validate the results. Jobaline commissioned two independe nt research firms to validate the output of our prediction models. This validation was done by surveying a representative sample of U.S. residents balanced by geographic region, income, ethnic ity, gender and age, comparing their subjective evaluation of voice clips with our prediction results. The survey takers matched the U.S. census in geography, age, ethnicity, income, and gender, at a 95% confidence interval and with a 5% margin of error. Surve y responses have to be triple verified in order to be included in calculating agreement between the survey taker and our prediction results. Figure 11 describes the verification process in detail. The market research concludes that, when asked how the sound of the recorded voice clips made them feel, 75% agreed with the Jobaline Voice Analyzer prediction of an interesting or engaging voice. Our market research data could also provide insights to feedback on our feature construction and modeling work. Some key takeaways are: 1. The demographic characteristics of the listener matter. For example, younger listeners (18-29 years old) or people in lower income brackets of less than $29K/year have more strict criteria of how they sense pleasant or engaging. The practical implication is that algorithms can be fine-tuned to the age range of the target audience the workers will speak to. Thus, a retailer that caters to a younger demographic might need individuals with voice characteristics different than a retailer that caters to an older demographic. 2. There is a significant drop in emotional response to voices of similar characteristics when the listener is exposed to voice clips longer than five seconds. More research has to be done to correlate this to various demographics, but this could affect things telemarketing or customer service firm, or a retailer based on the demographic they serve. 3. No positive or negative correlation was found between the emotion elicited and the age, ethnicity (accent), or education level of the speaker. 4. A slight bias of the respon dents toward female voices was noticed. Voices with similar characteristics, but from a female speaker ranked on average 11% better than voices from male speakers. This provides additional input for fine-tuning the algorithms; not based on gender, but on the unique attributes of the voice. Since the launch of Jobaline Voice Analyzer, we have received inquiries from media and interests in collaboration from academia. Here we want to share some of the discoveries we X  X e made while working with our voice data and lessons learned. It is an established knowledge that men and women have different fundamental frequencies in their voices. On average, the fundamental frequency for men is about 115 Hz and 220 Hz for women [8]. We selected one male voice and one female voice from our voice data set and show, in Figure 12, their average fundamental frequencies. The fundamental frequencies of the male voice (top graph) s how differences betw een two interview questions (red vs. blue), whereas the fundamental frequencies of the female voice (bottom graph) are well mixed within the same range for different interview questions. 
Figure 12. Fundamental freque ncies for one male voice (top Taking all job applicants on all interview questions, we have observed differences in voice charac teristics when job applicants answer different interview ques tions. Shown in Figure 13, the fundamental frequencies from voice clips on the question,  X  X reet me as if I am a customer X  (question 120 in Figure 13) are much higher than the fundamental frequencies on question,  X  X ell me about the last time you performed a similar job X  (question 218). Taking into consideration the differences in voice characteristics, we experimented with layering a decision tree by features related classification model. We are able to improve the classification accuracy by nearly 10%. The layered model will need to go through further market research validation as described in Section 4.3 before we put it into production. A voice paralinguistic feature ca n sometimes be critical for one classification task and be noise for another. For instance,  X  X ilence intervals X  can be a useful feature for predicting soothing response when the voice is intentional for keeping a steady rhythm, but it can also be noise for predicting engaging effect when they are indications of low energy in the voice. Dealing with voice signal often involves trade-offs between time domain and frequency domain. We have learned lessons to be mindful about using statistical features taken over one domain. For instance, when we use the maximum energy in the frequency domain over time as the main feature, it generally works well for overrate a voice clip that is very long in time. Therefore we need to take into account such limitations in the models/features when we use the scores from the model in a production environment where the voice clips were generated in free form. When we removed  X  X ilence intervals X  in our voice feature construction, it helped remove noise, but it also generated a boundary case where a mostly silent clip actually gets high engaging score. With Jobaline Voice Analyzer, employers will be able to identify service workers whose voices will better engage with their customers. Workers and employers will benefit from a shorter hiring cycle, thanks to the automated nature of the process. The deployed Jobaline Voice Analyzer system is starting to provide more quality input into our data science research and will enable more robust product features from Jobaline in the future. We have a number of exciting future projects that will explore the many rich features of the Jobaline voice database and tap into vast repositories of data science me thodologies and techniques. We have started work on building models for predicting emotion response of feeling  X  X almed X  when exposed to voices that the listener feel is soothing. Our curre nt findings indicate that we need to incorporate more paralinguistic features, such as cadence and emotion changes over time. Over the longer term as we deploy data science on more types of emotion responses, as depicted in Fi gure 1, we hope to synthesize and establish a set of research methodologies on paralinguistic voice features for modeling listener response for broader application areas such as human-machine interaction, opening the doors for creating artificial voices for machines and allowing for better communication by enabling an emotional machine-listener connection. [1] Cacciatore, S., Luchinat, C. , Tenori, L. 2014. Knowledge [2] Cowie, R., Douglas-Cowie, E., Savvidou, S. , McMahon, E., [3] Devillers, L., and Vidrascu, L. 2006. Real-life emotions [4] Fernandez, R. 2004. A Co mputational Model for the [5] Forsell, M. 2007. Acoustic Correl ates of Perceived Emotions [6] Kreiman, J., Van Lancker-Sidtis, D., and Gerratt, B.R., 2005. [7] Kreiman, J., Sidtis, D., 2011. Foundations of Voice Studies. [8] Lopatovska, I. and Arapakis, I. 2011. Theories, methods and [9] Mullor, M., Salazar, L., Li, Y., and Contreras, J. (Jobaline, [10] Picard, R.W. 2010. Emotion research by the people, for the [11] Polzehl, T., Moller, S., and Metze, F. 2010. Automatically [12] Polzin, T. S., and Waibel, A. 1998. Detecting emotions in [13] Quatier, T. F. 2002. Discrete-Time Speech Signal [14] Schuller, B., Steidl, S., Batliner, A., Burkhardt, F., Devillers, [15] Schuller, B. 2011. Voice and sp eech analysis in search of [16] Schuller, B., Steidl, S., Batliner, A., N X th, E., Vinciarelli, A., [17] Weiss, B. and Burkhardt, F. 2012. Is 'not bad' good enough? [18] Zhao, S., Rudzicz, F., Carvalho, L. G., M X rquez-Chin, C., 
