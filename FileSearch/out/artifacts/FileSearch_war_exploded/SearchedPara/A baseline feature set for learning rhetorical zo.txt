 At a time when experimental throughput in the field of molecular biology is increasing, it is necessary for biologists and people working in related fields to have access to so-phisticated tools to enable them to efficiently process large amounts of information in order to stay abreast of current research.
 Rhetorical zone analysis is an application of natural lan-guage processing in which areas of text in scientific papers are classified in terms of argumentation and intellectual con-tribution in order to pinpoint and distinguish certain types of information. Such analysis can be employed to assist in information extraction, helping to assess and integrate data generated by experiments into the scientific commu-nity X  X  store of knowledge.
 We present results for several experiments in automatic zone identification on the ZAISA-1 dataset, a new dataset com-posed of full biomedical research papers hand-annotated for rhetorical zones. We concentrate on general purpose and linguistically motivated features, and report results for a variety of sets of features. It is our intention to provide a baseline feature set for modeling, which can be extended in future work using combinations of heuristics and more sophisticated and task-specific modeling techniques. As increasing amounts of new data become available as the result of high throughput experiments in molecular biol-ogy, it becomes ever more important to have sophisticated methods for automatically extracting, assessing, and inte-grating new information from published papers into forms easily accessible to the scientific community. Information extraction (IE) is the task of mapping information from un-structured natural language texts such as journal articles to partially structured representations of meaning suitable for databases. This is now regarded as a fundamental technique for utilizing information contained in archived journal arti-cles and abstract collections such as MEDLINE, but there is much room for improvement. Major progress in natural language processing (NLP) approaches related to this task has been made by researchers in the extraction of biological named entities, i.e. the identification and classification of technical terms such as proteins, genes, drugs, or cell types, and biological interactions [5; 8; 20]), but further progress aimed at pin-pointing and organizing factual information remains a challenge [7].
 A step towards more sophisticated IE can be taken by anal-ysis and exploitation of rhetorical zones in a scientific text. The task of zone analysis (ZA) involves classification of areas of text, or zones in terms of argumentation and intellectual attribution. Teufel [22] proposed an analysis of text into rhetorical zones in a flat structure and provides an anno-tation scheme which is then applied to text summarization in the domain of computational linguistics [23; 24]. Mizuta and Collier [13; 15] propose the application of ZA to biology articles from an IE perspective. Other areas in which zone analysis may be of use are information retrieval (IR) and citation analysis[13; 17].
 In this paper, we present the results of machine learning experiments on zone analysis in the biomedical domain using the ZAISA-1 dataset annotated according to the version of the guidelines published in [14]. Pin-pointing and organizing factual information is an im-portant part of any IE task, but this consists of more than merely identifying factual statements within the texts. Con-textual information is often necessary to understand how to interpret the statements contained in sentences such that the pertinent factual content can be extracted. For exam-ple, a proposition in itself may refer to a current result or an old result, it may refer to a proposed theory or a disproven theory, it may refer to a description of the outcome of an experiment, or a description of the expected outcome. The distinctions between these rhetorical contexts are crucial for correctly interpreting the meaning of the research. Further-more, the necessary context information to interpret a fact may be subtle, and is likely to occur outside of the sentence in which the fact is found.
 In cases such as the following sentences, information which could be easily mistaken for results without knowledge of rhetorical zones is clearly intended as background ( BKG ) or problem ( PBM ) information, according to the zone an-notations. &lt; PBM &gt; Surprisingly, however, each of these mice is viable, fertile and fail to spontaneously give rise to tumors, an out-come predicted for a Myc antagonist. &lt; /PBM &gt; &lt; BKG &gt; However, incubation of these cells at intermedi-ate temperatures between restrictive and permissive temper-atures leads to flagella of intermediate lengths (Marshall and Rosenbaum, 2001). &lt; /BKG &gt; &lt; BKG &gt; However, the CAS/Cse1p binding site on human importin-maps to the convex face of Arm repeats 9 and 10 (Herold et al., 1998). &lt; /BKG &gt; Sentences and phrases which are ambiguous without proper recognition of their rhetorical context are frequently used. As pointed out in [19], sentences themselves contain propo-sitions, but texts are more than simple bags of sentences. Rather, contextual information is encoded in the coherence relations between sentences, and the facts contained in the sentences should be interpreted within the discourse. Hu-man annotation of rhetorical zones provides metatextual support for recognizing specific types of information and ex-ploiting discourse context.
 At present, a considerable amount of research in IE for biomedical texts has focused on abstracts [11; 10; 16; 8]. The target of biological information extraction should arguably be full texts, however, given their much richer sources of in-formation and the increasing ease of access in the form of online journals. Our approach to zone analysis aims to cap-italize on the richness of information available in full texts. The goal of zone analysis is to assign areas of text to (pos-sibly overlapping) rhetorical zone classes [23]. These zone classes vary in the nature of information they are intended to convey. The specific classes considered in this work fol-low those in [13; 15]. Zones are furthermore grouped into three separate groups, more broadly representing the kinds of information conveyed [14]. The set of zones is as follows: The first group includes zones whose information makes up the main content of the paper. The second group contains zones whose information compares or contrasts data or find-ings from the paper with other data or findings. The third group is composed a single class, which conveys meta-textual information about the paper and its organization. In the present work we concentrate on Group 1. The OWN class is exceptional in that zones are not classified directly as OWN but must be classified further as one of the subclasses of OWN . So for the task of classification we can ignore OWN itself. Furthermore we disregard ELS in the present experiments due to its rarity (only 6 sentences contain ELS annotations) in the current dataset.
 An important question from both a human annotation and a machine learning standpoint is what the basic units of clas-sification should be. According to the annotation guidelines in [14], the unit of annotation may be either a sentence or a phrasal constituent, with sentence-scope annotations being more common. In the current work, however, it is necessary to decide on a single discrete unit of classification, so we define our task as classification of sentences, and we do not analyze phrasal constituents as such. We consider a sentence to belong to a class if the sentence or any constituent within the sentence is annotated as belonging to the class. Thus, if a sentence contains a phrase which has been annotated as a RSL phrase, we consider the sentence to be a positive instance of the RSL class. In this sense, the classification approach taken here is coarser than the analysis specified in the annotation guidelines. In the current version of the guidelines, overlapping zones are allowed, so a sentence may belong to several sentence-scope zone classes in addition to containing constituent-scope zones.
 In most cases, accurately classifying sentences is enough to accurately annotate the zones of a text. However, in cases where sub-sentential phrases are highlighted by a human an-notator, a deeper level of analysis is required. In the present experiments, we take the approach of classifying sentences. In future work, this step may be used as a first pass, in cases where sub-sentential analysis is warranted. In this case a second pass aimed at classifying the sub-sentential constituents within multiply classified sentences, can be em-ployed. We classify sentences using binary classifiers, with a separate classifier representing each class. The classifiers for the vari-ous classes are independent of each other. We experimented with two well-known methods of classification, Naive Bayes and Support Vector Machines. Both classifiers are trained using labeled training examples in the ZAISA-1 dataset. Naive Bayes is a general purpose method of machine learn-ing widely used for tasks such as text classification. In this method, a simple statistical classifier is created based upon Bayes X  Law, notable for fast modeling and low computa-tional overhead. Its primary drawback is its assumption of statistical independence of the features used for modeling, making it less able to take advantage of the information available in complex feature sets containing inter-dependent features.
 John ate the apple subj john eat subj John ate the apple obj appole eat obj John wanted to eat v-ch eat want v-ch John sat in the park pcomp park in pcomp Figure 1: Simple examples of grammatical relations between words represented as dependency triples. SVMs are a machine learning classification technique which use a function called a kernel to map a space of data points in which the data is usually not expected to be linearly sep-arable onto a new space in which it is, with allowances for erroneous classification [25]. SVMs are known for high per-formance, particularly in tasks with large numbers of fea-tures. SVMs make no assumption of statistical indepen-dence, which makes them more suitable than Naive Bayes in cases where features may contain redundant, overlapping, or otherwise interdependent information, as is often the case in complex feature sets. For a tutorial on SVMs and details of their formulation we refer the reader to [4] and [6]. A detailed treatment of these models X  application to text clas-sification may be found in [9].
 We use Kudo X  X  TinySVM implementation for our experi-ments. 1 A polynomial kernel with the degree parameter set to 2 was used. A number of types of information are available to be in-corporated into the model. [22] gives an extensive list of possibilities for features directly applicable to the task of zone analysis. In the present work, we focus on establishing a baseline of modeling the data available to us, using a col-lection of general purpose, intuitive features. Since we have formulated the task as a classification task with sentences as its units of classification, we have experimented with tra-ditional text-classification features, including unigrams and bigrams of word tokens and lemmatized words, i.e. the mor-phologically uninflected form of the word. Rhetorical zones often strongly correspond with certain words and phrases, for example the phrase we found would probably suggest re-sults or insights. Although we do not use set phrases per se as features, it is expected that the word and lemma n-grams used should automatically incorporate such phrases into the model. We employ the Conexor FDG parser [21] for lemmatization and grammatical analysis and extract in-formation about the syntactic dependencies of each word, derived from the grammar. This information is encoded in the form of dependency triples which include the lemma form of the word in question, the lemma form of the word it is dependent on, and the syntactic relationship between the two words. Examples of dependency relations include verb-subject ( subj ), verb-object ( obj ), main verb-auxiliary verb ( v-ch ), preposition-complement ( pcomp ), and other lin-guistic relations. Figure 1 shows how these relations are represented as dependency triples.
 A standard approach when using word-based models in text classification and IR is to exclude stopwords , words judged in advance to be unlikely to yield useful information [18; 9]. These are usually common function words such as the www.tahoo.org/~taku/software/TinySVM determiners a or the , pronouns, and certain prepositions. We evaluate the usefulness of excluding a small list of high-frequency function words. The ZAISA-1 Dataset is composed of 20 full journal articles in the area of molecular biology; 5 articles from the Eu-ropean Molecular Biology Organization (EMBO) Journal, 5 articles from the Proceedings of the National Academy of Science (PNAS), 6 articles from the Nucleic Acid Re-search (NAR) Journal, and 4 articles from the Journal of Cell Biology (JCB). The articles have been hand annotated for rhetorical zone information by a linguist according to the guidelines published in [15].
 The full dataset consists of 3637 sentences. Counts of pos-itive examples by class may be viewed alongside the re-sults for machine learning experiments for the corresponding classes in Figure 3. A more detailed breakdown of the corre-spondence between classes and zones may be seen in Figure 2. In some experiments, only results sections are used. This subset of the data consists of 1727 sentences, 874 of which are positive examples of results. Two main sets of experiments were carried out, one on full articles, using all classes, and another on results sections only, using only the class RSL. Extracting information about new results is of primary importance in scientific IE and exploiting section knowledge allows us to focus attention to where those results are most likely to occur.
 The results in Figure 3 come from 10-fold cross validation, using 2 articles as a test data for each fold and training on the other 18. Although the unit of classification is sentences, the unseen data in actual applications would be full papers, and contextual information such as paper-specific unigrams and information from preceding and subsequent sentences plays an important role in the classification, so we are careful not to include sentences in the training data which come from the same paper as the sentence being classified. Feature sets for the reported experiments are broadly mo-tivated by the investigation in [13] and are presented here as collections of features representing lexical/syntactic in-formation, information about the main verb of the sentence, information about the location of the sentence in the text, and sequence information. Note also that in Figure 3 fea-tures are added cumulatively from left to right, and not taken away. The feature sets in the rightmost columns in-clude the features from the columns to the left. Percentage of words in each section by class In cases where a particular feature type has a large number of values, the most frequent 200 features only are considered for each such feature type. For example, a model using word unigrams would consist of 200 features, and a model using unigram and bigram features would consist of 400 features, 200 for each feature type. This is a common method of keeping the models within a reasonable size, and can also be helpful for modeling by limiting noise due to low-frequency features.
 In the experiments on full texts, features are extracted from the sentence being classified only, except in the  X  X one se-quence X  experiments, where a +1-1 window is employed; i.e., features are collected from the sentence being classified, as well as from the preceding and following sentences, yield-ing a weakly sequential model. Likewise, in the experiments on results sections the best models were produced using a +1-1 window. The results of the experiments on whole texts using all classes may be seen in Figure 3. Results of the experiments identifying only RSL classes in known results sections may be seen in Figure 5. Results of experiments specifically com-paring the benefits of location information derived from sec-tion headings to information derived from absolute location indices may be seen in Figure 4.
 On full articles, the highest overall f-score, 70, is obtained by the SVM model using the full feature set. Of the specific classes, recognition of MTH yields the highest f-score by a considerable margin at 87. In almost all cases, adding fea-tures yields some improvement in the SVM models, whereas increasing the complexity of the feature set does not benefit the Naive Bayes models as much. Location emerges as a key figure in many cases; the addition of location features yields the sharpest increase in overall f-scores for both mod-els. Contextual information derived from looking at features in a +1-1 window appears to be better exploited by the SVM model than by Naive Bayes.
 A comparison between two types of location information suggests that section information is considerably more help-ful than absolute location information but that the two types together yield a benefit in overall f-score which is somewhat greater than that gained by either of the two separately. The absolute location index feature yields no improvement in f-score over the model with all other non-location features, but the precision and recall values are somewhat more balanced. Section heading information yields improvement in precision and recall. Use of both features yields improvement in re-call. It would appear that the benefits of each information source are somewhat independent of each other, yielding a cumulative improvement when both feature types are in-cluded, although the contribution of the absolute location information is too small to be confident of.
 In the experiments on results sections only, where the goal was to identify those sentences belonging to RSL classes only, the highest f-score obtained is 80, incorporating a va-riety of lexical, syntactic, and location features, along with a +1-1 window. In general, it appears that some improvement may be gained by almost all the features employed. The SVM models in particular benefit from the larger feature sets. The rela-tive success of the Results sections-only experiments and the improvement gained by the location oriented features in the full text experiments confirm the intuition that actual location within the text is of key importance to the task of zone analysis.
 Results on n-gram style sequential models in [22] would ap-pear to give reason to doubt that sequential information can be usefully employed in this task, but as we see from the improvement gained by use of information from the pre-ceding and following sentences, local context does appear to Overall Prec/Rec 70/48 65/55 68/50 67/56 76/55 68/66 73/55 79/63 models for each zone class.
 texts. Overall precision, recall, and f-score results are shown. contribute to recognition. This also conforms to intuitions about the task. Sentence to sentence classification is not strongly sequential in the same way that word-to-word or letter-to-letter prediction tasks would be, which might ex-plain why a strongly sequential model would not optimally exploit the information available from local context. In the present experiments, the SVM model does a particularly good job of taking feature information from the local win-dow into consideration.
 In some cases where full sentences were mis-classified, the correct zone classification could have been identified with slightly deeper linguistic analysis, as in the following exam-ple: In contrast to loss of Mad proteins , Mnt deficiency was found to cause a phenotype remarkably similar to that caused by Myc overexpression and to predispose cells to tumorigen-esis in vivo. which was incorrectly classified by the learner as a result, when in fact the annotator identified the sentence as be-ing an insight. It can be surmised that the learner made the mistake because terms such as  X  X as found X  would be likely indicators for a results phrase. In fact,  X  X ause X  is a key here, since it expresses a biological process or property, but this clue is not salient enough in the unigram model to lead the learner to the correct classification. Allowing extra emphasis to be placed on words which are known in advance to be important to the task may help improve the model. In addition, recognizing the judgmental quality of the ad-verbial remarkably might have indicated to the learner that this sentence was an insight, rather than a strict result. A number of types of task-specific features employed by [22] were not explored in the present experiments, although many of them may have contributed implicitly through word or lemma n-gram features. There may be room for refine-ment of these n-gram feature sets. Particular phrases or words may benefit from being highlighted, and leaving some information out may also improve the models. The im-portance of adverbials is worth investigating, as suggested by the example given above, as is the possibility of creat-ing semantic classes to generalize over words with similar meanings or implications. Among the lexical features ex-perimented with, unigrams and bigrams contributed favor-ably to models. It remains an avenue worth investigating whether a more limited list of such rhetorical phrases would yield useful features. It also would be worth investigating methods for sub-selecting features in such a way as to max-imize the benefits of a given learning algorithm. It may be the case that Naive Bayes and SVMs are best exploited using different approaches to feature selection.
 The results for experiments using stopwords shown in Figure 6 suggest that, unlike the case in text classification and IR, the task of ZA benefits from the inclusion of function words. This is likely due to the fact that the task of ZA is connected with the rhetorical organization of a text, in which function words play a role, and furthermore the present analysis is concerned with classifying sentences, as opposed to texts. In the case of sentences, the presence of individual words, even function words, would be likely to have more influence on modeling than they would have on texts. The word the for example, occurs in virtually every English language text over a certain length, and is therefore best excluded from unigram models of text. Such words are not a given in every sentence, however, so it is intuitive that they may be able to make contributions to modeling on the sentence level. This paper reports the first results on zone analysis experi-ments in the biomedical domain, using the ZAISA-1 dataset. Several sets of experiments are carried out, including gen-eral zone analysis in full texts and specific recognition of re-sults zones within results sections. A variety of intuitive and general-purpose features are explored, yielding a foundation of results upon which to build in the future. The best per-forming models incorporated a mix of lexical and syntactic features and benefitted considerably by information about location within the text. Incorporating sequentiality into the model by means of a +1-1 window furthermore yielded marked improvement.
 Future work will involve further investigation of possible fea-ture sets, expanded to include a variety of task-specific and linguistically motivated features. In particular, specific em-phasis will be placed upon words and phrases deemed impor-tant to the classification task. Verbs and their modifiers will be given particular focus. Alternate modeling techniques, perhaps including employing multiple learners may also be a promising area of investigation. Research into sub-sentential analysis will also follow from the current work. [1] G.D. Bader, I. Donaldson, C. Wolting, B.F. Ouellette,
T. Pawson, C.W. Hogue. BIND-The Biommolecular Inter-action Network Database. Nucleic Acids Research , 29:242-245. 2001. [2] A. Bairoch, R. Apweiler. The SWISS-PROT protein se-quence database and its supplement TrEMBL in 200 Nu-cleic Acids Research , 28:302-303. 2000. [3] H.M. Berman, J. Westbrook, Z. Feng, G. Gilliland, T.N. Bhat, H. Weissig, I.N. Shindyalov and P.E. Bourne. The
Protein Data Bank/ Nucleic Acids Research , 28:235-242. 2000. [4] C. Burges. A tutorial on support vector machines for pat-tern recognition. Data Mining and Knowledge Discovery , 2(2):121 X 167, 1998. [5] M. Craven and J. Kumlien. Constructing biological knowledge bases by extracting information from text sources. ISMB X 99 , pp 77-86. 1999. [6] N. Cristianini and J. Shawe-Taylor. An Introduction to
Support Vector Machines and other Kernel-based Learn-ing Methods . Cambridge University Press, 2000. [7] S. Dickman. Tough mining; the challenges of searching the scientific literature. PLoS Biology , 1(2), pp 144-147. 2003. [8] K. Humphreys, G. Demetriou, and R. Gaizauskas. Two applications of information extraction to biological science journal articles: Enzyme interactions and protein struc-tures. In BSB2000 , pp 502-513. 2000. [9] T. Joachims. Learning to Classify Text Using Support Vector Machines . Kluwer Academic Publishers, 2001. [10] A. Koike, Y. Kobayashi, and T. Takagi. Kinase path-way database: an integrated protein-kinase and nlp-based protein-interaction resource. Genome Res , 17(6A):1231 X  1243, 2003. [11] A. Koike and T. Takagi. Prediction of protein-protein interaction sites using support vector machines. Protein
Engineering Design and Selection , 17(2):165 X 173, 2004. [12] L. Lo Conte, S.E. Brenner, T.J.P. Hubbard, C. Chothia,
A. Murzin. SCOP database in 2002: Refinements ac-commodate structural genomics. Nucleic Acids Research , 30:264-267. 2002. [13] Y. Mizuta and N. Collier. An annotation scheme for a rhetorical analysis of biology articles. In LREC2004 , pp. 1737-1740. 2004. [14] Y. Mizuta, T. Mullen and N. Collier. Annotation of
Biomedical Texts for Zone Analysis. NII Technical Report (NII-2004-007E,ISSN:1346-5597). Oct 2004. [15] Y. Mizuta, A. Korhonen, T. Mullen and N. Collier.
Zone analysis in biology articles as a basis for informa-tion extraction. In the Special Edition on Natural Lan-guage Processing in Biomedicine and Its Applications, In-ternational Journal of Medical Informatics . Elsevier. To appear. [16] S. Novichova, S. Egorov, and N. Darasalia. Medscan, a natural language processing engine for medline abstracts.
Bioinformatics , 19(13):1699-1706, 2003. [17] I. Tbahriti, C. Chichester, F Lisacek and P Ruch. Using
Argumentation to Retrieve Articles with Similar Citations from MEDLINE. JNLPBA , pp 8-14. 2004. [18] G. Salton and M. J. McGill. The SMART and SIRE Experimental Retrieval Systems. pp.118-155, New York:
McGraw-Hill. 1983. [19] H. Schauer and U. Hahn Phrases as carriers of coher-ence relations CogSci 2000 X  X roceedings of the 22nd An-nual Conference of the Cognitive Science Society , pp. 429-434. 2000. [20] L. Tanabe and W. Wilbur. Tagging gene and protein names in biomedical text. Bioinformatics , 18, pp 1124-1132. 2002. [21] P. Tapanainen and T. J  X arvinen. A non-projective de-pendency parser. In Proceedings of the 5th Conference on Applied Natural Language Processing, Washington D.C.,
Association of Computational Linguistics , pp 64-71. 1997. [22] S. Teufel. Arugmentative Zoning: Information Extrac-tion from Scientific Text PhD Thesis. University of Edin-burgh. 1999. [23] S. Teufel and M. Moens. Summarizing scientific arti-cles: Experiments with relevance and rhetorical status.
Computational Linguistics , 28(4):409-445, 2002. [24] S. Teufel and H. van Halteren. Agreement in hu-man factoid annotation for summarization evaluation. In
LREC2004 , 2004. [25] V.N. Vapnik. Statistical Learning Theory. Springer. 1998. [26] T. Wattarujeekrit, P. Shah and N. Collier PASBio:
Predicate-argument structures for event extraction in molecular biology. BMC Bioinformatics 5:155. 2004. [27] A. Zanzoni, L. Montecchi, M. Quondam G. Ausiello, M. Helmer-Citterich and G. Cesareni. MINT: A Molecular INTeraction database. FEBS Lett 513:135-140. 2002.
