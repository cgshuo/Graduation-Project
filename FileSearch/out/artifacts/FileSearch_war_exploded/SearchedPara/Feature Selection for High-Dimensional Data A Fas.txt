 Lei Yu leiyu@asu.edu Huan Liu hliu@asu.edu Feature selection is frequently used as a preprocessing step to machine learning. It is a process of choosing a subset of original features so that the feature space is optimally reduced according to a certain evaluation criterion. Feature selection has been a fertile field of research and development since 1970 X  X  and proven to be effective in removing irrelevant and redundant fea-tures, increasing efficiency in learning tasks, improv-ing learning performance like predictive accuracy, and enhancing comprehensibility of learned results (Blum &amp; Langley, 1997; Dash &amp; Liu, 1997; Kohavi &amp; John, 1997). In recent years, data has become increas-ingly larger in both number of instances and num-ber of features in many applications such as genome projects (Xing et al., 2001), text categorization (Yang &amp; Pederson, 1997), image retrieval (Rui et al., 1999), and customer relationship management (Ng &amp; Liu, 2000). This enormity may cause serious problems to many machine learning algorithms with respect to scalability and learning performance. For exam-ple, high dimensional data (i.e., data sets with hun-dreds or thousands of features) can contain high de-gree of irrelevant and redundant information which may greatly degrade the performance of learning al-gorithms. Therefore, feature selection becomes very necessary for machine learning tasks when facing high dimensional data nowadays. However, this trend of enormity on both size and dimensionality also poses severe challenges to feature selection algorithms. Some of the recent research efforts in feature selection have been focused on these challenges from handling a huge number of instances (Liu et al., 2002b) to dealing with high dimensional data (Das, 2001; Xing et al., 2001). This work is concerned about feature selection for high dimensional data. In the following, we first review models of feature selection and explain why a filter so-lution is suitable for high dimensional data, and then review some recent efforts in feature selection for high dimensional data.
 Feature selection algorithms fall into two broad cat-egories, the filter model or the wrapper model (Das, 2001; Kohavi &amp; John, 1997). The filter model relies on general characteristics of the training data to se-lect some features without involving any learning al-gorithm. The wrapper model requires one predeter-mined learning algorithm in feature selection and uses its performance to evaluate and determine which fea-tures are selected. As for each new subset of features, the wrapper model needs to learn a hypothesis (or a classifier). It tends to find features better suited to the predetermined learning algorithm resulting in superior learning performance, but it also tends to be more computationally expensive than the filter model (Lan-gley, 1994). When the number of features becomes very large, the filter model is usually chosen due to its computational efficiency.
 To combine the advantages of both models, algorithms in a hybrid model have recently been proposed to deal with high dimensional data (Das, 2001; Ng, 1998; Xing et al., 2001). In these algorithms, first, a goodness measure of feature subsets based on data characteris-tics is used to choose best subsets for a given cardinal-ity, and then, cross validation is exploited to decide a final best subset across different cardinalities. These algorithms mainly focus on combining filter and wrap-per algorithms to achieve best possible performance with a particular learning algorithm with similar time complexity of filter algorithms. In this work, we focus on the filter model and aim to develop a new feature selection algorithm which can effectively remove both irrelevant and redundant features and is less costly in computation than the currently available algorithms. In section 2, we review current algorithms within the filter model and point out their problems in the context of high dimensionality. In section 3, we describe corre-lation measures which form the base of our method in evaluating feature relevance and redundancy. In sec-tion 4, we first propose our method which selects good features for classification based on a novel concept, predominant correlation , and then present a fast algorithm with less than quadratic time complexity. In section 5, we evaluate the efficiency and effectiveness of this algorithm via extensive experiments on various real-world data sets comparing with other representa-tive feature selection algorithms, and discuss the im-plications of the findings. In section 6, we conclude our work with some possible extensions. Within the filter model, different feature selection al-gorithms can be further categorized into two groups, namely, feature weighting algorithms and subset search algorithms, based on whether they evaluate the goodness of features individually or through feature subsets. Below, we discuss the advantages and short-comings of representative algorithms in each group. Feature weighting algorithms assign weights to fea-tures individually and rank them based on their rel-evance to the target concept. There are a number of different definitions on feature relevance in machine learning literature (Blum &amp; Langley, 1997; Kohavi &amp; John, 1997). A feature is good and thus will be se-lected if its weight of relevance is greater than a thresh-old value. A well known algorithm that relies on rele-vance evaluation is Relief (Kira &amp; Rendell, 1992). The key idea of Relief is to estimate the relevance of fea-tures according to how well their values distinguish be-tween the instances of the same and different classes that are near each other. Relief randomly samples a number ( m ) of instances from the training set and up-dates the relevance estimation of each feature based on the difference between the selected instance and the two nearest instances of the same and opposite classes. Time complexity of Relief for a data set with M instances and N features is O ( mMN ). With m be-ing a constant, the time complexity becomes O ( MN ), which makes it very scalable to data sets with both a huge number of instances and a very high dimension-ality. However, Relief does not help with removing redundant features. As long as features are deemed relevant to the class concept, they will all be selected even though many of them are highly correlated to each other (Kira &amp; Rendell, 1992). Many other algo-rithms in this group have similar problems as Relief does. They can only capture the relevance of features to the target concept, but cannot discover redundancy among features. However, empirical evidence from fea-ture selection literature shows that, along with irrele-vant features, redundant features also affect the speed and accuracy of learning algorithms and thus should be eliminated as well (Hall, 2000; Kohavi &amp; John, 1997). Therefore, in the context of feature selection for high dimensional data where there may exist many redun-dant features, pure relevance-based feature weighting algorithms do not meet the need of feature selection very well.
 Subset search algorithms search through candidate feature subsets guided by a certain evaluation mea-sure (Liu &amp; Motoda, 1998) which captures the good-ness of each subset. An optimal (or near optimal) sub-set is selected when the search stops. Some existing evaluation measures that have been shown effective in removing both irrelevant and redundant features in-clude the consistency measure (Dash et al., 2000) and the correlation measure (Hall, 1999; Hall, 2000). Con-sistency measure attempts to find a minimum num-ber of features that separate classes as consistently as the full set of features can. An inconsistency is de-fined as two instances having the same feature values but different class labels. In Dash et al. (2000), dif-ferent search strategies, namely, exhaustive, heuristic, and random search, are combined with this evalua-tion measure to form different algorithms. The time complexity is exponential in terms of data dimension-ality for exhaustive search and quadratic for heuristic search. The complexity can be linear to the number of iterations in a random search, but experiments show that in order to find best feature subset, the number of iterations required is mostly at least quadratic to the number of features (Dash et al., 2000). In Hall (2000), a correlation measure is applied to evaluate the good-ness of feature subsets based on the hypothesis that a good feature subset is one that contains features highly correlated to the class, yet uncorrelated to each other. The underlying algorithm, named CFS, also exploits heuristic search. Therefore, with quadratic or higher time complexity in terms of dimensionality, existing subset search algorithms do not have strong scalabil-ity to deal with high dimensional data.
 To overcome the problems of algorithms in both groups and meet the demand for feature selection for high dimensional data, we develop a novel algorithm which can effectively identify both irrelevant and redundant features with less time complexity than subset search algorithms. In this section, we discuss how to evaluate the good-ness of features for classification. In general, a fea-ture is good if it is relevant to the class concept but is not redundant to any of the other relevant features. If we adopt the correlation between two variables as a goodness measure, the above definition becomes that a feature is good if it is highly correlated to the class but not highly correlated to any of the other features. In other words, if the correlation between a feature and the class is high enough to make it relevant to (or predictive of) the class and the correlation between it and any other relevant features does not reach a level so that it can be predicted by any of the other relevant features, it will be regarded as a good feature for the classification task. In this sense, the problem of fea-ture selection boils down to find a suitable measure of correlations between features and a sound procedure to select features based on this measure.
 There exist broadly two approaches to measure the correlation between two random variables. One is based on classical linear correlation and the other is based on information theory. Under the first approach, the most well known measure is linear correlation co-efficient . For a pair of variables ( X, Y ), the linear correlation coefficient r is given by the formula where x i is the mean of X , and y i is the mean of Y . The value of r lies between -1 and 1, inclusive. If X and Y are completely correlated, r takes the value of 1 or -1; if X and Y are totally independent, r is zero. It is a symmetrical measure for two variables. Other measures in this category are basically variations of the above formula, such as least square regression er-ror and maximal information compression index (Mi-tra et al., 2002). There are several benefits of choos-ing linear correlation as a feature goodness measure for classification. First, it helps remove features with near zero linear correlation to the class. Second, it helps to reduce redundancy among selected features. It is known that if data is linearly separable in the original representation, it is still linearly separable if all but one of a group of linearly dependent features are removed (Das, 1971). However, it is not safe to always assume linear correlation between features in the real world. Linear correlation measures may not be able to capture correlations that are not linear in nature. Another limitation is that the calculation requires all features contain numerical values.
 To overcome these shortcomings, in our solution we adopt the other approach and choose a correlation measure based on the information-theoretical concept of entropy , a measure of the uncertainty of a random variable. The entropy of a variable X is defined as and the entropy of X after observing values of another variable Y is defined as H ( X | Y ) =  X  where P ( x i ) is the prior probabilities for all values of X , and P ( x i | y i ) is the posterior probabilities of X given the values of Y . The amount by which the en-tropy of X decreases reflects additional information about X provided by Y and is called information gain (Quinlan, 1993), given by According to this measure, a feature Y is regarded more correlated to feature X than to feature Z , if IG ( X | Y ) &gt; IG ( Z | Y ). About information gain mea-sure, we have the following theorem.
 Theorem Information gain is symmetrical for two random variables X and Y.
 Proof Sketch: To prove IG ( X | Y ) = IG ( Y | X ), we need to prove H ( X )  X  H ( X | Y ) = H ( Y )  X  H ( Y | X ). This can be easily derived from H ( X, Y ) = H ( X ) + H ( Y | X ) = H ( Y ) + H ( X | Y ).  X  Symmetry is a desired property for a measure of cor-relations between features. However, information gain is biased in favor of features with more values. Fur-thermore, the values have to be normalized to ensure they are comparable and have the same affect. There-fore, we choose symmetrical uncertainty (Press et al., 1988), defined as follows.
 It compensates for information gain X  X  bias toward fea-tures with more values and normalizes its values to the range [0 , 1] with the value 1 indicating that knowl-edge of the value of either one completely predicts the value of the other and the value 0 indicating that X and Y are independent. In addition, it still treats a pair of features symmetrically. Entropy-based mea-sures require nominal features, but they can be applied to measure correlations between continuous features as well, if the values are discretized properly in ad-vance (Fayyad &amp; Irani, 1993; Liu et al., 2002a). There-fore, we use symmetrical uncertainty in this work. 4.1. Methodology Using symmetrical uncertainty ( SU ) as the goodness measure, we are now ready to develop a procedure to select good features for classification based on corre-lation analysis of features (including the class). This involves two aspects: (1) how to decide whether a fea-ture is relevant to the class or not; and (2) how to decide whether such a relevant feature is redundant or not when considering it with other relevant features. The answer to the first question can be using a user-defined threshold SU value, as the method used by many other feature weighting algorithms (e.g., Relief). More specifically, suppose a data set S contains N features and a class C . Let SU i,c denote the SU value that measures the correlation between a feature F i and the class C (named C -correlation), then a subset S 0 of relevant features can be decided by a threshold SU value  X  , such that  X  F i  X  S 0 , 1  X  i  X  N , SU i,c  X   X  . The answer to the second question is more complicated because it may involve analysis of pairwise correlations between all features (named F -correlation), which re-sults in a time complexity of O ( N 2 ) associated with the number of features N for most existing algorithms. To solve this problem, we propose our method below. Since F -correlations are also captured by SU values, in order to decide whether a relevant feature is redun-dant or not, we need to find a reasonable way to decide the threshold level for F -correlations as well. In other words, we need to decide whether the level of correla-tion between two features in S 0 is high enough to cause redundancy so that one of them may be removed from S 0 . For a feature F i in S 0 , the value of SU i,c quantifies the extent to which F i is correlated to (or predictive of) the class C . If we examine the value of SU j,i for  X  F j  X  S 0 ( j 6 = i ), we will also obtain quantified estima-tions about the extent to which F i is correlated to (or predicted by) the rest relevant features in S 0 . There-fore, it is possible to identify highly correlated features to F i in the same straightforward manner as we decide S 0 , using a threshold SU value equal or similar to  X  . We can do this for all features in S 0 . However, this method only sounds reasonable when we try to deter-mine highly correlated features to one concept while not considering another concept. In the context of a set of relevant features S 0 already identified for the class concept, when we try to determine the highly correlated features for a given feature F i within S 0 , it is more reasonable to use the C -correlation level be-tween F i and the class concept, SU i,c , as a reference. The reason lies on the common phenomenon -a fea-ture that is correlated to one concept (e.g., the class) at a certain level may also be correlated to some other concepts (features) at the same or an even higher level. Therefore, even the correlation between this feature and the class concept is larger than some threshold  X  and thereof making this feature relevant to the class concept, this correlation is by no means predominant. To be more precise, we define the concept of predom-inant correlation as follows.
 Definition 1 (Predominant Correlation). The corre-lation between a feature F i ( F i  X  S ) and the class C is predominant iff SU i,c  X   X  , and  X  F j  X  S 0 ( j 6 = i ), there exists no F j such that SU j,i  X  SU i,c .
 If there exists such F j to a feature F i , we call it a redundant peer to F i and use S P of all redundant peers for F i . Given F i  X  S 0 and S P ( S P i 6 =  X  ), we divide S P i into two parts, S P i + and S S Definition 2 (Predominant Feature). A feature is predominant to the class, iff its correlation to the class is predominant or can become predominant after re-moving its redundant peers.
 According to the above definitions, a feature is good if it is predominant in predicting the class concept, and feature selection for classification is a process that identifies all predominant features to the class concept and removes the rest. We now propose three heuristics that together can effectively identify predominant fea-tures and remove redundant ones among all relevant features, without having to identify all the redundant peers for every feature in S 0 , and thus avoids pairwise analysis of F -correlations between all relevant features. Our assumption in developing these heuristics is that if two features are found to be redundant to each other and one of them needs to be removed, removing the one that is less relevant to the class concept keeps more information to predict the class while reducing redun-dancy in the data.
 Heuristic 1 ( if S P inant feature, remove all features in S P identifying redundant peers for them.
 Heuristic 2 ( if S P S becomes predominant, follow Heuristic 1; otherwise only remove F i and decide whether or not to remove features in S P Heuristic 3 ( starting point ). The feature with the largest SU i,c value is always a predominant feature and can be a starting point to remove other features. 4.2. Algorithm and Analysis Based on the methodology presented before, we de-velop an algorithm, named FCBF (Fast Correlation-Based Filter). As in Figure 1, given a data set with input: S ( F 1 , F 2 , ..., F N , C ) // a training data set output: S best // an optimal subset 1 begin 2 for i = 1 to N do begin 3 calculate SU i,c for F i ; 4 if ( SU i,c  X   X  ) 6 end; 7 order S 0 list in descending SU i,c value; 8 F p = getFirstElement ( S 0 list ); 9 do begin 10 F q = getNextElement ( S 0 list , F p ); 11 if ( F q &lt;&gt; NULL) 12 do begin 14 if ( SU p,q  X  SU q,c ) 15 remove F q from S 0 list ; 17 else F q = getNextElement ( S 0 list , F q ); 18 end until ( F q == NULL); 19 F p = getNextElement ( S 0 list , F p ); 20 end until ( F p == NULL); 22 end ; N features and a class C , the algorithm finds a set of predominant features S best for the class concept. It consists of two major parts. In the first part (line 2-7), it calculates the SU value for each feature, selects rele-vant features into S 0 list based on the predefined thresh-old  X  , and orders them in descending order according to their SU values. In the second part (line 8-20), redundant features and only keeps predominant ones among all the selected relevant features. According to Heuristic 1, a feature F p that has already been deter-mined to be a predominant feature can always be used to filter out other features that are ranked lower than F p and have F p as one of its redundant peers. The iteration starts from the first element (Heuristic 3) in list (line 8) and continues as follows. For all the re-maining features (from the one right next to F p to the last one in S 0 list ), if F p happens to be a redundant peer to a feature F q , F q will be removed from S 0 list (Heuris-tic 2). After one round of filtering features based on F , the algorithm will take the currently remaining fea-ture right next to F p as the new reference (line 19) to repeat the filtering process. The algorithm stops until The first part of the above algorithm has a linear time complexity in terms of the number of features N . As to the second part, in each iteration, using the pre-dominant feature F p identified in the previous round, FCBF can remove a large number of features that are redundant peers to F p in the current iteration. The best case could be that all of the remaining features following F p in the ranked list will be removed; the worst case could be none of them. On average, we can assume that half of the remaining features will be removed in each iteration. Therefore, the time com-plexity for the second part is O ( N log N ) in terms of N . Since the calculation of SU for a pair of features is linear in term of the number of instances M in a data set, the overall complexity of FCBF is O ( MN log N ). The objective of this section is to evaluate our pro-posed algorithm in terms of speed, number of selected features, and learning accuracy on selected features. 5.1. Experiment Setup In our experiments, we choose three representative fea-ture selection algorithms in comparison with FCBF. One is a feature weighting algorithm, ReliefF (an ex-tension to Relief) which searches for several nearest neighbors to be robust to noise and handles multi-ple classes (Kononenko, 1994); the other two are sub-set search algorithms which exploit sequential forward search and utilize correlation measure or consistency measure to guide the search, denoted as CorrSF and ConsSF respectively. CorrSF is a variation of the CFS algorithm mentioned in section 2. The reason why we prefer CorrSF to CFS is because both ex-periments in Hall (1999) and our initial experiments show that CFS only produces slightly better results than CorrSF, but CorrSF based on sequential forward search runs faster than CFS based on best first search with 5 nodes expansion and therefore is more suitable for high dimensional data. In addition to feature se-lection algorithms, we also select two different learning algorithms, C4.5 (Quinlan, 1993) and NBC (Witten &amp; Frank, 2000), to evaluate the accuracy on selected fea-tures for each feature selection algorithm.
 The experiments are conducted using Weka X  X  imple-mentation of all these existing algorithms and FCBF is also implemented in Weka environment (Witten &amp; Frank, 2000). All together 10 data sets are selected from the UCI Machine Learning Repository (Blake &amp; Merz, 1998) and the UCI KDD Archive (Bay, 1999). A summary of data sets is presented in Table 1. For each data set, we run all four feature selection algorithms, FCBF, ReliefF, CorrSF, ConsSF, respec-tively, and record the running time and the number of selected features for each algorithm. We then ap-ply C4.5 and NBC on the original data set as well as each newly obtained data set containing only the se-lected features from each algorithm and record overall accuracy by 10-fold cross-validation. 5.2. Results and Discussions Table 2 records the running time and the number of selected features for each feature selection algorithm. For ReliefF, the parameter k is set to 5 (neighbors) and m is set to 30 (instances) throughout the experiments. From Table 2, we can observe that for each algorithm the running times over different data sets are consis-tent with our previous time complexity analysis. From the averaged values in the last row of Table 2, it is clear that FCBF runs significantly faster (in degrees) than the other three algorithms, which verifies FCBF X  X  su-perior computational efficiency. What is interesting is that ReliefF is unexpectedly slow even though its time complexity becomes O ( MN ) with a fixed sample size m . The reason lies on that searching for nearest neighbors involves distance calculation which is more time consuming than the calculation of symmetrical uncertainty values.
 From Table 2, it is also clear that FCBF achieves the highest level of dimensionality reduction by selecting the least number of features (with only one exception in USCensus90), which is consistent with our theoret-ical analysis about FCBF X  X  ability to identify redun-dant features.
 Tables 3 and 4 show the learning accuracy of C4.5 and NBC respectively on different feature sets. From the averaged accuracy over all data sets, we observe that, in general, (1) FCBF improves the accuracy of both C4.5 and NBC; and (2) of the other three algorithms, only CorrSF can enhance the accuracy of C4.5 to the same level as FCBF does. From individual accuracy values, we also observe that for most of the data sets, FCBF can maintain or even increase the accuracy. The above experimental results suggest that FCBF is practical for feature selection for classification of high dimensional data. It can efficiently achieve high degree of dimensionality reduction and enhance classification accuracy with predominant features. In this paper, we propose a novel concept of predomi-nant correlation, introduce an efficient way of analyz-ing feature redundancy, and design a fast correlation-based filter approach. A new feature selection algo-rithm FCBF is implemented and evaluated through extensive experiments comparing with related feature selection algorithms. The feature selection results are further verified by applying two different classification algorithms to data with and without feature selec-tion. Our approach demonstrates its efficiency and effectiveness in dealing with high dimensional data for classification. Our further work will extend FCBF to work on data with higher dimensionality (thousands of features). We will study in more detail redun-dant features and their role in classification, and com-bine FCBF with feature discretization algorithms to smoothly handle data of different feature types. We gratefully thank anonymous reviewers and Area Chair for their constructive comments. This work is in part supported by grants from NSF (No. 0127815, 0231448), Prop 301 (No. ECR A601), Ford and IMES at ASU for H. Liu.
 Bay, S. D. (1999). The UCI KDD Archive. http://kdd.ics.uci.edu.
 Blake, C., &amp; Merz, C. (1998). UCI repos-itory of machine learning databases. http://www.ics.uci.edu/  X  mlearn/MLRepository.html. Blum, A., &amp; Langley, P. (1997). Selection of relevant features and examples in machine learning. Artificial Intelligence , 97 , 245 X 271.
 Das, S. (2001). Filters, wrappers and a boosting-based hybrid for feature selection. Proceedings of the Eigh-teenth International Conference on Machine Learn-ing (pp. 74 X 81).
 Das, S. K. (1971). Feature selection with a linear de-pendence measure. IEEE Transactions on Comput-ers .
 Dash, M., &amp; Liu, H. (1997). Feature selection for clas-sifications. Intelligent Data Analysis: An Interna-tional Journal , 1 , 131 X 156.
 Dash, M., Liu, H., &amp; Motoda, H. (2000). Consistency based feature selection. Proceedings of the Fourth
Pacific Asia Conference on Knowledge Discovery and Data Mining (pp. 98 X 109). Springer-Verlag. Fayyad, U., &amp; Irani, K. (1993). Multi-interval dis-cretization of continuous-valued attributes for clas-sification learning. Proceedings of the Thirteenth
International Joint Conference on Artificial Intel-ligence (pp. 1022 X 1027). Morgan Kaufmann.
 Hall, M. (1999). Correlation based feature selection for machine learning . Doctoral dissertation, University of Waikato, Dept. of Computer Science.
 Hall, M. (2000). Correlation-based feature selection for discrete and numeric class machine learning. Pro-ceedings of the Seventeenth International Confer-ence on Machine Learning (pp. 359 X 366).
 Kira, K., &amp; Rendell, L. (1992). The feature selection problem: Traditional methods and a new algorithm. Proceedings of the Tenth National Conference on Artificial Intelligence (pp. 129 X 134). Menlo Park: AAAI Press/The MIT Press.
 Kohavi, R., &amp; John, G. (1997). Wrappers for feature subset selection. Artificial Intelligence , 97 , 273 X 324. Kononenko, I. (1994). Estimating attributes : Analy-sis and extension of RELIEF. Proceedings of the Eu-ropean Conference on Machine Learning (pp. 171 X  182). Catania, Italy: Berlin: Springer-Verlag. Langley, P. (1994). Selection of relevant features in ma-chine learning. Proceedings of the AAAI Fall Sym-posium on Relevance . AAAI Press.
 Liu, H., Hussain, F., Tan, C., &amp; Dash, M. (2002a).
Discretization: An enabling technique. Data Mining and Knowledge Discovery , 6 , 393 X 423.
 Liu, H., &amp; Motoda, H. (1998). Feature selection for knowledge discovery and data mining . Boston: Kluwer Academic Publishers.
 Liu, H., Motoda, H., &amp; Yu, L. (2002b). Feature se-lection with selective sampling. Proceedings of the Nineteenth International Conference on Machine Learning (pp. 395  X  402).
 Mitra, P., Murthy, C. A., &amp; Pal, S. K. (2002). Unsu-pervised feature selection using feature similarity.
IEEE Transactions on Pattern Analysis and Ma-chine Intelligence , 24 , 301 X 312.
 Ng, A. Y. (1998). On feature selection: learning with exponentially many irrelevant features as training examples. Proceedings of the Fifteenth International Conference on Machine Learning (pp. 404 X 412). Ng, K., &amp; Liu, H. (2000). Customer retention via data mining. AI Review , 14 , 569  X  590.
 Press, W. H., Flannery, B. P., Teukolsky, S. A., &amp; Vetterling, W. T. (1988). Numerical recipes in C . Cambridge University Press, Cambridge.
 Quinlan, J. (1993). C4.5: Programs for machine learn-ing . Morgan Kaufmann.
 Rui, Y., Huang, T. S., &amp; Chang, S. (1999). Image retrieval: Current techniques, promising directions and open issues. Journal of Visual Communication and Image Representation , 10 , 39 X 62.
 Witten, I., &amp; Frank, E. (2000). Data mining -pracitcal machine learning tools and techniques with JAVA implementations . Morgan Kaufmann Publishers. Xing, E., Jordan, M., &amp; Karp, R. (2001). Feature selection for high-dimensional genomic microarray data. Proceedings of the Eighteenth International Conference on Machine Learning (pp. 601 X 608). Yang, Y., &amp; Pederson, J. O. (1997). A comparative study on feature selection in text categorization.
Proceedings of the Fourteenth International Confer-
