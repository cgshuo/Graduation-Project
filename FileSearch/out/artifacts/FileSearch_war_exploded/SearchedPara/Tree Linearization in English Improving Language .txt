 To date, many natural language processing appli-cations rely on syntactic representations and also modify them by compressing, fusing, or translating into a different language. A syntactic tree emerg-ing as a result of such operations has to be lin-earized to a string of words before it can be out-put to the end-user. The simple and most widely used trigram LM has become a standard tool for tree linearization in English (Langkilde &amp; Knight, 1998). For languages with less rigid word order, LM-based approaches have been shown to perform poorly (e.g., Marsi &amp; Krahmer (2005) for Dutch), and methods relying on a range of linguistic fea-tures have been successfully applied instead (see Uchimoto et al. (2000) and Ringger et al. (2004), Filippova &amp; Strube (2007) for Japanese and German resp.). To our knowledge, none of the linearization studies have compared a LM-based method with an alternative. Thus, it would be of interest to draw such a comparison, especially on English data, where LMs are usually expected to work well.

As an improvement to the LM-based approach, we propose a combined method which distinguishes between the phrase and the clause levels:  X  it relies on a trigram LM to order words within  X  it finds the order of clause constituents (i.e., We show that such a differentiated approach is ben-eficial and that the proposed combination outper-forms the method which relies solely on a LM. Hence, our results challenge the widespread attitude that trigram LMs provide an appropriate way to lin-earize syntactic trees in English but also indicate that they perform well in linearizing subtrees cor-responding to phrases. Trigram models are easy to build and use, and it has been shown that more sophisticated n -gram models (e.g., with higher n , complex smoothing techniques, skipping, clustering or caching) are often not worth the effort of implementing them due to data sparse-ness and other issues (Goodman, 2001). This ex-plains the popularity of trigram LMs in a variety of NLP tasks (Jurafsky &amp; Martin, 2008), in partic-ular, in tree linearization where they have become de facto the standard tree linearization tool in ac-cordance with the  X  X vergenerate and rank X  principle: given a syntactic tree, one needs to consider all pos-sible linearizations and then choose the one with the all linearizations can be found recursively by gener-ating permutations of a node and its children. Unfor-tunately, the number of possible permutations grows factorially with the branching factor. Hence it is highly desirable to prohibit generation of clearly un-acceptable permutations by putting hard constraints encoded in the English grammar. The constraints which we implement in our study are the following: determiners, possessives, quantifiers and noun or ad-jective modifiers always precede their heads. Con-junctions, coordinated elements, prepositional ob-jects always follow their heads. These constraints allow us to limit, e.g., the total of 96 ( 2  X  2  X  4! ) possibilities for the tree corresponding to the phrase all the brothers of my neighbor (see Figure 1) to only two ( all the brothers of my neighbor, the all brothers of my neighbor ).

Still, even with such constraints, in some cases the list of possible linearizations is too long and has to be reduced to the first N , where N is supposed to be sufficiently large. In our experiments we break the permutation generation process if the limit of 20,000 variants is reached. The LM approach described above has at least two disadvantages: (1) long distance dependencies are not captured, and (2) the list of all possible lineariza-tions can be huge which makes the search for the best string unfeasible. However, our combined ap-proach is based on the premise that trigram LMs are well-suited for finding the order within NPs, PPs and other phrases where the head is not a finite verb. E.g., given a noun modified by the words big , red and the , a LM can reliably rank the correct order higher than incorrect ones ( the big red N vs. the red big N , etc.).

Next, on the clause level, for every finite verb in the tree we find the order of its dependents using the method which we originally developed for German (Filippova &amp; Strube, 2007), which utilizes a range of such linguistic features as PoS tag, syntactic role, length in words, pronominalization, semantic class, train two maximum entropy classifiers on all but the semantic features: 1. The first classifier determines the best starting 2. The second classifier is trained to determine Once the order within clause constituents as well as the order among them is found, the verb is placed right after the subject. The verb placing step com-pletes the linearization process.

The need for two distinct classifiers can be illus-trated with the following example: (1) a [ Earlier today ] [she] sent [him] [an email]. (1a,b) are grammatical while (1c) is hardly accept-able, and no simple precedence rule can be learned from pairs of constituents in (1a) and (1b): the tem-poral adjunct earlier today can precede or follow each of the other constituents dependent on the verb ( she, him, an email ). Thus, the classifier which determines the precedence relation is not enough. However, an adequate rule can be inferred with an additional classifier trained to find good starting points: a temporal adjunct may appear as the first constituent in a sentence; if it is not chosen for this position, it should be preceded by the pronominal-ized subject ( she ), the indirect object ( him ) and the short non-pronominalized object ( an email ). The goal of our experiments is to check the follow-ing hypotheses: 1. That trigram LMs are well-suited for phrase 2. That there is a considerable drop in perfor-3. That an approach which uses a richer represen-4.1 Data Street Journal articles from the period of 1987-92 (approx. 72 mill. words)  X  and automatically anno-tate them with sentence boundaries, part of speech tags and dependency relations using the Stanford parser (Klein &amp; Manning, 2003). We reserve a small subset of about 600 articles (340,000 words) for testing and use the rest to build a trigram LM with the CMU toolkit (Clarkson &amp; Rosenfeld, 1997, with Good-Turing smoothing and vocabulary size of 30,000). To train the maximum entropy classifiers we use about 41,000 sentences. 4.2 Evaluation To test the trigram-based approach, we generate all possible permutations of clause constituents, place the verb right after the subject and then rank the re-sulting strings with the LM taking the information on sentence boundaries into account. To test the combined approach, we find the best candidate for the first position in the clause, then put the remain-ing constituents in a random order, and finally sort them by consulting the second classifier.

The purpose of the evaluation is to assess how good a method is at reproducing the input from its dependency tree. We separately evaluate the perfor-mance on the phrase and the clause levels. When comparing the two methods on the clause level, we take the clause constituents as they are presented in the input sentence. Although English allows for some minor variation in word order and it might happen that the generated order is not necessarily wrong if different from the original one, we do not expect this to happen often and evaluate the perfor-mance rigorously: only the original order counts as the correct one. The default evaluation metric is per-phrase/per-clause accuracy: Other metrics we use to measure how different a generated order of N elements is from the correct one are: 1. Kendall X  X   X  ,  X  = 1  X  4 t 2. Edit distance related di , di = 1  X  m E.g., on the phrase level, the incorrectly generated phrase the all brothers of my neighbor ( X 1-0-2-3-4-5 X ) gets  X  = 0 . 87 , di = 0 . 83 . Likewise, given the input sentence from (1a), the incorrectly generated order of the four clause constituents in (1c)  X   X 1-0-2-3 X   X  gets  X  of 0.67 and di of 0.75. 4.3 Results The results of the experiments on the phrase and the clause levels are presented in Tables 1 and 2 respec-tively. From the total of 5,000 phrases, 55 (about 1%) were discarded because the number of admis-sible linearizations exceeded the limit of 20,000. In the first row of Table 1 we give the results for cases where, with all constraints applied, there were still several possible linearizations ( non-triv ; 1,797); the second row is for all phrases which were longer than one word ( &gt; 1 ; 2,791); the bottom row presents the results for the total of 4,945 phrases ( all ). Table 2 presents the results of the trigram-based (
TRIGRAM ) and combined ( COMBINED ) methods on the clause level. Here, we filtered out trivial cases and considered only clauses which had at least two constituents dependent on the verb (approx. 5,000 clauses in total). 4.4 Discussion The difference in accuracy between the performance of the trigram model on the phrase and the clause level is considerable  X  76% vs. 49%. The accuracy of 76% is remarkable given that the average length of phrases which counted as non-triv is 6.2 words, whereas the average clause length in constituents is 3.3. This statistically significant difference in per-formance supports our hypothesis that the  X  X vergen-erate and rank X  approach advocated in earlier studies is more adequate for finding the optimal order within phrases. The  X  value of 0.85 also indicates that many of the wrong phrase linearizations were near misses. On the clause level, where long distance dependen-cies are frequent, an approach which takes a range of grammatical features into account is more appro-priate  X  this is confirmed by the significantly better results of the combined method (67%). We investigated two tree linearization methods in English: the mainstream trigram-based approach and the one which combines a trigram LM on the phrase level with two classifiers trained on a range of linguistic features on the clause level. The results demonstrate (1) that the combined approach repro-duces the word order more accurately, and (2) that the performance of the trigram LM-based method on phrases is significantly better than on clauses. Acknowledgments: This work has been funded by the Klaus Tschira Foundation, Heidelberg, Ger-many. The first author has been supported by a KTF grant (09.009.2004). We would like to thank the anonymous reviewers for their feedback.

