 Document clustering is a popular research topic, which aims to partition documents into groups of similar objects (i.e., clusters), and has been widely used in many applications such as automatic topic extraction, document organization and filtering. As a recently proposed concept, Universum is a collection of X  X on-examples X  X hat do not belong to any con-cept/cluster of interest. This paper proposes a novel docu-ment clustering technique  X  Document Clustering with Uni-versum, which utilizes the Universum examples to improve the clustering performance. The intuition is that the Uni-versum examples can serve as supervised information and help improve the performance of clustering, since they are known not belonging to any meaningful concepts/clusters in the target domain. In particular, a maximum margin clus-tering method is proposed to model both target examples and Universum examples for clustering. An extensive set of experiments is conducted to demonstrate the effectiveness and efficiency of the proposed algorithm.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  clustering ; I.2.6 [ Artificial Intelli-gence ]: Learning Algorithms, Performance, Experimentation Clustering, Universum, Maximum Margin Clustering, Con-strained Concave-Convex Procedure (CCCP)
Document clustering is a very important topic in infor-mation retrieval, and has received substantial attentions for unsupervised document organization, automatic topic ex-traction, etc (e.g. [1, 2, 7, 13, 15, 21, 23]). It aims to parti-tion documents into groups of similar objects (i.e., clusters). From the information retrieval perspective, what document clustering does is to learn hidden patterns underlying the available documents, where these hidden patterns may con-tain some interesting concepts. A good clustering algorithm can automatically organize documents into a set of mean-ingful clusters for efficient browsing and navigation.
Current document clustering algorithms can be roughly divided into two categories (i.e., hierarchical methods and partitioning methods). The hierarchical methods divide the data into a hierarchical structure by employing bottom up or top down methods [6, 15, 18]. A typical algorithm for the hierarchical method is the hierarchical agglomerative clus-tering, which starts from taking each document as a single cluster, and then merges these documents gradually to build larger and larger clusters, until the whole dataset is merged as one single cluster. Partitioning methods divide the whole dataset into a fixed number of disjoint clusters [5]. A typical algorithm for the partitioning method is KMeans [5]. The basic motivation of KMeans is to find a set of clustering as-signments such that the sum of distances between examples and their associated cluster centers can be minimized. Be-sides KMeans, some probabilistic methods such as Gaussian Mixture Model (GMM) [14] and Probabilistic Latent Seman-tic Analysis (PLSA) [8], as well as some graph based meth-ods such as Normalized Cut (NCut) [19], have also shown promising results in clustering. However, one of the most significant drawbacks of these methods is that they only con-sider the target examples that need to be partitioned, but neglect other information that could be beneficial for clus-tering. It makes the clustering results only depend on the target examples. As shown in Fig.1(a), the traditional par-titioning method wrongly groups one data point (square) into the other cluster, without employing any other prior knowledge.

Universum is a set of examples that do not belong to any concept/cluster of interest, which are different from the tar-get examples that we need to cluster. Some previous work [4, 20, 28, 35] has demonstrated the benefits of treating these examples as prior knowledge imposed on the classification hyperplanes. However, there is no work on applying the Universum examples to the clustering problems. The major contribution of this paper is to show that Universum exam-ples can be used to improve the performance of document clustering. The intuition is that Universum examples should not have a clear pattern, or meaningful concepts/clusters in Figu re 1: (a) An example clustering result of a typical clustering algorithm (partitioning) with two classes of examples (i.e., circles for class 1 and squares for class 2). It is clear that one square is mis-classified; (b) An example clustering result with the guidance of Universum examples (i.e., check marks). Since Universum examples should not have a clear pattern/meaningful cluster to belong to, they should be distributed around the hyperplane, and provide some supervised information for the clustering prob-lem. As can be seen from this picture, the previous misclassified example is now correctly classified. the target domain. This intuition provides valuable guidance of the clustering process. As shown in Fig.1(b), since the Universum examples do not belong to any of the two clus-ters, it should only distribute around the hyperplane that separates these two clusters. With the guidance of these Universum examples, the square that was mis-clustered in Fig.1(a) can now be assigned to the right group. This paper proposes a novel Document Clustering with Universum ( DCU ) problem, which utilizes Universum ex-amples in document clustering. In particular, a formulation  X  Maximum Margin Document Clustering with Universum ( M 2 DCU ) is presented to solve this problem. This formu-lation is based on Maximum Margin Clustering [30], and aims at finding desired hyperplanes that maximize the mar-gins on the (target) examples that need to be partitioned, and minimize the margins on the Universum examples. The new formulation is not a convex optimization problem and therefore cannot be optimized directly. So, we first relax the original problem and then solve it by using Constrained Concave-Convex Procedure ( CCCP ) [26]. To accelerate the optimization speed for each sub optimization problem de-rived from CCCP, an extension of the Cutting Plane method [11] is designed. Our experiments on four real world datasets demonstrate the performance improvement brought by the Universum examples and show that the proposed method substantially outperforms state-of-the-art methods.
The rest of the paper is organized as: Section 2 intro-duces the related work. Section 3 puts forward the novel DCU problem. Section 4 proposes the M 2 DCU algorithm. Section 5 presents the experimental results. Section 6 con-cludes this paper and points out some future research direc-tions. The appendix proposes a novel optimization method to accelerate the optimization speed of the algorithm.
The concept of Universum was first proposed in solving classification problems, and was defined as a set of  X  X on-examples X  distributed in the same domain as the problem of interest, but known not belonging to any classes that need to be classified [28]. The channels of obtaining Univer-sum examples are very diverse. In [28], three simple ways are suggested, i.e., the examples from some other categories that are known unlikely to belong to any of the target cat-egories; the randomly generated examples; as well as the examples generated by randomly combining examples from different categories. Since they are not likely to belong to any of the target categories, when designing classifiers, these Universum examples should not be classified to any of the categories either.

Out of this motivation, in [28], Jeston et al. designed the first classification algorithm that utilizes Universum exam-ples  X  U -Support Vector Machines ( U -SVM). This work adds a penalty term on the Universum examples to the for-mulation of the traditional Support Vector Machines (SVM) [5] so that the output given by the classifier will minimize the empirical classification loss on the target examples, and will not give clear classification assignments for the Univer-sum examples. This work is reasonable and performs better than SVM, as shown in an extensive set of experiments [28]. However, it can only deal with binary supervised classifica-tion problems. Inspired by this paper, in [35], Zhang et al. extended the Universum algorithm to solve multi-class clas-sification problems and improved the performance of semi-supervised learning by using Universum examples. Later, Huang et al. proposed to use the Universum examples in a different way to solve the semi-supervised classification problems [9]. In [20], the authors analyzed some properties of U -SVM. In [4], Chen et al. designed a method to choose the most useful Universum examples for classification.
All of the previous work has contributed a lot to the use of Universum examples in classification problems. However, there is no prior work on how to use Universum examples in clustering problems. In fact, in clustering, it is natural to consider the Universum examples as examples that do not belong to any of the clusters in the target domain. In this way, the Universum examples can be viewed as some supervised information in an unsupervised problem. It is also interesting to notice that in the toy experiments of [20], U -SVM performs much better than SVM when the number of labeled examples is small. In a clustering problem, since no labeled example is given, we would also expect a great advantage over the traditional clustering algorithms by the utilization of the Universum examples.
The proposed method M 2 DCU is based on a recently de-veloped algorithm  X  Maximum Margin Clustering ( MMC ). MMC was first proposed in [30]. In [30], the authors bor-rowed the idea of a standard machine learning principle -maximum margin principle, and used it for clustering. In particular, they tried to assign each of n instances to two classes { X  1 , +1 } so that the separation between the two classes can be as large as possible. It is formulated as: where ter capture how C scales with the data set size, where C is a trade-off parameter for the two parts of this formulation. l  X  0 is a con stant controlling the class imbalance and e is an all-one vector [30]. The final clustering result is given by y . It is clear that if we subsequently run an SVM with the clustering assignment obtained from MMC, the margin would be maximal among all possible labelings.

In this paper, we consider the distribution of Universum examples as prior knowledge in MMC. With similar moti-vations, in [29, 32], the authors propose to use some known similarity relationships (e.g., must-link, cannot-link and fam-ily/possible-link) between examples as the prior information in clustering. However, in their work, all the documents are still associated with some meaningful clusters, while this is not the case for clustering with Universum. First of all, the new problem  X  Document Clustering with Universum (DCU) is formally proposed. Suppose we are given a set of documents/examples, represented by X = { x 1 , x 2 , . . . , x n } , x i  X  R d , where n is the total number of documents. Besides these documents, some Universum doc-uments/examples, which are in the same domain as X , but known not belonging to any meaningful groups in X , are also available, and denoted as X  X  = { x  X  1 , x  X  2 , . . . , x N is the number of Universum examples. For convenience, throughout this paper, the examples in X are denoted as  X  target examples/documents  X , while the examples in X  X  are referred to as  X  Universum examples/documents  X . The main objective of DCU is to divide the target examples into k clusters, with the help of Universum examples. A 1  X  n vector y is used here to denote the cluster assignment array, with y i being the cluster assignment for x i .
The proposed formulation tries to find a set of hyper-planes that can separate examples from different clusters, and minimize the confidence of assigning Universum exam-ples to any category. To model these hyperplanes, for each cluster p  X  { 1 , 2 , . . . , k } , we define a weight vector w the target examples, we expect their cluster assignments to be as determined as possible. Therefore, M 2 DCU intends to maximize the clustering margins/confidence on the target examples. For the Universum examples, since they should not be assigned to any one of the k clusters, we expect that the margins 1 /confidence on them should be small, which is expressed as: where  X  1 is a tolerance parameter, u  X  i = arg max p w T v that needs to be optimized on the Universum examples. The
I t can also be considered as the difference between the largest output and the second largest output. As we shall see later, this kind of margin definition on Universum exam-ples is also consistent with the margin defined on the target examples in Eq.(7).
Throughout this paper,  X  \  X  means ruling out. concrete formulation of M 2 DCU can be written as: where  X  y i ,r is the indication function, which equals 1 if r equals y i and otherwise 0. Constraint (4) is the large margin constraint on the target examples. Constraint (5) minimizes the margins on the Universum examples. l is a parameter that controls the cluster balance to avoid trivially  X  X ptimal X  solutions, which may assign most of the examples to one cluster. It is clear that the proposed formulation tries to maximize the margins/confidence on the target examples, and minimize the margins/confidence on Universum exam-ples simultaneously. By doing so, the maximum margin clus-tering procedure can be guided by the prior knowledge from the Universum examples.

Next we will show the number of variables involved in problem (3) can be reduced by n . In particular, Eq.(3) is equivalent to the following optimization problem: where u i = arg max p w T p x i , and the optimal y i equals u Furthermore, for the convenience of computation, three con-catenated vectors are introduced: where 0 is a 1  X  d zero vector. In x i ( p ) , only the ( p  X  1) d to pd -th elements are nonzero and equals x i and it is clear We propose to absorb  X  u i ,r into  X  i and use a separate vari-able  X  ir for each constraint. The function w T v  X  convex. To simplify the formulation, we relax this function lates the average value of the input function with respect to the subscript variable. Then, Eq.(6) is rewritten as: This form ulation is reasonable, however, it is both noncon-vex and nonsmooth. Therefore, it cannot be solved directly. We will show how to solve this problem by using CCCP in the following section.
The nonconvexity in problem (10) is caused by the con-straint (11) and (12). However, it can be considered as the difference between two different convex functions 3 . There-fore the Constrained Concave-Convex Procedure (CCCP) can be employed to solve this problem.

Given an initial point e w 0 , CCCP computes e w ( t +1) 4 lem (10) with their corresponding first order Taylor expan-sions at e w ( t ) and solves the resulting quadratic programming problem, until convergence. Since e w T x i ( u are non-smooth, their sub-gradients are used in their first order Taylor expansions. More precisely, for the t -th CCCP iteration, e w T x i ( u and e w T x  X  u i = arg max p ( e w decomposed into a set of convex subproblems and for the t -th CCCP iteration, the corresponding subproblem is: It is clear that by using CCCP together with the subgradi-ent, during each iteration, we are trying to maximize the clustering margin on the target examples, and minimize the margin on the Universum examples, based on the clus-ter assignments obtained in the previous iteration. It has been verified that CCCP decreases the objective function monotonically [26], and is guaranteed to find a local op-timal solution in a relatively small number of iterations. The whole algorithm is then described in Table 1. Here, J
Th e pro posed algorithm is straightforward. But if the number of target examples, as well as the Universum exam-ples goes large, or/and if the data dimension is high, directly solving problem (13) in the primal form may be very time consuming, since it involves a lot of optimization variables and constraints. There are many optimization alternatives to solve this efficiency problem [10, 24]. In this paper, we
F or constraint (11), e w T x i ( u function, and e w T x i ( r )  X   X  ir is convex and smooth. This constraint can be considered as the difference of these two convex functions, i.e., ( e w T x i ( r )  X   X  ir )  X  e w T the same way, constraint (12) can also be considered as the difference of two convex functions.
The superscript t is used to denote that the result is ob-tained from the t -th CCCP iteration.
 Algo ri thm : M 2 DCU
Input: 1. A set of target documents: X = { x 1 , ...., x n } 2. A set of Universum examples: X  X  = { x  X  1 , ...., x  X  2. param eters: regularization parameter C l and C u , tolerance parameter for Universum examples  X  1 = 0 . 5,
CCCP solution precision  X  2 = 0 . 01, cluster number k , cluster size balance parameter l Out put: The clust er assignment y
CCC P Iter ations : 2. Ini tialize e w 0 , t=0,  X  J = 10 3 , J (  X  1) = 10 3 3. whi le  X  J /J ( t  X  1) &gt;  X  2 do 4. Deriv e problem (13) by updating u ( t ) i = 5. t = t + 1 6. Get ( e w ( t ) , J ( t ) ) by solving problem (13). 8. end while 9. Clu ste r Assignment :
F or x i , the corresponding cluster assignment y i = suggest a novel optimization method, which is an extension of the cutting plane method [10], to solve problem (13). The detail of this optimization method is elaborated in the Ap-pendix A.
An extensive set of experiments is conducted to validate the effectiveness and the efficiency of the proposed method These experiments are conducted with Matlab r2008a on a 2.0GHZ Intel Quad Core computer with 4.0GB main mem-ory.
In practice, the Universum examples can be obtained eas-ily. In this paper, three different ways are first employed to generate the Universum examples. A Universum selection method is then used to choose the most useful Universum examples.

U rest : some documents that are not included in the clus-tering tasks [28, 35]. We will elaborate U rest for different clustering tasks when introducing datasets. Although this generation method sounds like needing category informa-tion, this is not necessary for a real application of clustering because it can obtain a separate set of documents that are not in categories in consideration. For example, we may be interested in clustering publications in some major computer science conferences (documents in major categories in their datasets), and we can assume U rest consists of articles in a set of very specialized conferences on minority subject areas in computer science or even articles in another discipline.
U rand : Generate uniformly distributed features within the
Th e co de and data can be found on the author X  X  homepage. rang e of the min and max values on each dimension of the target examples [28, 35].

U mean : In [28], the authors designed some Universum ex-amples which are combinations of different pictures from different categories. However, in a clustering problem, no la-bel information is given. Therefore, an alternative method is used to design the Universum examples in the cluster-ing problem. KMeans is first employed to pre-partition the whole dataset into k groups. k ( k  X  1) 2 Un iversu m examples are then generated by picking two clustering centers each time from the k groups and merging them together.
U select : In [4], the authors pointed out that the Universum examples that lie between different categories are the most helpful ones. Their basic motivation is reasonable. How-ever, their method is only designed for classification prob-lems, and is very time consuming. In this paper, we pick the most useful Universum examples for clustering in a different way. Our basic intuition is that the Universum examples that are closest to the target examples are the most useful ones for clustering. Although these Universum examples are close to the target examples, they do not belong to any of the clusters. So, they should have much higher impacts on the clustering results than the ones that lie far away. In or-der to measure the closeness of the Universum examples, a Gaussian Mixture Model is first trained on the target exam-ples, with k being the number of gaussian models. Then, for each Universum example, we calculate the probability that it is generated by using this Gaussian Mixture Model. The Universum examples with the highest generation probabili-ties are then selected for use, and are referred to as U select In our experiments, the top 10 percents of all the Universum examples are selected. As observed from these experiments, the selected proportions of U rest and U mean are much higher than that of U rand . This is because the randomly generated Universum often has a very low probability to be close to the target examples. But for U rest and U mean , they are often closer to the target examples.
We use four text datasets in our experiments. 20Newsgroup : This is a benchmark dataset 6 , which con-tains four main categories, i.e.,  X  X omp X ,  X  X ec X ,  X  X ci X ,  X  X alk X , as well as some other categories with small number of exam-ples, such as  X  X lt.atheism X ,  X  X isc.forsale X , etc. The four main categories are used for clustering, while examples in the re-maining categories are used as U rest .

WebKB This dataset contains webpages from computer science departments at around four different universities They are divided into 7 categories (i.e., student, faculty, staff, course, project, department and other). We choose webpages from four categories as target examples for clus-tering (i.e., course, faculty, project, student), and the re-maining 2 categories, i.e.,  X  X ther X  and  X  X taff X  will serve as U
Reuters-Volume I (ReutersV1) : It is an archive of over 800 , 000 manually categorized newswire stories [12]. A subset of ReutersV1 is used. There are in total 126 topics in it. In the experiments, we choose examples from three cat-egories  X  X CAT X ,  X  X 151 X , and  X  X 14 X  in this subset as target examples, and some examples from the remaining categories h ttp ://people.csail.mit.edu/jrennie/20Newsgroups/ http://www.cs.cmu.edu/  X  webkb/ are randomly selected as U rest . To avoid some overlap prob-lems, the target examples with more than one label from these three categories are removed.

Reuters21578 : This dataset contains documents col-lected from the Reuters newswire in 1987 8 . There are in total 135 categories in this dataset. In our experiments, we use two subsets of this data collection, i.e., Reuters21578-1 and Reuters21578-2. Reuters21578-1 contains 6 categories among these 135 topics. Similar to ReutersV1, the docu-ments associated with more than one of these 6 categories are not used. Reuters21578-2 contains another 10 categories. And the documents associated with more than one of these 10 categories are not used. For both of these sub-datasets, U rest are randomly selected from examples in the remaining categories.
 The basic information of these datasets is summarized in Table 2. For all of these documents, the tf-idf (normalized term frequency and log inverse document frequency) [15] features are extracted, with the stop words being removed and porter [15] as the stemmer. Furthermore, the most fre-quently appeared words are picked.
In experiments, the number of clusters k is set to be the true number of classes in target examples for all clustering algorithms. As described in Section 5.1, three different Uni-versum methods are used in this paper. But we will not use them directly. The selection method mentioned previously is used to select the most useful Universum examples. U select is then used for M 2 DCU as selected Universum examples. The class imbalance parameter l is set by grid search from the grid [0 . 001 , 0 . 01 , 0 . 1 , 1 , 10]. The parameter C from the exponential grid 2 [1:1:6] and the parameter C u fect brought by the Universum examples, we further report the performance of M 2 DCU-0, in which the C u in problem (13) is set to be 0, and tune the other parameters using exactly the same way as we do for M 2 DCU.

Several different kinds of clustering methods are used for comparison. It includes: the probability generative method  X  Gaussian Mixture Model (GMM) [14]; the methods based on matrix factorizations, such as Nonnegative Matrix Fac-torization (NMF) [31] and Probabilistic Latent Semantic Analysis (PLSA) [8]; spectral/graph based methods, such as Normalized Cut (Ncut) [33], Clustering with Local and Global Consistency (CLGR) [27] and KMeans; Maximum Margin based clustering, such as CPM3C [36]. For both KMeans and GMM, for each experiment, the result is sum-marized over 50 independent runs to avoid local optimal. For Ncut, its gaussian similarity is determined by local scaling [34]. The formulation of CPM3C [36] is similar to M 2 DCU-0, but there is a significant difference in their optimization methods. The outer iteration of CPM3C is the cutting plane method and its inner iteration is CCCP. However, when us-ing the cutting plane method to solve a nonconvex problem, the convergence property and optimality of the solution can-not be guaranteed. The parameters of CPM3C are tuned using exactly the same settings as M 2 DCU-0. h ttp ://www.daviddlewis.com/resources/testcollections /reuters21578/
The clustering accuracy (Acc) [25, 30, 36] is used to eval-uate the final clustering performance. Specifically, we first take a set of labeled examples, remove the labels of these examples and run the clustering algorithms, then we relabel these examples using the clustering assignments returned by the algorithms. Finally, we measure the percentage of correct classifications by comparing the true labels and the labels assigned by the clustering algorithms as follows: whe re, map (  X  ) is a function that maps each cluster index to a class label, which can be found by the Hungarian algorithm [16]. c i and y i are the cluster index of x i and the true class label.  X  ( a, b ) is a function that equals 1 when a equals b , and 0 otherwise.
Another evaluation metric we employed here is the Nor-malized Mutual Information (NMI) [22], which was origi-nally a symmetric measure to quantify the statistical infor-mation shared between two distributions. More precisely, for two random variables X and Y , NMI is defined as: Here, I ( X, Y ) is mutual information between X and Y , while H ( X ) and H ( Y ) are entropies of X and Y . If there is a one to one correspondence between the distribution of X and that of Y , NMI equals one. From the perspective of information theory, it means all of the information en-coded in X (sender) has already been correctly delivered to Y (receiver). Otherwise, if Y is merely a uniform distribu-tion, NMI equals zero, which means no information would be transferred from X to Y . It is clear that NMI ( X, X ) = 1, which achieves the maximal possible value of NMI. Given two clustering results, we can consider the clustering as-signments as distributions of random variables, and NMI in Eq.(14) can be estimated as: whe re n p refers to the number of data contained in the p -th cluster, b n q is the number of data belonging to the q -th cluster, and n p,q denotes the number of data that are in the intersection between the p -th and the q -th clusters. Suppose two clustering assignments, I and L , are given. sults and I i is the i -th cluster. L = {L 1 , L 2 , . . . , L the set of true data classes such that L i represents the i -th class.

The rand index measures the similarity between I and L in X that are in both of these two clusters. b represents the number of data pairs in X that are in different sets in these two clusters. c is the number of data pairs that are in the same set in I but in the different sets in L . d denotes the number of data pairs in X such that they are in different sets in I but the same set in L . It is natural to consider a + b as the number of agreements between I and L , while c + d represents the number of disagreements. If R equals 0, it means I and L are totally different, and if R equals 1, it means these two cluster assignments are exactly the same.
The clustering results are reported from Table 3 to Ta-ble 5. In Table 6, the CPU running time of these different methods is also given.
 From the clustering results, we can see that in all cases M 2 DCU performs better than M 2 DCU-0, which clearly demon-strates the benefits brought by the involvement of Univer-sum examples. When compared with the probability gener-ative methods, such as GMM, M 2 DCU prevails in all cases. For the methods based on matrix factorizations, such as NMF and PLSA, their performances are good in some cases. As can be seen from Table 4, the NMI performance of M 2 DCU on 20Newsgroup is worse than that of PLSA and NMF, and on Reuters21578-2, the Rand Index of NMF is also higher than M 2 DCU. But it is clear that for the other cluster-ing measurements on these datasets, the proposed method performs better than NMF and PLSA. For example, for the clustering accuracy and Rand Index measurements on 20Newsgroup, the performances of the proposed method are higher than NMF and PLSA. It is safe to say that in most cases, the proposed method performs much better than the matrix factorization based methods. Ncut, CLGR, and KMeans are spectral based methods, whose performances mainly rely on graph factorization. These methods are rea-sonable, which place the cluster boundaries in the low den-sity regions of the dataset. However, they do not utilize the Universum examples to further guide the clustering pro-cess, and therefore, their performances are inferior to that of M 2 DCU.

As a maximum margin clustering method, although the formulations of CPM3C and M 2 DCU-0 are similar, CPM3C uses the cutting plane method to solve a non-convex opti-mization problem. As previously claimed, the cutting plane method is not originally designed to solve non-convex opti-mization problems, and therefore, the performance of CPM3C cannot be guaranteed. Different from CPM3C, M 2 DCU uses the CCCP to solve the non-convex optimization problem, and the cutting plane method is suggested to solve the con-vex sub-problems derived from each CCCP iteration, and therefore it is theoretically much more elegant than CPM3C. As we shall see from experiments, in almost all cases, both M 2 DCU and M 2 DCU-0 perform better than CPM3C, which clearly validates our claims.

As for the CPU time, M 2 DCU is ranked among the top three fastest algorithms in three of the five datasets. For the experiments on Reuters21578-1 and Reuters21578-2, the proposed method are seemingly slower than some other algo-rithms. However, it is clear that the corresponding cluster-ing performances of the proposed method are much higher than those of the ones with faster CPU running times. We suspect that the lower efficiency of the proposed method on these two datasets is caused by the data distributions in these two datasets, which increases the number of con-vergence steps. It is interesting to notice that on WebKB dataset the CPU running time of M 2 DCU is shorter than runs.
 best performances on the corresponding datasets. M DCU-0 represents the case when the weight of the summarized over 50 independent runs.
 and GMM, the results are summarized over 50 independent runs. M 2 DCU-0 represents the case when the weight of the Universum term C It is interesting to note that in WebKB, M 2 DCU is faster than M Universum examples may sometimes reduce the number of CCCP iterations. Figu re 2: Parameter Sensibility on 20Newsgroup, with different C u . It is clear that the performance is improved by incorporating the Universum exam-ples. Figu re 3: Parameter Sensibility on 20Newsgroup, with different C l . From these two pictures, we can see that C l is relatively stable in a very large range of the whole parameter space. that of M 2 DCU-0. We suspect that sometimes, introducing the Universum examples can reduce the number of CCCP iterations and therefore result in a shorter running time.
Besides these comparison experiments, we also measure the parameter sensibility of the proposed method for C u , C , l . The experiments are conducted on 20Newsgroup. In these experiments, C u is tuned and evaluated through the grid [0 . 001 , 0 . 01 , 0 . 1 , 1 , 10]. C l is from the grid 2 tivity experiments are shown from Fig.2 to Fig.4. In Fig.2, the performances with different C u values are evaluated. C reflects the involvement of the Universum examples, where C u = 0 means no involvement for the Universum examples. As can be seen from the trend of the performances, we can conclude that M 2 DCU benefits a lot by introducing the Uni-versum examples. In Fig.3, it is shown that the performance of the proposed method increases with C l at first, and is rel-atively stable within a very large range of the whole parame-ter space, especially after C l reaches a relatively large value. And in Fig.4, we can see that the proposed method is rela-tively stable with respect to l . But the performance of the proposed method does decrease a little after a fixed l value. Therefore, a relatively small value of l is normally suggested. We have also observed similar behaviors on the other three datasets, i.e., WebKB, ReutersV1 and Reuters21578. But due to the limit of space, we cannot put these experimental results here.
Document clustering is an important research topic with Figur e 4: Parameter Sensibility on 20Newsgroup, with different l . The proposed method is relatively stable w.r.t l . But if l is too large, the perfor-mance will deteriorate. So, normally a relatively small value of l is suggested. many practical applications. Previous document clustering methods only consider the examples that need to be clus-tered, but neglect the useful examples that can be obtained from some other sources. As a newly proposed concept, Universum, which is defined as a set of  X  X on-examples X  has been receiving more and more attentions. These examples can often be obtained easily and have already been shown the capability of improving the performances in classifica-tion problems. In this paper, we proposed a novel research problem  X  Document Clustering with Universum (DCU), which extends the idea of the utilization of Universum exam-ples to document clustering problems, and suggest a method  X  Maximum Margin Document Clustering with Universum (M 2 DCU) to solve this problem. The experimental results show that the Universum examples can be used to improve the performance of clustering under the framework of Maxi-mum Margin Clustering, and the proposed method performs substantially better than state-of-the-art methods in most cases. In the future, we will further investigate: 1. how the Universum examples can be integrated to other clustering algorithms, such as KMeans, GMM, NMF; 2. how to design better ways to select Universum examples. We thank the anonymous reviewers for valuable comments. This research was partially supported by the NSF research grants IIS-0746830, CNS-1012208, IIS-1017837, CCF-0939370.
Formulation (13) is a standard quadratic programming subproblem for the t -th CCCP iteration. But if the num-ber of constraints, i.e., the number of target examples and Universum examples, is huge, it would be very time consum-ing to solve this problem. Some of the previous large scale optimization methods can be adapted to solve this prob-lem, such as [10] and [24]. In this section, inspired by [10], the cutting plane method [11], which has shown its effec-tiveness and efficiency in solving similar tasks, is suggested to solve this subproblem. But different from [10], which is mainly designed for optimization problems with only one set of constraints, in this paper, we propose a novel optimization method  X  Two-View Cutting Plane method that can be used to solve the optimization problem with two different sets of constrains, as is the case in problem (13).

Similar to [10], problem (13) is first transformed to the following equivalent form: N ( k  X  1)  X  and  X   X  are two newly introduced variables, where  X  = number of variables that need to be optimized is further reduced by ( n  X  k + N  X  2). However, it contains a large number of constraints. Fortunately, we don X  X  need to take all of these constraints into account. to solve the problem, the cutting plane algorithm is adapted to find two polynomially sized subset of constraints  X  and  X   X  from the whole set of constraints { 0 , 1 } n  X  k and { 0 , 1 } N respectively in problem (16). In particular, the constraint sets  X  and  X   X  are initially set to be empty, and are built step by step until the solution of the relaxed problem satisfies all the constraints up to the two cutting plane precisions  X  3 and  X  4 , i.e.,  X  c  X  X  0 , 1 } e w and  X  d  X  X  0 , 1 } N ,
I t means, the remaining exponential number of the two sets of constraints will not be violated up to the precision  X  and  X  4 respectively. Therefore, we don X  X  need to explicitly add them to  X  and  X   X  .

The cutting plane algorithm starts with two empty sets of constraints  X  and  X   X  as follows: After getting the solution e w ( t 0 ) 9 of the above optimization problem, the most violated constraints can be computed as: and d note the condition ( e w ( t s ) ) T 1 nk  X  optimization problem becomes:
Th rou ghout this paper, t s is used to denote the s -th iter-ation of the cutting plane iteration for solving the problem derived from the t -th CCCP iteration.
 Algo ri thm : Two-View Cutting Plane
Input: 1. Pro b lem (13). 2. Cu tt ing Plane precision for View 1:  X  3 = 0 . 01, for View 2:  X  4 = 0 . 01 Out put:
The clu ster hyperplane e w ( t ) and the objective function 1. s =  X  1.  X  =  X  ,  X   X  =  X  , H t s = true, H  X  t s = true 3. s = s + 1 and  X   X  . 5. Com p ute the most violated constraints, i.e., c t s ir and d t s i , by an d  X  =  X  set  X   X  by  X   X  =  X   X  8. end while T abl e 7: Algorithm description: Two-View Cutting Plane. It can be used to solve the step 6 in Table 1 efficiently.

Both  X  and  X   X  contain at most one constraint vector at this time. From this updated optimization problem, we can get the solution e w ( t 1 ) . Then, the constraints c t 1 and d can be computed similarly as that in Eq.(20) and Eq.(21), where the only difference is that e w ( t 0 ) would be replaced by satisfy the requirement in Eq.(17) and Eq.(18), i.e., both H approximation series of the problem (13) can be constructed by the expanding number of cutting plane procedures that cut off the current optimal solution from the feasible set [11].
The concrete procedure is summarized in Table 7. In Step 4, the problem can also be solved efficiently through the dual form [3], since there are only a few constraints involved. Here, due to the limit of space, the concrete dual form is omitted.

It is clear that the Two-View Cutting Plane algorithm is an iterative method. One of the nice properties of employing this method is the guaranteed fast convergence rate as can be derived similar to that in [10]. So, the algorithm will converge with polynomial number of constraints in a linear time, and therefore is much faster than directly solving the optimization problem (13).
