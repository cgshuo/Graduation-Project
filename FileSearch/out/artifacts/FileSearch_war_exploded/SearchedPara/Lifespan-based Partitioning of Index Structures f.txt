 Time-travel text search over a temporally evolving document collec-tion is useful in various applications. Supporting a wide range of query classes demanded by these applications require different in-dex layouts optimized for their respective query access patterns. The problem we tackle is how to efficiently handle different query classes using the same index layout.

Our approach is to use list intersections on single-attribute in-dexes of keywords and temporal attributes. Although joint predicate evaluation on single-attribute indexes is inefficient in general, we show that partitioning the index based on version lifespans coupled with exploiting the transaction-time ordering of record-identifiers, can significantly reduce the cost of list intersections.

We empirically evaluate different index partitioning alternatives on top of open-source Lucene, and show that our approach is the only technique that can simultaneously support a wide range of query classes efficiently, have high indexing throughput in a real-time in-gestion setting, and also have negligible extra storage costs. Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Search Process Keywords: Time-travel Text Search; Index Partitioning; Lucene
The ability to efficiently access historical data and analyze ( via keyword searches ) system states at a specific historic point (or range) in time is becoming essential for a number of web-scale applications. These applications include IT operational analytics, web crawling, and application provenance, where the data generated is typically in the form of unstructured text documents or partially structured key-value pairs (e.g. JSON data attributes). Each item or document gets created, undergoes multiple updates, creating a new document version each time, before finally getting deleted. Analytics used to derive insights from this temporally evolving data requires a plat-form that is capable of supporting efficient time-travel text-searches (i.e. queries with constraints on the temporal dimension and key-word matches) over a corpus of versioned documents.

To motivate this problem better, we take the specific example of IT operational analytics, where continuous monitoring, real-time problem detection and root cause diagnostics are required. The in-put data is a temporally evolving collection of machine-data doc-uments, where each document captures the current state of a fine-grained machine entity. Examples of such entities can be a running process, a mounted disk partition, a network connection, installed packages, etc. Even a single machine can have several thousands of such machine-entities, and the document corresponding to each fine-grained machine entity evolves over time as its state changes. This stream of machine-data documents collected by the monitor-ing infrastructure arrive rapidly and need to indexed at real-time to be made readily available for query-driven analysis. The type of queries that power these analytics can vary as well  X  ranging from analyzing the state of the system at a historic point in time, analyzing the intersection (invariants) or difference (diffs) between datacenter state snapshots between two different points in time, to analyzing the entities that got updated within a historic time interval. Section 2.2 describes the different types of queries relevant to an IT operational analytics scenario. The key thing to observe is that queries can have arbitrary combinations of temporal and keyword predicates.

A system that can deliver high performance on time-travel text-searches has to overcome several challenges on the indexing, stor-age and query processing fronts. The indexer must be able to pro-cess document versions as they arrive in real time, rather than batch processing the entire document corpus that is available upfront. The incoming data must be indexed such that query execution is efficient across different classes of queries  X  queries with just keywords or only temporal constraints or combinations of both. Even as there is a need to support efficient execution on a wide variety of query classes, this cannot come at the cost of high storage overheads or substantial reduction in indexing throughput. A lightweight index-ing solution is required, and replication of index structures to have different index layouts optimized for different query classes has to be avoided.

Prior works [1, 2, 4, 9] that support time-travel text search parti-tion the document collection in different ways to enable queries to quickly narrow down on the document versions of interest. These techniques either have (a) have high storage costs due to replication of long-living document versions [4], or (b) are primarily designed for situations where the entire archival collection is available upfront since it incurs high periodic index-rebuilding costs in real-time in-gestion settings [1], or (c) have performance degradation at high data rates and large datasets because of subtle dependencies of their parti-tioning technique on the expected available size of per-term memory buffers [2], or (d) have an index layout that is specifically optimized for the joint predicate (keyword+temporal) query and thereby cannot give good performance on other query classes (e.g. single-attribute queries) unless assumptions about the entire archival collection be-ing available upfront hold [9].

In this paper, we make a number of design choices to improve the performance across a number of dimensions. Rather than creating nested or multi-dimensional indexes, we create single-attribute index structures that retain good performance on single-attribute queries (pure temporal or pure keyword). For the joint predicate queries, we focus on optimizing the list intersection performance. This is done by taking advantage of the implicit temporal encoding (chrono-logical transaction time ordering) present in the record-identifiers of documents indexed by underlying indexing engine. This implicit ordering, coupled with partitioning based on lifespan allows us to effectively prune away index entries that do not satisfy the temporal constraints, resulting in good performance on the (keyword + tempo-ral) query. With respect to storage overheads, we have disjoint parti-tions and every document version is present in exactly one partition, implying storage optimality. Last but not the least, given that the determination of which partition a document version should be part is a function of only the attributes of the version getting indexed (i.e. lifespan attribute) instead of a complex global optimization function, our approach is able to achieve low indexing overheads and also does not require any periodic re-organization of the index layout.
We implemented our approach on top of open-source Lucene [15], and empirically evaluated indexing, storage, and query evaluation costs using several real and synthetic datasets. Our indexing scheme adds negligible storage and indexing overhead on top of a regular keyword index, and can also work in a real-time ingestion setting. Our results demonstrate that our approach can efficiently support a wide variety of query classes demanded by emerging applications (e.g. IT operational analytics). For each query class, our approach gives query performance that is comparable (or superior) to the best performing indexing technique for that particular query class. In contrast, each of the prior index organization schemes is significantly worse than our approach on query classes for which the scheme was not optimized for.

Finally, would like to mention that although the IT operational analytics use-case influenced the design requirements of our time-travel text search engine, the study of a full-fledged datacenter man-agement system built over our engine is outside the scope of this work 1 . Further, in order to demonstrate the generic value of our pro-posed technique, we have chosen public datasets that are not limited to machine-data in any way.
Each version of a temporally evolving document entity is associ-ated with a validity interval (i.e. [ birthtime , deathtime )) which de-notes the time-interval during which this version was valid. For a document that was successively updated at transaction times t ...,t n (in chronological time order), the document versions created are given by v 0 ,v 1 ,...,v n , and the validity intervals of these suc-cessive versions are  X  [ t 0 ,t 1 ) , [ t 1 ,t 2 ) ,..., [ t current unexpired version of the document is the one with deathtime set to the special value  X  . And, for a deleted document, the last version X  X  deathtime attribute is the time of deletion.
Our goal is to support a wide variety of query classes  X  searches comprising just keywords or only temporal constraints or combi-
This would require designing the types of machine-data to instru-ment and modelling the collected data into machine-documents. nations of both. Table 1 gives various instances of example query classes in our IT operational analytics use-case. The key observation here is that queries could span arbitrary unions/intersections over the birthtime , deathtime , and keyword predicates.

We will now give an example of how temporal search queries can enable valuable use-cases in IT operational analytics. Let X  X  say a system administrator wants to identify machines that were poten-tially vulnerable to the heartbleed bug, by retrieving the list of ma-chines where some affected application has been running over a vulnerable openssl package. This can be enabled by first indexing machine-data documents related to process entities, wherein each process document comprises process attributes like process.pid, pro-cess.command, process.libraries, process.hostmachine, etc. The ad-ministrator can then issue the alive-any-search ( k,t min with the keyword predicate k as (process.libraries:  X  X penssl-1.0.1 X ) and the temporal predicate attribute t min set to the release date of the first vulnerable openssl version and t max set to the date when all machines in the datacenter were patched with the openssl version having the security fix. This query would retrieve the list of poten-tially vulnerable process entities, and the machine documents corre-sponding to these entities could then be used to generate the list of &lt;application-type, host-machine&gt; tuples on which further forensic investigation would need to be done.
Search index query processing leverages the standard inverted in-dex or postings lists [24], wherein the search-index maintains a post-ings list for every distinct term. A term X  X  postings list contains a list of record-identifiers assigned by the indexing engine to document versions that contain that term. To answer keyword-only queries with arbitrary conjunctions/disjunctions, the search engine performs intersections and unions on the respective term postings lists.
To answer joint predicate (keyword+temporal) queries, temporal search frameworks (e.g. [4]) extend the inverted-index structure to embed the validity interval (i.e. [ birthtime , deathtime )) in the post-ings list payload . During query evaluation of the keyword predicates using standard postings list intersection/union operations, this pay-load information can then be used to check if the scanned postings list entries additionally satisfy the temporal predicate. The I/O cost is however proportional to the size of intermediate result set (i.e. satis-fying keyword-only predicates), which could be substantially larger than the size of final result set. To address this, prior works have proposed techniques to divide term postings lists into partitions such that index-partitions that have no chance of satisfying the temporal predicates could be completely ignored, thereby avoiding wasteful disk I/O. Note that there could still be wasteful reads (i.e. versions scanned but not satisfying query X  X  temporal predicates) in each par-tition since the partitioning is only at coarse granularity.
We use Figure 1 to highlight the pros and cons of different index-partitioning techniques. Figure 1(a) shows the Simple Time-tiers ap-proach, wherein the index is split into disjoint collections along the (c) Interval-tree based birthtime axis. With an ordering based only on birthtime , this layout becomes inefficient for snapshot-search ( k,t ) queries, since the de-sired results can comprise document versions that could have been born at any time between [0 ,t )  X  meaning a large number of parti-tions would need to be accessed to answer the queries.

To overcome this query inefficiency, long-living versions can be replicated across the time-boundaries resulting in the Replicated Time-tiers technique [4], as illustrated in Figure 1(b). In this approach, the snapshot-search ( k,t ) query can be answered by accessing a single index-partition, i.e. the partition corresponding to the time t of the query. However, this query efficiency comes at the cost of consider-able storage explosion (and corresponding increase in indexing time) that results from replication of long-living document versions across multiple partitions.

Interval Tree based partitioning is one alternative that can be to used to alleviate this storage explosion. Inspired from the temporal databases community [20], Interval Tree based partitioning, shown in Figure 1(c), partitions the text index into multi-level hierarchical partitions [9]. A version is made a part of the deepest partition (close to leaf level) whose time-boundaries fully encompass the version X  X  validity interval. The snapshot-search ( k,t ) can thus be answered by querying logarithmic (of total time span) number of partitions that lie in the path-to-root for the query time t .

This index however has significant drawbacks for keyword-only queries due to high disk seek costs resulting from accessing all index-partitions, which are fragmented on disk. Similar drawbacks result for wide time-intervals in temporal-only and (keyword+temporal) queries. To alleviate this, fragments of a term X  X  postings list can be chained together into consecutive disk blocks, which improves sequential access for multiple partitions. However, such techniques rely on the entire archival document collection being available up-front, and without apriori determination of the chaining order, peri-odic index-rebuilding costs become highly expensive. It is not appli-cable in our high data rate real-time ingestion setting.

One recent approach for time-travel text search has focused on partitioning the term index such that the unnecessary reads for each term is minimized. The Staircase technique [1, 2], shown in Fig-ure 1(d), uses a greedy-algorithm to partition postings list entries such that no entry within a partition subsumes another entry. Va-lidity interval v 1 is said to subsume v 2 when (( v 1 ( v 1 .d &gt; v 2 .d )) , where b and d denote birthtime and deathtime re-spectively. In Figure 1(d), D1 subsumes D2 and thereby needs to be separated out into different partitions, whereas {D2, D3, D4} satisfy staircase property.

With an idealized staircase sharding and sorted birthtime order on the posting list entries, it becomes simple to identify the start-ing and ending entries in each shard where the validity interval in-tersects with the query point. However, idealized sharding results in a large number of partitions because of increasing subsumption violations over time which degrades query performance. The hard subsumption constraint can be relaxed using cost-aware merging [1] and incremental sharding using bounded subsumptions [2], which balances the overall disk sequential read versus random seek costs. Because the merging technique is not suited for real-time process-ing, we compare our approach against an implementation of the lat-ter. A limitation with the staircase partitioning in general is that it is specifically optimized for the snapshot-search ( k,t ) and alive-any-search ( k,t min ,t max ) query in mind, and can result in unbounded amount of wasted reads for several query classes that cannot lever-age the birthtime ordered shard layout.
Overcoming the limitations of the prior index partitioning tech-niques is a significant challenge that needs to be addressed. The goal of our work is to design an indexing system that for a wide variety of query classes, minimizes the number of fragmented partitions ac-cessed by the query and also minimizes the amount of wasted reads in each partition. Further, we should be able to do this with mini-mal extra storage costs (i.e. not resorting to replication or different index layouts optimized for each query class). Further, the indexing logic must be applicable in a real-time ingestion setting and must be lightweight enough to cope with high incoming data rates.
This section describes the design of our proposed index partition-ing technique and explains how it supports a wide variety of query classes with minimal storage or indexing overhead.
Our temporal search index is a simple layer on top of the popu-lar text-search engine Lucene [15]. Our temporal search extensions handle the additional logic of maintaining birthtime and deathtime attributes and take document entities through their create, update, and delete life cycle. On receiving an update for a document entity with time-invariant key, our layer issues two API calls to the under-lying indexing engine. First, it issues an update to the deathtime attribute of the latest version (if any), changing it from  X  to the cur-rent time, while keeping the other attributes unchanged. Second, it also issues a create with birthtime attribute set to the current time, deathtime attribute set to the special time  X  and the version X  X  other attributes equal to the incoming content. (a) Query region Figure 3: Typical indexing engines assign recordid s in transaction time
Our design objective is to avoid the limitations of prior works [1, 2, 4, 9] that organize the index layout based on apriori assumptions of specific query access patterns (e.g. snapshot-search ( k,t ) query). We avoid this approach since multi-attribute nested indexes are in-efficient for single-attribute predicates like keyword-only queries as well as for keyword+temporal queries with a wide time-interval. To retain good performance on such queries, our approach is to explic-itly index the birthtime and deathtime attributes and evaluate joint predicate queries by resorting to list unions and intersections on these single-attribute index structures for birthtime , deathtime and keywords.

Lucene indexes numerical attributes (here birthtime and death-time ) using tries [16]. Each trie-node of the multi-level trie-structure represents a contiguous numeric-range, with trie-nodes at the leaf-levels representing smaller ranges, while those towards the root rep-resenting larger ones. By assigning a distinct term to each trie-node, the search-index uses the postings list for a trie-node X  X  term to track all the recordid s whose numeric attribute falls within the trie-node X  X  range.

The single-attribute indexes for each of birthtime , deathtime and keywords can thereby be realized using postings lists. Any query containing keyword and temporal predicates is evaluated via the stan-dard keyword-search query evaluation plan that uses postings list unions and intersections.
Today X  X  text search engines (e.g. Lucene) implement efficient al-gorithms/techniques [3] for intersecting postings lists maintained in sorted recordid order. In particular, they leverage an additional data-structure called skip-list that enables skipping portions of the list that are guaranteed to not be present in at least one other list being in-tersected with. Since no prior work in time-travel text search has explored this potential approach of leveraging list intersections with indexed temporal attributes, we now investigate whether a strawman approach of list intersection on these single-attribute indexes can ef-ficiently answer the joint predicate snapshot-search ( k,t ) query.
The effectiveness of skipping in list intersection algorithms de-pends on the distribution of the values in the lists. Figure 2 depicts this dependence using two scenarios. The first, Scenario-1 , demon-strates that in spite of the size of result set being small, the cost of list intersection is high if the values in the two lists interleave a lot. However, if the overlap between the ranges of values in the lists is low, as shown in Scenario-2 , the cost of list intersection is only pro-portional to the overlap region since the other portions can be easily pruned away. The distribution of recordid in underlying postings lists has a significant impact on the cost of list intersection.
Figure 3 describes how typical search indexes such as Lucene as-sign recordid s to documents. Each document version is depicted as a point, with the point X  X  coordinates corresponding to its birthtime and deathtime , and the point X  X  color/shade indicating its recordid . The current time is denoted as now and the special point  X  on deathtime axis represents un-expired versions.

Although recordid s are assigned in transaction time order, the dis-tribution of recordid s depends on whether the indexing engine uses an append-only or an update-in-place strategy. In update-in-place systems, updates to the attributes of an existing record are processed without changing the underlying recordid assigned to that record. As a result, recordid s are in birthtime order, as shown in Figure 3(b). In contrast, append-only systems treat an update as a delete followed by a create . Thus, the recordid of its previous latest version is marked as deleted and a new document corresponding to the recently ex-pired version is created with a new recordid . As a result, expired document version recordid s are in deathtime order while unexpired version recordid s are in birthtime order, as shown in Figure 3(a).
We will now examine how the underlying recordid assignment af-fects the list intersection cost of processing temporal search queries in our framework. Given that most indexing engines like Lucene adopt the append-only strategy for efficiency reasons, we take this indexing strategy as the default for our explanation below. Note how-ever, that our proposed partitioning technique shows similar benefits for both append-only and update-in-place indexing systems, and is depicted in Figure 7.

Figure 4(a) highlights the region corresponding to the snapshot-search ( k,t ) , which is specified via the temporal predicate (( birth-time  X  t )  X  ( deathtime &gt; t )). Out of all recordid s lying in this re-gion, only a subset of these satisfy the keyword predicate k , and this subset constitutes the result set as shown in Figure 4(g). Figure 4(b) and Figure 4(c) show the trie-node postings lists satisfying the query predicates for the birthtime and deathtime trie respectively, with the different widths being an illustration of these trie-nodes correspond-ing to different levels of the trie. Figure 4(d) shows the postings lists corresponding to keyword k .

To compute this result set, the search index performs list unions and intersections as per the query plan shown in Figure 4(e). The cost of the list intersection (i.e.  X  X ND" at the root of query plan) is high since this scenario is similar to that of Scenario-1 in Figure 2, wherein each of these three lists  X  birthtime , deathtime and key-word  X  span the entire range of recordid s themselves, as depicted in Figure 4(f). As a consequence, skipping will not be effective, and all postings lists will be scanned from the beginning to the end, resulting in lots of wasted work.
Our key insight is that we can exploit the temporal encoding in the recordid s (i.e. fact that they are assigned in transaction time or-der), to partition the entire collection into few disjoint partitions to reduce the overlap between the postings list corresponding to birth-time , deathtime and keyword list, which will then translate to im-proving the skipping efficiency in the underlying list intersection.
To quantify the effectiveness of skipping/pruning, let us exam-ine the interaction of the list intersection algorithm with the key-word postings list (K) in the snapshot-search ( k,t ) . In particular, we categorize the recordid s in the keyword X  X  postings list into 3 parts  X  (a) Result (R) -This is the set of recordid s whose tempo-ral attributes satisfy the temporal component of the query and thus are in the result set, (b) Pruned (P) -This is the set of recordid s whose temporal attributes do not satisfy the temporal predicates, but get easily pruned when doing the list intersection with the tempo-ral ( birthtime / deathtime ) postings lists since they correspond to a non-overlapping region of Figure 2, and (c) Wasted (W) -This is essentially (K-R-P) and represents recordid s that do not satisfy the temporal attributes of the query but were most likely examined by the list intersection algorithm since they belong to an overlapping region of Figure 2.
 Our first intermediate partitioning step, referred to as LS+AS , simply divides the recordid space into two partitions; latest for current or unexpired versions which have deathtime set to  X  and archival for archived versions which have a finite deathtime attribute, as shown in Figure 5(a). The final result is the union of versions satisfying the snapshot-search ( k,t ) in the two partitions. Note that since these partitions are disjoint, every query can simply aggregate the results from the two partitions without worrying about removal of dupli-cates.

This simple partitioning technique lowers query cost by reducing the overlap between the birthtime (B) , deathtime (D) and keyword (K) list. We will explain this using Figure 5(b). As described be-fore, archival versions of a document have recordid s in deathtime order, whereas unexpired (latest) versions have recordid s in birth-time order. Lets say the recordid assigned at time t is rid(t) . Con-sequently, in the latest collection, the recordid s in the birthtime list will range up to a maximum of rid(t). As a consequence, in spite of the deathtime and keyword list spanning the entire range of recor-did s, large portions of these lists having recordid values greater than rid(t) can be trivially pruned by the list intersection algorithm (since they correspond to Scenario-2 of Figure 2. Similarly, in the archival collection, the recordid s in the deathtime list will have a minimum value of rid(t). Consequently, in the archival collection, large por-tions of the birthtime and keyword postings list, having values less than rid(t) can be pruned away. The resulting benefits of pruning away these non-overlapping portions is depicted in Figure 5(c). The areas marked with P are the portions additionally pruned as com-pared to the basic unpartitioned approach where the entire region of the recordid space was examined by the list-intersection algorithm. As shown, the P1 region pruning benefits the deathtime and key-word posting lists, whereas P2 region pruning benefits the birthtime and keyword posting lists.
 Our next partitioning step further sub-divides the archival partition based on the lifespan of document versions. This leads to more ef-fective pruning and further reduction of the wasted (W) work, as il-lustrated in Figure 6. Let us first consider dividing the archival space into two partitions AS [0 ,L 0 ) and AS [ L 0 ,  X  ) as shown in Figure 6(a). AS [0 ,L 0 ) comprises of short-lived document versions with an maxi-mum lifespan L 0 , i.e. lifespan = ( deathtime  X  birthtime ) &lt; L AS [ L 0 ,  X  ) comprises the remaining expired document versions with finite lifespan &gt; L 0 . AS [0 ,L 0 ) is the region between the two par-allel lines deathtime = birthtime and deathtime = birthtime + L As before, assume that the recordid assigned at time t is rid(t) . In addition, assume that the recordid assigned at time ( t + L rid(t+ L 0 ) . Consequently, in partition AS [0 ,L 0 ) the birthtime list will range up to a maximum of rid(t+ L the recordid s in the deathtime list will have a minimum of rid(t), as shown in Figure 6(b). Consequently, in the AS [0 ,L 0 the overlap between the lists is a small region of the recordid space in interval [rid(t), rid(t+ L 0 )], and this reduced overlap translates to large portions of these lists getting pruned away. The pruned areas shown in Figure 6(c) are more than those in the LS+AS strategy (Fig-ure 5(c)). Figure 7: Shape/Area/Density of wasted work in our lifespan partitioning approach
Extending the above idea, we can further reduce the amount of wasted work by further partitioning AS [ L 0 ,  X  ) in the same way. Spe-cifically, we partition the archival space into a logarithmic (i.e. log-arithmic of maximum lifespan or operational time-horizon of the in-dexing system) number of partitions based on the lifespans of ver-sions. The logarithmic archival collections AS [0 ,L 0 ) , AS maximum lifespan bounds ( L 0 , L 1 , ... , L n  X  1 ). The i -th lifespan-partition is denoted as AS [ l,u ) , where l = L i  X  1 and u = L partition comprises of versions with lifespans in the interval [ l,u ) . Our justification of choosing such a scheme is inspired by our obser-vation that several real-world datasets of temporally evolving data exhibit a heavy-tail distribution of lifespans. Under such a heavy-tail distributions, having exponentially larger lifespan intervals lead to partitions having similar number of documents. This enables a good trade-off between minimizing the number of index-partitions, yet minimizing the amount of wasted work.

Since lifespan-based partitioning relies on document versions ar-riving in transaction time order to efficiently answer queries, index-ing documents out of order could affect query performance. How-ever, arrival order does not affect the correctness of query results. In-order arrival could be violated in two scenarios  X  skew in ar-rival of data from multiple sources and batch upload of historical data. Slight skew does not affect query performance since recordid s assigned around a time period will still be close to each other and temporally close document versions will map to the same or nearby disk blocks. Batch uploads on the other hand will not exhibit such a favorable recordid assignment. Our solution is to simply maintain a separate partition for batch data, and query this partition in addi-tion to the others so that batch uploaded data does not affect query performance of the rest of the partitions.
 Figure 7 shows the unpruned/wasted regions (i.e. W regions) when performing list intersection on our proposed partitioning approach. While the location of the wasted work regions in the birthtime / deathtime two-dimensional space differ between the append-only and the update-in-place indexing systems, the shape and area of re-gions are identical.

Modulo boundary effects that show up when the query time is close to the start of the historic time span, the wasted work area W is smaller than that of the result set R in all partitions. Though W increases exponentially as the partition size increases, the heavy-tailed nature of lifespan density distribution ensures that the actual amount of wasted work done in terms of CPU computation or disk I/O, cost ( W ) = area ( W )  X  density ( W ) , is approximately the same across all partitions.
 Table 2 shows the average sizes of wasted work W and result sets R across different queries for various real and synthetic datasets. In order to get normalized results, we constructed datasets of equal sizes by sampling 100K document versions from the full datasets. We further removed latest state versions since wasted work in the lat-est state region is zero, and thereby only analyze the cost of queries made to the archival region. We issued snapshot-search ( k,t ) queries for a keyword with 1% selectivity for 100 evenly spaced query times t across the entire historic time span. The table reports the average wasted work W and average result size R across these 100 queries. The ratio W/R ranges from 0.34 to 1.14 across datasets showing that average cost per query ( W + R ) is around twice the optimal cost (i.e. result set size R ).
 Our lifespan-based partitioning ensures that we can answer joint predicate queries efficiently in spite of using single-attribute indexes. Our simple scheme has the following good properties  X  (a) few par-titions -Having logarithmic number of partitions ensures low over-head of seek costs from accessing multiple index fragments in dif-ferent partitions, (b) disjoint partitions -Having disjoint partitions implies no replication and thereby minimal extra storage overheads, and also enables trivial aggregation of results across all partitions, (c) lightweight real-time aware indexing logic -Since the decision of which partition a version should belong to is a function of only the version X  X  self-attributes (i.e. birthtime / deathtime ), our indexing logic does not require the entire archival collection to be available upfront, and neither does it require periodic re-organization of the index, and is thereby a perfect fit for the real-time ingestion setting.
As described in Section 3, our temporal search index is a layer on top of the popular open-source Lucene [15] framework. This section describes the specifics of our index management.
Our approach for implementing index-partitioning is inspired by how Lucene implements tries in order to support range queries on indexed numeric attributes [16]. On top of its default postings list ab-straction, Lucene prefixes every numeric attribute X  X  field name with a prefix value that corresponds to the trie-node that the numeric value should get indexed into. This results in separate postings lists for each trie-node, thereby emulating the desired effect of having a trie index.

This allows us to implement not only our proposed lifespan par-titioning approach, but also the other comparative approaches as well. Our temporal-search layer prepends a partition-prefix to all field names of all document versions as they arrive into the system, which ensures that document versions get indexed in their respective partitions. At query time, the temporal search layer rewrites queries by prepending partition prefixes to field names and issuing queries to various partitions, as applicable for a particular query.
Efficient algorithms [3] to evaluate boolean queries involving con-junctions and disjunctions of attributes process all postings lists in the query tree in parallel, instead of first pre-computing the results of intermediate nodes. While Lucene evaluated non-numeric keyword queries using the former approach, we observed, to our surprise, that Lucene computed intermediate results for numeric range queries and was inefficient as a result. Hence, we changed Lucene X  X  numeric query evaluation plan to also process postings lists in parallel, thus making it as efficient as keyword search queries.
We additionally instrumented portions of the Lucene source-code to report the number of Lucene blocks accessed during list intersec-tion and union operations over postings lists. The skip-list data struc-ture in Lucene operates at the granularity of a Lucene block consist-ing of 128 recordid s. This instrumentation enabled us to verify that the end-to-end execution time of the query was indeed proportional to the number of Lucene blocks accessed, as reported in our evalua-tion. In this section, we evaluate our lifespan-based partitioning (i.e. LS+LogAS ) approach of supporting time-travel text search against other approaches in terms of indexing throughput, storage costs, and query response time.
Table 3 describes the datasets we used. We use two versions of the openly available Wikipedia corpus of articles [21]: Wikipedia(s) and Wikipedia(l) . The former is a 17 Million document revision sub-set whose uncompressed textual content totals 0.7 TB, and we index the full content of these document revisions (i.e. versions). With Wikipedia(l) , which comprises the entire 578M revision history of Wikipedia, we synthetically generate keywords with different pop-ularities for each version (i.e. revision) and index these synthetic keywords in order to perform a large-scale controlled study. Linux(r) dataset is based on the git revision history of the Linux kernel [13], and is replicated to achieve the same size as Wikipedia(l). On ob-serving that the Wikipedia and Linux datasets follow a heavy-tailed distribution (see Figure 8) with respect to their version lifespans  X  we additionally experimented with popular heavy-tailed distributions -Log-normal and Pareto , using the median lifespan of the Wikipedia dataset.
To experiment with real-world scenarios, we used the Wikipedia(s) dataset and created the query workload for this dataset by sampling 10,000 queries from the AOL search logs with clicks to the Wikipedia domain. For each query, we extract its keywords and randomly as-sociate it with different query classes, and also assign a temporal parameter to the query. We used a 12-core 2.40 GHz Intel Xeon E5645 server equipped with 32 GB of memory and a 2 TB SATA disk. At query time, we intentionally cap the OS physical memory (via boot settings) to 2 GB to ensure that queries are disk-bound.
In the absence of a real-world query workload for other datasets (Linux, Log-normal and Pareto), we indexed synthetic words with different term popularities and constructed a query workload by var-ying different configurable parameters such as the number of search terms, the query term X  X  popularity (i.e. selectivity), and the temporal parameters. We ensured that we indexed a large number of terms per term-popularity at indexing time, so that successive queries issued with different randomly chosen terms of the desired term-popularity would result in fetching the term postings lists from disk rather than from memory. These experiments on all large (i.e. 578M) datasets were performed on cloud VMs with 2 GB of memory and connected to SAN disk drives with total capacity 8 TB.
Here, we highlight the relevant implementation details and con-figuration settings used for the approaches we compared against.
LS+LogAS Our technique is configured to use a leaf level parti-tion having a lifespan of 1 day. With each successive level, the lifes-pan range increases by a factor of 10x, i.e. partition ranges being [0, 1 day), [1, 10 days) and so on.
 Figure 8: Lifespan distribution, as a fraction of total document ver-
Optimized Lucene is the strawman approach described in Sec-tion 3.3. The optimization we did over vanilla Lucene, is the fix we made (described in Section 4.2) to Lucene X  X  query evaluation plan for numeric attributes, which reduced the query execution times by a factor of  X  5x over vanilla Lucene.

Interval Tree is described in Section 2.4. We configured it with a partitioning granularity of 1 day at leaf level, and an arity of 10, similar to LS+LogAS approach. Our rational for choosing 1 day as the leaf level lifespan is to have sufficient fine-grained partitioning to cater to IT operational analytics use-cases where data volumes of TB / day are fairly common.

Optimized Staircase This is our adaptation 2 of the Staircase tech-nique described in Section 2.4, in order to cope with high data rates. The number of partitions created by their algorithm is inversely pro-portional to the size of memory buffer per term-shard (  X  ). At high data rates, the aggregate memory requirement of per-term buffers outgrows the machine X  X  available memory, requiring periodic partial flushing of memory buffer entries. This effectively leads to operat-ing at lower values of  X  , which in turn results in creation of a large number of partitions. To overcome this limitation, we do sharding at document-level , instead of a term-level. This completely eliminates the need for periodic buffer flushing, and also substantially (more than a factor of 10x) reduces indexing time since their algorithm is invoked on a per-document basis, instead of once per term in the document. We configured  X  = 1000 , the recommended setting in [2] for disk-bound settings.
 Replicated Time-tiers is described in Section 2.4. We configured Replicated Time-tiers with the same partitioning granularity of 1 day as with the other approaches. Table 4 compares the indexing performance across techniques.
Indexing Throughput: We used the Wikipedia(s) to measure in-dexing throughput in terms of rate of indexing of actual raw content. The lightweight indexing logic of our LS+LogAS results in good in-dexing throughput of 2.48 TB / day, adding only 15.6% overhead compared to Optimized Lucene . In contrast, Replicated Time-tiers
We verified the need of this adaptation in our high data rate setting with the first author of [2]. Table 4: Indexing throughput (T) in TB / day and the size of the index has an order of magnitude (10x) greater overhead, as index entries have to replicated into multiple partitions, making it unsuitable in high data rate settings.

Index Size: Our LS+LogAS approach only has 1.1% storage over-heads as compared to Optimized Lucene . As expected, Replicated Time-tiers has an order of magnitude more storage overhead (10x) compared to all the other approaches, and is completely impracti-cal for high data rate settings. We thereby do not experiment with Replicated Time-tiers further on the larger 578M datasets.
We measure the performance of different index partitioning tech-niques when subjected to the different query classes shown in Ta-ble 1. For each query class, we performed controlled experiments by varying the historic point in time t , the query time-interval, the num-ber of keywords, and the selectivity (or popularity) of each individual keyword in the query. For brevity reasons, we present results for the following default setup  X  t is approximately midway into the his-toric past, time-interval is 1 month, individual keyword selectivity is 1%, and search query comprises 3 keywords. We experimented with all the four large 578M datasets (i.e. Wikipedia(l), Linux(r), Log-normal, and Pareto). For lack of space, we omit the depiction of Log-normal dataset results which were found to be very similar to the Wikipedia(l) dataset.

The overall end-to-end execution time of a query (reported in Fig-ure 10) is a function of the number of partitions that the query needs to access (reported in Table 5) and the aggregate amount of index entries or Lucene blocks read across all the partitions (reported in Figure 9). We refer the reader to these charts to observe the relative query performance of different index partitioning techniques. Table 5: The number of partitions accessed as a function of index-
Firstly, observe that across all query classes, our LS+LogAS ap-proach, with respect to overall execution time, is either the best per-forming technique, or comes very close to the best performing tech-nique. In contrast, each of the other techniques performs badly for query classes that they are not designed for, as discussed below.
Optimized Lucene , which uses single-attribute index structures, has severely bad performance on the joint predicate snapshot , alive-any-search , and invariant-search queries because of very inefficient list intersection performance.

Interval Tree  X  X  performance deteriorates significantly with increa-sed time-range in its queries or for the keywordonly-search query that effectively spans entire time range. This is because it needs to ac-cess increased number of index partitions, resulting in high disk seek costs (5662 partitions for keywordonly-search and 35 partitions for month queries). Further, since Interval Tree uses payload based post-filtering within each partition, it reads a similar number of Lucene blocks within each partition irrespective of overall query selectivity, and this value can be high at partitions closer to the root level.
Optimized Staircase technique suffers from the creation of lots of partitions (  X  150) on high popularity terms and larger datasets, which is a consequence of their greedy incremental sharding algo-rithm. Even for query classes that can leverage its birthtime ordered staircase layout within each partition ( snapshot-search , alive-any-search , born-search , positivediff-search , and transient-search ), the increased seek costs result in overall increased execution times. To add to this, query classes that cannot leverage their specific layout ( invariant-search , died-search and negativediff-search ), incur a high amount of wasted reads in each partition.
Figure 11 compares the execution time of queries from the AOL search logs on the 17M Wikipedia(s) dataset where we indexed ac-tual content of Wikipedia articles. We observe that the trends with query response times are similar to the previous controlled expe-riments, except that the relative gap between Optimized Staircase and LS+LogAS is lower. This is the expected behaviour of Opti-mized Staircase , wherein the number of partitions created is rela-tively much lesser for small datasets (here, 11 partitions for the 17M dataset) as compared to large datasets (144 partitions for the 578M dataset)  X  resulting in reduced disk seek costs for small datasets.
We organize the related work into the following categories.
The focus of prior works in time-travel text search has been on de-vising index-partitioning techniques that can effectively prune away portions of the term postings-lists that do not satisfy the temporal predicate. Section 2.4 describes the limitations of prior works in detail, and is not summarized here for space reasons. Figure 11: Query Performance on AOL Search Logs executed over the 17M Wikipedia(s) index
In addition to index-partitioning, a few works have explored ap-proximate ranking techniques to reduce index storage costs by ex-ploiting the typically high similarity in content that exists between successive temporal versions of the same document. These approa-ches typically index only the deltas [10, 11] or index a single repre-sentative version for a series of largely similar successive versions (i.e. temporal coalescing [4]). We aim to integrate with these works in the future.
Works in temporal relational databases deal with management of temporally evolving data (see exhaustive survey by Salzberg and Tsotras [18]). Our work follows the notion of transaction-time model that was introduced in temporal databases, where changes can be made only to the latest version of a record. These versions (records) are identified by a primary key and the transaction-time is used to create the implicit validity intervals [ birthtime , deathtime ). In contrast to valid or bi-temporal time models, where validity intervals need not be related to create/update time, the notion of transaction-time directly applies in our IT operational analytics use case with real-time indexing. This notion enables us to design efficient update-friendly index structures.

The primary focus of work in this community is with the on-disk layout of the physical records and their associated primary-index for efficient retrieval via primary-key and/or time range pred-icates. The on-disk layouts that are typically aimed at supporting queries with pure time range predicates (e.g. append-only-tree [19], archival-time-index [20]), cannot efficiently answer queries contain-ing a combination of time and primary-key range predicates. On the contrary, the more complex on-disk record layouts aimed at support-ing a combination of time and primary-key range predicates (e.g. time-split B-tree index [14], nested ST-tree [7]), exhibit bad perfor-mance on queries with pure time predicates.

For a search-index application like ours, the focus is on efficiently supporting queries with multiple secondary attribute predicates, and our techniques optimize the on-disk layout of the secondary-indexes (i.e. postings lists) themselves. This stands in contrast with the work done in temporal databases, where the primary focus is on the layout of physical records. Although one might envision customizing these primary-index structures for use as secondary-index (e.g. adapting the nested ST-tree into AT-tree [7]), this community, to the best of our knowledge, did not explore the problem of how to perform ef-ficient intersections on the results obtained from secondary indexes, across a wide range of query classes.
One might think that treating birthtime and deathtime together as (lat, long) location tuple in geo-space, would enable us to leverage a wide range of geo-text indexes (see [5] for an exhaustive survey). Geo-text indexes capture geo-tagged objects containing both text descriptions and geo-locations, and answer queries involving spa-tial predicates (lat/long ranges) and text-keyword predicates. These approaches involve combinations of a spatial index (typically R-tree [8] or grid-file [17] based), along with a text index (postings-lists or bitmap based). Spatial and text index are coupled either loosely [23], or tightly [22] via aggregated structures in internal nodes to enable more pruning.

These index structures, however, are not applicable in our set-ting. While effective for certain classes of queries such as bounding-box queries in spatial dimension, these indexes cannot answer the snapshot ( t ) query, which has open intervals along each dimension of birthtime and deathtime . Second, they assume that queries con-tain both spatial and textual predicates, and thereby are designed to only cater to joint-predicate (spatial+text) query. These nested in-dex structures are not designed to support single-attribute queries (spatial-only or text-only). For such queries they suffer from high I/O costs resulting from disk seeks to access the multiple fragmented index-partitions at the leaves of the nested index.
A few approaches have improved the performance of list intersec-tion by employing intelligent recordid (or doc-id) assignment that makes skipping more effective. SFC-Quad [6] technique proposes assigning doc-ids to geo-objects based on their positions on a space-filling curve (Z-curve) spanning a spatial region, and then employ standard postings-list intersection techniques to answer geo-textual queries. The intuition behind this comes from the fact that geo-objects that are close by in the geographic space are clustered to-gether in the doc-id space of the postings list. The work by Kane et. al [12] assigns doc-ids to documents based on content-similarity, thereby resulting in documents with similar terms getting assigned doc-ids that are closer in the doc-id space. However, these tech-niques are practical only in a setting where the entire collection is known in advance, since maintaining the postings-list in a sorted or-der as per the desired doc-id assignment/ordering scheme conflicts with the efficient append-only indexing operations on postings lists.
This paper presents lifespan-based partitioning of index structures to efficiently address the problem of time-travel text searches over massive datasets. Our scheme leverages temporal information em-bedded in the underlying recordid assignment and partitions the doc-ument space into a small number of disjoint partitions based on the lifespan of document versions. Rather than having an index lay-out that is optimized for a specific type of query, our solution uses list intersection on single-attribute indexes of keywords and tempo-ral attributes to efficiently support a wide range of (keyword + tem-poral) queries. Our proposed technique significantly improves the list intersection skipping ability, and can efficiently narrow down to document versions that satisfy the temporal search predicates. As a result, our scheme supports a wide variety of query classes with per-formance comparable to the best solution for each query class, while simultaneously having an indexing logic that is lightweight enough to support high data rates in a real-time ingestion setting with negli-gible storage overheads.
We would like to thank Vasanth Bala for the motivating use-case of supporting keyword search on the historical state of the datacen-ter, Narendran Sachindran for discussions related to the design of our approach, Shai Erera for discussions related to internal implementa-tion details of Apache Lucene, and Avishek Anand for clarifying our questions with respect to the related work on Staircase partition-ing [1, 2]. We would also like to thank the anonymous reviewers for their valuable feedback. [1] A NAND , A., B EDATHUR , S., B ERBERICH , K., AND [2] A NAND , A., B EDATHUR , S., B ERBERICH , K., AND [3] B ARBAY , J., L OPEZ -O RTIZ , A., L U , T., AND S ALINGER [4] B ERBERICH , K., B EDATHUR , S., N EUMANN , T., AND [5] C HEN , L., C ONG , G., J ENSEN , C. S., AND W U , D. Spatial [6] C HRISTOFORAKI , M., H E , J., D IMOPOULOS , C., [7] G UNADHI , H., AND S EGEV , A. Efficient indexing methods [8] G UTTMAN , A. A dynamic index structure for spatial [9] H E , J., AND S UEL , T. Faster temporal range queries over [10] H E , J., Y AN , H., AND S UEL , T. Compact full-text indexing [11] H E , J., Z ENG , J., AND S UEL , T. Improved index [12] K ANE , A., AND T OMPA , F. W. Skewed partial bitvectors for [13] Linux source code dataset. https://www.kernel.org/ . [14] L OMET , D., AND S ALZBERG , B. Access methods for [15] Lucene. www.lucene.apache.org . [16] Lucene numeric range query. http://www.slideshare. [18] S ALZBERG , B., AND T SOTRAS , V. J. Comparison of access [19] S EGEV , A., AND G UNADHI , H. Event-join optimization in [20] V ERMA , R., AND V ARMAN , P. Efficient archival time index: [21] Wikipedia dataset. [22] W U , D., C ONG , G., AND J ENSEN , C. S. A framework for [23] Z HOU , Y., X IE , X., W ANG , C., G ONG , Y., AND M [24] Z OBEL , J., AND M OFFAT , A. Inverted files for text search
