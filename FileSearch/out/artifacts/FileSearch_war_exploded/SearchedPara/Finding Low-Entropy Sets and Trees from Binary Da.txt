 The discovery of subsets with special properties from bi-nary data has been one of the key themes in pattern dis-covery. Pattern classes such as frequent itemsets stress the co-occurrence of the value 1 in the data. While this choice makes sense in the context of sparse binary data, it disre-gards potentially interesting subsets of attributes that have some other type of dependency structure.

We consider the problem of finding all subsets of attributes that have low complexity. The complexity is measured by either the entropy of the projection of the data on the sub-set, or the entropy of the data for the subset when modeled using a Bayesian tree, with downward or upward pointing edges. We show that the entropy measure on sets has a monotonicity property, and thus a levelwise approach can find all low-entropy itemsets. We also show that the tree-based measures are bounded above by the entropy of the cor-responding itemset, allowing similar algorithms to be used for finding low-entropy trees. We describe algorithms for finding all subsets satisfying an entropy condition. We give an extensive empirical evaluation of the performance of the methods both on synthetic and on real data. We also discuss the search for high-entropy subsets and the computation of the Vapnik-Chervonenkis dimension of the data.
 Categories and Subject Descriptors: H.2.8 [Database Management]: Database applications X  data mining General Terms: Algorithms, Experimentation, Theory Keywords: Local Models, Pattern Discovery
Pattern discovery is the subarea of data mining concerned with finding combinations of variables that are in some sense locally interesting. The archetypical example is the pattern class of frequent itemsets [2], and it sometimes seems that this is the only such class that attracts significant research. Our aim in this paper is to chart one part of the space of more general pattern classes in binary data. Perhaps the most obvious way to generalize frequent itemsets is to al-low not only combinations of variables that have the value 1 in the same records but also combinations whose distribu-tion of values is significantly skewed in other ways. This leads naturally to examining the entropy of the variables: if the combination of variables has a skewed joint distribution, their joint entropy is low. Finding all low-entropy itemsets is easy because entropy is monotonic with respect to set inclu-sion: adding a variable to a set cannot decrease the entropy.
While the property of having low entropy is interesting in itself, it would often be beneficial to have an explanation of how the variables in the set are connected to each other. This leads us to examine two other pattern classes, which we call U-trees and D-trees (seen as Bayes networks, the trees have edges going up and down, respectively). U-trees are a subclass of tree-structured Bayes networks. We give an algorithm for mining U-trees and show by experiments that it is feasible for moderate-sized data. On the other hand, D-trees are also Bayes nets, but simpler than U-trees as every node has at most one parent. The best D-tree can be com-puted fast even for very large data sets. They have also been called Bayes trees, Markov trees [21], dependence trees [6], and Chow-Liu trees [18], but an important distinction is that we do not attempt to model the complete joint distribution but to find interesting local patterns. In other words, a D-tree is the Chow-Liu tree for some subset of the variables.
Thus there is a continuum of local pattern classes starting from frequent itemsets, which are inflexible but for which a multitude of fast mining algorithms has been developed. D-trees are more flexible and still fairly easy to find; U-trees are still more flexible, but our current algorithms are not as efficient as those for the less flexible classes. At the high end of flexibility are arbitrary low-entropy itemsets, which can capture any kind of interaction, and somewhat para-doxically can be found efficiently using an algorithm similar to frequent itemset mining algorithms. However, while the algorithm is efficient, the size of the output can be very large, which motivates the more direct algorithms for find-ing low-entropy trees. A drawback to low-entropy itemsets compared to the less flexible trees is that they are not as easily interpreted. From another viewpoint, there is a con-tinuum from frequent itemsets via fragments of order [10] and the trees of [14] to U-trees, where each pattern class has more structure than the previous one. Table 1: The marginal distributions related to the tree in Figure 1. Legend: Comp. = Theory of Com-putation; Prob. = Probability Theory; Wri. = Sci-entific Writing; Mat. = Maturity Test.

Figure 1 shows an example U-tree found from course en-rolment data at the University of Helsinki Department of Computer Science. The tree is the lowest-entropy 5-node U-tree found in one of the experiments described later. All courses in the tree are required for graduating, and the tree shows that students have completed different combinations of them. Table 1 shows the marginal distributions whose entropies are embodied in the tree. We see that the two the-oretical courses are correlated quite strongly with each other and less strongly with the other courses. Scientific Writing and Maturity Test 1 are even more strongly correlated, which makes sense since both are related to the final phase of writ-ing the Bachelor X  X  thesis. Software Engineering Project is a demanding practical course, which theoretically minded stu-dents may leave for last, while practical students often post-pone taking the theoretical courses perceived as difficult.
Related to low-entropy itemsets are of course high-entropy itemsets, and itemsets having high Vapnik-Chervonenkis di-
The Maturity Test is required in the Finnish system to complete a Bachelor X  X  degree after submitting the thesis; the candidate has to answer questions regarding the subject of the thesis in his or her mother tongue. mension; we discuss these pattern classes briefly. Since the entropy function is decreasing with respect to itemset in-clusion, high-entropy itemsets cannot be found with the same approach as low-entropy ones, and in fact the highest-entropy itemset is always the one containing all items. Thus we introduce two kinds of scaling for entropy, defining an itemset to be interesting either if its entropy is high among all itemsets of the same size, or if its entropy is high when compared to the entropies of its singleton subsets. We show that these properties are weakly monotonic: if an item-set X has high scaled entropy, then it has at least one sub-set X \{ A } having even higher scaled entropy. This again allows a levelwise algorithm to be used, albeit with less effi-cient pruning.

The rest of this paper is structured as follows. We in-troduce entropy and discuss finding low-entropy sets in Sec-tion 2. Then we move to low-entropy trees in Section 3, and address high-entropy sets and sets of high Vapnik-Chervo-nenkis dimension in Section 4. Section 5 describes extensive experiments, Section 6 discusses related work, and Section 7 is a brief conclusion.
We start by defining some notation and terminology. A 0/1 dataset D is an n  X  m binary matrix. The columns are often called items and the set of all items is denoted by I . A set of items is called an itemset. We denote items by A,B,... and itemsets by X,Y,Z,... . We omit the braces around singleton sets, e.g., we write A instead of { A } . We also follow the convention in information theory of denoting set union by a comma within the parentheses of entropy and information functions; e.g., H ( X,Y ) is the entropy of the random variable X  X  Y , and I ( X,Y ; Z ) is the mutual information between the two random variables X  X  Y and Z .
We denote by  X  X the set { 0 , 1 } | X | of all 0/1 vectors of length | X | . For a vector ~x  X   X  X , the probability P ( X = ~x ) is the fraction of rows whose projection onto X is equal to ~x . We sometimes use the shorthand P ( ~x ) to denote P ( X = ~x ) . The entropy of an itemset X in D is All logarithms are in base 2, and 0 log 0 = 0 by convention. We denote H ( X ) = H ( D ,X ) , leaving out the D when it is clear from the context. Entropy can be seen as a measure of how surprising the outcome is likely to be when we select a random vector ~x  X   X  X . Intuitively, entropy is increased by outcomes that are unlikely, because then  X  log P ( X = ~x ) is high; but since these contributions are weighted by the probabilities, a single unlikely outcome contributes little, and entropy is only maximized when each outcome is un-likely. Indeed, a simple application of Lagrange multipliers shows that the entropy function H ( X ) has its maximum value when all probabilities P ( X = ~x ) are equal; in this case the entropy is log |  X  X | = | X | .

Entropy is small when the most outcome are not surpris-ing. The minimum value H ( X ) = 0 is achieved when there is one value ~x 1 that is always the outcome, i.e., when P ( X = ~x ) = 1 . This can be contrasted with frequent itemsets: an itemset X has maximal frequency if P ( X = ~ 1) = 1 . Choos-ing ~x 1 = ~ 1 shows that then entropy is also minimized. More generally, high frequency implies low entropy, but not neces-sarily vice versa. This motivates the following generalization of frequent itemsets:
Definition 1. Given an entropy threshold , an item-set X is a low-entropy set in D if H ( X )  X  .

Low-entropy sets have a monotonicity property like that of frequent itemsets, which allows us to use e.g. the levelwise search [3] to find low-entropy sets.
 Proposition 2. For all datasets D , for all A  X  X and X 6 =  X  .

Proof. H ( X ) = H ( X \ A ) + H ( A | X \ A )  X  H ( X \ A ) with equality holding only when H ( A | X \ A ) = 0 , i.e., when there is a functional dependency X \ A  X  A . In the proof, we used the concept of conditional entropy: H ( X | Y ) is defined as H ( X,Y )  X  H ( Y ) , and it measures the uncertainty about X when Y is known. We used the fact that conditional entropy is always nonnegative. For the proof of this fact and other basic properties of entropy, the reader is referred to e.g. [8, Chapter 2].

Proposition 2 implies that low-entropy sets can be found by using a levelwise approach. One seeming complication in mining low-entropy sets is that the entropy H ( X ) is a sum over all possible combinations of values that the attributes in X can take, which involves 2 | X | different values. However, because of the convention 0 log 0 = 0 used in the definition of entropy, we only need to count those value combinations that appear in the data. Thus the database pass of the levelwise algorithm can be done using O ( c`n log n ) auxiliary space, where c is the number of candidate itemsets being examined, ` is the size of the candidates, and n is the number of records in the data. For each candidate X , we keep track of the combinations seen and a count for each combination; there are at most n combinations, each requiring space O ( ` ) , and each count takes O (log n ) space. Further savings can be obtained by storing the combinations in a binary tree. If the data fits into main memory, then the entropy computation can easily be performed individually for each itemset.
A potential drawback of mining low-entropy itemsets is that the sets do not have any structure that would help in interpretation. In this section we consider replacing itemsets by two kinds of tree patterns; the patterns impose a model on the attribute sets, and it is the entropy of the model that we seek to minimize. We call the tree patterns D-and U-trees; D-trees can be seen as tree-structured Bayesian networks where the edges are directed away from the root, i.e., down in the usual way of drawing trees in computer science. Correspondingly, in U-trees the edges are directed up, towards the root.

Both kinds of trees are formed by imposing a tree struc-ture on some set of attributes, and the differences lie in the semantics of the structure, i.e., how the entropy of the tree is defined. Formally, a tree T =  X  A,T 1 ,...,T k  X  consists of a root attribute A  X  X  and k  X  0 subtrees T 1 ,...,T k , each of which is a tree. We will only be interested in trees in which no attribute appears more than once.
 We define the entropy of a D-tree T =  X  A,T 1 ,...,T k  X  as where parent( T ) denotes the attribute at the root of the tree whose subtree T is. If T has no parent, we define H ( A | parent( T )) = H ( A ) . Thus we start summing entropies from the root, and for child nodes always condition the entropy on the parent node, but not on further ancestors. This implies that when we seek low-entropy D-trees, we will prefer trees where each child node is as closely determined by its parent as possible. For example, consider Figure 2. The left-hand tree has entropy and the right hand tree has entropy Therefore the left-hand tree is preferred if knowing A reduces one X  X  uncertainty about C more than does knowing B , and vice versa. A D-tree can be viewed as a Bayes network in which the edges are directed away from the root node.
Definition 3. Given an entropy threshold , a tree T is a low-entropy D-tree in D if H D ( T )  X  . The set of low-entropy D-trees is denoted by TP D ( D , ) .

For a U-tree T =  X  A,T 1 ,...,T k  X  , we define the entropy as follows. Let T j =  X  A j ,...  X  , i.e., denote by A j the attribute at the root of T j . Then That is, each node that has children contributes the entropy of the root attribute conditioned on (the joint distribution of) all the child attributes. Leaf nodes contribute the un-conditional entropy (i.e., H ( A | X  ) = H ( A ) ).
As an example of U-trees, consider Figure 3. The left-hand tree has entropy H ( A | B,C ) + H ( B ) + H ( C ) and the right hand tree has entropy H ( A | B ) + H ( B | C ) + H ( C ) . The difference between the entropies, is positive or negative depending on whether adding C as a condition in H ( A | B ) or H ( B ) gives more information.
In the same way that low-entropy itemsets give little infor-mation about the interconnections between items, U-trees where the nodes have many children pointing to them do not really explain the interconnections: they simply say that H ( X | A 1 ,...,A k ) is small for a possibly large set of child attributes A 1 ,...,A k . Therefore we add a parameter  X  to control the branching of the tree.

Definition 4. Given an entropy threshold and a branch-ing limit  X  , a tree T is a low-entropy U-tree in D if H U and each node in T has at most  X  child nodes. The set of all low-entropy U-trees in D is denoted by TP U ( D ,, X  ) .
The following result shows that the tree-structured pat-terns are indeed less flexible than the arbitrary low-entropy itemsets of Section 2 in the sense that any low-entropy tree is necessarily a low-entropy itemset.

Proposition 5. Let X be an itemset. If T is a D-tree for X , then H ( X )  X  H D ( T ) . If T is a U-tree for X , then H ( X )  X  H U ( T ) .

Proof. The tree T imposes a partial order on the vari-ables in X : A  X  B if there is a directed path from A to B in T . List the elements of X in any linearization of this partial order: X = { A 1 ,A 2 ,...,A n } , where i &lt; j whenever A i  X  A j . Denote by pred( A j ) the set of at-tributes in X from which there is an arc to A j . Now we can write and Because pred( A j )  X  { A 1 ,...,A j  X  1 } , every term on the right-hand side of (1) is less than or equal to the corre-sponding term on the right-hand side of (2).

This proposition implies also that one possible way to mine low-entropy trees is to mine first the low-entropy item-sets using e.g. the levelwise search and then fit tree struc-tures into each itemset.

Notice that in the proof of Proposition 5 we have equality if pred( A j ) = { A 1 ,...,A j  X  1 } . This is not possible for trees larger than two nodes, but it requires that the Bayesian network is fully connected.

Corollary 6. Let G be a fully connected directed acyclic graph and G is interpreted as a Bayesian network whose every node has the maximum-likelihood distribution from the data D . Then we have H ( X ) = H ( G ) .
 The root of a D-tree T can be selected arbitrarily, because H
D ( T ) can be written as a sum depending only on the en-tropies of the edges and the nodes, and the degrees of the nodes: P ( A,B )  X  E H ( A,B )  X  P A  X  V ( deg ( A,T )  X  1) H ( A ) .
Proposition 7. For D-trees the choice of root does not matter.

However, it is easy to see that for U-trees the topology of the tree can make a large difference. For example, consider the case where we have three binary variables: two inde-pendent variables with marginal probabilities 1/2, and their exclusive or. Then any U-tree with a root with two children has entropy 2 whereas any stick-shaped U-tree has entropy 3 : a single attribute does not tell anything about the other two, but any two of the attributes determine the third one completely.

The above example also shows that the entropy of the best D-tree for the attribute set X can be larger than the entropy of the best U-tree for X .
 Conjecture 8. If T is a D-tree for X , then there is a U-tree S for X such that H D ( T )  X  H U ( S ) .
 We next turn to the question of how to mine D-trees and U-trees. As noted above Proposition 5, the entropy of the set gives a lower bound for the entropy of the best D-and U-trees. However, the lower bound can be rather loose.
In the case of D-trees we look for the best tree for the given set of attributes. It turns out that the best D-tree for an itemset X can be obtained by adding an edge to the best D-tree for X \ A for some A  X  X .

Proposition 9. Let D ( X ) = ( X,E ) denote the mini-mum entropy D-tree for the itemset X . Then we have and D ( X \ v ) is a subtree of D ( X ) for some v  X  X and all X 6 =  X  .

Proof. Observe that D ( X \ v ) is a subtree of D ( X ) for and hence the claim holds.
 Hence the low-entropy D-trees can be found by e.g. breadth-first search by setting There are |I\ X || X | possible extensions of D ( X ) and each can be evaluated in constant time as there is no need to look at the data after computing the pairwise entropies of the items. Furthermore, the approach can be adapted to mine top-k D-trees instead of the D-trees with entropies below the given threshold.

Also in the case of U-trees, the entropy is defined as a sum of local conditional entropies. We proceed by finding first low U-trees where the leaves are immediate children of the root, and then using the low trees as building blocks. Low U-trees correspond closely to sets, as demonstrated by the following result.
Proposition 10. The entropy of a low U-tree on the at-tribute set X is at least as large as the entropy of X .
Proof. Let A  X  X be the root of the tree and X 0 = X \ A the set of the leaves. The entropy of the tree is by definition X
To mine low U-trees, we simply use the levelwise algorithm to find all low-entropy sets, let each attribute be the root in turn, and list the U-trees in the order of increasing entropy. We construct larger U-trees by connecting low-entropy low U-trees to already found low-entropy U-trees. In more de-tail, we maintain a priority queue of low-entropy U-trees. The queue is initialized by the low U-trees with sufficiently small entropy. Then the first tree is extended replacing a leaf by a low-entropy U-tree. A U-tree T and a low U-tree U may only be combined if the root of U is a leaf of T ; then the entropy of the combined tree V is because in T the entropy of the subtree consisting of root ( U ) is simply H ( root ( U )) , but in V it is H U ( U ) . If this entropy falls below the threshold, the combined tree is inserted in the list, and the process is continued until no more combinations have sufficiently low entropy.

Proposition 11. The U-tree mining algorithm described above finds all low-entropy U-trees.
 The algorithm lists the low-entropy U-trees in increasing order in entropy. So, adapting it for top-k mining of low-entropy U-trees is quite straightforward.
So far we have considered itemsets and trees that have low entropy, i.e., whose values are highly concentrated on one or a few combinations. If, on the contrary, the values are spread out more than for most itemsets, this may also be an interesting pattern. Thus an obvious pattern class could be the itemsets whose entropies are highest.

Intuitively, a high-entropy set is a set of maximum di-versity: a set of attributes whose value assignments shatter the data maximally. An example of such an itemset in the Course enrolment dataset is the set consisting of the courses Calculus I, Computer Uses in Education, Introduction to the Use of Computers, The metalanguage XML, and Unix Platform. The courses in the set are not excluding, but do not depend on each other either.

However, since entropy is increasing with respect to set inclusion, the itemset containing all items always has maxi-mal entropy, and thus high-entropy sets do not form a local pattern class in the usual sense. Instead, we define versions of entropy scaled such that small itemsets have a chance of attaining a high score.

The discussion in Section 2 shows that the maximum pos-sible value of H ( X ) increases linearly with the number of items in X . In order to compare entropies of different-sized sets, we define the scaled entropy of X to be H s ( X ) = H ( X ) / | X | . Another possibility is to consider the maxi-mum possible value of H ( X ) given the frequencies of all items A  X  X . If these frequencies are fixed, it follows from the chain rule of entropy [8, Chapter 2] that the en-tropy H ( X ) of the set X is maximized when the items con-stitute independent random variables, and in this case the entropy is P A  X  X H ( A ) . Thus we define the normalized en-tropy of X to be H n ( X ) = H ( X ) / P A  X  X H ( A ) .
Definition 12. Given a threshold , an itemset X is a high-scaled-entropy (HSE) set in D if H s ( X )  X  , and a high-normalized-entropy (HNE) set if H n ( X )  X  .
For normalized and scaled entropies, we do not have a monotonicity property like that of frequent itemsets. How-ever, the following weaker properties can be proved. We prove that any HSE or HNE itemset X contains at least one HSE or HNE itemset X 0  X  X with | X 0 | = | X | X  1 . This enables us to use a modified levelwise algorithm: we can prune those itemsets that have no HSE or HNE subsets.
Proposition 13. For all datasets D and for all sets X 6 =  X  of attributes there is an attribute A  X  X such that Proof. By definition, H ( X ) = H ( A | X \ A ) + H ( A ) . Hence, we can write Let X = { A 1 ,...,A k } . Then we can write Combining the observations above we get as claimed.

Proposition 14. For all datasets D and for all sets X 6 =  X  of attributes there is an attribute A  X  X such that
Proof. Observe that if p/q  X  r/s , we have The left-hand inequality follows from p ( q + s ) = pq + ps  X  pq + qr = q ( p + r ) , and the right-hand inequality is proven similarly. Using this inequality inductively on the fractions f ( A ) = H ( A | X \ A ) / H ( A ) for all A  X  X , we find that if A minimizes f ( A ) , Denote X \ A by X 0 , and enumerate the attributes in X 0 arbitrarily so that X \ A = { B 1 ,B 2 ,...,B n } . Consider the numerator of the fraction on the right-hand side. By weakening the conditions of the conditional entropies we can only increase the entropies, and thus X Thus we obtain from (3) and using again the inequality at the beginning of the proof, Since the numerator on the left-hand side is equal to H ( X ) , we have shown that H n ( X )  X  H n ( X 0 ) = H n ( X \ A ) . As the weak monotonicity results in a larger number of can-didates, optimizations in candidate counting are of interest. In practice many candidates can be eliminated by utilizing the fact that H ( X )  X  H ( X \ Y ) + H ( Y ) for any Y  X  X .
An interesting combinatorial variant of high-entropy sets is obtained by looking, for an itemset X , at the number of different value combinations for X that occur in the data. This corresponds to projecting the data to the columns of X and discarding duplicate rows. If there is a set X such that all the 2 | X | combinations occur, then the Vapnik-Chervo-nenkis dimension [4] of the data is at least | X | .
The property of all 2 | X | combinations occurring is down-wards closed, so the levelwise algorithm can be used. Such sets X are in a way maximally diverse in a combinatorial sense, and listing them could be useful in some situations.
In this section we briefly discuss our experiments on gener-ated data. We used a simple procedures to plant low-entropy itemsets into generated data. First, for each row u the set of attributes of an itemset X were all set to one with prob-ability 0.5, or all to zero with 0.5 probability respectively. Notice that, this will results to an itemset pattern with an entropy score of 1. Second, some noise was added to the rows by independently flipping each bit with probability r .
Using this procedure we generated data sets with low-entropy patterns of four attributes ranging the noise parame-ter r from 0.0 to 0.3. For each data set the generated number of rows was 100000. The results for this experiment showed that using either D-trees, U-trees or low-entropy itemsets the best 4 size patterns always correctly corresponded to the attributes of the planted patterns. The number of planted patterns used in the experiments were 2 and 4.
Next we describe experiments on two real data sets, one about courses completed by computer science students at the University of Helsinki, and another about terms used in computer science bibliographies. We show that it is feasible to run our algorithms on these data sets, and give examples of interesting patterns found in them. We also show that the patterns found are significant in the sense that when the data sets are permuted, the number of patterns found at a given entropy threshold decreases by a large factor. Table 2: D-tree results for the course data. The number of D-trees in the collection TP D ( D, ) with various values of the preprocessing frequency threshold  X  and entropy threshold . Legend: trees = number of trees found; cands = number of candidates examined; items = number of attributes in the data after preprocessing; max = size of the largest tree; time = running time in seconds.

In each case, we preprocess the data by removing at-tributes whose frequencies are outside a range. In both data sets there are many rarely occurring attributes  X  the course data includes seminars that were only given once, and the bibliography data has typographical errors, author names, and other rare words. Such attributes have low entropy but cannot be considered interesting, since the reasons for their rarity are known. We include the frequency threshold used as a parameter of the experiments. From the bibliography data we also remove very frequently occurring stop words.
The course data describes courses taken by students at the Department of Computer Science of the University of Helsinki. The data has 2405 observations corresponding to students and 5021 attributes corresponding to courses. On average, each student has taken 26.9 courses.

Tables 2, 3 and 4 show the number of D-trees, U-trees, and low-entropy itemsets with different parameter values com-puted from the course data. For all three pattern classes the number of elements in the answer set increases rapidly with decreasing frequency threshold  X  and increasing en-tropy threshold . This is similar to the behavior of fre-quent itemsets, whose number increases rapidly when the frequency threshold is decreased, and as in the case of fre-quent itemsets, for our pattern classes it is easy to iteratively find thresholds that produce outputs of desired size, and even for larger output sizes the running times stay feasible. Some example U-trees drawn from the results are shown in Figures 1, 4, and 5. The tree in Figure 1, already discussed in the introduction, is the lowest-entropy 5-node tree in the course enrolment data for a preprocessing frequency thresh-old of 0.15. The trees in Figures 4 and 5 are the best two 5-node U-trees left when we eliminate the most frequently taken courses (frequency &gt; 0 . 18 ). Both trees clearly have an AI component and a software engineering component, corresponding to two of the specialization areas within the department.

Similarly, the lowest-entropy 7-node D-tree (Figure 6) has clear AI and distributed systems components. The central Table 3: U-tree results for the course data. The number of U-trees in the collection TP U ( D,, X  ) with the maximum branching factor of  X  = 3 for various values of the preprocessing frequency threshold  X  and entropy threshold . Legend: trees = number of trees found; low = number of low trees found; max = size of the largest tree; cands = number of candidates examined; time = running time in sec-onds. Table 4: Low-entropy itemset results for the course data. The number of low-entropy sets for various values of the preprocessing frequency threshold  X  and entropy threshold . Legend: sets = number of itemsets found; cands = number of candidates ex-amined; max = size of the largest set found; time = running time in seconds. node of the tree is Theory of Computation. On one hand, the students taking Artificial Intelligence and AI Languages usu-ally consider Theory of Computation as a relevant course. On the other hand, the test of the branches correspond to students very close to graduation, which means that most of them has also passed Theory of Computation.

As already discussed, low-entropy itemsets are not as eas-ily interpretable as the tree patterns. One example of a low-entropy set found in the course data is shown in Ta-ble 5.

The bibliography data consists of terms used in bibliogra-phies on theory and foundations of computer science. 2 The data set contains 67043 bibliography entries. For the experi-ment we reduced the data set by taking a random 10% sam-http://liinwww.ira.uka.de/bibliography/Theory/Seiferas/ Table 5: Joint distribution of an example low-entropy itemset in the course data. Legend: CS = Concurrent Systems; SW = Scientific Writing; DB = Database Systems 1; MT = Maturity Test; ToC = Theory of Computation; UI = User Inter-faces; MSc = Master X  X  Thesis. ple of the data resulting in a set of 6695 bibliography entries. In addition to this. the frequent stop words  X  an , to , with , in , on , a , for , and , the , by some , is , from , and of  X  were re-moved. On average the resulting data set has 8.28 words per bibliography entry. Tables 6, 7 and 8 show the number of D-trees, U-trees, and low-entropy itemsets with different pa-rameter values computed from the bibliography data. The best D-tree of size 6 is shown as Figure 7; since the root of a D-tree can be selected arbitrarily, the edges in the figure are not directed.

To evaluate how well the D-trees, U-trees and low-entropy itemsets find significant structure in data we compare the scores of the patterns found from the course data set against the scores of patterns obtained form 10 randomized course data set instances. For this we use the swap randomization method described in [11]. The method preserves the row and column margins of the given dataset, but obscures the internal dependencies of the data. The idea is that if the Table 6: D-tree results for the bibliography data. The number of D-trees in the collection TP D ( D, ) with various values of the preprocessing frequency threshold  X  and entropy threshold . Legend: trees = number of trees found; cands = number of candidates examined; items = number of attributes in the data after preprocessing; max = size of the largest tree; time = running time in seconds.
Figure 7: Example D-tree from bibliography data. 0.025 0.6 2 353 249 3 892 1.96 0.025 0.6 3 353 249 3 1011 2.40 0.025 0.7 2 889 517 4 1123 2.01 0.025 0.7 3 894 522 4 1792 3.30 0.02 0.6 2 8515 3272 4 6036 7.57 0.02 0.6 3 9019 3776 4 20542 24.5 0.02 0.7 2 58234 5231 4 6513 18.3 0.02 0.7 3 67904 14901 4 35545 152 0.015 0.6 2 369755 17147 5 22615 617 0.015 0.6 3 438827 82399 5 180058 5220 0.015 0.7 2 4710983 21555 6 23376 13900 Table 7: U-tree results for the bibliography data. The number of U-trees in the collection TP U ( D,, X  ) with the maximum branching factor of  X  = 3 and  X  = 2 for various values of the preprocessing fre-quency threshold  X  and entropy frequency . Leg-end: trees = number of trees found; low = number of low trees found; max = size of the largest tree; cands = number of candidates examined; time = running time in seconds. true structure in the data is captured by the patterns, the number and scores of the found patterns should be better in the original data than in the randomized data sets instances.
The results of the experiment are depicted in Figure 8, 9, and 10. For each figure the upper image is the histogram of the scores of patterns found from the original course data and the lower image depicts the respective histogram for the aggregate patterns found in the 10 swap randomized course data sets. The mining was done using courses with a frequency of 0 . 2 or higher in the data and an entropy parameter of = 2 . 8 . For U-trees the maximum branching factor was limited to 4.

The results show that the amount of structure in terms of the number of patterns found from the original data com-pared to the randomized data instances is larger for all three proposed patterns classes. With the given parameters the number of generated D-trees is 1.8 times larger in the origi-nal data set compared to the randomized data set instances on the average and 1.6 time larger for the U-trees and 2.1 for the low-entropy sets. Moreover, the average score within Table 8: Low-entropy itemset results for the bib-liography data. The number of low-entropy sets for various values of the preprocessing frequency threshold  X  and entropy frequency . Legend: sets = number of itemsets found; cands = number of candi-dates examined; max = size of the largest set found; time = running time in seconds. Figure 8: Validation of D-tree results in course data. Upper pane shows the distribution of entropies for 11648 D-trees mined from the course data with frequency threshold 0 . 2 and entropy threshold 2 . 8 . Lower pane shows the respective distribution from 10 aggregated swap-randomized course data set in-stances (6297 trees on average). patterns of the same size is smaller and the average pattern size is larger in the original data for all pattern classes.
Decision trees [12, Chapter 9.2] are similar to D-trees in that low classification error implies low entropy. There are two crucial differences. First, D-trees do not split the data into groups corresponding to different values of an attribute, but all data are considered in each node. In other words, H ( A | B ) is used instead of e.g. H ( A | B = b ) . Second, decision tree algorithms seek greedily one tree, but the D-tree algorithm finds exhaustively all D-trees satisfying the criteria.

The work in [14] is a simple special case of the current setting. The trees of the earlier paper were defined to be present in a row of data if the attributes of a rooted subtree were present simultaneously: e.g. for the tree of Figure 1, this means that Scientific Writing and Maturity Test must Figure 9: Validation of U-tree results in course data. Upper pane shows the distribution of entropies of 19380 U-trees mined from the course data, lower pane shows the corresponding distribution in 10 ag-gregated randomizations of the data (11967 trees on average). Figure 10: Validation of low-entropy itemset results in course data. Upper pane shows the distribution of entropies of 13575 itemsets mined from the course data, lower pane shows the corresponding distribu-tion in 10 aggregated randomizations of the data (6365 itemsets on average). be present whenever anything lower in the tree is present, and if Probability Theory 1 is present, then the whole branch from Scientific Writing to Probability Theory 1 must be present. Such a tree constitutes a low-entropy U-tree, but U-trees are more general by allowing the distribution to be con-centrated on other combinations than positive conjunctions.
The task of finding interesting itemsets has been addressed mainly in the context of frequent itemset mining. Morishita and Sese have presented a branch-and-bound method for finding association rules like X  X  Y where the itemsets X and Y are not required to have high support but high corre-lation [19]. They also count those data rows where X and Y appear completely, whereas our entropy-based method counts arbitrary combinations. Similar strategies have been em-ployed by Zimmermann et al. [24, 5]. In [5] they describe a sequence of graph pattern classes that is to some extent paralleled by our sequence of itemset pattern classes.
Closer to our approach are Knobbe and Ho X  X  maximally informative k -itemsets, i.e., itemsets with as high entropy as possible [16]. The difference to our high-entropy itemsets is that Knobbe and Ho avoid the trivial result of the full itemset by restricting their itemsets to have a fixed num-ber k of elements, whereas we define scaled versions of en-tropy and thus avoid having to specify an extra parameter. Another information-theoretically motivated approach is to select itemsets that can be used to compress the data, re-cently introduced by Siebes et al. [22, 23]. They, however, consider only all-1s itemsets.

In real-valued data, the task of finding interesting sub-sets has been addressed by research areas such as subspace clustering [20, 1] and projection pursuit [9, 15]. Another method somewhat related to our task is the problem of learning the structure of a Bayesian network. This problem is computationally challenging, and the main methods are probabilistic [7, 13]; the best current exact algorithms are of order O ( n 2 n ) [17]. The key difference between Bayesian network structure learning and our approach is that we seek interesting subsets of the data, not complete models; also, of course, we investigate only fully connected or tree-structured networks and not arbitrary graphs, and we do not try to in-corporate any prior knowledge into the patterns.
We have considered the problem of finding low-entropy sets and trees from binary data. The approach we have chosen is a natural generalization of the discovery of frequent sets and association rules: a frequent set is a particular form of a low-entropy set.

We defined the concepts of low and high-entropy sets and two types of trees, and gave efficient algorithms for finding such sets. The experiments show that the methods are able to discover interesting sets and trees and that they are fea-sible to use also on large datasets. We also considered the search for high-entropy sets of variables.

Our approach searches for local structure: small subsets of variables for which the data can be modeled well, in the sense of having a small entropy or being amenable to be described by a tree. Such techniques are especially useful when dealing with datasets of large dimension, and when many of the dimensions are assumed to be relatively unimportant. [1] Agrawal, R., Gehrke, J., Gunopulos, D., and [2] Agrawal, R., Imielinski, T., and Swami, A. N. [3] Agrawal, R., Mannila, H., Srikant, R., [4] Anthony, M., and Biggs, N. Computational [5] Bringmann, B., Zimmermann, A., De Raedt, L., [6] Chow, C. K., and Liu, C. N. Approximating [7] Cooper, G. F., and Herskovits, E. A Bayesian [8] Cover, T. M., and Thomas, J. A. Elements of [9] Friedman, J. H., and Tukey, J. W. A projection [10] Gionis, A., Kujala, T., and Mannila, H.
 [11] Gionis, A., Mannila, H., Mielik X inen, T., and [12] Hastie, T., Tibshirani, R., and Friedman, J. The [13] Heckerman, D., Geiger, D., and Chickering, [14] Heikinheimo, H., Mannila, H., and Sepp X nen, [15] Huber, P. J. Projection pursuit. The Annals of [16] Knobbe, A. J., and Ho, E. K. Y. Maximally [17] Koivisto, M. Advances in exact Bayesian structure [18] Meil X , M., and Jordan, M. I. Learning mixtures of [19] Morishita, S., and Sese, J. Traversing itemset [20] Parsons, L., Haque, E., and Liu, H. Subspace [21] Pearl, J. Probabilistic Reasoning in Intelligent [22] Siebes, A., Vreeken, J., and van Leeuwen, M. [23] van Leeuwen, M., Vreeken, J., and Siebes, A. [24] Zimmermann, A., and De Raedt, L. CorClass:
