 This work investigates cluster labeling enhancement by uti -lizing Wikipedia , the free on-line encyclopedia. We describe a general framework for cluster labeling that extracts cand i-date labels from Wikipedia in addition to important terms that are extracted directly from the text. The X  X abeling qua l-ity X  of each candidate is then evaluated by several indepen-dent judges and the top evaluated candidates are recom-mended for labeling.

Our experimental results reveal that the Wikipedia labels agree with manual labels associated by humans to a cluster, much more than with significant terms that are extracted di-rectly from the text. We show that in most cases even when human X  X  associated label appears in the text, pure statisti cal methods have difficulty in identifying them as good descrip-tors. Furthermore, our experiments show that for more than 85% of the clusters in our test collection, the manual label (or an inflection, or a synonym of it) appears in the top five labels recommended by our system.

The volume of available electronic information is rapidly increasing with the advancements in digital processing. Mo re-over, massive amounts of textual data have brought about the need for efficient techniques that can organize the data in manageable forms. One of the popular approaches for orga-nizing textual data is the use of clustering algorithms, whi ch group a set of documents into coherent clusters. The algo-rithms X  goal is to create clusters, where documents within a cluster should be as similar as possible and documents in one cluster should be dissimilar from documents in other clusters.

In many applications of clustering, particularly in user-interface based applications, human users interact direct ly with the created clusters. In such settings we must label the clusters so that users can understand what the cluster is about. A lot of research is being done on clustering al-gorithms and their applications in information retrieval a nd data mining. However, comparatively little work has been done on cluster labeling.

A popular approach for cluster labeling is to apply statis-tical techniques for feature selection. This is done by iden ti-fying  X  X mportant X  terms in the text that best represent the cluster topic. However, a list of significant keywords, or even phrases, will many times fail to provide a meaningful readable label for a set of documents. In many cases, the suggested terms, even when related to each other, tend to represent different aspects of the topic underlying the clus -ter. In other cases, a good label may not occur directly in the text. Hence user intervention is required to infer a proper label from the suggested terms to successfully describe the cluster X  X  topic.

As an illustrating example, Table 1 shows the top five important terms extracted for six Open Directory Project (ODP) [14] topics using the JSD selection method (further described in Section 3). Each topic is represented by a clus-ter of 100 web documents randomly sampled from the corre-sponding ODP category. We observe that while the impor-tant terms extracted for all clusters seem to fairly represe nt the category X  X  topic, only the terms of the first two top-ics seem to provide sufficient labels for those clusters (such terms are underlined in Table 1). This is not true for the rest of the topics. For example, none of the important terms for the Electronics topic seem to provide a good label, even though all terms are strongly related to Electronics .
By analyzing a sample of 100 ODP categories, we discov-ered that the original label associated with the category, a s given by a human assessor, indeed appears in the category X  X  text in 85% of the categories. However, these human la-bels are rarely identified as  X  X ignificant X  by feature select ion methods. For example, the JSD method for feature selection identifies human labels as  X  X ignificant X  (appearing in the to p five most important terms) for only 15% of the categories. This result implies that human labels are not necessarily significant from a statistical perspective. This further mo ti-vated us to seek out a cluster labeling method using external resources.
In this work we investigate the contribution of external knowledge-bases for cluster labeling. This approach is clo se in spirit to the work described by Syed et al. [19], who identified topics associated with a set of documents using Wikipedia. Our method first finds Wikipedia pages relevant to the cluster to be labeled. It then used the meta-data of these pages, such as categories and titles, for labeling the cluster. The last column of Table 1 shows the Wikipedia labels extracted by the labeling system for a set of ODP topics, which agree much more with the given human anno-tated labels.

Given a cluster of documents, we first extract the most important terms (keywords and phrases) from the text. We then identify a list of related Wikipedia pages by search-ing Wikipedia using a query that is based on those terms. The Wikipedia categories and titles of related pages serve as potential candidates for cluster labeling. In addition, all important terms extracted from the text of the cluster are also considered as candidates. Each candidate is evaluated by several independent judges. Section 3 describes all judg es used by our system in full detail. Finally, all judgments are aggregated to select the top candidates for use as cluster labels.

We evaluated our work using a sample of the ODP collec-tion and the 20 News-Group (20NG) collection. Following the evaluation framework described in [21], we extracted a uniform sample of 100 categories from ODP, each associated with a manual label. Similarly, the 20NG benchmark that is frequently used for clustering analysis contains 20 clus ters of news-groups each associated with a manual label. We evaluated our labeling method by measuring its ability to predict the manual label of each of the clusters in the two benchmarks. Our experiments show that for both bench-marks, our labeling framework is able to provide Match@5  X  0.85 . This means that for more than 85% of the catgories, the manual label (or an inflection, or a synonym of it) ap-pears in the top five recommended labels. To the best of our knowledge, this is the highest evaluation score reporte d so far for the cluster labeling task.

The rest of the paper is organized as follows. Section 2 discusses related work. Section 3 describes the labeling sy s-tem and its components. Section 4 provides the results of some experiments we conducted with the labeling system. Section 5 summarizes and discusses future directions.
The most common approach for cluster labeling works by identifying  X  X mportant X  terms in the cluster content tha t characterize the cluster in contrast to other clusters [6]. Im-portant terms can be identified by naively selecting the most frequent terms in the cluster, by extracting the top weighte d terms in the cluster centroid, or using any other statistica l feature selection techniques [13]. For example, Geraci et a l. [10] used a modified version of the information gain measure to identify words that most represent the cluster X  X  content s and are least representative of the contents of other clus-ters. Several works also considered frequent phrases in the text for cluster labeling [15, 21]. Toda and Kataoka [20] also used named entities extracted from the text for label-ing. However, in many cases, a labeling approach that is solely based on the cluster content may have difficulties in providing discriminative labels, as illustrated in Table 1 . Several labeling solutions look for alternative resources . One of the first systems that dealt with cluster labeling is the Scatter/Gather application [6]. In this system, in addi -tion to the cluster X  X  important terms, the titles of documen ts that are mostly close to the cluster centroid are also consid -ered for labeling, since usually titles are much more readab le than a list of terms. On the web, the anchor text associated with the in-links to the cluster web pages can be considered as candidate labels since often anchors successfully descr ibe the pages to which they link. Glover et al. [11] demonstrated that labels extracted from the anchor text provide a much better description than those extracted from the page X  X  con -tent.

There is a lot of research on linguistic-based summariza-tion techniques for multiple documents which are also re-lated to the labeling task. For example, Radev et al. [16] generate a summary of multiple documents using cluster centroids produced by topic detection and tracking. How-ever, multi-document summaries are usually too long to be utilized as short comprehensive labels.

Several labeling approaches attempt to enrich content-only terms by exploiting external resources for labeling, f or example, the WordNet lexical database [4] was used to ex-tract root meanings of important terms and to determine semantic relationships among these terms. Gabrilovich and Markovich [9] utilized Wikipedia to represent the meaning of a text fragment as a weighted vector of Wikipedia con-cepts. Semantic relatedness between two fragments is then measured through the comparison of concept vectors us-ing the cosine similarity. Similarly, Syed et al. [19] find concepts common to a document, or a set of documents, using Wikipedia articles and spreading activation on the Wikipedia X  X  category links graph. Both works demonstrated that categories of Wikipedia articles can successfully de-scribe the document X  X  common concepts. Wikipedia has recently become one of the major knowledge resource for many information retrieval tasks, including text categori za-tion and clustering [8, 17, 12], computing semantic relat-edness between concepts [18, 9], and predicting document topics [19].

Our cluster labeling solution also leverages the labeling task by Wikipedia. However, our solution has several main distinctions from previous works. First, in our approach, candidate labels are extracted from Wikipedia pages in ad-dition to the important terms that are extracted directly from the cluster content. Thus, selected Wikipedia cate-gories  X  X ompete X  with inner terms for serving as the cluster labels. In general, Wikipedia is a very successful resource for labeling; however, inner terms should be considered for the cases when Wikipedia fails to cover the cluster content.
Second, in contrast to previous works which focus on iden-tifying document concepts [18, 9, 19] using Wikipedia, we look for a few focused labels that will be judged by a (hu-man) user as descriptive labels for a given documents X  clus-ter. This distinction is reflected by the novel additional ca n-didate judgment process applied by our system, and by the different evaluation paradigm applied in our work. While Gabrilovich and Markovitch evaluated their system X  X  abili ty to identify related text fragments, and Syed et al. eval-uated their system X  X  ability to predict the concepts of a Wikipedia X  X  labels agree with human annotated labels much m ore. unique Wikipedia article, we evaluate our system X  X  ability to agree with a predefined human label associated with a multi-documents set 1 . By a systematic evaluation procedure which is an integral part of our labeling framework, system parameters can be optimally tuned for each collection to be clustered and labeled.

Given several candidate labels identified by the system, there is still a need to evaluate which candidate to nominate for labeling. Candidate terms are usually evaluated using term statistics gathered from a given corpus. For exam-ple, measuring the pointwise mutual information ( P MI ) of a term with the rest of the cluster terms is a common evalua-tion approach [13]. P MI between two terms is usually mea-sured by statistical co-occurrence analysis in the collect ion to be clustered. Recently, de-Winter and de-Rijke [7], whil e comparing several labeling methods for a set of blog posts, measured PMI by gathering term co-occurrence statistics from the web. Similarly, we measure P MI independently over the web and over the Wikipedia corpus.

Unfortunately, there is still no standard evaluation metho d-ology for cluster labeling and there are no standard bench-marks to compare alternative labeling methods. In this work, we follow the evaluation framework suggested by Treer -atpituk and Callan [21]. We utilize the 20NG collection, and a random sample of categories from ODP as benchmarks. The labeling system is evaluated by its ability to produce a label for each category that is  X  X quivalent X  to the category  X  X  original associated label. Two labels are considered equiv -alent when one is identical, an inflection, or a Wordnet X  X  synonym of its counterpart. Our experimental results will show that our labeling system performs very well compared to previously reported results on similar benchmarks.
We propose a general framework for cluster labeling using external resources. Figure 1 provides an illustration of th is framework, which includes five main components: indexing, In addition to experimenting with individual documents, Syed et al. [19] also demonstrated their system X  X  capabilit ies for a few sets of documents that are related to the same concept. However, as the authors mentioned,  X  X esults were encouraging X , but their work lacks a systematic analysis fo r the multi-documents case.

Figure 1: A general framework for cluster labeling clustering, important terms extraction, candidate labels ex-traction, and candidate evaluation.

The general flow of the system can be summarized as fol-lows. The system receives a set of textual documents as input. Initially, the documents are parsed and indexed and an inverted index is generated. This index is primarily used by other components for gathering term statistics. The doc-uments are then clustered using the clustering component. For each generated cluster, the system extracts a set of im-portant terms that are estimated to best represent the con-tent of the documents of the cluster. The cluster important terms are then used to identify a list of candidate labels for the cluster. Candidate labels can be selected from the set of important terms or from external resources (e.g., Wikipedi a, or the general web). Finally, the set of candidate labels is evaluated and a list of top recommended labels is returned by the system. In the rest of this section, we describe each of the system components in more detail.
Documents are first parsed and tokenized, and then rep-resented as term vectors in the vector space over the sys-tem X  X  vocabulary. Term weights are determined by the well-known tf-idf weighting scheme of the vector-space model. The documents are indexed by generating a search index. For this purpose we use the Lucene open source search sys-tem 2 . The inverted index lets us obtain for each term t its term frequency ( tf ( t, d )) in each document d , and its inverse document frequency ( idf ( t )) in the entire collection. These statistics are later used by all components of the framework .
The clustering algorithms X  goal is to create coherent clus-ters for which documents within a cluster share the same topics, and the labels provided by the system are expected to express the mutual topic of documents within the cluster.
The clustering component receives as an input the collec-tion of documents D and returns a set of document clusters C = { C 1 , C 2 , . . . , C n } that cover the whole collection. Each cluster C i is a subset of documents of D , and a document may belong to more than one cluster.

A cluster is represented by the centroid of the cluster X  X  documents; however, the weights of terms in the cluster X  X  centroid are slightly modified to bias terms that are dis-tributed over many cluster documents. Thus, the weight of a term t in the centroid of a cluster C , is set to be: where the cluster term frequency ctf ( t, C ) = 1 | C | P and the cluster document frequency cdf ( t, C ) = log( n ( t, C )+ 1); n ( t, C ) is the document frequency of t in C .
The labeling framework is not limited to a specific cluster-ing algorithm; however, the coherence of the clusters ident i-fied by the system is expected to significantly affect the qual-ity of labeling. In the worst case, when there is no semantic relatedness between the cluster documents, no meaningful labels can be expected. In Section 4, we explore the effect of cluster coherency on the quality of cluster labels provid ed by the system.
Given a cluster C  X  X  as input, we now wish to find a list importance, to represent the content of the cluster X  X  docu-ments. Such terms consist of single keywords, and N-grams of different length [21].

A naive approach for term extraction is to select the top-k terms with maximal weights from the cluster X  X  centroid. Several clustering systems apply this selection approach f or cluster labeling [6]. In our experiments we use this term extraction approach as one of the baseline methods for com-parison.

Important term extraction is strongly related to feature selection which is the process of selecting a subset of the terms for text representation, and is frequently applied by text categorization and text clustering methods [13]. Com-mon approaches for feature selection evaluate terms accord -ing to their ability to distinguish the given text from the whole text. In our case our aim is to find a set of terms http://lucene.apache.org/ T ( C ) that best separates the cluster X  X  documents from the entire collection.

In this work, we extract important terms using the method described by Carmel et al. [3], which was originally propose d in the context of the query difficulty model. We look for a set of terms that maximizes the Jensen-Shannon Divergence (JSD) distance between the cluster C and the entire collec-tion. Each term is scored according to its contribution to the JSD distance between the cluster and the collection. The top scored terms are then selected as the cluster important terms. We will experimentally show the superiority of this set of terms for cluster labeling over the top weighted terms in the cluster centroid, and over sets of terms extracted by alternative standard feature selection methods.
Given the list of important terms T ( C ), we now wish to extract candidate labels for cluster C . We identify two dif-ferent types of sources from which it is possible to extract such candidate labels. The first type involves labels that ar e extracted directly from the cluster X  X  documents content. I n this case, we follow the spirit of previous works [6, 11, 21] and consider the set of important terms themselves as po-tential labels for the cluster. Nevertheless, there are man y cases in which important terms do not provide suitable la-bels or are not meaningful enough for end-users, as Table 1 shows. Therefore, we turn to external sources as compli-mentary sources for this task.

Similar to [19], we focus on Wikipedia as an external source from which candidate cluster labels can be extracted . We note that there may be other external sources that can be utilized for this task, such as domain-specific knowledge -bases, ontologies, or even more general sources such as the web. The main reason for focusing on Wikipedia is its at-tractive ability to provide high quality controlled conten t. Moreover, Wikipedia content has also been annotated by Wikipedia X  X  users. These manual annotations can provide high-quality meaningful labels.

The process of candidate label extraction from Wikipedia can be summarized as follows. We first generate a search index from the latest available Wikipedia dump 3 using the Lucene search system. Given the list of important terms T ( C ), we execute a query q against the Wikipedia index which consists of the disjunction of the important terms, where the query terms are further boosted according to their relative importance in T ( C ). The result of this query is a list of documents D ( q ) sorted by their similarity score to q . For each document d  X  D ( q ), we then consider both the document X  X  title and the set of categories associated with the document as potential candidate cluster labels (denote d L ( C )).
Candidate labels are evaluated by several judges 4 . Each judge gets as an input the set of candidate labels L ( C ) and the set of the cluster X  X  important terms, T ( C ). Then, each judge evaluates the candidates according to its evaluation policy. The scores of all judges are then aggregated and the labels with the highest aggregated scores are returned. http://download.wikimedia.org/enwiki/20080724/
Please note that the term  X  X udge X  is used throughout the paper to symbolize an automatic heuristic for candidate la-bels evaluation. We now present the judge types in more detail and further suggest several instantiations for each type.
The Mutual Information (MI) judge scores each candi-date by the average pointwise mutual information (PMI) of the label with the set of the cluster X  X  important terms, with respect to a given external textual corpus. The aver-age PMI of a given label with the set of important terms reflects the  X  X emantic distance X  of the label from the cluste r content. Hence, labels that are  X  X loser X  to the cluster con-tent are preferred. This approach is similar in nature to the evaluation process applied by [7] while evaluating labelin g methods for clusters of blog-posts.
 The MI judge gets as input the set of candidate labels L ( C ), the set of the cluster X  X  important terms T ( C ), and a corpus that identifies an external textual source where the PMI will be measured (e.g., the web). Given a candidate label l  X  X  ( C ), the following score is assigned to l : where  X  ( t ) denotes the relative importance of important term t  X  X  ( C ), and P t  X  X  ( C )  X  ( t ) = 1, The PMI between two terms is measured by: The probability of a term, or a pair of terms in the given corpus, is approximated by the maximum likelihood estima-tion where x stands for a single term or a pair of terms, #( x | corpus ) is the number of occurrences of x in the data, and #( corpus ) is an estimation of the number of terms in the corpus.
We utilize two instantiations of this judge using two dif-ferent external corpora for calculating the PMI values. The first judge uses the Wikipedia collection as a data source and the second evaluates PMI over the web, using the PMI eval-uation scheme described in [5]. For gathering web statistic s, we use the Google n-grams collection [2], which provides the frequency counts of English word n-grams generated from approximately 1-trillion word tokens gathered from the web , to estimate the term frequency in a large web collection.
It is important to note that the important terms are eval-uated by the MI judge in exactly the same way as Wikipedia candidate labels. Each important term is scored by the av-erage PMI with all other important terms and is compared with all other candidates according to that score.
The second type of judges, termed Score Propagation (SP) judge, scores each candidate label with respect to the scores of the documents in the result set associated with tha t label. This judge propagates documents X  scores to candi-dates that are not directly associated with those documents , but share common keywords with other related labels.
Given a candidate label l  X  L ( C ), by summing over the set of all documents in D ( q ) associated with label l , we ob-tain an aggregated weight for l , which represents the score propagation from D ( q ) to l : where n ( d ) denotes the number of candidate labels extracted from document d .

After scoring the labels, we score the label keywords as follows:
Finally, the score assigned to each candidate label l is set by the average score propagated back from its keywords. Formally: where n ( l ) denotes the number of l  X  X  unique keywords.
Important terms are judged identically to Wikipedia la-bels. Each important term is associated with all the results containing it and treated as a label. Hence, it is scored usin g the same scoring mechanism of the SP judge.
 Several instantiations of this judge type can be utilized. By using different scoring mechanisms we can expect dif-ferent candidates (since D ( q )) may be different) as well as different judgments. In our experiments, one such judge di-rectly applied the Lucene scores of documents in the result set, while another judge ignored the search engine scores an d used document ranking instead, setting the document score to be score ( d )  X  rank  X  1 ( d ).
The final stage in candidate evaluation is to aggregate the scores from the different judges for each label. Given a list of judges, ( J 1 , ...J m ), each candidate label is scored using a linear combination of the judge scores: where P i  X  i = 1. Finally, the set of top-k scored candidates are recommended for cluster labeling.

The  X  X ptimal X  set of judge weights {  X  } m i =1 can be learned using standard linear regression methods, given some train -ing data. In this work we experimented with several  X  X ea-sonable X  X ets of weights while leaving optimization for fut ure work.
We used two data collections for experimenting with the system. The first one is the 20 News Groups (20NG) data collection [1], which consists of newsgroup documents that were manually classified into 20 different categories. Each category includes 1,000 documents, for a total collection s ize of about 20,000 documents. The second collection was gath-ered by downloading pages from the Open Directory Project (ODP) [14]. For this purpose, we randomly selected 100 dif-ferent categories from the ODP hierarchy. Example cate-gories include, among others, sub-categories of the top lev el ODP categories such as Ceramic Art and Pottery . From each category we then randomly selected up to 100 docu-ments, resulting in a collection size of about 10,000 docu-ments.
 In both collections, the categories were manually labeled. These ground-truth  X  X orrect X  labels were later used to eval -uate our labeling system. We followed the evaluation framework proposed by [21]. In this framework, a proposed label for a given cluster is con -sidered correct if it is identical, an inflection, or a Wordne t synonym of the cluster X  X  correct label 5 . This is a conserva-tive evaluation approach that severely evaluates the label ing system while ignoring good labels that do not comply with these restrictive rules. Therefore, the evaluation scores re-ported in this work can be considered as lower bounds on the system X  X  real performance.

Given a collection of clusters, and the parameter k that indicates the number of required cluster labels, the system proposes up to k labels for each cluster. The system parame-ters we experimented with are the feature selection method, the number of important terms for querying Wikipedia, the number of Wikipedia results to be used for label extraction, and the judges used for candidate evaluation.

For each configuration, we evaluated the system X  X  perfor-mance using two measures:
Recall that given a set of top-k proposed cluster labels, a user might need to determine which labels among the top-k labels are the correct ones. Therefore, it is preferable for the system to provide the shortest possible list of suggesti ons that contains the correct label. Both measures evaluate thi s system capability. The higher these measures are and the lower k is, the better the system X  X  effectiveness as perceived by the user.
We now explore the effectiveness of using candidate labels extracted from Wikipedia in addition to important terms extracted directly from the cluster data. We also compare four different feature selection methods for identifying im -portant terms: the JSD method described in Section 3, the terms with highest ctf-cdf-idf values in the cluster centroid, and two standard feature selection methods, namely the mu-tual information and  X  2 methods (see [13] pages 263-267). For this purpose, we evaluated the system X  X  proposed la-bels with and without the candidate labels extracted from Wikipedia. In this experiment, we fixed the system X  X  param-eters to be 1) 20 important terms for querying Wikipedia, 2) 100 Wikipedia results for candidate extraction, and 3) the SP(rank) judge for candidate evaluation.
A candidate is considered WordNet synonym of a label if both appear in the same WordNet X  X  synset. Figure 2: The effectiveness of Wikipedia labels en-hancement for (a) 20NG data and (b) ODP data.
 Wikipedia labels provide an overall best perfor-mance for cluster labeling.

Figure 2 reports on the Match@K scores of each method for increasing values of k . As can be observed, using the im-portant terms extracted by the JSD method is much more ef-fective than the highest weighted terms baseline (up to 100% improvement for the 20NG data and 60% for the ODP data). It further performs well compared to the two other standard methods, where for the ODP data it even completely domi-nates these methods. We can further observe that enhancing the JSD important terms with Wikipedia X  X  labels provides the overall best performance, with Match@5 &gt; 0.85 and up to 39% and 21% improvement over the JSD method for the 20NG and ODP data, respectively.

It is also interesting to note that feature selection method s on ODP data require at least 50 terms to cover 85% of the clusters with a correct label, while the same effectiveness is achieved by a list of 5 terms only using Wikipedia. A similar observation holds for the 20NG dataset  X  17 terms are needed to cover 75% of the clusters while only 2 terms are needed using Wikipedia. Moreover, all the four feature selection methods fail to achieve the same performance of Wikipedia X  X  top-5 terms, even when considering the top-50 labels.

To conclude, these results clearly show that combining labels extracted from Wikipedia together with important terms extracted from the text is a very effective cluster la-beling approach.
We identify two significant parameters that can affect the quality of Wikipedia X  X  labels. The first parameter is the number of important terms that are used to query Wikipedia. The second is the number of top scored results from which candidate labels are extracted.

The number of the query X  X  important terms may affect the quality of documents returned as results. As we add more terms to the query, precision is expected to drop hence the higher the chance to have irrelevant documents to the clus-ter X  X  topic, therefore low quality candidates can be expect ed. The number of top results to consider affects the total num-ber of unique candidates that are extracted and evaluated. Analyzing too few results implies that good candidates will be omitted. Analyzing too many results corresponds to low quality candidates.

We now analyze the effect of these two parameters on the performance of the system using both data collections. We used the SP (rank) judge and measured the MRR score Figure 3: System X  X  performance with respect to the number of query terms and the number of top scored documents that are considered for the candidate la-bels extraction. for different parameter values on both collections. Figure 3 shows the results of this analysis. Figure 3(a) illustrate s the effect of the number of query terms. We observe that, for both data collections, with up to 20 terms, the system keeps gaining in performance and then, there is a plateau. We attribute this plateau to our choice of the query X  X  terms boosting scheme, which boosts terms according to their JSD score. The results show that adding very low boosted terms to the query is not detrimental.

Figure 3(b) illustrates the effect of the number of results that are considered. We observe that with up to 300 results for the 20NG data and with up to 75 results for the ODP data the system keeps gaining in performance, probably since more good candidates are discovered. For more docu-ments, the system X  X  performance degrades. This is probably due to the fact that low scored results are irrelevant to the cluster X  X  topic. Hence, candidate labels extracted from th ose results are irrelevant and introduce noise into the system X  s decision making scheme.
We next compare the performance of the different judges proposed in Section 3.5 for candidate evaluation. The sev-eral judges are evaluated according their ability to identi fy the correct labels, or more precisely, to rank the correct labels on top of the label list. For this evaluation, we sepa-rately apply each judge in the candidate evaluation process and measured the MRR score achieved by the system. We further apply all judges together, aggregating their score s while assigning each judge J i a weight  X  i that is relative to its performance when it is was evaluated separately. We report on the MRR@5 score of each of the judges and the ag-gregated score of all judges for both data collections. Figu re 4 shows the results of this comparison.

We first observe that for all judges, as k increases (i.e., more cluster labels are proposed) the MRR score increases. Overall, among the four different judges, the SP (rank) judge performs the best. Among the two instantiations of the MI judge, the one using the web corpus (denoted MI(Web) ) outperforms the one using the Wikipedia corpus (denoted MI(Wikipedia) ), with up to 10% better MRR score for both datasets. This may be attributed to the fact that the web corpus is much larger then the Wikipedia corpus hence statis -tics may be more accurate.

The two instantiations of the SP judge completely dom-inate the two instantiations of the MI judge (up to almost 70% better MRR score). Finally, we observe that using the Figure 4: The system X  X  MRR score for (a) 20NG data and (b) ODP data, using every judge separately and aggregating the scores of all judges. aggregated score of all judges slightly improves the system  X  X  performance for the ODP data.
A cluster of documents given to the labeling component is usually the corresponding result of the clustering algorit hm used by the system. Given a collection of documents clus-tered by a specific clustering algorithm, the clusters X  qual ity expresses how documents within any cluster are similar, and how dissimilar the pairs of clusters are. We now explore the effect of the cluster X  X  coherency on the labeling process.
Given a set of clusters C = { C 1 , C 2 , . . . , C n } we measure the clusters X  coherency as follows: where sim in ( C i ) is the cluster X  X  C i inner similarity, which is defined as the (weighted) average (cosine) similarity be-tween cluster C i  X  X  documents and its centroid. The outer similarity sim out ( C ) is a normalization factor that captures the similarity between each pair of clusters, and is defined as the average (cosine) pairwise similarity between C  X  X  clus-ter centroids. Therefore, the higher the inner similarity a nd the lower the outer similarity, the more coherent the cluste rs are.

To simulate noisy clusters in our framework, we intro-duced different noise levels into the original clusters, whi ch resulted in different levels of clusters X  coherency. To pro-duce noisy clusters, we adapted the method of [21]. For each cluster C i  X  C and a noise level p  X  [0 , 1], each docu-ment d  X  C i is swapped with a random document d 0 from another random cluster C j 6 = i  X  X  with probability p [21].
We now report on the effect of cluster coherency on the cluster labels proposed by the system using the two datasets . We introduced different levels of noise (up to noise level p = 0 . 4) into the each dataset X  X  original clusters. For each noise level p , we repeated the experiment 10 times and mea-sured the average MRR score of the SP (rank) judge with and without Wikipedia labels. Figure 5(a) shows the average cluster coherency level as a function of the noise level we in -troduced to the two datasets. We observe that, introducing more noise will result in less coherent clusters. Figure 5(b ) shows the average MRR score per noise level measured for the two datasets. We observe that as expected, introducing more noise will result in less coherent clusters and therefo re, the MRR score drops. Nevertheless, the drop in MRR score Figure 5: (a) The effect of noise level on the clus-ters X  coherency. (b) The effect of noise level on the system X  X  performance. A higher noise level implies less coherent clusters with a moderate drop in the MRR score. per noise level is quite moderate for both datasets which implies that the proposed system is robust and has good resiliency to noise.
In this work we investigated how cluster labeling can be enhanced by utilizing the Wikipedia knowledge-base. We described a general framework for cluster labeling that ex-tracts candidate labels from the text and from Wikipedia and then retrieves the top scored candidates according to th e evaluation of several independent judges. Our experimenta l results reveal that meta-data associated with Wikipedia ar -ticles, which are similar to the cluster X  X  textual content, can provide very good labels for clusters of textual documents. Our candidate extraction approach is based on identifying Wikipedia articles that are similar to the cluster X  X  conten t and then extracting titles and categories from those pages. This process can be enhanced in several ways. First, other types of meta-data from Wikipedia pages can be considered for labeling. For example, anchors of Wikipedia pages (the fragments of text associated with the page X  X  in-links and ou t-links) might provide good labels since an anchor-text of a hyper-link to a Wikipedia page often successfully describe s the topic of that page. Another enhancement direction is to consider the hierarchical structure of Wikipedia categori es. By analyzing the categories graph, candidates can be evalu-ated according to their specificity versus generality, i.e t heir relation with their ancestors and descendants in the hierar -chy.

Cluster labeling with Wikipedia is extremely successful, a s shown by our results, especially in collections of document s whose topics are covered well by Wikipedia concepts. For domain specific collections, with topics that are not com-pletely covered by Wikipedia, the proposed candidates may hurt the system X  X  performance due to their irrelevance to the documents X  topics. For such collections, an intelligen t decision should be made regarding the use of Wikipedia or another external resource; alternatively, a choice could b e made to focus only on inner terms for labeling. The deci-sion should be made by analyzing the given collection with respect to Wikipedia. Developing such a collection specific decision making as part of the labeling framework is left for further research.
 This work was done as a project for the IBM Classifica-tion Module (ICM). We would like to thank our colleagues: Michal Rosen-Zvi, Yaara Goldschmidt, and Josemina Mag-dalen, for useful discussions and feedback.
