 In this paper, we consider a novel scheme referred to as Cartesian contour to concisely represent the collection of frequent itemsets. Different from the existing works, this scheme provides a com-plete view of these itemsets by covering the entire collection of them. More interestingly, it takes a first step in deriving a gen-erative view of the frequent pattern formulation, i.e., how a small number of patterns interact with each other and produce the com-plexity of frequent itemsets. We perform a theoretical investigation of the concise representation problem and link it to the biclique set cover problem and prove its NP-hardness. We develop a novel ap-proach utilizing the technique developed in frequent itemset min-ing, set cover, and max k-cover to approximate the minimal bi-clique set cover problem. In addition, we consider several heuristic techniques to speedup the construction of Cartesian contour. The detailed experimental study demonstrates the effectiveness and ef-ficiency of our approach.
 H.2.8 [ Database Management ]: Database Applications X  Data Min-ing Algorithms, Theory frequent itemsets, Cartesian product, set cover, concise pattern rep -resentation
Frequent pattern discovery has initiated the acceleration of data mining research and is also becoming a core building block of data mining. Starting from frequent itemset mining [2], frequent pat-tern mining has grown into a rich subject in data mining, ranging over a variety of pattern domains. The powerful toolbox of fre-quent pattern mining not only serves as a basis for a list of other data mining techniques and more importantly, it can directly help researchers gain insight into their data [7]. Typically, domain ex-perts try to identify the interesting frequent patterns and associate them with domain knowledge for domain specific discovery. How-ever, a major drawback of the frequent pattern mining methodology is its often unwieldy number of discovered patterns, despite several existing efforts [11, 13, 5, 18, 8, 17]. Indeed, the collection of fre-quent patterns themselves are becoming the  X  X ata X  which needs to be mined.

The call to better understand a large collection of frequent pat-terns is resonated with a list of recent efforts in pattern summariza-tion or concise pattern representation on itemsets. Simply speak-ing, these methods try to help gain a global view of the whole col-lection of frequent itemsets. In [1], the authors propose to use K itemsets as a concise representation to approximately cover the ma-jority of the frequent itemsets. Typically, those itemsets are either maximal frequent itemsets or close to being maximal, with the con-dition that most of their subsets are frequent (subject to false posi-tive constraint). Several later works apply probability inference to restore the frequency of frequent itemsets [10, 19, 15].
In this paper, we consider a novel scheme to concisely represent the collection of frequent itemsets. Different from [1], this scheme provides a complete view of these itemsets by covering the entire collection of them. More interestingly, it takes a first step in deriv-ing a generative view of the frequent pattern formulation, i.e., how a small number of patterns interact with each other and produce the complexity of frequent itemsets. Here, the interaction is repre-sented as the union of two frequent itemsets. Intuitively, for some core itemsets, if they are frequent, then all the subsets of their union are also likely to be frequent. To make this observation useful in concise pattern representation, we further introduce the generalized Cartesian product between any two sets of the itemsets to include all the unions between any pair of itemsets, one from each set. Each such product can be considered as a basic source for the frequent itemsets and several such products will be utilized to represent the entire collection of the frequent itemsets.

Next, we formulate our problem precisely.
Given a collection of frequent patterns F  X  with  X  being the min-imum support level on transactional database D where I is the set of all items. we would like to find a concise representation to approximate the collection of frequent patterns. Our approx-imation scheme is as follows: Let X = X 1 , X 2 , , X m and let Y = Y 1 , Y 2 , , Y l , where X i and Y j are the itemsets, i.e. X i  X  I and Y i  X  I . We define a generalized Cartesian product (  X  ) between two sets of itemsets X and Y : X X  X  = { X 1  X  Y 1 , X 1  X  Y 2 , , X 1  X  Y l , , X m  X  Y 1 Given this, we define the covering sets of a set of itemsets as where 2 X i is the powerset of X i . Thus, the covering set of X  X  X  is
Given this, for a collection of frequent patterns F  X  , we seek a small number of generalized Cartesian products, which can cover F . Specifically, we would like to find a list of Cartesian products, i.e., X 1  X  X  1 , X 2  X  X  2 , X k  X  X  k , such that
Given this, we define the covering cost of a list of Cartesian prod-uct X 1  X  X  1 , X 2  X  X  2 , X k  X  X  k as the sum of covering costs from each individual Cartesian product: cost ( {X 1  X  X  1 , X 2  X  X  2 , X k  X  X  k } ) = X The covering cost of a Cartesian product cost ( X i  X  Y i defined differently depending on how we want to express the con-ciseness. In this paper, we define the covering cost of a Cartesian product as the sum of the sizes of its two sets, i.e., However, other cost functions are also possible. For instance, if we simply want to have the minimal number of Cartesian products, then, the cost is simply one, i.e. cost ( X i  X  X  i ) = 1 . Or if the cost needs to reflect the size of each itemset, the cost of covering by X i  X  X  i can also be defined as: The algorithms discussed in this paper can be generalized to handle these costs.

Given this, we can formally introduce the problem of construct-ing a concise representation of a collection of frequent itemsets.
D EFINITION 1. (Problem 1: Exact Representation) Find a list of Cartesian products, X 1  X  X  1 , X 2  X  X  2 , X k  X  X  k that C ( X 1  X  X  1 ) , C ( X 2  X  X  2 ) , . . . , C ( X k  X  X  where the cost ( {X 1  X  X  1 , X 2  X  X  2 , X k  X  X  k } ) is minimal.
Let us consider an example of Exact Representation. For the dataset in table 1, we only need three generalized Cartesian prod-ucts as shown below to exactly represent all the frequent itemsets with support  X  = 1 / 20 : The above problem requires the covering set from the list of Cartesian products to be exactly equal to the collection of frequent itemsets. In many cases, we may want to trade the exactness with the conciseness. In other words, we can relax the condition to al-low the cover to be approximate, especially, to cover some itemsets Table 1: A toy transactional database. Each cell is a set of items contained by one transaction. There is a total of 60 transac-tions. which may not be frequent itemsets, in order to reduce the cover-ing cost. Given this, we expand the collection of frequent itemsets F  X  to include itemsets which are nearly frequent and/or very simi-lar to a frequent itemset. We denote the expanded collection as There are several different choices for such an expanded collectio n. For instance, we may include the negative border, choose a slightly smaller support level, or require certain similarities. To handle all these different possibilities, we generalize the exact representation problem (problem 1 ) as follows.
 D EFINITION 2. (Problem 2: Approximate Representation) Given an expanded collection of frequent itemsets for F  X  as  X 
F  X   X  F  X  , find a list of Cartesian products, X 1  X  X  1 , X X k  X  X  k , such that C ( X 1  X  X  1 ) , C ( X 2  X  X  2 ) , . . . , C ( X cover F  X  : where the cost ( {X 1  X  X  1 , X 2  X  X  2 , X k  X  X  k } ) is minimal.
For example, if in the previous example we let  X  F  X  = F {{ A, G, H }} , then we can approximately represent the dataset of table 1 by two generalized Cartesian products:
Note that the exact representation problem becomes a special case of the approximate representation problem if we denote F . We also refer to both the exact and approximate representation of a collection of frequent itemsets as its Cartesian contour .
In this work, we make the following contributions. 1. We propose a new scheme (Cartesian contour) based on the generalized Cartesian products to concisely represent a collection of frequent itemsets. 2. We perform a theoretical investigation of this problem and link it to the minimal biclique set cover problem and prove its NP-hardness. 3. We develop a novel approach utilizing the techniques devel-oped in frequent itemset mining, set cover, and max k-cover to ap-proximate the minimal biclique set cover. 4. We consider several techniques to efficiently construct the Cartesian contour using our minimal biclique set cover algorithm. 5. We perform a detailed experimental study and demonstrate the effectiveness and efficiency of our approach.
In this section, we perform a theoretical investigation of the prob-lems in finding the minimal Cartesian contour for a collection of frequent itemsets. We link this problem to the minimal biclique set cover problem and demonstrate its NP-hardness.

We first simplify our concise representation from covering the collection of frequent itemsets to covering only the maximal fre-quent itemsets. Let M  X  = { M 1 , M 2 , . . . , M p } be the set of max-imal frequent itemsets under support level  X  . By definition, an itemset is maximal frequent if none of its supersets are frequent. Hence it X  X  easy to see that C ( M  X  ) = F  X  . Thus, we can simplify our problems by focusing on covering the maximal frequent item-sets.

Let M ( X i  X  X  i ) record only the maximal itemsets covered by the Cartesian product: Then, we basically want to find a list of Cartesian products, X Y , X 2  X  X  2 , X k  X  X  k , such that they cover all the maximal frequent itemsets M  X  : where the cost ( {X 1  X  X  1 , X 2  X  X  2 , X k  X  X  k } ) is minimal.
Now we can transform our problem as a biclique set cover in the bipartite graph. Let S be the ground set to be covered. Let G = ( A  X  B, E ) be a bipartite graph where an edge in E connects two vertices from A and B , respectively. Each edge e is associated with a subset S e  X  S and S =  X  e  X  E S e . Let C = ( X , Y , X  X Y ) be a subgraph of G , where X  X  A , Y  X  B , X  X Y  X  E . Thus, C is a biclique. Let S ( C ) be the union of sets covering all it X  X  edges, i.e., S ( C ) =  X  e  X  X  X Y S e . Further, we associate a cost with each biclique C , denoted as cost ( C ) . Given this, the minimal biclique set cover problem is defined as follows.

D EFINITION 3. (Minimal Biclique Set Cover) Given the ground set S and a bipartite graph G , where S =  X  e  X  E S e , we seek a list of bicliques, C 1 , C 2 , , C k , to cover the ground set, i.e.,
Figure 1 shows an example of minimal biclique set cover. Each edge of G is associated with a subset of the ground set, as marked on that edge. We can use bicliques C 1 and C 2 from G to cover the ground set.

We can transform our problem to a minimal biclique set cover problem as follows. We construct a bipartite graph, where each of its vertices corresponds to an itemset. Basically, let A = { X 1 , X 2 , . . . , X m } containing m itemsets and B = { Y 1 , Y 2 , . . . , Y l } containing l itemsets. At this point, for the easy of understanding of our problem, we can simply assume A = B = F  X  . Then, we construct an edge between itemsets X and Y j if and only if X i  X  Y i is a frequent itemsets, i.e., X  X  F . We further assign the set of maximal frequent itemsets being covered by X i  X  Y i , to the edge ( X i , Y i ) in the bipartite graph. Clearly, for the exact representation,  X  F  X  = F  X  , each edge either has a singleton (covering only a single maximal frequent itemset), or the empty set (  X  ). However, when  X  F  X   X  F  X  , each edge can associate a set with more than one maximal frequent itemset. In addition, we define the cost of a biclique C = ( X , Y , X  X Y ) as the cost of its corresponding Cartesian product: Thus, we have demonstrated that our problem is an instance of the minimal biclique set cover problem.

The NP-hardness of the minimal biclique set cover can be easily observed by noting that a single biclique can represent a candidate set. In addition, we have the following NP-hardness results of our Cartesian contour problems. The Proof is omitted due to the space limit.

T HEOREM 1. Problem 1 (Exact Representation) and 2 (Approx-imate Representation) are NP-hard.
In this section, we will develop a novel algorithmic framework to handle the minimal biclique set cover problem, which is the gener-alization of our Cartesian contour problem. In the next section, we will consider how to leverage this framework effectively to handle our Cartesian contour problem by considering how to construct a sparse bipartite graph and how to handle the approximation.
Since our problem is essentially using bicliques of a bipartite graph G to cover the ground set S . A natural approach is the greedy set cover algorithm [6]. Let R be the covered subset of S . For each biclique (or sub-biclique) C = ( X  X  X  , X  X Y ) in the bipartite graph G , we define the price of C as:
By greedy set cover algorithm, we will choose a biclique with minimum price in each iteration until the ground set S is covered ( R = S ) and we will have a logarithmic approximation bound [6]. However, the problem is that the total number of bicliques in the bipartite graph G is exponential ( O (2 | A | + | B | ) ). Thus, to directly apply the greedy set cover algorithm is computationally intractable as the number of candidate sets (bicliques) is too large
Our basic strategy to tackle this problem is as follows. First, we utilize the connection between frequent itemset mining with bi-cliques in a bipartite graph to quickly identify  X  X ne-side maximal bicliques X  (defined in section 3.3), which can be much smaller than the number of total bicliques. Then, we introduce a fundamental lemma which can select a sub-biclique from a (maximal) biclique with the cheapest price linearly and with a constant approximation bound. In the following we first introduce the fundamental lemma.
In this subsection, we introduce a fundamental lemma and al-gorithm to identify a sub-biclique with cheapest price. This tool forms a basic building block of our approach to construct the min-imal biclique set cover.
 D EFINITION 4. (Cheapest Sub-biclique Problem) Given a clique C = ( X  X  Y , X  X  Y ) , where each of its edges e  X  X  X  Y associates with a set S e . We would like to find a sub-biclique C  X  = ( X  X  X   X  , X  X Y  X  ) from C , where Y  X   X  Y , with the low-est covering price:
Note that we can easily generalize this problem to consider a set of elements, denoted as R , which has already been covered in the biclique. In this case, we can simply treat the set associated with each edge e as S e \ R . Clearly, the solution to our above problem can directly handle such generalization. Therefore, the price of a sub-biclique here is defined as  X  ( C  X  ) = |X| + |Y  X  | | S not consider R as in Formula ( 1).

It is easy to see the brute-force algorithm cannot work here be-cause the number of candidate sub-bicliques is 2 |Y| . We will in-troduce a polynomial time algorithm for this problem. To explain our algorithm, we first transform it into a classical SET COVER problem setting. Let Y = { Y 1 , Y 2 , . . . , Y |Y| } , and let C { Y j } , X  X { Y j } ) be a biclique containing only a single Y -vertex and the entire X vertices, referred to as single-Y -vertex biclique . Now, we treat each single-Y -vertex biclique C j , 1  X  | C as a candidate set, which covers all the elements associated with each of its edges, S e  X  X  X { Y of |Y| candidate sets (as well as single-Y -vertex bicliques).
Now, we consider the following variant of the set cover problem, the MAX k -COVER problem, which seeks k sets whose union has maximal cardinality. Specifically in our problem setting, Max k -COVER is to get k single Y -vertex bicliques of C , such that they cover a maximal number of elements. Notice that the union of the k single Y -vertex bicliques of C actually forms a biclique. Then, the MAX k -COVER problem in this setting basically corresponds to finding a sub-biclique ( X  X  X   X  , X  X Y  X  ) of k vertices in Y , |Y k , which maximize  X  e  X  X  X Y  X  S e . This is equivalent to minimizing for any fixed k . It is well-known that the greedy algorithm provides a bounded ( 1  X  1 e ) approximation ratio to this problem [9]. Note that the difference between our Cheapest Sub-biclique Problem and the MAX k -COVER problem is that the latter needs a fixed k and the former does not have such constraint. Given this, we simply need to generalize the greedy algorithm for the MAX k -COVER problem to help identify the cheapest sub-biclique.

Our greedy procedure for the cheapest sub-biclique is as follows. 1) We initialize an empty vertex set Y  X  =  X  ; 2) At each iteration, we choose the single-Y -vertex biclique C i with the maximal number of uncovered elements, 3) We try to add the vertex Y i to Y  X  to build a new biclique as reduced by the new vertex, i.e., we add Y i to Y  X  . Otherwise (we find the new vertex does not reduce the price), we will stop the iteration, and return the current biclique ( X  X  X   X  , X  X Y  X  ) as the one with cheapest price in C .
We refer to this procedure as OptimalSubBiclique . It has a worst case time complexity O ( |Y| 2 |X| ) , as in each iteration we need to update every single-Y -vertex biclique in C , pick up a lowest-price one and add it into the current biclique. This procedure can find a ( e  X  1 ) approximation ratio for our Cheapest Sub-biclique Problem.
L EMMA 1. Let the C  X  be the lowest-price sub-biclique in { ( X , Y  X  ) |Y  X   X  Y} . The OptimalSubBiclique procedure finds a sub-biclique C  X   X  X  ( X , Y  X  ) |Y  X   X  X } such that  X  ( C  X  Proof Sketch: Without loss of generality, let us assume the selected sub-biclique at the i -th iteration by greedily choosing the single-Y -vertex biclique with the maximal number of uncovered elements as C  X  i . However, our algorithm stops at the t + 1 -th iteration (i.e., C  X  = C  X  t ) when the price stops decreasing.
 To prove this lemma, we first prove the following claim: If in our OptimalSubBiclique algorithm, we do not stop greedily choosing and adding single-Y -vertex bicliques, the price of the later formed Basically, we will show: where f ( C  X  i ) is the number of elements covered by sub-biclique C .

To show this, it is easy to observe that the coverage size of a single-Y -clique will never go up, i.e., Thus, we have for any i &gt; t , from
Now we prove the lemma. It is straightforward that  X  ( C  X   X  ( C  X  ) because C  X  is the lowest-price sub-biclique. Therefore we focus on the proof of  X  ( C  X  )  X  e e  X  1  X  ( C  X  ) .

We consider the optimal sub-biclique of C for a fixed size k on the Y vertices, denoted as C  X  k : Thus, we can redefine the cheapest price sub-biclique C  X 
Given this, it is easy to see that our OptimalSubBiclique sim-ply applies the MAX k-COVER greedy procedure to find a sub-biclique C  X  k at each iteration which covers a maximal number of elements ( f ( C  X  k )) for ( k  X  t + 1) . This is within a (1  X  proximation ratio of the optimal one:
Then we can see  X  ( C  X  k )  X  e
Put together, we have 2
Figure 2 shows an example of OptimalSubBiclique : Given biclique C as shown in (1), we first choose the single-Y -vertex biclique Y 3  X  X  X 1 , X 2 , X 3 } with the greatest number (here is 4) of uncovered elements and put it into C  X  , as shown in (2). Then  X  ( C  X  1 ) = 4 / 4 = 1 . Following the same rule, secondly we choose the single-Y -vertex biclique Y 2  X  X  X 1 , X 2 , X 3 } with 3 uncovered elements and put it into C  X  , as shown in (3). Then  X  ( C Lastly we choose the single-Y -vertex biclique Y 1  X  X  X 1 Now it has only 1 uncovered element and the price  X  ( C  X  which is greater than 5 / 7 . Therefore we revert the last step and the final output is the biclique C  X  2 as shown in (3).
Based on OptimalSubBiclique and Lemma 1, we can find low-price sub-biclique, where one-side is fixed, from a given biclique. To release the power of this tool, we introduce the notation of one-side maximal biclique . Given a bipartite graph G = ( A  X  B, E ) , for a biclique C = ( X  X  X  , X  X Y ) , where X  X  A , Y  X  B , and X  X Y  X  E , if we add an extra vertex to X , it will not be a biclique of G , then, we refer to it as a A -side maximal biclique . Similarly, we refer to it as B -side maximal biclique. In other words, for instance, a B -side maximal biclique of G can be simply written as where B ( X ) = { b | ( x, b )  X  E for any x  X  X } simply includes all the vertices in B , which links to every vertex in X . An A -side or B -side maximal biclique is called as a one-side maximal biclique . It is easy to see that if a biclique is both A -side maximal and B -side maximal, it is a maximal biclique [12].

Recall in the SET COVER greedy algorithm (Subsection 3.1, we need to find a biclique with lowest price, which needs an enumera-tion of O (2 | A | + | B | ) bicliques of G . We can achieve such a task us-ing the B -side maximal biclique and the OptimalSubBiclique pro-cedure as follows. Let U be the set of all B -side maximal bicliques of G : Then, we can visit each B -side maximal biclique of U and invoke the OptimalSubBiclique procedure to find its lowest price subbi-clique, and then choose the one with lowest price among these sub-bicliques (a total of | U | ). However, the number of bicliques in U is still in the order of O (2 | A | ) . To bring down the number of exponen-tial B -side maximal bicliques, we propose to approximate U using frequent itemset mining technique. This is quite interesting as our initial goal is to concisely represent a collection of frequent item-sets, and its solution also needs frequent itemset mining, though on a different transactional database which is derived from the bipar-tite graph.
 Frequent itemsets and One-side Maximal Bicliques: To reveal the relationship between frequent itemsets and one-side maximal bicliques, we will have to represent the bipartite graph G = ( A  X  B, E ) as a transactional database D ( G ) . Let each row correspond to a vertex in A and each column corresponds to a vertex in B and if an edge between a vertex a and b , then, we have D ( a, b ) = 1 . For a support level  X  , let F  X  [ D ( G )] be the collection of all frequent itemsets whose support is greater than or equal to  X  . Further, for a frequent itemset I  X  F  X  [ D ( G )] , let T ( I ) be its supporting trans-actions, i.e., the transactions have I as the subset. Then, we can see the Cartesian product I  X  T ( I ) = { ( i, t ) | i  X  I and t  X  T ( I ) } corresponds to a B -side maximal biclique ( I  X  T ( I ) , I  X  T ( I )) of G , where I  X  A and T ( I )  X  B .
 Figure 3: Representing a bipartite graph as a transactional database
Given this, we construct the following set of B -side maximal bicliques to approximate U : Clearly, it includes only those B -side maximal bicliques, whose A -subsets either correspond to a frequent itemset in the transactional database D ( G ) , or contain a single vertex in A (for the complete-ness of set cover). Note that this allows us to utilize the fast frequent itemset mining algorithms to construct the set U  X  .

As an example, figure 3(1) is a transactional database converted from the bipartite graph shown in figure 1(1). Given  X  = 0 . 5 , figure 3(2) shows a B -side maximal biclique (there are 9 in total). Overall Algorithm: The general sketch of our biclique-covering algorithm is illustrated as Algorithm 1, which takes G , U In each iteration it will find the lowest-price sub-biclique (with X on the A side) from each biclique C ( X X  B ( X ) , X X  B ( X ))  X  U using the OptimalSubBiclique procedure (Line 4 ). Then it selects the cheapest C  X  from the set of selected sub-bicliques (Line 5 ). C will be added into the set of Bicliques _ F ound (Line 6 ). Set R which records the covered elements which are associated with each edge in the bipartite graph, and will be updated accordingly (Line 7 ). The while loop terminates when the ground set S =  X  e  X  E covered, i.e. R = S (Line 3 -8 ).

We note that there are well-established techniques to speedup the greedy set cover procedure [16, 14] and both our BicliqueCover al-gorithm and OptimalSubBiclique procedure can employ them. The basic idea of these techniques is to maintain a priority queue of the candidate sets and sort them based on their price. However, since the price of each candidate set cannot decrease (since more ele-ments are being covered, i.e., R grows), we can apply this property to prune the search space for each iteration significantly. Due to the space limitation, we omit the details here. Algorithm 1 BicliqueCover( G , U  X  ) Require: G = ( A  X  B, E ) is the bipartite graph Require: U  X  is the set of B -side maximal bicliques of G 1: R  X  X  X  ; 2: Bicliques _ F ound  X  X  X  ; 3: while R 6 =  X  e  X  E S e do 4: call OptimalSubBiclique to find C  X  = ( X  X  X   X  , X  X Y  X  5: choose the C  X  with minimum  X  ( C  X  ) among all the ones dis-6: Bicliques _ F ound  X  Bicliques _ F ound  X  X  C  X  } ; 8: end while 9: return Bicliques _ F ound
This algorithm has a bounded approximation ratio with respect to the candidate sets U  X  as stated in Theorem 2.
 The proof is omitted due to the space limit.

T HEOREM 2. The Biclique_Cover algorithm has e e  X  1 (ln( n ) + 1) approximation ratio with respect to the candidate set U Time Complexity of BicliqueCover Algorithm: Here, we con-sider the worst case time complexity of our algorithm (the speedup techniques are not considered since they typically do not reduce the worst case time complexity.) Further the BicliqueCover procedure assume the frequent itemsets of the transactional database D ( G ) transformed from the bipartite graph is given. Thus, the overall running time for the minimal biclique set cover problem includes the time of running the frequent itemsets mining algorithm, which we do not consider for the time complexity analysis of Biclique-Cover algorithm.

The worst case time complexity of the BicliqueCover algorithm bicliques in the final result. Assume the while loop in Algorithm 1 runs k times. Each time it chooses a C  X  with minimum  X  ( C from U  X  , which contains no more than | F  X  [ D ( G )] | + | A | can-didates. The OptimalSubBiclique procedure takes O ( | B || B || A | ) time. Since we need to do so for every biclique in U  X  , it takes is bounded by ( | F  X  [ D ( G )] | + | A | )  X | B | since each biclique in U  X  can be visited at most | B | times. Thus, we conclude that our BicliqueCover algorithm runs in polynomial time with respect to | F  X  [ D ( G )] | , | A | and | B | .
In the previous section, we present the BicliqueCover algorithm for the minimal biclique set cover algorithm. Following the discus-sion in Section 2, we will try to transform the Cartesian contour problem into a minimal biclique set cover problem. However, the time to construct the bipartite graph G = ( A  X  B, E ) can sig-nificantly affect the efficiency of the algorithm. This is because our algorithm relies on finding the itemsets in the transactional database D ( G ) , which correspond to the bipartite graph G . If G is very dense, then the itemsets of D ( G ) even with a high support level may be very large. Besides, since A corresponds to the items (columns) in the transactional database, we also need A to be small. Otherwise, the search space for the itemsets is too large. Given this, it is clear that the simple transformation as described in Section 2, which lets A and B contain all frequent itemsets F  X  , and links an edge between two itemsets if their union is in  X  F  X  , is prohibitive for a large number of maximal frequent itemsets.

In the following, we will introduce several techniques which help with constructing sparse bipartite graphs and present our overall algorithm for deriving a concise Cartesian Contour representation for a large number of (maximal) frequent itemsets.
In this subsection, we introduce a method to reduce the size of vertex sets A and B of the bipartite graph. Recall our goal is to use general Cartesian products (i.e. bicliques) to cover the collection of maximal frequent itemsets M  X  with minimum cost. The process is similar to the  X  X actorization X  of the M  X  , i.e., finding the fac-tors (the common itemsets) shared by a group of maximal itemsets. Considering a Cartesian product covering l  X  m maximal itemsets, then, each X i is contained in at least l maximal itemsets and each Y j is contained in at least m maximal itemsets. Thus we are interested in finding a complete set of  X  X aximal factors X  such that they are the largest subsets which are shared by a number of maximal itemsets in M  X  .

D EFINITION 5. (Maximal Factor Itemsets) An itemset X is a maximal factor itemset of M  X  if the following two condition holds: 1) X  X  M i  X  M j  X  . . .  X  M l where { M i , M j , . . . , M set of M  X  ; and 2) There does NOT exist another subset X that X  X  X  X  and X  X   X  M i  X  M j  X  . . .  X  M l .

The definition of maximal factor itemsets has the following im-portant property which enables us to reduce the size of A and B : Given a maximal factor itemset X which is a subset of M 1 . . .  X  M l and X  X   X  X , if
However, the reverse is not always true. Therefore, it is not hard to see that X  X  is a redundant information for our Cartesian contour (exact representation) problem as we can always use X to replace X . Regarding efficiency, we do not consider the the approximate representation problem here (for constructing A and B ). The next subsection will deal with that.

Now the problem is how we can find the maximal factor itemsets for M  X  . The following lemma builds the connection between the notion of maximal factor itemsets and the notion of closed item-sets . Given a transaction database, a closed itemset is an itemset all of whose immediate supersets do not have the same support as its. In the context of frequent itemset mining, we typically give a minimum support and refer to the closed itemsets whose support is no less than the minimum support as the frequent closed itemsets. In other words, the closed itemsets are the frequent closed itemsets with support zero.

L EMMA 2. For a collection of maximal frequent itemsets, M we construct a transactional database DB ( M  X  ) , which records each maximal itemset in M  X  as a unique transaction. Thus, the database has a number of |M  X  | transactions. Then, the closed itemsets of DB ( M  X  ) are maximal factor itemsets of the collection of maximal itemsets M  X  .
 For simplicity, we omit the proof here. Given this, we utilize only the closed itemsets in A and B , which is much less than the number of frequent itemsets.
Even though we have reduced the size of A and B , we still need to consider how to reduce the edge density of the bipartite graph. Recall the straightforward solution is simply connecting two item-sets (vertices) if their union is in the expanded frequent itemsets  X  F . Clearly, this will make the bipartite graph very dense. How-ever, if we construct a sparse bipartite graph, we may lose some good biclique candidates (or Cartesian products).

To deal with this problem, we introduce a novel  X  X wo-bipartite-graphs X  technique. First, we construct a sparse bipartite graph G which is used for finding the frequent itemsets with support  X  , F [ D ( G s )] . Then, we construct a dense bipartite graph G is used to build the set of B -side maximal bicliques, U  X  Sparse Bipartite Graph G s : We construct the edge set E s sparse bipartite graph G s = ( A  X  B, E s ) as follows: two vertices ( a, b ) have an edge if and only if the union of their corresponding itemsets is a maximal frequent itemset.
 Dense Bipartite Graph G d : We construct the edge set E d sparse bipartite graph G d = ( A  X  B, E d ) as follows: two vertices ( a, b ) have an edge if and only if the union of their corresponding itemsets is in the expanded set of frequent itemsets  X  F
Given this, after we find the frequent itemsets from G s , we con-struct the candidate biclique set as
U  X  = { ( X  X  B ( X ) , X  X  B ( X )) |X  X  F  X  [ D ( G s )]  X  A,
The idea is to reduce the number of frequent itemsets through the sparse bipartite graph ( | F  X  [ D ( G s )] | is small). Then, we use the dense graph to build a larger biclique by incorporating more vertices from B into each candidate bicliques. Since our Optimal-SubBiclique performs linear search over the vertices from B in each candidate biclique, we can traverse a large number of sub-bicliques without compromising the efficiency.
The last technique we consider is to handle the case where a large number of closed itemsets are being produced from DB ( M ) . In this case, we consider iteratively invoking the BicliqueCover pro-cedure, and utilize the frequent closed itemset here. Basically, we apply a minimum support  X  for DB ( M  X  ) and reduce it gradually in each round. Since we do not expect to cover M  X  in a single pass, we can also further shrink the size of A and B , by splitting those frequent closed itemsets into two disjoint parts. For instance, we can put all the itemsets whose size is smaller than a threshold in one dataset and the rest in another one. Then, we always let A be the part which has smaller number of itemsets, to reduce the search space of frequent itemset mining. In addition, we let A and B each contain an empty itemset so that coverage will be complete after several passes.

Given this, our overall procedure is as follows: 1) Find the frequent closed itemsets in DB ( M  X  ) ; 2) Split those discovered itemsets into A and B ; 3) Construct the sparse and dense bipartite graphs, G s and G respectively; 4) Find the frequent itemsets in D ( G s ) and build U  X  , the set can-didate bicliques, by using G d , A , and F  X  [ D ( G s )] ; 5) Invoke the BicliqueCover procedure to cover maximal frequent itemsets; 6) Remove the covered maximal itemsets from M  X  ; 7) Reduce the minimum support  X  gradually and then repeat the above steps until M  X  is empty (all maximal itemsets are covered). Consider the dataset of table 1 as a running example: Given sup-port  X  = 1 / 20 , the maximal frequent itemsets M  X  are listed in table 2. In addition, the frequent closed itemsets in DB ( M listed in table 3 given support  X  = 1 / 9 . Then we construct sparse and dense bipartite graphs as shown in Figure 4(1). The dark bold edges are edges in both sparse and dense bipartite graphs, and the light thin edges are edges in dense bipartite graph only. After in-voking the biclique cover procedure, we find three bicliques (i.e. three general Cartesian products) that cover all frequent itemsets F  X  with support  X  = 1 / 20 , as shown in Figure 4(2).

In this example our overall procedure finishes in one round. How-ever, as an example, if M  X  contains an additional maximal fre-quent itemset { O, P, Q } , our overall procedure can finish in two rounds given the same support  X  = 1 / 9 . In addition, here we only considered exact representation. If we let  X  F  X  = F  X   X  X { A, G, H }} , the edge between { A } and { G, H } will be added to the sparse bi-partite graph, and the approximate representation will contain only two bicliques (i.e. C 2 and C 3 in figure 4(2) will merge into one biclique).
 Figure 4: (1) sparse and dense bipartite graph (2) three bi-cliques that cover the frequent itemsets F  X 
Our work is closely related to the goal defined by the spanning set approach [1]. In this work, the authors propose to use K item-sets as a concise representation of a collection of frequent itemsets. Basically, the K itemsets are chosen to maximally cover the collec-tion of frequent itemsets. They consider two important cases: 1) if the K itemsets themselves are frequent, i.e., being chosen from the collection, there will be no false positive coverage. Based on the down-closure property, a subset of any of the K itemsets must be frequent. 2) if the K itemsets can be chosen outside the collection, they require that those K itemsets satisfy a certain false positive ratio. They further show that finding such K itemsets corresponds to the classical set-cover problem and thus is NP-hard. The well-known greedy algorithm is applied to find the concise representa-tion. The main differences between their work and ours are in the Table 4: Datasets Characters. I is the total number of items and T is the total number of transactions following three aspects: 1) First, their work is a special case of our work as a spanning itemset X in [1] is simply a Cartesian product between itself and an empty set, i.e., { X } X { X  X  . Indeed, in our work, we do incorporate such cases into consideration by allowing one side of Cartesian product to be an empty set. Careful readers should have observed that in Figure 4(1) there are two non-labeled vertices which actually represent empty sets. 2) Secondly, the goal of their work is to use a small number of itemsets to maximally represent the collection of frequent itemsets. Thus, their work typ-ically can cover only a proportion of the frequent itemsets. Specifi-cally, if no false positive is allowed, the approach in [1] essentially need all the maximal frequent itemsets. In this sense, there will no reduction in terms of the conciseness. While in our work, we focus on representing all the frequent itemsets using a small number of itemsets. 3) Finally, our representation scheme based on the Carte-sian product between itemsets is clearly different from [1] and we develop a set of novel techniques utilizing the set cover and MAX k -COVER approach to discover such a scheme.

Several methods consider to restoring the frequency for the col-lection of frequent itemsets. Yan et al.  X  X  work [19] introduces a pattern-profile approach. It partitions the itemsets into K clusters, and all frequent itemsets in a cluster are estimated based on the item-independence assumption. Wang et al. [15] propose the con-struction of a global Markov random field (MRF) model to estimate the frequencies of frequent itemsets. This model utilizes the depen-dence between items, specifically, the conditional independence, to reduce their estimation error. Jin et al. [10] derive a regression-based approach to cluster the frequent itemsets and restore the item-set frequency based on the regression model. However, these ap-proaches cannot provide a global view of the collection of frequent itemsets and their power to predict which itemsets are frequent or not seems also to be limited by their frequency restoration error. Our goal here is different as we focus on the the concise repre-sentation of the collection of frequent itemsets. It is an interest-ing research problem as to how this approach can contribute to the itemset frequency restoration.

Finally, our problem is also related to the hyperrectangle cover-ing problem [16]. However, as we discussed in Subsection 2, this problem is only a special case of the minimal biclique cover prob-lem, and the method developed in [16] cannot handle our problem. Even though both works link their roots to the set cover problem, this work has developed a set of new techniques to handle the com-plexity of using bicliques for set cover. Especially, we develop a fundamental lemma based on the MAX k-Cover to approximate the low-price sub-bicliques from a clique and techniques to employ the minimal set cover problem for the Cartesian contour represen-tation.
In our experimental evaluation, we will focus on answering the following questions: 1. How does Cartesian contour representation summarize the 2. How does the approximate representation performs compared 3. How fast can we construct such representation?
Here, we report our experimental evaluation on 4 real datasets and 1 synthetic dataset. All of them are publicly available from the FIMI repository 1 . The basic characteristics of the datasets are listed in Table 4. Borgelt X  X  implementation of the well-known Apriori algorithm [3] was used to generate frequent itemsets. The maximal frequent itemsets and closed maximal frequent itemsets are generated by MAFIA algorithm [4], which is publicly avail-able online 2 . Our algorithms were implemented in C++ and run on Linux 2 . 6 on an Intel Xeon 3.2 GHz with 4GB of memory. For each dataset, we vary the support level from high to low. We perform both the exact Cartesian contour representation and the approximate Cartesian contour representation. For the approxi-mate representation, we specify the expanded collection of frequent least one maximal frequent itemset M  X  M  X  , such that I is very close to M with only one item difference.

Table 5, 6 and 7 show the results for real datasets connect , chess , and pumsb , respectively. We can see that on average the exact rep-resentations need around 16% to 37% less number of itemsets than the number of maximal itemsets. Most numbers of Cartesian prod-ucts for the representations are on the order of low tens. The ap-proximate representation needs much less number of itemsets to cover all the frequent itemsets. On average, they only need around 60% of the number of itemsets required by the exact representation.
Table 8 and table 9 show the experimental results for real dataset retail and synthetic dataset T40I10D100K , respectively. In both results, the cost of exact representation is a little higher than the number of Cartesian products. We found that this is because one side of the Cartesian product contains only one or few itemsets or is simply an empty set, and the other side contains a large number http://fimi.cs.helsinki.fi/data/ http://himalaya-tools.sourceforge.net/Mafia/ of itemsets. However, as we allow approximation, we can reduce the representation cost significantly.

In Table 8, we can also observe that as  X  decreases, both the to-tal number of frequent itemsets and the exact representation cost increases. However, the number of Cartesian products for the exact representation decreases. Note that the representation cost in the present work is the total number of itemsets used in all the Carte-sian product, and our algorithm focuses on minimizing this cost. A reason to choose this criteria is that we can always do the ex-act representation with one side of the Cartesian product being an empty set and the other side being the set M  X  . In this case, using a single Cartesian product, we may cover a large number of frequent itemsets, but this product does not provide much compression than simply listing each individual frequent itemsets. In general, if one side of the Cartesian product has only very few itemsets and then other side has many, the representation cost would also be high even the Cartesian product can cover a large number of itemsets. As we mentioned before, most of the Cartesian products found in both real dataset retail and synthetic dataset T40I10D100K have such characteristic. Indeed, this also explains why in Table 9 the number of Cartesian products for some approximate representation is higher than that for the exact representation, but the approximate representation cost is smaller.

In addition, our algorithm is rather efficient. On average, it takes around 277 s, 52 s, 116 s, and 1 s to generate the exact representa-tion for datasets connect , chess , pumsb , and retail , respectively; it takes around 261 s, 50 s, 56 s, and 1 s to generate the approximate representation for datasets connect , chess , pumsb , and retail , re-spectively. For the synthetic dataset, it takes on an average of 140 s and 424 s to produce the exact and approximate representations, re-spectively.
In this paper, we introduced a Cartesian contour representation for covering the entire collection of frequent itemsets by the ob-servation that shorter itemsets interact with each other to produce longer frequent itemsets. We use the generalized Cartesian product to formalize such interaction and allows the concise representation of a collection of frequent itemsets. We transformed our representa-tion problem as an instance of the minimal biclique covering prob-lem. Based on this, we first developed a general approach for the minimal bicilque covering problem. Then, we developed several techniques to adapt this general approach to the Cartesian contour construction. Our experimental evaluation shows that our approach is both effective and efficient to concisely represent the collection of frequent itemsets. In the future, we are interested in utilizing this representation for restoration of the frequent itemset frequency, an d to derive generative model for frequent itemsets. [1] Foto Afrati, Aristides Gionis, and Heikki Mannila.
 [2] Rakesh Agrawal, Tomasz Imielinski, and Arun Swami.
 [3] Christan Borgelt. Apriori implementation. [4] Douglas Burdick, Manuel Calimlim, Jason Flannick, [5] Toon Calders and Bart Goethals. Non-derivable itemset [6] V. Chv X tal. A greedy heuristic for the set-covering problem. [7] Jiawei Han and Micheline Kamber. Data Mining: Concepts [8] Jiawei Han, Jianyong Wang, Ying Lu, and Petre Tzvetkov. [9] Dorit S. Hochbaum, editor. Approximation algorithms for [10] Ruoming Jin, Muad Abu-Ata, Yang Xiang, and Ning Ruan. [11] Roberto J. Bayardo Jr. Efficiently mining long patterns from [12] Jinyan Li, Guimei Liu, Haiquan Li, and Limsoon Wong. [13] Nicolas Pasquier, Yves Bastide, Rafik Taouil, and Lotfi [14] Ralf Schenkel, Anja Theobald, and Gerhard Weikum. Hopi: [15] Chao Wang and Srinivasan Parthasarathy. Summarizing [16] Yang Xiang, Ruoming Jin, David Fuhry, and Feodor F. [17] Dong Xin, Hong Cheng, Xifeng Yan, and Jiawei Han.
 [18] Dong Xin, Jiawei Han, Xifeng Yan, and Hong Cheng.
 [19] Xifeng Yan, Hong Cheng, Jiawei Han, and Dong Xin.

