 Public gures such as politicians make claims about \facts" all the time. Journalists and citizens spend a good amount of time checking the veracity of such claims. Toward auto-matic fact checking, we developed tools to nd check-worthy factual claims from natural language sentences. Speci cally, we prepared a U.S. presidential debate dataset and built clas-si cation models to distinguish check-worthy factual claims from non-factual claims and unimportant factual claims. We also identi ed the most-effective features based on their im-pact on the classi cation models' accuracy.
 H.2.8 [ Database Management ]: Database Applications| Data mining ; H.4.m [ Information Systems Applications ]: Miscellaneous; I.2.7 [ Arti cial Intelligence ]: Natural Lan-guage Processing| Text analysis Algorithms, Design, Experimentation computational journalism; fact checking; text classi cation
Public gures such as politicians make claims about\facts" all the time. Oftentimes there are false, exaggerated and mis-leading claims on important topics, due to careless mistakes and even deliberate manipulation of information. With tech-nology and modern day media helping spread information to mass audiences through all types of channels, there is a press-ing need for checking the veracity of factual claims important to the public. Journalists and citizens spend good amount of time doing that. More and more dedicated platforms and institutes are being created for fact checking. According to a census from the Duke University Reporters' Lab, 1 the number of fact checking platforms such as PolitiFact.com and FactCheckEU.org has increased from 59 (May 2014) to 89 (January 2015), a 50.8% increase in eight months. This genre of investigative reporting has become a basic feature of political coverage, especially during elections, and plays an important role in improving political discourse and increasing democratic accountability [8, 5].

The process of fact checking requires many challenging steps|extracting natural language sentences from speeches, interviews, press releases, campaign brochures and social media; separating factual claims from opinions, beliefs, hy-perboles, questions, and so on; detecting topics of factual claims and discerning which are \check-worthy"; assessing the veracity of such claims, which itself requires collecting information and data, interviewing experts, and presenting evidence and explanations. 2
Part of the goal of computational journalism [3, 4] is use computing to automate fact checking [11, 9]. A fully au-tomatic fact checking system is not yet within our reach. It calls for breakthroughs in several fronts related to the aforementioned fact checking steps. This paper's focus is on detecting check-worthy factual claims from natural language sentences, speci cally transcripts of presidential debates.
We model this problem as a classi cation task and we follow a supervised learning approach to tackle it. We con-structed a labeled dataset of spoken sentences by presiden-tial candidates during 2004, 2008 and 2012 presidential de-bates. (Data collection for earlier debates is still in progress.) Each sentence is given one of three possible labels|it is not a factual claim; it is an unimportant factual claim; it is an im-portant factual claim. We trained and tested several multi-class classi cation models using the labeled dataset. Exper-iment results demonstrated promising accuracy of the mod-els. We further identi ed and analyzed the most-effective features in the models.

We envision, during presidential debates of U.S. Election 2016, for every sentence spoken by the two candidates and extracted into transcripts, our model will immediately pre-dict whether the sentence has a factual claim and whether checking its truthfulness is important to the public. Fur-thermore, factual claims will be ranked by their signi cance, which will help professional and citizen journalists focus on the righ t target. Although so far we have only collected data related to presidential debates, the studied models can be possibly applied on other types of text, including speeches, radio/TV interviews, and social media.

To the best of our knowledge, no prior study has focused on computational methods for detecting factual claims and discerning their importance. The most relevant line of work is subjectivity analysis of text (e.g., [12, 1, 10]) which clas-si es sentences into objective and subjective ones. However, not all objective sentences are check-worthy important fac-tual claims. Wu et al. [11] studied how to model the quality of facts and nd their supporting arguments and counterar-guments. Vlachos and Riedel [9] analyzed the tasks in fact checking and presented a dataset of factual claims collected from PolitiFact.com and Channel4.com . Another area of related research is checking information credibility in micro-blog platforms. For instance, [13] nds trending rumors containing disputed factual claims. [2, 6] focus on assigning credibility scores to tweets. The scoring models are highly dependent on Twitter-speci c features such as the credibility of twitter users. A tweet with high credibility does not necessarily contain a check-worthy factual claims.
We categorize sentences in presidential debates into three categories. Below, each category is explained with examples.
Non-Factual Sentence ( NFS ) : Subjective sentences (opinions, beliefs, declarations) and many questions fall un-der this category. These sentences do not contain any factual claim. Below are some examples.
 But I think it's time to talk about the future.
 You remember the last time you said that?
Unimportant Factual Sentence ( UFS ) : These are factual claims but not check-worthy. In other words, the general public will not be interested in knowing whether these sentences are true or false. Fact-checkers do not nd these sentences as important for checking. Some examples are as follows.
 Next Tuesday is Election Day.
 Two days ago we ate lunch at a restaurant.

Check-worthy Factual Sentence ( CFS ) : These sen-tences contain factual claims and the general public will be interested in knowing whether the claims are true or false. Journalists look for these type of claims for fact checking. Some examples are: He voted against the rst Gulf War.
 Over a million and a quarter Americans are HIV-positive.
Our goal is to automatically detect CFS s. We model it as a supervised learning problem. Speci cally, we model it as a multi-class classi cation problem where the classes are NFS , UFS and CFS .
In order to construct a dataset for developing and evalu-ating approaches to detect check-worthy factual claims, we used presidential debate transcripts. The rst general elec-tion presidential debate was held in 1960. Since then, there have been 14 elections till 2012. In 1964, 1968 and 1972, no presidential debate was held. There were 2 to 4 debate episodes in each of the remaining 11 elections. A total of 30 debate episodes spanned 1960{2012. We parsed the de-bate transcripts and identi ed the speaker for each sentence. There are a total of 123 speakers including 18 presidential candidates, moderators and guests. The whole dataset con-sists of 28029 sentences. We are interested in sentences spoken by the presidential candidates only. There are 23075 such sentences. We discarded very short sentences (less than 5 words long) and we were remained with 20788 sen-tences. Figure 1 shows the distribution of sentences among 30 debate episodes. Figure 2 shows the average length of sentences. These gures show that recent candidates used more sentences and shorter ones than earlier candidates.
To label the sentences, we developed a data collection website. Journalists, professors and university students were invited to participate in the survey. There was a reward system to encourage high quality answers. A participant was given one sentence at a time and was asked to label it with one of the three possible options as shown in Figure 3. If the participant was not sure about their answer, they could click the\More Context"button to see ve preceding sentences of the given sentence. They could also click the \Skip" button to skip the sentence.

In 15 days, we accumulated 140 participants. To detect spammers and low-quality participants, we used 123 screen-ing sentences (48 NFS s, 32 UFS s and 43 CFS s). These sentences were picked from all debate episodes. Three do-main experts agreed upon their labels. On average, one out of every ten sentences given to a participant (without letting the participant know) was randomly chosen to be a screening questi on and selected from the pool of 123 sentences. The participants were scored in the range of [0.0, 1.0] based on their performance on the screening sentences. Those scored more than 0 : 85 were considered top-quality participants.
We aimed to get the latest debates labeled rst. Sen-tences from one debate episode were randomly presented to the participants. Once all sentences in an episode were labeled by at least two participants, we moved on to the next episode. The data collection is still in progress. So far, 2012, 2008 and 2004 presidential debates (12 debate episodes) have been labeled. For training and evaluating our classi cation models, we only used a sentence if its label was agreed upon by two top-quality participants. Thereby we got 1571 sentences (882 NFS s, 252 UFS s, 437 CFS s). Figure 4a shows the distribution of these sentences' class labels. One interesting observation is that recent presidential candidates were making more check-worthy factual claims than earlier candidates.
We extracted multiple categories of features from the sen-tences. Table 1 summarizes these features. We use the following sentence to explain the features.

When President Bush came into office, we had a budget surplus and the national debt was a little over ve trillion. Sentiment : We used AlchemyAPI 3 to calculate a sentiment score for each sentence. The score ranges from -1 (most negative sentiment) to 1 (most positive sentiment). The above sentence has a sentiment score -0.846376.
 Length : This is the word count of a sentence. Natural language toolkit NLTK 4 was used for tokenizing a sentence into words. The example sentence has length 21.
 Word ( W ) : We used words in sentences to build tf-idf features. After discarding rare words that appear in less than three sentences, we got 6130 words. We did not apply stemming or stopword removal.
 Parts of Speech (POS) Tag ( P ) : We applied NLTK POS tagger on all sentences. There are 43 POS tags in the corpus. We constructed a feature for each tag. For a sentence, the count of words belonging to a POS tag is the value of the corresponding feature. In the example sentence, there are 3 words ( came , had , was ) with POS tag VBD (Verb, Past Tense) and 2 words ( ve , trillion ) with POS tag CD (Cardinal Number) .
 Entity Type ( ET ) : We used AlchemyAPI to extract en-tities from the sentences. There are 2727 entities in the labeled sentences. These entities belong to 26 types. The above sentence has an entity \Bush" of type \Person" . We constructed a feature for each entity type. For a sentence, its number of entities of a particular type is the value of the corresponding feature.
 Feature Selection : There are 6201 features in total. To avoid over-tting and to attain a simpler model, we performed feature selection. We trained a random forest classi er for which we used GINI index to measure the im-portance of features in constructing each decision tree. The overall importance of a feature is its average importance over all the trees. Figure 5 shows the importance of the 30 best features in the forest. The black solid lines indicate the standard deviations of importance values. Category types are pre xes to feature names. It is unsurprising that P CD is the to p discriminator|check-worthy factual claims are more likely to contain numeric values ( 45% of CFS sentences in our dataset contain numeric values) and non-factual sentences are less likely to contain numeric values ( 6% of NFS sentences in our dataset contain numeric values). Figure 6 shows the value distributions across all three classes for the four most important features. It depicts the features' discriminative capacities.
We performed 4-fold cross-validation using several super-vised learning methods, including Multinomial Naive Bayes Classi er ( NBC ), Support Vector Classi er ( SVM ) and Ran-dom Forest Classi er ( RFC ). Table 2 shows these classi ers' performance in terms of precision (p), recall (r), f-measure (f) and Cohen's kappa coefficient ( ). We experimented with four combinations of features|Word ( W ), Word + POS Tag ( W P ), W ord + POS Tag + Entity Type ( W P ET ), and the 100 most important features ( best 100 ). Sentim ent and Length were included in all the combinations. The SVM classi er paired with W P achiev ed 70% , 72% and 70% weighted average precision, recall and f-measure, respec-tively. RFC and SVM outperformed NBC in most cases. To understand the level of agreement between classi ers and human participants, we used coefficient. According to the guideline set in [7], RFC and SVM agreed moderately with the participants and NBC agreed fairly.

All the classi cation models had better accuracy on NFS s and CFS s than UFS s. This is not surprising, since UFS is between the other two classes and thus the most ambiguous. The top-quality participants faced screening sentences 1395 times and made incorrect judgment 208 times ( 14.9% ). Fig-ure 4b shows the percentages of different error types among these 208 cases. For instance, UFS CFS represen ts the cases in which UFS s were incorrectly labeled as CFS s by participants. It is evident from this gure that even the top-quality participants made more mistakes when class UFS is in question. Figu re 6: Value Distributions of the Four Most Important Features
We presented a supervised learning based approach to automatically detect check-worthy factual claims from pres-idential debate transcripts. We conducted a closely moni-tored survey to collect labels on sentences from the debates. We performed feature extraction and important feature se-lection. Preliminary experiment results show that the mod-els achieved 85% precision and 65% recall in classifying check-worthy factual claims. We plan to carry on future research along the following directions: 2000) debate transcripts. We will analyze how classi ca-tion performance changes by training data from different years' debates. For the upcoming 2016 U.S. presidential election, we will offer a website that ranks check-worthy factual claims, which can assist journalists and citizens in prioritizing their fact checking endeavor.
 ing interviews, congressional records, and social media. and classi cation methods, to obtain better classi cation accuracy. We also plan to develop methods for tackling claims spanning over multiple sentences.
 Acknowledgments The authors have been partially sup-ported by NSF grants IIS-1018865, CCF-1117369 and IIS-1408928. Any opinions, ndings, and conclusions in this publication are those of the authors and do not necessarily re ect the views of the funding agencies.
