 Ranking is a key problem in many information retrieval (IR) applications, such as document retrieval and collaborative filtering. In this paper, we address the issue of learning to rank in document retrieval. Learning-based methods, such as RankNet, RankSVM, and RankBoost, try to cre-ate ranking functions automatically by using some train-ing data. Recently, several learning to rank methods have been proposed to directly optimize the performance of IR applications in terms of various evaluation measures. They undoubtedly provide statistically significant improvements over conventional methods; however, from the viewpoint of decision-making, most of them do not minimize the Bayes risk of the IR system. In an attempt to fill this research gap, we propose a novel framework that directly optimizes the Bayes risk related to the ranking a ccuracy in terms of the IR evaluation measures. The results of experiments on the LETOR collections demonstrate that the framework outper-forms several existing methods in most cases.
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval -Retrieval models Algorithms, Experimentation, Theory Learning to Rank, Ranking function
The rapid growth and popularity of the Web in the last decade has resulted in a huge number of information sources  X  The author is also with the Institute of Information Science, Academia Sinica, Taiwan.
 on the Internet, but it has made information retrieval (IR) more difficult for end users. Search engines have therefore become increasingly important in helping users accurately locate relevant content based on their information needs. IR can be formulated as a binary classification problem in which documents are categorized as relevant or irrelevant. How-ever, in practice, the textual content should have multiple degrees of relevance to a query. Therefore, the IR problem can also be formulated as a ranking problem, which means that, given a query, the documents are sorted by the ranking function, and then the ranked list is returned to the user. Ranking functions influence both the quality of the search results and users X  search experience directly. Research on ranking models has become a fundamental research topic. Many models and methods have been proposed to solve this problem, e.g., the Boolean model, vector space model [23], probabilistic model [22], and language modeling-based meth-ods [19]. In empirical IR models, tuning parameters by us-ing certain training data is a common practice; however, as ranking models become more sophisticated, parameter tun-ing becomes an increasingly challenging issue.

In the last decade, several human-judged relevance assess-ments have been made available for IR research. This makes it possible to incorporate many of the significant advances in machine learning into the design of ranking models. For this reason, many learning methods have been developed and applied to document retrieval and related fields. Basi-cally, these methods transform the ranking problem into bi-nary classification on pairs constructed between documents. In fact, methods like RankNet [3], RankSVM [7, 10], and RankBoost [6] typically minimize a loss function that is loosely related to the ranking accuracy in terms of the eval-uation measures, such as Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG) [9]. Therefore, a substantial amount of research effort has fo-cused on constructing ranking functions by optimizing these evaluation measures directly. Related algorithms, such as SV M map [28] and AdaRank [26], have proved effective for IR applications.

In this paper, we propose a learning to rank framework based on a Bayesian perspective. Under the framework, the Plackett-Luce Model is introduced as the probability model of permutations. Like the approach in [5], we trans-form the ranking scores to permutation probabilities such that the ranking function can be optimized indirectly from Bayesian decision inference. The optimal ranking function appears when the expected Bayes risk reaches the mini-mum. Accordingly, we call th e proposed learning to rank framework Bayes Rank. The framework is fairly general and provides flexibility for many applications, such as informa-tion retrieval, automatic summarization, and collaborative filtering. Under the framework, one can optimize the ex-pected performance of a ranking function by adopting an ar-bitrary permutation loss related to desired metrics. Similar to other learning algorithms, Bayes Rank also minimizes an upper bounding function of the ranking error, which means that the ranking error can be iteratively reduced during the training process. For document retrieval, we design a learn-ing algorithm for Bayes Rank with NDCG related permuta-tion loss based on multi-layer perceptron neural networks. The results of experiments on the LETOR collections [13, 20], containing both TREC and OHSUMED benchmarks, demonstrate that, in most cases, Bayes Rank achieves con-sistent improvements over the compared ranking algorithms, namely AdaRank [26], ListNet [5], and SV M map [28]. The remainder of this paper is organized as follows: In Section 2, we review previous works, and then formulate the ranking problem in terms of Bayes decision theory in Section3. InSection4,wedescribetheproposedlearn-ing framework and algorithm. In Section 5, we compare Bayes Rank with ListNet [5] and PermuRank [27] from a theoretical perspective. Section 6 contains the experiment results and a discussion of their implications. Then, in Sec-tion 7, we summarize our conclusions and indicate several directions for future research.
Information retrieval can b e viewed as a ranking problem or a decision-making problem. In this section, we review previous works on information retrieval from these two as-pects.
IR problems, such as document retrieval, can be formu-lated as ranking problems, which can be solved by various popular models, such as the Boolean model, vector space model, probabilistic model, and language model. In recent years, many attempts have been made to utilize machine learning methods to solve IR problems. Learning to rank approach which tries to construct a ranking model using some training data, has been addressed in pointwise, pair-wise, and listwise ways. The pointwise approach [17, 12] transforms the ranking problem into a regression or classifi-cation problem of a single document. The pairwise approach [24, 4, 30] defines a pairwise loss function and is concerned with classification of document pairs; typical methods in-clude RanSVM [7, 10], RankBoost [6], and RankNet [3].
The listwise approach [2] has become increasingly popular in recent years. It attempts to solve the ranking problem by minimizing a listwise surrogate loss function. ListNet [5], an extension of RankNet, defines the loss function as the cross entropy between two parameterized probability distri-butions of permutations. RankCosine [21] and ListMLE [25] inherit a similar structure from ListNet, except for the sur-rogate loss functions.

However, minimizing the surrogate loss does not guaran-tee that the IR performance in terms of evaluation measures can also be optimized. Let us take the pairwise case as an example and consider the following scenario. For a given query, two ranking functions, f A and f B , are considered to rank 10 documents, two of which are judged as  X  X elevant X  and Table 1: An example of the inconsistency between PER (pairwise error rate) and AP (average preci-sion) f
A 1000000001 8/16=50.0% (1/1+2/10)/2=0.600 f
B 0001100000 6/16=37.5% (1/4+2/5 )/2=0.325 the others are judged as  X  X rrelevant X  by human. The ranked lists produced by these two ranking functions are shown in Table 1, where the relevant and irrelevant documents are denoted as  X 0 X  and  X 1 X  respectively. We observe that f A curs 50% PER (pair error rate), but it yields a better AP (average precision) than f B , which only introduces 37.5% PER. The kind of mismatch occurs when the surrogate loss function is inconsistent with the evaluation metrics. SV M map [28], AdaRank [26], and PermuRank [27], tries to optimize the evaluation measures directly. Undoubtedly, they provide significant improvements over conventional meth-ods; however, when we view IR as a decision-making prob-lem, most of them do not minimize the Bayes risk of the system.
Information retrieval can be treated as a statistical decision-making problem [11, 29]. Given a user X  X  query, the re-trieval system faces a decision-making problem in that it must choose relevant documents from a hypothesized space and return a ranked list to the user. From the aspect, Zhai and Lafferty [29] proposed a risk minimization framework for information retrieval. However, they did not address the supervised learning scenario from the viewpoint of decision-making. Instead, they focused on developing retrieval meth-ods for various retrieval cases, such as set-based retrieval, rank-based retrieval, and aspect retrieval. To the best of our knowledge, not much work has considered these two aspects jointly, especially for direct optimization of IR performance. In this paper, we propose a learning to rank framework that addresses both aspects.
In document retrieval, documents related to a query are managed by a ranking model and presented to the user ac-cording to their relevance to the query. In practice, the rank-ing problem may be reduced to finding an appropriate scor-ing function that can evaluate individual documents. The ranking function sorts the documents in descending order of the assigned scores, and then forms a ranked list 1 . The nota-tions used in this paper are summarized in Table 2. Suppose stage, given a query q , the scoring function g  X  X  evaluates every document in D , and then compiles a score list, say { y 1 ,y 2 , ..., y n } . The documents are sorted according to the scores and presented to the user. In the supervised learn-ing stage, a set of training queries Q = { q 1 , ..., q m relevance mapping r  X  X  are given. The relevance mapping r , which can be regarded as a kind of scoring function in the function space G , reflects the relevance judgments. The
In this paper, ranked (document) list and permutation are identical. learning to rank approach tries to create the scoring func-tion automatically from the training data, which include the query set Q and the relevance mapping function r .
Given a user X  X  query q , a retrieval system attempts to make a decision  X  (  X  ; q ) that selects a ranked document list  X  from a set of possible permutations  X  q to return to the user. Note that we assume  X  is a random variable in the hypoth-esized permutation space  X  q with an unknown probability incurred by taking decision  X  when the perfect ranked list is  X  q = sort r according to the relevance mapping r . Generally,  X   X  q should be a subset of  X  q rather than a unique perfect permutation. However, for ease of presentation, we let  X   X  q be a perfect permutation hereafter. 2 In the retrieval stage, no explicit information about  X   X  q is presented; i.e., any arbitrary per-mutation could be  X   X  q . Therefore, we model the uncertainty by the conditional probability p (  X  ; q ), which corresponds to the probability that  X  would be judged as the perfect per-mutation for query q . As a result, in the general framework of Bayesian decision theory, the expected risk of taking de-cision  X  (  X  ; q )isgivenby The best decision  X   X  can be selected by minimizing the ex-pected risk as follows: The minimum expected risk is called the Bayes risk .In the supervised learning scenario, the ground-truth associ-ated with each query q is presented. Hence, the Bayes risk of  X   X  q over the training query set Q is given by
R = R is the expected Bayes risk over Q . If we assume that the prior p ( q ) is uniformly distributed and the permutation space  X  q is finite, the expected Bayes risk can be approxi-mated by
The assumption does not affect the correctness of the derivation.
 Figure 1: Changes in the probabilities of permuta-tions with different permutation-level losses during the learning process.
 where m is the number of queries in Q . Unlike many existing methods that embed the scoring function in the surrogate loss function, our approach tries to model the conditional probability p (  X  ; q ) where the scoring function is embedded. This strategy suggests two advantages: 1. It is not necessary to form ulate the ranking error as a 2. The permutation-level loss can be directly related to In other words, the learning process tends to adjust the pa-rameters in the scoring function such that a lower prob-ability is assigned to the permutation with a higher loss, and the probability of that with a lower loss is increased. This leads, indirectly, to minimization of the objective func-tion, i.e., the expected Bayes risk . Figure 1 illustrates the changes in the probabilities of permutations with different permutation-level losses during the learning process.
The permutation-level loss l (  X   X  q , X  ) is incurred by select-ing  X  from  X  q for query q when the perfect ranked list is  X  ; therefore, it can be directly related to an arbitrary IR evaluation metric that measures the distance between  X  and  X  . In general, we have to restrict the range of the loss, e.g., between zero and one, to bound the expected Bayes risk in order to prevent the model from being biased by some hard queries. To maximize the IR performance, the loss can be derived directly from the evaluation measures, i.e., l (  X   X  q , X  )=1 NDCG@n.
To model the ranked list appropriately, many probabilis-tic models have been proposed for modeling rank data, e.g., the Bradley-Terry-Luce model [1, 14], the Mallows model [15], and the Plackett-Luce model [18, 14]. Marden provided an excellent analytical review of the research on models for rank data [16], one of which is the Plackett-Luce model. It models a ranking as a sequential process and has been used
Algorithm 1 Learning Algorithm 1: Input :training queries Q = { q i } , 2: relevance mapping r 3: Initialize parameters:  X  ,learningrate  X  4: repeat 5: R  X  0 ,  X   X   X  0 6: for i =1 ,  X  X  X  ,m do 7: for j =1 ,  X  X  X  ,n do //Precalculation 8: Input q i ,d j to neural networks, 10: end for 11: for  X  k 1 do 12: for j =1 ,  X  X  X  ,n do 15: end for 16: R  X  R  X  p k (  X  k 1 ; q i ,  X  ) G k (  X  k 1 , X   X  q i ) 17: end for 18: end for 19: Update  X   X   X   X   X   X   X  20: Update  X  21: until R converges successfully in ListNet [5]. Cao et al. have introduced an increasing and strictly positive function to transform rank-ing scores into probabilities [5]. In line with their work, we adopt the following form of the Plackett-Luce model in our framework: where i and j are the rank indices and  X  ( i ) denotes the document with rank i in  X  . The permutation probability is estimated through the scoring function g . In [5], the authors clarified an important property for this form of Plackett-Luce Model. Given a scoring function, the ranked list of the documents sorted based on the scores has the highest permutation probability, while the list of documents sorted in the inverse order has the lowest permutation probability. The property implies that choosing the ranked list with the highest probability is equivalent to the way a typical ranking function selects a list.
In this paper, we take the multi-layer perceptron neural networks as an example of the scoring function in the rank-ing model and design the permutation-level loss as the op-posite of NDCG@k score, i.e., where Then, the objective function to be minimized becomes where  X  is the set of parameters in the neural networks. For each permutation  X  , we divide the document list into two sublists:  X  k 1 and  X  n k +1 , as follows: Because of the nature of the position-dependent loss, per-mutations with the same  X  k 1 incur the same loss  X  G k , i.e., G probabilities of those permutations directly, and evaluate Equation (8) by only considering the top k documents, i.e.,  X  . Therefore, Equation (8) can be re-written as where and in [5]. Taking the derivative of Equation (10) with respect to the parameter  X  yields  X  X   X  X  where  X  X  k (  X  k 1 ; q,  X  )  X  g ( d ; q,  X  ) = p k (  X  where rank ( d ) denotes the rank of document d in the ranked list  X  k 1 . The gradient of g ( d ; q,  X  )withrespectto  X  , can be found in [3]. Thus,  X  is updated using the gradient descent with a positive learning rate  X  : In the estimation of the gradient in Equation (14), the expo-nentiation operation incurs a high computational overhead; therefore, we evaluate exp ( g ( d ; q,  X  )) beforehand and then update  X  using the batch gradient descent algorithm. The learning algorithm is detailed in Algorithm 1 . In this section, we provide a proof of the correctness of Bayes Rank and discuss its relation to ListNet [5] and Per-muRank [27].
Theorem 1. Let  X   X  q be the ranked document list that pos-q  X  Q . Then, the bound holds on the ranking error where  X  =max q #  X  q ,and #  X  q is the size of the permutation space for query q .
 A proof of Theorem 1 is given in the Appendix. Since  X  is a fixed constant during the training process, the theorem implies that minimizing the expected Bayes risk R will lead to a continuous reduction of the upper bound of the ranking error. Xu et al. [27] classify the methods that directly opti-mize IR evaluation measures into three categories in terms of loss function optimization. Our method belongs to the first category, which minimizes the upper bound of the basic loss function defined according to the IR evaluation measures.
Bayes Rank bears some resemblance to ListNet [5], which models the ranking error as a surrogate function based on the cross entropy. It is assumed that there is uncertainty in the prediction of ranked lists using the ranking func-tion. In contrast, Bayes Rank focuses on modeling the con-ditional permutation probability so as to minimize the ex-pected Bayes risk from the decision-making aspect. We now show that, in some cases, the loss function of ListNet is the upper bound of the expected risk.
 For ListNet, the loss function for query q is defined as where p (  X  ; q ) represents the ranking probability, which can be defined as the top k probability provided by As the result, we have the following theorem:
Theorem 2. Equation (16) is an upper bound of expected p (  X  ; q, r ) .
 It is straightforward to verify that Theorem 2 holds by ap-plying Jensen X  X  inequality. ListNet can be viewed as maxi-mizing the performance in terms of p k (  X   X  q ; q, r ). From this perspective, Bayes Rank provides a tighter bound for opti-mizing such a measure.
To minimize the ranking error l (  X   X  q ,  X   X  q )of  X   X  q the permutation selected for q by the ranking model, Xu et al. introduced two types of bounds for direct optimization methods [27]. The type one bound, optimized by AdaRank [26], is defined directly on the IR measures; while the type two bound is defined with the pairs comprised of a perfect permutation and an imperfect permutation. PermuRank is Figure 2: Ranking accuracy of various methods on OHSUMED (LETOR 2.0). a generalized algorithm that minimizes the type two bound, which is derived from a loss function [27] as follows: where F (  X  ; q ) evaluates permutation  X  and [[  X  ]] is one when the condition is satisfied; otherwise, it is zero.
In contrast, the expected Bayes risk can also be extended to a generalized form of the upper bound of the ranking er-ror.

Theorem 3. For all  X   X   X  q , the following bound holds on the ranking error l (  X   X  q ,  X   X  q ) :
A proof of Theorem 3 is given in the Appendix. It implies that Bayes Rank does not try to minimize the two types of bounds defined by Xu et al. Instead, it adopts a new type of upper bounding function, as shown in Equation (18). As a result, in future research, it will be possible to develop new ranking models of soundness based on the new type of bound.
LETOR (LEarning TO Rank) [13] is a benchmark collec-tion constructed for learning to rank research. The second version (LETOR 2.0) has been widely used to evaluate vari-ous ranking algorithms; however, the provider acknowledged that there are some issues with LETOR 2.0 [20]. For exam-ple, some low-level information is missing, and the sampling of documents associated with each query is somehow biased. To make LETOR more reliable, the provider improved it in three ways and released LETOR 3.0 in December, 2008. We conducted our experiments on LETOR 2.0 and LETOR 3.0, both of which contain two datasets: OHSUMED and .Gov.
OHSUMED [8] is a subset of MEDLINE, a database of medical publications. There are totally 106 queries, each of which has about 152 documents on average for feature ex-traction. In contrast to LETOR 2.0, each query-document Figure 3: Ranking accuracy of various methods on OHSUMED (LETOR 3.0). pair in LETOR 3.0 has 45 features. The .Gov dataset was crawled in early 2002 and has been used as the data col-lection for TREC Web Track, which involves three research tasks: topic distillation (td ), homepage finding (hp), and named page finding (np). The dataset contains 125 queries in total. In LETOR 3.0, for each query-document pair, 64 features are extracted for learning and testing.

The whole collection was created as a set of document-query pairs, each represented as a feature vector and a corre-sponding relevance judgment. In the TREC collections, each example is labeled as relevant or irrelevant. For OHSUMED examples, there are three possible labels: relevant, possibly relevant, and irrelevant. All datasets are partitioned for 5-fold cross-validation. In each trial, three of the subsets are used for training, one for validation, and the other for testing the performance of the train ed model. The score reported is the average of the five folds.
We compared Bayes Rank with four popular listwise rank-ing algorithms, namely, AdaRank.MAP, AdaRank.NDCG [26], SV M map [28], and ListNet [5]. The evaluation tools used in the experiments and the results of the baseline rank-ing algorithms are all available on the LETOR website 3 .The neural networks used as the scoring function for Bayes Rank have only one hidden layer, and the number of neurons in the hidden layer is tuned on the validation sets. The exper-iment results of Bayes Rank using NDCG@1 and NDCG@2 as training measures are denoted as Bayes Rank1 and Bayes Rank2 respectively.
We use the abbreviations  X  X 2 X  and  X  X 3 X  to denote LETOR 2.0 and LETOR 3.0 respectively. Figures 2 and 3 show the results for the OHSUMED dataset. From Figure 2, we observe that all methods perform simi-larly. If we focus on the NDCG@1 measure, Bayes Rank1 outperforms the other methods on this dataset. However, surprisingly it performs worse than the other methods on L3. On the other hand, Bayes Rank2 achieves notable improve-ments consistently over the baseline methods. Note that http://research.microsoft.com/en-us/um/beijing/projects/letor/index.html Figure 4: Ranking accuracy of various methods on TD2003 (LETOR 2.0). Figure 5: Ranking accuracy of various methods on TD2003 (LETOR 3.0). there is no significant difference between the MAP measures of these methods. Figures 4 and 5 show the results for the TREC2003 dataset. We observe that ListNet is the best method on L2, almost outperforming all the other algorithms except AdaRank.NDCG at the very top position. However, on L3, Bayes Rank yields a promising performance across every position compared to the other methods. Figures 6 and 7 show the results for the TREC2004 dataset. Clearly, Bayes Rank2 achieves the best performance in this experiment. In terms of MAP, Bayes Rank obtains relative improvements of 4% and 14% over ListNet on L2 and L3 respectively. We also performed a significance test ( t-test ) on the improvements of Bayes Rank2 over the baseline algo-rithms on L2. As shown in Table 3, Bayes Rank2 achieves significant improvements. On L3, Bayes Rank1 performs as well as Bayes Rank2. The learning curve of Bayes Rank in terms of the expected NDCG and pairwise loss is shown in Figure 8. We observe that the pairwise loss is reversely correlated with the ex-pected NDCG, which means that we can also reduce the Table 3: The p -value of the t-test on the improvements of Bayes Rank over the baseline methods on TD2004 Figure 6: Ranking accuracy of various methods on TD2004 (LETOR 2.0). pairwise loss effectively as the number of training itera-tions increases. The experime nt results demonstrate that, in most cases, the proposed Bayes Rank framework is more effective than the compared listwise ranking algorithms, es-pecially on the newly released LETOR 3.0 collection. The results also indicate that Bayes Rank1 is not as effective as Bayes Rank2. The observation implies that as the truncation level of NDCG increases, more information about this met-ric becomes available for learning. However, for preventing the over-fitting problem, the regularization might become an important issue as the truncation level increases.
We have proposed a learning framework, called Bayes Rank, for learning to rank from the Bayesian decision inference. The framework tries to minimize the expected Bayes risk over the training set and can be regarded as a direct op-timization method for evaluation measures when the loss function is related to IR metrics. Experiment results show that Bayes Rank yields consistent improvements over base-line methods in most cases. Our contribution in this work is threefold: First, we propose a novel learning to rank frame-work from the Bayesian decision inference. The framework is fairly general and an arbitrary ranking model and loss function can be adopted; thus, it can be applied to other ranking problems. Second, we take the multi-layer percep-tron neural networks as the ranking function and develop a listwise learning algorithm based on minimization of the ex-pected Bayes risk . The effectiveness of the algorithm with the NDCG-based permutation-level loss is verified on the LETOR collections. Finally, we compare Bayes Rank with ListNet and PermuRank, and provide a new type of upper bound of the ranking error. As a result, in future research, it will be possible to develop new ranking models of soundness based on the proposed bounding function.

When considering non-positional dependent permutation-level losses, we may face a problem with the enormous of the Figure 7: Ranking accuracy of various methods on TD2004 (LETOR 3.0). hypothesized space of permutations. This computational is-sue exists in most listwise algorithms. In [27], the authors proposed keeping a small pool of permutations for training. Based on their technique, we can consider optimizing the MAP measure directly and evaluating the performance on the ad-hoc retrieval task with longer queries.
This work was supported in part by Taiwan e-Learning and Digital Archives Program (TELDAP) sponsored by the National Science Council of Taiwan under Grant: NSC98-2631-001-013. [1] R. Bradley and M. Terry. Rank analysis of incomplete [2] C. Burges, R. Ragno, and Q. Le. Learning to Rank [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, [4] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang, and H.-W. [5] Z.Cao,T.Qin,T.-Y.Liu,M.-F.Tsai,andH.Li.
 [6] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An [7] R. Herbrich, T. Graepel, and K. Obermayer. Large [8] W.Hersh,C.Buckley,T.J.Leone,andD.Hickam.
 [10] T. Joachims. Optimizing search engines using [11] J. Lafferty and C. Zhai. Document language models, [12] P. Li, C. Burges, and Q. Wu. Mcrank: Learning to [13] T.Y.Liu,J.Xu,T.Qin,W.Xiong,andH.Li.Letor: [14] R. Luce. Individual Choice Behavior: A Theoretical [15] C. MALLOWS. NON-NULL RANKING MODELS. I.
 [16] J. Marden. Analyzing and Modeling Rank Data . [17] R. Nallapati. Discriminative models for information [18] R. Plackett. The analysis of permutations. Applied [19] J. M. Ponte and B. B. Croft. A language modeling [20] T. Qin, T. Liu, J. Xu, and H. Li. How to make letor [21] T. Qin, X. Zhang, M. Tsai, D. Wang, T. Liu, and [22] S. Robertson and K. Sparck-Jones. Relevance [23] G. Salton, editor. Automatic text processing . [24] M.-F. Tsai, T.-Y. Liu, T. Qin, H.-H. Chen, and W.-Y. [25] F. Xia, T.-Y. Liu, J. Wang, W. Zhang, and H. Li. [26] J. Xu and H. Li. Adarank: a boosting algorithm for [27] J.Xu,T.Y.Liu,M.Lu,H.Li,andW.Y.Ma.
 [28] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A [29] C. Zhai and J. Lafferty. A risk minimization [30] Z. Zheng, H. Zha, T. Zhang, O. Chapelle, K. Chen, ProofofTheorem1.

Proof.  X   X  q is the ranked document list that possesses the maximal probability p (  X   X  q ; q, g ), which implies that Therefore, l (  X   X  q , X   X  q ) is upper bounded by #  X  q  X  we obtain where Proof of Theorem 3.

Proof. Since an exponential function is monotonically
