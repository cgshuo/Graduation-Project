 ORIGINAL PAPER Daniel Lopresti Abstract Errors are unavoidable in advanced computer vision applications such as optical character recognition, and the noise induced by these errors presents a serious challenge to downstream processes that attempt to make use of such data. In this paper, we apply a new paradigm we have pro-posed for measuring the impact of recognition errors on the stages of a standard text analysis pipeline: sentence bound-ary detection, tokenization, and part-of-speech tagging. Our methodology formulates error classification as an optimi-zation problem solvable using a hierarchical dynamic pro-gramming approach. Errors and their cascading effects are isolated and analyzed as they travel through the pipeline. We present experimental results based on a large collection of scanned pages to study the varying impact depending on the nature of the error and the character(s) involved. This data-set has also been made available online to encourage future investigations.
 Keywords Performance evaluation  X  Optical character recognition  X  Sentence boundary detection  X  Tokenization  X  Part-of-speech tagging 1 Introduction Despite decades of research and the existence of established commercial products, the output from optical character rec-ognition (OCR) processes often contain errors. The more highly degraded the input, the greater the error rate. Since such systems can form the first stage in a pipeline where later stages are designed to support sophisticated informa-tion extraction and exploitation applications, it is important to understand the effects of recognition errors on downstream text analysis routines. Are all recognition errors equal in impact, or are some worse than others? Can the performance of each stage be optimized in isolation, or must the end-to-end system be considered? What are the most serious forms of degradation a page can suffer in the context of natural language processing? In balancing the tradeoff between the risk of over-and under-segmenting characters during OCR, where should the line be drawn to maximize overall perfor-mance? The answers to these questions should influence the way we design and build document analysis systems.
Researchers have already begun studying problems relat-ing to processing text data from noisy sources. To date, this work has focused predominately on errors that arise during speech recognition. For example, Palmer and Ostendorf [ 18 ] describe an approach for improving named entity extraction by explicitly modeling speech recognition errors through the use of statistics annotated with confidence scores. The inau-gural Workshop on Analytics for Noisy Unstructured Text Data [ 25 ] and its followup workshops [ 21 , 24 ] have featured papers examining the problem of noise from a variety of per-spectives, with most emphasizing issues that are inherent in written and spoken language.

There has been less work, however, in the case of noise induced by OCR. Early papers by Taghva et al. [ 22 ]show that moderate error rates have little impact on the effective-ness of traditional information retrieval measures, but this conclusion is tied to certain assumptions about the IR model ( X  X ag of words X ), the OCR error rate (not too high), and the length of the documents (not too short). Miller et al. [ 17 ] study the performance of named entity extraction under a variety of scenarios involving both ASR and OCR output, although speech is their primary interest. They found that by training their system on both clean and noisy input material, performance degraded linearly as a function of word error rates.

Farooq and Al-Onaizan [ 5 ] proposed an approach for improving the output of machine translation when presented with OCR X  X d input by modeling the error correction process itself as a translation problem.

A paper by Jing et al. [ 10 ] studied the problem of summa-rizing textual documents that had undergone OCR and hence suffered from typical OCR errors. From the standpoint of per-formance evaluation, this work employed a variety of indirect measures: for example, comparing the total number of sen-tences returned by sentence boundary detection for clean and noisy versions of the same input text, or counting the num-ber of incomplete parse trees generated by a part-of-speech tagger.

In two later papers [ 12 , 13 ], we turned to the question of performance evaluation for text analysis pipelines, proposing a paradigm based the hierarchical application of approximate string matching techniques. This flexible yet mathematically rigorous approach both quantifies the performance of a given processing stage as well as identifies explicitly the errors it has made. Also presented were the results of pilot studies where small sets of documents (tens of pages) were OCR X  X d and then piped through standard routines for sentence bound-ary detection, tokenization, and part-of-speech tagging, dem-onstrating the utility of the approach.

In the present paper, we employ this same evaluation par-adigm, but using a much larger and more realistic dataset totaling over 3,000 scanned pages which we are also making available to the community to foster work in this area [ 14 ]. We study the impact of several real-world degradations on OCR and the NLP processes that follow it, and plot later-stage performance as a function of the input OCR accuracy. We conclude by outlining possible topics for future research. 2 Stages in text analysis In this section, we describe the prototypical stages that are common to many text analysis systems, discuss some of the problems that can arise, and then list the specific packages we use in our work. The stages, in order, are: (1) OCR, (2) sen-tence boundary detection, (3) tokenization, and (4) part-of-speech tagging. These basic procedures are of interest because they form the basis for more sophisticated natural language applications, including named entity identifica-tion and extraction, topic detection and clustering, and sum-marization. In addition, the problem of identifying tabular structures that should not be parsed as sentential text is also discussed as a pre-processing step.

A brief synopsis of each stage and its potential problem areas is listed in Table 1 . The interactions between errors that arise during OCR and later stages can be complex. Several common scenarios are depicted in Fig. 1 . It is easy to imagine a single error propagating through the pipeline and inducing a corresponding error at each of the later steps in the process (Case 1 in the figure). However, in the best case, an OCR error could have no impact whatsoever on any of the later stages (Case 2); for example, tag and tap are both verbs, so the sentence boundary, tokenization, and part-of-speech tagging would remain unchanged if g was misrecognized as p .
On the other hand, misrecognizing a comma ( , )asaperiod ( . ) creates a new sentence boundary, but might not affect the stages after this (Case 3). More intriguing are latent errors which have no effect on one stage, but reappear later in the processing pipline (Cases 4 and 5). OCR errors which change the tokenization or part-of-speech tagging while leaving sen-tence boundaries unchanged fall in this category (e.g., faulty word segmentations that insert or delete whitespace charac-ters). Finally, a single OCR error can induce multiple errors in a later stage, its impact mushrooming to neighboring tokens (Case 6).

In selecting implementations of the above stages to test, we choose to employ freely available open source software rather than proprietary, commercial packages. From the standpoint of our work, we require behavior that is representative, not necessarily  X  X est-in-class. X  For sufficiently noisy inputs, the same methodology and conclusions are likely to apply no matter what algorithm is used. Comparing different tech-niques for realizing a given stage to determine which is most robust in the presence of OCR errors would make an interesting topic for future research. 2.1 Optical character recognition The first stage of the pipeline is OCR, the conversion of the scanned input image from bitmap format to encoded text. Optical character recognition performs quite well on clean inputs in a known font. It rapidly deteriorates in the case of degraded documents, complex layouts, and/or unusual fonts. In certain situations, OCR will introduce many errors involv-ing punctuation characters, which has an impact on later-stage processing.

For our OCR stage, we selected the Tesseract open source software package [ 23 ]. The latest version at the time of our tests was 2.03. Since we are presenting it with relatively sim-ple text layouts, having to contend with complex documents is not a concern in our experiments. The performance of Tesseract on the inputs we tested is likely to be similar to the performance of a better-quality OCR package on nois-ier inputs of the same type. Figure 2 shows a portion of a dark photocopy page used in our studies, while Fig. 3 shows the OCR output from Tesseract . Note that the darkening and smearing of character shapes, barely visible to the human eye, lead to various forms of substitution errors (e.g., l h  X  l1 , rn  X  m ), as well as space deletion (e.g., of the  X  ofthe ) and insertion (e.g., project  X  pro ject ) errors. 2.2 Sentence boundary detection Procedures for sentence boundary detection use a variety of syntactic and semantic cues in order to break the input text into sentence-sized units, one per line ( i.e. , each unit is terminated by a standard end-of-line delimiter such as the Unix newline character). The sentence boundary detector we used in our test is the MXTERMINATOR package by Reynar and Ratnaparkhi [ 20 ]. An example of its output for a  X  X lean X  (error-free) text fragment consisting of two sentences is shown in Fig. 4 b. 1 2.3 Tokenization Tokenization takes the input text which has been divided into one sentence per line and breaks it into individual tokens which are delimited by white space. These largely corre-spond to word-like units or isolated punctuation symbols. In our studies, we used the Penn Treebank tokenizer [ 16 ]. As noted in the documentation, its operation can be sum-marized as: (1) most punctuation is split from adjoining words, (2) double quotes are changed to doubled single forward-and backward-quotes, and (3) verb contractions and the Anglo-Saxon genitive of nouns are split into their com-ponent morphemes, and each morpheme is tagged separately. Sample output for the tokenization routine is shown in Fig. 4 c. 2.4 Part-of-speech tagging Part-of-speech tagging takes the tokenized text as input and tags each token as per its part of speech. We used Ratnaparkhi X  X  [ 19 ] part-of-speech tagger MXPOST, which produced a total of 42 different part-of-speech tags for our data.

The example in Fig. 4 d illustrates another key point which will be discussed later; the evaluations we conduct in this work are relativistic . That is, there is no universal ground-truth, but rather we compare the performance of the various text analysis stages on clean and noisy versions of the same input documents. An  X  X rror X  is considered to have occurred when the two sets of results differ. There may already in fact be errors present, even for clean inputs. For example, the first two words in the noun phrase  X  X itness assessment programs X  should be labeled as adjectives (JJ), not as nouns (NN). 2.5 Table spotting in text As a practical matter, the NLP routines we have described are intended for application to sentential text. However, some collections, including the Reuters-21578 news corpus [ 11 ], contain samples that are primarily tabular. Attempting to parse such documents could introduce misleading results in the sorts of studies we have in mind. An example is shown in Fig. 5 .

Our past work on medium-independent table detection [ 8 , 9 ] can be applied to identify pages containing tables so that they can be held out from the dataset. This paradigm consists of a high-level framework that formulates table detection as an optimization problem along with specific table quality measures that can be tuned for a given application and/or the input medium. We assume that the input is a single-column document segmentable into individual, non-overlapping text lines. This assumption is not too restrictive since multi-column input documents can be first segmented into individ-ual columns before running our table detection algorithm.
When run on Split-000 of the Reuters dataset with an aggressive threshold, we determined our approach to table spotting exhibited a recall of 89% (tabular documents that were correctly excluded from the dataset) and an estimated precision of 100% (documents included in the dataset that were indeed non-tabular). We note there are other reasons why a given text might not be parsable, for example, in the Reuters corpus there are boilerplate reports of changes in commodity prices that, while not tabular, are not sentential either. Still, the net result of this pre-processing step is to yield a subset more appropriate to our purposes. The data-set we have made available to the community reflects these refinements [ 14 ]. 3 An evaluation paradigm Performance evaluation for text analysis of noisy inputs pres-ents some serious hurdles. The approach we described in our earlier papers makes use of approximate string matching to align two linear streams of text, one representing OCR output and the other representing the ground-truth [ 12 , 13 ]. Due to the markup conventions employed by sentence bound-ary detection, tokenization, and part-of-speech tagging, this task is significantly more complex than computing the basic alignments used for assessing raw OCR accuracy [ 3 , 4 ]. The nature of the problem is depicted in Fig. 6 , where there are four sentences detected in the original text and nine in the associated OCR output from a dark photocopy. Numerous spurious tokens are present as a result of noise on the input page. The challenge, then, is to determine the proper cor-respondence between purported sentences, tokens, and part-of-speech tags so that errors may be identified and attributed to their root causes.

Despite these additional complexities, we can build on the same paradigm used in OCR error analysis, employing an optimization framework that likewise can be solved using dynamic programming. We begin by letting S = s 1 s 2  X  X  X  s be the source document (the ground-truth), T = t 1 t 2  X  X  X  the target document (the OCR output), and defining dist 1 to be the distance between the first i symbols of S and the first j symbols of T . The initial conditions are: dist 1 0 , 0 = 0 dist 1 i , 0 = dist 1 i  X  1 , 0 + c 1 del ( s i ) (1) dist 1 0 , j = dist 1 0 , j  X  1 + c 1 ins ( t j ) and the main dynamic programming recurrence is: dist 1 i , j = min for 1  X  i  X  m , 1  X  j  X  n . Here deletions, insertions, and mismatches are charged positive costs, and exact matches are charged negative costs. The computation builds a matrix of distance values working from the upper left corner ( dist 1 to the lower right ( dist 1 m , n ).

By maintaining the decision(s) used to obtain the mini-mum in each step, it becomes possible to backtrack the com-putation and obtain, in essence, an explanation of the errors that arose in processing the input. This information is used in analyzing the performance of the procedure under study.
To generalize these ideas to later stages of text processing, consider the output of those stages and the errors that might arise. Tokenization, for example, might fail to recognize a token boundary thereby combining two tokens into one (a  X  X erge X ), or break a token into two more pieces (a  X  X plit X ). Similar errors may arise in sentence boundary detection.
In the paradigm we have developed, we adopt a three-level hierarchy. At the highest level, sentences (or purported sen-tences) are matched allowing for missed or spurious sentence boundaries. The basic entity in this case is a sentence string, and the costs of deleting, inserting, substituting, splitting, or merging sentence strings are defined recursively in terms of the next level of the hierarchy, which are tokens. As with the sentence level, tokens can be split or merged. Comparison of tokens is defined in terms of the lowest level of the hierar-chy, which is the basic approximate string matching model we began this section with (Eqs. 1 and 2 ).

In terms of dynamic programming, at the token level, the algorithm becomes: dist2 i , j = min where the inputs are assumed to be sentences and c del , c and c sub are now the costs of deleting, inserting, and substi-tuting whole tokens, respectively, which can be naturally defined in terms of the first-level computation.

Lastly, at the highest level, the input is a whole page and the basic editing entities are sentences. For the recurrence, we have: dist3 i , j = min with costs defined in terms of the second-level computation.
By executing this hierarchical dynamic programming from the top down, given an input page for the OCR results as processed through the text analysis pipeline and another page for the corresponding ground-truth, we can determine an optimal alignment between purported sentences, which is defined in terms of an optimal alignment between individ-ual tokens in the sentences, which is defined in terms of an optimal alignment between each possible pairing of tokens (including the possibilities that tokens are deleted, inserted, split, or merged). Once an alignment is constructed using the orthography of the input text strings, we may compare the part-of-speech tags assigned to corresponding tokens to study the impact of OCR errors on that process as well. This paradigm is depicted in Fig. 7 .

From a pragmatic standpoint, the optimization process can require a substantial amount of CPU time depending on the length of the input documents (we have observed runtimes of several minutes for pages containing  X  1 characters). There are, however, well-known techniques for speeding up dynamic programming (e.g., so-called  X  X eam search X ), which have little or no effect on the optimality of the results for the cases of interest. 4 Experimental evaluation In this section, we describe the data we used in our exper-iments, including the steps we took in preparing it. As has already been noted, we employ a relativistic analysis in this work. The text analysis pipeline, shown in Fig. 8 , is run twice for each input document: once for a clean (electronic) version and once for the output from an OCR process. The results of the two runs are then compared using the techniques of the previous section. Both because the evaluation integrates all three stages in a single optimization step, and because there is no need to laboriously construct a manual ground-truth for each part of the computation, it is possible to run much larger test sets than would otherwise be feasible. This substantial benefit is offset by the risk that some errors might be misclas-sified, since the  X  X ruth X  is not 100% trustworthy. Since our focus is on studying how OCR errors impact later stages and not on measuring absolute performance, such an approach seems justified. 4.1 Data preparation As suggested previously, our baseline dataset is derived from  X  X plit-000 X  of the Reuters-21578 news corpus [ 11 ]. After fil-tering out articles that consist primarily of tabular data, we formatted each of the remaining documents as a single page typeset in Times-Roman 12-point font. In doing so, we dis-carded articles that were either too long to fit on a page or too short to provide a good test case (fewer than 50 words).
Of the 925 articles in the original set, 661 remained after these various criteria were applied. These pages were then printed on a Ricoh Aficio digital photocopier and scanned back in using the same machine at a resolution of 300 dpi. One set of pages was scanned as-is, another two sets were first photocopied through one and two generations with the contrast set to the darkest possible setting, and two more sets were similarly photocopied through one and two generations at the lightest possible setting before scanning. This resulted in a test set totaling 3,305 pages. We then ran the resulting bitmap images through the Tesseract OCR package. Exam-ples of a region of a scanned page image and the associated OCR output were shown in Figs. 2 and 3 .

Basic OCR accuracy can be judged using a single level of dynamic programming, i.e., Eqs. 1 and 2 , as described elsewhere [ 4 ]. These results for the four datasets are pre-sented in Table 2 . As in the information retrieval domain, precision and recall are used here to reflect two different aspects of system performance. The former is the fraction of reported entities that are true, while the latter is the fraction of true entities that are reported. Note that the baseline OCR accuracy is quite high, but performance deteriorates for the degraded documents. It is also instructive to consider sep-arately the impact on punctuation symbols and whitespace, these results are also shown in the table. Punctuation symbols in particular are badly impacted, with a large number of false alarms (low precision), especially in the case of the Dark2 dataset where fewer than 80% of the reports are true. This phenomenon has serious implications for sentence boundary detection and later stages of text processing.

We then ran sentence boundary detection, tokenization, and part-of-speech tagging on both the original (ground-truth) news stories and the versions that had been OCR X  X d, comparing the results using the paradigm described earlier. This allowed us to both quantify performance as well as to determine the optimal alignments between sequences and hence identify the actual errors that had arisen. 4.2 Results An example of a relatively straightforward alignment pro-duced by our evaluation procedure is shown in Fig. 9 .This displays the effect of a single-character substitution error ( i being misrecognized as ; ). The result is three tokens where before there was only one. Not unexpectedly, two of the three tokens have inappropriate part-of-speech labels. In this instance, the OCR error impacts two text analysis stages and is, for the most part, localized; other errors can have Gro u nd-Tr u th effects that cascade through the pipeline, becoming ampli-fied at each stage.

A tabulation of the dynamic programming results for the three text-processing stages appears in Table 3 . While most of the data is processed with relatively high accuracy, the com-puted rates are generally lower than the input OCR accura-cies. Note, for example, that the overall OCR accuracy across all symbol classes is 99.3%, whereas the sentence boundary detection, tokenization, and part-of-speech tagging accura-cies are 94.7, 97.9, and 96.6%, respectively. Recall also that these measures are relative to the exact same procedures run on the original (error-free) text. This illustrates the cascad-ing effect of OCR errors in the text analysis pipeline. The dark photocopied documents show particularly poor results, undoubtedly because of the large number of spurious punc-tuation symbols they introduce. Here we see that character recognition errors can have, at times, a relatively large impact on one or more of the downstream NLP stages. Sentence boundary detection appears particularly susceptible in the worst case.

In addition to analyzing accuracy rates, it is also instruc-tive to consider counts of the average number of errors that occur on each page. This data is presented in Table 4 ,bro-ken down by error type for each of the NLP pipeline stages. While always altering the input text, in the best case an OCR error might result in no mistakes in sentence boundary detec-tion, tokenization, or part-of-speech tagging: those proce-dures could be agnostic (or robust) to the error in question. Here, however, we can see that on average, each page con-tains a number of induced tokenization and part-of-speech tagging errors, and sentence boundary detection errors also occur with some regularity.

Errors that arise in later stages of processing may be due to the original OCR error, or to an error it induced in an ear-lier pipeline stage. Whatever the cause, this error cascade is an important artifact of pipelined text analysis systems. In Figs. 10 , 11 , 12 , we plot the accuracy for each of the three NLP stages as a function of the input OCR accuracy for all 3,305 documents in our dataset. In viewing the charts, note that the x-axis (OCR accuracy) ranges from 90 to 100%, whereas the range for the y-axis is from 0 to 100%. Accu-racy of the NLP stages is nearly always uniformly lower than the OCR accuracy, sometimes substantially so. 4.3 Impact of OCR errors Because the paradigm we have described can identify and track individual OCR errors as a result of the string align-ments constructed during the optimization of Eq. 4 , we can begin to study which errors are more severe with respect to their downstream impact. Further analyzing a subset of the  X  X orst-case X  documents where OCR accuracy greatly exceeds NLP accuracy (recall plots of Figs. 10 , 11 , 12 ), we identify OCR errors that have a disproportionate effect; Table 5 lists some of these.
We see, for example, that period insertions induced 288 spurious sentence boundaries, and when this particular OCR error arose, it had this effect 94.1% of the time. On the other hand, period deletions occurred less frequently (at least in this dataset), and are much less likely to induce the deletion of a sentence boundary. Note also that relatively common OCR substitution errors nearly always lead to a change in the part-of-speech tag for a token. 5 Conclusions In this paper, we considered a text analysis pipeline consisting of four stages: OCR, sentence boundary detection, tokeniza-tion, and part-of-speech tagging. Using a formal algorith-mic model for evaluating the performance of multi-stage processes, we presented experimental results examining the impact of representative OCR errors on later stages in the pipeline. While most such errors are localized, in the worst case some have an amplifying effect that extends well beyond the site of the original error, thereby degrading the perfor-mance of the end-to-end system. Studies such as this pro-vide a basis for the development of more robust text analysis techniques, as well as guidance for tuning OCR systems to achieve optimal performance when embedded in larger appli-cations.

Since errors propagate from one stage of the pipeline to the next, sentence boundary detection algorithms that work reliably for noisy documents are clearly important. Similarly, the majority of errors that occurred in our study are tokeni-zation or part-of-speech tagging errors that would feed into additional text processing routines in a real system, contrib-uting to a further error cascade. One possible approach for attempting to address this issue would be to retrain existing systems on such documents to make them more tolerant of noise. This line of attack would be analogous to techniques now being developed to improve NLP performance on infor-mal and/or ungrammatical text [ 6 ]. However, this is likely to be effective only when noise levels are relatively low. Much work needs to be done to develop robust methods that can handle documents with high noise levels.

In the context of document summarization, a potential downstream application, we have previously noted that the quality of text analysis is directly tied to the level of noise in a document [ 10 ]. Summaries are not seriously impacted in the presence of minor errors, but as errors increase, the results may range from being difficult to read, to incomprehensi-ble. Here it would be useful to develop methods for assess-ing noise levels in an input image without requiring access to ground-truth. Such measurements could be incorporated into text analysis algorithms for the purpose of segmenting out problematic regions of the page for special processing (or even avoiding them entirely), thereby improving overall readability. Past work on attempting to quantify document image quality for predicting OCR accuracy [ 1 , 2 , 7 ] addresses a related problem, but one which exhibits some notable dif-ferences. Establishing a robust index that measures whether a given section of text can be processed reliably is one possible approach.

We also observe that our current study, while employ-ing real data generated from a large collection of scanned documents, is still limited in that the page layouts, textual content, and image degradations are all somewhat simplistic. This raises interesting questions for future research concern-ing the interactions between OCR errors that might occur in close proximity, as well as higher-level document anal-ysis errors that can impact larger regions of the page. Are such errors further amplified downstream? Is the cumulative effect more additive or multiplicative? The answers to ques-tions such as these will prove important as we seek to build more sophisticated systems capable of handling real-world document processing tasks for inputs that range both widely in content and quality.

Finally, we conclude by noting that datasets designed for studying problems such as the ones described in this paper can be an invaluable resource to the international research community. Hence, we are making our large collection of scanned pages and the associated ground-truth and interme-diate analyses available online [ 14 ]. References
