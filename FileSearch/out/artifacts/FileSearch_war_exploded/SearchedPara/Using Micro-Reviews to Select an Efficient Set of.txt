 Online reviews are an invaluable resource for web users try-ing to make decisions regarding products or services. How-ever, the abundance of review content, as well as the un-structured, lengthy, and verbose nature of reviews make it hard for users to locate the appropriate reviews, and dis-till the useful information. With the recent growth of so-cial networking and micro-blogging services, we observe the emergence of a new type of online review content, consisting of bite-sized, 140 character-long reviews often posted reac-tively on the spot via mobile devices. These micro-reviews are short, concise, and focused, nicely complementing the lengthy, elaborate, and verbose nature of full-text reviews.
We propose a novel methodology that brings together these two diverse types of review content, to obtain some-thing that is more than the sum of its parts. We use micro-reviews as a crowdsourced way to extract the salient aspects of the reviewed item, and propose a new formulation of the review selection problem that aims to find a small set of re-views that efficiently cover the micro-reviews. Our approach consists of a two-step process: matching review sentences to micro-reviews and then selecting reviews such that we cover as many micro-reviews as possible, with few sentences. We perform a detailed evaluation of all the steps of our method-ology using data collected from Foursquare and Yelp. H.4 [ Information Systems Applications ]: Miscellaneous; H.2.8 [ Database Applications ]: Data Mining Algorithms, Experimentation Micro-review, review selection  X  Work done while visiting Living Analytics Research Center, Singapore Management University.

Online reviews are pervasive. Today, for almost any prod-uct or service, we can find ample review content in various Web sources. For instance, Amazon.com hosts product re-views as part of an online shopping experience to assist their customers in determining which product is most suitable for their need. Yelp.com is a popular site for restaurant reviews, assisting diners to plan restaurant visits. Reviews are im-mensely useful in aiding decision-making, because they allow the readers to anticipate what their experience would poten-tially be based on the prior experiences of others, without having to make a trip to the store or the restaurant.
While useful, the deluge of online reviews also causes some issues. Readers are inundated by the numerous reviews, and it is not clear which reviews are worthy of a reader X  X  atten-tion. This is worsened by the length and verbosity of many reviews, whose content may not be wholly relevant to the product or service being reviewed. Reviewers often diverge, meandering around personal details that do not offer any insight about the place being reviewed. Furthermore, it is getting increasingly more difficult to determine the authen-ticity of a review, whether it has been written by a genuine customer sharing her experience, or by a spammer seeking to mislead 1 . Identifying and selecting high quality reviews to show to the users is a hard task, and it has been the focus of substantial amount of research [3, 7, 12, 10, 24, 9].
With the recent growth of social networking and micro-blogging services, we observe the emergence of a new type of online review content. This new type of content, which we term micro-reviews , can be found in micro-blogging services that allow users to  X  X heck-in X , indicating their current loca-tion or activity. For example, at Foursquare, users check in at local venues, such as restaurants, bars, coffee shops. At GetGlue.com, users check in to TV shows, movies, or sports events. Check-ins are also possible within social network-ing sites such as Facebook, or Twitter. After checking in, a user may choose to leave a 140 character-long message about their experience, effectively a micro-review of the place or the activity. Following the Foursquare terminology, we will refer to these messages as tips . In the case of restaurants, these tips are frequently recommendations (e.g., what to or-der) or opinions (what is great or not). For example, this is a Foursquare tip for a popular restaurant in New York:  X  X e patient. It X  X  worth the wait. Their ramen has crack in it. X  .
Micro-reviews serve as an alternative source of content to reviews for readers interested in finding information about a place. They have several advantages. First , due to the length restriction, micro-reviews are concise and distilled , identifying the most salient or pertinent points about the place according to the author. For example, the tip above focuses on the long wait and the quality of the ramen. Sec-ond , because some micro-reviews are written on site and in the moment right after checking in, they are spontaneous , expressing the author X  X  visceral reaction to her experience. This is in contrast to the relatively more contemplative and reflective nature of most reviews, which might express the delayed afterthought of the author. Third , because most authors check in by mobile apps, it is likely that these au-thors are actually at the place when they leave the tips, which makes the tips more likely to be authentic . Micro-blogging sites also have the ability, if necessary, to filter out tips without an accompanying check-in, thus, boosting the authenticity of the tips.
 Micro-reviews and reviews nicely complement each other. While reviews are lengthy and verbose, tips are short and concise, focusing on specific aspects of an item. At the same time, these aspects cannot be properly explored within 140 characters. This is accomplished in full-blown reviews which elaborate and contemplate on the intricacies of a specific characteristic. Marrying these two different reviewing ap-proaches can yield something greater than the sum of their parts: detailed reviews that focus on aspects of a venue that are of true importance to users. This is the goal of this work.
We consider the following problem. Given a collection of reviews, and a collection of tips about an item, we want to select a small number of reviews that best cover the content of the tips. The problem of review selection has been studied in the past [10, 24, 9]. The idea underlying all prior work is to select a small comprehensive set of reviews that carry the most information about an item. In all prior work this is modeled as a coverage problem , where the selected reviews are required to cover the different aspects of the item (e.g., product attributes), and the polarity of opinions about the item (positive and negative). To extract the aspects cov-ered by a review, and the sentiment polarity, off-the-shelf tools are usually applied, which rely on supervised tech-niques trained on manually collected data. Such approaches, although generally successful cannot generalize to arbitrary domains and capture the different aspects that users are in-terested in, or the different ways to describe them in natural language. Unsupervised techniques such as topic modeling have also been applied (e.g., [14]), however they suffer from the broadness of the topic definition.

We view tips as a crowdsourcing way to obtain the aspects of an item that the users care about, as well as the sentiment of the users. By covering the tips, we effectively identify the review content that is important, and the aspects of the item upon which the reviews need to expand and elaborate. In our formulation, which we outline below, we make sure that the selected reviews are compact, that is, the content does not diverge from what is important about the reviewed item. We view this as an important constraint. Reviews, especially for restaurants or other venues, are often read on mobile devices, where screens are small, and time is short. It is thus important to convey the necessary information as efficiently as possible.

Our review selection serves an additional purpose beyond identifying the best reviews to show to a user. It provides a summary of the content of the tips. Tips are short and focused, which is good for quickly zooming in on what is in-teresting about an item. However, this same property makes it hard to go through a large collection of the tips, since they are disjoint, fragmented and repetitive. On the other hand, full-text reviews make for a much more interesting reading, since there is enough space and time to eloquently describe the item that is being reviewed. By selecting the reviews that cover the tips, we effectively obtain a readable, flowing text that summarizes and expands upon the tip content.
Overview of our approach. We now give a high-level overview of our approach. Given an entity (e.g., a restau-rant), we assume we are given as input a collection of reviews R and a collection of tips T about the entity. Our goal is to select a subset of reviews S  X  R that covers the set of tips T as concisely (efficiently) and thoroughly as possible. We first need to define when a review R covers a tip t . Reviews and tips are of different granularity. A tip is short and concise, usually making a single point, while a review is longer and multi-faceted, discussing various aspects of an entity. Intuitively, a review covers a tip, if the point made by the tip appears within the text of the review. To make this more precise, we break a review into sentences, which are semantic units with granularity similar to that of the tips. Given a sentence s and a tip t we define a binary matching function F such that F ( s,t ) = 1 if s and t are sufficiently similar, and zero otherwise. Similarity between s and t means that s and t talk about the same thing, and we can think of one as covering the content of the other.
If a sentence s and a tip t are matched, then we say that s covers t . We will say that a review R covers a tip t if there is a sentence s  X  R that is matched to the tip t . We define the coverage of a review R as the number of tips covered by R . We also define the notion of the efficiency of a review R as the fraction of sentences in R that cover at least one tip. Our goal is to select a set of reviews that, collectively, have high coverage and high efficiency. Intuitively, this corresponds to a compact and comprehensive set of reviews covering most aspects of an item, while avoiding being verbose. In Sec-tion 3 we formulate the coverage problems considered in this paper, and present algorithms for constructing a solution.
The notion of similarity used in the definition of the func-tion F is critical for the successful identification of true matches. We consider three different notions of similarity between a sentence s and a tip t : syntactic similarity, where we require s and t to share common vocabulary; seman-tic similarity, where we require s and t to share common concepts; sentiment similarity, where we require s and t to share common sentiment (positive or negative). We define a methodology for matching a sentence with a tip that takes into account all three of these different similarity definitions. Our methodology is described in detail in Section 4.
We evaluate experimentally the two parts of our approach, the mapping between reviews and tips and the output of the review selection process, using real data collected from Foursquare and Yelp. We evaluate the quality of the map-ping, and we study the tradeoff between coverage and ef-ficiency both quantitatively and qualitatively. The experi-mental analysis is presented in Section 5.

Contributions. Although the content of micro-blogging sites has been studied extensively, micro-reviews is a source of content that has been largely overlooked in the literature. In this paper we study micro-reviews, and we show how they can be used for the problem of review selection. To the best of our knowledge we are the first to mine micro-reviews such as Foursquare tips and combine them with full-text reviews such as Yelp reviews. Our work introduces a novel formu-lation of review selection, where the goal is to maximize coverage while ensuring efficiency, leading to novel coverage problems. We propose heuristic algorithms for these prob-lems, and study them experimentally, demonstrating quan-titatively and qualitatively the benefits of our approach.
Our problem formulation, as far as we know, is novel, both in terms of the objective of covering micro-reviews, as well as in terms of the efficiency constraint. There are however related problem formulations that we discuss below.
Mining Reviews. Recently, there is a line of work that deals with the selection of a  X  X ood X  set of reviews. In [10], the objective is to select a set of reviews that cover all at-tributes (for a given set of attributes). In [24], the objec-tive is refined to also include both the positive and negative aspects of each attribute. The work in [9] further seeks to preserve the underlying distribution of positive and negative comments in the reviews. In [26], the objective is to cover more diversified opinion clusters, rather than just positive or negative. Related to review selection, [23] considers the problem of selecting a good set of photos based on quality, diversity, and coverage.

Our work is along the same lines, but is distinct in two ways. First , in terms of formulation, we seek to represent an underlying collection of micro-reviews, rather than at-tributes. Second , in terms of approach, while prior work relies on some variant of coverage formulation, ours is dis-tinct in introducing the efficiency requirement. To compare against this class of approaches which focus on coverage but not efficiency, we will compare against a max coverage algo-rithm as a baseline in Section 5. There also exists a variant of max coverage called budgeted max coverage [6] where the constraint is a total cost that cannot be exceeded. Our cov-erage formulation is different in how both constraints of cost and count apply, and in how the total cost is computed.
Related to the notion of finding a  X  X ood X  set of reviews is the problem of determining the quality of each individual review [13]. Sites such as Amazon or Yelp allow users to rate each review by its helpfulness or usefulness. Most re-view ranking works rely on a supervised regression or classi-fication approach, using the helpfulness votes as the target class [3, 7, 12]. One possible formulation to produce a set of reviews is to first rank all the reviews based on individ-ual merits, and then selecting the top K . The weakness of this formulation is that it ignores the potential similarities among the top reviews. It may well be that the top few reviews all represent the same information. For comparison, we introduce a baseline called Useful in Section 5, which ranks reviews by its usefulness votes, and selects the top K .
Our work is also related to the problem of review sum-marization, where the goal is to gain a quick overview of the underlying corpus of reviews. Existing approach vary in the kind of summary they produce. In [4, 27], the summary is a list of features, the statistics of positive and negative opinions, as well as some example sentences. In [2, 18], the summary is a list of short phrases. If we treat a review as a document, the summary could also take the form of a subset of sentences from the underlying documents [11]. Dif-ferent from these review summarization works, our objective is closer to micro-reviews summarization (using reviews).
Mining Micro-Reviews. Compared to the wealth of re-lated work on reviews, there has not been as much interest in micro-reviews within the research community. One related work focuses on very short comments on eBay left by buy-ers about sellers [14], but the problem there was to extract aspects from the comments. There are also works [5, 8] on analyzing opinions in micro-blogging services such as Twit-ter. However, because Twitter is a general micro-blogging platform, these opinions are usually about more general con-cepts (e.g., brands, hashtags) rather than specific entities (e.g., products, restaurants). Unlike Foursquare tips, tweets are not attached to any entity, and it is difficult to separate  X  X eviews X  from other types of content in Twitter.

Most of the previous work on Foursquare or other mo-bile check-in services does not view them as a source of micro-reviews, but rather as location-based social networks (LBSN), and it addresses problems such as mining user pro-files [25] and movement patterns [20], or protecting the pri-vacy of the users X  movement patterns [22]. In this section we formulate the review selection problem. We will model this problem as a coverage problem where the goal is to select a small set of reviews that cover as many of the tips as possible with as few sentences as possible. That is, we want to maximize both the coverage and the efficiency of the selected set of reviews, by requiring that there is little content that is not related to at least one tip.

In order to define when a review covers a tip we assume that we are given a mapping between review sentences and tips. We view a review R as a set of sentences R = { s 1 ,...,s and we use U s to denote the union of all review sentences from the reviews in R . The mapping is defined as a match-ing function F : U s  X T  X  X  0 , 1 } , where for a sentence s  X  X  and a tip t  X  X  we have: The notion of similarity between a sentence s and a tip t , and the conditions for matching are formally defined in Section 4. Intuitively, similarity implies that s and t talk about the same concept, using similar language, and having a similar viewpoint (positive or negative). We first give the definitions of coverage and efficiency. Given the collection of reviews R and the collection of tips T , and the matching function F , we define for each review R the set of tips T R that are covered by at least one sentence of review R . Formally: We say that R covers the tips in T R . We define the coverage Cov( R ) of review R as the number |T R | of tips covered by the review R . We also define the efficiency Eff( R ) of the review R as the fraction of sentences in R that cover at least one tip. Formally:
We can extend these definitions to the case of a collec-tion of reviews. For a set of reviews S  X  R , we define the coverage of the set S as: We also define the normalized coverage Cov( S ) as the frac-tion of tips covered by the set S , that is, Cov( S ) = Cov( S ) This normalized notion is useful for comparing between dif-ferent datasets, where the size of reviews and tips may vary.
Extending the definition of efficiency to a collection of reviews is a little more involved. We need a way to aggregate the efficiency of the individual reviews. We propose three possible definitions.
Eff min is useful for imposing a stringent condition on the efficiency of the reviews in the set S . For instance, by re-questing that the minimum efficiency is above some thresh-old, we gain a guarantee that all reviews in the set obey the threshold. The other two definitions Eff avg and Eff bag more flexible, because they consider the set S as a whole. This allows us to select some reviews with high coverage but slightly lower efficiency, if we can balance this choice with other reviews with high efficiency in the set. Eff bag is differ-ent from Eff avg in that it effectively gives longer reviews a higher weight in computing the aggregate efficiency of a set.
Maximizing both coverage and efficiency is a bi-criterion optimization problem, which has no single optimal solution. We transform it into a maximization problem by constrain-ing the efficiency, and asking for a maximum coverage solu-tion. Formally, our problem is defined as follows.
Problem 1 (EffMaxCoverage). Given a set of re-views R , a set of tips T , the matching function F between review sentences and tips, and parameters  X  and K , select a set S of K reviews such that the coverage Cov( S ) of the set is maximized, while the efficiency of the set is at least  X  , that is Eff( S )  X   X  .
 It is easy to see that the EffMaxCoverage is NP-hard. The proof follows from the fact that in the special case that  X  = 0, the EffMaxCoverage problem is the same as the MaxCoverage problem, where our goal is to simply select K reviews that maximize the coverage. Therefore, the Eff-MaxCoverage problem is NP-hard, and we need to look for approximation, or heuristic algorithms.

Our problem definition differs depending on the choice of the efficiency function. In the case that we use the Eff efficiency function we can show a further equivalence with the MaxCoverage problem. Requiring that Eff min ( S )  X   X  implies that each of the selected reviews must have individ-ual efficiency of at least  X  . Therefore, we can again show an equivalence with the MaxCoverage problem, where the universe of available reviews is restricted to the subset of reviews that have efficiency at least  X  .

It is well known that due to the submodularity property of the coverage function, the greedy algorithm that always se-lects the review whose addition maximizes the coverage pro-duces a solution with approximation ratio e/ ( e  X  1), where e is the base of the natural logarithm [19]. That is, the cover-age of the greedy algorithm is at least a e/ ( e  X  1) fraction of the coverage of the optimal algorithm. Therefore, we obtain the following lemma.

Lemma 1. The greedy algorithm for the EffMaxCover-age problem with the Eff min efficiency function has approx-imation ratio e/ ( e  X  1) .

We could not determine an approximation bound for the other two variants of the efficiency function. In the following, we provide a heuristic algorithm which, as a special case, includes the greedy approximation algorithm for Eff min . We present a general greedy algorithm for the EffMax-Coverage problem shown in Algorithm 1. The algorithm proceeds in iterations each time adding one review to the collection S . At each iteration, for each review R we com-pute two quantities. The first is the gain gain( R ), which is the increase in coverage that we obtain by adding this re-view to the existing collection S . The second quantity is the cost cost( R ) of the review R , which is proportional to the inefficiency 1  X  Eff( R ) of the review, that is, the fraction of sentences of R that are not matched to any tip. We select the review R  X  that has the highest gain-to-cost ratio, and guarantees that the efficiency of the resulting collection is at least  X  , where  X  is a parameter provided in the input. The intuition is that reviews with high gain-to-cost ratio cover many additional tips, while introducing little irrelevant con-tent, and they are desirable to be added to the collection.
The cost of the review is parameterized by a value  X   X  [0 , 1), provided as part of the input, which controls the ef-fect of efficiency in our selection of the review R specifically, the cost of a review is defined as follows: When  X  = 0, the review selection is not affected by the efficiency of the reviews, but only by the coverage. For  X  close to 1 the effect of the efficiency on the review selection is maximized. Values in between regulate the effect of effi-ciency in our selection. The higher the value of  X  , the higher the value of coverage that is needed for a low-efficiency re-view to be included in the set. For example, for  X  close to 1, a review R 1 with efficiency 0.5 needs to have at least 250% times more coverage to be picked over another review R with efficiency 0.8. For  X  = 0 . 5, R 1 only needs 25% more additional coverage to be picked over R 2 . Algorithm 1 The EffMaxCover algorithm.
 Input: Set of reviews R and tips T ; Efficiency function Eff; Output: A set of reviews S  X  X  of size K . 1: S =  X  2: while |S| &lt; K do 3: for all R  X  X  do 4: gain( R ) = Cov( S X  R )  X  Cov( S ) 5: cost( R ) =  X  (1  X  Eff( R )) + (1  X   X  ). 6: end for 7: E = { R  X  X  : Eff( S X  R  X  )  X   X  } 8: if E ==  X  then 9: break 10: end if 11: R  X  = arg max R  X  X  gain( R ) / cost( R ) 12: R = R\ R  X  13: end while 14: return S
We obtain different algorithms for different choices of the efficiency function. We study these different variations in detail in the experimental analysis. Note also that by vary-ing the parameters  X  and  X  we can obtain some existing algorithms as special cases. For  X  = 0 and  X  = 0 we ob-tain the greedy algorithm for the MaxCoverage problem. We refer to this algorithm as MaxCover . For  X  = 0 we ob-tain the greedy approximation algorithm for the case of the Eff min efficiency function.
In this section we define the matching function F used in Section 3 for the definition of the coverage problem. We want to match a sentence s and a tip t if they convey a similar meaning, and therefore one can be seen as covering the content of the other. We now consider the criteria for making the matching decision. The first criterion, considers the sentence and the tip as collections of words. If they share a substantial subset of textual content then we assume that they convey a similar meaning. In this case we say that they have high syntactic similarity . The second criterion considers the concept that is discussed. A sentence and a tip may discuss the same concept (e.g., a menu dish), but use different words (e.g., soup vs. broth). In this case we say that they have high semantic similarity . Finally, reviews as well as tips, express the opinions of their respective authors. Hence, in addition to sharing similar keywords and concepts, we would also like a matching sentence-tip pair to share the same sentiment (positive or negative). In this case we say that they have high sentiment similarity .

In the following, we elaborate further on each of the above three types of similarity, and how they can be defined and measured. We then describe how to combine them into the matching function F .

Syntactic similarity (SynSim). A review sentence and a tip are syntactically similar if they share important key-words. For example, a review sentence and a tip about the same Japanese restaurant both use distinctive words such as  X  X amen X  and  X  X oodle X  when referring to a specific dish. A well-established model for keyword similarity is the vector space model [16]. Each review sentence s , and each tip t , are associated with vectors s and t respectively. The dimen-sionality of the vectors is the size of the vocabulary. Each vector entry signifies the importance of the corresponding word. The degree of similarity between the sentence and the tip is then measured as the cosine similarity [16]. Therefore we have:
To compute the importance weights for the words we form a corpus of documents, where each document represents an entity (e.g., restaurant) and it consists of all the tips about this entity. We then use the standard tf-idf [16] weighting scheme for determining the importance of a word. The term frequency tf is the number of times the word appears in the entity document, while the inverse document frequency, idf , is determined by the number of different entity docu-ments in which the word appears. The important words are those frequently used to describe an entity, and unique to the entity.

Semantic similarity (SemSim). A review sentence and a tip are semantically similar, when they are describing the same concept, even if they do not use exactly the same key-words. For instance, when discussing ramen noodles, some may choose to use  X  X roth X , while others use  X  X oup X , although both refer to the same concept. There are two main chal-lenges in determining semantic similarity: first, identifying automatically concepts that are important to each entity; second, finding the words that are used to describe the con-cepts in text. To deal with these challenges, we seek an unsupervised approach that can work across different do-mains. Inspired by the work in text mining, we propose to discover the latent concepts from text using topic modeling.
While there are several potential topic models, here we de-scribe an approach based on the well-known Latent Dirichlet Allocation (LDA) [1]. For illustration, in Table 1, we show an example of topics discovered from the Foursquare tips of a couple of restaurants in New York. Due to space limitation, we show five out of 20 topics learned from each restaurant X  X  tips. Ippudo 2 is a Japanese restaurant serving ramen and pork buns. Some of the topics describe menu dishes (101), waiting time (102), drinks (104), and service (105). These are pertinent concepts in the restaurant domain. Similarly, for the fast-food joint Shake Shack 3 , the topics include menu dishes (201), queue (202), dessert (203), and location (205). This small example serves to demonstrate that the topics do reflect the pertinent concepts in each restaurant.
 Re staurant T opic # T op 5 keywords
I ppudo 1 01 ra men, pork, bun, modern, akamaru
S hake Shack 2 01 b urger, shack, shake, fri, chees Table 1: Example of topics for several restaurants
LDA associates each tip t with a probability distribution  X  over the topics, which captures which topics are most im-portant for a tip. Given the topics, and the corresponding language model for each topic as it is learnt from the tips, we can estimate the topic distribution  X  s for each review sentence s , which captures how well a sentence s reflects the topics being discussed in the corpus of tips. To measure the semantic similarity between a review sentence and a tip, we measure the similarity of the topic distributions  X  s and  X  . A commonly used distance measure between two proba-bility distributions is the Jensen-Shannon Divergence (JSD) [16]. Intuitively, a sentence and a tip are semantically sim-ilar if their topic distributions can describe each other well. Therefore, we have:
Sentiment similarity (SentSim). A matching pair of review sentence and tip should also represent the same sen-timent. Sentiment extraction from text is an active area of research [21]. Here, we cast the problem as a classification problem, where the goal is to predict the sentiment (positive or negative) of a sentence or a tip. We thus have two classes c + and c  X  . We use a maximum entropy classifier (MEM) [15], which has been demonstrated to work well for sentiment classification in text [21], using N-gram features. Given a document d (a sentence or a tip), the MEM classifier out-puts conditional probabilities P ( c + | d ) and P ( c  X  | d ) for the positive and negative classes, where P ( c + | d ) + P ( c
Given the classifier output for a document d , we trans-form the probability P ( c + | d )  X  [0 , 1] into polarity( d ) = 2 P ( c + | d )  X  1, in the range of -1 (extremely negative) to 1 (extremely positive). For P ( c + | d ) close to 1/2, the polar-ity is close to zero, which agrees with our intuition that in these cases the document has neutral polarity. We define the sentiment similarity between a sentence s and a tip t as the product of their polarities: it approaches 1 when the sentence and the tip X  X  polarities are similar; it approaches -1 when their polarities are opposite; it approaches 0 when the tip or the sentence is neutral. Therefore, we have:
Matching Function. Having defined the three main cri-teria for matching (syntactic, semantic, and sentiment), we would like to combine them to determine whether a review sentence s and a tip t match or not. One principled way to combine the three criteria is through a supervised binary classification framework, with two classes match and non-match , based on the three features we defined above: syntac-tic similarity SynSim( s,t ), semantic similarity SemSim( s,t ), and sentiment similarity SentSim( s,t ). For a sentence-tip pair ( s,t ) the classifier estimates the matching probability P ( s,t ). The binary mapping function F ( s,t ) is thus defined in terms of the matching probability, using on a threshold  X  , as follows: We discuss the choice of the threshold  X  in the experiments.
The objective of the experiments is to showcase the effec-tiveness of the proposed approach in finding a set of reviews that cover as many tips as possible, in an efficient manner. First, we will describe the real-life dataset used in the ex-periment. This is followed by an evaluation of the matching process described in Section 4. We then investigate how the coverage algorithms proposed in Section 3 behave under different parameter settings, as well as how they compare against the baselines. Our focus here is on effectiveness, rather than speed, as the matching can be done offline, and the greedy algorithm for review selection is fast.
The experiments require data coming from two differ-ent sources (reviews and micro-reviews), but concerning the same set of entities. We pick the domain of restaurants, be-cause it is one of the few domains where there are already active platforms for reviews as well as for micro-reviews. For reviews, we crawl Yelp.com to obtain the reviews of the top 110 restaurants in New York City with the highest number of reviews as of March 2012. For micro-reviews, we crawl the popular check-in site Foursquare.com to obtain the tips of the same 110 restaurants. However, some of the restau-rants in Foursquare.com have too few tips, which may not adequately reflect the restaurant X  X  information. Therefore, we retain only the 102 restaurants with at least 50 tips each. For these 102 restaurants, we have a total of 96,612 reviews, with a minimum of 584, and a maximum of 3460 per restau-rant. We also have a total of 14,740 tips, with a minimum of 51, and a maximum of 498 per restaurant. Note that we get the full set of reviews and tips of each restaurant at the time of extraction, and that these are the realistic sizes of the real-world data. It is also important to note that every restaurant is a distinct instance of the coverage problem.
Matching between a review sentence and a tip is by itself a very challenging problem. Our objective in this experiment is to establish that we achieve a reasonable level of quality in matching, such that the reviews selected by the coverage algorithms would be a good reflection of the covered tips.
To build the matching classifier, we generate the three real-valued features described in Section 4. For semantic similarity, we train LDA [1] topic models using the MALLET toolbox [17]. Because topic modeling is probabilistic, we average the semantic similarity over ten runs. To determine the sentiment polarity of each sentence and tip, we train a sentiment classifier using the Stanford Classifier toolkit [15] with textual features (word and letter n-grams).

To train the matching classifier, we sample 20 entities, and for each entity we sample 50 sentence-tip pairs sharing at least one common word. We assume no match otherwise. For these 1,000 pairs, we get three judges to label whether the pairs match in meaning, and take the majority label as the ground truth. Finally, we use the real-valued features and the majority labels to train the matching classifier using the MEM classifier from [15]. Based on the feature weights learned by the classifier, we find that among the three fea-tures, semantic similarity is the most important, followed by syntactic, and lastly sentiment.

To validate the effectiveness of the matching classifier, we conduct a five-fold validation, with 80:20 split between train-ing and testing in each fold. As metrics, we use precision and recall at the pair level. Precision is the fraction of true matching pairs within the set of classified matching pairs. Recall is the fraction of true matching pairs found by the classifier within the set of all true matching pairs. Because the objective of matching is to determine which review sen-tence will match a tip well, it is important to gain a high precision, so we can be confident that the reviews discovered by the coverage algorithms will reflect the underlying tips.
Number of topics. We study the performance of match-ing classifier as we vary the number of topics used for the semantic similarity. In Figure 1, we plot the precision-recall curve for  X  = 0 . 65 (discussed below). Besides showing the regular trade-off between precision and recall, it also shows that the effect of the number of topics is not significant. The performance for 20 X 40 topics is better than 10 (which may underfit), or 50 (which may overfit). The results for 20 top-ics are slightly better than the rest, especially in achieving higher precision, which is our main concern in the matching. In subsequent discussions, we show the results for 20 topics.
Threshold  X  . We also experiment with different values for the threshold  X  on the probability of matching P ( s,t ). Table 2 shows the precision and recall of the matching clas-sifier at different values of  X  . If we were to skip the matching classification, and simply take all the pairs with at least one common word as matching, we get a precision of only 43%, which means more than half of all matching pairs would be incorrect. As we increase the threshold  X  , the precision im-proves significantly. If we would like at least three-quarters of matching pairs to be correct, we need to put the threshold at 0.65 of higher. At this threshold, the recall is relatively low at 23%, but this can be compensated by the fact that a tip may be covered by many different sentences.

The last column shows the percentage of tips that can be covered by at least one sentence. At 0.65, we cover 83.5% of all tips, a substantial subset. For subsequent experiments on coverage, we will present results for  X  = 0 . 65.
To get an intuitive sense of the matching quality, we show some examples of matching pairs for the restaurant Ippudo in Table 3. The first pair discuss the pork buns, which is a specialty of the restaurant. The other pairs both discuss the waiting time, but while the second pair sound positive, the Table 3: Example of matching pairs for Ippudo, NY third pair show negative sentiment. These examples show-case how the features, i.e., syntactic, semantic, and senti-ment similarity, help to identify relevant matching pairs.
The objective of these experiments is to showcase the effi-cacy of the proposed EffMaxCover algorithm at finding the top K reviews with high coverage of tips, while satisfying the efficiency constraint. We will first show results for K = 5, before we investigate the effect of varying K .

The input to the algorithms is the sentence-tip matchings generated for all 102 restaurants in the dataset. To avoid degenerate cases of reviews that achieve very high efficiency simply by being very short, we restrict ourselves to reviews of at least five sentences. Our evaluation is based on the normalized coverage, defined as the fraction of tips that are covered by the top K reviews over the total number of cov-erable tips, and the average efficiency Eff avg ( S ), defined as the average efficiency of the individual reviews in the top K . To represent the results for all the restaurants, we average the coverage and efficiency values accross restaurants.
Baseline: MaxCover. We first establish the baseline level of performance by MaxCover , which also has the ob-jective of maximizing coverage, but does not consider the efficiency constraint. Because MaxCover is not constrained in the review selection, it obtains a relatively high coverage of 0 . 72, which is also the ceiling for EffMaxCover (because of the efficiency constraint). MaxCover  X  X  efficiency is only 0 . 43, and this is the floor for EffMaxCover that searches for a more efficient set of reviews.
EffMaxCover: Varying  X  . There are two ways in which EffMaxCover controls the efficiency of the selected set of reviews. The first is by the threshold  X  , which guar-antees the efficiency of the set is at least  X  . The second is by the parameter  X  which controls the sensitivity of the selec-tion process to the efficiency of the next review to be added to the set. To isolate the effect of  X  , we first fix  X  = 0, making the cost a constant, independent of the efficiency. Since MaxCover already has an efficiency of 0 . 43 and the  X  X ptimal X  coverage, we will focus on  X  &gt; 0 . 43, and investi-gate whether EffMaxCover can achieve a higher efficiency without much reduction in coverage. We vary  X  from 0.5 to 1.0, and plot the coverage and efficiency in Figure 2. Figure 2(a) shows that as  X  increases, the coverage of Eff-MaxCover algorithms decrease. Due to the constraint on the aggregate efficiency being at least  X  , we miss out on higher-coverage, but lower-efficiency reviews. Figure 2(b) shows that efficiency at first increases with  X  , because the re-views selected tend to be of increasingly higher efficiency. At some point though, when  X  &gt; 0 . 8, the efficiency decreases, because this requirement becomes too stringent, and many restaurants do not have reviews that meet this requirement. Among the different ways of aggregating efficiency for Eff-MaxCover , we observe EffMaxCover avg and EffMaxCover bag perform very similarly. On the other hand, EffMaxCover min performs differently. It tends to have higher efficiency but lower coverage, because every review selected has to meet the efficiency threshold, reducing the set of candidate re-views available, whereas the other two algorithms consider the efficiency of the whole set and may pick some reviews with high coverage, but with efficiency slightly below  X  if the reviews already in the set have high efficiency.
EffMaxCover: Varying  X  . Having fixed parameter  X  , we now study the effect of parameter  X  on the performance of the algorithm. Figure 3 shows how the coverage and effi-ciency change as  X  increases from 0 to 1 for EffMaxCover bag (the curves for other variants are similar and not shown due to space limitation). Following the previous discussion, we plot the curves for the values of  X  between 0.5 to 0.8.
At  X  = 0, the cost is a constant, and we rely entirely on  X  to maintain efficiency. As we increase  X  , the greedy selection of reviews will increasingly be sensitive to the cost (loss in efficiency). Figure 3 shows that for all values of  X  , as  X  increases, the efficiency increases while the coverage decreases. Interestingly, the gain in efficiency outpaces the loss in coverage. For example, for  X  = 0 . 5, from  X  = 0 to  X  = 1, efficiency increases from 0.54 to 0.76 (efficiency gain of 0.22), while the coverage reduces from 0.66 to 0.60 (coverage loss of 0.06). This shows that  X  is an effective way to gain efficiency with minimal loss in coverage.

In order to have a single metric that balances the cover-age vs. efficiency trade-off, inspired by the F1 measure in information retrieval, we use the harmonic mean of the two: Figure 3(c) plots the harmonic mean when varying  X  and  X  values. It shows that  X  = 0 . 5 and  X  = 0 . 6 tend to have a better balance between having high coverage and high effi-ciency. Of all the points in Figure 3(c), the combination with the highest harmonic mean of 0.67 is  X  = 0 . 5 and  X  = 0 . 9, which yields a coverage of 0.63 and an efficiency of 0.72. Subsequently, we will use this setting for EffMaxCover .
EffMaxCover: Varying K . We now compare the per-formance of EffMaxCover to MaxCover as well as to other baselines, for varying top K  X  [3 , 15] reviews. We consider the following additional baselines. MaxLength selects the longest K reviews, with the intuition that longer reviews may cover more tips. Conversely MinLength selects the shortest K reviews (not less than five sentences), with the intuition that shorter reviews may be more efficient. Yelp reviews may also be voted by users as being useful, and we consider the K reviews with the highest number of useful-ness votes as another baseline Useful . Finally, to emphasize the statistical significance of the results, we also compare to the performance of Random , which selects K reviews ran-domly. For Random , we average the coverage and efficiency across 1,000 random runs, and plot the median, as well as the error bar (min and max).

Figure 4(a) shows how coverage varies with K for various methods. As expected, MaxCover has the highest coverage, followed closely by the EffMaxCover variants. MaxLength and Useful also do better than Random , but worse than EffMaxCover . MinLength has the lowest coverage, as it has very few sentences to capture the tips.

Figure 4(b) shows that the efficiency of EffMaxCover al-gorithms is by far superior to all the baselines. This un-derlines the effectiveness of EffMaxCover in finding efficient reviews. The efficiency tends to decrease slightly with in-creasing K , which is expected as it gets increasingly more difficult to find high-coverage and high-efficiency reviews af-ter each selection. Interestingly, the efficiency of MaxLength and Useful fall below that of Random , which could be due to the length of the reviews, resulting in having many sen-tences that may not represent any tip. MinLength is more efficient than MaxLength , but is also worse than Random . This suggests that being short alone is not sufficient if it does not also capture the tips well.

To emphasize the efficacy of EffMaxCover at achieving both coverage and efficiency, we plot the harmonic mean of coverage and efficiency in Figure 4(c). It shows how the three EffMaxCover variants outperform the rest signif-icantly, followed by MaxCover . MaxLength and Useful are no better than Random , whereas MinLength is the worst.
Qualitative Analysis. In addition to quantitative study, we also conduct a qualitative analysis involving three human judges who are not related to this paper. To each judge, we show the top 3 reviews selected by an algorithm for a sample of 20 restaurants, and ask the judge to choose which aspects are mentioned in the reviews from a manually hand-picked list of aspects. Because the objective of this analysis is to investigate the trade-off between coverage and efficiency, we focus the comparison on two methods: the EffMaxCover bag algorithm as a representative of the EffMaxCover variants, and the MaxCover , as the closest competitor.

Table 4 shows that on average, the judges identify 5.1 as-pects for MaxCover , and 3.6 aspects for EffMaxCover This lower coverage of aspects is expected, and consistent with the previous experiments. On the other hand, the re-views selected by EffMaxCover bag are much more compact, with an average of 24.7 sentences total in three reviews, as compared to the lengthy 121 sentences by MaxCover . This suggests a gain in efficiency. If we look at the density of information covered, and determine the ratio of aspects cov-ered per sentence, the third column of Table 4 shows that EffMaxCover bag has much higher density of 0.15 aspects per sentence, as compared to 0.04 by MaxCover .

To illustrate the different types of reviews selected by the various criteria, as a case study, we show an example of the top review selected by each algorithm for the venue 53rd and 6th Halal Cart . This is a food cart serving middle-eastern fare in New York, well-known for its meat dishes and sauces. In Figure 5, we show the top review selected by EffMaxCover (all three variants selected the same), Max-Cover , Useful , and MinLength . Due to space limitation, we cannot reproduce MaxLength here, but we refer the reader to the following link: http://www.yelp.com/biz/53rd-and-6th-halal-cart-new-york#hrid:s1opbJu3mS3L-DSsOXmIYQ.

Figure 5(a) shows that EffMaxCover selects a compact review, which describes the main attributes of the place: a food cart, popular chicken lamb combo, sauces, and long lines. Figure 5(b) shows that MaxCover  X  X  top review also covers these attributes, but with a very long review. Parts of the review are not to the point. For instance, the first quarter ( X  X ackground X ) does not concern the restaurant di-rectly. Figure 5(c) shows that Useful  X  X  top review also covers these attributes, but not as compactly as EffMaxCover , with side references to Paris Hilton and Victoria Secret that are not pertinent to the restaurant. A similar conclusion can be drawn for MaxLength as well. MinLength  X  X  top review (Fig-ure 5(d)) is very short and only covers the generics ( X  X ast X ,  X  X heap X ,  X  X ood X ), without getting into helpful details such as the dishes and the sauces, like the other reviews above.
In this paper, we introduce the use of micro-reviews for finding an informative and efficient set of reviews. This se-lection paradigm is novel both in the objective of micro-review coverage, as well as in the efficiency constraint. The selection problem is shown to be NP-hard, and we design a heuristic algorithm EffMaxCover , which lends itself to several definitions of aggregate efficiency. The results are evaluated over a corpora of restaurants X  reviews and micro-reviews. Experiments show that EffMaxCover discovers re-view sets consisting of reviews that are compact, yet in-formative. Such reviews are highly valuable, as they lend themselves to quick viewing over mobile devices, which are increasingly the predominant way to consume Web content. The work is supported by the National Research Founda-tion under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office. This work has been supported by the Marie Curie Reintegration Grant project titled JMUGS which has re-ceived research funding from the European Union.
Figure 5: Top review for 53rd and 6th Halal Cart
