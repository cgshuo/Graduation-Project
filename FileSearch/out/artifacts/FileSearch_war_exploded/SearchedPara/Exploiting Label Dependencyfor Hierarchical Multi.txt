 Traditional classification tasks deal with assigning instances to a single label. In multi-label classification, the task is to find the set of labels that an instance can belong to rather than assigning a single label to a given instance. Hierarchical multi-label classi-fication is a variant of traditional classification where the task is to assign instances to a set of labels where the labels are related through a hierarchical classification scheme [1]. In other words, when an instance is labeled with a certain class, it should also be labeled with all of its superclasses, this is known as the hierarchy constraint .
Hierarchical multi-label classification is a widely studied problem in many domains such as functional genomics, text categorization, image annotation and object recogni-tion [2]. In functional genomics (which is the application that we focus on in this paper) the problem is the prediction of gene/protein functions. Biologists have a hierarchical organization of the functions that the genes can be assigned to. An individual gene or protein may be involved in more than one biological activity, and hence, there is a need for a prediction algorithm that is able to identify all the possible functions of a particular gene [2]. There are two types of class hierarchy structures: a rooted tree structure, such as the MIPS X  X  FunCat taxonomy [17], and a directed acyclic graph (DAG) structure, such as the Gene Ontology (GO) [7]. In this paper, we use the FunCat scheme.

Most of the existing research focuses on a  X  X lat X  classification approach, that oper-ates on non-hierarchical classification schemes, where a binary classifier is constructed for each label separately as shown in Figure 1 (a). This approach ignores the hierarchi-cal structure of the classes shown in Figure 1(b). Reducing a hierarchical multi-label classification problem to a conventional classification problem allows the possibility of applying the existing methods. However, since the prediction of the class labels has to be performed independently, such transformations are not capable of exploiting the interdependencies and correlations between th e labels [6]. Moreover, the flat classifica-tion algorithm fails to take advantage of the information inherent in the class hierarchy, and hence may be suboptimal in terms of efficiency and effectiveness [9]. 1.1 Our Contributions In this paper, we propose, HiBLADE, a hierar chical multi-label classification frame-work for modeling the pre-defined hierarchical taxonomy of the labels as well as for exploiting the existing correlations between different labels, that are not given by the taxonomical classification of the labels, to facilitate the learning process.
To the best of our knowledge, there is no work related to the hierarchical multi-label classification setting that exploits the correlations between different labels other than the domain-based pre-established taxonomical classification of the classes .Our intuition is that the domain-based taxonomi cal classification of the classes should be used as additional features while label dependencies are inferred from the data. Specif-ically, a novel approach to learn the label dependencies using a Bayesian framework and instance-based similarity is proposed. Bayesian framework is used to characterize the dependency relations among the labels r epresented by the directed acyclic graph (DAG) structure of the Bayesian network. A s opposed to other hierarchical multi-label classification algorithms, our algorithm has the following advantages: 1. The underlying pre-defined taxonomy of t he labels is explicitly expressed which 2. It is capable of addressing correlations an d interdependencies among the children 3. The use of a shared boosting model for the child labels for each label in the hierar-The rest of the paper is organized as follows: related work is discussed in Section 2. Our proposed method, HiBLADE, is presented in Section 3. In Section 4, we report our experimental results on several biomolecu lar datasets. Then, we conclude and discuss further research directions in Section 5. Since conventional classification methods, s uch as binary classification and multi-class classification, were not designed to directly t ackle the hierarchical classification prob-lems, such algorithms are referred to as flat classification algorithms [18]. It is important to mention that flat classification and other similar approaches are not considered to be a hierarchical classification approach, as the y create new (meta) classes instead of using pre-established taxonomies.

Different approaches have been proposed in the literature to tackle the hierarchi-cal multi-label classification problem [4,21,16]. Generally, these approaches can be grouped into two categories: the local classifier methods and the global classifier meth-ods. Moreover, most of the existing methods use a top-down class prediction strategy in the testing phase [3,16,18]. The local strategy treats any label independently, and thus ignores any possible correlation or interdependency between the labels. There-fore, some methods perform an additional step t o correct inconsistent predictions. For example, in [3], a Bayesian framework is developed for correcting class-membership inconsistency for the separate class-wise models approach. In [1], a hierarchical multi-label boosting algorithm, named HML-Boosting, was proposed to exploit the hierarchi-cal dependencies among the labels. HML-Boosting algorithm relies on the hierarchical information and utilizes the hierarchy to improve the prediction accuracy.

True path rule ( TPR ) is a rule that governs the annotation of GO and FunCat tax-onomies. According to this rule, annotating a gene to a given class is automatically transferred to all of its ancestors to maintain the hierarchy constraint [12]. In [20], a true path ensemble method was proposed. In this method, a classifier is built for each functional class in the training phase. A bottom-up approach is followed in the test-ing phase to correct the class-membership inconsistency. In a modified version of TPR ( TPR  X  w ), a parent weight is introduced. The we ight is used to explicitly modulate the contribution of the local predictions with the positive predictions coming from the descendant nodes. In [5], a hierarchical bottom-up Bayesian cost-sensitive ensemble (HBAYES-CS), is proposed. Basically, a calibrated classifier is trained at each node in the taxonomy. H-loss is used in the evaluation phase to predict the labels for a given node. In a recent work [2], we proposed a novel Hierarchical Bayesian iNtegration algorithm HiBiN, a general framework that uses Bayesian reasoning to integrate het-erogeneous data sources for accurate gene f unction prediction. On the other hand, sev-eral research groups have studied the effective exploitation of correlation information among different labels in the context of multi-label learning. However, these approaches do not assume the existence of any pre-defined taxonomical structure of the classes [6,11,14,23,24]. Let X = d be the d -dimensional input space and Y = { y 1 ,y 2 , ..., y L } be the finite set of
L possible labels. The hierarchical relationships among classes in Y are defined as follows: Given y 1 ,y 2  X  X  ,y 1 is the ancestor of y 2 , denoted by (  X  y 2 )= y 1 , if and only if y 1 is a superclass of y 2 .

Let a hierarchical multi-label training set D = { &lt;x 1 , Y 1 &gt;, ..., &lt; x N , Y N &gt; } , where x i  X  X  is a feature vector for instance i and Y i  X  X  is the set of labels associated with x i , such that y i  X  X  i  X  y i  X  X  i ,  X  (  X  y i )= y i .Having Q as the quality criterion for evaluating the model based on the pre diction accuracy, the objective function is defined as follows: a function f : D X  2 y . Here, 2 y is the power set of Y , such that Q is maximized, and y  X  f ( x )  X  y  X  f ,  X  (  X  y )= y . The function f is represented here by the HiBLADE algorithm.

Hierarchical multi-label learning aims to model and predict p ( child class | parent class ) . Our goal is to make use of hierarchical dependencies as well as the extracted dependencies among the labels y k where 1  X  k  X L and (  X  y k )= y m such that for each example we can better predict its labels. The problem then becomes how to identify and make use of such dependencies in an efficient way. 3.1 Training Scheme The training of each classifier is performed lo cally. During classification, the classifier at each class will only be presented with examples that are positive at the parent class of the current class. Hence, the reached exam ples are positive examples to the current class and/or to the siblings of that class. In ot her words, the training for each classifier is performed by feeding as negative training exa mples, the positive examples at the parent of the current class that are not positive examples at the current class. 3.2 Extending the Features The feature vector for each example is extende d to include the class labels of the levels higher in the hierarchy than the current level as given in line 9 of Algorithm 1. More formally, the feature vector for each instance j that belongs to a class i will have the 3.3 Label Correlation The other type of dependencies is modeled using Bayesian structure and instance-based similarity as shown in line 10 of Algorithm 1. For each class i , we get the children classes of class i and sharedModels algorithm (shown in Algorithm 2) is invoked. Algorithm 1. HiBLADE In each boosting iteration t , the entire pool is searched for the best fitted model other than the model that was built directly for that label and its corresponding combination candidate model. The chosen model h t c is then updated based on the following formula: where ji is the error results from applying model h t j on the examples in class i and ii is the error results from applying the model h t i on the examples in class i .  X  ij controls the proportional contribution of Bayesian-based and instance-based similarities.  X  ij is computed as follows: where b ij is the Bayesian correlation between class i and class j , and it is estimated as b ij = | i  X  j | / | j | ,where | i  X  j | is the number of positive examples in class i and class j and | j | is the number of positive examples in class j . s ij is the instance-based similarity between class i and class j . Each instance from one class is compared to each other instance from the other class. In HiBLADE, s ij is computed using the Euclidean distance between the positive examples in both classes that has the following formula: where l is the corresponding feature in the two vectors. s ij is normalized to be in the range of [0 , 1] .  X  is a threshold parameter that has a value in the range [0 , 1] . Setting  X  to 0 means that only instance-based similarity is taken into consideration in the learning process. While setting it to 1 means that only Bayesian-ba sed correlation is taken into consideration. On the other hand, any value of  X  between 0 and 1 combines both types of correlation. It is important to emphasize that these computations are performed only for the class that is found to be the most useful class with respect to the current class.
In the general case, both classes, the current class and the candidate class, contribute to the final prediction. In other words, any value of ji other than 0 , indicates the level of contribution from the candidate class. More specifically, if the error of the candidate class, ji , is greater than the error of the current class, ii ,thevalueof  X  ij will be small indicating that only a limited contribution of the candidate class is considered. In con-trast, if the error of the current class, ii , is greater than the error of the candidate class, ,then  X  ij will be high, and hence, the prediction decision will be dependable more on the candidate class. Finally, the models for the current class and the used candidate class are replaced by the new learned mode ls. At the end, the composite classifiers F c provide the prediction results.

Algorithm 2 shows the details of the shared models algorithm. The shared models al-gorithm takes as input the children classes of a particular class together with the feature vectors for the instances that are positive at the parent class. These instances will form the positive and negative examples for each one of the children classes. The algorithm begins by initializing a pool of M models, where M is the number of children classes, one for each class that is learned using a boosting-type algorithm such as ADABOOST. The number of base models to be generated is determined by T . In each iteration t and for each label in the set of the children labels, we look for the best fitted model, h ( x ) and the corresponding combination weights,  X  t l . The contribution of the selected words, if the error, ji of the candidate classifier is 0 , this will be a perfect model for the current label. Hence, equation (1) will be reduced to  X  ij =  X  ij . In this case, the contribution of that model depends on the level of correlation between the candidate class and the current one. On the other ha nd, if the current model is a perfect model, i.e., the error ii =0 , then equation 1 will be reduced to  X  ij =0 , which means that for the current iteration, there is no need to look at any other classifier.
 Algorithm 2. SharedModels We chose to demonstrate the performance of our algorithm for the prediction of gene functions in yeast using four bio-molecular datasets that were used in [20]. Valentini [20] pre-processed the datasets so that fo r each dataset, only genes that are annotated with FunCat taxonomy are selected. To make this paper self-contained, we briefly ex-plain the data collection process and the pre -processing steps performed on the data. Uninformed features that have the same v alue for all of the examples are removed. Class  X 99 X  in FunCat corresponds to an  X  X nclassified protein X . Therefore, genes that are annotated only with that class are excluded. Finally, in order to have a good size of positive training examples for each class, se lection has been performed to classes with at least 20 positive examples. Dataset characteristics are summarized in Table 1.
The gene expression dataset, Gene-Expr, is obtained by merging the results of two studies, gene expression measures relative to 77 conditions and transcriptional responses of yeast to environmental stress measured on 173 conditions [10]. For each gene prod-uct in the protein-protein interaction dataset, PPI-BG, a binary vector is generated that implies the presence or absence of protein-protein interaction. Protein-protein interac-tion data have been downloaded from BioGRID database [19,20]. In Pfam-1 dataset, a binary vector is generated for every gene product that reflects the presence or ab-sence of 4950 protein domains obtained from Pfam (Protein families) database [8,20]. For PPI-VM dataset, Von Mering experiments produced protein-protein data from yeast two-hybrid assay, mass spectrometry of purified complexes, correlated mRNA expres-sion and genetic interactions [22].
 4.1 Evaluation Metrics Classical evaluation measures such as precision, recall and F-measure are used by un-structured classification pr oblems and thus, they are inadequate to address the hierarchi-cal natures of the classes. Another approach that is used for the hierarchical multi-label learning is to use extended versions of the s ingle label metrics (precision, recall and F-measure). To evaluate our algorithm, we adopted both, the classical and the hierarchical evaluation measures. F 1 measure considers the joint contribution of both precision (P) and recall (R). F 1 measure is defined as follows: where TP stands for True Positive, TN for True Negative, FP for False Positive and FN for False Negative. When TP=FP=FN=0, we made F 1 measure to equal to 1 as the classifier has correctly classified all the examples as negative examples [9]. Hierarchical measures are defined as follows: where hP, hR and hF stands for hierarchical pr ecision, hierarchical recall and hierarchi-cal F-measure, respectively. P ( x ) is a subgraph formed by the predicted class labels for the instance x while C ( x ) is a subgraph formed by the true class labels for the instance x . p is one of the predicted class labels and c is one of the true labels for instance x . also computed both micro-avera ged hierarchical F-measure ( hF  X  1 ) and macro-averaged hierarchical F-measure hF M 1 . hF  X  1 is computed by computing hP and hR for each path in the hierarchical structure of the tree and then applying equation (7). On the other hand, hF M 1 is computed by calculating hF 1 for each path in the hierarchical structure of the classes independently and then averaging them. Having high hierarchical pre-cision means that the predictor is capable of predicting the most general functions of the instance, while having high hierarchical recall indicates that the predictor is able to predict the most specific classes [20]. The hierarchical F-measure takes into account the partially correct paths in the overall taxonomy. 4.2 Experimental Results and Discussion We analyzed the performance of the proposed framework at each level of the Fun-Cat taxonomy, and we also compared the proposed method with four other methods that follow the local classifier approach, n amely, HBAYES-CS, HTD, TPR and TPR-w. HBAYES-CS, TPR and TPR-w are described in the Related Work Section. HTD (Hier-archical Top-Down) is the baseline method that belongs to the local classifier strategy and performs hierarchical classification i n a top-down fashion. Since HiBLADE also belongs to the local classifier strategy, it is fair to have a comparison against a local classifier approach that does not consider any type of correlation between the labels. We also analyzed the effect of the proper choice of the threshold  X  on the performance of the algorithm. The setup for the experiments is summarized as follows:  X  Flat: This is the baseline method that does not take the hierarchical taxonomy of the  X  HiBLADE I : The proposed algorithm that considers Instance-based similarities  X  HiBLADE B : The proposed algorithm that considers classes correlation based on  X  HiBLADE C : The proposed algorithm that considers a combination of both First, we performed a level-wise analysis of t he F-measure of the FunCat classification tree on the four datasets. In measuring the level-wise performance, level 1 reflects the root nodes while all other classes are at depth i ,where 2  X  i  X  5 . We show the results for the top four levels in the hierarchy for the proposed method and the flat method. Moreover, we show the performance of the proposed framework with different  X   X  X  values while setting the number of boosting iterations to 50 iterations. Tables 2, 3, 4 and 5 show the results of per-level evaluation for Gene-Expr, PPI-BG, Pfam-1 and PPI-VM datasets, respectively. The most signifi cant measures for each level are highlighted. The proposed algorithm outperforms the flat classification method in most of the cases with significant differences in the perform ance measurements. The results in Tables 2, 3, 4 and 5 indicate that the deeper the level the better the performance of the proposed algorithm compared to the flat classification method. For example, in all of the datasets, the proposed algorithm outperformed the flat classification method in all the levels that are higher than level 1 . This result is consistent with our understanding of both of the classification schemes. In other words, the proposed method and the flat classification method have a similar learning procedure for the classes in the first level. However, the proposed method achieved better results for the deeper levels in the hierarchy.
To get more insights into the best choice of  X  threshold, we compare hierarchical precision, hierarchical recall, hierarchical F  X  1 measure and hierarchical F M 1 measure for Gene-Expr, PPI-BG, Pfam-1 and PPI-VM datasets for  X  =0 . 0 , 0 . 5 and 1 . 0 ,re-spectively, for 50 boosting iterations. Table 6 shows the results of the comparisons. The most significant measures are highlighted. As shown in Table 6, the combination of Bayesian-based correlation and instance-based similarity achieved the best perfor-mance results in most of the cases. For example, six of the highest performance values, in general, in this table are achieved when  X  =0 . 5 .
 Furthermore, we conducted comparisons of hierarchical F-measure with HBAYES-CS, HTD, TPR and TPR-w methods. HBAYES-CS is using Guassian SVMs as the base learners, while HTD, TPR and TPR-w are using Linear SVMs as the base learners. Figure 2 shows the F-measure of the different methods. By exploiting the label de-pendencies, the classifiers performance are effected positively. Our results show that the proposed algorithm significantly outperforms the local learning algorithms. Al-though there is no clear winner among the different versions of HiBLADE algorithm, HiBLADE always achieved significantly better results than the other methods.
 In this paper, we proposed a hierarchical multi-label classification framework for in-corporating information about the hierarchical relationships among the labels as well as the label correlations. The experimental results showed that the proposed algorithm, HiBLADE, outperforms the flat classification method and the local classifiers method that builds independent classifier for each cla ss. For future work, we plan to generalize the proposed approach to general graph structures and develop more scalable solutions using some other recent proposed boosting strategies [13,15].

