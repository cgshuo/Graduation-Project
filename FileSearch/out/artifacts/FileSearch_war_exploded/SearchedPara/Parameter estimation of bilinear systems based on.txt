 1. Introduction because many objects in engineering, economics, ecology and biology etc. can be described by using a bilinear system ( Lee et al., 1997; Mohler, 1991; Hua, 1990 ). A bilinear system is the simplest and most similar nonlinear system to a linear system in the form.
It has many advantages, such as its special variable structure property which makes it advantageous on system modeling. The research of parameter estimation of the bilinear system has been carried out and received extensive attentions. Accurate knowl-edge of these parameters is important to form the control laws.
Hence, it is of our interest to investigate an efficient model parameter tracking approach to achieve precise modeling results under different conditions without using complicated model structures.

Recursive Least Square (RLS) ( Godfrey and Jones, 1986 ), Recursive method of Instrumental Variable (RIV), Correlative Function method (COR), etc. exists for parameter estimation. RLS is usually considered that it is simple to use and costs little computation but it gives large estimation error when the system is influenced by colorful noises. Although COR can acquire more accurate para-meter estimation but the estimation error is still not satisfying.
Moreover, RIV can get accurate parameter but it costs large amount of complex computation. Most of these techniques have some fundamental problems including their dependence on unrealistic assumptions such as unimodal performance land-scapes and differentiability of the performance function, and trapping in local minima ( Ursem and Vadstrup, 2004 ). Also, if the searching space is undifferentiable or parameter span is non-linear, traditional recurrent methods cannot gain the global optimization ( Evsukoff et al., 2004; Montiel et al., 2004; Montiel et al., 2003; Juang et al., 2003; Goldkberg, 1989 ).

Heuristic algorithms especially with stochastic search techni-ques seem to be a more hopeful approach and provide a powerful means to solve this problem. These algorithms seem to be a promising alternative to traditional techniques, since they do not rely on any assumptions such as differentiability or continuity. In fact heuristic algorithms depend only on the objective function to guide the search. Because of this, Genetic Algorithm (GA) was 2008; Wang and Gu, 2007; Chang, 2007; Dai et al., 2002;
Beligiannis et al., 2005 ). Although the GA is efficient to find the global minimum of the search space, it consumes too much search time which is not proper for online identification. The Particle
Swarm Optimization (PSO) algorithm is an alternative. The main advantages of PSO are the simple concept, easy implementation and quick convergence. Due to this, recently PSO has attracted much attention and wide applications in various fields ( Chang and
Ko, 2009; Lin et al., 2008 ). The PSO algorithm is motivated by the behavior of organisms, such as fish schooling and bird flocking. It is characterized as a simple concept, which is both easy to implement and computationally efficient. Unlike other heuristic techniques, PSO has a flexible and well-balanced mechanism to enhance the global and local exploration abilities ( Abido, 2002 ). Based on this, this paper presents a novel PSO, namely Adaptive Particle Swarm Optimization (APSO), by introducing an adaptive inertia weight to rationally balance the global exploration and local exploitation abilities for basic PSO. An illustrative example for the modeling of bilinear systems is provided to confirm the validity, as compared with the GA and Linearly Decreasing Inertia Weight PSO (LDW-PSO), Nonlinear Inertia Weight PSO (NDW-PSO) and Dynamic Inertia Weight PSO (DIW-PSO) in terms of parameter accuracy and convergence speed. APSO is also improved to detect and determine the variation of parameters. In this case, a sentry particle is introduced to detect any changes in system parameters. If any change in parameters occurs, the sentry alerts the swarm to reset their best location memories and then the algorithm runs further to find the new optimum values. The performance of the proposed algorithm is demonstrated through identifying the parameters of a time varying bilinear system. Simulation results show that the proposed algorithm is a good promising particle swarm optimization algorithm for online parameter estimation.

The rest of paper is organized as follows: Next section describes a general form of problem formulation. Section 3 introduces the proposed APSO. In Section 4, the implementation of the proposed algorithm in online system parameter estimation is introduced. Sections 5 and 6 contain simulation results and conclusions, respectively. 2. Problem formulation
This paper considers the discrete bilinear system parameter estimation. The relationship between the input sequence { u ( n )}and output sequence { y ( n )}of such system is given by y  X  n  X  X  a  X  n  X  X  where p is the system order and P p bilinear term. Parameters which are needed to be estimated are a b and c ij .

The basic idea of parameter estimation is to compare the system responses with the parameterized model based on a performance function giving a measure of how well the model response fits the system response. Fig. 1 shows that the excitation input is given to both the real and the estimated systems. Then, the outputs are given as inputs to the fitness evaluator, where the fitness will be calculated. The sum of squared error for a number of samples is considered as  X  X  X itness of estimated model X  X  defined by SSE  X  respectively and N is the number of given samples. The calculated fitness is then input to the identifier algorithm to identify the best parameters for estimated system in fitting procedure by minimizing the sum of squared errors in response to excitation input. 3. The proposed APSO
Unlike population based evolutionary algorithms, PSO is motivated by the simulation of social behavior and each candidate solution is associated with a velocity. The candidate solutions, called  X  X  X articles X  X  then  X  X  X ly X  X  through the search space.
In the beginning, a population with the size of particles is created. Then, the velocity of every particle is constantly adjusted according to corresponding particle X  X  experience and particle X  X  companions X  experiences. It is expected that the particles will move towards better solution areas. The fitness of every particle can be evaluated according to the objective function of optimiza-tion problem. At each iteration, the velocity of every particle will be calculated as follows: v  X  o v t i  X  c 1 r 1  X  pbest t i x t i  X  X  c 2 r 2  X  gbest t the best previous position of this particle (memorized by every particle), gbest t is the best previous position among all the particles in t th iteration (memorized in a common repository), o is the inertia weight, c 1 and c 2 are acceleration coefficients and are known as the cognitive and social parameters respectively. Finally, r 1 and r 2 are two random numbers in the range [0, 1].
After calculating the velocity, the new position of every particle can be worked out x
The PSO algorithm performs repeated applications of the update equations above until the pre-specified number of generations G is reached.

Although PSO has shown some important advances by providing high speed of convergence in specific problems, it does exhibit some shortages. It found that PSO has a poor ability to search at a fine grain because it lacks velocity control mechanism ( Angeline, 1998 ). Many approaches are attempted to improve the performance of PSO by variable inertia weight. The inertia weight is critical for the performance of PSO, which balances global exploration and local exploitation abilities of the swarm. A big inertia weight facilitates exploration, but it makes the particle long time to converge. Conversely, a small inertia weight makes the particle fast converge, but it sometimes leads to local optimal.
Hence the linearly and nonlinearly decreasing inertia weight are proposed in the literature ( Chang and Ko, 2009; Jiao et al., 2008; Yang et al., 2007; Chatterjee et al., 2006; Kennedy et al., 2001;
Ratnaweera et al., 2004; Shi and Eberhart, 1998a,b ). In the following, the three well-known mechanisms from above men-tioned PSO algorithms are described that are used for comparison with the proposed PSO in the problem in hand. In the first one namely LDW-PSO, the inertia weight is adapted linearly as follows ( Shi and Eberhart, 1998a,b ): o t  X  where iter max is the maximal number of iterations, t is the current number of iterations. So as iterations go, o decreases linearly from o max to o min . In the second one namely NDW-PSO, the inertia weight is adapted nonlinearly as follows (Chatterjee et al., 2006): o t  X  o min  X  X  iter max iter iter where n is the nonlinear modulation index. In the last one namely
DIW-PSO, the inertia weight is adapted dynamically as follows ( Jiao et al.; 2008 ): where o init A (0,1] is the initial inertia weight and u A [1.0001,1.005].

PSO, they cannot truly reflect the actual search process without any feedback taken from how far particle X  X  fitness are from the estimated (or real) optimal value, when the real optimal value is known in advance. Actually, for the particle which its fitness is far away from the real optimal value, a big velocity is still needed to globally search the solution space and thus its inertia weight must set to larger values. Conversely, only a small movement is needed and so inertia weight must set to a small value to facilitate finer local explorations. Furthermore, introducing the same inertia weight for all particles, by ignoring the differences among particles performances simulated a roughly animal background, not a more precise biological model. In fact, during the search every particle dynamically changes its position, so every particle locates in a complex environment and faces different situation.
Therefore, every particle may have different trade off between global and local search abilities.
 weight is dynamically adapted for every particle by considering a measure called Adjacency Index (AI), which characterizes the nearness of individual fitness to the real optimal solution. Based on this index, every particle could decide how to adjust the values of inertia weight. For this purpose, the velocity updating rules in the proposed APSO is given by two different characteristics: (1) To incorporate the difference between particles into PSO, so (2) To truly reflect the actual search process, the inertia weight is
Definition 1. The Adjacency Index (AI) for every particle in t th iteration is defined as follows:
AI t i  X  F  X  pbest 1 i  X  F KN F  X  pbest t particle and F KN is the known real optimal solution value. of i th particle is far away from the real optimal value and it needs a strong global exploration therefore, a large inertia weight. On the other hand, a big AI i means that i th particle has a high adjacency to the real optimum and so it needs a strong local exploitation, therefore a small inertia weight. Hence, the value of inertia weight for every particle in t th iteration is dynamically calculated with the following transform function.

Remark 1. Note that in the most of engineering optimization problem such as system identification considered in this paper, the optimal value F KN is known in advance. However, if the optimal value is unknown, the Eq. (9) can be modified as
AI
In this case, for each particle the Adjacency Index changes according to the rate of its personal best fitness improvement.
Definition 2. The transfer function is o t  X  where a is a positive constant in the range (0,1]. Under the assumption and definitions above, it can be concluded that 0.5 r o i o 1.

Fig. 2 shows the change of inertia weight o with respect to AI with different values of a . The parameter a controls the decreasing speed of inertia weight. In order to observe the impact of a on the performance of APSO, parameter a is varied from 0.1 to 1 with step size 0.1.

According to Eqs. (9) and (11), during the search, the particles face different finesses; as a result they get different values of AI and then inertia weight. While the fitness of a particle is far away from the real global optimal, AI for this particle has a small value (a low adjacency) and the value of inertia weight will be large resulting strong global search abilities and locate the promising search areas. Meanwhile, the fitness of a particle achieves near the real global optimal, AI for this particle has a big value (a high adjacency) and inertia weight will be set small, depending on the local explorations and so accelerate convergence. 4. Implementation of APSO
In this section, the procedure of APSO in online system parameter identification is described. In this case, each particle represents all parameters of estimated model. The proposed algorithm sequentially gives a data set by sampling periodically.
While starting, in the first period, the best system parameter is found by minimizing the SSE introduced in Eq. (2). In this case, the simulation for next period does not begin until the fitness of global best becomes lower than a predefined threshold. After that, the estimated parameters will not be updated unless a change in the system parameters is detected. In order to detect any change in system parameters, the global optimum in the later period is noticed as a sentry particle. In the beginning of each of the next periods, the sentry reevaluates its fitness and if the fitness changes significantly or it becomes bigger than a predefined threshold, the changes in parameters are confirmed. If no changes are detected, the algorithm leaves this period without changing the positions of particles. In contrast, when any change in parameters occurs, the sentry alerts the swarm to reset their best location memories and then the algorithm runs further to find the new optimum values. For this purpose, the fitness of global optimum particle and personal bests of all particles are evaporated at the rate of a big evaporation constant. As a result, other particles have a chance to find better solutions than those stored on their pervious global and personal memories. Moreover, the velocities of particles are increased to search in a bigger solution space for new optimal solution.

Generally speaking, when a change in system parameters is detected the following changes in algorithm are done and then the proposed algorithm runs to identify the new optimal parameters: fitness  X  pbest i  X  X  fitness  X  pbest i  X  Ti  X  1 , ::: , S  X  12  X  fitness  X  gbest  X  X  fitness  X  gbest  X  T  X  13  X  V  X  V  X  b V max  X  14  X  where T is a big evaporation constant and b is a random variable between 0 and 1. The procedure for this algorithm is summarized as follows: Step 1. Give a data set in the current period.

Step 2. If it is the first period, initialize positions and velocities of a group of particles, then go to Step 4.

Step 3. Evaluate the fitness of the sentry particle. If any change in system parameters is detected by the sentry particle, reset the best location memories and velocities of particles using Eqs. (11) X (14). Else go to Step 1.
 Step 4. Evaluate fitness of each particle using Eq. (2).
Step 5. If the new position of i th particle is better than Pbest set Pbest i as the new position of the i th particle. If the fitness of best position of all new particles is better than fitness of gbest , then gbest is updated and stored.
 Step 6. Calculate the inertia weight using Eq. (11).

Step 7. Update the position and velocity of each particle according to the Eqs. (4) and (8).

Step 8. If the global optimum fitness is lower than a redefined threshold, output the global optimum and label it as the sentry particle, then go to the step 1. Else go to Step 4. 5. Simulation results
This section demonstrates the feasibility of the APSO-based parameter system identification. The results are compared to those obtained by LDW-PSO, NDW-PSO, DIW-PSO and GA. In all PSO algorithms, c 1  X  c 2  X  2( Kennedy and Eberhart, 1995 ). In LDW-PSO and NDW-PSO, o decreases from 0.9 to 0.4. Moreover, in NDW-PSO n is set to 1.2 ( Chatterjee et al., 2006 ). In DIW-PSO, u is set to 1.0002 ( Jiao et al., 2008 ). In APSO, the parameter o is determined using Eq. (11). In addition, in GA, the crossover probability P c and the mutation probability P m are set to 0.8 and 0.1, respectively. To perform fair comparison, the same computational effort is used in GA, LDW-PSO, NDW-PSO, DIW-PSO and APSO. That is, the maximum generation, population size and searching range of the parameters in GA are the same as those in LDW-PSO, NDW-PSO, DIW-PSO and APSO.
 In order to observe the impact of a on the performance of
APSO, different values of a , 20 particles, 200 and 400 maximum iterations for Examples 1 and 2, respectively, were conducted on parameter estimation of the following examples. For each experimental setting, 20 runs of the algorithm were performed. Table 1 listed the mean best fitness values averaged over 20 runs.
It is clear that the values in range [0.2, 0.8] for a can all lead to acceptable performance. In present paper, a is set to 0.5.
Simulation results have been carried out in two cases. In the first case, the proposed algorithm has been compared with GA,
LDW-PSO, NDW-PSO and DIW-PSO in offline parameter identifi-cation in terms of convergence speed and accuracy using two examples described by ( Wang and Gu, 2007 ). In the second case, the proposed APSO is applied to online parameter identification of Example 1 for its performance validation.

Example 1. A bilinear system is given as follows: y  X  k  X  1  X  X  ay  X  k 1  X  X  bu  X  k  X  cy  X  k  X  u  X  k  X  X  15  X  where a  X  0.898, b  X  0.28 and c  X  0.106.

Example 2. A MIMO bilinear system was given as follows: x  X  k  X  1  X  x  X  k  X  1  X  "# where
A  X 
D  X  5.1. Case 1: Offline In these examples, for the APSO, the known optimal value F is zero. In Example 1, the optimization process is repeated 20 times independently. The average and Standard deviation (denoted by Std) of results using GA ( Wang and Gu, 2007 ), LDW-
PSO, NDW-PSO, DIW-PSO and APSO are listed in Table 2 . Figs. 3 X 5 depict the great success of optimization process by using APSO in compared with DIW-PSO for the identified parameters a , b and c , respectively. The aim is to avoid confusion between the results of APSO and other algorithms, so only the result of DIW-PSO which is the best result obtained among other algorithms in terms of convergence speed are compared, excluding the rest.
Moreover, the convergence of the optimal SSE at each generation is plotted for all algorithms in Fig. 6 . Simulations results confirm the superiority of APSO algorithm in terms of accuracy and convergence speed without the premature convergence problem. In Example 2, the average and Std of results using GA ( Wang and Gu, 2007 ), LDW-PSO, NDW-PSO, DIW-PSO and
APSO are listed in Tables 3 and 4 . It is obvious that the results of the proposed APSO are good promising as compared with other algorithms. In addition, Figs. 7 X 10 illustrate three instance system parameters including A(1,1), A(1,2), B(1,1) obtained by APSO and DIW-PSO, respectively and the convergence trajectories of the optimal SSE at each generation for all algorithms. It again confirms the superiority of APSO algorithm in terms of convergence speed without the premature convergence problem. 5.2. Case 2: Online In this case, the variation of system parameters is considered.
This study is necessary to justify tracking the changes of system parameters using the proposed algorithm. If a change in the model parameter is detected by sentry particle, the APSO continues to run. In this moment, the simulation for next period does not begin until the fitness of global best becomes lower than a threshold of 10 5 in this period. After finding optimal parameters in this period, there will be no APSO iteration unless another change detects in system parameter. Based on this, a sudden change is applied for the system parameters. Notice that the slower parameters of time-varying change, the smaller choice threshold is to have better result.

The separated parts (I), (II) and (III) in Figs. 11 X 13 illustrate how the proposed algorithm tracks variation of system parameters in
Example 1. In part (I), the original parameters which are used in offline section are given. In part (II), parameter a is changed from 0.898 to 1, while b and c are kept unchanged. Finally, in part (III), parameter a is kept unchanged whereas parameters b and c are simultaneously varied from 0.106 to 0.12 and from 0.284 to 0.3, respectively. Considering Figs. 5 X 7 ,itisobviousthattheproposed algorithm can track any change in parameters. The dashed lines in these figures signify the moment that the sentry particle has detected some change in system parameters. Moreover, it can be reach on the previous values when simulation is terminated at the end of each part. Table 5 illustrates the results obtained by APSO in online identification. The results indicate that APSO successfully identify parameters variations.
 6. Conclusion
Online estimation of systems with unknown and varying parameter is a challenging problem. The difficulties of online implementation mainly come from the unavoidable computa-tional time to find a solution. This paper presented a novel nature-inspired optimization technique, namely APSO to solve this problem. An adaptive inertia weight mechanism was proposed in APSO to increase the convergence speed and accuracy of the basic PSO to save tremendous computation time. APSO was also improved to detect and determine varying parameters. Although the feasibility of APSO was shown for both offline and online identification of bilinear systems, it does not have any restriction to other systems. The future work is to apply APSO for both system identification and control in an adaptive manner. References
