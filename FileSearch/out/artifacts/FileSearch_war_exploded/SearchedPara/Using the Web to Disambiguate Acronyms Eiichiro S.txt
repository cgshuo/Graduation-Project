 Acronyms are short forms of multiword expres-sions (we call them definitions ) that are very con-venient and commonly used, and are constantly invented independently everywhere. What each one stands for, however, is often ambiguous. For example,  X  X CL X  has many different definitions, including  X  X nterior Cruciate Ligament (an in-jury), X   X  X ccess Control List (a concept in com-Computational Linguistics (an academic society). X  People tend to write acronyms without their defini-tion added nearby ( Table 1 ), because acronyms are used to avoid the need to type long expressions. 
Consequently, there is a strong need to disambigu-ate acronyms in order to correctly analyze or re-trieve text. It is crucial to recognize the correct acronym definition in information retrieval such as a blog search. Moreover, we need to know the meaning of an acronym to translate it correctly. To the best of our knowledge, no other studies have approached this problem. 
Figure 1 Acronyms and their definitions co-occur in some pages of the Web 
On the other side of the coin, an acronym should be defined in its neighborhood. For instance, one may find pages that include a certain acronym and its definition ( Figure 1 ). 
First, our proposed method obtains Web pages that include both an acronym and its definitions . 
Second, the method feeds them to the machine learner, and the classification program can deter-mine the correct definition according to the context information around the acronym in question. nitions for an acronym is given from sources ex-ternal to this work. Listing pairs of acronyms and their original definitions, on which many studies have been done, such as Nadeau and Turney (2005), results in high performance. Some sites such as http://www.acronymsearch.com/ or http://www.findacronym.com/ provide us with this function. explains our solution to the problem, and Section 3 reports experimental results. In Sections 4 and 5 we follow with some discussions and related works, and the paper concludes in Section 6. The idea behind this proposal is based on the ob-servation that an acronym often co-occurs with its definition within a single Web page ( Figure 1 ). For example, the acronym ACL co-occurs with one of its definitions,  X  X ssociation for Computa-tional Linguistics, X  211,000 times according to google.com. biguation (Pedersen and Mihalcea, 2005). The hit pages can provide us with training data for disam-biguating the acronym in question, and the snip-pets in the pages are fed into the learner of a classifier. Features used in classification will be explained in the latter half of this subsection. learning; any state-of-the-art method will suffice. In this paper we employed the decision-tree learn-ing program provided in the WEKA project. Collecting the training data from the Web Our input is the acronym in question, A, and the set of its definitions, {D k | k=1~K}. In the experiment, L is set to 1,000 . Thus, we have for each definition D k of A, at most 1,000 training data. Training the classifier From training data T l (A), we create feature vec-tors, which are fed into the learner of the decision tree with correct definition D k for the acronym A. A W 1 W 2 ... W m-1 W m , where m is from 2 to M, which is called the window size hereafter. snippet as features, which are binary, i.e., if the keyword exists in T l (A), then it is true. Otherwise, it is null. top N frequent words 1 , but for A in the bag con-sisting of all words in {T l (A)}. For example, key-words for  X  X CL X  are  X  Air, Control, and, Advanced, Agents, MS, Computational, Akumiitti, Cruciate, org, of, CMOS, Language, BOS, Agent, gt, HTML, Meeting, with, html, Linguistics, List, Active, EOS, USA, is, access, Adobe, ACL, ACM, BETA, Manager, list, Proceedings, In, A, League, knee, Anterior, ligament, injuries, reconstruction, injury, on, The, tears, tear, control, as, a, Injury, lt, for, Annual, Association, Access, An, that, this, may, an, you, quot, in, the, one, can, This, by, or, be, to, Logic, 39, are, has, 1, from, middot . X  3.1 Acronym and definition preparation We downloaded a list of acronyms in capital let-ters only from Wikipedia and filtered them by eliminating acronyms shorter than three letters. Then we obtained definitions for each acronym from http://www.acronymsearch.com/ and dis-carded acronyms that have less than five defini-tions. Finally, we randomly selected 20 acronyms. biguity is more than or equal to five. For each ac-ronym A, a list of definitions { D k | k=1~K K&gt;=5 }, whose elements are ordered by the count of page including A and D k , is used for the ex-periment. 3.2 Ambiguity and accuracy Here we examine the relationship between the degree of ambiguity and classification accuracy by using a cross-validation test for the training data. #Class M=2 M=5 M=10 Base 2 88.7% 90.1% 92.4% 82.3% #Class M=2 M=5 M=10 Base 5 78.6% 82.6% 86.0% 76.5% Ambiguity of two The first experiment was performed with the se-lected twenty acronyms by limiting the top two most frequent definitions. Table 2 summarizes the ten-fold cross validation. While the accuracy changes acronym by acronym, the average is high about 90% of the time. The M in the table denotes the window size, and the longer the window, the higher the accuracy. racy of the baseline method that always picks the most frequent definition. The proposed method achieves better accuracy than the baseline. Ambiguity of five Next, we move on to the ambiguity of five ( Table 3 ). As expected, the performance is poorer than the abovementioned case, though it is still high, i.e., the average is about 80%. Other than this, our observations were similar to those for the ambigu-ity of two. 4.1 Problem caused by biased distribution For some words, the baseline is more accurate than the proposed method because the baseline method reaches all occurrences on the Web thanks to the search engine, whereas our method limits the number of training data by L as mentioned in Section 2. The average quantity of training data was about 830 due to the limit of L, 1,000. The distribution of these training data is rather flat. This causes our classifier to fail in some cases. For example, for the acronym  X  X SP, X  the most fre-quent definition out of five has a share of 99.9% ( Table 4 ) on the Web, whereas the distribution in the training data is different from the sharp distri-bution. Thus, our classification accuracy is not as good as that of the baseline. quent out of five definitions has the much smaller share of 26.3% on the Web ( Table 5 ), whereas the distribution in the training data is similar to the flat distribution of real data. Furthermore, the de-cision tree learns the classification well, whereas the baseline method performs terribly. acronyms, our proposed method is beaten by the baseline method. The slanting line in Figure 2 shows the baseline performance compared with our proposed method. In the case where our method is strong, the gain is large, and where our method is weak, the reduction is relatively small. The average performance of our proposed method is higher than that of the baseline. would be to incorporate prior probability into the learning process. 4.2 Possible dissimilarity of training and real The training data used in the above experiment were only the type of snippets that contain acro-nyms and their definitions ; there is no guarantee for documents that contain only acronyms are similar to the training data. Therefore, learning is not necessarily successful for real data. However, we tested our algorithm for a similar problem in-troduced in Section 5.1, where we conducted an open test and found a promising result, suggesting that the above-mentioned fear is groundless. 5.1 Reading proper names The contribution of this paper is to propose a method to use Web pages for a disambiguation task. The method is applicable to different prob-lems such as reading Japanese proper names (Sumita and Sugaya, 2006). Using a Web page containing a name and its syllabary, it is possible to learn how to read proper names with multiple readings in a similar way. The accuracy in our experiment was around 90% for open data. 5.2 The Web as a corpus Recently, the Web has been used as a corpus in the NLP community, where mainly counts of hit pages have been exploited (Kilgarriff and Grefen-stette, 2003). However, our proposal, Web-Based Language Modeling (Sarikaya, 2005), and Boot-strapping Large Sense-Tagged corpora (Mihalcea, 2002) use the content within the hit pages. This paper proposed an automatic method of dis-ambiguating an acronym with multiple definitions, considering the context. First, the method obtains the Web pages that include both the acronym and its definitions. Second, the method feeds them to the learner for classification. Cross-validation test results obtained to date indicate that the accuracy of obtaining the most appropriate definition for an acronym is around 92% for two ambiguous defini-tions and around 86% for five ambiguous defini-tions. 
