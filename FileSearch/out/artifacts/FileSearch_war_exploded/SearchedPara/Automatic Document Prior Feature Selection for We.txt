 Document prior features, such as Pagerank and URL depth, can improve the retrieval effectiveness of Web Information Retrieval (IR) systems. However, not all queries equally benefit from the application of a document prior feature. This paper aims to investigate whether the retrieval perfor-mance can be further enhanced by selecting the best docu-ment prior feature on a per-query basis. We present a novel method for selecting the best document prior feature on a per-query basis. We evaluate our technique on the TREC .GOV Web test collection and its associated TREC 2003 Web search tasks. Our experiments demonstrate the effec-tiveness and robustness of our proposed selection method. Categories and Subject Descriptors: H.3.3 [Informa-tion Storage &amp; Retrieval]: Information Search &amp; Retrieval General Terms: Performance, Experimentation Keywords: Prior Feature Selection
Several previous studies have shown that integrating doc-ument prior features, such as Pagerank or URL depth, into a document weighting scheme can improve the retrieval per-formance of a Web IR system [1, 2]. However, not all queries benefit equally from the application of a given feature.
In this paper, we present a novel method for selecting the best document prior feature on a per-query basis. For a given query and its corresponding top retrieved documents, we propose to estimate the divergence between the retrieved document scores distribution prior to, and after the integra-tion of the document prior feature. We then observe that the divergence distribution can be fitted by a Gaussian dis-tribution. Based on this observation, we use a Bayesian decision mechanism to decide which document prior feature is the best for a given query. In this paper, we firstly assess whether it is indeed important to apply the best document prior feature on a per-query basis; secondly, we examine the effectiveness of our proposed document prior feature selec-tion method.
The distribution of retrieval scores has been studied to predict the effectiveness of search engines [6]. Instead, we use the divergence of retrieval scores to predict when a doc-ument prior feature should be applied. There are several different ways to estimate the divergence between the doc-ument scores distribution prior to, and after the integra-tion of the document prior feature. For example, Kullback-Leibler divergence [3] and Jensen-Shannon divergence [4]. Jensen-Shannon divergence has an upper bound (  X  1) while Kullback-Leibler does not. To limit the problem of sparse-ness, we use the Jensen-Shannon divergence: where for the top retrieved documents of a given query, X = { x i } , Y = { y i } and x i and y i are the relevance score of document i prior to, and after the integration of a given document prior feature, respectively. It is easy to verify that JS ( X, Y ) 6 = JS ( Y, X ). Following [7], we use the symmetric Jensen-Shannon divergence, resulting in divergence scores that are in the range (0, 2]: We examine the distribution of the divergence of retrieval scores prior to, and after the integration of the document prior feature, measured using Equation (2). As an example, for the TREC 2003 Web Track mixed task dataset, Fig-ure 1 shows a histogram of divergence scores distribution for Pagerank for those queries for which Pagerank led to a better retrieval performance than without the integration of Pagerank. The plot also shows a maximum-likelihood fit using a Gaussian distribution, suggesting that the diver-gence scores can be fitted using a Gaussian distribution. The maximum-likelihood fit involves the setting of the mean and variance, which needs to be set using training data. Note that we would obtain the same observation, i.e. a Gaussian fit, if all the queries of the TREC 2003 Web Track mixed task dataset were plotted.

Assume that we have two document prior features f 1 and f the most effective document prior feature. For this purpose, we describe the Bayesian decision mechanism, which will be used as our document prior feature selection decision mech-anism. For a given query, the probability of feature f j being the most effective document prior feature with a given di-vergence Score is defined as follows: pand to select the most effective document prior feature amongst n document prior features, where n &gt; 2. where P ( f j ) is the prior probability of feature f j being the most effective document prior feature for a given query. Us-ing a training set, it can be computed as the number of queries for which the application of f j led to the most ef-fective retrieval performance divided by the total number of queries; P ( Score | f j ) is the probability of obtaining diver-gence Score when the most effective document prior feature is f j . As we mentioned above, this distribution can be fitted by a Gaussian distribution, given as follows: where  X  and  X  are the mean and variance of the Guassian distribution, set using training. We do a simple normali-sation on P ( Score | f j ) as different document prior features have different divergence distributions, which might result in different  X  and different range of P ( Score | f j ). We normalise P ( Score | f j ) as follows: where  X  is the parameter that controls the range of P ( Score | f P ( Score ) = P n j =1 P ( f j ) P ( Score | f j ); n is the number of document prior features involved in the feature selection; P ( Score ) can be ignored as it does not affect the final deci-sion making.
 Table 1: MAP on test dataset. Values in paren-thesis denote percentage improvement over base-line. The best runs in each column are highlighted in bold. Values statistically different from the best in column are denoted  X  (Wilcoxon Matched-Pairs Signed-Ranks Test, p &lt; 0 . 05 ).
We use the standard .GOV Web test collection, and its corresponding TREC 2003 Web track mixed topics and rel-evance assessments. As a training dataset, we sample 80% of the TREC 2003 Web Track mixed topics task: 120 home-page finding topics, 120 named page finding topics, and 40 topic distillation topics, randomly chosen from the 350 topics available in this task. Our test dataset is the remaining 20% of the TREC 2003 Web track mixed topics task (30 home-page finding topics, 30 named page finding topics and 10 topic distillation topics). We repeat this sampling 3 times. The evaluation measure used in all our experiments is mean average precision (MAP). The range of i in Equation (1) is (0,1000] as TREC usually requires 1000 retrieved documents for each query.

For indexing and retrieval, we use the Terrier IR plat-to boost early precision, we apply a light version of Porter X  X  stemming algorithm for English. We index the body, an-chor text and titles of documents as separate fields and use the PL2F field-based Divergence From Randomness (DFR) weighting model [5] as a baseline retrieval system. The pa-rameters of the PL2F document weighting model are set by http://ir.dcs.gla.ac.uk/terrier Figure 1: Histogram of divergence scores distribu-tion split in equal divergence score ranges and the Gaussian fit of pagerank for TREC 2003 Web Track. optimising MAP on the training dataset, using a simulated annealing procedure.
 We experiment with two document prior features, namely Pagerank (PR) and URL depth (URL) [1]. Firstly, we assess the maximum performance that could be achieved by man-ually choosing the optimal document prior feature for each query. From Table 1, we can see that the manual document prior feature selection M ( P R, U RL ) can lead to a signifi-cant improvement over the PL2F baseline as well as systems where a given document prior feature (e.g. Pagerank) has been applied uniformly to all queries. This suggests that a document prior feature selection on a per-query basis can significantly enhance the retrieval performance of a Web IR system.

Our automatic feature selection method S ( P R, U RL ) also leads to similar marked improvements over the PL2F base-line as well as the uniform application of a given docu-ment prior feature, such as Pagerank or URL depth. Its overall performance, while naturally lower than the man-ual M ( P R, U RL ) method, is still fairly comparable. Note that the above two results are consistent across all our three random samplings, suggesting that our proposed feature se-lection method is robust.
We investigated the retrieval performance achieved with document prior feature selection on a per-query basis on the TREC 2003 Web Track, using a standard TREC Web test collection. We showed that the appropriate selection of a document prior feature selection on a per-query basis can significantly enhance the retrieval performance. More-over, we observed that our proposed automatic document prior feature selection method consistently and markedly increases the retrieval performance over baselines that only use a single type of document prior feature uniformly on all queries. In the future, we plan to apply a more thorough cross-validation, to assess the robustness of our method.
