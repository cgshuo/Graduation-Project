 We present an e ective multifaceted system for exploratory analysis of highly heterogeneous document collections. Our system is based on intelligently tagging individual docu-ments in a purely automated fashion and exploiting these tags in a powerful faceted browsing framework. Tagging strategies employed include both unsupervised and super-vised approaches based on machine learning and natural language processing. As one of our key tagging strategies, we introduce the KERA algorithm ( K eyword E xtraction for R eports and A rticles). KERA extracts topic-representative terms from individual documents in a purely unsupervised fashion and is revealed to be signi cantly more e ective than state-of-the-art methods. Finally, we evaluate our system in its ability to help users locate documents pertaining to military critical technologies buried deep in a large hetero-geneous sea of information.
 I.2.7 [ Arti cial Intelligence ]: Natural Language Process-ing| Text Analysis ; H.3.3 [ Information Storage and Re-trieval ]: Information Search and Retrieval; H.5.2 [ Information Interfaces and Presentation ]: User Interfaces| Natural Language Algorithms; Experimentation; Human Factors faceted navigation, faceted browsing, tag clouds, machine learning, keyphrase extraction, topic modeling
Given a large and diverse collection of unstructured text documents, how does one (1) characterize the subject ar-eas present and (2) use these discovered subject areas to eciently navigate the collection to locate critical informa-tion? Many previous works have investigated such questions within speci c domains such as microblog posts ( e.g., char-acterizing tweets [10]), but comparatively less attention has been paid to investigating more general and diverse con-texts. Unfortunately, in practice, approaches that may work well for domains consisting exclusively of a single document type ( e.g., tweets, emails, or scienti c articles) do not al-ways translate easily or directly to other more heterogeneous and \messy" document collections. In this work, we present a tag-based system in which tags ( i.e., terms or character strings automatically assigned to individual documents) are exploited to eciently characterize and explore document collections. Document collections of interest in our work exhibit a high degree of diversity ( e.g., arbitrary les re-siding on a high-capacity laptop drive or le server). The U.S. federal government, for instance, is often presented with the challenge of what essentially is exploratory anal-yses of highly heterogeneous document collections. These collections requiring analyses are nowhere near as homoge-neous as tweets, news stories, patents, or scienti c abstracts | the typical objects of study in text analytics research ( e.g., [2, 8, 21, 22, 26]). To better illustrate this point, we brie y describe three motivating examples. 1. Digital Investigations: In 2012, it was reported that 2. Intelligence Analysis: Intelligence Analysis gener-3 . Appraisal of Electronic Records: The National All three of the above examples necessitate the need for an ecient and intelligent way to explore heterogeneous, large-scale document collections for critical information of inter-est. In the present work, we explore the task of character-izing, browsing, and searching large collections of unstruc-tured text documents using faceted navigation . We present a system that e ectively discovers and exploits the use of in-formation facets to eciently characterize and search large document collections.
Information facets (or simply facets ) are classes of at-tributes describing objects in an information repository. They are used to facilitate searches, ltering, and navigation of the information by di erent dimensions of the data [20,24]. Faceted classi cation systems were rst conceived in the 1930s by S.R. Ranganathan, an Indian librarian considered to be the father of library science [20]. Today, faceted search is used extensively in information retrieval systems ( e.g., [24]). Electronic commerce sites, for instance, employ facets to fa-cilitate browsing products along various dimensions ( e.g., brand, price). The vast majority of faceted search systems popu-late the facet attributes from pre-existing elds in a data repository. One example is Twitter's use of hashtags , which are user-generated topic tags assigned to tweets [10]. An-other such example is an author or title eld in a publi-cation database. Unfortunately, for most large document collections, these manually-generated tags typically do not exist. For these cases, the attributes used to populate facets must be mined or discovered .

What facets should be used for unstructured text and how might we populate them in an automated fashion ( i.e., dis-cover them)? In the context of a physical library system, Ranganathan proposed classifying information according to ve, manually-populated facet categories he referred to as Personality, Matter, Energy, Space, and Time (PMEST) [20]. Motivated by the PMEST model and today's surge in electronic records, we propose organizing unstructured digital text collections by the following general set of dis-coverable facet categories. 2 Topic Facets. Topic Facets relate to the overall subject or \aboutness"of an electronic document. 3 In the present work, our focus is on the automated discovery of topic tags . Topic tags are key terms that capture or represent the overall topic of the document. Such tags can be used to characterize and navigate document collections and re ne search results. The
F aceted classi cation systems, in the context of computer systems, are also referred to as faceted search , faceted navi-gation , or faceted browsing .
By \discoverable", we mean that the attributes for these facets are populated in an automated fashion.
Sentiment analysis might also be included here. problem of discovering topics (and topic tags) from text col-lections has been extensively studied, of course. In Section 3, we discuss multiple approaches to building Topic Facets . Mention Facets. Documents often make mention of en-tities, relations, or events that are of importance despite being unrelated to the overall topic of the document. For instance, Personally Identi able Information (PII) such as social security numbers are often important to detect, yet are not necessarily representative of the subject of a given document. The same is true of username mentions in tweets. Mention Facets are populated by extracting such entities (or relations) from text and can be used to help discover and lo-cate information of interest within a document collection. Format Facets. The Format Facet allows navigation of document collections by le type and format. For homoge-neous text collections like tweets, the Format Facet is rather uninteresting, as all tweets have the same format ( e.g., plain text and up to 140 characters) . In our work, however, the document sets from which topic tags and entity mentions are extracted are highly diverse in both type and format. Col-lections can include technical reports, news articles, Power-point briefs, Excel les, Web pages, programming language source code, emails, and many other les. In practice, the availability of a Format Facet is especially important for helping to hone in on particular information elements of in-terest. For instance, di erent le types often cover di erent sets of topic areas and entities. Moreover, the le type and format a ect the way in which topics, entities, and terms of interest are extracted ( e.g., a scienti c report in PDF format vs. a .dat le containing Web search history).
 Location Facets. The meaning and implementation of the Location Facet is subject to interpretation and choice de-pending on the application. For documents residing on a le server or workstation, the Location Facet may comprise le paths or folders of documents. For tweets, on the other hand, it would make most sense to use the geolocation in-formation of the tweet for this facet.
 Time Facets. The way in which the Time Facet is im-plemented will also depend on the application. For tweets, it might be the time the tweet was posted, whereas an ex-tracted publication date would be of most interest for sci-enti c articles. For arbitrary les residing on a le server or laptop drive, it is sometimes useful to utilize the created or last-modi ed date for the Time Facet (which must be ex-tracted from the le metadata).
 Author Facets. As with the Location and Time facets, use of the Author facet will vary by application domain. In the case of news articles, the author might be extracted from the document content. For tweets, it will simply be the Twitter user account producing the tweet. For documents produced by Microsoft Oce applications ( e.g., Microsoft Word, Excel, and Powerpoint), the author might be taken as the Last-Author eld extracted from document metadata.
Throughout the remainder of this paper, we describe a concrete implementation of our proposed faceted classi ca-tion system. First, we summarize our contributions.
In this work, we explore the general problem of character-izing, navigating, and searching large collections of diverse documents. Our contributions are as follows: We begin a discussion of our work by providing an overview of our implemented system.
Sponsors of our research were in need of a tool to ana-lyze arbitrary document collections in order to help analysts identify documents pertaining to military critical technolo-gies. The term \military critical" here is de ned by senior ocials and subject matter experts. (We will sometimes simply use the term \critical" when referring to such tech-nologies.) However, over time, they became interested in the general problem of characterizing, browsing, and searching through arbitrary and heterogeneous document collections (where the de nition of critical will vary by application and even user). The document collections of interest here con-tain diverse sets of le types and formats that typically exist on le server and workstation drives. Examples include PDF articles and reports, Microsoft Oce documents, plain text log les containing Web browser history, HTML documents, programming language source code les, and more. Our ap-plication is the end result of these objectives and interests. Figure 1 shows a screenshot of one of the main interfaces. On the surface, it appears to be a standard search engine interface where users can type ad hoc search queries and view search results. However, the standard search function-ality is enhanced (on the left in Figure 1) with numerous information facets based on the faceted classi cation sys-tem described in Section 1.1. The facets are populated by intelligently tagging each document in the collection along various dimensions. Documents can be viewed either in their original form or using a \Quick View" feature in which case the plain text is shown with highlighted terms ( e.g., discov-ered topic-representative keywords, search terms entered by user). Most (but not all) of the facets take the form of tag clouds. A tag cloud is a visualization of a set of words where the relative sizes of the words are determined by either fea-tures of the word or features of the entity represented by the word. In our work, the sizes of tags indicate the number of documents assigned the tag. Tag clouds are used both as a visualization and as an interface for faceted browsing of document collections, as the tags in the cloud can be used to lter and re ne search results. Each facet can be expanded or collapsed by clicking the + and symbols, as shown in Figure 1. The four facets viewable in Figure 1 ( e.g., Top Dis-covered Keywords , Topic Clusters ) all fall under the category of Topic Facet from our aforementioned faceted classi cation system. For the remaining facets, there is a one-to-one cor-respondence with the facet types described in Section 1.1. All facets (both those appearing in Figure 1 and not) are described at length later.
 Tag Clouds as Lenses. Figure 2 shows a sample tag cloud displaying topic-representative keywords discovered using KERA , our unsupervised algorithm for keyterm ex-traction. This tag cloud facet can be viewed as a \lens" into document collections. The remaining facets may be viewed as controls used to point, zoom, and focus this\lens"to areas of high interest in the corpus. 4 For instance, when ltering the search results by folder using a Location Facet , the tag cloud shown in Figure 2 will dynamically re-generate to dis-play the top discovered keywords of only the re ned search results ( i.e., documents residing in the folder selected). In this way, users can quickly \triage" noisy document collec-tions for information of interest (in some cases even before opening and reading documents ). Although tag clouds have come under criticism in the past, our tag-based system is demonstrated to be surprisingly e ective in locating critical information of interest buried deep within document collec-tions. The key to achieving this success is constructing in-formative clouds free from noise. Throughout later sections, we describe how precisely we accomplish this. But rst, we brie y describe the implementation of our system. Implementation Details. The underlying engine driving our application is the Solr search server, 5 which natively supports text extraction, full text search, and faceted navi-gation. Documents in the Solr index are tagged using a series of supervised and unsupervised data mining algorithms, and it is these tags that power faceted browsing and tag clouds. All data mining algorithms are developed using the Python language and libraries including scikit-learn, 6 NLTK, 7 and Gensim topic modeling toolkit. 8 Moreover, all algorithms are implemented to process documents in a stream using both online and parallel processing. The graphical user in-terface is implemented using Flask 9 and AJAX-Solr. 10 Fi-nally, communication between Python scripts and Solr is
I n actuality, each facet can play either the role of a \lens" or a \lens control." http://lucene.apache.org/solr/ http://scikit-learn.org/stable/ http://code.google.com/p/nltk/ http://radimrehurek.com/gensim/ http://flask.pocoo.org/ http://github.com/evolvingweb/ajax-solr/wiki handled using the pysolr library. 11 For the rest of this pa-per, we describe the concrete facets employed in our system and the analytics algorithms used to populate them.
Topic Facets are intended to help discover and charac-terize the subject areas present in a document collection. Moreover, they allow users to better navigate the collection to nd information of interest. The rst Topic Facet we discuss is based on unsupervised keyterm extraction.
Our rst approach to populating a Topic Facet is based on extracting topic-representative terms ( i.e., keywords) from documents (shown as Top Discovered Keywords in Figures 1 and 2). Here, we present the KERA algorithm ( K eyword ht tp://code.google.com/p/pysolr/ E xtraction for R eports and A rticles). KERA is an unsuper-vised algorithm to extract keywords from individual unstruc-tured text documents ( i.e., it does not require an entire cor-pus like TF-IDF and other strategies). At its core, KERA is a descriptive model for keyword assignment. It is based on several key observations of human-assigned keywords 12 (especially those in scienti c and technical publications):
Many of these observations have also been noted in other works ( e.g., [21]). The KERA algorithm is shown in Al-gorithm 1. We now describe its three main components: collocation extraction, part-of-speech ltering, and ranking. Collocation Extraction. We rst employ the use of collo-cation extraction to identify candidate key terms. A colloca-tion is \an expression consisting of two or more words that corresponds to some conventional way of saying things." [16] We posit that it is these sets of words that are most likely to contain topic-representative phrases. Although it is pos-sible to extract collocations of three or more terms, we nd
W e refer here to those keywords that also appear as terms within the document itself. A lgorithm 1 KERA algorithm Re quire: D , an unstructured text document Require: K , the number of keywords to extract 1: # generate candidate keywords 2: terms 1 = extractCollocations ( D ) 3: terms 2 = extractN ounP hrases ( D ) 4: terms 3 = extractP roperN ounU nigrams ( D ) 5: candidates = ( terms 1 \ terms 2) [ terms 3 6: # rank candidates 7: for all c 2 candidates do 8: if c is unigram then 9: = normalized frequency of term c in D 10: else 11: = normalized collocation score 12: end if 14: rank score of term c = 2 + 1 5: end for 16: # optionally prune based on domain-speci c criteria 17: # candidates = prune ( candidates ) 18: return top K candidates based on rank score th at such phrases do not lend themselves to aggregation ( e.g., for use in tag clouds). At the same time, we nd that one word terms are not expressive enough for users to discern the topics of documents. Thus, we extract only collocated bigrams ( i.e., two-word expressions). Although our system supports multiple collocation extraction strate-gies including the log-likelihood ratio test [5] and Pointwise Mutual Information (PMI) [16], we currently use the log-likelihood ratio exclusively, as we nd it performs best with respect to Topic Facets . Using the log-likelihood ratio test, the collocation score for a bigram of words w 1 and w 2 is 2
P the bigram from the contingency table for w 1 and w 2 and m ij are the expected frequencies assuming that the bigram is independent [5,16].
 Part-of-Speech Filtering. As mentioned, the most ex-pressive keywords are typically noun phrases. Thus, we l-ter the set of collocations by removing terms that do not match the pattern (adjective)*(noun)+ . If the extracted phrases are greater than two terms, we begin truncating from the left until we are left with a bigram. Such ltering also helps to remove bogus words sometimes introduced by the text extraction process for non-plain-text document for-mats. To this ltered set, we add extracted unigrams that are proper nouns, as we nd such terms can be critical to the topic of documents. This is especially true of government, scienti c, and technical publications, as proper nouns often refer to a system, algorithm, program, or initiative being described.
 Ranking Keywords. Finally, we rank the extracted terms, as shown in Algorithm 1 and return the top K candidates. 13 Our ranking methodology takes into account both the posi-tion of terms within a document and the collocation score (or term frequency). The nal score is taken as the har-monic mean of these metrics. Prior to returning the nal set, one might optionally prune the candidates based on domain-
Cu rrently, we set K = 5 or K = 10 for KERA . speci c criteria. For instance, in our case, the set of proper noun unigrams may be pruned to only contain those uni-grams that are upper-case, since it is those terms that often signify important technical systems and programs. 14 Comparison to Other Approaches. Development of KERA was motivated by the fact that existing algorithms did not meet one or more of our needs. For instance, a num-ber of the existing approaches are either supervised, require an entire corpus, or both. Such characteristics are unac-ceptable, as supervised approaches are labor-intensive and corpus-based methods (such as those like TF-IDF that use inverse document frequency) may undervalue terms associ-ated with prevalent topics. TextRank [17] and RAKE [21] are two methods that are both unsupervised and operate on individual documents. Unfortunately, although both meth-ods can perform reasonably well when supplied only pa-per abstracts, they sometimes perform less well on longer, messier, and more realistic document structures. 15 For in-stance, Table 1 shows the keywords extracted for this very paper. Note that the keywords extracted by KERA are qualitatively superior to TextRank [17] and RAKE [21].
A second Topic Facet we employ is based on the con-cept of topic clusters. Topic modeling and clustering algo-rithms segment documents into groups, where the intent is for each group to consist of documents pertaining to a par-ticular topic or theme. Whereas many clustering algorithms produce \hard" clusters or disjoint sets of documents, topic models produce \soft" or overlapping clusters. Topic models and clustering strategies may also tag clusters with topic-representative words. In topic models like latent Dirichlet allocation or LDA [1], topics are modeled as word probability distributions, and these tags are simply the most probable words in a distribution. Our application supports multiple approaches to topic clustering including LDA [1], Hierarchi-cal Dirichlet Process (HDP) [23], Latent Semantic Indexing (LSI) [15], and K-Means [7]. All approaches are provided by
Ot her possible variations include discarding candidates when proper noun unigrams also appear as part of extracted bigrams, removal of unigrams that do not rst appear until later in the document, signi cance testing to lter the set of collocations, and setting always as normalized frequency.
We base these statements on the TextRank imple-mentation available at http://search.cpan.org/~kubina/ Text-Categorize-Textrank-0.51/ and the RAKE imple-mentation at http://github.com/aneesha/RAKE . Full ex-perimental results are omitted due to space limitations. th e machine learning libraries mentioned in Section 2. For the current deployment, we employ LDA exclusively. Docu-ments are assigned to a topic only if the topic proportion as-signed by LDA is greater than 0 : 3, and documents are tagged using the top 10 LDA-derived topic tags. LDA requires the number of topics, K , as input, and we currently set this heuristically based on the size of the document collection. However, in the future, we plan to migrate to HDP, which is a non-parametric approach to topic modeling [23]. The facet populated by LDA is labeled \Topic Clusters" and ap-pears as a menu showing the list of discovered topics. These topic clusters are labeled by LDA-derived tags and ordered by the topic ranking methodology described in [26].
All Topic Facets discussed thus far (including topic mod-els) are focused on identifying trends and hotspots within the topic collection. That is, they are not well-suited to nd-ing \needles in haystacks." A document pertaining to a lone topic of high interest to a particular user may not be identi -able in the presence of large topic clusters displayed in a tag cloud or other interface. To address this, we supplement the facets populated by KERA and LDA with additional tag cloud facets populated with supervised document classi ca-tion. We have previously reported our work on supervised machine learning for critical technologies in [14] . Thus, we only include brief and sparse descriptions here. For more information on the development of document classi ers in this domain, please see [14].
The facet labeled Military Critical Technology Finder in Figure 1 is populated using a set of binary supervised ma-chine learning classi ers. Each binary classi er is trained to identify documents pertaining to a particular critical tech-nology, and each tag in the cloud represents the positive class of a classi er. For any individual document, if no bi-nary classi er categorizes the document as positive, then the document is assigned the tag \other", which also appears in the cloud. We use LinearSVM as our main learning algo-rithm for all classi ers. Constructing training sets for these classi ers poses a number of challenges. For instance, when training these binary classi ers for arbitrary le collections ( e.g., a workstation hard drive), the negative class becomes highly heterogeneous. If this heterogeneity is not represented or otherwise addressed in the training set, performance can degrade. In addition, documents pertaining to critical tech-nologies can sometimes compromise a very small minority of all possible les encountered. This is known as the class imbalance problem and can also cause performance to suf-fer due to bias. To address these and other problems, we employ heavy use of active learning in a two step sampling procedure [14]. We rst employ active learning strategies ( e.g., minimum marginal hyperplane) to sample only the most informative of negative examples for the initial training set (which helps address heterogeneity). We, then, balance the training set by further sub-sampling this initial training set to produce the nal training set.
Using a very similar methodology to the one described in the previous section, we develop an additional classi er to categorize documents based on report type. That is, doc-uments are categorized into one of four categories: Techni-cal Information ( e.g., a research paper), Test Information ( e.g., a test plan for a system), Programmatic Information ( e.g., details of a program for development of a system), and Other ( i.e., everything else).
Users sometimes may be interested in locating documents not by topic but by mentions of particular entities, terms, or expressions of interest ( e.g., IP addresses). To address this, we employ the use of a Mention Facet , which allows users to upload a plain text le containing expressions of inter-est. These expressions can currently take the form of simple lists of terms, gazetteers ( i.e., entity dictionaries), or regu-lar expressions for patterns of interest ( e.g., a social security number). The results are displayed as either a tag cloud or menu, where the items are either explicit terms with matches in the document collection or high-level categories described by expressions ( e.g., tagging documents containing social security numbers with \PII"). Recall that our current re-search sponsor is speci cally interested identifying military critical information. Thus, for the rst deployment of our application, we populate the Mention Facet in the follow-ing manner. We take the training sets used for our binary classi ers described in Section 3.3.1 and extract the top 25 most discriminative terms based on information gain [15]. The entropy H of a set of labeled documents D measures impurity as follows: H( D ) = p + log 2 ( p + ) p log 2 ( p ), where p + and p are the proportions of positive and nega-tive documents in D , respectively. 16 The information gain IG of a word w in training set D , then, is the expected entropy reduction due to segmenting on w : IG( w; D ) = documents in D containing word w . Thus, words with the highest information gain in a training set are expected to be the most discriminative. (Although the Mention Facet can be used for many purposes, populating the facet in this fash-ion, in a sense, transforms it into yet another kind of Topic Facet .) We have also used KERA to populate the Mention Facet directly from only the positive training documents. Finally, we supplement this list of discriminative terms with a set of markings for sensitive documents ( e.g., \For Ocial Use Only", \FOUO").
Our nal set of facets are populated through direct extrac-tion from document metadata. The Format Facet is pop-ulated by tagging documents based on le type ( e.g., pdf, doc, ppt, txt) and is labeled \Top File Types." The Location Facet (labeled \Top Folders") is populated by tagging each document with the directories in its le path. The Time Facet (labeled \Date" in our application) is populated by extracting the Last-Modi ed time from documents. Finally, the Author Facet is populated using the Last-Author or Au-thor name (when available). The Location Facet is displayed as a menu listing the most populous folders, and the Time Facet is displayed as a calendar widget. All other facets are displayed as tag clouds. (Note that none of these facets are viewable in Figure 1.)
N ote that log 2 (0) is taken to be 0.
We conduct a series of case studies at the deployment sites using a prototype of our application undergoing eld testing. Motivated by the recent position paper \Machine Learning That Matters" by Wagsta [25], we focus on external valida-tion of our application by assessing time saved and insights gained in collaboration with domain experts. Although our system can be used for many purposes, we focus our eval-uation on the current application of interest to our spon-sors | locating information pertaining to military critical technologies within heterogeneous document collections. To locate such information, analysts at the sponsoring agency currently use simple keyword searches exclusively. Thus, we compare our new approaches to this existing approach. Since our system employs the use of multiple approaches to locate and discover information, we also draw comparisons among our new approaches. For reasons of sensitivity, we cannot reveal the deployment sites, the sponsoring agency, or technical subjects of interest to the agency. Thus, we redact information as necessary.
Search here involves the task of nding information per-taining to a particular military critical technology within a document collection. We consider a particular technol-ogy of high interest to our sponsors and assess how well the supervised approaches in our application are able to locate this critical information. We refer to this technology sim-ply as Technology-X . A case was provided to us containing 30,128 les acquired from workstation hard drives of roughly 11 users. The les spanned numerous le formats includ-ing Microsoft Oce, HTML, PDF, and plain text. Ana-lysts con rmed to us that the case was positive. That is, it was manually veri ed previously to contain information about Technology-X but not searched thoroughly. The les were spread across multiple media ( e.g., external USB hard drives, SATA drives, DVDs). We built machine learning classi ers and a custom mention search for Technology-X , as described in Sections 3.3 and 4. Upon loading and in-dexing the case into our application, we evaluated these approaches and compared results to those obtained from a manual review of the case by two analysts using their exist-ing methodology ( i.e., ad hoc keyword searches only). Re-sults are shown in Figure 3 as a Venn diagram.

As shown, both the classi er and the two analysts identi-ed 18 documents as pertaining to Technology-X . The ana-lysts also identi ed twelve additional les. Upon review by subject matter expert (SME), only seven of the twelve les were related to Technology-X , whereas the classi er achieved perfect precision. Of these seven false negatives, one was a gure with no accompanying text and a second was a 5 sen-tence email that was deemed critical by the SME. The most striking result, however, is the time savings achieved. The two analysts took roughly 7 hours (or 14 person-hours) to locate Technology-X documents. By contrast, the classi er identi ed 18 of the 25 in mere seconds . The remaining les ( i.e., all the seven false negatives) were located in less than 30 minutes using the Mention Search , Report Type Filter , and Top Folders facets in our application. We attribute most false negatives committed by the classi er to the fact that, due to political complications, the positive examples available to us were limited (only 51 examples were used). Given this and the breadth and depth of military critical technology information, unsupervised topic discovery is of high importance to this domain. We discuss this next.
Discovery involves browsing document collections and al-lows users to locate information for which they did not even know to look. A framework to facilitate discovery can clearly facilitate a search for something speci c, as well. Due to logistical and policy-related issues, we were not able to eval-uate discovery on the case described in Section 6.1. Instead, we were provided a new case to evaluate, which contained 39,515 les. Unlike the case from Section 6.1, we did not have any approximation of ground truth, as the case had not been formally reviewed. Here, we assess the knowledge discovered and summarize lessons learned from execution of our application on this case.
 Identi ed Critical Topics. Table 2 shows the two topics pertaining to military critical technologies discovered by our application (referred to as Technology-Y and Technology-Z . Of course, numerous non-critical topics within the document collection were also discovered (some of which were of a per-sonal, non-work-related nature). As for the critical topics, there were 89 documents found pertaining to Technology-Z and 232 documents pertaining to Technology-Y (including duplicate les). Through a subsequent exhaustive manual review of the case, we estimate that no additional informa-tion on military critical technologies of interest was present on this case. Using our facet-based system, most documents for these two critical topics were identi ed in less than an hour (and in some cases only minutes). By contrast, domain experts informed us that cases of this size typically require hours or days of analysis to produce similar results, which is consistent with our experience during the manual review. Several facets were identi ed as highly e ective in identify-ing these topics when used in combination with each other. We discuss these next.
 E ective Usage Patterns Discovered. The third col-umn of Table 2 displays the facet combinations that were found to be most e ective in identifying critical documents. The combination labeled as Method A indicates that the \Topic Cluster" facet populated by LDA was rst explored and used to lter the search results. The \Top Discovered Keywords" facet (populated by KERA ), then, was used to identify document sets related to the critical technology. The combination labeled as Method B indicates that the \Report Type Filter" was used to locate documents pertain-ing to Technical Information followed again by the\Top Dis-covered Keywords" facet. Finally, for Method C , the \Top Folders"facet ( i.e., our Location Facet ) was used to lter the search results, with the KERA -populated tag cloud being used to quickly assess documents within folders. Notice that multiple facet combinations often exist to locate the same set of critical documents. This highlights a signi cant ad-vantage to our multi-faceted system: topics are more likely to be discovered by users when more paths lead towards them. As shown in the table, Method A and Method B were employed heavily to nd both Technology-Y and Technology-Z . Method C was only used for Technology-Z , not Technology-Y . Since the les for Technology-Y were scat-tered across many directories, the \Top Folders" facet was not as useful. As can be seen, the \Top Discovered Key-words" facet populated by KERA played an indispensable role in all three methods, as it allowed for quick exploration and assessment of the document collection. 17 It was partic-ularly useful as a complement to the \Topic Cluster" facet, as we now explain.
 Using KERA as a Cluster Labeling Strategy. One of the issues with topic models like LDA is that the terms (or tags) they assign to topics are often not very expressive of the topic. In other words, in practice, it is quite dicult for humans to go directly from LDA-derived tags to a thematic label for the cluster without reading documents in the clus-ter. This has been recognized in other works on deployed applications based on topic models ( e.g., see [26]), and we found this to be the case in our evaluation, as well. However, from the e ective usage patterns observed previously, we ob-served that the \Top Discovered Keywords" facet populated by KERA is a highly e ective way to quickly determine the overall subject matter of a topic cluster (or even a folder). We cannot illustrate this on topics related to military crit-ical technologies due to their sensitive nature. Figure 4, however, shows tags produced for a non-critical document cluster (extracted from documents residing on the rst au-thor's laptop). Although the tag cloud generated by KERA is not quite a thematic label for the cluster, it is signi cantly more expressive than the tags assigned by a typical LDA implementation. This, then, illustrates yet another way in which tag cloud facets are useful \lenses" into document col-lections, as described in Section 2.
 LDA Performance on Critical Technologies. Table 3 shows the precision and recall with respect to the LDA clustering. A true positive is de ned as placement into an appropriately labeled cluster having a majority of the doc-
In more recent tests, the Mention Facet , when used to lo-cate documents with sensitive markings, also was found to be useful in combination with all methods listed in Table 2. uments pertaining to the same militarily critical topic (as judged by a SME). Critical documents placed into clusters with largely non-critical and possibly unrelated documents are considered false negatives. Non-critical documents ap-pearing in a cluster of largely critical documents are consid-ered false positives. Note the low precision for Technology-Z resulting from 84 of the 89 critical documents being placed into a larger cluster of non-critical documents. Since these non-critical documents were indirectly related to the topic covered by Technology-Z , LDA was unable to distinguish them from the truly critical documents. All 89 of these crit-ical documents, however, were ultimately located with help from other facets such as Top Folders and Top Discovered Keywords . These results (combined with an intolerance to false negatives by users in this domain) justify our decision to employ multiple facets | as opposed to relying only on topic models, which some other works have done ( e.g., [26]). Cri tical Technology Pre cision R ecall F-S core T echnology-Y 0. 89 0. 98 0 .93 Technology-Z 0. 56 0. 94 0 .70 Issues Requiring Future Investigation. We conclude our discussion of this case study by noting two issues ob-served during our evaluation. The rst relates to setting the number of topics, K , in LDA. Most works, including ours, set this value in a largely ad hoc fashion. Although there are heuristics and rules-of-thumb that have been proposed ( e.g., [3]), most machine learning practitioners acknowledge that the choice of K is \more art than science." 18 Guessing the correct value of K is particularly dicult for hetero-h ttp://cwiki.apache.org/MAHOUT/ latent-dirichlet-allocation.html g eneous document collections, as K can be severely under-estimated. Moreover, an incorrect setting of K can have detrimental e ects on the results. We have personally found this to be true in our evaluations. One approach to address-ing this is to employ the use of newer non-parametric topic models like HDP [23]. We plan to explore such methods in the future to address these issues. A second issue relates to KERA . Although bigrams are appropriate and well-suited for automated tag cloud generation, in some cases, they can produce sub-optimal results ( e.g., extracting \Dirichlet al-location" and not \latent Dirichlet allocation"). Some re-cent approaches to word segmentation based on probabilistic models can potentially be exploited for better keyterm ex-traction [6]. This, then, is another area for potential future exploration.
Given the diverse set of facets employed by our applica-tion, several di erent lines of related work exist. We brie y describe these areas here.
 Characterizing Large Document Collections. There are several works describing text analytic systems designed to characterized large text corpora ( e.g., [4, 10, 26]). Most systems focus on a particular document type ( e.g., tweets, emails), whereas as our system is designed with heteroge-neous document collections in mind.
 Topic Modeling, Clustering, and Categorization. Many text analytic systems perform topic analysis through use of topic models ( e.g., LDA [1], HDP [23]) or clustering algo-rithms like K-Means [1,7], which are both unsupervised. Su-pervised text classi cation approaches are also sometimes employed [15]. Our objective in this work is to bring to bear multiple approaches for topic analysis. As we have shown, using multiple approaches in concert with each other through a faceted browsing framework yields signi cant ad-vantages.
 Keyphrase Extraction. Several works describe algorithms to extract keywords and keyphrases from documents. Some approaches are supervised or require an entire corpus as in-put ( e.g., [2,27]), which, as described previously, is not suit-able for our purposes. TextRank [17] and RAKE [21] are two approaches that are purely unsupervised and operate on individual documents. However, as we have shown, they do not appear well-suited to automated tag cloud genera-tion. A related area of research is collocation extraction ( e.g., [5,16,19]), which we exploit in the KERA algorithm. Tag Cloud Research. Numerous works leverage tag clouds for both faceted navigation and corpus visualization ( e.g., [9, 11,28]). The overwhelming majority of this work focuses on manually-generated tags ( e.g., social-tagging systems) as op-posed to automated generation of tags, which is one of the foci of our work.
In this paper, we have proposed a demonstrably e ective system for exploratory analysis of arbitrary document col-lections. Our system, based on multiple information facets , is designed to address a major capability gap within the U.S. federal government: investigative analysis of highly heterogeneous document collections. We have presented a concrete implementation of this multi-faceted system that aids users in identifying information pertaining to military critical technologies embedded within large and arbitrary document collections. A prototype of our application was successfully deployed in May 2013. In the future, we plan to extend the tool in numerous ways including sentence-based summaries of topics and visualizations of topic clusters.
