 We propose a new recommendation algorithm for online doc-uments, images and videos, which is personalized. Our idea is to rely on the attention time of individual users captured through commodity eye-tracking as the essential clue. The prediction of user interest over a certain online item (a doc -ument, image or video) is based on the user X  X  attention time acquired using vision-based commodity eye-tracking durin g his previous reading, browsing or video watching sessions over the same type of online materials. After acquiring a user X  X  attention times over a collection of online material s, our algorithm can predict the user X  X  probable attention tim e over a new online item through data mining. Based on our proposed algorithm, we have developed a new online content recommender system for documents, images and videos. The recommendation results produced by our al-gorithm are evaluated by comparing with those manually labeled by users as well as by commercial search engines including Google (Web) Search, Google Image Search and YouTube.
 H.3.3 [ Information Search and Retrieval ]: Relevance feedback; H.3.7 [ Digital Libraries ]: User issues; H.5.2 [ User Interfaces ]: Input devices and strategies; H.4 [ Information Systems Applications ]: Miscellaneous Algorithms, Design, Experimentation, Human Factors, Mea-surement, Performance. Contact him at songhua DOT xu AT gmail DOT com.
 Personalized recommendation and ranking; web search; user attention; commodity eye-tracking; document, image and video recommendation; implicit user feedback.
Web surfing is a part of many people X  X  everyday life, which may include reading online documents, looking at images, watching videos, etc. Some of these online contents are the results of specific searches requested by the users; searchi ng is thus an important operation supported by the web. Up to now, the most common searching method is keyword based, and the searching as carried out by the current generation of commercial search engines is user-independent. Recentl y, however, there is a growing interest in user-dependent, or personalized searching, e.g., [23, 3]. Personalized searc h en-gines need to infer user search preferences which can be de-rived from user feedbacks. In this paper, we propose an algorithm which can return a personalized online content recommendation according to the user X  X  previous reading, browsing and video watching behaviors. The key feature of our algorithm is its ability to track a user X  X  attention time over online materials, which is obtained via a data mining process and based on data captured by a vision-based com-modity eye-tracking device.

As an independent thread of research, eye-tracking has attracted many researchers in the fields of human-computer interaction, user modeling, computer graphics and interac -tive techniques. The main advantage of eye-tracking is that it is not intrusive while acquiring user feedbacks. These feedbacks are needed for deriving users X  preferences in bui ld-ing adaptive systems. So far, applying modern eye-tracking technologies to recommender systems has been rare. In this paper, we propose to use commodity eye-tracking to develop a personalized recommender system for online content rec-ommendation, where the contents may include documents, images, and videos. With our commodity eye-tracking ap-proach, we can acquire a user X  X  attention over online mate-rials that the user has seen, and based on which predict his attention over materials that he has not yet seen.
For a target object O i , which could be a document, an image, or a video on the Internet, we denote the user U j attention on it as AT ( O i , U j ), which is the time the user spends on reading, browsing or watching the object. Let O 1 and O i 2 be two objects of the same type, i.e., both being documents or images or videos. Without loss of generality, let X  X  assume after they are both presented to and watched by the user U j , we have AT ( O i 1 , U j ) &gt; AT ( O i is reasonable to infer that U j is more interested in O i O
By the above heuristic, to recommend an optimal list of online materials most interesting to a user, our algorithm essentially only needs to predict the attention of the user on these objects. With the prediction results, we can then return an ordered list of online materials where the orderin g is by the predicted user attention times. The problem is very similar to those rating problems studied in recommender systems X  X .e., given a user X  X  rating on a number of objects, how to predict his rating on other objects which he has not yet rated.

Nevertheless, there is a major difference between the rat-ing scenario in conventional recommender systems and our situation. The traditional rating over an item is  X  X tomic X , which means that the user gives his overall preference for th e item, and not for its subcomponents or different features sep -arately. In contrast, in our scenario, user attention is com -positive, e.g., in the case of a video, the user X  X  attention i s the accumulated attention of watching a series of episodes o f the video; and in the case of reading an article, the attentio n is the sum of the attention on every paragraph or section of the article. In fact, in reality, when we human beings form our preferences, it is often a mixed decision in which we try to balance the various sides of a choice before making the decision. We are not aware of any recommender system in practice that allows a user to specify a rating over the var-ious components or facets of a product. This might be due to the difficulty of obtaining user feedback for the subcom-ponents as many users are already reluctant to provide their feedback on an object as a whole. Given the non-intrusive nature of eye-tracking for acquiring user feedback, we can obtain the user X  X  evaluation on the subcomponents of the target object. To leverage subcomponent rating, during our algorithm design, we carefully make use of the compositive structure of user attention in inferring user preference.
The remainder of the paper is organized as follows. We first survey the most related work in Sec. 3. We explain how to acquire user attention time for documents, images and videos respectively in Sec. 4. We discuss how to infer user attention via content-based estimation in Sec. 5. Given the user attention estimation, we introduce our algorithm for personalized online visual material recommendation based on the predicted user attention in Sec. 6. We present exper-iment results to demonstrate the effectiveness of our method in Sec. 7. We conclude the paper and point out some future work directions in Sec. 8.
Personalized search engines as a relatively new track of research is drawing more and more attention these days, e.g., [23, 3]. The existing personalized search engines so f ar rely on user feedbacks of various kinds, which can be broadly classified into two categories X  X xplicit and implicit; both of them can be used to infer user intentions or preferences for customizing the search [26, 30, 31]. Because users generall y are least interested to provide explicit feedbacks, the tre nd is to derive search preferences from implicit feedbacks [8, 9, 6]. Implicit feedbacks can be quite abundant, thus ensuring the reliability of the inference. The most popular implicit use r feedbacks currently utilized in commercial search systems are query history and click data.
Query history probably is the most widely used implicit user feedback at present. Google X  X  personalized search ser -vice (http://www.google.com/psearch) allows users to sto re their search history in their Google account which will be analyzed for personalizing their future search. In general , there exist two classes of methods for providing personal-ized search based on query history: those based on the whole query history of a user and those based on the query history in a particular search session. For the former, usually a user profile is maintained to describe his search preference . For example, Liu et al. [21] constructed user profiles using the whole search history through an adaptive Rocchio al-gorithm [14]. Speretta and Gauch [27] demonstrated that using user profiles can significantly improve search engine performance. The query history in a query session is also called the query chain [24]. It can be used to automatically suggest or to complete/expand a query question for a par-ticular user based on the query history so far in the same search session [12].
Click data is another type of implicit user feedback, which has been intensively utilized, e.g., [4, 15]. The basic idea is that when a user clicks on a document, the document is considered to be of more interest to the user than the unclicked ones. There are many ways to infer user prefer-ence from click behaviors. For example, a simple approach would be when a user clicks on the i -th link in a ranked list of webpages before having clicked on any of the first i  X  1 links, we can infer that the first i  X  1 documents are no more important than the i -th document. Among the sophisticated approaches, ranking SVM algorithm [11] has been applied to find the best webpage rank according to a user click dataset [16]. In [24], cross-query preference an d individual-query preference are extracted to train a webpa ge rank model through a ranking SVM algorithm. Sun et al. [29] proposed a method based on singular value decomposi-tion to improve the accuracy of a recommendation system through analyzing user click data.
Attention time, also referred to as display time or reading time, is a newly recognized type of implicit user feedbacks. It is receiving increasing popularity even though its relia bil-ity in predicting user interest has yet to be confirmed. One side of the opinion is represented by arguments made by Kelly and Belkin [18, 17], claiming that there is no reliable relationship between the interestingness of a document and its display time. In their study the display time is measured as the average reading time spent by a group of users on articles of different topics coming from the Web. The other side of the opinion, e.g., Halabi et al. [10], is that for a fixe d user in a certain query session, attention time gives a stron g indication of the user interest X  X he more time a user spends on reading a document, the more important the document is to him. We think these conclusions are not contradicting as display time is calculated differently by the two groups. In this paper, we propose using attention time of documents, images and videos to rank these online materials, which is analogous to the attention time idea for ranking documents only. Our basic assumption is that user specific and topic specific attention times do provide a credible indication of the user X  X  interest; based on this assumption we propose a personalized online material ranking algorithm and a recom -mender system based on the algorithm.

In our prior work [32], we have explored using attention time for user-oriented webpage re-ranking. Compared with that work, our new algorithm is capable of making person-alized recommendation on documents, images and videos while our prior work focuses exclusively on user-oriented webpage ranking. In this paper, we also employ vision-based commodity eye-tracking as a friendly user interaction mean s to acquire user attention, which was not explored previousl y. Last but not least, the user attention times studied in this paper are per words or image region for documents and im-ages respectively rather than for a whole document or image. This finer level of representation and analysis makes our at-tention time prediction more accurate and reliable.
Other types of implicit user feedbacks include display time , scrolling, annotation, bookmarking and printing behavior s. People have recently started to combine multiple types of implicit feedbacks for better inference of user interests [ 22]. Fox et al. [5] have made a comprehensive study and pro-posed a decision tree based method augmented by Bayesian modeling to infer user preference from a set of mixed types of implicit user feedbacks.
Eye-tracking is the technology to measure either the gaze, i.e., the spot a user is looking at, or the motion of the hu-man eyes (http://en.wikipedia.org/wiki/Eye tracking). In our work, we use eye-tracking to measure the attention time of a user over a document, image or video through identify-ing the part of the screen area the user is looking at and for how long. Unfortunately, commercial eye-tracking devices are very expensive. Some researchers therefore have turned to using ordinary web cameras as eyetracking devices [19, 1, 25, 20, 28, 7]. We did the same and have assembled an eye-tracking device using a simple web camera (Logitech Quick-cam Notebook Pro) and an existent eye-tracking algorithm available from the Opengazer project [33]. We additionally employed some vision techniques to create our custom eye-tracking component. This design of our eye-tracking com-ponent, or something similar, is cost effective and can be widely adopted on personal computers as many PCs these days are equipped with web cameras.
Through our commodity eye-tracking component, we ob-tain a number of fixation points on the screen, which indicate the detected gaze area of the user. For our recommender al-gorithm to work, we need to anchor these gaze samples onto the corresponding object segments. Object segment means a basic compositive unit of an object, e.g., a word in an articl e or a region in an image. By our assumption, the more gaze samples an object segment receives, the more interesting th e segment is to the user. We now look at how to anchor gaze samples onto the corresponding object segments for docu-ments, images and videos respectively. We summarize the segment definitions of different object types and their gaze-to-segment assignment methods in Table 1. This table also summarizes the user attention prediction methods for differ -ent types of objects, which will be discussed in Sec. 5.
For an online document, we define its object segments as individual words. We first introduce the term  X  X napshot of the document X  to refer to the part of the document that is displayed on the screen at the given moment. For example, if a user resizes the displaying window or scrolls to a dif-ferent part of the document, a new document snapshot is said to be formed. For each snapshot of the document, we assign the gaze samples to the corresponding words in the document in a  X  X ractional X  manner. We introduce a Gaus-sian kernel in the assignment process. Assuming at a certain moment, the detected gaze central point is at position ( x, y ) in the screen space, for each word w i that is displayed in the current document snapshot, we first compute the central displaying point of the word as the center of the bounding box of the word X  X  displaying region. We denote it as ( x i Then the fraction of the gaze sample to assign to the word w i is: The free parameters  X  x and  X  y specify how  X  X ocused X  a reader scans words when reading documents. In our cur-rent implementation, we initialize  X  x and  X  y to be the av-erage width and height of a word X  X  displaying bounding box in the document. The overall attention that a word in the document receives is the sum of all the fractional gaze sam-ples it is assigned in the above process. Notice that when a word occurs multiple times in the document, we accumulate all the gaze samples assigned to these occurrences. Finally , the overall attention of a user over a word is the sum of the word X  X  attention across all the documents the user has read previously. During our processing above, we remove stop words (http://en.wikipedia.org/wiki/Stop words) since they are not providing any substantial meaning and thus should not really have attracted the user attention. Notice that for words in the documents that are not displayed, their attention is unspecified rather than being assigned zero.
For an image, we define its object segments as rectangular regions in the image. How to determine these regions will be discussed shortly. Similar to our handling the case of docu-ments above, we also use a Gaussian kernel to fractionally assign gaze samples to these rectangular image regions. For a detected gaze point whose central position is ( x, y ) and Document word inhomogeneous 2D-Gaussian (1) based on word attention (4) prediction methods for different types of objects. a rectangular image region m i , we find the nearest point ( x , y  X  ) within the rectangular region m i to ( x, y ). Then the fraction of the gaze sample the rectangular region m i receives is: Here we do not differentiate between the horizontal and ver-tical standard deviations in the Gaussian kernel because ac -cording to our observation, when browsing images, human eyes process the visual information more equally in the ver-tical and horizontal directions than when reading texts.  X  is by default set as 1 cm and can be user tuned in order to maximize the accuracy of our user attention prediction algorithm which will be presented in Sec. 5.

Now we discuss how to determine the rectangular regions in the image for constructing image segments. First, the entire image is always treated as an image segment. The number of the gaze samples the whole image receives is the total number of the gaze samples detected that fall inside the image region when the image is being displayed on the screen. And then we find the position ( x h , y h ) in the image which has the highest gaze point density. Here the density of a position is defined as the number of gaze points whose hor-izontal and vertical distances to the point position ( x h are no farther than 6  X  m . Once such a highest density point is detected, we test whether the total gaze sample the rect-angular region receives is above a certain threshold  X  . If so, we will identify the rectangular region as an image segment, whose left bottom, right bottom, right upper and left upper corners are ( x h  X  3  X  m , y h  X  3  X  m ) , ( x h  X  3  X  m 3  X  m , y h + 3  X  m ), and ( x h  X  3  X  m , y h + 3  X  m ) respectively. Af-ter that, we remove both the rectangular region from the original image as well as all the gaze samples falling into the rectangular region. We then find the next highest den-sity point in the remaining part of the image. If its density is above  X  , we will identify a new rectangular region as an image segment and continue the search process. Otherwise, our process of image segment identification terminates. No-tice that the above image segment identification process is only executed when the image is not too small, i.e., larger than 6  X  m  X  6  X  m . Otherwise, we would only treat the whole image as an image segment.
Finally, for a video, its segments are simply the video keyframes. After a user watches a piece of video online, we first detect all the keyframes from the video using the keyframe detection algorithm proposed in [2]. And the de-tected gaze points that fall into the video displaying windo w for the duration of the video will be assigned to the nearest keyframes. Because most of the online videos are of a low resolution, unlike the way we are dealing with images, we do not further split the video keyframes into sub image regions . More concretely, we assume there is a gaze sample detected at time t i which falls in the video playing window. The two nearest video keyframes, Keyframe + and Keyframe  X  , to the time moment t i are at time moments t k  X  and t k + respec-tively. Then the fractions of gaze sample that the keyframes Keyframe + and Keyframe  X  receive, which are denoted as AT ( Keyf + ) and AT ( Keyf  X  ) respectively, are computed as follows:
AT ( Keyf + ) ,
Our proposed recommender algorithm deals with three types of online materials, i.e., documents, images and vide os, and so we study the prediction of user attention for each type in the following.
Assuming a document v i consists of n distinct words w 1 , document as the average of those words whose attentions by the user are known. Formally, the user U j  X  X  attention over document v i is predicted as: Here  X  ( w k , U j ) = 0 if either there is no attention sample acquired for user U j over the word w k , or the word w k stop word. Otherwise,  X  ( w k , U j ) = 1. The reason we derive an average attention time here is to normalize documents with different lengths so that our predicted user attention for documents would not bias longer documents.
Given an image v i consisting of n image segments, denoted as v i , { vs i, 1 , vs i, 2 , , vs i,n } , for each of the image seg-ment vs i,j ( j = 1 , , n ), we find  X  image segments whose attention by the user U j is known and which share the high-est content similarity with vs i,j . In our current experiments,  X  is set as min (10 , z ), where z is the size of the current training set, i.e., the number of image segments whose user attentions by U j are known. We assume these  X  image seg-ments are vs l i,j ( l = 1 , ,  X  ). Then we use the following equation to predict U j  X  X  attention for vs i,j : AT ( vs i,j , U j ) , where  X  ( vs l i,j , vs i,j ) returns the image content similarity be-tween vs l i,j and vs i,j . Empirically, we find the image content similarity measurement based on the feature of  X  X uto Color Correlogram X  [13] works best in our experiments. We adapt the code in the open source content based image retrieval li-brary (http://www.semanticmetadata.net/lire/) for the i m-plementation of the image similarity metric.  X  is a weight controlling how the values of  X  ( , ) will contribute to the estimation of user attention, and  X  is a small positive num-ber to avoid the divide-by-zero error. The function of  X  ( , ) defined below filters out the video pairs whose similarity is below a certain threshold: Using the above equation, we can predict the attention of the user U j over all the image segments, i.e., vs i,j ( j = 1 , , n ). Then the overall user attention for image v i the maximum sum of a non-overlapping set of all its image segments, i.e.: where v  X  i = { vs i, 1 , vs i, 2 , , vs i,m } is a subset of v vs i, 2 , , vs i,n } in which  X  vs i,x , vs i,y  X  v  X  i , x 6 = y  X  vs vs
For a video v i consisting of n keyframes, i.e., v i , { vs vs i, 2 , , vs i,m i } , we predict its user attention as the sum of the user attention over its individual keyframes, i.e.: To predict the user attention over a keyframe, we use a very similar approach to user attention prediction for im-ages. The only difference is that we do not consider image segment since most online videos are not of very high reso-lution and thus we do not try to detect image segments for the keyframe images (see Sec. 4.2.3). More concretely, for a video keyframe image vs i,j , we find  X  video keyframe images which are most similar to vs i,j from videos which the user has previously watched. In this nearest neighbour search process, we also use the image similarity metric based on the feature of  X  X uto Color Correlogram X  [13]. Assuming these  X  keyframe images are vs l i,j ( l = 1 , ,  X  ), then we can use (5) to predict the user U j  X  X  attention over the keyframe vs i,j . After U j  X  X  attentions over all the keyframe images of video v i are predicted, we plug them into (8) and derive our prediction over U j  X  X  likely attention over the video v i
Now we can construct a personalized online content rec-ommendation algorithm based on the acquired and predicted user attentions for individual users. To experiment with ou r algorithm, we have developed a prototype web search inter-face which consists of a client side for acquiring the gaze samples of individual users on different materials, i.e., do c-uments, images and videos, and a server side for producing a personalized online content recommendation based on the prediction of users X  attentions on various types of materia ls. On the client side, the acquisition method mentioned in Sec. 4 is employed. The client side periodically sends the captured user gaze sample records to the server side. The server side implements a search engine using Java. When the server side application receives a search query submitted by a user, the application will forward the query to a commercial search engine and fetch the first 300 records if they have not been previously downloaded locally. In the case of documents and images, the commercial search engine we use is Google. In the case of videos, we use YouTube as the search engine. Our search engine then predicts the user attention over each such record through the methods intro-duced in Sec. 5, if the attention of the user over the record is unknown. In designing our algorithm, we also take advan-tage of the existing ranks over these materials as produced by the commercial search engine. More concretely, we use the following equation to compute a normalized user atten-tion offset, whose range is between 0 and 1: where rank ( i ) denotes the rank of the material i among the 300 items retrieved by the commercial search engine. We choose such a function because it tentatively converts an item rank into a list of attention records where items ranking low in a list would receive significantly less attent ion. The parameter  X  d controls how sharp this dropoff is, whose typical value in our experiment is set as 0.2. Once a user U  X  X  attention AT ( i, U j ), either from sampling or prediction, and the attention offset AT offset ( i ) are known for the i -th material, we can derive the overall attention of U j over i simply as: The parameter  X  overall is a user tunable value moderating how much he would prefer the user oriented rank result to preserve the rank produced by the commercial search engine. Finally, our algorithm recommends online materials by re-turning a list of these items according to their respective overall user attention in descending order.

We have also implemented an automatic mechanism which sets  X  overall to a low value when there are relatively few samples in the user attention training set and gradually in-creases the value of  X  overall as the number of user attention training samples increases. The shows that our algorithm is a learning based method. However, initially, when the training set is small, like all the learning based algorithm s, our algorithm suffers from the cold start problem and tends to produce inferior results. Thus we need to  X  X orrow X  the commercial search engine X  X  item rank list while there is lit -tle data to be learned from at the beginning. In our current experiments we use the Sigmoid function to automatically vary the value of  X  overall with the input of the function to be the number of documents or images or videos in the training set multiplied by a constant (typically set to 0.1). A final note regarding (10) is that the term AT offset ( i ) is uniform for all the users, which is not personalized and is produced by the commercial search engines; the intermediate user at-tention term AT ( i, U j ) and the eventual user attention term Figure 1: Plot of 15 personalized image recommen-dation experiment results.
 Figure 2: Plot of 10 personalized document recom-mendation experiment results.
 AT overall ( i, U j ) are produced by our algorithm, which both are personalized for individual users. Because of these per -sonalized user attention predictions, our algorithm can ge n-erate personalized online content recommendations.
We conducted our experiments for document, image and video recommendations respectively. In each experiment, the user is asked to read, browse or watch the first few doc-uments, images or videos returned by Google or YouTube on the respective search queries. After that, he is asked to pro -vide a rank list which reflects his interests, i.e., his expec ted ideal ranks over these online materials. We then use our per-sonalized online content recommender algorithm introduce d in this paper to generate a personalized recommendation lis t with the user attention data after he has read, browsed or watched the first i items, namely the personalized item rank list after our algorithm has access to user attention data on the first i items. We compare both Google or YouTube ranks for these items and the item ranks produced by our al-gorithm with respect to the user supplied groundtruth ranks .
Figures 1 X 3 show the results of our personalized docu-ment, image and video recommendation experiments respec-Figure 3: Plot of 20 personalized video recommen-dation experiment results. For easy viewing, we plot the 1st to the 10th experiment results in (a) and the 11th to the 20th experiment results in (b). tively. First, in Figure 1 we show 15 results for personalize d image recommendation, from an experiment involving five users. Each time a user is asked to look through the first four pages of image search results returned by Google Im-age Search, i.e., the top sixty image search results. After t he user has browsed the first, the first two, and the first three pages of image results, our algorithm produces the person-alized ranks for these images, respectively. We also ask the users to identify the images relevant to their search intere st after the completion of the respective image search experi-ments. This information is used as groundtruth data to tell how well the Google Image Search and our algorithm per-form in recommending images to Internet users. In the fig-of those interested images to the user in the image ranking produced by Google as well as by our algorithm after the user has browsed the first, the first two, and the first three pages of images respectively. The smaller the average rank held by these user interested images, the earlier they appea r in the image search result list, which indicates a better im-age recommendation. In Figure 2, we show the results of our personalized document recommendation experiments. Each experiment is conducted by a different user under the same setting. Here we report the errors of each document rank with respect to the user provided groundtruth document rank. Notice that it is the user who conducts the document search experiment that provides his most desired document rank at the end of the respective experiment. In the figure, Rk
G is the error of the initial Google rank; Rk i is the error of the document rank produced by our algorithm after the user has read the first i documents. In all the experiments, the user is asked to read 20 documents. In computing the error, we associate the weights of { 0.9, 0.9, 0.9, 0.9, 0.7, 0.7, 0.7, 0.7, 0.5, 0.5, 0.5, 0.5, 0.3, 0.3, 0.3, 0.3, 0.1, 0.1, 0.1 , 0.1 } with ranking errors of these 20 documents respectively so that ranking errors made with the documents appearing ear-lier on the recommended document list are more emphasized because they are the most important ones for a user. Fig-ure 3 reports some experiment results for personalized vide o recommendation, which are conducted in a similar setting to the above document recommendation experiments except this time users are asked to watch twenty videos. In the fig-ure, Rk Y is the error of the initial YouTube video rank; Rk is the error of the video rank produced by our algorithm after the user has watched i videos. We also employ the same method to evaluate video recommendation errors with respect to the user provided groundtruth recommendation using the weighted sum of ranking errors as explained above.
In conclusion, by the results of the experiments above, we confirm that our personalized online content recommen-dation algorithm can indeed produce online content recom-mendations that are more reflective of the user X  X  interest and preference. We expect one can save significant searching time and enjoy improved web surfing experience by adopting our proposed personalized online content recommendation algorithm.
In this paper, we propose a new personalized online con-tent recommendation algorithm based on acquiring individ-ual users X  attention over their previously read documents, browsed images or watched videos and then predicting the users X  attention over materials they have not seen through a data mining process. Due to page limit, we are only able to report some of the experiment results we have obtained. Nevertheless, the reported statistics still clearly show t hat our new algorithm can satisfactorily produce a personalize d online content recommendation which is in better agree-ment with the user X  X  expectation and preference, as verified through comparison against the benchmark algorithms by Google and YouTube. Also, having validated the domain specific prototype recommender system we have developed here using empirical results, we hope we have demonstrated the potential of employing commodity eye-tracking tech-niques for acquiring massive non-intrusive user feedbacks in building various types of future personalized recommender systems.

In the future, we intend to improve the precision of the image content similarity metrics by incorporating more use r feedbacks. The similarity measurement is very important for producing a quality user-oriented content recommenda-tion. In addition to exploring more existing algorithms to see whether they work well with our algorithmic framework, we also plan to study the possibility of a new similarity al-gorithm designed for an online learning setting. We also intend to strengthen the data mining capability of our algo-rithm, to optimize the performance of its implementation to better predict user preferences. Finally setting up a scala ble online personalized online recommender system for massive user evaluation would be very meaningful and commercially attractive.
 We thank Tao Jin for helping us in some of the experiments. The first author would like to thank David Gelernter for many inspiring discussions on intelligent web, from which the idea of this paper stems. This work has a patent pend-ing. [1] T. Darrell, N. Checka, A. Oh, and L. Morency. [2] F. Dirfaux. Key frame selection to represent a video. [3] Z. Dou, R. Song, and J.-R. Wen. A large-scale [4] G. Dupret, V. Murdock, and B. Piwowarski. Web [5] S. Fox, K. Karnawat, M. Mydland, S. Dumais, and [6] X. Fu. Evaluating sources of implicit feedback in web [7] D. Gorodnichy. Perceptual cursor X  X  solution to the [8] L. A. Granka, T. Joachims, and G. Gay. Eye-tracking [9] Z. Guan and E. Cutrell. An eye tracking study of the [10] W. S. A. Halabi, M. Kubat, and M. Tapia. Time spent [11] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam. [12] C.-K. Huang, Y.-J. Oyang, and L.-F. Chien. A [13] J. Huang, S. R. Kumar, M. Mitra, W.-J. Zhu, and [14] T. Joachims. A probabilistic analysis of the rocchio [15] T. Joachims. Optimizing search engines using [16] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and [17] D. Kelly and N. J. Belkin. Reading time, scrolling and [18] D. Kelly and N. J. Belkin. Display time as implicit [19] K.-N. Kim and R. Ramakrishna. Vision-based [20] Y.-P. Lin, Y.-P. Chao, C.-C. Lin, and J.-H. Chen. [21] F. Liu, C. Yu, and W. Meng. Personalized web search [22] Y. Lv, L. Sun, J. Zhang, J.-Y. Nie, W. Chen, and [23] J. Pitkow, H. Sch  X  utze, T. Cass, R. Cooley, [24] F. Radlinski and T. Joachims. Query chains: learning [25] R. Ruddarraju, A. Haro, K. Nagel, Q. T. Tran, I. A. [26] G. Salton and C. Buckley. Improving retrieval [27] M. Speretta and S. Gauch. Personalized search based [28] M.-C. Su, S.-Y. Su, and G.-D. Chen. A low-cost [29] J.-T. Sun, H.-J. Zeng, H. Liu, Y. Lu, and Z. Chen. [30] R. White, J. M. Jose, and I. Ruthven. Comparing [31] R. White, I. Ruthven, and J. M. Jose. The use of [32] S. Xu, Y. Zhu, H. Jiang, and F. C. M. Lau. A [33] P. Zielinski. Opengazer: open-source gaze tracker for
