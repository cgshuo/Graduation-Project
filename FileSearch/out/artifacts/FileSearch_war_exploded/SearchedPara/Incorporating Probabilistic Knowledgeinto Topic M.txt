 Liang Yao, Yin Zhang ( The explosion of online text content, such as news, blogs, Twitter messages and product reviews has given rise to the challenge to understand the very dynamic sea of text. To address the challenge, we need to extract the concepts from the sea of text, for the reason that  X  X oncepts are the glue that holds our mental world together X  [ 19 ] and  X  X ithout concepts, there would be no mental world in the first place X  [ 6 ].
 Most text mining tasks, especially aspects extraction tasks, use statistical ever, these unsupervised models without any human knowledge often result in coherent concepts [ 8 , 18 ].
 In this paper, we propose a new probabilistic method, called Probase-LDA, which combines topic model and a probabilistic knowledge base, in particular LDA model and Probase. The proposed method explicitly models text content knowledge base to improve the coherence of topic models. domain knowledge into topic models, studies which focus on acquiring knowledge for machines, and studies which measure the coherence of topic models. In the remainder of the paper, we first describe our framework, then do experiments on real world data sets and analyze experimental results. Evaluations show the effectiveness of our method. Finally, we conclude our work. To overcome the shortcoming of semantic coherence in topic model, especially in LDA, some previous studies incorporate domain knowledge into the LDA model. Project (ODP) or The Cambridge International Dictionary of English (CIDE). The DF-LDA (Dirichlet Forest LDA) model in [ 2 ] can incorporate knowledge in the form of must-links and cannot links input by human beings. A must-link two words should not be in the same topic.
 (MDK-LDA) which is capable of using prior knowledge from multiple domains. In [ 13 ], a more advanced topic model, called MC-LDA (LDA with must-link LDA is a model that uses must-link and cannot-link knowledge like DF-LDA, it assumes that all knowledge is correct. GK-LDA (General Knowledge based LDA) is another model that uses the ratio of word probabilities under each topic to reduce the effect of wrong knowledge [ 12 ].
 ple domains to improve topics in each domain.
 this, we incorporate large-scale probabilistic knowledge into LDA model. knowledge bases. The fact-oriented knowledge bases focus on collecting black built semi-structured content like Wikipedia. YAGO [ 22 ] and Freebase [ 7 ]are two typical fact-oriented knowledge bases. The term-based knowledge bases focus on extracting concepts, instances, and their relations from web pages by using natural language processing and information extraction techniques. The most some kind of scores to serve taxonomy inference. To measure the coherence of topic models, Mimno et al. [ 18 ]presentedan automatic coherence measure of topic models using word co-occurrence in the training corpus, which automates the human judging method in [ 8 ]. At the same time, they put forward an unsupervised method which improves the coherence score by considering the word co-occurrence in the corpus. Newman et al. [ 20 ] showed that an automated evaluation metric based on word co-occurrence statis-tics gathered from Wikipedia could predict human evaluations of topic quality. Chuang et al. [ 15 ] measured the correspondence between a set of latent topics and a set of reference concepts when applying topic models for domain-specific tasks, which provides another way to assess the coherence. 3.1 Probase: A Large Scale IsA Taxonomy Probase 1 is a probabilistic knowledge base consisting of more than 2 concepts automatically extracted using syntactic patterns (such as the Hearst  X  X bama X  is an instance of the concept president. A unique advantage of Probase is that the concept-instance relationships are probabilistic such that for each combine the probabilities of concepts and entities in given text. 3.2 Exploiting Probase Knowledge exists in human minds, given a concept  X  X ountry X , more people may think of  X  X SA X  or  X  X hina X  instead of  X  X epal X ; given an item  X  X pple X , people will treat concept  X  X ruit X  or  X  X ompany X  more important than  X  X usic track X . It looks like human beings assign a typicality score for each instance in a concept or each concept an entity belongs to, and ranked them automatically when they think make machines do reasoning like human beings.
 Formally, typicality score can be driven from co-occurrences of concept and instance pairs as follows: terns X  sentences from the whole Web documents.
 best for entities conceptualization.
 topic modeling. First, we map each sentence in the training corpus to its top concepts by using a Naive Bayes approach as [ 21 ]: P ( c used to filter out noise and introduce concept diversities when calculating the likelihood P ( e i | c k ) in Equation (1).
 0.7) defined in Probase, and get a set of concepts of the corpus. The number of concepts is usually larger than the number of topic K , therefore, we employ a k -Medoids clustering algorithm to cluster these concepts (we tried k -Medoids, and found k -Medoids performs slightly better).
 the semantic distance between two concepts c 1 and c 2 as number of clusters to be K when performing k -Medoids clustering. 3.3 Asymmetric Dirichlet Priors After concept clustering, we use a simple method to generate asymmetric Dirich-let priors. In LDA, the asymmetric Dirichlet prior  X  dk can be interpreted as a prior observation count for the number of times a topic k ment d before having observed any actual words from that document. Similarly,  X  kw is a prior observation count on the number of times a vocabulary word w is sampled from a topic k before any actual observations. Based on these assumptions, we compute asymmetric Dirichlet priors as following sub-sections. Topic-Word Priors. For each word w (entity) in vocabulary under each topic is defined as where C w is the set of top concepts w belongs to, c i is a concept in means concept c i is in concept cluster t .
 Then for each topic t ,  X  t =(  X m w 1 ,..., X m w V ), where Dirichlet prior, m w i is the normalized value under topic In Equation (8) max and min represent the maximum and minimum values of {  X  Document-Topic Priors. The asymmetric Dirichlet prior vector for each docu-ment d is computed as follows: where w di is the i -th word in document d , tf idf w di is the TFIDF value of w , We also normalize the document-topic vector as  X  d =( sent the maximum and minimum values of {  X  dt | t =1 ,...,K } 3.4 Inference bility of the topic assignment at each position: P ( z dn where w dn is the n -th word in document d , z dn is the topic assignment of w , n number of times topic k is assigned to a word in d .
 Although the equation above looks exactly the same as that of LDA, the This section evaluates the proposed Probase-LDA model and compares it with four state-of-the-art baseline models: 1. LDA [ 5 ]: The classic unsupervised topic model. 2. DF-LDA [ 2 ]: A topic model that can use the user-provided knowledge. 3. GK-LDA [ 12 ]: A knowledge-based topic model that uses the ratio of word 4.1 Dataset and Settings lections from 50 product domains crawled from Amazon.com. Each domain has 1,000 reviews. The dataset has been pre-processed by the authors, each review has been divided into sentences, and each sentence is treated as a document. sampling lag is 20. The symmetric priors of all models are set as the number of topics K = 15 (when comparing priors for LDA, we change the number of topics and symmetric priors). The other parameters for baselines were set as suggested in their original papers. When running LTM, we use test including the test domain). Since DF-LDA and GK-LDA cannot mine any prior knowledge, we feed them the knowledge produced by LTM at each iteration. For Probase-LDA, we use the top three concepts of each sentence to form concept set when performing Bayesian inference. 4.2 Topic Coherence We evaluate topics generated by each model based on Topic Coherence [ 18 ]. Traditionally, topic models have been evaluated using perplexity. However, per-and may be contrary to human judgments [ 8 ]. Instead, the metric Topic Coher-is to discover meaningful or coherent topics, Topic Coherence is more suitable topics.
 learned at different learning iterations. Each value is the average over all 50 domains. Note that the results of Probase-LDA are produced by 7 independent runs, and are basically the same. Since LDA cannot use any prior knowledge, its results remain the same. From Table 1, we can see that Probase-LDA performs the best and has the highest Topic Coherence values in general, which shows that Probase-LDA finds higher quality topics than the baselines. Both LTM and GK-LDA perform better than LDA but worse than Probase-LDA, showing their ability of dealing with wrong knowledge to some extent. DF-LDA does not perform well. Without an automated way to deal with each piece of (correct or actually worse than LDA.
 In summary, we can say that the proposed Probase-LDA model can generate better quality topics than all baseline models. Improvements of Probase-LDA are all significant ( p&lt; 10  X  6 ) based on 2-tailed paired t-test. 4.3 Human Evaluation based on human judgment. Following [ 10 ], we recruited two human judges who generated topics. Since we have a lot of domains (50), we selected 10 domains for labeling. The selection was based on the knowledge of the products of the two human judges. Without enough knowledge, the labeling will not be reliable. We labeled the topics generated by Probase-LDA, LTM and LDA, the topics generated by LTM are at learning iteration 5. For labeling, we followed the instructions in [ 18 ].
 ently related to each other representing a semantic concept together; otherwise incoherent.
 Word Labeling. The topics labeled as coherent by both judges were used for word labeling. Each topical word was labeled as correct if it was coherently related otherwise incorrect.
 The Cohens Kappa agreement scores for topic labeling and word labeling are 0.879 and 0.827 respectively.
 ing the exact number of correct topical words, a natural way to evaluate these rankings is to use P recision @ n (or p @ n )whichwasalsousedin[ 10 , 11 ]. and LDA. On average, Probase-LDA discovers 0.6 more coherent topics than LTM and 3.3 more coherent topics than LDA over the 10 domains.
 model in each domain. It is clear that Probase-LDA achieves the highest p worse than Probase-LDA. This is consistent with the Topic Coherence results in Table 1. On average, for p @5 and p @10, Probase-LDA improves LTM by 5 9 . paired t-tests shows that the improvements of Probase-LDA are significant over LTM ( p&lt; 0 . 00003) and LDA ( p&lt; 0 . 00001) on p @5 and 4.4 Example Topics This section shows some example topics produced by Probase-LDA, LTM, and LDA in several domains to give an intuitive feeling of improvements made by marked in red. From Table 2 ( X  X onitor (Color) X  means topic Color in domain  X  X onitor X ), we can see that Probase-LDA discovers many more correct and meaningful topical terms at the top than the baselines. Note that for Probase-LDA X  X  topics that were not discovered by the baseline models, we tried to find the best possible matches from the topics of the baseline models. From the table, we can clearly see that Probase-LDA discovers more coherent topics than LTM and LDA. Apart from Table 2, many topics are significantly improved by Probase-LDA, including some commonly shared aspects such as Brand and User experience . In summary, we can say that Probase-LDA produces better results. 4.5 Comparing Priors for LDA four combinations of symmetric and asymmetric Dirichlets: symmetric priors over both  X  and  X  (LDA), a symmetric prior over  X  and an asymmetric prior over  X  (denoted SA), an asymmetric prior over  X  and a symmetric prior over (denoted AS), and asymmetric priors over both  X  and  X  (Probase-LDA). The four models (LDA, SA, AS, Probase-LDA), GK-LDA and LTM are performed with different number of topics. Each model was run with { 10 , 15 , 20 , 25 , 30 } for 2000 Gibbs sampling iterations with an initial burn-in of 200 iterations, others are all the same as in experimental settings. Figure 3 shows the average Topic Coherence of each model given different number of topics. Each value is the average over all 50 domains. Results of LTM and GK-LDA are at learning iteration 3 (have stabilized). From Figure 3, we note the following: 1. AS achieves higher Topic Coherence scores than SA and LDA, this is consis-tent with the discovery in [ 23 ], which shows the advantages of asymmetric
Dirichlet prior over the document-topic distributions. 2. SA is worse than all others at K = 15 and 20, but with the increase of the number of topics K , SA outperforms LDA, and GK-LDA and LTM. This shows that when the number of concept clusters increases from 10 to 30, the effectiveness of concept clustering becomes better. 3. Given different number of topics, Probase-LDA consistently achieves higher
Topic Coherence scores than the baseline models. Among them, LTM per-
Probase-LDA, improvements of Probase-LDA over LTM and others are all significant ( p&lt; 10  X  5 ) based on 2-tailed paired t-test. 4. GK-LDA performs better than LDA at K = 15 and 20, but worse than LDA at other points, which shows that when K increases, the proportion of wrong knowledge produced by LTM also increases, and LTM has a more effective way to deal with incorrect knowledge.
 We also compare Probase-LDA to LDA when varying the concentration parameter on the priors. We fixed other parameters as in Settings part and change the symmetric priors. Table 3 shows the average Topic Coherence scores over all 50 domains, we can see that Probase-LDA consistently outperforms LDA with different symmetric priors. This paper has presented Probase-LDA, which combines topic model and a prob-abilistic knowledge base, in particular LDA model and Probase. The proposed method explicitly models text content with large-scale knowledge, could extract world datasets show the effectiveness of the proposed method.

