
Center of Mathematical modeling, University of Chile, Santiago, Chile
Department of Industrial Engineering, University of Chile, Santiago, Chile Operations Research Department, Naval Postgraduate School, Monterey, CA, USA 1. Introduction
Capturing user activity at a web site and mining this information is part of the field which has been coined web usage mining [38]. E-business motivates the field with its desire to improve web sites in order to capture more users and sales. The data commonly used for web usage mining are sessions. A session is the sequence of pages visited by a single user at a single web site for a specified length of time. Direct monitoring of a web user X  X  activity (e.g. [7]) provides an accurate session but such tracking can constitute a violation of privacy [11] and can be forbidden by law [27]. When direct monitoring is not possible, sessions must be estimated ( sessionization ).

The primary sessionization input is the record of anonymous user activity collected by each web following: the time of document (web page) access, the user X  X  IP address, the agent field that identifies the user X  X  browser, and the document retrieved. A web server log by itself does not reflect the sequence of an individual user X  X  document access because, among other reasons, many individual users can share the same IP address (e.g. use of Network Address Translation (NAT) [9]).
This paper addresses the problem of estimating individual user sessions from web logs (sessioniza-tion). For testing purposes, we were granted special permission to track the browsing activities of users on an academic web site 1 and record every individual page accessed for 15 months. This was done using time-ordered access registers from cookies. We use these observed sessions for comparing our optimization models for sessionization and a commonly-used sessionization heuristic.

Prior to our work on sessionization [12,13,33] using integer programming, different heuristics had been proposed [6,7,10,21,34,40]. In this paper, we reintroduce the prior integer program with some modifications and present bipartite cardinality matching as an efficient algorithm for computing accurate sessions. We test the accuracy of the sessions obtained by our optimization models using our 15-month data set. We also propose several variations of our integer program to provide additional insights into session characteristics. Such variations consist of obtaining the most likely session by number of copies, by a given size (number of registers), and by fixing a page in a given order. Such variations provide additional insight into browsing tendencies and are thereby valuable in web site design.
The rest of this paper is organized as follows. Section 2 provides a brief summary of related re-search. Section 3 presents our optimization models for sessionization. Section 4 outlines our test data and presents results. Section 5 shows variations of the optimization models to explore the likelihood and characteristics of specific sessions. Section 6 provides conclusions. 2. Related work
Sessionization is part of the Know ledge Discovery in Databases (KDD) process [29] for web min-ing [30]. The KDD process is a sequence of steps for extracting novel patterns from large data reposi-tories. This process is roughly divided into the following stages: data collection, pre-processing, pattern discovery, and pattern analysis. In the case of web usage mining, data collection relates to the extraction of relevant registers from web logs. Pre-processing corresponds to register filtering that is needed be-cause not all registers correspond to user actions (e.g. web search engine robots [14]). It is well known that web usage pattern discovery algorithms (e.g. clustering [18]) are sensitive to the quality of the obtained sessions [5,19,32]. This motivates the search for more accurate sessionization algorithms.
Strategies for sessionization can be classified as reactive and proactive [34]. Proactive sessionization in some countries forbidden [34], or they are legally regulated to protect user privacy [27]. Examples include cookie-oriented session retrieval [7], where personal data are stored on the user X  X  computer, from which a complete session can be retrieved. Using URL rewriting [15] to store personal information, which is finally kept in logs, is another way to track the user. The most invasive example is web-tracking software (which is close to being spyware) on a user X  X  computer (or browser), which captures the entire session [28].

Reactive sessionization has fewer privacy concerns because it only uses web server log registers that do not include explicit user information [34]. However, a web log only provides an approximate way of retrieving a user X  X  session for several reasons. The same IP address and agent as recorded in the web log often contains the requests of several concurrent users without each user being uniquely identified. Additionally, a user X  X  activation of the back and forward browser buttons is often not recorded in the web log because, in most cases, the browser retrieves the page from its own cache [13]. A proxy server, acting as an internet web page cache to reduce network traffic, can also capture web requests that are not recorded in a web log (e.g. [17]).

Prior methods to estimate sessions from a web server log have been heuristic and most commonly based on limited session duration [34] (timeout heuristic). An information retrieval method with such a characteristic is called unsupervised. These heuristics form one session at a time so that a session does not exceed a maximum duration parameter, usually 30 minutes [10]. Another heuristic approach is to reconstruct sessions that share the same semantic [24].

Several authors have looked at the overall characteristics of sessions. They find that the size n of a web user session can be approximated by a power law ( n  X   X  / X  K (  X  ) ) distribution [20,37], where  X  K (  X  )= a session is the total number of registers in the session. The parameter  X  is the decay exponent. K is the total number of registers and is interpreted as the maximum number of possible sessions. Prior work uses this empirical property as a measure of sessionization quality (e.g. [12]).

A rich literature exists on mining sessions after they are identified. Techniques such as statistical analysis, association rules, clustering, classification, sequential pattern and dependency modeling are used to discover patterns of web user behavior [23,26,35]. 3. Optimization models for sessionization
We present two optimization models for sessionization. Each optimization model considers a groups of log registers having the same IP address and agent. Each explicitly enforces the link structure of the site in any constructed session. Each also constructs all sessions simultaneously unlike heuristics that construct one session at a time.

The first optimization model is a slight generalization of our sessionization integer program (SIP) that was originally presented in [12] without any comparison to real sessions. The second optimization model is a novel use of the well-known bipartite cardinality matching (BCM) problem [2]. There are several specialized algorithms available for solving BCM with complexity O ( of nodes and A is the number of arcs (e.g. [2]). SIP does not have the same polynomial time guarantee.
Proper construction of the bipartite graph ensures SIP binary variables and BCM graph have identical feasible regions. Sessions must follow constraints based on web site topology and time ordering. Ses-sionization requires identifying each register r from the web log as part of a unique web user X  X  visit. Each constructed session is an ordered list of log registers where each register can only be used once and in only one session. In the same session, register r 1 can be an immediate predecessor of r 2 only if: the two registers share the same IP address and agent; a link exists from the page requested by r 1 to the page requested by r 2 ; and the request time for register r 2 is within an allowable time window from the request time for register r 1 .

SIP permits a general objective function while the only BCM objective function minimizes the number of sessions. An objective function that minimizes the number of sessions is of potential interest because it provides the lower limit on the number of sessions for a given log file. Such an optimal solution also has intuitive appeal because fewer overall sessions from the same web server log should result in fewer, less interesting sessions that consist of only one visited page. Sessions of size one do not reflect a web user X  X  interaction with the web site, so they are not usually considered for web usage studies. 3.1. Sessionization integer program (SIP) We present the SIP formulation in NPS standard format [8] (Sections 3.1.1 to 3.1.6).

Our SIP uses a binary variable X ros that has value  X  X ne X  if a log register r is assigned as the oth position during session s , and zero otherwise. Each index r identifies a unique register, each index s identifies a unique user session, and the index o is the ordered position of a register during a session. index is introduced as the maximum size of a session.

The SIP objective function expresses the overall reward of all constructed sessions. By altering the a significant advantage over the commonly-used timeout heuristic that has no reward mechanism but it does present the challenge of selecting these objective function coefficients in a way that constructs as many real sessions as possible. This paper suggests possible values and prior work found little variabil-ity as long as the values of the objective function coefficients increase as function of o [12]. Such an increasing function has the general tendency to reward longer sessions (session with more registers). 3.1.1. Indices 3.1.2. Data [units] Used to produce the index sets below: 3.1.3. Index sets 3.1.4. Objective function coefficients 3.1.5. Binary variables 3.1.6. Formulation
The objective function, Eq. (1), expresses the overall reward of all constructed sessions. Selecting different values for the objective function coefficients, C ro , allows the flexibility to reward different session characteristics. For example, setting C ro =1  X  r, o =3 and C ro =0  X  r, o =3 provides an objective function for maximizing the number of sessions of size three. In Section 4.4, values for C ro are based on an earlier study [12].

Constraint set Eq. (2) ensures that each register is used exactly once. Constraint set Eq. (3) restricts each session to having at most one register assigned for each ordered position. Constraint set Eq. (4) ensures the proper ordering of registers in the same session. X ros  X  X  0 , 1 } X  r, o, s declares variables as binary. To improve solution time, we can fix (eliminate) a subset of these binary variables to zero ( bpage r =  X  ).

Other parameters like T min and T max are standard in sessionization and are set to 1 and 300 seconds respectively [10] for all computational results reported in this paper. The implementation must select a maximum session size parameter and larger values can increase solution time. Because there are typi-cally only a few large sessions sizes [20], we use 30 as our default and haven X  X  observed any noticeable improvement when increasing its value. 3.2. Bipartite cardinality matching (BCM)
The second optimization model we present for sessionization is bipartite cardinality matching ( BCM ) (e.g. [2]). The BCM problem consists of finding a matching of maximum cardinality in a bipartite undi-rected network. A bipartite graph consists of two sets of nodes with edges only connecting nodes in different sets. The matching of maximum cardinality is the set of edges such that each node is connected by at most one edge. We construct the bipartite undirected network from our web server log so that the matching of maximum cardinality is equivalent to minimizing the number of sessions. This is equivalent
In our network, each register is represented by two nodes, one on the from side (representing an immediate predecessor) and one on the to side (representing an immediate successor). Figure 1 shows a six-register example. On each side, we order the nodes (registers) in order of increasing time as recorded in the web server log. An arc exists from a node on the from side, r 1 , to a node on the to side, r 2 , if the register corresponding to r 1 could be an immediate predecessor of r 2 . The definition of what is a  X  X redecessor X  of r is the same as defined for set bpage r (see Section 3.1.3). For the example in Fig. 1, we assume seven arcs exist.

Given a solution of the BCM problem, we construct the sessions from the matching. A node on the from side that is not matched is the last node in a session. A node on the to side that is not matched is the first node in a session. A session follows the connected segments in the matching where (to aid in visualizing the sessions) we add directed arcs (from the to side to the from side) connecting nodes representing the same register. Figure 2 provides a solution to the example of Fig. 1. Nodes 4 and 6 end sessions, nodes 1 and 4 start sessions, which results in two sessions (1  X  2  X  3  X  5  X  6 and 4). 4. Test data and results
In this section, we compare results from BCM, SIP, a timeout heuristic and real sessions obtained by cookies. We use five sub-sites from the University of Chile Industrial Engineering Department web site (http://www.dii.uchile.cl), where permission was granted for tracking web user activities. These corre-spond to the main department site, three sub-sites from a master X  X  degree program, and a project web site. These sites consist of nearly 4,000 web pages. Each site has its own unique content and struc-ture without a unique framework for the format and design of the whole web site. Only one uses a content-management system that standardizes content addition. The others require manual insertion of new content. The main topics on these web sites include: general information about the Industrial Engi-neering Department; general information about faculty and staff; descriptions of the undergraduate and graduate programs; and news and information about upcoming events and conferences.

These sub-sites have a relatively simple construction consisting only of static HTML pages without relevant flash animation. They contain information about programs of study, academics, projects, and news. In a typical month, about 5% of the links are modified, 2% of the pages are new or deleted, and 30% of the words change their frequency of appearance. Most of the changes are on pages with news stories that are updated weekly (Churn and Scroll updating [31]). Based on these observations, we divide web logs by month and assume a static web structure (links and pages) during a month. 4.1. Web site and web log characteristics
We record the hyperlink structure of the web site by month. The average number of hyperlinks on the web site is 4,058 with a mean absolute difference per month of 109 (2.7%). We also record the number of pages each month. The average per month is 691 different pages with a mean absolute difference about of 15 pages per month (2.2%).
 The distribution of the number of links per page (Fig. 3) is an indicator of possible session variability. In this data set, there are on average 16 links per page, providing an estimated 16 L unique sessions of size L .
 The IP address is a commonly used attribute for separating sessions [10]. We find that only a few IP addresses account for the vast majority of all registers. Over 98 percent of all IP addresses (nearly 150,000) have less than 50 registers for the period of study. Figure 4 displays the number of registers for the 100 IP addresses that account for the most registers.
We also determine how many pages are visited by users from the same IP address. This provides some evidence of the diversity of navigation patterns from a given IP address. Figure 5 shows the number of different pages requested by the first 2,000 IP addresses that account for the greatest number of different pages requested. Of the IP addresses not shown, almost 65 percent visit three or less different pages for an entire month. 4.2. Data pre-processing
We retrieve 15 months of sessions using cookies [7]. The web site was modified to track web user navigation and recover sessions without personal information such as usernames or other personal iden-the agent field, and the accessed web page.

Each web page in the site included a JavaScript program for tracking each individual X  X  navigation actions (Fig. 6). This program uses both client-side and server-side elements to automatically collect the sessions. First, when a user starts navigation, this script sets a session identifier based on a randomly-generated number and stores it on the user X  X  computer using a cookie. This cookie updates the session and stores the collected session on a remote server. A session database collects accessed registers (time, IP address, agent field, web page) including a session identifier.

The cookie-based sessionization method has remarkable advantages regarding the automatic identi-fication of web user sessions. Its drawbacks, in addition to privacy [11], are related to its dependency on the  X  X nload X  and  X  X nbeforeunload X  JavaScript event used to identify when a user enters or leaves a page. Different web browsers handle and execute these functions differently resulting in registers with incorrect insertions or missing values. In addition, a cookie should record both the entering event (IN) and leaving event (OUT) of a page but this doesn X  X  always take place. To fix any issues, registers were processed to consider any IN or OUT event on different pages to be different registers.

Data collection started in June 2009 and finished in August 2010. Collected data was stored using the structure shown in Table 1. For the 15 months, 1,224,812 rows were inserted into the table, containing 382,047 sessions, 121,968 different IP addresses and 1,227 different web pages. Cleaning was necessary to improve data quality. It included deletion of URL s referencing non-web page documents (images or files) and irrelevant frames, as well as deletion of records from robot crawling activity.
After processing, a clean data set was obtained with a total of 708,007 rows, consisting of 360,748 sessions from 114,041 different IP addresses and visits to 1,192 web pages. Although there was a sig-nificant reduction in the total number of records (42.1%), it did not substantially alter a number of key components: only a 5.6% reduction in the number of web user sessions; only a 6.4% reduction in the number of IP addresses; and only a 2.8% reduction in the number of web pages.

Figure 7 shows the number of registers and sessions by month. As a rule of thumb, the number of sessions is on the order of half of the number of registers. There is clearly less activity during the summer months of January and February. The number of sessions varies little from June to November.
Consistent with prior research [20,37], we find the distribution of session size appears to follow a power law distribution known as the  X  X eb Surfer Law X  with little variation from month to month. It has a good linear approximation in logarithmic scale. More precisely, the distribution has a better piecewise linear fit (Fig. 8) where shorter and longer sessions show slightly different behavior. 4.3. Performance measures
We consider the sessions identified by cookie extraction to be the real sessions and propose several measures of comparison. The most aggregate performance measure is the total number of sessions. Of course, it is possible to find the same number of sessions without having identified a single real session so we use several other measures.

We adopt precision and recall as in [4,36], with exact matching of real sessions for constructing pre-cision and recall measures [7]. Let C be the set of all real sessions, M X  X  the set of sessions matched exactly, and S the set of all sessions constructed by sessionization. In this case, precision and recall are defined by Eqs (6) and (7).
The harmonic mean F = 2 pr p + r is called the F-score [4], taking positive values less than one, and where a higher score is better. The F-score only counts sessions that exactly match without any reward for partial matches.

Berendt et al. [7] propose an additional measure (an overlap measure) to count the degree of overlap between real and constructed sessions. As they define it, the degree of overlap between a real and con-structed session is the number of registers in common divided by the total number of registers in the real session. Sessions are compared by selecting pairs of maximal similarity. A final overlap measure is the total number of exactly matched registers from all pairs divided by the number of real registers.
Zhang et al. [40] propose the S-measure of similarity. The S-measure between two sessions is the intersection of the pages of the two sessions divided by the union of their pages. As with the overlap measure, sessions are compared by selecting pairs of maximal similarity and the final S-measure is the average over all pairs. One is the best possible value for the F-score, overlap, and the S-measure. 4.4. Results
We construct sessions using SIP (Section 3.1), BCM (Section 3.2), and a commonly-used timeout heuristic for all 15 months where we have obtained the real sessions. We report results by month, session size, and aggregated. All computation was done using a two core 1.6 Ghz PC with 2 Gbs of RAM. We generate BCM and SIP instances using GAMS [16] and solve them using CPLEX version 10.1.0 with default settings [22], controlled by a php script and MySQL 5.0.27 (e.g. [39]) as a data storage engine. 4.4.1. Reducing SIP solution time
It is easy to construct instances of SIP that cannot be solved in reasonable time. For example, a web server log of 100,000 registers, allowing a maximum of 5,000 sessions, and a maximum session size of 20 produces 10 10 binary variables and even more constraints.
 Fortunately, pre-processing allows us to partition the set of web log register R = { r } into M chunks R = M another. Thus the integer programing problem of finding sessions in R is reduced to an M equivalent sub-problems, each one restricted to a subset of registers C k .
 sorted by time, except for the first register where P ( r 0 )= r 0 . T max is the maximum time allowed between registers in the same session. Such division into chunks is equivalent to the original undivided problem as long as no register in one chuck could ever be part of a session in another chunk. We avoided making a chunk too small because there is a fixed time (overhead) associated with generating and solving each chunk. For our computational work (SIP), we used a minimum chunk size of 50 registers and T max = 300, resulting in between 6,000 to 7,500 chunks per month. 4.4.2. SIP processing
For SIP comparisons in this section, we set C ro =3 / 2 Log ( o )+( o  X  3) 2 / 12 o  X  r as this was found to work well in previous studies [12]. Results for other SIP objective function coefficient values are presented in the next section (Integer programming extensions).

Using GAMS and CPLEX, SIP requires about 7 hours to solve a given month. There is little difference in solution time or instance characteristics from month to month. As an example, for May 2010 there are 76,019 different SIP instances (chunks) (with T min =1 , T max = 300, and a maximum session size of 30). These instances range in size from about 12,000 to 942,000 binary variables and 11,800 to over 783,000 constraints.

For each SIP instance, we set the maximum time limit to 300 seconds and the relative gap to one percent. The 300 second (non-optimal) limit is reached in less than 30 instances (or chunks) each month. Additional solution time for these 30 or so instances had little impact on the prescribed sessions. These few chunks correspond to over 30% of the total computation time. Figure 9 shows the solution time as a function of the number of binary variables (up to 500,000 binary variables) for May 2010. 4.4.3. BCM processing
We solve BCM using the CPLEX linear programming solver. Each month takes, on average, only 33 minutes to solve optimally. 4.4.4. Timeout heuristic
For the timeout heuristic, each month takes, on average, a little more than one minute to solve. The heuristic is straightforward, because it considers filtering by IP and agent fields and separates each session by a timeout condition of 30 minutes [6]. 4.5. Comparison of sessionization methods
Figure 10 shows the total number of sessions found per month using the three presented methods and the total number of real sessions obtained by cookies. SIP and BCM underestimate the total number of sessions every month while the timeout heuristic overestimates the number of sessions. BCM and SIP show nearly the same number of session (on average they find only 14% fewer sessions than the total number of real sessions). The timeout heuristic on average overestimates the total number of sessions by 37%.

Table 2 summarizes results for SIP, BCM, and the timeout heuristic. Both SIP and BCM identify nearly 25% more real sessions (precision) than the traditional timeout heuristic, with SIP performing only slightly better. Both SIP and BCM have about a 13% better F-score when compared to the heuristic. The heuristic performs slightly better for the overall recall measure however by session size, we see that both SIP and BCM dominate the heuristic for all session sizes greater than one (Fig. 11). Figure 12 shows the F-score by month. SIP and BCM clearly and consistently outperform the heuristic. SIP is usually slightly better than BCM, but at the cost of significantly more computation time. The F-score for both SIP and BCM is better than the heuristic over a wide range of session sizes (Fig. 13), from a 16% improvement in F-score for sessions of size 2 to 12% for sessions of size 16. For sessions larger than 16, the optimization models do not always dominate, but performance is typically better. Furthermore, SIP has slightly better performance for the more numerous shorter sessions (less than 10), while BCM does better for larger sessions.

The S-measure for BCM of 0.8511 (Table 2) is better than the SIP S-measure of 0.7515 and signifi-cantly better than the timeout heuristic of 0.6979. Not only does BCM have a better total S-measure but it is better for every session size greater than 1 (Fig. 14).

BCM and SIP have almost identical overlap measures [7] by month (Fig. 15). Both SIP and BCM have nearly 20% more matched registers than the timeout heuristic. 5. Integer programming extensions We develop variations of our optimization models to further explore the likelihood of specific sessions and characteristics of sessions. Specifically, we find the maximum number of copies of a given session, the maximum number of sessions of a given size, and maximum number of sessions with a given web page requested in a given position for each session. All results presented in this section are using the data retrieved for May 2010. 5.1. Finding the maximum number of copies of a given session
The set of sessions can be ranked by its maximum number of copies. The interpretation for this ranking is to have the highest number of most-likely sessions that can be obtained from the web log. Such most-likely sessions have interest for web designers because these web user sessions can be used for generating shortcuts that are more likely to be used.

It is well known that web designers want to minimize the number of clicks to reach desired infor-mation. The maximum number of sessions of a given size can reveal how likely it is for users to reach information within a fixed number of steps.

The most likely set of sessions that have a common page in a given position has an important e-commerce application. The common page could be set as a buying action or intention. Using such information, web sites can be improved so that this page is reached in a smaller number of steps. This result could be implemented by offering a shortcut hyperlink to users that follows a given sequence of web pages obtained by this method.

To find the maximum possible number of copies of a given session from the registers of the web server log, we have two cases. When each page in the session is visited only once, this can be modeled as a maximum flow problem (e.g. [2]). The maximum flow problem seeks a solution that sends the maximum flow through a network from a source node to a sink node. We construct the network with a node for each register that corresponds to a page of the session, a source node and a sink node. An arc exists: from the source node to each node that corresponds to a potential first page in the session; between any two nodes where one can be an immediate predecessor of the other for the session; from each node that corresponds to the last page in the session to the sink; and from the sink to the source. The arc from the sink to the source has unlimited capacity, while all other arcs have an upper capacity of one.
We have a network with side constraints when one or more pages in the session repeat. The network is much like the network where each page is visited only once; we have a source node and a sink node, but we must now also keep track of the node X  X  order in the session. For each position in the session, we have a node for each register that corresponds to the page occurring in that order for the session. Therefore, we replicate a node corresponding to the same register the exact number of times its page repeats in the nodes corresponding to the same register, to be set at less-than-or-equal-to one.

By maximizing the number of copies of a given session, we find the maximum number of times the session could occur (Fig. 16 and Table 3). This provides us some additional guidance on how likely the session is to occur. In Table 3, we provide these values in the  X  X ax X  column. For sessions of size four, we see there are only a few sessions that can possibly occur very frequently. Specifically, only seven sessions could occur 20 or more times for the entire month. Only 17 sessions could occur more than 10 times for the entire month. We see that both BCM and SIP find session 1 to be most frequent (41 times), but after that differences occur. Session 18 (not shown) is the next most frequent session for SIP (with six sessions) but it is only the seventh most frequent session for BCM (with three sessions). Session 6 doesn X  X  occur at all in SIP and only one time in BCM.

It takes on average about one second to solve for a given session and chunk. 403 chunks per 80 sessions takes a total of approximately 9 hours. All instances are solved to optimality. 5.2. Maximum number of sessions of a given size We find the maximum number of sessions of a given size. Specifically, for a specific size session, C ro =0  X  r, o = size ,and C ro =1  X  r, o = size . Results for size two to ten are shown in Table 4. We see that relatively few sessions of size six (or higher) are possible. Note that the maximum number of sessions of size one is the number of registers. 5.3. Maximum number of sessions with a page in a fixed position  X  X hat are the most likely second or third pages in a session? X  is a question Web site designers often ask. Table 5 and Fig. 17 show results obtained by the SIP variation, along with BCM and SIP (with C ro =3 / 2 Log ( o )+( o  X  3) 2 / 12 o ) results for the third position. There are only four pages that could have been the third page visited in 100 or more sessions and only 19 pages that could have been the third page visited in 30 or more sessions. The BCM and SIP sessions have only some minor differences when compared to each other. Solution time is about 80 hours, an average solution time of under 5 seconds per instance. No instance reached the 300-second time limit. 6. Conclusion
We present a new approach for sessionization using bipartite cardinality matching (BCM) and integer programming (SIP). We test our approach using real sessions retrieved from an academic web site over 15 months. We compare real sessions, results obtained by our optimization models, and results from a commonly-used timeout heuristic. We find our optimization models dominate the timeout heuristic using several comparison measures. For example, SIP has precision of 77.9%, BCM has nearly the same precision 77.8%, and the timeout heuristic is a distant third with only 50.9%.

We also provide variations of our optimization models to further explore the likelihood of specific sessions and characteristics of sessions. Specifically, we find the maximum number of copies of a given session, the maximum number of sessions of a given size, and maximum number of sessions with a given web page requested in a given position for each session. Acknowledgments
This work has been partially supported by the Chilean Millennium Institute of Complex Engineer-ing Systems (ICM: P-05-004-F, CONICYT: FBO16) and its associated Engineering System Doctoral program, also FONDEF project D10I-1198 entitled,  X  X HALE: Web Hypermedia Analysis Latent Envi-ronment X . We thank Naval Postgraduate School Professor Matt Carlyle for sharing his network modeling insights.
 References
