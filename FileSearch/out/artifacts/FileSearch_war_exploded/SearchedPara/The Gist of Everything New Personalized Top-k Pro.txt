 Web 2.0 portals have made content generation easier than ever with millions of users contributing news stories in form of posts in weblogs or short textual snippets as in Twit-ter. Efficient and effective filtering solutions are key to allow users stay tuned to this ever-growing ocean of information, releasing only relevant trickles of personal interest. In clas-sical information filtering systems, user interests are formu-lated using standard IR techniques and data from all avail-able information sources is filtered based on a predefined absolute quality-based threshold. In contrast to this restric-tive approach which may still overwhelm the user with the returned stream of data, we envision a system which con-tinuously keeps the user updated with only the top-k rele-vant new information. Freshness of data is guaranteed by considering it valid for a particular time interval, controlled by a sliding window. Considering relevance as relative to the existing pool of new information creates a highly dy-namic setting. We present POL-filter which together with our maintenance module constitute an efficient solution to this kind of problem. We show by comprehensive perfor-mance evaluations using real world data, obtained from a weblog crawl, that our approach brings performance gains compared to state-of-the-art.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Search process ; H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Indexing methods ; H.4.m [ Information Systems ]: Miscel-laneous  X 
This work is partially supported by NCCR-MICS (grant number 5005-67322), the FP7 EU Project OKKAM (con-tract no.ICT-215032), and the German Research Founda-tion (DFG) Cluster of Excellence  X  X ultimodal Computing and Interaction X  (MMCI).
 Algorithms, Experimentation Top-k Query, Information Filtering, Data Streams
The world has turned into one large-scale interconnected information system with millions of users. With the advent of Web 2.0, yesterday X  X  end users are now content generators themselves and actively contribute to the Web. Each user action, for example uploading a picture, tagging a video, or commenting on a blog, can be interpreted as an event in a corresponding stream. Given the immense volume of this data and its vast diversity, there is a vital need for effective filtering methods which allow users to efficiently follow personally interesting information and stay tuned.
Currently popular methods place the filter on the data sources: mechanisms such as RSS and atom are used to notify users of newly published data on their favored we-blogs or news portals. However, with the currently avail-able functionalities, users can only decide to be notified of new posts on certain blogs or follow certain other users as in Twitter. This limits the number of subscriptions users make, as otherwise the amount of received information will be overwhelming for human processing. On the other hand, traditional information filtering systems [5, 27], aggregate all available information sources and allow users to specify their interests as profiles. Given a similarity measure be-tween the data and the profiles, only data which passes a certain quality-based threshold is returned to the user. Al-though this diversifies the returned results as opposed to the previous method, it can easily result in flooding the user with returning too many data. Choosing a suitable threshold to avoid overwhelming the user or returning very few results is very hard due to the ever changing nature of incoming data. This calls for a system which deems relevance as rel-ative to the existing pool of information [21], as opposed to absolute relevance. Furthermore, to account for the desire of consuming new information and to prohibit repeatedly returning highly relevant, but old information, data is con-sidered valid for only a certain time interval, controlled by a sliding window. Note that in the context of Web 2.0, all information come with explicit temporal annotations e.g., written at , uploaded at , which makes them natural items of a temporal stream; therefore the definition of a sliding win-dow is meaningful. The dynamism introduced as a result of considering relevance relative in a frequently changing infor-mation pool, as well as the scale of our envisioned system in handling huge number of users, poses fundamental new challenges which were nonexisting previously.

As an illustration, emphasizing the importance of the ad-dressed problem, consider a small scale case where 100 , 000 profiles are maintained at a single server. The naive ap-proach consists of evaluating every profile against the in-coming documents and re-evaluating the profiles upon re-sult expiration from scratch. According to our experiments, these operations, disregarding the cost for indexing the doc-uments, i.e., removing stopwords, calculating TF/IDF val-ues take on average, orders of tens of milliseconds per docu-ment on a quad-core Intel Xeon CPU E5530 @2.4GHz ma-chine. This means that the maximum supported rate of in-coming documents would be in order of hundred documents per second which is relatively small, given that today only in Twitter, around 600 messages are produced per second 1
We consider a stream S of documents where each doc-ument is uniquely identified and consists of a weight vec-tor, as in classical Vector Space Model, as well as its arrival time: d =  X  id,time,d  X  . Assuming m distinct terms avail-able for content identification, d is an m -dimensional vector d = ( w 1 ,...,w m ), where w i is the weight assigned to the i -th term. Terms which do not appear in the document have a zero weight. Any of the usual scoring schemes such as the TF/IDF methodology can be used for assigning the weights. We further assume in-order streams; items arrive in the same order that they are generated. In most streaming scenarios, as well as ours, recent items are of more inter-est than old ones. This is captured by the sliding window model. A sliding window ( W ) is assumed over the stream and items are considered valid while they belong to this win-dow. Sliding windows can be either count or time based, i.e., bounding the number of items either by count or focusing only on those that occurred in a particular time interval. Our solution can be applied to both types.

Similar to web search, we assume user interests, called profiles , are expressed as sets of terms with corresponding weights: We denote a profile by p = ( u 1 ,...,u m u i specifies the importance of the i -th term to the user. The relevance of a document to a profile is determined by a scoring function: sim ( d,p ) = g ( f w 1 ( u 1 ) ,...f make the following assumptions regarding g and f :
Note that taking the f functions as multiplication and g as summation, we arrive at the widely used cosine measure as the scoring function for normalized vectors.

We consider a main memory model and treat each pro-file as a top-k query. All queries should be continuously monitored to keep the users up-to-date while the valid pool of information changes due to arrival of new documents or expiration of old ones. This goal involves two main tasks: first is efficient and scalable profile filtering in order to avoid http://blog.twitter.com/2010/02/measuring-tweets.html comparing an incoming document against the large set of all existing profiles. Second is maintaining the top-k results of each profile as the window slides and some documents be-come invalid. In both tasks we focus on efficiency which is a necessary step towards ensuring scalability.
In this paper we make the following contributions. 1 . We design an efficient profile filtering algorithm, POL  X  2 . We use a skyline based method for result maintenance,
This paper is organized as follows: Section 2 presents the related work. Section 3 briefly describes the general struc-ture that we consider in this paper together with a baseline algorithm. Section 4 describes an efficient algorithm for pro-file filtering. Section 5 discusses the skyline-based algorithm for result maintenance which aims at avoiding re-evaluations to high extent. Section 6 presents the experimental evalua-tion and Section 7 concludes the paper.
Our envisioned problem is the conjunction of two funda-mental areas of research: Stream Processing and Informa-tion Filtering . Information filtering or selective dissemina-tion of information is considered dual to classical informa-tion retrieval. In information retrieval, similar to our sce-nario, users subscribe to a system with their favorite profiles and receive notification whenever a relevant item arrives at the system. Traditionally relevance is defined by a fixed threshold or a boolean model is used. Setting this threshold to a suitable value is a very difficult task: a low value causes the user to be overwhelmed with the amount of results re-turned, while a too high value results in too few or no results at all. Instead we aim at returning the top-k results with regard to a constantly changing pool of data. Information filtering has been considered in vector space [5, 27], boolean [28] and more recently AWP [25] models. As mentioned in Section 1.1, we also consider the vector space model due to its popularity in use and simplicity. In [27], a profile filter-ing scheme is proposed which is based on distinguishing the significant and insignificant terms of a profile based on the given threshold. Only the significant terms of a profile are indexed and they are selected in a way that completeness of results, with regard to the given threshold, is guaranteed. Note that this method is not applicable in our setting as we do not consider a fixed threshold for returning the results. In [5] the previous method, which reduces the cost of disk I/O at the expense of larger indices, is combined with a docu-ment batching process which takes advantage of the sparsity of the profile and document matrices and writes the partial similarity matrix to disk, improving the efficiency. Callan describes a document filtering system [8] based on the in-ference network model of information retrieval. Information filtering is essentially similar to bichromatic reverse nearest neighbor search (RNN). Given a database of points P , a set of query points Q , and a similarity measure between the members of P and Q , in bichromatic RNN search, with a query q  X  Q the goal is to find p  X  P which is closer to q than any other point of Q . Most proposed methods for this prob-lem consider two dimensions. In [16] two separate R-trees are used as the index structure for RNN search. [24] consid-ers the monochromatic version of RNN in two dimensions and is based on the geometric observation that the maxi-mum number of RNN X  X  in two dimensions for a query point is 6. Singh et al. in [23] propose an approximate method for RNN search in high dimensional data which first finds the NN X  X  of a query point with the hope that its RNN is actually among them. In our setting we focus on exact re-sults, as each query represents a user who wishes to receive complete answers.

Stream processing has been a hot topic in the past few years due to its suitability in modeling large number of today X  X  applications which are not captured well with the models offered by traditional database systems (for compre-hensible surveys see [4, 22]). A related problem in stream processing is top-k query answering over sliding windows. Mouratidis et al. [20] maintain a skyline [7] which repre-sents the possible top-k candidates. Their solution is op-timized for fixed queries and they focus on changes intro-duced by items timing out or new items arriving in. We also use the concept of skylines to maintain our result sets and prove the completeness of results returned given the fact that we do not maintain the complete skyline. Furthermore, their grid-based indexing structure is not usable in our set-ting, as we are considering high dimensional textual data which will cause an explosion in the index size in a parti-tioning structure. In a more general setting, [9] proposes indexing methods for answering adhoc top-k queries based on arrangements. Again, the dimensionality of the data in their setting is low, so their solution is not applicable to our problem. Jin et al. [14] consider top-k queries on uncertain streams where the data items are associated with existential probabilities. The authors in [11] focus on top-k monitor-ing over multiple non-synchronized streams, where complete score calculations are not possible.

The closest work in the literature to ours is the recent work by Mouratidis et al. [21] which considers processing continuous text search queries. They maintain the valid documents in inverted lists and execute a threshold algo-rithm. While this is similar to our setup, the key idea in their approach is to keep the state of the TA algorithm, for each query (i.e., profile), in a per-term index organized as a tree. Upon arrival of new documents, the tree is scanned for all potentially affected profiles and in case of a change in the score of the document at rank k , the thresholds are updated upwards. In the case of the removal of old docu-ments the index lists X  scan lines will be adapted downwards. The rationale behind this continuous adaptation of the scan lines is that tight bounds cause fewer documents having to be evaluated against the registered queries. However, these scan lines are too often not a good (tight) description of the actually more interesting score at rank k , leading to the problem that many profiles have to be checked for modifica-tion with almost every incoming document. An additional effect of the the scan line based indexing is the large num-ber of potential candidates held in the resultset. We address both problems (profile indexing and result maintenance) in this work and have implemented the approach by Mouratidis et al. [21] and include it in our experimental evaluation.
Continuous k nearest neighbors (kNN) queries on data streams is also a related topic which has been considered in [17, 6]. Koudas et al. present Disc [17] for indexing high dimensional points using space filling curves to give approx-imate answers to kNN queries. On the contrary we aim at providing exact results. B  X  ohm et al. in [6] consider a fixed number of queries and index queries instead of incoming tu-ples in a structure similar to VA files [26] to continuously provide exact answers in a sliding window model. They also maintain a skyline to decide which tuples should be kept, therefore minimizing the needed storage. Essentially all the above are similar to our problem, if we consider each pro-file as a continuous query. However none of the indexing methods which provide exact results are applicable to our setting due to the high dimensionality of textual data we are considering.

Mainly motivated by the wealth of news feeds and other online information streams, another problem is Topic Detec-tion and Tracking (TDT) which has been extensively stud-ied in the past few years [3, 2, 12]. The goal is to detect new events appearing in the data stream and tracking those events to later identify data which further discuss the same event. Mining common topics in multiple asynchronous text streams is considered in [1]. In another line of research re-lated to Web 2.0 applications with temporal considerations, Hotho et al. [13] consider discovering topic-specific trends in folksonomies which are collections of resources tagged by users (such as Flickr or del.icio.us 2 ). Their analysis is per-formed offline assuming the whole corpus of data is available and is based on the PageRank algorithm. Weblog evolution is considered in [18], where time graphs are introduced and used for community tracking again in an offline mode. In [19], the goal is to identify weblogs defined as starters and followers specified by certain linking relations in an efficient way. For a survey of temporal data analysis methods see [15] and the references within.
In this section we briefly describe the general structure that we consider. As mentioned in Section 1.1 we consider one data stream as the input to our system. We aim at providing real-time continuous exact results to the users, therefore results returned as the top-k most relevant doc-uments for each profile should consist of the top-k results over all valid documents in the system at each instance of time. With small number of queries (profiles) or infrequent updates in the result set of each profile, all queries could be re-evaluated at certain times to this end. However, given large number of profiles, this solution does not scale to pro-vide real-time monitoring of all profiles X  results. Instead, we index the profiles and assess the suitability of a new docu-ment for each profile as the document arrives in the system. We avoid evaluating all profiles against a new incoming doc-uments, by a profile filtering component . The profile filtering component receives a newly arrived document as input and returns a set of profiles which should be updated with regard to this document. As the baseline, we maintain an inverted list structure in the profile filtering: For each term t we keep a list of profiles which contain this term, i.e., weight of t is larger than zero for those profiles. We also main-tain a hashtable of profiles, where profiles with a pointer to their current result set are stored. When a new docu-ment d arrives, we evaluate all entries of all inverted lists corresponding to a non-zero weighted term in d . Evaluating an entry corresponding to a profile p consists of calculating http://flickr.com and http://del.icio.us sim ( d,p ) and comparing this with the current rank k result of p which can be known by accessing the profile hashtable. If the new document has a better similarity score, p  X  X  result set is updated with it.

As the window slides, some documents expire and the set of valid documents changes. Some profiles may need to be re-evaluated as the expired documents were part of their result set. The result maintenance component is concerned with efficiently performing this task. We keep a simple time-sorted list for tracking valid documents: newly arrived doc-uments are inserted at the head and those which expire drop out from the tail. For each document we maintain the set of profiles to which this document is a top-k result. Therefore, when a document expires, the set of profiles which should be re-evaluated is known. The actual method for performing the re-evaluation is not a main concern of this paper. How-ever, we assume sorted inverted lists are maintained such that the usual threshold algorithm (TA) [10] is employed for query evaluation. Figure 1 presents the general components and structures we consider.
Similar to the baseline described in Section 3, we use an inverted index of profiles to avoid examining all the existing profiles against the newly arriving documents. In contrast to the former approach, we utilize sorted versions of these lists. As we will describe in this Section, processing these sorted lists will enable early stopping to further reduce the number of examined profiles. Our method is similar to the well-known TA (threshold algorithm) [10] which is widely used in information retrieval.

As previously mentioned, we consider each profile as a con-tinuous top-k query over the stream of incoming documents. Let p.s denote the similarity score of the ranked k document with regard to profile p . Let T = { t 1 ,t 2 ,...,t m } be the set of distinct terms considered for content identification of both profiles and documents. p.u i represents the corresponding weight assigned to term t i in p . For each term t i , we build a list l i containing  X  p.id,p.v i  X  pairs where p.v i = p.u the list is sorted in decreasing order based on v i values. p.id denotes the unique identifier of profile p . A profile with l terms will only appear in l of such posting lists.

Assume a document d with the feature vector d = ( w 1 ,...w m ) arrives in the system. We do sorted accesses in a round robin fashion to all posting lists l i where w When a profile p is seen under one of these lists, we access the profile hash table for its complete weight vector and consequently calculate the similarity score between d and p . The result set of profile p is updated if where as mentioned before sim ( d,p ) satisfies the two condi-tions described in Section 1.1.

While accessing the sorted lists in a round robin fashion, we also check the following stopping condition. For a list l let v i be the last observed value under sorted access. We stop the above procedure when g ( f w 1 ( v 1 ) ,...f w m
We call the above procedure COL-filter (Completely Or-dered Lists) and show that it is complete: all profiles for which a new incoming document serves as a top-k result are identified before the stopping condition.

Theorem 1. COL-filter identifies all profiles for which a new incoming document d is a top-k result.
 As g is monotone and the lists are sorted, reaching the stop-ping condition means that for any non processed profile p , sim ( p,d ) &lt; p.s . For a detailed proof please see the ap-pendix.

Since the number of lists which are processed could be much larger than the number of terms a profile has, we take into account the maximum number of terms a profile can have in calculating the stopping condition. Assume the maximum number of terms per profile is m 0 . The stopping condition could be checked per list, i.e., the procedure can stop processing one list while the other lists should still un-dergo the procedure. Let I be a subset of size m 0 of the lists under process. We define the following:
Also, let I j be the set of all subsets I , where I includes l The algorithm stops processing l j if If g is symmetric, i.e., its value at any m -tuple of arguments is the same as its value at any permutation of that m -tuple, it is enough to do the test for one set I max which contains the ( m 0  X  1) lists with the largest f w i ( v i ) values along with l . The general steps are shown in Algorithm 1.

While COL-filter enables early stopping and avoids ac-cessing and assessing all profiles which appear in an inverted list of a term in an incoming document, it incurs high costs for maintaining the inverted lists. Contrary to standard in-formation retrieval inverted lists, where the lists are static and change rarely, the sorted lists in COL-filter change fre-quently. This is because the values we use for sorting depend on the similarity scores of profiles which change with time, as new documents arrive or old ones expire and are removed from the system. Note that each time a profile p is updated, i.e., a new incoming document qualifies as its top-k result, Algorithm 1 : The COL filtering algorithm
ProfileFilter Input : d = ( w 1 ,...,w m ) toProcess =  X  ; toUpdate =  X  ; if w i &gt; 0 then while toProcess 6 =  X  do return toUpdate ; p.s changes. As a result p.v i = p.u i /p.s changes, there-fore p  X  X  corresponding tuples in all lists which p appeared in should be updated. As a consequence of the high dy-namism inherent in the system, which is due to the high rate of incoming documents and their expiration, the number of necessary updates can be very high. The cost of maintain-ing the lists sorted can therefore overshadow the benefits of early stops. In the following, we propose a relaxation to completely sorting the lists, which requires significantly fewer number of updates and is cheaper to maintain.
We aim at decreasing the cost of maintaining the sorted lists by grouping entries and ordering the entries only based on a fixed number of predefined boundaries instead of main-taining full order. These boundaries are then used to test the stopping condition. Our Partially Ordered List method, POL-filter , is described below.

Similar to COL-filter, we maintain inverted lists for each term t i , denoted by l i with entries  X  p.id,p.v i  X  as defined above. For each list l i we consider r groups which we iden-tify by their boundaries: b i 1 &gt; ... &gt; b ir . The entries in l grouped based on these boundaries. An entry  X  p.id,p.v i longs to the group b ij if p.v i  X  b ij and p.v i &lt; b i ( j  X  1) the second condition is assessed only for j &gt; 1. The entries inside one group are not kept sorted. To process an incoming document d = ( w 1 ,...w m ), we start with the first group b in all lists l i where w i &gt; 0 and calculate the similarity scores of profiles in these groups with regard to d . The complete weight vector of a profile can be known, when necessary, by accessing the profile hash table in constant time. A profile X  X  result set is updated with d when sim ( d,p ) &gt; p.s . We con-tinue to assess the profiles in the next group in each list only if the following stopping condition is not satisfied: In the following rounds, the algorithm reads all profiles which appear in group b j ( t +1) where j identifies the list with the largest w j b j ( t ) value and t is the last group assessed in list j . The stopping condition, replacing b j ( t ) with b j ( t +1) above equation, is assessed each time a new group b j ( t +1) processed. It is easy to see, similar to the proof of Theo-rem 1, that this procedure is complete. Note that similar to COL-filter we can use the fact that the number of terms per profile is much smaller than the number of lists which are under process and improve the stopping condition.

The update cost in POL-filter is limited to maintaining the groups in each list. Since the entries in a group are not sorted, a hash table which provides constant insert and re-moval costs could be used to add or remove entries to groups. We move a profile p from a group when p.s changes and the group membership does not hold anymore for the current group. In this case, the algorithm identifies the newly qual-ified group and the two affected groups are updated. Note that p.s can change due to arrival of a new document which qualifies as p  X  X  top-k result or expiration of a previously top-k document.
While POL-filter decreases the maintenance cost, its ef-fectiveness on early stopping depends on the selected group boundaries. If the b i values are chosen to be too big, the stopping condition is not satisfied and the algorithm pro-cesses all the profiles in a list. On the other hand if they are chosen to be too small, the algorithm may process too many unnecessary profiles before it stops. Fixing the number of groups to r , we measure the extra cost POL-filter incurs by processing unnecessary profiles and aim at minimizing it. To calculate this extra cost, we assume that the lists are sorted also inside the groups, and calculate the number of extra profiles processed in a group. For simplicity let us assume we have a single term t and its corresponding list l . We later show how our discussion is extended to multiple lists. We denote the weight of t in an incoming document with w and the entries in l by p.v . We consider r groups with boundaries b 1 &gt; ... &gt; b r . In case of a single list, if group b  X  X  profiles have been assessed, the POL-filter X  X  stopping condition is g ( f w ( b i )) &lt; 1. Let W and V denote the ran-dom variables corresponding to w , weight of t , and v values. Assume f W ( x ) and f V ( x ) respectively show the probability distribution functions of W and V . Also assume the size of list l is q and the number of documents with term t is n . The following is the cost which POL-filter incurs: where b 0 is the largest value V can have and b r the smallest value. Since we have assumed the profiles are sorted also in of times the stopping condition is satisfied for a profile in group b i  X  1 and qPr ( b i &lt; V &lt; x ) counts the number of extra profiles POL-filter reads in this case. Since POL-filter checks the stopping condition each time a new group is assessed, it reads at most  X  size of a group  X  extra profiles compared to COL-filter. The above simplifies to: b 0 and b r are fixed values (maximum and minimum values of V ). So to minimize the cost, the negative part should be maximized. While this is easy for some distributions like the uniform distribution, it is not straight forward for others. In our experiments we estimate the distributions of interest by histograms and solve the above optimization problem numerically.

In deriving the previous optimization equations, we as-sumed that only one list is under process. However, in a real scenario often several lists are being processed and the stopping condition depends on all of them. Therefore g ( f w j ( b ji )) &lt; 1 is not a good estimate of the stopping condi-tion. We treat the lists independently and use the maximum number of terms a profile can have ( m 0 ) to estimate the stop-ping condition by g ( f w j ( b ji )) &lt; 1 /m 0 instead. To account for this change in the above equations, Pr ( W &lt; 1 /x ) should be replaced with Pr ( W &lt; 1 /xm 0 ).
So far we have considered the problem of efficient pro-file filtering when a new document arrives. Another impor-tant aspect of our streaming scenario is the sliding window which specifies the valid documents. In this section we first describe the challenges caused by this temporal factor and then describe our solution.

When a document which is part of the top-k results of a profile expires as the window slides, another document from the existing valid documents should replace it. The process of re-evaluating a top-k query usually incurs high cost on the system. Given the fact that we aim at supporting a large number of profiles, this cost can slow down the sys-tem and prevent it from timely and correct responses to the users. This problem is closely related to view maintenance discussed in the database community [29].

In the following, we consider a given profile p and when we mention the score of a document or the top-k results, it is with regard to this specific profile. Let us assume a set p.R of documents is maintained for p . To avoid ever re-evaluating p over the set of all valid documents, p.R should contain all documents which have a chance of becoming a top-k result in their life time. This set consists of the top-k current results as well as documents which have a smaller score or shorter life time compared to at most k  X  1 other documents. This concept has been previously exploited in the context of continuous top-k processing in [20] and in kNN queries in [6]. For a given profile p , we consider the documents in the time/score space where score corresponds to the similarity degree of a document and p , and time presents its arrival time. A document d 1 dominates d 2 if d 1 .time &gt; d and d 1 .score &gt; d 2 .score . The k -skyband [20] of a set of points is a subset of these points where each is dominated by at most k  X  1 other points. Clearly if a document is not in the k -skyband it can never be a top-k result of p , as at least k documents with higher similarity grades and longer life times exist.

While many new documents do not qualify as relevant re-sults to a profile due to their low similarity degrees, they are part of the k -skyband as a result of their time dimension: since we are considering in order streams, all incoming doc-uments are part of the k -skyband of all profiles at the time they arrive. Such documents remain in a profile X  X  k -skyband only for a short amount of time until they are dominated by fresher, more relevant documents. Inserting each incoming document to all profiles X  k -skybands, incurs a large space overhead as well as unnecessary CPU cost to actually main-tain the k -skyband.

Algorithm 2 : The overall Algorithm for removing ex-pired documents and inserting new documents
Input : newdocs , expireddocs foreach document d  X  expireddocs do tobeUpdated =  X  ; foreach document d  X  newdocs do
To circumvent these costs, we restrict the documents which are inserted to p.R to those which have scores larger than a threshold  X  and maintain the k -skyband over them. We call this part of the k -skyband the horizon . With suitable values of  X  , the horizon is expected to be more stable, i.e. its members do not disqualify frequently, and more promising, i.e. its members are more likely to actually become a top-k result. In the following we first describe our horizon result maintenance method and then discuss suitable values of  X  .
Consider a profile p and its corresponding set of docu-ments p.R . A re-evaluation over the set of all valid docu-ments is invoked when | p.R | &lt; k and in this case p.R is set equal to the obtained top-k results. A newly arrived doc-ument d is inserted to p.R if sim ( d,p )  X   X  . When a new document is inserted in p.R the dominance values (i.e., a counter) of existing documents are updated accordingly and those documents whose dominance value hits k are elimi-nated from p.R . Note that removing a document from p.R , either due to expiration or as a result of dominance by k other documents, does not affect the dominance values of other existing documents: all documents dominated by this document should have been removed before.

Fixing  X  to a predefined static value is not suitable for our dynamic setting as an appropriate value currently may be too small or big in future with regard to the corresponding window of valid documents. A too small value would result in all documents qualifying for insertion to p.R and ulti-mately maintaining the complete k -skyband. On the other hand, a too large value causes p.R to frequently contain less than k documents and firing numerous re-evaluations. A dy-namic value for  X  which adapts to the relevance of current documents is the remedy.

Let p.R.score denote the score of the ranked last docu-ment in p.R . We show that for any value of  X  smaller than or equal to p.R.score , p.R contains the correct top-k re-sults: the top-k results in p.R are the same as the result of evaluating p over all valid documents. We also show, by a contradicting example that this is the largest value which still guarantees the correctness of the horizon. Note that the correctness concern raises due to dynamically changing  X  .
Theorem 2. Let p.R.score denote the similarity score of the ranked last document in p.R . If  X  is changing dynami-cally, the necessary and sufficient condition for p.R to con-tain the correct top-k results is that  X   X  p.R.score .
For the proof please see the appendix.
To integrate the above described horizon maintenance scheme with the proposed profile filtering algorithm, the p.s values used in calculating p.v in Section 4 should be replaced with  X  . This will ensure that the profile filtering algorithm will not miss any profiles p where a new document should be inserted to p.R , although the document may not qualify as a top-k result of p currently. The overall steps for inserting documents and updating profiles are shown in Algorithm 2. D denotes the set of valid documents. First, expired docu-ments are removed from D . d.profiles denotes the affected profiles by d : all profiles p where d  X  p.R . An affected pro-file p of an expired document is re-evaluated if | p.R | &lt; k . Then, for each of the incoming new documents the profile filter returns the profiles which should be updated with this document. Note that if document d is inserted in p.R it is not necessarily a top-k result of p , but it is part of p  X  X  horizon.
We have implemented a simulation of the envisioned sys-tem in Java 1.6. The dataset is stored in an Oracle 11g database and replayed according to the timestamps attached to the entries.
 We have obtained the ICWSM 2009 Spinn3r Blog Dataset 3 . It consists of 44 million blog posts between the time pe-riod of August 1st and October 1st, 2008. Each blog entry (post) consists of plain text, a timestamp, a set of tags, and other meta information such as the blog X  X  homepage URL. The data is formatted in XML and is further arranged into tiers approximating to some degree search engine ranking. We have parsed the blog posts for the highest tier levels resulting in 2 , 444 , 780 distinct posts. After stemming and stopword removal, we have inserted the content of each blog as  X  term,score  X  pairs in the database where the TF/IDF methodology is used for assigning weights.

Profiles are generated by looking at frequently used topic descriptions of the blog entries, such as  X  X S election X . Each profile has between 3 and 5 out of 657 distinct terms and their corresponding weights are chosen uniformly at random. We did not use one of the standard search engine query logs as subscription queries are of a more general nature. We use the well-accepted cosine measure to calculate the similarity degree between a document and profile. Note that as mentioned in Section 1.1 the cosine measure has the two necessary properties of monotonicity and homogeneity. We assume that the document and profile vectors are normalized by their lengths: | p | = | d | = 1, so sim ( d,p ) = P m We consider the following three algorithms for profile filter-ing. http://www.icwsm.org/2009/data/ Table 1: Variations of the parameters as used in the experiments.

For all the above we have two alternatives for result main-tenance: (i) using a simple top-k list and re-evaluating when-ever one of the top-k results expires or (ii) maintaining each profile X  X  horizon , as explained in Section 5.
 We have also implemented the approach by Mouratidis et al. in [21], described shortly in Section 2, which we will refer to it as incr.-thresh .

We report on CPU time as our main measure of perfor-mance. Note that we do not report on accuracy measures as all algorithms report the exact top-k results. To better understand the effects of our proposed algorithms we have measured CPU time for different parts of the algorithms, in addition to the overall time. Also for the result maintenance algorithm we are interested in the space overhead imposed by retaining more than k documents per profile.

Depending on the sliding window size, the number of doc-uments inserted in the system, ordered on their timestamps, is such that at least 5 non-overlapping sliding windows have completed. All measurements are averaged over all pro-cessed documents after a warm-up phase of 500 documents. The parameters, their default values and range of variations are shown in Table 1. All algorithms are run on a quad-core Intel Xeon CPU E5530 @2.4 GHz, 48 GB main memory, 4 TB of hard drive and Microsoft Windows Server 2008 R2 x64 as operating system. update time. Figure 3: The effect of the number of profiles on average total time. Figure 4: The effect of sliding window size on aver-age total time.
 Table 2 shows the time measurements for the default sys-tem parameters for naive, col, pol and, incr.-thresh. Total time includes the time spent for removing old documents, updating the affected profiles by re-evaluating them, profile filtering for a new incoming algorithm and inserting it to the result sets of selected profiles. To have a better understand-ing of the effect of different approaches, Table 2 shows the time spent for each of these parts separately. Note that we do not show the time which is the same for all algorithms, like the time to insert a document in the term-document inverted list, but this is included in the total time. Further-more, for incr.-thresh we only show the total time, as this algorithm does not have same separated modules for the above mentioned tasks. The first observation is that a sig-nificant portion of time (31%) is spent in the profile filtering component in the naive-k algorithm. The col-k algorithm decreases this time by almost 68% at the expense of large update time for keeping its necessary structures up-to-date. Our proposed pol-k algorithm is successful in decreasing the time spent for profile filtering as well as limiting the update cost. Note that col-k has a bigger re-evaluation time, as re-evaluating a profile causes its p.s value to change, causing updates in the inverted lists which should be kept sorted for col-k. While col-k incurs a larger total time due to its huge update cost, pol-k achieves in total 8% improvement com-pared to naive-k. The next three rows of this table show the measurements for the horizon variation of the algorithms. The re-evaluation cost decreases for all algorithms by almost 84% at the expense of a relatively small increase in the result insertion time. In the horizon variations, result insertion is more costly as it involves updating the dominance counters and k-skyband maintenance. Since with the horizon method more profiles get qualified to have a document in their result set, we observe an increase in the profile filtering time for col-horizon and pol-horizon compared to their top-k coun-terparts. However, since the ranked last document in the result set, which defines the values of interest for keeping the lists ordered, changes less frequently than in the top-k method, the update cost for these algorithm decreases sig-nificantly. In total, we observe 60% decrease in the total time, from naive-k to pol-horizon which achieves the small-est total time among all algorithms. incr.-thresh inserts all documents that are in any index list above the scan line of that profile which causes the result set to grow very large. This has the benefit of eliminating the re-evaluations, but on the downside large space is consumed and a large result set should be kept sorted which incurs extra cost. Over-all, pol-horizon has a decrease in total time of 36% together with significant decrease in the resultset size it maintains compared to incr.-thresh.

As seen previously, the pol algorithm is successful in main-taining the decrease in profile filtering time as well as lim-iting the time spent for updating the required structures. The calculations necessary for choosing the boundary val-ues mentioned in Section 4.2 are performed only once after the warm-up phase. Figure 2 presents the effect of number of groups. Figure 2 (a) shows the total time for pol and col with the top-k and horizon variations. We observe that with as small as 10 groups, pol achieves very good decrease in the total time. In Figures 2 (b) and (c) we observer the profile filtering and update times for different number of groups. pol-k incurs almost 6 times less update cost compared to col-k, at the expense of small increase in its profile filtering time. As mentioned in the previous paragraph, the hori-Table 3: Number of re-evaluations and result set sizes when changing k. w=4500(ms) and #profiles = 20000. zon variations have smaller update cost and slightly larger profile filtering time.
 The effect of number of profiles on total time is shown in Figure 3. The total time for all algorithms increases with increasing the number of profiles, as the profile filtering and re-evaluation parts become more costly. However, the effect of our profile filtering algorithms are more visible for larger number of profiles. Also note that pol does not show any significant drop in decreasing the total time compared to col, although the number of groups are fixed for all profile cardinalities to 10. This is because by using the horizon maintenance module, the update cost in col decreases sig-nificantly, as shown in Figure 2(c). incr.-thresh has much larger total time, since similar to col-k it spends a lot of time updating the index lists X  scan lines. The pol-horizon algorithm achieves the minimum total time, decreasing it by up to 40% from incr.-thresh
We report on the effect of sliding window size on average total time in Figure 4. The average total time decreases with increasing size of the sliding window for all algorithms. This is mainly due to the decrease in number of necessary re-evaluations on average. With a bigger window size, high quality documents live longer and a larger time span allows for high quality documents to arrive before others expire and fire a re-evaluation. As seen in the Figure, with large enough window sizes pol-horizon and col-horizon have similar total time which is the result of fewer updates.

Table 3 reports on the average number of re-evaluations and result set size for the top-k and horizon variations. Note that the profile filtering algorithm does not have an effect on these values so we have not repeated the results by sep-arately reporting on them. First, note that the necessary number of re-evaluations drops from 7 to almost 100 times less for the horizon method compared to top-k based main-tenance. The very interesting observation is that with in-creasing the k value, the number of re-evaluations has an increasing trend for the top-k method but a decreasing one for the horizon algorithm. This is because for larger values of k the horizon grows much larger than k , significantly de-creasing the chance of the result set containing less than k results to fire a re-evaluation. However, the horizon method comes with the extra cost of maintaining the horizon.
In summary we observe that the pol-horizon combina-tion offers significant performance gains compared to the rest of algorithms. The horizon result maintenance algo-rithm causes small decrease in the improvement pol can of-fer in decreasing the profile filtering time. However, it dras-tically decreases the necessary update cost and number of re-evaluations while incurring only a small space over head over the system. A decrease of between 25% to 30% in over-all processing time, allows our envisioned system to scale better to larger number of profiles and higher data rates.
Motivated by the tremendous popularity of blogs, micro-blogging services like Twitter, and online newspapers, we address the problem of continuously processing large number of user defined subscription queries (profiles) over a stream of documents. The challenge in processing these queries in real-time lies not only in the fact that there are many queries, but also, and foremost, in the observation that data streams in at high rates. Both properties combined call for a careful profile filtering process, that omits evaluating too many profiles. Our approach significantly reduces the num-ber of necessary profiles evaluations, by organizing the user profiles in a so called inverted index. The key idea is to sort profiles not only on their weight w.r.t. a term but also according to the quality of the currently alive documents which are ranked high for a particular profile. This sorting criteria allows for an effective stopping condition for the pro-file filtering algorithm. Furthermore, we observe that keep-ing the entire lists completely sorted is infeasible as profiles frequently move up and down in the lists. We solve this by using group sorted lists, i.e., lists consisting of different groups which are sorted relative to one another, but without order inside groups. As the definition of the group bound-aries is crucial for the overall performance gain, we present a method to select these bounds by leveraging score distribu-tion information derived from histograms. We combine our proposed filtering algorithm with an effective skyline based result maintenance algorithm which cuts drastically on the number of necessary re-evaluations caused by expiring doc-uments. We evaluate our approach using a real world blog dataset demonstrating the performance gains compared to the state-of-the-art. [1] Xiang Wang, Kai Zhang, Xiaoming Jin, and Dou [2] James Allan, Jaime Carbonell, George Doddington, [3] James Allan, Ron Papka, and Victor Lavrenko.
 [4] Brian Babcock, Shivnath Babu, Mayur Datar, Rajeev [5] Timothy A. H. Bell and Alistair Moffat. The design of [6] Christian B  X  ohm, Beng Chin Ooi, Claudia Plant, and [7] Stephan B  X  orzs  X  onyi, Donald Kossmann, and Konrad [8] James P. Callan. Document filtering with inference [9] Gautam Das, Dimitrios Gunopulos, Nick Koudas, and [10] Ronald Fagin. Combining fuzzy information: an [11] Parisa Haghani, Sebastian Michel, and Karl Aberer. [12] Qi He, Kuiyu Chang, and Ee-Peng Lim. Analyzing [13] Andreas Hotho, Robert J  X  aschke, Christoph Schmitz, [14] Cheqing Jin, Ke Yi, Lei Chen 0002, Jeffrey Xu Yu, [15] Jon Kleinberg. Temporal dynamics of on-line [16] Flip Korn and S. Muthukrishnan. Influence sets based [17] Nick Koudas, Beng Chin Ooi, Kian-Lee Tan, and [18] Ravi Kumar, Jasmine Novak, Prabhakar Raghavan, [19] Michael Mathioudakis and Nick Koudas. Efficient [20] Kyriakos Mouratidis, Spiridon Bakiras, and Dimitris [21] Kyriakos Mouratidis and HweeHwa Pang. An [22] S. Muthukrishnan. Data streams: Algorithms and [23] Amit Singh, Hakan Ferhatosmanoglu, and Ali Saman [24] Ioana Stanoi, Divyakant Agrawal, and Amr El [25] Christos Tryfonopoulos, Manolis Koubarakis, and [26] Roger Weber, Hans-J  X  org Schek, and Stephen Blott. A [27] Tak W. Yan and Hector Garcia-Molina. Index [28] Tak W. Yan and Hector Garcia-Molina. Index [29] Ke Yi, Hai Yu, Jun Yang, Gangqiang Xia, and Yuguo
Proof 1. We show that for any profile which has not been updated before the stopping condition is reached, d does not serve as a top-k result. In other words we show that for such profiles, sim ( d,p ) &lt; p.s . If p has been seen in one of the sorted lists before the stopping condition, ac-cording to the algorithm its similarity score with d has been evaluated by looking up p in the profile hash table. There-fore if p has not been updated, clearly sim ( d,p ) &lt; p.s . Now assume p has not been observed in any of the sorted lists before the stopping condition. For a list l i let v last observed value under sorted access. Since the lists are sorted in descending values, p.v i &lt; v i . As a result of this and due to f and g  X  X  monotonicity, g ( f w 1 ( v 1 ) ,...f stopping criteria. Since p.v i = p.u i /p.s and due to f and g  X  X  homogeneity, we have g ( f w 1 ( v 1 ) ,...f w m ( v m )) = g ( f w 1 ( u 1 ) /p.s,...f which is equivalent to sim ( d,p ) &lt; p.s .

Proof 2. We first show that if  X   X  p.R.score , p.R con-tains the true top-k . Let d be the valid document with the largest score which is not in p.R at current time t and p.R.score k be the score of the ranked k document in p.R also at t current . We show that sim ( d,p ) &lt; R.score We should consider two cases: first d was inserted to p.R but then removed, or d was never inserted to p.R . Since d is valid, it was removed from p.R as a result of being domi-nated by k documents which means k documents with longer life times exist which have a higher score than d . These are indeed in p.R , as we have assumed d has the largest score among all valid documents not in p.R . So for the first case sim ( d,p ) &lt; p.R.score k e . In the second case, d was never inserted to p.R . Let t 1 denote the time when the most recent re-evaluation was performed. If t 1 &gt; d.time , the most recent re-evaluation was performed after d  X  X  ar-rival. Since d was not inserted in p.R , at least k docu-ments with higher scores than d existed at time t 1 . Since | p.R | = k after each re-evaluation,  X  at after t 1 and be-fore a new re-evaluation is equal to or larger than the score of the ranked k document at time t 1 . Since we have as-sumed the most recent re-evaluation happened in t 1 , either those top-k documents have not expired until t current documents with score larger than  X  t 1 have arrived, other-wise the size of p.R would be less than k at some point after t which is in contradiction with our assumption that the most recent re-evaluation was invoked at t 1 . In both cases R.score k  X   X  t 1 &gt; sim ( d,p ). Now assume t 1 &lt; d.time : the most recent re-evaluation happened before d  X  X  arrival. In this case, sim ( d,p ) &lt;  X  d.time , otherwise d was inserted in p.R . If no re-evaluations happens,  X  can only increase, as only higher scored documents can be inserted to p.R . Sim-ilar to the previous case, either documents in p.R at time d.time have not expired yet or higher scored documents have arrived, otherwise a re-evaluation would have been fired. In both cases, R.score k  X   X  d.time &gt; sim ( d,p ) which completes the proof for correctness of results when  X   X  R.score .
To show that this is also a necessary condition, we give an example of when p.R doesn X  X  contain top-k results if  X  &lt; R.score . For simplicity let k = 2, examples for other k can be constructed similarly. Let  X  = R.score + and &gt; 0. Assume R is empty and consider the following stream of documents (first attribute shows time and the second is score with regard to the specific profile we con-sider): d 1 (1 ,s 1 ), d 2 (2 ,s 2 ), d 3 (3 ,s 3 ), d 4 (4 ,s , s 1 &gt; s 3 , s 3 &gt; s 2 . Then when d 4 arrives,  X  = s cause d 2 is dominated only by d 3 so it isn X  X  removed. Now if s 4 = s 2 + / 2, d 4 is not inserted to R . Assume no new document arrives. When d 1 expires, d 2 and d 3 are reported as the top-k results although d 4 has higher score than d
