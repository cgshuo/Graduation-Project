 Huayan Wang huayanw@cs.stanford.edu Daphne Koller koller@cs.stanford.edu MAP-MRF (finding the most probable assignments for MRFs) is one of the most important components in learning and applying structured probabilistic models. In general this problem is NP-hard (Shimony, 1994). Many different methods have been proposed to approximate or solve it under specific circumstances. A large family of these methods is based on solving a dual problem of an LP relaxation. Different duals of different LP relaxations (with different tightness) have been used. These methods are usually formulated as max-product (max-sum) message passing over the MRF or its cluster (region) graph.
 Recent advances revealed that convergent versions of these algorithms can often be interpreted as block coordinate descent (BCD) in the dual (Meltzer et al., 2009; Sontag et al., 2011). However, most of them operate on local (small) blocks, such as MPLP (Globerson &amp; Jaakkola, 2007) and its generalizations (Sontag et al., 2008), max-sum diffusion (MSD) (Werner, 2007), and TRW-S (Kolmogorov, 2006). Given a block of dual variables (messages), these algorithms work by enforcing some consistency con-straint over the block, thereby achieve dual-optimal w.r.t. these dual variables. Meltzer et al. (2009) noted that it was difficult to generalize them to larger blocks while enforcing the same consistency constraints. Sontag et al. (2009) proposed a method (tree-BCD) that updates much larger blocks for a specific choice of dual. But it was not clear how to apply it to duals of tighter LP relaxations (e.g., a dual decomposition with cycle subproblems).
 We observe that the difficulties in generalizing these methods arise from the fact that they all impose too strong consistency constraints, which are sufficient but not necessary for the dual objective to be optimal on the blocks being updated. By loosing these constraints, we are able to perform BCD on much larger blocks.
 Indeed, we show that dual-optimality on blocks (of messages) can be established on a much broader basis X  X ith quite general choices of the dual objective itself as well as the blocks to be updated. Specifically, we illustrate this by deriving a  X  X nified X  message passing algorithm in the most general setup X  X n arbitrary dual decomposition. The resulted algorithm ( subproblem-tree calibration , or STC) has the following properties: 1. It is formulated as message passing on a graph-2. It subsumes MPLP, MSD and TRW-S as special 3. It achieves dual-optimal on blocks that can be In other words, our framework attempts to charac-terize the degrees of freedom we have in designing a message passing algorithm (for MAP inference). Understanding these flexibilities could help us better understand existing methods, as well as design more powerful ones. In practice, we observe that we often get stuck in sub-optimal dual states when only updating blocks chosen in a restricted manner, whereas being able to choose blocks with more flexibility could lead to better dual (and primal) states. 2.1. MAP inference, LP relaxation, and dual The MAP inference problem over X = { X 1: N } and graph structure G = { V,E } can be formulated as: where  X  ( X ) = P  X   X  X   X   X  ( X  X  ); A is the set of MRF cliques. Without loss of generality we choose A = V  X  E for a parametrization with unary and pairwise potentials 1 . We use lowercase x i  X  V al ( X i ) and x = { x 1: N } to denote assignments to the variables. Problem (1) is NP-hard in general (Shimony, 1994). A large family of MAP inference methods builds on solving a linear programming (LP) relaxation (Wainwright &amp; Jordan, 2008; Koller &amp; Friedman, 2009):  X  is all MRF parameters {  X  i ,  X  ij } concatenated in same ordering as  X  . Choosing different polytopes M results in different LP relaxations. One choice is the marginal polytope :
M G = {  X  | X  p ( X ) , X where p ( X i ) and p ( X i ,X j ) denote marginal distribu-tions of p ( X ). When M = M G the LP relaxation (2) is equivalent to the original problem (1). Therefore it is also NP-hard X  X he marginal polytope is defined by exponentially many faces (constraints).
 A widely used choice for M is the local polytope : which only has a polynomial number of constraints, and the LP relaxation (2) with M L is tractable. In general M L is a loose outer bound of M G . Any solution to (2) with M = M L is a vertex of M L . If it happens to be a vertex of M G , we have the exact solution to (1). This is usually not the case except for several special families such as the problems with tree structures or sub-modular potentials etc.
 Solving (2) using a standard LP solver is very inefficient (Yanover et al., 2006) even with a tractable choice of M . In practice we usually solve its dual LP. Indeed, it is more convenient to start with a dual decomposition , which directly gives us a dual LP (of some underlying primal LP that we do not explicitly deal with).
 Specifically, consider a decomposition of  X  ( X ) into subproblems c  X  C , parametrized by {  X  c } , which altogether give rise to a reparametrization of  X  ( x ): where x | c denotes restricting the joint assignment to the scope of subproblem c . The subproblems could be single nodes, edges, trees, or cycles that are assumed to be tractable.
 Note that (5) has exponentially many constraints. A succinct way to enforce them is to express the reparametrization in terms of messages: where the messages satisfy  X  c 0  X  c =  X   X  c  X  c 0 . The initial subproblem potentials  X  c 0 are assumed to satisfy (5). They can be constructed, for example, by splitting the original potentials. It is easy to check that subproblem potentials defined by (6) satisfy (5) for any choice of the messages.
 Note that the scopes of messages are defined by X c  X  X 0 c , which is the intersections among X c (with a subscript): the original MRF variables covered by subproblem c . This is to distinguish from X c (with a superscript), which are subproblem variables . For example, if subproblems c and c 0 both cover X 1 (orginal MRF variable), we would have X c 1 and X c 0 1 : two different variables with independent assignments. Since each subproblem has its own copy of variables X c , summing over their optimal values gives rise to an upper bound of (1) because each subproblem is maximized independently. This upper bound is a function of the messages: It turns out that (7) is a dual problem of (2) under some choice of M . The tightness of the underlying M depends on our choice of the subproblems. It has been shown (Komodakis et al., 2011) that choosing only tree structured subproblems leads to a dual of (2) with M = M L . Duals of tightened LP relaxations can be constructed by introducing complex subproblems such as cycles.
 Given a decomposition, our goal is to minimize (7) by rearranging subproblem potentials (via message pass-ing), thereby solving the underlying LP relaxation. 2.2. Bethe cluster (region) graph In principle the messages  X  c 0  X  c in (6) can be defined between all pairs of overlapping subproblems. However, a rather restricted form has been dominantly used in defining such messages: the Bethe cluster graph. It has a bipartite structure with one layer of  X  X actor X  nodes and one layer of small (usually unary) nodes encoding intersections between the factors. Specifically, consider using the set of subproblem C = { i } X  X  consisting of unary subproblems { i } and larger subproblems (factors) F . So the dual (7) takes a restricted form:
D ( {  X  f  X  i } ) = X where the messages are only defined between the two layers because of the bipartite structure. This construction is shown in Fig. 1 (a) and (c). Note that we used superscripts (instead of subscripts) in  X  i and X i to indicate that these are subproblem potentials and variables to distinguish from the orginal MRF potentials  X  i and variables X i .
 It has been shown (Meltzer et al., 2009; Sontag &amp; Jaakkola, 2009; Sontag et al., 2011) that many existing algorithms, including (Globerson &amp; Jaakkola, 2007; Sontag et al., 2008; 2011; Werner, 2007; Sontag &amp; Jaakkola, 2009; Tarlow et al., 2011; Komodakis interpreted as passing messages on the Bethe cluster graph and thereby optimizing the dual objective (8). This  X  X estricted X  design had arisen from the historical concern of satisfying the running intersection property , thus alleviating double-counting in loopy BP (Weiss, 2000). However, this is no longer relevant under the modern view of message passing as a BCD algorithm. To this end, we will introduce a more general graph-object: subproblem multi-graph (Sec. 3.1) to serve the role of traditional cluster graphs. This more general setup allows us to pass messages in more flexible ways and achieve better dual (and primal) states in many situations (as shown in experiments). 3.1. Subproblem multi-graph and subproblem For notation simplicity, in this paper we treat graphs G and trees T as sets consisting of nodes and edges. So we will use T  X  G to denote that T is a subgraph of G . And we will use e  X  T or c  X  T to denote that edge e or node c is in the tree.
 Given a dual decomposition with subproblems C , we build a graph-object as follows: Definition 1 (Subproblem Multi-Graph/Tree) . Given C , the subproblem multi-graph (SMG) G = ( V , E ) has one node for each c  X  C , and one edge between c and c 0 for each tuple ( c,c 0 , X  ) , where  X   X  V  X  E is shared by c and c 0 . A subproblem tree is a tree T  X  X  .
 Note that we use ( V,E ) for the MRF graph and ( V , E ) for SMG.
 This construction is illustrated in Fig. 1 (b) where we decompose the MRF (Fig. 1 (a)) into four subproblems.
 Note that if we include all unary subproblems into the decomposition, we would get a SMG similar to Fig. 1 (c) but with extra edges among the non-unary subproblems. So a tree in the Bethe cluster graph (which we call a Bethe tree ) is also a subproblem tree by definition. Therefore all conclusions in this paper on subproblem trees apply to Bethe trees.
 Fig. 1 (d) to (i) are examples of subproblem trees. They correspond to blocks updated by different algorithms as explained later. Note that (d) (e) and (i) are Bethe trees corresponding to the Bethe cluster graph (c); (f) and (h) are trees in the SMG (b); (g) is a Bethe tree of a different dual decomposition (with all edges and nodes as subproblems).
 For each SMG edge ( c,c 0 , X  )  X  E , we have messages 2  X  dual variables) associated with subproblem tree T is given by 3.2. Max-consistency and dual-optimal on Given a block B T associated with some subproblem tree T , we want to achieve dual-optimal w.r.t. that block: Definition 2 (Dual-optimal on T ) . The subproblem potentials {  X  c } are dual-optimal on T if we can not further decrease the dual objective by changing messages in B T .
 A message passing algorithm achieves this by enforcing some consistency constraint . We first identify a constraint that is equivalent to dual-optimal on T . Definition 3 (Assignments agree on T ) . Assignments to all subproblems { x c } c  X  X  agree on T , denoted as { x c } X  X  , if for  X  ( c,c 0 , X  )  X  X  we have x c  X  = x c 0 Definition 4 (Weak max-consistency on T ) . {  X  c } c  X  X  satisfies weak max-consistency if That is, maximizing each subproblem independently gets to the same optimal value as maximizing them while requiring the assignments to agree on the tree. This condition turns out to be equivalent to dual optimal on T (Proposition 1). We will show that our message passing algorithm (Sec. 3.3) enforces this constraint for arbitrary T .
 We now identify two other stronger constraints (enforced by existing algorithms) that are sufficient but not necessary for dual-optimal on T .
 Let M c  X  be the (log)-max-marginals of c on  X  : Note that if  X  = ( i,j )  X  E , X c |  X  = x  X  means X c i = x and X c j = x j .
 The following consistency constraint requires the subproblem to agree on their max-marginals over the tree: Definition 5 (Strong max-consistency 3 on T ) . {  X  c } c  X  X  satisfies strong max-consistency if M c  X  M c 0  X  ,  X  ( c,c 0 , X  )  X  X  .
 Another consistency constraint requires that the subproblem potentials match the max-marginals of the tree distribution: Definition 6 (MPLP max-consistency on T ) . For Bethe tree T with N unary clusters, {  X  i ,  X  f } i,f  X  X  satisfies MPLP max-consistency if  X  i = 1 N  X  T i and  X  max-marginals 4 of the tree T , and N f i is the number of unary clusters in the subtree rooted by i on the opposite side of f .
 Note that MPLP max-consistency is only defined for Bethe trees. With a slight abuse of notation, we used i  X  f in the above definition to denote that the scope of unary subproblem i is contained in the scope of factor subproblem f in a Bethe cluster graph.
 The relations among these consistency constraints are given below.
 Proposition 1. For any Bethe tree T , MPLP max-consistency =  X  weak max-consistency For any subproblem tree T (including Bethe trees), strong max-consistency =  X  weak max-consistency. Proof for this ( as well as all other propositions in the rest of this paper ) are given in Appendix (supplement material), where we also give an example that satisfies weak max-consistency but not the other two.
 The two stronger constraints are enforced by existing methods: Proposition 2. Max-sum diffusion (MSD) (Werner, 2007) performs BCD on the Bethe dual (8). Each BCD step enforces strong max-consistency for a Bethe tree consisting of one f  X  F and one i  X  f (as in Fig. 1 (d)).
 Proposition 3. TRW-S (Kolmogorov, 2006) per-forms BCD on the dual (7) where each subproblem c  X  X  is a tree (of the MRF). Each BCD step enforces strong max-consistency for a subproblem tree T with nodes V T = { c : scope( c )  X   X  } for a given  X   X  V  X  E , and edges ( c,c 0 , X  ) that constitute a tree over V T 5 . Proposition 4. MPLP (Globerson &amp; Jaakkola, 2007; Sontag et al., 2011) performs BCD on the Bethe dual (8). Each BCD step enforces MPLP max-consistency for a Bethe tree consisting of one f  X  X  and all i  X  f (as in Fig. 1 (e)).
 Proposition 5. Tree-BCD (Sontag &amp; Jaakkola, 2009) performs BCD on the Bethe dual (8) where all f are pairwise factors ( i,j ) . Each BCD step enforces MPLP max-consistency for a Bethe tree corresponding to a spanning tree of the MRF (as in Fig. 1 (g)).
 Note that a spanning tree of the MRF (with | V | X  1 edges) is much smaller than a spanning tree of the SMG (of a dual decomposition into all edges and nodes). The latter contains all (possibly O ( | V | 2 edges of the MRF. 3.3. The STC algorithm Now we show how to attain weak max-consistency for any subproblem tree (such as Fig. 1 (h) and (i)). To express the algorithm concisely, we use  X  X  X  y to denote two updates: x  X  x  X  m and y  X  y + m . We assume subproblem solvers for all c that output M c  X  . For tree-structured subproblems this is straightforward. For cycle subproblems we use (Felzenszwalb &amp; McAuley, 2011) which provides a fast way of computing the junction-tree messages.
 Our algorithm (Alg. 1) calibrates a subproblem-tree by an upstream pass and a downstream pass; both update subproblem potentials  X  X n place X  without storing any message (although conceptually it can be viewed as updating the messages in (7) as well). The Algorithm 1 Subproblem tree calibration (STC) downstream pass of STC can be performed for each children of a node either sequentially or in parallel . They differ in line 8 and 11 of Alg. 1. We call these two alternatives S-STC and P-STC, respectively. Any statement about STC in the following, if not specified, applies to both P-STC and S-STC.
 Overall we repeatedly choose different trees and perform the calibration.
 Proposition 6. STC enforces weak max-consistency for any T and any allocation weights a c satisfying a c  X  Note that this (together with Proposition 1) implies monotonicity and convergence of the overall algorithm. We now clarify the relations between STC and existing methods.
 Proposition 7. Applying P-STC to the Bethe tree specified in Proposition 4 with allocation weights a i = , a f = 0 is equivalent to MPLP.
 Proposition 8. Applying STC to the Bethe tree specified in Proposition 2 with allocation weights a f = 0 . 5 , a i = 0 . 5 is equivalent to MSD.
 Proposition 9. Applying STC to the subproblem tree specified in Proposition 3 with uniform allocation weights a c = 1 |T| is equivalent to TRW-S.
 This implies that STC actually enforces strong max-consistency when applied to settings of MSD and TRW-S. Indeed, we have: Proposition 10. If all edges of T have same scope  X  , P-STC is equivalent to S-STC. Both achieve strong max-consistency when applied with uniform allocation weights a c = 1 |T| This condition (all edges of T have the same scope) is satisfied in MSD and TRW-S.
 Given Proposition 9, we can easily generalize TRW-S beyond tree subproblems 6 . We call this generalized algorithm TRW-S*: applying STC to a subproblem tree consists of all subproblems sharing a given MRF node or edge, as in Fig. 1 (f). It will appear in experiments. 3.4. Fixed-point characterization In this part we give a fixed-point characterization of the STC algorithm, and we show that it is in fact equivalent to the weak tree agreement (WTA) condition (Kolmogorov, 2006). Indeed, the fact that TRW-S (Kolmogorov, 2006) is a special case of STC suggests that the latter should be at least as powerful as the former.
 Definition 7 (WTA2) . Subproblem potentials {  X  c } satisfy weak tree-agreement-two (WTA2) if for any subproblem tree T ,  X  X  x c }  X  T that are optimal for each c  X  X  individually. Proposition 11 (STC Fixed-Point) . If {  X  c } do not satisfy WTA2, we can always find T such that applying STC to T decreases the dual objective. If {  X  c } satisfy WTA2, applying STC to any T does not change the dual objective and preserves the WTA2 condition. We re-state the WTA condition from (Kolmogorov, 2006) in a slightly generalized form (allowing c to be arbitrary subproblems, not restricted to trees). Definition 8 (WTA) . Subproblem potentials {  X  c } satisfy weak tree-agreement (WTA) if we can find a subset of optimal assignments for each c , denoted as OPT ( c ) , such that: for any small subproblem tree T consists of two nodes c and c 0 connected by ( c,c 0 , X  ) , we have: for  X  x c  X  OPT ( c ) ,  X  x c 0  X  OPT ( c 0 ) , Comparing to WTA, WTA2 appears to assert a weaker constraint on more general T . This originates from the fact that STC calibrates more general T and enforces a weaker consistency constraint than that of MSD and TRW-S. However asymptotically they are equally powerful: Proposition 12. WTA2 is equivalent to WTA. 3.5. Choosing allocation weights In this part we clarify the role of allocation weights a , which are responsible for the message coefficients in the downstream pass of STC. As we have seen in Proposition 8 and 7, there are subtle (but important) differences between MPLP and MSD in this aspect. (This has also been noted in (Sontag et al., 2011).) In order to understand the role of allocation weights a c we first show some detailed characterization of STC. Proposition 13 (STC Allocation) . After STC, for each subproblem c we have: Intuitively, the downstream pass allocate  X  X nergy X  to all subproblems according to their allocation weights. We can further show that: Proposition 14 (Detailed Monotonicity) . Let D 0 , D 1 , D 2 be the dual objective value before STC, after upstream pass, and after downstream pass, respectively. We have D 0  X  D 1 = D 2 . Moreover, the dual objective remains constant in each single step of the downstream pass.
 That is, the downstream pass essentially moves around in a plateau of the dual, preparing for the next downhill move. Therefore the allocation weights determine where to settle on that plateau.
 When T is a Bethe tree, two styles of allocation have been used. One is the MPLP-style allocation, which assigns zero weights to all non-unary subproblems, and uniformly allocate among unary subproblems. This is the case for MPLP and Tree-BCD 7 . The other is the MSD-style allocation, which assigns uniform allocation weights to all subproblems. This is the case for MSD and its generalized version MSD++ (Sontag et al., 2011).
 Given our framework, it is straightforward to further generalize these two styles of allocation to spanning trees of the Bethe cluster graph (as in Fig. 1 (i)). We call these two algorithms MPLP# and MSD#, respectively 8 . They will appear in experiments. 3.6. Generate primal solutions Given subproblem potentials, solutions to the original MAP inference problem can be constructed in different ways (Komodakis et al., 2011). For example, when unary subproblems are present, we could simply take the assignments that minimize each unary subproblem. Or we could visit all subproblems in turn, and for each one commit the variables in its scope to the subproblem solution.
 In order to better leverage  X  X eliefs X  of different sub-problems as well as  X  X moothness X  across subproblems, we propose the following method, which works better in practice comparing to other heuristics. (The comparison is not shown due to space limit.) In practice, one could construct multiple assignments from different heuristics and choose the one with the best objective value.
 We visit the variables (in the original MRF) in some ordering, for example, X 1 ,X 2 ,  X  X  X  X N . And for X i we choose the assignment: x i = argmax That is, when visiting each X i , we choose its assignment to maximize the sum of all max-marginals from all subproblems covering X i . And then we fix X i = x i in all subproblems (this will affect subsequent visits to these subproblems). [Experiment Setup] We used three MAP inference tasks in experiments: (1) The protein design benchmark (Yanover et al., 2006). We use the 20 largest problems from that dataset, with number of variables from 101 to 180, number of edges from 1,973 to 3,005, variable cardinality up to 154. (2) Synthetic 20-by-20 grid with variable cardinality of 100. Potentials are dawn from N (0 , 1). (3) The  X  X bject detection X  task from PIC-2011 9 . There are 37 problem instances, each has 60 variables and 1,770 edges, variable cardinality from 11 to 21. Each task corresponds to one row in Fig. 2.
 For each tasks, we use two settings: with or without cycle subproblems (loose or tightened LP relaxation). The former corresponds to the left two columns in Fig. 2, and the latter corresponds to the right two columns. Each setting for each task is shown by a primal(right)-dual(left) pair of figures (averaged over all problem instances). Each primal-dual pair share one legend.
 The cycles in dual decomposition are selected in a static manner by applying the criterion of (Sontag et al., 2008) to the original edge potentials. We did this (instead of dynamically adding cycles) because our goal is to compare different dual methods and we want them to always operate on the same dual problem. For protein design and PIC, we selected 500 triangles for each problem instance. For grid, we used all 381 squares of size four. [Methods Compared] Among the methods com-pared, MPLP (Globerson &amp; Jaakkola, 2007), MSD++ (Sontag et al., 2011), Tree-BCD (Sontag &amp; Jaakkola, 2009), TRW-S MonoChain (Kolmogorov, 2006) are existing methods. TRW-S* is a straightforward generalization of a existing method. MPLP#, MSD#, and STC are new methods derived from our framework. Here STC means calibrating randomly chosen spanning trees of the SMG without unary subproblems (as in Fig. 1 (h)). For TRW-S MonoChain we used the code of (Szeliski et al., 2008). Note that it is only applicable to the grid problem with long monotonic chains. For TRW-S* we use MRF edges (and cycles for the tightened setting) as subproblems. All other methods have been explained in earlier sections. [Result Analysis] When cycle subproblems are present, we observe that STC and TRW-S* performs significantly better in all tasks. The crucial difference is that: these two methods allow the subproblems communicating through MRF edges, whereas all other methods are restricted to passing messages over MRF nodes (they use the Bethe cluster graph as in Fig. 1 (c)). This reveals the limitation of this bipartite construction.
 For sparse MRFs (the grid case), getting more subproblems involved in each BCD steps turns out to be important. As we can see from the second plot in the first column of Fig. 2. The methods that updates  X  X lobal X  blocks (STC, Tree-BCD, MPLP#, MSD#) performs significantly better than the ones with  X  X ocal X  blocks (MPLP, MSD++, and TRW-S*). Note that TRW-S MonoChain get stuck at an even worse dual state than the  X  X ocal X  methods. This is because its block selection (and ordering) is even more restrictive due to using monotonic chains.
 Overall, we observe that different methods tend to  X  X onverge X  to different dual objectives, Even though the dual objectives in each plot should have exactly the same optimal value. Note that all these methods perform BCD X  X hey achieve dual optimal on a block in each step. Therefore choosing blocks (as well as choosing plateau dual states) is very important to the performance (and final result) of a message passing algorithm. Generally speaking, being able to choose blocks and plateau states in a more flexible manner (as our framework reveals) could help us get to better dual states, whereas making these choices in a restricted manner (as most existing methods do) could easily get stuck. Our framework revealed two dimensions of flexibility in designing a message passing algorithm for MAP inference: choosing blocks to update, and choosing a dual state on a plateau in each BCD step. The STC algorithm can be applied with extreme flexibility in these choices. Although these choices appear to be important to performance, any known fixed strategy in making them does not seem to be optimal across different scenarios. If we could find principled and adaptive strategies in making these choices, we will be able to design much more powerful message passing algorithms.
 Acknowledgement This work has been supported by the Max Planck Center for Visual Computing and Communication. Felzenszwalb, P. F. and McAuley, J. J. Fast inference with min-sum matrix product. PAMI , 33(12):2549 X  2554, 2011.
 Globerson, Amir and Jaakkola, Tommi. Fixing max-product: Convergent message passing algorithms for MAP LP-relaxations. In NIPS , 2007.
 Koller, D. and Friedman, N. Probabilistic Graphical
Models: Principles and Techniques . MIT Press, 2009.
 Kolmogorov, V. Convergent tree-reweighted message passing for energy minimization. PAMI , 28(10): 1568 X 1583, October 2006.
 Komodakis, N. and Paragios, N. Beyond loose LP-relaxations: Optimizing MRFs by repairing cycles. In ECCV , pp. III: 806 X 820, 2008.
 Komodakis, Nikos, Paragios, Nikos, and Tziritas,
Georgios. MRF energy minimization and beyond via dual decomposition. PAMI , 33(3):531 X 552, 2011. Meltzer, Talya, Globerson, Amir, and Weiss, Yair.
Convergent message passing algorithms -a unifying view. In UAI , pp. 393 X 401, 2009.
 Shimony. Finding MAPs for belief networks is NP-hard. AIJ: Artificial Intelligence , 68, 1994. Sontag, David and Jaakkola, Tommi. Tree block coordinate descent for MAP in graphical models. AISTATS , 5:544 X 551, 2009.
 Sontag, David, Meltzer, Talya, Globerson, Amir,
Jaakkola, Tommi, and Weiss, Yair. Tightening LP relaxations for MAP using message passing. In UAI , pp. 503 X 510, 2008.
 Sontag, David, Globerson, Amir, and Jaakkola,
Tommi. Introduction to dual decomposition for inference. In Sra, Suvrit, Nowozin, Sebastian, and Wright, Stephen J. (eds.), Optimization for Machine Learning . MIT Press, 2011.
 Szeliski, R. S., Zabih, R., Scharstein, D., Veksler,
O., Kolmogorov, V., Agarwala, A., Tappen, M., and Rother, C. A comparative study of energy minimization methods for markov random fields with smoothness-based priors. IEEE Trans. Pattern
Analysis and Machine Intelligence , 30(6):1068 X  1080, June 2008.
 Tarlow, Daniel, Batra, Dhruv, Kohli, Pushmeet, and Kolmogorov, Vladimir. Dynamic tree block coordinate ascent. In ICML , 2011.
 Wainwright, Martin J. and Jordan, Michael I. Graph-ical models, exponential families, and variational inference. Foundations and Trends in Machine Learning , 1(1-2):1 X 305, 2008.
 Weiss, Y. Correctness of local probability propagation in graphical models with loops. Neural Computa-tion , 12:1 X 41, 2000.
 Werner, T. A linear programming approach to max-sum problem: A review. PAMI , 29(7):1165 X 1179, July 2007.
 Yanover, Chen, Meltzer, Talya, and Weiss, Yair. Lin-ear programming relaxations and belief propagation -an empirical study. JMLR , 7:1887 X 1907, 2006. In this section we prove all propositions in the paper. Instead of showing them in their original order, we follow a logical ordering to allow building later derivations on earlier conclusions.
 Lemma 1. Weak max-consistency is equivalent to:  X  X  x c }  X  T that maximizes each subproblem individually.
 Proof. If  X  X  x c } X  X  that maximizes each subproblem, we have:
However, we also have P c  X  X  max X c  X  c ( X c )  X  constraint in maximization. Therefore weak max-consistency holds.
 In the other direction, if we have weak max-consistency, i.e. P c  X  X  max X c  X  c ( X c ) = maximizer of RHS, denoted as { x c } , must be optimal for each c individually, otherwise the LHS can be increased by changing x c that is not optimal for the corresponding subproblem c .
 Lemma 2. For any { x c }  X  T , we have P  X  0 by only varying block B Proof. For any { x c } X  X  So it suffice to show that The follows from the fact that  X  c 0  X  c =  X   X  c  X  c 0 definition), and x c |  X  = x c 0 |  X  (because { x c } X  X  ). Proof of Proposition 1 Proposition. For any Bethe tree T , MPLP max-consistency =  X  weak max-consistency For any subproblem tree T (including Bethe trees), strong max-consistency =  X  weak max-consistency. Proof. We first show that MPLP max-consistency implies weak max-consistency. Note that according to the definition of  X  T i and  X  T f We denote this quantity (maximum of tree) as M T . To validate the weak max-consistency condition (10), we first note that the RHS of (10) equals M T . For the LHS (10) of we have: Note that N is the number of unary subproblems in T , so we have P i  X  X  max X i 1 N  X  T i ( X i ) = M T . It remains to show that all the remaining terms are zero. We note that for any i  X  f , i  X  T , and assignments x i consistent with x f . So we have equals zero with the assignments corresponds to the maximizer of the LHS of (17). Therefore all terms in the second sum of (18) are zero, which proves that MPLP max-consistency implies weak max-consistency. Now we show that strong max-consistency implies weak max-consistency. Using Lemma 1, we can always find such { x c }  X  T by following these step: choose arbitrary maximizer x c for subproblem c ; choose maximizers for neighbors of c that agrees with x c ; and so on. We can find { x c }  X  T (that maximizes each subproblem) in this manner because of the tree-structure and the fact that all adjacent subproblems in the tree T have identical max-marginals on their sepsets (strong max-consistency).
 Now we show that weak max-consistency is equivalent to dual-optimal on T . We have where the second equality follows from Lemma 2. The first expression in (21) is the dual objective (excluding terms outside T , that do not change). The last expression is a constant; therefore weak max-consistency (equal sign holds for (21)) implies dual-optimal on T .
 If weak max-consistency is not satisfied, from (21) we must have: However, from Proposition 6 10 we know that the value on the RHS can be attained by enforcing weak consistency with STC on T (that only changes dual variables in B T ). So we have not attained dual optimal on T yet. Therefore dual optimal on T implies weak max-consistency.
 Now we give an example to show that weak max-consistency is strictly weaker than the other two. Consider a subproblem tree consists of three nodes subproblem potentials given by: X 1 ,X 2  X  { 0 , 1 } are binary variables. It is easy to verify that they satisfy weak max-consistency (and dual optimal), because the assignment X 1 = 1, X 2 = 1 maximizes each subproblem individually. However, the max-marginal of  X  12 on X 1 is (0 , 0 . 2), which does not agree with  X  1 . Therefore strong max-consistency does not hold. It is also easy to verify that MPLP max-consistency does not hold.
 We use the following lemmas to characterize the behavior STC. Let T c be the subtree of T rooted by c , and c  X  be the set of all children of c .
 Lemma 3. If for all c  X  c 0  X  we have that: then (24, 25) also holds for c 0 after performing upstream updates for all c  X  c 0  X  . Proof. Note that  X   X  c denote the sepset of edge (  X  c,c, X  it is indexed by the children node  X  c because each node in the tree (except for root), as children node, is associated with exactly one edge. This lemma claims that the properties described by (24, 25) propagates in the upstream direction of the tree. It will be used as an induction step to characterize all subproblems when the upstream pass finishes.
 We use  X  c 0 new to denote the new potentials of c 0 after performing upstream pass along edge ( c,c 0 , X  c ) for all c  X  c 0  X  .
 = max where the first equality directly follows from upstream update rule; the second equality follows from the definition of max-marginals; the third equality uses the induction hypothesis (24); the fourth equality is because the trees rooted by different c do not overlap, therefore performing the maximization jointly or separately makes no difference; the last equality is moving the constant into the minimization and re-interpreting the sum and maximization scope.
 For each children c  X  c 0  X  ,  X  x c 0 we have = max = max = 0 , which proves (25) for c 0 where the first equality directly follows from the upstream update rule; the second equality follows from the definition of max-marginals; the last equality is because the expression inside the outer max is by definition non-positive and it attains zero when x c = Lemma 4. After upstream pass of STC, we have: where c 0 is the parent of c . where r is the root of T .
 Proof. We first note that (24,25) are trivial facts if c is a leaf node of T . Using Lemma 3, by induction we know that (24) hold for r when we have finished the upstream pass. That is: Maximizing it over x r on both sides we get (29). We also have that (25) holds for c after the upstream pass from c to its parent c 0 . All subsequent upstream updates will not touch c again. So (25) holds for all c 6 = r after the upstream pass has finished. This directly gives us (28).
 The lemmas above have fully characterized the STC upstream pass: the root subproblem collects all  X  X nergy X  over the tree, leaving a zero max-marginal for each non-root subproblem at their upstream sepsets. Now we consider the downstream pass through edge ( c,c 0 , X  c ), where c 0 is the parent of c . There are two different schedules of downstream updates: sequential and parallel, leading to S-STC, and P-STC, respectively. In S-STC we process different children of one parent c 0 one-by-one, for each children c we split a portion of the max-marginal of c 0 and add to c . In P-STC we process all children of c 0 altogether: splitting a portion of max-marginal of c 0 (possibly at different sepsets) at the same time and adding to its children. They are different because in S-STC the parent and its max-marginals have changed in processing each children before processing subsequent ones.
 We use the following lemma to characterize the basic operations of P-STC downstream pass. The basic operations of S-STC is a special case of it.
 Lemma 5. Let max X c 0  X  c 0 ( X c 0 ) =  X  . If for a given subset set of children of c 0 , denoted as { c 0  X  X  , we have: Then after performing the update in parallel for all c  X  { c 0  X  X  , with  X   X  c  X  0 , P Note that this holds for any subset of children { c 0  X  X  . We can pick { c 0  X  X  to be a single children to represent the basic downstream operation of S-STC.
 Proof. For the new potentials of parent subproblem c 0 we have = max = max  X  (1  X  X =(1  X  X The first equality follows directly from the update rule; the second equality simply splits the first term; the third inequality uses the fact that  X  c  X  0, P c  X  X  c 0  X  X   X  c  X  1; the last equality follows from the definition of max-marginals. Also note that for x c 0 = argmax  X  c 0 ( X c 0 ). Therefore we have proved (33).
 For the new potentials of children subproblem c we have where the first equality follows directly from the update rule; the second equality re-interprets the maximization; the third equality uses the assumption (31); the last equality follows from the definition of max-marginals and the fact that  X  c  X  0.
 Proof of Proposition 13 Proposition (STC Allocation) . After STC, for each subproblem c we have: Proof. Lemma 5 has established that the downstream pass of STC allocates  X  X nergy X  from parent sub-problems to children subproblems in the same way as splitting real numbers. Specifically, when the downstream pass has reached c 0 but not its children. We have (for both S-STC and P-STC): max After performing downstream update for all children of c 0 we have We can use induction over the tree (with initial condition given by Lemma 4, induction step given by Lemma 5) to show that (40) holds for all subproblems in the tree after the downstream pass finishes, which proves the proposition.
 Proof of Proposition 6 Proposition. STC enforces weak max-consistency for any T and any allocation weights a c satisfying a c  X  0 , P Proof. This directly follows from Proposition 13 by summing over c  X  X  on both sides of (38).
 Proof of Proposition 14 Proposition (Detailed Monotonicity) . Let D 0 , D 1 , D 2 be the dual objective value before STC, after up-stream pass, and after downstream pass, respectively. We have D 0  X D 1 = D 2 . Moreover, the dual objective remains constant in each single step of the downstream pass.
 Proof. Let  X  c 0 ,  X  c 1 and  X  c 2 be the subproblem potentials before STC, after upstream pass, and after downstream pass, respectively. Note that where the first equation follows from Lemma 4; the second equation follows from Lemma 2; the last inequality is obtained by eliminating the constraint and pushing the max into the sum.
 Therefore we have D 1  X  D 0 by noting that the two ends of (41) represent the dual objectives excluding the terms outside T (that are not changed).
 For D 2 we also have by using Proposition 13 and following the same reasoning as for D 1 . Therefore we have D 2 = D 1 . From Lemma 5 we can see that the dual objective remains constant in each step of the downstream pass.
 Proof of Proposition 12 Proposition. WTA2 is equivalent to WTA.
 Proof. If {  X  c } satisfies WTA, we show how to find { x c } for  X T to satisfy WTA2. For any T , we start from any x c  X  OPT ( c ). Because of WTA, we can always expand it to neighboring subproblems in T , and so on, until we have { x c } X  X  that minimize each subproblem individually.
 If {  X  c } satisfies WTA2, we show how to find { OPT ( c ) } to satisfy WTA. We initialize { OPT ( c ) } to contain all optimal assignments for each subproblem c , and repeatedly perform the following steps: 1. Find x c  X  OPT ( c ) that cannot be expanded to 2. Assert that x c is not the only assignment in 3. Remove x c from OPT ( c ).
 We can always make the assertion in Step 2 because of the WTA2 condition: if x c is the only assignment left in OPT ( c ), and it satisfies the condition in Step 1, consider the subproblem tree T with only one edge ( c,c 0 , X  ); we have the WTA2 condition on T violated. Also note that this process must end, because the total number of assignments in { OPT ( c ) } is finite. Therefore it must end in a state where we cannot find a x c in Step 1, and each OPT ( c ) is not empty. This { OPT ( c ) } satisfies WTA.
 Proof of Proposition 5 Proposition (STC Fixed-Point) . If {  X  c } do not satisfy WTA2, we can always find T such that applying STC to T decreases the dual objective. If {  X  c } satisfy WTA2, applying STC to any T does not change the dual objective and preserves the WTA2 condition. Proof. If {  X  c } do not satisfy WTA2, by definition we can find T such that there is no { x c }  X  T that are optimal for each c  X  X  . We therefore claim that: this is because if the equal sign holds, the maximizer of RHS must be optimal for each c  X  T (Lemma 1). So applying STC to T must increase the dual object because it does not change the sum over subproblems outside T , and it decreases the sum over subproblem within T . To see this we denote the new subproblem potentials as {  X  c new } , and we have = max where the first equality follows from Proposition 6; the second equality follows from Lemma 2; the last inequality uses (43).
 To show that STC preserves the dual objective when WTA2 holds, let us assume that {  X  c } satisfy WTA2. Consider applying STC to any T . From the assumption we know that  X  X  x c } X  X  that are optimal for each c  X  X  individually. Apply Lemma 1 we know that Again, let {  X  c new } be the subproblem potentials after applying STC to T . It has the same dual objective because: = max following the same reasoning as above.
 To show that STC preserves the WTA2 condition, we instead show that STC preserves the WTA condition (which is equivalent to WTA2 according to Proposition 12). Assuming {  X  c } satisfy WTA with respect to the optimal assignment sets { OPT ( c ) } , it is easy to check that any single message passing step in the STC algorithm will not make any assignment in { OPT ( c ) } non-optimal. Therefore WTA still holds with respect to the same { OPT ( c ) } after STC. Proof of Proposition 10 Proposition. If all edges of T have same scope  X  , P-STC is equivalent to S-STC. Both achieve strong max-consistency when applied with uniform allocation weights a c = 1 |T| Proof. It suffice to show that if all edges of T have the same scope  X  , and the allocation weights are uniform a c = 1 |T| , after running STC we have where the max-marginal of subproblem tree T over  X  is defined as Note that the condition  X  c  X  T , X c |  X  = x  X  implies { X c }  X  T given that all edges of T have the same scope  X  , so this definition of tree max-marginal is consistent with the definition used in Def. 6. We note that in this special case the STC algorithm is equivalent to the following: 1. Compute M c  X  for all c  X  T . Note that all these 2. Compute the average of these max-marginals 3. Add M  X   X  M c  X  to each subproblem.
 This is because if we were to run MAP inference for each subproblem individually using variable elimina-tion with  X  being the variable(s) last eliminated, all subproblem inference steps before reaching  X  will not change with any change in B T . Therefore both S-STC and P-STC in this case essentially averages the max-marginals on  X  . So the max-marginals on  X  for all subproblems become M  X  .
 It remains to show that M  X  = 1 |T|  X  T  X  . This follows directly from Proposition 6 (the weak max-consistency property enforced by STC).
 Proof of Proposition 8 Proposition. Applying STC to the Bethe tree specified in Proposition 2 with allocation weights a f = 0 . 5 , a i = 0 . 5 is equivalent to MSD.
 Proof. To prove the equivalence we use the update rules of MSD given in (Sontag et al., 2011). MSD computes messages  X  fi ( X i ) and defines the reparametrization as follows: Note that these correspond to (6) in our notation. The MDS updates are given by In STC we consider calibrating the Bethe tree specified in the proposition, and in MSD we consider updating  X  fi for a given f and a given i  X  f . We show that they result in the same updates to subproblem potentials  X  f and  X  i .
 We denote MSD messages and subproblem potentials upstream pass subtracts min-marginals from unary subproblems i  X  f and add to f . That is: Then we perform the downstream pass with allocation weights a i = 0 . 5 and a f = 0 . 5: Now consider MSD. For both  X  f and  X  i , only one message in their definitions is changed. Therefore which is the same as (54).
 which is the same as (55). Hence we have proved the equivalence.
 Proof of Proposition 2 Proposition. Max-sum diffusion (MSD) (Werner, 2007) performs BCD on the Bethe dual (8). Each BCD step enforces strong max-consistency for a Bethe tree consisting of one f  X  X  and one i  X  f .
 Proof. This directly follows from Proposition 10 and Proposition 8.
 Proof of Proposition 9 Proposition. Applying STC to the subproblem tree specified in Proposition 3 with uniform allocation weights a c = 1 |T| is equivalent to TRW-S.
 Proof. In proving Proposition 10, we noted that STC when applied to T with all edges sharing the same scope  X  is equivalent to averaging the max-marginals of all subproblems (trees in this case) on  X  . This is exactly what TRW-S does as described in (Kolmogorov, 2006).
 Proof of Proposition 3 Proposition. TRW-S (Kolmogorov, 2006) performs BCD on the dual (7) where each subproblem c  X  C is a tree (of the MRF). Each BCD step enforces strong max-consistency for a subproblem tree T with nodes V
T = { c : scope( c )  X   X  } for a given  X   X  V  X  E , and edges ( c,c 0 , X  ) that constitute a tree over V T . Proof. This directly follows from Proposition 10 and Proposition 9.
 Proof of Proposition 7 Proposition. Applying P-STC to the Bethe tree specified in Proposition 4 with allocation weights a i = , a f = 0 is equivalent to MPLP.
 Proof. Note that the equivalence only holds for P-STC but not S-STC in this case.
 Max-product linear programming (MPLP) was origi-nally proposed in (Globerson &amp; Jaakkola, 2007). We use its generalized form as described in (Sontag et al., 2011). MPLP maintains messages  X  fi ( X i ) to define the same reparametrization (49,50) as in MSD.
 The MPLP updates are given by In STC we consider calibrating the Bethe tree specified in the proposition, and in MPLP we consider updating  X  fi for a given f and all i  X  f . We show that they result in the same updates to subproblem potentials  X  f and  X  i .
 We denote MPLP messages and dual variables before pass subtracts min-marginals from unary subproblems i  X  f and add to f . That is: Then we perform the downstream pass (of P-STC) with allocation weights a i = 1 / | f | and a f = 0: Now consider MPLP. Substituting (58) into (50) we can see that the update to  X  f is exactly (62). For  X  i we note that only one term in RHS of (49) has changed for all i  X  f . Therefore  X  = b  X  i  X  b  X  fi +  X  fi = where the last equation uses the fact that  X   X  f i = b  X   X  f since the messages for f 0 6 = f are not changed. Note that the result is the same as (61) so we have proved the equivalence.
 Proof of Proposition 4 Proposition. MPLP (Globerson &amp; Jaakkola, 2007; Sontag et al., 2011) performs BCD on the Bethe dual (8). Each BCD step enforces MPLP max-consistency for a Bethe tree consisting of one f  X  X  and all i  X  f . Proof. We have shown that MPLP is a special case of P-STC. So it suffice to show that P-STC, when applied to the setting specified in Proposition 7, enforces MPLP max-consistency.
 Applying Def. 6 to the Bethe tree of MPLP, it suffice to show that The result of applying P-STC to this setting is given by (61)(62). Comparing them to (64) and using the definition of tree max-marginals  X  , it suffice to show distribution X  before updates). Indeed, Proof of Proposition 5 Proposition. Tree-BCD (Sontag &amp; Jaakkola, 2009) performs BCD on the Bethe dual (8) where all f are pairwise factors ( i,j ) . Each BCD step enforces MPLP max-consistency for a Bethe tree corresponding to a spanning tree of the MRF.
 Proof. In (Sontag &amp; Jaakkola, 2009), the tree-BCD algorithm was described as updating the beliefs f T = { f i ( x i ) ,f ij ( x i ,x j ) : ij  X  T } on a spanning tree T of the MRF as follows: where log  X  i , log  X  ij are (log)-max-marginals of the tree distribution over T , and n j  X  i is the number of nodes in the subtree of node i with parent j . Note that T is a special case of subproblem tree when the dual decomposition has MRF edges and nodes as subproblems; log  X  i , log  X  ij , and n j  X  i correspond to  X  i ,  X 
 Huayan Wang huayanw@cs.stanford.edu Daphne Koller koller@cs.stanford.edu MAP-MRF (finding the most probable assignments for MRFs) is one of the most important components in learning and applying structured probabilistic models. In general this problem is NP-hard (Shimony, 1994). Many different methods have been proposed to approximate or solve it under specific circumstances. A large family of these methods is based on solving a dual problem of an LP relaxation. Different duals of different LP relaxations (with different tightness) have been used. These methods are usually formulated as max-product (max-sum) message passing over the MRF or its cluster (region) graph.
 Recent advances revealed that convergent versions of these algorithms can often be interpreted as block coordinate descent (BCD) in the dual (Meltzer et al., 2009; Sontag et al., 2011). However, most of them operate on local (small) blocks, such as MPLP (Globerson &amp; Jaakkola, 2007) and its generalizations (Sontag et al., 2008), max-sum diffusion (MSD) (Werner, 2007), and TRW-S (Kolmogorov, 2006). Given a block of dual variables (messages), these algorithms work by enforcing some consistency con-straint over the block, thereby achieve dual-optimal w.r.t. these dual variables. Meltzer et al. (2009) noted that it was difficult to generalize them to larger blocks while enforcing the same consistency constraints. Sontag et al. (2009) proposed a method (tree-BCD) that updates much larger blocks for a specific choice of dual. But it was not clear how to apply it to duals of tighter LP relaxations (e.g., a dual decomposition with cycle subproblems).
 We observe that the difficulties in generalizing these methods arise from the fact that they all impose too strong consistency constraints, which are sufficient but not necessary for the dual objective to be optimal on the blocks being updated. By loosing these constraints, we are able to perform BCD on much larger blocks.
 Indeed, we show that dual-optimality on blocks (of messages) can be established on a much broader basis X  X ith quite general choices of the dual objective itself as well as the blocks to be updated. Specifically, we illustrate this by deriving a  X  X nified X  message passing algorithm in the most general setup X  X n arbitrary dual decomposition. The resulted algorithm ( subproblem-tree calibration , or STC) has the following properties: 1. It is formulated as message passing on a graph-2. It subsumes MPLP, MSD and TRW-S as special 3. It achieves dual-optimal on blocks that can be In other words, our framework attempts to charac-terize the degrees of freedom we have in designing a message passing algorithm (for MAP inference). Understanding these flexibilities could help us better understand existing methods, as well as design more powerful ones. In practice, we observe that we often get stuck in sub-optimal dual states when only updating blocks chosen in a restricted manner, whereas being able to choose blocks with more flexibility could lead to better dual (and primal) states. 2.1. MAP inference, LP relaxation, and dual The MAP inference problem over X = { X 1: N } and graph structure G = { V,E } can be formulated as: where  X  ( X ) = P  X   X  X   X   X  ( X  X  ); A is the set of MRF cliques. Without loss of generality we choose A = V  X  E for a parametrization with unary and pairwise potentials 1 . We use lowercase x i  X  V al ( X i ) and x = { x 1: N } to denote assignments to the variables. Problem (1) is NP-hard in general (Shimony, 1994). A large family of MAP inference methods builds on solving a linear programming (LP) relaxation (Wainwright &amp; Jordan, 2008; Koller &amp; Friedman, 2009):  X  is all MRF parameters {  X  i ,  X  ij } concatenated in same ordering as  X  . Choosing different polytopes M results in different LP relaxations. One choice is the marginal polytope :
M G = {  X  | X  p ( X ) , X where p ( X i ) and p ( X i ,X j ) denote marginal distribu-tions of p ( X ). When M = M G the LP relaxation (2) is equivalent to the original problem (1). Therefore it is also NP-hard X  X he marginal polytope is defined by exponentially many faces (constraints).
 A widely used choice for M is the local polytope : which only has a polynomial number of constraints, and the LP relaxation (2) with M L is tractable. In general M L is a loose outer bound of M G . Any solution to (2) with M = M L is a vertex of M L . If it happens to be a vertex of M G , we have the exact solution to (1). This is usually not the case except for several special families such as the problems with tree structures or sub-modular potentials etc.
 Solving (2) using a standard LP solver is very inefficient (Yanover et al., 2006) even with a tractable choice of M . In practice we usually solve its dual LP. Indeed, it is more convenient to start with a dual decomposition , which directly gives us a dual LP (of some underlying primal LP that we do not explicitly deal with).
 Specifically, consider a decomposition of  X  ( X ) into subproblems c  X  C , parametrized by {  X  c } , which altogether give rise to a reparametrization of  X  ( x ): where x | c denotes restricting the joint assignment to the scope of subproblem c . The subproblems could be single nodes, edges, trees, or cycles that are assumed to be tractable.
 Note that (5) has exponentially many constraints. A succinct way to enforce them is to express the reparametrization in terms of messages: where the messages satisfy  X  c 0  X  c =  X   X  c  X  c 0 . The initial subproblem potentials  X  c 0 are assumed to satisfy (5). They can be constructed, for example, by splitting the original potentials. It is easy to check that subproblem potentials defined by (6) satisfy (5) for any choice of the messages.
 Note that the scopes of messages are defined by X c  X  X 0 c , which is the intersections among X c (with a subscript): the original MRF variables covered by subproblem c . This is to distinguish from X c (with a superscript), which are subproblem variables . For example, if subproblems c and c 0 both cover X 1 (orginal MRF variable), we would have X c 1 and X c 0 1 : two different variables with independent assignments. Since each subproblem has its own copy of variables X c , summing over their optimal values gives rise to an upper bound of (1) because each subproblem is maximized independently. This upper bound is a function of the messages: It turns out that (7) is a dual problem of (2) under some choice of M . The tightness of the underlying M depends on our choice of the subproblems. It has been shown (Komodakis et al., 2011) that choosing only tree structured subproblems leads to a dual of (2) with M = M L . Duals of tightened LP relaxations can be constructed by introducing complex subproblems such as cycles.
 Given a decomposition, our goal is to minimize (7) by rearranging subproblem potentials (via message pass-ing), thereby solving the underlying LP relaxation. 2.2. Bethe cluster (region) graph In principle the messages  X  c 0  X  c in (6) can be defined between all pairs of overlapping subproblems. However, a rather restricted form has been dominantly used in defining such messages: the Bethe cluster graph. It has a bipartite structure with one layer of  X  X actor X  nodes and one layer of small (usually unary) nodes encoding intersections between the factors. Specifically, consider using the set of subproblem C = { i } X  X  consisting of unary subproblems { i } and larger subproblems (factors) F . So the dual (7) takes a restricted form:
D ( {  X  f  X  i } ) = X where the messages are only defined between the two layers because of the bipartite structure. This construction is shown in Fig. 1 (a) and (c). Note that we used superscripts (instead of subscripts) in  X  i and X i to indicate that these are subproblem potentials and variables to distinguish from the orginal MRF potentials  X  i and variables X i .
 It has been shown (Meltzer et al., 2009; Sontag &amp; Jaakkola, 2009; Sontag et al., 2011) that many existing algorithms, including (Globerson &amp; Jaakkola, 2007; Sontag et al., 2008; 2011; Werner, 2007; Sontag &amp; Jaakkola, 2009; Tarlow et al., 2011; Komodakis &amp; Paragios, 2008; Meltzer et al., 2009), can be interpreted as passing messages on the Bethe cluster graph and thereby optimizing the dual objective (8). This  X  X estricted X  design had arisen from the historical concern of satisfying the running intersection property , thus alleviating double-counting in loopy BP (Weiss, 2000). However, this is no longer relevant under the modern view of message passing as a BCD algorithm. To this end, we will introduce a more general graph-object: subproblem multi-graph (Sec. 3.1) to serve the role of traditional cluster graphs. This more general setup allows us to pass messages in more flexible ways and achieve better dual (and primal) states in many situations (as shown in experiments). 3.1. Subproblem multi-graph and subproblem For notation simplicity, in this paper we treat graphs G and trees T as sets consisting of nodes and edges. So we will use T  X  G to denote that T is a subgraph of G . And we will use e  X  T or c  X  T to denote that edge e or node c is in the tree.
 Given a dual decomposition with subproblems C , we build a graph-object as follows: Definition 1 (Subproblem Multi-Graph/Tree) . Given C , the subproblem multi-graph (SMG) G = ( V , E ) has one node for each c  X  C , and one edge between c and c 0 for each tuple ( c,c 0 , X  ) , where  X   X  V  X  E is shared by c and c 0 . A subproblem tree is a tree T  X  X  .
 Note that we use ( V,E ) for the MRF graph and ( V , E ) for SMG.
 This construction is illustrated in Fig. 1 (b) where we decompose the MRF (Fig. 1 (a)) into four subproblems.
 Note that if we include all unary subproblems into the decomposition, we would get a SMG similar to Fig. 1 (c) but with extra edges among the non-unary subproblems. So a tree in the Bethe cluster graph (which we call a Bethe tree ) is also a subproblem tree by definition. Therefore all conclusions in this paper on subproblem trees apply to Bethe trees.
 Fig. 1 (d) to (i) are examples of subproblem trees. They correspond to blocks updated by different algorithms as explained later. Note that (d) (e) and (i) are Bethe trees corresponding to the Bethe cluster graph (c); (f) and (h) are trees in the SMG (b); (g) is a Bethe tree of a different dual decomposition (with all edges and nodes as subproblems).
 For each SMG edge ( c,c 0 , X  )  X  E , we have messages 2  X  dual variables) associated with subproblem tree T is given by 3.2. Max-consistency and dual-optimal on Given a block B T associated with some subproblem tree T , we want to achieve dual-optimal w.r.t. that block: Definition 2 (Dual-optimal on T ) . The subproblem potentials {  X  c } are dual-optimal on T if we can not further decrease the dual objective by changing messages in B T .
 A message passing algorithm achieves this by enforcing some consistency constraint . We first identify a constraint that is equivalent to dual-optimal on T . Definition 3 (Assignments agree on T ) . Assignments to all subproblems { x c } c  X  X  agree on T , denoted as { x c } X  X  , if for  X  ( c,c 0 , X  )  X  X  we have x c  X  = x c 0 Definition 4 (Weak max-consistency on T ) . {  X  c } c  X  X  satisfies weak max-consistency if That is, maximizing each subproblem independently gets to the same optimal value as maximizing them while requiring the assignments to agree on the tree. This condition turns out to be equivalent to dual optimal on T (Proposition 1). We will show that our message passing algorithm (Sec. 3.3) enforces this constraint for arbitrary T .
 We now identify two other stronger constraints (enforced by existing algorithms) that are sufficient but not necessary for dual-optimal on T .
 Let M c  X  be the (log)-max-marginals of c on  X  : Note that if  X  = ( i,j )  X  E , X c |  X  = x  X  means X c i = x and X c j = x j .
 The following consistency constraint requires the subproblem to agree on their max-marginals over the tree: Definition 5 (Strong max-consistency 3 on T ) . {  X  c } c  X  X  satisfies strong max-consistency if M c  X  M c 0  X  ,  X  ( c,c 0 , X  )  X  X  .
 Another consistency constraint requires that the subproblem potentials match the max-marginals of the tree distribution: Definition 6 (MPLP max-consistency on T ) . For Bethe tree T with N unary clusters, {  X  i ,  X  f } i,f  X  X  satisfies MPLP max-consistency if  X  i = 1 N  X  T i and  X  max-marginals 4 of the tree T , and N f i is the number of unary clusters in the subtree rooted by i on the opposite side of f .
 Note that MPLP max-consistency is only defined for Bethe trees. With a slight abuse of notation, we used i  X  f in the above definition to denote that the scope of unary subproblem i is contained in the scope of factor subproblem f in a Bethe cluster graph.
 The relations among these consistency constraints are given below.
 Proposition 1. For any Bethe tree T , MPLP max-consistency =  X  weak max-consistency For any subproblem tree T (including Bethe trees), strong max-consistency =  X  weak max-consistency. Proof for this ( as well as all other propositions in the rest of this paper ) are given in Appendix (supplement material), where we also give an example that satisfies weak max-consistency but not the other two.
 The two stronger constraints are enforced by existing methods: Proposition 2. Max-sum diffusion (MSD) (Werner, 2007) performs BCD on the Bethe dual (8). Each BCD step enforces strong max-consistency for a Bethe tree consisting of one f  X  F and one i  X  f (as in Fig. 1 (d)).
 Proposition 3. TRW-S (Kolmogorov, 2006) per-forms BCD on the dual (7) where each subproblem c  X  X  is a tree (of the MRF). Each BCD step enforces strong max-consistency for a subproblem tree T with nodes V T = { c : scope( c )  X   X  } for a given  X   X  V  X  E , and edges ( c,c 0 , X  ) that constitute a tree over V T 5 . Proposition 4. MPLP (Globerson &amp; Jaakkola, 2007; Sontag et al., 2011) performs BCD on the Bethe dual (8). Each BCD step enforces MPLP max-consistency for a Bethe tree consisting of one f  X  X  and all i  X  f (as in Fig. 1 (e)).
 Proposition 5. Tree-BCD (Sontag &amp; Jaakkola, 2009) performs BCD on the Bethe dual (8) where all f are pairwise factors ( i,j ) . Each BCD step enforces MPLP max-consistency for a Bethe tree corresponding to a spanning tree of the MRF (as in Fig. 1 (g)).
 Note that a spanning tree of the MRF (with | V | X  1 edges) is much smaller than a spanning tree of the SMG (of a dual decomposition into all edges and nodes). The latter contains all (possibly O ( | V | 2 edges of the MRF. 3.3. The STC algorithm Now we show how to attain weak max-consistency for any subproblem tree (such as Fig. 1 (h) and (i)). To express the algorithm concisely, we use  X  X  X  y to denote two updates: x  X  x  X  m and y  X  y + m . We assume subproblem solvers for all c that output M c  X  . For tree-structured subproblems this is straightforward. For cycle subproblems we use (Felzenszwalb &amp; McAuley, 2011) which provides a fast way of computing the junction-tree messages.
 Our algorithm (Alg. 1) calibrates a subproblem-tree by an upstream pass and a downstream pass; both update subproblem potentials  X  X n place X  without storing any message (although conceptually it can be viewed as updating the messages in (7) as well). The Algorithm 1 Subproblem tree calibration (STC) downstream pass of STC can be performed for each children of a node either sequentially or in parallel . They differ in line 8 and 11 of Alg. 1. We call these two alternatives S-STC and P-STC, respectively. Any statement about STC in the following, if not specified, applies to both P-STC and S-STC.
 Overall we repeatedly choose different trees and perform the calibration.
 Proposition 6. STC enforces weak max-consistency for any T and any allocation weights a c satisfying a c  X  Note that this (together with Proposition 1) implies monotonicity and convergence of the overall algorithm. We now clarify the relations between STC and existing methods.
 Proposition 7. Applying P-STC to the Bethe tree specified in Proposition 4 with allocation weights a i = , a f = 0 is equivalent to MPLP.
 Proposition 8. Applying STC to the Bethe tree specified in Proposition 2 with allocation weights a f = 0 . 5 , a i = 0 . 5 is equivalent to MSD.
 Proposition 9. Applying STC to the subproblem tree specified in Proposition 3 with uniform allocation weights a c = 1 |T| is equivalent to TRW-S.
 This implies that STC actually enforces strong max-consistency when applied to settings of MSD and TRW-S. Indeed, we have: Proposition 10. If all edges of T have same scope  X  , P-STC is equivalent to S-STC. Both achieve strong max-consistency when applied with uniform allocation weights a c = 1 |T| This condition (all edges of T have the same scope) is satisfied in MSD and TRW-S.
 Given Proposition 9, we can easily generalize TRW-S beyond tree subproblems 6 . We call this generalized algorithm TRW-S*: applying STC to a subproblem tree consists of all subproblems sharing a given MRF node or edge, as in Fig. 1 (f). It will appear in experiments. 3.4. Fixed-point characterization In this part we give a fixed-point characterization of the STC algorithm, and we show that it is in fact equivalent to the weak tree agreement (WTA) condition (Kolmogorov, 2006). Indeed, the fact that TRW-S (Kolmogorov, 2006) is a special case of STC suggests that the latter should be at least as powerful as the former.
 Definition 7 (WTA2) . Subproblem potentials {  X  c } satisfy weak tree-agreement-two (WTA2) if for any subproblem tree T ,  X  X  x c }  X  T that are optimal for each c  X  X  individually. Proposition 11 (STC Fixed-Point) . If {  X  c } do not satisfy WTA2, we can always find T such that applying STC to T decreases the dual objective. If {  X  c } satisfy WTA2, applying STC to any T does not change the dual objective and preserves the WTA2 condition. We re-state the WTA condition from (Kolmogorov, 2006) in a slightly generalized form (allowing c to be arbitrary subproblems, not restricted to trees). Definition 8 (WTA) . Subproblem potentials {  X  c } satisfy weak tree-agreement (WTA) if we can find a subset of optimal assignments for each c , denoted as OPT ( c ) , such that: for any small subproblem tree T consists of two nodes c and c 0 connected by ( c,c 0 , X  ) , we have: for  X  x c  X  OPT ( c ) ,  X  x c 0  X  OPT ( c 0 ) , Comparing to WTA, WTA2 appears to assert a weaker constraint on more general T . This originates from the fact that STC calibrates more general T and enforces a weaker consistency constraint than that of MSD and TRW-S. However asymptotically they are equally powerful: Proposition 12. WTA2 is equivalent to WTA. 3.5. Choosing allocation weights In this part we clarify the role of allocation weights a , which are responsible for the message coefficients in the downstream pass of STC. As we have seen in Proposition 8 and 7, there are subtle (but important) differences between MPLP and MSD in this aspect. (This has also been noted in (Sontag et al., 2011).) In order to understand the role of allocation weights a c we first show some detailed characterization of STC. Proposition 13 (STC Allocation) . After STC, for each subproblem c we have: Intuitively, the downstream pass allocate  X  X nergy X  to all subproblems according to their allocation weights. We can further show that: Proposition 14 (Detailed Monotonicity) . Let D 0 , D 1 , D 2 be the dual objective value before STC, after upstream pass, and after downstream pass, respectively. We have D 0  X  D 1 = D 2 . Moreover, the dual objective remains constant in each single step of the downstream pass.
 That is, the downstream pass essentially moves around in a plateau of the dual, preparing for the next downhill move. Therefore the allocation weights determine where to settle on that plateau.
 When T is a Bethe tree, two styles of allocation have been used. One is the MPLP-style allocation, which assigns zero weights to all non-unary subproblems, and uniformly allocate among unary subproblems. This is the case for MPLP and Tree-BCD 7 . The other is the MSD-style allocation, which assigns uniform allocation weights to all subproblems. This is the case for MSD and its generalized version MSD++ (Sontag et al., 2011).
 Given our framework, it is straightforward to further generalize these two styles of allocation to spanning trees of the Bethe cluster graph (as in Fig. 1 (i)). We call these two algorithms MPLP# and MSD#, respectively 8 . They will appear in experiments. 3.6. Generate primal solutions Given subproblem potentials, solutions to the original MAP inference problem can be constructed in different ways (Komodakis et al., 2011). For example, when unary subproblems are present, we could simply take the assignments that minimize each unary subproblem. Or we could visit all subproblems in turn, and for each one commit the variables in its scope to the subproblem solution.
 In order to better leverage  X  X eliefs X  of different sub-problems as well as  X  X moothness X  across subproblems, we propose the following method, which works better in practice comparing to other heuristics. (The comparison is not shown due to space limit.) In practice, one could construct multiple assignments from different heuristics and choose the one with the best objective value.
 We visit the variables (in the original MRF) in some ordering, for example, X 1 ,X 2 ,  X  X  X  X N . And for X i we choose the assignment: x i = argmax That is, when visiting each X i , we choose its assignment to maximize the sum of all max-marginals from all subproblems covering X i . And then we fix X i = x i in all subproblems (this will affect subsequent visits to these subproblems). [Experiment Setup] We used three MAP inference tasks in experiments: (1) The protein design benchmark (Yanover et al., 2006). We use the 20 largest problems from that dataset, with number of variables from 101 to 180, number of edges from 1,973 to 3,005, variable cardinality up to 154. (2) Synthetic 20-by-20 grid with variable cardinality of 100. Potentials are dawn from N (0 , 1). (3) The  X  X bject detection X  task from PIC-2011 9 . There are 37 problem instances, each has 60 variables and 1,770 edges, variable cardinality from 11 to 21. Each task corresponds to one row in Fig. 2.
 For each tasks, we use two settings: with or without cycle subproblems (loose or tightened LP relaxation). The former corresponds to the left two columns in Fig. 2, and the latter corresponds to the right two columns. Each setting for each task is shown by a primal(right)-dual(left) pair of figures (averaged over all problem instances). Each primal-dual pair share one legend.
 The cycles in dual decomposition are selected in a static manner by applying the criterion of (Sontag et al., 2008) to the original edge potentials. We did this (instead of dynamically adding cycles) because our goal is to compare different dual methods and we want them to always operate on the same dual problem. For protein design and PIC, we selected 500 triangles for each problem instance. For grid, we used all 381 squares of size four. [Methods Compared] Among the methods com-pared, MPLP (Globerson &amp; Jaakkola, 2007), MSD++ (Sontag et al., 2011), Tree-BCD (Sontag &amp; Jaakkola, 2009), TRW-S MonoChain (Kolmogorov, 2006) are existing methods. TRW-S* is a straightforward generalization of a existing method. MPLP#, MSD#, and STC are new methods derived from our framework. Here STC means calibrating randomly chosen spanning trees of the SMG without unary subproblems (as in Fig. 1 (h)). For TRW-S MonoChain we used the code of (Szeliski et al., 2008). Note that it is only applicable to the grid problem with long monotonic chains. For TRW-S* we use MRF edges (and cycles for the tightened setting) as subproblems. All other methods have been explained in earlier sections. [Result Analysis] When cycle subproblems are present, we observe that STC and TRW-S* performs significantly better in all tasks. The crucial difference is that: these two methods allow the subproblems communicating through MRF edges, whereas all other methods are restricted to passing messages over MRF nodes (they use the Bethe cluster graph as in Fig. 1 (c)). This reveals the limitation of this bipartite construction.
 For sparse MRFs (the grid case), getting more subproblems involved in each BCD steps turns out to be important. As we can see from the second plot in the first column of Fig. 2. The methods that updates  X  X lobal X  blocks (STC, Tree-BCD, MPLP#, MSD#) performs significantly better than the ones with  X  X ocal X  blocks (MPLP, MSD++, and TRW-S*). Note that TRW-S MonoChain get stuck at an even worse dual state than the  X  X ocal X  methods. This is because its block selection (and ordering) is even more restrictive due to using monotonic chains.
 Overall, we observe that different methods tend to  X  X onverge X  to different dual objectives, Even though the dual objectives in each plot should have exactly the same optimal value. Note that all these methods perform BCD X  X hey achieve dual optimal on a block in each step. Therefore choosing blocks (as well as choosing plateau dual states) is very important to the performance (and final result) of a message passing algorithm. Generally speaking, being able to choose blocks and plateau states in a more flexible manner (as our framework reveals) could help us get to better dual states, whereas making these choices in a restricted manner (as most existing methods do) could easily get stuck. Our framework revealed two dimensions of flexibility in designing a message passing algorithm for MAP inference: choosing blocks to update, and choosing a dual state on a plateau in each BCD step. The STC algorithm can be applied with extreme flexibility in these choices. Although these choices appear to be important to performance, any known fixed strategy in making them does not seem to be optimal across different scenarios. If we could find principled and adaptive strategies in making these choices, we will be able to design much more powerful message passing algorithms.
 Acknowledgement This work has been supported by the Max Planck Center for Visual Computing and Communication. Felzenszwalb, P. F. and McAuley, J. J. Fast inference with min-sum matrix product. PAMI , 33(12):2549 X  2554, 2011.
 Globerson, Amir and Jaakkola, Tommi. Fixing max-product: Convergent message passing algorithms for MAP LP-relaxations. In NIPS , 2007.
 Koller, D. and Friedman, N. Probabilistic Graphical
Models: Principles and Techniques . MIT Press, 2009.
 Kolmogorov, V. Convergent tree-reweighted message passing for energy minimization. PAMI , 28(10): 1568 X 1583, October 2006.
 Komodakis, N. and Paragios, N. Beyond loose LP-relaxations: Optimizing MRFs by repairing cycles. In ECCV , pp. III: 806 X 820, 2008.
 Komodakis, Nikos, Paragios, Nikos, and Tziritas,
Georgios. MRF energy minimization and beyond via dual decomposition. PAMI , 33(3):531 X 552, 2011. Meltzer, Talya, Globerson, Amir, and Weiss, Yair.
Convergent message passing algorithms -a unifying view. In UAI , pp. 393 X 401, 2009.
 Shimony. Finding MAPs for belief networks is NP-hard. AIJ: Artificial Intelligence , 68, 1994. Sontag, David and Jaakkola, Tommi. Tree block coordinate descent for MAP in graphical models. AISTATS , 5:544 X 551, 2009.
 Sontag, David, Meltzer, Talya, Globerson, Amir,
Jaakkola, Tommi, and Weiss, Yair. Tightening LP relaxations for MAP using message passing. In UAI , pp. 503 X 510, 2008.
 Sontag, David, Globerson, Amir, and Jaakkola,
Tommi. Introduction to dual decomposition for inference. In Sra, Suvrit, Nowozin, Sebastian, and Wright, Stephen J. (eds.), Optimization for Machine Learning . MIT Press, 2011.
 Szeliski, R. S., Zabih, R., Scharstein, D., Veksler,
O., Kolmogorov, V., Agarwala, A., Tappen, M., and Rother, C. A comparative study of energy minimization methods for markov random fields with smoothness-based priors. IEEE Trans. Pattern
Analysis and Machine Intelligence , 30(6):1068 X  1080, June 2008.
 Tarlow, Daniel, Batra, Dhruv, Kohli, Pushmeet, and Kolmogorov, Vladimir. Dynamic tree block coordinate ascent. In ICML , 2011.
 Wainwright, Martin J. and Jordan, Michael I. Graph-ical models, exponential families, and variational inference. Foundations and Trends in Machine Learning , 1(1-2):1 X 305, 2008.
 Weiss, Y. Correctness of local probability propagation in graphical models with loops. Neural Computa-tion , 12:1 X 41, 2000.
 Werner, T. A linear programming approach to max-sum problem: A review. PAMI , 29(7):1165 X 1179, July 2007.
 Yanover, Chen, Meltzer, Talya, and Weiss, Yair. Lin-ear programming relaxations and belief propagation
