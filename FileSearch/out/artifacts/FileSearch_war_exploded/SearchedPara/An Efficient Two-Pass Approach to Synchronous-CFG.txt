 Syntax-driven (Galley et al., 2006) and hierarchi-cal translation models (Chiang, 2005) take advan-tage of probabilistic synchronous context free gram-mars (PSCFGs) to represent structured, lexical re-ordering constraints during the decoding process. These models extend the domain of locality (over phrase-based models) during decoding, represent-ing a significantly larger search space of possible translation derivations. While PSCFG models are often induced with the goal of producing grammati-cally correct target translations as an implicit syntax-structured language model, we acknowledge the value of n-gram language models (LM) in phrase-based approaches.

Integrating n-gram LMs into PSCFGs based de-coding can be viewed as online intersection of the PSCFG grammar with the finite state machine rep-resented by the n-gram LM, dramatically increasing the effective number of nonterminals in the decoding grammar, rendering the decoding process essentially infeasible without severe, beam-based lossy prun-ing. The alternative, simply decoding without the n-gram LM and rescoring N-best alternative transla-tions, results in substantially more search errors, as shown in (Zollmann and Venugopal, 2006).

Our two-pass approach involves fast, approximate synchronous parsing in a first stage, followed by a second, detailed exploration through the resulting hypergraph of sentence spanning derivations, using the n-gram LM to drive that search. This achieves search errors comparable to a strong  X  X ube Pruning X  (Chiang, 2007), single-pass baseline. The first pass corresponds to a severe parameterization of Cube Pruning considering only the first-best (LM inte-grated) chart item in each cell while maintaining un-explored alternatives for second-pass consideration. Our second stage allows the integration of long dis-tance and flexible history n-gram LMs to drive the search process, rather than simply using such mod-els for hypothesis rescoring.

We begin by discussing the PSCFG model for statistical machine translation, motivating the need for effective n-gram LM integration during decod-ing. We then present our two-pass approach and discuss Cube Pruning as a state-of-the-art baseline. We present results in the form of search error analy-sis and translation quality as measured by the BLEU score (Papineni et al., 2002) on the IWSLT 06 text translation task (Eck and Hori, 2005) 1 , comparing Cube Pruning with our two-pass approach. Probabilistic Synchronous Context Free Grammar (PSCFG) approaches to statistical machine transla-tion use a source terminal set (source vocabulary) T
S , a target terminal set (target vocabulary) T T and a shared nonterminal set N and induce rules of the form where (i) X  X  N is a nonterminal, (ii)  X   X  ( N  X  T minals, (iii)  X   X  ( N  X  X  T )  X  is a sequence of nonter-minals and target terminals, (iv) the number cnt (  X  ) of nonterminal occurrences in  X  is equal to the num-ber cnt (  X  ) of nonterminal occurrences in  X  , (v) one mapping from nonterminal occurrences in  X  to nonterminal occurrences in  X  , and (vi) w  X  [0 ,  X  ) is a non-negative real-valued weight assigned to the rule. We will assume  X  to be implicitly defined by indexing the NT occurrences in  X  from left to right starting with 1, and by indexing the NT occurrences in  X  by the indices of their corresponding counter-parts in  X  . Syntax-oriented PSCFG approaches typ-ically ignore source structure, instead focussing on generating syntactically well formed target deriva-tions. (Galley et al., 2006) use syntactic constituents for the PSCFG nonterminal set and (Zollmann and Venugopal, 2006) take advantage of CCG (Steed-man, 1999) categories, while (Chiang, 2005) uses a single generic nonterminal. PSCFG derivations function analogously to CFG derivations. Given a source sentence f , the translation task under a PSCFG grammar can be expressed as where tgt( D ) refers to the target terminal symbols generated by the derivation D and src( D ) refers to the source terminal symbols spanned by D . The score (also laxly called probability, since we never need to compute the partition function) of a deriva-tion D under a log-linear model, referring to the rules r used in D , is:
P ( D ) = where  X  i refers to features defined on each rule, and P LM is a g -gram LM probability applied to the target terminal symbols generated by the derivation D . Introducing the LM feature defines dependen-cies across adjacent rules used in each derivation, and requires modifications to the decoding strategy. Viewing the LM as a finite-state machine, the de-coding process involves performing an intersection between the PSCFG grammar and the g -gram LM (Bar-Hillel et al., 1964). We present our work under the construction in (Wu, 1996), following notation from (Chiang, 2007), extending the formal descrip-tion to reflect grammars with an arbitrary number of nonterminals in each rule. 2.1 Decoding Strategies In Figure 1, we reproduce the decoding algorithm from (Chiang, 2007) that applies a PSCFG to translate a source sentence in the same notation (as a deductive proof system (Shieber et al., 1995)), generalized to handle more than two non-terminal pairs. Chart items [ X, i, j, e ] : w span j  X  i words in the source sentence f 1  X  X  X  f n , starting at position i + 1 , and have weight w (equivalent to P ( D ) ), and e  X  ( T T  X  X  ? } )  X  is a sequence of target terminals, with possible elided parts, marked by ? . Functions p, q whose domain is T T  X  { ? } are defined in (Chiang, 2007) and are repeated here for clarity.
The function q elides elements from a target lan-guage terminal sequence, leaving the leftmost and rightmost g  X  1 words, replacing the elided words with a single ? symbol. The function p returns g -gram LM probabilities for target terminal sequences, where the ? delineates context boundaries, prevent-ing the calculation from spanning this boundary. We add a distinguished start nonterminal S to gener-ate sentences spanning target translations beginning with g  X  1  X  s  X  symbols and ending with g  X  1  X  X  s  X  symbols. This can e.g. be achieved by adding for each nonterminal X a PSCFG rule
We are only searching for the derivation of highest probability, so we can discard identical chart items that have lower weight. Since chart items are de-fined by their left-hand side nonterminal production, span, and the LM contexts e , we can safely discard these identical items since q has retained all context that could possibly impact the LM calculation. This process is commonly referred to as item recombina-tion. Backpointers to antecedent cells are typically retained to allow N -Best extraction using an algo-rithm such as (Huang and Chiang, 2005).

The impact of g -gram LM intersection during de-coding is apparent in the final deduction step. Gen-erating the set of consequent Z chart items involves combining m previously produced chart cells. Since each of these chart cells with given source span [ i, j ] is identified by nonterminal symbol X and LM con-text e , we have at worst |N| X  X T T | 2( g  X  1) such chart cells in a span. The runtime of this algorithm is thus where K is the maximum number of NT pairs per rule and n the source sentence length. Without se-vere pruning, this runtime is prohibitive for even the smallest induced grammars. Traditional pruning ap-proaches that limit the number of consequents after they are produced are not effective since they first re-quire that the cost of each consequent be computed (which requires calls to the g -gram LM).

Restrictions to the grammar afford alternative de-coding strategies to reduce the runtime cost of syn-chronous parsing. (Zhang et al., 2006)  X  X inarize X  grammars into CNF normal form, while (Watan-abe et al., 2006) allow only Griebach-Normal form grammars. (Wellington et al., 2006) argue that these restrictions reduce our ability to model translation equivalence effectively. We take an agnostic view on the issue; directly addressing the question of effi-cient LM intersection rather than grammar construc-tion. We propose a two-pass solution to the problem of online g -gram LM intersection. A naive two-pass approach would simply ignore the LM interactions during parsing, extract a set of N derivations from the sentence spanning hypergraph and rescore these derivations with the g -gram LM. In practice, this ap-proach performs poorly (Chiang, 2007; Zollmann and Venugopal, 2006). While parsing time is dra-matically reduced (and N -best extraction time is negligible), N is typically significantly less than the complete number of possible derivations and sub-stantial search errors remain. We propose an ap-proach that builds upon the concept of a second pass but uses the g -gram LM to search for alternative, better translations. 3.1 First pass: parsing We begin by relaxing the criterion that determines when two chart items are equivalent during parsing. We consider two chart items to be equivalent (and therefore candidates for recombination) if they have matching left-hand side nonterminals, and span. We no longer require them to have the same LM con-text e . We do however propagate the e, w for the chart item with highest score, causing the algorithm to still compute LM probabilities during parsing. As a point of notation, we refer to such a chart item by annotating its e, w as e 1 , w 1 , and we refer to them as approximate items (since they have made a first-best approximation for the purposes of LM calcula-tion). These approximate items labeled with e 1 , w 1 are used in consequent parse calculations.

The parsing algorithm under this approximation stays unchanged, but parsing time is dramatically re-duced. The runtime complexity of this algorithm is now O n 3 |N| K at the cost of significant search errors (since we ignored most LM contexts that we encountered).

This relaxation is different from approaches that do not use the LM during parsing. The sentence spanning item does have LM probabilities associ-ated with it (but potentially valuable chart items were not considered during parsing). Like in tra-ditional parsing, we retain the recombined items in the cell to allow us to explore new derivations in a second stage. 3.2 Second pass: hypergraph search The goal item of the parsing step represents a sen-tence spanning hypergraph of alternative deriva-tions. Exploring alternatives from this hyper-graph and updating LM probabilities can now reveal derivations with higher scores that were not consid-ered in the first pass. Exploring the whole space of alternative derivations in this hypergraph is clearly infeasible. We propose a g -gram LM driven heuris-tic search  X  X .Search X  of this space that allows the g -gram LM to decide which section of the hypergraph to explore. By construction, traversing a particular derivation item from the parse chart in target-side left-to-right, depth-first order yields the correctly or-dered sequence of target terminals that is the transla-tion represented by this item. Now consider a partial traversal of the item in that order, where we gener-ate only the first M target terminals, leaving the rest of the item in its original backpointer form. We in-formally define our second pass algorithm based on these partial derivation items.

Consider a simple example, where we have parsed a source sentence, and arrived at a sentence spanning item obtained from a rule with the following target side: and that the item X  X  best-score estimate is w . A par-tial traversal of this item would replace NP 2 with one of the translations available in the chart cell un-derlying NP 2 (called  X  X nwinding X ), and recalculate the weights associated with this item, taking into account the alternative target terminals. Assuming  X  X he nice man X  was the target side of the best scoring item in NP 2 , the respective traversal would main-tain the same weight. An alternative item at NP 2 might yield  X  X  nice man X . This partial traversal rep-resents a possible item that we did not consider dur-ing parsing, and recalculating LM probabilities for this new item (based on approximate items VP 3 and PP 1 ) yields weight w 2 : Alternative derivation items that obtain a higher score than the best-score estimates represent recov-ery from search errors. Our algorithm is based on the premise that these items should be traversed fur-ther, with the LM continuing to score newly gener-ated target words. These partially traversed items are placed on an agenda (sorted by score). At each step of the second pass search, we select those items from the agenda that are within a search beam of Z from the best item, and perform the unwind opera-tion on each of these items. Since we unwind partial items from left-to-right the g -gram LM is able to in-fluence the search through the space of alternative derivations.

Applying the g -gram LM on partial items with leading only-terminal symbols allows the integra-tion of high-/ flexible-order LMs during this sec-ond stage process, and has the advantage of explor-ing only those alternatives that participate in sen-tence spanning, high scoring (considering both LM and translation model scores) derivations. While we do not evaluate such models here, we note that H.Search was developed specifically for the integra-tion of such models during search.

We further note that partial items that have gen-erated translations that differ only in the word po-sitions up to g  X  1 words before the first nonter-minal site can be recombined (for the same rea-sons as during LM intersected parsing). For exam-ple, when considering a 3-gram LM, the two par-tial items above can be recombined into one equiv-alence class, since partial item LM costs resulting from these items would only depend on  X  X ice man X , but not on  X  X  X  vs.  X  X he X . Even if two partial items are candidates for recombination due to their termi-nal words, they must also have identical backpoint-ers (representing a set of approximate parse deci-sions for the rest of the sentence, in our example VP 3 PP 1 ). Items that are filed into existing equiv-alence classes with a lower score are not put onto the agenda, while those that are better, or have cre-ated new equivalence classes are scheduled onto the agenda. For each newly created partial derivation, we also add a backpointer to the  X  X arent X  partial derivation that was unwound to create it.

This equivalence classing operation transforms the original left-hand side NT based hypergraph into an (ordinary) graph of partial items. Each equiva-lence class is a node in this new graph, and recom-bined items are the edges. Thus, N -best extraction can now be performed on this graph. We use the extraction method from (Huang and Chiang, 2005).
The expensive portion of our algorithm lies in the unwinding step, in which we generate a new par-tial item for each alternative at the non-terminal site that we are  X  X nwinding X . For each new partial item, we factor out LM estimates and rule weights that were used to score the parent item, and factor in the LM probabilities and rule weights of the alter-native choice that we are considering. In addition, we must also update the new item X  X  LM estimates for the remaining non-terminal and terminal sym-bols that depend on this new left context of termi-nals. Fortunately, the number of LM calculations per new item is constant, i.e., does not dependent on the length of the partial derivation, or how unwound it is. Only ( g  X  1)  X  2 LM probabilities have to be re-evaluated per partial item. We now define this  X  X nwind-recombine X  algorithm formally. 3.2.1 The unwind-recombine algorithm
Going back to the first-pass parsing algorithm (Figure 1), remember that each application of a grammar rule containing nonterminals corresponds to an application of the third inference rule of the algorithm. We can assign chart items C created by the third inference rule a back-pointer (BP) target side as follows: When applying the third inference rule, each nonterminal occurrence ( X k ) k in the cor-responding Z  X   X   X ,  X   X  grammar rule corresponds to a chart cell [ X k , i k , j k ] used as an antecedent for the inference rule. We assign a BP target side for C by replacing NT occurrences in  X  (from the rule that created C ) with backpointers to their corresponding antecedent chart cells. Further we define the distin-guished backpointer P S as the pointer to the goal cell [ S, 0 , n ] : w  X  .

The deductive program for our second-pass al-gorithm is presented in Figure 2. It makes use of two kind of items. The first, { P  X   X  ; e 1 } : w , links a backpointer P to a BP target side, storing current-item vs. best-item correction terms in form of an LM context e 1 and a relative score w . The second item form [[ e ;  X  ]] in this algorithm corre-sponds to partial left-to-right traversal states as de-scribed above, where e is the LM context of the tra-versed and unwound translation part, and  X  the part that is yet to be traversed and whose backpointers are still to be unwound. The first deduction rule presents the logical axioms, creating BP items for each backpointer used in a NT inference rule appli-cation during the first-pass parsing step. The sec-ond deduction rule represents the unwinding step as discussed in the example above. These deduc-tions govern a search for derivations through the hy-pergraph that is driven by updates of rule weights and LM probabilities when unwinding non-first-best hypotheses. The functions p and q are as defined in Section 2, except that the domain of q is ex-tended to BP target sides by first replacing each back-pointer with its corresponding chart cell X  X  LM context and then applying the original q on the re-sulting sequence of target-terminals and ? symbols. 2 Note that w 0 , which was computed by the first de-duction rule, adjusts the current hypothesis X  weight that is based on the first-best instance of P to the actually chosen instance X  X  weight. Further, the ra-tio p ( eq (  X  lex  X  mid )) /p ( ee 1 ) adjusts the LM prob-abilities of P  X  X  instantiation given its left context, and p [ q ( e X  lex  X  mid ) q (  X  end )] /p [ q ( ee 1 ) q (  X  justs the LM probabilities of the g  X  1 words right of P . We evaluate our two pass hypergraph search  X  X .Search X  against the strong single pass Cube Pruning (CP) baseline as mentioned in (Chiang, 2005) and detailed in (Chiang, 2007). In the latter work, the author shows that CP clearly outperforms both the naive single pass solution of severe prun-ing as well as the naive two-pass rescoring approach. Thus, we focus on comparing our approach to CP.
CP is an optimization to the intersected LM pars-ing algorithm presented in Figure 1. It addresses when generating consequent items. CP amounts to an early termination condition when generating the set of possible consequents. Instead of generating all consequents, and then pruning away the poor per-formers, CP uses the K-Best extraction approach of (Huang and Chiang, 2005) to select the best K con-sequents only, at the cost of potential search errors. CP X  X  termination condition can be defined in terms of an absolute number of consequents to generate, or by terminating the generation process when a newly generated item is worse (by  X  ) than the current best item for the same left-hand side and span. To sim-ulate comparable pruning criteria we parameterize each method with soft-threshold based criteria only (  X  for CP and Z for H.Search) since counter based limits like K have different effects in CP (selecting e labeled items) vs H.Search (selecting rules since items are not labeled with e ). We present results on the IWSLT 2006 Chinese to English translation task, based on the Full BTEC corpus of travel expressions with 120K parallel sen-tences (906K source words and 1.2m target words). The evaluation test set contains 500 sentences with an average length of 10.3 source words.

Grammar rules were induced with the syntax-based SMT system  X  X AMT X  described in (Zoll-mann and Venugopal, 2006), which requires ini-tial phrase alignments that we generated with  X  X IZA++ X  (Koehn et al., 2003), and syntactic parse trees of the target training sentences, generated by the Stanford Parser (D. Klein, 2003) pre-trained on the Penn Treebank. All these systems are freely available on the web.

We experiment with 2 grammars, one syntax-based (3688 nonterminals, 0.3m rules), and one purely hierarchical (1 generic nonterminal, 0.05m rules) as in (Chiang, 2005). The large number of nonterminals in the syntax based systems is due to the CCG extension over the original 75 Penn Tree-bank nonterminals. Parameters  X  used to calculate P ( D ) are trained using MER training (Och, 2003) on development data. We evaluate each approach by considering both search errors made on the development data for a fixed set of model parameters, and the BLEU metric to judge translation quality. 6.1 Search Error Analysis While it is common to evaluate MT quality using the BLEU score, we would like to evaluate search errors made as a function of  X  X ffort X  made by each algo-rithm to produce a first-best translation. We con-sider two metrics of effort made by each algorithm. We first evaluate search errors as a function of novel queries made to the g -gram LM (since LM calls tend to be the dominant component of runtime in large MT systems). We consider novel queries as those that have not already been queried for a particular sentence, since the repeated calls are typically effi-ciently cached in memory and do not affect runtime significantly. Our goal is to develop techniques that can achieve low search error with the fewest novel queries to the g -gram LM.

To appreciate the practical impact of each algo-rithm, we also measure search errors as a function of the number of seconds required to translate a fixed unseen test set. This second metric is more sensitive to implementation and, as it turned out, even com-piler memory management decisions.

We define search errors based on the weight of the best sentence spanning item. Treating weights as negative log probabilities (costs), we accumulate the value of the lowest cost derivation for each sentence in the testing data as we vary pruning settings ap-propriate to each method. Search errors are reduced when we are able to lower this accumulated model cost. We prefer approaches that yield low model cost with the least number of LM calls or number of sec-onds spent decoding.

It is important to note that model cost (  X  log( P ( D )) ) is a function of the parameters  X  which have been trained using MER training. The parameters used for these experiments were trained with the CP approach; in practice we find that either approach is effective for MER training. 6.2 Results Figure 3 and Figure 4 plot model cost as a function of LM cache misses for the IWSLT Hierarchical and Syntax based systems, while Figure 5 plots decod-ing time. The plots are based on accumulated model cost, decoding time and LM cache misses over the IWSLT Test 06 set. For H.Search, we vary the beam parameter Z for a fixed value of  X  = 5 during pars-ing while for CP, we vary  X  . We also limit the total number of items on the agenda at any time to 1000 for H.Search as a memory consideration. We plot each method until we see no change in BLEU score for that method. BLEU scores for each parameter setting are also noted on the plots.

For both the hierarchical and syntax based gram-mars we see that the H.Search method achieves a given model cost  X  X arlier X  in terms of novel LM calls for most of the plotted region, but ultimately fails to achieve the same lowest model cost as the CP method. 3 While the search beam of Z mit-igates the impact of the estimated scores during H.Search X  X  second pass, the score is still not an ad-missible heuristic for error-free search. We suspect that simple methods to  X  X nderestimate X  the score of a partial derivation X  X  remaining nonterminals could bridge this gap in search error. BLEU scores in the regions of lowest model cost tend to be reason-ably stable and reflect comparable translation per-formance for both methods.

Under both H.Search and CP, the hierarchical grammar ultimately achieves a BLEU score of 19.1%, while the syntactic grammar X  X  score is ap-proximately 1.5 points higher at 20.7%. The hierar-chical grammar demonstrates a greater variance of BLEU score for both CP and H.Search compared to the syntax-based grammar. The use of syntac-tic structure serves as an additional model of target language fluency, and can explain the fact that syn-tax based translation quality is more robust to differ-ences in the number of g -gram LM options explored.
Decoding time plots shows a similar result, but with diminished relative improvement for the H.Search method. Profiling analysis of the H.Search method shows that significant time is spent simply on allocating and deallocating memory for partial derivations on top of the scoring times for these items. We expect to be able to reduce this overhead significantly in future work. We presented an novel two-pass decoding approach for PSCFG-based machine translation that achieves search errors comparable to the state of the art Cube Pruning method. By maintaining comparable, sen-tence spanning derivations we allow easy integration of high or flexible order LMs as well as sentence level syntactic features during the search process. We plan to evaluate the impact of these more power-ful models in future work. We also hope to address the question of how much search error is tolerable to run MER training and still generate parameters that generalize well to test data. This point is particularly relevant to evaluate the use of search error analysis.
