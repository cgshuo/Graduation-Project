 DEEPTI KHANDUJA, NEETA NAIN, and SUBHASH PANWAR , In the field of pattern recognition, character recognition is regarded as one of the most interesting and challenging steps for script processing. The handwritten character recognizer ( HCR ), a tool to recognize and convert scanned text script images into digitized form, broadly consists of three major stages: preprocessing, segmentation, and recognition. HCR initializes with preprocessing that includes binarization, skew, and slant normalization of the text document. In further steps, the preprocessed text image is divided into lines, lines are divided into words, and finally each word is partitioned into individual characters. In the simplest case, this character is fed to the recognition algorithm; however, to avoid complexity of the recognition system, a set of features is extracted for each character in order to reduce its dimensionality, and this reduced representation of character is fed into the classifier to classify the character into its appropriate class. To make this feature set extraction task more effective and refined, a new preprocessing step applied on individual characters, named thinning, has been introduced in this article. Since feature vector is the direct input to the recognition step, accuracy of the system depends on the effectiveness of the vector obtained. A number of approaches are introduced earlier to simulate the feature extraction, like character zone based [Rajashekararadhya and Ranjan 2009], direction based [Basli 2003], neural network based [Pradeep 2011], and transform based [Mowlaei 2002]. In most of the approaches, either structural features are estimated or spacial and statistical features are calculated. In this article, we have proposed an approach based on both structure and spacial distribution.

The article gives the details of the proposed approaches in Section 2. Section 4 defines the neural network classifier applied. Section 3 provides justification of the various heuristics used in the approach. Section 6 gives a performance analysis with existing techniques. Results and conclusions are appended in Section 5 and 7, respectively. The proposed approach after preprocessing and noise cleaning [Khanduja 2013] has been prorated in four steps: thinning, structural feature extraction, statistical feature extraction, and the complete approach. Before applying any feature extraction mechanism, as a preprocessing step, thinning is introduced. Thinning is a form of data compression technique that helps in reducing the amount of information input to the pattern recognition algorithm while maintain-ing the configuration of the pixels and topology of the image. Character recognition is a major research area that utilizes thinning to expedite the extraction of critical structural features of a character. Another crucial aspect of thinning is that the output should be located approximately to the medial axis of the character stroke. This step reduces chaos in the structural elements of the character and retains their generality more effectively. The proposed thinning algorithm is based on raster scan of the char-acter image, considering one pixel at a time and detecting the subsequent pixels by raster scanning. The algorithm for thinning is summarized in Algorithm 1.

The algorithm for thinning has been extensively tested on several Devanagari script characters, with the summarized results presented in Table I. Feature set generation is the method of converting highly redundant, variable, and diverse data to a small-in-size, robust, abstract, and complete set that conveys all features of the original data. Generally, data with greater size increases complexity and becomes time consuming for applications to process. Hence, mapping to a corresponding small set of data without loss of significant information makes the task easier.
For character recognition, extracting those features that are essential to differentiate among characters is a tedious task. A simple method that generates the most appro-priate and complete set of features is always needed. In this article, we have tried to fabricate a simple mathematical approach in Algorithm 2 based on a combination of statistical and structural feature extraction techniques.
 Structural features are used to acquire the topological information of the characters, ensuring minimal effect of shape/font/size variation on the feature set. The proposed technique captures the endpoint (EndPts), intersection point (IntersectPts), and pres-ence of hole (HasLoop) in the character image as shown in Figures 2(a) through 2(c). EndPts are composed of the number of single neighborhood pixels, whereas IntersectPts are composed of three or more neighborhood pixels. The HasLoop feature is true if the character contains one/more closed loop or hole is/are present in the character image. The procedure for computing structural features is illustrated in Procedure 3. Procedure StructuralFeatures(Image).
Figures 3(a) through 3(d) present the results of implementing Algorithm 2 on sample character images. To derive the enhanced features from the character image that can significantly capture the trend of foreground pixel distribution, as a statistical measure in a processed image, Procedure StaisticalFeatures(Zone i ). a two-way method is applied. First, a curve is fit on the character and the coefficients of the curve equation as features are determined; second, a count of the number of foreground pixels is used as a direct measure of distribution. To further improve the resolution of process output, we have segmented the character image into fixed small blocks of size 5  X  5. Each such block will derive a curve and count of foreground pixels.
Curve fitting is defined as a way to capture the trend of spatial distribution of pixels by assigning a single function across the entire range possibly subject to constraints. Each constraint can be a point, angle, or curvature. To fit a curve on the specified skeleton pixels, we postulate a function form f(x) as shown in Equation (4), a polynomial function of second order to describe the general trend in the data. Figure 4 shows the results of quadratic curve fitting with the coefficients labeled:
Let IR n  X  IR k be a smooth mapping from a set of n-dimensional data points D = p ,..., p Z ( f ) = x : f ( x ) = 0.

Let Z(f) be the set of zeros of function f =  X   X  : IR n  X  IR k .As  X  is known and also  X  (  X , p denote the distance from some x to Z (  X   X  ). This results in  X  2 distribution as defined by Equation (5) with variance  X  2 :
Curve fitting corresponds to the minimization of Equation (5) with respect to pa-rameters  X  1 , X  2 ..... X  q . Assuming that variance  X  2 is known, the problem is equivalent to minimizing the approximate Mean Square Error ( MSE ) in Equation (6) from the dataset D to the set of zeros of f =  X   X  as shown in Figure 5: All types of curves can be represented with the help of the n th order polynomial of arises while fitting a curve over some structure is what order ( n ) will be sufficient and optimally fit the structure. This depends both on the structure of the curve and on the number of data points available.

Let us understand these two aspects one by one. A close watch over the structure of Devanagari vowels, consonants, and in turn words gives a glimpse that almost all structures are of the nature of either straight lines, loops (circles), or half curves, which could be approximated using polynomials. For example, choosing an ( n  X  1) th order poly-nomial to fit n data points forces the curve through every point. However, by doing this, it has to be decided that no random variation exists in any data point. In such a case, interpolation, extrapolation, and differentiation of the resulting polynomial are ex-tremely unreliable, leading to oscillations and erratic behavior. To minimize this risk, it is better that a lower-order polynomial be chosen to fit the general tendency of the data. And the lowest nonlinear polynomial that can represent curves is the quadratic polynomial. Compared to linear curves, quadratic polynomials are more flexible, and compared to higher-degree polynomials, they are more stable and less complex. A re-gression analysis of various degrees of polynomials is illustrated in Table II. This further asserts the usage of quadratic polynomials as a feature vector. The basic prop-erty of polynomials is that they can possess one less peak and valley than their orders. Maxima or minima representing peak and valley respectively can be found by finding the values of x , where the derivative df / dx is zero using Equation (8):
Assuming a dataset, that is, zoned skeleton data points ( X , Y ) = ( x 1 , y 1 ) , (( x fit the data points using some measure of performance. A measure of performance that we have used is the empirical risk [Perez-Cruz et al. 2003]: where C ( f ( x ) , y ) is the cost residual function f ( x )  X  y .

It can be said the we need to find the function f ( x ) that minimizes the average risk in terms of fitting the curve on the dataset. On the contrary, the law of large numbers ensures that the empirical risk asymptotically converges to the expected risk for f ( x )  X  &gt;  X  , and for small samples, it cannot always be guaranteed that Empirical Risk Minimization ( ERM ) will also minimize the expected risk. Thus, we combine the approach of VC dimension [Hush and Scovel 2001] to deduce the class of functions f (  X  ) that shatters the dataset; that is, for every possible dichotomy, there is a function in f (  X  ) that models it. The VC dimension VC ( f ) is the size of the largest dataset that can be shattered by the set of functions f (  X  ). If the VC dimension of f (  X  )is , then there exists at least one set of points that can be shattered by f (  X  ). VC dimensions provide a bound on the expected risk as a function of the empirical risk.

For our experiments, the expected risk, that is estimated error on future data trend, is computed by using the empirical risk and VC confidence as shown in Equation (9): where h is the VC dimension of f (  X  )and N is the number of data points; also, N &gt; h . As the ratio N / h increases, the VC confidence decreases and the actual risk gets closer to empirical risk. A function f (  X  )with h points is said to generalize the set of data points if R ( f (  X  )) is approximately equal to 0.

In the proposed approach, experiments have been done by zoning the character into 4  X  4, 5  X  5 , and 6  X  6 zones, resulting in the average skeleton points with 8, 10 , and 12 pixels per zone, respectively. On computing the expected risk in each case, the value of R ( f ) is the resulting minimum for the 5  X  5 zone. For a random case, for the optimal quadratic curve y = X  17 . 556 x 2 + 31 . 930 x + 0 . 003, the corresponding minimal empirical loss is Remp = 6 . 91 and expected error R ( f ) = 0 . 78. Thus, in the proposed technique, the character image of 50  X  50 pixels is partitioned into 5  X  5 horizontal and vertical zones.

The reason to implement the hybrid feature extraction approach has been proved by measuring the performance of statistical, structural, and proposed techniques individually in Table III. The statistical feature extraction is based on discriminating the data using quantitative features of data, which hinders the detection of morphologi-cal patterns in the character. Furthermore, extracting features based on the statistical distribution of pixels in a character is inadequate to diagnose distortions and style variations. The classification stage uses the feature vector generated in the feature extraction stage to identify the text image. Classification is accomplished by a two-layer feed-forward neural network with back-propagation learning. The topology of the classifier consists of input and output layers with neurons determined by the length of feature vector and number of classes, respectively. The number of neurons in the first and second hidden layer are 70 and 40 , respectively. The network employs the gradient descendent rule in an attempt to minimize the squared error between the network output values and the target values for these outputs. The trained network is used to test an input character image entered by the user and the classifier maps any input pattern to a number of classifications. The probability density function ( pdf )ofeach of the classes is generated, and then an unknown image, X , belongs to class i if f i (X) &gt; f (X),  X  j = i, where f k is the pdf for class k . The proposed feature set generation technique has been trained and tested on 22,556 standard dataset samples. Tenfold cross-validation is used for testing results using 75% of the data as training and 25% for validation. The samples consist of handwrit-ten Hindi numerals, the character open-source database  X  X evanagari numeral and character database for offline handwritten character recognition X  [Dongre 2012], and a database obtained by the CVPR Unit, ISI Kolkata [Bhattacharya and Chaudhuri 2009]. The results for the various testing parameters have been summarized in Table IV. Equations (10) and (11) present the accuracy and error measure of the proposed method, which achieves an average accuracy of 93 . 4% and a false ratio of 6 . 6%: where TP = true positive, TN = true negative, FP = false positive, and FN = false negative. The proposed feature extraction technique has been compared with existing techniques with their proposed feature vector and parameter settings on the same datasets. The database used for comparison is obtained by ISI Kolkata [Bhattacharya and Chaudhuri 2009], of Devanagari numerals. The comparative performance details are presented in Table V. It is observed that the proposed approach by using a simple MLP classi-fier gives approximately the same results as obtained by using the complex classifier combination of HMM and ANN .

For comparative analysis of Devanagari characters, most of the approaches [Umapada Pal et al. 2008; Hanmandlu 2007; Sharma 2006] have used their own datasets, which are not publicly available for comparison. For comparative analysis though, we illustrate their claimed and published results with our proposed configura-tion in Table VI. This article proposes a model of an efficient feature set extraction technique using statistical and structural features of the text image for script-independent character recognition (it has been tested on English and Urdu also). Results generated by the experiments on the testbed images conform to the greater level of efficiency with respect to the size of the feature set, time utilized to train the network, and its complexity. The approach of Quadratic Polynomial Curve Fitting used for generating a feature set has been experimentally derived by testing various degrees of polynomials. For n data points, the complexity of the algorithm is O ( kn ), where k is the number of iterations. As the image is divided into 25 zones, the complexity achieves O(25*kn)  X  O(kn). As a measure of statistical comparison, the proposed approach gives accuracy of 93% in an established testbed environment These results uphold the application of the approach to generate feature vectors. Experimental results prove that the algorithm shows incorrect results for characters with similar structural appearance or character images resembling other characters due to writer variations. Some sample characters providing false results are shown in Figure 6; however, most of these characters are difficult to be recognized even by humans.

