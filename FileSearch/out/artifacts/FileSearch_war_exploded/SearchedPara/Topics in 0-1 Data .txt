 of them tends to increase the probability of seeing value 1 for the other variables. The term "topic" comes from in-formation retrieval: if a document concerns a certain topic, then the occurrence of some words is more probable than in the case when the document does not concern that topic. A single document can discuss many topics, and all words belonging to a topic need not appear in a document about that topic. The concept of a topic is not restricted to document data. For example, in market basket data one can consider the customers having different topics in mind when they enter the store. A customer might for example want to purchase beer; the actual brand is selected only later, and perhaps she/he buys more than one brand. 
The problem of finding topics in data has been consid-ered using various approaches. Examples of the approaches are identification of finite mixtures, latent semantic index-ing, probabilistic latent semantic indexing, nonnegative ma-trix factorization, and independent component analysis (see, some more detail in Section 6. 
We describe a simple topic model, corresponding to a gen-erative model of the observations. The model states that there is a number of topics in the data, and that the oc-currences of topics are independent. Given that the topic occurs, the words belonging to that topic are also considered to be independent. Later, we consider an extension of the model where the probabilities of topics vary from document to document (as in, e.g., [11, 14]). 
The first question to address is whether actual data sets can be considered to be results of such generative process. Our definition of topic models implies, e.g., that negative correlations between variables are absent. We show that this is indeed the case on real data sets: while there are negative correlations, they are typically quite weak and cannot be considered to be violations of the model. 
Given the class of topic models, the problem is then whether the model parameters can be estimated from the data. Our variate Bernoulli distributions, a nonidentifiable class [10]. However, while in Bernoulli distributions the information obtained from () and 1 are on an equal footing, in our model the values 0 and 1 are not symmetric. This implies that for models where the topics are almost disjoint (e.g., the e-separability condition of Papadimitriou et al. [14]) we can ef-ficiently identil~y the topics and their parameters. Our main focus is whether there are some simple theoretical arguments that imply that; simple topic models can be estimated from 
The rest of this paper is organized as follows. In Sec-
In this section we first introduce the simple topic model 
Given a set U of attributes, a k-topic model T = (~, q) 
A document is sampled from 7-as follows. First, one 
Given a topic model 7-= (~, q), the weight w(i) of topic i ~A~V q(i, A), i.e., the expected value of ones gener-primary set of attributes of topic i, and for A E Ui we 
Figure 1 illustrates an e-separable topic model. The at-
A possible drawback with the above model for the gen-the probability in the data of the event A = 1 p(AB) the probability of A = lAB = 1. Then 
Furthermore, p(AB) = s,q(i, A)q(i, B). Thus we r(A,B) = p(A)p(B)/p(AB) for all pairs A and are close to a constant, and between subsets most a~ ~ (r(A,B)-7,) 2 
This almost trivial method actually works quite nicely on 
Probe algorithm. Our second method is still quite sim-
The probe distance d(A, B) of two attributes is defined by 
Our algorithm is as follows. Compute distances d(A, B) main focus; rather, we aim at giving results indicating why 
In this section we consider the properties of the probe algorithm given in the previous section. We first consider the case of 0-separable models, which naturally are quite simple. We show that for large sample sizes the distance be-tween two attributes in the same topic tends to 0, and that the expected distance between two attributes belonging to different topics is quite large. We then consider the case of e-separable models, and show that the same results con-tinue to hold under some additional conditions. Most of the results are formulated under the assumption of no sample effects, i.e., by assuming infinite sample size. 
We start with a lemma showing that for 0-separable mod-els the distance between two attributes in the same topic goes to 0 as the sample size grows. 
To evaluate how well do our algorithms perform, we gener-ated artificial data according to our topic models described in Section 2. The data consisted of 100 attributes and 10 topics, each topic having a random number of primary at-tributes, and the number of observations was 100000. We performed tests on a e-separable model with ~ --0, 0.01 and 0.1. In all experiments with the first (constant topic proba-bilities) model, the topic probabilities s~ were the same, so that we were able to test the effect of e in model estimation accuracy. 
Ratio algorithm. First we considered the ratios r(A, B) = p(A)p(B)/p(AB). Recall that this should yield si, probabil-otherwise, as then A and B are independent and their joint probability is separable. By listing these ratios in a matrix one can easily distinguish which topics belong to the same topic, as all of them have approximately the same ratio. In this way we can estimate the topic structure of the data, and also the topic probabilities s~ and topic-attribute probabili-ties q(i, A) of A in topic i. Comparing to the true probabil-ities, the mean squared errors (MSE) of topic probabilities and the MSEs of topic-attribute probabilities are listed in Table 1 for ~ --0, 0,01 and 0.1. These figures are averages of 10 experiments. The variance between experiments was very small. 
E MSE of topic probs. MSE of topic-attr, probs. 0 0.92.10 -4 1.00.10 -~ 0.01 1.04.10 -4 1.02.10 -3 0.1 1.01  X  10 -4 1.03.10 -3 Table 1: Mean squared errors of estimated topic and topic-attribute probabilities in the ratio algorithm. 
In our varying-probability topic model, the topic proba-bilities sl are randomly drawn for each document, and the ratio algorithm is not applicable. 
Probe algorithm. Sammon mapping [17] is a conve-nient way to visualize how the attributes are grouped into distict topics. Figure 2 shows the Sammon map of the probe distances of the attributes in the 0-separable model. We can see that the attributes are nicely grouped into about 10 clus-ters, most of which are clear in shape. The clusters are not of equal size, as each topic has a random number of primary attributes. In the case of  X  = 0.01, the clusters are a bit more vague in shape but still visible; with  X  = 0.1, no clear clusters are seen anymore. The probe algorithm is quite re-sistant to the extension of varying topic probabilities: the Sammon maps are remarkably similar to those obtained for the nonvarying-probability topic models. 
Maximum entropy model. We also considered whether the maximum entropy method described in e.g. [16, 15] might be useful in finding topics. The method is used to answer queries about the data as follows: first, one mines frequent sets with some threshold [1, 2], and then finds the maximum entropy distribution [3, 9] consistent with the fre-quent sets. We performed experiments using simulated data to see whether the results are consistent with the topic mod-els used to generate the data. The results (not shown) indi-cate that this method does find results consistent with topic 
Correlations. To determine the validity of the model 
After preprocessing, we determined the probabilities p(A) p(AB) for all words A, t3 (using word frequencies) and 
Probe algorithm. We studied the behavior of the probe 
The probe distances of the terms were computed, and Figure 3: Histogram of ln(Icov(A,B)l ) for positive (solid) and negative (dashdotted) eovariances for words A, B in Theory. A short vertical line marks ln(1/67066) = -11.1. dist. terms dist. terms 0.50 stoc focs 0.91 jacm libtr 0.63 infctrl tcs i0.92 extended abstract 0.63 tr libtr 0.93 stacs icalp 0.67 icalp tcs 0.94 actainf tcs 0.75 infctrl icalp 0.95 fct jess 0.76 eurocrypt crypto 0.95 fct mfcs 0.79 mfcs tcs 0.96 stacs jcss 0.81 infctrl jcss 0.96 jacm tr 0.81 mfcs icalp 0.96 i sijdm damath 0.81 jcss tcs I 0.97 ipps jpdc 0.84 mfcs infctrl' 0.98 stoc tr 0.86 mfcs jcss 0.98 icpp jpdc 0.88 jess icalp 0.99 sicomp libtr 0.88 ipps icpp 0.99 stacs infctrl 0.89 mst jcss 0.99 stacs tcs Table 2: Term pairs with minimum probe distance in the Theory data set on Foundations of Computer Science; 'infctrl' is Information and Computation (formerly Information and Control) and 'tcs' is Theoretical Computer Science. For each term pair, the pair members belong to the same topical field, be it theoretical computer science, technical reports, cryptogra-phy, parallel processing, discrete mathematics etc. All these terms appear quite often in the data base, which makes the estimation of their probe distances reliable. 
Does the method find topics? For example, listing the 10 terms with minimum probe distance to 'stoc' we get 'focs', and 'ipl'. Computing the average distances of every term in this list to all other terms in the list, and taking the average of these averages, we get a distance of 1.17. On the other hand, computing the average distances of every term in this list to all other terms in the vocabulary, and again taking the average, yields 2.30. So the terms close to 'stoc' are also very close to one another but less close to other terms, and can thus be seen as forming a sort of topic. A similar comparison can be done to the closest neighbors of distances. 
We used Sammon's mapping to project the data into two 
For comparison, we also projected the data into its 20-
The idea of looking at topics in 0-1 data (or other discrete al. gave some arguments justifying the performance of LSI. 
Their basic model is quite general and we have adopted their basic formalism; to obtain the results on LSI they have to re-strict the documents to stem from a single topic. Of course, some restrictions are unavoidable. Hofmann [11] has considered the case of probabilistic LSI. 
His formal model is close to ours, having the form P(wld ) = ~z P(zld)P(wlz), where the z's are topics, d refers to a document, and w to a word. Hofmann's main interest is in good estimation of all the parameters using the EM al-gorithm, while we are interested in having some reasoning explaining why the methods would find topics. 
Cadez et al. [4] have considered the estimation of topic-like market-basket data, with the added complication that the same customer has multiple transactions, leading to the need of individual weights. 
Our topic models are fairly close to the class of finite mix-tures of multivariate Bernoulli distributions, a nonidentifi-able class [10] (see also [5]). However, for those models, the values 0 and 1 have symmetric status, while for the topic models defined above this is not the case. We conjecture that the class of topic models is essentially identifiable provided that the topics are almost disjoint in, e.g., the e-separability sense. 
In nonnegative matrix factorization (NMF), an observed data matrix V is presented as a product of two unknown matrices: V = WH. All three matrices have nonnegative entries. Lee and Seung [13] give two practical algorithms for finding the matrices W and H given V. Restriction to binary variables is not straightforward in these algorithms. 
Independent component analysis (ICA) [6, 12] is a statisti-cal method that expresses e~ set of observed multidimensional sequences as a combination of unknown latent variables that are more or less statistically independent. Topic identifica-tion in 0-1 data can be interpreted in the ICA terminology as finding latent binary sequences, unions of which form the ob-served binary data. ICA in its original form relies heavily on matrix operations; for sparse data, union is roughly equiva-lent to summation, so methods for ICA could be considered for the problem at hand. Nevertheless, most existing ICA al-gorithms are suitable for continuosly distributed data with 
Gaussian noise --the case of 0-1 variables and Bernoulli noise is quite different, and practical ICA algorithms tend to fail in this case. 
We have considered the problem of finding topics in 0-1 data. We gave a formal description of topic models, and showed that relatively simple algorithms can be used to find topics from data generated using such models. We showed that the probe algorithm works reasonably well in practice. 
Lots of open issues remain, both on the theoretical and on the practical side. The detailed relationship of our model compared to, e.g., Hofmann's model remain to be studied. 
We conjecture that the topic models are identifiable, in con-trast with general mixtures of multivariate Bernoulli distri-butions. Understanding the behavior of LSI is still open. 
Similarly, seeing how nonnegative matrix factorization is connected to the other approaches is open, as are the ways of extending ICA to the Bernoulli case. association rules between sets of items in large databases. In SIGMOD '93, pages 207-216, 1993. A. I. Verkamo. Fast discovery of association rules. In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, Advances in Knowledge Discovery and Data Mining, chapter 12, pages 307-328. AAAI Press, 1996. Della Pietra. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39-71, 1996. modeling of transaction data with applications to profiling, visualization, and prediction. In KDD ~001, pages 37--46, San Fransisco, CA, Aug. 2001. identifiability of finite mixtures of multivariate Bernoulli distributions. Neural Computation, 12:141-152, 2000. concept? Signal Processing, 36:287-314, 1994. attributes by external probes. In Knowledge Discovery and Data Mining, pages 23-29, 1998. G. W. Furnas, and R. A. Harshman. Indexing by latent semantic analysis. Journal of the American Inducing features of random fields. IEEE Transactions 19(4):380-393, 1997. Non-uniqueness in probabilistic numerical identification of bacteria. Journal of Applied Probability, 31:542-548, 1994. 
In SIGIR '99, pages 50--57, Berkeley, CA, 1999. Component Analysis. John Wiley &amp; Sons, 2001. objects by non-negative matrix factorization. Nature, 401:788-791, Oct. 1999. 
S. Vempala. Latent semantic indexing: A probabilistic analysis. In PODS '98, pages 159-168, June 1998. models for query approximation with large sparse binary datasets. In UAI-$O00, 2000. for transaction data. In KDD POOl, 2001. 18(5):401--409, May 1969. 
