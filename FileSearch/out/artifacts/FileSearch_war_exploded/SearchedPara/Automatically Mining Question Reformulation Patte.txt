 More and more web users tend to use natural lan-guage questions as queries for web search. Some commercial natural language search engines such as InQuira and Ask have also been developed to answer this type of queries. One major challenge is that var-ious questions can be formulated for the same infor-mation need. Table 1 shows some alternative expres-sions for the question  X  X ow far is it from Boston to Seattle X . It is difficult for search systems to achieve satisfactory retrieval performance without consider-ing these alternative expressions.

In this paper, we propose a method of automat-ically mining 5w1h question 1 reformulation pat-terns to improve the search relevance of 5w1h ques-tions. Question reformulations represent the alter-native expressions for 5w1h questions. A question reformulation pattern generalizes a set of similar question reformulations that share the same struc-ture. For example, users may ask similar questions  X  X ow far is it from X 1 to X 2  X  where X 1 and X 2 represent some other cities besides Boston and Seat-tle. Then, similar question reformulations as in Ta-ble 1 will be generated with the city names changed. These patterns increase the coverage of the system by handling the queries that did not appear before but share similar structures as previous queries.
Using reformulation patterns as the key concept, we propose a question reformulation framework. First, we mine the question reformulation patterns from search logs that record users X  reformulation behavior. Second, given a new question, we use the most relevant reformulation patterns to generate question reformulations and each of the reformula-tions is associated with its probability. Third, the original question and these question reformulations are then combined together for retrieval.

The contributions of this paper are summarized as two folds. First, we propose a simple yet effective approach to automatically mine 5w1h question re-formulation patterns. Second, we conduct compre-hensive studies in improving the search performance of 5w1h questions using the mined patterns. In the Natural Language Processing (NLP) area, dif-ferent expressions that convey the same meaning are referred as paraphrases (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Pang et al., 2003; Pas  X ca and Dienes, 2005; Bannard and Callison-Burch, 2005; Bhagat and Ravichandran, 2008; Callison-Burch, 2008; Zhao et al., 2008). Para-phrases have been studied in a variety of NLP appli-cations such as machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006), ques-tion answering (Ravichandran and Hovy, 2002) and document summarization (McKeown et al., 2002). Yet, little research has considered improving web search performance using paraphrases.

Query logs have become an important resource for many NLP applications such as class and at-tribute extraction (Pas  X ca and Van Durme, 2008), paraphrasing (Zhao et al., 2010) and language mod-eling (Huang et al., 2010). Little research has been conducted to automatically mine 5w1h question re-formulation patterns from query logs.
 Recently, query reformulation (Boldi et al., 2009; Jansen et al., 2009) has been studied in web search. Different techniques have been developed for query segmentation (Bergsma and Wang, 2007; Tan and Peng, 2008) and query substitution (Jones et al., 2006; Wang and Zhai, 2008). Yet, most previous research focused on keyword queries without con-sidering 5w1h questions. Our framework consists of three major components, which is illustrated in Fig. 1.
 3.1 Generating Reformulation Patterns From the search log, we extract all successive query pairs issued by the same user within a certain time period where the first query is a 5w1h question. In such query pair, the second query is considered as a question reformulation. Our method takes these query pairs, i.e. Set = { ( q, q r ) } , as the input and outputs a pattern base consisting of 5w1h question reformulation patterns, i.e. P = { ( p, p r ) } ). Specif-ically, for each query pair ( q, q r ) , we first collect all common words between q and q r except for stop-words ST 2 , where CW = { w | w  X  q, w  X  q  X  , w /  X  ST } . For any non-empty subset S i of CW , the words in S i are replaced as slots in q and q r to con-struct a reformulation pattern. Table 2 shows exam-ples of question reformulation patterns. Finally, the patterns observed in many different query pairs are kept. In other words, we rely on the frequency of a pattern to filter noisy patterns. Generating patterns using more NLP features such as the parsing infor-mation will be studied in the future work. 3.2 Generating Question Reformulations We describe how to generate a set of question refor-mulations { q new r } for an unseen question q new .
First, we search P = { ( p, p r ) } to find all ques-tion reformulation patterns where p matches q new . Then, we pick the best question pattern p  X  accord-ing to the number of prefix words and the total num-ber of words in a pattern. We select the pattern that has the most prefix words, since this pattern is more likely to have the same information as q new . If sev-eral patterns have the same number of prefix words, we use the total number of words to break the tie.
After picking the best question pattern p  X  , we fur-ther rank all question reformulation patterns con-taining p  X  , i.e. ( p  X  , p r ) , according to Eq. 1.
Finally, we generate k question reformulations q r by applying the top k question reformulation patterns containing p  X  . The probability P ( p r | p  X  ) as-sociated with the pattern ( p  X  , p r ) is assigned to the corresponding question reformulation q new r . 3.3 Retrieval Model Given the original question q new and k question re-formulations { q new r } , the query distribution model (Xue and Croft, 2010) (denoted as QDist) is adopted to combine q new and { q new r } using their associated probabilities. The retrieval score of the document D , i.e. score ( q new , D ) , is calculated as follows:
In Eq. 2,  X  is a parameter that indicates the prob-ability assigned to the original query. P ( p r the probability assigned to q new r P ( q  X  | D ) are calculated using the language model (Ponte and Croft, 1998; Zhai and Lafferty, 2001). A large scale search log from a commercial search engine (2011.1-2011.6) is used in experiments. From the search log, we extract all successive query pairs issued by the same user within 30 minutes (Boldi et al., 2008) 3 where the first query is a 5w1h question. Finally, we extracted 6,680,278 question reformulation patterns.

For the retrieval experiments, we randomly sam-ple 10,000 natural language questions as queries from the search log before 2011. For each question, we generate the top ten questions reformulations. The Indri toolkit 4 is used to implement the language model. A web collection from a commercial search engine is used for retrieval experiments. For each question, the relevance judgments are provided by human annotators. The standard NDCG@ k is used to measure performance. 4.1 Examples and Performance Table 3 shows examples of the generated questions reformulations. Several interesting expressions are generated to reformulate the original question.
We compare the retrieval performance of using the question reformulations (QDist) with the perfor-mance of using the original question (Orig) in Table 4. The parameter  X  of QDist is decided using ten-fold cross validation. Two sided t-test are conducted to measure significance.

Table 4 shows that using the question reformula-tions can significantly improve the retrieval perfor-mance of natural language questions. Note that, con-sidering the scale of experiments (10,000 queries), around 3% improvement with respect to NDCG is a very interesting result for web search. 4.2 Analysis In this subsection, we analyze the results to better understand the effect of question reformulations.
First, we report the performance of always pick-ing the best question reformulation for each query (denoted as Upper) in Table 5, which provides an upper bound for the performance of the question re-formulation. Table 5 shows that if we were able to always picking the best question reformulation, the performance of Orig could be improved by around 30% (from 0.2926 to 0.3826 with respect to NDCG@1). It indicates that we do generate some high quality question reformulations.

Table 6 further reports the percent of those 10,000 queries where the best question reformulation can be observed in the top 1 position, within the top 2 posi-tions and within the top 3 positions, respectively.
Table 6 shows that for most queries, our method successfully ranks the best reformulation within the top 3 positions.

Second, we study the effect of different types of question reformulations. We roughly divide the question reformulations generated by our method into five categories as shown in Table 7. For each category, we report the percent of reformulations which performance is bigger/smaller/equal with re-spect to the original question.

Table 7 shows that the  X  X ore specific X  reformula-tions and the  X  X quivalent X  reformulations are more likely to improve the original question. Reformu-lations that make  X  X orphological change X  do not have much effect on improving the original ques-tion.  X  X ore general X  and  X  X ot relevant X  reformu-lations usually decrease the performance.

Third, we conduct the error analysis on the ques-tion reformulations that decrease the performance of the original question. Three typical types of er-rors are observed. First, some important words are removed from the original question. For example,  X  X hat is the role of corporate executives X  is reformu-lated as  X  X orporate executives X . Second, the refor-mulation is too specific. For example,  X  X ow to effec-tively organize your classroom X  is reformulated as  X  X ow to effectively organize your elementary class-room X . Third, some reformulations entirely change the meaning of the original question. For example,  X  X hat is the adjective of anxiously X  is reformulated as  X  X hat is the noun of anxiously X .

Fourth, we compare our question reformulation method with two long query processing techniques, i.e. NoStop (Huston and Croft, 2010) and DropOne (Balasubramanian et al., 2010). NoStop removes all stopwords in the query and DropOne learns to drop a single word from the query. The same query set as Balasubramanian et al. (2010) is used. Table 8 re-ports the retrieval performance of different methods.
Table 8 shows that both NoStop and DropOne per-form worse than using the original question, which indicates that the general techniques developed for long queries are not appropriate for natural language questions. On the other hand, our proposed method outperforms all the baselines. Improving the search relevance of natural language questions poses a great challenge for search systems. We propose to automatically mine 5w1h question re-formulation patterns from search log data. The ef-fectiveness of the extracted patterns has been shown on web search. These patterns are potentially useful for many other applications, which will be studied in the future work. How to automatically classify the extracted patterns is also an interesting future issue. We would like to thank W. Bruce Croft for his sug-gestions and discussions.
