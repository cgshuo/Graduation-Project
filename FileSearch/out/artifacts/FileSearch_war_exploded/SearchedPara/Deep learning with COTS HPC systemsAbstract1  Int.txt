 Adam Coates acoates@cs.stanford.edu Brody Huval brodyh@stanford.edu Tao Wang twangcat@stanford.edu David J. Wu dwu4@cs.stanford.edu Andrew Y. Ng ang@cs.stanford.edu Bryan Catanzaro bcatanzaro@nvidia.com NVIDIA Corporation, 2701 San Tomas Expressway, Santa Clara, CA 95050 A significant amount of effort has been put into de-veloping deep learning systems that can scale to very large models and large training sets. With each leap in scale new results proliferate: large models in the literature are now top performers in supervised vi-sual recognition tasks (Krizhevsky et al., 2012; Cire-san et al., 2012; Le et al., 2012), and can even learn to detect objects when trained from unlabeled im-ages alone (Coates et al., 2012; Le et al., 2012). The very largest of these systems has been constructed by Le et al. (Le et al., 2012) and Dean et al. (Dean et al., 2012), which is able to train neural networks with over 1 billion trainable parameters. While such extremely large networks are potentially valuable objects of AI research, the expense to train them is overwhelming: the distributed computing infrastructure (known as  X  X istBelief X ) used for the experiments in (Le et al., 2012) manages to train a neural network using 16000 CPU cores (in 1000 machines) in just a few days, yet this level of resource is likely beyond those available to most deep learning researchers. Less clear still is how to continue scaling significantly beyond this size of network. In this paper we present an alternative approach to training such networks that leverages in-expensive computing power in the form of GPUs and introduces the use of high-speed communications in-frastructure to tightly coordinate distributed gradient computations. Our system trains neural networks at scales comparable to DistBelief with just 3 machines. We demonstrate the ability to train a network with more than 11 billion parameters X 6.5 times larger than the model in (Dean et al., 2012) X  X n only a few days with 2% as many machines.
 Buoyed by many empirical successes (Uetz &amp; Behnke, 2009; Raina et al., 2009; Ciresan et al., 2012; Krizhevsky, 2010; Coates et al., 2011) much deep learning research has focused on the goal of building larger models with more parameters. Though some techniques (such as locally connected networks (Le-Cun et al., 1989; Raina et al., 2009; Krizhevsky, 2010), and improved optimizers (Martens, 2010; Le et al., 2011)) have enabled scaling by algorithmic advan-tage, another main approach has been to achieve scale through greater computing power. Two axes are avail-able along which researchers have tried to expand: (i) using multiple machines in a large cluster to in-crease the available computing power, ( X  X caling out X ), or (ii) leveraging graphics processing units (GPUs), which can perform more arithmetic than typical CPUs ( X  X caling up X ). Each of these approaches comes with its own set of engineering complications, yet significant progress has been made along each axis (Raina et al., 2009; Krizhevsky et al., 2012; Ciresan et al., 2012; Uetz &amp; Behnke, 2009; Dean et al., 2012). A clear advantage might be obtained if we can combine improvements in both of these directions (i.e., if we can make use of many GPUs distributed over a large cluster). Unfor-tunately, obvious attempts to build large-scale systems based on this idea run across several major hurdles. First, attempting to build large clusters of GPUs is difficult due to communications bottlenecks. Consider, for instance, using widely-implemented map-reduce in-frastructure (Dean &amp; Ghemawat, 2004; Chu et al., 2007) to employ our GPUs in a  X  X ata parallel X  mode, where each GPU keeps a complete copy of the neu-ral network parameters but computes a gradient using a different subset of the training data. The network parameters must fit on a single GPU X  X imiting us to, say, 250 million floating-point parameters (1 GB of storage). Our GPU code is capable of computing a gradient for these parameters in just milliseconds per training image, yet copying parameters or gradients to other machines will take at least 8 seconds over com-modity Ethernet X  X everal orders of magnitude slower. Parallelizing the gradient computations with  X  X odel parallelism X , where each GPU is responsible for only a piece of the whole neural network, reduces band-width requirements considerably but also requires fre-quent synchronization (usually once for each forward-or backward-propagation step). This approach works well for GPUs in a single server (which share a high-speed bus) (Krizhevsky, 2010; Krizhevsky et al., 2012) but is still too inefficient to be used with Ethernet net-works. For these reasons, we turn to the use of high-end networking infrastructure to remove the commu-nications bottleneck between servers and enable us to exploit both fast GPU computation and to  X  X cale out X  to many servers. Our cluster incorporates Infiniband interconnects, which are dramatically faster (in terms of both bandwidth and latency) than typical Ethernet networks.
 The second major problem with building larger sys-tems is a software challenge: managing computation and communication amongst many GPUs significantly complicates algorithm design. For instance, we must create well-optimized code for the GPUs themselves, sometimes requiring algorithm-specific assumptions to maximize performance. Similarly, while low-level mes-sage passing software deals with some of the communi-cations difficulties, we have found the message-passing metaphor cumbersome for building deep learning algo-rithms. In this paper, we will highlight several useful engineering solutions we have come across that greatly simplify development for systems like ours. Conceiv-ably, these solutions could be boiled down to a soft-ware library, packaged and optimized for use by other researchers in the future.
 In the remainder of this paper we will detail the im-plementation of our large-scale model-parallel train-ing system for deep neural networks as developed for COTS HPC computing infrastructure. After describ-ing our base hardware and software setup, we will de-tail several pieces of our software implementation in Section 4. We will then verify the scalability of our approach experimentally and present several results obtained from our implementation in Section 5. In particular, we will demonstrate the ability to replicate some of the experiments from (Le et al., 2012) (the largest training system in the literature) with just 3 machines, and also give results from an 11 billion pa-rameter network trained with our cluster in just a few days. Our cluster is comprised of 16 servers, each with 2 quad-core processors. Each server contains 4 NVIDIA GTX680 GPUs and an FDR Infiniband adapter. The GPUs each have 4GB of memory and are capable of performing about 1 TFLOPS (single-precision) with well-optimized code. The Infiniband adapter connects to the other servers through an Infiniband switch, and has a maximum throughput of 56 Gbps along with very low end-to-end latency (usually microseconds for small messages).
 This particular server configuration was chosen to bal-ance the number of GPUs with CPUs, which we have found to be important for large-scale deep learning. In previous work, multi-GPU systems have demon-strated their ability to rapidly train very large neural networks (Ciresan et al., 2011; Krizhevsky et al., 2012) (usually convolutional neural networks). Such systems rely on high-speed access to other GPUs across the host PCI bus to avoid communications bottlenecks X  and it makes sense to put many GPUs into a single server in this case. But this approach scales only to 4 GPUs or perhaps 8 GPUs before the host machine be-comes overburdened by I/O, power, cooling, and CPU compute demands. As a result, we have limited our-selves to 4 GPUs per server and relied on Infiniband to make communication feasible amongst GPUs in sepa-rate servers.
 All of our software is written in C++ and built atop the MVAPICH2 (Wang et al., 2011) MPI implementa-tion. MPI provides a standard message passing inter-face that allows multiple processes in a cluster to ex-change blocks of data. MVAPICH2 handles all of the low-level communications over Infiniband in response to MPI API calls and includes integrated support for GPUs. Pointers to data in GPU memory can be pro-vided as arguments to MPI calls to initiate transfers from one GPU to another, even when the destination GPU is in another server.
 This off-the-shelf software infrastructure gives us a foundation on top of which to build our deep learn-ing system. When our system starts up, every server spawns one process for each of its GPUs. Each pro-cess claims one GPU and is assigned an ID number ( X  X ank X ) by the MPI implementation. Since each GPU has its own process, all communication amongst GPUs occurs through MPI. Though message-passing is a very low-level operation (and is not especially natural for building deep learning systems), we will show later how most of the communication can be abstracted eas-ily making it much simpler to build deep learning al-gorithms on top of MPI. In this paper we will focus on the implementation of the sparse autoencoder described in (Le et al., 2012), though other variants could be implemented as well (Ranzato et al., 2007; Glorot et al., 2011). Closely following (Le et al., 2012), our network is constructed from stacks of neurons with each stack composed of three layers: a linear filtering layer, a pooling layer, and a local contrast normalization layer (Figure 1). This stack is replicated 3 times to form a 9 layer net-work.
 The first two layers implement selective features ( X  X im-ple cells X ) and invariant features (Hyv  X arinen &amp; Hoyer, 2000; Hyv  X arinen et al., 2001) ( X  X omplex cells X  (Hubel &amp; Wiesel, 1959)). These elements are common to many other architectures (Garrigues &amp; Olshausen, 2010; LeCun et al., 2004; Riesenhuber &amp; Poggio, 1999), though we note that like (Le et al., 2012) we use untied filter banks X  X very neuron has its own parameters, in contrast to convolutional networks (LeCun et al., 1989; Krizhevsky et al., 2012) where spatially trans-lated neurons use the same filter. The contrast nor-malization layer has been found empirically to be use-ful in many systems (Jarrett et al., 2009) and appears to aid training of higher layers in the network. Each of the layers makes use of  X  X ocal receptive fields X  (Le-Cun et al., 1989; Raina et al., 2009; Krizhevsky, 2010): each neuron (linear filter, pooling unit, or local con-trast unit) uses only a small window of inputs from the layer below to compute its output, which will be a necessary feature for our distributed implementation. We train this network in a greedy, layer-wise fash-ion (Hinton et al., 2006; Bengio et al., 2006). The pooling and normalization layers have fixed parame-ters like those in (Le et al., 2012), and thus we need only train the filter layers. To do so, we optimize the following unsupervised learning objective over the lin-ear filter parameters, W , and a scalar scaling parame-ter  X  : Here x ( i ) is the i  X  X h training example (a training image, or features computed by lower layers of the network), V j is a vector of weights for the j  X  X h pooling unit 1 and  X  is a sparsity penalty (set to 0.1 in all of our exper-iments). W ( k ) is the filter for the k  X  X h neuron, which is constrained to have unit norm. Note also that in this formulation the reconstruction penalty (first line of (1)) uses W &gt; as the  X  X ecoding X  weights rather than using a separate matrix of weights. This saves a sub-stantial amount of memory, which will be important for training our largest networks.
 The optimization problem (1) is solved using a stan-dard mini-batch stochastic gradient descent proce-dure with momentum (Rumelhart et al., 1986; Hin-ton, 2010). The gradient for the entire objective can be computed using gradient back-propagation. To en-force the normalization constraint in our gradient de-scent procedure we define W ( k ) =  X  W ( k ) / ||  X  the objective above and then optimize over  X  W . We now describe in more technical detail several key aspects of our implementation of the training algo-rithm above for our HPC cluster. To begin, we note that we must solve two basic problems to arrive at an efficient (and hopefully not too complex) implementa-tion: (i) we require highly optimized GPU code ( X  X er-nels X ) for all major computational operations, and (ii) we must develop a scheme for distributing the compu-tations over many GPUs and managing the communi-cation between them (which, since we are using MPI, will involve passing messages between GPUs). Fortu-nately, these problems can be dealt with separately, so we will visit each in turn.
 As a preliminary, we note that the first layer of our network takes in a mini-batch of images which can be represented as a 4D array of size M -by-w -by-w -by-c , where M is the mini-batch size, w is the image width and height, and c is the number of input chan-nels. In our experiments we will use a large unlabeled dataset of 200x200 color images so each image may be thought of as a 3D grid of 200-by-200-by-3 values, and each mini-batch is just an array of M such 3D grids. The output of the network layer can similarly be rep-resented as a 4D grid of M -by-r -by-r -by-d responses, where r and d are determined by the size and number of filters. This layout is shown in Figure 2, which we will explain in more detail below. We will think of our GPU and MPI code as operating on these 4D arrays. 4.1. CUDA kernels Our cluster uses NVIDIA GTX680 GPUs, and our GPU code is written with NVIDIA X  X  CUDA lan-guage (NVI). We will not detail the particulars of the code, but instead describe a few basic observations that have enabled us to write highly optimized ker-nels for the most important computations.
 Deep learning systems, including the sparse auto-encoder in Section 3, rely on just a few major oper-ations. Point-wise operations (e.g., addition or scalar nonlinearities) are very easy to implement efficiently. The more difficult operations to implement are those that involve local connectivity. In particular, the weight matrix W in Eq. 1 is very sparse: the filter W ( k ) for each neuron has non-zero entries only for indices j corresponding to x j in a local spatial region. This sparsity means that we cannot use optimized linear algebra code designed for dense matrices, and generic sparse linear algebra code tends to be much slower. Unfortunately, writing optimized code for this oper-ation turns out to be difficult. Recent GPUs rely heavily on instruction-level parallelism (ILP) in ad-dition to thread parallelism and have fairly sophisti-cated cache hierarchies and instruction pipelines. Op-timizing code for such architectures is thus increas-ingly difficult to perform without expert knowledge of each GPU architecture. Indeed, our own code to compute y = WX (where X is a matrix represent-ing a mini-batch of images 2 ) achieved disappointing performance: 300 GFLOPS on GPUs capable of more than 1 TFLOPS peak. As well, experience from convo-lutional neural network implementations (Krizhevsky, 2010), like storing the filter coefficients in cache mem-ory, has turned out not to be applicable: for our largest networks, a single filter can be larger than the entire shared memory cache of the GPU.
 The main insight that we have used to implement much better kernels is to make a small change to our neural network structure so that computation of Y = WX may be very efficiently implemented as a large number of smaller dense matrix multiplies. In particular, if we have a set of neurons F that share a single receptive field (i.e., for every k  X  F the filters W ( k ) have the same sparsity pattern), then we can compute the responses using a dense matrix-matrix multiplication: Y F = W F X F . Here, W F is the ma-trix of filters for neurons in F obtained by extract-ing the non-zero columns of W and the correspond-ing rows of X (denoted X F ). This situation is de-picted in Figure 3. Provided the number of neurons in each set F (number of rows of W F ) and the num-ber of images in a mini-batch (columns of X ) are large enough, each block of filter responses Y F may be com-puted almost identically to standard matrix-matrix multiplications X -we need only alter the fetching of columns in W and rows in X to follow the correct pattern.
 For our implementation we referenced the highly-optimized MAGMA BLAS matrix-matrix multiply kernels (Tomov et al., 2011), which make use of ad-vanced techniques including pre-fetching, exploitation of ILP, and careful register usage. By following the basic skeleton but mapping row and column fetches to the appropriate locations in our filter array W and in-puts X we are able to execute operations like Y = WX at speeds competitive with a full dense multiply. To compute the linear responses Y = WX efficiently with our optimized kernel, we must have a set F of many neurons sharing identical receptive fields. To ensure that we have such structure, we use block local connectivity as shown in Figure 2. In this setup, we group neurons into 3D blocks where all of the neurons in each block share an identical receptive field. We aim to make the blocks large enough to ensure efficient ex-ecution of our GPU code. 3 We can make the blocks in Figure 2 larger by expanding their width or depth, but in order to keep the total number of neurons constant we must also use a larger step size s (e.g., s = 4). The four most-used computations in our code are: Wx , W &gt; x ,  X  x &gt; 4 and (surprisingly) the normaliza-tion of the columns of W . This last operation is sim-ple but memory bandwidth-limited. The first three, on the other hand, all use the approach described above. Their computational throughput is shown in Table 1. On some models of consumer GPUs (e.g., an overclocked GTX580) the fastest kernel can exceed 1 TFLOPS.
 GTX 580 OC 1221 1015 887 798 4.2. Communication with MPI Given implementations of the basic mathematical op-erations on the GPU, it is possible to build a single-GPU system to train neural networks of the form in Section 3. To expand to multiple GPUs we need to first divide up the computational work amongst the GPUs in the cluster and then organize their commu-nication so that the end result matches what would be produced on a single GPU. In this paper, we will parallelize across GPUs using a strictly model parallel scheme: each GPU is responsible for a separate part of the computation, but all of the GPUs work together on the same mini-batch of input samples.
 We think of the GPUs in our cluster as being arranged in a multi-dimensional grid. For simplicity we will use a 2D grid of 2-by-2 GPUs as an example. Recall that our input mini-batch as well as all of the neuron re-sponses can be thought of as 4D arrays of values. A natural way to break up these arrays over our GPUs is to partition the spatial dimensions (width and height) evenly over our 2D grid. For example, at the input layer where we have a M-by-200-by-200-by-3 pixel ar-ray, each GPU claims a M -by-100-by-100-by-3 chunk of the input image. The colored regions in Figure 4 show how we might split an input image and neuron responses for the next layer over a grid of 4 GPUs. This distribution of the arrays also implies one possible distribution for computation: each GPU is responsible for computing the responses of any neurons that have been assigned to it by the above partitioning scheme. For example, in Figure 4, GPU 3 must compute all of the neuron responses in the large green block of the top layer. Given this partition of neurons over GPUs we can also partition the filter weights W so that filter W ( k ) is stored on the same GPU as the k  X  X h neuron. In principle, computation of the filter responses is carried out using the same GPU code as for a single-GPU implementation, but with one caveat: it is often the case that the inputs needed to compute the output of the k  X  X h neuron will reside on several GPUs and thus we need to arrange for these  X  X issing X  inputs to be fetched from their homes before computation can be performed. In Figure 4, one block of neurons on GPU 3 might need to access inputs from both GPU 3 and GPU 2.
 Implementing these fetches is messy and often difficult to think about during development. We have found that a simple distributed array abstraction hides vir-tually all communication from the rest of the compu-tational code. For each such array, GPU i specifies an output window output i in the global array that it plans to fill with results. Similarly, it specifies a sec-ond window input i that it will need to read in order to continue the computation. At runtime GPU i will send a message to GPU j containing all of the values in the window output i  X  input j and receive a message from GPU j containing values from input i  X  output j . The received values are written to a local array (along with the outputs from GPU i ), yielding a globally con-sistent view of data in window input i . Computation can then proceed as usual. In Figure 4, the input win-dow input 3 for GPU 3 in our example is shown with a black dotted line.
 In practice, the input window used by Layer N over-laps substantially with the output window used by Layer N  X  1, and thus most of the needed data is al-ready available. This abstraction is most useful though when this arrangement does not quite work out X  X or instance when a neuron has a large receptive field and thus requires values from many GPUs (not just neigh-boring GPUs in the grid), or when uneven partitions result in strange configurations of the input and out-put areas. These  X  X dge cases X  are hidden by the array implementation and the GPU code sees only a local memory buffer that is properly populated with data. Though we will not describe it in further detail here, we note that a small tweak is needed to al-low overlapping output windows X  X .e., regions of the global distributed array that are written by multiple GPUs. Simply: the overlapping response values are summed up. We use this primarily to implement back-propagation. 5.1. Scaling efficiency We have evaluated the efficiency of our system as we scale out to many GPUs. To measure the efficiency of scaling, we performed short optimization runs with varying numbers of GPUs and varying sizes of neu-ral networks. For each run we recorded the average time taken to compute an update to all of the lay-ers, as would be done during full joint training of the network. This computation requires us to do a feed-forward pass through all layers and compute the ob-jective function for each stack. We must then perform a complete backward pass from the top of the network to the bottom and compute gradient updates for all of the parameters. This exercises all of the compo-nents of our system and is representative of the most demanding computations performed by deep learning systems, including those that use fine-tuning from su-pervised objectives.
 We report the time taken to perform a mini-batch up-date for several network sizes in Figure 5. Figure 6 shows the factor speedup obtained relative to a sin-gle GPU, normalized by the number of parameters in each network. As can be seen in Figure 6, using many GPUs does not yield significant increases in computa-tional throughput for small networks, but our system excels when working with much larger networks. In absolute terms, our system is fast enough to train the largest networks we have used (with over 11 billion parameters) in just a few days. A single mini-batch update (mini-batch size of 96 images) for the 3rd stack of such a network takes less than 0.6 seconds and a full epoch through our 10 million image training set takes about 17 hours. In total, allowing for 2 epochs to train the 2nd and 3rd stacks in our networks (the 1st stack trains very quickly), our 11 billion parameter models can be trained in roughly 3 days. 5.2. High-level, object-selective features It was recently shown that very large neural networks trained from only unlabeled data can learn to iden-tify objects, such as human faces, in images (Le et al., 2012). We have replicated such a result using our sys-tem but at drastically reduced expense. We will briefly describe these experiments.
 We construct an unlabeled training set by harvesting 10 million YouTube video thumbnails. We rescale each thumbnail so that its height is 200 pixels, and crop out the central 200-by-200 pixel region. These 200-by-200 color images are contrast normalized and whitened offline then distributed to local disk on our cluster for training.
 We trained a network with 3 stacks as in Section 3 where each filtering layer used 20-by-20-by-d receptive fields (where d is the depth of the input array), filter blocks of size 4-by-4-by-8 with a step size of s =4 pix-els between receptive fields. 5 This setup is very close to the one in (Le et al., 2012). The entire network has about 1.8 billion filter parameters. Following (Le et al., 2012), we tested each neuron in the trained net-work by recording its response to images from a labeled diagnostic set containing 13152 labeled faces from the Labeled Faces in the Wild (Huang et al., 2007) dataset and 48000 distractor images from ImageNet. The neu-ron X  X  selectivity for  X  X aces X  is quantified by computing the highest classification accuracy 6 it can achieve on this set. Like (Le et al., 2012) and a similar result in (Coates et al., 2012), we have found that some neu-rons in our network are selective for faces. These neu-rons are able to classify a diagnostic image as  X  X ace X  or  X  X ot-face X  with 88% accuracy (whereas random guess-ing would achieve only 64.7% on our benchmark). We have performed the same test for other objects with re-sults summarized in Table 2. We have also visualized the optimal response for these object-selective neurons in Figure 7. This is done using the constrained opti-mization procedure as in (Le et al., 2012) with similar results. Finally, to demonstrate the scalability of our system, we also trained a much larger network from the same dataset. This network used 20-by-20-by-3 filters for the first layer but with filter blocks of size 4-by-4-by-18 (step size of s =4). The 2nd and 3rd stacks used the same filter block size, but larger filters: each filter is 32-by-32-by-18 elements. We continue to use the same 5-by-5 pooling and contrast normalization, though it is likely that improvements can be obtained by adjusting these parameters in the future.
 Surprisingly, we have found that the most object-Human faces 64.7% 64.8% 88.2% 86.5% Upper body 64.7% 64.8% 80.2% 74.5% selective neurons in this large network are less selec-tive for our test object classes than in the smaller net-work, although they are still much better than random guessing. The classification performance of the best neurons in our 11 billion parameter network are listed in Table 2. We have found that the introduction of nonlinearities and substantial hyperparameter tuning improves these numbers slightly, but it is possible that new algorithms or analysis methods will be needed for the sizes of networks that we are now able to train. We have presented details of a very large scale deep learning system based on high performance comput-ing infrastructure that is widely available. With our system we have shown that we can comfortably train networks with well over 11 billion parameters X  X ore than 6.5 times as large as the one reported in (Dean et al., 2012) (the largest previous network), and using fewer than 2% as many machines.
 Though it has taken significant effort to make the best use of HPC infrastructure, we have described in this paper several components useful for deep learning that are efficient and easily implemented. In particular, we found that distributed arrays allow us to hide commu-nications during our forward and backward propaga-tion steps and that highly optimized GPU kernels can be built with semantics and implementation akin to matrix-matrix multiplication code to handle locally-connected neuron computations X  X  major source of complexity and optimization issues in our other expe-riences with GPUs. These are components that, with wider adoption of HPC systems, might reasonably be packaged into optimized software libraries.
 Though we are clearly able to train extremely large neural networks, we have not yet identified a combina-tion of algorithms and architecture yielding much bet-ter results for our unsupervised tests. Our 11 billion parameter network relies heavily on an architecture that has not changed much from the ( X  X mall X ) 1 bil-lion parameter network in (Le et al., 2012). Neverthe-less, with such large networks now relatively straight-forward to train, we hope that wider adoption of this type of training machinery in deep learning will help spur rapid progress in identifying how best to make use of these expansions in scale. Thanks to the Stanford CSD-CF staff for assembling and supporting our cluster. Thanks to the authors of MAGMA BLAS and MVAPICH2 for technical sup-port. We are grateful to Quoc Le for help reproducing the results of (Le et al., 2012) and for useful advice. Thanks also to Bill Daly, Cliff Woolley, Michael Bauer, and Geoffrey Fox for helpful conversations.
 Bengio, Y, Lamblin, P, Popovici, D, and Larochelle, H. Greedy layer-wise training of deep networks. In Neural Information Processing Systems , 2006. Chu, C, Kim, S. K, Lin, Y.-A, Yu, Y, Bradski, G, Ng,
A. Y, and Olukotun, K. Map-reduce for machine learning on multicore. Advances in Neural Informa-tion Processing Systems , 19:281, 2007.
 Ciresan, D. C, Meier, U, Masci, J, Gambardella, L. M, and Schmidhuber, J. Flexible, high performance convolutional neural networks for image classifica-tion. In International Joint Conference on Artificial Intelligence , pp. 1237 X 1242, 2011.
 Ciresan, D. C, Meier, U, and Schmidhuber, J. Multi-column deep neural networks for image classifica-tion. In Computer Vision and Pattern Recognition , pp. 3642 X 3649, 2012.
 Coates, A, Lee, H, and Ng, A. Y. An analysis of single-layer networks in unsupervised feature learning. In 14th International Conference on AI and Statistics , pp. 215 X 223, 2011.
 Coates, A, Karpathy, A, and Ng, A. Y. On the emer-gence of object-selective features in unsupervised feature learning. In Advances in Neural Informa-tion Processing Systems , 2012.
 Dean, J and Ghemawat, S. Mapreduce: Simplified data processing on large clusters. In Sixth Sympo-sium on Operating System Design and Implementa-tion , 2004.
 Dean, J, Corrado, G. S, Monga, R, Chen, K, Devin, M, Le, Q. V, Mao, M. Z, Ranzato, M, Senior, A, Tucker,
P, Yang, K, and Ng, A. Y. Large scale distributed deep networks. In Advances in Neural Information Processing Systems 25 , 2012.
 Garrigues, P and Olshausen, B. Group sparse coding with a laplacian scale mixture prior. In Advances in Neural Information Processing Systems 23 , pp. 676 X 684, 2010.
 Glorot, X, Bordes, A, and Bengio, Y. Deep sparse rec-tifier neural networks. In 14th International Con-ference on Artificial Intelligence and Statistics , pp. 315 X 323, 2011.
 Hinton, G, Osindero, S, and Teh, Y. A fast learning algorithm for deep belief nets. Neural Computation , 18(7):1527 X 1554, 2006.
 Hinton, G. A practical guide to training restricted boltzmann machines. Technical report, University of Toronto, 2010.
 Huang, G. B, Ramesh, M, Berg, T, and Learned-
Miller, E. Labeled faces in the wild: A database for studying face recognition in unconstrained en-vironments. Technical Report 07-49, University of Massachusetts, Amherst, October 2007.
 Hubel, D and Wiesel, T. Receptive fields of single neurones in the cat X  X  striate cortex. The Journal of physiology , 148(3):574 X 591, 1959.
 Hyv  X arinen, A and Hoyer, P. Emergence of phase-and shift-invariant features by decomposition of natural images into independent feature subspaces. Neural Computation , 12(7):1705 X 1720, 2000.
 Hyv  X arinen, A, Hoyer, P, and Inki, M. Topographic independent component analysis. Neural Computa-tion , 13(7):1527 X 1558, 2001.
 Jarrett, K, Kavukcuoglu, K, Ranzato, M, and LeCun,
Y. What is the best multi-stage architecture for ob-ject recognition? In 12th International Conference on Computer Vision , pp. 2146 X 2153, 2009.
 Krizhevsky, A. Convolutional Deep Belief Networks on CIFAR-10. Unpublished manuscript, 2010.
 Krizhevsky, A, Sutskever, I, and Hinton, G. Ima-genet classification with deep convolutional neural networks. In Advances in Neural Information Pro-cessing Systems 25 , pp. 1106 X 1114, 2012.
 Le, Q. V, Ngiam, J, Coates, A, Lahiri, A, Prochnow,
B, and Ng, A. Y. On optimization methods for deep learning. In International Conference on Machine Learning , 2011.
 Le, Q, Ranzato, M, Monga, R, Devin, M, Chen, K,
Corrado, G, Dean, J, and Ng., A. Building high-level features using large scale unsupervised learn-ing. In International Conference on Machine Learn-ing , 2012.
 LeCun, Y, Boser, B, Denker, J. S, Henderson, D,
Howard, R. E, Hubbard, W, and Jackel, L. D. Back-propagation applied to handwritten zip code recog-nition. Neural Computation , 1:541 X 551, 1989. LeCun, Y, Huang, F. J, and Bottou, L. Learning meth-ods for generic object recognition with invariance to pose and lighting. In Computer Vision and Pattern Recognition , volume 2, pp. 97 X 104, 2004.
 Martens, J. Deep learning via hessian-free optimiza-tion. In 27th International Conference on Machine
Learning , volume 951, pp. 2010, 2010. nVidia CUDA Programming Guide . NVIDIA Corpo-ration, 2701 San Tomas Expressway, Santa Clara, CA 95050. URL http://docs.nvidia.com/ .
 Raina, R, Madhavan, A, and Ng, A. Large-scale deep unsupervised learning using graphics processors. In 26th International Conference on Machine Learn-ing , 2009.
 Ranzato, M, Poultney, C, Chopra, S, and LeCun, Y.
Efficient learning of sparse representations with an energy-based model. In Advances in Neural Infor-mation Processing Systems , 2007.
 Riesenhuber, M and Poggio, T. Hierarchical models of object recognition in cortex. Nature neuroscience , 2, 1999.
 Rumelhart, D, Hinton, G, and Williams, R. Learning representations by back-propagating errors. Nature , 323(6088):533 X 536, 1986.
 Tomov, S, Nath, R, Du, P, and Dongarra, J. MAGMA
Users Guide . UT Knoxville ICL, 2011. URL http: //icl.cs.utk.edu/magma/ .
 Uetz, R and Behnke, S. Large-scale object recogni-tion with CUDA-accelerated hierarchical neural net-works. In Intelligent Computing and Intelligent Sys-tems , 2009.
 Wang, H, Potluri, S, Luo, M, Singh, A. K, Sur, S, and Panda, D. K. MVAPICH2-GPU: optimized
GPU to GPU communication for InfiniBand clus-ters. Computer Science-Research and Development ,
