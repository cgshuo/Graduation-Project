 ORIGINAL PAPER Hadi Grailu  X  Mojtaba Lotfizad  X  Hadi Sadoghi-Yazdi Abstract Recently, the mixed raster content model was proposed for compound document image compression. Most state-of-the-art document image compression methods, such as DjVu , work on the basis of this model but they have some disadvantages, especially for Farsi and Arabic document images. First, the Farsi/Arabic script has some characteris-tics which can be used to further improve the compression performance. Second, existing segmentation methods have focused on well-separating the textual objects from the back-ground and/or optimizing the rate-distortion trade-off; never-theless,theyhavenotconsideredthetextreadabilityandOCR facility. Third, these methods usually suffer from the unde-sired jaggy artifact and misclassifying the important textual details. In this paper, MRC-based document image compres-sion method is proposed which compromises rate-distortion trade-off better than the existing state-of-the-art document compression methods. The proposed method has higher per-formance in the aspects of segmentation, bi-level mask layer compression, OCR facility, and the overall compression. It uses a 1D pattern matching technique for compression of masklayer.Italsousesasegmentationmethodwhichissensi-tive enough to the small textual objects. Experimental results show that the proposed method has considerably higher compression performance than that of the state-of-the-art compression method DjVu , as high as 1.75 X 2.3.
 Keywords Document image compression  X  Bi-level textual image compression  X  Document segmentation  X  MRC model  X  OCR facility 1 Introduction In recent years, compound document image compression has become an important research field due to economical rea-sons for compressing huge amount of scanned documents. Compound document images contain graphic and textual content along with pictures. They are a very common form of documents, found in magazines, web sites, etc. [ 1 ].
Adifferencebetweenthetextualandnaturalimagesrelates to their need to different resolutions in space and value [ 2 ]. Textual images, contrary to the natural ones, need higher resolution in space than the color values of pixels. Proper reproduction of the textual areas requires not so much tonal resolutionassufficientspatialresolutiontopreservethefinest serifs and other letterform features [ 3 , 4 ]. Document images properties are so important and distinct from those of other image types that part 6 of the JPEG2000 standard is consid-ered for compound documents with the file format JPM [ 5 ]. Another difference between document and natural images is that the former usually contain important visual details including font irregularities, paper color and paper texture. They are particularly important for historical documents, and may also be crucial in documents with tables, mathematical or chemical formulas, and handwritten text [ 6 ].
The mixed raster content (MRC) model was developed as a compression scheme for such compound images in which the original document image is decomposed to usually fore-ground, background, and mask layers each of which contain special information. At each pixel, the value of the binary mask is used to select between the foreground and back-ground pixels in order to reconstruct the document image. Typically, the foreground layer contains the color of text and line drawings, and the background layer contains pic-tures, graphics, and paper background. The MRC model could achieve high compression ratios while maintaining high reconstructed image quality [ 2 , 3 , 7 ]. It is ITU standard (T.44) originally proposed for color facsimile [ 8 ]. It has also been proposed as the JPEG2000 architecture framework for thecompoundimagefileformat[ 5 ].AkeyconceptintheT.44 implementation of MRC is that each of the three (or more) layers is compressed independently by a suitable encoder. This approach has the important advantages of simplicity and modularity, but at the cost of some coding overhead [ 2 , 3 ]. Figure 1 illustrates a three-layer MRC document containing a background layer, a foreground layer, and a binary mask layer.

A number of MRC-based compression approaches have been proposed and studied in previous researches. Among them, DjVu [ 6 ], DigiPaper [ 9 ] and LuraDocument [ 10 , 11 ] have already been employed in commercial products. We can consider the DjVu compression and segmentation method as one of the best existing state-of-the-art ones because many modern approaches have compared their performance with that of DjVu [ 2 , 12 , 13 ]. Its compression performance is bet-ter than that of single-encoder compression methods as well as other mentioned commercial products [ 6 , 9 , 10 ]. Figure 2 a shows a sample text image with the spatial resolution of 300dpi.Alargescaledpartoftheresultingcompressedimage by JPEG2000 and DjVu compression methods is shown in Fig. 2 b and c, respectively. We have adjusted both methods to compress the original image approximately 30 times. As can be seen, the multi-encoder MRC-based method, i.e., DjVu , results noticeably better image quality because, in contrast to JPEG2000 , it has not any ringing effect and has well pre-served the edges quality.

DjVu has some disadvantages; first, it suffers from the jaggy artifact problem similar to other ones. Figure 3 bshows part of a DjVu -compressed text image. In the original scanned image, shown in Fig. 3 a, the anti-aliasing effect, generated in the scanning process, provides smoother and finer appear-ance along edges. Comparatively, as a result of losing the anti-aliasing effect, the DjVu decoded image exhibits objec-tional  X  X aggy X  artifacts around the text edges. This problem is not desirable in high-quality MRC document coding [ 2 ].
Second, DjVu uses a wavelet-based encoder to encode the background and foreground layers which cause to color diffusion from each of two adjacent color regions with uni-form color distribution. A sample of this undesired effect, which reduces the quality of reconstructed image, is shown in Fig. 3 d.

Third problem relates to text segmentation which perhaps is the most critical step in MRC encoding. Segmentation dif-ferentiates textual and graphical regions within an image to create a binary mask layer. It is an important factor to main-tain the quality of decoded images and to reduce the bit-rate [ 13 ]. To our best knowledge [ 13  X  15 ] most existing segmen-tation algorithms (including DjVu ) have not considered the text readability and pleasure as well as OCR facility to mod-ify the segmentation results. In addition, they usually have not enough segmentation precision to separate the textual details in Farsi and Arabic document images; thus, misclas-sifying such important textual details. These misclassifica-tions reduce text quality and readability especially in Farsi and Arabic document images where such textual details fre-quently occur and have rather important effect on text quality, readability, and meaning.
 A sample Farsi document image and the corresponding DjVu decoded image are shown in Fig. 4 in which some sam-ple words are highlighted by rectangles in order to dem-onstrate some disadvantages of DjVu segmentation. Some words from the original and compressed images are shown in Fig. 4 c and d in larger scale. As can be seen from Fig. 4 b or d, some dots and strokes are mistakenly segmented as the background image and consequently, smoothed due to dif-ferent encoder used for the background layer. As shown in the left word image of Fig. 4 d, two single dots which have similar intensity profiles are mistakenly segmented to differ-ent classes. Also, the stroke in the right word of Fig. 4 d, in spite of having relatively large intensity and area (shown in Fig. 4 c), is mistakenly segmented as the background image; furthermore in this word image, the two groups of triple and double dots are mistakenly attached together. Figure 4 d, in addition to misclassifying the textual objects and mistak-enly attaching groups of dots, shows another disadvantage of DjVu segmentation; i.e., the jaggy artifacts.
 Jaggy artifacts have more undesirable effects on Farsi and Arabic document images due to some reasons; first, these artifacts render the word segmentation difficult. Word seg-mentation is used in almost all Farsi and Arabic OCR sys-tems [ 16 ]. Second, they may cause to flatten or distort textual details such as serrations and serifs which are important in recognition tasks. Third, they may even cause to break thin parts of some Farsi and Arabic letters, leading to erroneous OCR results.

Edge reconstruction is the fourth problem of most MRC-based compression methods including the DjVu . While many MRC-based methods can yield much higher compression ratios than conventional color image compression methods, the binary representation tends to distort fine document det-ails, such as thin lines and text edges [ 2 ]. An intrinsic lim-itation of the standard MRC model is that the binary mask can only represent discontinuous transitions between text, line art, and background colors. In practice, this type of hard transition can introduce substantial artifacts since real documents often contain subtle continuous-tone transitions between regions. These continuous transitions are useful in anti-aliasing, and allow the document to be encoded with lower dpi, which can further reduce bit rate. There is not much research to solve this problem. Zaghetto et al. [ 17 ] has triedtosolvethisproblembyblurringboundaryregionsusing a Gaussian filter but it lowers the quality of the reconstructed image and changes image values at the transition and/or tex-tual regions. In this paper, we try to solve this problem by modeling the edge regions by a polynomial function of a low order.

As the fifth problem, bi-level mask layer compression has a relatively high effect on the MRC-based encoding of Farsi and Arabic document images. Mask layer compression is usually performed by pattern matching (PM)-based bi-level textual image compression methods. In these methods first, all patterns (or marks) in the mask layer are extracted. Each pattern is a set of connected (neighboring) black pixels. Then, all similar patterns are grouped together and assigned a pro-totype which is usually the most similar pattern to each of them. Each pattern is not necessarily fully similar to its cor-responding prototype and is somewhat different. The differ-ence image is called  X  X esidual pattern X  or  X  X rror map X , and its pixels are called  X  X esidual pixels X  or  X  X rror pixels X  [ 18 ]. The set of all prototypes is called the library. Each prototype in the library has an index number. For each pattern of the input image, the corresponding prototype index and its rela-tive position with respect to the previously processed pattern, as well as the corresponding residual pattern are computed and saved. Finally, all prototypes, number sequences, and probably the whole or part of the residual patterns should be encoded.

The performance of the PM technique is reduced in Farsi and Arabic text images. In the Farsi/Arabic script, contrary to the printed Latin script, letters normally may attach together and produce many different patterns. A relatively high num-ber of these patterns are fully/partially subset of, or simi-lar to some others; thus, cause to increase the number of library prototypes (or library size) and decrease the average number of occurrence of them and consequently, decrease the compression performance. Detecting such attaching pat-terns and exploiting them to reduce the library size has con-siderable effect on the compression performance. Neverthe-less, existing textual image compression methods (including DjVu ) have not used this property to increase the compression ratio.

In this paper, MRC-based document image compression method is proposed for archiving purposes as well as distrib-uting high-quality, high-resolution images of scanned docu-ment in color over the Internet. It uses a segmentation method which is sensitive enough to extract textual details and simul-taneously, tries to improve the text readability and OCR facil-ity; furthermore, we model the edge regions by a polynomial function of low order to solve the problem of preserving con-tinuous-tone transitions in edge regions and consequently, improve the reconstructed edge quality. In order to compress the bi-level mask layer, the proposed method uses a 1D-PM-basedmethodwhichoperatesinthechaincodetrans-formation domain.
 The remainder of this paper is organized as follows. Section 2 reviews the existing MRC-based document com-pression and segmentation methods. Section 3 describes the proposed compression method. Section 4 shows the experi-mental results and finally Sect. 5 concludes the paper. 2 Review of the existing document compression and segmentation methods In MRC-based document image compression each of the decomposed three layers are encoded separately. The fore-ground layer usually is sub-sampled and encoded using con-ventional natural image compression methods or standards such JPEG or JPEG2000 . The background layer is also usu-ally encoded using such standard methods. The mask layer is a bi-level textual image and mostly encoded using PM-based textual compression methods such as jb2 . Document seg-mentation has critical effect on the overall compression per-formance as well as the bi-level mask layer compression; thus, in this section we only review existing MRC-intended document segmentation and bi-level textual image compres-sion methods.

We may consider document segmentation as a specific binarization procedure which tries to keep only text and table objects which in this paper, for simplicity, we refer to all of them as the textual objects. Improper thresholding causes blotches, streaks, and erasures on the document, con-founding segmentation and recognition tasks. The merges, fractures, and other deformations in the character shapes as a consequence of incorrect thresholding are the main rea-sons of OCR performance deterioration [ 19 ]. In addition, the resulting quality and compression ratio of a MRC document encoder is highly dependent on the segmentation algorithm used to compute the binary mask [ 13 ].

Several segmentation methods have been proposed in the past for layer-based compression. We can consider the document segmentation consisting of thresholding (or bina-rization) and modification procedures. Most existing seg-mentation methods have directly/indirectly implemented this scheme. There are many methods for image thresholding but it seems that only a specific group of them are appropriate for document images [ 13 , 19 , 20 ]. In [ 19 , 20 ] the existing thres-holding methods are evaluated and finally it concludes that for document applications, techniques from locally adaptive and shape categories appear in the top seven thresholding methods.

The k -means algorithm is frequently used as a segmenta-tion approach for layer-based compression in methods such as DjVu .Theadaptivethresholdingandclusteringapproaches have also been combined [ 9 , 10 ]. Rate-distortion based block segmentation approaches have been used which optimize bit-rate and distortion for each selected encoder [ 21 , 22 ]. The bottom-up scheme [ 23 ] is a method which merges seg-mented pixels repeatedly to minimize the total of variance of foreground and background. The segmentation method in [ 24 ] proposes a relatively simple segmentation algorithm for implementing the basic 3-layer MRC model which assumes only a limited buffering of the image is available, and the algorithm is required to be one-pass. In [ 12 ] a segmentation method is proposed to separate the text from the image in documents in which the text overlaps the background. Two phases are used to accomplish the segmentation. In the first phase, which involved color transformation and clustering analysis, the monochromatic document image is partitioned into three planes: the dark plane, the medium plane and the bright plane. In the second phase, an adaptive threshold is determined to refine the text by adaptive binarization and block extraction. In [ 7 ] a segmentation algorithm is pro-posed for MRC with the multiple extracted constant color areas (MECCA) model which has the advantages of its ease of decomposition and text enhancement and noise reduction features. The three-layer model is a simple implementation of MRC. However, it has the disadvantage that the result-ing files, when coded as PDF, may not be printable on some Postscript and PDF printers because of the resources required during decomposition for 2 continuous-tone (contone) lay-ers. This problem can be avoided if the decomposition con-tains only one contone layer. As a result, MRC with Multiple Extracted Constant Color Areas (MECCA) model is pro-posed in [ 7 ]. The MECCA model contains one background layer, N foreground layers and N mask layers, where N is a non-negative integer. While the background layer can be a contone bitmap, the foreground layers are restricted to be constant colors. In [ 13 ] a segmentation method is proposed based on the MRC standard (ITU-T T.44). The algorithm consists of two components: cost optimized segmentation (COS) and connected component classification (CCC). The COS algorithm consists of an initial block-wise segmenta-tion followed by its refinement based on the optimization of an objective function. The CCC method is then applied to the segmentation result obtained by COS algorithm. CCC works by first extracting each connected component in the initial segmentation, computing a feature vector for each compo-nent, and then classifying each connect component as either text or non-text.

Among the MRC-based compression approaches, DjVu [ 6 ], LuraDocument [ 10 , 11 ] and DigiPaper [ 9 ] have already been employed in commercial products. DjVu performs the segmentationwithak-means basedclusteringmethodtogen-erate the mask layer. In LuraDocument , the bitonal mask is generated by first applying an adaptive threshold procedure to the original document, creating a quantized document. The original and quantized documents are then used by a text detection procedure to generate the final bitonal mask [ 10 , 11 ]. The segmentation algorithm of DigiPaper uses the tokenized representation to help with the segmentation. It uses the adaptive thresholding to convert the input image to binary image. It also relies on a number of attributes of text for modifying the binarization results. In [ 2 ] a method called resolution-enhanced rendering (RER) is proposed for jointly optimizing the MRC encoder and decoder to achieve low distortion rendering of edge transitions in the MRC binary mask layer. The method works by adaptively dithering the mask layer of a three-layer MRC encoding to produce the intermediate tone levels required for low distortion render-ing. To our best knowledge, there are very few compression methods for Farsi-like scripts. In [ 25 ] a retrieval system for historical ottoman text images is proposed where a PM-based approach is used. The proposed compression method is based on a special template matching technique in order to break the complex patterns into smaller building sub-patterns and then, using the PM technique. It has not any further approach, such as encoding scheme, to increase the compression ratio, because  X  X ncoding X  is not their aim.

None of the mentioned segmentation methods have con-sidered the text readability and pleasure into account in sep-arating textual objects from background. They also, have not considered the OCR recognition rate measure for evaluat-ing their segmentations, whereas it is an important factor in MRC-based OCR applications.
 Themasklayer,asmentionedbefore,ismostlyencodedby PM-based bi-level textual image compression methods. The PM idea was originally introduced in [ 26 ]. It does not encode the residual patterns and thus is a lossy compression method. Also it does not introduce any compression method for proto-types. The combined symbol matching (CSM) method [ 27 ] was introduced for improving the performance of the pre-vious method; It lefts the infrequently used patterns and encodes the residual patterns using a two dimensional run-length coding. The proposed method in [ 28 ] uses a preloaded library with alphabet characters with some different fonts. The pattern matching and substitution (PMS) method [ 29 ] can also handle graphics in textual images. In this method, large bodies of image graphics are divided into some smaller parts, each of which is considered as a pattern. The weighted AND-NOT (WAN) [ 30 ] is introduced as an improvement to the CSM method where a better pattern matcher is used. One of the authors further stated elsewhere that the per-formance of the CSM, PMS, and WAN methods decreases when the symbol size is decreased or the quantization noise is increased. Thus, he proposed the combined size independent strategy (CSIS) [ 31 ], which is similar to the PMS method but tries to operate independently of the symbol size by perform-ing the symbol size normalization. In [ 32 ], a multistage lossy method is proposed, where after some preprocessing based on a histogram analysis and skew correction, a PM with a rigorous comparison of patterns is applied. Some of similar patterns may be considered as dissimilar due to quantiza-tion noise. So, at the next step, such patterns are found and merged by a multistage structural clustering. This method uses the Q-coder for encoding the prototypes, and arithme-tic coding for encoding the number sequences. The mgtic method [ 33 , 34 ] is based on the pattern matching and uses an enhanced encoder in which the reconstructed image, using the prototypes, is used for conditional context-based arithme-tic coding of the original image. The lossless SPM method [ 35 ] uses a similar encoder but has a difference where a com-bination of the library prototype and current pattern is used as the context for encoding the current pattern. This tech-nique is named soft pattern matching [ 35 ]. The lossy SPM method performs some further processing for lossy com-pression. The most important process is the selective pixel reversal in which the color of the poorly predicted pixels is reversed if certain conditions are satisfied. The SPM method has a rather better performance than that of the mgtic method [ 35 ].
 The early standards such as ITU-T Group3 ( G 3 ) and Group 4 ( G 4 ) facsimile standards have not used the PM tech-nique and operate on the basis of various run-length coding techniques and Huffman codes [ 33  X  35 ]. In these standards, the run time duration is more important than the compres-sion ratio. The ISO/IEC JBIG1 standard [ 36 ] for lossless binary image compression uses the adaptive context-based arithmetic coding using a special 10-pixel template for defin-ing the context [ 36 ] and has results better than those of the mentioned standards [ 34 , 35 ] but still does not employ the PM technique [ 35 ]. The JBIG2 standard is the best existing compression standard, introduced in the last decade and has better compression performance than that of all previously mentioned standards [ 37  X  39 ]. This standard has not specified any specific encoder but the most important existing JBIG2 -based methods, such as jb2 , work on the basis of the soft pattern matching technique and therefore are similar to the SPM method in this respect. The jb2 method is a lossy textual binary image compression which has been used in the DjVu document image compression method [ 6 ].

In this paper, we propose MRC-based lossy compression method for Farsi and Arabic compound document images which has high segmentation and compression performance regarding to some quantitative measures. In equal compres-sion ratios, it has higher image quality than that of DjVu . It proposes a segmentation method which helps to improve image quality, text readability and OCR facility. It uses a 1D-PM-based bi-level textual image compression method in thechaincodetransformationdomainwhichdetectsattaching characters and exploit them to further increase the compres-sion ratio. 3 The proposed MRC-based document image compression method The diagram of the proposed document image compression method is shown in Fig. 5 . As can be seen, the input document image is first segmented using the proposed segmentation method to generate the foreground, background, and mask layers. The mask layer contains the extracted bi-level tex-tual objects and is encoded using a 1D-PM-based compres-sion method in the chaincode transformation domain. The foreground layer is converted to a sequence of color vectors each of which corresponds to the color of the corresponding extracted textual objects of the mask layer. The background layer is segmented into low variance, high variance, and segmentation map layers each of which is compressed sepa-rately. Each of the following sections describes a set of num-bered blocks of the flowchart of Fig. 5 ; Sect. 3.1 describes the proposed segmentation method (block 1), Sect. 3.2 describes the 1D-PM-based compression method in the chaincode tran-sformation domain (block 2), and finally, Sect. 3.3 describes the procedure of handling the foreground and background layers (blocks 3 to 8). 3.1 Document segmentation Our segmentation method, as illustrated in the flowchart of Fig. 6 , consists of three components of binarization, refine-ment, and boundary smoothing. The binarization component binarizes the input document image on the basis of finding the transition regions in the input image. The resulting binary image may contain some non-textual objects. These objects are removed by the refinement component which uses some prior knowledge on properties of textual objects. This com-ponent, as shown in the flowchart of Fig. 6 , has an interac-tion with the binarization component. This interaction helps the binarization component to select the threshold value in a compression-ratio-optimized scheme. Finally, the bound-aries of extracted binary textual objects are smoothed in order to improve the quality and readability of textual objects. This boundary smoothing is performed by optimizing an objec-tive function over the computed transition regions of every textual object. Each of the subsequent sub-sections describes a block of the flowchart of Fig. 6 in order. 3.1.1 Binarization In order to binarize the input document image, we first, con-vert it to the YUV color space and perform all consequent pro-cedures in the Y -plane. The binarization procedure, of which stages are shown in the flowchart of Fig. 7 , works on the basis of finding the image transition regions between textual and background regions. Transition regions have relatively high edge strength; thus, as shown in Fig. 7 , at the first stage of binarization procedure, we compute the edge-strength image and normalize it to the interval [0,255] respectively as ES ( x , y ) = I ( x , y )  X | X  I ( x , y ) | (1) ES N ( x , y ) = round where I ( x , y ) is the input image intensity and | X  I | the absolute value of the image gradient. ES N is the normal-ized edge-strength image. Multiplying the absolute image gradient by the image intensity alleviates the undesired effects of the spot noise. Figure 8 atocshowasampleFarsi color document image and corresponding grayscale and nor-malized edge-strength images, respectively.
At the second stage of the flowchart of Fig. 7 , three prepro-cessing procedures are performed which emphasize textual regions more than other non-textual regions. The first pro-cedure uses this assumption that textual objects occur close to each other, thus the local mean of edge-strength values in the textual regions is higher than that of the background regions. At this preprocessing, we emphasize the normalized ES N at each current pixel, P , according to the local mean of its neighboring pixels inside a window of the size S 1  X  S with the center of P as PES 1 ( x , y ) = where W S 1 , x , y denotes the neighbor pixels inside a window of the size S 1  X  S 1 with S 1 = 16 (at 300 dpi) and the center of the pixel located at the coordinate ( x , y ) . Then we nor-malize the image PES 1 to obtain the image PES 1 N . Figure 8 d shows the PES 1 N image for the document image of Fig. 8 a.
By performing experiments on our document images database, we found that after performing the previous preprocessing, most of the corresponding image values of PES 1 N in the textual regions belong to the interval [20,125]. Thus, the second procedure performs a kind of histogram reshaping in order to emphasize these regions as PES 2 ( x , y ) = After computing the PES 2 we normalize it to the interval [0,255] to obtain the image PES 2 N . Figure 9 ashowsthe PES 2 N image corresponding to Fig. 8 a.

The third preprocessing procedure works on the basis of this assumption that textual objects have continuous and rel-atively long boundaries as well as relatively strong edges. Thus, it emphasizes the textual boundaries and generate the result PES 3 ( x , y ) by tracking a specific route for every pixels of PES 2 N . This route has a fixed length of N R and consists of those connected pixels of current pixel which has the maxi-mum local values inside PES 2 N . For example, if we consider the circled pixel of Fig. 10 as the current pixel of PES 2 N then the corresponding route of connected maximum values with the length of N R = 6 is shown by white arrows in the same figure. For computing the route R x , y for a current pixel, P ( x , y ) , we find and save the maximum value among its 8 neighbors, set P 0 value to zero, and repeat this procedure for the corresponding pixel of detected maximum value until N
R maximum values have been obtained. The value of N R is found experimentally using some training images. This parameter is dependent on spatial resolution.
 After computing the corresponding route of all pixels of PES 2 N , we can generate the PES 3 image as PES 3 ( x , y ) = PES 2 N ( x , y )  X  mean ( R x , y ) (5) where R x , y denotes the corresponding route to the current pixel located at the coordinate ( x , y ) . After computing the PES 3 we normalize it to the interval [0,255] to obtain the PES 3 N . The corresponding PES 3 N image of Fig. 8 aisshown in Fig. 9 b.

After performing the preprocessing procedures, at the last stage of the flowchart of Fig. 7 , we binarize the PES 3 N For binarization, we should choose a proper threshold value. This threshold selection is performed by an interaction bet-ween this stage and the second stage of the flowchart of Fig. 6 . The details are explained in the next section. 3.1.2 Refinement The refinement component, in combination with binariza-tion component, tries to remove non-textual objects from binarized document image (or  X  X ask layer X  in the context of MRC model). The refinement procedure uses some prior knowledge about text properties to remove non-textual obj-ects. It has an interaction with the binarization component of Fig. 6 in order to well determine the threshold value for binarizing the image PES 3 N . The flowchart of the refine-ment procedure is shown in Fig. 11 . As can be seen, PES 3 N is the input image to the refinement procedure. Some can-didate thresholds are determined and examined to select the best one. We divided the interval [20,125] into NI = 3 sub-intervals and selected the corresponding first numbers as the candidate thresholds.

At the next stage, a sequence of procedures which are marked by A, B, C and D in Fig. 11 , is performed in order. The procedure A binarizes the image PES 3 N using current threshold.Inthebinarizedimage,therewillbesomeopenand closed connected transition regions (CTR) for which some samples are shown in Fig. 12 . A CTR is a connected compo-nent which designates a continuous transition region. Each CTR in the binarized image is related to an object which belongs to text, table, graphics and noise categories. The procedure B tries to keep only text and table objects and remove the others.

The stages of procedure B are as follows. First, we remove those CTRs whose areas are smaller than T area = 5 (at 300 pi) and are not located near any larger one because they are most likely related to noise. The remaining CTRs are grouped into closed and open ones. The former, in contrast to the lat-ter, encloses some inner regions. The inner regions relate to bodies of binary objects. Some samples of closed and open CTRs are shown in Fig. 12 with white color. Then, we extend open CTRs to a certain extent in order to probably convert them to closed ones. CTR extension is performed by succes-sively absorbing the neighbor non-CTR pixels according to their corresponding PES 3 N values. At each step, we search among the neighbor non-CTR pixels and label that pixel with the maximum PES 3 N value as a new CTR pixel. We repeat this procedure at most N EX , max times which amounts to 15% of the corresponding CTR area. We stop the extension proce-dure whenever no open CTR exists or the number of absorbed pixels is reached to N EX , max . For example, as designated in Fig. 13 by straight white lines, if we consider the circled open CTR in Fig. 12 and its corresponding pixels in PES 3 N then we can extend this CTR by successively searching the max-imum value among neighbor non-CTR pixels and label it as the new CTR pixel. Although this sample CTR has an area of 6 pixels and thus round ( 6  X  0 . 15 ) = 1, but we have repeated the CTR extension iteration three times in Fig. 13 only for illustration purpose. The new selected CTR pixels are circled and marked in numerical order. At the end of CTR extension, we remove all remaining open CTRs. During CTR extension, it may absorb another open/closed CTR and form a larger CTR. In such cases, we compute the new N EX , max according to the new CTR area.
After processing all CTRs, we perform the procedure C, in order to obtain the final binary document image. This pro-cedure finds the binary objects (or textual connected com-ponents) of document image and removes the non-textual objects by using some prior knowledge about text properties. For this, first it finds the inner regions of CTRs and extends them by a morphological dilation. The dilation mask is a disk with a diameter equal to half the average thickness of the cor-responding CTR. Then, it uses the following textual objects (TOs) properties to remove the non-text objects a. TOs occur near each other in a horizontal line. b. TOs have roughly uniform color and are inside a region c. Small TOs can only occur near the larger ones. If the d. Farsi/Arabic alphabet letters have at most three dots, e. For each group of TOs which are located at the same After generating the final document image, the procedure D compresses it using a simple lossy PM technique. In this method, first all patterns in the input image are extracted suc-cessively and compared with the prototypes to find a match. Thistemplatematchingisperformedusingtheweightederror map [ 40 ]. If a match occurs, we process the next pattern otherwise the current pattern is added to the library as a new prototype. Finally, we compute the average number of occur-rence of prototypes (ANO) which is defined as the number of the image patterns divided by the number of the library proto-types. At the last stage of Fig. 11 we compare the computed ANO i  X  X  with i = 1 ,..., NI and select the binary image corresponding to the maximum ANO as the final segmented document image. 3.1.3 Boundary smoothing The segmented document image of the second component in the flowchart of Fig. 6 includes textual connected components (CC) or objects with some jaggy artifacts. A sample of such objects and the corresponding CTR are shown in Fig. 14 a and b, respectively. As can be seen from Fig. 14 a, the object boundary is not smooth enough, thereby causing a decrease in readability and pleasure as well as the OCR facility. The boundary smoothing component of Fig. 6 tries to smooth the boundaries of each textual object like Fig. 14 a, by maximizing an objective function over the corresponding CTR pixels like Fig. 14 b. This objective function is defined as CF = If we assume a route inside a CTR, such as the one shown in Fig. 14 d with connected black-line-surrounded pixels, then, the aim is to find the best route which the above objective function is maximized over its pixels. If we assume a pixel of current route is located at the coordinate ( x , y ) , like the one marked by a black circle in Fig. 14 d, then K 1 ( x , degree of smoothness which is defined as the absolute angle between the lines L 1 and L 2 in Fig. 14 d which connect the current pixel to the next and previous ones, respectively. It takes on values in the interval [0 , X  ]. d 1 ( x , y ) and d are distances of current pixel to the left and right nearest non-CTR ones. N is the number of pixels of current route. The first term in Eq. (6) forces the route to be as smooth as pos-sible. The second term tries to keep the route in the middle of transition region if the smoothness is satisfied. Finally, the last term forces the route length to be as small as possible. The optimization of this objective function is solved here by the dynamic programming technique [ 40 ]. The result of applying the proposed boundary smoothing technique to the sample image of Fig. 14 a is shown in Fig. 14 c. 3.1.4 Modeling the edge regions As stated before, an intrinsic limitation of a standard MRC imaging model is that the binary mask can only represent discontinuous transitions between text, line art, and back-ground colors. In practice, this type of hard transition can introduce substantial artifacts since real documents often contain subtle continuous-tone transitions between regions. These continuous transitions are useful in anti-aliasing, and allow the document to be encoded with lower dpi, which can further reduce bit rate. In principal, it is possible to add edge detail to the foreground or background layers; however, in practice, this detail is lost when these layers are sub-sampled and encoded using lossy natural image coders at acceptable bit rates.

In this section, we model the edge regions by a proper polynomial function of a low order. By performing some experiments on the intensity profile of edge regions in docu-ment images of our database we found that this profile has a polynomial function behavior with a low order; furthermore, this behavior is rather constant on the entire document image. A sample Farsi grayscale textual image is shown in Fig. 15 a; the corresponding intensity profile over the dashed line in Fig. 15 a is shown in Fig. 15 b. As can be seen this profile in each edge region has a polynomial behavior at each of the two sides thus, we can model all edges inside an image by this half-polynomial function.

In the proposed compression method, we have chosen a polynomial function of order 2 for each document image and computed its three coefficients using some training images of our database. Then, we save and encode these coefficients to help the decoder to reconstruct the continuous transition of edge regions. The proposed edge modeling procedure has very low computational complexity and approximately, does not decrease the compression ratio because we need to save only three coefficients for a given text image. 3.2 Bi-level mask layer compression In order to encode the mask layer, which is a bi-level textual image, we use a 1D-PM-based technique in the chaincode transformation domain. The block diagram of this method is shown in Fig. 16 . At the first stage, it converts the 2D image signal to the corresponding 1D chaincode (CC) sig-nal (by concatenating the chaincode sub-signals of extracted patterns) and coordinates signal. Then, at the second stage, it produces the library of 1D chaincode prototypes by detect-ing the repetitive sub-signals inside the CC signal using the proposed technique of detecting the repeated sub-signals. At the last stage of Fig. 16 , it encodes the prototype and resulting residual chain code sub-signals using the proposed encoder. Each of the following sub-sections describes a block of Fig. 16 in order. 3.2.1 Computing the chain code signal In order to compute the CC sequence of any pattern, we should first choose a boundary pixel of that pattern as a start pixel then we track the pattern boundary in a clockwise direc-tion from the start pixel and use a proper code number from Fig. 17 aorb[ 41 ], depending on the direction of the move-ment along the boundary. We repeat this operation, until we again reach the start pixel. In this paper, we always assume that the start pixel is located in the most right (and at top) of the bi-level pattern like the pixel A in Fig. 17 d. If a pattern has inner holes, like Fig. 17 c, we compute and concatenate the chaincode sequences of inner boundaries using their start pixels (like Fig. 17 d). We extract the patterns of input image in a serpentine order, like Fig. 17 e, and concatenate the cor-responding chaincode sequences in order to obtain the final  X  X C signal X . In addition to chain code sequences, we should compute and concatenate the coordinates of start pixels to form the  X  X oordinates signal X . 3.2.2 Detecting prototype sub-signals In the Farsi/Arabic script, contrary to the printed Latin script, letters normally may attach together and produce many dif-ferent patterns some of which are fully or partially subsets of some others. Detecting such situations and exploiting them to reduce the library size, has a considerable effect on the compression ratio.

There are some types of repetitive patterns in typical Farsi and Arabic textual images. Figure 18 a shows a part of a sam-ple Farsi text image in which some samples of these repetitive patterns are marked by numerical marks. All similar repet-itive patterns are marked by the same and distinct number. The first type of repetitive patterns is full repetition. In this type, a pattern or a group of patterns (which may correspond to a word, phrase and even a sentence) is repeated in the text image. In Fig. 18 a, the cases marked by numbers 1 and 2 are samples of repetition of a pattern, and the samples marked by numbers 3, 4, 5 and 6 are samples of repetition of a group of patterns. The second type of the repetitive patterns is partial repetition in which some part of a pattern is repeated in the text image. A sample of this type is marked in Fig. 18 aby number 7. The third type of pattern repetition is boundary repetition in which some part of CC sequence of a pattern repeats inside the CC signal of the text image. Two samples of this type of repetition are marked by numbers 8 and 9 in Fig. 18 a and their large-scaled illustrations are shown in Fig. 18 b and c.
 It is obvious that if we find all repetitive sub-signals inside CC signal of a text image, we would detect all existing repet-itive patterns of any type. Indeed, if we find larger repetitive sub-signals, we would detect larger repetitive patterns.
It should be noted that in library size reduction, we aim to compromise between three factors which include the length of a group of detected repetitive sub-signals, the correspond-ing number of repetitions (or the number of members of that group), and the corresponding average degree of similarity between repeated sub-signals inside that group. Usually, by increasing the length of a repetitive sub-signal, the number of repetitions and the average degree of similarity are decreased. Among these three factors, the first factor causes to further reduce the library size and consequently increase the overall compression ratio, but, the last two factors cause to increase the variance of the corresponding residual CC signal (which will be defined later), and thus, decrease the overall compres-sion ratio. Consequently, we should compromise between these factors in order to increase the overall compression ratio. To this end, we can define a proper objective function of these factors and optimize it.

In the proposed technique for detecting repetitive sub-signals, the aim is to find all groups of repeated sub-signals which simultaneously maximize the mentioned three factors as much as possible. We convert the input text image to the corresponding CC signal and perform all subsequent proce-dures in the new 1-D domain. We define W opt ( X = X 0 ) as the optimal length of a group of repetitive sub-signals, one of which starts at the point (index) X = X 0 in the CC signal. The proposed algorithm, for each point X of the CC signal, considers a  X  X andidate sub-signal X  which is a sub-signal that starts from this point and has the length W as a parameter. Then, it compares this candidate sub-signal with all possi-ble co-length sub-signals of the CC signal, using the cross correlation similarity measure. Finally, by varying W and optimizing an objective function, it computes a score func-tion Score ( X = X 0 ) and compares it with a threshold in order to determine whether the current sub-signal can be a library prototype or not; if so, then its proper length, W ( X = X 0 ) , is computed as well.
Corresponding to each specific value of W, we obtain a similarity curve, whose value at a point n represents the degree of similarity between current candidate sub-signal (starting at point X = X 0 ) and that sub-signal which starts at the point n of the CC signal. The algorithm considers W as a parameter which varies in a predetermined interval by a predetermined step. By computing all similarity curves cor-responding to all possible values of W , we obtain a 3D simi-larity surface versus the axes W and n . This similarity surface corresponds to the point X = X 0 .

As W is increased, the number of repetitive sub-signals and the average degree of their similarity generally is decreased. Thus, we should define and optimize a proper objective function in order to compromise this trade-off and determine the proper length of the current candidate sub-signal, W opt ( X = X 0 ) . We define this objective function as follows to find the proper length of the current candidate sub-signal W where L is the length of the CC signal, and K 1 and K 2 are weights of the mentioned factors, i.e., NP, SV, and SSL which correspond to the number of peaks, similarity value, and sub-signal length, respectively. These weights are experimentally set to 1. The index i is related to the i  X  X h allowable value of W. After finding the best value of W at the point X = X 0 we compute the following score function i Score ( X = X 0 ) = We repeat the above procedure to compute the values of W find all peaks of the score function, Score( X ), and select those which are above a predetermined threshold, T opt . Finally, we use the corresponding indices of the selected peaks and the values of W opt ( X ) at these indices, to compute the best repeated sub-signals (library prototypes).

In terms of computational complexity ( C ), we can for-mulate total number of operations as follows: if we assume L
Ch is the length of image chaincode signal, for any specific value for length of a sub-signal, L Sub , (which starts from a point N cand ) and any specific index value X in the chaincode signal from which the candidate sub-signal is compared to, we should perform approximately P  X  L sub operations (sum-mation and/or multiplication) in order to compute the cross correlation measure ( P is a constant). Since, X changes from 1to L Ch (in steps of Step 1 ), N cand changes from 1 to L steps of Step 2 ) , and L Sub changes in a predetermined range (say N 1 to N 2 ) , thus we need total number of operations equal to C = = P whereweusedStep 1 = Step 2 = 3 (at 300dpi). L Ch depends onsomecharacteristicsoftextimagemostimportantofwhich include image dpi. The parameters N 1 and N 2 depend on image dpi as well as how long the repeated patterns we desire to detect. Consequently, as can be understood, spatial reso-lution is the most important factor which affects the compu-tational complexity. 3.2.3 The compression method At the first stage of the proposed mask layer compression, the corresponding CC and coordinates signals of the input bi-level mask layer are computed. At the second stage, we employ some lossy procedures in order to improve the image quality and increasing the compression ratio. These proce-dures include boundary smoothing and spot noise removal, and quantizing the coordinate signal values by dividing them by 2 and rounding the results [ 41 ]. At the third stage, the best repetitive sub-signals are detected using the proposed algorithm for detecting the repetitive sub-signals. For each group of similar sub-signals, a prototype is determined, for example, by selecting the first-encountered one as the pro-totype signal. Finally, the library is formed. Each prototype in the library has an index number. Then, each prototype is subtracted from the corresponding matched parts in the CC signal. We call the resulting CC signal as the  X  X esidual chain code X  (RCC) signal. The start index of each matched sub-signal (in the CC signal) is saved in an index vector of [ Idx , N , N that the prototype signal with the index Idx , is repeated at the points N 1 , N 2 ,... of the CC signal.

Next, RCC and prototype signals are converted to their corresponding DCC signal; this conversion is because of the usefulpropertyofDCCsignalsincodingapplications.There-fore, at the end of the third stage of the proposed bi-level mask compression, we have a residual DCC (RDCC) signal, some prototype DCC signals (PDCC), some index vectors, and coordinates signal. These sequences are encoded at the next (fourth) stage. RDCC and PDCC signals are encoded using run-length coding followed by multi-symbol QM coder of [ 35 ]. The sequence of index vectors and coordinates signal are encoded using a modified multi-symbol QM coder. In this modification, for encoding any new arrived number, we first subtract it from the rounded average of all previously encoded numbers in the current sequence and then encode the result by the multi-symbol QM coder. This subtraction is meant for producing smaller numbers and a higher compres-sion efficiency; because the smaller the numbers, the smaller is the length of the output bit-stream of the multi-symbol QM-coder [ 35 ]. 3.3 Handling the foreground and background layers To handle the foreground layer (block 3 of Fig. 5 ), since we have assumed that textual objects have rather constant color, we can assign a single color vector to every textual object. This task has two advantages; first, the quality of final reconstructed document image is preserved to an acceptable extent. Second, the overall compression ratio is increased. This tokenized representation is somewhat similar to that of DigiPaper [ 9 ]. After modeling the foreground layer with the mentioned sequence of color vectors, we use the sequence of vector quantization, run-length encoding, and modified multi-symbol QM coder techniques, to encode the color vec-tor sequence.

To handle the background layer (blocks 4 and 5 of Fig. 5 ), we first, segment this layer to some regions using the k-means clustering with number of clusters, N C , equal to 4; thus, each region (among say N 0 resulting regions) in the segmented background, belongs to one of the four clusters (or classes) of  X  X igh variation X ,  X  X edium variation X ,  X  X ow variation X , and  X  X ery low variation X . After performing this segmentation, we reclassify each region to one of the two classes of  X  X igh Var-iation (HV) X  and  X  X ow Variation (LV) X  as follows: F ( i ) = if F ( i )&gt; T  X  R ( i )  X  LV
Otherwise ,  X  R ( i )  X  HV where N 0 is the number of regions (or segments), R ( i ) the i  X  X h region, and A ( i ) is the area of i  X  X h region. After seg-menting the background layer, we should save and encode the segmentation map to help the decoder to properly recon-struct the background layer. Segmentation map is a bi-level image which contains some line-drawings. We encode the segmentation map, (block 8 of Fig. 5 ), by first, chaincode transformation and then, encoding the result sequence using the modified multi-symbol QM coder.

The LV layer is encoded (block 7 of Fig. 5 ) similar to the foreground layer; i.e., by modeling each segment of this layer by a constant color vector, and encoding the sequence of color vectors by employing the vector quantization, run-length coding, and the modified multi-symbol QM coder techniques, respectively.

In order to encode the HV layer, since it has not necessarily rectangular shape we use a simple pixel reordering technique to fit it into a square region and thus, prevent assigning bits to empty (or non-HV) regions. To this end, we first, form an empty square image with a height equal to round ( where A HV is the total area of HV regions; then, we scan the HV layer in an up-to-bottom and left-to-right order and put every HV-pixel in the square image to fill it. The filling order is also up-to-bottom and left-to-right. Now, by encod-ing the resulted square image and its height, the decoder can reconstruct the HV layer. We use the JPEG2000 standard with moderate compression ratio to encode the filled square image (block 6 of Fig. 5 ). 4 Experimental results The document image database used in this work has approxi-mately 800 Farsi, Arabic, and English document images with spatial resolutions of 100, 200, 300 and 600dpi obtained by scanning some scientific Farsi, Arabic, and English maga-zines and books. Figure 19 shows some sample document images from this database.

We can evaluate the performance of the proposed com-pression method in three stages. At the first stage, we evaluate the segmentation performance of the proposed segmentation method of Sect. 3.1 . At the second stage, we evaluate the compression performance of the lossy version of the pro-posed bi-level textual image compression method, and finally at the last stage, we evaluate the overall lossy compression performance of the proposed compound document image compression method.

In order to evaluate the segmentation performance of the proposed method we can use the quantitative measures of Precision and Recall [ 42 , 43 ] for each textual image. To our best knowledge, these measures have not been used in MRC-based text separation yet. The former is defined as the num-ber of true positives (i.e., the number of objects correctly labeled as belonging to the text class) divided by the total number of objects labeled as text objects (i.e., sum of the true positives and false positives, which are objects incor-rectly labeled as belonging to the text class); and the latter is defined as the number of true positives divided by the total number of objects that actually belong to the text class (i.e., the sum of true positives and false negatives, which are objects which were not labeled as belonging to the text class but should have been) [ 43 ]. We can compute these measures for each textual image of our database and finally, compute their average value to obtain the average Precision and aver-age Recall of the proposed segmentation method as well as that of the DjVu . After doing so, we obtained the average Precision and average Recall of the proposed segmentation method respectively as 95 and 92% and those of the DjVu segmentation method respectively as 90 and 79%. As can be seen, DjVu segmentation method has rather high precision but, has low recall mostly due to missing the small textual objects.

Often, there is an inverse relationship between Precision and Recall, where it is possible to increase one at the cost of reducing the other. Usually, Precision and Recall scores are not discussed in isolation. Instead, either values for one measure are compared for a fixed level at the other measure (e.g., precision at a recall level of 75%) or both are combined into a single measure, such as the F-measure, which is the weighted harmonic mean of precision and recall [ 42 , 43 ]. The F -measure or F 1 -score is defined as F If we compute the F 1 -measure using the above precision and recall numbers, we obtain 0.93 and 0.84 for the proposed segmentation method and the DjVu segmentation method, respectively.

We may compare the computational complexity of the proposed and DjVu segmentation methods by comparing the corresponding run times. In our implementation, the average run time of the proposed segmentation, for the images of our database, was roughly 1.8 times longer than that of the DjVu segmentation method.

Document segmentation has considerable effect on the compression performance as well as the OCR recognition rate. In order to evaluate the effect of the proposed segmen-tation method, we fed the bi-level textual images, resulting from both the proposed and the DjVu segmentation meth-ods, to the Farsi/Arabic OCR system of [ 16 ]. It should be noted that the Farsi and Arabic scripts are the same, but, some Arabic textual images contain some guide marks for better pronunciation; in addition, the Arabic script has fewer number of letters than the Farsi script.

The proposed OCR system of [ 16 ] has used two approac-hes of structural and neural network. The structural approach has higher recognition rate [ 16 ]; thus, we employed this approach. After applying the OCR system to the segmented document images of our database, we achieved the average recognition rates of 78 and 93%, respectively, before and after employing the proposed boundary smoothing technique of Sect. 3.3 ; while the average recognition rate was 83% for DjVu -segmented document images without considering the problem of missing the small (but important) textual objects in DjVu ; if we consider this problem into account, the recog-nition rate will be significantly decreased.

We can consider at least two reasons for relatively low recognition rate of DjVu ;first, DjVu segmentation usually miss some small but important textual objects such as dots and strokes. In the Farsi/Arabic script removing or adding one or more dots may convert a letter into another. Figure 20 shows a sample word image as well as the corresponding segmentation results of the proposed and DjVu segmentation methods; as can be seen, DjVu has missed a dot and converted the original character to another; therefore, change the OCR results.

Second, DjVu may mistakenly attach small textual objects, especially the dots, and consequently, change the OCR resu-lts. Figure 21 shows a sample word image as well as the seg-mentation results of the proposed and DjVu methods. As can be seen, DjVu has mistakenly attached all dots.

The third reason relates to the OCR system of [ 16 ] which is used in our experiments. For Farsi/Arabic OCR, after word segmentation, some features are extracted and classified into predetermined classes. The OCR system of [ 16 ]usedthe moment invariants used by Hu [ 44 ] which are a non-linear combination of geometric moments and has the desirable property of being invariant under image translation, scaling and rotation. The central moments, which are invariant under any translation, are defined as M  X  M where m and n are dimensions of the image. The first four (among seven) moment invariants that have been used by Hu are given by  X   X   X   X  These functions can be normalized to make them invari-ant under a scale change by using the normalized central moments instead of the central moments. The normalized central moments are defined by m These, when substituted into Eq. (15), will give some mom-ents which are invariant to translation, scale change and rota-tion [ 16 ]. By computing the feature vectors of characters of training images, we obtain a spatial distribution or cluster for each character. Each cluster has a center which corresponds to the candidate character of that cluster. Generally, more similar the shape of a given character to a specific candidate character, nearer is the feature vector of that character to the candidate character; and consequently, more reliable the rec-ognition result of that character. The segmented characters of the proposed segmentation method are more similar to the corresponding candidate (or ideal) characters than those of DjVu ; thus, the average recognition rate of the proposed segmentation method is higher than that of DjVu . Figure 22 shows three character images of a Farsi/Arabic alphabet let-ter; Fig. 22 a is a candidate (or ideal) character, Fig. 22 bisa sample segmented by the proposed method, and Fig. 22 cisa sample segmented by DjVu . As can be seen, the segmentation result of the proposed method is noticeably more similar to the ideal one.

At the second stage of our performance evaluation, we evaluate the compression performance of the proposed bi-levelmasklayercompression(inthelossymode)byapply-ing this method to the corresponding mask layer images of our document images. We have classified the mask layer images into six classes. These six classes include mostly graphics, mixed Farsi-graphics, mostly Farsi, mixed Farsi-Arabic, mostly Arabic, and English textual images, respec-tively. As mentioned before, the Arabic and Farsi scripts are similar but some Arabic textual images contain some guide marks for better pronunciation. The mostly Arabic class of images contains such images. Other Arabic textual images are included in the mostly Farsi class.

We have compared the compression ratio of the proposed method with that of the lossy bi-level textual image com-pression method jb2 whichisusedinthe DjVu document image compression [ 6 ]. The method jb2 is a variant of the JBIG2 standard and it is a lossy compression method. In order to employ the lossy version of the proposed compres-sion method, we have used some lossy procedures which include boundary smoothing, spot noise removal, and quan-tizing the coordinate signal values by dividing them by 2 and rounding the results. The results of applying some of these lossy procedures are shown in Fig. 23 . An advantage of the proposed lossy procedures is that they usually improve or preserve the image quality and/or readability especially for Farsi and Arabic textual images.

The compression ratio curves of the proposed method to that of jb2 , for each image class, is shown in Fig. 24 versus the four spatial resolutions 100, 200, 300 and 600dpi, respec-tively. Also the relative compression ratio of the proposed method to jb2 versus the six image class numbers is shown in the bar chart of Fig. 25 a for the four spatial resolutions. As can be seen, the best compression ratio of the proposed method occurs for the mostly Farsi or Arabic textual images mostly due to the library size reduction technique. The incre-ment percentage of compression ratio due to the proposed library size reduction technique is shown in the curve of Fig. 25 b for each of the six image classes; as can be seen, the proposed library size reduction technique has its highest effect for the Farsi and Arabic image classes.

For English textual images, as is apparent from Fig. 25 a, the proposed method compression ratio is still better than that of jb2 . In addition, we have the best lossy compression per-formance for mostly Farsi or Arabic textual images as high as four times and the lowest one for English textual images as high as two times at 300dpi.

The last performance evaluation stage relates to the over-all lossy compression performance of the proposed document image compression method. Figure 26 a to c show part of a sample Farsi document image with the corresponding fore-ground and background layers, respectively.

Figure 27 a and b show the corresponding LV and HV layers of the background layer of Fig. 26 c, respectively. Figure 27 c shows the square image resulting from reorder-ing the HV image pixels, and finally, Fig. 27 dshowsthe resulting compressed image using the JPEG2000 standard. Figure 28 a and b show the final compressed image using the proposed lossy compression and DjVu methods, respec-tively, at 300dpi; the former is compressed by 0.21 bpp (bit per pixel) and the latter is compressed by 0.26 bpp . As can be seen, the reconstructed image quality resulting from the proposed method is higher than that of the DjVu method at the same bit rate. This result is more emphasized by com-puting the MOS (mean opinion score) measure which takes values in the range [0,5] and is computed by averaging the scores taken by some human viewers. We obtained this value as 4.2 for the proposed compression method and as 3.3 for DjVu .
 The compression ratio curves of the proposed method and DjVu , for each image class, is shown in Fig. 29 versus the four spatial resolutions of 100, 200, 300, and 600 dpi, respec-tively. Also the relative compression ratio of the proposed method to that of DjVu versus the six image class num-bers is shown in the bar chart of Fig. 30 for the four spatial resolutions. As can be seen, the proposed method has con-siderably higher average compression ratio than that of the state-of-the-art compression method of DjVu ,ashighas 1.75 X 2.3, at 300dpi, depending on the image class. In addi-tion, the proposed method is more appropriate for Farsi/ Arabic text images than English ones. 5 Conclusion In this paper, MRC-based document image compression method was proposed which was intended for Farsi and Arabic document images and for archiving purposes as well as distributing high-quality, high-resolution images of scanned documents in color over the Internet. It attempted to enable fast transmission of document images over low-speed connections, while faithfully reproducing the visual aspect of the document, including color, fonts, pictures, and paper texture. It compromised rate-distortion trade-off bet-ter than the existing state-of-the-art document compression method of DjVu . It had higher performance in the aspects of segmentation, bi-level mask layer compression, and the overall compound document image compression regarding to some quantitative measures.

The proposed compression method used an effective seg-mentation method for MRC-based document segmentation which was sensitive enough to small textual details. It had three components of gradient-based binarization, structural analysis and boundary smoothing. The proposed compres-sion method modeled the edge regions by a polynomial function of low order to solve the problem of preserving continuous-tone transitions in the edge regions and conse-quently, improve the reconstructed image quality. In order to encode the mask layer, it used a 1D-pattern matching (PM)-based bi-level image compression method in the chain-code transformation domain.

Experimental results of performing the proposed com-pression method on some machine-printed Farsi, Arabic, and English document images of our database showed that the proposed method had considerably higher compression performance than that of the state-of-the-art compression method of DjVu , as high as 1.75 X 2.3 at 300dpi. The max-imum compression ratios were obtained for Farsi and Arabic document images. In addition, for OCR-intended-compres-sion purposes, the proposed segmentation method had higher OCR performance regarding to the quantitative measure of recognition rate.
 References
