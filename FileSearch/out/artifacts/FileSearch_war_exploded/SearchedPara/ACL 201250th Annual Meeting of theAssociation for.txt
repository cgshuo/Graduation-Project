 partial semantic information about Chinese charac-ters. Hence, one may use radicals as hints to link the meanings and writings of Chinese characters. For instance,  X   X   X (he2, river) [Note: Chinese char-acters will be followed by their pronunciations, denoted in Hanyu pinyin, and, when necessary, an English translation.],  X   X   X (hai3, sea), and  X   X   X (yang2, ocean) are related to huge water sys-tems, so they share the semantic radical,  X  , which is a pictogram for  X  X ater X  in Chinese. Applying the concepts of pictograms, researchers designed games, e.g., (Lan et al., 2009) and animations, e.g., (Lu, 2011) for learning Chinese characters. 
The aforementioned approaches and designs mainly employ visual stimuli in activities. We re-port exploration of using the combination of audio and visual stimuli. In addition to pictograms, more than 80% of Chinese characters are phono-semantic characters ( PSCs , henceforth) (Ho and Bryant, 1997). A PSC consists of a phonological component ( PC , henceforth) and a semantic com-ponent. Typically, the semantic components are the radicals of PSCs. For instance,  X   X   X (du2),  X   X   X (du2),  X   X   X  (du2),  X   X   X (du2) contain different radicals, but they share the same phonological components,  X   X   X (mai4), on their right sides. Due to the shared PC, these four characters are pro-nounced in exactly the same way. If a learner can learn and apply this rule, one may guess and read  X   X   X (du2) correctly easily. 
In the above example,  X   X   X  is a normal Chinese character, but not all Chinese PCs are standalone characters. The characters  X   X   X (jian3),  X   X   X  (jian3), and  X   X   X (jian3) share their PCs on their right sides, but that PC is not a standalone Chinese character. In addition, when a PC is a standalone character, it might not indicate its own or similar pronunciation when it serves as a PC in the hosting character, e.g.,  X   X   X  and  X   X   X  are pronounced as /mai4/ and /du2/, respectively. In contrast, the pro-nunciations of  X   X   X ,  X   X   X ,  X   X   X , and  X   X   X  are /tao2/. 
Pronunciations of specific substrings in words of alphabetic languages are governed by grapheme-phoneme conversion (GPC) rules, though not all languages have very strict GPC rules. The GPC rules in English are not as strict as those in Finish components such as the manage-ment of players X  accounts and the maintenance of players X  learning profiles. Yet, due to the page limits, we focus on the parts that are most relevant to the demonstration. 
Figure 1 shows a screenshot when a player is playing the game. This is a game of  X  X hac-a-mole X  style. The target PC appears in the upper middle of the window ( X   X   X (li3) in this example), and a character and an accompanying monster (one at a time) will pop up randomly from any of the six holes on the ground. The player will hear the pro-nunciation of the character (i.e.,  X   X   X (li3)), such that the player receives both audio and visual stim-uli during a game. Players X  task is to hit the mon-sters for the characters that contain the shown PC. The box at the upper left corner shows the current credit (i.e., 3120) of the player. The player X  X  credit will be increased or decreased if s/he hits a correct or an incorrect character, respectively. If the player does not hit, the credit will remain the same. Play-ers are ranked, in the Hall of Fame, according to their total credits to provide an incentive for them to play the game after school. 
In Figure 1, the player has to hit the monster be-fore the monster disappears to get the credit. If the player does not act in time, the credit will not change. 
On ordinary computers, the player manipulates the mouse to hit the monster. On multi-touch tablet computers, the play can just touch the monsters with fingers. Both systems will be demoed. 2.1 Challenging Levels At the time of logging into the game, players can choose two parameters: (1) class level: lower class (i.e., grades 1 and 2), middle class (i.e., grades 3 and 4), or upper class (i.e., grades 5 and 6) and (2) speed level: the duration between the monsters X  popping up and going down. The characters for lower, middle, and upper classes vary in terms of frequency and complexity of the characters. A stu-dent can choose the upper class only if s/he is in the upper class or if s/he has gathered sufficient credits. There are three different speeds for the monsters to appear and hide: 2, 3, and 5 seconds. Choosing different combinations of these two pa-The data structure of a game is simple. When com-piling a game, a teacher selects the PC for the game, and prepares six characters that contain the PC (to be referred as an In-list henceforth) and four characters as distracter characters that do not contain the PC (to be referred as an Out-list hence-forth). The simplest internal form of a game looks like {target PC=  X   X   X , In-list=  X   X  X  X  X  X  X  X  X   X , Out-list=  X   X  X  X  X  X  X   X  }. We can convert this struc-ture into a game easily. Through this simple struc-ture, teachers choose the PCs to teach with charac-ter combinations of different challenging levels. 
During the process of playing, our system ran-domly selects one character from the list of 10 characters. In a game, 10 characters will be pre-sented to the player. 3 Preliminary Evaluation and Analysis The game platform was evaluated with 116 stu-dents, and was found to shorten students X  response times in Chinese naming tasks. 3.1 Procedure and Participants The evaluation was conducted at an elementary school in Taipei, Taiwan, during the winter break between late January and the end of February 2011. The lunar new year of 2011 happened to be within this period. 
Students were divided into an experimental group and a control group. We taught students of the experimental group and showed them how to play the games in class hours before the break be-gan. The experimental group had one month of time to play the games, but there were no rules asking the participants how much time they must spend on the games. Instead, they were told that they would be rewarded if they were ranked high in the Hall of Fame. Table 2 shows the numbers of participants and their actual class levels. 
As we explained in Section 2.1, a player could choose the class level before the game begins. Hence, for example, it is possible for a lower class player to play the games designed for middle or even upper class levels to increase their credits faster. However, if the player is not competent, the credits may be deducted faster as well. In the eval-uation, 20 PCs were used in the games for each class level in Table 1. 
Pretests and posttests were administered with the standardized (1) Chinese Character Recognition lower class students were very young, so we con-jectured that it was harder for them to remember the writing of Jhuyin symbols after the winter break. Hence, after the evaluation, we strengthened the feedback by adding Jhuyin information. In Fig-ure 2, the Jhuyin information is now added beside the sample Chinese words, i.e.,  X   X  X  X   X  (li3 mian4). 4 An Open Authoring Tool for the Games Our game platform has attracted the attention of teachers of several elementary schools. To meet the teaching goals of teacher in different areas, we have to allow the teachers to compile their own games for their needs. 
The data structure for a game, as we explained in Section 2.3, is not complex. A teacher needs to determine the PC to be taught first, then s/he must choose an In-list and an Out-list. In the current im-plementation, we choose to have six characters in the In-list and four characters in the Out-list. We allow repeated characters when the qualified char-acters are not enough. 
This authoring process is far less trivial as it might seem to be. In a previous evaluation, even native speakers of Chinese found it challenging to list many qualified characters out of the sky. Be-cause PCs are not radicals, ordinary dictionaries would not help very much. For instance,  X   X   X  (mai2),  X   X   X (li2),  X   X   X (li3), and  X   X   X (li3) belong to different radicals and have different pronuncia-tions, so there is no simple way to find them at just one place. 
Identifying characters for the In-list of a PC is not easy, and finding the characters for the Out-list is even more challenging. In Figure 1,  X   X   X  (li3) is the PC to teach in the game. Without considering the characters in In-list for the game, we might believe that  X   X   X  (jia3) and  X   X   X  (cheng2) look equally similar to  X   X   X , so both are good distract-ers. If, assuming that  X   X   X (li3) is in the In-list,  X   X   X  (jia3) will be a better distracter than  X   X   X  (cheng2) for the Out-list, because  X   X   X  and  X   X   X  are more similar in appearance. By contrast, if we have  X   X   X  in the In-list, we may prefer to having  X   X   X  (cheng2) than having  X   X   X  in the Out-list. 
Namely, given a PC to teach and a selected In-list, the  X  X uality X  of the Out-list is dependent on the characters in In-list. Out-lists of high quality influence the challenging levels of the games, and will become a crucial ingredient when we make the games adaptive to players X  competence. 4.1 PC Selection Chinese charac-ters was the same, but we utilized different object functions in selecting and ranking the characters. We considered all elements in a character to rec-ommend charac-ters for In-lists, but focused on the inclusion of target PCs in the decomposed characters to rec-ommend characters for Out-lists. Again our rec-ommendations for the Out-lists were not perfect, and different ranking functions affect the perceived usefulness of the authoring tools. 
Figure 3 shows the step to choose characters in the Out-list for characters in the In-list. In this ex-ample, six characters for the In-list for the PC  X   X  had been chosen, and were listed near the top:  X   X  X  X  X  X  X  X  X  X   X . Teachers can find characters that are similar to these six correct characters in separate pull-down lists. The screenshot shows the operation to choose a character that is similar to  X   X   X  (yao2) from the pull-down list. The selected character would be added into the Out-list. 4.3 Game Management We allow teachers to apply for accounts and pre-pare the games based on their own teaching goals. However, we cannot describe this management subsystem for page limits. 5 Evaluation of the Authoring Tool We evaluated how well our tools can help teachers with 20 native speakers. 5.1 Participants and Procedure We recruited 20 native speakers of Chinese: nine of them are undergraduates, and the rest are gradu-ate students. Eight are studying some engineering fields, and the rest are in liberal arts or business. The subjects were equally split into two groups. The control group used only paper and pens to au-thor the games, and the experimental group would use our authoring tools. We informed and showed the experimental group how to use our tool, and members of the experimental group must follow an illustration to create a sample game before the evaluation began. Every subject must author 5 games, each for a (Jared et al., 1990). Evidence shows that foreign students did not take advantage of the GPC rules in Chinese to learn Chinese characters (Shen, 2005). Hence, it should be interesting to evaluate our sys-tem with foreign students to see whether our ap-proach remains effective. Acknowledgement References 
Computational models for the interpretation and elaboration of metaphors should allow speakers to exploit the same flexibility of expression with m a-chines as they enjoy with other humans. Such a goal clearly requires a great deal of knowledge, since m e t aphor is a knowledge -hungry mechanism par excellance (see Fass, 1997). However, much of the knowledge required for metaphor interpret a tion is already implicit in the large body of met a phors that are active in a community (see Martin, 1990; Mason, 2004). Existing metaphors are the m selves a valuable source of knowledge for the pr o duction of new metaphors, so much so that a system can mine the relevant knowledge from corpora of fi g-urative text (e.g. see Veale, 2011; Shutova, 2010).
One area of human -machine interaction that can clearly benefit from a competence in metaphor is that of information retrieval (IR). Speakers use metaphors with ease when eliciting information from each other, as e.g. when one suggests that a certain CEO is a t y rant or a god, or tha t a certain company is a dinosaur while another is a cult. Those that agree might r e spond by elaborating the metaphor and providing su b stantiating evidence, while those that disagree might refute the metaphor and switch to another of their own choosing. A well -chosen metaphor can provide the talking points for an informed conversation, allowing a speaker to elicit the desired knowledge as a comb i-nation of objective and subjective elements.

In IR, such a capability should allow searchers to express their inf ormation needs subjectively, via a f fective metaphors like  X  X  is a cult X . The goal, of course, is not just to retrieve documents that make explicit use of the same metaphor  X  a literal matc h-ing of non -literal texts is of limited use  X  but to wide range of metaphors and an even wider range of topics, requires a robustly shallow approach. We exploit the fact that the Google n -grams (Brants and Franz, 2006) contains a great many copula metaphors of the form  X  X  is a Y X  to unde r-stand how X is typically viewed on the web. We further exploit a large dictionary of affective ster e-otypes to provide an understanding of the +/ -pro p-erties and behaviors of each source concept Y. Combining these resources allows the Met a phor Magnet system to understand the implications of a metaphorical query  X  X  as Z X  in terms of the qual i-ties that are typically considered salient for Z and which have been corpus -attested as apt for X.
We describe the construction of our lexicon of affective stereotypes in section 2. Each stereotype is associated with a set of typical p roperties and beha v iors (like sprawling for giant , or inspiring for guru ), where the overall affect of each stereotype d e pends on which subset of qualities is activated in a given context (e.g., giant can be construed pos i-tively or negatively, as can baby , soldier , etc.). We describe how Metaphor Magnet exploits these st e-reotypes in sec tion 3, before providing a worked example in section 4 and screenshots in section 5. We construct the lexicon in two stages. In the first stage, a large collection of stereotypical descri p-tions is harvested from the Web. As in Liu et al . (2003), our goal is to acquire a lightweight co m-mon -sense representation of many everyday co n-cepts. In the s e cond stage, we link these common -sense qualitie s in a support graph that captures how they mutually su p port each other in their co -description of a stereotyp i cal idea. From this graph we can estimate positive and negative valence scores for each property and behavior, and default averages for the stere o types that exhibit them .
Similes and stereotypes share a symbiotic rel a-tionship: the former exploit the latter as reference points for an evocative description, while the latter are pe r petuated by their constant re -use in similes. Expan d ing on the approac h in Veale (2011), we use two kinds of query for h arvesting stereotypes from the w eb. The first,  X  X s ADJ as a NOUN X , a c-quires typical adjectival properties for noun co n-cepts; the second,  X  X ERB +ing like a NOUN X  and  X  X ERB + ed like a NOUN X , acquires typical ve rb behaviors. Rather than use a wildcard * in both cally negative words, and a set +R of typically po s itive words. Given a few seed members of -R (such as sad , disgusting , evil , etc.) and a few seed members of +R (such as happy , wonderful , etc.), we find many other candidates to add to +R and -R by considering n eig h bors of these seeds in N . After three iterations in this fashion, we populate +R and -R with approx. 2000 words each.
 N -( p ) as follows: We can now assign positive and negative v a lence scores to each vertex p by interpolating from re f-erence va l ues to their neighbors in N : If a term S denotes a stereot ypical idea and is d e-scribed via a set of typical properties and behaviors t ypical(S ) in the lexicon, then: Thus, (5) and (6) calculate t he mean affect of the properties and behaviors of S , as represented via typical(S ) . We can now use (3) and (4) to separate typical(S ) into those elements that are more neg a-tive than positive (putting a negative spin on S ) and into those that are more posit ive than negative (pu t ting a positive spin on S ): 2.1 Evaluation of Stereotypical Affect In the process of populating +R and -R , we ident i-fy a r eference set of 478 positive st e reotypes (such as saint and hero ) and 677 negative stere o types (such as tyrant and monster ). When we use these refe r ence points to test the effectiveness of (5) and (6)  X  and thus, indirectly, of (3) and (4) and of the M  X  src (S)  X  src (T )  X  {S} is an apt vehicle for T if: and the degree to which M is apt for T is given by: We can construct an interpretation for T is S by considering not just {S}, but the stereotypes in src (T) that are apt for T in the context of T is S , as well as the stereotypes that are commonly used to describe S  X  that is, src (S)  X  that are also apt for T: (13) interpretation (T, S) In effect then, the interpretation of T is S is itself a set of apt metaphors for T that expand upon S. The elements { M i } of in terpretation (T, S) can now be sorted by aptness ( M i T, S ) to produce a ranked list of interpretations (M 1 , M 2 ... M n ). For any inte r-pretation M, the salient features of M are thus: If T is S is a creative IR query  X  to find doc u-ments that view T as S  X  then interpr e t a tion (T, S) is an expansion of T is S that i n cludes the co m-mon metaphors that are consistent with T viewed as S. F or any viewpoint M i , salient (M i , T, S ) is an e x pa n sion of M i that i n cludes all of the qualities that T is lik e ly to e x hi b it when it behaves like M i . Consider the query  X  Google is Microsoft  X , which expresses a need for documents in which Google exhibits qualities typically associated with M i-crosoft. Now, both Google and Microsoft are co m-plex concepts, so there are many ways in which they can be considered similar or dissimilar, either in a good or a bad light. However, the most sa lient aspects of M i crosoft will be those that underpin our common metaphors for Microsoft, i.e., stere o-types in src (Microsoft). These metaphors will pr o-vide the talking points for the inte r pretation . metaphors, 57 for Microsoft and 50 for Google: src (Microsoft) = { king, master, threat, bully, giant, More fo cus is achieved with the simile query  X  X oogle is as  X  powerful as Microsoft X . In expli c it similes, we need to fo cus on just a sub set of the salient prope r ties, using e.g. this variant of (10): In this -powerful case, the interpretation becomes: { bully, giant, devil , monopoly , dinosaur, ...} Metaphor Magnet is designed to be a lightweight web application that provides both HTML output (for h u mans ) and XML (for client applications). The system allows users to enter queries such as Google is  X  Microsoft , life is a + game , Steve Jobs is Tony Stark , or even Rasputin is Karl Rove (queries are case -sensitive). Each query is e x panded into a set of apt met aphors via mappings in the Google n -grams, and each metaphor is expanded into a set of co n textually apt qualities. In turn, each quality is then expanded into an IR query that is used to r e-trieve relevant hits from Google. In effect, the sy s-tem allows user s to interface with a search engine like Google using metaphor and other affective language forms. The demonstration system can be a c cessed using a standard browser at this URL :
Metaphor Magnet can exploit t he properties and behaviors of its stock of almost 10,000 stereotypes, and can infer salient qualities for many pro p er -named entities like Karl Rove and Steve Jobs using a combination of copula statements from the Google n -grams (e.g.,  X  Steve Jobs is a vis ionary  X ) and category assignments from Wikipedia .

The interpretation of the simile/query  X  Google is as -powerful as Microsoft  X  thus highlights a sele c-tion of affective viewpoints on the source concept, Microsoft , and picks out an apt selection of vie w-point s on the target Google . Metaphor Magnet di s-plays both selections as phrase clouds in which each hyperlinked phrase  X  a combination of a n apt st e reotype and a salient quality  X  is clickable, to yield linguistic evidence for the selection and co r-responding w eb -search results (via a Google gad g-et) . The phrase cloud representing Microsoft in this simile is shown in the screenshot of Figure 1, while the phrase cloud for Google is shown in Fi g ure 2. Figure 2 . A screenshot of a phrase cloud for the perspective cast upon the target term  X  Google  X  by the simile  X  Google is as  X  powerful as M i crosoft  X . ported, and the returned list often contains meaning-less results.

This demonstration introduces QuickView , which employs a series of NLP technologies to extract useful information from a large volume of tweets. Specifically, for each tweet, it first conducts nor-malization, followed by named entity recognition (NER). Then it conducts semantic role labeling (SRL) to get predicate-argument structures, which are further converted into events, i.e., triples of who did what. After that, it performs sentiment analysis (SA), i.e., extracting positive or negative comments about something/somebody. Next, tweets are clas-sified into predefined categories. Finally, non-noisy tweets together with the mined information are in-dexed.

On top of the index, QuickView enables two brand new scenarios, allowing users to effectively access the tweets or fine-grained information mined from tweets.
 Categorized Browsing . As illustrated in Figure 1(a), QuickView shows recent popular tweets, enti-ties, events, opinions and so on, which are organized by categories. It also extracts and classifies URL links in tweets and allows users to check out popular links in a categorized way.
 Advanced Search . As shown in Figure 1(b), Quick-View provides four advanced search functions: 1) search results are clustered so that tweets about the same/similar topic are grouped together, and for each cluster only the informative tweets are kept; 2) when the query refers to a person or a company, two bars are presented followed by the words that strongly suggest opinion polarity. The bar X  X  width 2. We present core components of QuickView , fo-
The rest of this paper is organized as follows. In the next section, we introduce related work. In Sec-tion 3, we describe our system. In Section 4, we evaluate our system. Finally, Section 5 concludes and presents future work. Information Extraction Systems . Essentially, QuickView is an information extraction (IE) system. However, unlike existing IE systems, such as Evita (Saur  X   X  et al., 2005), a robust event recognizer for QA system, and SRES (Rozenfeld and Feldman, 2008), a self-supervised relation extractor for the web, it targets tweets, a new genre of text, which are short and informal, and its focus is on adapting existing IE components to tweets.
 Tweet Search Services . A couple of tweet search services exist, including Twitter, Bing social search and Google social search 3 . Most of them provide only keyword-based search interfaces, i.e., return-ing a list of tweets related to a given word/phrase. In contrast, our system extracts fine-grained in-formation from tweets and allows a new end-to-end search experience beyond keyword search, such as clustering of search results, and search with events/opinions.
 NLP Components . The NLP technologies adopted in our system , e.g., NER, SRL and classification, have been extensively studied on formal text but rarely on tweets. At the heart of our system is the re-use of existing resources, methodologies as 3.1 Overview Architecture . QuickView can be divided into four parts, as illustrated in Figure 2. The first part in-cludes a crawler and a buffer of raw tweets. The crawler repeatedly downloads tweets using the Twit-ter APIs, and then pre-filters noisy tweets using some heuristic rules, e.g., removing a tweet if it is too short, say, less than 3 words, or if it contains any predefined banned word. At the moment, we focus on English tweets, so non-English tweets are filtered as well. Finally, the un-filtered are put into the buffer.

The second part consists of several tweet extrac-tion pipelines. Each pipeline has the same configura-tion, constantly fetching a tweet from the raw tweet buffer, and conducting the following processes se-Each processed tweet, if not identified as noise, is put into a shared buffer for indexing.

The third part is responsible for indexing and querying. It constantly takes from the indexing buffer a processed tweet, which is then indexed with various entries including words, phrases, metadata (e.g., source, publish time, and account), named en-tities, events, and opinions. On top of this, it answers any search request, and returns a list of matched re-sults, each of which contains both the original tweet and the extracted information from that tweet. We implement an indexing/querying engine similar to Lucene 6 in C#. This part also maintains a cache of recent processed tweets, from which the following information is extracted and indexed: 1) top tweets; 2) top entities/events/opinions in tweets; and 3) top accounts. Whether a tweet/entity/event/opinion ranks top depends on their re-tweeted/mentioned times as well as its publisher, while whether an ac-count is top relies on the number of his/her followers and tweets.

The fourth part is a web application that returns related information to end users according to their browsing or search request. The implementation of the web application is organized with the model-view-control pattern so that other kinds of user in-terfaces, e.g., a mobile application, can be easily im-plemented.
 Deployment . QuickView is deployed into 5 work-stations 7 including 2 processing pipelines, as illus-trated in Table 1. The communication between com-ponents is through TCP/IP. On average, it takes 0.01 seconds to process each tweet, and in total about 10 million tweets are indexed every day. Note that QuickView  X  X  processing capability can be enhanced in a straightforward manner by deploying additional pipelines. 3.2 Core Components Because of limited space, we only discuss two core components of QuickView : NER and SRL.
 NER . NER is the task of identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and loca-tions. Existing solutions fall into three categories: 1) based approach (M ` arquez et al., 2005), i.e., label-ing the words according to their positions relative to an argument (i.e., inside, outside, or at the be-ginning); and 3) Markov Logic Networks (MLN) based approach (Meza-Ruiz and Riedel, 2009), i.e., simultaneously resolving all the sub-tasks using learnt weighted formulas. Unsurprisingly, the per-formance of the state-of-the-art SRL system (Meza-Ruiz and Riedel, 2009) drops sharply when applied to tweets.
 The SRL component of QuickView is based on CRF, and uses the recently labeled tweets that are similar to the current tweet as the broader context. Algorithm 1 outlines its implementation, where: train denotes a machine learning process to get a labeler l , which in our work is a linear CRF model; the cluster function puts the new tweet into a clus-ter; the label function generates predicate-argument structures for the input tweet with the help of the trained model and the cluster; p , s and cf denote a predicate, a set of argument and role pairs related to the predicate and the predicted confidence, respec-tively. To prepare the initial clusters required by the SRL component as its input, we adopt the predicate-argument mapping method (Liu et al., 2010) to get some automatically labeled tweets, which (plus the manually labeled tweets) are then organized into groups using a bottom-up clustering procedure.
It is worth noting that: 1) our SRL component uses the general role schema defined by PropBank, which includes core roles such as A0, A1 (usually indicating the agent and patient of the predicate, re-spectively), and auxiliary roles such as AM-TMP and AM-LOC (representing the temporal and loca-tion information of the predicate, respectively); 2) only verbal predicates are considered, which is con-sistent with most existing SRL systems; and 3) fol-lowing M ` arquez et al. (2005), it conducts word level labeling. Overall Performance . We provide a textbox in the home page of QuickView to collect feedback. We have got 165 feedbacks, of which 85.5% are posi-tive. The main complaint is related to the quality of the extracted information.
 Core Components . We manually labeled the POS, a unified framework was adopted to decode with different models and ease the implementation of decoding algorithms. Moreover, some useful utilities were distributed with the toolkit, such as: a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training that allows for various evaluation metrics for tuning the system. In addition, the toolkit provides easy-to-use APIs for the development of new features. The toolkit has been used to build translation systems that have placed well at recent MT evaluations, such as the NTCIR-9 Chinese-to-English PatentMT task (Goto et al., 2011). 
We implemented the toolkit in C++ language, with special consideration of extensibility and efficiency. C++ enables us to develop efficient translation engines which have high running speed for both training and decoding stages. This property is especially important when the programs are used for large scale translation. While the development of C++ program is slower than that of the similar programs written in other popular languages such as Java, the modern compliers generally result in C++ programs being consistently faster than the Java-based counterparts. 
The toolkit is available under the GNU general public license 1 . The website of NiuTrans is http://www.nlplab.com/NiuPlan/NiuTrans.html . As in current approaches to statistical machine translation, NiuTrans is based on a log-linear sequence of commands. Given a number of sentence-pairs and the word alignments between them, the toolkit first extracts a phrase table and two reordering models for the phrase-based system, or a Synchronous Context-free/Tree-substitution Grammar (SCFG/STSG) for the hierarchical phrase-based and syntax-based systems. Then, an n -gram language model is built on the target-language corpus. Finally, the resulting models are incorporated into the decoder which can automatically tune feature weights on the development set using minimum error rate training (Och, 2003) and translate new sentences with the optimized weights. 
In the following, we will give a brief review of the above components and the main features provided by the toolkit. 3.1 Phrase Extraction and Reordering Model We use a standard way to implement the phrase extraction module for the phrase-based model. That is, we extract all phrase-pairs that are consistent with word alignments. Five features are associated with each phrase-pair. They are two phrase translation probabilities, two lexical weights, and a feature of phrase penalty. We follow the method proposed in (Koehn et al., 2003) to estimate the values of these features. 
Unlike previous systems that adopt only one reordering model, our toolkit supports two different reordering models which are trained independently but jointly used during decoding. z The first of these is a discriminative z The second model is the MSD reordering is repeated for several times until no better weights (i.e., weights with a higher BLEU score) are found. In this way, our program can introduce some randomness into weight training. Hence users do not need to repeat MERT for obtaining stable and optimized weights using different starting points. 3.5 Decoding Chart-parsing is employed to decode sentences in development and test sets. Given a source sentence, the decoder generates 1-best or k -best translations in a bottom-up fashion using a CKY-style parsing algorithm. The basic data structure used in the decoder is a chart , where an array of cells is organized in topological order. Each cell maintains a list of hypotheses (or items ). The decoding process starts with the minimal cells, and proceeds by repeatedly applying translation rules or composing items in adjunct cells to obtain new items. Once a new item is created, the associated scores are computed (with an integrated n -gram language model). Then, the item is added into the list of the corresponding cell. This procedure stops when we reach the final state (i.e., the cell associates with the entire source span). 
The decoder can work with all (hierarchical) phrase-based and syntax-based models. In particular, our toolkit provides the following decoding modes. z Phrase-based decoding . To fit the phrase-z Decoding as parsing (or string-based 4.1 Multithreading The decoder supports multithreading to make full advantage of the modern computers where more than one CPUs (or cores) are provided. In general, the decoding speed can be improved when multiple threads are involved. However, modern MT decoders do not run faster when too many threads are used (Cer et al., 2010). 4.2 Pruning To make decoding computational feasible, beam pruning is used to aggressively prune the search space. In our implementation, we maintain a beam for each cell. Once all the items of the cell are proved, only the top-k best items according to model score are kept and the rest are discarded. Also, we re-implemented the cube pruning method described in (Chiang, 2007) to further speed-up the system. 
In addition, we develop another method that prunes the search space using punctuations. The idea is to divide the input sentence into a sequence of segments according to punctuations. Then, each segment is translated individually. The MT outputs are finally generated by composing the translations of those segments. 4.3 APIs for Feature Engineering To ease the implementation and test of new features, the toolkit offers APIs for experimenting with the features developed by users. For example, users can develop new features that are associated with each phrase-pair. The system can automatically recognize them and incorporate them into decoding. Also, more complex features can be activated during decoding. When an item is created during decoding, new features can be introduced into an internal object which returns feature values for computing the model score. 5.1 Experimental Setup We evaluated our systems on NIST Chinese-English MT tasks. Our training corpus consists of 1.9M bilingual sentences. We used GIZA++ and the  X  X row-diag-final-and X  heuristics to generate word alignment for the bilingual data. The parse trees on both the Chinese and English sides were 
Moses: phrase 36.69 34.99 0.11 + cube pruning 36.51 34.93 0.47
NiuTrans: phrase 37.14 35.47 0.14 + cube pruning 36.98 35.39 0.60 + cube &amp; punct pruning 36.99 35.29 3.71 + all pruning &amp; 8 threads 36.99 35.29 21.89 + all pruning &amp; 16 threads 36.99 35.29 22.36 Table 2: Effects of pruning and multithreading techniques. promising results. For example, the string-to-tree system significantly outperforms the phrase-based and hierarchical phrase-based counterparts. In addition, Table 1 gives a test of different decoding methods (for syntax-based systems). We see that the parsing-based method achieves the best BLEU score. On the other hand, as expected, it runs slowest due to its large search space. For example, it is 5-8 times slower than the tree-parsing-based method in our experiments. The forest-based decoding further improves the BLEU scores on top of tree-parsing. In most cases, it obtains a +0.6 BLEU improvement but is 2-3 times slower than the tree-parsing-based method. 5.3 System Speed-up We also study the effectiveness of pruning and multithreading techniques. Table 2 shows that all the pruning methods implemented in the toolkit is helpful in speeding up the (phrase-based) system, while does not result in significant decrease in BLEU score. On top of a straightforward baseline (only beam pruning is used), cube pruning and pruning with punctuations give a speed improvement of 25 times together 7 . Moreover, the decoding process can be further accelerated by using multithreading technique. However, more than 8 threads do not help in our experiments. We have presented a new open-source toolkit for phrase-based and syntax-based machine translation. It is implemented in C++ and runs fast. Moreover, it supports several state-of-the-art models ranging from phrase-based models to syntax-based models, Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita and Benjamin K. Tsou. 2011. Overview of the Patent Machine Translation Task at the NTCIR-9 Workshop. 
In Proc. of NTCIR-9 Workshop Meeting, pages 559-578. Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of HLT/NAACL 2003, pages 127-133. Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proc. of ACL 2007, pages 177 X 180. Zhifei Li, Chris Callison-Bu rch, Chris Dyer, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Jonathan Weese, and Omar Zaidan. 2009. Joshua: An Open Source Toolkit for Parsing-Based Machine Translation. In Proc. of the Workshop on Statistical Machine Translation, pages 135 X 139. Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-String Alignment Template for Statistical Machine Translation. In Proc. of ACL 2006, pages 609-616. Haitao Mi, Liang Huang and Qun Liu. 2008. Forest-
Based Translation. In Proc. of ACL 2008, pages 192-199. Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL 2003, pages 160-167. Adam Pauls and Dan Klein. 2011. Faster and Smaller 
N-Gram Language Models. In Proc. of ACL 2011, pages 258 X 267. David Vilar, Daniel Stein, Matthias Huck and Hermann Ney. 2010. Jane: Open Source Hierarchical Translation, Extended with Reordering and Lexicon Models. In Proc. of the Joint 5th Workshop on 
Statistical Machine Translation and MetricsMATR, pages 262-270. Dekai Wu. 1996. A polynomial-time algorithm for statistical machine translation. In Proc. of ACL1996, pages 152 X 158. Deyi Xiong, Qun Liu and Shouxun Lin. 2006. 
Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation. In Proc. of ACL 2006, pages 521-528. Andreas Zollmann and Ashish Venugopal. 2006. Syntax Augmented Machine Transla tion via Chart Parsing. 
In Proc. of HLT/NAACL 2006, pages 138-141. 
Many applications could potentially benefit from automatic language identification, but building a customized solution per-application is prohibitively expensive, especially if human annotation is re-quired to produce a corpus of language-labelled training documents from the application domain. What is required is thus a generic language identi-fication tool that is usable off-the-shelf , i.e. with no end-user training and minimal configuration.

In this paper, we present langid.py , a LangID tool with the following characteristics: (1) fast, (2) usable off-the-shelf, (3) unaffected by domain-specific features (e.g. HTML, XML, markdown), (4) single file with minimal dependencies, and (5) flexible interface langid.py is trained over a naive Bayes clas-sifier with a multinomial event model (McCallum and Nigam, 1998), over a mixture of byte (1  X  n  X  4). One key difference from conventional text categorization solutions is that langid.py was designed to be used off-the-shelf . Since langid.py implements a supervised classifier, this presents two primary challenges: (1) a pre-trained model must be distributed with the classi-fier, and (2) the model must generalize to data from different domains , meaning that in its default con-figuration, it must have good accuracy over inputs as diverse as web pages, newspaper articles and mi-croblog messages. (1) is mostly a practical consid-eration, and so we will address it in Section 3. In order to address (2), we integrate information about the language identification task from a variety of do-mains by using LD feature selection (Lui and Bald-win, 2011).

Lui and Baldwin (2011) showed that it is rela-tively easy to attain high accuracy for language iden-sick, 1975). The Aho-Corasick string matching al-gorithm processes an input by means of a determin-istic finite automaton (DFA). Some states of the au-tomaton are associated with the completion of one of the tion. Thus, we can obtain our document represen-tation by simply counting the number of times the DFA enters particular states while processing our in-put. The DFA and the associated mapping from state to n and embedded as part of the pre-trained model.
The naive Bayes classifier is implemented using numpy , 1 the de-facto numerical computation pack-age for Python. numpy is free and open source, and available for all major platforms. Using numpy in-troduces a dependency on a library that is not in the Python standard library. This is a reasonable trade-off, as numpy provides us with an optimized im-plementation of matrix operations, which allows us to implement fast naive Bayes classification while maintaining the single-file concept of langid.py . langid.py can be used in the three ways: Command-line tool: langid.py supports an interactive mode with a text prompt and line-by-line classification. This mode is suitable for quick in-teractive queries, as well as for demonstration pur-poses. langid.py also supports language identi-fication of entire files via redirection. This allows a user to interactively explore data, as well as to inte-grate language identification into a pipeline of other unix -style tools. However, use via redirection is not recommended for large quantities of documents as each invocation requires the trained model to be unpacked into memory. Where large quantities of documents are being processed, use as a library or web service is preferred as the model will only be unpacked once upon initialization.
 Python library: langid.py can be imported as a Python module, and provides a function that ac-cepts text and returns the identified language of the text. This use of langid.py is the fastest in a single-processor setting as it incurs the least over-head.
 Web service: langid.py can be started as a web service with a command-line switch. This
We do not perform explicit encoding detection, but we do not assume that all the data is in the same encoding. Previous research has shown that explicit encoding detection is not needed for language iden-tification (Baldwin and Lui, 2010). Our training data consists mostly of UTF8-encoded documents, but some of our evaluation datasets contain a mixture of encodings. In order to benchmark langid.py , we carried out an empirical evaluation using a number of language-labelled datasets. We compare the empirical results obtained from langid.py to those obtained from other language identification toolkits which incor-porate a pre-trained model, and are thus usable off-the-shelf for language identification. These tools are listed in Table 3. 5.1 Off-the-shelf LangID tools TextCat is an implementation of the method of Cavnar and Trenkle (1994) by Gertjan van Noord. It has traditionally been the de facto LangID tool of choice in research, and is the basis of language iden-tification/filtering in the ClueWeb09 Dataset (Callan and Hoy, 2009) and CorpusBuilder (Ghani et al., 2004). It includes support for training with user-supplied data.

LangDetect implements a Naive Bayes classi-fier, using a character without feature selection, with a set of normaliza-tion heuristics to improve accuracy. It is trained on supplied data.

CLD is a port of the embedded language identi-fier in Google X  X  Chromium browser, maintained by Mike McCandless. Not much is known about the internal design of the tool, and there is no support provided for re-training it.

The datasets come from a variety of domains, such as newswire (TCL), biomedical corpora (EMEA), government documents (E URO GOV, E U -RO PARL) and microblog services (T-BE, T-SC). A number of these datasets have been previously used in language identification research. We provide a orders of magnitude faster than TextCat , but this advantage is reduced on larger documents. This is primarily due to the design of TextCat , which re-quires that the supplied models be read from file for each document classified. langid.py generally outperforms LangDetect , except in datasets derived from government documents (E URO GOV, E URO PARL). However, the difference in accuracy between langid.py and LangDetect on such datasets is very small, and langid.py is generally faster. An abnormal result was obtained when testing LangDetect on the EMEA corpus. Here, LangDetect is much faster, but has extremely poor accuracy (0.114). Analysis of the results re-veals that the majority of documents were classified as Polish. We suspect that this is due to the early termination criteria employed by LangDetect , together with specific characteristics of the corpus. TextCat also performed very poorly on this corpus (accuracy 0.362). However, it is important to note that langid.py and CLD both performed very well, providing evidence that it is possible to build a generic language identifier that is insensitive to domain-specific characteristics. langid.py also compares well with CLD . It is generally more accurate, although CLD does bet-ter on the EMEA corpus. This may reveal some insight into the design of CLD , which is likely to have been tuned for language identification of web with language identification priors based on a user X  X  previous messages and by the content of links em-bedded in messages. Tromp and Pechenizkiy (2011) present a method for language identification of short text messages by means of a graph structure.

Despite the recently published results on language identification of microblog messages, there is no dedicated off-the-shelf system to perform the task. We thus examine the accuracy and performance of using generic language identification tools to iden-tify the language of microblog messages. It is im-portant to note that none of the systems we test have been specifically tuned for the microblog domain. Furthermore, they do not make use of any non-textual information such as author and link-based priors (Carter et al., to appear).

We make use of two datasets of Twitter messages kindly provided to us by other researchers. The first is T-BE (Tromp and Pechenizkiy, 2011), which con-tains 9659 messages in 6 European languages. The second is T-SC (Carter et al., to appear), which con-tains 5000 messages in 5 European languages.

We find that over both datasets, langid.py has better accuracy than any of the other systems tested. On T-BE, Tromp and Pechenizkiy (2011) report accuracy between 0.92 and 0.98 depending on the parametrization of their system, which was tuned specifically for classifying short text messages. In its off-the-shelf configuration, langid.py attains an accuracy of 0.94, making it competitive with the customized solution of Tromp and Pechenizkiy (2011).

On T-SC, Carter et al. (to appear) report over-all accuracy of 0.90 for TextCat in the off-the-shelf configuration, and up to 0.92 after the inclusion of priors based on (domain-specific) extra-textual information. In our experiments, the accuracy of TextCat is much lower (0.654). This is because Carter et al. (to appear) constrained TextCat to output only the set of 5 languages they considered. Our results show that it is possible for a generic lan-guage identification tool to attain reasonably high accuracy (0.89) without artificially constraining the set of languages to be considered, which corre-sponds more closely to the demands of automatic language identification to real-world data sources, where there is generally no prior knowledge of the languages present. content such as tweets and short messages before further processing. The approaches include supervised or unsupervised methods based on morphological and phonetic variations. However, most of the multilingual chat systems on the Internet have not yet integrated this feature into their systems but requesting users to type in proper language so as to have good translation. This is because the current techniques are not robust enough to model the different characteristics featured in the social media content. Most of the techniques are developed based on observations and assumptions made on certain datasets. It is also difficult to unify the language uniqueness among different users into a single model. exploiting a personalized dictionary for each user, to support the use of user-defined short-forms in a multilingual chat system -AsiaSpik . The use of this personalized dictionary reduces the reliance on the availability and dependency of training data and empowers the users with the flexibility and interactivity to include and manage their own vocabularies during chat. 2 ASIASPIK System Overview AsiaSpik is a web-based multilingual instant messaging system that enables online chats written in one language to be readable in other languages by other users. Figure 1 describes the system process. It describes the process flow between Chat Client , Chat Server , Translation Bot and Normalization Bot whenever Chat Client starts chat module. Client checks if the normalization option for that language used by the user is active and activated. If 3 Personalized Normalization Personalized Normalization is the main distinction of AsiaSpik among other multilingual chat system. It gives the flexibility for user to personalize his/her short-forms for messages in English. 3.1 Related Work The traditional text normalization strategy follows the noisy channel model (Shannon, 1948). Suppose the chat message is C and its corresponding standard form is S , the approach aims to find language model and ) | ( S C P is an error model. The objective of using model in the chat message normalization context is to develop an appropriate error model for converting the non-standard and unconventional words found in chat messages into standard words. normalization as translation from the texting language into the standard language. Choudhury et al. (2007) model the word-level text generation process for SMS messages, by considering unintentional typos as hidden Markov model (HMM) state transitions and emissions, respectively. Cook and Stevenson (2009) expand the error model by introducing inference from different erroneous formation processes, according to the sample error distribution. Han and Baldwin (2011) use a classifier to detect ill-formed words, and generate correction candidates based on morphophonemic similarity. These models are effective on their experiments conducted, however, much works remain to be done to handle the diversity and dynamic of content and fast evolution of words used in social media and networking. are made mostly unintentionally by the writers, abbreviations or slangs found in chat messages are introduced intentionally by the senders most of the time. This leads us to suggest that if facilities are given to users to define their abbreviations, the dynamic of the social content and the fast of user-defined abbreviations and short-forms, we define a personalized model ) | ( each user based on his/her dictionary profile. Each personalized model is loaded into the memory once the user activates the normalization option. Whenever there is a change in the entry, the entry X  X  probability will be re-distributed and updated based on the following model. This characterizes the AsiaSpik system which supports personalized and dynamic chat normalization. are optimized by minimum error rate training (Och, 2003), which searches for weights maximizing the normalization accuracy using a small development set. We use standard state-of-the-art open source tools, Moses (Koehn, 2007), to develop the system and the SRI language modeling toolkit (Stolcke,2003) to train a trigram language model on the English portion of the Europarl Corpus (Koehn, 2005). 3.3 Experiments We conducted a small experiment using 134 chat messages sent by high school students. Out of these messages, 73 short-forms are uncommon and not found in our default dictionary. Most of these defined by two users. Note that expansions for  X  X tg X  and  X  X gt X  are defined differently and expanded differently for the two users.  X  X e X  in the message box indicates the message typed by the user while  X  X xpansion X  is the message expanded by the system. 
The normalization approach is a simple probabilistic model making use of the normalization probability defined for each short-form and the language model probability. The model can be further improved by fine-tuning the normalization probability and incorporate other feature functions. The baseline model can also be further improved with more sophisticated method without changing the architecture of the full system. 
AsiaSpik is a demonstration system. We would like to expand the normalization model to include more features and support other languages such as Malay and Chinese. We would also like to further enhance the system to convert the translated English chat messages back to the social media language as defined by the user. References are not intended to help the user completing any specific task, but to provide a means for participa-ting in a game, or just for chitchat or entertain-ment. Typical examples of chat-oriented dialogue systems are the so called chat bots (Weizenbaum, 1966; Ogura et al. , 2003, Wallis, 2010). 
In this paper, we introduce IRIS (Informal Res-ponse Interactive System), a chat-oriented dialogue system that is based on the vector space model framework (Salton et al. , 1975; van Rijsbergen, 2005). From the operational point of view, IRIS belongs to the category of example-based dialogue systems (Murao et al. , 2003). Its dialogue strategy is supported by a large database of dialogues that is used to provide candidate responses to a given user input. The search for candidate responses is per-formed by computing the cosine similarity metric into the vector space model representation, in which each utterance in the dialogue database is represented by a vector. 
Different from example-based question answer-ing systems (Vicedo, 2002; Xue et al. , 2008), IRIS uses a dual search strategy. In addition to the cur-rent user input, which is compared with all existent utterances in the database, a vector representation of the current dialogue history is also compared with vector representations of full dialogues in the database. Such a dual search strategy allows for in-corporating information about the dialogue context into the response selection process. The rest of the paper is structured as follows. Section 2 presents the architecture of IRIS as well as provides a general description of the dataset that has been used for its implementation. Section 3 presents some illustrative examples of dialogues generated by IRIS, and Section 4 presents the main conclusions of this work. which main objectives are: first, to greet the user and self-introduce IRIS and, second, to collect the name of the user. This module uses a basic parsing algorithm that is responsible for extracting the user X  X  name from the provided input. The name is the first vocabulary term learned by IRIS, which is stored in the vocabulary learning repository. 
Once the dialogue initiation has been concluded the dialogue management system gains back the control of the dialogue and initializes the current history vector. Two types of vector initializations are possible here. If the user is already know by IRIS, it will load the last stored dialogue history for that user; otherwise, IRIS will randomly select one dialogue history vector from the dialogue data-base. After this initialization, IRIS prompts the user for what he desires to do. From this moment, the example-based chat strategy starts. 
For each new input from the user, the dialogue management module makes a series of actions that, after a decision process, can lead to different types of responses. In the first action, the dynamic repla-cement module searches for possible matches bet-ween the terms within the vocabulary learning repository and the input string. In a new dialogue, the only two terms know by IRIS are its own name and the user name. If any of this two terms are identified, they are automatically replaced by the placeholders &lt;self-name&gt; and &lt;other-name&gt; , res-pectively. 
In the case of a mature dialogue, when there are more terms into the vocabulary learning repository, every term matched in the input is replaced by its corresponding definition stored in the vocabulary learning database. 
Just after the dynamic replacement is conducted, tokenization and vectorization of the user input is carried out. During tokenization, an additional checking is conducted by the dialogue manager. It looks for any adaptation command that could be possibly inserted at the beginning of the user input. More details on adaptation commands will be given when describing the style/manner adaptation module. Immediately after tokenization, unknown vocabulary terms (OOVs) are identified. IRIS will consider as OOV any term that is not contained in either the dialogue or vocabulary learning data-bases. In case an OOV is identified, a set of heuris-tics (aiming at avoiding confusing misspellings with OOVs) are applied to decide whether IRIS should ask the user for the meaning of such a term. case they occur in the response, by their actual values. 
The final action taken by IRIS is related to the style/manner adaptation module. For this action to take place the user has to include one of three pos-sible adaptations commands at the beginning of her/his new turn. The three adaptation commands recognized by IRIS are: ban (*), reinforce (+), and discourage ( X ). By using any of these three charac-ters as the first character in the new turn, the user is requesting IRIS to modify the vector space repre-sentation of the previous selected response as follows:  X  Ban (*): IRIS will mark its last response as a  X  Reinforce (+): IRIS will pull the vector space  X  Discourage ( X ): IRIS will push the vector 2.2 Dialogue Data Collection For the current implementation of IRIS, a subset of the Movie-DiC dialogue data collection has been used (Banchs, 2012). Movie-DiC is a dialogue corpus that has been extracted from movie scripts which are freely available at The Internet Movie Script Data Collection ( http://www.imsdb.com/ ). In this subsection, we present a brief description on the specific data subset used for the implementa-tion of IRIS, as well as we briefly review the process followed for collecting the data and ex-tracting the dialogues. 
First of all, dialogues have to be identified and parsed from the collected html files. Three basic elements are extracted from the scripts: speakers, utterances and context. The speaker and utterance elements contain information about the characters who speak and what they said at each dialogue turn. On the other hand, context elements contain all the additional information (explanations and descriptions) appearing in the scripts. 3 Some Dialogue Examples In this section we show some real examples of interactions between IRIS and human users. First, we present some interesting examples of good per-formance, as well as illustrate some of the learning capabilities of IRIS. Then, we present some of the common failures which identify specific points of attention for further improvements. 3.1 Good Performance Examples Our first example illustrates the beginning of a typical chat session between IRIS and a new user. This example is depicted in Table 2. Table 2: Beginning of a chat session between IRIS 
For the dialogue depicted in Table 2, turn num-bers 1, 2 and 3 are processed by the dialogue intia-tion/ending module. The example-based dialogue management strategy starts from turn 4 onwards. Notice that as far as this is a new user, not previous dialogue history exists, so in this case a random history vector has been selected and instead of focusing in the sports topic suggested by the user, IRIS  X  X akes the initiative X  of asking for a date. In our second example, which is presented in Table 3, we illustrate the beginning of a typical chat session between IRIS and a returning user. For this particular user, her last interaction with IRIS was about sports. 
Similar to the previous example, turn 1 is pro-cessed by the dialogue intiation/ending module and the example-based dialogue management strategy starts from turn 2 onwards. In this particular case, IRIS is much more centered on the sports topic as this context information has been already provided rice with some seafood on it today ? , which is the utterance used by IRIS to retrieve and select the response it provides in turn 12. 3.2 Common Failure Examples In this subsection we focus our attention in the most common failures exhibited by IRIS. Some of these failures put in evidence specific points of attention that should be taken into account for further improvements of the system. 
Our first example illustrates the problem of IRIS lack of consistency in issues for which consistent answers are required. Two specific chat segments in which IRIS provides inconsistent responses are presented in Table 5. 
The first example presented in Table 5 constitu-tes a serious consistency problem. In this case IRIS has reported two different ages in the same chat session. The second case, although not so serious as the previous one, also constitutes a consistency failure. In this case IRIS states Football is my life just two turns after saying I hate sports . 
Our second example, which is presented in Ta-ble 6, illustrates a problem derived from the noise that is still present in the dataset. Table 6: Example of noise in the dialogue dataset 
In the particular example illustrated in Table 6, as seen from turn 3, a context element has been the Universities of Edinburgh, Zagreb, Copenhagen, and Uppsala. 2 LetsMT! Key Features The LetsMT! platform 1 (Vasi  X  jevs et al., 2011) gathers public and user-provided MT training data and enables generation of multiple MT systems by combining and prioritising this data. Users can upload their parallel corpora to an online repository and generate user-tailored SMT systems based on data selected by the user. 
Authenticated users with appropriate permissions can also store private corpora that can be seen and used only by this user (or a designated user group). All data uploaded into the LetsMT! repository is kept in internal format, and only its metadata is provided to the user. Data cannot be downloaded or accessed for reading by any means. The uploaded data can only be used for SMT training. In such a way, we encourage institutions and individuals to contribute their data to be publicly used for SMT training, even if they are not willing to share the content of the data. 
A user creates SMT system definition by specifying a few basic parameters like system name, source/target languages, domain, and choosing corpora (parallel for translation models or monolingual for language models) to use for the particular system. Tuning and evaluation data can be automatically extracted from the training corpora or specified by the user. The access level of the system can also be specified -whether it will be public or accessible only to the particular user or user group. 3 SMT Training and Decoding Facilities The SMT training and decoding facilities of LetsMT! are based on the open source toolkit Moses. One of the important achievements of the project is the adaptation of the Moses toolkit to fit into the rapid training, updating, and interactive access environment of the LetsMT! platform. 
The Moses SMT toolkit (Koehn et al., 2007) provides a complete statistical translation system distributed under the LGPL license. Moses includes all of the components needed to pre-process data and to train language and translation models. Moses is widely used in the research community and has also reached the commercial sector. While the use of the software is not closely monitored, Moses is known to be in commercial use by companies such as Systran (Dugast et al., 2009), Asia Online, Autodesk (Plitt and Masselot, 2010), Matrixware 2 , Adobe, Pangeanic, Logrus 3 , and Applied Language Solutions (Way et al., 2011). The SMT training pipeline implemented in Moses involves a number of steps that each require a separate program to run. In the framework of 
The application logic layer contains a set of modules responsible for the main functionality and logic of the system. It receives queries and commands from the interface layer and prepares answers or performs tasks using data storage and the HPC cluster. This layer contains several modules such as the Resource Repository Adapter, the User Manager, the SMT Training Manager, etc. The interface layer accesses the application logic layer through the REST/JSON and SOAP protocol web services. The same protocols are used for communication between modules in the application logic layer. The data is stored in one central Resource Repository (RR). As training data may change (for example, grow), the RR is based on a version-controlled file system (currently we use SVN as the backend system). A key-value store is used to keep metadata and statistics about training data and trained SMT systems. Modules from the application logic layer and HPC cluster access RR through a REST-based web service interface. 
A High Performance Computing Cluster is used to execute many different computationally heavy data processing tasks  X  SMT training and running, corpora processing and converting, etc. Modules from the application logic and data storage layers The general architecture of the Resource implemented in terms of a modular package that can easily be installed in a distributed environment. RR services are provided via Web API X  X  and secure HTTP requests. Data storage can be distributed over several servers as is illustrated in Figure 3. Storage servers communicate with the central database server that manages all metadata records attached to resources in the RR. Data resources are organised in slots that correspond to file systems with user-specific branches. Currently, the RR package implements two storage backends: a plain file system and a version-controlled file system based on subversion (SVN). The latter is the default mode, which has several advantages over non-revisioned data storage. Revision control systems are designed to handle dynamically growing collections of mainly textual data in a multi-user environment. Furthermore, they keep track of modifications and file histories to make it possible to backtrack to prior revisions. This can be a strong advantage, especially in cases of shared data access. Another interesting feature is the possibility to create cheap copies of entire branches that can be used to enable data compromising data integrity for others. Finally, SVN also naturally stores data in a compressed format, which is useful for large-scale document collections. In general, the RR implementation is modular, other storage backends may be added later, and each individual slot can use its own backend type. 
Another important feature of the RR is the support of a flexible database for metadata. We decided to integrate a modern key-value store into the platform in order to allow a maximum of flexibility. In contrast to traditional relational databases, key-value stores allow the storage of arbitrary data sets based on pairs of keys and values without being restricted to a pre-defined schema or a fixed data model. Our implementation relies on TokyoCabinet 4 , a modern implementation of schema-less databases that supports all of our 
Furthermore, our system also includes tools for automatic sentence alignment. Import processes automatically align translated documents with each other using standard length-based sentence alignment methods (Gale and Church, 1993; Varga et al., 2005). 
Finally, we also integrated a general batch-queuing system (SGE) to run off-line processes such as import jobs. In this way, we further increase the scalability of the system by taking the load off repository servers. Data uploads automatically trigger appropriate import jobs that will be queued on the grid engine using a dedicated job web-service API. 6 Evaluation for Usage in Localisation One of the usage scenarios particularly targeted by the project is application in the localisation and translation industry. Localisation companies usually have collected significant amounts of parallel data in the form of translation memories. They are interested in using this data to create customised MT engines that can increase productivity of translators. Productivity is usually measured as an average number of words translated per hour. For this use case, LetsMT! has developed plug-ins for integration into CAT tools. In addition to translation candidates from translation memories, translators receive translation suggestions provided by the selected MT engine running on LetsMT!. As part of the system evaluation, project partner Moravia used the LetsMT! platform to train and 
This demonstration system brings together exist-ing online data resources and software toolkits to create a low-cost framework for evaluation of pedes-trian route instruction systems. We have built a web-based environment containing a simulated real world in which users can simulate walking on the streets of real cities whilst interacting with differ-ent navigation systems. This evaluation framework will be used in the near future to evaluate a series of instruction-giving dialogue systems. The GIVE challenge developed a 3D virtual in-door environment for development and evaluation of indoor pedestrian navigation instruction systems (Koller et al., 2007; Byron et al., 2007). In this framework, users can walk through a building with rooms and corridors, similar to a first-person shooter game. The user is instructed by a navigation sys-tem that generates route instructions. The basic idea was to have several such navigation systems hosted on the GIVE server and evaluate them in the same game worlds, with a number of users over the in-ternet. Conceptually our work is very similar to the GIVE framework, but its objective is to evaluate sys-tems that instruct pedestrian users in the real world. The GIVE framework has been successfully used for comparative evaluation of several systems generat-ing instructions in virtual indoor environments.
Another system,  X  X irtual Navigator X , is a simu-lated 3D environment that simulates the real world for training blind and visually impaired people to learn often-used routes and develop basic naviga-tion skills (McGookin et al., 2010). The framework game which consists of users solving several pieces of a puzzle to discover the location of the treasure chest. In order to solve the puzzle, they interact with game characters (e.g. a pirate) to obtain clues as to where the next clue is. This sets the user a number of navigation tasks to acquire the next clues until they find the treasure. In order to keep the game interest-ing, the user X  X  energy depletes as time goes on and they therefore have limited time to find the treasure. Finally, the user X  X  performance is scored to encour-age users to return. The game characters and enti-ties like keys, chests, etc. are laid out on real streets making it easy to develop a game without develop-ing a game-world. New game-worlds can be easily scripted using Javascript, where the location (lati-tude and longitude) and behaviour of the game char-acters are defined. The game-world module serves game-world specifications to the web-based client. 3.2 Broker The broker module is a web server that connects the web clients to their corresponding different naviga-tion systems. This module ensures that the frame-work works for multiple users. Navigation systems are instantiated and assigned to new users when they first connect to the broker. Subsequent messages from the users will be routed to the assigned navi-gation system. The broker communicates with the navigation systems via a communication platform thereby ensuring that different navigation systems developed using different languages (such as C++, Java, Python, etc) are supported. 3.3 Navigation system The navigation system is the central component of this architecture, which provides the user instruc-tions to reach their destinations. Each navigation system is run as a server remotely. When a user X  X  client connects to the server, it instantiates a navi-gation system object and assigns it to the user ex-clusively. Every user is identified using a unique id (UUID), which is used to map the user to his/her re-spective navigation system. The navigation system is introduced in the game scenario as a buddy sys-tem that will help the user in his objective: find the treasure. The web client sends the user X  X  location to the system periodically (every few seconds). subroutines to access the required information such as the nearest amenity, distance or route from A to B, etc. These subroutines provide the interface between the navigation systems and the database. 3.6 Web-based client The web-based client is a JavaScript/HTML pro-gram running on the user X  X  web browser software (e.g. Google Chrome). A snapshot of the webclient is shown in figure 2. It has two parts: the streetview panel and the interaction panel.
 Streetview panel: the streetview panel presents a simulated real world visually to the user. When the page loads, a Google Streetview client (Google Maps API) is created with an initial user coordinate. Google Streetview is a web service that renders a panoramic view of real streets in major cities around the world. This client allows the web user to get a panoramic view of the streets around the user X  X  vir-tual location. A gameworld received from the server is overlaid on the simulated real world. The user can walk around and interact with game characters using the arrow keys on his keyboard or the mouse. As the user walks around, his location (stored in the form of latitude and longitude coordinates) gets updated locally. Streetview also returns the user X  X  point of view (0-360 degrees), which is also stored locally. Interaction panel: the web-client also includes an  X   X  X n-situ X  or incremental route instruction sys- X  Interactive navigation systems: these systems Navigation systems can be evaluated using two kinds of metrics using this framework. Objective metrics such as time taken by the user to finish each navigation task and the game, distance trav-elled , number of wrong turns , etc. can be directly measured from the environment. Subjective met-rics based on each user X  X  ratings of different features of the system can be obtained through user satisfac-tion questionnaires. In our framework, users are re-quested to fill in a questionnaire at the end of the game. The questionnaire consists of questions about the game, the buddy, and the user himself, for exam-ple:  X  Was the game engaging?  X  Would you play it again (i.e. another similar  X  Did your buddy help you enough?  X  Noise in user speech: for systems that take  X  Adaptation to users: returning users may have
Errors in GPS positioning of the user and noise in user speech can be simulated at the server end, thereby creating a range of challenging scenarios to evaluate the robustness of the systems. We plan to organise a shared challenge for outdoor pedestrian route instruction generation, in which a variety of systems can be evaluated. Participating research teams will be able to use our interfaces and modules to develop navigation systems. Each team will be provided with a development toolkit lexicographers, human translators and second language learners (Bowker and Barlow 2004; Bourdaillet et al., 2010; Gao 2011). 
Identifying the translation equivalents, translation spotting , is the most challenging part of a bilingual concordancer. Recently, most of the existing bilingual concordancers spot translation equivalents in terms of word alignment-based method. (Jian et al., 2004; Callison-Burch et al., 2005; Bourdaillet et al., 2010). However, word alignment-based translation spotting has some drawbacks. First, aligning a rare (low frequency) term may encounter the garbage collection effect (Moore, 2004; Liang et al., 2006) that cause the term to align to many unrelated words. Second, the statistical word alignment model is not good at many-to-many alignment due to the fact that translation equivalents are not always correlated in lexical level. Unfortunately, the above effects will be intensified in a domain-specific concordancer because the queries are usually domain-specific terms, which are mostly multi-word low-frequency terms and semantically non-compositional terms. 
Wu et al. (2003) employed a statistical association criterion to spot translation equivalents in their bilingual concordancer. The association-based criterion can avoid the above mentioned effects. However, it has other drawbacks in translation spotting task. First, it will encounter the contextual effect that causes the system incorrectly spot the translations of the strongly collocated context. Second, the association-based translation spotting tends to spot the common subsequence of a set of similar translations instead of the full translations. Figure 1 illustrates an example of contextual effect , in which  X  X an K'uan X  is incorrectly spotted as part of the translation of the query term  X   X  X  X  X  X  X  X   X  (Travelers Among Mountains and Streams), which is the name of the 
The remainder of this paper is organized as follows. Section 2 describes the DOMCAT system. In Section 3, we describe the evaluation of the DOMCAT system. Section 4 contains some concluding remarks. 2 The DOMCAT System Given a query, the DOMCAT bilingual concordancer retrieves sentence pairs and spots translation equivalents by the following steps: 1. Retrieve the sentence pairs whose source 2. Extract translation candidate words from the 3. Spot the candidate words for each target In step 1, the query term can be a single word, a phrase, a gapped sequence and even a regular expression. The parallel corpus is indexed by the suffix array to efficiently retrieve the sentences. 
The step 2 and step 3 are more complicated and will be described from Section 2.1 to Section 2.3. 2.1 Extract Translation Candidate Words After the queried sentence pairs retrieved from the parallel corpus, we can extract translation candidate words from the sentence pairs. We compute the local normalized correlation with respect to the query term for each word e in each target sentence. The local normalized correlation is defined as follows: where q denotes the query term, f denotes the source sentence and e denotes the target sentence,  X  is a small smoothing factor. The probability p ( e | f ) is the word translation probability derived from the entire parallel corpus by IBM Model 1 (Brown et al., 1993). The sense of local normalized probability of word e being part of translation of the query term q under the condition of sentence pair ( e, f ). from unrelated words much better than the Dice coefficient. 
The rationale behind the normalized correlation is that the nc value is the strength of word e generated by the query compared to that of generated by the whole sentence. As a result, the normalized correlation can easily separate the words generated by the query term from the words generated by the context. On the contrary, the Dice coefficient counts the frequency of a co-occurred word without considering the fact that it could be generated by the strongly collocated context. 2.2 Translation Spotting Once we have a translation candidate list and respective nc values, we can spot the translation equivalents by the following spotting algorithm. For each target sentence, first, spot the word with highest nc value. Then extend the spotted sequence to the neighbors of the word by checking their nc values of neighbor words but skipping function words. If the nc value is greater than a threshold  X  , add the word into spotted sequence. Repeat the extending process until no word can be added to the spotted sequence. 
The following is the pseudo-code for the algorithm: 
Then, we modify the Dice coefficient by replacing the co-occurrence frequency with normalized frequency as follows: exploited as our criterion for assessing the association strength between the query and the spotted sequences. 3 Experimental Results 3.1 Experimental Setting We use the Chinese/English web pages of the National Palace Museum 2 as our underlying parallel corpus. It contains about 30,000 sentences in each language. We exploited the Champollion Toolkit (Ma et al., 2006) to align the sentence pairs. The English sentences are tokenized and lemmatized by using the NLTK (Bird and Loper, 2004) and the Chinese sentences are segmented by the CKIP Chinese segmenter (Ma and Chen, 2003). 
To evaluate the performance of the translation spotting, we selected 12 domain-specific terms to query the concordancer. Then, the returned spotted translation equivalents are evaluated against a manually annotated gold standard in terms of recall and precision metrics. We also build two different translation spotting modules by using the GIZA++ toolkit (Och and Ney, 2000) with the intersection/union of the bidirectional word alignment as baseline systems. 
To evaluate the performance of the ranking criterion, we compiled a reference translation set for each query by collecting the manually annotated translation spotting set and selecting 1 to 3 frequently used translations. Then, the outputs of each query are ranked by the nf_dice function and evaluated against the reference translation set. We also compared the ranking performance with the Dice coefficient. 3.2 Evaluation of Translation Spotting We evaluate the translation spotting in terms of the Recall and Precision metrics defined as follows: the normalized correlation increased the recall rate without losing the precision rate. This may indicate that the normalized correlation can effectively conquer the drawbacks of the word alignment-based translation spotting and the association-based translation spotting mentioned in Section 1. Table 1. Evaluation of the translation spotting queried by 12 domain-specific terms. 
We also evaluate the queried results of each term individually (as shown in Table 2). As it shows, the normalized correlation is quite stable for translation spotting. The meaning of the coverage rate can be interpreted as: how many percent of the query can find an acceptable translation in the top-n results. We use the reference translations, as described in Section 3.1, as acceptable translation set for each query of our experiment. Table 3 shows the coverage rate of the nf_dice function compared with the Dice coefficient. As it shows, in the outputs ranked by the Dice coefficient, uses usually have to look up more than 3 sentences to find an acceptable translation; while in the outputs ranked by the nf_dice function, users can find an acceptable translation in top-2 sentences. A weighted finite-state transducer consists of a set of states and transitions between states. There is an initial state and a subset of states are final. Each tran-sition is labeled with an input symbol from an input alphabet; an output symbol from an output alpha-bet; an origin state; a destination state; and a weight. Each final state has an associated final weight. A path in the WFST is a sequence of transitions where each transition X  X  destination state is the next transi-tion X  X  origin state. A valid path through the WFST is a path where the origin state of the first transition is an initial state, and the the last transition is to a final state. Weights combine along the path according to the semiring of the WFST.

If every transition in the transducer has the same input and output symbol, then the WFST represents a weighted finite-state automaton. In the OpenFst library, there are a small number of special sym-bols that can be used. The symbol represents the empty string, which allows the transition to be tra-versed without consuming any symbol. The  X  (or failure) symbol on a transition also allows it to be traversed without consuming any symbol, but it dif-fers from in only allowing traversal if the symbol being matched does not label any other transition leaving the same state, i.e., it encodes the semantics of otherwise , which is useful for language models. For a more detailed presentation of WFSTs, see Al-lauzen et al. (2007). The Thrax grammar compiler 3 compiles grammars that consist of regular expressions, and context-dependent rewrite rules, into FST archives (fars) of weighted finite state transducers. Grammars may 3.2 Detailed Description A Thrax grammar consists of a set of one or more source files, each of which must have the extension .grm . The compiler compiles each source file to a single FST archive with the extension .far . Each grammar file has sections: Imports and Body, each of which is optional. The body section can include statements interleaved with functions, as specified below. Comments begin with a single pound sign (#) and last until the next newline. 3.2.1 Imports
The Thrax compiler compiles source files (with the extension .grm ) into FST archive files (with the extension .far ). FST archives are an Open-Fst storage format for a series of one or more FSTs. The FST archive and the original source file then form a pair which can be imported into other source files, allowing a Python-esque include system that is hopefully familiar to many. Instead of working with a monolithic file, Thrax allows for a modular con-struction of the final rule set as well as sharing of common elements across projects. 3.2.2 Functions
Thrax has extensive support for functions that can greatly augment the capabilities of the language. Functions in Thrax can be specified in two ways. The first is inline via the func keyword within grm files. These functions can take any number of input arguments and must return a single result (usually an FST) to the caller via the return keyword: func DumbPluralize[fst] { } Alternatively, functions can be written C++ and added to the language. Regardless of the func-tion implementation method (inline in Thrax or subclassed in C++), functions are integrated into the Thrax environment and can be called directly by using the function name and providing the necessary arguments. Thus, assuming someone has written a function called NetworkPluralize that retrieves the plural of a word from some web-site, one could write a grammar fragment as follows:
One can also create temporary symbols on the fly by enclosing a symbol name inside brackets within an FST string. All of the text inside the brackets will be taken to be part of the symbol name, and future encounters of the same symbol name will map to the same label. By default, la-bels use  X  X rivate Use Area B X  of the unicode ta-ble (0x100000 -0x10FFFD), except that the last two code points 0x10FFFC and 0x10FFFD are reserved for the  X  X BOS] X  and  X  X EOS] X  tags discussed below. 3.3 Standard Library Functions and Built-in functions are provided that operate on FSTs and perform most of the operations that are available in the OpenFst library. These include: closure, con-catenation, difference, composition and union. In most cases the notation of these functions follows standard conventions. Thus, for example, for clo-sure, the following syntax applies: fst* (accepts fst 0 or more times); fst+ (accepts fst 1 or more times); fst? (accepts fst 0 or 1 times) fst { x,y } (accepts fst at least x but no more than y times).

The operator  X  X  X  is used for composition: a @ b denotes a composed with b . A  X : X  is used to de-note rewrite , where a : b denotes a transducer that deletes a and inserts b . Most functions can also be expressed using functional notation:
The delimiters &lt; and &gt; add a weight to an expres-sion in the chosen semiring: a &lt; 3 &gt; adds the weight 3 (in the tropical semiring by default) to a .
Functions lacking operators (hence only called by function name) include: ArcSort , Connect , Determinize , RmEpsilon , Minimize , Optimize , Invert , Project and Reverse .
 Most of these call the obvious underlying OpenFst function.

One function in particular, CDRewrite is worth further discussion. This function takes a transducer and two context acceptors (and the alphabet ma-chine), and generates a new FST that performs a context dependent rewrite everywhere in the pro-vided contexts. The context-dependent rewrite algo-rithm used is that of Mohri and Sproat (1996), and would parse a sequence of tab-separated pairs, using utf8 parsing for the left-hand string, and the symbol table my symtab for the right-hand string. The OpenGrm NGram library contains tools for building, manipulating and using n -gram language models represented as weighted finite-state trans-ducers. The same finite-state topology is used to en-code raw counts as well as smoothed models. Here we briefly present this structure, followed by details on the operations manipulating it.
 An n -gram is a sequence of n symbols: w 1 ...w n . Each state in the model represents a prefix history of the n -gram ( w 1 ...w n  X  1 ), and transitions in the model represent either n -grams or backoff transi-tions following that history. Figure 1 lists conven-tions for states and transitions used to encode the n -grams as a WFST.
 This representation is similar to that used in other WFST-based n -gram software libraries, such as the AT&amp;T GRM library (Allauzen et al., 2005). One key difference is the implicit representation of &lt; s &gt; and &lt; /s &gt; , as opposed to encoding them as symbols in the grammar. This has the benefit of including all start and stop symbol functionality while avoiding common pitfalls that arise with explicit symbols.
Another difference from the GRM library repre-sentation is explicit inclusion of failure links from states to their backoff states even in the raw count files. The OpenGrm n -gram FST format is consis-tent through all stages of building the models, mean-ing that model manipulation (e.g., merging of two tory  X  X  X ; and the bottom state represents the history  X  X  X . Since this is a bigram model, histories consist of at most one prior symbol from the vocabulary. Dou-ble circles represent final states, which come with a final weight encoding the negative log count of end-ing the string at that state. Only the  X  X  X  history state and the unigram state are final states, since our ex-ample string ends with the symbol  X  X  X . (The unigram state is always final.) The transitions are backoff transitions, and the weights on each n -gram transi-tion are negative log counts of that symbol occurring following the history that the state represents. Hence the bigram  X  b b  X  occurs once, yielding a negative log of zero for the transition labeled with  X  X  X  leaving the state representing the history  X  X  X . 4.2 N-gram Model Parameter Estimation Given counts, one can build a smoothed n -gram model by normalizing and smoothing, which is ac-complished with the ngrammake command line binary. The library has several available smooth-ing methods, including Katz (1987), absolute dis-counting (Ney et al., 1994), Kneser-Ney (1995) and (the default) Witten-Bell (1991). See Chen and Goodman (1998) for a detailed presentation of these smoothing methods. Each of these smoothing meth-ods is implemented as a relatively simple derived subclass, thus allowing for straightforward exten-sion to new and different smoothing methods. To make a smoothed n -gram model from counts:
Figure 2(b) shows the model built using the de-fault Witten-Bell smoothing from the count FST in 2(a). The topology remains identical, but now the n -gram transition weights and final weights are neg-ative log probabilities. The backoff transitions (la-beled with ) have the negative log backoff weights, which ensure that the model is correctly normalized.
Models, by default, are smoothed by interpolat-ing higher-and lower-order probabilities. This is even true for methods more typically associated with backoff (rather than mixture) smoothing styles, such as Katz. While the smoothing values are estimated using interpolation, the model is encoded as a back-off model by pre-summing the interpolated proba-bilities, so that the backoff transitions are to be tra-versed only for symbols without transitions out of the current state. While these backoff transitions are labeled with , see Section 4.4 for discussion of ap-plying them as failure transitions. the topic has explored the translation of sentences into many languages (Navigli and Ponzetto, 2010; Lefever et al., 2011; Banea and Mihalcea, 2011), as well as the projection of monolingual knowledge onto another language (Khapra et al., 2011).

In our research we focus on knowledge-based methods and tools for multilingual WSD, since knowledge-rich WSD has been shown to achieve high performance across domains (Agirre et al., 2009; Navigli et al., 2011) and to compete with su-pervised methods on a variety of lexical disambigua-tion tasks (Ponzetto and Navigli, 2010). Our vi-sion of knowledge-rich multilingual WSD requires two fundamental components: first, a wide-coverage multilingual lexical knowledge base; second, tools to effectively query, retrieve and exploit its informa-tion for disambiguation. Nevertheless, to date, no integrated resources and tools exist that are freely available to the research community on a multi-lingual scale. Previous endeavors are either not freely available (EuroWordNet (Vossen, 1998)), or are only accessible via a Web interface (cf. the Mul-tilingual Research Repository (Atserias et al., 2004) and MENTA (de Melo and Weikum, 2010)), thus providing no programmatic access. And this is de-spite the fact that the availability of easy-to-use li-braries for efficient information access is known to foster top-level research  X  cf. the widespread use of semantic similarity measures in NLP, thanks to the availability of WordNet::Similarity (Peder-sen et al., 2004).

With the present contribution we aim to fill this gap in multilingual tools, providing a multi-tiered contribution consisting of (a) an Application Pro-gramming Interface (API) for efficiently accessing the information available in BabelNet (Navigli and tion of WordNet and Wikipedia); (c) the correspond-ing (possibly empty) WordNet 3.0 synset offset ; (d) the number of senses in all languages and their full listing; (e) the number of translation re-lations and their full listing; (f) the number of se-mantic pointers (i.e., relations) to other Babel synsets and their full listing. Senses encode in-formation about their source  X  i.e., whether they come from WordNet ( WN ), Wikipedia pages ( WIKI ) or their redirections ( WIKIRED ), or are automatic translations ( WNTR / WIKITR )  X  and about their language and lemma. In addition, translation rela-tions among lexical items are represented as a map-ping from source to target senses  X  e.g., 2 3,4,9 means that the second element in the list of senses (the English word bank ) translates into items #3 (German Bank ), #4 (Italian banca ), and #9 (French banque ). Finally, semantic relations are encoded using WordNet X  X  pointers and an additional sym-bol for Wikipedia relations ( r ), which can also specify the source of the relation (e.g., FROM IT means that the relation was harvested from the Ital-ian Wikipedia). In Figure 1, the Babel synset in-herits the WordNet hypernym ( @ ) relation to finan-cial institution 1 n (offset bn:00034537n ), as well as Wikipedia relations to the synsets of F INAN -CIAL I NSTRUMENT ( bn:02945246n ) and E TH -ICAL BANKING ( bn:02854884n , from Italian). BabelNet API. BabelNet can be effectively ac-cessed and automatically embedded within applica-tions by means of a programmatic access. In order to achieve this, we developed a Java API, based on Apache Lucene 3 , which indexes the BabelNet tex-tual dump and includes a variety of methods to ac-cess the four main levels of information encoded in BabelNet, namely: (a) lexicographic (information about word senses), (b) encyclopedic (i.e. named en-off-line paths connecting any pair of Babel synsets, which are collected by iterating through each synset in turn, and performing a depth-first search up to a maximum depth  X  which we set to 3, on the basis of experimental evidence from a variety of knowledge base linking and lexical disambiguation tasks (Nav-igli and Lapata, 2010; Ponzetto and Navigli, 2010). Next, these paths are stored within a Lucene index, which ensures efficient lookups for querying those paths starting and ending in a specific synset. Given a set of words as input, a semantic graph factory class searches for their meanings within BabelNet, looks for their connecting paths, and merges such paths within a single graph. Optionally, the paths making up the graph can be filtered  X  e.g., it is possi-ble to remove loops, weighted edges below a certain threshold, etc.  X  and the graph nodes can be scored using a variety of methods  X  such as, for instance, their outdegree or PageRank value in the context of the semantic graph. These graph connectivity mea-sures can be used to rank senses of the input words, thus performing graph-based WSD on the basis of the structure of the underlying knowledge base.
We show in Figure 3 a usage example of our disambiguation API. The method which performs WSD ( disambiguate ) takes as input a col-lection of words (i.e., typically a sentence), a KnowledgeBase with which to perform dis-the main method of lines 21 X 26, where we disam-biguate the sample sentence  X  bank bonuses are paid in stocks  X  (note that each input word can be written in any of the 6 languages, i.e. we could mix lan-guages). We benchmark our API by performing knowledge-based WSD with BabelNet on standard SemEval datasets, namely the SemEval-2007 coarse-grained all-words (Navigli et al., 2007, Coarse-WSD, hence-forth) and the SemEval-2010 cross-lingual (Lefever and Hoste, 2010, CL-WSD) WSD tasks. For both experimental settings we use a standard graph-based algorithm, Degree (Navigli and Lapata, 2010), which has been previously shown to yield a highly competitive performance on different lexical disam-biguation tasks (Ponzetto and Navigli, 2010). Given a semantic graph for the input context, Degree se-lects the sense of the target word with the highest vertex degree. In addition, in the CL-WSD setting we need to output appropriate lexicalization(s) in different languages. Since the selected Babel synset can contain multiple translations in a target language for the given English word, we use for this task an ily performing multilingual and cross-lingual WSD out-of-the-box. In comparison with other contribu-tions, our toolkit for multilingual WSD takes pre-vious work from Navigli (2006), in which an on-line interface for graph-based monolingual WSD is presented, one step further by adding a multilin-gual dimension as well as a full-fledged API. Our work also complements previous attempts by NLP researchers to provide the community with freely available tools to perform state-of-the-art WSD us-ing WordNet-based measures of semantic related-ness (Patwardhan et al., 2005), as well as supervised WSD techniques (Zhong and Ng, 2010). We achieve this by building upon BabelNet, a multilingual  X  X n-cyclopedic dictionary X  bringing together the lexico-graphic and encyclopedic knowledge from WordNet and Wikipedia. Other recent projects on creating multilingual knowledge bases from Wikipedia in-clude WikiNet (Nastase et al., 2010) and MENTA (de Melo and Weikum, 2010): both these resources offer structured information complementary to Ba-belNet  X  i.e., large amounts of facts about entities (MENTA), and explicit semantic relations harvested from Wikipedia categories (WikiNet).
 BabelNet and its API are available for download at http://lcl.uniroma1.it/babelnet .
 given text, and, in addition, should not contain du-plicated information, i.e., sentences which can be in-ferred from other sentences in the summary. Detect-ing these inferences can be performed by an RTE system.

Since first introduced, several approaches have been proposed for this task, ranging from shallow lexical similarity methods (e.g., (Clark and Har-rison, 2010; MacKinlay and Baldwin, 2009)), to complex linguistically-motivated methods, which incorporate extensive linguistic analysis (syntactic parsing, coreference resolution, semantic role la-belling, etc.) and a rich inventory of linguistic and world-knowledge resources (e.g., (Iftene, 2008; de Salvo Braz et al., 2005; Bar-Haim et al., 2007)). Building such complex systems requires substantial development efforts, which might become a barrier for new-comers to RTE research. Thus, flexible and extensible publicly available RTE systems are ex-pected to significantly facilitate research in this field. More concretely, two major research communities would benefit from a publicly available RTE system: 1. Higher-level application developers, who 2. Researchers in the RTE community, that would and 3. We will use this tool to illustrate various in-ference components in the demonstration session. 2.1 Inference algorithm In this section we provide a high level description of the inference components. Further details of the al-gorithmic components appear in references provided throughout this section.

B IU T EE follows the transformation based paradigm, which recognizes textual entailment by converting the text into the hypothesis via a sequence of transformations. Such a sequence is often referred to as a proof , and is performed, in our system, over the syntactic representation of the text -the text X  X  parse tree(s). A transformation modifies a given parse tree, resulting in a generation of a new parse tree, which can be further modified by subsequent transformations.

Consider, for example, the following text-hypothesis pair: T ext : ... Obasanjo invited him to step down as president H ypothesis : Charles G. Taylor was offered asylum in
This text-hypothesis pair requires two major transformations: (1) substituting  X  X im X  by  X  X harles G. Taylor X  via a coreference substitution to an ear-lier mention in the text, and (2) inferring that if  X  X  accept Y X  then  X  X  was offered Y X .

B IU T EE allows many types of transformations, by which any hypothesis can be proven from any text. Given a T-H pair, the system finds a proof which generates H from T, and estimates the proof validity. The system returns a score which indicates how likely it is that the obtained proof is valid, i.e., the transformations along the proof preserve entail-ment from the meaning of T.

The main type of transformations is application of entailment-rules (Bar-Haim et al., 2007). An entail-ment rule is composed of two sub-trees, termed left-hand-side and right-hand-side , and is applied on a parse-tree fragment that matches its left-hand-side, by substituting the left-hand-side with the right-hand-side. This formalism is simple yet power-ful, and captures many types of knowledge. The simplest type of rules is lexical rules , like car  X  tem concludes that T entails H. The complete de-scription of the cost model, as well as the method for learning the parameters w and b is described in (Stern and Dagan, 2011). 2.2 System flow The B IU T EE system flow (Figure 1) starts with pre-processing of the text and the hypothesis. B IU T EE provides state-of-the-art pre-processing utilities: Easy-First parser (Goldberg and Elhadad, 2010), Stanford named-entity-recognizer (Finkel et al., 2005) and ArkRef coreference resolver (Haghighi and Klein, 2009), as well as utilities for sentence-splitting and numerical-normalizations. In addition, B
IU T EE supports integration of users X  own utilities by simply implementing the appropriate interfaces. Entailment recognition begins with a global pro-cessing phase in which inference related computa-tions that are not part of the proof are performed. Annotating the negation indicators and their scope in the text and hypothesis is an example of such cal-culation. Next, the system constructs a proof which is a sequence of transformations that transform the text into the hypothesis. Finding such a proof is a sequential process, conducted by the search algo-rithm. In each step of the proof construction the sys-tem examines all possible transformations that can be applied, generates new trees by applying selected transformations, and calculates their costs by con-structing appropriate feature-vectors for them. New types of transformations can be added to B
IU T EE by a plug-in mechanism, without the need to change the code. For example, imagine that a researcher applies B IU T EE on the medical domain. There might be some well-known domain knowl-edge and rules that every medical person knows. Integrating them is directly supported by the plug-in mechanism. A plug-in is a piece of code which im-plements a few interfaces that detect which transfor-mations can be applied, apply them, and construct appropriate feature-vectors for each applied trans-formation. In addition, a plug-in can perform com-putations for the global processing phase.

Eventually, the search algorithm finds a (approx-imately) lowest cost proof. This cost is normalized as a score between 0 and 1 , and returned as output.
Training the cost model parameters w and b (see subsection 2.1) is performed by a linear learn-visual tracing tool , Tracer , which presents detailed information on each proof step, including potential steps that were not included in the final proof. In the demo session, we will use the visual tracing tool to illustrate all of B IU T EE  X  X  components 4 . 3.1 Modes Tracer provides two modes for tracing proof con-struction: automatic mode and manual mode . In au-tomatic mode, shown in Figure 2, the tool presents the complete process of inference, as conducted by the system X  X  search: the parse trees, the proof steps, the cost of each step and the final score. For each transformation the tool presents the parse tree before and after applying the transformation, highlighting the impact of this transformation. In manual mode, the user can invoke specific transformations pro-actively, including transformations rejected by the search algorithm for the eventual proof. As shown in Figure 3, the tool provides a list of transformations that match the given parse-tree, from which the user chooses and applies a single transformation at each step. Similar to automatic mode, their impact on the parse tree is shown visually. 3.2 Use cases Developers of knowledge resources, as well as other types of transformations, can be aided by Tracer as follows. Applying an entailment rule is a process of first matching the rule X  X  left-hand-side to the text parse-tree (or to any tree along the proof), and then substituting it by the rule X  X  right-hand-side. To test a ter linguistically motivated proof, but it turns out that this proof has higher cost than the one found by the system, it implies a limitation of the learning phase which may be caused either by a limitation of the learning method, or due to insufficient training data. In this paper we described B IU T EE , an open-source textual-inference system, and suggested it as a re-search platform in this field. We highlighted key advantages of B IU T EE , which directly support re-searchers X  work: (a) modularity and extensibility, (b) a plug-in mechanism, (c) utilization of entail-ment rules, which can capture diverse types of knowledge, and (d) a visual tracing tool, which vi-sualizes all the details of the inference process. This work was partially supported by the Israel Science Foundation grant 1112/08, the PASCAL-field in which it was awarded, etc. However, this type of exploration is still severely limited insofar that it only allows exploration by topic rather than content . Put differently, we can only explore accord-ing to what a document is about rather than what a document actually says . For instance, the facets for the query  X  X sthma X  in the faceted search engine Yippy include the concepts allergy and children , but do not specify what are the exact relations between these concepts and the query ( e.g. , allergy causes asthma, and children suffer from asthma).

Berant et al. (2010) proposed an exploration scheme that focuses on relations between concepts, which are derived from a graph describing textual entailment relations between propositions . In their setting a proposition consists of a predicate with two arguments that are possibly replaced by variables, such as  X  X  control asthma X  . A graph that specifies an entailment relation  X  X  control asthma  X  X af-fect asthma X  can help a user, who is browsing doc-uments dealing with substances that affect asthma, drill down and explore only substances that control asthma. This type of exploration can be viewed as an extension of faceted search, where the new facet concentrates on the actual statements expressed in the texts.

In this paper we follow Berant et al. X  X  proposal, and present a novel entailment-based text explo-ration system, which we applied to the health-care domain. A user of this system can explore the re-sult space of her query, by drilling down/up from one proposition to another, according to a set of en-tailment relations described by an entailment graph . In Figure 1, for example, the user looks for  X  X hings X  extracted from the corpus, entailment graphs bor-rowed from Berant et al. (2010), and the UMLS 1 taxonomy. To the best of our knowledge this is the first implementation of an exploration system, at the proposition level, based on the textual entailment re-lation. 2.1 Exploratory Search Exploratory search addresses the need of users to quickly identify the important pieces of information in a target set of documents. In exploratory search, users are presented with a result set and a set of ex-ploratory facets, which are proposals for refinements of the query that can lead to more focused sets of documents. Each facet corresponds to a clustering of the current result set, focused on a more specific topic than the current query. The user proceeds in the exploration of the document set by selecting spe-cific documents (to read them) or by selecting spe-cific facets, to refine the result set. correspond to propositional templates rather than to concepts. In this section we extend the scope of state-of-the-art exploration technologies by moving from stan-dard concept-based exploration to proposition-based exploration, or equivalently, statement-based explo-ration. In our model, it is the entailment relation between propositional templates which determines the granularity of the viewed information space. We first describe the inputs to the system and then detail our proposed exploration scheme. 3.1 System Inputs Corpus A collection of documents , which form the search space of the system.
 Extracted Propositions A set of propositions, ex-tracted from the corpus document. The propositions are usually produced by an extraction method , such as TextRunner (Banko et al., 2007) or ReVerb (Fader et al., 2011). In order to support the exploration process, the documents are indexed by the proposi-tional templates and argument terms of the extracted propositions.
 Entailment graph for predicates The nodes of the entailment graph are propositional templates, where edges indicate entailment relations between templates (Section 2.2). In order to avoid circular-ity in the exploration process, the graph is trans-formed into a DAG, by merging  X  X quivalent X  nodes that are in the same strong connectivity component (as suggested by Berant et al. (2010)). In addition, for clarity and simplicity, edges that can be inferred by transitivity are omitted from the DAG. Figure 2 illustrates the result of applying this procedure to a fragment of the entailment graph for  X  X sthma X  ( i.e. , for propositional templates with  X  X sthma X  as one of the arguments).
 Taxonomy for arguments The optional concept taxonomy maps terms to one or more pre-defined concepts, arranged in a hierarchical structure. These terms may appear in the corpus as arguments of predicates. Figure 3, for instance, illustrates a sim-ple medical taxonomy, composed of three concepts (medical, diseases, drugs) and four terms (cancer, asthma, aspirin, flexeril). more general template, by moving along the entail-ment hierarchy. For example, the user in Figure 5, expands the root  X  X ssociate X with asthma X  , in order to drill down through  X  X  affect asthma X  to  X  X  control Asthma X  .

Selecting a propositional template (Figure 1, left column) displays a concept taxonomy for the argu-ments that correspond to the variable in the selected template (Figure 1, middle column). The user can explore these argument concepts by drilling up and down the concept taxonomy. For example, in Fig-ure 1 the user, who selected  X  X  control Asthma X  , explores the arguments of this template by drilling down the taxonomy to the concept  X  X ormone X  .

Selecting a concept opens a third column, which lists the terms mapped to this concept that occurred as arguments of the selected template. For example, in Figure 1, the user is examining the list of argu-ments for the template  X  X  control Asthma X  , which are mapped to the concept  X  X ormone X  , focusing on the argument  X  X rednisone X  . access. The exploration application is the front-end user application for the whole exploration process described above (Section 3.2). As a prominent use case, we applied our exploration system to the health-care domain. With the advent of the internet and social media, patients now have access to new sources of medical information: con-sumer health articles, forums, and social networks (Boulos and Wheeler, 2007). A typical non-expert health information searcher is uncertain about her exact questions and is unfamiliar with medical ter-minology (Trivedi, 2009). Exploring relevant infor-mation about a given medical issue can be essential and time-critical.
 System implementation For the search service, we used SolR servlet, where the data service is built over FTP. The exploration application is im-plemented as a web application.
 Input resources We collected a health-care cor-pus from the web, which contains more than 2M sentences and about 50M word tokens. The texts deal with various aspects of the health care domain: answers to questions, surveys on diseases, articles on life-style, etc. We extracted propositions from the health-care corpus, by applying the method de-scribed by Berant et al. (2010). The corpus was parsed, and propositions were extracted from depen-dency trees according to the method suggested by Lin and Pantel (2001), where propositions are de-pendency paths between two arguments of a predi-This work was partially supported by the Israel Ministry of Science and Technology, the PASCAL-2 Network of Excellence of the European Com-munity FP7-ICT-2007-1-216886, and the Euro-pean Communitys Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT).
 Corpus (BNC Consortium, 2007). This necessitates manual annotation which is time-consuming and error-prone when carried out by individual linguists.
To overcome these issues, CS NIPER implements a web-based multi-user annotation scenario in which linguists formulate and refine queries that identify a given linguistic construction in a corpus and as-sess the query results to distinguish instances of the phenomenon under study ( true positives ) from such examples that are wrongly identified by the query ( false positives ). Each expert linguist thus acts as a rater rather than an annotator. The tool records as-sessments made by each rater. A subsequent evalua-tion step measures the inter-rater agreement. The ac-tual annotation step is deferred until after this evalu-ation in order to achieve high annotation confidence.
CS NIPER implements an annotation-by-query ap-proach which entails the following interlinking func-tionalities (see fig. 1):
Query development: Corpus queries can be de-veloped and refined within the tool. Based on query results which are assessed and labeled by the user, queries can be systematically evaluated and refined for precision. This transfers some of the ideas of 1. The media was now calling Reagan the front-2. It was Reagan whom the media was now calling 3. It was the media who was now calling Reagan 4. It was now that the media were calling Reagan 5. Reagan the media was not calling the front-
NCCs are linguistic constructions that deviate in characteristic ways from the unmarked lexico-grammatical patterning and informational ordering in the sentence. This is exemplified by the con-structions of sentences (2) -(5) above. While ex-pressing the same propositional content, the order of information units available through the permissi-ble grammatical constructions offers interesting in-sights into the constructional inventory of a lan-guage. It also opens up the possibility of comparing seemingly closely related languages in terms of the sets of available related constructions as well as the relations between instances of canonical and non-canonical constructions.

In linguistics, a cleft sentence is defined as a com-plex sentence that expresses a single proposition where the clefted element is co-referential with the following clause. E.g., it-clefts are comprised of the following constituents:
The NCCs under study pose interesting chal-lenges both from a linguistic and a natural language processing perspective. Due to their deviation from the canonical constructions, they come in a vari-ety of potential construction patterns as exemplified above. Non-canonical constructions can be expected to be individually rarer in any given corpus than their canonical counterparts. Their patterns of usage and their discourse functions have not yet been described exhaustively, especially not in representative corpus studies because they are notoriously hard to identify without suitable software. Their empirical distribu-tion in corpora is thus largely unknown.

A major task in recognizing NCCs is distin-guishing them from structurally similar construc-3. evaluation of inter-rater agreement and query
In the following section, we review previous work to support linguistic annotation tasks. We differentiate three categories of linguistic tools which all partially fulfill our requirements: querying tools , annotation tools , and transformation tools .
Linguistic query tools: Such tools allow to query a corpus using linguistic features, e.g. part-of-speech tags. Examples are ANNIS2 (Zeldes et al., 2009) and the IMS Open Corpus Workbench (CWB) (Christ, 1994). Both tools provide powerful query engines designed for large linguistically annotated corpora. Both are server-based tools that can be used concurrently by multiple users. However, they do not allow to assess the query results.

Linguistic annotation tools: Such tools allow the user to add linguistic annotations to a corpus. Examples are MMAX2 (M  X  uller and Strube, 2006) and the UIMA CAS Editor 1 . These tools typically display a full document for the user to annotate. As NCCs appear only occasionally in a text, such tools cannot be effectively applied to our task, as they of-fer no linguistic query capabilities to quickly locate potential NCCs in a large corpus.

Linguistic transformation tools: Such tools al-low the creation of annotations using transforma-tion rules. Examples are TextMarker (Kluegl et al., 2009) and the UAM CorpusTool (O X  X onnell, 2008). A rule has the form category := pattern and creates new annotation of the type category on any part of a text matching pattern . A rule for the annotation of passive clauses in the UAM CorpusTool could be passive-clause := clause + containing be% partici-ple . These tools do not support the assessment of the results, though. In contrast to the querying tools, transformation tools are not specifically designed to operate efficiently on large corpora. Thus, they are hardly productive for our task, which requires the analysis of large corpora. We present CS NIPER , an annotation tool for non-canonical constructions. Its main features are: refine and re-run the query.

As each user may use different queries, they will typically assess different sets of query results. This can yield a set of sentences labeled by a single user only. Therefore, the tool can display those sentences for assessment that other users have assessed, but the current user has not. This allows getting labels from all users for every NCC candidate. 4.3 Assessment (Figure 3) If the query results match the expectation, the user can switch to the assessment mode by clicking the  X  Begin assessment button. At this point, an An-notationCandidate record is created in the database for each sentence unless a record is already present. These records contain the offsets of the sentence in the original text, the sentence text and the construc-tion type. In addition, an AnnotationCandidateLabel record is created for each sentence to hold the as-sessment to be provided by the user.

In the assessment mode, an additional 8  X  Label column appears in the KWIC view. Clicking in this column cycles through the labels correct , wrong , check and nothing . When the user is uncertain, the label check can be used to mark candidates for later review. The view can be 9  X  filtered for those sen-tences that need to be assessed, those that have been assessed, or those that have been labeled with check .  X  comment can be left to further describe difficult cases or to justify decisions. All changes are imme-diately saved to the database, so the user can stop assessing at any time and resume the process later.
The proper assessment of a sentence as an in-stance of a particular construction type sometimes depends on the context found in the preceding and following sentences. For this purpose, clicking on the 11  X  book icon in the KWIC view displays the sentence in its larger context (fig. 4). POS tags are shown in the sentence to facilitate query refinement. 4.4 Evaluation (Figure 5) The evaluation function provides an overview of the current assessment state (fig. 5). We support two evaluation views: by construction type and by query .
By construction type: In this view, one or more  X  corpora , 13  X  types , and 14  X  users can be selected for evaluation. For these, all annotation candidates and the respective statistics are displayed. It is pos-If a candidate is neither TP nor FP , it is unknown ( UNK ). When calculating precision , UNK candi-dates are counted as FP . The estimated precision is the precision to be expected if TP and FP are equally distributed over the set of candidates. It takes into account only the currently known TP and FP and ig-nores the UNK candidates. Both values are the same once all candidates have been labeled by all users. 4.5 Annotation When the assessment process is complete, corpus annotations can be generated from the assessed can-didates. Here, we employ the thresholded major-ity vote approach that we also use to determine the TP / FP in sect. 4.4. Annotations for the respective NCC type are added directly to the corpus. The aug-mented corpus can be used in further exploratory work. Alternatively, a file with all assessed candi-dates can be generated to serve as training data for identification methods based on machine learning. We have presented CS NIPER , a tool for the an-notation of linguistic phenomena whose investiga-tion requires the analysis of large corpora due to a relatively low frequency of instances and whose identification requires expert knowledge to distin-guish them from other similar constructions. Our tool integrates the complete functionality needed for the annotation-by-query workflow. It provides dis-tributed multi-user annotation and evaluation. The feedback provided by the integrated evaluation mod-ule can be used to systematically refine queries and improve assessments. Finally, high-confidence an-notations can be generated from the assessments. bases like Wikipedia, are much more widely available than parallel translation data. 
While methods for the use of parallel corpora in machine translation are well studied (Koehn, 2010), similar techniques for comparable corpora have not been thoroughly worked out. Only the latest research has shown that language pairs and domains with little parallel data can benefit from the exploitation of comparable corpora (Munteanu and Marcu, 2005; Lu et al., 2010; Smith et al., 2010; Abdul-Rauf and Schwenk, 2009 and 2011). 
In this paper we present the ACCURAT analysing comparable corpora and extracting parallel data which can be used to improve the performance of statistical and rule/example-based MT systems. 
Although the toolkit may be used for parallel data acquisition for open (broad) domain systems, it will be most beneficial for under-resourced languages or specific domains which are not covered by available parallel resources. 
The ACCURAT toolkit produces: language specific taggers (named entity recognisers (NER) or term extractors) and performs multi-lingual NE (section 2.3) or term mapping (section 2.4), thereby producing bilingual NE or term dictionaries. The workflow also accepts pre-processed documents, thus skipping the tagging process. 
Since all tools use command line interfaces, task automation and workflow specification can be done with simple console/terminal scripts. All tools can be run on the Windows operating system (some are also platform independent). 2 Tools and Methods This section provides an overview of the main tools and methods in the toolkit. A full list of tools is described in ACCURAT D2.6. (2011). 2.1 Comparability Metrics We define comparability by how useful a pair of documents is for parallel data extraction. The higher the comparability score, the more likely two documents contain more overlapping parallel data. The methods are developed to perform lightweight comparability estimation that minimises search space of relatively large corpora (e.g., 10,000 documents in each language). There are two comparability metric tools in the toolkit: a translation based and a dictionary based metric. 
The Translation based metric (Su and Babych, 2012a) uses MT APIs for document translation into English. Then four independent similarity feature functions are applied to a document pair: These similarity measures are linearly combined in a final comparability score. This is implemented by a simple weighted average strategy, in which each and/or by phrase generation (Koehn et al., 2003). Our toolkit exploits comparable corpora in order to find and extract comparable sentences for SMT training using a tool named LEXACC (  X  tef  X  nescu et al., 2012). 
LEXACC requires aligned document pairs (also m to n alignments) for sentence extraction. It also allows extraction from comparable corpora as a whole; however, precision may decrease due to larger search space. 
LEXACC scores sentence pairs according to five lexical overlap and structural matching feature functions. These functions are combined using linear interpolation with weights trained for each language pair and direction using logistic regression. The feature functions are: the individual feature functions differ. For instance, the locality feature is more important for the English-Romanian pair than for the English-Greek pair. Therefore, the weights are trained on parallel corpora (in our case -10,000 pairs). the Cartesian product between source and target document sentences. It reduces the search space using two filtering steps (  X  tef  X  nescu et al., 2012). The first step makes use of the Cross-Language Information Retrieval framework and uses a search engine to find sentences in the target corpus that are the most probable translations of a given sentence. In the second step (which is optional), 
Table 2 shows results for English-Romanian on corpora consisting of 310,740 unique English and 81,433 unique Romanian sentences. 
Useful pairs denote the total number of parallel and strongly comparable sentence pairs (at least 80% of the source sentence is a translation in the target sentence). The corpora size is given only as an indicative figure, as the amount of extracted parallel data greatly depends on the comparability of the corpora. 2.3 Named Entity Extraction and Mapping The second workflow of the toolkit allows NE and terminology extraction and mapping. Starting with named entity recognition, the toolkit features the first NER systems for Latvian and Lithuanian (Pinnis, 2012). It also contains NER systems for English (through an OpenNLP NER 2 wrapper) and Romanian ( NERA ). In order to map named entities, documents have to be tagged with NER systems that support MUC-7 format NE SGML tags. The toolkit contains the mapping tool NERA2 . The mapper requires comparable corpora aligned in the document level as input. NERA2 compares each NE from the source language to each NE from the target language using cognate based methods. It also uses a GIZA++ format statistical dictionary to map NEs containing common nouns that are frequent in location names. This approach allows frequent NE mapping if the cognate based method fails, therefore, allowing increasing the recall of the mapper. Precision and recall can be tuned with a confidence score threshold. 2.4 Terminology Mapping During recent years, automatic bilingual term mapping in comparable corpora has received greater attention in light of the scarcity of parallel data for under-resourced languages. Several methods have been applied to this task, e.g., contextual analysis (Rapp, 1995; Fung and McKeown, 1997) and compositional analysis (Daille and Morin, 2008). Symbolic, statistical, and hybrid techniques have been implemented for bilingual lexicon extraction (Morin and Prochasson, 2011). 
Our terminology mapper is designed to map terms extracted from comparable or parallel use. The workflows provide functionality for the extraction of parallel sentences, bilingual NE dictionaries, and bilingual term dictionaries from comparable corpora. 
The methods, including comparability metrics, parallel sentence extraction and named entity/term mapping, are language independent. However, they may require language dependent resources, for instance, POS-taggers, Giza++ translation dictionaries, NERs, term taggers, etc. 3 The ACCURAT toolkit is released under the Apache 2.0 licence and is freely available for download after completing a registration form 4 . Acknowledgements The research within the project ACCURAT leading to these results has received funding from the European Union Seventh Framework Programme (FP7/2007-2013), grant agreement no 248347. References to detect emotions (Busso et al. , 2004), but then the accompanied cameras and microphones were necessary. Some researchers tried to use sensors to watch the heart beat and the body temperature of residents to know their current emotion for further applications, but the problem was that users had to wear sensors and it was inconvenient. Instead of watching body signals, we postulate that the communications among people is one of the important factors to influence their emotions. Therefore, we tried to find clues from the textual conversations of the residents in order to detect their psychological state. There are many ways to categorize emotions. Different emotion states were used for experiments in previous research (Bellegarda, 2010). To find suitable categories of emotions, we adopted the three-layered emotion hierarchy proposed by Parrott (2001) 1 . Six emotions are in the first layer, including love, joy, surprise, anger, sadness and fear. The second layer includes 25 emotions, and the third layer includes 135 emotions. Using this hierarchical classification benefits the system. We can categorize emotions from rough to fine granularities and degrade to the upper level when the experimental materials are insufficient. How to map categories in other researches to ours becomes clearer, and annotators have more information when marking their current emotion. 
As to the music, most researchers looked for the emotions in songs or rhythms (Yang and Chen, 2011; Zbikowski, 2011). They classified music into different emotional categories and developed the system to tell what emotion a song might bring to a listener. However, if the aim is to create a 
Personal Preference Learning: When users change the settings, the new ones are recorded. IlluMe learns the preference and then performs the user adaptation. After a period of time users will have their unique ambient creating system. Unlimited Melodies and Rich Light Colors: Users can add their songs in the smart phone for selection at any time. The learning process will help propose the new songs to create ambient later. 
Instant State Update: IlluMe watches the user input from messenger when the software is on. Therefore, it is able to change the music and lighting according to the detected emotion within a preset time period and users will feel like the environment is interacting with them. 2.2 System Framework Figure 1 demonstrates the system framework of IlluMe. The system automatically watches the User Messages from messenger logs. The Emotion Analysis component detects the emotion of users, while the Ambient Learning Model determines the music and lighting accordingly, considering also the Personal Information of users. 
After the lights are on and the music is played, the user can change the settings they are not satisfying. A smart phone ( Mobile Device ) is used to change the settings, w ith two controllers on it: the Preference Controller and the Ambient Controller . The former takes the User Input for new settings, and then the music and lighting are changed by the latter. At the same time, the Preference Controller also sends the new settings to Ambient Learning Model to be recorded for user adaptation when creating the next ambient. The Emotion Analysis Component and Ambient Learning Model are two programs in a personal computer, and the Personal Info is saved in the personal computer, too. ANT wireless personal network protocol (Dynastream) is adopted to send the control signals to the Lighting . The LED lighting board is utilized to implement the Lighting of 65,536 colors. 2.3 Operation Flowchart of User Interface The IlluMe system provides a user interface to change the settings by a smart phone ( Mobile Device ), functioning as a remote control. Users can select the location of music or the lighting, e.g. the living room or the bedroom, and the control mode, The emotion analysis that IlluMe performed is to find the emotions that texts in messenger logs bear in order to create a comfort ambient by sound and lighting accordingly. To achieve this, the system needs to understand the Internet language first, and then detect emotions and categorize them. The system works on the Chinese chatting environment and analyzes Chinese texts to detect emotions. The materials, approaches, and preliminary results in the development phase are described in this section. 3.1 Experimental Materials Two dictionaries, the Chinese sentiment dictionary NTUSD (Ku and Chen, 2007) and the Chinese emotion dictionary (Lin et al. , 2008), were adopted for detecting emotions. The former categorized sentiment words into positive and negative, while the latter into eight emotion types: awesome, heartwarming, surprising, sad, useful, happy, boring, and angry. Notice that these eight emotion Bedroo m Living Roo m rarely used in Taiwan. Su reported that the most popular type of creative use of writing systems is  X  X huyin Wen X  (  X  X  X  X  ). In  X  X huyin Wen X  the complete phonetic representation of a character is reduced to a consonant, or sometimes a vowel. This creative use appeared commonly in the collected conversations. Generally we had to figure out the missing vowels to understand the word, but in our system a reversed approach (dropping vowels) was adopted to make sure the system did not miss any possible match of dictionary terms observed in the conversations. 
When messenger users typed characters by their phonetics (consonants and vowels), very often they selected the wrong one from candidates of the same pronunciation, or they were just too lazy to select so the writing system chose the default candidate for them. In these cases, the system could not find a match because of wrong composite characters. Transforming characters in both dictionaries and conversations into their zhuyin representations before detecting emotions also help recover this kind of errors. 3.3 Emotion Detection from Texts Section 3.2 shows how the system dealt with the error prone Internet texts and found the dictionaries terms. Ku and Chen X  X  (2007) approach for calculating sentiment scores was then adopted to give scores to these terms. The scores of terms of different emotional categories were summed up and the emotion category of the highest score was selected as the detected emotion. The Ambient Learning Model takes the detected emotion and selects the corresponding music and lighting by the Na X ve Bayes classifier trained by the annotated materials. 3.4 Experiment and Preliminary Results Table 2 shows that using enhanced NTUSD (an augmented version of NTUSD) together with zhuyin transformation achie ves the best results for emotion classification (positive/negative). 
Ku (2008) reported the set precision of their approach was 0.489 when texts were categorized into positive, neutral and negative. Though they had one additional neutral category, our system achieved the precision of 0.620 when processing the noisy Internet texts without word segmentation and part of speech tagging, which was satisfactory. of both the individual and the group to create a unique ambient for each user. Through the work we aim to apply the language technology to redefine the concept of a small house or working space. They should be a family-like existence which possesses th e intellectual capacity to observe human behavior and emotion, and create consoling spaces according to the residents X  different status. Therefore we implemented emotion analysis technique to equip a space with the ability to observe the status of its residents and interact with them accordingly. The instant interior lightings and music change can be viewed as a new form of  X  X onversation X . Residents can not only take the ambient provided by IlluMe, but can also give feedbacks. The concept of collaborative filtering was also implemented as we viewed the proposing of ambient as a kind of recommendation. 
Through the demonstration of the IlluMe system, we hope to show anothe r way to apply language technology in life and retrieve the positive and relaxing atmosphere to rebuild our sense of trust and safety toward space, and finally recollect the long-lost attachment toward it. materials and user feedbacks for learning, and make the materials a corpus for the research community. Facebook will be a source of text collection to gather more complete personal conversations for emoti on detection. Making the IlluMe components real products like the home lighting system, the intelligent table lamp, or the music album promoter is also a future plan. As demonstrating the IlluMe system by our original model house may be difficult in transportation and it may need a large space for demonstration, we will demonstrate the lightings by several table lamps, in which the LED lighting board resides. Other software will be performed on the smart phone and the personal computer. 5.1 Demonstration Outline There are three purposes of the demonstration: first, to show how we apply the emotion analysis and recommendation technique in an ambient creating system; second, to illustrate actual and live Dynastream Innovations Inc., ANT AT3 RF Transceiver 
Chipset_Datasheet_Rev1.2, http://www.thisisant.com/. Ku, Lun-Wei and Chen, Hsin-Hsi. 2007. Mining Opinions from the Web: Beyond Relevance Retrieval. 
Journal of American Society for Information Science and Technology, Special Issue on Mining Web 
Resources for Enhancing Information Retrieval, 58(12), 1838-1850. Ku, Lun-Wei, Liu, I-Chien, Lee, Chia-Ying, Chen, Kuan-hua. and Chen, Hsin-His. 2008. Sentence-Level Opinion Analysis by CopeOpi in NTCIR-7. Proceedings of the 7th NTCIR Workshop Meeting, Tokyo, Japan. 260-267. Lin, Kevin Hsin-Yih, Yang, Changhua, and Chen, Hsin-His. 2008. Emotion Classification of Online News 
Articles from the Reader X  X  Perspective. Proceedings of the 2008 IEEE/WIC/ACM International Conference on Web Intelligence. 220-226. Ortony, A. and Turner, T. J. 1990. What's basic about basic emotions? Psychological Review, 97, 315-331. Parrott, W. 2001. Emotions in Social Psychology, Psychology Press, Philadelphia. Sarwar, Badrul, Karypis, George, Konstan, Joseph, and Riedl, John. 2001. ItemBased Collaborative Filtering Recommendation Algorithms . Proceedings of the 
International World Wide Web Conference (WWW 2001), 285-295. Su, Hsi-Yao. 2003. The Multilingual and Multi-
Orthographic Taiwan-Based Internet: Creative Uses of Writing Systems on College-Affiliated BBSs. 
Journal of Computer-Mediated Communication 9(1). http://jcmc.indiana.edu/vol9/issue1/su.html. Yang, Yi-Hsuan and Chen, Homer H., Fellow, IEEE. 2011. Ranking-Based Emotion Recognition for Music Organization and Retrieval. IEEE 
Transactions on audio, speech, and language processing, 19(4). Zbikowski, Lawrence M., 2011. Music, Emotion, Analysis. Music Analysis, Blackwell Publishing Ltd, Oxford, UK. Zhang, Jiawei and Dai, Jiaxing. 2006. Qiantan shixia qingnian wangluo yongyu  X  huoxing wen  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  http://www.shs.edu.tw/works/essay/2006/03/2006032 816043532.pdf Zheng, Vincent W., Cao, Bin, Zheng, Yu, Xie, Xing and Yang, Qiang. 2010. Collaborative Filtering Meets Mobile Recommendation: A User-centered Approach 
Proceedings of Twenty-Fourth National Conference on Artificial Inte lligence (AAAI-10). flexible behaviours can be realised, such as providing utterances in installments (Clark, 1996) that prompt for backchannel signals, which in turn can prompt different utterance continuations, or starting an utter-ance before all information required in the utterance less conventional type of speech-based system that could profit from iSS is  X  X abelfish-like X  simultaneous speech-to-speech translation.

Research on architectures, higher-level process-ing modules and lower-level processing modules that would enable such behaviour is currently underway (Skantze and Schlangen, 2009; Skantze and Hjal-marsson, 2010; Baumann and Schlangen, 2011), but a synthesis component that would unlock the full potential of such strategies is so far missing. In this paper, we present such a component, which is capa-ble of (a) starting to speak before utterance processing has (b) handling edits made to (as-yet unspoken) parts of (c) enabling adaptations of delivery parameters such (d) autonomously making appropriate delivery-(e) providing information about progress in delivery; (f) running in real time.

Our iSS component is built on top of an exist-ing non-incremental synthesis component, MaryTTS (Schr X der and Trouvain, 2003), and on an existing architecture for incremental processing, I NPRO TK (Baumann and Schlangen, 2012). We aim for a fully incremental speech synthesis com-ponent that can be integrated into dialogue systems.
There is some work on incremental NLG (Kilger and Finkler, 1995; Finkler, 1997; Guhe, 2007); how-ever, that work does not concern itself with the actual synthesis of speech and hence describes only what would generate the input to our component. 3.1 Background on Speech Synthesis Text-to-speech ( TTS ) synthesis normally proceeds in a top-down fashion, starting on the utterance level (for stress patterns and sentence-level intonation) and descending to words and phonemes (for pronunci-ation details), in order to make globally optimised decisions (Taylor, 2009). In that way, target phoneme sequences annotated with durations and pitch con-tours are generated, in what is called the linguistic pre-processing step.

The then following synthesis step proper can be executed in one of several ways, with HMM -based and unit-selection synthesis currently being seen as producing the perceptually best results (Taylor, 2009). The former works by first turning the target sequence into a sequence of HMM states; a global optimiza-tion then computes a stream of vocoding features that optimize both HMM emission probabilities and continuity constraints (Tokuda et al., 2000). Finally, the parameter frames are fed to a vocoder which gen-erates the speech audio signal. Unit-selection, in contrast, searches for the best sequence of (variably sized) units of speech in a large, annotated corpus of recordings, aiming to find a sequence that closely matches the target sequence.

As mentioned above, Dutoit et al. (2011) have pre-sented an online formulation of the optimization step in
HMM -based synthesis. Beyond this, two other fac-tors influenced our decision to follow the HMM -based approach: (a) HMM -based synthesis nicely separates the production of vocoding parameter frames from the production of the speech audio signal, which allows for more fine-grained concurrent processing (see next subsection); (b) parameters are partially independent in the vocoding frames, which makes it possible to manipulate e. g. pitch independently (and outside of the HMM framework) without altering other parameters or deteriorating speech quality. 3.3 Technical Overview of Our System As a basis, we use MaryTTS (Schr X der and Trou-vain, 2003), but we replace Mary X  X  internal data struc-tures with structures that support incremental spec-ifications; these we take from an extant incremen-tal spoken dialogue system architecture and toolkit, I NPRO TK (Schlangen et al., 2010; Baumann and Schlangen, 2012). In this architecture, incremental processing as the processing of incremental units (
IU s), which are the smallest  X  X hunks X  of information at a specific level (such as words, or phonemes, as can be seen in Figure 1). IU s are interconnected to form a network (e. g. words keep links to their asso-ciated phonemes, and vice-versa) which stores the system X  X  complete information state.

The iSS component takes an IU sequence of chunks of words as input (from an NLG component). Crucially, this sequence can then still be modified, through: (a) continuations , which simply link further words to the end of the sequence; or (b) replacements , where elements in the sequence are  X  X nlinked X  and other elements are spliced in. Additionally, a chunk can be marked as open ; this has the effect of linking to a special hesitation word , which is produced only if it is not replaced (by the NLG ) in time with other material.

Technically, the representation levels below the chunk level are generated in our component by MaryTTS X  X  linguistic preprocessing and converting the output to IU structures. Our component provides for two modes of operation: Either using MaryTTS X  HMM optimization routines which non-incrementally solve a large matrix operation and subsequently iter-atively optimize the global variance constraint (Toda and Tokuda, 2007). Or, using the incremental algo-rithm as proposed by Dutoit et al. (2011). In our implementation of this algorithm, HMM emissions are computed with one phoneme of context in both directions; Dutoit et al. (2011) have found this set-ting to only slightly degrade synthesis quality. While the former mode incurs some utterance-initial delay, switching between alternatives and prosodic alter-ation can be performed at virtually no lookahead, while requiring just little lookahead for the truly incremental mode. The resulting vocoding frames then are attached to their corresponding phoneme units. Phoneme units then contain all the information We will describe the features of iSS, their implemen-tation, their programming interface, and correspond-ing demo applications in the following subsections. 4.1 Low-Latency Changes to Prosody Pitch and tempo can be adapted on the phoneme IU layer (see Figure 1). Figure 2 shows a demo in-terface to this functionality. Pitch is determined by a single parameter in the vocoding frames and can be adapted independently of other parameters in the HMM approach. We have implemented capabilities of adjusting all pitch values in a phoneme by an offset, or to change the values gradually for all frames in the phoneme. (The first feature is show-cased in the application in Figure 2, the latter is used to cancel utterance-final pitch changes when a continuation is appended to an ongoing utterance.) Tempo can be adapted by changing the phoneme units X  durations which will then repeat (or skip) parameter frames (for lengthened or shortened phonemes, respectively) when passing them to the crawling vocoder . Adapta-tions are conducted with virtually no lookahead, that is, they can be executed even on a phoneme that is currently being output. 4.2 Feedback on Delivery We implemented a fine-grained, hierarchical mech-anism to give detailed feedback on delivery. A new progress field on IU s marks whether the IU  X  X  produc-tion is upcoming, ongoing, or completed. Listeners may subscribe to be notified about such progress changes using an update interface on IU s. The appli-cations in Figures 2 and 4 make use of this interface to mark the words of the utterance in bold for com-pleted, and in italic for ongoing words (incidentally, the screenshot in Figure 4 was taken exactly at the boundary between  X  delete  X  and  X  the  X  ). 4.3 Low-Latency Switching of Alternatives A major goal of iSS is to change what is being said while the utterance is ongoing. Forward-pointing same-level links (SLLs, (Schlangen and Skantze, 2009; Baumann and Schlangen, 2012)) as shown in Figure 3 allow to construct alternative utterance paths beforehand. Deciding on the actual utterance continuation is a simple re-ranking of the forward 4.5 Autonomously Performing Disfluencies In a multi-threaded, real-time system, the crawling vocoder may reach the end of synthesis before the NLG component (in its own thread) has been able to add a continuation to the ongoing utterance. To avoid this case, special hesitation word s can be in-serted at the end of a yet unfinished utterance. If the crawling vocoder nears such a word, a hesitation will be played, unless a continuation is available. In that case, the hesitation is skipped (or aborted if currently ongoing). 2 4.6 Type-to-Speech A final demo application show-cases truly incremen-tal HMM synthesis taken to its most extreme: A text input window is presented, and each word that is typed is treated as a single-word chunk which is im-mediately sent to the incremental synthesizer. (For this demonstration, synthesis is slowed to half the regular speed, to account for slow typing speeds and to highlight the prosodic improvements when more right context becomes available to iSS.) A use case with a similar (but probably lower) level of incre-mentality could be simultaneous speech-to-speech translation, or type-to-speech for people with speech disabilities. We have presented a component for incremental speech synthesis (iSS) and demonstrated its capa-bilities with a number of example applications. This component can be used to increase the responsivity and naturalness of spoken interactive systems. While iSS can show its full strengths in systems that also generate output incrementally (a strategy which is currently seeing some renewed attention), we dis-cussed how even otherwise unchanged systems may profit from its capabilities, e. g., in the presence of intermittent noise. We provide this component in the hope that it will help spur research on incremental natural language generation and more interactive spo-ken dialogue systems, which so far had to made do with inadequate ways of realising its output. how to enable the ease of development and mainte-nance of high-quality information extraction rules, also known as annotators , or extractors .

Developing extractors is a notoriously labor-intensive and time-consuming process. In order to ensure highly accurate and reliable results, this task is traditionally performed by trained linguists with domain expertise. As a result, extractor develop-ment is regarded as a major bottleneck in satisfying the increasing text analytics demands of enterprise applications. Hence, reducing the extractor devel-opment life cycle is a critical requirement. Towards this goal, we have built WizIE , an IE development environment designed primarily to (1) enable devel-opers with little or no linguistic background to write high quality extractors, and (2) reduce the overall manual effort involved in extractor development.
Previous work on improving the usability of IE systems has mainly focused on reducing the manual effort involved in extractor development (Brauer et al., 2011; Li et al., 2008; Li et al., 2011a; Soder-land, 1999; Liu et al., 2010). In contrast, the fo-cus of WizIE is on lowering the extractor develop-ment entry barrier by means of a wizard-like en-vironment that guides extractor development based on best practices drawn from the experience of trained linguists and expert developers. In doing so, WizIE also provides natural entry points for differ-ent tools focused on reducing the effort required for performing common tasks during IE development. Underlying our WizIE are a state-of-the-art IE rule language and corresponding runtime en-gine (Chiticariu et al., 2010a; Li et al., 2011b). The runtime engine and WizIE are commercially avail-is of great interest to marketers as it helps pre-dict future purchases (Howard and Sheth, 1969). During the first phrase of IE development (Fig. 2), WizIE guides the rule developer in turning such a high-level business requirement into concrete ex-traction tasks by explicitly asking her to select and manually examine a small number 1 of sample doc-uments, identify and label snippets of interest in the sample documents, and capture clues that help to identify such snippets.

The definition and context of the concrete extrac-tion tasks are captured by a tree structure called the extraction plan (e.g. right panel in Fig. 2). Each leaf node in an extraction plan corresponds to an atomic extraction task, while the non-leaf nodes de-note higher-level tasks based on one or more atomic extraction tasks. For instance, in our running ex-ample, the business question of identifying intention of purchase for movies has been converted into the extraction task of identifying MovieIntent mentions, which involves two atomic extraction tasks: identi-fying Movie mentions and Intent mentions.

The extraction plan created, as we will describe later, plays a key role in the IE development process in WizIE . Such tight coupling of task analysis with actual extractor development is a key departure from conventional IE development environments. 3.2 Rule Development Once concrete extraction tasks are defined, WizIE guides the IE developer to write actual rules based on best practices. Fig. 3(a) shows a screenshot of the second phase of building an extractor, the Rule Development phase. The Extraction Task panel on the left provides information and tips for rule development, whereas the Extraction Plan panel on the right guides the actual rule development for each extraction task. As shown in the figure, the types of rules associated with each label node fall into three categories: Basic Features , Can-displayed in the editor, for further editing 2 .
Once the developer completes an iteration of rule development, WizIE guides her in testing and refin-ing the extractor, as shown in Fig. 3(b). The An-notation Explorer at the bottom of the screen gives a global view of the extraction results, while other panels highlight individual results in the context of the original input documents. The Annotation Ex-plorer enables filtering and searching results, and comparing results with those from a previous iter-ation. WizIE also provides a facility for manually labeling a document collection with  X  X round truth X  annotations, then comparing the extraction results with the ground truth in order to formally evalu-ate the quality of the extractor and avoid regressions during the development process.

An important differentiator of WizIE compared with conventional IE development environments is a suite of sophisticated tools for automatic result ex-planation and rule discovery . We briefly describe them next.
 Provenance Viewer. When the user clicks on an ex-tracted result, the Provenance Viewer shows a com-plete explanation of how that result has been pro-saw , since past tense in not usually indicative of in-tent (Liu et al., 2010).
 Pattern Discovery. Negative contextual clues such as the verb  X  X aw X  above are useful for creating rules that filter out false positives. Conversely, positive clues such as the phrase  X  X ill see X  are useful for creating rules that separate ambiguous matches from high-precision matches. WizIE  X  X  Pattern Discovery component facilitates automatic discovery of such clues by mining available sample data for common patterns in specific contexts (Li et al., 2011a). For example, when instructed to analyze the context be-tween SelfRef and MovieName mentions, Pattern Dis-covery finds a suite of common patterns as shown in Fig. 4. The developer can analyze these patterns and choose those suitable for refining the rules. For example, patterns such as  X  X ave to see X  can be seen as positive clues for intent, whereas phrases such as  X  X ook ... to see X  or  X  X ent to see X  are negative clues, and can be used for filtering false positives. Regular Expression Generator. WizIE also en-ables the discovery of regular expression patterns. The Regular Expression Generator takes as input a the profiling run completes, WizIE displays the top 25 most expensive rules and runtime operations, and the overall throughput (amount of input data pro-cessed per unit of time). Based on this information, the developer can hand-tune the critical parts of the extractor, rerun the Profiler, and validate an increase in throughput. She would repeat this process until satisfied with the extractor X  X  runtime performance. 3.4 Delivery and Deployment Once satisfied with both the result quality and runtime performance, the developer is guided by WizIE  X  X  Export wizard through the process of ex-porting the extractor in a compiled executable form. The generated executable can be embedded in an ap-plication using a Java API interface. WizIE can also wrap the executable plan in a pre-packaged applica-tion that can be run in a map-reduce environment, then deploy this application on a Hadoop cluster. A preliminary user study was conducted to evalu-ate the effectiveness of WizIE in enabling novice IE developers. The study included 14 participants, all employed at a major technology company. In the pre-study survey, 10 of the participants reported no prior experience with IE tasks, two of them have seen demonstrations of IE systems, and two had brief involvement in IE development, but no expe-rience with WizIE . For the question  X  X ccording to your understanding, how easy is it to build IE appli-cations in general ? X  , the median rating was 5, on a of identifying intent to purchase movies from blogs and forum posts as described in Section 3. We start by demonstrating the process of developing two rel-atively simple extractors for identifying MovieIntent and MovieRating mentions. We then showcase com-plex state-of-the-art extractors for identifying buzz and sentiment for the media and entertainment do-main, to illustrate the quality and runtime perfor-mance of extractors built with WizIE .
 have developed a tool for real -time analysis of sentiment expressed through Twitter, a m icro -blogging service , toward the incumbent President, Barack Obama, and the nine republican challengers -four of who m remain in the running as of this writing. With this analysis, we seek to explore whether Twitter provides insights into the unfolding of the campaign s and indications of shifts in public opinion.

Twitter allows users to post tweets, messages of up to 140 character s , on its social network. Twitter usage is growing rapidly. The company reports over 100 million active users worldwide, together sending over 250 million tweets each day ( Twitter, 2012 ) . It was actively used by 13% of on -line American adults as of May 2011, up from 8% a year prior ( Pew Research Center, 2011 ) . More than two thirds of U.S. congress members have created a Twitter account and many are a ctively using Twitter to reach their constituents ( Lassen &amp; Brown, 2010 ; TweetCongress, 2012 ) . Since October 12, 2012, we have gathered over 36 million tweets ab out the 2012 U.S. presidential candidates , a quarter million per day on average. During one of the key political events, the Dec 15, 2011 primary debate in Iowa, we collected more than half a million relevant tweets in just a few hours . This kind of  X  X ig data X  vastly outpaces the capacity of traditional content analysis approaches, calling for novel computational approaches.
Most work to date has focused on post -facto analysis of tweets, with results coming days or even months after the collection time. H owever, Choy et al., 2011 ; Tumasjan et al., 2010 ; Zeitzoff, 2011 ) . For example, Tumasjan (2010) found tweet volume about the political parties to be a good predictor for the outcome of the 2009 German election, while Choy et al. (2011) failed to pr edict with Twitter sentiment the ranking of the four candidates in Singapore  X  X  2011 presidential election .
Past studies of political sentiment on social networks ha ve been either post -hoc and/or carried out on small and static sample s . To address these i ssues, w e built a unique infrastructure and sentiment model to analyze in real -time public sentiment on Twitter toward the 2012 U .S. presidential candidates . Our effort to gauge political sentiment is based on bringing together social science scholarship w ith advanced computational methodology: our approach combines real -time data processing and statistical sentiment model ing informed by, and contributing to, an understanding of the cultural and political practices at work through the use of Twitter. For accuracy and speed, we built our real -time data processing infrastructure on the IBM X  X  InfoSphere Streams platform ( IBM, 2012 ) , w hich enables us to write our own analysis and visualization modules and assemble them into a real -time processing pipeline. Streams applications are highly scalable so we can adjust our system to handle higher volume of data by adding more servers and by d istributing processing tasks. Twitter traffic often balloons during big events (e.g. televised debates or primary election days) and stays low between events, making high scalability strongly desirable. Figure 1 shows our system X  X  architecture and its modu les. Next, we introduce our data source and each individual module.
 idiosyncratic uses, such as emoticon s , URLs, RT for re -tweet, @ for user mentions, # for hashtags, and repetitions. It is necessary to preprocess and normalize the text.

As standard in NLP practices, the text is tokenized for later processing. We use certain rules to handle the special cases in tweets. We compa red several Twitter -specific tokenizers, such as T weet M otif ( O'Connor et al., 2010 ) and found Christopher Potts X  basic Twitter tokenizer best suited as our base. In summary, our tokenizer correctly handles URLs, common emoticons, phone numbers, HTML tags, twitter mention s and hashtag s , numbers with fractions and decimals, repetition of symbols and Unicode characters (see Figure 2 for an example). 3.3 Sentiment Model The design of the sentiment model used in our system was based on the assumption that the opinions expressed would be highly subjective and contextualize d. Therefore, for generating data for model training and testing, we used a crowd -sourcing approach to do sentiment annotation on in -domain political data.
 To create a baseline sentiment model, we used Amazon Mechanical Turk (AMT) to get as varied a popul ation of annotators as possible. We designed an interface that allowed annotators to perform the annotations outside of AMT so that they could participate anonymously. The Turkers were asked their age, gender, and to describe their political orientation. Then they were shown a series of tweets and asked to annotate the tweets' sentiment (positive, negative, neutral, or unsure), whether the tweet was sarcastic or humorous , the sentiment on a scale from positive to negative, and the tweet auth or's political orientation on a slider scale from conservative to liberal. Our sentiment model is based on the sentiment label and the sarcasm and humor labels. Our training data consists of nearly 17000 tweets ( 16% positive, 56% negative , 18% neutral, 10% unsure ) , including nearly 2000 that were multiply an notated ( Figure 3 ) to display volume and sentiment by candidate as well as trending words and system statistics. The dashboard pulls updated data from a web server and refresh es its display every 30 seconds. In Figure 3 , the top -left bar graph shows the number of p ositive and negative tweets about each candidate ( right and left bars, respectively) in the last five minutes as an indicator of sentiment towards the candidates. We chose to display both positive and negative sentiment, instead of the difference between t hese two, because events typically trigger sharp variation s in both positive and negative tweet volume. The top -right chart displays the number of tweets for each candidate every minute in the last two hours . We chose this time window because a live -broadc ast primary debate usually lasts about two hours. The bottom -left shows system statistics, including the total number of tweets, the number of seconds since system start and the average data rate. The bottom -right table shows trending words of the last fiv e minutes, computed using TF -IDF measure as follows: tweets about all candidates in a minute are treated as a single  X  X ocument X ; trending words are the tokens from the current minute with the highest TF -IDF weights when using the last two hours as a corpus ( i.e., 120  X  X ocuments X ). Qualitative examination suggests that the simple TF -IDF metric effectively identifies the most prominent words when an event occurs.

The dashboard gives a synthetic overview of volume and sentiment for the candidates, but it is of ten desirable to view selected tweets and their sentiments. The dashboard includes another page establish an online feedback loop between users and the sentiment model, so users X  judgment serves to train the model actively and iteratively. In Section 3.3, we described our preliminary sentiment model that automatically classifies tweets into four categories: positive, negative, neutral or unsure. It copes well with the negative bias in po litical tweets. In addition to evaluating and accurate picture of the online political landscape. Our real -time data processing infrastructure and statistical sentiment model evaluates public sentiment changes in response to emerging political events and news as they unfold . The architecture and method are generic, and can be easily adopted and extended to other do mains (for instance, we used the system for gauging sentiments about films and actors surrounding Oscar nomination and selection).
 The applications of automatic recognition of cate-gories, or tagging, in natural language processing (NLP), range from part of speech tagging to chunk-ing to named entity recognition and complex scien-tific discourse analyses. Currently, there is a variety of tools capable of performing these tasks. A com-monly used approach involves the use of machine learning to first build a statistical model based on a manually or semi-automatically tagged sample data and then to tag new data using this model. Since the machine learning algorithms for building mod-els are well established, the challenge shifted to fea-ture engineering , i.e., developing task-specific fea-tures that form the basis of these statistical models. This task is usually accomplished programmatically which pose an obstacle to a non-technically inclined audience. We alleviate this problem by demonstrat-ing Argo 1 , a web-based platform that allows the user to build NLP and other text analysis workflows via a graphical user interface (GUI) available in a web browser. The system is equipped with an ever grow-ing library of text processing components ranging from low-level syntactic analysers to semantic an-notators. It also allows for including user-interactive components, such as an annotation editor, into oth-erwise fully automatic workflows. The interoper-ability of processing components is ensured in Argo by adopting Unstructured Information Management Architecture (UIMA) (Ferrucci and Lally, 2004) as the system X  X  framework. In this work we explore the capabilities of this framework to support machine Figure 1: Screen capture of Argo X  X  web-based inter-face. Argo X  X  main user interface consists of three panels as shown in Figure 1. The left-hand panel includes user-owned or shared storable objects; the middle panel is a drawing space for constructing workflows and the right-hand panel displays context-dependent information. The storable objects are categorised into workflows , represented as block diagrams of interconnected processing components, documents that represent the user X  X  space intended for upload-ing resources and saving processing results, and ex-ecutions that provide past and live workflow exe-cution details and access points to user-interactive components should such be present in a workflow. Component interoperability in Argo is ensured by UIMA which defines common structures and inter-faces. A typical UIMA processing pipeline consists of a collection reader , a set of analysis engines and a consumer . The role of a collection reader is to fetch a resource (e.g., a text document) and deposit it in a common annotation structure , or CAS , as the sub-ject of annotation. Analysis engines then process the subject of annotation stored in the CAS and populate the CAS with their respective annotations. The con-sumer X  X  role is to transform some or all of the an-notations and/or the subject of annotation from the CAS and serialise it into some storable format.
Readers, analysers and consumers are represented graphically in Argo as blocks with incoming only, incoming and outgoing, and outgoing only ports, re-spectively, visible in the middle of Figure 1. sented as the Feature Generator, CRF++ Trainer and CRF++ Tagger blocks. Figure 2a shows a pro-cess of building a statistical model supported by a document reader, common, well-established pre-processing components (in this case, to establish boundaries of sentences and tokens), and the previ-ously mentioned editor for manually creating anno-tations 6 . The manual annotations serve to generate tags/labels which are used in the training process to-gether with the features produced by Feature Gener-ator. The trained model is then used in the workflow shown in Figure 2b to tag new resources. Although the tagging workflow automatically recognises the labels of interest (based on the model supplied in CRF++ Tagger), in practice, the labels need further correction, hence the use of Annotation Editor after the tagger. 4.1 Training and tagging At present, our implementation of the training and tagging components is based on the conditional ran-dom fields (CRF) (Lafferty et al., 2001). Our choice is dictated by the fact that CRF models are currently one of the best models for tagging and efficient algo-rithms to compute marginal probabilities and n -best sequences are freely available.

We used the CRF++ implementation 7 and wrapped it into two UIMA-compatible components, CRF++ Trainer and CRF++ Tagger. The trainer deals with the optimisation of feature parameters, whereas word observations are produced by Feature Generator, as described in the following section. 4.2 From annotations to features The Feature Generator component is an intermedi-ary between annotations stored in CASes and the training component. This component is customis-able via the component X  X  settings panel, parts of which are shown in Figure 3. The panel allows the user to 1) identify the stream of tokens 8 (Figure 3a), 2) identify the stream of token sequences (usually
Figure 4: UML diagram of transformation types resentation of the token field X  X  value ultimately be-comes the value of the generated feature. If the user declares one or more transformations then these are applied on the token field X  X  value in sequence, i.e., an outcome of the preceding transformation be-comes an input of the following one. Figure 4 shows the various transformations currently available in the system.

Context windows allow for enriching the current token X  X  feature set by introducing observations from surrounding tokens as n-grams. For example, the selected feature definition in Figure 3b,  X  X urface has symbols X , declares the covered text as the feature X  X  basis and defines two transformations and two con-text windows. The two transformations will first transform the covered text to a collapsed shape (e.g.,  X  X F-kappa X  will become  X  X #a X ) and then produce  X  X  X  or  X  X  X  depending on whether the collapsed shape matches the simple regular expression  X # X  (e.g.,  X  X #a X  will become  X  X  X ). The two context win-dows define six unigrams and four bigrams, which will ultimately result in this single feature defini-tion X  X  producing ten observations for training. We show the performance of taggers trained with two distinct sets of features, basic and extended. The basic set of features uses token fields such as the covered text and the part of speech without any transformations or context n-grams. The extended set makes the full use of Feature Generator X  X  settings and enriches the basic set with various transforma-tions and context n-grams. The transformations in-no other information supporting the tokens in the BioNLP/NLPBA dataset. To compensate for it we automatically generated part of speech and chunk la-bels for each token.

The chosen datasets/tasks are by no means an exhaustive set of representative comparative-setup datasets available. Our goal is not to claim the su-periority of our approach over the solutions reported in the respective shared tasks. Instead, we aim to show that our generic setup is comparable to those task-tuned solutions.
 We further explore the options of both Feature Generator and CRF++ Trainer by manipulating la-belling formats (IOB vs IOBES (Kudo and Mat-sumoto, 2001)) for the former and parameter esti-mation algorithms (L 2 -vs L 1 -norm regularisation) for the latter. Ultimately, there are 32 setups as the result of the combinations of the two feature sets, the two datasets, the two labelling formats and the two estimation algorithms. 5.1 Results Table 1 shows the precision, recall and f-scores of our extended-feature setups against each other as well as with reference to the best and baseline solu-tions as reported in the respective shared tasks. The gap to the best performing solution for the chunking task is about 1.3% points in F-score, ahead of the baseline by 15.7% points. Respectively for the NER task, our best setup stands behind the best reported solution by about 7% points, ahead of the baseline by about 18% points. In both instances our solution would be placed in the middle of the reported rank-ings, which is a promising result, especially that our setups are based solely on the tokens X  surface form, part of speech, and (in the case of the NER task) chunk. In contrast, the best solutions for the NER task involve the use of dictionaries and advanced analyses such as acronym resolution.

The tested combinations of the labelling formats and parameter estimation algorithms showed to be inconclusive, with a difference between the best and worst setups of only 0.35% points for both tasks.
The advantage of using the extended set of fea-tures over the basic set is clearly illustrated in Table 2. The performance of the basic set on the chunking dataset is only at the level of the baseline, whereas for the NER task it falls nearly 6% points behind the SRC BB/G53025X/1 From Text to Pathways) and Korea Institute of Science and Technology Informa-tion (KISTI Text Mining and Pathways).

Depending on whether or not parsers are explic-itly used for obtaining linguistically annotated data during training, the systems are also divided into two categories: formally syntax-based systems that do not use additional parsers (Wu, 1997; Chiang, 2005; Xiong et al., 2006), and linguistically syntax-based systems that use PCFG parsers (Liu et al., 2006; Huang et al., 2006; Galley et al., 2006; Mi et al., 2008; Mi and Huang, 2008; Zhang et al., 2009), HPSG parsers (Wu et al., 2010; Wu et al., 2011a), or dependency parsers (Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). A classification 1 of syntax-based SMT systems is shown in Table 1.
Translation rules can be extracted from aligned string-string (Chiang, 2005), tree-tree (Ding and Palmer, 2005) and tree/forest-string (Galley et al., 2004; Mi and Huang, 2008; Wu et al., 2011a) data structures. Leveraging structural and linguis-tic information from parse trees/forests, the latter two structures are believed to be better than their string-string counterparts in handling non-local re-ordering, and have achieved promising translation results. Moreover, the tree/forest-string structure is more widely used than the tree-tree structure, pre-sumably because using two parsers on the source and target languages is subject to more problems than making use of a parser on one language, such as the shortage of high precision/recall parsers for languages other than English, compound parse error rates, and inconsistency of errors. In Table 1, note that tree-to-string rules are generic and applicable to many syntax-based models such as tree/forest-to-izes word order mistakes.
 In this table, Akamon-Forest differs from Akamon-Comb by using different configurations: Akamon-Forest used only 2/3 of the total training data (limited by the experiment environments and time). Akamon-Comb represents the system com-bination result by combining Akamon-Forest and other phrase-based SMT systems, which made use of pre-ordering methods of head finalization as de-scribed in (Isozaki et al., 2010b) and used the total 3 million training data. The detail of the pre-ordering approach and the combination method can be found in (Sudoh et al., 2011) and (Duh et al., 2011).
Also, Moses (hierarchical) stands for the hi-erarchical phrase-based SMT system and Moses (phrase) stands for the flat phrase-based SMT sys-tem. For intuitive comparison (note that the result achieved by Google is only for reference and not a comparison, since it uses a different and unknown training data) and following (Goto et al., 2011), the scores achieved by using the Google online transla-tion system 4 are also listed in this table.

Here is a brief description of Akamon X  X  main fea-tures:  X  multiple-thread forest-based decoding: Aka- X  translation rule extraction: as former men-Figure 1 shows the training and tuning progress of the Akamon system. Given original bilingual par-allel corpora, we first tokenize and lowercase the source and target sentences (e.g., word segmentation of Chinese and Japanese, punctuation segmentation of English).

The pre-processed monolingual sentences will be used by SRILM (Stolcke, 2002) or BerkeleyLM (Pauls and Klein, 2011) to train a n-gram language model. In addition, we filter out too long sentences factored representation of the syntactic features of a word/phrase, as well as a representation of their semantic content. Phrases and words represented by signs are composed into larger phrases by applica-tions of schemata . The semantic representation of the new phrase is calculated at the same time. As such, an HPSG parse tree/forest can be considered as a tree/forest of signs (c.f. the HPSG forest in Fig-ure 2 in (Wu et al., 2010)).

An HPSG parse tree/forest has two attractive properties as a representation of a source sentence in syntax-based SMT. First, we can carefully control the condition of the application of a translation rule by exploiting the fine-grained syntactic description in the source parse tree/forest, as well as those in the translation rules. Second, we can identify sub-trees in a parse tree/forest that correspond to basic units of the semantics, namely sub-trees covering a pred-icate and its arguments, by using the semantic rep-resentation given in the signs. Extraction of trans-lation rules based on such semantically-connected sub-trees is expected to give a compact and effective set of translation rules.

A sign in the HPSG tree/forest is represented by a typed feature structure (TFS) (Carpenter, 1992). A TFS is a directed-acyclic graph (DAG) wherein the edges are labeled with feature names and the nodes  X  the training script on translation rule extraction,  X  the MERT script on feature weight tuning on a  X  the decoding script on a test set.

Based on Akamon, there are a lot of interesting directions left to be updated in a relatively fast way in the near future, such as:  X  integrate target dependency structures, espe- X  better pruning strategies for the input packed  X  derivation-based combination of using other  X  taking other evaluation metrics as the opti-We thank Yusuke Miyao and Naoaki Okazaki for their invaluable help and the anonymous reviewers for their comments and suggestions.
 (1) Discussant 1: I believe that school uniform is a good idea because it improves student attendance. (2) Discussant 2: I disagree with you. School uniform is a bad idea because people cannot show their person-ality.

In (1), the writer is expressing positive attitude regarding school uniform. The writer of (2) is ex-pressing negative attitude (disagreement) towards the writer of (1) and negative attitude with respect to the idea of school uniform. It is clear from this short dialog that the writer of (1) and the writer of (2) are members of two opposing subgroups. Dis-cussant 1 supports school uniform, while Discussant 2 is against it.

In this demo, we present an unsupervised system for determining the subgroup membership of each participant in a discussion. We use linguistic tech-niques to identify attitude expressions, their polar-ities, and their targets. We use sentiment analy-sis techniques to identify opinion expressions. We use named entity recognition, noun phrase chunk-ing and coreference resolution to identify opinion targets. Opinion targets could be other discussants or subtopics of the discussion topic. Opinion-target pairs are identified using a number of hand-crafted rules. The functionality of this system is based on our previous work on attitude mining and subgroup detection in online discussions.

This work is related to previous work in the areas of sentiment analysis and online discussion mining. Many previous systems studied the problem of iden-
The polarity of a word is usually affected by the context in which it appears. For example, the word fine is positive when used as an adjective and neg-ative when used as a noun. For another example, a positive word that appears in a negated context be-comes negative. To address this, we take the part-of-speech (POS) tag of the word into consideration when we assign word polarities. We require that the POS tag of a word matches the POS tag provided in the list of polarized words that we use. The negation issue is handled in the opinion-target pairing step as we will explain later.

The next step in the pipeline is to identify the can-didate targets of opinion in the discussion. The tar-get of attitude could be another discussant, an entity mentioned in the discussion, or an aspect of the dis-cussion topic. When the target of opinion is another discussant, either the discussant name is mentioned explicitly or a second person pronoun (e.g you, your, yourself) is used to indicate that the opinion is tar-geting the recipient of the post.

The target of opinion could also be a subtopic or an entity mentioned in the discussion. We use two methods to identify such targets. The first method depends on identifying noun groups (NG). We con-sider as an entity any noun group that is mentioned by at least two different discussants. We only con-sider as entities noun groups that contain two words or more. We impose this requirement because in-dividual nouns are very common and considering all of them as candidate targets will introduce sig-nificant noise. In addition to this shallow pars-ing method, we also use named entity recognition (NER) to identify more targets. The named en-tity tool that we use recognizes three types of en-tities: person, location, and organization. We im-pose no restrictions on the entities identified using this method.

A challenge that always arises when perform-ing text mining tasks at this level of granularity is that entities are usually expressed by anaphori-cal pronouns. Jakob and Gurevych (2010) showed experimentally that resolving the anaphoric links neg dependency relation.

It is likely that the same participant P i expresses sentiment towards the same target T R k multiple times in different sentences in different posts. We keep track of the counts of all the instances of posi-tive/negative attitude P i expresses toward T R k . We represent this as P i m +  X  X  X  X  number of times P i expressed positive (negative) at-titude toward T R k .

Now, we have information about each discussant attitude. We propose a representation of discus-sants  X  attitudes towards the identified targets in the discussion thread. As stated above, a target could be another discussant or an entity mentioned in the discussion. Our representation is a vector contain-ing numerical values. The values correspond to the counts of positive/negative attitudes expressed by the discussant toward each of the targets. We call this vector the discussant attitude profile (DAP) . We construct a DAP for every discussant. Given a dis-cussion thread with d discussants and e entity tar-gets, each attitude profile vector has n = ( d + e )  X  3 dimensions. In other words, each target (discussant or entity) has three corresponding values in the DAP: 1) the number of times the discussant expressed pos-itive attitude toward the target, 2) the number of times the discussant expressed a negative attitude to-wards the target, and 3) the number of times the the discussant interacted with or mentioned the target. It has to be noted that these values are not symmet-provides full access to the system functionality. It can be used to run the whole pipeline to detect sub-groups or any portion of the pipeline. For example, it can be used to tag an input text with polarity or to identify candidate targets of opinion in a given in-put. The system behavior can be controlled by pass-ing arguments through the command line interface. For example, the user can specify which clustering algorithm should be used.

To facilitate using the system for research pur-poses, the system comes with a clustering evaluation component that uses the ClusterEvaluator package. 4 . If the input to the system contains subgroup labels, it can be run in the evaluation mode in which case the system will output the scores of several different clustering evaluation metrics such as purity, entropy, f-measure, Jaccard, and RandIndex. The system also has a Java API that can be used by researchers to de-velop other systems using our code.

The system can process any discussion thread that is input to it in a specific format. The format of the input and output is described in the accompa-nying documentation. It is the user responsibility to write a parser that converts an online discussion thread to the expected format. However, the sys-tem package comes with two such parsers for two different discussion sites: www.politicalforum.com and www.createdebate.com.

The distribution also comes with three datasets We presented a demonstration of a discussion min-ing system that uses linguistic analysis techniques to predict the attitude the participants in online discus-sions forums towards one another and towards the different aspects of the discussion topic. The system is capable of analyzing the text exchanged in dis-cussions and identifying positive and negative atti-tudes towards different targets. Attitude predictions are used to assign a subgroup membership to each participant using clustering techniques. The sys-tem predicts attitudes and identifies subgroups with promising accuracy.
 Razvan Bunescu and Raymond Mooney. 2005. A short-est path dependency kernel for relation extraction. In
Proceedings of Human Language Technology Confer-ence and Conference on Empirical Methods in Nat-ural Language Processing , pages 724 X 731, Vancou-ver, British Columbia, Canada, October. Association for Computational Linguistics.
 Amitava Das and Sivaji Bandyopadhyay. 2011. Dr sen-timent knows everything! In Proceedings of the ACL-
HLT 2011 System Demonstrations , pages 50 X 55, Port-land, Oregon, June. Association for Computational
Linguistics.
Automatic detection and classification of the er-rors produced by MT systems is a challenging prob-lem. The cause of such errors may depend not only on the translation paradigm adopted, but also on the language pairs, the availability of enough linguistic resources and the performance of the linguistic pro-cessors, among others. Several past research works studied and defined fine-grained typologies of trans-lation errors according to various criteria (Vilar et al., 2006; Popovi  X  c et al., 2006; Kirchhoff et al., 2007), which helped manual annotation and human analysis of the systems during the MT development cycle. Recently, the task has received increasing at-tention towards the automatic detection, classifica-tion and analysis of these errors, and new tools have been made available to the community. Examples of such tools are AMEANA (Kholy and Habash, 2011), which focuses on morphologically rich lan-guages, and Hjerson (Popovi  X  c, 2011), which ad-dresses automatic error classification at lexical level.
In this work we present an online graphical inter-face to access A SIYA , an existing software designed to evaluate automatic translations using an heteroge-neous set of metrics and meta-metrics. The primary goal of the online interface is to allow MT develop-ers to upload their test beds, obtain a large set of met-ric scores and then, detect and analyze the errors of their systems using just their Internet browsers. Ad-ditionally, the graphical interface of the toolkit may help developers to better understand the strengths and weaknesses of the existing evaluation measures and to support the development of further improve-ments or even totally new evaluation metrics. This information can be gathered both from the experi-tion they use and the names of a few examples 1 . Lexical similarity: n -gram similarity and edit dis-Syntactic similarity: based on part-of-speech tags, Semantic similarity: based on named entities, se-
Such heterogeneous set of metrics allow the user to analyze diverse aspects of translation quality at system , document and sentence levels. As discussed in (Gim  X  enez and M ` arquez, 2008), the widely used lexical-based measures should be considered care-fully at sentence level, as they tend to penalize trans-lations using different lexical selection. The combi-nation with complex metrics, more focused on ad-equacy aspects of the translation (e.g., taking into account also semantic information), should help re-ducing this problem. A
SIYA operates over a fixed set of translation test cases, i.e., a source text, a set of candidate trans-lations and a set of manually produced reference translations. To run A SIYA the user must provide a test case and select the preferred set of metrics (it may depend on the evaluation purpose). Then, A
SIYA outputs complete tables of score values for all the possible combination of metrics, systems, documents and segments. This kind of results is valuable for rapid evaluation and ranking of trans-lations and systems. However, it is unfriendly for MT developers that need to manually analyze and compare specific aspects of their systems.

During the evaluation process, A SIYA generates a number of intermediate analysis containing par-tial work outs of the evaluation measures. These data constitute a priceless source for analysis pur-poses since a close examination of their content al-lows for analyzing the particular characteristics that Syntactic information. A SIYA considers three lev-els of syntactic information: shallow, constituent and dependency parsing. The shallow parsing an-notations, that are obtained from the linguistic pro-cessors, consist of word level part-of-speech, lem-mas and chunk Begin-Inside-Outside labels. Use-ful figures such as the matching rate of a given (sub)category of items are the base of a group of metrics (i.e., the ratio of prepositions between a reference and a candidate). In addition, depen-dency and constituency parse trees allow for captur-ing other aspects of the translations. For instance, DP-HCWM is a specific subset of the dependency measures that consists of retrieving and matching all the head-word chains (or the ones of a given length) from the dependency trees. Similarly, CP-STM, a subset of the constituency parsing family of mea-sures, consists of computing the lexical overlap ac-cording to the phrase constituent of a given type. Then, for error analysis purposes, parse trees com-bine the grammatical relations and the grammati-cal categories of the words in the sentence and dis-play the information they contain. Figure 2 and 3 show, respectively, several annotation levels of the sentences in the example and the constituency trees. Semantic information. A SIYA distinguishes also three levels of semantic information: named enti-ties, semantic roles and discourse representations. The former are post-processed similarly to the lex-ical annotations discussed above; and the semantic predicate-argument trees are post-processed and dis-played in a similar manner to the syntactic trees. Instead, the purpose of the discourse representation analysis is to evaluate candidate translations at doc-ument level. In the nested discourse structures we could identify the lexical choices for each discourse sub-type. Presenting this information to the user re-mains as an important part of the future work. This section presents the web application that makes possible a graphical visualization and interactive ac-cess to A SIYA . The purpose of the interface is twofold. First, it has been designed to facilitate the use of the A SIYA toolkit for rapid evaluation of test beds. And second, we aim at aiding the analysis of the errors produced by the MT systems by creating Figure 4: The bar charts plot to compare the metric scores for several systems tion can swap among the three levels of granularity, and it can also be transposed with respect to sys-tem and metric information (transposing rows and columns). When the metric basis table is shown, the user can select one or more metric columns in or-der to re-rank the rows accordingly. Moreover, the source, reference and candidate translation are dis-played along with metric scores. The combination of all these functionalities makes it easy to know which are the highest/lowest-scored sentences in a test set.
We have also integrated a graphical library 2 to generate real-time interactive plots to show the met-ric scores graphically. The current version of the in-terface shows interactive bar charts, where different metrics and systems can be combined in the same plot. An example is shown in Figure 4. 4.2 Graphically-aided Error Analysis and Human analysis is crucial in the development cy-cle because humans have the capability to spot er-rors and analyze them subjectively, in relation to the underlying system that is being examined and the scores obtained. Our purpose, as mentioned previ-ously, is to generate a graphical representation of the information related to the source and the trans-lations, enabling a visual analysis of the errors. We have focused on the linguistic measures at the syn-tactic and semantic level, since they are more robust than lexical metrics when comparing systems based on different paradigms. On the one hand, one of the views of the interface allows a user to navigate and inspect the segments of the test set. This view highlights the elements in the sentences that match a genre, style, dialect). In contrast, Farr  X  us et al. (2011) classify the errors that arise during Spanish-Catalan translation at several levels: orthographic, morpho-logical, lexical, semantic and syntactic errors.
Works towards the automatic identification and classification of errors have been conducted very re-cently. Examples of these are (Fishel et al., 2011), which focus on the detection and classification of common lexical errors and misplaced words using a specialized alignment algorithm; and (Popovi  X  c and Ney, 2011), which addresses the classifica-tion of inflectional errors, word reordering, missing words, extra words and incorrect lexical choices us-ing a combination of WER, PER, RPER and HPER scores. The AMEANA tool (Kholy and Habash, 2011) uses alignments to produce detailed morpho-logical error diagnosis and generates statistics at dif-ferent linguistic levels. To the best of our knowl-edge, the existing approaches to automatic error classification are centered on the lexical, morpho-logical and shallow syntactic aspects of the transla-tion, i.e., word deletion, insertion and substitution, wrong inflections, wrong lexical choice and part-of-speech. In contrast, we introduce additional lin-guistic information, such as dependency and con-stituent parsing trees, discourse structures and se-mantic roles. Also, there exist very few tools de-voted to visualize the errors produced by the MT systems. Here, instead of dealing with the automatic classification of errors, we deal with the automatic selection and visualization of the information used by the evaluation measures. The main goal of the A SIYA toolkit is to cover the evaluation needs of researchers during the develop-ment cycle of their systems. A SIYA generates a number of linguistic analyses over both the candi-date and the reference translations. However, the current command-line interface returns the results only in text mode and does not allow for fully ex-ploiting this linguistic information. We present a graphical interface showing a visual representation of such data for monitoring the MT development cy-cle. We believe that it would be very helpful for car-rying out tasks such as error analysis, system com-parison and graphical representations. strategy is obvious. First, since the length of search query is limited , suspicious sentence s are usually queried and examined indepen dently . T herefore, it is harder to identify document level plagia rism than sentence level plagiarism . Second, manually check ing whether a query sentence plagia rizes certain website s requires specific domain and language knowledge a s well as considerable amount of energy and time. To overcome the above shortcomings , we introduce an online plagiarism detection system using natural language processing techniques to simulate the above reverse -engineering approach. We develop a n ensemble framework that integrates lexical, synta ctic and semantic features to achieve th is goal. Our system is language independent and we have implemented both Chinese and English versions for evaluation. Plagiarism detection has been widely discussed in the past decades ( Zou et al., 2010) . Table 1 . summarize s some of them : to complete ly match to the query sentence . This strategy allows us to not only identify the copy/paste type of plagiarism but also re -wr i te/edit type of plagiarism. 3.2 Sentence -b ased Plagiarism Detection Since not all outputs of a search engine contain an exact copy of the query , we need a model to quantify how likely each of them is the source of plagiarism . For better efficiency, our experiment exploits the sni ppet of a search output to represent the whole document. That is, we want to measure how likely a snipp et is the plagiarized source of the query. We designed several model s which utilized rich lexical, syntactic and semantic features to pur sue this goal, and the details are discussed below . 3.2.1 Ngram Matching (NM) One straightforward measure is to exploit the n -gram similarity between source and target texts. We first enumerate all n -grams in source, and then calculate the overlap percentage wi th the n -grams in the target. The larger n is, t he harder for this feature to detect plagiarism with insert ion , replacement, and deletion. In the experiment, we choose n=2. 3.2.2 Reordering of Words (RW) P lagiari sm can come from the reordering of words. W e argue that the permutation distance between S 1 and S 2 is an important indicator for reordered plagiarism. Th e permutation distance is defined as the minimum number of pair -wise exchanging of matched words needed to transform a sentence, S 2 , to contain the same order of matched words as another sentence , S 1 . As mentioned in ( S X rensena and Sevaux , 2005) , the permutation distance can be calculated by the following expression w here
S 1 (i) and S 2 (i) are ind ices of the i th matched word in sentence s S 1 and S 2 respectively and n is the number of matched words between the sentence s S 1 and S 2 . Let  X  = n 2  X  n normalized term, which is the maximum possible distance between S 1 and S 2 , t hen the reordering a Noun Phrase can become a Pr e position Phrase , i.e.  X  by ... X  , in the passive form while the object in a Verb Phrase can become a new subject in a Noun Phrase. Here we utilize the Stanford D ependency provide d by Stanford P arser to match the tag/phrase between active and passive sentences . 3.2.5 Semantic Similarity ( LDA ) Plagiarists, sometimes, chang e words or phrases to those with similar meanings. While previous works ( Y. Lin et al . , 2006) often explore semantic similarity using lexical databases s u c h a s WordNet to find synonyms, w e exploit a topic model, specifically latent Dirichlet allocation (LDA , D. M. Blei et al . , 2003 ) , to extract the semantic features of sentences. Given a set of documents represented by their word sequences , and a topic number n, LDA learns the word distribution for each topic and the topic distribution for each document which maximize the likelihood of the word co -occurrence in a document. The topic distribution is often taken as semantic s of a document. We use LDA to obtain the topic distribution of a query and a candidate snippet, and compare the cosine similarity of them as a measure of their semantic similarity. 3.3 Ensemble Similarity Score s Up to this point, for each snippet the system generates six similarity scores to measure the degree of plagiarism in different aspects. In this stage, we propose two strategies to linearly combine the scores to make better prediction. The firs t strategy utilizes each model X  X  predictability (e.g. accuracy) as the weight to linear ly combine the scores. In other words, the models that perform better individually will obtain higher weights. In the second strategy we exploit a learning model (in the experiment section we use Liblinear) to learn the weights directly. 3.4 Document Le vel Plagiarism Detection For each query from the input article, our system assigns a degree -of -plagiarism score to some plausible source URLs. Then, for each URL, the system sums up all the scores it obtains as the final score for document -level degree -of -plagiarism. We set up a cutoff threshold to obtain the most plausible URLs. At the end, our system highlights the suspicious areas of plagiarism for display. an ensemble of three features outperforms the state -of -the -art by 26%.
 4.2 Evaluating the Full System
To evaluate the overall system, w e manually collect 60 real -world review articles from the Internet for books (20), movies (20), and music albums (20) . Unfortunately for an online system like ours, there is no ground truth available for recall measure. We conduct two differement evalautions. First we use the 60 articles as inputs to our system, ask 5 human annotators to check whether the articles returned by our system can be considered as plagiarism. Among all 60 review articles, o ur system identifi es a considerable ly high number of copy/paste articles, 231 in total . However, i dentifying this type of plagiarism is trivial, and has been done by many similar tools. Instead we focus on the so -called smart -plagiarism which cannot be found through quoting a query in a searc h engine . Table 4 . shows the precision of the smart -plag i arism articles returned by our system. The precision is very high and outperforms a commertial tool Microsoft Plagiarism Detector .
In the second evaluation, we first choose 30 reviews randomly. Then we use each of them as queries into Google and retrieve a total of 5 636 pieces of snippet candidate s . We then ask 63 human beings to annotate whether those snippet s represent plagiarism case s of the original review article. Eventually we have obtained an annotated 
S amples of our system  X  s finding can be found here, http://tinyurl.com/6pnhurz We developed an online demos system using JAVA (JDK 1.7). The system currently supports the detection of documents in both English and Chinese. Users can either upload the plain text file of a suspicious document, or copy/paste the content onto the text area , as shown below in Fig ure 2 .

Then the system will output some URLs and snippets as the pote ntial source of plagiarism . ( see Figure 3 . )
Comparing with other online plagiarism detection systems, ours exploit more sophisticate d features by modeling how human beings plagiari ze online sources . We have exploited sentence -level plagiarism detection on lexical, syntactic and semantic levels. Another noticeable fact is that our approach is almo st language independent. Given a parser and a POS tagger of a language, our framework can be extended to support plagiarism detection for that language. However, a more multilingual resource equipped with an easy-to-use API would not only enable us to perform all of the aforementioned tasks in additional languages, but also to explore cross-lingual applica-tions like cross-lingual IR (Etzioni et al., 2007) and machine translation (Chatterjee et al., 2005).
This paper describes a new API that makes lexical knowledge about millions of items in over 200 lan-guages available to applications, and a correspond-ing online user interface for users to explore the data. We first describe link prediction techniques used to create the multilingual core of the knowledge base with word sense information (Section 2). We then outline techniques used to incorporate named enti-ties and specialized concepts (Section 3) and other types of knowledge (Section 4). Finally, we describe how the information is made accessible via a user in-terface (Section 5) and a software API (Section 6). UWN (de Melo and Weikum, 2009) is based on WordNet (Fellbaum, 1998), the most popular lexi-cal knowledge base for the English language. Word-Net enumerates the senses of a word, providing a short description text (gloss) and synonyms for each meaning. Additionally, it describes relationships be-tween senses, e.g. via the hyponymy/hypernymy re-lation that holds when one term like  X  publication  X  is a generalization of another term like  X  journal  X .
This model can be generalized by allowing words in multiple languages to be associated with a mean-ing (without, of course, demanding every meaning be lexicalized in every language). In order to ac-complish this at a large scale, we automatically link senses. The feature space is constructed using a se-ries of graph-based statistical scores that represent properties of the previous graph G i  X  1 and addition-ally make use of measures of semantic relatedness and corpus frequencies. The most salient features x ( x,z ) are of the form: The formulae consider the out-neighbourhood y  X   X ( x,G i  X  1 ) of x , i.e. its translations, and then ob-serve how strongly each y is tied to z . The function sim  X  computes the maximal similarity between any sense of y and the current sense z . The dissim func-tion computes the sum of dissimilarities between senses of y and z , essentially quantifying how many alternatives there are to z . Additional weighting functions  X  ,  X  are used to bias scores towards senses that have an acceptable part-of-speech and senses that are more frequent in the SemCor corpus.

Relying on multiple iterations allows us to draw on multilingual evidence for greater precision and recall. For instance, after linking the German  X  Fled-ermaus  X  to the animal sense of  X  bat  X , we may be able to infer the same for the Turkish translation  X  yarasa  X . Results We have successfully applied these tech-niques to automatically create UWN, a large-scale multilingual wordnet. Evaluating random samples of term-sense links, we find (with Wilson-score in-tervals at  X  = 0 . 05 ) that for French the preci-sion is 89 . 2%  X  3 . 4% (311 samples), for German 85 . 9%  X  3 . 8% (321 samples), and for Mandarin Chinese 90 . 5%  X  3 . 3% (300 samples). The over-all number of new term-sense links is 1,595,763, for 822,212 terms in over 200 languages. These figures can be grown further if the input is extended by tap-ping on additional sources of translations. The UWN Core is extended by incorporating large amounts of named entities and language-and domain-specific concepts from Wikipedia (de Melo and Weikum, 2010a). In the process, we also obtain While obtaining an exact solution is NP-hard and APX-hard, we can solve the corresponding Linear Program using a fast LP solver like CPLEX and sub-sequently apply region growing techniques to obtain a solution with a logarithmic approximation guaran-tee (de Melo and Weikum, 2010b).

The clean connected components resulting from this process can then be merged to form aggregate entities. For instance, given WordNet X  X  standard sense for  X  fog  X , water vapor, we can check which other items are in the connected component and transfer all information to the WordNet entry. By extracting snippets of text from the beginning of Wikipedia articles, we can add new gloss descrip-tions for fog in Arabic, Asturian, Bengali, and many other languages. We can also attach pictures show-ing fog to the WordNet word sense.
 Taxonomy Induction The above process con-nects articles to their counterparts in WordNet. In the next step, we ensure that articles without any di-rect counterpart are linked to WordNet as well, by means of taxonomic hypernymy/instance links (de Melo and Weikum, 2010a).

We generate individual hypotheses about likely parents of entities. For instance, articles are con-nected to their Wikipedia categories (if these are not assessed to be mere topic descriptors) and categories are linked to parent categories, etc. In order to link categories to possible parent hypernyms in Word-Net, we adapt the approach proposed for YAGO (Suchanek et al., 2007) of determining the headword of the category name and disambiguating it.

Since we are dealing with a multilingual scenario that draws on articles from different multilingual Wikipedia editions that all need to be connected to WordNet, we apply an algorithm that jointly looks at an entity and all of its parent candidates (not just from an individual article, but all articles in the same connected component) as well as superordinate par-ent candidates (parents of parents, etc.), as depicted in Figure 2. We then construct a Markov chain based on this graph of parents that also incorporates the possibility of random jumps from any parent back to the current entity under consideration. The sta-tionary probability of this Markov chain, which can be obtained using random walk methods, provides us a ranking of the most likely parents. cule  X  for  X  minuscule  X ), pronunciation information (e.g. how to pronounce  X  nuclear  X ), and so on. Frame-Semantic Knowledge Frame semantics is a cognitively motivated theory that describes words in terms of the cognitive frames or scenarios that they evoke and the corresponding participants in-volved in them. For a given frame, FrameNet provides definitions, involved participants, associ-ated words, and relationships. For instance, the Commerce_goods-transfer frame normally involves a seller and a buyer, among other things, and different words like  X  buy  X  and  X  sell  X  can be cho-sen to describe the same event.

Such detailed knowledge about scenarios is largely complementary in nature to the sense re-lationships that WordNet provides. For instance, WordNet emphasizes the opposite meaning of the words  X  happy  X  and  X  unhappy  X , while frame seman-tics instead emphasizes the cognitive relatedness of words like  X  happy  X ,  X  unhappy  X ,  X  astonished  X , and  X  amusement  X , and explains that typical participants include an experiencer who experiences the emo-tions and external stimuli that evoke them. There have been individual systems that made use of both forms of knowledge (Shi and Mihalcea, 2005; Cop-pola and others, 2009), but due to their very different nature, there is currently no simple way to accom-plish this feat. Our system addresses this by seam-lessly integrating frame semantic knowledge into the system. We draw on FrameNet (Baker et al., 1998), the most well-known computational instantiation of frame semantics. While the FrameNet project is generally well-known, its use in practical applica-2010), which contains multilingual synsets but does not connect named entities from Wikipedia to them in a multilingual taxonomy.
 Our goal is to make the knowledge that we have de-rived available for use in applications. To this end, we have developed a fully downloadable API that can easily be used in several different programming languages. While there are many existing APIs for WordNet and other lexical resources (e.g. (Judea et al., 2011; Gurevych and others, 2012)), these don X  X  provide a comparable degree of integrated multilin-gual and taxonomic information.
 Interface The API can be used by initializing an accessor object and possibly specifying the list of plugins to be loaded. Depending on the particular application, one may choose only Princeton Word-Net and the UWN Core, or one may want to in-clude named entities from Wikipedia and frame-semantic knowledge derived from FrameNet, for in-stance. The accessor provides a simple graph-based lookup API as well as some convenience methods for common types of queries.

An additional higher-level API module imple-ments several measures of semantic relatedness. It also provides a simple word sense disambiguation method that, given a tokenized text with part-of-composing and revising writing. Different from existing tools, its context-sensitive and first-language-oriented features enable EFL writers to concentrate on their ideas and thoughts without being hampered by the limited lexical resources. Based on the studies that first language use can positively affect second language composing, FLOW attempts to meet such needs. Given any L1 input, FLOW displays appropriate suggestions including translation, paraphrases, and n-grams during composing and revising processes. We use the following example sentences to illustrate these two functionalities. 
Consider the sentence  X  We propose a method to  X . During the composing stage, suppose a writer is unsure of the phrase  X  solve the problem  X , he could write  X   X  X  X  X  X   X , a corresponding word in his native language, like  X  We propose a method to  X  X  X  X  X   X . The writer X  X  input in the writing area of FLOW actively triggers a set of translation suggestions such as  X  solve the problem  X  and  X  tackle the problem  X  for him/her to complete the sentence. 
In the revising stage, the writer intends to improve or correct the content. He/She is likely to change the sentence illustrated above into  X  We try all means to solve the problem . X  He would select the phrase  X  propose a method  X  in the original sentence and input a L1 phrase  X   X  X  X   X  , which specifies the meaning he prefers. The L1 input triggers a set of context-aware suggestions corresponding to the translations such as  X  try our best  X  and  X  do our best  X  rather than  X  try your best  X  and  X  do your best  X . The system is able to do that mainly by taking a context-sensitive approach. FLOW then inserts the phrase the writer selects into the sentence. paraphrasing demonstrates the strength of semantic equivalence. Another line of research further considers context information to improve the performance. Instead of addressing the issue of local paraphrase acquisition, Max (2009) utilizes the source and target contexts to extract sub-sentential paraphrases by using pivot SMT systems. 2.2 N-gram suggestions After a survey of several existing writing tools, we focus on reviewing two systems closely related to our study. 
PENS (Liu et al, 2000), a machine-aided English writing system, provides translations of the corresponding English words or phrases for writers X  reference. Different from PENS, FLOW further suggests paraphrases to help writers revise their writing tasks. While revising, writers would alter the use of language to express their thoughts. The suggestions of paraphrases could meet their need, and they can reproduce their thoughts more fluently. 
Another tool, TransType (Foster, 2002), a text editor, provides translators with appropriate translation suggestions utilizing trigram language model. The differences between our system and TransType lie in the purpose and the input. FLOW aims to assist EFL writers whereas TransType is a tool for skilled translators. On the other hand, in TransType, the human translator types translation of a given source text, whereas in FLOW the input, where we use the user-composed context { e 1 , e 2 , Although there exist more sophisticated models which could make a better prediction, a simple na X ve-Bayes model is shown to be accurate and efficient in the lexical disambiguation task according to (Yarowsky and Florian, 2002). Therefore, in this paper, a na X ve-Bayes model is used to disambiguate the translation of f . In addition to the context-word feature, we also use the context-syntax feature, namely surrounding POS tag Pos , to constrain the syntactic structure of the prediction. The TB-NP model could be represented in the following equation: According to the Bayes theorem, The probabilities can be estimated using a parallel corpus, which is also used to obtain bilingual phrase alignment. 3.3 Paraphrase Suggestion Unlike the N-gram prediction, in the paraphrase ...e k }, which he/she wants to paraphrase. The model takes the m words { r 1 , r 2 , ... r m } and n words { l selected k words respectively. The system also accepts an additional foreign language input, { f 1 , f 2 , ... f l }, which helps limit the meaning of suggested paraphrases to what the user really wants. The output would be a set of paraphrase suggestions that the user-selected phrases can be replaced by those paraphrases precisely. Context-Sensitive Paraphrase Suggestion (CS-PS) The CS-PS model first finds a set of local paraphrases P of the input phrase K using the Both of the paraphrase models CS-PS and TB-PS perform quite well in assisting the user in the writing task. However, there are still some problems such as the redundancy suggestions, e.g.,  X  look forward to  X  and  X  looked forward to  X . Besides, although we used the POS tags as features, the syntactic structures of the suggestions are still not consistent to an input or selected phrases. The CS-NP and the TB-NP model also perform a good task. However, the suggested phrases are usually too short to be a semantic unit. The disambiguation model tends to produce shorter phrases because they have more common context features. 1 Introduction The primary function of S.E.R. technology is simple and clear: as a realtime risk control management technology to assist monitoring huge amount of new media related information and giving a warning for utility users X  sake in efficiency way. crawling all new media based information data relating to the client 24-hour a day so that the influential opinion/reports can be monitored, recorded, conveniently analyzed and more importantly is to send a warning signal before the issue outburst and ruining the authorities X  reputation. These monitor and alert services are based on the socialnomics theory and provide two main sets of service functionalities to clients for access online: Monitor and alert of new media related information under the concept of cloud computing including two functionalities. the dramatic growth of Web X  X  popularity, time becomes the most crucial factor. Monitoring functionalities of S.E.R. technology provides an access to the service platform realtime and online. All scalable mass social data coming from social network, forum, news portals, blogosphere of its login time, its social account and the content are monitored and recorded. In order to find key a concept space is appropriate for representing relations among people, article and keywords. A concept space is graph of terms occurring within objects linked to each other by the frequency with which they occur together. Hsieh (2009) explored the possibility of discovering relations between tags and bookmarks in a folksonomy system. By applying concept space, the relationship of topic can be measured by two keyword sets. identify the characteristic. One of the indicators is used to define the opinion in blog entries which is  X  X logs tend to have certain levels of topic consistency among their blog entries. X  The indicator uses the KL distance to identify the similarity of blog entries (Song, 2007). However, the opinion blog is easy to read and do not change their blog topics iteratively, this is the key factor that similarity comparison can be applied on this feature. 2.2 Opinion Discovery in Blog Entries The numbers of online comments on products or subjects grow rapidly. Although many comments are long, there are only a few sentences containing distinctive opinion. Sentiment analysis is often used to extract the opinions in blog pages. aspects such as a word. The semantic relationship between opinion expression and topic terms is emphasized (Bo, 2004). It means that using the polarity of positive and negative terms in order to present the sentiment tendency from a document. Within a given topic, similarity approach is often used to classify the sentences as opinions. Similarity approach measures sentence similarity based on shared words and synonym words with each sentence in documents and makes an average score. According to the highest score, the sentences can assign to the sentiment or opinion category (Varlamis, 2008). aspects of language used to express opinions and evaluation. Subjectivity classification can prevent the polarity classifier from considering irrelevant misleading text. Subjectivity detection can compress comments into much shorter sentences which still retain its polarity information comparable to the entire comments (Rosario, 2004; Yu, 2003). 3.1 Bilingual Sentiment Opinion Analysis BSOA technique under the S.E.R. technology is implemented along with lexicon based and domain knowledge. The research team starts with concept expansion technique for building up a measurable keyword network. By applying particularly Polysemy Processing Double negation Processing Adverb of Degree Processing sophisticated algorithm as shown in Figure 1, so that to rule out the irrelevant factors in an accurate and efficiency way. 
Aim at the Chinese applications; we develop the system algorithm based on the specialty of Chinese language. The key approach crawl the hidden sentiment linking words, and then to build the association set. We can, therefore, identify feature-oriented sentiment orientation of opinion more conveniently and accurately by using this association set analysis. 3.2 Social Network Influence Analysis Who are the key opinion leaders in the opinion world? How critical do the leaders diffusion power matters? Who do they influence? The more information we have, so as the social networking channels, the more obstacles of monitoring and finding the real influential we are facing right now. 
Within a vast computer network, the individual computers are on what so-called the periphery of the network. Those nodes who have many links pointing to them is not always the most influential in the group. We use a more sophisticated algorithm that takes into account both the direct and indirect links in the network. This SNIA technique under the S.E.R. technology provides a more accurate evaluation and prediction of who really influences thought and affects the whole. Using the same algorithm, in reverse, we can announcement in June. 1st, 2011 under the pressure of the outbreak of Taiwan X  X  food contamination storm, which in general estimated causing NT$10,000 million approximately profit lost in Taiwan X  X  food industry. This DEPH website was to use the S.E.R. technology not only to collect 5 authorities X  data (Food and Drug Administration of Health Department in Executive Yuan, Taipei City government) 24 hours a day but also gathering 3 news portals  X  Google, Yahoo, and UDN, 303 web online the latest news information approx., allowed every personal could instantly check whether their everyday food/drink has or failed passing the toxin examination by simply key-in any related words (jelly, orange juice, bubble tea). This website was highly recommended by the Ministry of Economic Affairs because of it fundamentally eased people X  X  fear at the time. 4.2 Brand/Product Monitoring A world leading smart phone company applying the S.E.R Technology service platform to set up its customer relationship management (CRM) platform for identifying the undiscovered product-defects issues, monitoring the web-opinion trends that targeting issues between its own and competitor X  X  products/services mostly. This data processing and analyzing cost was accordingly estimated saving 70 % cost approximately. 4.3 Online to Offline Marketing In order to develop new business in the word-of-mouth market, Lion Travel which is the biggest travel agency in Taiwan sat up a branch  X  X inmedia X . The first important thing for a new company to enter the word-of-mouth market is to own a sufficient number of experts who can affect most people X  X  opinion to advertisers, however, this is a hard work right now. S.E.R. helps Xinmedia to easily find many traveling opinion leader, and those leaders can be products spokesperson to more accurately meet the business needs. More and more advertisers agree the importance of the word-of-mouth market, because Xinmedia do created better accomplishments for advertisers X  sales by experts X  opinion. With this benefit, we can accept different domain source and do not afraid data anomaly. We also apply training mechanism automatically if the tagging word arrive the training standard. 
As shown in Figure 5, top side shows the analyzed information of whole topic thread. We just show the first post of this thread. As we can see, we provide three training mode, Category, Sentiment and Same Problem. The red word shows the positive sentiment and blue word shows the negative sentiment respectively. The special case is the  X  X ame Problem X . In forum, some author may just type  X +1 X ,  X  X e2 X ,  X  X e too X  to show they face the same problem. Therefore, we have to identify what they agreed or what they said. We solve this problem by using the relation between the same problem word and its name entity. 
To senior manager, they may not spend times on detail issue. S.E.R. provides a quick summary of relevant issue into a cluster and shows a ratio to indicate which issue is important. 6 Conclusions In this networked era, known social issues get monitored and analyzed over the Net. Information gathering and analysis over Internet have become centuries (Lieberman et al., 2007). Figure 1 illus-trates how burned gradually overtook burnt , becom-ing more frequent around 1880. Unfortunately, as a study of verb regularization, this analysis is skewed by a significant confound: both words can serve as either verbs (e.g., the house burnt ) or adjectives (e.g., the burnt toast ). Because many words have multiple syntactic interpretations, such confounds often limit the utility of raw ngram frequency data. In this work we provide a new edition of the Google Books Ngram Corpus that contains over 8 million books, or 6% of all books ever published (cf. Section 3). Moreover, we include syntactic anal-ysis in order to facilitate a fine-grained analysis of the evolution of syntax. Ngrams are annotated with part-of-speech tags (e.g., in the phrase he burnt the toast , burnt is a verb; in the burnt toast , burnt is an adjective) and head-modifier dependencies (e.g., in the phrase the little black book , little modifies book ).
The annotated ngrams are far more useful for ex-they were extracted from, and can also account for the contribution of rare ngrams to otherwise frequent grammatical functions. The Google Books Ngram Corpus has been avail-able at http://books.google.com/ngrams since 2010. This work presents new corpora that have been extracted from an even larger book collec-tion, adds a new language (Italian), and introduces syntactically annotated ngrams. The new corpora are available in addition to the already existing ones. 3.1 Books Data The new edition of the Ngram Corpus supports the eight languages shown in Table 1. The book vol-umes were selected from the larger collection of all books digitized at Google following exactly the pro-cedure described in Michel et al. (2011). The new edition contains data from 8,116,746 books, or over 6% of all books ever published. The English cor-pus alone comprises close to half a trillion words. This collection of books is much larger than any other digitized collection; its generation required a substantial effort involving obtaining and manually scanning millions of books. 3.2 Raw Ngrams We extract ngrams in a similar way to the first edi-tion of the corpus (Michel et al., 2011), but with some notable differences. Previously, tokenization was done on whitespace characters and all ngrams occurring on a given page were extracted, includ-ing ones that span sentence boundaries, but omitting 4.1 Part-of-Speech Tagging Part-of-speech tagging is one of the most funda-mental disambiguation steps in any natural lan-guage processing system. Over the years, POS tag-ging accuracies have steadily improved, appearing to plateau at an accuracy level that approaches hu-man inter-annotator agreement (Manning, 2011). As we demonstrate in the next section, these numbers are misleading since they are computed on test data that is very close to the training domain. We there-fore need to specifically adapt our models to handle noisy and historical text.
 We perform POS tagging with a state-of-the-art 2 Conditional Random Field (CRF) based tagger (Laf-ferty et al., 2001) trained on manually annotated treebank data. We use the following fairly standard features in our tagger: current word, suffixes and prefixes of length 1, 2 and 3; additionally we use word cluster features (Uszkoreit and Brants, 2008) for the current word, and transition features of the cluster of the current and previous word.

To provide a language-independent interface, we use the universal POS tagset described in detail in Petrov et al. (2012). This universal POS tagset de-fines the following twelve POS tags, which exist in similar form in most languages: N OUN (nouns), V P A merals), C ONJ (conjunctions), P RT (particles),  X . X  (punctuation marks) and X (a catch-all for other cat-egories such as abbreviations or foreign words).
Table 2 shows the two most common words for included. For treebanks with non-projective trees we use the pseudo-projective parsing technique to trans-form the treebank into projective structures (Nivre and Nilsson, 2005). To standardize and simplify the dependency relations across languages we use unla-beled directed dependency arcs. Table 3 shows un-labeled attachment scores on the treebank evaluation sets with automatically predicted POS tags. 4.3 Syntactic Ngrams As described above, we extract raw ngrams ( n  X  5 ) from the book text. Additionally, we provide ngrams annotated with POS tags and dependency relations.
The syntactic ngrams comprise words (e.g., burnt ), POS-annotated words (e.g. burnt VERB ), and POS tags (e.g., VERB ). All of these forms can be mixed freely in 1-, 2-and 3-grams (e.g., the ADJ toast NOUN ). To limit the combinatorial explosion, we restrict the forms that can be mixed in 4-and 5-grams. Words and POS tags cab be mixed freely (e.g., the house is ADJ ) and we also allow every word to be annotated (e.g., the DET house NOUN is VERB red ADJ ). However, we do not allow annotated words to be mixed with other forms (e.g., both the house NOUN is ADJ and the house NOUN is red are not allowed). Head-modifier dependencies between pairs of words can be expressed similarly (we do not record chains of dependencies). Both the head and the modifier can take any of the forms described above. We use an arrow that points from the head word to the modifier word (e.g., head = &gt; modifier or modifier &lt; = head ) to indicate a dependency relation. We use the desig-nated ROOT for the root of the parse tree (e.g.,
ROOT = &gt; has ). al., 2006), which consists entirely of questions; and the PPCMBE corpus (Kroch et al., 2010), which contains modern British English from 1700 to 1914 and is perhaps most close to our application domain.
Since the English treebanks are in constituency format, we used the StanfordConverter (de Marn-effe et al., 2006) to convert the parse trees to de-pendencies and ignored the arc labels. The depen-dency conversion was unfortunately not possible for the PPCMBE corpus since it uses a different set of constituency labels. The tagset of PPCMBE is also unique and cannot be mapped deterministically to the universal tagset. For example the string  X  X ne X  has its own POS tag in PPCMBE, but is ambigu-ous in general  X  it can be used either as a number (NUM), noun (NOUN) or pronoun (PRON). We did our best to convert the tags as closely as possible, leaving tags that cannot be mapped untouched. Con-sequently, our evaluation results underestimate the accuracy of our tagger since it might correctly dis-ambiguate certain words that are not disambiguated in the PPCMBE evaluation data.

Table 4 shows the accuracies on the different do-mains for our baseline and adapted models. The baseline model is trained only on newswire text and hence performs best on the newswire evaluation set. Our final model is adapted in two ways. First, we add the the Brown corpus and QuestionBank to the training data. Second, and more importantly, we es-timate word cluster features on the books data and use them as features in the POS tagger.

The word cluster features group words determin-istically into clusters that have similar distributional properties. When the model encounters a word that was never seen during training, the clusters allow the model to relate it to other, potentially known words. This approach improves the accuracy on rare words, and also makes our models robust to scanning er-Ion, Radu, 91 Janarthanam, Srinivasan, 49 Kao, TingHui, 157 Kazemzadeh, Abe, 115 Kolluru, BalaKrishna, 121 Ku, Lun-Wei, 97 Ku, Tsun, 163 Lee, Chia-Ying, 1 Lee, Lian Hau, 31 Lemon, Oliver, 49 Li, Guofu, 7 Li, Haizhou, 37 Li, Qiang, 19 Li, Yunyao, 109 Lin, Shou-de, 145 Lin, Wan-Yu, 145 Lin, Yuri, 169 Liu, Chao-Lin, 1 Liu, Xiaohua, 13 Liu, Xingkun, 49 Lui, Marco, 25 M ` arquez, Llu  X   X s, 139 Matsuzaki, Takuya, 127 Michel, Jean-Baptiste, 169 Microsoft, QuickView Team, 13 Narayanan, Shrikanth, 115 Navigli, Roberto, 67 Orwant, Jon, 169 Peng, Nanyun, 145 Petrov, Slav, 169 Pinnis, M  X  arcis, 91 Ponzetto, Simone Paolo, 67
Radev, Dragomir, 133
 General Chair.
 Tutorials recently held at ACL events were not selected. and graduate students who are not familiar with your topic. X  Enjoy, Qualitative Modeling of Spatial Prepositions and Motion Expressions State-of-the-Art Kernels for Natural Language Processing Probabilistic Semantic Extraction in Large Corpus Multilingual Subjectivity and Sentiment Analysis Deep Learning for NLP (without Magic) Graph-based Semi-Supervised Learning Algorithms for NLP Sunday July 8, 2012
Morning 12:30-2:00 Lunch Break
Afternoon 2:00-5:30 Multilingual Subjectivity and Sentiment Analysis
The ability to understand spatial prepositions and motion in natural language will enable a variety of new applications involving systems that can respond to verbal directions, map travel guides, display in-cident reports, etc., providing for enhanced infor-mation extraction, question-answering, information retrieval, and more principled text to scene render-ing. Until now, however, the semantics of spatial re-lations and motion verbs has been highly problem-atic. This tutorial presents a new approach to the semantics of spatial descriptions and motion expres-sions based on linguistically interpreted qualitative reasoning. Our approach allows for formal inference from spatial descriptions in natural language, while leveraging annotation schemes for time, space, and motion, along with machine learning from annotated corpora. We introduce a compositional semantics for motion expressions that integrates spatial primi-tives drawn from qualitative calculi.

No previous exposure to the semantics of spatial prepositions or motion verbs is assumed. The tu-torial will sharpen cross-linguistic intuitions about the interpretation of spatial prepositions and mo-tion constructions. The attendees will also learn about qualitative reasoning schemes for static and dynamic spatial information, as well as three annota-tion schemes: TimeML, SpatialML, and ISO-Space, for time, space, and motion, respectively.

While both cognitive and formal linguistics have examined the meaning of motion verbs and spatial prepositions, these earlier approaches do not yield precise computable representations that are expres-sive enough for natural languages. However, the previous literature makes it clear that communica-Introduction
In recent years, machine learning (ML) has been used more and more to solve complex tasks in dif-ferent disciplines, ranging from Data Mining to In-formation Retrieval or Natural Language Processing (NLP). These tasks often require the processing of structured input, e.g., the ability to extract salient features from syntactic/semantic structures is criti-cal to many NLP systems. Mapping such structured data into explicit feature vectors for ML algorithms requires large expertise, intuition and deep knowl-edge about the target linguistic phenomena. Ker-nel Methods (KM) are powerful ML tools (see e.g., (Shawe-Taylor and Cristianini, 2004)), which can al-leviate the data representation problem. They substi-tute feature-based similarities with similarity func-tions, i.e., kernels, directly defined between train-ing/test instances, e.g., syntactic trees. Hence fea-ture vectors are not needed any longer. Additionally, kernel engineering, i.e., the composition or adapta-tion of several prototype kernels, facilitates the de-sign of effective similarities required for new tasks, e.g., (Moschitti, 2004; Moschitti, 2008).
 Tutorial Content
The tutorial aims at addressing the problems above: firstly, it will introduce essential and simplified the-ory of Support Vector Machines and KM with the only aim of motivating practical procedures and in-terpreting the results. Secondly, it will simply de-scribe the current best practices for designing ap-plications based on effective kernels. For this pur-pose, it will survey state-of-the-art kernels for di-verse NLP applications, reconciling the different ap-Machine learning is everywhere in today X  X  NLP, but by and large machine learning amounts to numerical optimization of weights for human designed repre-sentations and features. The goal of deep learning is to explore how computers can take advantage of data to develop features and representations appro-priate for complex interpretation tasks. This tuto-rial aims to cover the basic motivation, ideas, mod-els and learning algorithms in deep learning for nat-ural language processing. Recently, these methods have been shown to perform very well on various NLP tasks such as language modeling, POS tag-ging, named entity recognition, sentiment analysis and paraphrase detection, among others. The most attractive quality of these techniques is that they can perform well without any external hand-designed re-sources or time-intensive feature engineering. De-spite these advantages, many researchers in NLP are not familiar with these methods. Our focus is on insight and understanding, using graphical illustra-tions and simple, intuitive derivations. The goal of the tutorial is to make the inner workings of these techniques transparent, intuitive and their results in-terpretable, rather than black boxes labeled  X  X agic here X .

The first part of the tutorial presents the basics of neural networks, neural word vectors, several simple models based on local windows and the math and algorithms of training via backpropagation. In this section applications include language modeling and POS tagging.

In the second section we present recursive neural networks which can learn structured tree outputs as well as vector representations for phrases and sen-tences. We cover both equations as well as applica-tions. We show how training can be achieved by a Banea, Carmen, 4 Bengio, Yoshua, 5 Mani, Inderjeet, 1 Manning, Christopher D., 5 Mihalcea, Rada, 4 Moschitti, Alessandro, 2 Pustejovsky, James, 1 Socher, Richard, 5 Subramanya, Amar, 6 Talukdar, Partha Pratim, 6 Wiebe, Janyce, 4 Xing, Eric, 3
 A total of 17 tutorial proposals were submitted to the ACL 2012 Tutorials track from which six were finally accepted. I am grateful to the ACL community for the diverse, and high-quality proposals I experts from the NLP community where necessary. The final selection was approved by the ACL 2012 General Chair.
 from linguistically motivated approaches to current developments in machine learning. (3) Novelty: Tutorials recently held at ACL events were not selected.
 in mind that the target audience is not your buddies whom you may want to impress, but researchers and graduate students who are not familiar with your topic. X  Local Chairs, the Publication Chairs and the ACL 2012 General Chair for making it happen. Enjoy, Tutorial Chair: Qualitative Modeling of Spatial Prepositions and Motion Expressions State-of-the-Art Kernels for Natural Language Processing Topic Models, Latent Space Models, Sparse Coding, and All That: A Systematic Understanding of Probabilistic Semantic Extraction in Large Corpus Multilingual Subjectivity and Sentiment Analysis Deep Learning for NLP (without Magic) Graph-based Semi-Supervised Learning Algorithms for NLP Sunday July 8, 2012
Morning 9:00-12:30 Qualitative Modeling of Spatial Prepositions and Motion Expressions 12:30-2:00 Lunch Break
Afternoon 2:00-5:30 Multilingual Subjectivity and Sentiment Analysis
