 In the last couple of years, data fusion has been investigated by quite a few re-searchers in information retrieval. Several data fusion methods such as CombSum [3], CombMNZ [3], the linear combination method [2,7,8], the probabilistic fusion method [4], Borda fusion [1], Condorcet fusion [5], and the correlation method [10,11], have been proposed, and extens ive experiments using TREC data have been reported to evaluate these methods. E xperimental results show that, in gen-eral, data fusion is an effective techni que for effectiveness improvement.
The linear combination data fusion method is a very flexible method since different weights can be assigned to different systems. However, it is unclear that which weighting schema is good. In some previous researches, different search methods such as golden section search [7,8] and conjugate gradient [2] were used to search suitable weights for component systems. One major drawback of using these methods is their very low efficiency. Because of this, data fusion with only two or three component systems were investigated in [2] and [7,8]. In some situations such as the WWW and digital libraries, documents are updated frequently, then each component system X  X  performance may change considerably from time to time. The weights for the systems should be updated accordingly. In such a situation, it is very difficult or impossible to use those low efficient weighting methods.

In some data fusion experiments, (e.g., in [1,6,9,11]), a simple weighting schema was used: for a system, its weight is set as its average performance over a group of training queries. There is a straightforward relationship between performance and weight. This method can be used in very dynamic situations since weights can be calculated and mo dified very easily. However, it has not been investigated how good this schema is. We would like to investigate this is-sue further. We shall demonstrate that, a power function weighting schema, with a power of 2 or 3, is more effective than the simple weighting schema (which is a special case of power function, power = 1) for data fusion, though both of them can be implemented in the same way. In addition, we shall demonstrate that combined weights which concern both p erformances of component results and dissimilarities among component results c an further improve the effectiveness of the fused results. Suppose we have n information ret rieval systems ir 1 , ir 2 ,..., ir n ,andforagiven query q , each of them provides a result r i . w i is the (performance) weight as-signed to system ir i . Then for any document d in one or all results, the linear combination method uses the equation M ( d, q )= n i =1 w i  X  s i ( d, q )tocalculate is the calculated score of d for q . All the documents can be ranked using their calculated scores.

In this section we only consider per formance weights. For each system ir i , suppose its average performance over a group of training queries is p i ,then p i is set as ir i  X  X  weight ( w i ) in the simple weighting schema, which has been used in previous research (e.g., in [1,6,9,11]). The purpose of our experiment is to try to find some other schemas which are more effective than the simple weighting schema but can be implemented as efficiently as the simple weighting schema. In order to achieve this, we set w i as a power function of p i . Besides p i ,we the weighting schema, then those systems with better performance have a larger impact on the fused results, and those results with poorer performance have a smaller impact on the fused results. 4 groups of TREC data were used for the experiment 1 . Their information is summarized in Table 1. From Table 1 we can see that these 4 groups of submitted results (called runs in TREC) a re different in many ways. Therefore, they comprise a good combination for us to evaluate data fusion methods.
The Zero-one linear normalization method was used for score normaliza-tion. For a resultant list, the top-scored document was given a score of 1, the bottom-scored document was given a score of 0, and any other document was given a score of between 0 and 1 accordingly.
 For all the systems involved, we evaluated their average performance in Mean Average Precision (MAP) over a group of queries. Then 0.5, 1.0, 1.5, 2.0, 2.5, and 3.0 were used as the power values to calculate weights for the linear combination method. In a year group, we randomly chose m ( m =3,4,5,6,7,8,9,or10) component results. Then data fusion methods were performed using those chosen results. For a particular setting of m component results in a year group, 200 runs were carried out. Mean Average Precision (MAP) was used to evaluate the fused retrieval results. Besides the linear combination method with different weighting schemas, CombSum and CombMNZ were also involved in the experiment.

Tables 2 shows the performance of the fused result. Each data point is the average of 8*200* q num measured values. Here 8 is the different number (3, 4,..., 9, 10) of component results used, 200 is the number of runs for each setting, and q num is the number of queries in each year group. The improvement rate over the average performance of all component results is shown as well.
 Comparing CombMNZ with CombSum, CombMNZ is not as good as Comb-Sum in all 4 year groups. With any of the weighting schemas chosen, the linear combination method performs better t han the best component result, Comb-Sum, and CombMNZ in all 4 year groups. Comparing with all different weight-ing schemas used, we can find that the larger the power is used for weighting calculation, the better the linear combination method performs.

Two-tailed tests were carried out to c ompare the differences between all the data fusion methods involved. The tests show that the differences between any pair of the data fusion methods are statistically significant at a level of .000 ( p&lt; 0 . 001, or the probability is over 99.9%). From the worst to the best, the data fusion methods are ranked as follows: CombMNZ, CombSum, LC(0.5), LC(1.0), LC(1.5), LC(2.0), LC(2.5), LC(3.0).

We also compare the fused result wit h the best component result. The im-provement rate of the fused results over the best component results is shown in Table 3. This improvement rate is a hard measure. Both CombMNZ and CombSum do not perform as well as the best component result in some year groups; while they perform better than t he best component result in some other year groups. With any of the weighting schemas chosen, the linear combina-tion method performs better than the best component result, CombSum, and CombMNZ in all 4 year groups. Consider the hardness of this measure, the fused result is very good.

In the above experiment, we observe d that a power of 3 obtained the best fusion result. Some more experiments are desirable to find out how does a power of bigger than 3 performs. Now we consider combined weights which combine performance weights and dissimilarity weights. There have been a few different ways of calculating dis-similarity weights. In [10,11], the overlap rate and Spearman rank coefficient of two results were used. In this paper, we use another way to do it. First, we nor-malize the scores in both component results r 1 and r 2 for a given query q using the Zero-one normalization method. Then we calculate the Euclidean distance D ( r i ,r j ,q ).

Suppose that we have n component results r i ( i =1, 2, ..., n ), then component result r i  X  average distance from all other comp onent results can be calculated as dis i can be obtained by averaging dis i,q over all the queries. For each system ir , its combined weight is defined as Here a and b are positive numbers, p i is the average performance of system ir i over all the queries.

The experiment was carried out using th e same data as in Section 2. Combined weights, calculated using Equation 2, w ere assigned to each component result. We varied b (0.5, 1, 1.5, and 2) while fixing a to be 3. According to our investigation in Section 2, 3 is a good value for that. Experimental results are shown in Table 4.
Compared with the situation that only performance weights are used, com-bined weights can bring small but significant improvement (about 0.5%). It sug-gests that the setting n =0 . 5 is not as good as the other settings ( n =1,1.5,and 2). However, the three settings ( n =1, 1.5, and 2) are very close in performance for data fusion.

Finally, let us compare LC(1.0)(the simple weighting schema) with LC(3.0:1.5) (combined weight, with a performance power of 3 and a dissimilarity power of 1.5). LC(3.0:1.5) performs better than L(1.0) in all 4 year groups by 1% to 3.5%. Although the improvement looks small, it is statistically significant ( p&lt; 0 . 001) in all 4 year groups. Since these two sche mas require the same relevance judgment information, we consider LC(3.0:1.5) is certainly a better choice than LC(1.0) for data fusion. In this paper we have presented our preliminary work about assigning appropri-ate weights for the linear combination data fusion method. From the experiments conducted with the TREC data, we observe that for performance weighting, a series of power functions (e.g., a power of 2, 2.5, or 3) are better than the simple weighting schema, in which the performance weight of a system is assigned as its average performance (power equals to 1). The power function schema can be implemented as efficiently as the simple weighting schema. We have also demon-strated that the combined weights, which comprise performance weights and dissimilarity weights, can be used for data fusion to further improve effective-ness. We believe that the investigation outcome in this paper is very useful for practical application of the data fusion technique. As our next stage work, we will investigate the linear combination method with more weighting options. Acknowledgements. Xiaoqin Zeng and Lixin Han X  X  work was partially sup-ported by the NSFC, Chin a, under Grant s 60571048 and 60673186.

