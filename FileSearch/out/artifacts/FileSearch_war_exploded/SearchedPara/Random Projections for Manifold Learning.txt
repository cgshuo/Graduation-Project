 Recently, we have witnessed a tremendous increase in the siz es of data sets generated and processed by acquisition and computing systems. As the volume of the da ta increases, memory and processing requirements need to correspondingly increase at the same r apid pace, and this is often prohibitively expensive. Consequently, there has been considerable inte rest in the task of effective modeling of high-dimensional observed data and information; such mode ls must capture the structure of the information content in a concise manner.
 A powerful data model for many applications is the geometric notion of a low-dimensional man-ifold . Data that possesses merely K  X  X ntrinsic X  degrees of freedom can be assumed to lie on a K -dimensional manifold in the high-dimensional ambient spa ce. Once the manifold model is iden-tified, any point on it can be represented using essentially K pieces of information. Thus, algorithms in this vein of dimensionality reduction attempt to learn the structure of the manifold given high-dimensional training data.
 While most conventional manifold learning algorithms are a daptive (i.e., data dependent) and non-linear (i.e., involve construction of a nonlinear mapping) , a linear, nonadaptive manifold dimen-sionality reduction technique has recently been introduce d that employs random projections [1]. Consider a K -dimensional manifold M in the ambient space R N and its projection onto a random subspace of dimension M = CK log( N ) ; note that K &lt; M  X  N . The result of [1] is that the pairwise metric structure of sample points from M is preserved with high accuracy under projection from R N to R M . This result has far reaching implications. Prototypical de vices that directly and inexpensively ac-quire random projections of certain types of data (signals, images, etc.) have been developed [2, 3]; these devices are hardware realizations of the mathematica l tools developed in the emerging area of Compressed Sensing (CS) [4, 5]. The theory of [1] suggests th at a wide variety of signal processing tasks can be performed directly on the random projections acquired by these devices, thus saving valuable sensing, storage and processing costs.
 The advantages of random projections extend even to cases wh ere the original data is available in the ambient space R N . For example, consider a wireless network of cameras observ ing a scene. To perform joint image analysis, the following steps might be e xecuted: In situations where N is large and communication bandwidth is limited, the domina ting costs will be in the first transmission/collation step. On the one hand, to reduce the communication needs one may perform nonlinear image compression (such as JPEG) at each n ode before transmitting to the central processing. But this requires a good deal of processing powe r at each sensor, and the compression would have to be undone during the learning step, thus adding to overall computational costs. On the other hand, every camera could encode its image by computing (either directly or indirectly) a small number of random projections to communicate to the central p rocessor. These random projections are obtained by linear operations on the data, and thus are ch eaply computed. Clearly, in many situations it will be less expensive to store, transmit, and process such randomly projected versions of the sensed images. The question now becomes: how much info rmation about the manifold is conveyed by these random projections, and is any advantage i n analyzing such measurements from a manifold learning perspective? dimensional manifold can be performed not just in the high-d imensional ambient space R N but also in an intermediate, much lower-dimensional random project ion space R M , where M = CK log( N ) . See, for example, the toy example of Figure 1. Our contributi ons are as follows. First, we present a theoretical bound on the minimum number of measurements per sample point required to estimate the intrinsic dimension (ID) of the underlying manifold, up to an accuracy level comparable to that of the Grassberger-Procaccia algorithm [7, 8], a widely use d geometric approach for dimensionality estimation. Second, we present a similar bound on the number of measurements M required for Isomap [6]  X  a popular manifold learning algorithm  X  to be  X  X e liably X  used to discover the nonlinear structure of the manifold. In both cases, M is shown to be linear in K and logarithmic in N . Third, we formulate a procedure to determine, in practical setting s, this minimum value of M with no a priori information about the data points. This paves the way for a we akly adaptive, linear algorithm ( ML-RP ) for dimensionality reduction and manifold learning.
 The rest of the paper is organized as follows. Section 2 recap s the manifold learning approaches we utilize. In Section 3 presents our main theoretical contrib utions, namely, the bounds on M required to perform reliable dimensionality estimation and manifol d learning from random projections. Sec-tion 4 describes a new adaptive algorithm that estimates the minimum value of M required to provide a faithful representation of the data so that manifold learn ing can be performed. Experimental re-sults on a variety of real and simulated data are provided in S ection 5. Section 6 concludes with discussion of potential applications and future work. An important input parameter for all manifold learning algo rithms is the intrinsic dimension (ID) of a point cloud. We aim to embed the data points in as low-dimens ional a space as possible in order to avoid the curse of dimensionality. However, if the embeddin g dimension is too small, then distinct data points might be collapsed onto the same embedded point. Hence a natural question to ask is: given a point cloud in N -dimensional Euclidean space, what is the dimension of the m anifold that best captures the structure of this data set? This problem ha s received considerable attention in the literature and remains an active area of research [7, 9, 10].
 For the purposes of this paper, we focus our attention on the G rassberger-Procaccia (GP) [7] algo-rithm for ID estimation. This is a widely used geometric tech nique that takes as input the set of pairwise distances between sample points. It then computes the scale-dependent correlation dimen-sion of the data, defined as follows.
 Definition 2.1 Suppose X = ( x where I is the indicator function. The scale-dependent correlatio n dimension of X is defined as The best possible approximation to K (call this b K ) is obtained by fixing r range over which the plot is linear and the calculating D practical issues involved with this approach; indeed, it ha s been shown that geometric ID estimation algorithms based on finite sampling yield biased estimates o f intrinsic dimension [10, 11]. In our the effect of running the GP algorithm on a sufficient number o f random projections produces a dimension estimate that well-approximates the GP estimate obtained from analyzing the original point cloud.
 The estimate b K of the ID of the point cloud is used by nonlinear manifold lear ning algorithms (e.g., Isomap [6], Locally Linear Embedding (LLE) [12], and Hessia n Eigenmaps [13], among many others) to generate a b K -dimensional coordinate representation of the input data p oints. Our main analysis will be centered around Isomap. Isomap attempts to preserve the metric structure of the manifold, i.e., the set of pairwise geodesic distances of an y given point cloud sampled from the manifold. In essence, Isomap approximates the geodesic dis tances using a suitably defined graph and performs classical multidimensional scaling (MDS) to o btain a reduced K -dimensional repre-sentation of the data [6]. A key parameter in the Isomap algor ithm is the residual variance , which is equivalent to the stress function encountered in classical MDS. The residual variance is a measure of how well the given dataset can be embedded into a Euclidean space of dimension K . In the next section, we prescribe a specific number of measurements per d ata point so that performing Isomap on the randomly projected data yields a residual variance th at is arbitrarily close to the variance produced by Isomap on the original dataset.
 We conclude this section by revisiting the results derived i n [1], which form the basis for our de-velopment. Consider the effect of projecting a smooth K -dimensional manifold residing in R N onto a random M -dimensional subspace (isomorphic to R M ). If M is sufficiently large, a stable near-isometric embedding of the manifold in the lower-dime nsional subspace is ensured. The key advantage is that M needs only to be linear in the intrinsic dimension of the manifold K . In addition, M depends only logarithmically on other properties of the man ifold, such as its volume, curvature, etc. The result can be summarized in the following theorem.
 Theorem 2.2 [1] Let M be a compact K -dimensional manifold in R N having volume V and to R M and Suppose M &lt; N . Then, with probability exceeding 1  X   X  , the following statement holds: For every pair of points x, y  X  X  , and i  X  X  1 , 2 } , where d points x and y .
 The condition number  X  controls the local, as well as global, curvature of the manif old  X  the smaller the  X  , the less well-conditioned the manifold with higher  X  X wist edness X  [1]. Theorem 2.2 has been proved by first specifying a finite high-resolution sampling on the manifold, the nature of which depends on its intrinsic properties; for instance, a planar manifold can be sampled coarsely. Then the Johnson-Lindenstrauss Lemma [14] is applied to these point s to guarantee the so-called  X  X sometry constant X   X  , which is nothing but (2). dimensional input point cloud (i.e., the set of all pairwise distances between points belonging to the dataset) is preserved up to a distortion that depends on  X  . This immediately suggests that geometry-based ID estimation and manifold learning algorithms could be applied to the lower-dimensional, randomly projected version of the dataset.
 The first of our main results establishes a sufficient dimensi on of random projection M required to maintain the fidelity of the estimated correlation dimensio n using the GP algorithm. The proof of the following is detailed in [15].
 Theorem 3.1 Let M be a compact K -dimensional manifold in R N having volume V and condi-tion number 1 / X  . Let X = { x supported on M . Let b K be the dimension estimate of the GP algorithm on X over the range ( r condition holds: Let  X  be a random orthoprojector from R N to R M with M &lt; N and Let b K ( r with probability exceeding 1  X   X  .
 Theorem 3.1 is a worst-case bound and serves as a sufficient co ndition for stable ID estimation using random projections. Thus, if we choose a sufficiently small v alue for  X  and  X  , we are guaranteed estimation accuracy levels as close as desired to those obta ined with ID estimation in the original signal space. Note that the bound on b K number of projections required to estimate b K becomes higher with increasing manifold dimension K .
 The second of our main results prescribes the minimum dimens ion of random projections required to maintain the residual variance produced by Isomap in the p rojected domain within an arbitrary additive constant of that produced by Isomap with the full data in the a mbient space. This proof of this theorem [15] relies on the proof technique used in [16].
 Theorem 3.2 Let M be a compact K -dimensional manifold in R N having volume V and condition number 1 / X  . Let X = { x density supported on M . Let  X  be a random orthoprojector from R N to R M with M &lt; N . Fix 0 &lt;  X  &lt; 1 and 0 &lt;  X  &lt; 1 . Suppose Define the diameter  X  of the dataset as follows: where d Define R and R embedding of the original dataset X and projected dataset  X  X respectively. Under suitable con-structions of the Isomap connectivity graphs, R with probability exceeding 1  X   X  . C is a function only on the number of sample points n . Since the choice of  X  is arbitrary, we can choose a large enough M (which is still only logarithmic in
N ) such that the residual variance yielded by Isomap on the ran domly projected version of the this result is derived from a worst-case analysis. Note that  X  acts as a measure of the scale of the dataset. In practice, we may enforce the condition that the d ata is normalized (i.e., every pairwise distance calculated by Isomap is divided by  X  ). This ensures that the K -dimensional embedded representation is contained within a ball of unit norm cente red at the origin.
 Thus, we have proved that with only an M -dimensional projection of the data (with M  X  N ) we can perform ID estimation and subsequently learn the stru cture of a K -dimensional manifold, up to accuracy levels obtained by conventional methods. In S ection 4, we utilize these sufficiency results to motivate an algorithm for performing practical m anifold structure estimation using random projections. In practice, it is hard to know or estimate the parameters V and  X  of the underlying manifold. Also, of GP and Isomap on the point cloud in the ambient space. Thus, often, we may not be able fix a definitive value for M . To circumvent this problem we develop the following empiri cal procedure that we dub it ML-RP for manifold learning using random projections .
 We initialize M to a small number, and compute M random projections of the data set X = { x 1 , x 2 , ..., x n } {  X  x : x  X  X } , we estimate the intrinsic dimension using the GP algorithm . This estimate, say b K , is used by the Isomap algorithm to produce an embedding into b K -dimensional space. The resid-ual variance produced by this operation is recorded. We then increment M by 1 and repeat the entire process. The algorithm terminates when the residual variance obtained is smaller than some tolerance parameter  X  . A full length description is provided in Algorithm 1.
 The essence of ML-RP is as follows. A sufficient number M of random projections is determined by a nonlinear procedure (i.e., sequential computation of Iso map residual variance) so that conventional Algorithm 1 ML-RP M  X  1
 X   X  Random orthoprojector of size M  X  N . while residual variance  X   X  do end while return M return b K GP on native data. manifold learning does almost as well on the projected datas et as the original. On the other hand, the random linear projections provide a faithful represent ation of the data in the geodesic sense. In this manner, ML-RP helps determine the number of rows that  X  requires in order to act as an operator that preserves metric structure. Therefore, ML-R P can be viewed as an adaptive method for linear reduction of data dimensionality. It is only weak ly adaptive in the sense that only the stopping criterion for ML-RP is determined by monitoring th e nature of the projected data. The results derived in Section 3 can be viewed as convergence proofs for ML-RP. The existence of a certain minimum number of measurements for any chosen erro r value  X  ensures that eventually, M in the ML-RP algorithm is going to become high enough to ensur e  X  X ood X  Isomap performance. Also, due to the built-in parsimonious nature of ML-RP, we ar e ensured to not  X  X vermeasure X  the manifold, i.e., just the requisite numbers of projections o f points are obtained. This section details the results of simulations of ID estima tion and subsequent manifold learning on real and synthetic datasets. First, we examine the performa nce of the GP algorithm on random pro-jections of K -dimensional dimensional hyperspheres embedded in an ambi ent space of dimension N = 150 . Figure 2(a) shows the variation of the dimension estimate p roduced by GP as a function of the number of projections M . The sampled dataset in each of the cases is obtained from dra wing n = 1000 samples from a uniform distribution supported on a hypersph ere of corresponding dimen-sion. Figure 2(b) displays the minimum number of projection s per sample point required to estimate the scale-dependent correlation dimension directly from t he random projections, up to 10% error, when compared to GP estimation on the original data.
 We observe that the ID estimate stabilizes quickly with incr easing number of projections, and indeed converges to the estimate obtained by running the GP algorit hm on the original data. Figure 2(b) illustrates the variation of the minimum required projecti on dimension M vs. K , the intrinsic dimen-hand rotation databases N = 3840.
 Figure 4: Performance of ML-RP on the above databases. (left) ML-RP on the face database ( N = 4096 ). M &gt; 60 , the Isomap variance is indistinguishable from the varianc e obtained in the ambient space. sion of the underlying manifold. We plot the intrinsic dimen sion of the dataset against the minimum number of projections required such that b K is equivalent to choosing  X  = 0 . 1 in Theorem 3.1). We observe the predicted linearity (Theore m 3.1) in the variation of M vs K .
 Finally, we turn our attention to two common datasets (Figur e 3) found in the literature on dimension collection of 698 artificial snapshots of a face ( N = 64  X  64 = 4096 ) varying under 3 degrees of freedom: 2 angles for pose and 1 for lighting dimension. The s ignals are therefore believed to reside on a 3D manifold in an ambient space of dimension 4096. The han d rotation database is a set of 90 images ( N = 64  X  60 = 3840 ) of rotations of a hand holding an object. Although the image appearance manifold is ostensibly one-dimensional, estim ators in the literature always overestimate its ID [11].
 Random projections of each sample in the databases were obta ined by computing the inner product projected points closely approximates the variance obtain ed with full image data. This behavior of convergence of the variance to the best possible value is eve n more sharply observed in the hand are particularly encouraging and demonstrate the validity of the claims made in Section 3. Our main theoretical contributions in this paper are the exp licit values for the lower bounds on the minimum number of random projections required to perform ID estimation and subsequent manifold learning using Isomap, with high guaranteed accuracy level s. We also developed an empirical greedy algorithm (ML-RP) for practical situations. Experiments o n simple cases, such as uniformly gener-ated hyperspheres of varying dimension, and more complex si tuations, such as the image databases displayed in Figure 3, provide sufficient evidence of the nat ure of the bounds described above. The method of random projections is thus a powerful tool for e nsuring the stable embedding of low-dimensional manifolds into an intermediate space of reason able size. The motivation for developing results and algorithms that involve random measurements of high-dimensional data is significant, particularly due to the increasing attention that Compress ive Sensing (CS) has received recently. It is now possible to think of settings involving a huge number o f low-power devices that inexpen-sively capture, store, and transmit a very small number of me asurements of high-dimensional data. of the data to the central processing node, ML-RP provides a s imple solution to the manifold learn-ing problem and ensures that with minimum transmitted amoun t of information, effective manifold learning can be performed. The metric structure of the proje cted dataset upon termination of ML-RP closely resembles that of the original dataset with high p robability; thus, ML-RP can be viewed as a novel adaptive algorithm for finding an efficient, reduce d representation of data of very large dimension.
 [1] R. G. Baraniuk and M. B. Wakin. Random projections of smoo th manifolds. 2007. To appear [2] M. B. Wakin, J. N. Laska, M. F. Duarte, D. Baron, S. Sarvoth am, D. Takhar, K. F. Kelly, and [3] S. Kirolos, J.N. Laska, M.B. Wakin, M.F. Duarte, D.Baron , T. Ragheb, Y. Massoud, and R.G. [4] E. J. Cand`es, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruc-[5] D. L. Donoho. Compressed sensing. IEEE Trans. Info. Theory , 52(4):1289 X 1306, September [6] J. B. Tenenbaum, V.de Silva, and J. C. Landford. A global g eometric framework for nonlinear [7] P. Grassberger and I. Procaccia. Measuring the strangen ess of strange attractors. Physica D [8] J. Theiler. Statistical precision of dimension estimat ors. Physical Review A , 41(6):3038 X 3051, [9] F. Camastra. Data dimensionality estimation methods: a survey. Pattern Recognition , 36:2945 X  [10] J. A. Costa and A. O. Hero. Geodesic entropic graphs for d imension and entropy estimation in [11] E. Levina and P. J. Bickel. Maximum likelihood estimati on of intrinsic dimension. In Advances [12] S. Roweis and L. Saul. Nonlinear dimensionality reduct ion by locally linear embedding. Sci-[13] D. Donoho and C. Grimes. Hessian eigenmaps: locally lin ear embedding techniques for high [14] Sanjoy Dasgupta and Anupam Gupta. An elementary proof o f the JL lemma. Technical Report [15] C. Hegde, M. B. Wakin, and R. G. Baraniuk. Random project ions for manifold learning -[16] M. Bernstein, V. de Silva, J. Langford, and J. Tenenbaum . Graph approximations to geodesics [17] B. K  X egl. Intrinsic dimension estimation using packin g numbers. In Advances in NIPS , vol-
