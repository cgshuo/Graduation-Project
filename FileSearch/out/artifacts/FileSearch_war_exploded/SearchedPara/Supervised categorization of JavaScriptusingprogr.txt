 1. Introduction
As the web has become more interactive, tasks such as form processing, uploading, scripting, applets and plug-ins have transformed the web page into a dynamic application. While a boon to human users, such dynamic aspects of web page scripting and applets impede the machine parsing and understanding of web pages. When such functionality is present, these crucial functionalities are lost if not processed. Pages with
JavaScript, Macromedia Flash and other plug-ins are only starting to be indexed and analyzed by web crawl-ers and indexers. Interactivity in web pages is increasing as more web content is provided using complex con-tent management systems, which use scripting to create a more interactive experience for the human user.
Asychronous Javascripting and XML (AJAX) is an example of such an emerging technique. If automated indexers are to keep providing accurate and up-to-date information, methods are needed to glean information about the dynamic aspects of web pages.
To address this problem, we consider a technique to automatically categorize uses of JavaScript, a popular web scripting language. In many web pages, JavaScript realizes many of the dynamic features of web pages.
We chose to focus on JavaScript as (1) its code is inlined within an HTML page and (2) embedded JavaScript often interacts with other static web page components (unlike applets and plug-ins). We leverage on both of these key properties in our analysis.

An automatic categorization of JavaScript can assist both indexing software to accurately model web pages X  functionality and requirements and browsers to selecting allow certain scripting functions to run and to disable others. Pop-up window blocking, which has been extensively researched, is just one of the myriad uses of JavaScript that would be useful to categorize. Such software can assist automated web indexers to report useful information to search engines and allow browsers to block annoying script-driven features of web pages from end users.

We start with a system that employs traditional text categorization metrics as a baseline. Although the resulting baseline system performs reasonably, we pursue a machine learning framework that draws on fea-tures from text categorization, program comprehension and code metrics to improve performance. We show that the incorporation of features that leverage knowledge of the JavaScript language together with program analysis improves categorization accuracy. We conduct evaluation of our methods on the widely used WT10G corpus ( Hawking, 2004 ) to validate our claims and show that the performance of our system eliminates over 50% of errors over the baseline.

In the next section, we examine earlier attempts at source code categorization and discuss how features of the JavaScript language and techniques in program analysis can assist in categorization. We then present our methods that distills features for categorization from the principles of program analysis. We describe our experiment and analysis, and conclude by discussing our manual analysis and future directions of our work. 2. Background
A survey of previous work shows that the problem of automated computer software categorization is rel-atively new. We believe that this is due to two reasons. First, programming languages are generally designed to be open-ended and largely task-agnostic. Languages such as FORTRAN, Java and C are suitable for a very wide range of tasks and attempting to define a fixed categorization scheme for programs is largely subjective, and likely to be ill-defined. Second, the research fields of textual information retrieval (IR) and program ana-lysis have largely developed independently of each other. We feel that these two fields have a natural overlap which can be exploited.

Unlike natural language texts, program source code is unambiguous to the compiler and has exact syntactic structures. This means that syntax plays an important role that needs to be captured. This has been largely ignored by text categorization research.

Ugurel et al. (2002) and Krovetz et al. (2003) are perhaps the first work that uses IR methods to attack this problem. They employ support vector machines for source code classification in a two-phase process consist-ing of programming language classification followed by topic classification. In the second, topic classification task, they largely relied on each software projects X  README file and comments. From the source code itself, only included header file names were used as features. We believe a more advanced treatment of the source code itself can assist such topic classification. These features, including syntactic information and some lan-guage-specific semantic information, could be important and useful for classification. Other recent work in of this work is that informing these structural features with knowledge about the syntax of the programming language can improve source code classification.

Program analysis, in which formal methods are employed, has developed into many subfields. The subfield of program comprehension develops models to explain how human software developers learn to comprehend existing code ( Mathias, James, Cross, Hendrix, &amp; Barowski, 1999 ). These models show that developers use both top X  X own and bottom X  X p models in their learning process ( von Mayrhauser &amp; Vans, 1994 ). Top X  X own models imply that developers may use a model of program execution to understand a program. Formal ana-lysis via simulated code execution ( Blazy &amp; Facon, 1998 ) may yield evidence for automated categorization techniques. Comprehension also employs code metrics, which measure the complexity and performance of programs.
Of particular interest to our problem scenario are code reuse metrics, such as ( Krsul &amp; Spafford, 1995; Kon-togiannis, 1997; Kapser &amp; Godfrey, 2004, 2005 ), as JavaScript instances are often copied wholesale or mod-ified from standard examples. In our experiments, we assess the predictive strength of these metrics on program categories.

We believe that a standard text categorization approach to this problem can be improved by adopting fea-tures distilled from program analysis. Prior work shows that the use of IR techniques, such as latent semantic analysis can aid program comprehension ( Maletic &amp; Marcus, 2000 ). The key contribution of our work shows that the converse is also true: the formal and exact results of program analysis are good features that assist in the IR frameworks for source code categorization. 3. JavaScript categorization
The problem of JavaScript program categorization is a good proving ground to explore how these two fields can interact and inform each other. JavaScript is a niche language  X  mostly confined to web pages and performing limited number of tasks. We believe this is due to the restrictions of HTML and HTTP, and because web plug-ins are more conducive an environment for applications that require true interactiv-ity and fine-grained control. We feel this property makes the text categorization approach well-defined, in contrast to categorization of programs in other programming languages, a hypothesis that we verify later in Section 6 . Secondly, JavaScript has an intimate relationship with the HTML elements in the web page.
Form controls, divisions and other web page objects are controlled and manipulated in JavaScript. As such, we can analyze how the web page X  X  text and its HTML tags, in the form of its document object model (DOM), affect its categorization.

What type of categorization is suitable for JavaScript? Recalling our purpose, the categories should indicate to the end user the usefulness or functionality of the script. We examined two previous source code classifi-cations to judge whether they can be used for our work: (1) Ugurel et al. (2002) proposed 11 topic categories for source code topic classification task, including cir-(2) www.js-examples.com is an existing JavaScript categorization from a well-known tutorial site. This site
While a good starting point, the js-examples categorization has two weaknesses that made it unusable for our purposes. First, the classification is intended for the developer, rather than the consumer. Examples that gorize JavaScript functionality with respect to the end user. Second, the classification is used for example scripts, which are usually truncated and for illustrative purposes. Real-world JavaScript does not illustrate these niceties.
 To deal with these shortcomings, we decided to modify the js-examples scheme based on a study of Java-
Script instances in actual web documents. We see that JavaScript that natively occurs in actual web pages is different. Actual web pages often embed multiple JavaScript instances to achieve different functionality. Also, scripts can be invoked at load time or by triggering events that deal with interaction with the browser. For example, a page could have a set of scripts that performs browser detection (that runs at load time) and another separate set that validates form information (that runs only when the text input is filled out). In addition, some scripts are only invoked as subprocedures of others. All these aspects make these real-world JavaScript instances more difficult to handle than the ones on the example site.

As such, we perform categorization on individual JavaScript functional units, rather than all of the scripts on a single page. A functional unit , or simply unit , is defined as a JavaScript instance, combined with all its (potentially) called subprocedures. Given a web page, JavaScript code fragments are segmented from one another by how they are invoked (automatically upon loading or by a particular user action, like a mouse click on a form input). All relevant variable declarations, function calls, objects and HTML that are associated with the invoked code fragment form the unit. Fig. 1 shows an example.

We base our categorization of JavaScript on these automatically extracted units. Based on our corpus study, we created a classification of JavaScript into 33 discrete categories, shown in Table 1 . These categories are based on functionality rather than by their implementation technique. A single other category is used for scripts whose purpose is unclear or contains more than one basic functionality.

We have made our dataset, annotations and categories freely available for research use and encourage oth-ers to make use of this resource. The dataset consists of the complete set of 18,683 documents which contain JavaScript, taken from the WT10G corpus. Web pages with erroneous JavaScripts are removed or corrected.
We also provide the 1637 JavaScript functional unit instances used in our evaluation, along with a single set of gold standard annotations made by the first author. We have also made our system prototype freely available online for research purpose. Details of these resources will be presented at the conclusion of the paper. 4. Methods
Given such a categorization scheme, a standard text categorization approach would tokenize a training set (already classified) into units and use the resulting tokens as features to build a model. New, unseen test units are then tokenized and the resulting features are compared to the models of each category. The category most similar to the test unit is inferred as its category.

A simple approach could use a compiler X  X  own tokenization, treating the resulting tokens as separate dimensions for categorization. Using the compiler X  X  tokenization is important, as whitespace may not indicate all token separations (e.g.,  X  X  x = 2 X  X ). An n dimensional feature vector results, where n is the vocabulary size (i.e., total number of unique tokens that occur in all training unit instances). Usually, tens of thousands of features result, and the positive instances of a typical feature are rare, perhaps limited to a few training exam-ple each.

We improve on this text categorization baseline in three ways. We first show how tokenization can be improved by exploiting the properties of the programming language. Second, we show that certain code met-rics can help. Third, features distilled from program comprehension in the form of static analysis and dynamic execution allow us to analyze how objects interact with each other; evidence which can further improve a unit X  X  classification. 4.1. Using language features for improved tokenization
We can improve the tokenization by leveraging the syntax of the programming language. A syntactic anal-ysis of JavaScript types the unit X  X  tokens and can distinguish the tokens as numeric constants, string constants, comments, language-specific reserved keywords, arithmetic operators, regular expression variable or method names. These types are illustrated in Table 2 .

As JavaScript X  X  is aimed at Web constructs, we further distinguish string constants as URLs, file extensions of images and multimedia, HTML tags and color values. Tokens of these types are tagged as such and their aggregate counts are used as additional features for categorization, as illustrated in Table 3 .
Variable and method names are special as they often convey the semantics of the program. However, for convenience, programmers frequently use abbreviations or short forms for these names. For example, in the
JavaScript statement var currMon = mydate.getMonth (), currMon, mydate and getMonth are short forms for  X  X  X urrent month X  X ,  X  X  X y date X  X  and  X  X  X et month X  X  respectively (see Table 4 ).

To a classification system, tokens such as currMon and curMonth are distinct and unrelated. To connect these forms together, we need to normalize these non-standard words ( Rowe &amp; Laitinen, 1995 ). We normalize such words by identifying likely splitting points and then expanding them to full word forms.
Splitting is achieved by identifying case changes and punctuation use. Tokens longer than six letter in length are also split into smaller parts using entropy reduction, previously used to split natural languages without delimiters (e.g., Chinese). An expansion phase follows to map commonly abbreviated shortenings to their word equivalents (e.g.,  X  X  curr  X  X  and  X  X  cur  X  X  !  X  X  current  X  X ) using a small (around 20 entries) hand-compiled dictionary. 4.2. Code metrics
Complexity metrics measure the complexity of a program with respect to data flow, control flow or a hybrid of the two. Recent work in metrics ( Kapser &amp; Godfrey, 2004; Krsul &amp; Spafford, 1995 ) has been applied to specific software families and most metrics are targeted to much larger software projects (thousands of lines of code) than a typical JavaScript unit (averaging around 28 lines) in our corpus. As such, we start with simple, classic complexity metrics to assess their impact on categorization. We also designed two additional metrics that capture some of the unique properties that we encounted in JavaScript. 4.2.1. Standard metrics (1) Counting metrics (2) Structure metrics (3) Information flow metrics
The above listed are classic software metrics employed in software measurement tasks and are generally applicable for different general-purpose programming languages. 4.2.2. JavaScript language-specific metrics
As stated earlier, software metrics are now being developed for specific languages and for specific task domains, we have developed some additional metrics which attempt to capture idiosyncratic phenomenon that occur in our corpus.
 Similar/repeating statements (SS) counts the number of statements with highly similar/repeating structure.
The similarity measure of structure is examined by abstract syntax trees given in Section 4.2.3 . This is help-ful in detecting scripts that perform similar tasks in a conditional manner. For example, displaying different greetings based on the time (e.g., morning, afternoon, evening).

Number of built-in object references (BO) counts the number of built-in Java objects (e.g., math, date) ref-erenced by the unit.

Number of object function invoked (OF) counts the number of object-function invoked in the code. Object functions are a unique feature of JavaScript. One can define and instantiate an object via a function. Fig. 2 gives an example of function object. This is helpful in detecting scripts involving complicated tasks like games or e-commerce. 4.2.3. Code reuse using edit distance
We can also measure code reuse (also referred to as clone or plagiarism detection). This is particularly use-ful as many developers copy (and occasionally modify) scripts from existing web pages. Thus similarity detec-tion may assist in classification. Dynamic programming can be employed to calculate a minimum edit distance between two inputs using strings, tokens, or trees as elements for computation.

We employ a standard string edit distance (SED) algorithm to calculate similarity between two script instances. We use the class of the minimal distance training unit as a separate feature for classification. How-ever, this measure does not model the semantic differences that are introduced when edits result in structural differences as opposed to variable renaming. A minimal string edit distance may introduce drastic semantic changes, such as an addition of a parameter or deletion of a conditional statement.

To address this we again turn to program analysis. In program analysis, abstract syntax trees (ASTs) ( Baxter, Yahin, Moura, Sant X  X nna, &amp; Bier, 1998 ) are often used to model source code and correct for these discrepancies. An AST is a parse tree representation of the source code that models the control flow of the unit and stores data types of its variables. Therefore we can use the AST model to define a tree-based edit distance (TED) measure between two JavaScript units. TED algorithms are employed in syntactic similarity detection ( Yang, 1991 ). However, as is shown in Fig. 3 , the given two code fragments are of the same functionality, but have different syntactic structures. Hence, syntactic difference does not necessarily imply functionality similar-ity and vice versa. In this manner, a standard TED algorithm used in syntactic similarity detection is not likely to outperform a simple SED algorithm for our task. We can also measure similarity using a token-based edit distance (lexical-token based edit distance, LED), in which source code is parsed into a stream of tokens for distance computation ( Kamiya, Kusumoto, &amp; Inoue, 2002 ). Edit costs are assigned appropriately depending on token types and values. We have implemented all three models and have assessed each approach X  X  effectiveness. 4.3. Program comprehension using the document object model So far we have considered JavaScript units as independent of their enclosing web pages. In practice, since JavaScript units may be triggered by HTML objects and may then manipulate these HTML objects in turn, a
JavaScript unit has an intimate relation with its page and is often meaningful only in context. These objects are represented by a document object model (DOM). 1 In fact, a unit which does not interact with a DOM object cannot interact with the user and is considered uninteresting. Many variables used in JavaScript are DOM objects whose data type can only be inferred by examining the enclosing HTML document. Fig. 4 illustrates two examples where the script references DOM objects.

We classify references to DOM objects into four categories: gets , sets , creates , and calls . These are illus-trated in the JavaScript unit in Fig. 5 : on line, DoIt () gets a reference to a form object; on line 4, the input object represented by frm.txt is set to a value  X  X  ok  X  X ; on line 2, a new window object is created by the method window. open ; and on line 3; the object document calls its write method. The count of each of these
DOM object references is added as an integer feature for categorization. 4.3.1. Static analysis
Certain aspects of the communication between the DOM objects and the target JavaScript can be done by an inspection of the code. We extract two types of information based on this static analysis: triggering infor-mation and variable data types.

Certain classes of JavaScript are triggered by the user X  X  interaction with an object (e.g., a form input field) and others occur when a page is loaded, without user interaction. This triggering type (interactive, non-inter-active) is extracted for each unit by an inspection of the HTML. For units triggered by interaction, we further extract the responsible DOM object and event handler. We also extract the lexical tokens from the enclosing web page elements for interactive units. For example, an input button with a text value  X  X  restore  X  X  is likely to indicative of the class calculator .

DOM object settings and values may flow from one procedure to another. We recover the data type of objects by tracing the flow as variables are instantiated and assigned. This is done with the assistance of the abstract syntax tree described in Section 4.2.3 . A variable and its data type form a single unified token (e.g., newWin  X  WINDOW ) used for categorization. In addition, all the JavaScript unit X  X  interaction with
DOM objects are then traced statically and recorded (e.g. GET :: INPUT . value ) as static analysis features for categorization. Table 5 shows some example DOM objects together with their respective methods and attri-butes that are traced during static analysis. 4.3.2. Dynamic analysis
Static analysis is not able to recover certain information that occurs at run time. Dynamic analysis (i.e., simulated execution of code) can extract helpful features along a default path of execution. Although dynamic analysis is incomplete (in the sense that it only examines a single execution path), such analyses can determine exact values of variables and may help by discarding unimportant paths.
 We illustrate how dynamic analysis can yield additional features for categorization in Fig. 6 . This sample JavaScript unit creates a dynamic text banner that scrolls in window X  X  status bar. The function window.set
Timeout () displays the string represented by  X  X  banner ( X  X  +index+  X  X ) X  X  after 100 ms, which makes the ban-ner text in the window change over time. Without dynamic analysis, we cannot recover what value msg. sub-string (0, index) refers to. More importantly, dynamic analysis allows us to extract the value of the  X  X  banner ( X  X  +index+  X  X ) X  X  variable. In this example, dynamic analysis also recovers the fact that the variable X  X  value is changing, hence a new feature is added to the feature set (i.e., CHANGES :: WINDOW.status ). 5. Evaluation
We tested our methods on the WT10G corpus, containing approximately 1.7M web pages from over 11k distinct servers. After pre-processing and cleaning, over 18k pages contained processable JavaScript scripts units. String identical duplicates and structurally identical script units were then removed. This resulted in a final corpus of 1637 units. The low ratio ( 1:10) of unique scripts to script instances validates our claim that many scripts are simply clones.

We perform supervised text categorization using a support vector machine approach. SVMs were chosen as the machine learning framework as they handle high-dimensional datasets efficiently. This is extremely impor-tant as our feature sets contain over 5000 features. Specifically, we used the generic SVM algorithm (SMO) provided with WEKA ( Witten &amp; Frank, 2005 ). We use a randomized, ten-fold cross validation of the final corpus of 1637 script units, which excludes the other category. Instance accuracy is reported in the results. We report instance accuracy which has been used in previous work ( Ugurel et al., 2002 ).

Experiments were conducted on a single desktop machine with two gigabytes of main memory. Feature cre-ation for all script instances was fast, taking approximately three minutes. Similarly speedy, a 10-fold cross validation using the SVM classifier took 10 min. The exception to this was when edit distance needed to be computed. These features were computed in a brute-force, pairwise manner and took up to 10 h to generate. We are currently looking at lower complexity approaches that can speed up this feature generation step.
Our experiments aim to measure the performance difference using different sets of machine learning fea-tures. In all of the experiments, the baseline model tokenizes units and passes the tokens as individual features to the learner.

Table 6 shows the component evaluation in which we selected certain combination of features as input to the SVM classifier. Here, we can see a simple majority class categorization performs poorly, as this dataset consists of many classes without a dominating class. However, a simple text categorization baseline, in which strings are delimited by whitespaces performs very well and is accurate on 87% of the test instances. When informed lexical tokenization is done and combined with features from software metrics, static and dynamic analysis, we are able to improve categorization accuracy to around 94%. Perhaps unsurprisingly, using only software metrics and program comprehension features fail to contribute good classifiers. However, when cou-pled with a strong lexical feature component, we show improvement.

A good baseline performance may seem discouraging for research, but many important problems exist which exhibit the same property (e.g., spam detection, part of speech tagging). These problems are important and small performance gains are quite relevant. As such we also calculate the error reduction that is achieved by our methods over the text categorization baseline. By this metric, almost half of the classification errors are corrected by the introduction of our techniques. We now examine the individual feature types. 5.1. Lexical analysis
We hypothesized that token features and variable and function name normalization would enhance perfor-mance. The results show that simple typing of tokens as keywords, strings, URLs and HTML tags is effective at removing 8% of the categorization errors. Variable and function name splitting, expansion are proven to be less effective on our corpus. When both techniques are used together, synergy results removing 17% of errors. This validates our earlier hypothesis that program language features do positively impact program categorization. 5.2. Metrics We also break down our composite metric feature set into its components to assess their predictive strength. Our results also show that edit distance alone is not sufficient to build a good categorizer. Such a code reuse metric is not as accurate as our simple text categorization baseline. A finding of our work is that applying pub-lished software metrics  X  X  X s-is X  X  may not boost categorization performance much, rather these metrics need to be adapted to the classes and language at hand. Only when collectively used with lexical analysis is perfor-mance increased. 5.3. Program comprehension
Static and dynamic features alone do not perform well, but their combination greatly reduces individual mistakes (29% and 51% for the static and dynamic analyses, respectively). The combined feature set also does not beat the simple lexical approach, but does serve to augment its performance. 6. Annotation evaluation
Recall that the goal of our JavaScript categorization is to separate useful JavaScript units from those that are useless or annoying to the end user. To do this, we argued that a functionality-based categorization is a good starting basis and we used a single set of annotations to train a classifier to achieve such categorization.
Two issues emerge from this argument: (1) do the proposed functionality-based classes actually correlate with end users X  perception of usefulness? and (2) do people agree with our proposed classification scheme and are able to replicate our annotation.

To answer the first question, we conducted a survey to establish the  X  X  X sefulness X  X  ( X  X  X elpfulness X  X ) of each of our proposed categories. 16 subjects, all of who were daily computer and Internet users, took part in the sur-vey. The average rating among subjects is shown in Table 7 , where the rating  X  X +5 X  X  indicates the most useful correlated with other categories.
 We see that the proposed JavaScript categories indeed have different utility and helpfulness to end users.
Although the standard deviation (given in the third column of Table 7 ) is large, the average correlation between raters is 0.414, showing a moderate trend of our subjects to rate in the same manner. Out of all 16 subjects, only one had a slight inverse correlation with a few other subjects.

People largely agree with each other on the usefulness of most categories. Functionality such as pop-up , dynamic-text banner and static-text banner are considered annoying by most users, while some others like form processing are considered useful. A couple of them (e.g., background color ) are neither considered annoying nor helpful to end users, which we conclude as neutral functionality.

To settle the second issue, we needed to examine the reliability of our categorization scheme. We compiled an annotation guide which explained each of the categories and a web-based application to ask subjects to label data. Subjects were required to read the tutorial on JavaScript and the entire annotation guide before starting their annotation, and comparison notes among different categories were also provided during annotation.

As the entire 1.6K corpus is too large to expect volunteers to annotate, we randomly selected two docu-ments to represent each class (excluding the other class), giving a total of 2  X  32 = 64 instances for annotation. This distribution was not known to our additional annotators.

We then used the Kappa statistic ( Cohen, 1960 ) to measure the reliability of our annotation. The kappa statistic measures the agreement between multiple raters on ordinal data, correcting for chance agreement, portion of times they would agree by chance. j ranges between 1 (perfect agreement between annotators) and 0 (meaning no agreement, but merely chance agreement). We have asked four people (including one of the authors) to do the annotation. The kappa for our task based on these four annotators is 0.607; higher than 0.4 which is sometimes used as the border between an ill-defined task and one that is replicable.
A closer study of the disagreements in annotation show that a few categories lead to a disproportionate amount of disagreements. General classes such as trivial , interaction , form-processing were often mislabeled with more specific classes. Classes dealing with the same general functionality were also sometimes confused (the different classes for time related functions, or form related functions). Despite these systematic errors, we feel that our kappa score of 0.6 indicates that the classes are largely replicable and well-defined. 7. Conclusion and future work We have presented a novel approach to the problem of program categorization. In specific, we target Java-web page. A key contribution of our work is to create a functional categorization of JavaScript instances, based on a corpus study of over 18k web pages with scripts. To encourage other researchers to use our dataset as a standard reference collection, we have made our dataset, annotations and resulting system prototype freely available. 2
Although previous work ( Ugurel et al., 2002; Krovetz, Ugurel, &amp; Giles, 2003 ) has examined the use of text classification approaches to classify source code, our method is the first method that employs the source code in a non-trivial way. Different from previous work which classified code into topic categories, our work attempts to achieve a more fine-grained functional categories with less data. In this work, rather than treating the problem merely as a straightforward text categorization problem, we incorporate and adapt metrics and features that originate in program analysis. While our baseline does well, performance is greatly improved by utilizing program analysis. By the use of careful lexical analysis, static analysis and mock execution, a 52% overall reduction of categorization error is achieved. We believe this result provides strong evidence that pro-gram categorization can benefit from adapting work from program analysis. We have also validated our hypothesis that functionality based classification is feasible and useful by carrying out two human evaluations which examined our classification scheme X  X  usefulness and replicability.
 We have currently deployed our system as part of a smart JavaScript filtering system, that filters out specific
JavaScript units that are irrelevant to the web page. The aim is to assist users in filtering such material and to summarize such information for users to make more informed web browsing choices. JavaScript code contin-ues to impact the design and interactivity of the Web today in new and difficult guises (e.g., AJAX). We hope to extend our analysis further where possible to discover additional functionality to help automated indexers.
In future work, we can expand our current work to a larger dataset, adding in more entries in the hand-compiled dictionary (currently limited to 20 entries) for token normalization. We also hope to extend our tech-niques to subject-based classification on a wider range of computer languages. Acknowledgement
The authors sincerely thank the reviewers from the 2nd Asia Information Retrieval Symposium and Infor-mation Processing and Management for their valuable comments and Geunbae Lee for inviting us to publish in this special issue.
 References
 Current generat i on web pages are no longer s i mple stat i ctexts . As the web has pro-gressed to encompass more i nteract i v i ty, form process i ng, upload i ng, scr i pt i ng, applets and plug-i ns have allowed the web page to become a dynam i c appl i cat i on . Wh i le a boon to the human user, the dynam i c aspects of web page scr i pt i ng and applets i mpede the mach i ne pars i ng and understand i ng of web pages . Pages w i th JavaScr i pt, Macromed i a Flash and other plug-i ns are largely i gnored by web crawlers and i ndexers . When func-t i onal i ty i s embedded i n such web page extens i ons, key metadata about the page i s often lost . Th i s trend i s grow i ng as more web content i sprov i ded us i ng content management systems wh i ch use embedded scr i pt i ng to create a more i nteract i ve exper i ence for the human user . If automated i ndexers are to keep prov i d i ng accurate and up-to-date i n-format i on, methods are needed to glean i nformat i on about the dynam i c aspects of web pages .
 of JavaScr i pt, a popular web scr i pt i ng language . In many web pages, JavaScr i pt real-i zes many of the dynam i c features of web page i nteract i v i ty . Although understand i ng embedded applets and plug-i ns are also i mportant, we chose to focus on JavaScr i pt as 1) i ts code i s i nl i ned w i th i n an HTML page and 2) embedded JavaScr i pt often i nteracts w i th other stat i c web page components (unl i ke applets and plug-i ns) .
 model web pages X  funct i onal i ty and requ i rements . Pop-up block i ng, wh i ch has been extens i vely researched, i s j ust one of the myr i ad uses of JavaScr i pt that would be useful to categor i ze . Such software can ass i st automated web i ndexers to report useful i nfor-mat i on to search eng i nes and allow browsers to block annoy i ng scr i pt-dr i ven features of web pages from end users .
 features from text categor i zat i on, program comprehens i on and code metr i cs . We start by develop i ng a basel i ne system that employs trad i t i onal text categor i zat i on tech-n i ques . We then show how the i ncorporat i on of features that leverage knowledge of the JavaScr i pt language together w i th program analys i s, can i mprove categor i zat i on ac-curacy . We conduct evaluat i on of our methods on the w i dely-used WT10G corpus [4], used i n TREC research, to val i date our cla i ms and show that the performance of our system el i m i nates over 50% of errors over the basel i ne .
 features of the JavaScr i pt language and techn i ques i n program analys i s can ass i st i n cat-egor i zat i on . We then present our methods that d i st i lls features for categor i zat i on from the pr i nc i ples of program analys i s . We d e s c r i be our exper i mental setup and analys i s and conclude by d i scuss i ng future d i rect i ons of our work . A survey of prev i ous work shows that the problem of automated computer software categor i zat i on i s relat i vely new . We b e l i eve that th i s i s due to two reasons . F i rst, pro-gramm i ng languages are generally des i gned to be open-ended and largely task-agnost i c . Languages such as FORTRAN, Java and C are su i table for a very w i de range of tasks, and attempt i ng to define a fixed categor i zat i on scheme for programs i s largely sub-j ect i ve, and l i kely to be i ll-defined . Second, the research fields of textual i nformat i on retr i eval (IR) and program analys i s have largely developed i ndependently of each other . We feel that these two fields have a natural overlap wh i ch can be explo i ted .
 and has exact syntact i c structures . Th i s means that syntax plays an i mportant role that needs to be captured, wh i ch has been largely i gnored by text categor i zat i on research . th i s problem . They employ support vector mach i nes for source code class i ficat i on i na two-phase process cons i st i ng of programm i ng language class i ficat i on followed by top i c class i ficat i on . In the second, top i c class i ficat i on task, they largely rel i ed on each soft-ware pro j ects X  README file and comments . From the source code i tself, only i ncluded header file names were used as features . We b e l i eve a more advanced treatment of the source code i tself can ass i st such top i cclass i ficat i on . These features, i nclud i ng syntact i c i nformat i on and some language-spec i fic semant i c i nformat i on, could be i mportant and useful for class i ficat i on . Other recent work i n categor i z i ng web pages [14] has rev i ved i nterest i n the structural aspect of text . One hypothes i softh i s work i s that i nform i ng these structural features w i th knowledge about the syntax of the programm i ng language can i mprove source code class i ficat i on .
 favored over approx i mat i on . The subfield of program comprehens i on develops models to expla i n how human software developers learn and comprehend ex i st i ng code [9] . These models show that developers use both top-down and bottom-up models i nthe i r learn i ng process [12] . Top-down models i mply that developers may use a model of program execut i on to understand a program . Formal analys i sv i a code execut i on [2] may y i eld useful ev i dence for categor i zat i on .
 formance of programs . Of part i cular i nterest to our problem scenar i o are code reuse metr i cs, such as [3,5,7], as JavaScr i pt i nstances are often cop i ed and mod i fied from standard examples . In our exper i ments, we assess the pred i ct i ve strength of these met-r i cs on program categor i es .
 proved by adopt i ng features d i st i lled from program analys i s . Pr i or work shows that the use of IR techn i ques, such as latent semant i c analys i s can a i d program comprehens i on [8] . The key contr i but i on of our work shows that the converse i s also true: program analys i sass i sts i n text categor i zat i on . The problem of JavaScr i pt program categor i zat i on i s a good prov i ng ground to explore how these two fields can i nteract and i nform each other . JavaScr i pt i s mostly confined to web pages and performs a l i m i ted number of tasks . We b e l i eve th i s i s due to the restr i ct i ons of HTML and HTTP, and because web plug-i ns are more conduc i ve an en-v i ronment for appl i cat i ons that requ i re true i nteract i v i ty and fine-gra i ned control . Th i s property makes the text categor i zat i on approach well-defined, i n contrast to categor i za-t i on of programs i n other programm i ng languages . Secondly, JavaScr i pt has an i nt i mate relat i onsh i pw i th the HTML elements i n the web page . Form controls, d i v i s i ons and other web page ob j ects are controlled and man i pulated i n JavaScr i pt . As such, we can analyze how the web page X  X  text and i ts HTML tags, i n the form of a document ob j ect model (DOM), affect categor i zat i on performance .
 task, i nclud i ng circuits , database ,and development . They have assessed the i rworkon d i fferent types of languages, such as Java, C, and Perl . The i r work are based on large software systems and therefore these categor i es are des i gned for top i cal class i ficat i on of general software systems and does not fit our doma i nwell .
 www.js-examples.com . Th i ss i te has over 1,000 JavaScr i pt examples collected world-w i de . To allow developers to locate appropr i ate scr i pts qu i ckly, the web s i te categor i zes these examples i nto 54 categor i es, i nclud i ng ad , encryption , mouse , music and variable . made i t unusable for our purposes . F i rst, the class i ficat i on i s i ntended for the developer, rather than the consumer . Examples that have s i m i lar effects are often categor i zed d i ffer-ently as the i mplementat i on uses d i fferent techn i ques . In contrast, we i ntend to categor i ze JavaScr i pt funct i onal i ty w i th respect to the end user . Second, the class i ficat i on i sused for example scr i pts, wh i ch are usually truncated and for i llustrat i ve purposes . We b e l i eve that the i r class i ficat i on would not reflect actual JavaScr i pt embedded on web s i tes . based on a study of JavaScr i pt i nstances i n actual web documents . We use the WT10G corpus, commonly used i n web IR exper i ments, as the bas i s for our work . In the WT10G corpus, we see that JavaScr i pt that nat i vely occurs i n actual web pages are d i fferent and more d i fficult to handle . Actual web pages often embed mult i ple JavaScr i pt i nstances to events that deal w i th i nteract i on w i th the browser . For example, a page could have a set of scr i pts that performs browser detect i on (that runs at load t i me) and another separate set that val i dates form i nformat i on (that runs only when the text i nput i s filled out) . In add i t i on, some scr i pts are only i nvoked as subprocedures of others .
 than all of the scr i pts on a s i ngle page . A functional unit ,ors i mply unit , i sdefined as a JavaScr i pt i nstance, comb i ned w i th all of (potent i ally) called subprocedures . Any HTML i nvolved i nthetr i gger i ng of the un i t i salso i ncluded . F i gure 1 shows an exam-ple .
 Based on our corpus study, we created a class i ficat i on of JavaScr i pt i nto 33 d i screte categor i es, shown i n table 1 . These categor i es are based on funct i onal i ty rather than by the i r i mplementat i on techn i que . As i ngle other category i s used for scr i pts whose purpose i s unclear or wh i ch conta i ns more than one bas i c funct i onal i ty . use and encourage others to make use of th i s resource . Deta i ls of these resources w i ll be presented at the conclus i on of the paper . G i ven such a categor i zat i on, a standard text categor i zat i on approach would token i ze pre-class i fied i nput un i ts and use the result i ng tokens as features to bu i ld a model . New, unseen test un i ts are then token i zed and the result i ng features are compared to the mod-els of each category . The category most s i m i lar to the test un i t would be i nferred as i ts category .
 result i ng tokens as separate d i mens i ons for categor i zat i on . An n d i mens i onal feature vector results, where n i s the total number of un i que tokens that occur i n all tra i n i ng un i t i nstances .
 token i zat i on can be i mproved by explo i t i ng the propert i es of the language . Second, we show that certa i n code metr i cs can help . Th i rd, features d i st i lled from program compre-hens i on i n the form of stat i c analys i s and dynam i c execut i on allow us to analyze how ob j ects i nteract w i th each other, wh i ch i n turn i nfluence an un i t X  X  class i ficat i on . 4.1 Us i ng Language Features for Improved Token i zat i on A syntact i c analys i s of a programm i ng language i s i nstruct i ve as i t helps to type the program X  X  tokens . After bas i c comp i ler-based token i zat i on, we d i st i ngu i sh the tokens of each un i t as to whether they are numer i c constants, str i ng constants, operators, var i -able and method names, or language-spec i fic reserved keywords, or part of comments . As JavaScr i pt draws from Java and web constructs, we further d i st i ngu i sh regular ex-press i on operators, URLs, file extens i ons i mages and mult i med i a, HTML tags and color values . Tokens of these types are tagged as such and the i r aggregate type counts are used as features for categor i zat i on .
 program . However, for conven i ence, programmers frequently use abbrev i at i ons or short forms for these names . For example, i ntheJavaScr i pt statement var currMon = mydate.getMonth() , currMon , mydate and getMonth are short forms for  X  current month  X  ,  X  my date  X  and  X  get month  X  respect i vely .
 connect these forms together, we need to normal i ze these non-standard words (NSW) to resolve th i s feature m i smatch problem [10] . We normal i ze such words by i dent i fy i ng l i kely spl i tt i ng po i nts and then expand i ng them to full word forms . Spl i tt i ng i s ach i eved by i dent i fy i ng case changes and punctuat i on use . Tokens longer than s i x letter i n length are also spl i t i nto smaller parts us i ng entropy reduct i on, prev i ously used to spl i t natural languages w i thout del i m i ters ( e.g. Ch i nese) . A follow i ng expans i on phase i s carr i ed out, i nwh i ch commonly abbrev i ated shorten i ngs are mapped to the word equ i valents ( e.g.  X  curr  X  and  X  cur  X   X   X  current  X  )us i ng a small (around 20 entr i es) hand-comp i led d i ct i onary .
 4.2 Code Metr i cs Complex i ty metr i cs measure the complex i ty of a program w i th respect to data flow, control flow or a hybr i d of the two . Recent work i n metr i cs [3,5] has been appl i ed to spec i fic software fam i l i es and most metr i cs are targeted to much larger software pro j ects (thousands of l i nes of code) than a typ i cal JavaScr i pt un i t (averag i ng around 28 l i nes) . As such, we start w i th s i mple, class i c complex i ty metr i cs to assess the i r i mpact on categor i zat i on . Examples of them are: Cyclomat i c Complex i ty (CC) Cyclomat i c complex i ty i saw i dely used control flow Number of Attr i butes (NOA) i s a data flow metr i c that counts the number of fields Informat i onal fan-i n (IFIN) i san i nformat i on flow metr i c, defined as IFIN = P + i n our corpus . These metr i cs count language structures that we found were prevalent i n the corpus and may be i nd i cat i ve of certa i n program funct i onal i ty . S i m i lar Statements (SS) counts the number of statements w i th s i m i lar structure . Bu i lt-i n Object References (BOR) counts the number of bu i lt-i nob j ects ( e.g. date , based on the syntax of the language, d i scussed next . 4.2.1 Code Reuse Us i ng Ed i tD i stance As i de from complex i ty metr i cs, we can also measure code reuse (also referred to as clone or plag i ar i sm detect i on) . Th i s i s part i cularly useful as many developers copy (and occas i onally mod i fy) scr i pts from ex i st i ng web pages . Thus s i m i lar i ty detect i on may ass i st i nclass i ficat i on . Dynam i c programm i ng can be employed to calculate a m i n i -mum ed i td i stance between two i nputs us i ng str i ngs, tokens, or trees as elements for computat i on . between two scr i pt i nstances . We use the class of the m i n i mal d i stance tra i n i ng un i tasa separate feature for class i ficat i on . However, th i s measure does not model the semant i c d i fferences that are i ntroduced when ed i ts result i n structural d i fferences as opposed to var i able renam i ng . Am i n i mal str i ng ed i td i stance may i ntroduce drast i c semant i c changes, such as an add i t i on of a parameter or delet i on of a cond i t i onal statement . code and correct for these d i screpanc i es . An AST i s a parse tree representat i on of the source code that model the control flow of the un i t and stores data types of i ts var i -ables . Therefore we can use the AST model to define a tree-based ed i td i stance (TED) measure between two JavaScr i pt un i ts . TED algor i thms are employed i n syntact i cs i m-i lar i ty detect i on [15] . However, as i sshown i nF i gure 2, the g i ven two code fragments are of the same funct i onal i ty, but have d i fferent syntact i c structures . Hence, syntact i c dard TED algor i thm used i n syntact i cs i m i lar i ty detect i on i s not l i kely to outperform a s i mple SED algor i thm for our task . As i de from the tree-based ed i td i stance measure, we can also measure s i m i lar i ty from a lex i cal-token approach (lex i cal-token based ed i t d i stance, LED) [6], i nwh i ch source codes are parsed i nto a stream of lex i cal-tokens, and these tokens become the elements for computat i on . Ed i t costs are ass i gned appro-pr i ately depend i ng on token types and values . We h av e i mplemented all three models and have assessed each approach X  X  effect i veness . 4.3 Program Comprehens i on Us i ng the Document Object Model So far we have cons i dered JavaScr i pt un i ts as i ndependent of the i r enclos i ng web pages . In pract i ce, s i nce JavaScr i pt un i ts may be tr i ggered by HTML ob j ects and may man i p-ulate these HTML ob j ects i n turn, a JavaScr i pt un i t has an i nt i mate relat i on w i th i ts page and i s often mean i ngful only i n context . These ob j ects are represented by a docu-ment ob j ect model (DOM) 1 . In fact, a un i twh i ch does not i nteract w i th a DOM ob j ect cannot i nteract w i th the user and i s cons i dered un i nterest i ng . Many var i ables used i n JavaScr i pt are DOM ob j ects whose data type can only be i nferred by exam i n i ng the en-clos i ng HTML document . Ta b l e 3 i llustrates two examples where the scr i pt references DOM ob j ects .
 These are i llustrated i n the JavaScr i pt un i t i nF i gure 3: on l i ne 1 DoIt() gets a refer-ence to a form ob j ect, on l i ne 4 the i nput ob j ect represented by frm.txt i s set to a value  X  ok  X  , and on l i ne 3 the ob j ect document calls i ts write method . The count of each of these DOM ob j ect references i s added as an i nteger feature for categor i zat i on . 4.3.1 Stat i cAnalys i s Certa i n aspects of the commun i cat i on between the DOM ob j ects and the target JavaScr i pt can be done by a stra i ghtforward analys i s of the code . We extract two types of i nformat i on based on th i s stat i c analys i s: tr i gger i ng i nformat i on and var i able data type i nformat i on .
 ( e.g. a form i nput field) and others occur when a page i s loaded, w i thout user i nter-act i on . Th i str i gger i ng type ( i nteract i ve, non-i nteract i ve) i s extracted for each un i tby an i nspect i on of the HTML . For un i ts tr i ggered by i nteract i on, we further extract the respons i ble DOM ob j ect and event handler . We also extract the lex i cal tokens from the enclos i ng web page elements for i nteract i ve un i ts . For example, an i nput button w i th atextvalue  X  restore  X  X  sl i kely to tr i gger a un i t whose class i s form restore ; l i ke-w i se, button i nputs w i th text labels such as  X  0  X  ,  X  1  X  , and  X  9  X  are i nd i cat i ve of the class calculator .
 cover the data type of ob j ects by trac i ng the flow as var i ables are i nstant i ated and as-s i gned . Th i s i s done w i th the ass i stance of the abstract syntax tree descr i bed i n4 . 2 . 1 . Avar i able and i ts data type form a s i ngle un i fied token ( e.g. newWin  X  WINDOW ) used for categor i zat i on . In add i t i on, all the JavaScr i pt un i t X  X  i nteract i on w i th DOM ob-j ects are then traced stat i cally and recorded ( e.g. GET::INPUT.value ) as stat i c analys i s features for categor i zat i on . 4.3.2 Dynam i cAnalys i s Stat i c analys i s i s not able to recover certa i n i nformat i on that occurs at run t i me . Dy-nam i c analys i s( i.e. , execut i on of code) can extract helpful features along the s i ngle, default path of execut i on . Although dynam i c analys i s i s i ncomplete ( i n the sense that i t only exam i nes a s i ngle execut i on path), such analyses can determ i ne exact values of var i ables and may help by d i scard i ng un i mportant paths .
 gor i zat i on i nF i gure 4 . Th i s sample JavaScr i pt un i t, taken from the WT10G corpus, creates a dynam i c text banner that scrolls i nw i ndow X  X  status bar . The funct i on window.setTime-out() d i splays the str i ng represented by  X  X  X anner X  X +index+ X  X ) X  X  after 100 m i ll i seconds, wh i ch makes the banner text i nthew i ndow change over t i me . W i thout dynam i c analys i s, we cannot recover what value msg.substring(0,index) refers to . More i mportantly, dynam i c analys i s allows us to extract the value of the express i on  X  X  X anner( X  X +index+ X  X ) X  X  . In th i s example, dynam i c analys i s also recovers the fact that the var i able X  X  value i s chang i ng, hence a new feature i s added to the feature set ( i.e. CHANGES::WINDOW.status ) . We tested the above methods on the WT10G corpus, conta i n i ng approx i mately 1 . 7M web pages from over 11K d i st i nct servers . After pre-process i ng and clean i ng of the WT10G corpus, over 18 K pages conta i ned processable JavaScr i pt scr i pts un i ts . Str i ng i dent i cal dupl i cates and structurally-i dent i cal scr i pt un i ts were then removed . Th i sre-sulted i n a final corpus of 1,637 un i ts, wh i ch are un i que i n textual form and structure . The h i gh rat i o of the number of scr i pt i nstances to un i que scr i pts val i dates our cla i m that many scr i pts are s i mply clones .
 (SVM) . SVMs were chosen as the mach i ne learn i ng framework as they handle h i gh-d i mens i onal datasets effic i ently . Th i s i s extremely i mportant as feature vectors conta i n anywhere from 3,000 to 8,000 features, depend i ng on wh i ch feature sets are used i nthe model configurat i on . Spec i fically, we used the gener i c SVM algor i thm (SMO) prov i ded w i th WEKA [13] . We use a random i zed, ten-fold cross val i dat i on of the final corpus of 1,637 scr i pt un i ts, wh i ch excludes the other category . Instance accuracy i s reported i n the results . Due to space l i m i tat i ons, we report i nstance accuracy wh i ch has been used i nprev i ous work [11] and have om i tted other IR metr i cs such as prec i s i on and recall . mach i ne learn i ng features . In all of the exper i ments, the basel i ne model token i zes un i ts and passes the tokens as i nd i v i dual features to the learner .
 of features as i nput to the SVM class i fier . Here, we can see the ma j or i ty class cate-gor i zer performs poorly, as th i s dataset cons i sts of many classes w i thout a dom i nat i ng class . However, a s i mple text categor i zat i on basel i ne, i nwh i ch str i ngs are del i m i ted by wh i tespaces performs very well, accurate on 87% of the test i nstances . When i nformed lex i cal token i zat i on i s done and comb i ned w i th features from software metr i cs, stat i c and dynam i c analys i s, we are able to i mprove categor i zat i on accuracy to around 94% . Perhaps unsurpr i s i ngly, us i ng only software metr i cs and program comprehens i on fea-tures fa i l to contr i bute good class i fiers . However, when coupled w i th a strong lex i cal feature component, we show i mprovement .
 s i gn i ficant, as demonstrated by the use of a one-ta i led t-test . We bel i eve s i gn i ficance i s ach i eved due to the large scale of the evaluat i on set X  X  degrees of freedom present i nthe class i ficat i on problem .
 i mportant problems ex i st wh i ch exh i b i t the same property ( e.g. spam detect i on, part of speech tagg i ng) . These problems are i mportant and small ga i ns i n performance do not make advances i n these problems less relevant . As such we also calculate the error reduct i on that i s ach i eved by our methods over the text categor i zat i on basel i ne . By th i s metr i c, almost half of the class i ficat i on errors are corrected by the i ntroduct i on of our techn i ques .
 Lex i cal Analys i s. We hypothes i zed that token features and var i able and funct i on name normal i zat i on would enhance performance . The results show that s i mple typ i ng of tokens as keywords, str i ngs, URLs and HTML tags i s effect i ve at remov i ng 8% of the categor i zat i on errors . Less effect i ve i swhenvar i able and funct i on names are normal-i zat i on through spl i tt i ng and expans i on . When both techn i ques are used together, the i r synergy i mproves performance, remov i ng 17% of errors . Th i sval i dates our earl i er hypothes i s that program language features do pos i t i vely i mpact program categor i zat i on . Metr i cs. We also break down our compos i te metr i c feature set i nto i ts components to assess the i r pred i ct i ve strength . Our results also show that ed i td i stance alone i s not suffic i ent to bu i ld a good categor i zer . Such a code reuse metr i c i s not as accurate as our s i mple text categor i zat i on basel i ne . Afind i ng of our work i s that apply i ng publ i shed software metr i cs  X  as-i s  X  may not boost categor i zat i on performance much, rather these metr i cs need to be adapted to the classes and language at hand . Only when collect i vely used w i th lex i cal analys i s i s performance i ncreased .
 Program Comprehens i on. Stat i c and dynam i c features alone perform do not perform well, but the i r comb i nat i on greatly reduces i nd i v i dual m i stakes (29% and 51% for the stat i c and dynam i c analyses, respect i vely) . The comb i ned feature set also does not beat the s i mple lex i cal approach, but serves to augment i ts performance .
 A Note on Effic i ency. The exper i ments i nth i s paper were conducted on a s i ngle, mod-ern desktop mach i ne w i th two g i gabytes of ma i n memory . In general, feature creat i on i s fast, for all 1 . 6K scr i pt i nstances i n our corpus took approx i mately 3 m i nutes, and a 10-fold cross val i dat i on of the SMO class i fier takes about 10 m i nutes . The except i on to the feature creat i on i s when ed i td i stance-based code reuse metr i cs were computed . These features are computed i n a brute-force, pa i rw i se manner and took up to ten hours to generate . We are currently look i ng i nto faster approaches that may lower the com-plex i ty of the approach . Our results are prom i s i ng, but we would l i ke to call attent i on to some of the shortcom-i ngs of our work that we are currently address i ng: Annotator Agreement. Our corpus i s annotated by one of the paper authors . Wh i le th i s prov i des for cons i stency, the annotator notes that some i nstances of problemat i c, even for a language whose appl i cat i ons are largely d i st i nct . We feel th i s a source of some errors and are work i ng on further annotat i on and find i ng i nter-annotator agreement . A reasonable upper bound of performance may be less than 100%, mean i ng that our performance ga i ns may be more s i gnficant than d i scussed i nth i s paper . Dynam i c Analys i s Incompleteness. Many tasks are executed cond i t i onally depend i ng on the browser X  X  type . In our dynam i c analys i s, we assume scr i pts are only executed under as MSIE 4 . 0, wh i ch causes certa i n analyses to fa i l to extract data . As browser check i ng i sub i qu i tous i n JavaScr i pts, we may relax th i s constra i nt and follow all exe-cut i on pathways that are cond i t i onal on the browser . We present a novel approach to the problem of program categor i zat i on . In spec i fic, we target JavaScr i pt categor i zat i on, as i ts use i s largely confined to a small set of purposes and i s closely t i ed to i ts enclos i ng web page . A key contr i but i on of our work i sto create a funct i onal categor i zat i on of JavaScr i pt i nstances, based on a corpus study of over 18,000 web pages w i th scr i pts . To encourage our researchers to use our dataset as a standard reference collect i on, we have made our dataset, annotat i ons and result i ng system freely ava i lable 2 .
 to class i fy source code, our method i s the first method that employs the source code i n a non-tr i v i al way . D i fferent from prev i ous work wh i ch class i fied code i nto top i c cat-egor i es, our work attempts to ach i eve a more fine-gra i ned funct i onal categor i es w i th less data . In th i s work, rather than treat i ng the problem merely as a stra i ghtforward text categor i zat i on problem, we i ncorporate and adapted metr i cs and features that or i g-i nate i n program analys i s . Our corpus study confirms that many such scr i pts are i ndeed cop i es or s i mple mod i ficat i ons . Wh i le our basel i ne does well, performance i s greatly i mproved by ut i l i z i ng program analys i s . By careful lex i cal analys i s, 10% of errors are el i m i nated . Further i mprovements us i ng stat i c analys i s and execut i on results i n a 52% overall reduct i on of categor i zat i on error . We bel i eve they prov i de ev i dence that program categor i zat i on can benefit from adapt i ng work from program analys i s .
 filters out spec i fic JavaScr i pt un i ts that have funct i onal i ty i rrelevant to the web page ( e.g. banner, pop-up) . We plan to extend th i s work to other scr i pt i ng languages and decomp i led plug-i ns appear i ng on web pages . The a i m of such work i s to ass i st end users to filter i rrelevant mater i al and to summar i ze such i nformat i on for users to make more i nformed web brows i ng a w i der var i ety of class i ficat i on ( i nclud i ng sub j ect-based class i ficat i on) on a w i der range of computer languages i n future work .

