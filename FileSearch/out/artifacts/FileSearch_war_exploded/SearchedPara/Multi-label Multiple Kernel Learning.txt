 problems with the optimal convergence rate, to compute the s olution efficiently. Drosophila embryogenesis [7]. Comparative analysis of such images can potentially reveal new variable number of labels by human curators in the Berkeley Drosophila Genome Project (BDGP) groups of images by employing the vocabulary-guided pyrami d match algorithm [9]. By applying formulation, while it is much more efficient. among labels. We propose to capture such information throug h a hypergraph as described below. 2.1 Hypergraph Spectral Learning Following the spectral graph embedding theory [10], we prop ose to compute the low-dimensional embedding through a linear transformation W by solving the following optimization problem: where  X  ( X ) = [  X  ( x each other in the embedded space.
 k be reformulated as KCCA is a special case of the proposed formulation. 2.2 A Semi-infinite Linear Program Formulation Following the MKL framework [12], we propose to learn an opti mal kernel matrix by integrating multiple candidate kernel matrices, that is, where { K linear constraints, as summarized in the following theorem : Theorem 2.1. Given a set of p kernel matrices { K where S Z = [ z 1 , , z k ] , H is obtained from C such that HH T = C , and H = [ h 1 , , h k ] . in Theorem 2.1 can be solved by the column generation techniq ue as in [14]. smooth convex optimization problem efficiently in the next s ection.
 By rewriting the formulation in Theorem 2.1 as and exchanging the minimization and maximization, the SILP formulation can be expressed as where f ( Z ) is defined as problem in Eq. (9) can be solved analytically: can be expressed as Proof. Define the Lagrangian function for the optimization problem in Eq. (9) as where {  X  tion with respect to  X  It follows from the complementarity condition that  X  have  X  in Eq. (9), we obtain that f Following 1 = P p problem in Eq. (7) by the following smooth unconstrained min imization problem: where f formulation in Eq. (13) is convex and has a guaranteed approx imation bound controlled by  X  . Lemma 3.2. The problem in Eq. (13) is a convex optimization problem.
 Proof. The optimization problem in Eq. (13) can be expressed equiva lently as cone constraints, the problem in Eq. (13) is a convex optimiz ation problem. Lemma 3.3. Let f ( Z ) and f f ( Z ) | X   X  log p .
 Proof. The term  X  P p bility distribution, since  X   X  0 and  X  T e = 1 . Hence, this term is non-negative and f is known from the property of entropy that  X  P p i.e.,  X   X   X  P p j =1  X  j log  X  j  X   X  log p . This completes the proof of the lemma. tion f Lemma 4.1. Let f f ( Z ) can be bounded from above as where L set of Z can be bounded as tr ( Z T Z )  X  R 2 C Proof. To compute the Lipschitz constant for the gradient of f second order derivatives as follows: where vec ( ) converts a matrix into a vector, D with the k th diagonal block as K where L We next derive the upper bound for tr ( Z T Z ) . To this end, we first rewrite S Since min f this inequality, it can be verified that tr ( Z T Z )  X  R 2 After the optimal Z is obtained from the Nesterov X  X  method, the optimal {  X  f ( X i )  X  f  X  ( X 0 ) for i = 1 , , N, we have where Z  X  = arg min  X  log p , we have By setting  X  = O (1 /N ) , we have that L of pattern images. The performance of the approximate formula tion is also validated. from the FlyExpress database ( http://www.flyexpress.net ). We apply nine local descrip-spacing on each image. These local descriptors are commonly used in computer vision problems apply the vocabulary-guided pyramid match algorithm [9] to construct kernels between the image matrix based on which the low-dimensional embedding is comp uted. We use the expansion-based ratio of 1:1. This process is repeated ten times, and the aver aged performance is reported. Performance Evaluation It can be observed from Tables 2 and 3 that in terms of both macr o and of macro F1 score. The number of terms used are 20, 30, and 40, and the nu mber of image sets used are 1000, 1500, and 2000.  X  X ILP X ,  X  X PP X ,  X  X VM1 X , and  X  X n iform X  denote the performance constructed, and  X  X CCA X  denotes the case where C = Y T ( Y Y T )  X  1 Y . # of labels 20 30 40 # of sets 1000 1500 2000 1000 1500 2000 1000 1500 2000 SILP star 0.4396 0.4903 0.4575 0.3852 0.4437 0.4162 0.3768 0.4019 0.3927 SILP clique 0.4536 0.5125 0.4926 0.4065 0.4747 0.4563 0.4145 0.4346 0.4283 SILP KCCA 0.3987 0.4635 0.4477 0.3497 0.4240 0.4063 0.3538 0.3872 0.3759 APP star 0.4404 0.4930 0.4703 0.3896 0.4494 0.4267 0.3900 0.4100 0.3983 APP clique 0.4510 0.5125 0.4917 0.4060 0.4741 0.4563 0.4180 0.4338 0.4281 APP KCCA 0.4029 0.4805 0.4586 0.3571 0.4313 0.4146 0.3642 0.3914 0.3841 SVM1 0.3780 0.4640 0.4356 0.3523 0.4352 0.4200 0.3741 0.4048 0.3955 Uniform 0.3727 0.4703 0.4480 0.3513 0.4410 0.4191 0.3719 0.4111 0.3986 BIK 0.4241 0.4515 0.4344 0.3782 0.4312 0.3996 0.3914 0.3954 0.3827 # of labels 20 30 40 # of sets 1000 1500 2000 1000 1500 2000 1000 1500 2000 SILP star 0.4861 0.5199 0.4847 0.4472 0.4837 0.4473 0.4277 0.4470 0.4305 SILP clique 0.5039 0.5422 0.5247 0.4682 0.5127 0.4894 0.4610 0.4796 0.4660 SILP KCCA 0.4581 0.4994 0.4887 0.4209 0.4737 0.4532 0.4095 0.4420 0.4271 APP star 0.4852 0.5211 0.4973 0.4484 0.4875 0.4582 0.4355 0.4541 0.4346 APP clique 0.5013 0.5421 0.5239 0.4673 0.5124 0.4894 0.4633 0.4793 0.4658 APP KCCA 0.4612 0.5174 0.5018 0.4299 0.4828 0.4605 0.4194 0.4488 0.4350 SVM1 0.4361 0.5024 0.4844 0.4239 0.4844 0.4632 0.3947 0.4234 0.4188 Uniform 0.4390 0.5096 0.4975 0.4242 0.4939 0.4683 0.3999 0.4358 0.4226
BIK 0.4614 0.4735 0.4562 0.4189 0.4484 0.4178 0.3869 0.3905 0.3781 graph outperforms the classical KCCA consistently.
 SILP versus the Approximate Formulation In terms of classification performance, we can observe kernel matrix in the MKL framework. The resulting formulati on leads to a non-smooth min-max on the task of automated annotation of gene expression patte rn images. the 23 kernels by SILP star and APP star on a data set of 40 labels and 1000 image sets. perform a detailed analysis of the weights in the future.
 1R01-GM079688-01) and National Science Foundation (IIS-0 612069 and IIS-0643494).
