 In recent years, wireless access to the WWW through mobile phones and other handheld devices has been growing significantly. Also new models of such mo-bile and handheld devices have been developed rapidly, however, there are many shortcomings associated with these devices, such as small screen, low bandwidth, and memory capacity. As the limited resolution and small screen restrict the amount of information to be displayed, it is very difficult to load and view a large document on the devices. Summarizing a document is currently becoming one of the most crucial solutions for dealing with this problem on these de-vices. Most automatic summarization systems extract important sentences from a source document and concatenate them together according to several crite-ria [5]. Sentences extracted as a summary tend to be so long and complex that even a summary is not easy to be displayed in a small device [15]. Therefore, compressing a sentence into an abstraction is practically helpful to display in a small device.
 a headline and a body. A headline of a news article is the most concise summary that represents a body. Also, Wasson [12] reported that a body usually begins with a good summarizing leading text. Generally speaking, a headline is shorter than a leading text in most cases. Therefore, our work is based on the premise that a headline can be considered as a compressed form of a leading text in news articles. We will refer a first sentence of a first paragraph in a news article as the leading sentence , and the compressed form for a leading sentence as compressed sentence in this work. Let us take a look at the following example. the sentence (1b) is the headline corresponding to (1a). Imagine that we make a headline for the given sentence (1a). As you can see, the relatively unimportant words are dropped from the leading sentence (1a), and then the remaining words 1 st ,4 th ,5 th and 8 th of (1a) build up the headline (1b). When generating a headline, even some postpositions that are attached to the tail of words are omitted from the words in the leading sentence. The postpositions (in grayed boxes) in (1a) are deleted from the words 1 st ,5 th and 8 th , respectively. ideas how to convert a leading sentence into a headline: The first is to remove inessential words such as additional adjunct phrases from leading sentences. The second is to replace longer and complex expressions with shorter and simpler ones. The third is to omit functional words such as postpositions in Korean if the meaning of a sentence remains same even without the functional words. erate a compressed sentence from the collection of pairs of leading sentences and their headlines. A compressed sentence generated automatically resembles a headline of news articles, so it can be one of the briefest forms preserving the core meaning of an original sentence. Also, this approach can be easily applicable to other languages only if a parallel corpus for the language is available online. briefly. Section 3 and 4 describe a model and a system for sentence compression respectively. Section 5 discusses results and an evaluation of our system. Finally we draw conclusions in Section 6. Many researches on automated text summarization have been performed since late 1950s and progressed very briskly while the amount of online text on the Internet keeps growing and growing [9]. Recently the researches have focused on extractive summarization, which extracts the most important sentences in texts and re-arrange them to be more readable and coherent. In this paper, we use leading sentences as summaries based on lead-based approach [12]. In case that the length of the leading sentences is very short, the leading sentences are enough as summaries for mobile devices and other handheld devices with limited screen size. To lighten this problem on limited screen size, a few of researchers have started to address the problem of generating compressed sentences. coherent abstracts that capture the most important pieces of information in the original documents. To do this, it uses a noisy-channel and a decision-tree approach. Their basic idea is originated from sentence-to-sentence translation based on a noisy-channel model, that is, they rewrite documents as source sen-tences into abstracts as target sentences. This works had been improved by their colleague, Lin [8] by using the concept on re-ranking using an oracle. theory. At the first step of the fractal summarization, a brief summary is gener-ated, and then a detail summary is produced on demands of users interactively. In this work, the important information is captured from the source text by exploring the hierarchical structure and salient features of the document. The compressed document is produced iteratively using the contractive transforma-tion in the fractal theory.
 This work is to learn how to decide what words or constituents can be dropped using a neural network model of Weka machine learning tool [13] and generates grammatically compressed sentences along with the number of words in the orig-inal sentence, the number of words in the constituent of a rewriting rule, and the depth of the constituent (distance to the root).
 generate subtitles for the deaf from transcripts of a television program, and used a hybrid method combining a statistic approach and a rule-based approach. The statistic approach was used for getting a ranking in the generated sentence alter-natives and the rule-based approach for filtering out ungrammatical sentences that are generated.
 readers. This method is composed of two components: one is a sentence analyzer which provides a syntactic analysis and disambiguation of the newspaper text, and the other is a simplifier which adapts the output of the analyzers to aid readability of aphasic people. The analyzer is language-dependent because it can treat the special syntax like passive voice and complex sentences. Also this system has some difficulties in learning rules on the text simplification. 3.1 Training Corpus Our training corpus is the collection of pairs of a leading sentence and its headline of news article, which is one of the data sources that we can easily gather. For the sake of simplifying a model, we collect only the pairs that satisfy the following 4 conditions. Assume a leading sentence S l =( w 1 , w 2 , ... , w N ) and its headline C1: N  X  M
C2: exists a simple function g : S c  X  S l ; it indicates that every word in S c is
C4: g ( x )= x or stem ( g ( x )) = stem ( x ), where stem ( x ) returns a stem of a pletely. We collect 1,304 pairs automatically from the news article corpus [10], which satisfy all the above conditions. 3.2 Marked Tree A marked tree for a sentence is a syntactic dependency tree with marked nodes. Its nodes are labeled over a word, its part-of-speech, and a mark symbol (0 or 1) and its edges are also labeled over syntactic categories between nodes. A leading sentence is syntactically analyzed into a marked tree. The nodes for the words common to a leading sentence and its headline are marked with 1 in a marked tree. The nodes for the words not included in a headline are marked with 0. Fig. 1 as an example shows the marked tree for the sentence (1a). In Fig. 1, the mark symbol on the node is actually represented by a background color of white (0) or gray (1). The nodes corresponding the words 1 st ,4 th ,5 th and 8 th are marked with 1 because they are also included in the headline (1b). 3.3 Modeling for Sentence Compression Our main goal is to generate compressed sentences resembling headlines of news articles. This is compression problem which is to produce a shorter one from an original longer sentence. In this work, a compressed sentence is generated from the marked nodes (henceforth a node marked with 1 is simply referred as  X  X arked node X ) in a marked tree, so it is very important to determine which nodes are marked with  X 1 X  out of all nodes in a syntactic dependency tree. To achieve this goal, we can compute the score of a marked tree for a given sentence as in [3] and generate a compressed sentence with marked nodes in a marked tree.
 is composed of the i -th word w i ,itsPOStag t i , and its mark m i ,thatis n i = ( w i ,t i ,m i ), and let h ( i ) be a position of the head of the node n i in a sentence, and let r i be a relation between the node n i and its head n h ( i ) and e i be an a marked tree represented as T m =( e 1 ,e 2 , ..., e N ) and the score of the marked tree T m can be calculated by Equation (1): where N is the number of the nodes in a marked tree. Our goal is to find the marked tree that maximizes Equation (1). It is not easy to estimate Pr ( m i = 1 | T m ), therefore it can be approximated as Equation (2).
 ment described later, we will smooth the score of Equation (2) using Equation (3) to avoid the data sparseness problem.
 where cou n t ( x ) is a frequency of x . In order to simplify the calculation of Equa-tion (1), we assume that the number of dependency trees for a given sentence is only one. Suppose that a dependency tree has N nodes. Then we should cal-culate the scores for the possible 2 N marked trees to find the marked tree that maximizes Equation (1). To compute the calculation more efficiently, we use a greedy method which is not optimal, but near-optimal. The selection policy of the greedy method first selects the highest significance node described immedi-ately below. The higher the significance of the node is, the higher the possibility that the node over the other nodes is marked with 1 is. The significance of a node is propositional to how many times the node is included in a compressed sentence, how many times the node given its parent is marked is included in a compressed sentence, and the inverse of a node depth. The significance M ( n i ) of a node can be defined as Equation (4).

M ( n i )= Pr ( m i =1 | t i ,r i )  X  Pr ( m i =1 | m h ( i ) =1 ,t i ,r i ,t h ( i ) )  X  where d ( n i ) is the depth of the node n i in a dependency tree, and  X  is a constant. The first and the second of the right-hand side of Equation (4) will be estimated as the same manner as Equation (3). Our system for sentence compression puts three steps together as shown in Fig. 2. The first step is syntactic parsing which analyzes an input sentence and produce a syntactic dependency tree. The second step is node-marking which decides if each node on the dependency syntactic tree is marked or not. The final step is surface realization which selects a lexical string and then generates a compressed sentence as a final output. 4.1 Syntactic Parsing Using a Korean probabilistic parser [7], we parse an input sentence. The original parser produces restricted phrase structures of a sentence although dependency structures are required in this work. The two structures are convertible [4] and it is not difficult to convert each other. In this work, we modify the original parser in order to provide dependency structures, which consist of nodes and relations as you can see in Fig. 1. 4.2 Node Marking Node marking is to determine whether each node in a syntactic dependency tree is included in a compressed sentence, or not. As previously mentioned, we use a greedy method to find a maximum marked tree with K marked nodes. The significance of each node can be calculated by Equation (4). We set the constant  X  in Equation (4) as 2.0 in this version. The simplified algorithm to determine a marked tree with the best score is shown in Fig. 3. The score of a marked tree in Equation (1) is an absolute score, which can not reflect the difference of the number of marked nodes. Therefore, the score cannot be directly compared with each other in case that there is a difference in the number of marked nodes. For the sake of compensation of the difference in the number of marked nodes, the score is normalized by the average of products, k Score ( T m ) as you can see in Fig. 3. 4.3 Surface Realization Surface realization takes a marked tree and produces a surface form as a com-pressed sentence by solving some morphology and syntactic problems. As our system is designed to generate a compressed sentence of which word order is same as that of an original sentence, the word order in surface realization is beyond question. In this work, there are basically three ways to realize surface strings for a compressed sentence: as-is , omission ,and replacement .Thewayof as-is produces an original word exactly as it is without any changes. The way of omission generates a reduced form of an original word which omits functional morphemes like postpositions. The way of replacement brings forth a totally dif-ferent form, which preserves the meaning of an original word. The replacement is kind of paraphrase in short form such as abbreviation, Korean sino-word 1 and so on. How to realize a surface form in a compressed sentence is surprisingly important as it can affect the fluency and understandability of a compressed sentence. In the following sentences, (1c) is the list of the marked nodes in the marked tree; (1b) is the headline for the sentence (1a). An input for the surface realization is (1c), and an output should be a similar one to (1b). surface realization. The case of replacement is considered as as-is .UsingC4.5, we train the surface realization how to generate surface forms for a list of marked nodes. A part of the training data we used in C4.5 are shown in Table 1 2 .The position in a sentence, the part-of-speech of a content part and a functional part of a word are used as the features in training data. The fourth column of Table 1 can be easily inferred by the difference between two words in (1b) and in (1c). Using decision rules that C4.5 induces, we can finally generate a compressed sentence similar to (1b) from the string (1c).
 5.1 Training and Testing Corpus The 1,304 pairs of the leading sentences and the headlines of news articles are used as a training/testing corpus in this paper. The average number of words in the leading sentences in the corpus is 12.67, while that in the headlines is 5.77. We divide the corpus into the 10 sub-parts for 10-fold cross validation. 5.2 Experimental Results The measures of the precision and the recall are used for the evaluation. The precision is calculated as the number of correctly marked nodes divided by the total number of nodes marked by the system. The recall is calculated as the number of correctly marked nodes divided by the number of marked nodes in the corpus. The average number of words in the compressed sentences created by our system is 6.85, which is longer than that in the headlines in the corpus. It results in far higher recall than precision as shown in Table 2. The interpretation of 77.3% precision is that one or two words among 6.85 words are incorrectly chosen in the compressed sentences.
 ated by the surface realization. As the surface realization is simply implemented in this version, we can get 94.5% accuracy in the compressed sentences. cision and the recall, we estimate the acceptability of the compressed sentences by humans. We present the original leading sentences and the generated com-pressed sentences to judge, and he/she is asked to assign  X  X  X  on the compressed sentences if they are acceptable as a compressed sentence, and assign  X  X  X  if they are not. The 74.51% of the total sentences are allowable as the compressed, as showninTable3.
 In this paper, we propose the system that can automatically learn how to gen-erate a compressed sentence from a collection of the pairs of leading sentences and their headlines. A compressed sentence generated automatically resembles a headline of news articles, so it can be one of the briefest forms preserving the core meaning of an original sentence. Also, this approach can be easily appli-cable to other languages only if a parallel corpus in the languages is available online. The surface realization should be improved to enable to paraphrase a longer expression into a shorter one in order to compress an input sentence more efficiently and more fluently.
