 The field of question answering has been moving away from simple  X  X actoid X  questions such as  X  X ho invented the paper clip? X  to more complex informa-tion needs such as  X  X ho is Aaron Copland? X  and  X  X ow have South American drug cartels been using banks in Liechtenstein to launder money? X , which cannot be answered by simple named-entities. Over the past few years, NIST through the TREC QA tracks has implemented an evaluation methodology based on the notion of  X  X nformation nuggets X  to as-sess the quality of answers to such complex ques-tions. This paradigm has gained widespread accep-tance in the research community, and is currently be-ing applied to evaluate answers to so-called  X  X efini-tion X ,  X  X elationship X , and  X  X pinion X  questions.
Since quantitative evaluation is arguably the sin-gle biggest driver of advances in language technolo-gies, it is important to closely examine the charac-teristics of a scoring model to ensure its fairness, re-liability, and stability. In this work, we identify a potential source of instability in the nugget evalua-tion paradigm, develop a new scoring method, and demonstrate that our new model addresses some of the shortcomings of the original method. It is our hope that this more-refined evaluation model can better guide the development of technology for an-swering complex questions.

This paper is organized as follows: Section 2 provides a brief overview of the nugget evaluation methodology. Section 3 draws attention to the vi-tal/okay nugget distinction and the problems it cre-ates. Section 4 outlines our proposal for building  X  X ugget pyramids X , a more-refined model of nugget importance that combines judgments from multiple assessors. Section 5 describes the methodology for evaluating this new model, and Section 6 presents our results. A discussion of related issues appears in Section 7, and the paper concludes with Section 8. To date, NIST has conducted three large-scale eval-uations of complex questions using a nugget-based evaluation methodology:  X  X efinition X  questions in TREC 2003,  X  X ther X  questions in TREC 2004 and TREC 2005, and  X  X elationship X  questions in TREC 2005. Since relatively few teams participated in the 2005 evaluation of  X  X elationship X  questions, this work focuses on the three years X  worth of  X  X efini-tion/other X  questions. The nugget-based paradigm has been previously detailed in a number of pa-pers (Voorhees, 2003; Hildebrandt et al., 2004; Lin and Demner-Fushman, 2005a); here, we present only a short summary.

System responses to complex questions consist of an unordered set of passages. To evaluate answers, NIST pools answer strings from all participants, re-moves their association with the runs that produced them, and presents them to a human assessor. Us-ing these responses and research performed during the original development of the question, the asses-sor creates an  X  X nswer key X  comprised of a list of  X  X uggets X  X  X ssentially, facts about the target. Ac-cording to TREC guidelines, a nugget is defined as a fact for which the assessor could make a binary decision as to whether a response contained that nugget (Voorhees, 2003). As an example, relevant nuggets for the target  X  X ARP X  are shown in Table 1. In addition to creating the nuggets, the assessor also manually classifies each as either  X  X ital X  or  X  X kay X . Vital nuggets represent concepts that must be in a  X  X ood X  definition; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. The distinction has important implications, described below.

Once the answer key of vital/okay nuggets is cre-ated, the assessor goes back and manually scores each run. For each system response, he or she de-cides whether or not each nugget is present. The final F-score for an answer is computed in the man-ner described in Figure 1, and the final score of a system run is the mean of scores across all ques-tions. The per-question F-score is a harmonic mean between nugget precision and nugget recall, where recall is heavily favored (controlled by the  X  param-eter, set to five in 2003 and three in 2004 and 2005). Nugget recall is computed solely on vital nuggets Table 1: Answer nuggets for the target  X  X ARP X . (which means no credit is given for returning okay nuggets), while nugget precision is approximated by a length allowance based on the number of both vi-tal and okay nuggets returned. Early in a pilot study, researchers discovered that it was impossible for as-sessors to enumerate the total set of nuggets con-tained in a system response (Voorhees, 2003), which corresponds to the denominator in the precision cal-culation. Thus, a penalty for verbosity serves as a surrogate for precision.

Note that while a question X  X  answer key only needs to be created once, assessors must manually determine if each nugget is present in a system X  X  re-sponse. This human involvement has been identified as a bottleneck in the evaluation process, although we have recently developed an automatic scoring metric called P OURPRE that correlates well with hu-man judgments (Lin and Demner-Fushman, 2005a). Table 2: Number of questions with few vital nuggets in the different testsets. Previously, we have argued that the vital/okay dis-tinction is a source of instability in the nugget-based evaluation methodology, especially given the manner in which F-score is calculated (Hildebrandt et al., 2004; Lin and Demner-Fushman, 2005a). Since only vital nuggets figure into the calculation of nugget recall, there is a large  X  X uantization ef-fect X  for system scores on topics that have few vital nuggets. For example, on a question that has only one vital nugget, a system cannot obtain a non-zero score unless that vital nugget is retrieved. In reality, whether or not a system returned a passage contain-ing that single vital nugget is often a matter of luck, which is compounded by assessor judgment errors. Furthermore, there does not appear to be any reliable indicators for predicting the importance of a nugget, which makes the task of developing systems even more challenging.

The polarizing effect of the vital/okay distinction brings into question the stability of TREC evalua-tions. Table 2 shows statistics about the number of questions that have only one or two vital nuggets. Compared to the size of the testset, these numbers are relatively large. As a concrete example,  X  X 16 X  is the target for question 71.7 from TREC 2005. The only vital nugget is  X  X irst F16s built in 1974 X . The practical effect of the vital/okay distinction in its current form is the number of questions for which the median system score across all submitted runs is zero: 22 in TREC 2003, 41 in TREC 2004, and 44 in TREC 2005.

An evaluation in which the median score for many questions is zero has many shortcomings. For one, it is difficult to tell if a particular run is  X  X etter X  than another X  X ven though they may be very different in other salient properties such as length, for exam-ple. The discriminative power of the present F-score measure is called into question: are present systems that bad, or is the current scoring model insufficient to discriminate between different (poorly perform-ing) systems?
Also, as pointed out by Voorhees (2005), a score distribution heavily skewed towards zero makes meta-analysis of evaluation stability hard to per-form. Since such studies depend on variability in scores, evaluations would appear more stable than they really are.

While there are obviously shortcomings to the current scheme of labeling nuggets as either  X  X ital X  or  X  X kay X , the distinction does start to capture the intuition that  X  X ot all nuggets are created equal X . Some nuggets are inherently more important than others, and this should be reflected in the evaluation methodology. The solution, we believe, is to solicit judgments from multiple assessors and develop a more refined sense of nugget importance. However, given finite resources, it is important to balance the amount of additional manual effort required with the gains derived from those efforts. We present the idea of building  X  X ugget pyramids X , which addresses the shortcomings noted here, and then assess the impli-cations of this new scoring model against data from TREC 2003, 2004, and 2005. As previously pointed out (Lin and Demner-Fushman, 2005b), the question answering and sum-marization communities are converging on the task of addressing complex information needs from com-plementary perspectives; see, for example, the re-cent DUC task of query-focused multi-document summarization (Amig  X  o et al., 2004; Dang, 2005). From an evaluation point of view, this provides op-portunities for cross-fertilization and exchange of fresh ideas. As an example of this intellectual dis-course, the recently-developed P OURPRE metric for automatically evaluating answers to complex ques-tions (Lin and Demner-Fushman, 2005a) employs n -gram overlap to compare system responses to ref-erence output, an idea originally implemented in the R
OUGE metric for summarization evaluation (Lin and Hovy, 2003). Drawing additional inspiration from research on summarization evaluation, we adapt the pyramid evaluation scheme (Nenkova and Passonneau, 2004) to address the shortcomings of the vital/okay distinction in the nugget-based evalu-ation methodology.

The basic intuition behind the pyramid scheme (Nenkova and Passonneau, 2004) is simple: the importance of a fact is directly related to the number of people that recognize it as such (i.e., its popularity). The evaluation methodology calls for assessors to annotate Semantic Content Units (SCUs) found within model reference sum-maries. The weight assigned to an SCU is equal to the number of annotators that have marked the particular unit. These SCUs can be arranged in a pyramid, with the highest-scoring elements at the top: a  X  X ood X  summary should contain SCUs from a higher tier in the pyramid before a lower tier, since such elements are deemed  X  X ore vital X .

This pyramid scheme can be easily adapted for question answering evaluation since a nugget is roughly comparable to a Semantic Content Unit. We propose to build nugget pyramids for answers to complex questions by soliciting vital/okay judg-ments from multiple assessors, i.e., take the original reference nuggets and ask different humans to clas-sify each as either  X  X ital X  or  X  X kay X . The weight as-signed to each nugget is simply equal to the number of different assessors that deemed it vital. We then normalize the nugget weights (per-question) so that the maximum possible weight is one (by dividing each nugget weight by the maximum weight of that particular question). Therefore, a nugget assigned  X  X ital X  by the most assessors (not necessarily all) would receive a weight of one. 1
The introduction of a more granular notion of nugget importance should be reflected in the calcu-lation of F-score. We propose that nugget recall be modified to take into account nugget weight:
Where A is the set of reference nuggets that are matched within a system X  X  response and V is the set of all reference nuggets; w m and w n are the weights of nuggets m and n , respectively. Instead of a binary distinction based solely on matching vital nuggets, all nuggets now factor into the calculation of recall, subjected to a weight. Note that this new scoring model captures the existing binary vital/okay dis-tinction in a straightforward way: vital nuggets get a score of one, and okay nuggets zero.

We propose to leave the calculation of nugget pre-cision as is: a system would receive a length al-lowance of 100 non-whitespace characters for ev-ery nugget it retrieved (regardless of importance). Longer answers would be penalized for verbosity.
Having outlined our revisions to the standard nugget-based scoring method, we will proceed to describe our methodology for evaluating this new model and demonstrate how it overcomes many of the shortcomings of the existing paradigm. We evaluate our methodology for building  X  X ugget pyramids X  using runs submitted to the TREC 2003, 2004, and 2005 question answering tracks (2003  X  X efinition X  questions, 2004 and 2005  X  X ther X  ques-tions). There were 50 questions in the 2003 testset, 64 in 2004, and 75 in 2005. In total, there were 54 runs submitted to TREC 2003, 63 to TREC 2004, and 72 to TREC 2005. NIST assessors have man-ually annotated nuggets found in a given system X  X  response, and this allows us to calculate the final F-score under different scoring models.

We recruited a total of nine different assessors for this study. Assessors consisted of graduate students in library and information science and computer sci-ence at the University of Maryland as well as volun-teers from the question answering community (ob-tained via a posting to NIST X  X  TREC QA mailing list). Each assessor was given the reference nuggets along with the original questions and asked to clas-sify each nugget as vital or okay. They were pur-posely asked to make these judgments without refer-ence to documents in the corpus in order to expedite the assessment process X  X ur goal is to propose a re-finement to the current nugget evaluation methodol-ogy that addresses shortcomings while minimizing the amount of additional effort required. Combined with the answer key created by the original NIST assessors, we obtained a total of ten judgments for every single nugget in the three testsets. 2
We measured the correlation between system ranks generated by different scoring models using Kendall X  X   X  , a commonly-used rank correlation mea-sure in information retrieval for quantifying the sim-ilarity between different scoring methods. Kendall X  X   X  computes the  X  X istance X  between two rankings as the minimum number of pairwise adjacent swaps necessary to convert one ranking into the other. This value is normalized by the number of items being ranked such that two identical rankings produce a correlation of 1 . 0 ; the correlation between a rank-ing and its perfect inverse is  X  1 . 0 ; and the expected correlation of two rankings chosen at random is 0 . 0 . Typically, a value of greater than 0 . 8 is con-sidered  X  X ood X , although 0 . 9 represents a threshold researchers generally aim for.

We hypothesized that system ranks are relatively unstable with respect to individual assessor X  X  judg-ments. That is, how well a given system scores is to a large extent dependent on which assessor X  X  judgments one uses for evaluation. This stems from an inescapable fact of such evaluations, well known from studies of relevance in the information retrieval literature (Voorhees, 1998). Humans have legitimate differences in opinion regarding a nugget X  X  impor-tance, and there is no such thing as  X  X he correct an-swer X . However, we hypothesized that these varia-tions can be smoothed out by building  X  X ugget pyra-mids X  in the manner we described. Nugget weights reflect the combined judgments of many individual assessors, and scores generated with weights taken into account should correlate better with each indi-vidual assessor X  X  opinion. To verify our hypothesis about the instability of us-ing any individual assessor X  X  judgments, we calcu-lated the Kendall X  X   X  correlation between system scores generated using the  X  X fficial X  vital/okay judg-ments (provide by NIST assessors) and each individ-ual assessor X  X  judgments. This is shown in Table 3. The original NIST judgments are listed as  X  X ssessor 0 X  (and not included in the averages). For all scoring models discussed in this paper, we set  X  , the param-eter that controls the relative importance of preci-sion and recall, to three. 3 Results show that although official rankings generally correlate well with rank-ings generated by our nine additional assessors, the agreement is far from perfect. Yet, in reality, the opinions of our nine assessors are not any less valid than those of the NIST assessors X  X IST does not occupy a privileged position on what constitutes a good  X  X efinition X . We can see that variations in hu-man judgments do not appear to be adequately cap-tured by the current scoring model.

Table 3 also shows the number of questions for which systems X  median score was zero based on each individual assessor X  X  judgments (out of 50 Table 4: Kendall X  X   X  correlation between system rankings generated using the ten-assessor nugget pyramid and those generated using each individual assessor X  X  judgments. (Assessor 0 represents the original NIST assessors.) questions for TREC 2003, 64 for TREC 2004, and 75 for TREC 2005). These numbers are worrisome: in TREC 2004, for example, over half the questions (on average) have a median score of zero, and over three quarters of questions, according to assessor 9. This is problematic for the various reasons discussed in Section 3.

To evaluate scoring models that combine the opin-ions of multiple assessors, we built  X  X ugget pyra-mids X  using all ten sets of judgments in the manner outlined in Section 4. All runs submitted to each of the TREC evaluations were then rescored using the modified F-score formula, which takes into ac-count a finer-grained notion of nugget importance. Rankings generated by this model were then com-pared against those generated by each individual as-sessor X  X  judgments. Results are shown in Table 4. As can be seen, the correlations observed are higher than those in Table 3, meaning that a nugget pyramid better captures the opinions of each individual asses-sor. A two-tailed t-test reveals that the differences in averages are statistically significant ( p &lt;&lt; 0 . 01 for TREC 2003/2005, p &lt; 0 . 05 for TREC 2004).
What is the effect of combining judgments from different numbers of assessors? To answer this question, we built ten different nugget pyramids of varying  X  X izes X , i.e., combining judgments from one through ten assessors. The Kendall X  X   X  corre-Figure 2: Average agreement (Kendall X  X   X  ) between individual assessors and nugget pyramids built from different numbers of assessors. Figure 3: Fraction of questions whose median score is zero plotted against number of assessors whose judgments contributed to the nugget pyramid. lations between scores generated by each of these and scores generated by each individual assessor X  X  judgments were computed. For each pyramid, we computed the average across all rank correlations, which captures the extent to which that particular pyramid represents the opinions of all ten assessors. These results are shown in Figure 2. The increase in Kendall X  X   X  that comes from adding a second as-sessor is statistically significant, as revealed by a two-tailed t-test ( p &lt;&lt; 0 . 01 for TREC 2003/2005, p &lt; 0 . 05 for TREC 2004), but ANOVA reveals no statistically significant differences beyond two as-sessors.

From these results, we can conclude that adding a second assessor yields a scoring model that is sig-nificantly better at capturing the variance in human relevance judgments. In this respect, little is gained beyond two assessors. If this is the only advantage provided by nugget pyramids, then the boost in rank correlations may not be sufficient to justify the ex-tra manual effort involved in building them. As we shall see, however, nugget pyramids offer other ben-efits as well.

Evaluation by our nugget pyramids greatly re-duces the number of questions whose median score is zero. As previously discussed, a strict vital/okay split translates into a score of zero for systems that do not return any vital nuggets. However, nugget pyramids reflect a more refined sense of nugget im-portance, which results in fewer zero scores. Fig-ure 3 shows the number of questions whose median score is zero (normalized as a fraction of the en-tire testset) by nugget pyramids built from varying numbers of assessors. With four or more assessors, the number of questions whose median is zero for the TREC 2003 testset drops to 17; for TREC 2004, 23 for seven or more assessors; for TREC 2005, 27 for nine or more assessors. In other words, F-scores generated using our methodology are far more dis-criminative. The remaining questions with zero me-dians, we believe, accurately reflect the state of the art in question answering performance.

An example of a nugget pyramid that combines the opinions of all ten assessors is shown in Table 5 for the target  X  X ARP X . Judgments from the original NIST assessors are also shown (cf. Table 1). Note that there is a strong correlation between the original vital/okay judgments and the refined nugget weights based on the pyramid, indicating that (in this case, at least) the intuition of the NIST assessor matches that of the other assessors. In balancing the tradeoff between advantages pro-vided by nugget pyramids and the additional man-ual effort necessary to create them, what is the opti-mal number of assessors to solicit judgments from? Results shown in Figures 2 and 3 provide some an-swers. In terms of better capturing different asses-sors X  opinions, little appears to be gained from going beyond two assessors. However, adding more judg-ments does decrease the number of questions whose median score is zero, resulting in a more discrim-inative metric. Beyond five assessors, the number of questions with a zero median score remains rela-Table 5: Answer nuggets for the target  X  X ARP X  with weights derived from the nugget pyramid building process. tively stable. We believe that around five assessors yield the smallest nugget pyramid that confers the advantages of the methodology.

The idea of building  X  X ugget pyramids X  is an ex-tension of a similarly-named evaluation scheme in document summarization, although there are impor-tant differences. Nenkova and Passonneau (2004) call for multiple assessors to annotate SCUs, which is much more involved than the methodology pre-sented here, where the nuggets are fixed and asses-sors only provide additional judgments about their importance. This obviously has the advantage of streamlining the assessment process, but has the po-tential to miss other important nuggets that were not identified in the first place. Our experimental results, however, suggest that this is a worthwhile tradeoff. The explicit goal of this work was to develop scor-ing models for nugget-based evaluation that would address shortcomings of the present approach, while introducing minimal overhead in terms of additional resource requirements. To this end, we have been successful.

Nevertheless, there are a number of issues that are worth mentioning. To speed up the assessment process, assessors were instructed to provide  X  X nap judgments X  given only the list of nuggets and the tar-get. No additional context was provided, e.g., docu-ments from the corpus or sample system responses. It is also important to note that the reference nuggets were never meant to be read by other people X  X IST makes no claim for them to be well-formed de-scriptions of the facts themselves. These answer keys were primarily note-taking devices to assist in the assessment process. The important question, however, is whether scoring variations caused by poorly-phrased nuggets are smaller than the varia-tions caused by legitimate inter-assessor disagree-ment regarding nugget importance. Our experiments appear to suggest that, overall, the nugget pyramid scheme is sound and can adequately cope with these difficulties. The central importance that quantitative evaluation plays in advancing the state of the art in language technologies warrants close examination of evalua-tion methodologies themselves to ensure that they are measuring  X  X he right thing X . In this work, we have identified a shortcoming in the present nugget-based paradigm for assessing answers to complex questions. The vital/okay distinction was designed to capture the intuition that some nuggets are more important than others, but as we have shown, this comes at a cost in stability and discriminative power of the metric. We proposed a revised model that in-corporates judgments from multiple assessors in the form of a  X  X ugget pyramid X , and demonstrated how this addresses many of the previous shortcomings. It is hoped that our work paves the way for more ac-curate and refined evaluations of question answering systems in the future. This work has been supported in part by DARPA contract HR0011-06-2-0001 (GALE), and has greatly benefited from discussions with Ellen Voorhees, Hoa Dang, and participants at TREC 2005. We are grateful for the nine assessors who provided nugget judgments. The first author would like to thank Esther and Kiri for their loving support.
