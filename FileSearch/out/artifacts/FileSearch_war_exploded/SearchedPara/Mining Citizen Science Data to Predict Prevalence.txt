 The Cornell Laboratory of Ornithology X  X  mission is to in-terpret and conserve the earth X  X  biological diversity through research, education, and citizen science focused on birds. Over the years, the Lab has accumulated one of the largest and longest-running collections of environmental data sets in existence. The data sets are not only large, but also have many attributes, contain many missing values, and poten-tially are very noisy. The ecologists are interested in identi-fying which features have the strongest effect on the distri-bution and abundance of bird species as well as describing the forms of these relationships. We show how data min-ing can be successfully applied, enabling the ecologists to discover unanticipated relationships. We compare a variety of methods for measuring attribute importance with respect to the probability of a bird being observed at a feeder and present initial results for the impact of important attributes on bird prevalence.
 Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications X  data mining, scien-tific databases General Terms: Experimentation Keywords: Attribute importance, bagging, decision trees, model inspection, partial dependence function, sensitivity analysis
Ecology is fundamentally the science of understanding the distribution and abundance of organisms. Ecologists inter-ested in efficient environmental manipulation for conserva-tion and management of wild birds have two general needs: (1) to be able to accurately predict where a species is and  X  This work was supported by NSF ITR award EF-0427914. Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. is not found; and (2) to understand the causes of presence and absence of a species. Within ecology, the conventional paradigm for analyzing data and gaining insights has been the formulation and testing of a small set of statistical mod-els that are assumed, based on expert opinion, to be the most likely descriptions of the biological processes at work.
This conventional paradigm is now becoming unwork-able, overwhelmed by increasingly available large ornitho-logical data sets with many potentially important features (e.g., geographic data sets based on satellite imagery). One example of this is the Avian Knowledge Network (AKN, http://avianknowledge.net ), a group of university, gov-ernmental, and non-governmental ornithological organiza-tions that are combining their existing databases of bird distribution information. Currently, over 25 million bird ob-servation records exist in the AKN X  X  data warehouse, each record associated with data on over 200 environmental fea-tures, not even counting the additional geographic data. This volume of data requires new scalable analytical tools that provide ecologists with initial insights (hypotheses) to be subsequently examined in greater detail.

For this, ecologists need to identify features that are strongly associated with interesting patterns of species X  oc-currence and visualize their effects. The two main challenges addressed in this paper are (1) to develop and evaluate prac-tical strategies for automatically identifying subsets of im-portant features, and (2) to visualize the effects of important features.

Our analytical tools need to deal with several data quality challenges, including missing data, observer-specific biases, biases inherent in each of the multiple data-collection proto-cols, and potentially high inter-correlation among features. The tools must not only be able to accurately predict birds X  distributions, but also identify a potentially limited sub-set of features that are important in predicting distributions of birds. Empirical experience from other ecological studies suggests that only a small number of environmental features will likely have large impact on the distribution and abun-dance of each bird species.

In this paper, we report on our initial steps in using data mining to explore the AKN data. To eliminate protocol bias, we are limiting ourselves to data collected under a single col-lection protocol, examining only data on presence/absence of species. We make the following contributions: Section 5 contains our conclusions and future directions.
The data examined come from Project FeederWatch (PFW, http://birds.cornell.edu/pfw ), a winter-long survey of North American bir ds observed at bird feeders. PFW has been running since t he winter of 1987-88, and as of March 2005 had over 1 million submissions, which report a total of 11.7 million bird sightings. Participants report all observed species; therefore the data also imply the absence of all species that were not reported. For the 100 most in-teresting species, this adds about 90 million  X  X ird absence X  records to the data set.

Each PFW location and submission is described by mul-tiple attributes, which are provided by project participants. These attributes can be roughly grouped into features re-lated to observer effort, weather during the observation pe-riod, and attractiveness of the location and neighborhood area for birds. The observer-provided attributes were sup-plemented with several hundred additional descriptions of the environment that came from a variety of geographic data sets, e.g., the U.S. Census Bureau X  X  2000 census (human im-pact), the USGS National Elevation Dataset, the USGS Na-tional Landcover Dataset, and various descriptions of local climatic conditions (e.g., monthly snow depths, wind speed, temperature) from the National Climatic Data Center X  X  Cli-mate Atlas of the United States.

After data were screened to exclude those observation records that were improperly submitted, or did not have suf-ficient fields to be included in the analysis, a total of about 800,000 observation records with 197 interesting attributes were available for analysis. Recall that each record indicates the presence of a set of species and implies the absence of all other species. Hence for each bird species there are 800,000 records about its presence or absence, constituting a mas-sive amount of information about bird occurrence. These records still included a considerable fraction of fields where the participant-reported attributes were missing.
Because of the ecologists X  expectation that species X  pop-ulation trends and the most important influences on any species X  distribution will vary across the continent, we sub-divided the continent into ecologically-relevant units, us-ing 37 existing Bird Conservation Regions (BCRs; see http://www.nabci-us.org/map.html ). There are 600+ BCR-species pairs with sufficient data for a data mining analysis. In this paper we limit our attention to the sim-pler, but nevertheless very challenging, problem of analyz-ing data from nine BCR-species pairs; specifically we ana-lyze the American Goldfinch, Dark-eyed Junco, and House Finch in BCR X  X  5, 22, and 30. Due to space constraints, however, we only present results for House Finch in BCR 30 (the U.S. Atlantic coastal plain region from southern-most Maine to northern-most Virginia); the high-level conclusions hold for the other 8 pairs. BCR 30 has 92,514 observations, during which the House Finch is present 55,860 times.
In Section 1, we noted the ecologists X  primary goals of pre-dicting and understanding the causes of bird species X  distri-butions and abundances. Motivated by this first ecological goal, one facet of our work has focused on providing pre-cise predictions of changing presence of feeder birds both within a winter, and across years. Motivated by the ecolo-gist X  X  second goal, we have explored multiple techniques for identifying a small number of attributes, from the larger-dimensional attribute set, that together have the most im-portant role in predicting bird X  X  presence or absence.
Data mining tools are well suited for the exploratory anal-ysis of species occurrence in space and time. For exam-ple, the yearseason graph in Figure 5 illustrates how data mining analysis can produce summaries of changing distri-bution, in this instance showing the inter-annual trend in presence of House Finches at feeders, after accounting for the effects of all other features. (See analysis details in Sec-tion 4.) A steep, disease-induced decline in House Finch occurrence clearly emerges from our analysis.

Conventional ecological analyses of species occurrence based on statistical techniques (e.g., Generalized Linear Models [13]) can detect changes in occurrence and identify important subsets of variables. However, these techniques require expert training and tuning, especially when there are complications like missing data, the need to fit non-linear effects, and potentially many interactions. These chal-lenges quickly overwhelm standard techniques when faced with large sample sizes and large numbers of attributes.
Data mining tools offer better flexibility and near au-tomatic application compared to standard statistical tech-niques. Many data mining methods scale to large sample sizes and large numbers of attributes. Decision trees are par-ticularly well suited to dealing with missing data. Yet, there has been little emphasis to date on extracting information about variable importance from these predictive models.
Approaches that rely on comparisons of the predicted probability surface tend to be conceptually simple but com-putationally expensive. The essential problem is that com-paring predicted probabilities as a function of any single focal attribute, such as in Figure 5, requires enough pre-dictions at each value of the focal attribute to effectively marginalize over the effects of all other attributes (see also discussion in Section 4). The number of predictions nec-essary for this marginalization step increases dramatically with the number of attributes, quickly becoming infeasible. The problem is further compounded when interactions are considered for pairs and tuples of attributes.

We therefore need a fast heuristic approach to identifying sets of important attributes that enables ecologists to later refine and investigate the effect of important attributes. Our approach focuses on mining the structure inherent in the predictive models. Then interaction plots are produced to visualize the effects of important attributes. We describe our approaches and compare them in the following section.
Our general approach is to first build highly accurate non-parametric models that capture the relationships between attributes. Then we determine important attributes by an-alyzing the model. For the analysis we selected bagged deci-sion trees as our model of choice [2, 1], because trees can cap-ture non-linear relationships and they handle missing values gracefully. In recent work it has been shown that bagged trees are competitive with the best available learning meth-ods [5, 14]. Decision trees provide the added benefit of being intelligible. The tree structure reveals valuable information about attribute importance based on which attributes the tree selects for splits.

We have examined and compared several approaches to measuring variable importance. One of these methods is a  X  X lack box X  method; i.e., it examines only the inputs and outputs of the model and does not depend on the structure of the model itself and therefore can be applied to any classifier. The other methods are  X  X hite box X  techniques that analyze the structure of individual trees in an ensemble.
In the following sections we discuss these methods and compare their results. For all experiments the tree ensemble consists of 100 ID3 trees 1 , built using the IND package [4]. The data sets are partitioned into roughly 2/3 training and 1/3 testing. Building the entire ensemble on a single pro-cessor modern PC (3.6 GHz, 1 GB RAM) takes about 2 hours.
We use Breiman X  X  sensitivity analysis technique [3] to as-sess the importance of a feature while treating the learned model as a black box (this method is related to randomiza-tion and permutation tests used in statistics [12]). The idea is to compare the performance of the model on a test set before and after noise is added to the target feature.
To measure the importance of feature A ,all A -values are shuffled, essentially permuting the original vector of values (when viewing the data set as a matrix whose rows are the different observation reco rds and columns correspond to the different features). If the attribute is important, perfor-mance should drop on the perturbed test data set compared to the real one, because the model relies on the spoiled val-ues when making predictions.
 There are many different measures of model performance. For our sensitivity analysis, we selected a diverse set of com-monly used measures to avoid measure-related bias. In par-ticular, we based the importan ce rankings on three different metrics: accuracy (ACC), root mean squared error (RMS), and ROC area [15] (ROC). The performance was measured on a separate test set, i.e., none of the test records was used for training the bagged trees.
 Table 1 shows the feature sensitivity results for the House Finch in BCR 30, sorted by RMS. An entry in the table reports the relative loss in pe rformance between the real and perturbed test data set. For example, let x be the model X  X  accuracy on the real test data, and y be its accuracy after permuting the latitude values. The relative loss for latitude is ( x  X  y ) /x . Relative loss for RMS is computed as ( y  X  x since lower RMS scores indicate better performance.
Sensitivity analysis is a relatively fast method for estimat-
Experiments with other tree types indicated that the choice of tree had fairly little effect on the ensemble performance. Table 1: Top-20 attributes for sensitivity analysis, sorted by RMS ing variable importance. Once the model is trained, we only need to evaluate its performance for different perturbed test data sets, one for each attribute. This is much faster than the costly approach of re-training models for different sets of attributes, as required for feature selection methods [10, 11, 8]. Nevertheless, for large high-dimensional data sets like PFW, even sensitivity analysis requires considerable re-sources: evaluating the sensitivity of a single feature using the 32K test cases for BCR 30 takes about 4-5 minutes. Using this approach for all 197 features of interest (or even pairs or larger sets of features) and for all 600+ BCR-species combinations requires access to expensive high-performance computing resources. In the following section we propose efficient heuristics to address this issue.
The methods discussed in this section leverage the fact that we are using ensembles of decision trees. We can in-spect the learned trees to see which attributes have been selected. Because selected attributes separate positive and negative observations, they are clearly important predictors. If an attribute is  X  X mportant X  for many of the trees in the ensemble, then we have strong evidence of its overall im-portance. The main challenge is defining a good measure to quantify an attribute X  X  importance in a tree and in an ensemble of bagged trees.

We have implemented several ranking methods that use only the information about the tree structure and how a training set is partitioned by the different trees. This in-formation is available once the ensemble is built, so there is no need to generate new models or new predictions in order to calculate these rankings. This is a clear advantage over black box methods like sensitivity analysis or feature selec-tion. We can compute the complete ranking of all features in less than 2 minutes (no matter which of the methods in-troduced below we are using), compared to 4-5 minutes per feature for sensitivity analysis X  X  factor of 500 speedup!
Theimportancescoreofanattributeforthetreeensem-ble is computed by summing the importance scores on the individual trees. To illustrate the differences between the methods, we will use the simple tree shown in Figure 1. It splits on three attributes: A , B ,and C . The training set has 100 records; numbers in parentheses indicate the number of records affected by the corresponding split (i.e., the num-ber of records in the corresponding subtree). We consider the following methods for computing attribute importance scores on a single tree.

Number of nodes (#nodes) . An attribute X  X  score is the number of nodes in the tree that selected the attribute for the split. In our example attribute A gets importance score 2, while B and C receive importance scores of 1 each. This method will give too much weight to continuous at-tributes, because the tree can split on them more often. The other methods address this issue.

Weighting by height (height) . Greedy tree growing al-gorithms usually choose the most important attributes early, so they appear higher in the tree structure. This method weights each node inversely proportionally to the length of the path from it to the root. The root itself is considered to have importance 1, so in the example attribute A receives importance 1 + 1 / 3 (importance of root + importance of the rightmost subtree split), while attributes B and C each have importance 1 / 2. The example in Figure 1 illustrates a problem with height-based weighting. Attributes B and C receive the same weight, whereas splitting on them af-fects different numbers of cases in the data set. To correct for this, the following methods take into consideration the number of training cases affected by the split.

Weighting by size of training set  X  multiple counting (multiple) . This method weights a node by the number of training cases in its subtree, i.e., the cases af-fected by the split at this node. In the example, attributes A , B ,and C receive scores of 160, 80 and 20, respectively.
Weighting by size of training set  X  single count-ing (single) .Aswith #node ranking, there is a risk that continuous attributes will be over-weighted when using the multiple counting of training points. In the example, the 60 records in the lower-right subtree with parent node A are counted twice towards A  X  X  score. To fix this problem, single counting assigns weight zero to all nodes that have an ances-tor with the same split attribute. In the example A receives an importance score of 100 instead of 160, while the scores for B and C do not change.

Weighting by size of training set  X  giving weight to the path (path) . This method compromises between single and multiple counting. Intuitively, training records from every leaf are distributed evenly between the splits on the path from the root to the leaf. Each split is still counted, even if there is another split on the same attribute in an ancestor node. In our example, the 30 records from the rightmost leaf are distributed between the two splits on A and the one split on B , i.e., 20 points go to A and 10 to B . Similarly, the 10 points from the leftmost leaf are given to
A and C , in this case 5 points to each. Counting from left to right, A receives an importance score of 5 + 5 + 10 + Figure 2: Comparison of different rankings (first 50 features shown). X-axis represents attributes in the order induced by ranking single , y-axis measures their position in other rankings. 20 + 20 = 60, B gets 0 + 0 + 10 + 10 + 10 = 30 and C gets 5+5+0+0+0 = 10. It is worth mentioning that importance scores for all attributes sum to the size of the training set used to build the tree. A similar method was used by Friedman [7] for estimating attribute importance in an ensemble of rules.
All three measures based on the size of training set in splitting nodes are very similar (Fig. 2). This result is sur-prising, because different ways of handling continuous at-tributes could in theory have significant influence on the resulting rankings. In practice we observed only minor dif-ferences. #nodes and height produced rankings that are very simi-lar to each other, but differ from the previous group. Fig. 3 shows that height and #nodes almost always agree, but are very different from the diagonal where they would be if they were correlated with single . Subsequent tests showed that results of these methods are less reliable than those of single , multiple and path (see Section 3.4).

One of the sensitivity analysis rankings  X  sensitivity-rms  X  shows a lot of similarity with the three most reliable methods from the  X  X hite-box X  group (Fig. 2). sensitivity-acc tends to agree with them only for the top ranked features and then shows a significant amount of discrepancy (Fig. 3). Because accuracy is known to be a high variance measure, while RMS is very stable, we have more confidence in the results of sensitivity-rms .( sensitivity-roc produced results similar to sensitivity-acc and therefore is omitted from the plot.)
The most important result here is the fact that our very fast white-box methods essentially identify the same top 20 features as the much more expensive black-box method. This result is also true for the other 8 BCR-species pairs we analyzed. It means that we can take advantage of the faster methods without sacrificing result quality.
There is no guarantee that taking the top-ranked features from any of these importance measures will yield an ensem-ble with good predictive power. While prediction accuracy Figure 3: Rankings that do not agree well with sin-gle . The farther from the diagonal each point is, the larger is the disagreement. Figure 4: Performance as a function of the num-ber of features used for training. Each line repre-sents a different method for ordering features by importance X  X ielding slightly different sets of fea-tures. is not the only goal of this study, it is a necessary precon-dition. Clearly we cannot hope to learn something about this domain by studying inaccurate models. Also, ecologists are interested in comparing the important attributes of a species occurrence in different BCRs. This can be achieved by comparing rankings, but only after checking that some minimum predictive performance is met in all analyses.
As a sanity check, we compared the performance of bagged trees trained using all features with bagged trees trained using only the top 20 features from the different importance rankings. With all features, the bagged trees achieve a RMS of 0.3469, accuracy of 0.8336, an d area under the ROC curve of 0.9012.

Figure 4 plots the ensemble X  X  RMS performance when only the top N features from each ranking are used, for different values of N . Because the rankings differ from each other, different features are included at each point for the different lines (Table 2 and the RMS column from Table 1). The overall pattern is similar for accuracy and ROC area, so we omit those graphs.

We make several observations from Figure 4. First, the Table 2: Top-20 attribute rankings;  X  X umfeeders  X  X s abbreviated as  X  X f  X . ensembles built using only 20 features perform quite well, al-though not quite as well as ensembles using all the features. The top 20 features do seem to catch most of the predictive power found in the full feature set. This gives us some con-fidence in relying on these measures as indicators of which features are important for modeling the PFW domain.
Second, while the rankings from single counting, path counting, and sensitivity-rms analysis show similar behav-ior, the height-based ranking behaves very differently. This agrees with the finding above that the height importance measure is not as highly correlated with the other measures.
One surprising aspect of this graph is that all the lines go up at least once: path at feature 2, single at feature 5, sensitivity at feature 6, and height for the first half of the graph. This phenomenon is partly caused by the feature dayselapsed ; whenever it is added, performance gets worse in this graph. Given that all the measures rank this fea-ture highly, and the ecologists believe it to be an important predictor, this is ra ther surprising.

Overall, we have identified methods for analyzing the en-semble model that produce very similar rankings of attribute importance. We have also shown that the resulting rankings are reasonable: models generated using only the top 20 fea-tures show good performance.
While the importance heuristics can be used to choose reasonable small sets of features, the heuristics will not find many other sets of important features that perform as well or even better. We have performed additional experiments where we trained the model using the top 16 features from Table 2 X  X  single column. Retraining the model without ac-cess to the latitude and longitude features (i.e., using only 14 features) results in equivalent performance. In our envi-ronmental data, attributes tend to be correlated and thus contain varying degrees of redundant information.
In this sense, the importance heuristics necessarily are in-ferior to feature selection at determining important features. The heuristics only find features that are important relative to the learned model, and not necessarily features that are important to many or all models.

As described in Section 1, one of the main goals for this project is identifying features that are important in predict-ing the abundance of bird species. Section 3 presented sev-eral heuristic methods for finding potentially important fea-tures. In order to decide if a ce rtain feature requires closer examination, ecologists need information about how the fea-ture affects the probability of observing the bird.
To provide such information we estimate and plot the probability of spotting the bird given different values of the feature in question. Figure 5 contains several examples of this kind of graph; for convenience we will refer to these graphs as trend plots . The rest of this section describes how we generate trend plots and discusses some sample plots.
We explore two methods for plotting trends: 1) comput-ing conditional probabilities directly from the data; and 2) computing Friedman X  X  partial dependence function [6] for the feature of interest, using the previously learned model to estimate probabilities. We will refer to these methods as data ,and partial , respectively.

Data: Given each value of the feature of interest, we compute the probability of seeing a bird. This is just the mean of all the points in our data set that have the given value of that feature. Points lacking a value for the feature (i.e., missing value) are not used. Continuous features are discretized into 5% quantiles to yield twenty distinct values for plotting, with each data point summarizing roughly the same number of data records. The top of each bin (quantile) is plotted on the x-axis. Note that continuous features are discretized identically for both methods.

Partial: For each value v to plot for feature X ,create an artificial data set D v by setting X = v for all the points in the test set. 2 Each artificial data set is labeled by the (previously learned) bagged tree model. The probability of observation when X = v is computed by averaging the predictions for the set D v . Missing values are a non-issue with this method.

The motivation behind partial dependence functions is that the target feature X may have high correlation with another feature Y for some values of X .If X is not an important influence but Y is, marginalizing to find X  X  X  in-fluence on seeing a bird (using the data method above) can make X look like an important indicator for values where it correlates well with Y (the truly important attribute). As a result, perceived observation trends as a function of X may be exaggerated or may not exist at all.

Substituting X = v for all points breaks up potential co-variances and forces the model to focus more on the impact of
X having value v . The only thing that changes between plot points is the value of X  X  holding all other features constant in some sense, while still maintaining the natural distribution of their values.

In theory, partial dependence functions can produce mis-leading plots in cases where we generate many new points in regions of the feature space unsupported by our data. The model, which was not trained on the data from those re-gions, can produce unpredictable results that will harm our trend plots. The detailed description of a similar problem
The mean value of each quantile is used as the substitution value for continuous features. can be found in [9]. In our analysis, we have not discovered this problem yet.
Because computing partial dependence functions for all 197 features is too computationally expensive, we examined the top 20 features from the single counting ranking (see Section 3.2). 3 Figure 5 shows six trend plots. Brief descrip-tions of the features plotted are given below.
 Each graph shows the probability of observing the House Finch in BCR 30 as a function of a given feature. The data and partial lines are marked with x  X  X  and o  X  X , respectively. As a general rule, the partial plots are much smoother than the data plots, which exhibit much more local variance. In most cases, however, both methods show the same general trends. Most of the comments below will focus on the partial plots, because they are easier to read and interpret. yearseason : The observed decline in occurrence is con-sistent with ecologists X  background knowledge that a novel bacterial pathogen, first appearing in 1994, has caused de-clines in abundance of House Finches across Northeastern North America. latitude/longitude : As we know from Section 3 these two features both have high importance ranking. Other ex-periments (omitted due to space constraints) also showed that they are highly associated both with each other and with many other attributes. We believe that these attributes describe spatial gradients and possibly act as proxies for other attributes that also exhibit spatial variation. The greater range of variation in the latitude effect may be due to the large North-South orientation of BCR 30. numfeeders hanging : This feature counts the number of hanging bird feeders in the observation area. As the num-ber of feeders increases from 0 to 5, we see an increasing probability of observation. The plateau effect past 6 feeders suggests that once there are sufficient feeders, adding more does not increase the chances of seeing a bird. dayselapsed : This variable counts the number of days elapsed since the beginning of the PFW season. Since the season begins on November 1, day 31 is the beginning of De-cember (for example). The observed pattern of probability of occurrence is consistent with the known partial-winter mi-gratory behavior of House Finch populations in the Eastern United States, where a proportion of the winter population migrates. pop00 sqmi : Thisisthe human population per square mile, as measured during the 2000 census. This is a good example of the partial plot differing from the data plot. The former suggests that the influence of population density on House Finch occurrence is relatively small, despite the fact that the model considers it important. The latter, however, would indicate that the probability of seeing a House Finch increases dramatically as population density goes up. Taken together, it seems more likely that population density cor-relates with other important indicators (especially given the large peaks and valleys in the data line).

The example of pop00 sqmi also shows that an attribute can be important for model prediction even though its par-tial line is close to a flat line. A flat trend line does not prove that a feature is unimportant. Rather, it just shows
Given the high correlation between the ranking methods for the top 20 features, the choice of ranking method is basically arbitrary.
 that in this marginalized setting the feature does not carry much predictive weight. Combined with other features, how-ever, it may be very important for making good predictions. Therefore, examining trend plots is not a viable way to iden-tify important feature sets by itself.
Finding important features for predicting the presence or absence of species is one of the major goals of ecology. The large data size, the large number of features, and the inher-ent quality issues of data collected by citizen science projects make this a truly challenging problem. In this paper we analyzed techniques where the importance of a feature is determined by how heavily an accurate data mining model relies on the feature for its predictions. More expensive ap-proaches like feature selection did not scale, resulting in poor response times even for this limited study.

We presented very fast heuristics for measuring attribute importance that are based on analyzing the structure of de-cision trees. An interesting outcome of this study is that all heuristics that measure importance by the number of train-ing cases affected by a node split produce almost identical feature rankings. Furthermore, the top 20 of these rank-ings are also highly correlated with those computed by much more expensive sensitivity analysis.

Once a small set of interesting features is identified, ex-pensive trend plots can be generated to gain a better un-derstanding of how certain features affect the observation probability for a species.
 The analysis presented in this paper was applied to 9 BCR-species pairs, for a single project (PFW). As pointed out earlier, ecologists ultimately want to compare and con-trast such results for all of the roughly 600+ pairs containing sufficient data. This, together with the rapid growth of data poses further demands for even faster techniques. For ex-ample, sensitivity analysis will not be a practical option at this scale.

Major directions of our future work include further anal-ysis of resulting models, important features and relation-ships between them. We plan to extend our analysis from identifying single important features to detecting groups of interacting attributes.
