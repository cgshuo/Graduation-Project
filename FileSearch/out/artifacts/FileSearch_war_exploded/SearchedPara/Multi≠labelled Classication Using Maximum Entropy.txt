 Many classication problems require classiers to assign each sin-gle document into more than one category, which is called multi-labelled classication . The categories in such problems usually are neither conditionally independent from each other nor mutually exclusive, therefore it is not trivial to directly employ state-of-the-art classication algorithms without losing information of relation among categories. In this paper, we explore correlations among categories with maximum entropy method and derive a classi-cation algorithm for multi-labelled documents. Our experiments show that this method signicantly outperforms the combination of single label approach.
 Categories and Subject Descriptors: H.3.3 [Information Sys-tems]: Information Search and Retrieval General Terms: Algorithms, Experimentation Keywords: multi-labelled classication, maximum entropy method
Data classication is the task of assigning each of the given data to a set of predened categories. In general, all classication prob-lems can be categorized as either single-labelled, or multi-labelled problems. Single-labelled data classication assumes that the pre-dened data categories are mutually exclusive and each data point can belong to exactly one category. Binary classication is the sim-plest case of the single-labelled problem where each data point is assigned to one of two predened categories. To date, many clas-sication methods, such as Naive Bayes, SVM, and Logistic Re-gression, have been developed to address the single-labelled clas-sication problem. On the other hand, with multi-labelled clas-sication, the data categories may not be either mutually exclu-sive or conditionally independent, and each data point can belong to multiple categories simultaneously. Multi-labelled classication problems are very common in the areas of document analysis and information retrieval. For example, a newspaper article about the presidential election may talk about a wide range of topics such as politics, economy, and foreign relations; an email discussing the ongoing business work may also include topics about the past va-cation the sender had experienced with his friends; etc. For docu-ment retrieval, a user may want to retrieve the news simultaneously belonging to multiple categories, which requires classiers to cor-rectly assign documents to all categories.

Despite the value and the signicance of the problem, research on multi-labelled classication has received much less attention compared to its single-labelled counterpart. Currently the most common solution to the multi-labelled classication problem is to decompose the problem into multiple, independent binary classi-cation problems, and determine the nal labels for each data point by aggregating the classication results from all the binary classi-ers. More precisely, for a given m predened categories, m classiers are independently created, one for each category, and are used to determine if a given data point belongs to the correspond-ing category or not. The nal category label for the data point is determined by combining the category labels generated by these binary classiers. The advantage of this approach is that a multi-labelled classier can be readily built using many start-of-the-art binary classiers off the shelf, such as SVM. However, when there exist strong correlations among categories, data classication per-formance may deteriorate because this approach employs a set of independent binary classiers to conduct data classications, and mutual correlations among different categories are completely ig-nored. More specically, given the input variables, the optimal es-timate should be the labels with the largest joint probability, instead of the combination of labels with largest individual probabilities of categories. Later we illustrate the difference in Section 3.2.
To take the dependencies among data categories into account, a straight-forward approach is to transform the multi-labelled clas-sication problem into a single-labelled problem by treating each possible combination of categories as a new class. In other words, a multi-labelled classication problem with ten predened classes would be transformed to a single-labelled classication problem with 1024 classes each of which corresponds to a possible combi-nation of the original data classes. However, this approach faces the problem of data sparseness because there could be very few data points in many combinations of the data classes.

In this paper, we propose a multi-labelled data classication method by explicitly modelling the mutual correlations among data categories using the maximum entropy principle. Our method ac-complishes the multi-labelled data classication task by construct-ing a conditional probability model Pr( y j x ) from the training data set, where x is the feature vector of the input data point, and is the class membership vector in which each element y whether x belongs to the i 'th class or not. In contrast to traditional approaches where Pr( y j x ) is usually determined by the class pri-ors and feature vectors of the input data, we construct Pr( y j x ) including an additional term  X  the dependencies among the data classes. We emplo y the Maximum Entrop y (ME) method to es-timate the parameters during the model construction process. To reect the estimation errors between the empirical and the real distrib utions, we introduce the regularization parameters, which serv e to avoid the over-tting problem for the model construction. This measure is in analogy to the penalized logistic regression, ex-cept for the items serving for the correlation among cate gory la-bels. Our experimental evaluations sho w that the proposed multi-labelled classication method reveals statistically signicant per -formance impro vements compared to traditional approaches.
The remainder of the paper is organized as follo ws. The related work is discussed in Section 2. In Section 3, we describe the model of multi-label maximum entrop y. Then, we present the experiments and results in Section 4. Finally Section 5 concludes the paper .
There is limited work on the problem of multi-labelled classi-cation. In the literature, man y research studies tak e ranking-based approaches which assign a real-v alued score to each document-cate gory pair , and classify each document by choosing all the cat-egories with the scores abo ve the given threshold. Schapire and Singer [16] proposed BoosT exter which essentially is an enhance-ment to AdaBoost to build the ranks for all document-cate gory pairs by using the boosting techniques. Elisseef f and Weston [8] developed a method using a kernel SVM as the ranking function for document-cate gory pairs. Crammer and Singer [6] proposed a family of one-against-rest online ranking algorithms that create a weight vector for each cate gory , and compute the ranking between a document and a cate gory using the inner product of the docu-ment' s feature vector and the cate gory' s weight vector .
Although ranking-based approaches pro vide a unique way to handle the multi-labelled classication problem, the y generally do not explicitly model the correlations among data cate gories. An-other problem such methods are facing with is that it is dif cult to determine into how man y cate gories a particular data should be classied, and thresholds are usually selected heuristically .
Another common approach to the multi-labelled classication problem is the modeling of classication using generati ve prob-abilistic models. McCallum [14] described a method based on generati ve model which assumes that each multi-labelled docu-ment is generated by a mixture of single-labelled document mod-els. The method resorts to na X ve Bayes model for each cate gory model by assuming the independence between words given cate-gory . The method emplo ys the expectation-maximization (EM) to estimate the model parameters and the mixture parameters. Ueda and Saito [17] also proposed a probabilistic generati ve model that uses a dif ferent mixture approach. The adv antage of these meth-ods is that the y explicitly model the cate gory correlations, and re-quire no threshold for determining the cate gory label for each data point. Ho we ver, because these methods usually assume words in-dependence and mixture of cate gory features within their proba-bilistic models, data classication accuracies could be limited be-cause these assumptions usually do not reect the real-w orld data congurations.

A closely related approach in the literature is the one proposed by Godbole and Sara wagi [10] that stacks two levels of SVM' s with heterogeneous features. Each lower level SVM is a single-labelled, one-against-rest classier with the original text features as the in-put. Combining the original text features, the outputs of the lower level SVM' s are used as the input of the higher level SVM' s which determine the nal cate gory label for each document. In Section 4, we implement a variation of this method, and compare it with our proposed multi-labelled data classication method for performance evaluations.

In addition, there is some other work closely related to the multi-labelled classication problem. Clare and King [4] developed a method, which uses a modied entrop y measure to extend the algo-rithm C4.5 to allo w nodes containing multiple labels. The method also uses resampling strate gies to deal with classes with small num-bers of examples. Similarly , Comite et al. [5] extended the alter -nating decision trees (ADT rees) algorithm for multi-labelled prob-lems. Each node of their multi-label ADT rees is associated with a set of real values, one for each label. Har -Peled et al. [11] de-scribed a constraint classication frame work. Under the frame-work, classication problems are translated into a binary classi-cation in a higher dimensional space with certain constraints. The paper also presented a meta-learning algorithm that learns via a single linear classier consistent with the constraints. Ho we ver, the correlations among labels were not explicitly discussed in the paper . Cai and Hofmann [2] proposed a hierarchical approach for multi-labelled/multi-class classication problem, where the prede-ned taxonomy is used to redene the loss functions. Gao et al. [9] extended their binary maximal gure-of-merit learning algorithm to multi-labelled classication problem. The method optimizes the performance against the approximated evaluation criteria, but the discriminant function for classication is still based on indi vidual cate gories.
Here we briey revie w the method of single-labelled data classi-cation using the maximum entrop y model. When we consider the relaxation of the constraints, it is equi valent to the penalized logis-tic regression method. Then, we give an example to demonstrate why the approach of combining single-labelled classiers does not work well for the multi-labelled classication problem, which im-plies the importance of modeling the dependenc y among data cat-egories. Finally , we propose the multi-labelled maximum entrop y approach to model the dependenc y among cate gory labels.
Let x = ( x feature vectors of the input data; let y denote the cate gory label vec-tor of a particular data point (we describe the details of ent situations). Statistical approaches accomplish the data classi-cation task by estimating the conditional probability Pr( y j x ) the training data, and determine the cate gory label ^ y of a given data point with the feature vector x using the follo wing equation: where Y represents the label space of the entire data set.
In this section, we briey introduce the single-labelled data clas-sication using the maximum entrop y model. Detailed descriptions of the method can be found in [15]. For simplicity , we only describe the binary classication case. Therefore, the label space Y = B where B is a binary space, containing 0 and 1 1 . Since y one dimension, we denote it as y . 1 In literature, some use 1 and 1 . Two representations are equi va-lent except for resulting dif ferent parameter values.
The principle of the maximum entr opy model (MEM) [12] is sim-ple: model all that is kno wn and assume nothing about what is unkno wn. In other words, given a collection of facts, the MEM chooses a model which is consistent with all the facts, but other -wise is as uniform as possible. In real implementations, facts are usually represented as a set of constraints, and the optimal model is acquired by maximizing the model' s entrop y under the given con-straints.

Let ~ P ( x ; y ) , Q ( x ; y ) denote the empirical and the model dis-trib utions, respecti vely . Traditional MEM-based data classication methods typically use the follo wing constraints for model selection: where hi x represents an element of the feature vector x . The abo ve two constraints serv e to force the model under construction to comply with the two statistical properties of the training data set: the prior probability of each cate gory , and the correlations among the cate-gories and features of the given data.

For the problem of data classication, the model to be estimated is the conditional probability Q ( y j x ) (denoted as a function of and x , q ( y j x ) , from now on) and the MEM obtains the optimal q ( y j x ) by maximizing the follo wing entrop y subject to the con-straints Eq. (2) and P where H ( x ; y j Q ) is the entrop y of x and y given distrib ution with parameter q . By expanding H ( x ; y j Q ) and ignoring the con-stants irrele vant to q ( q j x ) , we have the second part of Eq. (3).
The minimization of Eq. (3) is a typical constrained optmization problem that can be solv ed using Lagrange Multiplier algorithms. The Lagrangian of Eq. (3) is:
L ( q ( y j x ) ; b; w ; ( x )) = h log q ( y j x ) i Q where b , w = ( w tipliers. Omitting the mathematical deri vations (refer to [12, 7] for deri vation details), the optimal model ^ q tak es the form of where Z ( x ) = P
The constraints in Eq. (2) assume the model distrib ution equals the empirical distrib ution. Ho we ver, for a limited number of train-ing data, there exist estimation errors. Without considering such errors, the solution may lead to generation errors. To have a rob ust estimation, Chen and Rosenfeld [3] proposed to introduce max-imum a posteriori probability (MAP) model under the Gaussian prior into the constraints. Assuming and which follo w Gaussian distrib utions with zero means and variances of 2 =n and 2 =n ( n is the number of documents), respecti vely , we rewrite Eq. (2) as Eq. (6). where C is a parameter that can be used to set the tolerance of the estimation errors.
 With the rene wed constraints, the Lagrangian becomes: L ( q ( y j x ) ; ; ; b; w ; ; ( x )) = h log q ( y j x ) i where b , w = ( w grangian multipliers.
 By solving Eq. (7) and ignoring constants, we have
L ( b; w ) = D y ( b + w &gt; x ) + log Z ( x ) E where Actually , term and the feature terms, respecti vely . In man y applications, the bias term is not regularized, which means to set by adding a constant feature, the bias term is treated the same as the feature terms, which is equi valent to there are a lar ge number of training data, the dif ference between these two settings is very small. In our experiments, we set
Eq. (8) is actually penalized logistic regression (cf. [21]). The classication task is to nd the optimal parameters ^ b and imize L ( b; w ) in Eq. (8). Plugging the optimal parameters, ^ ^ w , into Eq. (5), we have optimal conditional distrib ution which is used to classify a given document with feature vector
For the multi-labelled classication problem, let y = ( y where m is the total number of cate gories, and each dimension of y indicates the membership of the data point in cate gory assuming the independence among the cate gories, the approach of combining single-labelled classiers for multi-labelled data classi-cation can be expressed as follo ws:
The follo wing example sho ws why combining single-labelled classiers does not always produce correct results for the multi-labelled classication problem when the cate gories are not inde-pendent. Assume that the joint distrib ution Pr( y data point x is sho wn in Table 1. Further assume that we trained two single-labelled classiers independently , which yields the con-ditional probabilities Pr( y ble. Because Pr( y data x is assigned to the rst cate gory y assigned to the second cate gory y ing to Table 1, Pr( y labels for data x is y combining the two single-labelled classiers is not correct!
Clearly , the approach of combining single-labelled classiers without considering the dependence among cate gory labels has its limitation on the multi-labelled classication problem. Therefore, we develop a multi-labelled data classier using the maximum en-trop y model in the follo wing section.
For the multi-labelled classication problem, we can extend the constraints in Eq. (6) to where 's and 's are estimate errors.

As the pre vious example sho ws, correlations among cate gories are important to the multi-labelled classication problem. To cap-ture such information, we add a new type of constraints to the maxi-mum entrop y model to require the model to comply with the second order statistical property y where 's are estimate errors.

Although it is possible to use other higher order statistics to model the cate gory dependencies, the cost of emplo ying such statis-tics may surpass the benets the y bring about. The higher order the statistics, the more parameters the model needs to estimate. With limited training data, models involving higher order statistics can hardly capture true distrib utions of the underlying data, and are lik ely to end up with little dif ference or even deteriorated perfor -mances compared to models using lower order statistics.
Again, the problem in our hands is to obtain the optimal q ( y j x ) that maximizes the entrop y in Eq. (3) subject to the constraints in Eq. (10), (11) and P pendix for the deri vation details), we have where Z ( x ) = P tion; b = ( b strict upper triangle matrix) are Lagrangian multipliers that need to be determined. By simplifying the Lagrangian and ignoring con-stants, we have L ( b ; R; W ) = D y &gt; ( b + R y + W x ) + log Z ( x ) where jj jj and ularization coef cients, and there values are to be specied by the user .

Here, the task of nding the optimal ^ q ( y j x ) becomes the prob-lem of nding the optimal b , W , and R that minimizes the La-grangian: Eq. (14) can be solv ed using gradient descent approaches. The deri vatives of L with respect to its parameters are There are man y gradient descent methods off the shelf. In [13], Malouf compared several algorithms for maximum entrop y param-eter estimation and suggests that the limited memory variable met-ric (LMVM) method is the fastest solv er for document classica-tion problems. Therefore, in our implementation we use LMVM in the TAO package [1] to estimate the parameters.

Once we have ^ b ; ^ R; ^ W , classifying a document with feature vec-tor x is equi valent to To label a data point, we can enumerate all possible label sets in to nd the most probable one using Eq.(15).
To sho w the benet of using multi-labelled maximum entrop y method, we evaluate the method against other methods on two real data sets.
The rst data set is the Reuters-21578 document corpus that con-tains 21578 documents collected from the Reuters newswire in 1987. It is a standard text cate gorization benchmark test set that consists of 135 document cate gories. In our experiments, we used the ten (10) lar gest cate gories for performance evaluations. Table 2 sho ws the statistics of document labels in our training set. It is observ ed from the table that only 6 : 5% of the documents in the training set possess multiple labels (i.e., multiple cate gories). Table 2: Numbers of multi-labelled document in the training set of Reuters-21578 data set.

To prepare the features for documents, we follo w the widely used bag-of-w ord approach. The features used in our experiments are words that appear more than once in the corpus. All the docu-ments are processed with the follo wing steps: remo ving SGML tags, downcasing, remo ving words on the SMAR T stoplist, stem-ming. The abo ve pre-processing has resulted in a total of 11084 words as the nal features. We emplo yed the TFIDF weighting scheme and the normalization in creating the feature vector for each document. We used the modied Apte ( X ModApte X ) split to create the training and the testing sets that consist of 9603 and 3299 doc-uments, respecti vely .

The second data set is an email corpus collected by us from six public domain mailing lists 2 . Our original purpose for creating such an email corpus is to monitor the R&amp;D acti vities of a project group and disco ver the contrib utions of each emplo yee through mining and analysis of emails among the group members. To serv e these purposes, we have dened the follo wing nine cate gories for email classication: (1) Topic Raising (RAISE), (2) Question Ask-ing (ASK), (3) Work Report (REP), (4) Information Announce-ment(INFO), (5) Dele gation (DEL), (6) Solution Proposal (SP), (7) Positi ve Comments (POSCOM), (8) Ne gati ve Comments (NEG-COM), and (9) Others (O THERS). Our pre-processing on the email corpus includes remo val of irrele vant information and extraction of implicit features. We remo ve the follo wing items from the body of each email: attachments (pictures, executable codes), mark er characters, quoted materials, email header , signature, time informa-tion, reply information, deb ug message, compiling message, source codes, etc. The extracted implicit features include: reply relation, reply indicator , hyper -links, ftp sites, itemization symbols,  X for -warded X  mark in email title, type of attached data, etc. In our email corpus, a lar ge percentage of emails are assigned with multi-labels. For example, 57 : 1% (474/830) of the emails in class RAISE also belong to class ASK; 41 : 2% (474/1150) of the emails in class ASK also belong to class RAISE. In our experiments, we use the rst eight (8) cate gories, and treat emails in the OTHERS cate gory as having no labels. We found that 34 : 6% of documents in the training set have more than one label (see Table 3). The percentage of multi-Table 3: Numbers of multi-labelled document in the mailing list data set. labelled documents in our email corpus is signicantly higher than that of the Reuters-21578 corpus. The procedures used for cre-ating the feature vector of each email are similar to those for the Reuters-21578 corpus, which results in a total of 3947 words as the nal feature set.
 Table 4 and 5 sho ws the mutual information, the p -values of Pearson' s chi-square test of pairs of cate gories in the Reuters-21578 and mailing list data set. From the table, we can see that some val-ues of mutual information are clear not zero and some p -values sho w that the dependenc y between cate gories is signicant (the smaller p -values indicate the stronger dependenc y between cate-gories). Hence, the approach of combining single-labelled classi-ers is insuf cient for these data. 2 The mailing lists are evolution-hack ers@lists.ximian.com, freebsd-amd64@freebsd.or g, freebsd-sparc64@freebsd.or g, gnome-de vel@gnome.or g, image-sig@p ython.or g, and public-esw@w3.or g.
For performance comparisons, we implemented two traditional methods and conducted performance evaluations using the same data corpora. The rst method is the combination of multiple, independent single-labelled classiers each of which emplo ys the single-labelled maximum entrop y model (which is equi valent to the penalized logistic regression), as described in Section 3.1. This method is denoted as  X COMB X  in our experiments. The second method is developed by stacking another layer of the penalized lo-gistic regression on top of the rst method, which adopts the idea of the approach described in [10]. We use the penalized logis-tic regression instead of SVM' s because we want to use the same loss function for data classication model so that the results are more comparable. This method is denoted as  X HF X  in our experi-ments. Our proposed multi-labelled classication method based on the maximum entrop y model is denoted as  X MLME X .

For a given document i , let y ( i ) and ^ y ( i ) be the true and the predicted label sets, respecti vely . We use the classication accurac y AC dened belo w as our performance metric.
 where n denotes the total number of documents in the test, is the delta function that equals one if x = y for all dimensions and equals zero otherwise. AC computes the percentage of the documents whose predicted labels are exactly the same as their true labels.

Though the accurac y measures are compatible with the loss func-tion of classication, which is considered as a smoothed version of 0 1 loss, we are also interested in the practical goal of informa-tion retrie val. For multiple label data sets, we usually use micro-averaged F where p and r are the precision rate and the recall rate computed globally over all binary decisions of all document-cate gory pairs, respecti vely . Since the micro-a veraged F all binary decisions, the partial correctness of labeling is credited. The rst experiment is on the Reuters-21578 document corpus. We used ten-fold cross validations to choose optimal regulariza-tion parameters for all the three methods. Table 6 sho ws the eval-uation results using the optimal parameters on ten 9 1 random splits. To compare the performance of dif ferent methods, we use one-sided Wilcoxon signed-rank test[18] which is a nonparamet-ric paired test without assuming the underline distrib ution of the tested values. Here, the alternati ve hypothesis is whether multi-labelled maximum entrop y method (MLME) has higher accuracies (or , F rank test between the given experiment and multi-labelled maxi-mum entrop y method are sho wn in Table 6. Although the impro ve-ment of accuracies and F signicant (usually , if the p -value is smaller than 0 : 05 signicant). The impro vement is not much, partially because that the data set only contains 6 : 5% multi-labelled documents and the percentage of documents not belonging to the ten cate gories are relati vely lar ge, 32 : 4% .

The second experiment is on the mailing list data set. Table 7 sho ws the accurac y and F eters on ten 9-1 random splits. The accurac y and F in the upper triangle. The p -values of Pearson' s chi-squar e test ( ) for pairs of categories are sho wn in the lower triangle. The numbers in the diagonal are the proportions of categories. in the upper triangle. The p -values of Pearson' s chi-squar e test ( ) for pairs of categories are sho wn in the lower triangle. The numbers in the diagonal are the proportions of categories. MLME method are better than those of the other two methods for every split, and the impro vement is statistically signicant.
The intention of the proposed multi-labelled maximum entrop y model is to include the correlations among cate gories into the model. Since the additional parameter R gory i and cate gory j in the model, we expect that R related the correlation between cate gory i and cate gory 1 and 2 plot correlations among cate gories and corresponding pa-rameters of R from one of the experiment runs. The gures clearly sho w that the relation between the correlation and parameter signicant and a pair with lar ge correlation usually has a lar ger R parameter , especially when the correlations are far from zero. The R parameters enforce the correlations among cate gories in the model. These gures conrm our assumption and indicate that the correlation terms (strictly speaking the second order moments of la-bels) are important in these multi-labelled classication problems.
In this paper , we propose a maximum entrop y method for multi-labelled classication, in which the correlations among cate gory labels are explicitly considered in the model. The experimental results sho w that multi-labelled classication is benecial in the model considering the correlation between classes, especially when the correlation is relati vely strong. By examining the parameters of the model, the experiments conrm our assumption that the corre-lation terms are important in multi-labelled classication tasks. One dra wback of this method is in computing the term Z of Eq. (13). One possible solution is to use a stochastic approach. Another possible solution is to approximate Z ( x ) with the sum of several important q ( y j x ) .

During the simplication of the model, we assume that estimate errors are independent from each other . We do not kno w how lar ge the impact is when this assumption does not hold. The future work may also involv e the investigation of correlations among estimate errors.
 We would lik e to thank Mei Han, Tao Li and anon ymous revie w-ers for useful comments and discussion. We thank the Mathemat-ics and Computer Science Division of Ar gonne National Lab for making Toolkit for Adv anced Optimization (TAO) package pub-licly available.
 Here are some details of how we deri ve Eq. (13) . For parameters and avoid the extreme results during estimating the parameters of the model. Assuming the joint probability of estimate errors should be reasonably lar ge, say greater than a small number , we write
To simplify this constraint of Eq. (16), we assume that those es-timate errors are independent to each other . Hence, we can rewrite Table 6: The accuracies and F Reuters-21578 dataset and their one-sided Wilcoxon signed-rank test vs MLME. this constraint in logarithm format as
According to the central limit theorem, the estimation errors fol-low normal distrib ution. Let and constraint can be simplied as where C is a constant deri ved from , 's and n .
 The Lagrangian of Eq. (3) subject to Eq. (10, 11, 18) and P y q ( y j x ) = 1 where b , R (strict upper triangle matrix), W , ( 0 ), and are the Lagrangian multipliers. Table 7: The accuracies and F mailing list data set and their one-sided Wilcoxon signed-rank test vs MLME.

The Karush-K uhn-T uck er (KKT) conditions require the deri va-tives of the Lagrangian with respect to its parameters must be zeros to maximize L . Therefore, we have:
When is zero, the problem is trivial. No w we assume that &gt; 0 . It allo ws us to express q , , and as functions of W , and : where the partition function, Z ( x ) = P W x )) . By plugging Eq. (21) into Eq. (19), we have Eq. (13). [1] Benson, S. J., McInnes, L. C., Mor  X  e, J., &amp; Sarich, J. (2004). TAO user manual (revision 1.7) (Technical Report ANL/MCS-TM-242). Mathematics and Computer Science
Division, Ar gonne National Laboratory . http://www .mcs.anl.go v/tao. [2] Cai, L., &amp; Hofmann, T. (2004). Hierarchical document cate gorization with support vector machines. CIKM '04:
Proceedings of the Thirteenth ACM confer ence on Information Figur e 1: The corr elations among categories and their corr e-sponding parameters of R in the Reuters-21578 data set. The dotted line is the linear regr ession of the data points, which in-dicates the trend of relation between the corr elations and and knowledg e mana gement (pp. 78 X 87). Washington, D.C.,
USA: ACM Press. [3] Chen, S. F., &amp; Rosenfeld, R. (1999). A Gaussian prior for smoothing maximum entr opy models (Technical Report CMU-CS-99-108). School of Computer Science Carne gie
Mellon Uni versity . [4] Clare, A., &amp; King, R. D. (2001). Kno wledge disco very in multi-label phenotype data. PKDD '01: Proceedings of the 5th Eur opean Confer ence on Principles of Data Mining and
Knowledg e Disco very (pp. 42 X 53). Springer -Verlag. [5] Comite, F. D., Gilleron, R., &amp; Tommasi, M. (2001). Learning multi-label alternating decision trees and applications.
Proceedings of CAP'01 (pp. 195 X 210). [6] Crammer , K., &amp; Singer , Y. (2002). A new family of online algorithms for cate gory ranking. Proceedings of the 25th annual international ACM SIGIR confer ence on Resear ch and development in information retrie val (pp. 151 X 158). Tampere,
Finland: ACM Press. [7] Della Pietra, S., Della Pietra, V. J., &amp; Laf ferty , J. D. (1997). Inducing features of random elds. IEEE Transactions on
Pattern Analysis and Mac hine Intellig ence , 19 , 380 X 393. [8] Elisseef f, A., &amp; Weston, J. (2002). A kernel method for multi-labelled classication. Advances in Neur al Information Processing Systems 14 (pp. 681 X 687). Cambridge, MA: MIT
Press. [9] Gao, S., Wu, W., Lee, C.-H., &amp; Chua, T.-S. (2004). A mfom learning approach to rob ust multiclass multi-label text cate gorization. ICML '04: Twenty-r st international confer ence on Mac hine learning . Banf f, Alberta, Canada: ACM Press. [10] Godbole, S., &amp; Sara wagi, S. (2004). Discriminati ve methods for multi-labeled classication. PAKDD . Figur e 2: The corr elations among categories and their corr e-sponding parameters of R in the mailing list data set. The dot-ted line is the linear regr ession of the data points, which indi-cates the trend of relation between the corr elations and [11] Har -Peled, S., Roth, D., &amp; Zimak, D. Constraint classication for multiclass classication and ranking. In S. T.
S. Beck er and K. Obermayer (Eds.), Advances in neur al information processing systems 15 . MIT Press. [12] Jaynes, E. T. (1957). Information theory and statistical mechanics. Physical Re vie w , 106 , 620 X 630. [13] Malouf, R. (2002). A comparison of algorithms for maximum entrop y parameter estimation. Proc. of the sixth
CoNLL . [14] McCallum, A. (1999). Multi-label text classication with a mixture model trained by EM. AAAI'99 Workshop on Text
Learning . [15] Nigam, K., Laf ferty , J., &amp; McCallum, A. (1999). Using maximum entrop y for text classication. IJCAI-99 Workshop on Mac hine Learning for Information Filtering (pp. 61 X 67). [16] Schapire, R. E., &amp; Singer , Y. (2000). Booste xter: A boosting-based system for text cate gorization. Mac hine
Learning , 39 , 135 X 168. [17] Ueda, N., &amp; Saito, K. Parametric mixture models for multi-labeled text. Advances in Neur al Information Processing
Systems 15 . MIT Press. [18] Wilcoxon, F. (1945). Indi vidual comparisons by ranking methods. Biometrics , 1 , 80 X 93. [19] Yang, Y., &amp; Liu, X. (1999). A re-e xamination of text cate gorization methods. Proceedings of the 22nd Annual International Confer ence on Resear ch and De velopment in Information Retrie val (SIGIR'99) (pp. 42 X 49). Berkle y: ACM
Press. [20] Zhang, T., &amp; Oles, F. J. (2001). Text cate gorization based on regularized linear classication methods. Inf . Retr . , 4 , 5 X 31. [21] Zhu, J., &amp; Hastie, T. (2003). Classication of gene microarrays by penalized logistic regression. Biostatistics .
