 KDD is a complex and demanding task. While a large num-ber of methods has been established for numerous problems, many challenges remain to be solved. New tasks emerge requiring the development of new methods or processing schemes. Like in software development, the development of such solutions demands for careful analysis, specification, implementation, and testing. Rapid prototyping is an ap-proach which allows crucial design decisions as early as pos-sible. A rapid prototyping system should support maximal re-use and innovative combinations of existing methods, as well as simple and quick integration of new ones.
This paper describes Ya le , a free open-source environ-ment for KDD and machine learning. Ya le provides a rich variety of methods which allows rapid prototyping for new applications and makes costly re-implementations unneces-sary. Additionally, Ya le offers extensive functionality for process evaluation and optimization which is a crucial prop-erty for any KDD rapid prototyping tool. Following the paradigm of visual programming eases the design of pro-cessing schemes. While the graphical user interface supports interactive design, the underlying XML representation en-ables automated applications after the prototyping phase.
After a discussion of the key concepts of Ya le , we illus-trate the advantages of rapid prototyping for KDD on case studies ranging from data pre-p rocessing to result visualiza-tion. These case studies cover tasks like feature engineer-ing, text mining, data stream mining and tracking drifting Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. concepts, ensemble methods and distributed data mining. This variety of applications is also reflected in a broad user base, we counted more than 40,000 downloads during the last twelve months.
 Track: Industrial Track Categories and Subject Descriptors: I.5.2 [Computing Methodologies]: Pattern Recognition General Terms: Design, Experimentation Keywords: KDD system, rapid prototyping, multimedia mining, audio and text mining, data stream mining, data pre-processing, result visualization, distributed data mining, feature construction
It is well known that knowledge discovery (KD) is a highly complex process. Like software development, it requires careful analysis, specification, implementation and testing. Prototyping plays an important role in this process. On the one hand, prototyping helps to identify adequate methods and optimal parameters. This enables developers to make crucial design decisions as early as possible in the knowl-edge discovery process. Costly redesign at later stages can be avoided. On the other hand, prototyping helps to control several risks. Most importantly, the performance of the en-visioned system can be estimated beforehand. This gives the customer an impression of the final result and its limitations. It also helps to clarify misunderstandings concerning the en-visioned outcome. Another important aspect is to estimate computation time and cost of the final system. Especially for applications with tight constraints on these resources (e.g. real time systems), such an estimation is essential in order to decide to which extent knowledge discovery can be applied.

A prototyping framework for knowledge discovery must meet several requirements. First, it should be very flexible with respect to methods for preprocessing and data analysis. This implies that it must offer a very broad range of differ-ent methods and that incorporating new methods is easy. Also, it should be able to process different kinds of input data, as time series or text data, without additional effort for the user. Second, a rapid prototyping tool for knowledge discovery must provide extensive functionality for evalua-tion and optimization. Finally, prototyping tools must be very easy to use. In particular, they should not require the user to learn a complex formalism. Many prototyping tools therefore employ the paradigm of visual programming.
The Ya le system 1 was developed to meet the above re-quirements. Ya le is an environment for machine learning experiments and data mining supporting the paradigm of rapid prototyping. Experiments can be made up of a large number of arbitrarily nestable operators and their setup is describedbyXMLfileswhichcaneasilybecreatedwitha graphical user interface. Applications of Ya le cover both research and real-world data mining tasks.
Real-world knowledge discovery processes typically con-sist of one or more data pre-processing, machine learning, evaluation, and visualization steps. Hence, a data mining platform should allow complex experiment designs, trans-parent data handling, comfortable parameter handling and optimization, flexible rearrangements, and extendibility. Fi-nally, a rapid prototyping environment for knowledge discov-ery must ensure that even complex experiments can easily be designed. In this section we will discuss some of the basic concepts of Ya le which eases the design of new methods.
Knowledge discovery (KD) processes are often viewed as sequential invocations of single methods. For example, after loading the data, one might apply a preprocessing step fol-lowed by an invocation of a classification learning method. The result in this case is the learned model, which can be applied on new and unseen data. A possible abstraction of these single methods is the concept of operators .Eachoper-ator receives its input, performs a defined action and deliv-ers some output. Hence, the sequential method invocations correspond to an operator chain. Although this chain model is sufficient for many basic knowledge discovery tasks, flat chains are often insufficient to model complex KD processes.
A common approach for more complex experiment designs is designing the operator combinations as directed graph. Each vertex of the graph corre sponds to a single operator. If two operators are connected, the output of the first op-erator should be used as input of the second. On the one hand, designing knowledge discovery processes with help of directed graphs is very powerful. On the other hand, it has a main drawback: due to missing restrictions and necessary topological sorting the design of experiments is often not intuitive and automatic validations are harder to perform.
Ya le offers a compromise between the simplicity of oper-ator chains and the power of directed graphs by modeling KD processes as operator trees . Similar to programming languages, using operator trees allows concepts like loops, conditions, or other meta application schemes. The leafs in the operator tree correspond to simple steps in the mod-eled process like learning a prediction model or applying a http://yale.sf.net Figure 1: The operator tree for a complex data min-ing experiment. The upper label denotes the opera-tor instance name, the lower label the operator class name. preprocessing filter. Inner nodes of the tree correspond to more complex or abstract steps in the process. This is of-ten necessary if the children should be applied several times like, for example, in loops. In general, inner operator nodes define the data flow through their children. The root of the tree corresponds to the whole experiment.

Figure 1 shows a nested KD process for feature selec-tion using a genetic algorithm with an inner cross-validation for evaluating candidate feature sets and an outer cross-validation for evaluating the genetic algorithm as a feature selector. This setup is modeled as an operator tree. The data flow is the same as for depth first search (DFS) with only one exception. If an inner operator node performs a loop, the input and output objects might be passed more than once to the children before the final output is gener-ated and given to the next sibling of the parent.
Operators define their expected inputs and delivered out-puts as well as their obligatory and optional parameters, which enables Ya le to automatically check the nesting of operators, the types of the objects passed between the oper-ators, and the mandatory parameters. This eases the design of complex data mining experiments. Following the concept of visual programming, users can easily create operator trees with the help of a graphical user interface. Break points be-fore or after operators can be used to check intermediate results and the data flow. This eases both debugging and interpreting the results.

Ya le internally uses XML to describe the operator trees which is easily readable by humans and machines. The XML description of an experiment consists of nested tag elements for all operators. Each operator element defines its instance name and operator class via XML attributes and the param-eters via inner tags. This XML experiment configuration define an interchange format for data mining experiments and also ensures the reproducibility of experiments. Since default parameters do not need to be specified, the XML description is a very concise representation of the operator tree structure.

The XML experiment configurations can be used as a scripting language for KD experiments. Each operator cor-responds to a single step or function invocation like prepro-cessing or model learning. In consequence, the data mining step of other programs can easily be configured by alter-ations of the basic XML scripts. This property was for ex-ample used by the Network Media Organizer Nemoz 2 .
Ya le  X  X  most important characteristic is the ability to nest operator chains and build complex operator trees. In order to support this functionality the Ya le data core acts like a data base management system and provides a multi-layered data view on a common data table which underlies all views. The views on this table do not contain the data itself but only references on rows or columns of this table. In case of a data set, views on the rows of the table correspond to subsets of the data, and views on the columns correspond to the selected features used to represent the examples.
All views are maintained by a stack of views. For exam-ple, the first view can select a subset of examples and the second view can select a subset of features. The result is a single view which reflects both views. The number of lay-ered views is virtually not limited. Other supported views include conditioned data set filters allowing only examples fulfilling a given condition, missing value filters, weighting views applying a weight on examples or features, or views creating new attributes on the fly.

This multi-layered view concept is a very efficient way to store different views on the same data table. This is espe-cially important for automatic data preprocessing tasks like feature generation or selection. In order not to unnecessarily copy the data set or subsets of it, Ya le manages views on this table, so that only references to the relevant parts of the table need to be copied or passed between operators. For example, the population of an evolutionary operator may consist of several data views -instead of several duplicates of parts of the data set.
No matter whether a data set is stored in memory, in a file, or in a database, Ya le internally uses special types of data tables to access the data via an uniform external interface. The data handling is hence transparent to the operators. Ya le achieves this transparent data handling by supporting several types of data sources and hiding internal data transformations and partitioning from the user or other operators. Therefore, operators do not have to cope with the actual data format or different data views.

In order to guide transformations of the feature space or the automatic search for the best preprocessing, the user can define additional meta data . Meta data include the type of attributes or their unit according to the international system of units (SI). This information is for example used by feature http://nemoz.sf.net Figure 2: A 2D plot of the radius function deter-mined with a Support Vector Clustering algorithm. This operator is part of the Clustering plugin. generation algorithms in order to prevent the aggregation of features which do not have the same unit of measurement. The definition of meta information is optional, Ya le tries to guess the correct data types automatically.
A rapid prototyping environment for knowledge discov-ery should already support a huge number of operations and state of the art methods. For example, Figure 2 shows the visualization of the result of a support vector clusterer. Together with the available plugins, Ya le provides more than 400 operators. This set of operators include a wide range of in-and output operators ,(Arff,C4.5,csv,orsparse formats, databases like Oracle, mySQL, or Postgres, and other file formats like dBase, text and audio files etc.), ma-chine learning operators (support vector machines, decision tree and rule learners, lazy learners, bayesian learners, lo-gistic regression, gaussian processes, meta learning tech-niques, association rule mining algorithms and clustering schemes etc.), data preprocessing operators (discretization, example and feature filtering, normalization, sampling, di-mensionality reduction, and additional and infinite value re-plenishment filters etc.), feature space transformation oper-ators (forward selection, backward elimination, genetic algo-rithms, weight guided, feature weighting and relevance cal-culation, feature construction and extraction etc.), evalua-tion operators and meta optimization operators (parameter optimization or experiment loops etc.). Of course, a wide range of visualization tools like online plots of your data or experiment results a re also provided.

Although Ya le provides this broad variety of operators for different data mining tasks, it might sometimes be nec-essary to implement a missing functionality. In order to im-plement a new operator, the developer simply needs to define the expected inputs, the delivered outputs, the mandatory and optional parameters, and the core functionality of the operator. Ya le provides an exhaustive application program-ming interface (API) with clean interfaces in order to allow quick access to the main methods. The operator description in XML allows Ya le to automatically create corresponding GUI elements. An easy-to-use plugin mechanism is provided in order to add functionality in a modular way which allows contributions by the Ya le community that do not require changes in the Ya le core. Several plugins are available in the download section of our web site.

Beside the graphical and the command line interface, Ya le can also be directly used from other programs. Clear pro-gramming interfaces define an easy way of applying single operators, operator chains, or complete operator trees on input data. Both the command line version and a Java API allow the invocation of Ya le from programs without using the GUI. Since Ya le is entirely written in Java, it runs on any major platform and operating system.
In the next section, we present several case studies which exemplify the basic design concepts of Ya le and show their relation to a broad variety of knowledge discovery tasks.
For many data mining applications, the predictive perfor-mance of a model can be improved drastically by considering several base learners in parallel. The use of ensemble meth-ods is especially appealing if the available amount of train-ing data is huge, which is true for most real-world knowledge discovery tasks. As a consequence of the super-linear run-time of most learning algorithms, learning from all the data is often too costly. A common work-around is to learn an ensemble from a single subsample. This often yields sub-optimal results, because it does not take full advantage of the available training data.

For large-scale data mining, Ya le offers a direct JDBC interface to relational databases. This eases the induction of base models from subsamples of tractable size. Moreover, due to the nestable operators, the induction of individual base models, each based on a separate database subsample, is possible with negligible efforts. This strategy prevents overfitting and circumvents the problems of scalability with-out waisting precious information during model building.
A strong point of Ya le in the context of ensemble meth-ods is the fact that all predictions and associated confidences are handled the same way as regular features are, but with-out losing their predictive semantics during cross-validations and similar operations. Hence, if required, predictions and associated confidence values may be used as regular base fea-tures for subsequent learning steps. This facilitates stacking, the optimization of base model weights for boosting, and it eases the integration of novel ensemble methods as Ya le operators for prototypical evaluation.

During the last years a variety of ensemble algorithms has been evaluated using the Ya le framework. The Bayesian Boosting operator can optionally start from a given pre-viously constructed ensemble, e.g. the result of a previous operator or a model read from file, and use it to weight the data at hand according to its current predictive perfor-mance. It then augments the ensemble by adding additional base models as required for the task. This way an ensemble of heterogeneous base learners can be constructed.
Apart from classification, the task of sequential subgroup discovery for given prior knowledge has recently been ad-dressed successfully [18].
Text mining covers a wide range of topics both in terms of methods and applications. Ya le and its plugins offer tools to tackle many of them. This includes e.g. information fil-tering of news texts for the automated generation of person-alized news from multiple sources, the automated sorting of documents or web pages into pre-defined categories, or e-mail routing, i.e. the automatic forwarding of e-mail mes-sages to the most appropriate person or department in a company or organization.

Text mining usually starts with data pre-processing. Tex-tual data often is unstructured, e.g. free text in natural lan-guage, or semi-structured, like e.g. in HTML documents, i.e. pages in the World Wide Web, or in XML documents. Many machine learning methods are designed for example sets with examples in an attribute value vector representa-tion. Hence the text data needs to be pre-processed, e.g. by transforming the free text in documents into document vectors in the vector space model [17]. In the vector space model ( bag of words model ), the order of the words in a document is ignored. Each word occurring in a document is considered as an attribute, and the frequency of this word in a particular document is taken as the value of this attribute for this document. Very frequent and hence uninformative words (stop words) like articles, prepositions, etc. are often removed. Afterwards, these term frequency (TF) values are often weighted to down-weight frequent words, e.g. by mul-tiplying the term frequency values with the logarithm of the inverse document frequency (IDF) [16].

The Word Vector Tool plugin 3 for Ya le provides oper-ators for pre-processing operations as stop word removal, word stemming, document vector generation, term weight-ing, etc. It allows to integrate sets of texts into Ya le ator chains just like any other kind of example sets that can be described by attribute value pair vectors.
 A particular problem for text mining are drifting concepts. Text data might be collected over an extended period of time and the target concept to be learned from the data stream may change. An important example is personalized informa-tion filtering, i.e. the adaptive classification of documents with respect to a dynamic user interest.

The concept drift plugin for Ya le enables Ya le to per-form data stream mining and concept drift tracking. This plugin provides operators to simulate data streams and con-cept drift on arbitrary data sets as well as to handle real concept drift on real-world data. The methods range from totally ignoring the drift, learning with complete or no mem-ory of old examples, and using time windows of fixed or adaptive size [9]. It also provides other example selection and weighting strategies [8] as well as variants of advanced boosting-like ensemble methods for handling concept drift [19] derived from those described in the previous section.
Ya le allows the seamless integration of all the aforemen-tioned steps from text data pre-processing via data stream simulation and handling, classifier learning, and concept drift tracking to operator chain and parameter variation and optimization as well as result visualization. The Word Vec-tor Tool and the concept drift plugin are the basis for many other text mining applications, e.g. text clustering or key-word extraction. http://wvtool.sf.net/
Distributed computing plays an important role in the data mining process for several reasons. First, Data Mining of-ten requires huge amounts of resources in storage space and computation time. For scalable systems, it is important to develop mechanisms that distribute the work load among several sites. Second, data is often inherently distributed into several databases, making a centralized processing of this data very inefficient and prone to security risks.
The Ya le distributed data mining plugin allows to per-form distributed data mining experiments in a simple and flexible way. The experiments are not actually executed on distributed network nodes. The plugin only simulates this. Simulation makes it easy to experiment with diverse network structures and communication patterns. Optimal methods and parameters can be identified efficiently before putting the system into use. The network structure can for example be optimized as part of the general parameter optimization. While this cannot replace testing the system in an actual network, it makes the development stage much more effi-cient. This follows the general philosophy of Ya le as rapid prototyping tool for large scale data mining applications.
Ya le was used for prototyping the distributed multime-dia organization system Nemoz 4 . Choosing the right rep-resentations of examples and hypotheses is essential to en-able successful multimedia mining. Feature construction is an approach to find such a representation independently of the underlying learning algorithm. Unfortunately, the con-struction of features usually implies searching a very large space of possibilities and is often computationally demand-ing. Therefore, we proposed an approach to feature con-struction that is based on transferring information between different data mining tasks [11]. Tasks are stored together with a corresponding set of constructed features in a dis-tributed case base. The case base is then used to constrain and guide the feature construction for new tasks. This is achieved by a new representation model for data mining tasks and a corresponding highly efficient distance measure. This approach is unique as it enables us to apply feature construction not only on a large scale, but also in scenarios in which communication cost plays an important role.
This kind of simulation is a good example for the power of the Ya le data model. Different nodes share the same underlying database with different views. Hence, data and attribute information does not need to be replicated. Shar-ing examples and attributes among nodes is simply accom-plished by  X  X electing X  them. A freely definable cost matrix allows to account communication cost and time in a very flexible way. The same holds for the network structure.
The aim of cluster analysis is to group data points into natural clusters of similar data points. Such natural clusters describe the original data space in a structured way. They can be however covered by noisy features, sparsity or redun-dant features. Feature space t ransformation is therefore an important preprocessing step for many clustering tasks. In a current project, we explore possibilities to apply feature selection and transformation to unsupervised learning [12]. http://nemoz.sf.net
Feature space transformation for unsupervised learning is inherently a multi-objective optimization problem. On the one hand, the quality of the resulting clusters should be im-proved. On the other hand, we want to preserve as much of the original data space as possible, such that the discov-ered clusters are a valid representation of the original data. We use the capabilities of Ya le to apply multi-objective feature space optimization as wrapper approach and com-bine it with several clustering algorithms as well as feature selection and construction schemes. All points of the result-ing Pareto set describe optimal solutions concerning cluster quality and minimal transformation of the original data.
Multi-objective optimization requires usually a large num-ber of individuals and generations. Ya le enables such large scale experiments as the internal data management does not duplicate any data points. Again, the individual feature sets are merely different views on the same data.
Other applications of Ya le include the prediction of pa-rameters for chemical proce sses [5] and learning the pre-processing of time series data [10]. The natural language processing and text mining architecture Gate [4] has an in-ternal Machine Learning API which will be based on Ya le in a future version. MusicMiner 5 is a music browser which extracts the necessary audio features by using the value se-ries plugin of Ya le .
A look at the KDnuggets software directory 6 reveals that a host of commercial data mining tools exists but only a few are freely available on an open source basis. Among the com-mercial tools, leading mining suites such as SPSS Clemen-tine 7 and others usually offer a flat workflow-oriented view on the operations, using no nestable operators, and includ-ing a fairly limited range of standard learning algorithms. Tasks such as cross validation, parameter optimization or automatic feature selection a re rarely directly supported. This situation is similar concerning the free software, but here at least developers may choose to extend the function-ality since the source code is available.

Probably the most well-known open source data mining package is Weka [20], a collection of Java implementations of machine learning algorithms. All Weka algorithms are di-rectly and seamlessly available in Ya le . Other open source projects can be classified according to whether they provide a GUI with an integrated data flow model (such as Weka or Tanagra 8 ), or consist of programming libraries like ADaM
The preparation of data is supported in Ya le by numer-ous feature selection and construction operators. However, Ya le is applied to a single input data table. The Mining-Mart software [13, 6] can be used to combine data from sev-eral tables, or to prepare larg e data sets inside a relational database instead of main memory as in Ya le . MiningMart provides operators that ease the integration with Ya le .
Ya le  X  X  XML-based experiment storage format can be seen as a language to model data mining processes which is inter-http://musicminer.sf.net http://www.kdnuggets.com/software/ http://www.spss.com/clementine/ http://chirouble.univ-lyon2.fr/  X  ricco/tanagra/ http://datamining.itsc.uah.edu/adam/ preted by the Ya le system. A number of other approaches use modeling languages for the mining process. MiningMart (see above) uses M4 which includes, besides modeling the mining process, a two-layered data meta model and can be stored both in a relational database or in XML files [7, 6]. KDDML [15] has been developed as a middleware language for the support of KDD applications. Elements in KDDML are operators with functional semantics; this allows to nest operators like in Ya le , though Ya le uses procedural se-mantics. Similarly, modeling languages for KDD processes have also been developed in research on KDD over grids; see [3] for an overview of this area. Two XML-based languages being developed in this context are DPML [1] and DSCL [2]. The XML-based syntax used by Ya le is suitable for distributed processing as well; this is a direction for future work. Finally, the new PMML version 3.0 10 [14] includes fa-cilities to model data transformations executed on a data set before mining. PMML is not process-oriented but provides a standard to describe machine-learned models in XML. Fu-ture work will integrate PMML import and export facilities into the Ya le environment.
This paper presented Ya le , a prototyping tool for com-plex KD tasks. Ya le offers a wide variety of different al-gorithms and methods, which can be flexibly combined and arbitrarily nested. KD processes are represented as operator trees. This enables users to easily incorporate loop facilities into their KD experiments. Loops are essential for many tasks like parameter optimization, feature selection, or ap-plying iterative learning schemes as boosting. Ya le provides an internal data management system that allows arbritrary views on the data, without duplicating it. This is essential for large scale feature construction and the simulation of dis-tributed data mining. This data management also keeps the data handling as transparent as possible to end users as well as to operator developers. Finally, Ya le is easy to extend and many plugins already exist that enrich its base func-tionality. These plugins currently cover text, audio, time series, and multimedia processing, data stream simulation and concept drift handling, clustering, and distributed data mining.

Ya le is currently used in more than 20 countries world-wide for research and development as well as real-world ap-plications in companies as well as for research and teaching at universities. During the last twelve months, there were more than 40,000 downloads from the Ya le web page 11 and more than 160,000 web page visits. Encouraged by this in-terest in our framework, we hope that in the future even more people will find Ya le useful for their professional and academic work. http://www.dmg.org/pmml-v3-0.html http://yale.sf.net
