 Recently, increasing attention has been given to a possible reinterpretation of information retrieval issues in the more general probabilistic framework offered by Quantum Theory. In this paper, we investigate the use of the well-known wave-like phenomenon of Quantum Interference for topic models such as Latent Dirichlet Allocation (LDA). We use interfer-ence effects in order to model interactions between latent topics. Our aim is to elaborate a way to build more precise document models starting from original LDA estimations. Experiments in ad-hoc retrieval show statistically significant improvements on several TREC collections.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Retrieval Models Keywords: Quantum Interference; Topic Models.
Latent Dirichlet Allocation (LDA) [3] is a well-known prob-abilistic topic model based upon the vision that documents are mixtures of topics and a topic is a probability distribu-tion over terms. To generate a document, LDA first samples a per-document multinomial distribution over topics from a Dirichlet. Then, it repeatedly samples a topic from this multinomial and samples a term from the topic distribution. Therefore, each topic has a given probability of appearing in a document and a term has a probability of being gen-erated by a topic. In Language Modeling (LM) framework for information retrieval, the application of LDA has been shown to be very effective [9, 10]. Indeed, LDA produces a  X  X emantic X  document model which can be used for matching queries to documents beyond the term level, thus addressing the vocabulary mismatch between queries and documents.
Under a Dirichlet, the topic proportions are nearly inde-pendent. This hampers the ability of LDA to capture topic correlations, which are often present in natural language documents. A number of hierarchical probabilistic models tried to model correlations between topics [3, 8]. However, the application of such models for IR tasks has not shown to be as effective as one would expect [10].

In LDA, each term is assumed to be generated by one la-tent single topic. This entails that the probability of seeing a given term in a document is computed by marginalizing over its unknown topic assignment, i.e. by applying the Law of Total Probability (LTP) 1 . This calculation does not take into account the inherent interactions between topics in generating document terms. The present work is motivated by the following observation: if two topics war and oil are well represented in a document, one would expect the term Iraq to have a high probability in the document model. By positing that each term is generated by one topic, i.e. by applying the LTP, the term Iraq has a high chance to be less represented than topic-specific terms such as army , fighter , extraction or pipeline . We propose to address this problem by modeling interactions between topics. To this end, we take inspiration from the well-known phenomenon of Quan-tum Interference (QI).

QI is considered one of the largest mysteries in Quan-tum Theory (QT) [5]. However, it does not constitute in itself a probabilistic conundrum. It can be interpreted as the result of a special probabilistic parametrization which allows for relaxing disjointness constraints in a handy way. In this work, we take inspiration from QI and modify the document model estimations obtained through the LTP by adding an interference term which accounts for the interac-tions between topics. We elaborate an analogy with one of the most known interference experiments, the double-slit ex-periment [5]: topics are associated to waves and a document is represented as a superposition of such waves, allowing for the appearance of interference. We do not intend to capture global topics correlations by modifying the training phase, as attempted in the works cited above, which have been shown to be ineffective in retrieval tasks [10]. Our aim is to elaborate a computationally affordable way to build more precise document models starting from original LDA esti-mations. In our model, the term Iraq is naturally boosted in the final document model because there is an interference between several topics on such term that must be taken into account.
In order to capture global correlations between topics, in [3] the Dirichlet is substituted with a logistic normal dis-Figure 1: (a) The double-slit experiment [5]. a ( b ) is the tribution, whose covariance matrix specifies the correlation between pair of topics. In order to model beyond-pairwise correlations, Li and McCallum [8] elaborate the Pachinko Allocation Model (PAM) and choose to represent and learn arbitrary-arity topic correlations by using a directed acyclic graph. In this model, each term is generated by a topic, sampled from a hierarchy of super-topics. These models generally allow to obtain lower perplexity results on held-out data and to discover fine-grained topics resulting in more human-readable topic distributions. However, they suffer a larger computational burden in the training phase compared to LDA. From an IR point of view, the thorough study con-ducted by Yi and Allen [10] shows that the increased seman-tic resolution of correlated topic models does not seem to increase retrieval performance over the original LDA model.
In IR, interference effects have inspired and motivated sev-eral studies. Zuccon and Azzopardi [11] proposed a novel ranking principle in which interference-like effects are used to revise the probability of relevance of a document based on the documents that have already been retrieved. By adopt-ing a more theoretical standpoint, the work by Melucci [1] questions interference effects from a query expansion per-spective and shows that taking into account interference ef-fects is important in order to increase the retrieval effec-tiveness. The work by Gonz  X ales and Caicedo [2] is perhaps the most related to our investigation. The authors propose Quantum Latent Semantic Analysis (QLSA), a modifica-tion of the traditional Latent Semantic Analysis [6] (LSA) obtained by changing the document representation used. QLSA does not aim to explicitly model topic interactions. The authors argue that interference effects between latent dimensions could naturally arise by adopting the new docu-ment representations. It is difficult to understand how topic interactions could be modeled by merely changing document representations and without modifying the decomposition strategy. Moreover, it is not clear how interference directly affects retrieval performance. Differently from that work, we work out an explicit interference formula upon the LDA model and we apply it successfully in the retrieval phase.
In LDA, a document model is obtained by marginalizing over topic assignments, i.e. by applying the LTP. Formally: where z  X  X  1 ,...,K } is a topic index, w  X  X  1 ,...,N } a term of a vocabulary,  X  d = (  X  d 1 ,..., X  dK ) the topic proportions for the document d and  X  is a N  X  K matrix  X  = (  X  1 ,..., X  K containing the distributions over terms defined by each topic  X  k = (  X  k 1 ,..., X  kN ). By Eq. 1, the probability of a term w in the final document model will be within the convex hull defined by the extremal points  X  kw , i.e. it can never be higher than max k  X  kw , its maximum probability assigned by a topic distribution. This could harm terms such as Iraq which are not likely to be topic-specific, but important if topics such as war and oil are highly represented in d .
A strict analogy can be drawn with the double-slit ex-periment [1, 5, 11]. In short, if a photon is shot towards a barrier with two slits, the probability of detection at posi-tion s on a screen behind the barrier is not the average of the probabilities of detection at the same position if it had passed through either one of the slits, i.e. as calculated with the LTP (Fig. 1a). The photon somewhat passes through both slits and interferes with itself, much like a wave. It is said that the photon propagates in a superposition of two waves, one scattering from each slit and traveling towards the detection screen. The amount of interference at a given position depends on the phase difference between the two waves hitting that position. Generally, the interference dis-tribution can be written as c =  X c + I (  X c ), where  X c is the expected distribution and I (  X c ) is the interference term.
If the topics are considered as slits, the document as the photon and a term as a position behind the barrier, Eq. 1 naturally translates the classical account of the double-slit experiment, in which we compute the expected position be-hind the barrier p ( w |  X  LDA d ) by positing that the photon passes through only one of the slits k . QI effects break this straightforward prediction by putting a document in a su-perposition of topics such that any topic can contribute in generating a term w (Figure 1b).

In order to take into account interference effects, we should (1) represent the wave scattering from each topic slit and (2) represent a document as a superposition of such waves. In the next section, these notions are translated into the quan-tum formalism.
In QT, the probabilistic picture of random experiments can be elegantly expressed within a complex Hilbert space H n of dimensionality n [5]. Given a unit vector u  X  H k u k 2 = 1, the projector on such vector uu  X  is called a dyad and is an elementary event of the quantum proba-bilistic space. The symbol  X  denotes the complex conjugate transpose. In this setting, orthogonal dyads correspond to disjoint events. If { u 1 ,...,u n } is an orthonormal basis of H n , then the collection of dyads { u 1 u  X  a classical sample space. A convenient orthonormal ba-sis is the standard basis, noted by E = { e 1 ,...,e n } where e = (  X  i 1 ,..., X  in ),  X  ij = 1 iff i = j , else  X  ij = 0.
Quantum particles such as photons are represented as states v = ( v 1 ,...,v n ), v  X  H n , k v k 2 = 1. Each entry of such vector is a complex number that can be represented in its polar form, v j = | v j | e i X  j , where | v j | is called amplitude , e j = cos  X  j + i sin  X  j is the complex exponential, i = and  X  j is called phase . These two quantities are necessary in order to take into account the wave-behavior of quantum particles. Each state defines a probability distribution on the Hilbert space by means of the Born X  X  rule [5]: the prob-ability of a dyad uu  X  given a state v is the squared cosine of the angle between v and u , i.e. p ( uu  X  | v ) = | u  X  that if the event is a member of the standard basis E , its probability is simply the square of the amplitude of the cor-responding entry in the state vector, i.e. p ( e i e  X  i | v ) = | v
In the case of LDA, we associate to the vocabulary sample space the standard basis in H N , where N is the size of the vocabulary. Therefore, each term event e w e  X  w corresponds to an orthogonal dimension of the vector space. In our analogy of the double-slit experiment, each topic z  X  X  1 ,...,K } cor-responds to a wave defining a probability distribution over terms. This situation can be modeled by defining K states { z 1 ,...,z K } in H N . In this work, we define the states z order to reproduce the LDA statistics over the vocabulary sample space. Hence, we set z k = ( z k 1 ,...,z kN ), where: where  X  kw is a free variable representing the phase of the wave corresponding to topic k for term w . The role of such quantity will be clearer shortly when the interference formula will be introduced. Following the parameteriza-tion given in Eq. 2, one can show that the probability of a term given a topic corresponds to the LDA statistics, i.e. p ( e w e  X  w | z k ) = (
Until now, we assigned a clear mathematical status to each topic wave. In order to allow interference between top-ics, a document must be represented as a superposition of topics. Superposition is not a classical probabilistic con-cept. From a mathematical point of view, superposition is obtained by linear combination. Therefore, one can rep-resent a document as a superposition of the topic states { z 1 ,...,z K } by d = 1 Z d P k  X  k z k , where the coefficients  X  P in the document and the normalization factor Z d ensures that d is a state defining a proper probability distribution on the underlying vector space. In this work, we choose to set  X  k = p p ( z = k |  X  d ) = tions estimated by the traditional LDA model, and therefore written as d = ( d 1 ,...,d N ) where: up to a normalization factor. Superposition enables the top-ics to interact by interference effects. Indeed, the probability of a term in a given document is calculated by: The equation above defines an interference document model: the first component corresponds to the classical document model given by the LTP and corresponding to Eq. 1; the sec-ond part corresponds to the interference term which boosts or penalizes the probability for term w in the final document model depending on the phase differences  X  iw  X   X  jw . If a pair of topics is in phase for a given term w , i.e.  X  iw  X   X  jw then cos(  X  iw  X   X  jw ) = 1 and the interference term will be positive: the probability of seeing w in the final document model increases. Note that if  X  w,i,j,  X  iw  X   X  jw =  X / 2, then cos(  X  iw  X   X  jw ) = 0: the interference term disappears, i.e the original LDA document model is recovered.

Modeling topic interactions by setting phase differences is certainly flexible but it is challenging to determine how actually one should set or estimate such parameters. This would certainly be an interesting research direction for fu-ture works. In this study, we choose to simplify this task by assuming that (1) the interference does not depend on the particular term, i.e.  X  w, cos(  X  iw  X   X  jw ) = cos(  X  i  X   X  (2) the interference is proportional to a similarity measure between topic distributions, i.e. cos(  X  i  X   X  j )  X   X (  X  We choose to define  X  as the cosine similarity between topic distributions, i.e.  X (  X  i , X  j ) =  X  &gt; i  X  j k  X  here is that if two topics share common terms, then they interfere positively. On the contrary, if two topics are or-thogonal to each other, the interference term will vanish. Note that we are discarding the possibility of negative inter-ference between topics, i.e. 0  X   X (  X  i , X  j )  X  1. Introducing negative interference terms certainly increases the modeling power of the proposed model. What we often observe in practice is that two topics can be unrelated or positively related somehow. This is at least the case for the topics that we extract in LDA, in which we only consider how top-ics can jointly generate terms in documents, but not how a topic precludes another. The goal of this paper is not to cre-ate a complex model to account for all kinds of interference between topics. Instead, we show that when the possible interference is considered in some (albeit simplified) way, the modified model performs better than the original LDA model. For this purpose, discarding negative interference between topics appears as a reasonable simplification. The final interference document model has the form: where Z d = 1 + 2 P i&lt;j p  X  di  X  dj  X (  X  i , X  j ) 2 is a normalization factor that can be computed offline. If w has a high proba-bility of being generated by two related topics, i.e. if both  X (  X  i , X  j ) and  X  iw  X  jw are large, and these topics have high probability of being present in the document, i.e. if  X  di also large, then w gets boosted in the final document model. Therefore, the model will penalize topic-specific terms and favor terms that are less probable but shared by important topics. With respect to our previous example, this could favor Iraq because it is likely to be shared by related topics such as war and oil .
In our experiments, we choose three commonly used TREC newswire corpora: AP and SJMN collections containing re-spectively 242,918 and 90,257 documents with topics 51-150; the WSJ collection containing 173,252 documents with top-ics 51-100 and 151-200. These small-sized collections are of-ten used in the literature for keeping the training process of LDA computationally affordable. The performance is mea-sured using Mean Average Precision (MAP) and is evaluated on the top 1000 retrieved documents. Statistical significance for MAP is determined using a two-sided Fisher X  X  random-ization test with 25,000 permutations and  X  &lt; 0 . 05.
As done by previous work [9, 10], we rely on the query like-lihood scoring function for the ad-hoc task evaluated here. We use as a first baseline the classical formula of the lan-guage model based on Dirichlet smoothing (denoted LM ) [10]: where  X  ML d , X  ML C are the maximum likelihood estimators of the document LM and collection LM and  X  d = (  X   X  + | d | controls the amount of smoothing. As a second baseline, we report LDA-Based document models [9] (denoted LBDM ), which integrate semantic information mined by LDA, i.e. p ( w |  X  LDA d ), by adding a level of smoothing over LM : where  X   X  [0 , 1] controls the amount of semantic matching. Finally, our proposed interference model (denoted QLBDM ) is obtained by substituting p ( w |  X  LDA d ) in the above equa-tion with the interference model p ( e w e  X  w | d ) calculated using Eq. 5. The free parameters are set following [10]:  X  = 1000 and the semantic smoothing parameter  X  is optimized by linear search over { 0 , 0 . 1 ,..., 1 } for each collection, method and number of topics tested. LDA is trained by running 50 iterations of Collapsed Gibbs Sampling. The number of topics K ranges in { 50 , 100 , 200 , 400 , 600 } .
The MAP obtained by the baseline LM was .2217 for AP, .2014 for SJMN and .2842 for WSJ. The obtained results for LBDM and QLBDM are reported in Table 1. As found in previ-ous work [10, 9], our results show that LDA is indeed bene-ficial for IR and its performance increases with the number of topics used. Moreover, QLBDM statistically outperforms LBDM across different collections and number of topics. In-terestingly, QLBDM reaches the best performances obtained by LBDM at a fraction of the number of topics K . For ex-ample, on SJMN, QLBDM at K = 100 reaches comparable performance of LBDM at K = 600. A similar behavior can be observed on WSJ and AP. It seems that interference is especially effective for small K . In this case, the topic distri-butions are likely to be coarse-grained thus showing a large overlap. The interference term can be helpful in highlighting and boosting shared patterns in the topic distributions thus producing a more precise document model. In addition, the diminishing returns can be explained by analyzing Fig. 2. We plot the empirical cumulative distribution of the cosines F cator function. By increasing K ,  X (  X  i , X  j ) becomes smaller in average, i.e. topics share less common terms. Therefore, the interference factor will play a smaller role.

In Fig. 3, we analyze how LBDM and QLBDM behave with respect to the smoothing parameter  X  for different values of K . Even if not reported explicitly, the pattern on AP was found to be similar. Generally, for any  X  &gt; 0 . 1, QLBDM stays significantly above the baseline LM . This behavior seems to be more pronounced for high values of K . As a result, QLBDM is less sensitive to the choice of smoothing parameter  X  than LBDM . For LBDM , the optimal  X   X  [0 . 6 , 0 . 8], while for  X   X  [0 . 3 , 0 . 5]. The smaller values for QLBDM suggest that interference generates more smoothed document models by F igure 2: The cumulative distribution F  X  ( x ) for SJMN F igure 3:  X  variation on SJMN (left) and WSJ (right). reducing the relative differences between terms probabili-ties. In order to highlight such differences, the smoothing parameter  X  should be set lower.
We presented an application of QI for modeling topic in-teractions in LDA. Our main focus was to take into account the inherent interactions between topics in generating doc-ument terms. The interference term is related to a measure of similarity between pair of topics. The model penalizes topic-specific terms favoring terms that are less probable but shared by similar important topics. Experimental eval-uation showed the usefulness of our approach in that signif-icant improvements are obtained over two strong baselines across different number of topics and collections. However, much work remains. Solutions involving more reasonable phase factors have the potential to make yet more signifi-cant improvements over LBDM . This will be part of our future investigation.
