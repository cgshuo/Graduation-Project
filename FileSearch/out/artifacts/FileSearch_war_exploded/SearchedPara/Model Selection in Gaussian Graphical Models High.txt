 p related to these structural assumptions.
 penalized by the  X  under certain scalings of the sparsity, sample size, and amb ient model dimension. tency of  X  by E ( X ) = { s 6 = t |  X  tions to be specified, we prove that the  X  itself, as opposed to the weights  X   X  proof. In Section 4, we provide some simulations that illust rate our results. the paper. Given a vector u  X  R d and parameter a  X  [1 ,  X  ] , we use k u k  X  induced matrix-operator norm max importance in this paper are the spectral norm ||| U ||| of
U ; the  X   X  / X   X  -operator norm , given by and the  X  element-wise maximum max induces the Frobenius norm ||| U ||| f ( n ) = O ( g ( n )) and f ( n ) =  X ( g ( n )) .  X  -regularization. We then state our main result, and discuss some of its consequences. 2.1 Gaussian MRFs and  X  Consider an undirected graph G = ( V, E ) with p = | V | vertices, and let X = ( X denote a p -dimensional Gaussian random vector, with variate X Gauss-Markov random field (MRF) is described by a density of t he form Clifford theorem [7], it must satisfy  X   X  tion matrix X  X hat is, the set E ( X   X  ) := { i, j  X  V | i 6 = j,  X   X  In this paper, we study the minimizer of the  X  ting hh A, B ii := P objective function takes the form Here b  X  denotes the sample covariance X  X hat is, b  X  := 1 drawn in an i.i.d. manner according to the density (2). The qu antity  X  regularization parameter. and k  X  k this problem always has a unique solution, so that there is no ambiguity in equation (3). We let E ( b  X ) = { ( i, j ) | i 6 = j, b  X  structural properties of b  X  . In particular, we define both the sparsity index corresponding to the total number of edges, and the maximum degree or row cardinality degree in the graph G , where we include the diagonal in the degree count. 2.2 Statement of main result shown that this Hessian takes the form vertex pairs, so that entry  X   X  more specific expression  X   X  an edge-based counterpart to the usual covariance matrix  X   X  . S pairs (  X , m ) for which  X   X  We require the following conditions on the Fisher informati on matrix  X   X  : graphical model. [A2] Covariance control: There exist constants K bounded  X  norm [1].
 respectively. With this notation, we have: (A1) and (A2). Suppose the penalty is set as  X  triple ( n, d, p ) satisfies the scaling set w.h.p. X  X n particular, for some constant c &gt; 0 .
 bound ||| b  X   X   X   X  ||| 2 2.3 Comparison to neighbor-based graphical model selectio n by Meinshausen and B  X uhlmann [9], in which each node is linea rly regressed with an  X  dominates its analog for the Lasso. 2.3.1 Illustration of irrepresentability: Diamond graph eter  X   X  [0 , 1 /  X  2] : the diagonal entries are set to  X   X  to edges are set to  X   X  the non-edge is set as  X   X  irrepresentability condition holds while our log-determi nant counterpart fails. 2.3.2 Illustration of irrepresentability: Star graphs  X  the non-edge entries are set as  X   X  while the log-determinant counterpart fails. and where we relax Assumption [ A 2 ], and allow quantities K e  X  used as a proof technique for certifying the behavior of the  X  3.1 Primal-dual witness approach differential of the norm k k Lemma 1. For any  X  regularized log-determinant problem (3) has a unique solution b  X   X  0 characterized by where e Z is an element of the subdifferential  X  k b  X  k  X  k e satisfy the sub-differential conditions, since e Z conditions to belong to the sub-differential.
 e  X  none of the entries in e  X  the lower bound assumption in Theorem 1 on the value of the min imum value  X   X  rem 1. We solved the  X  to diagonal entries set as  X   X  quantities ( K Dependence on graph size: n of Theorem 1.
 Dependence on the maximum node degree: n Thus, it might be possible to tighten our theory under certai n regimes.
