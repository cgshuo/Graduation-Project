 I.5.2 [ Pattern Recognition ]: Classifier design and evalua-tion; I.2.7 [ Artificial Intelligence ]: Text analysis Algorithms, Experimentation
Latent Semantic Indexing (LSI) [1] is an important tech-nique for text retrieval and categorization, and a statisti-cal framework for LSI has been established [ ? ]. Several ap-proaches for improving its performance has been proposed, such as adaptive sprinkling [ ? ], model averaging [ ? ], among others. In this study, we propose a novel approach to en-hance Latent Semantic Indexing (LSI) by exploiting cate-gory labels. Specifically, in the term-document matrix, vec-tors for terms either appearing in category labels or seman-tically close to category labels are scaled before perform-ing Singular Value Decomposition (SVD) to boost their im-pact on the generated left singular vectors. As a result the similarities among documents in the same category are in-creased. Furthermore, an adaptive scaling strategy is de-signed to better exploit the hierarchical structure of cate-gories in hierarchical text categorization. A comparative ex-perimental study on two typical text data sets demonstrates the significant performance enhancement of our approach for hierarchical text categorization.
In text categorization, terms in a document that also ap-pear in category labels are more effective in categorizing the document than other terms. Therefore, it is desirable to design a strategy to boost their impact. Motivated by this intuition, we propose to scale the term vectors of category labels in the term-document matrix before performing Sin-gular Value Decomposition (SVD). Formally, a term vector t is scaled as  X  t = (1 + q ) t , where q is a positive real number.
Given that there are a number of terms that are seman-tically similar to labels, it is natural to extend the scaling to such terms. We refer to terms either appearing in cate-gory labels or semantically close to category labels as label-relevant terms. Formally, for a term t appearing in labels, the corresponding set of label-relevant terms is defined as where sim ( s, t ) (the similarity between s and t ) is defined to be the ( s, t ) entry in the term-term similarity matrix XX based on LSI: X = U r  X  r V T r , XX T = U k  X  2 k U T k , and the rank operator is applied to select the l terms closest to t .
We will show that our label-driven scaling method in-creases the similarity of a query with a document of the same category by making two assumptions. Let x = ( x 1 , x 2 , ..., x denote the query vector, and y = ( y 1 , y 2 , ..., y n ) refer to the vector of a training document belonging to the same cate-gory as the query. Without loss of generality, assume that the first k components of the vectors correspond to label-relevant terms, which are scaled by a factor t . After scaling, the document vector is denoted as y 0 = ( y 0 1 , y 0 2 , ..., y
The cosine distance between the two vectors dist ( t ) is dist ( t ) = simplify the notation, the following abbreviations are intro-duced: C 1 = P If we take the derivative of dist ( t ) with respect to t , it can be verified that dist 0 ( t ) = C 1 S 2  X  C 2 S 1 t
It is evident that if making either one of the following two assumptions: C 2 S 1 = 0 , C 1 S 2 6 = 0 or t &lt; C 1 S 2 positive, and dist ( t ) monotonically increases. This indicates that the scaling increases the similarity of the query with the document.

The rationale of the second assumption is explained as fol-lows. When the query and the document belong to the same category, they are more likely to both contain label-relevant terms. The co-occurrence of these terms will cause the pair-wise product C 1 to be large in comparison to C 2 , since the latter only contains terms not related to labels. The extent of  X  X arge X  is measured by the ratio S 1 S the query and the document do not belong to the same cat-egory, it is likely that C 1 is not large in comparison to C which may cause dist 0 ( t ) to be negative. In such a case, the similarity between the query and the document will be decreased. Therefore, scaling can both increase the simi-larity of the query and the document when they are in the same category and may also decrease the similarity when they are in different categories. Both effects are favorable for improving the classification accuracy.

In hierarchical text categorization, the categories are or-ganized into a hierarchy, and categories in lower levels are more specific than those in upper levels. Therefore, it is reasonable to design an adaptive scaling strategy to reflect the differences. Specifically, the scaling factors for vectors of label-relevant terms should be dependent on the position of the category node in the hierarchy. Assume that the cat-egories in leaf nodes receive a scaling factor q , then for a node i , its scaling factor is defined by q i = q c ( i ) represents the number of leaf nodes among the descendants of the node i , and c ( i ) = 1 if i represents a leaf node. Each test document is considered as a query, and its similarities with documents of different categories are computed using the cosine metric. The category of the test document is obtained by the k-Nearest-Neighbor method.
The data sets in our experiments include Reuters-21578 and the 20 Newsgroups collection. The categories in both data sets are organized as taxonomies and the label for each category is predefined. All documents in the data sets were preprocessed. After stop word removal and stemming, we filtered out terms with less than two characters. No fea-ture selection was performed in our experiments. To decide label-relevant terms for each category, we carried out LSI with the amount of dimension reduction set at 50. In the classification process the number of nearest neighbors was set at 20, with other values generating similar results.
We compared the performance of two variants of our ap-proach (uniform scaling marked by NADP and adaptive scal-ing marked by SLSI) with two other approaches: (1) kNN classifiers whose similarities are obtained in the LSI space (marked by K-LSI); (2) hierarchical SVM classifiers using the linear kernel and default parameter values (marked by SVM). We summarized the overall results in Tables 1, 2 and 3 for the three category trees. The difference between NADP and SLSI is that the former applies uniform scaling to all the nodes in the hierarchy while the latter applies adaptive scal-ing. In most cases, both label-driven scaling and adaptive scaling significantly improve the classification performance, and in some cases SLSI even outperforms SVM. In this paper, we proposed a novel approach to enhance Latent Semantic Indexing (LSI) by making two contribu-tions: label-driven scaling and hierarchy-dependent adap-tive scaling. Experimental results on real-world data show that our approach is able to substantially improve the per-formance of hierarchical text categorization. This work was supported by National Natural Science Foundation of China (No. 60573077), MSRA Internet Ser-vices Theme, MOE-MS Key Laboratory of Multimedia Com-puting and Communication of USTC (No. 07122807, 06120805)
