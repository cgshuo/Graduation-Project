 Web user forums (or simply  X  X orums X ) are online platforms for people to discuss information and obtain information via a text-based threaded discourse, gener-ally in a pre-determined domain (e.g. IT support or DSLR cameras). With the advent of Web 2.0, there has been an explosion of web authorship in this area, and forums are now widely used in various areas such as customer support, com-munity development, interactive reporting and online eduction. In addition to providing the means to interactively participate in discussions or obtain/provide answers to questions, the vast volumes of data contained in forums make them a valuable resource for  X  X upport sharing X , i.e. looking over records of past user interactions to potentially find an immediately applicable solution to a current problem. On the one hand, more and more answers to questions over a wide range of domains are becoming available on forums; on the other hand, it is becoming harder and harder to extract and access relevant information due to the sheer scale and diversity of the data.

One potential way to enhance informati on access and support sharing in fo-rums is to improve information retrieval (IR) effectiveness over forum threads. To this end, Elsas [1] amassed a forum dataset for forum thread retrieval and conducted initial experiments. We build on this earlier work, in exploring the hypothesis that incorporating thread dis course structure [2,3] into the IR model can improve retrieval effectiveness.

The discourse structure of a thread is modelled as a rooted directed acyclic graph (DAG), with the posts in the thread represented as nodes in the DAG. The reply-to relations between post s take the form of directed edges ( Link s) between nodes in the DAG, and dialogue acts ( DA s) are used to label the edges. For the purposes of illustration, we use an annotated example thread from Elsas X  Ancestry dataset [1], made up of 5 posts from 3 distinct participants, as shown in Fig. 1. In this example, UserA ini tiates the thread with a question ( DA = Question-question ) in the first post, seeking information about his/her great-grandfather. In response, UserB asks for more details about the question ( DA = Question-confirmation ). Then UserA responds to User B to add extra information to his/her original question ( DA = Question-add ). Finally, UserC proposes a solution to the original question ( DA = Answer-answer ), and UserA confirms that this answer is correct ( DA = Resolution ). It should be noted that the discourse structure of most threads actually takes the form of a tree, as shown in Fig. 1. However, in some rare cases, a given post can reply to two or more previous posts, producing a DAG structure.

Specifically in this paper, we automatically infer the thread discourse structure of a target forum dataset by using a discourse parser that is trained over out-of-domain annotated data. We then incorporate information derived from this thread discourse structure into a state-of-the-art IR model for forum retrieval, and find that thread discourse structure can, indeed, benefit thread retrieval. We also investigate the reason behind the improvements. As far as we are aware, there has been very little IR work that is specifically tar-geted at web user forum data. The most closely-related work is that performed by Elsas [1], on which this work is directly based; we describe the relevant de-tails of Elsas X  work later in this paper. Other closely-related work was done by Seo et al. [4], in improving thread retrieval by automatically inferring thread structure and incorporating it into the retrieval model. They explore different thread document representations, such as at the thread-level (i.e. concatenate all the posts in a thread into a single document), pair-level (i.e. treat each pair of posts as a document), dialogue-level (i.e. treat each sub-thread in a thread as a document), and combinations of these. They show that using the linking structure of threads boosts thread retrie val effectiveness. Elsas and Carbonell [5] conducted preliminary research on thread retrieval and also showed that thread structure is useful in thread ranking. Additionally, they found that message/post selection can contribut e to thread retrieval.

Research on thread discourse structure analysis and classification over user forums has gained in momentum in recent years. Fortuna et al. [6] defined 5 post-level dialogue acts to descr ibe the levels of agreement (i.e. agreement , dis-agreement , insult ) and identify questions and answers (i.e. question and answer ) inforumposts.Xietal.[7]defined5prev alent types of post-l evel dialogue acts in forum threads. This set of dialogue acts was then adapted and extended by us in earlier work [2] to describe possible types of posts in troubleshooting-oriented online forums. Specifically, we devised a post-level dialogue act set and anno-tated a set of threads from forums.cnet.com . In this work, we proposed a set of novel features, which they applied to the separate tasks of post link classification and dialogue act classification. We later applied the same basic methodology to dialogue act classification over one-on-one live chat data with provided message dependencies [8], demonstrating the generalisability of the original method. In both cases, however, we tackled only a single task, either link classification (op-tionally given dialogue act tags) or dialogue act classification, but never the two together.

In later work, we delved into the task of thread discourse structure parsing further [3]. We used the same features as [2], but different parsing approaches. Specifically, we approached thread discourse structure parsing as a joint link and dialogue act classification task, using conditional random fields [9] and depen-dency parsing [10]. We also demonstrated that our discourse structure parsing method was able to perform equally well over partial threads as complete threads, by experimenting with  X  X n situ X  classification of evolving threads.

There has also been research focusing on particular types of dialogue acts, such as question  X  answer pairs in emails [11] and forum threads [12], question  X  context  X  answer in forum threads [12], initiation X  X esponse pairs (e.g. question X  X nswer , assessment X  X greement ,and blame X  X enial ) in forum threads [13], as well as request and commitment in emails [14].

Thread discourse structure can be used to facilitate different tasks in web user forums. For example, we demonstrated that the information extracted from thread discourse structure can be used to improve Solvedness (i.e. whether the problem presented in the thread is solved or not) classification of forum threads [15]. Additionally, threading information has been shown to enhance retrieval ef-fectiveness for post-level retrieval [7,4], t hread-level retrieval [4,5], sentence-level shallow information extraction [16], and near-duplicate thread detection [17]. Moreover, Wang and Rose [13] demonstrated that initiation X  X esponse pairs (e.g. question X  X nswer , assessment X  X greement ,and blame X  X enial ) from online forums have the potential to enhance thread summarisation and automatically gener-ate knowledge bases for Community Question Answering (cQA) services such as Yahoo! Answers. Furthermore, Kim et al. [18] showed that dialogue acts can be used to classify student online discussion s in web-enhanced courses. Specifically, they use dialogue acts to identify discussion threads that may have unanswered questions and need the attention of an instructor. 3.1 The Ancestry Forum Dataset The Ancestry.com Forum Dataset ( Ancestry ) was created by Jonathan Elsas and Ancestry.com, a website which supports historical genealogical research. The Ancestry dataset contains a full snapshot of the Ancestry.com online fo-rum ( boards.ancestry.com ) from December 1995 to July 2010. The dataset in-cludes 22 , 054 , 728 posts spanning 9 , 040 , 958 threads, from 165 , 358 sub-forums. The total number of unique users is 3 , 775 , 670. The Ancestry dataset is pre-sented at the post-level, and information associated with each post includes: post identifiers, the subforum name, thread identifier, author name/identifier, timestamp (at the day level), URL of the original post, post title and post body. The inter-post link structure of each thread, in terms of the reply-to structure generated by users when posting to the thread, are also provided.
 The Ancestry dataset also comes with a selected set of 191 queries from Ancestry.com X  X  query log, and pairwise pre ference relevance judgements for each query over the Ancestr y.com forum data.

To create the pairwise preference relev ance judgements annotation, a docu-ment pool is simulated as the first step. Firstly, Indri ( lemurproject.org ), Terrier ( terrier.org ), Zettair ( www.seg.rmit.edu.au/zettair )andAn-cestry.com X  X  ranked boolean system are applied over the whole dataset to pro-duce post rankings, with each ranking containing 1000 posts. Then, three ag-gregation methods, namely Mean , Max and Pseudo-Cluster Selection ( PCS )[19], are used to convert each post ranking to a thread ranking. Lastly, the document pool is created by combining the top 100 threads of each thread ranking. The document pool contains 374 unique threads per query on average.
Relevance assessment is conducted by Ancestry.com, by collecting document-pair preferences [20]. This approach pre sents side-by-side document pairs ( L , R ) and collects judgements: L is preferred to R , R is preferred to L , L and R are duplicates, L is bad or R is bad. During the assessment process, a document pair selection algorithm, which is described in detail in [1], is used to reduce the number of assessments.

Out of the 191 queries, 50 queries wer e first selected for a pilot assessment, with each query annotated by two assesso rs. The results of the pilot assessment were analysed and used as a guide to set the parameters of the document pair selection algorithm, as well as adjust asse ssor training and assessment guidelines. Then, each of the remaining 141 querie s was assessed by one assessor, with the adjusted parameters of the pair selection algorithm. 3.2 The CNET Forum Dataset The CNET forum dataset of Kim et al. [2] 1 contains 1332 annotated posts span-ning 315 threads, collected from the Operating System, Software, Hardware and Web Development sub-forums of CNET . 2 Each post is labelled with one or more links (including the possibility of null-links, where the post doesn X  X  link to any other post), and each link is labelled with a dialogue act. The dialogue act set is made up of 5 super-categories: Question , Answer , Resolution (confirmation of the question being resolved), Reproduction (external confirmation of a proposed solution working) and Other .The Question category contains 4 sub-classes: ques-tion , add , confirmation and correction . Similarly, the Answer category contains 5 sub-classes: answer , add , confirmation , correction and objection . For example, the label Question-add signifies the Question superclass and add subclass, i.e. addition of extra information to a question. 3.3 The ILIAD Forum Dataset The ILIAD (Improved Linux Information Acces s by Data Mining) dataset [21] contains 1158 posts spanning 250 threads, collected from Linuxquestions 3 and Debian mailing lists. 4 We hand-annotated the discourse structure of the ILIAD dataset [15], based on a slightly modified version of the dialogue act set from our earlier work [2]. As part of this annotation, we proposed an additional Question-information dialogue act, for posts which provide information in non-troubleshooting threads. We also slightly adjusted the definition of the Resolution dialogue act. For full details of the ILIAD dataset and the annotations over it, see [21] and [15], respectively. As explained in Section 3.1, the relevance judgements in the Ancestry dataset are pairwise preferences, rather than t raditional absolute preferences (judge-ments). As analogues to absolute evaluation measures such as Precision at a cutoff ( P @ k ) and Average Precision ( AP ), Elsas [1] uses Precision of Prefer-ences at a cutoff ( ppref @ k ) and a modified version of Average Precision of Pref-erences ( mAPpref ), which was originally proposed by Carterette [22]. ppref @ k represents the proportion of correctly ordered preferences to ordered preferences, where at least one document/thread in the pair is ranked above k . mAPpref is the average of ppref values over the ranks (i.e. k ) of all documents which have ever been preferred to any other documents. While ppref used by Elsas [1] is unchanged, the original APpref proposed by Carterette [22] is the average of ppref values over the ranks (i.e. k ) at which the recall of preferences ( rpref ) increases. rpref is the proportion of correctly ordered preferences to the total number of preferences made by assessors.

For comparability, the primary evaluation metrics used in this paper are ppref @10 and mAPpref , based on the evaluation script provided by Elsas [1]. 5 Elsas [1] conducted a series of IR experiments over the Ancestry dataset, using 4 retrieval systems with various configurations. The retrieval was done at the post-level, and 3 different aggregation methods were used to convert the post-level rankings to thread-level rankings. A summary of the retrieval systems with the configurations used, as well as the aggregation methods, is presented in Table 1.
According to the experiments of Elsas [1], Indri with bag-of-words ( BoW ) and dependence model ( DM : [23]) query formulation perform the best; our ex-periments support this conclusion. The DM used is a full dependency variant of a Markov Random Field, which assumes that all query terms are in some way dependent on each other. It considers the BoW representation (with weight 0 . 8) of the whole query, as well as ordered representation (with weight 0 . 1) and unordered representation (with weight 0 . 1) of the subsets of the query. We tried to reproduce the results presented in [1] using Indri -BoW and Indri -DM for post-level retrieval with three different aggregation methods: Mean , Max and Pseudo-Cluster Selection ( PCS ). Our experimental results are displayed alongside the results reported in [1] in Table 2. Although there are slight differ-ences between our results and Elsas X  [1] re sults, the overall results are compa-rable. Because Indri -DM with PCS ( Indri-DM-PCS ) obtains the best results for both mAPpref and ppref @10, it will be used as our baseline IR method.
Following the work of Seo et al. [4], we als o experimented with retrieval based on contexts of differing size, such as the thr ead-level, pair-level, dialogue-level, and various combinations of these. None of these experiments resulted in better results than the Indri-DM-PCS baseline, and the results are omitted from the paper. It is not practical for us to manually annotate the discourse structure of the whole Ancestry dataset nor just the portion of the dataset retrieved by the different IR systems. Rather, we opt to use automatically-predicted discourse structure. To build a discourse parser for Ancestry threads, we randomly selected and annotated 50 threads from the whole dataset to use for parameter tuning.
Discourse structure parsing, as discussed in [3], can be addressed in several ways. If a structured classification approach, such as a conditional random field (CRF), is used, we can either classify the links ( Link ) and dialogue act ( DA )sep-arately and compose them afterwards (denoted as Composition ), or classify the combined Link and DA (e.g. treat 0+Question-question as a single label) directly (denoted as Combine ). Another approach is to treat discourse parsing as a de-pendency parsing problem. Dependency parsing [24] is the task of automatically predicting the dependency structure of a token sequence, in the form of binary asymmetric dependency relations with dependency types. The joint classifica-tion task of Link and DA is a natural fit for dependency parsing, in that the task is intrinsically one of inferring labelled dependencies between posts.
For discourse parsing, we follow our earlier work [3]. All experiments were carried out based on stratified 10-fold cross-validation, stratifying at the thread level to ensure that all posts from a given thread occur in a single fold. Addi-tionally, we augment the training data with the CNET and ILIAD datasets. The results are evaluated using post -level micro-averaged F-score (  X  =1).Allthree discourse parsing methods described ab ove were tested in our experiments, us-ing CRFSGD [25] and MaltParser [10]. For features, we experimented with all the features proposed in our earlier work [3], as well as many of our own features. We found that using CRFSGD with a simple feature indicating whether a post X  X  au-thor is the initiator of the thread and the Combine approach achieves the highest Link and DA joint ( LD ) F-scores, as shown in Table 3. Because the availability of annotated discourse structure data cannot always be assumed, we decided to use only out-of-domain data to train the discourse parsers. Therefore, only the configurations of CNET , ILIAD and CNET+ILIAD are used in later experiments. The basic idea of using the discourse structure to enhance existing IR systems is to use either links ( Link s) or dialogue acts ( DA s) to modify the document ranking. For example, in the framework of Pseudo-Cluster Selection ( PCS ), one could imagine that a retrieved Answer-answer (i.e. an independent answer to a question) post should be weighted higher than Other posts (including irrelevant posts), and thus contribute more to the thread ranking score. Under this as-sumption, we examined all the correctly predicted instances from the parsers described in Section 6 over our Ancestry development set, and found that the correctly predicted set only cont ains 5 dialogue acts, namely: Question-question ( Qq ), Question-add ( Qadd ), Answer-answer ( Aa ), Answer-add ( Aadd ), and Resolu-tion ( Res ). Therefore, only predictions for th ese 5 dialogue acts are considered. Build on the Indri-DM-PCS system, our system ( Indri-DM-LD ) modifies the post-level rankings based on the predicted DA types of the posts. If a post X  X  pre-dicted DA type belongs to the selected DA subset ( DASubset ), it is considered to be more important than other posts and it s score is increased/ promoted by a cer-tain factor. In addition to the 5 dialogue acts ( DAs+ALL ), we experimented with omitting one DA at a time (e.g. DAs  X  Qq = the five DA sminus Question-question predictions), to gauge the impact of each DA on the overall results.

Furthermore, in the model of PCS , one crucial parameter is the k which governs the number of retrieved posts that are used to calculate the thread-level ranking scores. Because of the potential int eraction between this parameter k and our DA promotion model Indri-DM-LD , we also examined the effect of k in the baseline system Indri-DM-PCS as well as in our system Indri-DM-LD . We found that while k = 5 produces the best results for Indri-DM-PCS , k = 4 is the best setting for our Indri-DM-LD system. All experimental results reported in this paper are based on these respective k settings.

Table 4 presents the mAPpref / ppref @10 results for our Indri-DM-LD system with different DASubset configurations and promotion factors (i.e. 30% X 70%). We test for statistical significance over the Indri-DM-PCS baseline with the two-tailed t -test ( p&lt; 0 . 05).

From Table 4 we can see that our system outperforms the Indri-DM-PCS baseline system ( mAPpref = . 657 and ppref @10 = . 664) in most cases, demon-strating the superiority of our method. Our best results ( mAPpref = . 674 and ppref @10 = . 678) are achieved using the combined CNET and ILIAD datasets for discourse parser training, the DASubset of DAs  X  Qq ,anda DA promotion factor of 50%. The intuition behind Question-question posts not warranting promotion is that they contain question and not answer data, and are less likely to contain information relevant to the resolution of a query. It is important to reinforce that the discourse structure informati on used in these experiments was derived automatically based on out-of-domain data.

To investigate the mechanics behind our system, we conducted error analysis over Indri-DM-PCS vs. Indri-DM-LD . In one case, there are two threads, namely Thread1 and Thread2, which relate to Query 38 ( jacob lazarus; great synagogue, dukes place, london ). In the gold-standard annotation, Thread1 is preferred to Thread2. The posts retrieved by Indri-DM system are posts 3, 4 and 9 for Thread1 and posts 2, 7 and 12 for Thread2. Under the Indri-DM-PCS baseline system, Thread2 is ranked higher than Thread1. However, with Indri-DM-LD and DAs  X  Qq , the correct ordering of Thread1 and Thread2 is predicted, as the DA of post 12 in Thread2 is Question-question while the DA of all other posts is in DAs  X  Qq . As a consequence, the relative promotion of Thread1 is greater than Thread2, and the correct ranking is derived.

During our experiments, we demonstrated that making use of discourse struc-ture of forum threads can boost retrieval e ffectiveness. As an alternative to full discourse parsing, we experimented with simply promoting all non-first posts (under the assumption that first posts are most likely to be Question-question posts). The best results achieved for this simple method are mAPpref = . 667 and ppref @10 = . 670. Although the mAPpref score is significantly better than the baseline, the ppref @10 is not (and both results are slightly below the best re-sults achieved with discourse parsing, of mAPpref = . 674 and ppref @10 = . 678). Nevertheless it shows the potential of usi ng a lighter-weight version of discourse structure to improve IR effectiveness. We w ill explore this line of research further in future work. Inthisresearch,wehaveexploredthehy pothesis that IR over forum threads can be improved by incorporating thread dis course structure in the form of a rooted DAG over posts, with edges labelled with dialogue acts. When compared to previous research conducted over the Ancestry dataset, we achieved significantly better results using automatically-predicted thread discourse structure.
In future work, we plan to firstly investigate more ways to capture thread discourse structure information. Furthermore, we intend to look into means of exploiting the structural information of threads for the purpose of IR, and their interaction with thread discourse stru cture. For example, the same dialogue act of Answer-answer may contribute to the thread ranking differently if it appears at different positions in a thread (e.g. second post vs. last post).
 Acknowledgements. NICTA is funded by the Australian government as rep-resented by Department of Broadband, Communication and Digital Economy, and the Australian Research Council through the ICT centre of Excellence pro-gramme.

