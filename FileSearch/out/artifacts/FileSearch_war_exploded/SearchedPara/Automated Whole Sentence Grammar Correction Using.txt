 The process of editing written text is performed by humans on a daily basis. Humans work by first identifying the writer X  X  intent, and then transform-ing the text so that it is coherent and error free. They can read text with several spelling errors and gram-matical errors and still easily identify what the au-thor originally meant to write. Unfortunately, cur-rent computer systems are still far from such ca-pabilities when it comes to the task of recogniz-ing incorrect text input. Various approaches have been taken, but to date it seems that even many spell checkers such as Aspell do not take context into consideration, which prevents them from find-ing misspellings which have the same form as valid words. Also, current grammar correction systems are mostly rule-based, searching the text for de-fined types of rule violations in the English gram-mar. While this approach has had some success in finding various grammatical errors, it is confined to specifically defined errors.

In this paper, we approach this problem by mod-eling various types of human errors using a noisy channel model (Shannon, 1948). Correct sentences are produced by a predefined generative proba-bilistic model, and lesioned by the noise model. We learn the noise model parameters using an expectation-maximization (EM) approach (Demp-ster et al., 1977; Wu, 1983). Our model allows us to deduce the original intended sentence by looking for the the highest probability parses over the entire sentence, which leads to automated whole sentence spelling and grammar correction based on contex-tual information.

In Section 2, we discuss previous work, followed by an explanation of our model and its implementa-tion in Sections 3 and 4. In Section 5 we present a novel technique for evaluating the task of auto-mated grammar and spelling correction, along with the data set we collected for our experiments. Our experiment results and discussion are in Section 6. Section 7 concludes this paper. Much of the previous work in the domain of auto-mated grammar correction has focused on identi-fying grammatical errors. Chodorow and Leacock (2000) used an unsupervised approach to identify-ing grammatical errors by looking for contextual cues in a  X  2 word window around a target word. To identify errors, they searched for cues which did not appear in the correct usage of words. Eeg-olofsson and Knutsson (2003) used rule-based meth-ods to approach the problem of discovering preposi-tion and determiner errors of L2 writers, and var-ious classifier-based methods using Maximum En-tropy models have also been proposed (Izumi et al., 2003; Tetreault and Chodorow, 2008; De Felice and Pulman, 2008). Some classifier-based methods can be used not only to identify errors, but also to deter-mine suggestions for corrections by using the scores or probabilities from the classifiers for other possi-ble words. While this is a plausible approach for grammar correction, there is one fundamental dif-ference between this approach and the way humans edit. The output scores of classifiers do not take into account the observed erroneous word, changing the task of editing into a fill-in-the-blank selection task. In contrast, editing makes use of the writer X  X  erro-neous word which often encompasses information neccessary to correctly deduce the writer X  X  intent.
Generation-based approaches to grammar correc-tion have also been taken, such as Lee and Sen-eff (2006), where sentences are paraphrased into an over-generated word lattice, and then parsed to se-lect the best rephrasing. As with the previously men-tioned approaches, these approaches often have the disadvantage of ignoring the writer X  X  selected word when used for error correction instead of just error detection.

Other work which relates to automated grammar correction has been done in the field of machine translation. Machine translation systems often gen-erate output which is grammatically incorrect, and automated post-editing systems have been created to address this problem. For instance, when translat-ing Japanese to English, the output sentence needs to be edited to include the correct articles, since the Japanese language does not contain articles. Knight and Chander (1994) address the problem of select-ing the correct article for MT systems. These types of systems could also be used to facilitate grammar correction.

While grammar correction can be used on the out-put of MT systems, note that the task of grammar correction itself can also be thought of as a machine translation task, where we are trying to  X  X ranslate X  a sentence from an  X  X ncorrect grammar X  language to a  X  X orrect grammar X  language. Under this idea, the use of statistical machine translation techniques to correct grammatical errors has also been explored. Brockett et al. (2006) uses phrasal SMT techniques to identify and correct mass noun errors of ESL stu-dents. D  X  esilets and Hermet (2009) use a round-trip translation from L2 to L1 and back to L2 to cor-rect errors using an SMT system, focusing on errors which link back to the writer X  X  native language.
Despite the underlying commonality between the tasks of machine translation and grammar correc-tion, there is a practical difference in that the field of grammar correction suffers from a lack of good quality parallel corpora. While machine translation has taken advantage of the plethora of translated documents and books, from which various corpora have been built, the field of grammar correction does not have this luxury. Annotated corpora of gram-matical errors do exist, such as the NICT Japanese Learner of English corpus and the Chinese Learner English Corpus (Shichun and Huizhong, 2003), but the lack of definitive corpora often makes obtaining data for use in training models a task within itself, and often limits the approaches which can be taken.
Using classification or rule-based systems for grammatical error detection has proven to be suc-cessful to some extent, but many approaches are not sufficient for real-world automated grammar correc-tion for various of reasons. First, as we have already mentioned, classification systems and generation-based systems do not make full use of the given data when trying to make a selection. This limits the system X  X  ability to make well-informed edits which match the writer X  X  original intent. Second, many of the systems start with the assumption that there is only one type of error. However, ESL students often make several combined mistakes in one sentence. These combined mistakes can throw off error detec-tion/correction schemes which assume that the rest of the sentence is correct. For example, if a student erroneously writes  X  X uch poeple X  instead of  X  X any people X , a system trying to correct  X  X any/much X  er-rors may skip correction of much to many because it does not have any reference to the misspelled word  X  X oeple X . Thus there are advantages in looking at the sentence as a whole, and creating models which al-low several types of errors to occur within the same sentence. We now present our model, which sup-ports the addition of various types of errors into one combined model, and derives its response by using the whole of the observed sentence. Our noisy channel model consists of two main com-ponents, a base language model and a noise model. The base language model is a probabilistic lan-guage model which generates an  X  X rror-free X  sen-tence 1 with a given probability. The probabilistic noise model then takes this sentence and decides whether or not to make it erroneous by inserting various types of errors, such as spelling mistakes, article choice errors, wordform choice errors, etc., based on its parameters (see Figure 1 for example). Using this model, we can find the posterior proba-bility p ( S orig | S obs ) using Bayes rule where S orig the original sentence created by our base language model, and S obs is the observed erroneous sentence.
For the language model, we can use various known probabilistic models which already have de-fined methods for learning the parameters, such as n-gram models or PCFGs. For the noise model, we need some way to learn the parameters for the mis-takes that a group of specified writers (such as Ko-rean ESL students) make. We address this issue in Section 4.

Using this model, we can find the highest likeli-hood error-free sentence for an observed output sen-tence by tracing all possible paths from the language model through the noise model and ending in the ob-served sentence as output. To actually implement our model, we use a bigram model for the base language model, and various noise models which introduce spelling errors, ar-ticle choice errors, preposition choice errors, etc. All models are implemented using weighted finite-state tranducers (wFST). For operations on the wF-STs, we use OpenFST (Allauzen et al., 2007), along with expectation semiring code supplied by Markus Dryer for Dreyer et al. (2008). 4.1 Base language model The base language model is a bigram model imple-mented by using a weighted finite-state transducer (wFST). The model parameters are learned from the British National Corpus modified to use Amer-ican English spellings with Kneser-Ney smoothing. To lower our memory usage, only bigrams whose words are found in the observed sentences, or are determined to be possible candidates for the correct words of the original sentence (due to the noise mod-els) are used. While we use a bigram model here for simplicity, any probabilistic language model having a tractable intersection with wFSTs could be used. For the bigram model, each state in the wFST rep-resents a bigram context, except the end state. The arcs of the wFST are set so that the weight is the bi-gram probability of the output word given the con-text specified by the from state, and the output word is a word of the vocabulary. Thus, given a set of n words in the vocabulary, the language model wFST had one start state, from which n arcs extended to each of their own context states. From each of these nodes, n + 1 arcs extend to each of the n context states and the end state. Thus the number of states in the language model is n + 2 and the number of arcs is O ( n 2 ) . 4.2 Noise models For our noise model, we created a weighted finite-state transducer (wFST) which accepts error-free in-put, and outputs erroneous sentences with a spec-ified probability. To model various types of human errors, we created several different noise models and composed them together, creating a layered noise model. The noise models we implement are spelling errors, article choice errors, preposition choice er-rors, and insertion errors, which we will explain in more detail later in this section.

The basic design of each noise wFST starts with an initial state, which is also the final state of the wFST. For each word found in the language model, an arc going from the initial state to itself is created, with the input and output values set as the word. These arcs model the case of no error being made. In addition to these arcs, arcs representing prediction errors are also inserted. For example, in the article choice error model, an arc is added for each possible ( input, output ) article pair, such as a : an for making the mistake of writing an instead of a . The weights of the arcs are the probabilities of introducing errors, given the input word from the language model. For example, the noise model shown in Figure 2 shows a noise model in which a will be written correctly with a probability of 0.9, and will be changed to an or the with probabilities 0.03 and 0.07, respectively. For this model to work correctly, the setting of the probabilities for each error is required. How this is done is explained in Section 4.3. 4.2.1 Spelling errors
The spelling error noise model accounts for spelling errors made by writers. For spelling er-rors, we allowed all spelling errors which were a Damerau-Levenshtein distance of 1 (Damerau, 1964; Levenshtein, 1966). While allowing a DL dis-tance of 2 or higher may likely have better perfor-mance, the model was constrained to a distance of 1 due to memory constraints. We specified one param-eter  X  n for each possible word length n . This param-eter is the total probability of making a spelling error for a given word length. For each word length we distributed the probability of each possible spelling error equally. Thus for word length n , we have n deletion errors, 25 n substitution errors, n  X  1 transposition errors, and 26( n + 1) insertion er-rors, and the probability for each possible error is length for spelling errors to 22, giving us 22 param-eters. 4.2.2 Article choice errors
The article choice error noise model simulates in-correct selection of articles. In this model we learn n ( n  X  1) parameters, one for each article pair. Since there are only 3 articles ( a, an, the ), we only have 6 parameters for this model. 4.2.3 Preposition choice errors
The preposition choice error noise model simu-lates incorrect selection of prepositions. We take the 12 most commonly misused prepositions by ESL writers (Gamon et al., 2009) and specify one param-eter for each preposition pair, as we do in the article choice error noise model, giving us a total of 132 parameters. 4.2.4 Wordform choice errors
The wordform choice error noise model simulates choosing the incorrect wordform of a word. For ex-ample, choosing the incorrect tense of a verb (e.g. went  X  go ), or the incorrect number marking on a noun or verb (e.g. are  X  is ) would be a part of this model. This error model has one parameter for every number of possible inflections, up to a maximum of 12 inflections, giving us 12 parameters. The param-eter is the total probability of choosing the wrong inflection of a word, and the probability is spread evenly between each possible inflection. We used CELEX (Baayen et al., 1995) to find all the possible wordforms of each observed word. 4.2.5 Word insertion errors
The word insertion error model simulates the ad-dition of extraneous words to the original sentence. We create a list of words by combining the prepo-sitions and articles found in the article choice and preposition choice errors. We assume that the words on the list have a probability of being inserted erro-neously. There is a parameter for each word, which is the probability of that word being inserted. Thus we have 15 parameters for this noise model. 4.3 Learning noise model parameters To achieve maximum performance, we wish to learn the parameters of the noise models. If we had a large set of erroneous sentences, along with a hand-annotated list of the specific errors and their correc-tions, it would be possible to do some form of super-vised learning to find the parameters. We looked at the NICT Japanese Learner of English (JLE) corpus, which is a corpus of transcripts of 1,300 Japanese learners X  English oral proficiency interview. This corpus has been annotated using an error tagset (Izumi et al., 2004). However, because the JLE cor-pus is a set of transcribed sentences, it is in a differ-ent domain from our task. The Chinese Learner En-glish Corpus (CLEC) contains erroneous sentences which have been annotated, but the CLEC corpus had too many manual errors, such as typos, as well as many incorrect annotations, making it very diffi-cult to automate the processing. Many of the correc-tions themselves were also incorrect. We were not able to find of a set of annotated errors which fit our task, nor are we aware that such a set exists. Instead, we collected a large data set of possibly erroneous sentences from Korean ESL students (Section 5.1). Since these sentences are not annotated, we need to use an unsupervised learning method to learn our pa-rameters.

To learn the parameters of the noise models, we assume that the collected sentences are random out-put of our model, and train our model using the EM algorithm. This was done by making use of the V -expectation semiring (Eisner, 2002). The V -expectation semiring is a semiring in which the weight is defined as R  X  0  X  V , where R can be used to keep track of the probability, and V is a vector which can be used to denote arc traversal counts or feature counts. The weight for each of the arcs in the noise models was set so that the real value was the probability and the vector V denoted which choice (having a specified error or not) was made by select-ing the arc. We create a generative language-noise model by composing the language model wFST with the noise model wFSTs, as shown in Figure 3. By using the expectation semiring, we can keep track of the probability of each path going over an erroneous arc or non-erroneous arc.

Once our model is set up for the E step using the initial parameters, we must compute the expected number of noise model arc traversals for use in cal-culating our new parameters. To do this, we need to find all possible paths resulting in the observed sen-tence as output, for each observed sentence. Then, for each possible path, we need to calculate the prob-ability of the path given the output sentence, and get the expected counts of going over each erroneous and error-free arc to learn the parameters of the noise model. To find a wFST with just the possible paths for each observed sentence, we can compose the language-noise wFST with the observed sentence wFST. The observed sentence wFST is created in the following manner. Given an observed sentence, an initial state is created. For each word in the sen-tence, in the order appearing in the sentence, a new state is added, and an arc is created going from the previously added state to the newly added state. The new arc takes the observed word as input and also uses it as output. The weight/probability for each arc is set to 1. Composing the sentence wFST with the language-noise wFST has the effect of restricting the new wFST to only have sentences which out-put the observed sentence from the language-noise wFST. We now have a new wFST where all valid paths are the paths which can produce the observed sentence. To find the total weight of all paths, we first change all input and output symbols into the empty string. Since all arcs in this wFST are ep-silon arcs, we can use the epsilon-removal operation (Mohri, 2002), which will reduce the wFST to one state with no arcs. This operation combines the to-tal weight of all paths into the final weight of the sole state, giving us the total expectation value for that sentence. By doing this for each sentence, and adding the expectation values for each sentence, we can easily compute the expectation step, from which we can find the maximizing parameters and update our parameters accordingly. 4.4 Finding the maximum likelihood correction Once the parameters are learned, we can use our model to find the maximum likelihood error-free sentence. This is done by again creating the lan-guage model and noise model with the learned pa-rameters, but this time we set the weights of the noise model to just the probabilities, using the log semiring, since we do not need to keep track of ex-pected values. We also set the language model input for each arc to be the same word as the output, in-stead of using an empty string. Once again, we com-pose the language model with the noise models. We create a sentence wFST using the observed sentence we wish to correct, the same way the observed sen-tence wFST for training was created. This is now composed with the language-noise wFST. Now all we need to do is find the shortest path (when using minus-log probabilities) of the new wFST, and the input to that path will be our corrected sentence. We now present the data set and evaluation tech-nique used for our experiments. 5.1 Data Set To train our noise models, we collected around 25,000 essays comprised of 478,350 sentences writ-ten by Korean ESL students preparing for the TOEFL writing exam. These were collected from open web postings by Korean ESL students ask-ing for advice on their writing samples. In order to automate the process, a program was written to download the posts, and discard the posts that were deemed too short to be TOEFL writing samples. Also discarded were the posts that had a  X  X re X  or  X  X e.. X  in the title. Next, all sentences containing Korean were removed, after which some characters were changed so that they were in ASCII form. The remaining text was separated into sentences solely by punctuation marks . , ! , and ? . This resulted in the 478,350 sentences stated above. Due to the process, some of the sentences collected are actually sen-tence fragments, where punctuation had been mis-used. For training and evaluation purposes, the data set was split into a test set with 504 randomly se-lected sentences, an evaluation set of 1017 randomly selected sentences, and a training set composed of the remaining sentences. 5.2 Evaluation technique In the current literature, grammar correction tasks are often manually evaluated for each output cor-rection, or evaluated by taking a set of proper sen-tences, artificially introducing some error, and see-ing how well the algorithm fixes the error. Man-ual evaluation of automatic corrections may be the best method for getting a more detailed evaluation, but to do manual evaluation for every test output re-quires a large amount of human resources, in terms of both time and effort. In the case where artificial lesioning is introduced, the lesions may not always reflect the actual errors found in human data, and it is difficult to replicate the actual tendency of hu-mans to make a variety of different mistakes in a single sentence. Thus, this method of evaluation, which may be suitable for evaluating the correction performance of specific grammatical errors, would not be fit for evaluating our model X  X  overall perfor-mance. For evaluation of the given task, we have incorporated evaluation techniques based on current evaluation techniques used in machine translation, BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007).

Machine translation addresses the problem of changing a sentence in one language to a sentence of another. The task of correcting erroneous sentences can also be thought of as translating a sentence from a given language A, to another language B, where A is a broken language, and B is the correct language. Under this context, we can apply machine trans-lation evaluation techniques to evaluate the perfor-mance of our system. Our model X  X  sentence correc-tions can be thought of as the output translation to be evaluated. In order to use BLEU and METEOR, we need to have reference translations on which to score our output. As we have already explained in section 5.1, we have a collection of erroneous sentences, but no corrections. To obtain manually corrected sen-tences for evaluation, the test and evaluation set sen-tences and were put on Amazon Mechanical Turk as a correction task. Workers residing in the US were asked to manually correct the sentences in the two sets. Workers had a choice of selecting  X  X mpossi-ble to understand X ,  X  X orrect sentence X , or  X  X ncorrect sentence X , and were asked to correct the sentences so no spelling errors, grammatical errors, or punctu-ation errors were present. Each sentence was given to 8 workers, giving us a set of 8 or fewer corrected sentences for each erroneous sentence. We asked workers not to completely rewrite the sentences, but to maintain the original structure as much as pos-sible. Each hit was comprised of 6 sentences, and the reward for each hit was 10 cents. To ensure the quality of our manually corrected sentences, a native English speaker research assistant went over each of the  X  X orrected X  sentences and marked them as cor-rect or incorrect. We then removed all the incorrect  X  X orrections X .

Using our manually corrected reference sen-tences, we evaluate our model X  X  correction perfor-mance using METEOR and BLEU. Since METEOR and BLEU are fully automated after we have our ref-erence translations (manual corrections), we can run evaluation on our tests without any need for further manual input. While these two evaluation methods were created for machine translation, they also have the potential of being used in the field of grammar correction evaluation. One difference between ma-chine translation and our task is that finding the right lemma is in itself something to be rewarded in MT, but is not sufficient for our task. In this respect, eval-uation of grammar correction should be more strict. Thus, for METEOR, we used the  X  X xact X  module for evaluation.

To validate our evaluation method, we ran a sim-ple test by calculating the METEOR and BLEU scores for the observed sentences, and compared them with the scores for the manually corrected sen-tences, to test for an expected increase. The scores for each correction were evaluated using the set of corrected sentences minus the correction sentence being evaluated. For example, let us say we have the observed sentence o , and correction sentences c 1 , c 2 , c 3 and c 4 from Mechanical Turk. We run METEOR and BLEU on both o and c 1 using c 2 , c 3 and c 4 as the reference set. We repeat the process for o and c 2 , using c 1 , c 3 and c 4 as the reference, and so on, until we have run METEOR and BLEU on all 4 correction sentences. With a set of 100 manually labeled sen-tences, the average METEOR score for the ESL sen-tences was 0.8327, whereas the corrected sentences had an average score of 0.9179. For BLEU, the av-erage scores were 0.7540 and 0.8786, respectively, as shown in Table 1. Thus, we have confirmed that the corrected sentences score higher than the ESL sentence. It is also notable that finding corrections for the sentences is a much easier task than finding various correct translations, since the task of editing is much easier and can be done by a much larger set of qualified people. For our experiments, we used 2000 randomly se-lected sentences for training, and a set of 1017 an-notated sentences for evaluation. We also set aside a set of 504 annotated sentences as a development set. With the 2000 sentence training, the perfor-mance generally converged after around 10 itera-tions of EM. 6.1 Comparison with Aspell To check how well our spelling error noise model is doing, we compared the results of using the spelling error noise model with the output results of using the GNU Aspell 0.60.6 spelling checker. Since we  X  X  X  BLEU  X  X  X  are using METEOR and BLEU for our evaluation metric, we needed to get a set of corrected sentences for using Aspell. Aspell lists the suggested spelling corrections of misspelled words in a ranked order, so we replaced each misspelled word found by Aspell with the word with the highest rank (lowest score) for the Aspell corrections. One difference between Aspell and our model is that Aspell only corrects words which do not appear in the dictionary, while our method looks at all words, even those found in the dictionary. Thus our model can correct words which look correct by themselves, but seem to be incorrect due to the bigram context. Another differ-ence is that Aspell has the capability to split words, whereas our model does not allow the insertion of spaces. A comparison of the scores is shown in Ta-ble 2. We can see that our model has better per-formance, due to better word selection, despite the advantage that Aspell has by using phonological in-formation to find the correct word, and the disadvan-tage that our model is restricted to spellings which are within a Damerau-Levenstein distance of 1. This is due to the fact that our model is context-sensitive, and can use other information in addition to the mis-spelled word. For example, the sentence  X  X n contast, high prices of products would be the main reason for dislike. X  was edited in Aspell by changing  X  X on-tast X  to  X  X ontest X , while our model correctly selected  X  X ontrast X . The sentence  X  X o i can reach the theater in ten minuets by foot X  was not edited by Aspell, but our model changed  X  X inuets X  to  X  X inutes X . Another difference that can be seen by looking through the results is that Aspell changes every word not found in the dictionary, while our algorithm allows words it has not seen by treating them as unknown tokens. Since we are using smoothing, these tokens are left in place if there is no other high probability bigram to take its place. This helps leave intact the proper nouns and words not in the vocabulary. 6.2 Noise model performance and output Our next experiment was to test the performance of our model on various types of errors. Table 3 shows the BLEU and METEOR scores of our various error models, along with the number of sentences achiev-ing improved and reduced scores. As we have al-ready seen in section 6.1, the spelling error model increases the evaluation scores from the ESL base-line. Adding in the article choice error model and the word insertion error models in addition to the spelling error noise model increases the BLEU score performance of finding corrections. Upon observ-ing the outputs of the corrections on the develop-ment set, we found that the corrections changing a to an were all correct. Changes between a and the were sometimes correct, and sometimes incor-rect. For example,  X  X hich makes me know a exis-tence about X  was changed to  X  X hich makes me know the existence about X ,  X  X hen I am in a trouble. X  was changed to  X  X hen I am in the trouble. X , and  X  X any people could read a nonfiction books X  was changed to  X  X any people could read the nonfiction books X . For the last correction, the manual corrections all changed the sentence to contain  X  X any people could read a nonfiction book X , bringing down the evalu-ation score. Overall, the article corrections which were being made seemed to change the sentence for the better, or left it at the same quality.
The preposition choice error model decreased the performance of the system overall. Looking through the development set corrections, we found that many correct prepositions were being changed to incorrect prepositions. For example, in the sentence  X  X istrust about desire between two have been growing in their relationship. X , about was changed to of , and in  X  X s time goes by, ... X , by was changed to on . Since these changes were not found in the manual corrections, the scores were decreased.

For wordform errors, the BLEU and METEOR scores both increased. While the wordform choice noise model had the most sentences with increased scores, it also had the most sentences with decreased scores. Overall, it seems that to correct wordform errors, more context than just the preceding and fol-lowing word are needed. For example, in the sen-tence  X  X here are a lot of a hundred dollar phones in the market. X , phones was changed to phone . To infer which is correct, you would have to have access to the previous context  X  X  lot of X . Another example is  X ..., I prefer being indoors to going outside ... X , where going was changed to go . These types of cases illus-trate the restrictions of using a bigram model as the base language model.

The word insertion error model was restricted to articles and 12 prepositions, and thus did not make many changes, but was correct when it did. One thing to note is that since we are using a bigram model for the language model, the model itself is biased towards shorter sentences. Since we only in-cluded words which were needed when they were used, we did not run into problems with this bias. When we tried including a large set of commonly used words, we found that many of the words were being erased because of the bigrams models proba-bilistic preference for shorter sentences. 6.3 Limitations of the bigram language model Browsing through the development set data, we found that many of our model X  X  incorrect  X  X orrec-tions X  were the result of using a bigram model as our language model. For example,  X .., I prefer being in-doors to going outside in that... X  was changed to  X .., I prefer being indoors to go outside in that... X . From the bigram model, the probabilities p ( go to ) and p ( outside go ) are both higher than p ( going to ) and p ( outside going ) , respectively. To infer that going is actually correct, we would need to know the previ-ous context, that we are comparing  X  X eing indoors X  to  X  X oing outside X . Unfortunately, since we are using a bigram model, this is not possible. These kind of errors are found throughout the corrections. It seems likely that making use of a language model which can keep track of this kind of information would in-crease the performance of the correction model by preventing these kinds of errors. We have introduced a novel way of finding grammar and spelling corrections, which uses the EM algo-rithm to train the parameters of our noisy channel approach. One of the benefits of this approach is that it does not require a parallel set of erroneous sen-tences and their corrections. Also, our model is not confined to a specific error, and various error models may be added on. For training our noise model, all that is required is finding erroneous data sets. De-pending on which domain you are training on, this can also be quite feasible as we have shown by our collection of Korean ESL students X  erroneous writ-ing samples. Our data set could have been for ESL students of any native language, or could also be a data set of other groups such as young native En-glish speakers, or the whole set of English speakers for grammar correction. Using only these data sets, we can train our noisy channel model, as we have shown using a bigram language model, and a wFST for our noise model. We have also shown how to use weighted finite-state transducers and the expectation semiring, as well as wFST algorithms implemented in OpenFST to train the model using EM. For evalu-ation, we have introduced a novel way of evaluating grammar corrections, using MT evaluation methods, which we have not seen in other grammar correction literature. The produced corrections show the re-strictions of using a bigram language model. For fu-ture work, we plan to use a more accurate language model, and add more types of complex error models, such as word deletion and word ordering error mod-els to improve performance and address other types of errors.
 We are grateful to Randy West for his input and assistance, and to Markus Dreyer who provided us with his expectation semiring code. We would also like to thank the San Diego Supercomputer Center for use of their DASH high-performance computing system. Allauzen, C., Riley, M., Schalkwyk, J., Skut, W., and Mohri, M. (2007). OpenFst: A general and efficient weighted finite-state transducer li-brary. In Proceedings of the Ninth International
Conference on Implementation and Application of Automata, (CIAA 2007) , volume 4783 of Lec-ture Notes in Computer Science , pages 11 X 23. Springer. http://www.openfst.org .
 Baayen, H. R., Piepenbrock, R., and Gulikers, L. (1995). The CELEX Lexical Database. Release 2 (CD-ROM) . Linguistic Data Consortium, Univer-sity of Pennsylvania, Philadelphia, Pennsylvania. Brockett, C., Dolan, W. B., and Gamon, M. (2006).
Correcting ESL errors using phrasal SMT tech-niques. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th annual meeting of the Association for Com-putational Linguistics , ACL-44, pages 249 X 256,
Morristown, NJ, USA. Association for Computa-tional Linguistics.
 Chodorow, M. and Leacock, C. (2000). An unsu-pervised method for detecting grammatical errors.
In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference , pages 140 X 147, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
 Damerau, F. J. (1964). A technique for computer detection and correction of spelling errors. Com-mun. ACM , 7:171 X 176.
 De Felice, R. and Pulman, S. G. (2008). A classifier-based approach to preposition and determiner er-ror correction in L2 English. In Proceedings of the 22nd International Conference on Computa-tional Linguistics -Volume 1 , COLING  X 08, pages 169 X 176, Morristown, NJ, USA. Association for Computational Linguistics.
 Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of the
Royal Statistical Society. Series B (Methodolog-ical) , 39(1):pp. 1 X 38.
 D  X  esilets, A. and Hermet, M. (2009). Using auto-matic roundtrip translation to repair general errors in second language writing. In Proceedings of the twelfth Machine Translation Summit , MT Summit XII, pages 198 X 206.
 Dreyer, M., Smith, J., and Eisner, J. (2008). Latent-variable modeling of string transductions with finite-state methods. In Proceedings of the 2008 Conference on Empirical Methods in Natural
Language Processing , pages 1080 X 1089, Hon-olulu, Hawaii. Association for Computational Linguistics.
 Eeg-olofsson, J. and Knutsson, O. (2003). Au-tomatic grammar checking for second language learners -the use of prepositions. In In Nodalida . Eisner, J. (2002). Parameter estimation for prob-abilistic finite-state transducers. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL) , pages 1 X 8, Philadelphia.
 Gamon, M., Leacock, C., Brockett, C., Dolan, W. B., Gao, J., Belenko, D., and Klementiev,
A. (2009). Using statistical techniques and web search to correct ESL errors. In Calico Journal , Vol 26, No. 3, pages 491 X 511, Menlo Park, CA, USA. CALICO Journal.
 Izumi, E., Uchimoto, K., and Isahara, H. (2004).
The NICT JLE corpus exploiting the language learnersspeech database for research and educa-tion. In International Journal of the Computer, the
Internet and Management , volume 12(2), pages 119 X 125.
 Izumi, E., Uchimoto, K., Saiga, T., Supnithi, T., and
Isahara, H. (2003). Automatic error detection in the Japanese learners X  English spoken data. In
Proceedings of the 41st Annual Meeting on Asso-ciation for Computational Linguistics -Volume 2 , ACL  X 03, pages 145 X 148, Morristown, NJ, USA. Association for Computational Linguistics.
 Knight, K. and Chander, I. (1994). Automated postediting of documents. In Proceedings of the twelfth national conference on Artificial intelli-gence (vol. 1) , AAAI  X 94, pages 779 X 784, Menlo
Park, CA, USA. American Association for Artifi-cial Intelligence.
 Lavie, A. and Agarwal, A. (2007). Meteor: an au-tomatic metric for MT evaluation with high levels of correlation with human judgments. In StatMT  X 07: Proceedings of the Second Workshop on Statistical Machine Translation , pages 228 X 231,
Morristown, NJ, USA. Association for Computa-tional Linguistics.
 Lee, J. and Seneff, S. (2006). Automatic grammar correction for second-language learners. In Pro-ceedings of Interspeech .
 Levenshtein, V. I. (1966). Binary codes capable of correcting deletions, insertions, and reversals. So-viet Physics Doklady , 10:707 X 710.
 Mohri, M. (2002). Generic epsilon-removal and input epsilon-normalization algorithms for weighted transducers. In International Journal of
Foundations of Computer Science 13 , pages 129 X  143.
 Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002). Bleu: a method for automatic evaluation of machine translation. In ACL , pages 311 X 318. Shannon, C. (1948). A mathematical theory of communications. Bell Systems Technical Journal , 27(4):623 X 656.
 Shichun, G. and Huizhong, Y. (2003). Chinese
Learner English Corpus. Shanghai Foreign Lan-guage Education Press.
 Tetreault, J. R. and Chodorow, M. (2008). The ups and downs of preposition error detection in
ESL writing. In Proceedings of the 22nd Interna-tional Conference on Computational Linguistics -Volume 1 , COLING  X 08, pages 865 X 872, Mor-ristown, NJ, USA. Association for Computational Linguistics.
 Wu, C.-F. J. (1983). On the convergence properties of the EM algorithm. Ann. Statist. , 11(1):95 X 103.
