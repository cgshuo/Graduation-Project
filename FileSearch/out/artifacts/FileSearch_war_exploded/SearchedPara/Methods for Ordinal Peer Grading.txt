 Massive Online Open Courses have the potential to revolu-tionize higher education with their wide outreach and acces-sibility, but they require instructors to come up with scalable alternates to traditional student evaluation . Peer grading  X  having students assess each other  X  is a promising approach to tackling the problem of evaluation at scale, since the num-ber of  X  X raders X  naturally scales with the number of students. However, students are not trained in grading, which means that one cannot expect the same level of grading skills as in traditional settings. Drawing on broad evidence that ordinal feedback is easier to provide and more reliable than cardinal feedback [5, 38, 29, 9], it is therefore desirable to allow peer graders to make ordinal statements (e.g.  X  X roject X is bet-ter than project Y X ) and not require them to make cardinal statements (e.g.  X  X roject X is a B- X ). Thus, in this paper we study the problem of automatically inferring student grades from ordinal peer feedback, as opposed to existing methods that require cardinal peer feedback. We formulate the ordi-nal peer grading problem as a type of rank aggregation prob-lem, and explore several probabilistic models under which to estimate student grades and grader reliability. We study the applicability of these methods using peer grading data col-lected from a real class  X  with instructor and TA grades as a baseline  X  and demonstrate the efficacy of ordinal feedback techniques in comparison to existing cardinal peer grading methods. Finally, we compare these peer-grading techniques to traditional evaluation techniques.
 H.4 [ Information Systems Applications ]: Miscellaneous Algorithms, Experimentation, Theory Peer Grading, Ordinal Feedback, Rank Aggregation
The advent of MOOCs (Massive Online Open Courses) promises unprecedented access to education given their rela-tively low costs and broad reach, empowering learning across a diverse range of subjects for anyone with access to the In-ternet. Classes frequently have upwards of 20000 students, which is orders of magnitude larger than a conventional uni-versity class. Thus, instructors are forced to rethink class-room logistics and practices so as to scale to MOOCs.
One of the key open challenges is student evaluation for such large classes. Traditional assessment practices, such as instructors or teaching assistants (TAs) grading individ-ual student assignments, are simply infeasible at this scale. Consequently, assignments in most current MOOCs take the form of simple multiple-choice questions and other schemes that can be graded automatically. However, relying on such rigid testing schemes does not necessarily serve as a good in-dicator of learning and falls short of conventional test-design standards [16, 17]. Furthermore, such a restrictive test-ing methodology limits the learning outcomes that can be tested, or even limits the kinds of courses that can be offered. For example, liberal-arts courses and research-oriented classes require more open-ended assignments and responses (e.g., essays, project proposals, project reports).

Peer grading has the potential to overcome the limitations outlined above while scaling to the size of even the largest MOOCs. In peer grading, students  X  not instructors or TAs  X  provide feedback on the work of other students in their class [14, 22], meaning that the number of  X  X raders X  naturally grows with the number of students. While the scaling properties of peer grading are attractive, there are several challenges in making peer grading work.

One key challenge lies in the fact that students are not trained graders, which argues for making the feedback pro-cess as simple as possible. Given broad evidence that for many tasks ordinal feedback is easier to provide and more reliable than cardinal feedback [5, 38, 29, 9], it is there-fore desirable to base peer grading on ordinal feedback (e.g.  X  X roject X is better than project Y X ). Unfortunately, all ex-isting methods for aggregating peer feedback into an overall assessment require that students provide cardinal feedback (e.g.  X  X roject X is a B- X ). Furthermore, the efficacy of simple techniques for aggregating cardinal feedback, such as aver-aging, has been questioned [7, 10, 30]. While probabilistic machine learning methods have recently been proposed to address these challenges [32], they still face the problem that students may be grading on different scales. For example, students may have a preconception of what constitutes a B+ based on the university they come from. These scales may also be non-linear as the difference between an A+ and an A may not be the same as the difference between a C+ and a C.

To overcome the problems of cardinal feedback, we in-troduce the task of ordinal peer grading in this paper. By having students give ordinal statements and not cardinal statements as feedback, we offload the problem of develop-ing a scale from the student onto the peer grading algorithm. The key technical contributions of this paper lie in the de-velopment of methods for ordinal peer grading, where the goal is to automatically infer an overall assessment of a set of assignments from ordinal peer feedback. Furthermore, a secondary goal of our methods is to infer how accurately each student provides feedback, so that reliable grading can be incentivized (e.g., as a component of the overall grade). To this effect, we propose several machine learning methods for ordinal peer grading , which differ by how probability dis-tributions over rankings are modeled. For these models, we provide efficient algorithms for estimating assignment grades and grader reliabilities.

To study the applicability of our methods in real-world settings, we collected peer-assessment data as part of a university-level course. Using this data, we demonstrate the efficacy of the proposed ordinal feedback techniques in comparison to the existing cardinal feedback techniques. Furthermore, we compare our ordinal peer grading methods with tradi-tional evaluation techniques that were used in the course in parallel. Using this classroom data we also investigate other properties of these techniques, such as their robustness, data dependence and self-consistency. Finally, we analyze the re-sponses to a survey completed by students in the classroom experiment, indicating that most students found the peer grading experience (receiving and providing feedback) help-ful and valuable.
We begin by formally defining the peer grading problem, as it presents itself from a machine learning perspective. We are given a set of | D | assignments D = { d 1 ,...,d ( e.g., essays, reports) which need to be graded. Grading is done by a set of | G | graders G = { g 1 ,...,g | G | } ( e.g., student peer grader, reviewers), where each grader receives a subset D g  X  D to assess. The choice of assignments for each grader can be uniformly random, or can follow a deterministic or sequential design. In either case, the number of assignments that any grader assesses | D g | is much smaller than the total number of assignments | D | ( e.g., | D g | X  10).

Each grader provides feedback for his or her set of assign-ments D g . Ordinal and cardinal peer grading differ in the type of feedback a grader is expected to give: Cardinal Peer Grading (CPG): In cardinal peer grad-Ordinal Peer Grading (OPG): In ordinal peer grading,
D g (  X  D ) Set of items graded by grader g s d (  X &lt; ) Predicted grade for item d (larger is better)  X  g (  X &lt; + ) Predicted reliability of grader g d 2  X  d 1 d 2 is preferred/ranked higher than d 1 (in  X  )  X  1  X   X  2  X  way of resolving ties in  X  2 to obtain  X  1 Independent of the type of feedback that graders provide, the goal in peer grading is twofold.

We call the first goal grade estimation , which is the task of estimating the true quality of the assignments in D from the grader feedback. We distinguish between two types of grade estimation, which differ by how they express assign-ment quality. In ordinal grade estimation , the goal is to infer a ranking  X   X  of all assignments in D that most accu-rately reflects some true ordering (by quality)  X   X  . In cardi-nal grade estimation , the goal is to infer a cardinal grade  X  s for each d  X  D that most accurately reflects each true grade s . Note that the type of feedback does not necessarily de-termine whether the output of grade estimation is ordinal or cardinal. In particular, we will see that some of our methods can infer cardinal grades even if only given ordinal feedback.
The second goal is grader reliability estimation , which is the task of estimating how accurate the feedback of a grader is. Estimating grader reliability is important for at least two reasons. First, identifying unreliable grades allows us to downweight their feedback for grade estimation. Second, and more importantly, it allows us to incentivize good and thorough grading by making peer grading itself part of the overall grade. In the following, we will typically represent the reliability of a grader as a single number  X  g  X &lt; +
In the following sections, we derive and evaluate methods for grade estimation and grader reliability estimation in the Ordinal Peer Grading setting.
The grade estimation problem in Ordinal Peer Grading can be viewed as a specific type of rank aggregation problem. Rank aggregation describes a class of problem related to combining the information contained in rankings from mul-tiple sources. Many popular methods used today [15, 25, 11] build on classical models and techniques such as the semi-nal work by Thurstone [39], Mallows [28], Bradley &amp; Terry [8], Luce [27] and Plackett [33]. These techniques have been used in different domains, each of which have branched off their own set of methods.

Search Result Aggregation (also known as Rank Fu-sion or Metasearch ) has the goal of merging search result rankings from different sources to produce a single output ranking. Such aggregation has been widely used to improve over the performance of any single ranker in both supervised and unsupervised settings [3, 34, 40, 31]. Rank aggregation for search differs from Ordinal Peer Grading in several as-pects. First, grader reliability estimation is not a goal in itself. Second, the success of search result aggregation de-pends mostly on correctly identifying the top items, while grade estimation aims to accurately estimate the full rank-ing. Third, ties and data sparsity are not an issue in search result aggregation, since (at least in principle) input rank-ings are total orders over all results. Algorithm 1 Normal Cardinal-Score (NCS) Algorithm (called PG 1 in [32]) is used as a baseline in our experiments  X  g  X  Gamma (  X  0 , X  0 ) . Grader Reliability Estimate  X  s d ,  X   X  g and  X  b g . Using MLE
Social Choice and Voting Systems perform rank ag-gregation on preferences that a set of individuals stated over competing items/interests/candidates. The goal is to iden-tify the most preferred alternatives given conflicting pref-erences [2]. Commonly used aggregation techniques are the Borda count and other Condorcet voting schemes [3, 13, 26]. These methods are ill-suited for the OPG problem, as they do not model voter reliability, typically assume rankings of all alternatives (or at least leave the choice of alternatives up to the voter), and usually focus on the top of the rankings.
Crowdsourcing is probably the most closely related ap-plication domain, where the goal is to merge the feedback from multiple crowdworkers [19, 6]. Due to the differing quality of these workers, modeling the worker reliability is essential [35, 11]. The key difference in our setting is that the number of items is large and we would like to correctly order all of them, not just identify the top-few.

Rank-aggregation has also been used for other settings such as multilabel/multiclass classification (by combining different classifiers) [23] or for learning player skills in a gam-ing environment [18]. Is is impossible to survey the vast liter-ature on this topic and thus we refer the interested reader to a comprehensive survey on the topic [24]. These techniques have also been adapted for educational assessment [4], via a graphical model based approach, for modeling the diffi-culty of questions and estimating the correct answers in a crowdsourced setting. However these techniques are neither applicable for a peer grading setting nor can they handle open-ended answers (like essays).
With the advent of online courses, peer grading has been increasingly used for large classes with mixed results [7, 10, 30]. While most previous uses of peer grading have re-lied on simple estimation techniques like averaging cardi-nal feedback scores, recently a probabilistic learning algo-rithms has been proposed for peer grade estimation [32]. However, this method requires that students provide car-dinal scores as grades. A second limitation of the method in [32] is that they incentivize grader reliability by relating it to the grader X  X  own assignment score. However, such a setup is inappropriate when there are groups (such as our setting) or where external graders/reviewers are used ( e.g., conference reviewing). In addition, such an indirect incen-tive is harder to communicate and justify compared to the direct grader reliability estimates used in our case. Lastly their approach requires that each student grades some as-signments that were previously graded by the instructor in order to estimate grader reliability. This seems wasteful, given that students are only able to grade a small number of assignments in total. We empirically compare their cardinal peer grading technique (Algorithm 1, using MLE instead of Gibbs sampling) with the ordinal peer grading techniques proposed in this paper.

Overall, given the limited amount of attention that the peer grading problem has received in the machine learning literature so far, we believe there is ample opportunity to improve on the state-of-the-art and address shortcomings that currently exist [36], which is reinforced by concurrent work on the topic by others [12, 37].
In this section, we develop ordinal peer grading methods for grade estimation and then extend these methods to the problem of grader reliability estimation. Our methods are publicly available as software at www.peergrading.org , where we also provide a web service for peer grade es-timation . These methods require as data an i.i.d. sample of orderings where each ordering sorts a subset of assignments according to the judgment of grader g i .
Our grade estimation methods are based on models that represent probability distributions over rankings. In partic-ular, we extend Mallow X  X  Model (Sec 3.1.1), the Bradley-Terry model (Sec 3.1.3), Thurstone X  X  model (Sec 3.1.4), and the Plackett-Luce model (Sec 3.1.5) as appropriate for the ordinal peer grading problem.
Mallow X  X  model [28] describes a distribution over rankings  X  in terms of the distance  X  (  X   X , X  ) from a central ranking  X   X  , which in our setting is the true ranking  X   X  of assignments by quality.
 While maximum likelihood estimation of  X   X  given observed rankings is NP-hard for many distance functions [13, 34], tractable approximations are known for special cases. In this work, we use the following tractable Kendall- X  distance [20], which assumes that both rankings are total orderings over all assignments.

Definition 1. We define the Kendall- X  Distance  X  K be-tween ranking  X  1 and ranking  X  2 as It measures the number of incorrectly ordered pairs between the two rankings. In our case, the rankings that students provide can have ties. We interpret these ties as indifference ( i.e., agnostic to either ranking), which leads to the following model, where the summation in the numerator is over all total orderings  X  0 consistent with the weak ordering  X  . Note also that the input ranking  X  may only sort a subset of assignments. In such cases, we appropriately restrict the normalization constant in (4). For Kendall- X  distance, this normalization constant can be computed efficiently, and it only depends on the number of elements in the ranking. Algorithm 2 Computing MLE ranking for Mallows Model 1: C  X  D . C contains unranked items 2: for i = 1 ... | D | do 3: for d  X  C do 4: x d  X  P g  X  G  X  g | d 0  X  C : d 0  X  g d | X  X  d 0  X  C : d 5: d  X   X  min d  X  C x d . Select highest scoring item 6: r ( X   X  ) d  X   X  i . Rank as next item 7: C  X  C/d  X  . Remove d  X  from candidate set 8: return  X   X  The numerator can likewise be computed efficiently. Note that ties in the grader rankings  X  ( g ) do not affect the normal-ization constant under the interpretation of indifference.
Under this modified Mallow X  X  model, the maximum like-lihood estimator of the central ranking  X   X  is Computing the maximum likelihood estimate  X   X  as an esti-mate of the true ranking by quality  X   X  requires finding the Kemeny-optimal aggregate , which is known to be NP-hard [13]. However numerous approximations have been studied in the rank aggregation literature [13, 21, 1]. In this work we use a simple greedy algorithm as shown in Algorithm 2.
As an alternative algorithm for computing the estimated ranking, we utilize a Borda count-like approximation for the Mallows model (which we denote as MAL BC ), where Line 2 of Algorithm 2 is replaced with Mallow X  X  model presented above has two shortcomings. First, it does not output a meaningful cardinal grade for the assignments, which makes it applicable only to ordinal grade estimation. Second, the distance  X  K does not distinguish between misordering assignments that are similar in quality from those that have a large quality difference.

To address these two shortcomings, we propose an exten-sion which estimates cardinal grades  X  s d for all assignments. To this effect, we introduce the following score-weighted ranking distance, which scales the distance induced by each misranked pairs by its estimated grade difference.

Definition 2. The score-weighted Kendall- X  distance  X 
SK over rankings  X  1 ,  X  2 given cardinal scores s d is Treating ties in the grader rankings as described above re-sults in a score-weighted version of the Mallows model (MALS). We use the following maximum a posteriori estimator to es-timate the scores  X  s .  X  s = argmax s Note that  X   X  can be obtained by sorting items as per  X  s Pr (  X s ) = Q d  X  D Pr ( X  s d ) is the prior on the latent item scores. In our experiments we model Pr ( X  s d )  X  X  (0 , 9), and use the same prior in all of our methods. While the resulting ob-jective is not necessarily convex, we use Stochastic Gradient Descent (SGD) for grade estimation and initialize the grades using a scaled-down Mallows solution.
The above models define distributions over rankings as a function of a ranking distance, and they require approximate methods for solving the maximum likelihood problem. As an alternative, we can utilize rank aggregation models based on distributions over pairwise preferences, since a ranking of n items can also be viewed as a set of preferences over the 2 item pairs. The Bradley-Terry model [8] is one model for pairwise preferences, and it derives a distribution based on the differences of underlying item scores s d through a logistic link function.
 Since each preference decision is modeled individually, the feedback from the grader could be a (possibly inconsistent) set of preferences that does not necessarily have to form a consistent ordering. The following is the maximum a poste-riori estimator used in this paper.  X  s = argmax s The resulting objective is (jointly) log-convex in all of the es-timated grades  X  s d , with the gradients taking a simple form. Hence SGD can be used to estimate the global optimal grades efficiently. We treat ties as the absence of a pref-erence. One can also extend this model to incorporate ties more explicitly, but we do not discuss this for brevity. An alternate to the logistic link function of the Bradley-Terry model is to utilize a normal distribution for the pair-wise preferences. Like the Bradley-Terry model, the re-sulting Thurstone model [39] model can be understood as a random utility model using the following process: For each pair of items d i ,d j , the grader samples (latent) val-the pair based on the two values. The mean of the normal distribution of d i is the quality s d i . Maximum a posteri-ori estimation of the scores s requires maximization of the following function: F is the CDF of the standard normal distribution. This objective function is log-convex and we use SGD to optimize it.
A drawback of the pairwise preference models is that they can be less expressive than models built on distributions over rankings. An extension to the Bradley-Terry model (the Plackett-Luce model [33]) allows us to use distributions over rankings, while still retaining convexity and simplicity Algorithm 3 Alternating SGD-based Minimization Require: N  X  0 (Number of iterations), Likelihood L 1: Obj  X  X  X  log L 2:  X s  X  SGD S ( Obj, X  = 1 ) . Est. scores w/o reliabilities 3: for i = 1 ...N do 4:  X   X  SGD G ( Obj,  X s ) . Estimate reliabilities 5:  X s  X  SGD S ( Obj, X  ) . Est. scores with reliabilities 6: return  X s , X  of gradient computation. This model can be best understood as a multi-stage experiment where at each stage, an item d is drawn (w/o replacement) with probability  X  e s d i probability of observing ranking  X  ( g ) under this process is: The resulting maximum a posteriori estimator is  X  s = argmax s
While the methods discussed in Section 3.1 allow us to estimate assignment grades from ordinal feedback, they still do not give us means to directly estimate grader reliabilities  X   X  . However, there is a generic way of extending all methods presented above to incorporate grader reliabilities. Using Mallow X  X  model as an example, we can introduce  X   X  variability parameter as follows: The resulting estimator of both  X   X  and  X   X  is  X   X ,  X   X  = argmax  X , X  where Pr ( X   X  g ) is the prior on the grader reliability. In this work we use a Gamma prior  X   X  g  X  Gamma (10 , 0 . 1).
Similarly, the other objectives can also be extended in this manner as seen in Table 2. While many of the ex-tended objectives, such as the one above in Eq. (13), are convex in the grader reliabilities  X   X  g (for given  X   X  ), they un-fortunately are not jointly convex in the reliabilities and the estimated grades. We thus use an iterative alternating-minimization technique, which alternates between minimiz-ing the log-objective to estimate the assignment grades and minimizing the log-objective to estimate the grader relia-bilities. This iterative alternating approach using stochas-tic gradient descent is used for all joint estimation tasks in this paper. Note that methods which estimate the reliabil-ities using Algorithm 3 are denoted by a +G suffix to the method, while those that simply estimate the assignment grades are represented by the method name alone.
In the following we present experiments that compare or-dinal and cardinal peer grading methods. We evaluate their ability to predict instructor grades, their variability, their Table 2: Summary of the ordinal methods studied which model the grader X  X  reliabilities, including the ability to output cardinal scores and if the resulting objective is convex in these scores. robustness to bad peer grading, and their ability to identify bad graders. We also present the results from a qualitative student survey to evaluate how students perceived the peer grading process.
We use a real dataset consisting of peer feedback, TA grades, and instructor grades for evaluating the peer grading methods proposed in this paper. This data was collected as part of a senior-undergraduate and masters-level class with an enrollment of about 170 students. The class was staffed with 9 Teaching Assistants (TAs) that participated in grad-ing, and a single Instructor. This size of class is attractive, since it is large enough for collecting a substantial number of peer grades, while at the same time allowing traditional instructor and TA grading to serve as a baseline. The avail-ability of instructor grades makes our data different from other peer-grading evaluations used in the past (e.g., [32]). We are happy to provide the data to other researchers sub-ject to IRB approval.

The dataset consists of two parts that were graded in-dependently, namely the poster presentation and the final report of an 8-week long course project. Students worked in groups of 3-4 students for the duration of the project, and there were a total of 44 project groups. While student worked in groups, peer grading was performed individually via the Microsoft Conference Management Toolkit (CMT) system. The peer grading process was performed single-blind for the posters and double-blind for the reports, and the reviewer assignments were made uniformly at random. Students were given clear directives and asked to focus on aspects such as novelty and clarity (among others) while de-termining their grade. They were also asked to justify their grade by providing feedback comments. Students were told that a part of their grade depends on the quality of their peer feedback.

All grading was done on a 10-point (cardinal) Likert scale, where 10 was labeled  X  X erfect X , 8  X  X ood X , 5  X  X orderline X , 3  X  X eficient X  and 1  X  X nsatisfactory X . This will allow us to com-pare cardinal and ordinal peer grading methods, where or-dinal methods merely use the ordering (possibly with ties) implied by the cardinal scores. Note that in a true applica-tion of ordinal peer grading accuracy could improve, since it would allow simplifying the grading instructions and reduce cognitive overhead if students did not have to worry about the precise meaning of specific cardinal grades.

The following describes the grading processes used at each stage, and Table 3 summarizes some of the key statistics. Table 3: Statistics for the two datasets (PO=Poster, FR=Report) from the classroom experiment along with the staff (TAs/Meta/Instructor) and student grade distributions.
The poster presentations took place in a two-hour poster session. Two groups did not present their poster. Students were encouraged to rotate presenting their poster. This likely increased variability of grades, since different review-ers often saw different presenters. Students and TAs took notes and entered their reviews via CMT afterwards.
The TA Grades were independent, meaning that the TAs did not see the peer reviews before entering their review. There were on average 1.85 TA reviews for each poster.
The Peer Grades totaled on average 23.71 reviews for each poster, with each peer reviewer reviewing 6.73 posters on average.

The final Meta Grade for each poster was determined as follows. One of the TAs that already provided an indepen-dent review was selected as a meta-reviewer. This TA was asked to aggregate all the arguments brought forward in the reviews and make a final grade on the same 10-point scale. The instructor oversaw this process, but intervened only on very few grades.
At the end of the project, groups submitted a report of about 10 pages in length. The reviewing process was similar to that of the poster presentations, but with one important difference  X  namely that all project reports were graded by the TAs and the instructor without any knowledge of the peer reviews, as detailed below.

On average each report received 13.32 Peer Grades as the overall score on each of the peer reviews (students were also asked for component scores like  X  X larity X , etc.).

Each report also received two TA Grades , which the TAs submitted without knowledge of the peer reviews.

Finally, each report received an Instructor Grade , follow-ing the traditional process of project grading in this class. The instructor and head TA each graded half the projects and determined the grade based on their own reading of the paper, taking the TA reviews as input. These grades were provided without viewing the peer reviews. We can therefore view the instructor grades as an assessment that is entirely independent of the peer grades (in contrast to the Meta Grades for the posters, which have some dependency).
A commonly used measure for reporting student perfor-mance (among many standardized tests) is the percentile rank relative to all students in the class. Following this practice, we use percentile rank as the grade itself (a letter grade can easily be derived via curving), and report ranking metrics as our main indicators of performance. In particu-lar, we use the following variant of Kendall- X  that accounts for ties. Figure 1: Comparing peer grading methods (w/o grader reliability estimation) against Meta and In-structor Grades in terms of E K (lower is better). Note that this measure is not symmetric, assuming that the first argument is a target ranking and the second argument is a predicted ranking. It treats ties in the target ranking as indifference . Ties in the predicted ranking are treating as a lack of information, incurring a 1 2 error ( i.e., equivalent to breaking ties randomly). Such a correction is necessary for evaluation purposes, since otherwise predicted rankings with all ties (which convey no information) would incur no error. Normalizing  X  KT (  X  1 , X  2 ) and accounting for the fact that we may have more than one target ranking leads to the following error measure.

Definition 3. Given a set of target rankings S g , we de-fine the Kendall- X  error E K of predicted ranking  X  I as: This error macro-averages the (normalized)  X  KT errors for each target ranking. Due to the normalization, they lie be-tween 0 (indicating perfect agreement) and 100% (indicating reversal with target rankings). A random ranking has ex-pected E K error of 50%.
The first question we address is in how far peer grading resembles the grades given by an instructor. Specifically, we investigate whether ordinal peer grading methods achieve similar performance as cardinal peer grading methods, even though ordinal methods receive strictly less information.
For all methods considered in this paper, Figure 1 shows the Kendall- X  error E K compared to the Meta Grades for the Posters, and compared to the Instructor Grades for the Reports. The errorbars show estimated standard deviation using bootstrap-type resampling.

On the posters, none of the methods show significantly worse performance than another method. In particular, there is no evidence that the cardinal methods are perform-ing better than the ordinal methods. A similar conclusion also holds for the reports. However, here the ordinal meth-ods based on Mallow X  X  model perform better than the car-Figure 2: Comparing peer grading methods (w/o grader reliability estimation) against TA Grades in terms of E K , using TA grades as the target ranking. dinal NCS 1 method [32] (see Algorithm 1), as well as some of the other ordinal methods. Simply averaging the cardinal scores of the peer graders, which we call Score Averaging (SCAVG), performs surprisingly well.

In summary, most methods achieve an E K between 20% and 30% on both problems, but all have large standard de-viations. The E K appears lower for the posters than for the projects, which can be explained by the fact that the Meta Grade was influenced by the peer grades. But how good is an E K between 20% and 30%?
We now consider how Peer Grading compares to having each assignment graded by a TA. For medium sized classes, TA grading may still be feasible. It is therefore interesting to know if TA grading is clearly preferable to Peer Grading when it is feasible. But more importantly, the inter-judge agreement between multiple TAs can give us reference points for the accuracy of Peer Grading.
 As a first reference point, we estimate how well the TA Grades reflect the Meta Grades for the posters and the In-structor Grades for the reports. In particular, we consider a grading process where each assignment is graded by a single TA that assigns a cardinal grade. Each TA grades a fraction of the assignments, and a final ranking of the assignments is then computed by sorting all cardinal grades. We call this grading process TA Grading .
 We can estimate the E K of TA grading with the Meta Grades and the Instructor Grades, since we have multiple TA grades for most assignments. We randomly resample a TA grade from the available grades for each assignment, compute the ranking, and then estimate mean and standard deviation of the E K over 5000 samples. This leads to a mean E
K of 22 . 0  X  16 . 0 for the posters and 22 . 2  X  6 . 8 for the reports. Comparing these to the E K of the peer grading methods in Figure 1, we see that they are comparable to the performance of many peer grading methods  X  even though the E K of TA grading is favorably biased. Note that Meta Grades and the Instructor Grades were assigned based on the same TA grades we are evaluating against.
We tuned the hyperparameters of the NCS model to max-imize performance. We also used a fixed grader reliability parameter in the NCS model, since it provided better per-formance than with reliability estimation (NCS+G). Figure 3: Self-consistency of peer-grading methods (w/o grader reliability estimation) in terms of E K .
To avoid this bias and provide a fairer comparison with TA grading, we also investigated how consistent peer grades are with the TA grades, and how consistent TA grades are be-tween different TAs. Figure 2 shows the E K of the peer grad-ing methods when using TA Grades as the target ranking for both the Posters and the Reports. Variances were again esti-mated via bootstrap resampling. Note that TA Grades were submitted without knowledge of the Peer Grades. Overall, the peer grades have an E K with the TA Grades that is sim-ilar to the E K with the respective Final grades considered in the previous subsection. Again, there is no evidence that the ordinal peer grading methods are less predictive of the TA Grades than the cardinal peer grading methods.

To estimate E K between different TAs, we use the fol-lowing resampling procedure. In a leave-one-out fashion, we treat the grades of a randomly selected TA as the tar-get ranking and compute the predicted ranking by sampling from the other TAs grades as described above. Averaging over 5000 repetitions reveals that the E K between the TAs is 47 . 5  X  21 . 0 for the posters and 34 . 0  X  13 . 8 for the reports.
These numbers can be compared to the E K of peer grad-ing methods in Figure 2. For the Reports, peer grades are roughly as consistent with the TA grades as other TA grades are. For the posters the peer grading methods are substan-tially more predictive of TA grades than other TA grades. The reason for this is at least twofold. First, the peer grad-ing methods have access to much more data, which reduces variability (especially since presentations were not always given by the same student). Second, the peer grading meth-ods have enough data to correct for different grading scales, while offsets in grading scales can have disastrous conse-quences in TA grading.

Finally, we also consider the self-consistency of the peer grading methods. Analogous to the self-consistency of TA grading, we ask how similar are the grades we get if we repeat the grading procedure with a different sample of as-sessments. We randomly partition peer reviewers into two equally sized datasets. For each peer grading method, we perform grade estimation on both datasets, which generates two rankings of the assignments. Ties in these rankings are broken randomly to get total orderings. Figure 3 shows the E K between the two rankings (over 20 sampled partitions). For the posters, peer grading is substantially more self con-sistent than TA grading, and for the reports all peer grading methods have lower E K estimates than TA grading as well.
Overall, we conclude that there is no evidence that TA grading would have led to more accurate grading outcomes than peer grading.
How many reviewers are necessary for accurate peer grad-ing, and how many reviews does each peer grader need to do? To gauge how performance changes with the number of peer reviews, we performed two sets of experiments. First, we created 20 smaller datasets by downsampling the number of peer reviewers. The results are shown in the two rightmost graphs of Figure 4. Overall, the methods degrade gracefully when the number of reviewers is reduced. Overall, we find that most ordinal methods scale as well as cardinal methods for both datasets.

A second way of increasing or reducing the amount of available data lies in the number of assignments that each student grades. Thus we repeated the experiment, but in-stead downsampled the number of assignments per reviewer (corresponding to a lower workload for each grader). The leftmost two plots of Figure 4 show the results. Again, we find that performance degrades gracefully.
Peer grading can only work in practice, if graders are suf-ficiently incentivised to report an accurate assessment. This can be achieved by giving a grade also for the quality of the grading. In the following, we investigate whether the grader reliability estimators proposed in Section 3.2 can identify graders that are not diligent.

For both the posters and the projects, we add 10  X  X azy X  peer graders that report random grades drawn from a nor-mal distribution whose mean and variance matches that of the rest of the graders 2 . For the ordinal methods, this results in a random ordering. We then apply the peer grading meth-ods, estimating the respective reliability parameters  X  g each grader using 10 iterations of the alternating optimiza-tion algorithm. We then rank graders by their estimated  X  .

Figure 5 (top) shows the percentage of lazy graders that rank among the 20 graders with the lowest  X  g . The error bars show standard error over 50 repeated runs with different lazy graders sampled. Most ordinal methods significantly
Otherwise it would be easy to identify these graders. Figure 5: Percentage of times a grader who ran-domly scores and orders assignments is among the 20 least reliable graders ( i.e., bottom 12.5%). outperform the cardinal NCS method for both the posters and the reports. The variants of Mallow X  X  model perform very well, identifying around 70-80% of the lazy graders for the reports and all 10 lazy graders for the posters. The better performance for the posters than for the reports was to be expected, since students provide 7 instead of 4 grades.
Figure 5 (bottom) shows the results of a heuristic base-line. Here, grade estimation without reliability estimation is performed, and then graders are ranked by their E K with the estimated ranking  X   X  . For almost all methods, this per-forms worse, clearly indicating that reliability estimation is superior in identifying lazy graders. We find similar results even when there are 100+ lazy graders, and we investigate robustness in the following section. Figure 6: Change in E K (using Instructor and Meta Grades as target ranking) for (Left) Posters and (Right) Final Reports with the addition of an in-creasing number of lazy graders i.e., E K ( With Lazy )  X  E ( Without Lazy ) . A negative value indicates that performance improves on adding this noise.
 Table 4: Average runtime (with and without grader reliability estimation) and their standard deviation of different methods in CPU seconds.
While Section 4.6 showed that reliability estimation in or-dinal peer grading is well-suited for identifying lazy graders, we would also like to know what effect these lazy graders have on grade estimation performance. We study the ro-bustness of the peer grading methods by adding an increas-ing number of lazy graders. Figure 6 shows the change in E
K (w.r.t. Instructor/Meta grades) after adding 10/50/100 lazy graders (compared to the E K with no lazy graders). We find that in most cases performance does not change much relative to the variability of the methods. Interest-ingly, in some cases performance also improves on adding this noise. A deeper inspection reveals that noise is most beneficial for methods whose original E K performance was weaker than that of the other methods. For example, the Thurstone model showed the weakest performance on the Reports and improves the most.
While prediction accuracy is the prime concern of grade inference, computational efficiency needs to be sufficient as well. Table 4 show the average runtimes and their standard deviations for the posters and the reports. All methods are tractable and most finish within seconds. The Score-Weighted Mallows model is less efficient for problems where a each grader assesses many assignments, since the gradi-ent computations involves computing the normalization con-stant (which involves summing over all rankings). However, training scales linearly with the number of graders. Another method that requires more time is the Thurstone model. Table 5: Response categories for survey questions. The main bottleneck here is the computation of the gradi-ent as it involves a lookup of a CDF value from the normal distribution table.
A final point that we would like to explore is that peer grading is not only about grade estimation, but also about generating useful feedback. In particular, the cardinal or ordinal assessments were only a small part of the peer feed-back. Peer graders had to write a justification for their as-sessment and comment on the work more generally.

To assess this aspect of peer grading, a survey was con-ducted at the end of class as part of the course feedback pro-cess. This survey included two questions about the student X  X  peer grading experience in the class; more specifically, about how helpful the feedback they received was, and how valu-able the experience of providing feedback was to them. Both questions were to be answered in free-form text. Of the 161 students that participated in the project, 120 students re-sponded to at least one of the questions, with 119 answering the question about receiving feedback (mean response length in characters: 62.93; stdev: 77.22) and 118 the question about providing feedback (mean: 100.36; stdev: 105.74). Following standard practice from survey analysis, we cre-ated five categories for coding these open-ended responses as show in Table 5. While the first four categories (roughly) follow a decreasing scale of approval, the last serves as a catch-all (including missing responses).

All free-text responses were manually assigned to these categories by four external annotators (who were not in-volved with the class and had not seen the comments before). For all the 237 student comments ( i.e., responses), the an-notators were asked to choose the category that was most appropriate/best describes the comment . To check inter-annotator agreement we used the Fleiss Kappa measure.  X  values of 0 . 8389 and 0 . 6493 for the two questions indicate high annotator agreement. The final assignment of response to category was done by majority vote among the four an-notators (score of 0.5 each if tied between categories).
Table 6 summarizes the results of the survey after cod-ing. Overall, around 68% found it at least somewhat helpful to receive peer feedback, and around 74% found substan-tial value in providing the peer feedback. Interestingly, of the 26% of the students who expressed that receiving peer feedback was not (really) helpful to them, 17% still found it valuable to provide peer feedback. Overall, we conclude that the vast majority of students found some value in the peer grading process.
In this work we study the problem of student evaluation at scale via peer grading using ordinal feedback. We cast this as a rank aggregation problem and study different probabilistic Table 6: Results of the student survey, coded ac-cording to the categories in Table 5. models for obtaining student grades, as well as estimating the reliability of the peer graders. Using data collected from a real course, we find that the performance of ordinal peer grading methods is at least competitive with cardinal meth-ods for grade estimation, even though they require strictly less information from the graders. For grader reliability es-timation, Mallow X  X  model outperforms all other methods, and it shows consistently good and robust performance for grade estimation as well. In general, we find that ordinal peer grading is robust and scalable, offering a grading accu-racy that is comparable to TA grading in our course.
This research was funded in part by NSF Awards IIS-1217686 and IIS-1247696, and the JTCII Cornell-Technion Research Fund.

