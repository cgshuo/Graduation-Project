 ORIGINAL PAPER V. Rabeux  X  N. Journet  X  A. Vialard  X  J. P. Domenger Abstract This article proposes an approach to predict the result of binarization algorithms on a given document image according to its state of degradation. Indeed, historical docu-ments suffer from different types of degradation which result in binarization errors. We intend to characterize the degrada-tion of a document image by using different features based on the intensity, quantity and location of the degradation. These features allow us to build prediction models of bina-rization algorithms that are very accurate according to R values and p values. The prediction models are used to select the best binarization algorithm for a given document image. Obviously, this image-by-image strategy improves the bina-rization of the entire dataset.
 Keywords Document image analysis  X  Quality evaluation  X  Binarization prediction 1 Introduction This paper involves quality evaluations of document images. Document quality evaluation is needed at every stage of the digitization workflow, for instance, in the scanning stage to ensure that the scanner X  X  settings are optimal, in the process-ing stage to apply the best algorithms (for example, restora-tion, binarization, OCR) and in the visualization stage to pro-vide the best image quality.

To improve the results of the processing stage, it is neces-sary to take into account specific image defects. Document imagesmaysufferfromseveraltypesofdegradation.Accord-ingto[ 1 ], degradation can have different origins:  X  wrong scanner settings: non-uniform illumination, focus,  X  the document itself: non-flat paper surface, spots, bleed- X  preprocessing algorithms, such as those involving high
As ancient documents often present significant degrada-tion,wefocusthispaperontheirqualityevaluation.However, the global methodology is suited for any damaged document images.

Essentially, most document analysis systems are created by sequentially applying algorithms (preprocessing, bina-rization, layout analysis, OCR, indexing). These chains of algorithms are ad hoc workflows, built for a specific set of images. In such a workflow, the result of one algorithm can affect the result of all of the following ones. For example, Fig. 1 illustrates the effect of a binarization algorithm on the result of a layout analysis. Thus, choosing the best algorithm available for one specific image is very important at each step of a workflow. This choice needs to be automated to avoid tremendous manual work.

Currently, most proposals to automate the selection process of a document image workflow are based on perfor-mance evaluation. For example, the authors of [ 3 ] propose a software architecture for comparing algorithms and evaluat-ing their performance on a complete workflow. Obtaining the best workflow implies an evaluation of a significant amount of algorithm combinations on a representative dataset. In [ 4 ], the authors propose an original method to improve the over-all OCR process by combining several global thresholding binarization results.

To automate the selection process, confidence rates can also be used. A confidence rate provides an estimate of how well the algorithm performed on an image. Confidence rates are well known in OCR systems; unfortunately, most other processing algorithms do not associate confidence rates with their results. Moreover, even if available, confidence rates are not always relevant (see Fig. 2 ).

We propose another approach, based on algorithm predic-tion models, to select the best algorithm for a specific task. Our approach is based on the following fact: the global qual-ity of a document image directly affects the result of any processing algorithm (binarization, segmentation). Thus, we aim to predict the result of an algorithm according to the degradation type and quantity of the processed document. In this paper, we focus on binarization algorithm prediction.
For a given binarization algorithm and a set of images with their binarization ground truth, the significant correla-tion between algorithm performance and the quality of the images allows us to build a prediction function. The docu-ment image quality is characterized by new, dedicated fea-tures. This prediction function can forecast the binarization algorithm result for any new image on which quality features have been previously computed.

In the following sections, we first present the state of the art for image quality evaluation and for algorithm prediction in the context of document image analysis (Sect. 2 ). We then introduce different features that characterize ancient docu-ment degradation. These features rely on a decomposition of the document gray levels in three different classes: ink pixels, degradation pixels and background pixels (Sect. 3 ). We characterize the degradation layer by analyzing the dis-tribution of its intensities, its quantity and its location within the image. The proposed features, dedicated to binarization evaluation, are presented in Sect. 4 . Section 5 details the methodology used for creating algorithm prediction models. Prediction models of several binarization methods are then presented, all of which present very high accuracy. Finally, Sect. 6 explains how to use the prediction models to select the best binarization algorithm for a specific image. 2 Related works The first part of our work is to identify the degradation within document images. The related works are ancient document image enhancement methods with a first step that often con-sists of identifying and localizing specific degradations pix-els.

Among the methods focusing on pixel degradation iden-tification, the authors of [ 5 ] propose a directional wavelet transform to identify bleed-through pixels. The authors of [ 6 ] also localize document pixels suffering from bleed-through by a recto X  X erso registration: a parameter optimiza-tion method aims to find the appropriate transformation matrix that minimizes the difference between gray recto pix-elsandinkpixelsfromtheverso.Therectopixelscorrespond-ing to the verso ones can then be labeled as bleed-through pixels. The problem addressed in [ 7 ] is the localization of pixels that suffer from illumination defects. This problem occurs when scanning documents with large bookbindings. The authors propose a line-by-line thresholding to localize the boundary of the dark area near the bookbinding.
The pixel identification methods previously mentioned are dedicated to the restoration of one specific defect (for exam-ple,bleed-through,illumination).However,typically,ancient documents suffer from a combination of defects. For exam-ple, the recto X  X erso registration to localize bleed-through pixels may fail with a document suffering from geometri-cal distortions. A global approach has been chosen in recent restoration methods [ 8 , 9 ]. We also believe that a robust iden-tification of defect pixels has to be performed globally.
The second part of our work involves predicting the result of a binarization algorithm. To our knowledge, there are no studies on binarization prediction. The existing work on algo-rithm prediction for document image analysis is only found in the OCR field, which typically use the quality features of characters to create prediction models.

The first features related to character quality were intro-duced in [ 10 ]. In this article, the authors evaluate the qual-ity of binary text documents by analyzing black and white connected components. The OCR result is predicted by sim-ply thresholding the quality ratios (proportion of thick and broken characters). Each document image is finally labeled as good or poor. In [ 11 ], two new measures are introduced to account for speckles and connected characters. A linear regression is used to predict the OCR performance on hand-written black and white documents. The authors of [ 12 ] com-plete the set of features with new ones (Black Density Factor, Stroke Thickness Factor), which are used as inputs to a neural network to classify images into two classes (poor or good). By reusing a script identification engine, the method pro-posed in [ 13 ] can select the better of two OCRs according to a classification of the text image as broken , clean or merged . This classification is based on the computation of classical shape features of word images (compactness, Cartesian and centralized moments, etc.) and on a connected component per word distribution.

Other works propose strategies to select the best restora-tion algorithm. As in OCR prediction methods, dedicated defect features are computed on a binary image. These values are then used as inputs for different types of semi-supervised classification algorithms. The authors of [ 14 ] use the features of [ 12 ] with three new ones from [ 15 ] (Small Speckles Factor, Font Size Factor and Broken Characters Factor) to select a restoration algorithm. The restoration algorithm selection is based on decision rules using thresholds. Another automatic restoration method selection is presented in [ 15 ]. In this lat-ter article, the restoration algorithm selection is based on a linear classifier.

Previous methods suffer from two main drawbacks. First, most of them require a connected component extraction and, therefore,abinarizationstep.Thesemethodsstronglydepend on the accuracy of this preprocessing step. We believe that a better approach consists of directly analyzing the defect pixels in the initial grayscale image.

Second, none of the presented articles dealing with pre-diction models analyze the significance of each feature. Only the authors of [ 16 ] propose an interesting correlation analy-sis between several quality metrics and the parameters of the degradation model used to produce the test images. This pre-liminary study shows that, even if most features are highly correlated with the defects, some are not. However, this study does not address the essential issue of the selection of rel-evant quality features to avoid overfitting of an algorithm prediction. 3 Degradation layer extraction As in [ 17 ], we assume that an ancient document can be mod-eled as the combination of several information layers. Here, we consider three different layers: the text pixel layer, the background pixel layer and the degradation pixel layer. In ancient documents, most of the degradation (for example, bleed-through, spots, speckles, non-uniform illumination, ink loss) appears as connected components with grayscale values that differ from background and ink pixels. Figure 3 a shows several types of degradation in which the pixel gray intensities vary from low (ink spots) to high values (light bleed-through). We do not measure each type of degradation separately. On the contrary, we globally measure and char-acterize the document degradation by distinguishing three different layers of pixels according to the pixels X  gray level. Let us denote the gray level of pixel p by g ( p ) .Let I set of ink pixels, D be the set of degradation pixels and the set of background pixels defined as follows: 1. I ={ p , g ( p )  X  s 0 } ink layer 2. D ={ p , s 0 &lt; g ( p )&lt; s 1 } degradation layer 3. B ={ p , g ( p )  X  s 1 } background layer
Setting the two thresholds s 0 and s 1 can be determined using any classification algorithm. Our experiments used a 3-means clustering algorithm. Figure 3 shows that most degradation present in a document image can be extracted using these two thresholds. A few gray pixels (for example, from the background or inside characters) are misclassified. Obviously, it is not possible to perfectly classify these pixels using only the gray-level histogram. 4 Quality features definition This section details new features used to characterize docu-ment image degradation. All features are based on an analysis of the three layers previously extracted. A first set of global features is extracted directly from the three grayscale his-tograms without spatial consideration. A second set of spatial features is dedicated to the characterization of the localiza-tion of the degradation surrounding ink components. 4.1 Global features The global grayscale histogram contains information charac-terizing document quality. Figure 4 and Table 2 illustrate the differences between the histograms of a clean and a severely degraded document image.

We aim to compute the following global statistic features of the grayscale histogram: mean, variance and skewness. The skewness quantifies the asymmetry of the histogram. For example, a negative skewness indicates that the distribution of pixels gray levels has relatively few low values. We denote the mean of the global histogram by  X  , its variance by v its skewness by s . A good value for the skewness is a high negative value: the left tail of the histogram is longer, the intensities are concentrated on the right, and the histogram has relatively few gray values. In that case, the image is likely easily binarized (see the images in Fig. 4 a and Table 2 line 2) The mean, variance and skewness are also computed on the three sub-histograms to characterize each layer distribution (ink, background and degradation).

This step provides 12 features:  X   X , v, s (global histogram)  X   X  I ,v I , s I (ink histogram)  X   X  D ,v D , s D (degradation histogram)  X   X  B ,v B , s B (background histogram).

Thepreviousglobalfeaturescharacterizingthehistograms cannot precisely represent the relationship between the ink layer, the degradation layer and the background layer. There-fore, we introduce two last global features extracted from the grayscale histogram to characterize the distance between the three layers. We assume that the mean intensity difference between the layers is directly correlated to the binarization result. For example, if the mean intensity value of the degra-dation layer is close to the ink intensity value, degradation pixels can be misclassified as ink pixels.

We define two features, MI I and MI B , where MI I corresponds to the distance between the average intensity of degradation pixels and the average intensity of ink pixels, and MI B is the distance between the average intensity of degradation pixels and the average intensity of background pixels MI
The gray values of the three layers are not the only char-acteristics that could affect a binarization algorithm. The amount of degradation pixels is also directly correlated with the binarization performance. We aim to measure this perfor-mance as the relative quantity of ink and degradation pixels. We define MQ as the ratio: MQ =
This first family of features leads to the computation of a vector of dimension 15 for each document image. 4.2 Spatial features Binarization is a segmentation task meant to extract objects of interest (for example, characters, drawings). A good bina-rization should preserve the shape of the objects and avoid the creation of unwanted black or white components. Obviously, the location of the degradation pixels is a significant charac-teristic that can influence the binarization result. Figure 5 illustrates the main situations observed in real documents in which the degradation pixels spatially interfere with ink pixels. For example, the binarization results worsen if dark spots overlap characters (Fig. 5 b, c). In other words, an ink component may be even more deformed because it is con-nected with a degraded component. The following features are meant to capture the possible creation of unwanted black components and the possible deformation of the characters through the binarization process.

Let S be a set of pixels. We denote the set of the four connected components of S by CC ( S ) . In the rest of the section, we use the following notations: C I = CC ( I ), C CC ( D ) and C B = CC ( B ) .

Let c I  X  C I be an ink component and c D  X  C D be a degradation component. We denote the predicate returning true by SG ( c I , c D ) if c I and c D are connected: Wedistinguishthreedifferentcasesthatcanproducedifferent types of binarization errors: 1. If c I and c D are not connected (Fig. 5 a), the original 2. If c I and c D are connected (Fig. 5 b), the original char-3. MSG measures the possible extent of ink component
Given all of the previously defined features, each doc-ument image is characterized by a vector of dimension 18. The Table 1 shows the values of the three spatial deformation features on the examples in Fig. 5 .

The feature MA is equal to 1 in Fig. 5 a, b because one degradation component is connected to one ink component. The feature MSG equals to 0 in Fig. 5 a because no compo-nents are connected. The feature MSG has a lower value in Fig. 5 b than in Fig. 5 c because the union area size between the ink component and the degradation component is smaller. 4.3 Case study This section analyzes the 18 features computed on two dif-ferent document images containing several defects (Table 2 ). These two examples emphasize the link between the 18 fea-tures and the f-score obtained after having binarized the images with Otsu X  X  and Sauvola X  X  methods (a global vs. a local thresholding method). There are multiple ways to mea-sure binarization accuracy. In this paper, we used the f-score.
The first document image (Table 2 , line 1) is damaged by a large spot that overlaps text lines. The gray levels of the spot are close to the gray level of the text pixels. Because the Otsu X  X  method is based on a global threshold, the spot pix-els tend to be misclassified as ink. On the contrary, the local method is more likely to achieve a correct separation of ink and background on the defective area, which explains why the respective f-scores of the Otsu X  X  and Sauvola X  X  meth-ods are 0.4 and 0.7 on this image. The second document image (Table 2 , line 2) presents a non-even background with speckles. Moreover, the ink color is light relative to the back-ground color. On this image, the respective f-scores of Otsu and Sauvola are 0.8 and 0.4. The Sauvola X  X  method is not robust to the background speckles, which are classified as ink. The faded ink defect is a drawback for a global method and lowers the performance of Otsu X  X  method.

Table 2 shows that specific defects that reduce binarization performance are captured by the proposed features. Even if the global features based on histogram analysis are mean-ingful, they are not sufficient in that case to choose the best binarization method. The ink pixels X  mean value  X  I of the first image is lower than that of the second one, indicating that the ink layer seems easier to identify using a global threshold-ing method. However, the skewness of the ink s I is negative, indicating that most pixels are concentrated on the right part of the distribution: there are more gray pixels than really dark pixels. The skewness of the second global histogram s is much higher than that of the first image, indicating that the background of the second image is easy to separate using a global thresholding method. This separation is confirmed by the global variance v . Without additional information, the global thresholding method seems adapted to the second image, but we cannot draw a similar conclusion for the first image.

In the first image, the values of MI I and MI B are low, indicating that a global thresholding method is likely to fail to correctly classify the pixels. The value of MSG is also high, indicating that there are large spots around the characters. Generally, window-based methods have better results on this type of document.

On the second image, the values of MI I and MI B are even lower: Otsu X  X  method will also yield a bad result for the second image, but other features such as s or the relatively low value of v indicate that failure may be relative. Moreover, the value of MA is high, meaning that many components do not touch text pixels. This type of degradation is likely to produce binarization errors with window-based methods such as Sauvola X  X  method.

According to the computed features, it is preferable to use Sauvola X  X  method for the first image and Otsu X  X  for the second. Doing so is consistent with the f-scores of the two binarization methods.

The proposed features characterize three different aspects of degradation: intensity, quantity and location. The next sec-tion details a methodology that uses these features to predict the result of a binarization algorithm, which is applied to the prediction of 12 binarization algorithms on the DIBCO dataset. 5 Predicting binarization method accuracy This section presents a unified methodology that is able to predict the result of most binarization methods (for example, adaptive thresholding, clustering, entropic, document ded-icated). Our methodology is evaluated on 12 binarization methods used in document analysis. The methods are ref-erenced in the text by their author X  X  names. 1. Bernsen [ 2 ] is a local adaptive thresholding technique. 2. Kapur et al. [ 18 ] is an entropy-based thresholding 3. Kittler and Illingworth [ 19 ] is a clustering-based thresh-4. Li and Tam [ 20 ] is a cross-entropic thresholding method 5. Niblack [ 21 ] is a locally adaptive thresholding method 6. Ridler and Calvard [ 22 ] is an iterative thresholding 7. Sahoo et al. [ 23 ] is an entropy-based thresholding 8. Shanbag [ 24 ] is a fuzzy entropic thresholding technique 9. Sauvola and Pietik X inen [ 25 ] is a locally adaptive thresh-10. Otsu [ 26 ] is a two-class global thresholding method. 11. White and Rohrer [ 27 ] is a locally adaptive thresholding 12. Lu et al. [ 28 ] is a recent method based on an ad hoc
Some binarization methods rely on parameters. In this article, we do not focus on parameter optimization. There-fore, we chose to use the parameters given by the authors of each method in their corresponding original articles. Table 3 summarizes the values of these parameters. Importantly, note that the prediction models created are only able to predict the performance of a binarization method with a specific set of parameters. However, a binarization method can have several prediction models, one for each set of parameters. To illus-trate the difference between two sets of parameters, we will create two different prediction models for Sauvola X  X  method. The second set of parameters was manually chosen (Table 3 ).
In order to assess the accuracy of a binarization method, several measures can be used. Most of them are presented in [ 29 , 30 ]. The authors of [ 31 ] propose an interesting study of these measures regarding the human perception of image quality. Their main conclusion is that the human perception is consistent with the classical measures for the ranking of bests and worsts of binarization methods. In our experiments, we choose to use the f-score measure.

To predict the accuracy of the binarization method, we follow a methodology based on a step-wise linear regres-sion. Section 5.1 presents the dataset we used to train and validate our prediction models. This predictive methodology can be applied to all types of binarization methods and is presented in a general way in Sect. 5.2 . We then detail the prediction models corresponding to three popular binariza-tion methods for document images: Otsu X  X , Sauvola X  X  and Shijian Lu X  X  (Sect. 5.3 ). The prediction model accuracy for the other methods is presented in Sect. 5.4 to highlight the generality of the presented methodology. 5.1 The dataset To create a precise and usable prediction model, we need a dataset of images with their binarization ground truth. This dataset needs to be heterogeneous. In our case, a well-distributed dataset should contain images with various levels of defects leading to various f-scores for the different bina-rization methods.
 We use a dataset obtained by merging the DIBCO 1 and H-DIBCO 2 datasets [ 29 , 30 , 32 ]. These datasets are primarily used as data for binarization contests and contain a heteroge-neous set of images from difficult to easy to binarize. Table 4 summarizes some statistical results of the f-score measures for 12 binarization algorithms applied to all DIBCO images (36 images).

The DIBCO datasets are currently the reference used for binarization contests and in other scientific articles dealing with binarization problems. These images were selected by the DIBCO team on the fact that they have different charac-teristics and degradation amounts inducing different effects on binarization methods. The DIBCO datasets are used for performance evaluation [ 33 , 34 ] or for learning and training steps [ 35 ]. 5.2 Using step-wise multivariate linear regression to predict To estimate the f-score of a binarization algorithm, we auto-matically build a prediction model based on the most signif-icant features among the 18. More precisely, the prediction modeliscomputedusingmultivariatestep-wiselinearregres-sion [ 36  X  39 ], followed by repeated random sub-sampling validation (cross-validation).

The linear regression models, as an hyperplane, the rela-tionship between the features and the ground-truth f-scores. This result can then be used to predict a f-score according to a set of computed features. The prediction can be improved by using only a pertinent subset of features among the 18 independent computed features. There are three main ways to carry out a selection. First, the forward strategy consists in computing a criteria (linked to the R 2 value) by adding one feature at a time. On the contrary, a second approach (backward strategy) consists in starting with all the features and deleting them one at a time. After each deletion, the cri-terion is computed. The last strategy consists in testing all the possible combinations. As we have only 18 features, we decided to use the exhaustive strategy.

This overall process which is presented in Fig. 6 can be divided into five steps: 3 1. Feature computation: The 18 proposed features are 2. F-score computation: We run the binarization algorithm 3. Generation of the predictive model : This step con-4. Evaluation of model accuracy :The R 2 value 5. Model validation : Because of the relatively few images 5.3 Prediction models of commonly used binarization The prediction model for Otsu X  X , Sauvola X  X  and Shijian Lu X  X  binarization algorithms was generated with the methodology described in Sect. 5.2 . The coefficients associated with the most significant selected features, their p values and the inter-cept of the linear predictive function are detailed in Tables 5 , 6 and 7 . If a feature is not present in a table, then it was not selected by the step-wise algorithm. As mentioned in the previous section, the cross-validation for each model gives the pair (  X   X ,
Otsu X  X  binarization method The most significant selected features for Otsu X  X  prediction model are MI I ,v I ,v B , X   X  and v (see Table 5 for the coefficients of the predictive function). For Otsu X  X  prediction model, we can explain the feature selection by the fact that Otsu X  X  binarization method is based on global thresholding. That is why features such as MI The model X  X  R 2 equals 0.93, which is considered very good [ 39 ].

The cross-validation gives a  X   X  coefficient of 0.989 and  X  R 2 of 0.987. These results indicate that our model does not depend on the chosen training data.

Sauvola X  X  binarization method ForSauvola X  X binarization method, we created two different models (Table 6 ). The first one corresponds to the set of parameters proposed by the authors in their original article, and the second one corre-sponds to the set of parameters that were manually chosen (see Table 3 ).

For the first model (original parameters), the selected fea-tures for Sauvola are as follows: MI B , MQ , MA , X , s , s ,v The resulting prediction model also seems accurate with an R 2 value of 0.8372. Note that MQ and MA are selected for this binarization method. Indeed, window-based meth-ods are sensitive to noise components near ink components. The cross-validation gives a mean slope coefficient  X   X  of 0.85 and an Theresultsaresimilarusingthesecondmodelthatpredicts Sauvola X  X  binarization method f-scores with our manually chosen parameters. However, the feature MSG is introduced in this model, which can be explained by the fact that we changed the window size parameter (51 pixels instead of 15). Indeed, using this window size, the results of Sauvola are sensitive to large gray components surrounding characters. The cross-validation step gives a slope coefficient  X   X  equal to 1.114 and an R 2 of 0.94.

These results allow us to conclude that these models are accurate and can be used in practice.

Shijian Lu X  X  binarization method The selected features and their estimated coefficients for Shijian Lu X  X  predic-tion model are presented in Table 7 . The step-wise linear regression selects two spatial deformation features, MA and MSG , and a global feature, MI ing because this method is a combination of several global and window-based techniques. The prediction model is also very accurate (0 . 86). The cross-validation gives a and a mean slope  X   X  of 1.06. 5.4 Accuracy of other prediction models The same experiment was conducted on the other binariza-tion methods. Table 8 sums up the selected features and the significant information to validate or not a binarization pre-diction model.
Among the 18 features, most models embed about 7 fea-tures. Globally, the selected features are consistent with the binarization algorithm: the step-wise selection process tends to keep global (respectively local) features for global (respec-tively local) binarization algorithms. We also note that MS never selected by any prediction model. Indeed, the binariza-tion accuracy is measured at the pixel level (f-score). With this accuracy measure, the feature MSG becomes more sig-nificant than MS , which may not have been the case with another evaluation measure.
 The R 2 values show the quality of each prediction model. The prediction models of Sahoo and Niblack binarization methods were not kept for the statistical validation step since the R 2 values were below 0.7. For these two binarization models, new features have to be created in order to obtain more accurate prediction models.

The two values prediction model on the validation step. A than 0 . 7 indicates that it is possible to predict the results of a binarization method [ 39 ]. As a result, 12 binarization methods can be well predicted. The mean percentage error ( mpe ) is the average difference between predicted f-scores and real f-scores. This value is around 5%. 6 Automatic and optimal selection of binarization methods The methodology previously explained allows the creation of an accurate prediction model for any binarization method. Given a document image, a binarization method and its pre-diction model, we can compute all of the features required by the model and use them as inputs. The result is the predicted accuracy of this specific binarization method for this specific image. Given several binarization prediction models, we can create a binarization process that uses these prediction mod-els to select the optimal binarization method for each image of a dataset.

For instance, Shijian Lu X  X  method is the binarization method, which gives the best results on average. However, in some borderline cases, Shijian Lu X  X  significantly fails, while other methods perform better. This is illustrated in Fig. 7 where the bleed-through defect disrupts methods that use a local analysis of the image.

More generally, Table 9 presents some f-score statis-tics obtained from binarizing the DIBCO dataset. The first line corresponds to the best theoretical f-scores (having the ground truth, we know for each image the binarization method will provide the best f-score). The second line cor-responds to the f-scores obtained using only Shijian Lu X  X  method. The last line corresponds to the f-scores obtained using our automatic binarization selection.

We analyze the accuracy of our binarization method selec-tion algorithms in several ways. As expected, the method has only a slightly better (2%) mean accuracy than using only Shijian Lu X  X  method. What is significant is that the standard deviation lowers from 0.12 to 0.04. It means that the worst binarization result of our method is much higher than Shijian Lu X  X  (56%). We also compared our method with the optimal selection that we can compute from the ground truth. The results are very similar, indicating that the prediction models are accurate enough to select the best binarization method for each image (70% perfect match). The mean error of our method is 0.009 (standard deviation equals 0.02), and the worst error equals 0.06.

These results are very encouraging and show that this naive selection technique can be used to improve the bina-rization of a document. Moreover, the selection errors can be minimized by promoting models with the highest R 2 and with the lowest possible p values. In other words, a model with a high confidence rate (good R 2 and p values) may be selected even if its predicted f-score is not the highest one. 7 Conclusion and research perspectives This paper presented 18 features that characterize the quality of a document image. These features are used in step-wise multivariate linear regression to create prediction models for 12 binarization methods. Repeated random sub-sampling cross-validation shows that the models are accurate (max per-centage error equals 11%). Moreover, given the step-wise approachofthelinearregression,thesemodelsarenotoverfit. As a result, 10 models out of 12 are validated and show suf-ficient accuracy to be used in an automated selection method of the optimal binarization method for each image.
One of our future research goals is to apply the same methodology to predict OCR error rates. However, OCRs today are very complex engines that are able to restore doc-uments and perform layout analysis. Therefore, OCR failure cases are not only the result of a document X  X  quality but also of its complexity (font, tables, figures, mathematical formu-las). This complexity has to be evaluated with new OCR dedicated features.

Our second research goal is to improve the binarization algorithm selection method. We believe that the method can be tuned by studying different strategies. One notion is to take into account R 2 and p values measures in the automatic selec-tion of a method. Another idea is to weight predicted f-scores withcomputationalcosts:withsimilaraccuracy,choosingthe quickest one may be preferable.

At last, two prediction models are rejected due to their lack of accuracy. New dedicated features have to be created and used in the presented methodology to circumvent this issue.
 References
