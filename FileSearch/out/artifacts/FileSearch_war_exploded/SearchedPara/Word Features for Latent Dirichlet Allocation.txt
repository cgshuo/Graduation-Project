 Latent Dirichlet Allocation [4] assigns topics to documents and generates topic distributions over between words. Nonetheless, it achieves a surprisingly high quality of coherence within topics. The inability to deal with word features makes LDA fall short on several aspects. The most obvious one is perhaps that the topics estimated for infrequently occurring words are usually unreliable.  X  X ermany X  and  X  X erman X , or  X  X olitics X ,  X  X olitician X , and  X  X olitical X  should, by default, belong to cohesion across languages, a problem that has been researched but is far from being fully solved, especially for non-aligned corpora [6]. For example, we know that  X  X emocracy X  and  X  X emocracia X  therefore should have aligned topics) reduces the statistical strength of a model. or distributional word similarity features.
 of a collapsed Gibbs sampler. Instead, we use a hybrid approach where we perform smooth opti-misation over the word smoothing coefficients, while retaining a collapsed Gibbs sampler to assign modular and can be added to existing Gibbs samplers without modification.
 We present experimental results on multi-language topic synchronisation which clearly evidence the standard LDA, which is unable to leverage this type of information. Figure 1: LDA: The topic distribution for each word (  X  v ) has as smoother the Dirich-let distribution with a parameter  X  (indepen-dent of the word). 1.1 Related work Loosely related works that use logistic models to induce structure in generative models are [17], weights, and [10], which incorporated features into unsupervised models using locally normalized models. More related to our work is [5], which encodes correlations between synonyms, and [1] of [1], where we can encode the strength of the links between each pair of words. fraction (at least 25%) of the documents are paired up.
 a list of matched word pairs m (where each pair has one word in each language) and corresponding matching priors  X  that encode the prior knowledge on how likely the match is to occur. The topics are defined as distributions over word pairs, while the unmatched words come from a unigram than two languages their experimental section was focused on the bilingual case. One of the key differences between [6] and our method is that we do not hardcode word informa-features. Furthermore, our model automatically extends to multiple languages without any modifi-can be incorporated as a module in existing LDA implementations. We begin by briefly reviewing the LDA model of [4] as captured in Figure 1. It assumes that Nonparametric extensions in terms of the number of topics can be obtained using Dirichlet process models [2] regarding the generation of topics. Our extension deals with the word smoother  X  . Instead of treating it as a constant for all words we attempt to infer its val-ues for different words and topics respectively. That is, we assume that (1c) is replaced by (which dealt with topical side information over documents). The corresponding graphical model is given in Figure 2. The above dependency allows us to incorporate features of words as side great affinity to  X  k, scandal and we might estimate y such that this is achieved. 2.1 Detailed Description We now discuss the directed graphical model from Figure 2 in detail. Whenever needed we use the that we only need to update  X  and  X  (or indirectly y ). We define the standard quantities n n document m , that is p ( z mn |  X  m )=  X  m,z denotes the smoother for topic k .
 Collapsed distribution p ( z m |  X  ) : Integrating out  X  m and using conjugacy yields where  X  is the gamma function:  X  ( x )= Word distribution p ( w mn | z mn ,  X  ) : We assume that given a topic z mn the word w mn is drawn from a multinomial distribution  X  w standard as per the basic LDA model.
 coordinates of  X  k are identical. Nor do we assume that all  X  k are the same.
 Collapsed distribution p ( w | z,  X  ) : Integrating out  X  k for all topics k yields the following 2.2 Priors e.g. the ( X  X oyota X ,  X  X ia X ) and the ( X  X ush X ,  X  X heney X ) tuples, rather than generally related words. For this purpose we design a similarity graph G ( V, E ) with words represented as vertices V and magnitude of  X  uv can denote the similarity between words u and v .
 In the following we denote by y kv the topic dependent smoothing coefficients for a given word v and topic k . We impose the smoother corrections. A similar model was used by [3] to capture temporal dependence between topic mod-instantiated at subsequent years. topic assignments z mn . 3.1 Document Likelihood The likelihood contains two terms: a word-dependent term which can be computed on the fly while the uncollapsed likelihood in terms of z,  X  ,  X  ,  X  ,  X  . We have Define  X   X  := "  X  " p ( w, z |  X  ,  X  )= The above product is obtained simply by canceling out terms in denominator and numerator where dense count table. 3.2 Collapsed Sampler In order to perform inference we need two components: a sampler which is able to draw from p ( their values after the word w mn and associated topic z mn have been removed from the statistics. Standard calculations yield the following topic probability for resampling: In the appendix we detail how to addapt the sampler of [19] to obtain faster sampling. 3.3 Topic Smoother for  X  the negative log-likelihood is 2.2, which smooths between closely related words only. After choosing edges  X  uv according to y function. We have The gradient with respect to y k is analogous. To demonstrate the usefulness of our model we applied it to a multi-lingual document collection, where we can show a substantial improvement over the standard LDA model on the coordination between topics of different languages. 4.1 Dataset Since our goal is to compare topic distributions on different languages we used a parallel corpus [11] with the proceedings of the European Parliament in 11 languages. We focused on two language pairs: English/French and English/Portuguese.
 We treated the transcript of each speaker in each session as a document, since different speakers usually talk about different topics. We randomly sampled 1000 documents from each language, preprocessing we were left with 2415 documents, 805 in each language, and a vocabulary size of 23883 words. 4.2 Baselines We compared our model to standard LDA, learning  X  and  X  , both asymmetric 6 . 4.3 Prior We imposed the graph based prior mentioned in Section 2.2. To build our similarity graph we used the English-French and English-Portuguese dictionaries from http://wiki.webz.cz/dict/ , augmented with translations from Google Translate for the most frequent words in our dataset. As the dictionary.
 In our model  X  = exp( y kv + y v ) , so we want to keep both y kv and y v reasonably low to avoid LDA model, where to learn an asymmetric beta we simply removed y kv to get  X  = exp( y v ) . 4.4 Methodology In our experiments we used all the English documents and a subset of the French and Portuguese the number of pages is English is far greater than in any other language.
 We compared three approaches. First, we run the standard LDA model with all documents mixed together  X  this is one of our baselines, which we call STD1.
 iterations of the Gibbs sampler we include only English documents; in the second half we add the Finally, as a control experiment we run the standard LDA model in this same setting: first English documents, then all languages mixed. We call this STD2.
 We repeat every experiment 5 times with different randomisations. 4.5 Evaluation Evaluation of topic models is an open problem  X  recent work [7] suggests that popular measures based on held-out likelihood, such as perplexity, do not capture whether topics are coherent or not. Furthermore, we need a set of measures that can assess whether or not we improved over the standard LDA model w.r.t. our goal  X  to synchronize topics across different languages  X  and there X  X  no reason to believe that likelihood measures would assess that: a model where topics are Therefore, to evaluate our model we compare the topic distributions of each English document with its corresponding French pair (and analogously for the other combinations: English/Portuguese and French/Portuguese), with these metrics: Mean ) 2 distance: 4.6 Results In Figure 3 we compare our method (DC) to the standard LDA model (STD1 and STD2, see section ment over the standard LDA model.
 In Figures 4 and 5 we do the same for the English-Portuguese and Portuguese-French pairs, re-spectively, with similar results. Note that we did not use a Portuguese-French dictionary in any experiment.
 In Figure 6 we plot the word smoothing prior for the English word democracy and its French and our model (DC), with 20% of the French and Portuguese documents used in training. In STD1 we don X  X  have topic-specific priors (hence the horizontal line) and the word democracy has a much only 20% of the French and Portuguese ones). In DC, however, the priors are topic-specific and quite similar, as this is enforced by the similarity graph. To emphasize that we do not need a parallel corpus we ran a second experiment where we selected the same number of documents of each language, but assuring that for each document its corre-amongst the different languages is clear.
Figure 3: Comparison of topic distributions in English and French documents. See text for details. Figure 6: Word smoothing prior for two words in the standard LDA and in our model. The x-axis is the index to the topic. See text for details. Although we have implemented a specific type of feature encoding for the words, our model admits a large range of applications through a suitable choice of features. In the following we discuss a number of them in greater detail. when ambiguous (e.g., information is a word in both French and English). See text for details. 5.1 Single Language [16]. Hence, one could build a graph as outlined in Section 2.2 with edges only between words which exceed a level of proximity.
 Lexical Similarity: For interpolation between words one could use a distribution over substrings makes the sampler less sensitive to issues such as stemming: after all, two words which reduce to similar topic assignments.
 Synonyms and Thesauri: Given a list of synonyms it is reasonable to assume that they belong to related topics. This can be achieved by adding edges between a word and all of its synonyms. multiple meanings of a word will not prove fatal. 5.2 Multiple Languages allow one to synchronise topics even in the absence of dictionaries. However, it is important that different meanings in English and German). which among other things allows us to synchronise topics across different languages. We performed a number of experiments in the multiple-language setting, in which the goal was to show that our ferent languages. Our experimental results reveal substantial improvement over the LDA model in the quality of topic alignment, as measured by several metrics, and in particular we obtain much improved topic alignment even across languages for which a dictionary is not used (as described in tactical information such as parse trees. For instance, noun / verb disambiguation or named entity they will also aid in obtaining an improved topical mixture model.
 NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program. [1] David Andrzejewski, Xiaojin Zhu, and Mark Craven. Incorporating domain knowledge into [2] C. Antoniak. Mixtures of Dirichlet processes with applications to Bayesian nonparametric [3] David M. Blei and John D. Lafferty. Dynamic topic models. In W. W. Cohen and A. Moore, [4] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent Dirichlet allocation. Journal of [5] Jordan Boyd-Graber, David Blei, and Xiaojin Zhu. A Topic Model for Word Sense Disam-[6] Jordan Boyd-Graber and David M. Blei. Multilingual topic models for unaligned text. In [7] Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish, Chong Wang, and David Blei. Reading [8] Thomas L. Griffiths and Mark Steyvers. Finding scientific topics. Proceedings of the National [9] Woosung Kim and Sanjeev Khudanpur. Lexical triggers and latent semantic analysis for [11] Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In Machine [12] Dong C. Liu and Jorge Nocedal. On the limited memory BFGS method for large scale opti-[13] David Mimno, Hanna M. Wallach, Jason Naradowsky, David A. Smith, and Andrew McCal-[14] David M. Mimno and Andrew McCallum. Topic models conditioned on arbitrary features [15] Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen. Mining multilingual topics from [16] Patrick Pantel and Dekang Lin. Discovering word senses from text. In David Hand, Daniel [17] Noah A Smith and Shay B Cohen. The Shared Logistic Normal Distribution for Grammar In-[19] Limin Yao, David Mimno, and Andrew McCallum. Efficient methods for topic model infer-[20] Bing Zhao and Eric P. Xing. BiTAM: Bilingual Topic AdMixture Models for Word Alignment.
