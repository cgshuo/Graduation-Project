 With massive high-dimensional data now commonplace in research and industry, there is a strong and growing de-mand for more scalable computational techniques for data analysis and knowledge discovery. Key to turning these data into knowledge is the ability to learn statistical models with high interpretability. Current methods for learning statisti-cal models either produce models that are not interpretable or have prohibitive computational costs when applied to massive data. In this paper we address this need by pre-senting a scalable algorithm for partial least squares regres-sion (PLS), which we call compression-based PLS (cPLS), to learn predictive linear models with a high interpretability from massive high-dimensional data. We propose a novel grammar-compressed representation of data matrices that supports fast row and column access while the data matrix is in a compressed form. The original data matrix is grammar-compressed and then the linear model in PLS is learned on the compressed data matrix, which results in a significant reduction in working space, greatly improving scalability. We experimentally test cPLS on its ability to learn linear models for classification, regression and feature extraction with various massive high-dimensional data, and show that cPLS performs superiorly in terms of prediction accuracy, computational efficiency, and interpretability.
Massive data are now abundant throughout research and industry, in areas such as biology, chemistry, economics, dig-ital libraries and data management systems. In most of these fields, extracting meaningful knowledge from a vast amount of data is now the key challenge. For example, to remain competitive, e-commerce companies need to constantly ana-lyze huge data of user reviews and purchasing histories [24]. In biology, detection of functional interactions of compounds and proteins is an important part in genomic drug discov-ery [29, 7] and requires analysis of a huge number of chem-ical compounds [3] and proteins coded in fully sequenced genomes [4]. There is thus a strong and growing demand for developing new, more powerful methods to make better use of massive data and to discover meaningful knowledge on a large scale.

Learning statistical models from data is an attractive ap-proach for making use of massive high-dimensional data. However, due to high runtime and memory costs, learning of statistical models from massive data  X  especially models that have high interpretability  X  remains a challenge.
Partial least squares regression (PLS) is a linear statistical model with latent features behind high-dimensional data [26, 35, 36] that greedily finds the latent features by optimiz-ing the objective function under the orthogonal constraint. PLS is suitable for data mining, because extracted latent features in PLS provide a low-dimensional feature represen-tation of the original data, making it easier for practitioners to interpret the results. From a technical viewpoint, the optimization algorithm in PLS depends only on elementary matrix calculations of addition and multiplication. Thus, PLS is more attractive than other machine learning meth-ods that are based on computationally burdensome mathe-matical programming and complex optimization solvers. In fact, PLS is the most common chemoinformatics method in pharmaceutical research.

However, applying PLS to massive high-dimensional data is problematic. While the memory for the optimization algo-rithm in PLS depends only on the size of the corresponding data matrix, storing all high-dimensional feature vectors in the data matrix consumes a huge amount of memory, which limits large-scale applications of PLS in practice. One can use lossy compression (e.g., PCA [14, 9] and b -bit minwise hashing [12, 20]) to compactly represent data matrices and then learn linear models on the compact data matrices [21]. However, although these lossy compression-based methods effectively reduce memory usage [21, 30], their drawback is that they cannot extract informative features from the learned models, because the original data matrices cannot be recovered from the compressed ones.

Grammar compression [2, 27, 16] is a method of lossless compression (i.e., the original data can be completely re-covered from grammar-compressed data) that also has a wide variety of applications in string processing, such as pattern matching [37], edit-distance computation [13], and q -gram mining [1]. Grammar compression builds a small context-free grammar that generates only the input data and is very effective at compressing sequences that contain many repeats. In addition, the set of grammar rules has a convenient representation as a forest of small binary trees, which enables us to implement various string operations without decompression. To date, grammar compression has been applied only to string (or sequence) data; however, as we will see, there remains high potential for application to other data representations. A fingerprint (or bit vector) is a powerful representation of natural language texts [23], bio-molecules [32], and images [11]. Grammar compression is expected to be effective for compressing a set of finger-prints as well, because fingerprints belonging to the same class share many identical features.

Contribution . In this paper, we present a new scalable learning algorithm for PLS, which we call lossless compression-based PLS (cPLS) , to learn highly-interpretable predictive linear models from massive high-dimensional data. A key idea is to convert high-dimensional data with fingerprint rep-resentations into a set of sequences and then build grammar rules for representing the sequences in order to compactly store data matrices in memory. To achieve this, we propose a novel grammar-compressed representation of a data ma-trix capable of supporting row and column access while the data matrix is in a compressed format . The original data matrix is grammar-compressed, and then a linear model is learned on the compressed data matrix, which allows us to significantly reduce working space. cPLS has the following desirable properties: 1. Scalability : cPLS is applicable to massive high-dimensional 2. Prediction Accuracy : cPLS can achieve high predic-3. Usability : cPLS has only one hyper parameter, which 4. Interpretability : Unlike lossy compression-based meth-
We experimentally test cPLS on its ability to learn linear models for classification, regression and feature extraction with various massive high-dimensional data, and show that cPLS performs superiorly in terms of prediction accuracy, computational efficiency, and interpretability.
Several efficient algorithms have been proposed for learn-ing linear models on a large scale. We now briefly review the state of the art, which is also summarized in Table 1.
Principal component analysis (PCA) [14] is a widely used machine learning tool, and is a method of lossy compression, i.e., the original data cannot be recovered from compressed data. There have been many attempts to extend PCA [31, 28] and present a scalable PCA in distributed settings for an-alyzing big data [9]. For classification and regression tasks, a data matrix is compressed by PCA, and linear models are learned on the compressed data matrix by a supervised learning method (SL) , which is referred to as PCA-SL . De-spite these attempts, PCA and its variants do not look at the correlation structure between data and output variables (i.e., class labels/response variables), which results in not only the inability of feature extractions in PCA but also the inaccurate predictions by PCA-SL.

Li et al. [21] proposed a compact representation of finger-prints for learning linear models by applying b -bit minwise hashing (bMH) . A d -dimensional fingerprint is conceptually equivalent to the set s i  X  { 1 ,...,d } that contains element i if and only if the i -th bit in the fingerprint is 1. Li et al. X  X  method works as follows. We first pick h random permuta-tions  X  i , i = 1 ,..,h , each of which maps [1 ,d ] to [1 ,d ]. We then apply a random permutation  X  on a set s i , compute the minimum element as min(  X  ( s i )), and take as a hash value its lowest b bits. Repeating this process h times generates h hash values of b bits each. Expanding these h values into a (2 b  X  h )-dimensional fingerprint with exactly h 1 X  X  builds a compact representation of the original fingerprint. Linear models are learned on the compact fingerprints by SL, which is referred to as bMH-SL . Although bMH-SL is applicable to large-scale learning of linear models, bMH is a method of lossy compression and cannot extract features from linear models learned by SL. Other hashing-based ap-proaches have been proposed such as Count-Min sketch [5], Vowpal Wabbit [34], and Hash-SVM [25]. However, like bMH-SL, these algorithms cannot extract features, which is a serious problem in practical applications.

Stochastic gradient descent (SGD) [8, 33] is a computa-tionally efficient algorithm for learning linear models on a large-scale. SGD samples  X  feature vectors from an input dataset and computes the gradient vector from the sampled feature vectors. The weight vector in linear models is up-dated using the gradient vector and the learning rate  X  , and this process is repeated until convergence. Unfortunately however, learning linear models using SGD is numerically unstable, resulting in low prediction accuracy. This is be-cause SGD has three parameters (  X  ,  X  , and C ) that must be optimized if high classification accuracy is to be attained. Online learning is a specific version of SGD that loads an input dataset from the beginning and updates the weight vector in a linear model for each feature vector. AdaGrad [8] is an efficient online learning that automatically tunes pa-rameters of  X  and  X  in SGD. Although online learning is space-efficient (owing to its online nature), it is also numer-ically unstable. Even worse, AdaGrad is applicable only
Figure 1: Illustration of grammar compression. to differentiable loss functions, which limits its applicability to simple linear models, e.g., SVM and logistic regression, making the learned model difficult to interpret.

Despite the importance of scalable learning of interpretable linear models, no previous work has been able to achieve high prediction accuracy for classification/regression tasks and high interpretability of the learned models. We present a scalable learning algorithm that meets both these demands and is made possible by learning linear models on grammar-compressed data in the framework of PLS. Details of the proposed method are presented in the next section.
Given a sequence of integers S , a grammar-compressor generates a context-free grammar (CFG) that generates S and only S . The grammar consists of a set of rules 1 . Each rule is of the form Z i  X  ab . Symbols that appear on the left-hand side of any rule are called non-terminals . The remain-ing symbols are called terminals , all of which are present in the input sequence. Informally, a rule Z i  X  ab indicates that on the way to recovering the original sequence from its grammar-compressed representation, occurrences of the symbol Z i should be replaced by the symbol pair ab (the resulting sequence may then be subject to yet more replace-ments). A data structure storing a set of grammar rules is called a dictionary and is denoted by D . Given a non-terminal, the dictionary supports access to the symbol pair on the right-hand of the corresponding grammar rule, i.e., D [ Z i ] returns ab for rule Z i  X  ab . The original sequence can be recovered from the compressed sequence and D . The set of grammar rules in D can be represented as a forest of (pos-sibly small) binary trees called grammar trees , where each node and its left/right children correspond to a grammar rule. See Figure 1 for an illustration.

The size of a grammar is measured as the number of rules plus the size of compressed sequence. The problem of finding the minimal grammar producing a given string is known to be NP-complete [2], but several approximation algorithms exist that produce grammars that are small in practice (see, e.g., [27, 19, 16]). Among these is the simple and elegant Re-Pair [19] algorithm, which we review next.
The Re-Pair grammar compression algorithm by Larsson and Moffat [19] builds a grammar by repeatedly replacing the most frequent symbol pair in an integer sequence with a
In this paper we assume without loss of generality that the grammar is in Chomsky Normal Form. new non-terminal. Each iteration of the algorithm consists of the following two steps: (i) find the most frequent pair of symbols in the current sequence, and then (ii) replace the most frequent pair with a new non-terminal symbol, gener-ating a new grammar rule and a new (and possibly much shorter) sequence. Steps (i) and (ii) are then applied to the new sequence and iterated until no pair of adjacent symbols appears twice.

Apart from the dictionary D that stores the rules as they are generated, Re-Pair maintains a hash table and a pri-ority queue that together allow the most frequent pair to be found in each iteration. The hash table, denoted by H , holds the frequency of each pair of adjacent symbols ab in the current sequence, i.e., H : ab  X  N . The priority queue stores the symbol pairs keyed on frequency and allows the most frequent symbol to be found in step (i). In step (ii), a new grammar rule Z 1  X  ab is generated where ab is the most frequent symbol pair and Z 1 is a new non-terminal not appearing in a sequence. The rule is stored in the dictionary D . Every occurrence of ab in the sequence is then replaced by Z 1 , generating a new, shorter sequence. This replacement will cause the frequency of some symbol pairs to change, so the hash table and priority queue are then suitably updated.
Let s c denote a sequence generated at c -th iteration in the Re-Pair algorithm. For input sequence s in Figure 1, the most frequent pair of symbols is 12. Thus, we generate rule Z 1  X  12 to be added to the dictionary D and replace all the occurrences of 12 by non-terminal Z 1 in s . After four iterations, the current sequence s 4 has no repeated pairs, and thus the algorithm stops. Dictionary D has four grammar rules that correspond to a forest of two small trees.
As described by Larsson and Moffat [19], Re-Pair can be implemented to run in linear time in the length of the input sequence, but it requires the use of several heavyweight data structures to track and replace symbol pairs. The overhead of these data structures (at least 128 bits per position) pre-vents the algorithm from being applied to long sequences, such as the large data matrices.

Another problem that arises when applying Re-Pair to long sequences is the memory required for storing the hash table: a considerable number of symbol pairs appear twice in a long sequence, and the hash table stores something for each of them, consuming large amounts of memory.

In the next section, we present scalable Re-Pair algorithms that achieve both space-efficiency and fast compression time on large data matrices. Specifically, our algorithms need only constant working space.
Our goal is to obtain a compressed representation of a data matrix X of n rows and d columns. Let x i denote the i th row of the matrix represented as a fingerprint (i.e. binary vector). An alternative view of a row that will be useful to us is as a sequence of integers s i = ( p 1 ,p 2 p &lt; p 2 &lt;  X  X  X  &lt; p m , where p i  X  s i if and only if x In other words the sequence s i indicates the positions of the 1 bits in x i .

In what follows we will deal with a differentially encoded form of s i in which the difference for every pair of adjacent elements in s i is stored, i.e., s i = ( p 1 ,p 2 ,...,p m as s gi = ( p 1 ,p 2  X  p 1 ,p 3  X  p 2 ,...,p m  X  p m  X  1 ential encoding tends to increase the number of repeated symbol pairs, which allows the sequences s gi to be more ef-fectively compressed by the Re-Pair algorithm. A grammar compressor captures the underlying correlation structure of data matrices: by building the same grammar rules for the same (sequences of) integers, it effectively compresses data matrices with many repeated integers.
We now present two ideas to make Re-Pair scalable with-out seriously deteriorating its compression performance. Our first idea is to modify the Re-Pair algorithm to identify top-k frequent symbol pairs in all rows s c gi in step (i) and replace all the occurrences of the top-k symbol pairs in all rows s in step (ii), generating new k grammar rules and new rows s gi . This new replacement process improves scalability by reducing the number of iterations required by roughly a fac-tor of k .

Since we cannot replace both frequent symbol pairs ab and bc in triples abc in step (ii), we replace the first appearing symbol pair ab , preferentially. However, such preferential replacement can generate a replacement of a pair only once and can add redundant rules to a dictionary, adversely af-fecting compression performance. To overcome this prob-lem, we replace the first and second appearances of each fre-quent pair at the same time and replace the next successive appearance of the frequent pair as usual, which guarantees generating grammar rules that appear at least twice.
Our second idea is to reduce the memory of the hash table by removing infrequent symbol pairs. Since our modified Re-Pair algorithm can work storing compressed sequences s gi at each iteration c in a secondary storage device, the hash table consumes most of the memory in execution. Our modified Re-Pair generates grammar rules from only top-k frequent symbol pairs in the hash table, which means only frequent symbol pairs are expected to contribute to the com-pression. Thus, we remove infrequent symbol pairs from the hash table by leveraging the idea behind stream mining tech-niques originally proposed in [15, 6, 22] for finding frequent items in data stream. Our method is a counter-based al-gorithm that computes the frequency of each symbol pair and removes infrequent ones from the hash table at each in-terval in step (i). We present two Re-Pair algorithms using lossy counting and frequency counting for removing infre-quent symbol pairs from the hash table. We shall refer to the Re-Pair algorithms using lossy counting and frequency counting as Lossy-Re-Pair and Freq-Re-Pair, respectively.
The basic idea of lossy counting is to divide a sequence of symbols into intervals of fixed length and keep symbol pairs in successive intervals in accordance with their appearance frequencies in a hash table. Thus, if a symbol pair has ap-peared h times in the previous intervals, it is going to be kept in the next h successive intervals.

Let us suppose a sequence of integers made by concate-nating all rows s gi of X and let N be the length of the sequence. We divide the sequence into intervals of fixed-length ` . Thus, the number of intervals is N/` . We use hash table H for counting the appearance frequency of each symbol pair in the sequence. If symbol pair ab has count H ( ab ), it is ensured that ab is kept in hash table H until the next H ( ab )-th interval. If symbol pair ab first appears in the q -th interval, H ( ab ) is initialized as qN/` + 1, which en-sures that ab is kept until at least the next interval, i.e., the ( qN/` + 1)-th interval. Algorithm 1 shows the pseudo-code of lossy counting.
 The estimated number of symbol pairs in the hash table is O ( ` ) [22], resulting in O ( ` log ` ) bits consumed by the hash table.
 Algorithm 1 Lossy counting. H : hash table, N : length of an input string at a time point, ` : length of each interval. Note that lossy counting can be used in step (i) in the Re-Pair algorithm. 1: Initialize N = 0 and  X  = 0 2: function LossyCounting ( a,b ) 3: N = N + 1 4: if H ( ab ) 6 = 0 then 5: H ( ab ) = H ( ab ) + 1 6: else 7: H ( ab ) =  X  + 1 8: if b N ` c6 =  X  then 9:  X  = b N ` c 10: for each symbol pair ab in H do 11: if H ( ab ) &lt;  X  then 12: Remove ab from H
The basic idea of frequency counting is to place a limit, v , on the maximum number of symbol pairs in hash table H and then keep only the most frequent v symbol pairs in H . Such frequently appearing symbol pairs are candidates to be replaced by new non-terminals, which generates a small number of rules.

The hash table counts the appearance frequency for each symbol pair in step (i) of the Re-Pair algorithm. When the number of symbol pairs in the hash table reaches v , Freq-Re-Pair removes the bottom percent of symbol pairs with respect to frequency. We call the vacancy rate. Algo-rithm 2 shows the pseudo-code of frequency counting. The space consumption of the hash table is O ( v log v ) bits. Algorithm 2 Frequency counting. H : hash table, | H | : number of symbol pairs in H , v : the maximum number of symbol pairs in H , : vacancy rate. Note that frequency counting can be used in step (i) in the Re-Pair algorithm. 1: function FrequencyCounting ( a,b ) 2: if H ( ab ) 6 = 0 then 3: H ( ab ) = H ( ab ) + 1 4: else 5: if | H | X  v then 6: while v (1  X  / 100) &lt; | H | do 7: for each symbol pair a 0 b 0 in H do 8: H ( a 0 b 0 ) = H ( a 0 b 0 )  X  1 9: if H ( a 0 b 0 ) = 0 then 10: Remove a 0 b 0 from H 11: H ( ab ) = 1
In this section, we present algorithms for directly accessing rows and columns of a grammar-compressed data matrix, which is essential for us to be able to apply PLS on the compressed matrix in order to learn linear regression models.
Accessing the i -th row corresponds to recovering the orig-inal s i from grammar-compressed s c gi . We compute this op-eration by traversing the grammar trees. For recovering the i -th row s i , we start traversing the grammar tree having a node of the q -th symbol s c gi [ q ] as a root for each q from 1 to | s c gi | . Leaves encountered in the traversal must have inte-gers in sequence s gi , which allows us to recover s gi via tree traversals, starting from the nodes with non-terminal s c from s gi by cumulatively adding integers in s gi from 1 to | s gi | , i.e, s i [1] = s gi [1], s i [2] = s gi [2] + s s [ | s gi | ] + s i [ | s gi | X  1].
Accessing the j -th column of a grammar-compressed data matrix requires us to obtain a set of row identifiers R such that x ij = 1 for i  X  [1 ,n ], i.e., R = { i  X  [1 ,n ]; x This operation enables us to compute the transpose X | from X in compressed format, which is used in the optimization algorithm of PLS.

P [ Z i ] stores a summation of terminal symbols as inte-gers at the leaves under the node corresponding to terminal symbol Z i in a grammar tree. For example, in Figure 1, P [ Z 1 ] = 3, P [ Z 2 ] = 6, P [ Z 3 ] = 8 and P [ Z be implemented as an array that is randomly accessed from a given non-terminal symbol. We shall refer to P as the weight array. The size of P depends only on the grammar size.

The j -th column is accessed to check whether or not x ij 1 in compressed sequence s c gi , for each i  X  [1 ,n ]. We effi-ciently solve this problem on grammar-compressed data ma-trix by using the weight array P . Let u q store the summa-tion of weights from the first symbol s c gi [1] to the q -th symbol s [ q ], i.e., u q = P [ s c gi [1]] + P [ s c gi [2]] +  X  X  X  + P [ s u 0 = 0. If u q is not less than j , the grammar tree with the node corresponding to a symbol s c gi [ q ] as a root can encode j at a leaf. Thus, we traverse the tree in depth-first or-der from the node corresponding to symbol s c gi [ q ] as follows. Suppose Z = s c gi [ q ] and u = u q  X  1 . Let Z ` (resptively Z a (respectively b ) of Z  X  ab in D . (i) if j &lt; u , we go down to the left child in the tree; (ii) otherwise, i.e., j  X  u , we add P [ Z ` ] to u and go down to the right child. We continue the traversal until we reach a leaf. If s = j at a leaf, this should be x ij = 1 at row i ; thus we add i to solution set R . Algorithm 3 shows the pseudo-code for column access.
In this section we present our cPLS algorithm for learn-ing PLS on grammar-compressed data matrices. We first review the PLS algorithm on uncompressed data matrices. NIPALS [35] is the conventional algorithm for learning PLS and requires the deflation of the data matrix involved. We thus present a non-deflation PLS algorithm for learning PLS on compressed data matrices.
Let us assume a collection of n data samples and their out-put variables ( x 1 ,y 1 ) , ( x 2 ,y 2 ) ,..., ( x n ,y n output variables are assumed to be centralized as P n i =1 Algorithm 3 Access to the j -th column on grammar-compressed data matrix. R : solution set of row identifiers i at column j s.t. x ij = 1. 1: function AccessColumn ( j ) 2: for i in 1 ..n do 3: u 0 = 0 4: for q in 1 .. | s c gi | do 6: if j  X  u q then 7: Recursion ( i,j,s c gi [ q ] ,u q  X  1 ) 8: break 1: function Recursion ( i , j , Z , u ) 2: if Z is a terminal symbol then 3: if u + Z = j then 4: Add i to R 5: return 6: Set Z l (resp. Z r ) as a (resp. b ) of Z  X  ab in D 7: if u + P [ Z l ] &gt; j then 8: Recursion ( i , j , Z l , u ) . Go to left child 9: else 10: Recursion ( i , j , Z r , u + P [ Z l ]) . Go to right child 0. Denote by y  X  &lt; n the vector of all the training output variables, i.e., y = ( y 1 ,y 2 ,...,y n ) | .

The regression function of PLS is represented by the fol-lowing special form, where the w i are weight vectors reducing the dimensionality of x ; they satisfy the following orthogonality condition: We have two kinds of variables w i and  X  i to be optimized. Denote by W  X  &lt; d  X  m the weight matrix i -th column of which is weight vector w i , i.e., W = ( w 1 ,w 2 ,...,w  X   X  &lt; m be a vector whose i -th element is  X  i , i.e.,  X  = (  X  1 , X  2 ,..., X  m ) | . Typically, W is first optimized and then  X  is determined by minimizing the least squares error without regularization, By computing the derivative of equation (2) with respect to  X  and setting it to zero,  X  is obtained as follows:
The weight vectors are determined by the following greedy algorithm. The first vector w 1 is obtained by maximizing the squared covariance between the mapped feature X w and the output variable y as follows: w 1 = argmax w cov 2 ( X w,y ), subject to w | X | X w = 1, where cov ( X w, y ) = y | X w . The problem can be analytically solved as w 1 = X | y .
For the i -th weight vector, the same optimization problem is solved with additional constraints to maintain orthogonal-ity, subject to w | X | X w = 1, w | X | X | w j = 0, j = 1 ,...,i  X  1. The optimal solution of this problem cannot be obtained an-alytically, but NIPALS solves it indirectly. Let us define the i -th latent vector as t i = X w i . The optimal latent vectors t are obtained first and the corresponding w i is obtained later. NIPALS performs the deflation of design matrix X to ensure the orthogonality between latent components t i follows, X = X  X  t i t | i X . Then, the optimal solution has the form, w i = X | y .

Due to the deflation, X = X  X  t i t | i X , NIPALS completely destroys the structure of X . Thus, it cannot be used for learning PLS on grammar-compressed data matrices. We present a non-deflation PLS algorithm for learning PLS on grammar-compressed data matrices. Our main idea here is to avoid deflation by leveraging the connection be-tween NIPALS [35] and the Lanczos method [18] which was originally proposed for recursive fitting of residuals without changing the structure of a data matrix.

We define residual r i +1 = ( r i  X  ( y | t i  X  1 ) t i  X  1 tialized as r 1 = y . The i -th weight vector is updated as w X without deflating the original data matrix X . The i -th la-tent vector is computed as t i = X w i and is orthogonalized by applying the Gram-Schmidt orthogonalization to the i -th la-tent vector t i and previous latent vectors t 1 , t 2 ,..., t &lt; n  X  ( i  X  1) . The non-deflation PLS algorithm updates the residual r i instead of deflating X , thus enabling us to learn PLS on grammar-compressed data matrices. cPLS is the non-deflation PLS algorithm that learns PLS on grammar-compressed data matrices. The input data ma-trix is grammar-compressed and then the PLS is learned on the compressed data matrix by the non-deflation PLS al-gorithm. Our grammar-compressed data matrix supports row and column accesses directly on the compressed format for computing matrix calculations of addition and multi-plication, which enables us to learn PLS by using the non-deflation PLS algorithm. Let X G be the grammar-compressed data matrix of X . Algorithm 4 shows the pseudo-code of cPLS. Since our grammar-compression is lossless, the cPLS algorithm on grammar-compressed data matrices learns the same model as the non-deflation PLS algorithm on uncom-pressed data matrices and so achieves the same prediction accuracy.
 Algorithm 4 The cPLS algorithm. X G : the grammar-compressed data matrix of X . 1: r 1 = y 2: for i = 1 ,...,m do 3: w i = X | G r i . access to column 4: if i = 1 then 5: t 1 = X G w i . access to row 6: else 8: t i = t i / || t i || 2 10: Compute the coefficients  X  using equation (3).
We perform feature extraction after line 3 at each iter-ation in Algorithm 4. The features corresponding to the top-u largest weights w i are extracted. Due to the orthogo-nality condition (1), the extracted features give users a novel insight for analyzing data, which is shown in Section 7.
The cPLS algorithm has three kinds of variables to be optimized: w i , r i , and t i . The memory for w m is O ( md ) and the memory for t m and r i is O ( mn ). Thus, the total memory for the variables in cPLS is O ( m min( n,d )) highly depending on parameter m . The parameter m controls the amount of fitting of the model to the training data and is typically chosen to optimize the cross validation error. Since the cPLS algorithm learns the model parameters efficiently, m can be set to a small value, which results in overall space-efficiency.
In this section, we demonstrate the effectiveness of cPLS with massive datasets. We used five datasets, as shown in Table 2.  X  X ook-review X  consists of 12,886,488 book reviews in English from Amazon [24]. We eliminated stop-words from the reviews and then represented them as 9,253,464 dimensional fingerprints, where each dimension of the finger-print represents the presence or absence of a word.  X  X om-pound X  is a dataset of 42,682 chemical compounds that are represented as labeled graphs. We enumerated all the sub-graphs of at most 10 vertices from the chemical graphs by using gSpan [38] and then converted each chemical graph into a 52,099 dimensional fingerprint, where each dimension of the fingerprint represents the presence or absence of a chemical substructure.  X  X ebspam X  is a dataset of 16,609,143 fingerprints of 350,000 dimensions 2 .  X  X P-interaction X  is a dataset of 216,121,626 compound-protein pairs, where each compound-protein pair is represented as a 3,621,623 dimen-sional fingerprint and 300,202 compound-protein pairs are interacting pairs according to the STITCH database [17]. We used the above four datasets for testing the binary classi-fication ability.  X  X P-intensity X  consists of 1,329,100 compound-protein pairs represented as 682,475 dimensional fingerprints, where the information about compound-protein interaction intensity was obtained from several chemical databases (e.g., ChEMBL, BindingDB and PDSP Ki). The intensity was observed by IC50 (half maximal (50%) inhibitory concen-tration). We used the  X  X P-intensity X  dataset for testing the regression ability. The number of all the nonzero dimensions in each dataset is summarized in the #nonzero column in Table 2, and the size for storing fingerprints in memory by using 32bits for each element is written in the memory col-umn in Table 2. We implemented all the methods by C++ and performed all the experiments on one core of a quad-core Intel Xeon CPU E5-2680 (2.8GHz). We stopped the execu-tion of each method if it had not finished within 24hours in the experiments. In the experiments, cPLS did not use a secondary storage device for compression, i.e., cPLS com-pressed data matrices by loading all data in memory.
First, we investigated the influence on compression perfor-mance of the top-k parameter in our Re-Pair algorithms. For this setting, we used the Lossy-Re-Pair algorithm, where pa-rameter ` is set to the total length of all rows in an input data matrix in order to keep all the symbols in the hash table. We examined k = { 1  X  10 4 , 2 . 5  X  10 4 , 5  X  10 4 , 7 . 5  X  10 for the Book-review, Compound and Webspam datasets and examined k = { 1  X  10 5 , 2 . 5  X  10 5 , 5  X  10 5 , 7 . 5  X  10 for the CP-interaction and CP-intensity datasets.
The dataset is downloadable from http://www.csie.ntu. edu.tw/  X cjlin/libsvmtools/datasets/binary.html.
Figure 2 shows compression size and compression time for various top-k . We observed a trade-off between com-pressed size and compression time for all the datasets. The smaller the compressed size, the larger the compression time for larger values of k . In particular, significantly faster com-pression time was possible at the cost of only slightly worse compression. For example, Lossy-Re-Pair took 57,290 sec-onds to compress the Book-review dataset and its size was 1,498 mega bytes (MB) for k =10000. When k =100000, com-pression time dropped to 20,004 seconds (less than half), while compressed size increased negligibly to 1,502MB.
The same trends for the Book-review dataset were ob-served in the other datasets, which suggests that in prac-tice a large value of k can be chosen for fast compression, without adversely affecting compression performance. No-tably, we observed our compression method to be particu-larly effective for the larger datasets: CP-interaction and CP-intensity. The original sizes of CP-interaction and CP-intensity were 125GB and 110GB, respectively, while the compressed sizes of CP-interaction and CP-intensity were at most 5GB and at 535MB, respectively. Our compression method thus achieved compression rates of 4% and less than 1% for CP-interaction and CP-intensity, respectively. Such significant reductions in data size enable the PLS algorithm to scale to massive data.
 Next, we evaluated the performance of Lossy-Re-Pair and Freq-Re-Pair, where parameters ` = { 1MB, 10MB, 100MB, 1000MB } were examined for Lossy-Re-Pair, and parameters v = { 1MB, 10MB, 100MB, 1000MB } and = { 30 } were ex-amined for Freq-Re-Pair. Table 3 shows the compressed size, compression time and the working space used for the hash table in Lossy-Re-Pair and Freq-Re-Pair. We observed that both Lossy-Re-Pair and Freq-Re-Pair achieved high com-pression rates using small working space. Such efficiency is crucial when the goal is to compress huge data matrices that exceed the size of RAM; our Re-Pair algorithm can compress data matrices stored in external memory (disk). For compressing the CP-interaction dataset, Lossy-Re-Pair and Freq-Re-Pair consumed 16GB and 13GB, respectively, achieving a compressed size of 5GB. We observed the same tendency for the other datasets (See Table 3).
We evaluated the classification and regression capabilities of cPLS, PCA-SL, bMH-SL and SGD. Following the previ-ous works [39, 21], we randomly selected 20% of samples for testing and used the remaining 80% of samples for training. cPLS has one parameter m , so we selected the best param-eter value among m = { 10 , 20 ,..., 100 } that achieved the highest accuracy for each dataset. The PCA phase of PCA-SL has one parameter deciding the number of principal com-ponents m , which was chosen from m = { 10 , 25 , 50 , 75 , 100 } whose maximum value of 100 is the same as that of cPLS X  X  parameter m . Linear models were learned with LIBLIN-EAR [10], one of the most efficient implementations of lin-ear classifiers, on PCA X  X  compact feature vectors, where the hinge loss of linear SVM for classification and the squared er-ror loss for regression were used with L 2 -regularization. The learning process of PCA-SL [14, 9] has one parameter C for L -regularization, which was chosen from C = { 10  X  5 , 10  X  4 ..., 10 5 } . For PCA-SL [14, 9], we examined all possible com-binations of two parameters ( m and C ) and selected the best combination achieving the highest accuracy for each dataset. The hashing process of bMH-SL [21] has two parameters (the number of hashing values h and the length of bits b ), so we examined all possible combinations of h = { 10 , 30 , 100 } and b = { 8 , 16 } . As in PCA-SL, linear models were learned with LIBLINEAR [10] on bMH X  X  compact feature vectors, where the hinge loss of linear SVM for classification and the squared error loss for regression were used with L regularization. The learning process of bMH-SL [21] has one parameter C for L 2 -regularization, which was chosen from C = { 10  X  5 , 10  X  4 , ..., 10 5 } . For bMH-SL, we exam-ined all possible combinations of three parameters ( h , b , and C ) and selected the best combination achieving the highest accuracy for each dataset. We implemented SGD on the basis of the AdaGrad algorithm [8] using the lo-gistic loss for classification and the squared error loss for regression with L 2 -regularization. SGD [8] has one param-eter C for L 2 -regularization, which was also chosen from C = { 10  X  5 , 10  X  4 ,..., 10 5 } . We measured the prediction ac-curacy by the area under the ROC curve (AUC) for classifi-cation and Pearson correlation coefficient (PCC) for regres-sion. Note that AUC and PCC return 1 for perfect inference in classification/regression, while AUC returns 0.5 for ran-dom inference and PCC returns 0 for random inference. We report the best test accuracy under the above experimental settings for each method below. Book-review CP-interaction CP-intensity (sec). cPLS PCA-SL bMH-SL SGD [1] P. Bille, P. H. Cording, and I. L. Gortz. Compact [2] M Charikar, E. Lehman, D. Liu, R. Panigrahy, [3] B. Chen, D. Wild, and R. Guha. PubChem as a source [4] The Uniprot Consortium. The universal protein [5] G. Cormode and Muthukrishnan S. An improved data [6] D. Demaine, A. L  X opez-Ortiz, and I. Munro. Frequency [7] C.M. Dobson. Chemical space and biology. Nature , [8] J. Duchi, E. Hazan, and Y. Singer. Adaptive [9] T. Elgamal, M. Yabandeh, A. Aboulnaga, [10] R. Fan, K. W. Chang, C. J. Hsieh, X. R. Wang, and [11] D. A. Forsyth and J. Ponce. Computer Vision: A [12] A. Gionis, P. Indyk, and R. Motwani. Similarity search [13] D. Hermelin, D. H. Landau, and O. Weimann. A [14] I. T. Jolliffe. Principal Component Analysis . Springer, [15] R. Karp, S. Shenker, and C. Papadimitriou. A simple [16] J. C. Kieffer, E. Yang, G. J. Nelson, and P. C. [17] M. Kuhn, D. Szklarczyk, A. Franceschini, [18] C. Lanczos. An iteration method for the solution of [19] J. Larsson and A. Moffat. Offline dictionary-based [20] P. Li and A. C. K  X  onig. b-bit minwise hashing. In [21] P. Li, A. Shrivastava, J. L. Moore, and A. C. K  X  onig. [22] G. Manku and R. Motwani. Approximate frequency [23] C. D. Manning and H. Sch  X  utze. Foundations of [24] J. McAuley and J. Leskovec. Hidden factors and [25] Y. Mu, G. Hua, W. Fan, and S. Chang. Hash-SVM: [26] R. Rosipal and N. Kr  X  amer. Overview and recent [27] W. Rytter. Application of Lempel-Ziv factorization to [28] B. Sch  X  olkopf, A. Smola, and K. R. M  X  uller. Nonlinear [29] B. Stockwell. Chemical genetics: Ligand-based [30] Y. Tabei and Y. Yamanishi. Scalable prediction of [31] M. E. Tipping and C. M. Bishop. Mixtures of [32] R. Todeschini and V. Consonni. Handbook of [33] Y. Tsuruoka, J. Tsujii, and S. Ananiadou. Stochastic [34] K. Weinberger, A. Dasgupta, J. Langford, A. Smola, [35] H. Wold. Path models with latent variables: The [36] S. Wold, M. Sj  X  ostr  X  om, and L. Eriksson. [37] T. Yamamoto, H. Bannai, S. Inenaga, and M. Takeda. [38] X. Yan and J. Han. gSpan: graph-based substructure [39] H.F. Yu, C.J. Hsieh, K.W. Chang, and C.J. Lin. Large
