 important in a number of areas of AI and statistical data anal ysis. AI researchers have proposed that by storing enough everyday relational facts and generalizi ng appropriately to unobserved proposi-tions, we might capture the essence of human common sense. Fo r instance, given propositions such coffee), (glass, can-contain, juice), (glass, can-contai n, water), (glass, can-contain, wine), and so (cup, can-contain, wine). Modelling relational data is als o important for more immediate appli-cations, including problems arising in social networks [2] , bioinformatics [16], and collaborative filtering [18].
 We approach these problems using probabilistic models that define a joint distribution over the truth that a new relation ( a, r, b ) is true with probability P ( T ( a, r, b ) = 1 | S ) . In addition to making predictions on new relations, we also w ant to understand the data X  X hat is, to hidden variables over simple hypotheses, the posterior dis tribution over the hidden variables will For example, the Infinite Relational Model (IRM) [8] represe nts simple laws consisting of partitions checks that the clusters to which a , r , and b belong are compatible. The main advantage of the IRM is its ability to extract meaningful partitions of objects a nd relations from the observational data, which greatly facilitates exploratory data analysis. More elaborate proposals consider models over more powerful laws (e.g., first order formulas with noise mod els or multiple clusterings), which are currently less practical due to the computational difficult y of their inference problems [7, 6, 9]. tions than interpretable models of similar complexity, as w e demonstrate in our experimental results section. Factorization models learn a distributed represe ntation for each object and each relation, and make predictions by taking appropriate inner products. Their strength lies in the relative ease of their continuous (rather than discrete) optimization, and in their excellent predictive performance. However, it is often hard to understand and analyze the learn ed latent structure.
 The tension between interpretability and predictive power is unfortunate: it is clearly better to have a model that has both strong predictive power and interpreta bility. We address this problem by introducing the Bayesian Clustered Tensor Factorization ( BCTF) model, which combines good in-terpretability with excellent predictive power. Specifica lly, similarly to the IRM, the BCTF model same time, every entity has a distributed representation: e ach object a is assigned the two vectors a , a R (one for a being a left argument in a relation and one for it being a right argument), and within a cluster to have similar distributed representatio ns (and similarly for relations). The experiments show that the BCTF model achieves better pre dictive performance than a number of related probabilistic relational models, including the IRM, on several datasets. The model is scal-able, and we apply it on the Movielens [15] and the Conceptnet [10] datasets. We also examine the structure found in BCTF X  X  clusters and learned vectors. Finally, our results provide an exam-ple where the performance of a Bayesian model substantially outperforms a corresponding MAP estimate for large sparse datasets with minimal manual hype rparameter selection. We begin with a simple tensor factorization model. Suppose t hat we have a fixed finite set of objects O and a fixed finite set of relations R . For each object a  X  O the model maintains two vectors a , a R  X  R d (the left and the right arguments of the relation), and for ea ch relation r  X  R it maintains a matrix R  X  R d  X  d , where d is the dimensionality of the model. Given a setting of these parameters (collectively denoted by  X  ), the model independently chooses the truth-value of each relation ( a, r, b ) from the distribution P ( T ( a, r, b ) = 1 |  X  ) = 1 / (1 + exp(  X  a  X  particular, given a set of known relations S , we can learn the parameters by maximizing a penalized log likelihood log P ( S |  X  )  X  Reg (  X  ) . The necessity of having a pair of parameters a of a single distributed representation a , will become clear later.
 Next, we define a prior over the vectors { a Process. Once the partitions are chosen, each cluster C samples its own prior mean and prior di-agonal covariance, which are then used to independently sam ple vectors { a belong to cluster C (and similarly for the relations, where we treat R as a d 2 -dimensional vector). sufficiently tight, the value of a  X  belong. At the same time, the distributed representations h elp generalization, because they can rep-resent graded similarities between clusters and fine differ ences between objects in the same cluster. Thus, given a set of relations, we expect the model to find both meaningful clusters of objects and relations, as well as predictive distributed representati ons.
 More formally, assume that O = { a follows: { c Figure 1: A schematic diagram of the model, where the arcs rep resent the object clusters and the vectors within each cluster are similar. The model predicts T ( a, r, b ) with a  X  {  X  2 ,  X  ,  X  DP } are the model hyperparameters. Two of the above terms are giv en by denotes the probability of the partition induced by c under the Chinese Restaurant Process with concentration parameter  X  . The Gaussian likelihood in Eq. 2 is far from ideal for modell ing binary conjugate and Gibbs sampling easier.
 Defining P (  X  | c,  X  ) takes a little more work. Given the partitions, the sets of pa rameters { a and { R } become independent, so The distribution over the relation-vectors is given by where | c mixture model [13]. We further place a Gaussian-Inverse-Ga mma prior over (  X ,  X ) : where  X  is a diagonal matrix whose entries are  X  2 R  X  and scale parameter  X  . This prior makes many useful expectations analytically co mputable. The terms P ( { a dently.
 Inference We now briefly describe the MCMC algorithm used for inference . Before starting the Markov chain, we find a MAP estimate of the model parameters using the method of conjugate gradient (but we do not optimize over the partitions). The MAP estimate is the n used to initialize the Markov chain. Each step of the Markov chain consists of a number of internal steps. First, given the parameters  X  , the chain updates c = ( c split-and-merge algorithm (where the launch state was obta ined with two sweeps of Gibbs sampling starting from a uniformly random cluster assignment) [5]. N ext, it samples from the posterior mean Eq. 5.
 Next, the Markov chain samples the parameters { a means and covariances. This step is tractable since the cond itional distribution over the object vec-tors { a object vectors. This conditional independence is importan t, since it tends to make the Markov chain mix faster, and is a direct consequence of each object a having two vectors, a object a was only associated with a single vector a (and not a over { a } would not factorize, which in turn would require the use of a s lower sequential Gibbs sampler. In the current setting, we can further speed up the i nference by sampling from conditional distributions in parallel. The speedup could be substantia l, particularly when the number of objects is large. The disadvantage of using two vectors for each obje ct is that the model cannot as easily capture the  X  X osition-independent X  properties of the obje ct, especially in the sparse regime. Sampling { a objects. While we do the same for { a even for small values of d (e.g. 20). Finally, we make a small symmetric multiplicativ e change to each hyperparameter and accept or reject its new value accor ding to the Metropolis-Hastings rule. In this section, we show that the BCTF model has excellent pre dictive power and that it finds inter-pretable clusters by applying it to five datasets and compari ng its performance to the IRM [8] and the Multiple Relational Clustering (MRC) model [9]. We also compare BCTF to its simpler counter-part: a Bayesian Tensor Factorization (BTF) model, where al l the objects and the relations belong to a single cluster. The Bayesian Tensor Factorization model i s a generalization of the Bayesian prob-methods [3, 14, 1]. In what follows, we will describe the data sets, report the predictive performance of our and of the competing algorithms, and examine the struc ture discovered by BCTF. 3.1 Description of the Datasets We use three of the four datasets used by [8] and [9], namely, t he Animals, the UML, and the Kinship dataset, as well the Movielens [15] and the Conceptnet datas ets [10]. 3.2 Experimental Protocol To facilitate comparison with [9], we conducted our experim ents the following way. First, we nor-malized each dataset so the mean of its observations was 0. Ne xt, we created 10 random train/test splits, where 10% of the data was used for testing. For the Con ceptnet and the Movielens datasets, we used only two train/test splits and at most 30 clusters, wh ich made our experiments faster. We report test root mean squared error (RMSE) and the area under the precision recall curve (AUC) [9]. For the IRM 1 we make predictions as follows. The IRM partitions the data i nto blocks; we compute the smoothed mean of the observed entries of each block and us e it to predict the test entries in the same block. 3.3 Results We first applied BCTF to the Animals, Kinship, and the UML data sets using 20 and 40-dimensional vectors. Table 1 shows that BCTF substantially outperforms IRM and MRC in terms of both RMSE and AUC. In fact, for the Kinship and the UML datasets, the sim ple tensor factorization model trained by MAP performs as well as BTF and BCTF. This happens b ecause for these datasets the number of observations is much larger than the number of para meters, so there is little uncertainty about the true parameter values. However, the Animals datas et is considerably smaller, so BTF performs better, and BCTF performs even better than the BTF m odel.
 We then applied BCTF to the Movielens and the Conceptnet data sets. We found that the MAP es-timates suffered from significant overfitting, and that the f ully Bayesian models performed much better. This is important because both datasets are sparse, which makes overfitting difficult to com-bat. For the extremely sparse Conceptnet dataset, the BCTF m odel further improved upon simpler Figure 5: Results on the Movielens dataset. Left: The covariance between the movie vectors. Right: The which is also similar, according to the covariance, to { Cell Dysfunction, Disease, Mental Dysfunc-the BCTF model is able to predict held-out relations much bet ter.
 Figures 5 and 6 display the learned clusters for the Movielen s and the Conceptnet datasets. For the Movielens dataset, we show the most frequently-rated movie s in each cluster where the clusters are sorted by size. We also show the covariance between the movie vectors which are sorted by the clusters, where we display only the 100 most frequently-rat ed movies per cluster. The covariance matrix is aligned with the table on the right, making it easy t o see how the clusters relate to each other. For example, according to the covariance structure, clusters 7 and 9, containing Hollywood action/adventure movies are similar to each other but are di ssimilar to cluster 8, which consists of comedy/horror movies.
 For the Conceptnet dataset, Fig. 6 displays the 100 most freq uent objects per category. From the co-variance matrix, we can infer that clusters 8, 9, and 11, cont aining concepts associated with humans taking actions, are very similar to each other, and are very d issimilar to cluster 10, which contains covariances between vectors in each of these clusters. We introduced a new method for modelling relational data whi ch is able to both discover meaningful representations when applied to modelling relational data , since even simple tensor factorization models can sometimes outperform the more complex models. In deed, for the kinship and the UML datasets, the performance of the MAP-based tensor factoriz ation was as good as the performance of the BCTF model, which is due to the density of these dataset s: the number of observations was much larger than the number of parameters. On the other hand, for large sparse datasets, the BCTF model significantly outperformed its MAP counterpart, and i n particular, it noticeably outperformed BTF on the Conceptnet dataset.
 A surprising aspect of the Bayesian model is the ease with whi ch it worked after automatic hy-perparameter selection was implemented. Furthermore, the model performs well even when the initial MAP estimate is very poor, as was the case for the 40-d imensional models on the Conceptnet dataset. This is particularly important for large sparse da tasets, since finding a good MAP estimate requires careful cross-validation to select the regulariz ation hyperparameters. Careful hyperparam-eter selection can be very labour-expensive because it requ ires careful training of a large number of models.
 Acknowledgments The authors acknowledge the financial support from NSERC, Sh ell, NTT Communication Sciences Laboratory, AFOSR FA9550-07-1-0075, and AFOSR MURI.

