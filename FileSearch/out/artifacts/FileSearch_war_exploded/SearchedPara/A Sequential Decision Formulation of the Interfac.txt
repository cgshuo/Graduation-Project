 The Interface Card model is a promising new theoretical framework for modeling and optimizing interactive retrieval interfaces, but how to systematically instantiate it to solve concrete interface optimization problems remains an open challenge. We propose a novel formulation of the Interface Card model based on sequential decision theory, leading to a general framework for formal modeling of user states and stopping actions. The proposed framework naturally con-nects optimization of interactive retrieval with Markov De-cision Processes and Partially Observable Markov Decision Processes, and enables the use of reinforcement learning al-gorithms for optimizing interactive retrieval interfaces. Sim-ulation and user study experiments demonstrate the effec-tiveness of the proposed model in automatically adjusting the interface layout in adaptation to inferred user stopping tendencies in addition to user interaction and screen size.
Formal modeling of information retrieval process is one of the most important fundamental research problems in infor-mation retrieval. It not only enables formalization of the retrieval task as a well-defined computation problem, which is required for designing any effective retrieval algorithm, but also provides a foundation for quantitative evaluation of a retrieval system.

The Probability Ranking Principle (PRP) was an early at-tempt to formalize the retrieval task as a problem of ranking documents for a query. It motivated and laid out a theoret-ical foundation for studying and deriving many traditional retrieval models to optimize ranking in the past few decades [21, 19, 18, 9, 1, 24, 6], as well as more recent works on learning to rank [11]. PRP also provides a foundation and justification for designing evaluation measures such as Mean Average Precision (MAP) and Normalized Discounted Cu-mulative Gain (nDCG) [10] to measure the accuracy of a ranked list of retrieval results.
 c 2016 ACM. ISBN 978-1-4503-4069-4/16/07 ...$15.00.
However, PRP relies on two unrealistic assumptions: (1) independent utility/relevance of documents, and (2) sequen-tial browsing, making it hard to study issues such as diver-sity (which requires  X  X ependent relevance X ) or study how to optimize IR interface design within the PRP framework. To address the limitation due to assumption (1), a more general PRP for Interactive IR (namely PRP-IIR) was proposed [8], which captures the the dependency of the utility of docu-ments examined later by a user on that of the documents already seen by the user in a theoretical framework. Such a framework also provides a theoretical justification for novel evaluation measures proposed for novelty and diversity (e.g., the  X  -NDCG [5]), and rank-biased precision [16]. The recent Interface Card Model (ICM) [26] further generalizes PRP-IIR to address the limitation due to assumption (2), and frames the retrieval problem as to optimize a sequence of  X  X nterface cards X  to be presented to a user in an interactive manner so as to minimize the user X  X  effort while maximizing the gain. The framework is shown to not only cover both PRP and PRP-IIR as special cases, but also enable auto-matic design of an interactive navigation interface in adap-tion to the screen sizes and the uncertainty about a user X  X  information need [26]. From evaluation perspective, ICM implies that an evaluation measure should be  X  X ware X  of the actions taken by users on a retrieval result and the effort of taking an action, thus providing a justification for measures such as the time-based gain measure [23] where user effort is measured based on the time spent by the user.

ICM provides a high-level theoretical framework for op-timizing interactive retrieval, but it does not specify a sys-tematic way to instantiate it for solving a concrete interface optimization problem, leaving how to further refine this gen-eral framework an open challenge. In this paper, we address this challenge and propose a novel formulation of the Inter-face Card Model based on sequential decision theory, which leads to a general instantiation of ICM that can explicitly model user states and stopping actions in search in a formal framework. It naturally connects the optimization of inter-active retrieval with Markov Decision Process (MDP) and Partially Observable Markov Decision Process (POMDP) [20], thus enabling the use of reinforcement learning algo-rithms for optimizing interactive retrieval interfaces in gen-eral. We refer to the new model as Interface Card Model with User States (ICM-US).

While the proposed ICM-US model remains a high-level framework, it has several important advantages over ICM. Firstly, ICM-US explicitly models user states, which can po-tentially include many relevant variables about a user that we want to model (e.g., patience, attention tendency, and r eadability) in optimizing retrieval results. In this paper, we will particularly examine the modeling of stopping actions which is related to a user X  X  patience in search, and derive a framework to optimize the interface design with considera-tion of users X  stopping tendencies. Secondly, ICM-US opens up many opportunities to use sequential decision theories and reinforcement learning algorithms to optimize interac-tive retrieval. In this paper, we will show that it is possible to use the ICM-US framework to define and solve the inter-face optimization problems studied in [26] in more elegant and more general ways. Specifically, we work out the  X  X lain card X  X ase in the sequential decision theory context and take the user stopping tendencies into consideration, and math-ematically prove that a more general ranking principle is the solution to the Bellman Equation. In the  X  X avigational card X  case, we consider user stopping tendencies and define a more general interface optimization problem. The prob-lem is proved to be NP-Hard and we conduct experiments to tackle it in two ways: (a) we conduct simulated experiments to solve the Bellman Equation under reasonable simplifi-cation assumptions; (b) we approximately solve the opti-mization problem in more general settings and conduct user studies to examine the empirical benefit of ICM-US. The results demonstrate that ICM-US is effective for automati-cally adjusting the interface layout in adaptation to inferred user stopping tendencies in addition to user interaction and screen size.
The proposed ICM-US framework is related to several lines of recent work on POMDP and reinforcement learn-ing (e.g.,[13, 12, 22]) and economic IR models (e.g., [2, 3]), especially because all these works tend to also model user interactions formally. The main difference between ICM-US and these work is that ICM-US is based on the Interface Card Model, which is a very general framework for modeling retrieval process (framed as choosing optimally a sequence of interface cards), thus ICM-US can potentially model and op-timize very complicated interaction interfaces, whereas the other works cannot optimize the design of an interface due to the restriction to mostly a ranking-based formulation of the retrieval problem. However, the specific techniques and models proposed in these existing work can all potentially contribute to further refinement of ICM-US to make it even more operational. For example, the economic IR work would enable ICM-US to incorporate a user decision model for re-fining the user actions (e.g., modeling how the user actions depend on the user state).

Our user action model characterizes how users make nav-igational decisions and stopping decisions in an information seeking process, which is related to Information Foraging theories [17], where models of user actions driven by infor-mation scent were proposed to describe how users navigate on the web following hyperlinks. Recent works in user search behavior analysis proposed and evaluated novel models for users X  navigation actions [25], stopping actions [14], and for characterizing users X  patience levels [16] in a search session. Our ICM-US framework is more focused on the  X  X rthogo-nal X  X uestion of how to optimize interactive search interfaces given a learned user action model; such line of research and the existing works in user action modeling and analysis serve to complement each other and collectively lead to more ef-fective search systems for users.
In [26], the authors proposed the Interface Card model for optimizing interactive retrieval interfaces in a general setting as follows. Let t be the interaction lap under consideration, q be an interface card the system could issue to the user, a t +1  X  A ( q t ) be an action the user takes in the following lap as a response to q t , 1 and f t c be the constraint function for q . Let c t be the context accumulated till the current user action that starts from c 0 = i , the prior information the system has about the user, and is incrementally updated by the rule c t +1 = ( c t , q t , a t +1 ). Let p ( a t +1 | c action model for characterizing how likely the user issues action a t +1 given context c t and card q t . Let u t be the estimated user surplus, which equals the difference between the user X  X  reward and their cost for issuing an action. Then the Interface Card optimization problem is defined as: Definition 3.1 (Interface Card Optimization) . In each lap t , the interface system should play a card q t that maximizes the expected surplus u t given the current context and under the current constraint, where the expectation is taken with respect to the user action model:
Instead of directly extending Equation (1) as in [26], we first introduce its intrinsic relation to sequential decision the-ories, and then redefine and expand the instantiations in [26] in a more systematic way and derive new interesting results. In contrast to the typical practices adopted by other recent works in applying sequential decision theories in informa-tion retrieval (e.g., [13]), we are not deriving our framework based on sequential decision theories; all our derivation is self-contained and solely relies on the Interface Card model and our own assumptions, and the formalisms in sequential decision theories we observe at the end are natural conse-quences of the derivation.

We first relate the interface optimizations in consecutive laps using the notion of context surplus: Definition 3.2 (Context Surplus) . The context surplus is the maximum expected surplus across all possible cards q t subject to the constraint under the given context: subject to f t c ( q t )  X  0.
 Assumption 3.1 (Accumulative Surplus) . The action sur-plus u ( a t +1 | c t , q t ) takes the form of an arithmetic sum:
A s in [26], we assume the action set A ( q t ) is countable; the case of uncountable action set could be handled via trivial changes to the model. The two components are: (a) u 0 ( c t , q t , a t +1 ) -the immedi-ate action surplus of action a t +1 given card q t and under context c t , which is the difference between the immediate action reward r 0 ( c t , q t , a t +1 ) and the immediate action cost s next lap.

Conceptually, we assume that the user obtains surplus in an accumulative fashion: it is usually reasonable in real world cases, which is also the reason why it became a stan-dard practice in reward modeling in sequential decision the-ories. However, there might also be cases where the user surplus takes a non-additive form. For example, certain fu-ture reward could be conditioned on some particular card in advance, e.g. some instructive cards at the beginning to help the user better understand the interactive interface. In such cases, our assumption would become invalid and we would need to step back to the more general form of the original Interface Card model; we leave it to future work.

There will always be a terminal lap in an interactive re-trieval process, e.g. when the user fulfilled their information need or they could not find anything interesting and give up the interaction. To make our discussion more concise and modular, for the moment we assure that our proposed for-mulation could naturally characterize the case of terminal laps, and we will come back to this in Section 4.

In contrast to the diminishing reward model adopted in a large portion of sequential decision theories, where the fu-ture reward is multiplied by a discount factor  X  &lt; 1, we do not penalize future reward in the current most general form of our framework. Nevertheless, we will show in Section 4 that the diminishing reward model could be derived as a spe-cialization of our framework to capture the user X  X  stopping actions in the interaction process, which gives the diminish-ing reward model a much deeper and more principled reason of existence.
 With Equation (2) and (3), we rewrite Equation (1) as: Definition 3.3 (Interface Card Bellman Equation) . The expected surplus in consecutive laps satisfies: and the optimal card the system should pick in lap t is: with q t in both equations subject to f t c ( q t )  X  0.
It is often convenient to define: and transform Equation (4) to an equivalent form:
Definition 3.3 forms the basis for all subsequent deriva-tions in this paper. Due to space limitations, we will only write out derivations for Equation (4) or its equivalent form Equation (7) in most places; the part for Equation (5) could be trivially filled out in each case.

Now we propose a fundamental assumption underlying all our following derivations regarding the modeling of the user: Assumption 3.2 (User State) . In each lap t , the user lies in a unique, unambiguous user state z t that contains all the necessary information to determine the user action model and the immediate action surplus when given any interface card the system issues.

The user state is a general concept encapsulating many real world cases. For example, the user X  X  information need could be one straightforward type of user state, and there could also be more subtle user states such as binary states indicating whether the user is in the exploration or exploita-tion stage of their information seeking process; it is at the will of the practitioners of our framework to decide on the type of user states they would like to model. We will be define the user state z t in various forms in the following sections to fulfill diversified modeling needs.

The user state z t is closely related to the context c t in that they both characterize about the user for the system, but they are intrinsically different. There are two key in-sights we could obtain if we look back at Equation (4): (a) the user action model and the immediate action surplus are essentially all we need to fully carry out the computations in our framework; (b) it was the context that determined the user action model and the immediate action surplus when given the interface card, but its role is now fulfilled by the user state. More fundamentally, the context is always ex-plicit to the system and contains all the information the system has at hand to speculate about the user, whereas the user state is intrinsic to the user and may or may not be known to the system, yet it is the essential information the system ever needs to know about the user. In other words, the user action model and the immediate action surplus are independent of the context given the user state -the user state serves as the sole linkage between the context and all the computational parts of our framework.

We first assume the user state is hidden from the interface system, and the interface system could only guess about the user state relying on the context.
 Assumption 3.3 (Initial User State Distribution) . There exists an initial probability distribution over the user states, denoted by d 0 , specifying how likely the user is in each user state at lap t = 0 that the system could estimate from the prior information i about the user, i.e. the initial context c . We write p ( z 0 | d 0 ) = p ( z 0 | i ) = p ( z 0 | c 0
In cases where there is no prior information about the user, the system could naturally assume a uniform distribu-tion over all possible user states.

When the interaction starts, we assume that the user states may probabilistically transition from lap to lap when the user observes an interface card: Assumption 3.4 (User State Transition Function) . In each lap t , There exists a transition function p t T ( z t +1 specifies the probability of the user transitioning from z z t +1 when the user observes q t .
Now, starting from p ( z 0 | d 0 ) = p ( z 0 | c 0 ), we could esti-mate the user state distribution d t inductively on t based on Bayes X  rule: where  X   X  1 = p ( a t +1 | d t , q t ) is computed by:
The context c t now provides no useful information addi-tional to our estimated user state distribution d t for compu-tational purposes, so it could essentially be replaced by d in all places. If we define u 0 ( d t , q t , a t +1 ) as: then Equation (4) becomes: Definition 3.4 (Interface Card Bellman Equation (Par-tially Observable User States)) .

E ( u t | d t ) = max subject to f t c ( q t )  X  0.
 Equation (11) takes the exact same form as the standard Bellman Equation for Partially Observable Markov Decision Process (POMDP), where the user state z t , the user state distribution d t , the interface card q t and the user action a respectively play the roles of state, belief state, action and observation (or often called evidence), and u 0 ( d t , q and E ( u t | d t ) respectively serve as the the reward function and the value function for belief states [20]; Equation (8) is the standard forward equation used for updating the belief state based on the action and the observation. In the lan-guage of POMDP, the state the interface system is interested in is the user state, and the actions the system could take are the interface cards it could issue to the user. However, since the user state is not fully observable, the system could only decide on the optimal action to perform according to the estimated distribution of user states, i.e. the belief state, and the estimation is computed based on the observations the system could collect about the user state, which are not surprisingly the user actions. We will see a concrete example of realizing Equation (11) to solve an interface optimization problem in Section 6.

Though the user states would usually be invisible to the system, we could often assume that the system is actually
T he reward function is defined on state-action-observation triples instead of on state-action pairs or just states as done in many studies which are special cases of our definition. aware of the user states for the sake of modeling conve-nience, either by explicitly asking or confirming about the user states, or by interpreting that the user actions (e.g. queries or clicks) have exact mappings to user states. In such cases, the belief state d t is collapsed into an observable user state z t . With a series of simplifications (omitted here due to space limitations), Equation (11) becomes: Definition 3.5 (Interface Card Bellman Equation (Fully Observable User States)) .

E ( u t | z t ) = max subject to f t c ( q t )  X  0.
 Equation (12) takes the exact same form as the standard Bellman Equation for Markov Decision Process (MDP), which is not a surprise since MDP could be derived as a special case of POMDP if observations uniquely determine states. We will see an example of realizing Equation (12) to solve a concrete interface optimization problem in Section 5.
Definition 3.3 and its instantiations in the forms of Equa-tion (11) and (12) have opened up enormous opportunities for research studies to apply the Interface Card model to a very wide range of real world problems via tools from se-quential decision and reinforcement learning theories. In this work, as an initial step, we will utilize our new framework to study a basic yet nontrivial aspect of interactive informa-tion retrieval -stopping actions. We claimed in Section 3 that our framework could naturally characterize such cases, and we now establish the formalism, starting by making the following assumption: Assumption 4.1 (User Stops Interaction) . The interactive process is always ended by the user -the system would al-ways respond to user actions with optimized cards whereas the user may or may not choose to continue the interaction.
Our assumption applies to most real world scenarios where the user may choose to stop the interaction either because of satisfaction of their information need or frustration due to lack of useful information. There might occasionally be cases where the system appears to be the one terminating the interaction, e.g. if the system determines that nothing in its database is interesting to the user and chooses to issue a  X  X erminal card X  that attempts to stop the interaction with some possible explanations. In such cases, the user is still free to choose between leaving the system and starting an-other round of interaction, and indeed many would choose the latter, so we could also model the termination as the user X  X  choice when facing the system X  X   X  X erminal card X . Definition 4.1 (Stopping Action) . In each lap t , there is a stopping action a t +1 B  X  A ( q t ) for any interface card q and its estimated probability under the current context c ture surplus of the the new context following the stopping action E ( u t +1 | c t +1 ) = 0, where c t +1 = ( c t , q
In the language of sequential decision theories, observing a user stopping action is analogous to entering a terminal state (or belief state for POMDP) for the system.

The stopping rate is typically dependent on the interface card the user faces: e.g., it would be smaller if the user is interested in certain content on the card, and larger if the content on the card does not look appealing to the user for the past couple of laps. However, for the sake of modeling and inference convenience, we may sometimes assume that the stopping rate is constant: Assumption 4.2 (Constant Stopping Rate) . The stopping rate is a constant determined by the prior information c 0 i of the user and is independent of the past interactions and the interface card; we denote it by:  X  0 = p ( a t +1 p ( a t +1 B | c t , q t ),  X  c t , q t , and we restrict that 0 &lt;  X 
Assumption 4.2 is a X  X ouble-sided sword X : in the language of bias-variance trade-off, it leads to a potentially large bias in exchange for less variance. In this work, we will be cau-tious and make use of it only to our own advantage: we will keep it for the rest of this section to derive one interesting theoretical result and continue to rely on it in Section 5 to simplify the computation, but we will discard it and resume the dependency of the stopping rate on the full context and the interface card in Section 6 for more realistic scenarios.
Now, with Assumption 4.2 in effect, if we define the user continuation action model as: for all a t +1 6 = a t +1 B , then Equation (7) becomes: Definition 4.2 (Interface Card Bellman Equation (Con-stant Stopping Rate)) .

E ( u t | c t ) = max subject to f t c ( q t )  X  0.

We could further extend Equation (14) to derive its in-stantiated forms for the MDP and POMDP cases based on Equation (12) and (11); we omit such derivations here due to space limitations, but we will be developing concrete models as examples of these two forms in Section 5 and 6, respec-tively.

More interestingly, the term 1  X   X  0 in Equation (14) clearly resembles the discount factor  X  in a form of the standard Bellman Equation for MDP and POMDP frequently seen in sequential decision theories (as well as many recent works in applying sequential decision theories in information re-trieval, e.g., [13]) for modeling diminishing reward, and the role of the (belief) state transition probabilities is here ful-filled by the user continuation action model. The resem-blance is not a coincidence -it reveals a fundamental in-sight into the diminishing reward model in sequential deci-sion theories. Traditionally, in addition to establishing an upper bound for the value function for mathematical conve-niences, a major purpose for setting the discount factor is to express to the model our intuitive preference for quicker reward; in our case, we want to reduce the cost of excessive interaction laps for the user. In economic theories, every cost is intrinsically an opportunity cost , which is formally defined to be the value of the best alternative [7]. In the setting of interactive retrieval, when the opportunity cost is higher than the expected surplus of carrying on the in-teraction longer, the user is very likely to switch to their best alternative way of spending their time, e.g. working on something else or simply relaxing, and from the system X  X  perspective, this is exactly the stopping action of the user. Therefore, our model for user stopping actions turns out to draw fundamental connections from economic theories to se-quential decision theories by providing deeper explanations to the diminishing reward model. We could look even fur-ther into discrete choice models [15] in modern economic studies to derive more rigorous formalisms for user decision modeling, and we leave it to future works.
As the first concrete extension of our proposed framework, we revisit the  X  X lain card X  setting in [26]. We augment it with stopping actions, and formally re-define the optimiza-tion problem based on our new sequential decision formula-tion for Markov Decision Process (MDP) defined in Equa-tion (12). We will mathematically prove that a more gener-alized ranking principle with user stopping tendencies taken into consideration is the solution to the Bellman Equation.
In the  X  X lain card X  setting in [26], the main assumption is that each interface card q t is an atomic choice e t placed on a ranked list and that the user examines and either ac-cepts or rejects the choices in a sequential manner. Now, we additionally allow that the user may also stop after exam-ining each choice. We continue to use p ( e t ), r ( e t ), s ( e respectively denote the probability of the user X  X  interest in accepting e t , the immediate reward of accepting e t and the immediate cost of examining e t in lap t , and these quantities are assumed to be independent of the past user actions as long as they have all been reject actions. An accept action is regarded as a  X  X atisfying X  stopping action -it terminates the current interaction session and starts a new one with an updated set of parameters. We now formally define this framework in the MDP language, starting with defining the user state and the user action model: Definition 5.1 (Plain User State) . The plain user state z at lap t is defined to be the set of choices not yet examined by the user till the previous lap: z t = { e : q t  X  6 = e,  X  t Definition 5.2 (Plain User Action Model) . The plain user action model for user state z t and choice e t is a distribution over the accept action a t +1 0 , the reject action a t +1 stopping action a t +1 B defined under the constant stopping rate assumption as: where 1 z t ( e t ) is the indicator function for testing whether e  X  z t , and we assume p ( e t )  X  1  X   X  0 ,  X  e t .
Essentially, the plain user action model claims that the user will always reject a choice they have rejected before (as long as no accept action has taken place in the middle), which is very reasonable in real world situations.
For technical conveniences, we define the c hoice surplus of set of all possible choices is finite, so the cardinality of z || z t || , is also finite; (b) if the choice shown on the interface is the only element left in z t , then the user X  X  reject action is regarded as a  X  X rustrating X  stopping action; (c) s ( e ) &gt; 0,  X  e , and (d) u ( e )  X  0,  X  e . Now, we could mathematically solve Equation (12) in closed form: Theorem 5.1 (Optimal Plain Card) . Let z t 6 =  X  . Define: Suppose z t = { e j } n j =1 = { e 1 , e 2 , . . . , e n } and e Then e 1 is an optimal card for z t and: Further, the complete optimization solution is to sequen-tially show e  X  z t to the user in descending order of  X  ( e ). Proof. We prove by induction on || z t || : 1. || z t || = 1. Let z t = { e 1 } , then: 2. Suppose n &gt; 1 and the theorem holds for all z s.t.
The definition for  X  ( e ) in Equation (16) is identical to that of  X  ( e ) in [26] except an additional multiplier involving  X   X  ( e ) = p ( e )
If  X  0  X  0, i.e. when the user never abandons the search, then the two forms become equal. In the more general case where  X  0 &gt; 0, our new form of  X  ( e ) may lead to a dif-ferent ranking result where some choices with larger p ( e ) values may be promoted because of the additional multi-plier. Intuitively, when the user has a high tendency to stop, they would examine less choices on average, so by promoting choices with larger p ( e ) values to higher places, the system could hope for a better chance of an accept action, and thus at least some reward, before the user leaves. Therefore, given a proper  X  0 value (e.g. learned from user interaction logs), our new ranking principle defined by  X  ( e ) could enable rank-ing with user stopping tendencies taken into consideration and form a basis for novel ranking algorithms on top of a richer user model.
In this section, we revisit the X  X avigational card X  X etting in [26] and formally re-define and solve the optimization prob-lem based on our sequential decision formulation for Par-tially Observable Markov Decision Process (POMDP) de-fined in Equation (11). We will again incorporate stopping actions, but we will discard the constant stopping rate as-sumption and resume the more generalized and realistic set-ting: the stopping rate depends on the card and the full context. We will demonstrate that our new formalism leads to automatic interface adjustment based on users X  stopping tendencies in addition to the context and the screen size.
The  X  X avigational card X  setting in [26] assumed the inter-face is backed by a set of information items denoted again by e (e.g. websites), each associated with some related tags (e.g. topics of websites), and the items and tags are respectively represented on the interface by item and tag blocks denoted by b . A card q t could contain any combination of item blocks and/or tag blocks, as long as the total area they occupy does not exceed the screen area: f t c ( q t ) = P b  X  q t w ( b )  X  1  X  0, where w ( b ) represents the space block b occupies relative to the entire screen size; the system may freely determine the layout of the interface by increasing or decreasing the number of item / tag blocks to display. Facing such a card q , the user may either select a displayed block or issue the  X  X ext card X  X ction a t +1 N if nothing on the card interests them, the probabilities of which follow an item action model that depends on the item the user is interested in. The interface system estimates the user action model as an expectation of the item action model taken with respect to the estimated probability distribution of the user X  X  interest in each item based on the latest context.

In this work, we incorporate the stopping action a t +1 B all action set A ( q t ). If the user finds an interesting item block and selects it, we consider it as a  X  X atisfying X  stopping action which triggers the interface system to jump to the item X  X  corresponding page. We define the user interest as a hidden user state and assume it doesn X  X  change across laps: Definition 6.1 (User Interest State) . A user interest state z denotes the user X  X  interested item e t in lap t : z t = e Assumption 6.1 ( Persistent User Interest) . The user in-terest state does not change across laps: p ( e t +1 | e t if e t +1 = e t , 0 otherwise. From now on, we use z 0 = e denote the user interest state.

Essentially, Definition 6.1 implicitly assumes that the user X  X  interest is focused on only one item within each lap, and As-sumption 6.1 extends it to the whole interaction process. Both these two parts are sometimes inaccurate in reflecting real world scenarios, but they serve to exponentially reduce the complexity of our optimization problem.

With our simplification assumptions, Equation (8) and (9) respectively reduce to:
In order to make the computations more tractable, we follow [26] and assume a constant immediate action cost s a finite item set E = { e 1 , e 2 , . . . , e n } , and assume that we don X  X  have any useful prior information so we start with a flat belief state d 0 : p ( e j | d 0 ) = 1 /n, 1  X  j  X  n . We further make the following two common assumptions before separating into the two experiment sections: Assumption 6.2 (Uniform Item Reward) . The reward to the user for selecting any item block in any lap is the same and is denoted by r 0 .

Instead of modeling the actual expected reward of select-ing an item block, r 0 could be regarded as measuring an eventual success in the interactive navigation, i.e. as the value of locating any interesting item to the user versus not finding anything at all.
 Assumption 6.3 (Simple User Interest Action Model) . Let v ( e, b ) be a measure of the intrinsic relation 3 between item e and block b . Let  X   X  0, 0 &lt;  X  &lt; 1. Given a user interest state e 0 and an interface card q t , the user issues an action based on the following simple user interest action model : 1. If the item block for e 0 , b 0 e , is on q t (i.e. b 0 2. Otherwise, if q t contains at least one tag block related 3. Otherwise, the user either selects  X  X ext card X  or stops: The uniform user interest action model denotes the simple user interest action model where each v ( e, b ) is either 1 or 0.
Conceptually,  X  captures the chance the user misses to identify a related tag, and  X  captures the user X  X  stopping tendency given they have not identified any interesting block -either they missed one or there wasn X  X  any indeed. Despite the fact that  X  is treated as a constant, we are not assuming a constant stopping rate; the stopping rate is depending both
P lease refer to [26] for a detailed explanation. on the card q t and on the belief state d t . With sufficient user interaction log data, we may apply reinforcement learning algorithms to learn a more refined user action model in the real world. In this study, we assume the simple user interest action model to reduce the learning complication and focus more on the modeling part.

To apply our model in real problems, the only missing component now is the actual planning in the POMDP frame-work, and it is well known that planning in POMDP is NP-hard [20]. We reduce the planning problem to manageable forms via imposing additional assumptions on the cards in Section 6.1 and via employing planning heuristics in Section 6.2, and we leave explorations of general planning solutions to future work.
In this section, we directly use the standard value itera-tion algorithm for POMDP in sequential decision theories [20] to explicitly solve our interface optimization problem defined in Equation (11). We assume the user follows the uniform user interest action model, and we set  X  = 0: the user never misses a related tag. Similar to [26], we also as-sume a complete tag set:  X  item subset E  X   X  E ,  X  block b s.t. v ( e, b ) = 1 E  X  ( e ), i.e. 1 if e  X  E  X  , 0 otherwise. In addition, we make the following assumption on the card the interface system could issue in order to reduce to a linear belief state space: Assumption 6.1.1 (Exclusive Blocks) . Any item e is re-lated to at most one block on any card q t :  X  e ,  X  at most one b  X  q t s.t. v ( e, b ) = 1.

Intuitively, this is a reasonable strategy for the system especially given that the system has access to a complete tag set: showing multiple blocks related to some item would not only confuse the user, but the system in turn would also be less precise in narrowing down into the item the user is truly interested in, ending up unnecessarily increasing the number of interaction laps.

More interestingly, this assumption guarantees that the belief state d t is always a uniform distribution over a subset of the items, which could be trivially reasoned via induc-tion (omitted here due to space limitations). Furthermore, due to the assumptions of uniform item reward and com-plete tag set, all belief states d t over the same number of items would appear identical to the system in terms of plan-ning: any optimal policy for a belief state d t over e.g. items { e 1 , e 2 , e 3 } is completely reflective to one for a belief state d t over items { e 4 , e 5 , e 6 } or any other item subset of size 3. Therefore, the belief state space has now been reduced from a high-dimensional continuous space all the way to a one-dimensional discrete space of size n . From now on, if d is over a set of n items, we say the size of d t , || d t
We conducted simulation experiments to employ the stan-dard value iteration algorithm to solve the Bellman Equa-tion via dynamic programming. The experiments were per-formed for two screen sizes: a medium size (M) holding at most two item blocks or four tag blocks, and a small size (S) holding at most one item block or two tag blocks. We tested two values for  X  : 0 . 02 for simulating a relatively patient (P) user, and 0 . 2 for a relatively impatient (I) user. In total, we had four settings abbreviated as  X  X P X ,  X  X I X ,  X  X P X , and  X  X I X . We found in our experiment runs that varying s 0 and r 0 within reasonable ranges did not affect the experiment S I SP MI
MP o utcome in any fundamental way, and here we report the results we obtained when we set s 0 = 1 and r 0 = 10.
Figure 1 shows the value function of belief states as a func-tion of their size. For all four settings, the value function decreases as the uncertainty of the belief state, i.e. their size, increases, and the decreasing rates are all diminishing, im-plying that the interface system could reduce the belief state size in an exponential manner through interacting with the user. It is also clear that the value function is higher for medium screens than for small screens, and higher for pa-tient users than for impatient users: a larger screen naturally helps the user navigate to their interested items in less laps, and a more patient user is more likely to stick to the inter-action until they obtain the reward. Further, the difference between the value function for patient and impatient users is much smaller on the medium screen compared to that on the small screen, suggesting that a larger screen is helpful in attracting the user to stick to the interaction by showing a wide variety of blocks to cater for the user X  X  interest.
Figure 2 shows the optimal policy our model determined for each belief state size in terms of the interface layout. The interface is decided to be full of item blocks when the uncer-tainty of the belief state is low, and automatically changes to a combination of item and tag blocks and further to solely tag blocks as the uncertainty increases, which is a conse-quence of the tag blocks X  advantage in more quickly nar-rowing down the belief state size using less screen space as compared to the item blocks. It is not possible to display both item and tag blocks in the small screen, so the lay-out directly  X  X umps X  from all item blocks to all tag blocks, and the  X  X ump X  also takes place when the belief state size is relatively small as compared to the case on medium screen, reflecting the more urgent need for more space-efficient tag blocks on smaller screens. More interestingly, the layout is also automatically adaptive to user stopping tendencies: when the user is less patient, our model intelligently ad-justs the layout to show more tag blocks on both small and medium screens -with the hope of a better chance to hold the user onto the interaction when they see a tag related to their interest. Therefore, our proposed novel formulation of the Interface Card model in the language of sequential de-cision theories successfully led to automatic interface layout optimization results which could not only adapt to user in-terest and screen size, but also better cater for both patient and impatient users.
In this section, we apply our theoretical framework to solve interface optimization problems in real world settings. The assumptions we made in Section 6.1 would hardly exist in the real world (e.g. the complete tag set assumption), so we are again facing a general NP-Hard POMDP planning problem. We continue to assume that the user follows the uniform user interest action model, but in contrast to Sec-tion 6.1, we now allow  X  &gt; 0, which permits the possibility of the user missing to identify a related tag as is often the case in the real world.

Instead of using other sophisticated planning algorithms, we employ a straightforward and widely adopted heuristic in sequential decision theories, the dual-mode control heuristic [4], which picks actions (i.e. interface cards in our case) that lead to a minimal expected entropy value of the belief state. We slightly modify the heuristic to accommodate user stopping actions: Definition 6.2 (Entropy Heuristic (with stopping actions)) . subject to f t c ( q t )  X  0. ( H ( d t +1 ) denotes the entropy of d t +1 and 1 B ( a t +1 ) is shorthand for the indicator function of testing whether a t +1 = a t +1 B .)
The additional term 1 B ( a t +1 ) r 0 we put into the dual-mode control heuristic measures the eventual reward of find-ing an interesting item if the user abandons the search (which we assumed to be uniform for all items and denoted by r 0
In order to assess the effectiveness of our proposed model in automatically optimizing the interface layout of real in-teractive retrieval systems, we built prototype interface sys-tems similar to those used in [26]: we fetched popular news articles (as items) from the New York Times Most Popular API 4 together with their associated keywords (as tags), and we used Amazon Mechanical Turk (AMT) 5 to conduct user studies. We employed a straightforward randomized algo-rithm similar to the one used in [26] to select the optimal interface card in each lap, but based on our new objective function defined in Equation (25). On the user side, we ran-domly partitioned the AMT workers into two groups, one being encouraged to stick to the interaction and thus play-ing the role of patient users, and the other being encouraged to freely give up the interaction and thus playing the role of impatient users; we refer to these two groups respectively as h ttp://developer.nytimes.com/ http://www.mturk.com patient (P) and impatient (I) users. On the interface side, w e varied the screen size and developed two sets of inter-faces, one for a medium sized screen (M) being able to hold at most two item blocks or eight tag blocks, and the other for a very small screen (S) being able to hold at most one item block or four tag blocks. We again have four settings in total:  X  X P X ,  X  X I X ,  X  X P X , and  X  X I X .

We built two types of interfaces for each of the four set-tings for comparison. The first type is the baseline inter-face built based on the Interface Card Model (ICM) [26] without user stopping tendencies taken into consideration -it is essentially always assuming a  X  X erfectly patient X  user. The second type is based on our new Interface Card Model with User States (ICM-US) with user stopping tendencies being considered, and it employs a straightforward learning method to infer users X  stopping tendencies. More specifi-cally, in the uniform user interest action model, if we treat  X  as the only free variable we would make inference about and all other parameters as given, then the Maximum Likelihood Estimate (MLE) of  X  is:  X  where  X #() X  denotes the number of occurrences of the en-closed action in the interaction log. In our experiment, we estimated that  X  MLE = 0 . 029 and 0 . 145 respectively for the group of patient and impatient users. Ideally, we could learn  X 
MLE for each individual user, but due to the limited amount of log data we could obtain, we decided to learn its value for each user group collectively. We noticed that our estimated  X 
MLE value for both user groups differed within  X  8% across the two screen sizes, implying that our uniform user inter-est action model is an adequately reasonable assumption for real world users.

We measure the effectiveness of the two types of interfaces using two metrics: (a) whether the users end up success-fully finding an interesting article or not, referred as  X  suc-cess?  X , and (b) how many laps the user spent for reaching an interesting article, referred as  X  #lap  X . We use one-sided McNemar X  X  test for comparing  X  X uccess? X  and one-sided Wilcoxon sign-ranked test for comparing  X #lap X . Table 1 shows the significance levels of our comparison tests for all four settings, where we adopt the convention in R 6 and use  X   X ,  X   X   X  and  X   X  X  X   X  to represent a p-value within the range of clearly observed that our new interface is significantly bet-ter than the baseline interface at helping impatient users navigate to an interesting article without significantly in-creasing the number of laps they needed; the differences for patient users are not significant, which is also expected be-cause the two types of interfaces would generate very similar optimization results due to a very small  X  value used in our new interface.
 Table 1: Significance levels of comparison tests.
 h ttps://www.r-project.org/ Figure 3: ICM interface after clicking  X  X olleges and Universities X .
 Figure 4: ICM-US interface after clicking  X  X olleges a nd Universities X .

Interestingly, the differences in  X  X uccess? X  and  X #lap X  are b oth less significant on the very small screen than on the medium sized screen for impatient users, contradicting to our expectation that our new model should benefit smaller screens more which are in nature less effective in keeping users engaged with the system. We speculate that, on the very small screen, both the baseline and the new interface would decide that the very limited screen space is more suit-able for the  X  X eyword layout X  in most of the laps where the system is still very uncertain about the user X  X  interest, and the difference only occurs after the system could narrow down the user X  X  interest to within a few articles (as was also observed in Figure 2).

To give better intuitions into the interface optimization outcomes of our proposed model, we show some example screens we observed when we used our interfaces ourselves. Figure 3 and 4 display the second screen we observed on the very small screen respectively in the baseline interface and in our new interface (fed with the  X  MLE value estimated from the group of impatient users), after we click the  X  X ol-leges and Universities X  keyword on the first screen (where both interfaces chose to display four popular keywords due to the very limited screen space). Given that there are three news articles associated with the clicked keyword, the base-line interface decided to adopt the  X  X rticle layout X  and dis-play the three articles one by one to the user (of which the first one is shown in Figure 3), with the hope that the user will thoroughly go over the three articles with the system. However, it is possible that an impatient user would stop if the first shown article does not interest them, and they would be even more likely to stop if they find the second shown article again not interesting. The new interface, on the other hand, considered user stopping tendencies in the optimization computation and thus chose to adopt the  X  X ey-word layout X  and display three keywords within the same screen each corresponding to one of the three articles. Even a n impatient user would likely identify a keyword related to their interest and continue on with the interaction by click-ing it, and they would immediately reach their interested article in the next screen. It is also possible that the user might fail to recognize a related keyword, but the system determined that this would be a minor risk and would not affect the higher overall benefit of attracting an impatient user to stick longer to the interaction and eventually find something interesting. Note that all the reasoning above is only for our purpose to appreciate the intelligent layout deci-sion of the interface system; the interface system is not built on top of any ad hoc logic to fulfill any part of our reasoning; its decision relies only on our optimization framework that elegantly captures all our intuitions in a single formulation.
Similar automatic layout decisions are observed on the medium sized screen as well, and the screen is even more capable in interface adaptations thanks to its additional in-termediate layout choice of devoting half the space to an article and half to keywords, but we do not show the ex-ample screens here due to space limitations. The results demonstrate the clear effectiveness of our proposed model in automatically optimizing interface layouts according to users X  stopping tendencies. We proposed a novel refinement of the Interface Card Model based on sequential decision theory, i.e. ICM-US, that can facilitate formal user modeling and naturally con-nect optimization of interactive retrieval with Markov De-cision Process and Reinforcement Learning (RL) in a gen-eral way, thus enabling the use of RL to solve potentially a wide range of problems of optimal interface design. ICM-US opens up many opportunities for formally modeling user be-havior in interactive retrieval as well as incorporating user behavior models into an optimal retrieval algorithm, making the retrieval algorithm  X  X ensitive X  to user behavior.
An obvious future direction is to further explore the large space of specific refinements of ICM-US and apply ICM-US to many applications to optimize interactive search and interface design. Another very interesting direction is to use ICM-US to analyze interactive search log data for discovery of interesting user behavior patterns or testing hypotheses about users X  search behavior. [1] G. Amati and C. J. Van Rijsbergen. Probabilistic [2] L. Azzopardi. Modelling interaction with economic [3] L. Azzopardi and G. Zuccon. An analysis of theories [4] A. Cassandra, L. Kaelbling, and J. Kurien. Acting [5] C. L. Clarke, M. Kolla, G. V. Cormack, [6] H. Fang and C. Zhai. An exploration of axiomatic [7] R. Frank and B. Bernanke. Principles of [8] N. Fuhr. A probability ranking principle for [9] D. Hiemstra and W. Kraaij. Twenty-one at TREC-7: [10] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [11] T.-Y. Liu. Learning to rank for information retrieval. [12] J. Luo, X. Dong, and H. Yang. Session search by direct [13] J. Luo, S. Zhang, and H. Yang. Win-win search: [14] D. Maxwell, L. Azzopardi, K. J  X  arvelin, and [15] D. McFadden and K. Train. Mixed MNL models for [16] A. Moffat and J. Zobel. Rank-biased precision for [17] P. Pirolli and S. Card. Information foraging. [18] J. M. Ponte and W. B. Croft. A language modeling [19] S. E. Robertson and K. S. Jones. Relevance weighting [20] S. Russell and P. Norvig. Artificial Intelligence: A [21] G. Salton, A. Wong, and C. S. Yang. A vector space [22] M. Sloan and J. Wang. Dynamic information retrieval: [23] M. D. Smucker and C. L. Clarke. Time-based [24] H. Turtle and W. B. Croft. Evaluation of an inference [25] W.-C. Wu, D. Kelly, and A. Sud. Using information [26] Y. Zhang and C. Zhai. Information retrieval as card
