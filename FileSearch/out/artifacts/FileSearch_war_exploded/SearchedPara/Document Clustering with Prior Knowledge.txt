 Document clustering is an important tool for text analysis and is used in many different applications. We propose to incorporate prior knowledge of cluster membership for doc-ument cluster analysis and develop a novel semi-supervised document clustering model. The method models a set of documents with weighted graph in which each document is represented as a vertex, and each edge connecting a pair of vertices is weighted with the similarity value of the two corresponding documents. The prior knowledge indicates pairs of documents that known to belong to the same clus-ter. Then, the prior knowledge is transformed into a set of constraints. The document clustering task is accom-plished by finding the best cuts of the graph under the constraints. We apply the model to the Normalized Cut method to demonstrate the idea and concept. Our experi-mental evaluations show that the proposed document clus-tering model reveals remarkable performance improvements with very limited training samples, and hence is a very ef-fective semi-supervised classification tool.
 H.3.3 [ Information Systems ]: Information Search and Re-trieval; I.5.3 [ Computing Methodologies ]: Clustering Algorithms, Experimentation, Theory Semi-supervisedlearning, Spectralclustering, Clusteringwith prior knowledge
Document clustering is the unsupervised classification of a given document set into clusters such that document within  X  This work was done when at NEC Labs America.
 Copyright 2006 ACM 1-59593-369-7/06/0008 ... $ 5.00. each cluster are more similar between each other than those in different clusters. As an important tool for exploratory dataanalysis, thedocumentclusteringisenablingtechniques for a wide range of information retrieval tasks such as effi-cient organization [16, 24], browsing [3, 6] and summariza-tion [14] of large volumes of text documents.

Document clustering generally assumes no training sam-ples from the user, and aims to partition an unlabeled sam-ple set into the user specified number of disjoint clusters through an unsupervised, exploratory process. Most clus-tering techniques define loss functions over all possible clus-ter assignment. The loss functions measure the degree to which the clustering goal is met. By optimizing certain loss functions, the optimal assignment should be achieved. The optimization problem is typically solved without considering any prior knowledge on cluster assignment of the data sets. However, in practical, there is usually some prior knowl-edge available for many data clustering problems. The prior knowledge includes, but not limited to, cluster assignment of some data instances and size of cluster. It comes either from the nature of the data sets or derived clues through intended exploration of data sets. The prior knowledge can be used to form constraints on the objective functions of cluster anal-ysis. By applying the constraints to the optimization prob-lem, prior knowledge of data clustering is incorporated into clustering methods to improve the clustering performance.
In this paper, we present a novel semi-supervised clus-tering model that incorporates prior knowledge about doc-uments X  membership for document cluster analysis. It can also be viewed as a technique that enables the user to con-trol the document clustering process by providing his/her prior knowledge specific to the target data set to be clus-tered. The prior knowledge is provided by indicating pairs of documents that are known to belong to the same cluster, and the number of such training samples can be arbitrary. The user X  X  prior knowledge is then enforced as a set of con-straints, and the document clustering task is accomplished by finding the cluster set that globally optimizes the con-strained cost function. The proposed clustering model can be applied to any spectral clustering methods, and in this paper, we apply the model to the Normalized Cut method, which has been shown to be the one of the best spectral clus-tering method [17], to demonstrate the idea and concept.
Our clustering model proposed in this paper can be con-sidered as a mechanism of incorporating the user X  X  super-vision into the unsupervised document clustering process, which results in a manageable mixture of supervised clas-sification and unsupervised clustering. When there is no prior knowledge about the target data set, the model equals the ordinary data clustering method. When the user pro-vides the prior knowledge, the model starts to show the characteristic of the supervised classification, which is the attempt to generate the cluster set that minimizes the er-rors on the training samples. The more training samples the user provides, the more resemblance the model bears to the supervised data classification. Our experimental evaluations have shown that this data clustering model is very effective for clustering document corpora with extreme distributions, and for performing data clustering with special users X  needs.
The remainder of the paper is organized as follows. We review previous work on various data clustering methods in Section 2. Section 3 presents our proposed model, which contains the weighted graph model, constraints, and con-strained spectral clustering model. In Section 4, we discuss details of our experiments. We conclude the paper in Section 5 and discuss several possible future research topics.
In this section, we first review problems associated with traditional data clustering methods, and then review related works on semi-supervised clustering/classification.
To date, almost every clustering method employs a cer-tain cost function to reflect the designer X  X  general knowledge about the internal structures and distributions of target data corpora. The data clustering task is accomplished by find-ing the cluster set that optimizes the predefined cost func-tion. For example, K-Means [8] uses the sum of squared distances between the data points and their corresponding cluster centers as the cost function, and partitions a data set into K clusters that locally minimizes this cost func-tions. Spectral clustering models the given data set using an affinity graph, and the data clustering task is accom-plished by finding the best cuts of the graph that optimize certain predefined cost functions. Many cost functions, such as the Ratio Cut [4], Average Association [17], K-Means [25], Normalized Cut [17], Min-Max Cut [5], etc, have been pro-posed, and the optimization of the cost functions usually leads to the computation of the top eigenvectors of certain graph affinity matrices. As data clustering results are de-rived by solving certain eigen-systems, they are guaranteed to be globally optimal with respect to the predefined cost functions.

The above data clustering methods, as well as most other methods in the literature, all make use of such cost functions that enforce the concept of maximizing the intra-cluster sim-ilarity, and/or minimizing the inter-cluster similarity [9]. This approach works well in general cases, but fails dras-tically when target data sets possess complex, extreme data distributions, and when the user has special needs for the data clustering task.

Figure 1(a) shows an artificially generated data set with an extreme distribution. The data set consists of a total of 100 data points which form three natural clusters: a circular ring with 80 data points, a group of 10 data points centered at (0 , 0), and another group of 10 data points centered at (25 , 0). Assume that the user wants to group the data set (b) Clustering into two clusters using Normalized Cut. Figure 1: An artificial data set and its clustering using Normalized Cut. into two clusters: the circular ring with the group of 10 data points inside it as the first cluster, and the remaining group of 10 data points outside the circular ring as the second cluster. If we apply the ordinary Normalized Cut data clus-tering method to this data set, we will obtain the two data clusters as shown in Figure 1 (b) where data points belong-ing to the same cluster are depicted using the same symbol. Obviously, the Normalized Cut method fails drastically by generating a completely unexpected clustering result.
The above artificial example brings about a very impor-tant question: For each given data set, there are always many possible ways of partitioning the data set. Which partition is the most appropriate one? The answer to this question depends on both the internal structure/distribution of the data set, and the user X  X  data clustering need. With traditional data clustering methods, certain cost functions are employed to specify the designer X  X  general knowledge about the internal structures and distributions of the target data sets, while no mechanisms are provided to reflect the user X  X  prior knowledge or special data clustering needs on particular data sets. This is one of the main causes that traditional methods are prone to generate unsatisfactory re-sults when target data sets possess complex distributions, and when users have special data clustering needs [12]. There have been research studies that modify traditional K-means clustering to incorporate labeled data. Wagstaff, et al. introduced two types of constraints: the  X  X ust-link X  and the  X  X annot-link X  constraints [19, 20], and their semi-supervised K-means produces data partitions by ensuring none of the user specified constraints are violated. Basu, et al. also developed a semi-supervised K-means that make use of labeled data to generate initial seed clusters, and to guide the clustering process [1].

Another work closely related to our proposed data cluster-ing model is the one from Yu and Shi, which performs image regionsegmentationsusingtheNormalizedCutmethodwith partial grouping constraints [23, 22]. The proposed method projects the original data to a feasible solution space based on constraints, and then generalizes Rayleigh-Ritz theorem [7] to get optimal solution in a relaxed continuous domain. In this new solution space, the constraints were strictly en-forced with no exceptions (i.e., X  X ard X  constraints). The per-formance evaluations were conducted by showing region seg-mentation results of a few images, and there is no quantita-tive measures for the performance improvement. Compared with their method, our proposed data clustering model in-troduces a penalty term with a penalty coefficient matrix to incorporate the user X  X  prior knowledge on the data set. This approach essentially imposes a set of  X  X oft X  constraints on the data clustering process, which enables the user to control the degree of enforcement of the prior knowledge. With the penalty coefficient matrix, the user is even able to enforce prior knowledge from multiple sources with dif-ferent strengths. Another advantage of our method is that it tolerates false constraints with errors because of its soft constraint nature.

Besidesthesemi-supervisedclustering, therearealsomany research works on semi-supervised classification in the lit-erature, such as semi-supervised EM [15], co-training [13], spectral graph transducer [11], Gaussian random field based semi-supervised learning [27], transductive SVM [2, 10], etc. Among these works, the transductive SVM extends the stan-dard SVM by attempting to find a boundary that has the maximum margin on both the labeled data and unlabeled data. It is reported that the transductive SVM outperforms the standard SVM by a large margin, and has become a benchmark for semi-supervised learning methods.
We present a document clustering model that enables the user to supervise/control the clustering process by provid-ing his/her prior knowledge specific to the target data set to be clustered. The user provides his/her prior knowledge by showing an arbitrary number of training samples each of which indicates a pair of documents which the user wishes to be grouped into the same cluster. Our clustering model encodes the user X  X  prior knowledge with a set of constraints to the cost function, and the document clustering task is car-ried out by solving a constrained optimization problem. The document clustering model can be applied to any spectral clustering methods, and in this paper we apply the model to the Normalized Cut method to demonstrate the basic idea and the concept.
Normalized Cut method, as well as other spectral cluster-ing methods, model the given document set using a undi-rected graph G ( V , E ,W )where V , E , W denote the vertex set, edge set, and graph affinity matrix, respectively. In graph G ,eachvertex V i  X  V represents a document vector, and each edge ( i,j )  X  E is assigned a weight w ij  X  W to reflect the similarity between the documents i and j .Let S i , S j denote two document clusters of the given data set V , and W ( S i , S j ) denote the sum of similarities between the two clusters S i and S j : The cost function employed by Normalized Cut is defined as: where K is the total number of data clusters. In Eq.(2), the numerator W ( S i ,  X  S i ) measures how tightly the clus-ter S i is connected with the rest of the data set, while the denominator W ( S i , V )measureshowcompacttheen-tire data set is. Given the target data set, the Normalized Cut method attempts to partition the data into the clusters that minimize this cost function. The strategy for finding the optimal solution to this cost function is as follows. Let N be the total number of data points in the data set V , X i =[ x 1 i ,x 2 i ,...,x Ni ] T be the indicator vector of the clus-ter S i in which each element x ki takes a binary value { to indicate if the k  X  X h data point in the data set V belongs to S i or not. It is easy to prove that the following identities hold true: where D is the diagonal matrix such that D  X  e = W  X  e , e = [1 , 1 ,..., 1] T . Using the above identities, the cost function F where Y i = D Then we have Y T Y = I .

By introducing the above cluster indicator vector X i ,we can turn the problem of finding the optimal clustering re-sult into the following optimization problem: Find the set of K indicator vectors [ X 1 ,X 2 ,...,X K ] with binary-valued elements that minimizes Eq.(7). Generally, solving this opti-mization problem has been proven to be NP-hard. However, if we relax all the elements of each indicator vector X i from binary values to real values, the above optimization problem can be easily solved under the constraint of Y T Y = I .As in [7], where  X  1 ,  X  X  X  , X  K are the largest K eigenvalues of the matrix D are the eigenvectors of the K largest eigenvalues.
The K eigenvectors Y 1 ,  X  X  X  ,Y K obtained above encode the cluster membership information for the given data set V . However, since these eigenvectors take real values for their elements, they do not directly indicate the cluster mem-bership for each data point. A common approach for de-riving the final cluster set is to project each data point into the eigen-space spanned by the above K eigenvectors (e.g., V i is represented by the i  X  X h row vector of the ma-trix Y =[ Y 1 ,Y 2 ,  X  X  X  ,Y K ]), and apply the K-mean algorithm within this eigen-space.
The proposed document clustering model enables the user tosupervise/controltheclusteringprocessbyprovidinghis/her prior knowledge specific to the target data set to be clus-tered. The prior knowledge is provided in the form of indi-cating several pairs of documents which the user wishes to be grouped into the same cluster. Assume that user has indi-cated that data points V i ,V j  X  V belong to the same cluster. To encode this prior knowledge, we introduce the constraint u ic =1, u jc =  X  1, and the rest of the elements all equal zero. The user can specify an arbitrary number of training pairs, and for each training pair, a constraint vector U c is intro-duced to encode the knowledge. Let U T =[ U 1 ,U 2 ,...,U C ] be the constraint matrix in which each column U c is a con-straint vector, and X =[ X 1 ,X 2 ,...,X K ] be the indicator matrix in which each column X i is the indicator vector for the cluster S i . Obviously To enforce the user X  X  prior knowledge, we add a penalty term to the original Normalized Cut cost function as follows: F where  X   X  0 is a control parameter that controls the degree of enforcement of the user X  X  prior knowledge. The larger value the parameter  X  takes, the stronger enforcement of the user X  X  prior knowledge we will have, and the stricter the data clustering result will obey to the user X  X  training samples; vise versa. In order to applying different control values for each constraint, the  X  can be defined as a diagonal matrix,  X  = diag (  X  1 , X  2 ,  X  X  X  , X  n ). Similar to the Normalized Cut method, we try to find the best data clustering result by computing the indicator matrix X =[ X 1 ,X 2 ,...,X K ] that minimizes the cost function F CNC .

Using the symbols Y i and Y defined in Section 3.1, we can rewrite Eq.(9) as follows: By replacing X i in Eq.(10) with Y i ,wehave F Similar to Eq.(7), if we relax Y i to take real values, the above timal solution. According to [7], the matrix D  X  is achieved when Y 1 ,...,Y K are the eigenvectors of these K smallest eigenvalues.

Once the K eigenvectors are obtained, we project all the data points in the data set V into the eigen-space spanned by the these K eigenvectors, and apply the K-mean algo-rithm within this eigen-space. The algorithm of applying our proposed data clustering model to the Normalized Cut method is summarized as follows: 1. Create the graph affinity matrix W =[ w ij ]inwhich 2. Compute the diagonal matrix D such that D  X  e = W  X  e , 3. Formtheconstraintmatrix U =[ U 1 ,U 2 ,  X  X  X  ,U C ]where 4. Form the matrix  X  W = D  X  5. Project each document V i into the eigen-space spanned
The computational cost of the above algorithm depends mainly on the total cost for computing the eigenvectors. The eigenvectors can be obtained by Lanczos method. Its com-putation cost is proportional to the number of nonzero ele-mentsoftheobjectmatrix W , nnz ( W ). Thus the cost of the algorithm is O ( kN Lanczos nnz (  X  W )), where k is the number of eigenvectors desired and N Lanczos denotes the number of Lanczos iteration steps.
In this section, we present the details of our performance evaluations, including the descriptions of the data sets, the evaluation metrics and the performance comparisons.
We have evaluated the performance of our document clus-tering model using two data sets: the Reuters-21578 and the Table 1: Statistics Reuters-21578 document corpus. 20 Newsgroups document corpora. The Reuters-21578 doc-ument corpus has been one of the most widely used date sets for document clustering/classification purposes. This doc-ument corpus contains 21578 documents which have been manually clustered into 135 clusters. Each document in the corpus has been assigned one or more labels indicating which topic/topics it belongs to. In our test, we discarded docu-ments with multiple category labels, and removed the clus-ters with less than 40 documents. This has lead to a data set that consists of 21 clusters with a total of 8631 documents. Table 1 provides the statistics of the Reuters document cor-pus.

The Newsgroupsdata set contains about 20000 documents that were collected from 20 newsgroups in the public do-main. The topics of these newsgroups are very diversified, ranging from computer graphics, automobiles, to religions, politics, etc. The number of news articles in each newsgroup is roughly the same.

We pre-processed each document by tokenizing the text into bag-of-words. Then, we applied downcasting, stop-words removing, and stemming to the words. Based on the processed words, a feature vector is constructed with TFIDF weighting and normalization for each document. To study the performance of our proposed constrained Normalized Cut model (abbreviated as CNC), we have com-pared the performance of the model against two popular un-supervised clustering methods of K-means and Normalized Cut (NC) as well as two representative semi-supervised clas-sification methods of Semi-supversed K-means (SK-means) [19]andTransductiveSVM(TSVM)[10]withReuters-21578 and 20 Newsgroups data sets. The goals of the empiri-cal evaluation include (1) testing whether the constrained Normalized Cut model can utilize the transductive setting by comparing it against unsupervised clustering methods; (2) comparing against existing semi-supervised classification methods. For comparisons against the unsupervised clus-tering methods, we use the normalized mutual information metric MI, which measures the similarity between two sets of data partitions (the generated partition and the actual classes of document labels).

Given the two sets of document clusters C , C , their mu-tual information metric MI( C , C ) is defined as: where p ( c i ), p ( c j ) denote the probabilities that a document randomly selected from the data set belongs to the clusters bility that this randomly selected document belongs to both clusters c i and c j at the same time. MI( C , C ) takes values between zero and max(H( C ) , H( C )), where H( C ) and H( are the entropies of C and C , respectively. It reaches the maximum max(H( C ) , H( C )) when the two sets of document clusters are identical, whereas it becomes zero when the two sets are completely independent. Another important char-acter of MI( C , C ) is that, for each c i  X  X  , it does not need to find the corresponding counterpart in C , and the value keeps the same for all kinds of permutations. To simplify comparisons between different pairs of cluster sets, instead of using MI( C , C ), we use the following normalized metric MI( C , C ) which takes values between zero and one:
For comparisons with the semi-supervised classification methods, we use the accuracy metric AC as the perfor-mance measures. The AC measures how accurate a learning method assigns labels to a data set compared to the ground truth, and therefore is more appropriate for evaluating clas-sification tasks. The accuracy of classification denotes the percentage of correctly classified test data out of the total number of test data, which is defined as below
We have tested the CNC model using both the Reuters and the Newsgroup document corpora, and compared its re-sults with the following four methods: (1) K-means, (2) Nor-malized Cut, (3) Semi-supervised K-means, and (4) Trans-ductive SVM. K-means and NC are the two most popular unsupervised data clustering methods, whereas SK-means and TSVM are the representative semi-supervised data clus-tering and classification methods, respectively. Through these comparison studies, we reveal the relative positions of our CNC model in comparisons with the representative clus-tering, semi-supervised clustering and classification meth-ods.

For comparisons with the K-means and NC, we conducted evaluations using different number of clusters ( k ) ranging from 2 to 6 without loss of generality. For each given clus-ter number k , 10 test runs were conducted, each on a test set created by mixing k topics randomly selected from the Reuters corpus. The final performance score was obtained by averaging the scores from 10 test runs. We also tested the CNC model with different amount of constraints. For each test run, the constraints were generated by pairing randomly selected documents that belong to the same topic, and the percentages of the randomly selected documents range from 2.5% to 7.5% of the total documents within the test set. Similar setting is also applied to the Newsgroups corpus. Table 2 and 3 show the normalized mutual information MI values on Reuters and Newsgroups, respectively. In the table, the first row (with the label  X  X -means X ) and the sec-ond row (with the label  X  X C X ) show the results generated by the traditional K-means and NC methods, respectively. The remaining rows show the results generated by the CNC model under different percentages of training samples. It is clear from the tables that NC always outperforms K-means, and that as long as the training samples are provided, CNC always outperforms NC with a large margin. The more con-straints to the clustering process, the more improvements to the clustering accuracy. If no constraint is available, CNC becomes equivalent to NC, and will generate the same doc-ument clustering accuracies as shown in the second row of the table. To further reveal how the constraints affect the document clustering performance, we computed MI in two different ways. The first batch of MI X  X  (from row 3 to row 5) were computed by treating the constraints as part of the entire test set, while the second batch of MI X  X  (from row 6 to row 8) were computed by excluding the constraints from the test set. In other words, MI X  X  from row 3 to row 5 indi-cate the document clustering accuracies of the CNC model on both the training and the test data, while MI X  X  from row 6 to row 8 show the document clustering accuracies on the test data only. As expected, MI X  X  computed from the test data only are slightly lower than those computed from both the training and the test data. Considering the small differ-ence of the two sets of MI, the CNC model has a very good generalization ability. Figure 2: The performance changes when  X  varies.
The coefficient  X  for the penalty term in Eq.(12) regulates the degree of enforcement of the user X  X  prior knowledge.  X  . When  X  changes from 5 to 30, the clustering results for 2 clusters are plotted in Figure 2. With the increase of  X  ,prior knowledge is enforced with stronger and stronger degree. Since all constraints are correct, the clustering performance keeps improved. Without loss of generality, we have set  X  to 20 in our experiments.

TorevealtheeffectivenessofCNCmodelasasemi-supervised classification method, we conducted performance compar-isons with the SK-Means and TSVM classifier as well. We adopted the linear SVM here because many previous re-search studies suggest that the linear SVM generates excel-lent document classification results on the Reuters as well as many other document corpora [21]. In most semi-supervised classificationsettings, theclassificationprocessusuallystarts with a handful number of training samples such as 2 or 9 [19, 10]. In our experiment, 2 . 5%, 5 . 0%, and 7 . 5% of the test data, which are equal to about 2  X  45 documents in most cases, were selected as training samples. All the three methods were then tested using the remaining documents in the test run. The results are listed in Table 4. We also calculated the p -values of the one-sided Wilcoxon signed-rank test [18] between the CNC and the TSVM as well as the CNC and SK-means. The one sided Wilcoxon signed-rank test is a nonparametric paired test without assuming the underline distribution of the tested values. The results with p -value smaller than 0 . 05 are considered statistically significant.

For the Newsgroups data set, we randomly select 100 data samples from each newsgroup and pair them together to form ten data sets each with 200 data samples. A certain percentage (2.5%, 5.0%, and 7.5%) of data, which are equal to 5, 10, and 15 data, were selected as training samples in our experiments. Table 5 presents the experimental results of the three semi-supervised learning methods based on 10 test runs. The Newsgroups data set contains more outliers than the Reuters data, thus clustering the newsgroups data is a more challenging task. The CNC outperforms TSVM with less than 15 training samples and outperforms SK-means with all of the three setting. The results are statistically significant.

Our finding can be summarized as follows: (1) As long as the constraints are provided, the CNC model always outper-forms the traditional K-means and NC methods. (2) The CNC model always outperforms the SK-means by a large margin. (3) When the number of training samples is very small, the CNC model significantly outperforms the TSVM ( p -value &lt; 0 . 05). When the number of training samples grows, (7.5% for Newsgroup, 10% for Reuters), CNC per-forms comparable with TSVM.
In this paper, we proposed a constrained spectral cluster-ing (CNC) method to incorporate user X  X  prior knowledge during the document cluster analysis. This method en-ables users to control the document clustering process to either achieve desired cluster structures or get accurate clus-ter results. We tested our algorithm using both Reuters-21578 and 20-newsgroup document corpora, and compared the results with both the unsupervised clustering and semi-supervised clustering/classification methods. The experi-mental evaluations reveal that our proposed CNC model is a very effective semi-supervised document clustering tool, especially with very low amount of training samples.
During the research work, we are inspired to further ex-tend the method in several directions. We would like to form constraints for prior knowledge related to cannot-link constraints. We hope to explore new approaches to utilize the prior knowledge more efficiently to achieve even higher performance improvement with very few constraints. It is also interesting to investigate different objective functions to incorporate user X  X  constraints.
The authors want to thank Mei Han for her valuable com-ments, Yihong Gong for his help on editing, and the review-ers for their constructive comments and suggestions. [1] S. Basu, A. Banerjee, and R. J. Mooney [2] K. P. Bennett, and A. Demiriz Semi-Supervised [3] D. Cutting, D. Karger, J. Pederson, and J. Tukey. A [4] P. K. Chan, D. F. Schlag, and J. Y. Zien. Spectral [5]C.Ding,X.He,H.Zha,M.Gu,andH.Simon.
 [6] D. Eichmann, M. Ruiz, and P. Srinivasan.
 [7] G. H. Golub and C. F. Van Loan. Matrix [8] J. Hartigan and M. Wong. A k-means clustering [9] A. K. Jain, M. N. Murty, and P. J. Flynn. Data [10] T. Joachims Transductive inference for text [11] T. Joachim. Transductive Learning via Spectral [12] R. S. Michalski and R. E. Stepp. Learning from [13] T. Mitchell. The role of unlabeled data in supervised [14] J.L.Neto, A.D.Santos, C.A.A. Kaestner, and A.A. [15] K. Nigam, R. Ghani, Analyzing the effectiveness and [16] S. Siersdorfer and S. Sizov Restrictive Clustering and [17] J. Shi and J. Malik. Normalized cuts and image [18] F. Wilcoxon. Individual comparisons by ranking [19] K. Wagstaff, S. Rogers, and S. Schroedl. Constrained [20] K. Wagstaff and C. Cardie. Clustering with [21] Y. Yang and X. Liu A re-examination of text [22] S. X. Yu and J. Shi Grouping with Bias. Neural [23] S. X. Yu and J. Shi Segmentation Given Partial [24]H.J.Zeng,Q.C.He,Z.Chen,W.Y.Ma,andJ.Ma [25] H. Zha, C. Ding, M. Gu and H. Simon. Spectral [26] http://svmlight.joachims.org/ [27] X. Zhu, Z. Ghahramani, J. Lafferty. Semi-Supervised
