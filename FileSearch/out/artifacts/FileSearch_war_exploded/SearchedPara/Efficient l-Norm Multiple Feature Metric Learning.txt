 Previous metric learning approaches are only able to learn the metric based on single concatenated multivariate feature representation. However, for ma ny real world problems with multiple feature representation such as image categorization, the model trained by previous approaches will degrade because of sparsity brought by significant dimension growth and uncontrolled influence from each feature channel. In this paper, we propose an efficient distance metric learning model which adapts Distance Metric Learning on multiple feature representations. The aim is to learn the Mahalanobis matrices for each independent feature and their non-sparse l p -norm weight coefficients simultaneously by maximizing the margin of the overall learned distance metric among the pairs from the same class and the distance of pairs fro m different classe s. We further extend this method to nonlinear kernel learning and category specific metric learning, which demonstrate the applicability of using many existing kernels for image data and exploring the hierarchical semantic structures for large scale image datasets. Experiments on various datasets demonstrate the promising power of our method. I.5.2 [ Pattern Recognition ]: Design Methodology  X  Classifier design and evaluation General Terms : Algorithms, Performance, Experimentation A good distance metric stands in the core for many learning methods, for example, the kernel methods and nearest neighbor method. A commonly used distance metric is the Euclidean distance, a choice which has both the advantages of simplicity and generality. Despite of these advantages, for many real world classification tasks, the Euclidean distance is not well adapted. Metric learning is an emerging area of statistical learning in which the goal is to learn a more powerful distance metric from a set of labeled samples or weakly labeled samples. With a learned distance metric, even the simplest lazy learning method such as k -NN can achieve good generalization power. In fact, significant improvements have been observed within several distinguished solutions for this problem, such as neighborhood components information theoretic metric learning [3]. For many real world applications, there will be multiple descriptions for a single data item. For example, Web image can be represented by shape feature, texture feature, color feature and textual feature extracted from the surrounding text. These features may have heterogeneous statistical characteristics. Previously, to make use of multiple features, a common way is concatenating all the features into one single feature vector. Then a classification model is obtained based on this concatenated representation. However, this will lead to explosive dimension increases, and the contribution of each original feature channel cannot be controlled. Moreover, the correlation among feat ures usually contains noisy information that may lead to unpredictable risk of model Mahalanobis matrix usually have O ( k 2 ) coefficients compared with O ( k ) for linear SVM, where k denotes the average dimension of each feature representation. Th erefore, the significant increase of computation as well as over-fitting brought by metric learning with concatenated feature are tw o major intrinsic weaknesses for applying previous metric learning approaches to application with multiple feature representation. Recently, Multiple Kernel Learning [1][10][11][12] has been proposed to solve the problem of learning with multiple features and kernel representations. The aim is to maximize the margin on the training data and approxima tely find sparse weights to calculate the similarity using several features and kernels. Kloft et al. [8] and Vishwanathan et al . [14] proposed l imposes arbitrary non-sparse l p norm on the kernel weight coefficients. Promising results and good generalization power have been reported by their studies. In this paper, borrowing the idea from l p -MKL [14], we try to learn an overall combined distance metric by simultaneously optimizing a set of distance metrics on each independent feature channel. Our model requires only O ( Mk 2 ) coefficients to optimize instead of O ( M metric learning with concatenated features, so the curse-of-dimensionality problem will be alleviated. We formulated our problem into the max-margin framework. The Frobenius norm and l p squared norm are imposed on each Mahalanobis matrix and the weight coefficients of feature channels respectively. By considering the cluttered distribution in feature space of the image data, we make two extensi ons of our method so that it is capable of real world image categorization. Firstly, the model can be easily kernelized as the inner product of each feature channel is preserved. Therefore, the existing image kernels are applicable to the specific features, such as spatial pyramid kernel and  X  kernel for visual bag-of-word feature. Secondly, we adapt our method to learn categorical specifi c metrics. In this paper, we study the capability of our localized metric learning method so as to provide a guidance to reach a good balance of computational complexity and performance for large scale image applications. In this paper, we propose an efficient multiple feature distance metric learning method by consid ering all the mentioned issues. We will introduce our work as follows: We describe our distance metric model in Section 2. Experiments are conducted and discussed in Section 3. In S ection 4 we summarize the paper. Suppose we are given a set of N training samples with class labels, denoted by {( , ) | 1,..., } ii ci N  X  x , where x sample and c i denotes its class label. For each x i , we calculate M data pair ( , ) ij xx , their original distance and learned distance metric with respect to different features are represented by d and  X  () , m ij d . In this paper, we define the overall distance metric and similarity between any data pairs as: Where A ( m ) denotes the Mahalanobis matrix for the m feature. The training data is repr esented by a set of triplets as: We introduce a non-negative weight coefficient b feature representation as well as a set of initial Mahalanobis minimization and [14], we introduce the following convex objective function as: of identity matrix, or it can be any Mahalanobis matrices learned on some other dataset. Following the similar derivation of [14], we obtain the dual problem as: min ( ) ( ) , . . 0 Where 1/ 1/ 1 pq  X  X  , 1 denotes a vector with all ones, and: sd d d d Q trx x xxxx xx For p &gt;1, b m has the following form: In this paper, A (m) is not guaranteed to be positive semi-definite. Although in most case we observe that there is no negative eigenvalues for A (m) , one can set the negative eigenvalues to be 0 to make A ( m ) to be positive semi-definite. In future study, we will investigate the positive semi-definiteness of the learned metric. The previous description can be easily kernelized by replacing all x by () x  X  , where  X  is the feature map corresponding to any given kernel. We denote the original kernel for each feature channel as () m K , then the learned kernel is given by:  X  (, ) (,) Kx x K x x b
K xx K xx K xx K xx Where ij  X  is calculated by reorganizing the support vector representation into the weighted combination of training pairs. Since a single metric is not robust to deal with real world image categorization with hundreds to thous ands of classes, a better way is to learn multiple metrics where a subset of classes share a single metric. We use an automatic hierarchical grouping method according to the overall average similarity on multiple feature representations among different cla sses. We calculate the average multiple feature representations on the images from each class, so that each semantic class will have an average image feature on the concatenated feature representation. We conduct hierarchical clustering on these feature repres entations. Then a hierarchical visual semantic structure can be obtained. We use different level of the hierarchical structure fro m this hierarchy so that each metric corresponds to different cat egory structures and different average number of classes. Since the dual problem (4) is di fferentiable with respect to  X  , there are many possible solutions th at can be used to solve this problem. We propose a special purpose solver based on the recent proposed coordinate gradient de scent method [7], which applies Newton-Raphson method to solve a one-variable sub-problem each time until convergence. Moreover, we implement two acceleration schemes proposed by [7], namely, random permutation of sub-problems and shrinking with gradient thresholding. Due to limited space, we omit the details of the optimization process. Readers may refer to our technical report. Our method requires that the data should be organized as pairs, it leads to O( N 2 ) scale in terms of training data size. However, as has been discussed in literatures such as [6], the neighborhood of each data is most influential to the model. Thus using this heuristic reduces the training data size to O( N ) scale. For the linear case in our method, we adopt Locality Sensitive Hashing [4] for approximated nearest neighbor s earch. For the kernel version, we adopt the KLSH [9]. We build KLSH for each feature channel, and the overall hash code for each image is the concatenation of the hash code vector from all feature channels. We conduct experiments on 3 differe nt image dataset, the Yale Face B face recognition dataset, the NUS-WIDE-OBJ [2] and the ImageNet-250 dataset [16] which is composed of 250 classes of images covering many common visu al concepts including various kinds of categories. Each image category in ImageNet-250 contains more than 500 images. For Yale Face B, We extract 4 types of image features on this dataset, the original gray level feature, the LBP feature on the or iginal image, the Gabor feature from 40 filters with different scales and orientations, and the Gabor LBP feature. For NUS-WIDE-OBJ, five image features are provided by [2] including color f eatures (CM, CH, CORR), edge histogram (EDH) and wavelet features (WT). We calculate 9 types of features and image kernels for ImageNet-250. Note that in linear cases, we reduce the dimensionality on each single feature with dimensionality higher than 300 by using PCA. The ratios of random training/testing split for the three datasets are 0.7/0.3, default and 0.6/0.4, respectivel y. We conduct evaluation on 10 different random splits for each dataset, and all the reported results are the averaging of all the data splits as done in [15]. We denote our linear and kernelized multiple feature metric learning method as MFDML-L a nd MFDML-K. We compare our methods with 6 baseline approach es using concatenated feature representation on the three datasets: (1) EUC: the Euclidean metric on the concatenated featur e. (2) EUC-PCA: the Euclidean metric on the concatenated feature representation after dimensional reduction us ing PCA. (3) ITML [3]. (4) LFDA [13]: localized FDA. (5) LMNN: La rge Margin Nearest Neighbor method [15]. (6) NCA [6]. We implement our code using Matlab. The computing devices include a desktop computer with Intel i5 760 CPU and 4G RAM and anothe r desktop with Intel Core2 E7500 and 4G RAM. Another important issue is the setting of model parameters. Although the optimal setting can be found by cross validation, we notice that the performance is not very sensitive to different We heuristically set p =2 for all the experiments as [14] shows that this setting provides good solution We adopt Mean Accuracy ( MA ) which records the average percent of correct predictions among all the classes. We use k -NN as the learning and predicting model. Suppose the number of returned nearest neighbors for each query is R N , we denote the number of images from each category in the retrieved nearest neighbors as a vector 1 [ ,..., ] C N N and R q multiple category specific metrics are learned for different subset of classes, we denote the task of learning each metric as T decision output of query x is calculated as follows: Where  X  , (, )| task, we use the task specific similarity  X  t K . In this part, we conduct singl e metric MFDML-L and MFDML-K since there is no semantic structure existing in the face data. The results of the performance on test data using with different number of nearest neighbors are demonstrated in Figure 1. We observe that for almost all the best recognition rate was achieved when the number of nearest neighbors is set to be 3. When the nearest number increases, the pe rformance will degrades possibly because the training data only contains less than 2K training images. A slightly different resu lt is observed in our approach MFDML-L, where the highest recognition rate is achieved when the neighborhood size is 7. The best performance of our method MFDML-K slightly outperforms the state-of-the-art LMNN. Another observation for Figure 1 is that the performance of our method decreases much slowly co mpared with other approaches, which demonstrates the robustness of our method on the setting of nearest neighbors. Finally, the highest performance for the baselines is 98.6% by LMNN, which outperforms the recognition rate 95.95% reported in [15] which uses the pixel values as the feature. The highest performance for our method MFDML-K reaches 98.98%. The results strongly support the claim that distance metric learning with multiple features usually achieves better generalization than single feature for image classification. We study how our multiple metrics improves the performances with different number of kernel s. To this end, we conduct experiment on ImageNet-250 data sets. We test MFDML-K for this experiment. We use the data driven hierarchical clustering method as introduced in Section 2.4 to form multi-level category group structures. The performan ce with different number of metrics and kernels are shown in Table 1. We demonstrate the results using different number of metrics (row) and number of features (column). We see that when using more features, the performance is improved. This obs ervation is consistent with many previous MKL studies. For the semantic categorization on ImageNet-250 dataset, when the number of metrics increases, the performance is improved. When th e number of metrics is small, increasing the number of metrics will lead to significant improvements. When the number of metrics is more than 62, increasing the number of metrics l ead to slight improvement. In fact, for large scale image applications, one can seek a better tradeoff between performance and efficiency by using different number of metrics. In this experiment, the recognition rates of the best single feature (wavelet feature) and using all the features are shown in Table 3. We observe an interesting phenome non that for some feature such as EDH, the distance metric learned by LMNN or ITML underperform the simplest Euclid ean metric. The model trained by LMNN and ITML on the concatenated feature representation even underperform the model trai ned by wavelet feature (WT), which shows the inflexibility of previous DML approaches on learning with heterogeneous feature. However, our method achieves remarkable improvement at almost all the situations except that when the neighborhood size is 3, LMNN achieves the highest performance by using WT. This result proves the Figure 1: The recognition accuracies on Yale Face B dataset with respect to different numbers of nearest neighbors. effectiveness of our distance metric learning when facing with multiple features, especially on the classification problems on Web image. When the number of nearest neighbors k increases, the performance is likely to be enhanced in the experiment because of the complicated neighborhood structure. We observe a decrease in the improvement rate for larger neighborhood size. Therefore, there will be a neighborhood size k 0 around 20 that our methods and other baseline approaches achieve the best results. In this section, we conduct experiments to compare our methods with several state-of-the-art similarity learning and distance metric learning approaches mentioned in previous sections on visual semantic categorizati on. We denote learning category specific metrics for each class using LMNN by st-LMNN, and learning one unified metric using LMNN as u-LMNN. We conduct our category specific linear metric learning and kernel learning method for each visual category, which corresponds to 250 learned metrics. The results are shown in Table 2. We see that MFDML-K achieves the highest performance by using all the features. We see from both the performance of st-LMNN and our approaches that multiple metrics are indeed helpful for enhancing the performance. MFDML-L does not significantly outperform st-LMNN because we conduct PCA to avoid dimension explosion, which is an important pre-processing technique required by many existing linear distance metric l earning method. In general, our methods achieve promising results on real world image semantic categorization, especially when using image kernels. 
EUC EUC-PCA st-LMNN u-LMNN NCA
ITML LFDA MFDML-L MFDML-K In this paper, we address the problem of metric learning with multiple features. We propose a new distance metric learning method which learns an overall distance metric by optimizing a set of Mahalanobis matrix for several feature representations at one time. We made two extensions to facilitate our method with learning with real world image da ta. The first is kernelizing the model so that it can take advantage of many existing image kernels, and the second is to learn multiple metrics according to a pre-computed semantic structure. The experiments on three image datasets show remarkable performance on the effectiveness and efficiency of our method compared with several state-of-the-art DML approaches. For future study, we will study how to combine our method with the existing LSH technique for efficient metric learning and the applications of retrieval. This work was supported in part by National Natural Science Foundation of China: 61025011, 60833006 and 61070108, in part by Beijing Natural Scien ce Foundation: 4092042 and 4111003, and in part by NSF IIS 1052851, Google Faculty Research Award, gift grants from FXPAL, and NEC Labs of America, Cupertino, CA, to Dr. Qi Tian, respectively. 
