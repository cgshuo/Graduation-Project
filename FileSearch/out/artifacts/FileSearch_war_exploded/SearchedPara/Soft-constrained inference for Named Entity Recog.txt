 1. Introduction
The data used by Decision Support Systems are assumed to be structured and quantifiable. However, thanks to the grow-ing of the web and the spread of Document Management Systems, most of the valuable information are embedded in textual documents that need to be processed to extract relevant information in a machine readable form to become actionable. In to a set of predefined categories such as names of persons, organizations, locations, dates, and quantities.
Early NER systems have been defined as rule-based approaches with a set of fixed and manually coded rules provided by costs, in terms of human effort, to reveal and formulate hand-crafted rules, several research communities ranging from under the form of boolean conditions, are based on inductive learner where rules can be learnt automatically from labeled examples. The inductive rule learning approach has been instantiated according to different learning paradigm: bottom-up  X  2008; Bohannon et al., 2009; Beckerle, Martucci, &amp; Ries, 2010 ).
 also complex relationships may hold among them. This decision making paradigm can be addressed in two different ways: process is a sequence of segments. More formally, a segmentation s of an input sentence x  X  x ments s 1 ... s p with p 6 N . Each segment s j consists of a start position l tackled as a sequence of tokens and the output of the decision process is a sequence of labels y  X  y
Nowadays, the state of the art to model a NER problem is represented by Linear Chain Conditional Random Fields ( Lafferty strictly related to the underlying Markov assumption: given the observation of a token, the corresponding hidden state
Cohen, 2004; Galen, 2006 ) to model long distance relationships and (2) introducing additional domain knowledge in terms declarative rules from data.

The introduction of constraints allows us to improve the global label assignment by correcting mistakes of local predic-tions. The label assignment problem is therefore solved through a constrained optimization problem where the extra-performances of CRF in NER tasks.
 ing its mathematical programming formulation. In Section 4 the experimental investigation on both benchmark and datasets is described, while in Section 5 conclusions and ongoing research are summarized. 2. Conditional Random Fields labels (hidden states) y  X  y 1 ; ... ; y N given the corresponding tokens (observations) x  X  x of CRF ( Lafferty et al., 2001 ) is given below:
Definition 1 ( Conditional Random Fields ). Let G  X  X  V ; E  X  be a graph such that Y  X  X  Y of G . Then  X  X ; Y  X  is a Conditional Random Field, when conditioned on X , the random variables Y with respect to the graph: p  X  Y v j X ; Y w ; w  X  v  X  X  p  X  Y
Thus, a CRF is a random field globally conditioned on the observation X . Throughout the paper we tacitly assume that the graph G is fixed. A Linear-Chain Conditional Random Field is a Conditional Random Field in which the output nodes are linked by edges in a linear chain. The graphical representation of a general CRF and a Linear-Chain CRF is reported in Fig. 1 . In the following, Linear-Chain CRF are assumed.
 According to the Hammersley X  X lifford theorem ( Hammersley &amp; Clifford, 1971; Clifford, 1990 ), given cliques in G , the conditional probability distribution of a sequence of labels y given a sentence x can be written as: where U C represents the set of maximal cliques C 2 C , the vertices x observations x , and Z  X  x  X 2 X  0 ; 1 is the partition function for global normalization. Formally Z  X  x  X  is defined as: ments of the clique C have certain values, can be conveniently simplified as inner products between a parameter vector x and a set of feature function f . Considering that each clique U ten as: where f k  X  y t ; y t 1 x ; t  X  is an arbitrary feature function over its arguments and x have to be learned from data and are used to tune the discriminative power of each feature function. In particular when for a token x t a given feature function f k is active, i.e. a given property is verified, the corresponding weight x to take into account f k : (1) if x k &gt; 0 it increases the probability of the tag sequence y and (2) if ability of the tag sequence y ; (3) if x k  X  0 has no effect whatsoever.
 The partition function Z  X  x  X  assumes the form: functions such that p  X  y j x  X  can be rewritten as follows: where I and J represent the given and fixed set of state feature function s ture function model respectively the sequence of observations x with respect to the current state y the previous state y t 1 to the current state y t . The parameters k sition feature function.

The choice of the feature functions strongly depends from the application context. For instance, an example of state fea-ture function is while a transition feature function could be
In our case, where the CRF are assumed in a linear form, transition feature functions are assumed to verify dependencies and/or an end label. 2.1. Training: estimating the parameters of CRF
The learning problem in CRF relates to the identification of the best feature functions s vealing the corresponding weights k i and l j . These parameters can be quantified either by exploiting some background domain knowledge or by learning from training data. When no background knowledge is available, several learning approaches can be adopted for estimating k i and l j . Among them a widely used approach consists of maximizing the con-ditional log-likelihood of the training data. Given a training set set Y of entity labels, the conditional (penalized) log-likelihood is defined as follows:
A standard approach to parameter learning computes the gradient of the objective function to be used in an optimization
Newton approach known as Limited-memory Broyden X  X letcher X  X oldfarb X  X hanno (L-BFGS) ( Malouf, 2002 ) due the main advantage of providing a dramatic speedups in case of huge number of feature functions. 2.2. Inference: finding the most probable state sequence x  X  x 1 ; ... ; x N . This problem can be solved approximately or exactly by determining y such that:
The most common approach to tackle the inference problem is represented by the Viterbi algorithm ( Baum &amp; Petrie, 1966 ). Now, consider d t  X  y t  X  y j x  X  as the probability of the most likely path of generating the sequence y  X  y path can be derived by one of the most probable paths that could have generated the subsequence y given d t  X  y t  X  y j x  X  as: we can derive the induction step as:
The recursion terminates in y  X  argmax 3. Introducing background knowledge in CRF functions. However, some more complex relationships among output variables (labels to be predicted) could exist when the label Title .

Two different strategies are suitable to include relationships in the probabilistic model: the feature way and the con-to the necessity of learning additional parameters or defining higher order models, the second paradigm allows us to keep formulating the inference process as a constrained optimization problem.
The main idea underlying the Constrained Viterbi algorithm is concerned with the clamping of some hidden variables to through a given sub-path C . The related constraint b C can be encoded in the induction step as follows: defined constraint.

Although the Constrained Viterbi algorithm is suitable to deal with short distance relationships in an efficient way, the shortest path problem. A simple representation of a labeling shortest path problem is given in Fig. 2 .
Given n tokens and m labels that each token can take, we can define a graph X  X  X  U ; W  X  as composed of nm  X  2 nodes and  X  n 1  X  m 2  X  2 m edges. 1 The label of each token is represented by a node / arc connecting two adjacent nodes /  X  t 1  X  y and / ty 0 is denoted by a directed edge w c yy 0 ; t  X  log  X  M t  X  yy 0 j x  X  X  . 2 The problem consists in minimizing the cost of visiting the nodes / linear programming problem that allow us to introduce additional background knowledge in terms of constraints. Let e denote a decision variable restricted to be 1 if the edge w the shortest path problem 3 can be formalized as follows:
Any relationships that should be preserved over the output prediction could be smoothly represented as Boolean function assumptions: (1) constraints must be manually defined by a domain expert and (2) constraints must be satisfied, i.e. they obtained). In order to overcome the mentioned weak points, a combination of learning constraints from data and a two-stages ILP approach is proposed. 3.1. Soft constrained inference
In this section we outline in detail the approach proposed in this paper. First we describe a general approach to extract from the available data additional knowledge in the form of logic rules. Then we describe how such knowledge can be straints whose violation is minimized in the objective function. 3.1.1. Learning constraints from data
In order to provide a completely automated Named Entity extraction process, with no need of human effort to define and include domain specific complex relationships, we defined a procedure for learning constraints from data. In a sequential tification of some common patterns and their extraction from data in the form of logic rules is formulated, as originally proposed in Felici and Truemper (2002, 2005) , as a sequence of minimum cost satisfiability problems. These problems are solved efficiently with an ad hoc algorithm based on the decomposition strategies presented in Truemper (2004) . The ing data. Moreover, each conjunctive clause can be easily associated with an integer linear constraints with standard techniques.
 Interesting logic relationship that are extracted from data may fall into one of the following categories:
Adjacency : if label A is associated to token x t , then label B must be associated to token x
Precedence : if label A is associated to token x t , then label B should be associated to token x
State Change :if x t is a given delimiter punctuation mark (d), then label A and label B should be associated to token x and x t  X  1 respectively
Begin-End : if label A is associated to token x 0 , then label B should be associated to token x
Presence and Precedence : if label A appears, then label B cannot appear before label A output prediction. The constraints associated with the rules can thus be represented in a compact way with the notation: where L is a matrix composed of H rows containing the coefficients of the components of vector e for each of the H con-straints, and r is the binary vector of size H representing the violation of the constraints. 3.1.2. Relaxing inference using ILP
In order to introduce the automatically learned logical rules for constraining the inference phase without occurring in feasibility problems, a twofold approach is proposed. The first step is aimed at determining the optimal solution of the shortest path by solving the problem formulation presented in Eqs. (12) X (14) , i.e. to determine the optimal solution e equivalent to the one solved by Viterbi algorithm. Concerning the second step, we introduce a set of constraints in order to identify a labeling solution consistent with the logical rules previously extracted, while ensuring a good approxima-formulated as follows:
The objective function (22) is penalized whenever a logical relationship is violated, i.e. when r play the same role as in the shortest path problem, while constraint (23) states that the variables e 0 6 s 6 1 . The parameter vector c represents the cost of violating a given constraint. In particular, the element c label B), the cost of violating all the constraints related to l is computed as follows: 4. Experimental results
The proposed method has been tested with success on different datasets and benchmark models. In order to provide a complete understanding of the experimental campaign, we firstly introduce the performance criteria used to evaluate qual-comparative analysis and finally the computational results. 4.1. Performance criteria The performance in terms of effectiveness has been measured by using four well known evaluation metrics, i.e.
F-Measure, Precision, Recall and Accuracy. The F-Measure metric represents a combination of Precision and Recall typical of Information Retrieval. Given a set of labels Y we compute the Precision and Recall for each label y 2 The F-Measure for each class y 2 Y is computed as the harmonic mean of Precision and Recall: The Accuracy measure can be summarized as follows:
Micro and macromeasures differs in the computation of global performance: macro-averaging gives equal weight to each according to its dimension. 4.2. Datasets
In order to compare the proposed inference method against the traditional ones, we performed experiments on four data-contains 740 instances, where 50 are used as training and 690 as testing. Each postal address has been manually annotated according the following labels: Street, City, Civic Number, State and Zip Code .

A further benchmark concerns with the CoNLL-2003 corpus ( Tjong Kim Sang &amp; Meulder, 2003 ), which consists of Reuters news stories annotated with four entity types: Person, Location, Organization and Miscellaneous . It is composed of 945 instances used as training and 246 as testing.
 benchmark has been split for training and testing the models: 300 instances have been used as training set, while the remaining 200 instances as testing.

The last benchmark concerns with the Advertisements corpus ( Grenager, Klein, &amp; Manning, 2005 ), which comprises 4.3. Model settings
In order to evaluate the contribution of the proposed inference method, we considered two main approaches for a com-Viterbi algorithm on the SemiCRF segment-based model (SemiCRF-Viterbi), and Soft-constrained inference on traditional CRF model (CRF-Soft). This allows us to compare the proposed inference method with the main state of the art solutions: CRF-Viterbi as baseline token-based approach, and SemiCRF-Viterbi as reference of segment-based approaches able to consider long range dependencies. Concerning the model settings, the following features have been included during the training phase of traditional CRF models: EdgeFeatures : denote transition feature functions that solely dependent upon the current label and the previous one. StartFeatures : evaluate whether the current label in consideration is a start state.
 EndFeatures : evaluate whether the current label in consideration is an end state.
 counts of the number of times the token occurs in a state (label).
 UnknownFeatures : check if the current token is not observed at the time of training.

ConcatRegexFeatures : match the token with the character patterns. Character patterns are regular expressions for check-ing whether the token is capitalized word, a number, small case word, whether the token contains any special characters and like.

Concerning SemiCRF, we used the same set of word-level features, as well their logical extensions to segments. In partic-cators for words and capitalization patterns in 3-word windows before and after the segment. We also used indicators for each segment length and combined all word-level features with indicators for the beginning and end of a segment. 4.4. Results
The proposed inference relaxation has been compared with two state-of-the-art approaches, i.e Viterbi algorithm on CRF and Accuracy (A) on the considered datasets both in terms of macro and microaverage. The performance are detailed by constraints on the relaxed ILP formulation.

Concerning the US50 dataset, the results reported in Table 5 show that the proposed inference approach allows to improve the prediction accuracy with respect to the traditional approaches both by including constraint separately and are State Change and Begin X  X nd : while some punctuation marks as  X  X  X omma X  X  clearly denote a State Change between adjacent labels, a Begin X  X nd constraint helps to capture recurrent patterns in US address specifications.
If we focus on results in Table 6 about the CoNLL-2003 dataset, we can easily deduce that a free natural language text is more complex to be modeled than structured or semi-structured instances of US50. Although the overall results obtained by performance. In particular, Begin X  X nd and Presence and Precedence constraints are redundant in respect of the generated Batumi X  ( Location =  X  X eorgia X ).

Concerning the Cora benchmark, the computational results in Table 7 confirm once more that the learned constraints allows the relaxed inference to achieve good performance of the assessed criteria, by ensuring significant improvements straint learning and subsequently an effective inference phase on testing instances.

Regarding the Advertisements dataset, the results are reported in Table 8 . This benchmark is characterized not only by a natural language, the proposed approach is able to capture some global relationships as constraints to be included in the with the entity Contact .
 find a more appropriate labeling solution thanks to the global relationships encoded as constraints.
At the end of the computational comparison across several dataset, we would like to stress once more that the proposed approach consists in adding information (constraints) during the inference phase, without modifying the learning step. On the contrary, most of the works proposed in the literature are instead based on adding knowledge during the learning pro-lower computational complexity and (2) adaptability to changes in the domain setting (the probabilistic model generated presented in Chang et al. (2012) , where the two dataset Advertisements and Cora have been exploited for validation pur-poses. If we compare the performance achieved by the proposed inference approach with the ones obtained by L + I ,we can easily note that our relaxed ILP formulation is able to achieve higher performance: our approach is able to ensure an accuracy of 93.99% against 89.72% on the Cora dataset and 78.50% against 75.28% on the Advertisements dataset. This high-lation making the inference with a more expressive model. 5. Conclusions and ongoing research
In this paper we have presented an ILP approach for relaxing inference for Information Extraction problems by using Con-ditional Random Fields. The proposed approach is based on the automatic extraction of background knowledge from training data and in the incorporation of such knowledge in the inference problem. The proposed ILP formulation is aimed at incor-porating the additional knowledge hidden in long distance dependencies through nondeterministic soft constraints. The straints is designed to preserve these complex relationships during the output prediction. Experimental results show that our method significantly outperforms the current state of the art approaches, obtaining remarkable performance gains across several domains and types of data (benchmark and real datasets).

Concerning ongoing research, an interesting direction relates to how effectively label any named entity with hundreds of the computational limitation of the learning algorithms. Traditional approaches to train CRF are based on gradient-based to find the optimal feature subset that could be used for training.
 Acknowledgement
W911NF-09-2-0053. Any opinions, findings, conclusions or recommendations are those of the authors and do not necessarily eJRM project (ref. PON01_01286).
 References
