 Information Sciences Institute University of Southern California based system. 1. Introduction
The alignment template translation model (Och and Ney 2004) and related phrase-based unit of translation from words to phrases , that is, substrings of potentially unlimited size (but not necessarily phrases in any syntactic theory). These phrases allow a model deletions that are sensitive to local context. This makes them a simple and powerful mechanism for translation.
 et al. 1993). Following convention, we call the source language  X  X rench X  and the target language  X  X nglish X ; the translation of a French sentence f into an English sentence e is modeled as:
The phrase-based translation model P ( f | e )  X  X ncodes X  e into f by the following steps: 1. segment e into phrases  X  e 1  X  X  X   X  e I , typically with a uniform distribution over 2. reorder the  X  e i according to some distortion model; 3. translate each of the  X  e i into French phrases according to a model P (
Other phrase-based models model the joint distribution P ( e , f ) (Marcu and Wong 2002) the basic architecture of phrase segmentation (or generation), phrase reordering, and phrase translation remains the same.
 strings that are common enough to have been observed in training. But Koehn, Och, and
Marcu (2003) find that phrases longer than three words improve performance little for training corpora of up to 20 million words, suggesting that the data may be too sparse to learn longer phrases. Above the phrase level, some models perform no reordering model that reorders phrases independently of their content (Koehn, Och, and Marcu 2003; Och and Ney 2004), and some, for example, the Alignment Template System (Och et al. 2004; Thayer et al. 2004), hereafter ATS, and the IBM phrase-based system consider the following Mandarin example and its English translation: to be reversed. When we run a phrase-based system, ATS, on this sentence (using the experimental setup described herein), we get the following phrases with translations: where we have used subscripts to indicate the reordering of phrases. The phrase-based phrase reordering) and  X  X s one of the few countries X  correctly (using a combination of phrase translation and phrase reordering), but does not invert these two groups as it should.
 of the phrase-based approach, but rather capitalizes on them: Because phrases are good for learning reorderings of words, we can use them to learn reorderings of phrases as well. In order to do this we need hierarchical phrases that can contain other phrases.
For example, a hierarchical phrase pair that might help with the above example is where 1 and 2 are placeholders for subphrases (Chiang 2005). This would capture the fact that Chinese prepositional phrases almost always modify verb phrases on the 202 left, whereas English prepositional phrases usually modify verb phrases on the right. as a discontinuous phrase pair and as a phrase-reordering rule. Thus it is considerably more powerful than a conventional phrase pair.
 would capture the fact that Chinese relative clauses modify NPs on the left, whereas
English relative clauses modify on the right; and the pair would render the construction zhiyi in English word order. These three rules, along with some conventional phrase pairs, suffice to translate the sentence correctly: in the next section as rules of a synchronous context-free grammar (CFG). annotation.

Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001) that has been fruitful but has not previously produced systems that can compete with phrase-based systems differs from early syntax-based statistical translation models in combining the idea of hierarchical structure with key insights from phrase-based MT: Crucially, by incorpo-rating the use of elementary structures with possibly many words, we hope to inherit phrase-based MT X  X  capacity for memorizing translations from parallel data. Other in-sights borrowed from the current state of the art include minimum-error-rate training of log-linear models (Och and Ney 2002; Och 2003) and use of an m -gram language model. employing a grammar (to our knowledge) to perform better than phrase-based systems in large-scale evaluations. 2 2. Related Work guish between different categories, they typically do not distinguish very many. Our proaches, exemplified by that of Yamada and Knight (2001), do make use of parallel syntactically annotated corpora are comparatively small, obtaining parsed parallel text in quantity usually entails running an automatic parser on a parallel corpus to produce noisy annotations.
 straightforward generalization of phrase extraction from word-aligned parallel text, has been independently proposed before in various settings. The method of Block (2000) is
The same method has also been used by Probst et al. (2002) and Xia and McCord (2004) in conjunction with syntactic annotations to extract rules that are used for reordering prior to translation. Finally, Galley et al. (2004) use the same method to extract a very large grammar from syntactically annotated data. The discontinuous phrases used by
Simard et al. (2005) have a similar purpose to synchronous grammar rules; but they have variables that stand for single words rather than subderivations, and they can interleave in non-hierarchical ways. 3. Grammar
The model is based on a synchronous CFG, elsewhere known as a syntax-directed transduction grammar (Lewis and Stearns 1968). We give here an informal definition and then describe in detail how we build a synchronous CFG for our model. 3.1 Synchronous CFG
In a synchronous CFG the elementary structures are rewrite rules with aligned pairs of right-hand sides: where X is a nonterminal,  X  and  X  are both strings of terminals and nonterminals, and  X  is a one-to-one correspondence between nonterminal occurrences in  X  and nonterminal occurrences in  X  . For example, the hierarchical phrase pairs (3), (4), and (5) previously presented could be formalized in a synchronous CFG as: 204 where we have used boxed indices to indicate which nonterminal occurrences are linked by  X  . The conventional phrase pairs would be formalized as: Two more rules complete our example:
A synchronous CFG derivation begins with a pair of linked start symbols. At each step, two linked nonterminals are rewritten using the two components of a single rule. When denoting links with boxed indices, we must consistently reindex the newly introduced symbols apart from the symbols already present. For an example using these rules, see
Figure 1. 3.2 Rule Extraction process begins with a word-aligned corpus: a set of triples f , e , sentence, e is an English sentence, and  X  is a (many-to-many) binary relation between positions of f and positions of e . The word alignments are obtained by running GIZA++ (Och and Ney 2000) on the corpus in both directions, and forming the union of the two sets of word alignments.
 aligned to a word inside the other, but no word inside one phrase can be aligned to a word outside the other phrase. For example, suppose our training data contained the fragment with word alignments as shown in Figure 2a. The initial phrases that would be extracted are shown in Figure 2b. More formally: Definition 1
Given a word-aligned sentence pair f , e ,  X  ,let f j position i to position j inclusive, and similarly for e j phrase pair of f , e ,  X  iff: 1. f k  X  e k for some k  X  [ i , j ]and k  X  [ i , j ]; 2. f k  X  e k for all k  X  [ i , j ]and k /  X  [ i , j ]; 3. f k  X  e k for all k /  X  [ i , j ]and k  X  [ i , j ].
 other phrases and replace the subphrases with nonterminal symbols. For example, given the initial phrases shown in Figure 2b, we could form the rule 206 as shown in Figure 2c. More formally: Definition 2
The set of rules of f , e ,  X  is the smallest set satisfying the following: 1. If f j i , e j i is an initial phrase pair, then 2. If (X  X  ,  X  )isaruleof f , e ,  X  and f j i , e j i is an initial phrase pair such because it makes training and decoding very slow, but also because it creates spurious ambiguity  X  X  situation where the decoder produces many derivations that are distinct yet have the same model feature vectors and give the same translation. This can result in for the minimum-error-rate training algorithm (see Section 4.3). To avoid this, we filter our grammar according to the following constraints, chosen to balance grammar size and performance on our development set: 1. If there are multiple initial phrase pairs containing the same set of 2. Initial phrases are limited to a length of 10 words on either side. 3. Rules are limited to five nonterminals plus terminals on the French side. 4. Rules can have at most two nonterminals, which simplifies the decoder 5. It is prohibited for nonterminals to be adjacent on the French side, a major 6. A rule must have at least one pair of aligned words, so that translation
Variations of constraints (1) and (2) are also commonly used in phrase-based systems. 208 3.3 Other Rules
Glue rules. Having extracted rules from the training data, we could let X be the gram-for robustness and for continuity with phrase-based translation models, we allow the grammar to divide a French sentence into a sequence of chunks and translate one chunk at a time. We formalize this inside a synchronous CFG using the rules (14) and (15), which we call the glue rules , repeated here:
These rules analyze an S (the start symbol) as a sequence of Xs which are translated without reordering. Note that if we restricted our grammar to comprise only the glue rules and conventional phrase pairs (that is, rules without nonterminal symbols on the right-hand side), the model would reduce to a phrase-based model with monotone translation (no phrase reordering).
 tion modules to translate the numbers, dates, numbers, and bylines in the sentence, and insert these translations into the grammar as new rules. 3 by phrase-based systems as well, but here their translations can plug into hierarchical phrases, for example, into the rule allowing it to generalize over numbers of years. 4. Model
Given a French sentence f , a synchronous CFG will have, in general, many derivations
We now define a model over derivations D to predict which translations are more likely than others. 4.1 Definition
Following Och and Ney (2002), we depart from the traditional noisy-channel approach and use a more general log-linear model over derivations D : where the  X  i are features defined on derivations and the  X  define as products of functions on the rules used in a derivation: Thus we can rewrite P ( D )as
The factors other than the language model factor can be put into a particularly con-venient form. A weighted synchronous CFG is a synchronous CFG together with a function w that assigns weights to rules. This function induces a weight function over derivations:
If we define then the probability model becomes
It is easy to write dynamic-programming algorithms to find the highest-weight transla-tion or k -best translations with a weighted synchronous CFG. Therefore it is problematic translation quality. We return to this challenge in Section 5. 4.2 Features have the following features: 210
Next, there are penalties exp(  X  1) for various other classes of rules:
Finally, for all the rules, there is a word penalty exp( terminal symbols. This allows the model to learn a general preference for shorter or longer outputs. 4.3 Training features, we need counts for the extracted rules. For each sentence pair in the training data, there is in general more than one derivation of the sentence pair using the rules extracted from it. Because we have observed the sentence pair but have not observed the derivations, we do not know how many times each derivation has been seen, and therefore we do not actually know how many times each rule has been seen. ble rules as though we observed them in the training data, a distribution that does not necessarily maximize the likelihood of the training data. each initial phrase pair occurrence, then distribute its weight equally among the rules obtained by subtracting subphrases from it. Treating this distribution as our observed data, we use relative-frequency estimation to obtain P (  X  synchronous CFG according to (22) that is ready to be used by the decoder. 5. Decoding
In brief, our decoder is a CKY (Cocke-Kasami-Younger) parser with beam search to-
Given a French sentence f , it finds the English yield of the single best derivation that has French yield f : require a more expensive summation over derivations.
 ing English language-model probabilities for possible translations, which is the primary technical challenge. 5.1 Basic Algorithm
Schabes, and Pereira 1995; Goodman 1999). A parser in this notation defines a space designated goals (the items to be proven), and a set of inference rules of the form and proceeds by applying inference rules to prove more and more items until a goal is proven.
 can be thought of as a deductive proof system whose items can take one of two forms: The axioms would be 212 and the inference rules would be length of the input string f .
 Chomsky normal form, and then for each sentence, we could find the best parse using
CKY. Then it would be a straightforward matter to revert the best parse from Chomsky normal form into the original form and map it into its corresponding English tree, whose yield is the output translation. However, because we have already restricted the number of nonterminal symbols in our rules to two, it is more convenient to use a modified CKY algorithm that operates on our grammar directly, without any conversion to Chomsky normal form. The axioms, inference rules, and goals for the basic decoder are shown in yet incorporate a language model, let us call it the  X  LM parser.
 ordered such that every item comes after its possible antecedents: smaller spans before larger spans, and X items before S items (because of the unary rule S with each item, a tuple of back-pointers to the antecedents from which the item was deduced (for axioms, an empty tuple is used). If two items are added to a cell that are equivalent except for their weights or back-pointers, then they are merged (in the MT decoding literature, this is also known as hypothesis recombination ), with the merged item taking its weight and back-pointers from the better of the two equivalent items. (However, if we are interested in finding the k -best derivations, the merged item gets the multiset of all the tuples of back-pointers from the equivalent items. These back-pointers are used below in Section 5.2.) it has a constraint that prohibits any X from spanning a substring longer than a fixed limit  X  on the French side, corresponding to the maximum length constraint on initial rules during training. This gives the decoding algorithm an asymptotic time complexity of
O ( n ). In principle  X  should match the initial phrase length limit used in training (as it does in our experiments), but in practice it can be adjusted separately to maximize accuracy or speed. 5.2 Generating k -best Lists k -best derivations. These are used for minimum-error-rate training and for rescoring with a language model (Section 5.3.1). We describe here how to do this using the lazy algorithm of Huang and Chiang (2005). Part of this method will also be reused in our algorithm for fast parsing with a language model (Section 5.3.4).
 values statically. The heart of the k -best algorithm is a function M which takes a set L of tuples of (virtual) lists with an operator virtual list: 214 plementation of M ERGE P RODUCTS would simply calculate all possible products and sort; however, if we are only interested in the top part of the result, we can implement M
ERGE P RODUCTS so that the output values are computed lazily and the input lists are accessed only as needed. To do this, we must assume that the multiplication operator is monotonic in each of its arguments. By way of motivation, consider the simple case
L = { L 1 , L 2 } . The full set of possible products can be arranged in a two-dimensional grid (see Figure 5a), which we could then sort to obtain M because of our assumptions, we know that the first element of M
L previously enumerated, the next cell must be one of the cells (shaded gray) adjacent to the previously enumerated ones and we need not consider the others (shaded white). In this way, if we only want to compute the first few elements of M the grid.
 queue is initialized with the best element from each L  X  L list: To enumerate the next element of the list, we first insert the elements adjacent to the previously enumerated element, if any (lines 9 X 13, where b whose i th element is 1 and is zero elsewhere), and then enumerate the best element the priority queue subroutines H EAPIFY ,I NSERT ,andE 2001).
 forest; then we simply apply M ERGE P RODUCTS recursively to the whole forest, using memoization to ensure that we generate only one k -best list for each item in the forest. ing it to output the translations as well is a matter of modifying line 5 to package the operator  X  in line 9 with one that not only multiplies weights but also builds partial translations out of subtranslations.
 216 5.3 Adding the Language Model rescoring it with the LM; second, incorporating the LM directly into the grammar in a construction reminiscent of the intersection of a CFG with a finite-state automaton; third, a hybrid method which we call cube pruning . 5.3.1 Rescoring. One easy way to incorporate the LM into the model would be to decode
However, because the number of possible translations is exponential in n , we may have to set k extremely high in order to find the true best translation (taking the LM into account) or something acceptably close to it. 5.3.2 Intersection. A more principled solution would be to calculate the LM probabilities which each state corresponds to a sequence of ( m  X  1) English terminal symbols. We can then intersect the English side of our weighted CFG G with this finite-state machine to produce a new weighted CFG that incorporates M .Thus P rule weights (22) just like the other features. (For notational consistency, however, we write the LM probabilities separately from the rule weights.) In principle this method grammar necessitates pruning of the search space, which can cause search errors. different construction proposed by Wu (1996) for inversion transduction grammar and bigram LMs. We present an adaptation of his algorithm to synchronous CFGs with two nonterminals per right-hand side and general m -gram LMs. First, assume that the LM expects a whole sentence to be preceded by ( m  X  1) start-of-sentence symbols s and followed by a single end-of-sentence symbol / s . The grammar can be made to do this simply by adding a rule and making S X  the new start symbol.

T is the English terminal alphabet, and is a special placeholder symbol that stands for an elided part of an English string. the function q elides symbols when all their m -grams have been accounted for. These functions let us correctly calculate the LM score of a sentence piecemeal. For example, let m = 3 and  X  X  g i s f X  stand for  X  X olorless green ideas sleep furiously. X  Then Table 1 shows some values of p and q .
 signifying that a subtree rooted in X has been recognized spanning from i to j on the French side, and its English translation (possibly with parts elided) is e . tion can combine up to two starred strings, which each have up to 2( m symbols. This is far too slow to use in practice, so we must use beam-search to prune the search space down to a reasonable size. contains all the rules with the same French side and left-hand side. From here on, let us 218 each cell, we throw out any item that has a score worse than: define a heuristic
Similarly for rules, function to the score of each item. 5.3.4 Cube Pruning. Now we can develop a compromise between the rescoring and intersection methods. Consider Figure 9a. To the left of the grid we have four rules with the same French side, and above we have three items with the same category and span, and items can be used to deduce a new item (whose scores are shown in the grid), and all these new items will go into the same chart cell (partially listed on the right). The intersection method would compute all twelve items and add them to the new chart cell, where most of them will likely be pruned away. In actuality, the grid may be a cube (one dimension for rules and two dimensions for two nonterminals) with up to b elements, whereas the target chart cell can hold at most b items (where b is the limit on the size of the cell imposed during pruning). Thus the vast majority of computed preemptively prune the rest of the items without computing them, a method we refer to as cube pruning .
 rules to the left of the grid can be thought of like a 4-best list for a single (X  X  cong X); the three items above the grid, like a 3-best list for the single don X  X  know what k is in advance. If we could use M ERGE P new items best-first, then we could enumerate them until one of them was pruned from the new cell; then the rest of items, which would have a worse score than the pruned item, could be preemptively pruned.
 the inputs to M ERGE P RODUCTS and in the priority queue inside M according to their + LM score, including the heuristic function h .The use takes one or more antecedent items and forms their consequent item according to the + LM parser. Note that the LM makes this  X  only approximately monotonic. This means that the enumeration of new items will not necessarily be best-first. To alleviate this problem, we stop the enumeration not as soon as an item falls outside the beam, but guess as to how much the scores of the enumerated items can fluctuate because of the
LM. A simpler approach, and probably better in practice, would be simply to set  X  = 0, that is, to ignore any fluctuation, but increase  X  and b to compensate.
 thefirst(unlikeinthe k -best algorithm). Supposing a threshold beam of  X  = 5and 220 a margin of  X  = 0 . 5, we quit upon considering the next item, because, with a score of 7.7, it falls outside the beam by more than  X  . The rest of the grid is then discarded. erator; it takes a tuple of antecedent + LM items and returns a consequent + LM item according to the inference rules in Figure 8. The procedure R a  X  LM chart chart as input and produces a + LM chart chart . The variables u , v stand for items in  X  LM and u , v ,foritemsin + LM, and the relation v v is defined as follows: to the target cell until the cell is judged to be full (lines 16 X 20). 6. Experiments
The implementation of our system, named Hiero, is in Python, a bytecode-interpreted language, and optimized using Psyco, a just-in-time compiler (Rigo 2004), and Pyrex, a Python-like compiled language, with C++ code from the SRI Language Modeling
English translation. Our evaluation metric is case-insensitive BLEU-4 (Papineni et al. 2002), as defined by NIST, that is, using the shortest (as opposed to closest) reference sentence length for the brevity penalty.
 6.1 Experimental Setup
We ran the grammar extractor of Section 3.2 on the parallel corpora listed in Table 2 with the exception of the United Nations data, for a total of 28 million words (English side). 8 We then filtered this grammar for our development set, which was the 2002 NIST MT evaluation dry-run data, and our test sets, which were the data from the 2003 X 2005
NIST MT evaluations. Some example rules are shown in Table 3, and the sizes of the filtered grammars are shown in Table 4.
 models with modified Kneser X  X ey smoothing (Kneser and Ney 1995; Chen and
Goodman 1998): one on 2.8 billion words from the English Gigaword corpus, and the other on the English side of the parallel text (28 million words). 6.2 Evaluating Translation Speed Table 5 shows the average decoding time on part of the development set for the three 3 GHz Xeon machine. For these experiments, only the Gigaword language model was except where noted in Table 5. Note that values for  X  and  X  are only meaningful relative feature weights were obtained by minimum-error-rate training using the cube-pruning feature weights optimized for the  X  LM model, but rescoring used the same weights as the other experiments.
 cube-pruning method (  X  = 0, 0 . 1, and 0 . 2). The LM rescoring decoder ( k = 10 fastest but has the poorest BLEU score. Identifying and rescoring the k -best derivations pruning decoder runs almost as fast as the rescoring decoder and translates almost as general the optimal setting will depend on the other beam settings and the scale of the feature weights. 6.3 Evaluating Translation Accuracy
We compared Hiero against two baselines: the state-of-the-art phrase-based system ATS system with monotone translation (no phrase reordering).
 of 159 million words (English side). The second language model was also trained on the English side of the whole bitext. Phrases of up to 10 in length on the French side were extracted from the parallel text, and minimum-error-rate training (Och 2003) was 222 performed on the development set for 17 features, the same as used in the NIST 2004 and 2005 evaluations. 9 These features are similar to the features used for our system, but also include features for phrase-reordering (which are not applicable to our system),
IBM Model 1 in both directions, a missing word penalty, and a feature that controls a fallback lexicon.
 the limitation that extracted rules cannot have any nonterminal symbols on their right-hand sides. In other words, only conventional phrases can be extracted, of length up to 5. These phrases are combined using the glue rules only, which makes the grammar equivalent to a conventional phrase-based model with monotone translation. Thus a controlled test of the effect of hierarchical phrases.

Monotone to maximize their BLEU scores on the development set; the feature weights by Collins, Koehn, and Ku  X  cerov  X  a (2005). 7. Conclusion added complexity introduced by hierarchical structures. Here we have addressed the modeling challenge by taking only the fundamental idea from syntax, that language is hierarchically structured, and integrating it conservatively into a phrase-based model we have presented our approach as a logical outgrowth of the phrase-based approach.
Moreover, hierarchical structure improves translation accuracy significantly. 224 as well as rules that contain multiple lexical items instead of one, an m -gram model whose structure cuts across the structure of context-free derivations, and large amounts of training data for meaningful comparison with modern systems X  X hese all threaten to make training a synchronous grammar and translating with it intractable. We have shown how, through training with simple methods inspired by phrase-based models, and translating using a modified CKY with cube pruning, this challenge can be met. fact that moving from flat structures to hierarchical structures significantly improves translation quality suggests that more specific ideas from syntax may be valuable as well. There are many possibilities for enriching the simple framework that the present model provides. But the course taken here is one of organic development of an approach known to work well at large-scale tasks, and we plan to stay this course in future work towards more syntactically informed statistical machine translation.
 Acknowledgments References 226
