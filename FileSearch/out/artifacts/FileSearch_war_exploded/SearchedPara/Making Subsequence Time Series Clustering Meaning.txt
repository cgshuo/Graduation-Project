
Recently, the startling claim was made that sequential time series clustering is meaningless. This has important consequences for a significant amount of work in the litera-ture, since such a claim invalidates this work X  X  contribution. In this paper, we show that sequential time series cluster-ing is not meaningless, and that the problem highlighted in these works stem from their use of the Euclidean distance metric as the distance measure in the subsequence vector space. As a solution, we consider quite a general class of time series, and propose a regime based on two types of sim-ilarity that can exist between subsequence vectors, which give rise naturally to an alternative distance measure to Euclidean distance in the subsequence vector space. We show that, using this alternative distance measure, sequen-tial time series clustering can indeed be meaningful. We repeat a key experiment in the work on which the  X  X ean-ingless X  claim was based, and show that our method leads to a successful clustering outcome.
Data miners are often in terested in extracting features from a time series of data [7]. For example, consider a time series where t is the time index and n is the number of obser-vations in the series. Such a time series could represent the closing price of a particular stock in the stock market, or the value returned by a sensor on a mobile robot, etc. Clustering is a technique that is very often proposed as the means for extracting features from time series data, for ex-ample in [4, 8, 9, 1] and many others (see Keogh et. al. [3] for a literature review). When the value of n is relatively small, and the interest is in comparing a set of complete time series produced by some event or process, clustering proceeds in the same way as for the conventional clustering of discrete objects; group together into the same cluster all similar time series. When n is large, it is impractical to con-duct  X  X hole series X  clustering, and indeed, often we wish to find repeating features in a single time series, or in a number of time series produced by the same process. Here subse-quence clustering using the sliding windows technique is a commonly proposed approach. If X is our time series, and w&lt;n is the window length, a single subsequence z p is extracted as and a sequence Z of subsequences can be formed using the sliding windows technique by simply forming the set Z = z then proceeds by forming k clusters, each containing  X  X im-ilar X  z p , using whichever of the many clustering algorithms that are available [2]. Subsequence time series clustering (from here on referred to as STS-clustering) is a widely used technique in the data mining community, often as a subrou-tine of some other technique or method. For example, in their literature review, Keogh et. al. show its use in such diverse areas as rule discovery, indexing, classification, pre-diction, and anomaly detection.

Amazingly, the validity of sequential time series cluster-ing as a data mining technique has recently been called into question [3]. This has important consequences for work we have just surveyed, since such a claim may show it to be invalid. The conclusion in [3] is based on the finding that STS-clustering produces sine-type waves as cluster repre-sentatives. To help clarify our discussion of their result, we now repeat their Cylinder-Bell-Funnel experiment. Consider three features, denoted Cylinder, Bell, and Funnel, shown in Figure 1. Concatenate each of the fea-tures into a single  X  X ase X  time series, and then concate-
Figure 1. The Cylinder, Bell and Funnel fea-tures
Figure 2. Non-intuitive result of subsequence clustering using the sliding windows tech-nique on a data series of concatenated fea-turesfromFigure1 nate a number (say 20) of these base time series into a fi-nal time series 1 . Given there are three features in Figure 1, each with length 128 , if we then conduct STS-clustering on this time series, with window length w = 128 ,andthe number of clusters k =3 , we might hope to recover as cluster representatives the Cylinder, Bell, and Funnel fea-tures. The final three cluster representatives found using STS-clustering, with the k-means clustering algorithm as the clustering method, are the three sine-type wave features shown in Figure 2. This non-intuitive result is not pecu-liar to this particular experiment. Work in [3] found that sine-type waves were produced by STS-clustering no mat-ter what clustering algorithm, number of clusters, or data set was used. This explains their conclusion that STS-clustering is meaningless, since, if any data set produces the same type of cluster representatives, then the output pro-duced by the method is independent of its input, and hence is meaningless.
 In this paper we propose a solution to the above dilemma. We show that two fundamental errors in the work in STS-clustering to date have been made. The first concerns how clusters are formed by the clustering method chosen, while the second concerns how the cluster centre (i.e. the repre-sentative of the cluster) is chosen. We will show that both errors are the result of how distance in the subsequence vec-tor space is measured, and that once distance is measured in an appropriate way, STS-clus tering can indeed be meaning-ful.
We commence our investigation by exposing the reason why the cluster representatives in Figure 2 are so smooth. Given the original time series is full of sharp edges, it seems strange that the final cluster representatives are smooth sine-type waves. Assume we have a very simple data series con-sisting of 17 points which form a single step (as shown in Figure 3 (top)). If we apply STS-clustering to this data se-ries, using the k-means clustering method, with the number of clusters k =3 , and with a window length w =4 , we ob-tain the cluster representatives shown in Figure 3 (bottom), where these representatives have been placed along the hor-izontal axis to coincide with the features they represent in the data series. At first glance, the clustering looks sensi-ble, since the three representatives would seem to represent the three main features in the original data: when the signal is low, when it is high, and the step. However, a curious property of the step cluster representative is that it has been smoothed. Where the step in the original data was com-pleted in one time unit of the horizontal axis, the step in the step-feature cluster representative now takes three time units. Figure 4 shows the three subsequence members of
Figure 3. A simple time series containing a single step (top), and the cluster representa-tives of a three-cluster STS-clustering of this data (bottom) the step cluster. Note that, if cluster C j ,j =1 ...k con-tains r j members z i ,i =1 ...r j , the k-means clustering al-gorithm determines a cluster representative as the point  X  which minimises where d ( ., . ) (from now on) denotes Euclidean distance, and where, from an implementation point of view,  X  z j can be calculated simply as the mean of all members in C j (i.e. 1 /r j r j i =1 z i ). Then, the mean of the three cluster mem-bers in Figure 4 is exactly the step-feature cluster repre-sentative shown in Figure 3. It seems then that Equation 3 does not provide a valid means for determining a cluster representative, since it gives rise to a curious  X  X moothing effect X , and this might be the reason why we get smoothed sine-type waves as cluster representatives in Figure 2. To-ward exploring this possibility, we note that if we take the medoid member of the cluster (i.e. as per Equation 3, but where  X  z j  X  C j ) we are guaranteed to retain the sharpness of any feature 2 .
Figure 4. The three members of the step clus-ter shown in Figure 3 (bottom)
We turn now again to the non-intuitive results of Figure 2. Maybe the clustering of the data series in this figure is correct, but that in determining each cluster representative according to Equation (3), the smoothing effect has degen-erated the cluster representatives into smoothed sine-type waves. Conceivably, the medium-coloured centre cluster representative is the (smoothed) cylinder feature, with the left and right humps being the (smoothed) funnel and bell features respectively. To investigate this possibility, we first present two definitions.
 Definition 1 A cyclic data series is one where | x t  X  x t for some fixed  X  for all t =1 ... ( n  X   X  ) ,where is a small value compared to x t and where  X  is usually quite a bit smaller than n .
 That is, a cyclic data series is one that, except for noise, repeats itself, where the value represents the level of noise in the data.
 Definition 2 Let X be a data series and Z be the series of subsequences obtained by using the sliding windows tech-nique on X . If we conduct a clustering on Z (ie. we are STS-clustering X ) to obtain a set of clusters C j ,j =1 ...k , a  X  X egment X  in C j is a set of members of C j that were orig-inally contiguous in Z .

Then it is clear that our data series of concatenated Cylin-der, Bell and Funnel features is cyclic, and that the cluster representatives shown in Figure 2 will each result from a set of cluster member data points made up of a number of distinct segments . We saw that we can avoid the smooth-ing effect by simply taking the medoid member of a cluster as its representative. However plotting the medoid of all points in a cluster will not expose to us whether points rep-resenting different features exist in the one cluster. What is required is to plot the medoid of each segment in a clus-ter, and this is what we do. Figure 5 shows just such a
Figure 5. The data point set for each cluster shown in Figure 2 includes representatives for all three Cylinder, Bell and Funnel features plot, where the top, middle and bottom plots show segment medoids from the left, middle and right clusters of Figure 2 respectively. It is a little difficult, in Figure 5, to distin-guish precisely between individual segment medoids, since we have shown many segment medoids from the one cluster on a single plot. However, one thing plain to see; there ex-ist all three Cylinder, Funnel and Bell features in each plot in Figure 5. That is, data points representing each of these features exist in each of the clusters found in Figure 2, and so it is definitely not the case that the left cluster in Figure 2 is made up of only the Funnel feature, nor is the middle or right cluster made up of only the Cylinder and Bell features respectively. Clearly, the smoothing effect described above is not the only force at work in the non-intuitive results of Figure 2, and further investigation is required.
Up to this point, our presentation has been caged in terms of the sequential time series framework that is popular in the data mining community. It now becomes useful to view this method in the wider context of the method of delays, as used in the research field of Dynamical Systems [6]. Forming a set of subsequences of length w from a time series X using the sliding windows method, and representing them in a w -dimensional space, can be viewed as just a special case of the method of delays, where Equation 2 generalises into where z p is called a delay vector, and where q is a lag intro-duced so that the members in z p need not be contiguous in the original time series X . The sequence of delay vectors Z is then formed as the set Z = { z p | p = ( w  X  1) q +1 ,...,n From the dynamical systems perspective, the idea is that a system will live in some phase space, and the aim is to reconstruct a space equivalent 3 to phase space, so that one can study the reconstructed space and know that any findings made will also hold for the original phase space. The dynamical systems literature has shown that, in gen-eral, such a space (called a Delay Space) can be formed as a space housing vectors constructed according to Equation 4, with certain restrictions on the values of q and w [5]. One of the central ideas in the approach is that the evolu-tion of the dynamics of a system will form a data series that traces out some trajectory in delay space, and it is really this idea that we are interested in using to pursue the rea-sons for our strange results in Figure 5. The delay space for the Cylinder-Bell-Funnel experiment is of a dimension too great to represent graphically, so we take, as an example, a data series obtained from a small-swing, mathematical pen-dulum; having dynamics governed by the differential equa-tion  X   X  +  X  =0 . The data series produced by this system is a simple sinusoid  X  =  X  0 cos (  X t +  X  ) , and if we assume a re-alistic measurement process that includes noise, one can see in Figure 6 how this data series forms a trajectory in delay space, where the ellipse in the top plot corresponds to the trajectory in a delay space formed with w =2 and q =1 , and the ellipse in the bottom plot to one formed with w =2 and q =10 . An interesting aside is to note how taking q =1 (i.e. the classical sliding windows approach) results in a  X  X ollapsed X  trajectory lying along the bisectrix of de-lay space. This is almost never optimal from the clustering point of view, since dissimilar points will then lie close to-gether. It is almost always more optimal to select q&gt; 1 where methods for determining q can be found in [6]. It is for this reason that we focus on the q =10 case in the following discourse.
Figure 7. Clusters formed using k-means based on the Euclidean distance measure
With the concept in place of a data series as a trajectory in delay space, we now turn our attention back to Figure 5. We saw in that figure how each cluster was made up of a mixture of the Bell, Cylinder and Funnel features. A hint as to why this occurs can be seen by a 3-cluster clustering of the pendulum delay vectors shown in Figure 7, where the delay vectors in the same cluster have been marked with the same symbol; either a circle, a cross, or a triangle. Note how the cluster with the cross marker extends across two regions of very distinct types of delay vectors. That is, this cluster contains delay vectors representing the pendulum dynam-ics of: (i) the pendulum swinging left to right, and (ii) the pendulum swinging right to left. The reason this sort of clustering results is plain to see; we are using Euclidean dis-tance as the measure of  X  X imilarity X , and clearly these two regions are close by this measure. This phenomenon also seems the likely reason for the clustering outcome of the Bell-Cylinder-Funnel data in Figure 5; where delay vectors representing very different dynamics were also clustered to-gether. Also of note in Figure 7 is the plot of the cluster representative for each cluster (as cross-hairs), determined as per Equation (3). The surprising result is that none of the representatives actually lie am ong the members they rep-resent. This fact explains the smoothing effect identified in Section 2. More specifically, we identify two reasons for this phenomenon: (a) because the spectrum of dynam-ics represented by the delay vectors in the cluster are non-contiguous (such as in the cross-marker cluster in Figure 7), or (b) because (where the spectrum of dynamics in the cluster is contiguous) the trajectory of vectors in the cluster exhibits curvature (such as in the circle and triangle marker clusters in Figure 7). In both cases (a) and (b), calculat-ing the mean according to Equation 3 will cause the cluster centre to not lie among the cluster members, resulting in the selection of a smoothed cluster representative.
In the previous section we identified two problems with the STS-clustering approach used to date: it incorrectly clusters data that represents very different dynamics into the same cluster, and it incorrectly selects the cluster represen-tative. Both problems stemmed from the use of Euclidean distance as the measure of (dis)similarity in delay space. Clearly the use of Euclidean distance is incorrect, but the obvious question then is what sort of distance metric should be used? In this section we propose a solution for the quite general class of time series produced by time-invariant, de-terministic dynamical systems.

Figure 8. Delay vectors showing temporal similarity, formal similarity, and a combina-tion of both
At the heart of finding an appropri ate distance metric for delay vector space is the issue of defining precisely when two delay vectors are similar/dissimilar. We identify two types of similarity between delay vectors, (i) temporal sim-ilarity, and (ii) similarity of form, which we will call formal similarity. Figure 8 shows pictorially what we mean by tem-poral and formal similarity. Each plot represents a distinct delay vector, labelled z a to z e respectively. We propose, for example, that z a , z b and z c exhibit temporal similarity and that z a and z d exhibit formal similarity, with z a and z exhibiting both temporal and formal similarity. Temporal similarity is in some way a measure of how likely it is that vector z b or z c will evolve into z a . Clearly both z b can evolve into z a if the signal remains low, however it is more probable that z b , compared to z c , will evolve into z since there is less signal to come. This means we should measure z a as being closer to z b than to z c , and it seems clear that the measure for this type of similarity should be the distance along the trajectory that connects z a , z b in delay space. We described formal similarity as a measure of similarity of form, or shape, between two delay vectors. However, given the subjective nature of the notion of form or shape, what is required is a definition. First define the idea of a flow.
 Definition 3 Let S define a local neighbourhood to z p in delay space. Define a vector field V on S such that V repre-sents the evolution of the system producing our data series for each point in S (i.e. that V ( z p ) for z p  X  X  is the tangent vector at z p of the trajectory in delay space representing the system X  X  evolution through z p ). Then we denote V as defining the flow in S .
 Next, define precisely what we mean by formal similarity. Definition 4 Let  X  be a surface (in general a hyper-surface) defined on our local neighbourhood S ,where z p lies on  X  .Form  X  such that it is at all points orthogonal to the flow in S (i.e. if we denote T  X  z  X  all tangent vectors to  X  at z  X  p  X   X  , then the dot product &lt;
V ( z  X  will be the tangent vector to the flow at z  X  p ). Then any vec-tor z  X  p lying on  X  is defined to have purely formal similarity with z p , where the dissimilarity of z p and z  X  p can be mea-sured as the length of the shortest path lying on  X  between z and z  X  p .
 Note that our definition of formal similarity is based on Definition 3, and that in this definition we have implicitly restricted the class of time series under discussion. The as-sumption is that, each time we visit a particular point in delay space, there will exist a unique tangent vector that de-scribes the direction of the evolution of our dynamical sys-tem. Let the dynamical system from which our time series emanates be described by the following equations, where S n is the current true state of the system, S n +1 next true state, F is a nonlinear map from S n to S n +1 , X n +1 is the measured value of S n +1 (i.e. an element in our time series) through the measurement function s ,  X  2 a noise source perturbing the system by a random amount each time step; generally termed dynamical noise,  X  1 is also a noise source; this time associated with the measurement process, and  X  is a vector of system parameters. Generally the dynamical system described by Equations 5 and 6 is de-noted as time-invariant if  X  does not change over time; it is denoted as stochastic if  X  2 causes a significant component of the final time series data, and as deterministic otherwise. Clearly, a time-invariant, deterministic system will produce a unique S n +1 for each S n , resulting in a unique tangent vector to z p in delay space. It may be envisaged that this will only be the case for zero measurement noise  X  1 ,but there are methods (e.g. [6,  X  10.3]) for removing the effects of  X  1 from the measurement process. In many situations, a time-invariant, deterministic dynamic system with measure-ment noise is an appropriate model for a time series source, however, be it, or be it not the case for a specific application, it is important to note this restriction on the framework we are proposing here. Note that deterministic dynamical sys-tems can produce time series of high complexity -especially since they are capable of exhibiting chaotic behaviour.
With this clarification made, we continue on with our discussion of temporal and formal similarity. Note that the idea of temporal similarity comes naturally from the prob-lem at hand; it is clear how a z p will evolve into some z +1 depending on the dynamics of the system. This is in contrast to our presentation of formal similarity, where we have used our delay space formulation to define this notion. Given some delay vector z p , we can imagine the process of creating a delay vector similar to z p by perturbing each x only one combination of the many possible perturbations of each x k will result in the next delay vector z p +1 ;wherethe particular combination of perturbations for this case is de-fined in delay space in the direction of the tangent vector to z . There are, of course, a whole raft of possible perturba-tions that could be applied to each x k  X  z p which is not this one, and we have chosen to define a delay vector with purely formal similarity to z p as one where each x k is perturbed in a way that the relative proportion of perturbations applied to each x k causes the selection of points (delay vectors) ly-ing in the direction orthogonal to the flow at z p . While we do not prove this is the best proportion of perturbations to apply, it is intuitively seems the right one since it results in a type of similarity most  X  X istinct X  to temporal similarity.
Our preceding discussion on similarity of delay vectors clearly suggests an alternative measure of distance to Eu-clidean distance in delay space; the measure should be some function of two distances: one measured always parallel to the flow, and the other measured always orthogonal to the flow. That is, we should choose to measure the similarity between two delay vectors as the combination of their tem-poral and formal similarities. The practical implementation of such an algorithm is not trivial, and is the focus of our ongoing work. For the purposes of this paper, we conduct a STS-clustering experiment that uses the type of metric just proposed, however, for the sub-class of time-invariant, de-terministic dynamical systems that produce time series that are cyclic (see Definition 1). Our aim is not propose an al-gorithm for cyclic data series per se, but rather to show on a real data series that the abstract concepts of temporal and formal similarity defined above do in fact lead to a valid clustering outcome. Specifically, we propose the following algorithm for calculating the distance between any two de-lay vectors formed from a cyclic data series.
 Algorithm 1 Let X be a cyclic data series X = x t | t = 1 ...n where | x t  X  x t +  X  | &lt; for some fixed  X  , and small. Assume we have a delay vector sequence Z cre-ated from X with lag q and vector length w ,i.e. Z = z | p = ( w  X  1) q +1 ...n where z p is as per Equation 4. Then, create the single-cycle, mean data series (i.e. a sequence  X  z integral part only) of ( n  X  r ) / X  .Each z p will contribute to the value of one, and only one,  X  z r . Denote as z  X  p  X  z r for z p . Then, the distance between any two delay vec-tors z a and z b can be calculated as d ( z a ,z  X  a )+ d
In essence, the algorithm finds the mean trajectory of the cycle, and then calculates distance between any two points as the sum of three distances: (a) the two distances given by the distance between each of the points and its nearest point on the mean trajectory, and (b) a third distance as the dis-tance along the flow between the two  X  X earest X  points just mentioned. One can see that this approach is in the spirit of our discussion above, since step (a) measures orthogonal to the flow, and (b) measures along it. K-means clustering, using Algorithm 1 as the distance measure, can then occur in the usual way.
 Algorithm 2 (k-means) : (1) Randomly select initial clus-ter centres  X  z j ,j =1 ...k , (2) Associate each z p  X  Z to its closest  X  z j using distance measured according to Algo-rithm 1, (3) Find the new cluster centres  X  z j as the points which minimise the sum of distances between  X  z j and each z have not moved.
Figure 9 shows the clustering outcome for the pendu-lum data from Figure 6 using Algorithm 2. In contrast to the clustering in Figure 7, where k-means based on Eu-clidean distance was used, the clusters in Figure 9 are well formed, in that each represents a contiguous spectrum of pendulum dynamics. Even though the two regions in the cross-marker cluster in Figure 7 may be close in terms of Euclidean (i.e. X  X traight-line X ) distance, in delay space, their distance along the flow (i.e. temporal distance) is large, and hence they are not clustered together in Figure 9. Figure 9 also shows, as cross-hairs, the cluster centres found by Algorithm 2. Unlike the representatives found in Figure 7, these lie among cluster representatives, and in each case look to give a good balanced representation of all members in their cluster.

With a successful clustering of the (cyclic) pendulum data, we now turn back to the o riginal problem of cluster-ing the (cyclic) Bell-Cylinder-Funnel data. Again, we use Algorithm 2 for clustering and Algorithm 1 for measuring distance. Figure 10 shows the results. The top window in the figure shows we have successfully recovered the Bell, Cylinder and Funnel features, as desired. The bottom win-dow shows an alternate set of features that were found us-ing a different set of initial seed s for the k-means algorithm. Both sets of features are a valid result of clustering the Bell-Cylinder-Funnel data series. One can see this fact more clearly by noting that the series can be formed by the con-catenation of either of these sets of features. In fact, there will be whole spectrum of feature sets that will see the k-
Figure 9. Correct clustering: clusters contain regions of delay vectors representing similar pendulum dynamics Figure 10. Final, correct clustering of the Bell-
Cylinder-Funnel data series means algorithm return very similar total su m-of-distance values; corresponding to the different ways that one cycle can be broken into 3 equal length sequences.
There are a number of conclusions to this work. First and foremost, that sequential time series clustering can in-deed be meaningful, and that it is not, as recommended by Keogh et. al., intrinsically flawed by definition. Second, that the key step in making it meaningful is by measuring distances in delay space correctly. We showed that the Eu-clidean distance measure that has been adopted by work in the area to date is flawed, and we introduced the idea of temporal and formal similarity in delay space for the class of time series produced by time-invariant, deterministic dy-namical systems. We saw that the use of Euclidean distance gave rise to a number of problems, including the cluster-ing together of regions in delay space that are close by this measure, but that really represent very distinct dynamics of the underlying time-series-producing system. In addition we saw that, even when the clustering outcome was correct, taking the cluster representative as the point with minimum average Euclidean Distance to all members in the cluster did not lead to a sensible outcome because of the generally  X  X urved X  nature of clusters in delay space. These results were in contrast to those obtained by our approach based on temporal and formal similarity, where experiments showed that sensible outcomes are achieved in both the areas of forming the clusters, and choosing the cluster representa-tive. Finally, we saw that the sliding windows technique (i.e. lag q =1 ) is generally suboptimal, and should be avoided, since it causes the delay space representation of the data series dynamics to be aligned closely along the bi-sectrix of delay space. Further work to that presented here involves determining a distance measuring algorithm for our temporal and formal similarity paradigm for the more general class of data series (i.e. other than cyclic) that can be produced by time-invariant, deterministic dynamical sys-tems. Our work to date in this regard is promising, and we hope to report on our findings soon.

