 Many information integration tasks require computing similarity between pairs of objects. Pairwise similarity computations are par-ticularly important in record linkage systems, as well as in cluster-ing and schema mapping algorithms. Because the computational cost of estimating similarity between all pairs of instances grows quadratically with the size of the input dataset, computing similar-ity between all object pairs is impractical and becomes prohibitive for large datasets and complex similarity functions, preventing scal-ing record linkage to large datasets. Blocking methods alleviate this problem by efficiently selecting a subset of object pairs for which similarity is computed, leaving out the remaining pairs as dissimi-lar. Previously proposed blocking methods require manually con-structing a similarity function or a set of predicates followed by hand-tuning of parameters. In this paper, we introduce an adap-tive framework for training blocking functions to be efficient and accurate for a given domain. We describe two types of blocking functions and provide learning algorithms for them. The effective-ness of proposed techniques is demonstrated on real and simulated datasets.
Many intelligent data analysis tasks required during information integration on the Web involve computing similarity between pairs of instances. For example, in record linkage (also known as ob-ject identification [35], de-duplication [31], entity matching [9, 33, 2] and identity uncertainty [29, 23]), similarity must be computed between all pairs of records to identify groups of records that re-fer to the same underlying entity. Many clustering algorithms, e.g., greedy agglomerative or spectral clustering, require similarity to be computed between all pairs of instances to form the pairwise simi-larity matrix, which is then used by the clustering algorithm [17, 27]. A number of schema mapping methods also rely on pair-wise similarity computations between descriptions of concepts or records belonging to them [10].

Because the number of similarity computations grows quadrati-cally with the size of the input dataset, scaling the above tasks to large datasets is problematic. Additionally, even for small datasets Copyright 200X ACM X-XXXXX-XX-X/XX/XX ... $ 5.00. estimation of the full similarity matrix can be difficult if compu-tationally costly similarity functions are used. At the same time, in many tasks, the majority of similarity computations are unnec-essary because most instance pairs are highly dissimilar and have no influence on the task output. Avoiding the unnecessary com-putations results in a sparse similarity matrix, and a number of al-gorithms become practical for large datasets when provided with sparse similarity matrices.

Blocking methods efficiently select a subset of record pairs for subsequent similarity computation, ignoring the remaining pairs as highly dissimilar and therefore irrelevant. A number of blocking algorithms have been proposed by researchers in recent years [12 , 18, 15, 24, 1, 6, 19, 37]. These techniques typically form groups (blocks) of observations using indexing or sorting. This allows ef-ficient selection of instance pairs from each block for subsequent similarity computations. Some blocking methods are based on the assumption that there exists a pre-defined similarity metric that cor-rectly captures similar pairs for the domain and task at hand, e.g., edit distance [6, 19] or Jaccard similarity [24], while others assume that forming blocks based on lexicographic sorting of records on some field(s) yields an optimal strategy [15, 37]. Manual selection of fields and parameter tuning are required by all existing blocking strategies to reduce the number of returned dissimilar pairs while retaining the similar pairs.

Since an appropriate blocking strategy can be highly domain-dependent, the ad-hoc construction and manual tuning of blocking methods is difficult. It may lead to over-selection of many dis-similar pairs that impedes efficiency, or, worse, under-selection of important similar pairs that decreases accuracy. Because there can be many potentially useful blocking criteria over multiple record fields, there is a need for automating the process of constructing blocking strategies so that all same-entity or same-cluster pairs are retained while the maximum number of dissimilar pairs is discarded.
In this paper, we formalize the problem of learning an optimal blocking strategy using training data. In many record linkage do-mains, some fraction of instances contains true entity identifiers, e.g., UPC (bar code) numbers for retail products, SSN numbers for individuals, or DOI identifiers for citations. Presence of such labeled data allows evaluating possible blocking functions and se-lecting from them one that is optimal: it selects all or nearly all positive record pairs (that refer to the same entity), and a minimal number of negative pairs (that refer to different entities).
We propose to construct blocking functions using sets of basic blocking predicates that can efficiently select instance pairs satis-fying some binary similarity predicate. Table 1 contains examples of predicates for specific record fields in different domains. We for-mulate the problem of learning an optimal blocking function as the task of finding a combination of blocking predicates that captures Domain Blocking Predicate Census Data Same 1 st Three Chars in LastName Product Normalization Common token in Manufacturer Citations PublicationYear same or off-by-one Table 1: Examples of blocking predicates from different record linkage domains all or nearly all coreferent object pairs and a minimal number of non-coreferent pairs. Our approach is general in the sense that we do not place restrictions on the similarity predicates computed on instance pairs selected by blocking, such as requiring them to be an inner product or to correspond to a distance metric.

We consider two types of blocking functions: disjunctions of blocking predicates and predicates combined in disjunctive nor-mal form (DNF). While finding a globally optimal solution for these formulations is NP-hard, we describe an effective approxima-tion method for them and discuss implementation issues. Empiri-cal evaluation on synthetic and real-world record linkage datasets demonstrates the efficiency of our technique.
While there are many tasks in data mining and artificial intelli-gence applications that require computing pairwise similarity over a dataset, in this paper we focus on the record linkage application. Our techniques can be directly extended to clustering and schema mapping, while extending them for use with kernel methods re-mains an interesting challenge for future work.

Record linkage is the problem of determining which database records refer to the same underlying entity. It is an integral part of many systems that combine information from multiple sources on the web. For example, record linkage plays a central role in comparison shopping systems such as Froogle and Yahoo! Shop-ping , where offers made by different merchants for the same prod-uct must be linked in order to allow price comparisons [3]. An-other web-related instance of record linkage is citation matching in bibliographic databases such as CiteSeer and Google Scholar [13]. Other domains where record linkage problem is commonly studied include census and mailing data where multiple records describ-ing unique individuals and organizations must be linked [36, 31, 6], healthcare records where medical history and epidemiological data is integrated from multiple providers or time periods [28], and databases containing information automatically collected from the web, where duplicates appear due to variations in spelling and nam-ing [35, 25].

The problem of identifying coreferent records has been studied in several research communities under different names that include record linkage [12, 36], the merge/purge problem [36], duplicate detection [26, 31, 4], reference/citation matching [24, 20], object identification [35, 34], entity name matching and clustering [9, 33], hardening soft databases [8], fuzzy de-duplication [6], iden-tity uncertainty [29, 23], robust reading [21], reference reconcilia-tion [11], and entity resolution [2].
 Most systems solve the record linkage problem in three stages. First, a subset of all record pairs is selected that must contain all pairs of coreferent records. Second, similarity is estimated for all of these candidate pairs using one or more similarity functions [9, 35, 31, 4]. Finally, linkage decisions are made for all candidate pairs using the computed pairwise similarities, either in isolation for each pair or collectively over the entire set of records. In this work, we focus on the first stage where candidate pairs are selected for subsequent similarity calculations. This selection is performed by blocking algorithms, which maximally reduce the number of candidate pairs while retaining all coreferent pairs, which allows scaling linkage to large datasets. While previously proposed block-ing algorithms rely on pre-selected or hand-constructed blocking functions, we employ machine learning techniques to learn block-ing functions automatically, leading to efficient and accurate per-formance for a given domain.
Let us formally define the problem of learning an optimal block-ing function. We assume that a training dataset D train = ( available that contains n records known to refer to m true objects: X = { x 1 , . . . , x n } are the records where each x i contains one or more fields, and Y = { y 1 , . . . , y n } are the true object identifiers for each record: y i  X  X  1 , . . . , m } . We assume that a set of binary blocking predicates P = { p 1 , . . . , p t } is available, where each blocking pred-icate p i (  X  ,  X  ) can be applied to a pair of records ( 1 indicating whether they share any blocks. At the same time, every blocking predicate p i corresponds to an indexing function h takes an individual record as its only argument. When applied to each record x i , it produces one or more values corresponding to the identifiers of blocks to which the record belongs. For example, the second predicate in Table 1, Common token in Manufacturer , re-turns 1 for any two records that have common tokens in their Man-ufacturer fields. The indexing function for that predicate places every record in one or more blocks each of which corresponds to a token in the Manufacturer field.

The set of blocking predicates P consists of t = s  X  d predicates generated from s  X  X ase X  predicates applied to d record fields. For example, base predicates may include  X  X ontain Common Token X  ,  X  X xact Match X  , or  X  X ame 1 st Three Chars X  . When these base predicates are applied to each of the d record fields, s  X  cates for specific fields are obtained, examples of which are shown in Table 1.

Multiple blocking predicates are combined by a blocking func-tion f P (  X  ,  X  ) . Like the individual predicates, f pairwise equality function on a pair of records. Pairs for which the blocking function returns 1 comprise the set of candidate pairs re-turned for subsequent similarity computation, while pairs for which the blocking function returns 0 are ignored. However, efficient gen-eration of the candidate set must be performed using inverted in-dices or sorting, not by evaluating the function on all pairs, and the particular generation strategy depends on the actual formulation of the blocking function.

Given the set P of t possible blocking predicates, the objective of the adaptive blocking framework is to identify an optimal block-ing function f  X  P that combines a subset of the predicates so that the set of candidate pairs it returns includes all or nearly all corefer-ent (positive) record pairs and a minimal number of non-coreferen t (negative) record pairs.

Formally, this objective can be expressed as follows: B value indicating that up to  X  coreferent pairs may remain uncov-ered, thus accommodating noise and particularly difficult corefer-ent pairs. The optimal blocking function f  X  P must be found in a hypothesis space that corresponds to some method of combining the individual blocking predicates. In this paper, we consider two classes of blocking functions:
Each type of blocking functions leads to a distinct formulation of the objective (1), and we consider them individually in the fol-lowing subsections.
Given a set of potential blocking predicates P = { p 1 , . . . , disjunctive blocking function corresponds to selecting some sub-set of predicates P  X   X  P , performing blocking using each p and then selecting pairs that are indexed in the same block for at least one predicate. The blocking function corresponding to the disjunctive blocking function based on some subset of predicates = { p i 1 , . . . , p i k } is the disjunction of the blocking predicates in P  X  : f P  X  ( x i , x j ) =  X  , we define ing predicate or the overall blocking function returns 1 for a pair ( x i , x j ) , we say that this pair is covered by the corresponding pred-icate or the function.

Learning the optimal blocking function f  X  P requires selecting a subset P  X  of predicates that results in all or nearly all coreferent pairs being covered by at least one predicate in P  X  , and a minimal number of non-coreferent pairs being covered. Then the general adaptive blocking problem (1) can be written as follows: where w is a binary vector of length t encoding which of the poten-tial blocking criteria are selected, and p ( x i , x j ) is a vector of binary values returned by all t predicates for pair ( x i , x j )
This formulation of the learnable blocking problem is equivalent to the Red-Blue Set Cover problem if  X  = 0 [5]. Figure 1 illustrates the equivalence. The problem of selecting a subset of predicates is represented by a graph with three sets of vertices. The bottom row of  X  vertices corresponds to positive (coreferent) record pairs
Figure 1: Red-blue set cover view of disjunctive blocking designated as the set of blue elements B = { b 1 , . . . , row of  X  vertices corresponds to negative (non-coreferent) record pairs designated as the set of red elements R = { r 1 , . . . , middle row of t vertices represents the set of blocking predicates P , where each p i  X  P corresponds to a set covering some red and blue elements. Every edge between an element vertex and a predicate vertex indicates that the record pair represented by the element ver-tex is covered by the predicate. Learning the optimal disjunctive blocking function is then equivalent to selecting a subset of pred-icate vertices with their incident edges so that at least  X  (positive) vertices have at least one incident edge, while the cover cost , equal to the number of red(negative) vertices with at least one incident edge, is minimized.
In some domains, a disjunctive combination of blocking predi-cates may be an insufficient representation of the optimal blocking strategy. For example, in US Census data, conjunctions of predi-cates such as  X  X ame Zip AND Same 1 st Char in Surname X  yield useful blocking criteria [37]. To incorporate such blocking criteria, we must extend the disjunctive formulation described above to a formulation based on combining predicates in disjunctive normal form (DNF). Then, that hypothesis space for the blocking function must include disjunctions of not just individual blocking predicates, but also of their conjunctions.

A search for the optimal DNF blocking function can be viewed as solving an extended variant of the red-blue set cover problem. In that variant, the cover is constructed using not only the sets rep-resenting the original predicates, but also using additionally con-structed sets representing predicate conjunctions. In Figure 1, the additional sets for predicate conjunctions would be represented by additional vertices in the middle row with edges connecting them to red and blue elements that all predicates in the conjunction cover.
The learnable blocking problem based on DNF blocking func-tions is then equivalent to selecting a set of predicate and conjunc-tion vertices so that at least  X   X   X  positive (blue) vertices have at least one incident edge, while the cost, equal to the number of neg-ative (red) nodes with at least one incident edge, is minimized.
As discussed in Section 1, in many record linkage domains su-pervision is available in the form of records for which the true en-tities to which they refer are known. Such labeled records com-prise the training database D train = ( X , Y ) that can be used to gen-erate the pairwise supervision for learning the blocking function in the form of coreferent (positive) and non-coreferent (negativ e) record pairs. If the training database is large, it may be imprac-tical to explicitly generate and store in memory all positive pairs and negative pairs. In that case, training pairs can be generated and processed iteratively for each blocking predicate using inverted in-dices during the training process. Also, random sampling of pairs can be employed, in which case Chernoff bounds must be used for the sampling process. In semi-supervised clustering tasks, training sets of must-link (same-cluster) and cannot-link (different-cluster) pairs provide supervision for learning blocking functions.
The equivalence of learning optimal disjunctive blocking and the red-blue set cover problem described in Section 2.2.1 has discour-
Figure 2: The algorithm for learning disjunctive blocking aging implications for the practitioner. The red-blue set cover prob-lem is NP-hard, and Carr et al. [5] have shown that unless P=NP, it cannot be efficiently approximated within a factor O ( 2 log 1  X   X  t 4 0, where in our setting t is the number of predicates under consid-eration. On the other hand, several algorithms have been proposed that approximate the red-blue set cover problem [5, 30]. We pro-pose to employ a slightly modified version of Peleg X  X  greedy algo-rithm that has approximation ratio 2 p t log  X  [30]. It is particularly appropriate for the problem at hand as it involves early discarding of particularly costly sets (blocking predicates), leading to more space-efficient learning of the blocking function. In the remaining discussion, we use the term  X  X locking predicates X  in place of  X  X ets X  considered in the original set cover problem.
 The outline of the algorithm A PPROX RBS ET C OVER is shown in Figure 2. The algorithm is provided with training data in the form of  X  coreferent record pairs B = { b 1 , . . . , b  X  } and  X  non-coreferent records pairs R = { r 1 , . . . , r  X  } , where each r i record pair ( x i be the number of negative pairs it covers, covered positives B the set of positive pairs it covers, and coverage b ( p i ) of covered positives, b ( p i ) = | B ( p i ) | . For each negative pair r ( x P that cover it; degree for a positive pair, deg ( analogously. In step 1, the algorithm discards blocking predicates that cover too many negative pairs as specified by the parameter  X  . Then, negative pairs covered by too many predicates are discarded in step 4, which intuitively corresponds to disregarding negative pairs that are highly similar and are placed in the same block by most predicates.

Next, a standard weighted set cover problem is set up for the re-maining predicates and pairs by setting the cost of each predicate to be the number of negatives it covers and removing the negatives. The resulting weighted set cover problem is solved in steps 6-11 us-ing Chvatal X  X  greedy approximation algorithm [7]. The algorithm iteratively constructs the cover, at each step adding the blocking predicate p i that maximizes a greedy heuristic: the ratio of the number of previously uncovered positives over the predicate cost. To soften the constraint requiring all positive pairs to be covered, we add an early stopping condition permitting up to  X  positives to remain uncovered. In practice,  X  should be set to 0 at first, and then gradually increased if the cover identified by the algorithm is too costly for the application at hand (that is, when covering all positives incurs covering too many negatives).
Learning DNF blocking can be viewed as an extension of learn-ing disjunctive blocking where not only individual blocking pred-icates may be selected, but also their conjunctions. We assume that conjunctions that include up to k predicates are considered. Because enumerating over all possible conjunctions of predicates results in an exponential number of predicate sets under considera-tion, we propose a two-stage procedure, shown in Figure 3.
First, a set of t ( k  X  1 ) predicate conjunctions of lengths from 2 to k is created in a greedy fashion. Candidate conjunctions are con-structed iteratively starting with each of the predicates. At each step, another predicate is added to the current conjunction so that the ratio between the number of positives and the number of nega-tives covered by the conjunction is maximally improved.

A candidate set of conjunctions of lengths from 2 to k is con-structed in this fashion and added to P , the set of individual pred-icates. Then, the A PPROX RBS ET C OVER algorithm described in the previous section is used to learn a blocking function that corre-sponds to a DNF formula over the blocking predicates.
Efficiency considerations, which are the primary motivation for this work, require the learned blocking functions to perform the ac-tual blocking on new, unlabeled data in an effective manner. After the blocking function is learned using training data, it should be applied to the test data (for the actual linkage or clustering task) without explicitly constructing all pairs of records and evaluating the predicates on them. This is achieved by applying the indexing function to every record in the test dataset for all blocking predi-cates or conjunctions of predicates that comprise the learned block-ing function. As a result, an inverted index is constructed for each predicate or predicate conjunction of the blocking function. In each inverted index, each block is associated with a list of instances as-signed to the block by the predicate X  X  indexing function. Disjunc-tive and DNF blocking can then be performed by iterating over the lists for each block in the inverted indices and returning all pairs of records that co-occur in at least one such list.
We evaluate the efficiency of the proposed methods for learning blocking functions using metrics commonly used in record linkage and information retrieval: precision and recall. In the context of blocking, these are defined with respect to the number of coreferent and non-coreferent record pairs in a test set captured by a blocking function f P :
In the above definition, precision is the fraction of all candidate pairs returned by the blocking function that are truly coreferent, while recall is the proportion of truly coreferent functions that have been returned by the blocking function. Note that for a fixed recall level, precision is inversely proportional to the number of similarity computations that must be performed on the record pairs returned by the blocking function, since that number is equal to the total number of pairs from sets R and B returned by the blocking func-tion. Therefore, higher precision directly translates into a propor-tional saving in run time for the overall record linkage system or clustering algorithm. These savings are more substantial if collec-tive, graph-based linkage or clustering methods are used [29, 23, 34, 2], as the time complexity of these methods increases superlin-early with the number of candidate record pairs.

We generate precision-recall curves over 10 runs of two-fold cross-validation. During each run, the dataset is split into two folds by randomly assigning all records for every underlying entity to one of the folds. The blocking function is then trained using record pairs generated from the training fold. The learned blocking func-tion is used to perform blocking on the test fold. Precision and recall defined above are collected during blocking for plotting on precision-recall curves. Each curve is obtained by evaluating pre-cision iteratively after obtaining candidate record pairs from the predicates or predicate conjunctions in the order in which they were added by the greedy algorithm.
 We present results on two datasets: Cora and Addresses . The Cora dataset contains 1295 5-field citations to 122 computer sci-ence papers. While it is a relatively small-scale dataset, it requires computationally intensive string similarity functions and benefits from collective linkage methods, justifying the need for blocking [4, 24]. Addresses is a dataset containing names and addresses of 50,000 9-field records for 10,000 individuals that was generated us -ing the DBG EN program provided by Hernandez and Stolfo [15]. We use the following  X  X ase X  blocking predicates:  X  X xactString-Match X  ,  X  X ontainCommonToken X  ,  X  X ontainCommonTokenNGram X  with N = 2 , 3 , 4 and  X  X irstNCommonChars X  with N = 1 , 3
Figures 4 and 5 show the precision-recall curves obtained us-ing the two types of learned blocking functions described above. For DNF blocking, k is the maximum length of conjunctions con-structed by the greedy algorithm. Increasing the maximum con-junction size beyond 3 did not lead to performance improvements, as the longer conjunctions were rarely preferred by the learning al-gorithms over the shorter conjunctions. From these results, we ob-serve that DNF blocking is more accurate than disjunctive blocking at 100% recall. The predicate conjunctions learned by the DNF al-gorithm can be viewed as  X  X igh-precision, low-recall X  features that cover a smaller number of positives but very few negatives com-pared to single predicates.

Table 2 shows the average number of blocking predicates com-prising the learned functions of each type (for DNF blocking, con-junctions are counted as single predicates since at blocking time a single inverted index is built for each conjunction). These results demonstrate that because DNF blocking functions are comprised of Table 2: Average number of predicates in learned blocking functions  X  X igh-precision, low-recall X  features compared to disjunctive and weighted blocking, a larger number of such features is required to cover all positives and obtain 100% recall, while leading to higher precision.
A number of blocking methods have been proposed by researchers within the context of the record linkage task [15, 18, 24, 1, 6, 19, 14, 37]. These methods largely focus on improving efficiency un-der the assumption that an accurate blocking function is known. In [37], Winkler discusses the need for methods that automate the search for an optimal blocking strategy and suggests an unsuper-vised method for estimating the quality of existing hand-constructed blocking predicates. Integrating this capture-recapture methodol-ogy with our approach is an interesting avenue for future work.
A number of methods for fast nearest-neighbor searching [16, 22] are closely related to the adaptive blocking problem, however these techniques typically rely on strong metric assumptions in the data space, while our approach works with arbitrary blocking pred-icates.
In this paper, we have formalized the problem of adapting the blocking function to a given domain, described two types of block-ing functions, and provided learning algorithms for training the blocking functions. In future work, we are interested in extend-ing our adaptive blocking approach to other information integration tasks, such as clustering and schema mapping. An interesting open question is whether blocking can be used for kernel methods [32], where the need to construct a kernel matrix over all pairs of in-put instances along with the requirement that the matrix be positive semidefinite presents a significant obstacle to scaling up to large datasets. We would like to thank Mauricio Hernandez for providing the DBG EN data generator, and Joseph Modayil, Julia Chuzhoy and Alex Sherstov for useful comments. This work was supported by a research grant from Google Inc. [1] R. Baxter, P. Christen, and T. Churches. A comparison of fast [2] I. Bhattacharya and L. Getoor. A latent dirichlet model for [3] M. Bilenko, S. Basu, and M. Sahami. Adaptive product [4] M. Bilenko and R. J. Mooney. Adaptive duplicate detection
Figure 5: Blocking accuracy results for the Addresses dataset [5] R. D. Carr, S. Doddi, G. Konjevod, and M. Marathe. On the [6] S. Chaudhuri, K. Ganjam, V. Ganti, and R. Motwani. Robust [7] V. Chvatal. A greedy heuristic for the set covering problem. [8] W. W. Cohen, H. Kautz, and D. McAllester. Hardening soft [9] W. W. Cohen and J. Richman. Learning to match and cluster [10] A. Doan, J. Madhavan, P. Domingos, and A. Y. Halevy. [11] X. Dong, A. Halevy, and J. Madhavan. Reference [12] I. P. Fellegi and A. B. Sunter. A theory for record linkage. J. [13] C. L. Giles, K. Bollacker, and S. Lawrence. CiteSeer: An [14] L. Gu and R. Baxter. Adaptive filtering for efficient record [15] M. A. Hern  X  andez and S. J. Stolfo. The merge/purge problem [16] P. Indyk and R. Motwani. Approximate nearest neighbors: [17] A. K. Jain, M. N. Murty, and P. J. Flynn. Data clustering: A [18] M. A. Jaro. Advances in record-linkage methodology as [19] L. Jin, C. Li, and S. Mehrotra. Efficient record linkage in [20] S. Lawrence, K. Bollacker, and C. L. Giles. Autonomous [21] X. Li, P. Morie, and D. Roth. Robust reading: Identification [22] T. Liu, A. Moore, A. Gray, and K. Yang. An investigation of [23] A. McCallum and B. Wellner. Conditional models of identity [24] A. K. McCallum, K. Nigam, and L. Ungar. Efficient [25] S. N. Minton, C. Nanjo, C. A. Knoblock, M. Michalowski, [26] A. E. Monge and C. P. Elkan. An efficient [27] A. Ng, M. Jordan, and Y. Weiss. On spectral clustering: [28] G. N. Nor  X  en, R. Orre, and A. Bate. A hit-miss model for [29] H. Pasula, B. Marthi, B. Milch, S. Russell, and I. Shpitser. [30] D. Peleg. Approximation algorithms for the label-covermax [31] S. Sarawagi and A. Bhamidipaty. Interactive deduplication [32] B. Sch  X  olkopf and A. J. Smola. Learning with kernels -[33] W. Shen, X. Li, and A. Doan. Constraint-based entity [34] P. Singla and P. Domingos. Object identification with [35] S. Tejada, C. A. Knoblock, and S. Minton. Learning [36] W. E. Winkler. The state of record linkage and current [37] W. E. Winkler. Approximate string comparator search
