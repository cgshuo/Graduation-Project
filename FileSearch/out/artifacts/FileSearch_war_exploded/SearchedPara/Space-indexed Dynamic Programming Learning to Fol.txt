 J. Zico Kolter kolter@cs.stanford.edu Adam Coates acoates@cs.stanford.edu Andrew Y. Ng ang@cs.stanford.edu Yi Gu guyinet@stanford.edu Charles DuHadway duhadway@stanford.edu Computer Science Department, Stanford University, CA 94305 We consider the task of learning to accurately follow a trajectory, for example in a car or helicopter. This is one of the most basic and fundamental problems in reinforcement learning and control. One class of ap-proaches to this problem uses dynamic programming. These algorithms typically start at the last time-step T of a control task, and compute a simple (say, lin-ear) controller for that time-step. Then, they use dy-namic programming to compute controllers for time-steps T  X  1, T  X  2 and so on down to time-step 1. Some examples of algorithms in this family include (Jacobson &amp; Mayne, 1970; Bagnell et al., 2004; Atke-son &amp; Morimoto, 2003; Lagoudakis &amp; Parr, 2003), and all of them output time-varying/non-stationary poli-cies that choose the control action as a function of time. Given that following a trajectory requires one to choose very different control actions at different parts of the trajectory  X  for example, the controls while driving a car on a straight part of the trajectory are very different from the controls needed during a turn  X  these dynamic programming algorithms therefore ini-tially seem well-suited for trajectory following. However, a weakness in the naive dynamic program-ming approach is that the control policies are time-indexed . That is, these algorithms output a sequence of controllers  X  1 ,  X  2 , . . . ,  X  T and execute controller  X  at time t . However, as time passes, the uncertainty over the state increases, and this can greatly degrade controller performance. For example, suppose we are driving a car around a track with both straight and curved portions, and suppose that the controller at time t assumed the car was on a curved portion. If, due to the natural stochasticity of the environment, the car was actually on a straight portion of the track at this time, the resulting controller would perform very poorly, and this problem increases over time. This problem can be alleviated slightly be  X  X e-indexing X  the controllers by state during execution. That is, at time t we do not necessarily execute controller  X  t , but in-stead we examine all the controllers  X  1 , . . . ,  X  T , and execute the controller whose corresponding state is closest to the current state  X  several variations on this approach exist, and we will discuss them further in Section 5. However, there are two fundamental lim-itations of this general method. First, because we are executing a different policy from the one learned by dynamic programming, it is difficult to provide any of the performance guarantees that often accompany the purely time-indexed dynamic programming algo-rithms. Second, and more fundamentally, the uncer-tainty over states in the distant future often make it extremely difficult to learn any good policy using the time-indexed algorithms. This means that regardless of how we re-index the controllers during execution, we are unlikely to obtain good performance.
 In this paper we propose a method for space-indexed dynamic programming that addresses both these con-cerns. More precisely, we will define a spatial index variable d that measures how far we have traveled along the target trajectory. Then, we will use policies  X  d that depend on d  X  where we are along the trajec-tory  X  rather than the current time t . In order to learn such policies, we define the notion of a space-indexed dynamical system , and show how various dynamical systems can be rewritten such that their dynamics are indexed by d instead of by time t . This then allows us to extend various dynamic programming algorithms to produce space-indexed policies  X  in particular, we develop a space-indexed versions of the Differential Dynamic Programming (DDP) (Jacobson &amp; Mayne, 1970) and Policy Search by Dynamic Programming (PSDP) algorithms (Bagnell et al., 2004). Finally, we successfully apply this method to several control tasks, both in simulation and in the real world.
 The remainder of this paper is organized as follows. In Section 2 we show how to transform a standard (time-indexed) dynamical system into a space-indexed dynamical system. Using this transformation, in Sec-tion 3 we develop space-indexed versions of the DDP and PSDP algorithms. In Section 4 we present ex-perimental results on several control tasks. Finally, in Sections 5 and 6 we discuss related work and conclude the paper. Standard dynamic programming algorithms are very efficient because they know in advance that the policies  X  , . . . ,  X  T will be executed in a certain sequence (and that each policy will be executed only once), and can thus solve for them in reverse order. The key difficulty of generalizing a dynamic programming algorithm to the space-indexed setting is that it is difficult to know in advance where in space (i.e., how far along the tra-jectory) the vehicle will be at each step, and thus which space-indexed policy will be executed when. For ex-ample, if the vehicle is currently at space-index d , then there is no guarantee that executing policy  X  d for one time-step will put the vehicle in space-index d +1. But if  X  d might be executed multiple times before switch-ing to  X  d +1 , then in general its parameters cannot be solved for in closed form during the dynamic program-ming backup step, and require some complex policy search instead. In this section we discuss a method for addressing this problem. Specifically, we will rewrite the dynamics of a system so that the states and transi-tions are indexed by the spatial-index variable d rather than by the time t .
 Suppose we are given a general non-linear dynamics model in the form of a (possibly stochastic) differen-tial equation  X  s = f ( s, u ), where s  X  R n denotes the state vector u  X  R m denotes the control input, and  X  s denotes the derivative of the state vector with respect to time. While some classical control algorithms op-erate directly on this differential equation, a common technique in reinforcement learning and control is to create a discrete-time model of the system by numerical integration, where s t and u t denote the state and input at time t respectively, and w t is a zero-mean IID noise term (typically taken to be Gaussian with some prespecified covariance, for example). A simple but very common method for achieving this discretization is by Euler integration. In this case the state evolves as where  X  t is the integration time constant (the vari-ance of w t will scale linearly with the time constant as well). Note that even though the system evolves in continuous time, by making the decision to model it as a discrete-time system, we have made a decision to explicitly represent the state only at certain instants in time ( t =  X  t, t = 2 X  t, . . . ).
 When transitioning to a space-indexed dynamical sys-tem, we instead will explicitly represent the state only when it is at certain points along the trajectory. We begin by representing the time-indexed state as s = [ x t ,  X  t ] T , where x  X  R p represents what we refer to as the spatial portions of the state (in this paper we typically consider the spatial portions of the state to be the 2D or 3D position). Now, assume we are given a target trajectory in the space of x , such as the curved path shown in Figure 1. We choose a total of D discrete points along the trajectory, and designate the target state at these points as x  X  1 , x  X  2 , . . . , x the space-indexed dynamical system, we will explicitly represent the state only when the state lies on a hy-perplane which is orthogonal to the target direction of travel and which passes through the one of the target points x  X  d . More formally, we let  X  x  X  d be the instanta-neous direction of motion (along the target trajectory) at point d . We will then explicitly represent the state only when ( x  X  x  X  d ) T  X  x  X  d = 0. This situation is depicted in Figure 1.
 Because we constrain the state in this manner, our space-indexed state will have a different set of variables as our time-indexed state. In particular, we represent the space-indexed state as  X  s d = [ t d ,  X  d ,  X  d ] T R denotes the time of the system,  X  d  X  R p  X  1 denotes the lateral deviation from the target trajectory  X  for x ) T  X  x  X  d = 0 can be represented using p  X  1 dimensions and this gives the lateral deviation term  X  and  X  d denotes the non-spatial portions of the state as before. The time variable t d is kept for completeness, but it can be ignored if the policies do not depend on time. Now we can rewrite the dynamics so that they are in-dexed by space rather than time; this will give us an equation for computing  X  s d +1 from  X  s d . For simplicity, we develop an Euler integration-like method, but the technique could also be extended to higher-order nu-merical integration methods. Rather than simulating the system forward by a fixed time step, we solve for  X  t such that the next state will lie exactly on the d +1 plane. Temporarily ignoring the noise term, we solve for  X  t . Note that a positive solution for  X  t may not always exist, but it typically does except in degener-ate cases, when the vehicle starts moving perpendic-ular or backward with respect to the desired direc-tion, and this is unlikely given any reasonable con-troller. When  X  t &gt; 0 does exist, we can compute s t + X  t = s t + f ( s t , u t ) X  t and use this to find the next space-indexed state us our final space-indexed simulator in the form where  X  w d is a noise term. Although the distribution of  X  w d is in general quite complex, we can apply meth-ods from stochastic calculus to efficiently draw sam-ples from this distribution. However, in practice we find that a simpler approximate approach works just as well: we compute s t + X  t as above, assuming no noise, then add noise as in the time-indexed model. This will result in a point that may no longer lie exactly on the space-index plane, so lastly we form the line between the states s t and s t + X  t and let  X  s d +1 be the point where this line intersects the d + 1 plane. In this section we use the techniques presented in the previous section to develop space-indexed versions of the Differential Dynamic Programming (DDP) and Policy Search by Dynamic Programming (PSDP) al-gorithms. We begin by defining notation. Let S be the state space and A be the action space (so that in the context of the dynamical system above, S = R n and A = R m ). Since, as described above, there ex-ists a one-to-one mapping from time-indexed states to space-indexed states, all the quantities below can be equivalently expressed in terms of the space-indexed state. A reward function is a mapping R : S  X  R and a policy is a mapping  X  : S  X  A . Given a non-stationary sequence of policies (  X  t , . . . ,  X  T ) we define the value function 3.1. Space-indexed DDP We first review (time-indexed) DDP briefly. DDP ap-proximates the dynamics and cost function of a system along a specific sequence of states. Given an initial controller  X  init , DDP simulates the system to gener-ate a sequence of states s 1 , . . . , s T . It then linearizes the dynamics around these points to obtain a time-varying linear dynamical system, and forms a second-order (quadratic) approximation to the reward func-tion. This system can then be solved by the Linear Quadratic Regulator (LQR) algorithm (Anderson &amp; Moore, 1989), which results in a new controller and a new sequence of states. This process is repeated until convergence.
 Space-indexed DDP proceeds in the same manner. Given some initial controller  X  init and space-indexed system forward for D space-indexes, resulting in a set of states  X  s 1 , . . . ,  X  s D . Using our dynamics model, we form the first order Taylor expansion of the dynamics at each point along the trajectory, which results in the (space-indexed) linearized dynamics: As in standard DDP, we form a quadratic approxima-Q d is some (usually PSD) matrix. This reduces the problem to an LQR problem, which can be solved ef-ficiently using a backward recursion. 3.2. Space-indexed PSDP We now briefly review PSDP. As input, PSDP takes a time horizon T , a restricted policy class  X , and a se-quence of baseline distributions over the states space , . . . , T , where we can informally think of t as providing a distribution over which states would be visited at time t by a  X  X ood X  policy. Given policies  X  t +1 , . . . ,  X  T , PSDP computes (or approximates via Monte-Carlo sampling and parameter search) By starting with t = T and proceeding down to t = 1, the algorithm is able to generate a sequence of policies that can perform well on the desired task. The space-indexed version of PSDP proceeds exactly as above, replacing the time t with the space index d and using the space-indexed simulator to generate the Monte-Carlo samples.
 Just as in the time-indexed version, the space-indexed version of PSDP comes with nontrivial performance guarantees, formalized by the theorem below. The theorem follows immediately from the equivalent the-orem for the time-indexed version of PSDP, and from the fact that the space-indexed dynamics and reward function do not depend on time.
 Theorem 3.1 [following (Bagnell et al., 2004)] Sup-pose  X  = (  X  1 , . . . ,  X  D ) is a policy returned by an  X  -approximate version of state-indexed PSDP where on each step the algorithm obtains  X  d such that Then for all  X  ref  X   X  D , where is the baseline distribution over space-index states (without the time component) provided to SI-PSDP, d var denotes the average variational dis-tance, and  X   X  This bound not only provides a performance guarantee for the space-indexed PSDP algorithm, it also helps to elucidate the advantage of space-indexing over time-indexing. The bound implies that to make PSDP and SI-PSDP perform as well as possible, it would be best to provide them with  X   X  , the baseline distribution of the optimal controller, as the baseline distribution. But for time-indexed PSDP, the natural stochasticity of the environment can cause this distribution to be highly spread out over the state space, even for the optimal policy. Therefore, when performing the maxi-mization (2), it is likely that no policy in the class will perform very well, since this would require a policy that could operate well over many different regions of the state space. Thus, regardless of whether or not we re-index the resulting controllers by state during exe-cution, the time-indexed version of PSDP would fail to find a good policy. In contrast, if we are doing a good job following the trajectory, then we would expect that the distribution over states at each space-index would be much tighter, allowing the space-indexed PSDP to perform much better. 4.1. Autonomous Driving We begin by considering the problem of autonomously and accurately following a trajectory with a car, such as that shown in Figure 4. Our first set of experiments were carried out in a simulator of the vehicle, built following (Rossetter &amp; Gerdes, 2002) (with model pa-rameters such as the vehicle dimensions, total weight, etc). To follow the desired trajectory, we applied the space-indexed DDP algorithm described above.
 Prior to the work presented in this paper, significant engineering effort went into a hand-designed trajec-tory following controller; this was an initial version of the controller described in (Hoffmann et al., 2007), which was a hand-optimized, linear, regulation con-troller that computes its actions as a function of state features such as lateral error, orientation error, and so on. We used this controller to generate an initial trajectory for our space-indexed DDP algorithm; how-ever, the results of space-indexed DDP were actually very insensitive to the choice of this initial controller. Figure 2 shows the performance of the space-indexed DDP algorithm and the hand-tuned controller in sim-ulation, when following an oval-like track at 30mph, along with a magnified view of the show the perfor-mance on one of the turns. We see that space-indexed DDP outperforms the hand-tuned controller; our con-troller has an RMS lateral error 0.26m, whereas the hand-tuned controller X  X  RMS error is 1.18m.
 Figure 3 shows a comparison between the performance of space-indexed and time-indexed dynamic program-ming. Due to the stochasticity of the simulator, the actual state s t , for large t , is increasingly unlikely to be close to where the t -th step of the linearization oc-curred. Therefore the linearized approximation is less likely to be an accurate approximation of the  X  X ocal X  dynamics at time t . This is reflected in the figures: the time-indexed controllers initially perform well, but as time passes the controllers start to be executed at incorrect points along the trajectory, eventually lead-ing the vehicle to veer off course. Using the space-indexed controller, however, the vehicle is able to ac-curately track the trajectory even for an arbitrarily long amount of time. For this relatively simple tra-jectory following task, re-indexing the time-indexed controllers by their spatial state, as described in the introduction, does perform well. However, as we will demonstrate in the next section, for more complex con-trol tasks this is not the case.
 We also tested our method on the actual vehicle; the vehicle itself is described further in (Thrun &amp; al., 2006). Figure 4 shown a typical result from an ac-tual run on the vehicle moving at 10mph. The RMS error on the actual vehicle was about 0.17m, and the target and actual trajectories are indistinguishable in the figure. 4.2. Autonomous Driving with Obstacles We next consider the more challenging control task of following a trajectory in the presence of obsta-cles. For this task we evaluated our methods on an RC car, shown in Figure 5. Since we want to learn a single controller that is capable of avoiding obsta-cles placed at arbitrary points along the trajectory, DDP is a poor algorithm (DDP learns a controller for a single fixed trajectory, but in this task we need to follow different trajectories depending on the lo-cation of the obstacles). Therefore, we apply the space-indexed version of PSDP to this task. Videos of the resulting controllers for this task are available at: http://cs.stanford.edu/  X  kolter/icml08videos . In greater detail, we applied the space-indexed PSDP algorithm as follows. The native action space for the car domain is a two-dimensional input specifying the velocity and steering angle (between -28 degrees and 28 degrees), but for our task we kept the velocity fixed at 1.5 m/s and discretized the commanded steering angle into five equally spaced angles. To generate the initial distribution for PSDP, 1 , . . . , D , we sampled 2000 different trials in a simulator of the car, using a PD controller. Then, for each space index from D  X  1 down to 1, we (approximately) solved the optimization problem (2) by first trying each possible action for each of the 2000 sampled states, then executing the learned controller for all subsequent space-indices to compute learn the optimal action as a linear function of vari-ous state features; in particular, each controller was of the form: u steer = arg max i w T i  X  ( s ) where w i  X  R n is a weight vector learned by a multi-class SVM-like algorithm 3 , and  X  ( s )  X  R n is a feature vector. In our setting the features comprised of 1) the x and y loca-tion of the car, 2) the sine and cosine of the current car orientation, 3) 16 exponential RBF functions, spaced uniformly around the car, indicating the presence of an obstacle, and 4) a constant term. In addition to the space-indexed version, we also evaluated the per-formance of a pure time-indexed version, and a time-indexed version where we re-index the controllers as follows: at time t rather than execute the controller  X  , we examine all the controllers  X  1 , . . . ,  X  T and ex-ecute the controller  X  t  X  with minimum distance from the current state to the mean of the distribution t  X  . Table 1 shows the average cost incurred and total num-ber of collisions for the different controllers in 1000 simulated trials, where each trial had three randomly placed obstacles on the trajectory. As can be seen, the space-indexed version outperforms all the other variants of the algorithm as well as a hand-tuned PD controller that we previously spent a good deal of time trying to tune. The performance benefits of the space-indexed controller become even more pronounced on the real system. Figure 6 shows typical resulting tra-jectories from the space-indexed controller, the pure time-indexed controller, and the time-indexed con-troller with re-indexing. Due to the stochasticity of the real domain, the pure time-indexed approach per-forms very poorly. Re-indexing the controllers helps significantly, but the space-indexed version still per-forms substantially better (an incurred cost of 49.32 for space-indexed versus 59.74 for time-indexed with re-indexing, and the latter controller will nearly always hit at least one of the obstacles on the track). As seen in the figure, the space indexed version is able to track the trajectory well, while reliably avoiding obstacles. 4.3. Autonomous Helicopter Flight We also apply these ideas to a simulated autonomous helicopter. This work used a stochastic simulator of the autonomous helicopter shown in Figure 7, and we considered the problem of making accurate, high-speed (5m/s) turns on this helicopter. We applied the space-indexed PSDP algorithm to this task due to the fact that the policy search setting allowed us to greatly restrict the class of control policies  X  d under consid-eration (the space of all control policies for helicopter flight is very large, so we wanted to limit the risk of un-expected behavior). In particular, the  X  X ctions X  of the controllers corresponded to picking a location of set point x  X  which is then fed into a regulation controller, such as those described in (Bagnell &amp; Schneider, 2001; Ng et al., 2004). Space constraints preclude a full de-scription of the environment and algorithm, but the overall algorithm proceeds as in the previous section. Figure 8 shows a typical result of applying the space-indexed PSDP algorithm to this task, along with the trajectory taken by a simple linear regulation con-troller. By varying the set point differently at different points along the trajectory, the space-indexed PSDP algorithm follows the trajectory much more accurately. The idea that a control policy should be dependent on the system X  X  spatial state is by no means a new idea in the reinforcement learning and control literature. In the Markov Decision Process (MDP) formalism (Put-erman, 1994), a policy is a mapping from states (which typically describe the spatial state of the system) to actions. In light of this observation, many classical dynamic programming algorithms such as value iter-ation or policy iteration can be viewed as perform-ing dynamic programming on spatial states. However, in high-dimensional, continuous state spaces, the well-known  X  X urse of dimensionality X  renders a naive appli-cation of these algorithms intractable. Indeed, it is this reality that often motivates the jump to the trajectory following approach, where we want to find polices that perform well along the trajectory in particular. There are also a number of methods for trajectory fol-lowing. A common approach is to design a  X  X egulation controller X  that can keep the vehicle stable at a spec-ified position x  X  . By smoothly moving x  X  along the target trajectory, we can cause the vehicle to move along this path (Franklin et al., 1995; Dorf &amp; Bishop, 2000). This approach works well when the regulation controller has very high bandwidth  X  i.e., if it can track x application areas such as control of robot arms. But in more general settings in which the actual state x of the vehicle tends to lag well behind changes to x  X  , one often ends up manually and laboriously adjusting the regulation controller to try to obtain proper trajectory following performance. There are methods for com-pensating for this lag such as feedback linearization (Sastry, 1999), and there have also been many meth-ods devised for trajectory following on specific systems (Egerstedt &amp; Hu, 2000; Johnson &amp; Calise, 2002). How-ever, we know of no method for trajectory following in the general case of the nonholonomic, underaccuated vehicles that we consider.
 The idea of partitioning the state space into regions, and using different controllers in the different regions, is a common practice in control, and often is referred to as gain-scheduling (Leith &amp; Leithead, 2000). Taken in the general sense, the algorithm we present in this paper can be viewed as a method for gain-scheduling, though more often the term is used for a particular application of this approach to the contexts of lin-ear parameter varying systems. Such methods typ-ically linearize the dynamical system around certain operating points, learn controllers at each point, and smoothly interpolate between controllers at various lo-cations. However, the focus of this work is often to prove stability of such controllers using Lyapunov the-ory, and the overall approach is substantially different from what we consider here.
 Model predictive control (MPC) (Garcia et al., 1989) (indirectly) addresses the issue of state uncertainty in-creasing over time, by explicitly computing new con-trollers at every time step in an online manner. How-ever, MPC is generally orthogonal to the ideas we present here, since one could just as easily use a space-indexed dynamic programming method for the local controller in MPC. Furthermore, MPC can often times be computationally impractical to run real-time. An alternative approach is to use a local control method, such as DDP, in order to estimate the value function along several trajectories, and use these local estimates to build an approximate global model of the value function (Atkeson, 1994; Tassa et al., 2007). How-ever, since these methods employ DDP, which is a time-indexed algorithm, they can potentially suffer the same problems as time-indexed methods in general. In this paper we presented a space-indexed dynamic programming method for trajectory following. We showed how to convert standard time-indexed dynam-ical systems into equivalent space-indexed dynamical systems, and used this formulation to derive space-indexed versions of two well-known dynamic program-ming algorithms, DDP and PSDP. Finally, we success-fully applied these methods to several control tasks, and demonstrated superior performance compared to their time-indexed counterparts.
 Acknowledgments. This work was supported by the DARPA Learning Locomotion program under con-tract number FA8650-05-C-7261. We thank the anony-mous reviewers for helpful comments, Sam Schreiber and Quan Gan for assistance with the RC car, Pieter Abbeel for assistance with the helicopter domain, Mark Woodward, Mike Montemerlo, Gabe Hoffmann, David Stavens and Sebastian Thrun for assistance with the DARPA Grand Challenge vehicle.

