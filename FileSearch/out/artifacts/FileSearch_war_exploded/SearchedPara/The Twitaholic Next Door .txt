 In this paper we present a Friend Recommender System for micro-blogging. Traditional batch processing of mas-sive amounts of data makes it difficult to provide a near-real time friend recommender system or even a system that can properly scale to millions of users. In order to over-come these issues, we have designed a solution that repre-sents user-generated micro posts as a set of pseudo-cliques. These graphs are assigned a hash value using an original Concept-Sensitive Hash function, a new sub-kind of Locally-Sensitive Hash functions. Finally, since the user profiles are represented as a binary footprint, the pairwise comparison of footprints using the Hamming distance provides scalability to the recommender system. The paper goes with an online application relying on a large Twitter dataset, so that the reader can freely experiment the system.
 H.4 [ Information Systems Applications ]: Miscellaneous; E.1 [ Data Structures ]: Graphs and networks Theory, Algorithms Social Networks, Twitter, Friends, Recommender System, Graph, Pseudo-Clique, Locally-Sensitive Hash.
 systems predict links in a social graph. Both types of rec-ommenders rely on a characterization of the user. For exam-ple [12] is using Twitter for topical news recommendation. The system crawls Twitter and RSS feeds, indexes terms from both sources (as well as user X  X  friends on Twitter) us-ing Lucene X  X  TF-IDF library and provides recommendation based either on the user X  X  public timeline or friends X  time-lines matching with the article terms. Regarding user rec-ommender, Twopics [9] uses Wikipedia to query for named entities in tweets in order to build a topic profile. Even if no performance analysis is provided, [5] showed that executing SPARQL queries over the web of linked data takes at least 20 seconds even with all data locally retrieved in advance, which discards de facto such an approach for real-time pur-pose. Given the hindrance due to the Web of Data speaking of performance at query time, only a single document ap-proach can be used when scalability is at stake. The bag of words approach to characterize users or documents (as used in [12, 4]) has shown its limitations, and machine learning techniques have been developed to go beyond bag-of-words representation. The most popular are continuous Condi-tional Random Fields [7] and Latent Dirichlet Association (LDA) [1]. The main drawback of these machine learning techniques is the learning part, which is prohibitively ex-tensive for real-time processing. This explains why the on-line learning paradigm is gaining momentum in the Machine Learning community. In this work, we investigate whether original document-centric approach that makes use of statis-tics and graph techniques could still scale while preserving the advantage to exploit Semantic relatedness between terms in documents.
In order to provide the most efficient user profile from tweets, we applied standard text pre-processing strategies (tokenization and stop words deletion). Then the next step is inspired by the paper of Matsuo &amp; Ishizuka [8] on key-words extraction. They present a method to extract key-words from a single document using statistical information. In a first step their algorithm computes the co-occurrence matrix of the terms in each sentence of the document. Fre-quent terms are counted, and then clustered by pairs ac-cording to a threshold. The most representative term of the cluster item serves as a representative for terms in the same cluster. Instead of clustering terms in a pairwise man-ner as proposed in [8], we opted for a graph representation. We look for clusters of terms in order to model the user-generated content. In our work, each term (one of the graph edges) is given a weight, which is the  X  X reshness X  associated to this cluster. It is an age indication, calculated as the current time of computation minus the date of last use of the term. This information is normalized accross all terms employed by a given user.

To sum up, our algorithm characterizes the user by ex-tracting the meaningful clusters from his sequence of tweets and encode this as a graph. We model the document ex-traction result as a graph. As a consequence, the problem of measuring semantic relatedness is to look for clusters in the graph. From a graph point of view, pairwise clustering from [8] is equivalent to the search for connected components in the graph of terms. In graph theory, a maximal complete subgraph is called a clique (i.e. a clique is a subgraph with a density equal to 1). The problem of finding cliques in a cliques when they are represented as hash values. Using a cryptographic hash function to a pseudo-clique would only result in obtaining possibly completely different hash values for two near-similar pseudo-cliques. This is because a cryp-tographic hash function needs to avoid preserving distances between original data and encoded data. Unlike crypto-graphic hash functions, Locally Sensitive Hash (LSH) func-tions[6] aim at preserving likelihood between original data and their corresponding hash values. Our intuition is to make use of a LSH function to hash terms in the graph of terms, and then to generate a footprint from the graph of hashs. [3] proposed SimHash, a hash function for generating a footprint out of a graph. SimHash can be applied to any kind of resource (document, images . . . ), and in our case a graph. In SimHash, the resource, usually a document, is splitted into token, possibly weighted. Each token is then represented as its hash value, as the result of a traditional cryptographic function applied to the token, which is orig-inally a string. Then, a vector V, of length of the desired hash size, is initialized to 0. For each hash value for the set of tokens, the i th element of V is decreased by the cor-responding token X  X  weight if the i th bit of the hash value is 0. Otherwise, the i th element of V is increased by the corresponding token X  X  weight. SimHash works well even for small fingerprints [3]. Nonetheless, the use of a traditional function on string for obtaining hash values of a token, re-stricts its usage to the detection of near duplicates. SimHash was historically applied to the detection of near-duplicates of webpages. It is possible to use SimHash in order to make a footprint of a pseudo-clique with, given our user profile representation, the following settings: bag of terms in English. We created a hash function that produces a hash value from a term depending on its loca-tion in the Wordnet hypernym tree. In order to do so, we built a reverse table of Wordnet that associate a hash value to Wordnet nodes. The hash value is constructed by per-forming a depth-first search of Wordnet entries, and tagging each entry with a unique and minimal binary value, whose length is determined by the number of siblings for each en-try. In order to uniquely encode all concepts, 117 bits are needed. In other words, we associated a unique hash value of 117 bits for each node in Wordnet, and store this in an indexed table. We called this table  X  X ashwordnet X . This table was constructed once and is shipped in the server-side friend recommender software presented at Section 5. Consequently, each term in the pseudo-clique is associated with a unique hash from this table. Two terms belonging to the same concept in Wordnet will have the same hash values. Furthermore, two terms whose associated concepts are closely similar in Wordnet will be represented by similar hash values. When the SimHash function is applied, it will be sensitive to this similarity of token X  X  hash values, instead of only relying on the strict equality of strings between to-kens. The footprint can be expected to convey the Wordnet concepts information to which the terms in the pseudo-clique refer to.
The complete architecture of the system presented in the paper is provided at Figure 2. We have implemented the en-tire software architecture described earlier in this paper on a server-side component in the Java programming language. The system relies on a dataset that consists of 15,000,000 tweets issued by 150,000 users. The dataset also have the precomputed user profiles, which means the footprint of each user encoded from their tweets in the dataset, and according to our concept-sensitive hash function. It is hosted on a clus-ter of three Apache Cassandra 3 instances in our laboratory private infrastructure.
 A Web interface asks from the conference attendee its Twitter account name and the number k of friend recom-mendation that the end-user whishes. After the Twitter secure authentication through Twitter OAuth 4 service, the server-side component retrieves the end-user X  X  tweets. It constructs the end-user X  X  profile, which means it goes through all the steps as illustrated at Figure 2. Finally, the Web ap-plication displays the sorted list of the k closest users from our dataset, whose footprint are the closest to the end-user X  X  footprint according to a Hamming distance computation. Because the end-user X  X  tweets will be processed in seconds, and a recommendation is made out of two millions profiles, this demonstrates the scalability of the solution. The ap-plication is accessible online for anyone to experiment the solution on his own account(s) 5 .
This paper introduced a new scalable and near real-time approach for friend recommendation on Twitter. The friend http://cassandra.apache.org/ http://oauth.net/ http://demo-satin.telecom-st-etienne.fr/ lshrecommender/
