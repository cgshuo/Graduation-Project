 Recent studies ( Zapirain et al. (2013); Srikumar and Roth (2011)) have shown the value of prep o-sitional phrases in joint modeling with verbs for semantic role labeling. Although recent studies have shown improved preposition disambigu a-tion, they have received little systematic trea t-ment from a lexicographic perspective. Recently, a new corpus has be en made available that pro m-ises to be much more representative of prepos i-tion behavior. Our initial examination of this corpus has suggested clear indications of senses previously overlooked and reduced prominence for senses thought to constitute a large r ole in preposition use.
 In section 2, we describe the interface to the Pattern Dictionary of English Prepositions (PDEP), identifying how we are building upon data developed in The Preposition Project (TPP) and investigating its sense inventory with corp o-ra also made available under TPP . Section 3 d e-scribes the procedures for tagging a represent a-tive corpus drawn from the British National Co r-pus, including some findings that have emerged in assessing previous studies of preposition di s-ambiguation. Section 4 describes how we are able to investigate the relationship of WordNet, FrameNet, and VerbNet to this effort and how this examination of preposition behavior can be used in working with these resources. Section 5 describes how we can use PDEP for the analy sis of semantic role and semantic relation invent o-ries. Section 6 describes how we envision further developments of PDEP and how the data are available for further analysis. In section 7, we present our conclusions for PDEP. Litkowski and Hargraves (2005) and Litkowski and Hargraves (2006) describe The Preposition Project (TPP) as an attempt to describe prepos i-tion behavior using a sense inventory made available for public use from the Oxford Di c-tionary of Engl ish ( Stevenson and Soanes, 200 3 ) by tagging sentences drawn from FrameNet. In TPP, each sense was characterized with its co m-plement and attachment (or governor) properties, its class and semantic relation, substitutable prepositions, its syntactic position s, and any FrameNet frame and frame element usages (where available). The FrameNet sentences were sense -tagged using the sense inventory and were later used as the basis for a preposition disa m-biguation task in SemEval 2007 (Litkowski and Hargraves, 2007).

Initial results in SemEval achieved a best a c-curacy of 69.3 percent (Ye and Baldwin, 2007). The data from SemEval has subsequently been used in several further investigations of prepos i-tion disambiguation. Most notably, Tratz (2011) achieved a result of 8 8.4 percent accuracy and Srikumar and Roth (2013) achieved a similar result. However, Litkowski (2013 b ) showed that these results did not extend to other corpora, concluding that the FrameNet -based corpus may not have been representative, with a reduction of accuracy to 39.4 percent using a corpus deve l-oped by Oxford.

Litkowski (2013a) announced the creation of the TPP corpora in order to develop a more re p-resentative account of preposition behavior. The TPP corpora includes three subcorpora: (1) the full S emEval 2007 corpus (drawn from FrameNet data, henceforth FN), (2) sentences taken from the Oxford English Corpus to exe m-plify preposition senses in the Oxford Dictionary of English (henceforth, OEC), and (3) a sample of sentences drawn from the written por tion of the British National Corpus (BNC) , using the Word Sketch Engine as implemented in the sy s-tem for the Corpus Pattern Analysis of verbs (henceforth, CPA or TPP ).

We have used the TPP data and the TPP co r-pora to implement an editorial interface, the P a t-tern Dictionary of English Prepositions (PDEP). 1 This dictionary is intended to identify the prot o-typical syntagmatic patterns with which prepos i-tions in use are associated, identifying linguistic units used sequentially to make well -formed structures an d characterizing the relationship b e-tween these units. In the case of prepositions, the units are the complement (object) of the prepos i-tion and the governor (point of attachment) of the prepositional phrase. The editorial interface is used to make changes in the underlying dat a-bases , as described in the following subsections. Editorial access to make changes is limited, but the system can be explored publicly and the u n-derlying data can be accessed publicly, either in its entirety or through publicly avail able scripts used in accessing the data during editorial oper a-tions.

Standard dictionaries include definitions of prepositions, but only loosely characterize the syntagmatic patterns associated with each sense. PDEP takes this a step further, looking for p rot o-typical sentence contexts to characterize the pa t-terns. PDEP is modeled on the principles of Co r-pus Pattern Analysis (CPA), developed to cha r-acterize syntagmatic patterns for verbs. 2 These principles are described more fully in Hanks (2013). Currently, CPA is being used in the pr o-ject Disambiguation of Verbs by Collocation to develop a Pattern Dictionary of English Verbs (PDEV). 3 PDEP is closely related to PDEV, since most syntagmatic patterns for prepositions are related to the main verb in a clause. P DEP is viewed as subordinate to PDEV, sufficiently so that PDEP employs significant portions of code being used in PDEV, with appropriate modific a-tions as necessary to capture the syntagmatic pa t-terns for prepositions. 4 2.1 The Preposition Inventory After a st art page for entry into PDEP, a table of all prepositions in the sense inventory is di s-played. Figure 1 contains a truncated snapshot of this table. The table has a row for each of 304 prepositions as identified in TPP. The second column indicates the numb er of patterns (senses) for each preposition. The next two columns show the number of TPP (CPA) instances that have been tagged and the total number of TPP i n-stances that have been obtained as the sample from the total number of instances in the BNC.
Additional columns not shown in Figure 1 show (1) the status of the analysis for the prep o-sition, (2) the number of instances from FrameNet (i.e., FN Insts, as developed for SemEval 2007), and (3) t he number of instances from the Oxford English Corpus (i.e., OEC Insts). The number of prepositions with FrameNet instances is 57 (larger than the 34 prepositions used in SemEval). There are no OEC instances for 57 prepositions. There are no TPP instances for 41 prepositions. Notwithstan d-ing the lack of instances, there are TPP chara c-te r izations for all 304 prepositions.

The BNC frequency shown in Figure 1 pr o-vides a basis for extrapolating results from PDEP to the totality of prepositions. In total, the nu m-ber of instances in the BNC is 5,391,042, which can be used as the denominator when examining the relative frequency of any preposition (e.g., between has a frequency of 0.0109, 58 , 865/5,391,042). 5 In general, the target sample size was 250 CPA instances. If the number available was less than 250, all instances were used. The TPP CPA corpus contains 250 instances for 170 prepos i-tions. Where the number of senses for a prepos i-tion was large (about 15 or more), larger samples from , at , into , over , like , and through ) were drawn. 2.2 Preposition Patterns When a row in Figure 1 is clicked, the prepos i-tion is selected and a new page is opened to show the patterns for that preposition. Figure 2 shows the four patterns for below . Each pattern is pr e-sented as an instance of the template [[Gove r-nor]] prep [[Complement]] , followed by its primary implicature, where the current definition is substituted for the preposition.

The display in Figure 2 provides an overview for each preposition, with the top line showing the number of tagged instances ava ilable from each corpus. For the TPP instances, this ident i-fies the number of instances that have been tagged and the number that remain to be tagged. In the body of the table, the first column shows the TPP sense number . The next three columns show the nu mber of instances that have been tagged with this sense. Note that the top line of the pattern list includes a menu option for adding a pattern, for the case when we find that a new sense is required by the corpus evidence.

Clicking on any row in the pattern list opens the details for that pattern, with a pattern box entitled with the preposition and the pattern number, as shown in Figure 3. The pattern box contains data developed in TPP and several new fields intended to cap ture our enhancements.
TPP data include the fields for the C ompl e-ment , the G overnor , the TPP C lass , the TPP R elation , the S ubstitutable P repositions , the S yntactic P osition , the Quirk R eference , the S ense R elation , and the C omment . We have added the checkb oxes for complement type (common nouns, proper nouns, WH -phrases, and -ing phrases), as well as a field to identify a pa r-ticular lexical item (lexset) if the sense is an id i-omatic usage. We have added the S elector fields for the complement and the governor . For the complement, we have a field Category to hold its ontological category (using the shallow onto l-ogy being developed for verbs in the DVC pr o-ject mentioned above). 6 We also provided a field for the S emantic C lass of the governor; this field has not yet been implemented.
 We have added two Cluster/Relation fields. The Cluster field is based on data available from Tratz (2011), where senses in the SemEval 2007 data have been put into 3 4 clusters. The Relation field is based on data available from Srikumar and Roth (2013), where senses in the SemEval 2007 data have been put into 32 classes. A key element of Srikumar and Roth was the use of these classes to model semantic relations across prepositions (e.g., grouping all the Temporal senses of the Se mEval prepositions). In the pa t-tern box, each of these two fields has a drop -down list of the clusters and relations, enabling us to categorize the senses of other prepositions with these classes. Below, we describe how we are able to use the TPP classes a nd relations along with the Tratz clusters and Srikumar rel a-tions in an analysis of these classes across the full set of prepositions, instead of just those used in SemEval.

Any number of pattern boxes may be opened at one time. The data in any of the fields may be altered (with the menu bar changing color to red) and then saved to the underlying databases. An individual pattern box may then be closed.

The drop -down box labeled Corpus Insta nces in the menu bar is used to open the set of corpus instances for the given sense. As shown in Figure 2, this sense has 6 FN instances, 20 OEC i n-stances, and 15 TPP instances. The drop -down box has an option for each of these sets, along with an option for all TPP instances that have not yet been tagged. When one of these options is selected, the corresponding set of instances is opened in a new tab, discussed in the next se c-tion. 2.3 Preposition Corpus Instances As indicated, selecting an instance set from the pattern box opens this set in a separate tab, as shown in Figure 4. This tab, labeled Annotation: below (3(1b)) , identifies the preposition and the sense, if any, assoc iated with the instance set (the sense will be identified as unk if the set has not yet been tagged. The instance set is displayed, identifying the corpus, the instance identifier, the TPP sense (if identified, or  X  X nk X  if not), the l o-cation in the sentenc e of the target preposition, and the sentence, with the preposition in bold. This tab is where the annotation takes place. Any set of sentences may be selected ; each s e-lected sentence is highlighted in yellow (as shown in Figure 6 ). The sense value may be changed using the drop -down box labeled Tag Instances in the menu bar. This drop -down box contains all the current senses for the preposition, along with possible tags x (to indicate that the instance is invalid for the preposition) and unk (to indicate th at a tagging decision has not yet been made). The sense tags in Figure 4 were originally untagged in the CPA (TPP) corpus and were tagged in this manner.

In general, sense -tagging follows standard le x-icographic principles, where an attempt is made to group instances that appear to represent di s-tinct senses. PDEP provides an enhanced env i-ronment for this process. Firstly, we can make use of the current TPP sense inventory to tag se n tences. Since the pattern sets (definitions) are based on the Oxford Dictiona ry of English , the likelihood that the coverage and accuracy of the sense distinctions is quite high. However, since prepositions have not general ly received the close attention of words in other parts of speech, PDEP is intended to ensure the coverage and a c-curacy. During the tagging of the SemEval i n-stances, the lexicographer found it necessary to increase the number of senses by about 10 pe r-cent. S ince the lack of coverage of FrameNet is well -recognized, the representative sample d e-veloped for the TPP corpus should provide the basis for ensuring the coverage and accuracy.
In addition to adhering to standard lexic o-graphic principles, the availability of the tagged FN and OEC instances can be used as the basis for tagging decisions. Where available, these tagged instances can be opened in separate tabs and used as examples for tagging the unknown TPP instances. 3.1 Examining Corpus I nstances The main contribution of the present work is the ability to interactively examine characteristics of the context surrounding the target preposition in the corpus instances. In the menu bar shown in Figure 4 , there is an Examine item. Next to it ar e two drop -down boxes, one labeled WFRs (word -finding rules) and one labeled FERs (feature e x-traction rules ) . These rules are taken from the system described in Tratz and Hovy (2011) and Tratz (2011). 7 The TPP corpora described in Litkowski (2013a) include s full dependency parses and feature files for all sentences. Each sentence may have as many as 1500 features d e-scribing the context of the target preposition. We have made the feature files for these sentences (1309 MB) available for exploration in PDEP.

In our system, we make available seven word -finding rules and nine feature extraction rules. The word -finding rules fall into two groups: words pertaining to the governor and words pe r-taining to the complement. The five governor word -finding rules are (1) verb or head to the left ( vl ), (4) word to the left ( wl ), and (5) governor ( h ). The two complement word -finding rules are (1) syntactic preposition complement ( c ) and (2) heuristic preposition complement ( hr ). The fe a-ture extraction rules are (1) word class ( wc ), (2) part of speech ( pos ), (3) lemma ( l ), (4) word ( w ), (5) WordNet lexical name ( ln ), (6) WordNet synonyms ( s ), (7) WordNet hypernyms ( h ), (8) whether the word is capitalized ( c ), and (9) affi x-es ( af ). Thus, we are able to examine any of 63 WFR FER combinations for whatever corpus set happens to be open.

In addition to these features, we are able to d e-termine the extent to which prepositions assoc i-ated with FrameNet lexical units and VerbNet class es occur in a given corpus set. In Figure 4, there is a checkbox labeled FN next to the FERs drop -down list to examine FrameNet lexical units. There is a similar checkbox labeled VN to examine members of VerbNet classes. These boxes appear only when either of these resources has identified the given preposition as part of its frame (75 for FrameNet and 31 for VerbNet) .

When a particular WFR -FER combination is selected and the Examine menu item is clicked, a new tab is opened showing the values for those fea tures for the given corpus set, as shown in Figure 5. The tab shows the WFR and FER that were used, the number of features for which the value was found in the feature data, the values, and the count for each feature. The description column is used when di splaying results for the part of speech, the affix type, FrameNet fram e elements, and VerbNet classes, since the value column for these hits are not self -explanatory. The example in Figure 5 is showing the lemma, which requires no further explanation.

For most features (e.g., lemma or part of speech), the number of possible values is rel a-tively small, limited by the number of instances in the corpus set. For features such as the WordNet lexical name, synonyms and hypernyms, the number of values may be much larger. For FrameNet and VerbNet, the feature examination is limited to the combination of the WFR for the governor ( h ) and the FER lemma ( l ), both of which will generally identify verbs in the value column.

The general objective of examining features is to identify those that are diagnostic of specific senses. When applied to the full untagged TPP corpus set, this process is akin to developing word sketches for prepositions (Kilgarriff et al., 2004). However, since we have tagged corpus sets for most preposition senses, we can begin our efforts looking at these sets. The hypothesis is that the tagged corpora will show patterns which can then be used for tagging instances in the TPP corpus. 8
The first step in examining features generally is to look at the word classes and parts of speech for the complement and the governor. 9 These are useful for filling in the ir checkboxes in Figure 3. Another useful feature is word to the left ( wl ), which can be used to verify the syntactic position checkboxes, particularly the adverbial positions (adjunct, subjunct, disjunct, and conjunct) . These first steps provide a general overview of a sense X  X  behavior.

The next step of feature examination delves more into the semantic characteristics of the complement and the governor. Tratz (2011) r e-ported that the use of heuristics provided a more acc urate identification of the preposition co m-plement; this is the WFR hr in our system. After getting some idea of the word class and the part of speech, we next examine the WordNet lexical name of the complement to determine its broad semantic grouping. As mentioned, this feature may return a number of values larger than the size of the corpus set, since WordNet senses for a given lexeme may be polysemous . Notwithstan d-ing, this feature examination generally shows the dominant categories and can be used to ch ara c-terize and act as a selector for the complement in the pattern details. Similar procedures are used for characterizing the governor selection criteria.
In the example in Figure 3, for below , sense 3(1b) , our preliminary analysis shows hr:pos:cd (i.e., a cardinal number) and hr:l:average, standard (i.e., the lemmas average and stan d-ard ) are particularly useful for identifying this sense. 3.2 Selecting Corpus Instances In addition to enabling feature examination, PDEP also facilitates selection of corpus ins tan c-es. We can use the specifications for any WFR -FER combination, along with one of the values (as shown in Figure 5), to select the corpus i n-stances having that feature. Figure 6 shows, in part, the result of the WFR hr and FER l with the value average , against the instances in the open corpus set.

As shown in the menu bar in Figure 6, we can select all instances and unselect all selections. Based on any selections, we can then tag such instances with one of the options that appear in the Tag Instances drop -down box. In the speci f-ic example, we could change all the selected i n-stances to some other sense, if we have decided that the current assignment is not the best.
The selection mechanism is not used absolut e-ly. For example, in examining the untagged i n-stances for over , we used the specification hr:ln:noun.time (looking for instances with the heuristic complement having the WordNet lex i-cal name noun.time ). Out of 500 instances, we found 122 with this property. We then scrolled through the selected items , deselecting instances that did not provide a time period, and then tagged 99 instances with the sense 14(5) , with the meaning expressing duration . Once we have made such a tagging, we can look at just those instances the next time we examine this sense. In this case, we might decide, pace the TPP lexico g-rapher X  X  comment, that the instances should be broken down into those which express a time period and those which describe  X  X ccompanying circumstances X  (e.g., over coffee ). 3.3 Accuracy of Features PDEP uses t he output from Tratz X  system (2011) , which is of high quality, but which is not always correct. In addition, the TPP corpus also has some shortcomings, which are revealed in exa m-ining the instances. The TPP corpus has not been cleaned in the same manner as the FN and the OEC corpora. As a result, we see many cases which are more difficult to parse and hence, from which to generate feature sets. We believe this provides a truer real -world picture of the co m-plexities of preposition behavior. As a result, in t he Tag Instances drop -down box, we have i n-cluded an option to tag a sentence as x , to ind i-cate that it is not a valid instance.

A small percentage of the TPP instances are ill -formed, i.e., incomplete sentences; these are marked as x . For some prepositions , e.g., down , a substantial number of instances are not prepos i-tions, but rather adverbs or particles. For some phrasal prepositions, such as on the strength of , the phrase is literal, rather than the preposition idiom; in this case, 20 of 124 instances we re marked as x . The occurrence of these invalid i n-stances provides an opportunity for improving taggers, parsers, and semantic role labelers. Since the PDEP system enables exploration of features from WordNet, FrameNet, and VerbNet, we are able to make some assessment of these resources.

WordNet played a statistically significant role in the systems developed by Tratz (2011) and Srikumar and Roth (2013). This includes the WordNet lexicographer X  X  file name (e.g., noun.time ), synsets, and hypernyms. We make extensive use of the file name, but less so from the synsets and hypernyms. However, in general, we find that the file names are too coarse -grained and the synsets and hypernyms too fine -grained for generalizations on the se lectors for the co m-plements and the governors. The issue of gran u-larity also affects the use of the DVC ontology. We discuss this issue further in section 6, on i n-vestigations of suitable categorization schemes for PDEP.

In using FrameNet, our results illu strate the unbalanced corpus used in SemEval 2007 (as suggested in Litkowski (2013b)). For the sense of of ,  X  X sed to indicate the contents of a contai n-er X , we first examined the FrameNet corpus set for that sense, which contains 278 instances (out of 4482, or 6.2 percent). Using PDEP, w e found that FrameNet feature values for the governor accounted for 264 of these instances (95 pe r-cent) , all of which were related to the frame el e-ments Contents or Stuff . However, in the TPP corpus, only 3 out of 750 instanc es were ident i-fied for this sense (0.4 percent). Thus, while FrameNet culled a large number of instances which had these frame element realizations, th e-se instances do not appear to be representative of their occurrence in a random sample of of uses. We ha ve seen similar patterns for the other SemEval prepositions.

A similar situation exists for Cause senses of major prepositions: for (385 in FrameNet, 5/500 in TPP), from (71 in FrameNet, 16/500 in TPP), of (68 in FrameNet, 0/750 in TPP), and with (127 in F rameNet, 8/750 in TPP). Each of these cases further emphasizes how the SemEval 2007 i n-stances are not representative and thus degrade the ability to apply existing preposition disa m-biguation results beyond these instances. ) We discuss Cause senses further in the wider context of all PDEP prepositions in the next section on class analyses. )
As indicated earlier, VerbNet identifies fewer prepositions in its frames than FrameNet. We believe this is the case since VerbNet prepos i-tions are generally arguments, r ather than a d-juncts. Many of the FrameNet prepositions are evoking peripheral and extra -thematic frame e l-ements, so the number of prepositions is corr e-spondingly higher. Also, VerbNet contains fewer members in its verb classes. As a result, the number of h its when using VerbNet is somewhat smaller, although some use of VerbNet classes is possible with the governor selectors.

PDEP provides a vehicle for expanding the items in all these resources. While prepositions are not central to these resources, their suppor t-ing role provides additional information that might be useful in developing and using these other resources. In SemEval 2007, Yuret (2007) investigated the possibility of using the substitutable prepositions as the basis for disambigu ation (as part of more general lexical sample substitution). Although his methodology yielded significant gains over the baseline, his best results were only 54.7 pe r-cent accuracy , concluding that preposition use is highly idiosyncratic . Srikumar and Roth (2013) broadened this perspective by considering a class -based approach by collapsing semantically -related senses across prepositions, thereby deri v-ing a semantic relation inventory. While their emphasis was on modeling semantic relations, they achieved an accuracy of 83.53 percent for preposition disambiguation.
 As mentioned above, PDEP has a field for the Srikumar semantic relation, initially populated for the SemEval prepositions, and being exten d-ed to cover all other prepositions. For example, Srikumar and Roth identified 21 temporal senses across 14 SemEval prepositions, while we have thus far identified 62 senses across 50 prepos i-tions. Similar increases in the sizes of other cla s-ses occur as well. For causal senses, Srikumar and Roth identified 11 se nses over 7 prepos i-tions, while PDEP has 27 senses under 25 prep o-sitions.

PDEP enables an in -depth analysis of TPP classes, Tratz clusters, and Srikumar semantic realations. First, we query the database underl y-ing Figure 3 to identify all senses with a par tic u-lar class. We then examine each sense on each list in detail.

We follow the procedures laid out above for examining the features to add information about selectors, complement types , and categories. We use this information to tag the TPP instances, con servatively assuring the tagging, e.g., leaving untagged questionable instances. Finally, we carefully place each sense into a preposition class or subclass, grouping senses together and making annotations that attempt to capture any nuance of meaning that distinguishes the sense from other members of the class.

To build a description of the class and its su b-classes, we make use of the Quirk reference in Figure 3 (i.e., the relevant discussions in Quirk et al. (198 5 )). We build the description of a class as a separate web page and make this available as a menu item in Figure 3 (not shown for the Scalar class when that screenshot was made). The d e-scription provides an overview of the class, ma k-ing use of the TPP data and the Quirk discussion, and indicating t he number of senses and the number of prepositions. Next, the description provides a list of the categories within the class, characterizing the complements of the category and then listing each sense in the category, with any nuance of meaning as necessar y. Finally, we attempt to summarize the selection criteria that have been used across all the senses in the class.
The process of building a class description r e-veals inconsistencies in each of the class fields. When we place a preposition sense into the c lass, we may find it necessary to make changes in the underlying data.

At the top level, these class analyses in effect constitute a coarse -grained sense inventory. As the subclasses are developed, a finer -grained analysis of a particular area is available . We b e-lieve these analyses may provide a comprehe n-sive characterization of particular semantic roles that can be used for various NLP applications. As indicated above, each of the tables shown in the figures is generated in Javascript through a system call to a PHP script. Each of these scripts is described in detail at the PDEP web site. Each script returns data in Javascript Object Notation (JSON), enabling users to obtain whate ver data is of interest to them and perhaps using this data dynamically.

While PDEP provides access to a large amount of data, the architecture is very flexible and easy to extend. For this, we are grateful for the Tratz parse r and the DVC code.

In building PDEP, we found it necessary to reprocess the SemEval 2007 data of the full 28,052 sentences that were available through TPP, rather than just those that were used in the SemEval task itself. Tagging, parsing, and crea t-ing fe ature files for these sentences took less than 10 minutes, with an equal time to upload the fe a-ture files. We would be able to add or substitute new corpora to the PDEP databases with rel a-tiv e ly little effort.

Similarly, we can add new elements or modify e xisting elements that describe preposition pa t-terns. This would require easily -made modific a-tions to the underlying MySQL database tables. The PHP scripts that access these tables are also easily developed or modified. Most of these scripts use less than 1 00 lines of code.

In developing PDEP, we have added various resources incrementally. This applies to such resources as the DVC ontology, FrameNet, and VerbNet. Each of these resources required rel a-tively little effort to integrate into PDEP. We will contin ue to investigate the utility of other r e-sources that will assist in characterizing prepos i-tion behavior. We have begun to look at the noun clusters used in Srikumar and Roth (2013) for better characterizing complements. We are also examining an Oxford nou n hierarchy as another alternative for complement analysis. We are e x-amining the WordNet de tour to FrameNet, as described in Burchardt et al. (2005), particularly for use in further characterizing the governors. We recognize that an important element of PD EP will be in its utility for preposition disa m-biguation. While we have not yet begun the ne c-essary experimentation and evaluation, we b e-lieve the representativeness and sample sizes of the TPP corpus (mostly with 250 or more se n-tences per preposition) sho uld provide a basis for constructing the needed studies. We expect that this will follow techniques used by Cinkova et al. (2012), in examining the Pattern Dictionary of English Verbs developed as the precursor to DVC.

We expect that interaction with the N LP community will help PDEP evolve into a useful resource, not only for characterizing preposition behavior, but also for assisting in the develo p-ment of other lexical resources. We have described the Pattern Dictionary of Engli sh Prepositions (PDEP) as a new lexical resource for examining and recording preposition behavior. PDEP does not introduce any ideas that have not already been explored in the investig a-tion of other parts of speech. However, by brin g-ing together work from these disparate sources, we have shown that it is possible to analyze preposition behavior in a manner equivalent to the major parts of speech. Since dictionary pu b-lishers have not previously devoted much effort in analyzing preposition behavior, we believ e PDEP may serve an important role, particularly for various NLP applications in which semantic role labeling is important.

On the other hand, PDEP as described in this paper is only in its initial stages. In following the principles laid out for verbs in PDEV, a main goal is to provide a sufficient characterization of how frequently different preposition patterns (senses) occur, with some idea of a statistical characterization of the probability of the co n-jun c tion of a preposition, its complement, and its governor. Better development of a desired sy n-tagmatic characterization of preposition beha v-ior, consistent with the principles of TNE, is still needed. Since preposition behavior is strongly linked to verb behavior, further effort is needed to link PDEP to PDEV.

The resource will benefit from futher exper i-mentation and evaluation stages. We expect that desired improvements will come from usage in various NLP tasks, particularly word -sense di s-ambiguation and semantic role labeling. In pa r-ticular, we anticipa te that interaction with the NLP community will identify further enhanc e-ments, developments, and hints from usage.
 Stephen Tratz (and Dirk Hovy) provided consi d-erable assistance in using the Tratz parser. Vivek Srikumar graciously provided his data on prep o-sition classes. Vitek Baisa similarly helped with the adaptation of the PDEV Javascript modules. Orin Hargraves, Patrick Hanks, and Eduard Hovy continued to provide valuable insights. Reviewer comments helped sharpen the draft version of the paper.

