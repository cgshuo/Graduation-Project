 In an effort to keep up with the tremendous growth of the World Wide Web (WWW), accurately. Information on the web is mainly presented in the form of text documents time. 
Prevalent VSM (Vector Space Model) used in clustering documents has some [16,17,18,19,20] and article structure principle [21]. conclude this paper. concepts with traditional data clustering methods. Generally speaking, web document clustering methods attempt to segregate the documents into groups where each group focused on the second one. steps: clustering. The VSM represents a web document as a feature vector of the terms that Model, the cosine measure and the Jaccard measure are the most common similarity Space Model is how to choose terms from documents and how to weigh the selected terms. Choosing Terms from a Document web document preprocessing the following parsing and extractio n steps are needed: to define the concept  X  X selessness X , is a headachy problem. Weight Selected Terms is the number of documents containing term y j . defined as the inner product of document vector X i and document vector X j : 3.1 Granularity Theory problem with lattices composed of every partition. Those works provide new methods and thinking ways. and knowledge engineers can handle the same dataset in different lays, this provides granular computing are as follows: 1) Fuzzy Set 
Fuzzy set, introduced by Zadeh in 1965, is a generalization of classical set theory for each element to range over the unit interval [0, 1]. 2) Rough Set setting of pre-topological spaces. 3.2 Article St ructure Principle granularity. 3.3 Intrinsic Limitations of VSM web document clustering results. VSM is a common and successful data model for web A. Interoperability of OLAP operations In VSM, suppose we treat a term as a feature of a web document object, the document operations fail to work. On this base, a web document can be represented as another logic model by adding a new granularity. B. Document  X  X alse correlation X  correlation X , depicted as figure 1, in the course of clustering web documents. identical can be drawn from comparing the two documents at the document granularity local distribution of them is different. C. Frequent Occurrence of  X  X ero-valued X  Similarity As we have seen, in VSM a single document is usually represented by relatively few terms. The document vector which is characteristic of high-dimension and sparseness relation between document and document. Expanded Vector Space Vector (EVSM) model in which web document is represented as a  X  X ocument-Paragraph-Term X  (D-P-T) configuration characterized as multi-level tolerance rough set theory. 4.1  X  X -P-T X  Configuration In this framework, a web document is represented as following logic layers: 1) Document layer: 
Here D is a web document, Did is id of the web document, Title is title of the web document, body is body of the web document which is composed of a paragraph set, length is total length of the paragraphs. 2) Paragraph layer: the web document containing current paragraph, Position is position of the paragraph the paragraph. 3) Term layer: 
Here term is a term of a paragraph, Tid is id of the term, Pid denotes the id of the current term. Weight denotes a weight produced from a weighing system. 4.2 EVSM Based on Tolerance Rough Set Tolerance Rough Set Model (TRSM) is an expanded model of the classical rough set lower approximation ) ( X B  X  are defined as below: 
With above TRSM, we apply granular computing to paragraph level granularity. For approximation ) ( _ X  X  and lower approximation ) ( _ X  X  as following: 
Suppose X is a term set expressing a vague concept, (X) _  X  is core connotation of  X  X ero-valued X  similarity can be greatly lessened by using upper approximation of the concept expressed by paragraph level granularity knowledge. 4.3 Improved TFIDF Weighing System in EVSM Model formalized as below: 
To demonstrate the use of the EVSM framework, we detail the process of converting web document by an example as follows. be T = {t1, t2, t3, t4}, the frequency data is listed in Table 1. approximations of the paragraph pi (I = 1, 2 , ..., 7) can be computed as below: TFIDF, result is listed in Table 2. 4.4 Evaluation on Paragraph Granularity X  X  Representing Ability Rate and Paragraph Relative Length. 1) Paragraph Position 
We classify all paragraphs in one web document into by paragraph position in web weight of the position attribute of the paragraph. document. PP p i . denotes position weight of the paragraph p i . 2) Paragraph Relative Length So we give the following definition (Paragraph Relative Length, abbreviated as PRL): 3) Term Repeating Rate 
Article structure principle holds that high some terms occur very frequently in some position to give importance to some viewpoint of author. We define Term Repeating Rate (TRRate) as the following formula: From above three measures, we can define weight and Term Repeating Rate. The concrete values of PW, LW and TRW can be manually given by domain experts or automatically given by computer. LabelDocument Algorithm by attribute Paragraph Position, Paragraph Relative Length and Term Repeating Rate. Second, assign document to the optimum by the value of membership to topic cluster, motivated by high-voting principle of multi-database mining [13]. Algorithm  X  LabelDocument Output  X  label of web document d (1) for each d p i  X  do compute end for (2) for each T T j  X  do for each d p i  X  do end for end for return label. WDCBKG Algorithm frequency threshold change rate
Output: K web document clusters k T T T , , , 1 1 " (1) preprocess web document collection and convert it paragraph vectors with the guidance of the data model EVSM. (2) cluster paragraphs with k-means (3) label the web documents with LabelDocument. experiments were conducted on a Dell Workstation PWS650 with 2GB main memory and Win2000 OS. 6.1 Dataset Selection To evaluate our proposed algorithm WDCBKG, we download 15013 web documents from sub-directory of Yahoo! News. The documents distribution is listed in Table 3. 6.2 Experimental Results conducted on a Dell Workstation PWS650 with 2 GB main memory and Win2000 OS. 
We access our proposed approach from three aspects as following: 1) Performance of clustering results evaluate clustering results by compar ing WDCBKG algorithm with VSM_Kmeans can see that, compared to VSM_Kmeans, performance of WDCBKG is great improved. 2) Scalability We conduct a group of experiments with different data set that is of different size. EVSM_WDCBKG doesn X  X  decease with the size of experimental data set increased but size is concerned, our approach is scalable. 3) Sensitiveness to tolerance threshold parameter Tolerance threshold parameter is rather important to our WDCBKG. From our other hand, too large tolerance threshold can make EVSM tend to VSM, both cases can lead to worse performance. From Figure 3 we can understand our experimental result best, however, when it equals 2,3 or 8, the performance is worst. efficient and promising. 
