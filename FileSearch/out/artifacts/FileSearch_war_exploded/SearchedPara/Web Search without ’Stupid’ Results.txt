 One of the main targets of any search engine is to make every user fully satisfied with her search results. For this reason, lots of efforts are being paid to improving ranking models in order to show the best results to users. However, there is a class of documents on the Web, which can spoil all efforts being shown to the users. When users receive results, which are not only irrelevant, but also completely out of the picture of their expectations, they can get really frustrated. So, we attempted to find a method to determine such documents and reduce their negative impact upon users and, as a consequence, on search engines in general. H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval Classifier Learning, Online experiments user frustration, classifier, search results ranking, searcher behavior
With the improvement of search engines, more and more various types of documents X  relevance labels and relevance scales appear: topical relevance, freshness, readability, etc. On the one hand, they are used for training ranking algo-rithms in order to improve the search engine X  X  ranking and, on the other hand, they play an important role in evalua-tion of systems X  quality. The most commonly used relevance scales are the following: binary scale[14] and 5-grade scale (Nav (grade 4), Key (grade 3), HRel (grade 2), Rel (grade 1), Non (grade 0), Junk (grade 0)) [3]. It is worth men-tioning that, traditionally, the documents which are even slightly related to the users X  intent are allocated into one of the first 4 groups of the latter scale, whereas all non-spam irrelevant documents get the  X  X on X  label. Search sys-arate subclass (hence, using 7 relevance labels instead of 6 used in [3]) and find ways to minimize their impact upon users. On the one hand, our study is aimed at training the classifier to make it capable of separating  X  X tupids X  from ir-relevant documents, as well as other documents. On the other hand, the study is aimed at using the classifier for re-ducing the impact of  X  X tupid X  documents upon users. There are several methods of using such a classifier, and our study focused on two of them: filtering-out  X  X tupid X  results from SERPs and reducing their weight in rankings. In order to understand, whether any users are affected by the changes in the rankings obtained in these two ways, the search re-sults produced with the help of that classifier were evaluated first with offline metrics (we had to make sure that the ini-tial ranking has not been degraded in terms of the 5-grade scale) and then with online metrics.

The remainder of the paper is organized as follows. The next section describes the classifier and the methods we use for dealing with  X  X tupid X  documents in search results. Our experimental results are presented in Section 3. Section 4 concludes the paper and describes a few directions for the future work.
In this section, we introduce our approach. The first part contains the details of our classifier construction, which was used to change search results in two ways: by re-ranking and by filtering.
At first, to reduce the number of stupid results displayed to the users and to decrease the effect produced by such results, we had to find out how to identify such documents automatically.

Our dataset contained a sample from the logs of a large commercial search engine Yandex. The logs contained queries and search results returned by the engine. These query-URL pairs were manually labeled using the 6 relevance la-bels described in [3] by experts, who paid attention to doc-uments X  relevance depending on all most probable query in-tents. After that, we chose documents with  X  X on X  labels and additionally asked the experts to divide them into two groups:  X  X on X  and  X  X tupid X .  X  X tupid X  labels were assigned to documents, which had no connection to the query topic, including all imaginable interpretations of the same query, and made the puzzled experts wonder how could such an-swers have ever been found by a search engine. Other doc-uments were left with  X  X on X  labels. For our dataset we used about 113,000 queries, which provided 2,762,098 judged query-URL pairs. The share of  X  X tupids X  among all  X  X on X  documents was 6.7%. For the purpose of the classifier train-ing we split up the relevance labels and used the binary clas-sification:  X  X tupid X  vs. all labels. The classifier was trained using a proprietary implementation of the gradient boosted decision trees algorithm [5] -MatrixNet 1 . We have divided all features into groups and conducted a series of feature ablation experiments.

In general, we had four groups of features, which basically correspond to a subset of features used to train the ranking algorithm of the search engine under study: http://api.yandex.com/matrixnet re-ranking by offline search quality metrics. Then we con-ducted some online experiments to determine whether there are any effects for users. The proprietary spam filtering algorithms had been used to remove  X  X pam X  and  X  X unk X  doc-uments from search results before all evaluations and ex-periments started. Our baseline involves no re-ranking or filtering of  X  X tupids X .
To evaluate our approach, we additionally collected two sets of queries sampled from Yandex query log. One of them, named  X  X ong-tail X , contained 1500 low-frequency queries 2 , which are often hard to answer. Therefore, search engines show many irrelevant and  X  X tupid X  results in SERPs for such queries. Another set, named  X  X ommon X , consists of 900 queries sampled randomly from the query log.
 As the quality evaluation metric, we chose the Expected Reciprocal Rank (ERR)[2] the primary effectiveness mea-sure used in TREC [3]. We computed the ERR (see Equa-tion 1) of search results with respect to the experts X  labels and the 5-grade relevance scale. Since  X  X on X ,  X  X tupid X  and  X  X unk X  documents do not satisfy users, we used their rele-vance labels with the same zero-weight (zero grade for  X  X on X  and  X  X unk X  documents was also used in official TREC eval-uations [3]). The main target of such evaluation is to make sure, that our classifier is careful enough to deal with irrel-evant documents only. So we did not use negative grades for  X  X tupids X  in this study as we did not want to get false positive growth in ERR.
 where R(g)= 2 g  X  1 16 and g 1 , g 2 , ..., g k are the relevance grades associated with the top-k documents.

Besides we calculated the share of  X  X tupid X  documents in the top-5 and top-10 of search results. Thus, we assessed whether there is a significant reduction in the number of  X  X tupid X  documents in the filtered search results.

In order to use the classifier as a filter which removes only  X  X tupid X  documents from the search results, it was necessary to choose some appropriate threshold. We have established the following criteria for the threshold: there should be a significant negative difference in the share of  X  X tupid X  doc-uments and zero or positive growth of ERR. The classifier returns the document X  X  probability of being  X  X tupid X  in the range of [0,1]. The probabilities from 0.1 to 0.9 with step of 0.05 were estimated. As Table 2 shows, a good decision is to choose the threshold of 0.7 -there is no significant difference in search results quality, but there is a significant 3 reduction of  X  X tupid X  documents in search results.

To measure the effectiveness of our  X  X nti-stupid X  re-ranking, we used the same sets of queries. We assumed that the search engine was good enough in ranking results, hence we did not expect to see significant relevance improvements here. However we do need to validate that re-ranking does not tend to reduce the number of relevant documents and lowers the scores of  X  X on X  documents only. The ERR results in Table 3 show that our changes do not significantly degrade asked less than 300 times in two months ** -p-value &lt; 0,01; * -p-value &lt; 0,05. T-test was used was shown for these users) and the  X  X reatment X  group (the results of the production system with re-ranking were shown to these users). We performed this experiment within two weeks in September 2013.

As a result of the experiment, we observed that the click-through rates (CTR) had significantly 5 grown up (0,97%  X  on the average) for all positions for the treatment group. Just like clicks in TDI, it also demonstrates that users started getting into more interactions with the SERPs generated by the re-ranking. Also positive changes were observed for the following click metrics:  X  X osition of the first click X  (-0.71%  X  ),  X  X osition of the first long click 6  X  (-0.43%  X  ). It means that top search results also became more attractive to users. Whereas,  X  X bandonment Rate X  and  X  X ime to first click X  metrics stayed without any significant changes.
Based on the results of the conducted experiments we are able to draw a conclusion that the demotion of  X  X tupids X  have positive impact on absolute click metrics. In addition, along with the changes in the mentioned metrics, we have noticed one more change in users behavior in the treatment group: their dwell time after click decreased. It may indicate changes not only in the number of user actions, but also in their character. It makes us think that transformation of user attitude towards the changed search system in general should also be studied in addition to click metrics.
In the scope of the present study we aimed at investigat-ing whether reducing of the number of  X  X tupid X  documents in search results has any impact on users. To answer this research question, we trained a classifier and used it for filter-ing and re-ranking. It should be noted, that with the use of the generally accepted 5-grade relevance scale such ranking modifications are not visible in the results of the ERR met-ric. In fact, two systems, having no significant differences in ERR results, might be perceived by users differently. We demonstrate this with the results of two different online ex-periments. Applying the classifier for filtering or decreasing of  X  X tupid X  documents X  scores caused changes in users be-havior: they started making more clicks on changed SERPs. We observed significant improvements in terms of click met-rics in spite of the fact that only  X  X tupid X  documents have been removed/demoted, leaving the irrelevant ones intact. As a result, we may conclude that it is necessary to distin-guish stupids from not so extremely irrelevant documents and grade them differently too to let rankers prefer moder-ately irrelevant documents over stupids.

Relying on our findings, we consider the following direc-tions for our future work: 1)Along with allocating  X  X tupids X  to a separate subclass, we suppose that it is useful to introduce an additional grade for  X  X tupid X  documents into relevance scale. We are going to try to train list-wise learning to rank algorithms taking this grade into account. It is also necessary to give additional instructions to experts: user psychology causing different reactions in case of  X  X tupids X  and moderately irrelevant doc-uments should be taken into account. 2) It is crucial to spot  X  X tupids X , but they should be re-moved correctly. We plan to study focused stupids filtration schemes for some groups of queries (e.g. dividing them into navigational/informational or commercial/regional) and try  X  -p-value &lt; 0.001; Mann-Whitney test was used clicks followed by no further clicks for 30 seconds or more
