 Takafumi Kanamori kanamori@is.na goya-u.a c.jp There are a wide variet y of mac hine learning al-gorithms for binary classi cation. Supp ort vector machine (SVM) is one of the most successful clas-si cation algorithms in mo dern mac hine learning ( Sch X olkopf &amp; Smola , 2002 ). The minimax probabil-ity mac hine (MPM) ( Lanc kriet et al. , 2002 ) and Fisher discriminan t analysis (FD A) ( Fukunaga , 1990 ) also ad-dress the binary classi cation problem. Their problem settings assume that only the mean and covariance ma-trix of eac h class are kno wn. The optimal hyperplane of MPM is determined by minimizing the worst-case (maxim um) probabilit y of misclassi cation of unseen test samples over all possible class-conditional distri-butions. FD A is to nd a direction whic h maximizes the pro jected class means while minimizing the class variance in this direction.
 The purp ose of this pap er is to pro vide a uni ed frame-work for learning algorithms, including SVM, MPM, and FD A, from the viewp oint of robust optimiza-tion ( Ben-T al et al. , 2009 ). Robust optimization is an approac h that handles optimization problems de ned by uncertain inputs. A simple example of robust opti-mization is where w is the parameter to be optimized under the constrain t w 2 W and x is an uncertain input in the problem. The unc ertainty set U represen ts the uncer-tain ty of the input. ( 1 ) determines the decision making parameter w whic h maximizes the bene t x &gt; w for the worst-case setup among x 2U .
 For binary classi cation, we regard the means x + and x of the data points of eac h class as uncertain inputs and prepare uncertain ty sets U + and U of those un-certain inputs. We assume that x of ( 1 ) exists in the Mink owski di erence U of U + and U , i.e.,
U = U + U := f x + x x + 2U + ; x 2U g ; and de ne W by f w k w k 2 = 1 g , where kk is the Euclidean norm. Then we transform ( 1 ) into We call it robust classi c ation model (RCM) 1 . This problem alw ays seems to be non-con vex because of W . However, it reduces to a con vex problem that includes a constrain t k w k 2 1 instead of k w k 2 = 1 when U + and U do not intersect. In this pap er, we sho w that RCM ( 2 ) reduces to the learning metho ds men tioned above, dep ending on a prescrib ed uncertain ty set U . For example, we sho w that MPM is a special case of ( 2 ) with an ellipsoidal uncertain ty set U . When U + and U are de ned as re-duced con vex hulls ( Bennett &amp; Bredensteiner , 2000 ), ( 2 ) reduces to -SVM ( Sch X olkopf et al. , 2000 ) if U + U = ; and reduces to E -SVM ( Perez-Cruz et al. , 2003 ), otherwise. The di erence between these learn-ing metho ds turns out only to be in the de nition of U of ( 2 ).
 The rst con tribution of handling the uni ed mo del ( 2 ) is to obtain new learning metho ds. For ex-ample, we can obtain non-con vex varian ts of MPM and FD A by mimic king Perez-Cruz et al.'s extension ( Perez-Cruz et al. , 2003 ) from con vex -SVM to non-con vex E -SVM.
 The second con tribution is to pro vide theoretical re-sults to above learning metho ds at once by dealing with the uni ed mo del ( 2 ). Indeed, we pro vide sta-tistical interpretation for ( 2 ) on the basis of the con-ventional statistical learning theory . We sho w that ( 2 ) with some corresp onding uncertain ty set is a good appro ximation for the worst-case minimization of ex-pected loss functions under uncertain probabilities. We also pro vide a generalized local optim um searc h algorithm, that is applicable to non-con vex varian ts of learning mo dels. We pro ve theoretical results on the local optim um searc h algorithm.
 The pap er is organized as follo ws. In Section 2 , we elucidate the uni ed mo del, RCM ( 2 ), for classi ca-tion problems. In Section 3 , we sho w RCM's connec-tion with existing learning algorithms and obtain non-con vex varian ts for MPM and FD A in the same way as non-con vex E -SVM. In Section 4 , we give a statistical interpretation of RCM in terms of minimizing the up-per and lower bounds of the worst-case exp ected loss. In Section 5 , we describ e a local optim um searc h algo-rithm for non-con vex RCM. We summarize our con tri-butions and future work in Section 6 . 2.1. Problem Settings We shall start by introducing the problem setting and the notations. The observ ed training samples are de-Let M + be the set of indices of training samples with the lab el +1; likewise for M . Let j M + j = m + and j M j = m , where jj sho ws the size of the set. The goal of the classi cation task is to obtain a classi-er that minimizes the prediction error rate for unseen test samples. For the sak e of simplicit y, we shall fo-cus on linear classi ers, i.e., x &gt; w + b where w ( 2 R is a vector and b ( 2 R ) is a bias parameter. Most of the discussions in this pap er can be directly applied to kernel classi ers ( Sch X olkopf &amp; Smola , 2002 ). Con-cretely , the change from x 2X to the kernel function k ( ; x ) mak es statemen ts of Sections 2 -4 hold for ker-nel classi ers, while the algorithm in Section 5 needs small mo di cation.
 We shall assume that the training samples are not reliable because of noise or measuremen t errors. To mak e a classi cation mo del less sensitiv e to noise in the training samples, we shall focus on represen tativ e points of eac h class, denoted by x + and x . These points are not necessarily individual samples, but may be means of the data points of eac h class. Since the training samples are not reliable, it is reasonable to assume that x + and x will involve some uncertain ty. The largest possible sets of x + and x are denoted by U + and U , resp ectiv ely, and these sets are de ned on the basis of training samples. Throughout this pa-per, we will assume that both U + and U are con-vex and compact and that they have interior points. Then, their Mink owski di erence U is con vex and has a nonempt y interior.
 The way of constructing the uncertain ty set U is a very imp ortan t issue in practice. If we set U too large in ( 2 ), the optimal decision is very robust to uncer-tain data x but too conserv ativ e. Moreo ver, if we de-ne U with complicated functions, we cannot easily solv e ( 2 ). Man y robust optimization studies have used polyhedral sets and ellipsoidal sets as U for the sak e of computational tractabilit y. We sho w examples of U + and U in Section 3 . We migh t possibly deal with more complicated problem setting beyond con vex U + and U by using kernelization techniques. 2.2. Prop erties of RCM To geometrically interpret RCM ( 2 ), Figure 1 sho ws the ellipsoidal uncertain ty sets U + , U and their Mink owski di erence. We can separate the problem ( 2 ) into two cases, i.e., whether U + and U have an intersection or not, whic h is equiv alen t to whether U includes 0 or not. As sho wn in Theorem 2.2 , there is a large di erence in computational e ort between the two cases. Before giving an intuitiv e geometric interpretation of RCM in Theorem 2.2 , we introduce Lemma 2.1 that further separates the case 0 2U into two cases: U includes 0 in its interior, int( U ), or on its boundary , bd( U ). In the geometric sense, 0 62U holds when U + and U are disjoin t. 0 2 U implies that U + and U are join t. In particular, 0 2 bd( U ) implies that U + and U touc h externally .
 Lemma 2.1. The optimal value of RCM ( 2 ) is pos-itive if and only if 0 62 U . It is zero if and only if 0 2 bd( U ) , and it is negative if and only if 0 2 int( U ) . We can pro ve \if" parts by using the supp orting hy-perplane theorem to three cases ( 0 62 U , 0 2 bd( U ) and 0 2 int( U )). By taking the con trap ositiv e of all the \if" parts, we also can pro ve \only if" parts. Let U be a parametrized uncertain ty set for RCM ( 2 ) suc h that U 1 U 2 holds for 1 2 . Then the follo wing inequalit y holds: This indicates that the optimal value of ( 2 ) is non-increasing with resp ect to the inclusion relation of un-certain ty sets. Figure 1 (righ t) plots the non-increasing optimal value of RCM ( 2 ) with resp ect to . An uncer-tain ty set U 2 migh t exist suc h that the optimal value of ( 2 ) becomes zero.
 The follo wing theorem sho ws that when 0 62 U , the equalit y constrain t k w k 2 = 1 in ( 2 ) can be replaced by k w k 2 1 without changing the solution. Moreo ver, k w k 2 = 1 can be replaced by k w k 2 1 when 0 2 U . Figure 1 (left and middle) illustrates Theorem 2.2 . Theorem 2.2. For an unc ertainty set such that 0 62 U , RCM ( 2 ) is equivalent to Mor eover, the problem is equivalent to min An optimal w of ( 3 ) can be obtaine d from x = k x k by using the optimal x 2 U of ( 4 ) . For an unc ertainty set such that 0 2 int( U ) , RCM ( 2 ) is equivalent to Mor eover, the problem is equivalent to min x 2 U c k x k , wher e U c is the closur e of the complement of the con-vex set U . An optimal w of ( 5 ) can be obtaine d from x = k x k by using the optimal x 2 U c .
 Proof. Assume 0 62 U . By applying the discussion on the minim um norm dualit y ( Luen berger , 1969 ) to ( 3 ), we can con rm the equiv alence of ( 3 ) and min x 2U k x k , and the optimal solution w = x = k x k . On the other hand, in the case of 0 2 int( U ), the equiv alence of ( 5 ) and min x 2 U c k x k is pro ved from Prop osition 3.1 of ( Briec , 1997 ) under the assumption that a con vex U has a nonempt y interior. Hence, it is enough to sho w that there exists an optimal solution w of ( 3 ) (or ( 5 )) suc h that k w k = 1, because the di erence between ( 2 ) and ( 3 ) (or ( 5 )) is only the norm constrain t of w . Lemma 2.1 ensures that the optimal value of ( 3 ) is positiv e, because Since the optimal solution w of ( 3 ) satis es 0 &lt; k w k 1, the follo wing inequalities hold: The last inequalit y comes from the optimalit y of w . These inequalities imply that w = k w k is also an op-timal solution of ( 3 ) and that k w k = 1. For the case of 0 2 int( U ), we can similarly sho w that the optimal
Ellip. ( 11 )
Ellip. ( 15 ) RCH ( 8 ) E -SVM min -SVM value of ( 5 ) is negativ e and that an optimal solution w of ( 5 ) exists suc h that k w k = 1.
 For 0 2 int( U ), RCM ( 2 ) is essen tially a non-con vex problem, and we need to use non-con vex optimization metho ds to solv e it. Section 5 describ es an optimiza-tion algorithm for non-con vex problems of ( 2 ). We will sho w that RCM can be reduced to supp ort vector mac hine (SVM), minimax probabilit y mac hine (MPM), or Fisher discriminan t analysis (FD A) de-pending on the prescrib ed uncertain ty set U . In Ta-ble 1 , \ " means that the corresp onding cases nev er happ en. \ existing mo dels as far as we kno w. The mo dels indi-cated by We denote an optimal solution of ( 2 ) as w and de ne the bias term b suc h that the decision boundary passes through the mid-p oint of x + and x , i.e., b = ( x + + x ) &gt; w = 2. Here, x + 2 U + and x 2 U stand for the optimal solutions of the inner-minimization in ( 2 ) for w = w . 3.1. Hard-Margin SVM, -SVM and E -SVM Whenev er a data set is linearly separable, there are man y hyperplanes that correctly classify all training samples. Vapnik-Cherv onenkis theory indicates that a large margin classi er has a small generalization er-ror. The problem can be transformed into a quadratic programming problem and the classi cation metho d is called hard-margin supp ort vector classi cation ma-chine (HM-SVM). Here, we de ne the uncertain ty set (con vex hull, CH) as follo ws: where con v means con vex hull. By using the Wolfe dualit y, the equiv alence of HM-SVM and RCM ( 4 ) is obvious for U + \U = ; .
 HM-SVM has been extended to cop e with non-separable data. C -SVM ( Cortes &amp; Vapnik , 1995 ) and -SVM ( Sch X olkopf et al. , 2000 ) are typical examples of \soft-margin" SVMs. There is a corresp ondence between C -SVM and -SVM. That is, the classi er es-timated by C -SVM with C 2 (0 ; 1 ) can be obtained from -SVM with a parameter 2 ( min ; max ] [0 ; 1], and vice versa. Crisp and Burges ( 2000 ) sho wed max = 2 min f m + ; m g =m and gave a geometric inter-pretation for min . For 2 ( max ; 1], the optimization problem of -SVM is unbounded, and for 2 [0 ; min ), -SVM pro vides a trivial solution ( w = 0 and b = 0). Perez-Cruz et al. ( 2003 ) devised extended -SVM (E -SVM) as a way of avoiding suc h a trivial solution: min s : t : y i ( x &gt; i w + b ) i ; i 0 ; i 2 M; k w k 2 By forcing the norm of w to be unit y, a non-trivial and meaningful solution is obtained for any 2 [0 ; min ), but this comes at the exp ense of con vexit y. It further-more pro vides the same solution as -SVM for other values of . In that sense, E -SVM can be regarded as an extension of -SVM. It was exp erimen tally found in ( Perez-Cruz et al. , 2003 ) that E -SVM often has bet-ter generalization performance than -SVM.
 In order to connect (E) -SVM with RCM, we de ne U as The set ( 8 ) is essen tially equal to a reduc ed convex hull (RCH) ( Bennett &amp; Bredensteiner , 2000 ) or soft convex hull ( Crisp &amp; Burges , 2000 ). For linearly non-separable data set, U + and U intersect with small . Crisp and Burges ( 2000 ) sho wed that min is the largest suc h that two RCHs, U + and U , intersect. The mo del that nds min corresp onds to the case of 0 2 bd( U ) in the \RCH" of Table 1 . Barb ero et al. ( 2012 ) transformed -SVM and E -SVM ( 7 ) into RCM ( 2 ) with U in order to give them a geometric in-terpretation. Using the results, we can relate -SVM, E -SVM, and RCM ( 2 ) as sho wn in Table 1 . 3.2. Minimax Probabilit y Machine and Its The minimax probabilit y mac hine (MPM) only uses the mean and covariance matrix of eac h class for clas-si cation tasks ( Lanc kriet et al. , 2002 ). Supp ose that x + (or x ) is a d -dimensional random vector with mean x + (or x ) and covariance + (or ). We as-sume that x + 6 = x and that are positiv e de nite. The MPM minimizes the misclassi cation probabili-ties under the worst-case setting as follo ws: max where x + ( x + ; + ) refers to the class of distribu-tions that have mean x + and covariance + , but are otherwise arbitrary; likewise for x . In practice, the mean vectors and covariance matrices of eac h class are estimated from the training samples.
 Lanc kriet et al. ( 2002 ) represen ted problem ( 9 ) as a con vex optimization problem kno wn as a second-order cone program (SOCP) and sho w the dual form: of ( 9 ) corresp onds to of ( 10 ) as = Therefore, MPM ( 9 ) is the problem to nd the small-est positiv e (denoted by max ) suc h that the two ellipsoids intersect, i.e., 0 2 bd( U max ).
 The idea of MPM is com bined with the idea of the mar-gin maximization in ( Nath &amp; Bhattac haryy a , 2007 ). Giv en acceptable false positiv e and negativ e rates, + and , the linear classi er can be estimated by min In this pap er, we call this mo del the \margin max-imized MPM" (MM-MPM). In the same way as in MPM, ( 12 ) can be transformed into an SOCP .
 Robust optimization techniques for ellipsoidal uncer-tain ty ( Ben-T al et al. , 2009 ) transform RCM ( 2 ) with U = U into and U touc h. For 2 [0 ; max ), U + + \U = ; holds, and RCM ( 13 ) is equiv alen t to MM-MPM ( 12 ) with = comparing the dual form of MM-MPM and the dual of ( 13 ), that is equiv alen t to ( 4 ). Furthermore, ( 13 ) with 3.3. Fisher Discriminan t Analysis and Its In Fisher discriminan t analysis (FD A) as in MPM ( 9 ), a discriminan t hyperplane is computed from the means and covariances of random vectors x + and x . The hyperplane is determined from the optimal solution w to the follo wing problem ( Fukunaga , 1990 ): The problem nds a direction whic h maximizes the pro jected class means while minimizing the class vari-ance in this direction.
 Lik ewise for MPM, FD A has a probabilistic interpre-tation under the worst-case scenario. Using the ellip-soidal uncertain ty set de ned by
U = f x = ( x + x ) + ( + + ) 1 = 2 u k u k g ; FD A ( 14 ) can be represen ted as FD A can be extended to RCM ( 2 ) with the uncertain ty set U for a prescrib ed parameter &gt; 0. Let max be the optimal value of ( 16 ). Then, along the same lines as the MPM in Section 3.2 , we nd that RCM ( 2 ) with U = U max is equiv alen t to FD A.
 Indeed, RCM ( 2 ) with U is transformed into Esp ecially for 2 [0 ; max ), the norm constrain t is replaced with the con vex constrain t k w k 2 1 without changing the optimal solution. Here, MM-FD A refers to this estimator. In replacing the Euclidean norm k w k with the L 1 -norm k w k 1 , MM-FD A is equiv alen t to a sparse feature selection mo del based on FD A (FS-FD) ( Bhattac haryy a , 2004 ). We can give a statistical interpretation for RCM on the basis of statistical learning theory . Let us start by introducing a loss function ` : R ! R that de nes the loss of the decision function x &gt; w + b regarding the sample ( x ; y ) as ` ( y ( x &gt; w + b )).
 A goal of the classi cation task is to obtain an accurate classi er. For this purp ose, it is reasonable to minimize w and b . Let us de ne p ( x j y ) as the conditional prob-abilit y densit y of x , given the binary lab el y , and + and as the marginal probabilities of the positiv e and negativ e lab els, resp ectiv ely. E [ ` ( y ( x &gt; computed by Since the true probabilit y distribution is unkno wn, we cannot minimize the exp ected loss directly .
 Now let us consider the ambiguit y of the probabilit y distribution p ( x j y ). Let P + and P be sets of proba-bilit y densities. Eac h set of probabilities expresses the uncertain ty of the conditional probabilities p ( x j + 1) and p ( x j 1), resp ectiv ely. We can use the min-max decision rule for the uncertain ty of p ( x j y ) as follo ws: The worst-case minimization problem is dicult to solv e. Therefore, we prop ose to solv e RCM ( 2 ), since we can pro ve that RCM ( 2 ) is a good appro ximation for minimizing the worst-case exp ected loss.
 To relate ( 17 ) and RCM, we rstly give an equiv alen t form ulation for RCM. Here, we de ne x + and x as the mean of the input vector x under the conditional probabilities p ( x j + 1) and p ( x j 1), resp ectiv ely, i.e., x = abilit y distributions in P have the mean vector. Let U + and U be Supp ose that the uncertain ty sets of probabilit y densi-ties, P , are both con vex; i.e., a mixture of two prob-abilit y densities also lies in the uncertain ty set. Then U + and U are con vex sets.
 Theorem 4.1. Supp ose that ` ( z ) is a non-incr easing function. An optimal solution of the RCM with the unc ertainty sets U + and U in ( 18 ) is also optimal to wher e J ( w ; b ; x + ; x ) = + ` ( x &gt; + w + b ) + ` ( x &gt; Proof. For a xed w and x 2 U , minimizing J ( w ; b ; x + ; x ) resp ect to b is equiv alen t to Since the objectiv e function above is non-increasing in ( x + x ) &gt; w , there exists a non-increasing function ( z ) suc h that Hence, one has As a result, the optimal solution of the RCM is also optimal for problem ( 19 ).
 Theorem 4.2. We assume that i) ` ( z ) is convex, de-creasing, and second-or der di er entiable, and that ii) 0 ` 00 ( z ) L 2 R holds for all z . Supp ose that x 2U is in the ball with the radius c , i.e., k x k c . Then, for the optimal value J of ( 19 ) , one has min Proof. The con vexit y of ` ( z ) leads to a lower bound, J ( w ; b ; x + ; x ), and the Taylor expansions of ` ( z ) around z = x &gt; + w + b and z = x &gt; w + b yield an upp er bound, J ` ( w ; b ; x + ; x ) + Lc 2 2 , of E [ ` ( y ( x Even when the min-max operation is applied, J and J + Lc 2 2 remain bounds for the worst-case exp ected loss ( 17 ).
 The theorem implies that problem ( 19 ) minimizes the bounds of ( 17 ). Noticing that the optimal solution of problem ( 19 ) is available by solving RCM as sho wn in Theorem 4.1 , Theorem 4.2 implies that RCM min-imizes the upp er and lower bounds of the worst-case exp ected loss ( 17 ) at the same time.
 There are various ways to estimate the bias term b for RCM. The simplest way is to use b = ( x + + x ) &gt; w = 2. Another promising metho d is to construct an appropriate statistical mo del for the pro jected sam-ples ( x &gt; i w ; y i ) ; i 2 M . The pro jected samples, x i 2 M , are scattered in one-dimensional space, from whic h we can estimate b on the basis of the statistical mo del. The RCM has a signi can tly larger range of param-eter or than an existing con vex mo del suc h as MPM, MM-MPM, FD A or FS-FD (see Table 1 ).
 Therefore, the RCM enhances a possibilit y of im-pro ving these existing classi cation mo dels. Indeed, Perez-Cruz et al. ( 2003 ) exp erimen tally sho wed that the generalization performance of E -SVM is often better than that of original -SVM. In this section, we prop ose a solution metho d that is generalized from the local algorithms of ( Perez-Cruz et al. , 2003 ; Takeda &amp; Sugiy ama , 2008 ). 5.1. Two-stage Optimization Strategy Supp ose that we solv e RCM ( 2 ) with the uncertain ty set U with one parameter and that U 1 int( U 2 ) holds for 1 &lt; 2 . Let us de ne max suc h that the optimal value of ( 2 ) with U = U max is zero. First, we need to compute max in order to con rm that the given problem ( 2 ) is essen tially con vex or not. Algorithm 5.1.
 Step 1: Cho ose e w 0 satisfying k e w 0 k = 1 and &gt; 0. Let t 0.

Step 2: Solv e the follo wing program: where g ( w ) = min x 2U x &gt; w , and let the optimal solution be b w t .

Step 3: If k e w t b w t k , terminate and output e w e w t .

Step 4: Otherwise, let e w t +1 b w t = k b w t k . Let t t + 1. Rep eat Steps 2{4.
 The parameter max is obtained as the optimal solu-tion of the con vex problem: When U are ellipsoidal sets of ( 11 ) (or ( 15 )), the problem reduces to MPM ( 10 ) (or FD A ( 16 )). When U are RCHs, the problem reduces to a linear pro-gramming problem and gives us min .
 If the input parameter is equal to max , we have already obtained an optimal solution from ( 20 ). If &lt; max , we next solv e the con vex problem ( 4 ) by using a standard optimization soft ware. 5.2. Local Optimization Algorithm for For &gt; max , RCM ( 2 ) is essen tially equiv alen t to ( 5 ) that includes a non-con vex constrain t, k w k 2 1. We next need to solv e ( 2 ) as a non-con vex problem. In the area of global optimization, non-con vex RCM ( 5 ) (precisely , a problem constructed by taking dual for the inner-minimization in ( 5 )) is kno wn as a reverse convex program (RCP), or canonic al d.c. program-ming . This di ers from a con ventional con vex program only by the presence of a rev erse con vex constrain t ( k w k 2 1 in the curren t case). When all functions are linear except for the rev erse con vex constrain t, the RCP problem is esp ecially called line ar reverse con-vex program (LR CP). E -SVM is an LR CP, for whic h Perez-Cruz et al. ( 2003 ) prop osed a local optim um searc h algorithm and Takeda and Sugiy ama ( 2008 ) prop osed a global optim um searc h algorithm. Here, we sho w a local optim um searc h algorithm (Al-gorithm 5.1 ) that is generalized from the local algo-rithms ( Perez-Cruz et al. , 2003 ; Takeda &amp; Sugiy ama , 2008 ) of E -SVM for non-con vex RCM. It is essen-tially the same as the local algorithm, Algorithm 7, in ( Takeda &amp; Sugiy ama , 2008 ) when U of g ( w ) is an RCH ( 8 ) and = 0.
 RCM ( 2 ) requires maximizing g ( w ) = min x 2U x &gt; w sub ject to a non-con vex constrain t, w &gt; w = 1. In-stead of solving the non-con vex problem directly , we can iterativ ely solv e the relaxation problems ( 21 ) (in Algorithm 5.1 ). Since g ( w ) is conca ve, ( 21 ) can be solv ed by using con vex minimization techniques. The non-con vex constrain t of ( 2 ) is linearized at e w in the algorithm, and the linear constrain t e w &gt; t w = 1 is updated every iteration. Note that the negativit y of the optimal value of ( 21 ) is guaran teed because of 0 2 int( U ). As the algorithm pro ceeds, the solutions e w t impro ve, i.e., g ( e w t ) g ( b w t ) &lt; g ( b w t = k b w t k ) = g ( e w because e w &gt; t b w t = 1 together with e w &gt; t e w k b w t k &gt; 1. Note that e w t is a feasible solution for ( 21 ). Hence, if ( 21 ) has no better solutions than e w e w t is returned as an optimal solution b w t of ( 21 ). The algorithm terminates after that.
 The computation of g ( w ) may be dicult for general uncertain ty sets. However, we do not need an explicit form ula for g ( w ) in ( 21 ). If U is a con vex set, we can obtain a dual form ulation (max-problem) for g ( w ) and replace the max-min problem ( 21 ) with a simple max-problem, that is, a one-lev el con vex problem. In-deed, when the uncertain ty set is an RCH ( 8 ) of data points, we can tak e the dual for g ( w ) = min x 2U x &gt; and change ( 21 ) into a linearized E -SVM ( 7 ) whose constrain t is e w &gt; t w = 1 instead of k w k 2 = 1. When the algorithm is applied to the RCM having ellipsoidal uncertain ty, we analytically obtain the optimal value g ( w ) for any w . Indeed, for ellipsoidal uncertain ty ( 11 ), g ( w ) is equal to the one deriv ed by multiplying the objectiv e function of ( 13 ) by -1.
 Theorem 5.2. For any &gt; 0 , Algorithm 5.1 termi-nates in a nite numb er of iter ations.
 Proof. Let the negativ e value g opt be the optimal value Otherwise, the algorithm terminates. By evaluating g ( e w t +1 ) g ( e w t ), we have The above inequalit y and the boundedness of g ( e w t ) suc h that k b w t k = 1 + t ; 0 &lt; t = o (1), that leads to k e w t b w t k = stopping rule k e w t b w t k with positiv e is satis ed in a nite num ber of iterations.
 We can sho w that Algorithm 5.1 with = 0 termi-nates within a nite num ber of iterations when the uncertain ty set of RCM ( 2 ) is represen ted by a con vex polyhedron by mimic king the pro of of Theorem 8 for E -SVM in ( Takeda &amp; Sugiy ama , 2008 ).
 Here, supp ose that g ( e w ) is di eren tiable, i.e., g ( w ) has a unique subgradien t at e w as @g ( e w ) = arg min x 2U x &gt; e w = fr g ( e w ) g . For example, g ( w ) is di eren tiable under ellipsoidal uncertain ty ( 11 ) (or ( 15 )). Then Theorem 5.3 sho ws a sucien t condition for the local optimalit y of the solution e w if it is ob-tained by Algorithm 5.1 with = 0.
 Theorem 5.3. Supp ose that g ( e w ) is di er entiable. Algorithm 5.1 that terminates with = 0 provides a lo-cal solution e w to RCM ( 2 ) when the maximum eigen-value of r 2 g ( e w ) is less than g ( e w ) . Proof. Note that e w is the optimal solution of ( 21 ) at the nal iteration. Therefore, e w satis es the rst-and second-order necessary conditions: where is a Lagrange multiplier. We can sho w = g ( e w ) &gt; 0 by noticing that r g ( e w ) is a minimizer to min x 2U x &gt; e w = g ( e w ) and using ( 23 ). When the maxim um eigen value of r 2 g ( e w ) is less than , e w satisfying ( 23 ) and ( 24 ) also satis es the second-order sucien t conditions for the local optimalit y of ( 2 ): where I is the iden tity matrix and ( 0) is a multi-plier. This sho ws that e w is a local solution of ( 2 ). Theorem 5.3 may be extendable to the non-di eren tiable case of g ( e w ), though more assumptions are necessary (see Theorems 3.2.16, 3.2.20 and 3.2.21 in ( Polak , 1997 )). We dev elop ed the robust classi cation mo del (RCM), a mo del whic h includes SVM, MPM and FD A for spe-ci c uncertain ty sets. The choice of uncertain ty set is signi can t in this mo del. This mo del enables ex-tensions and impro vemen ts to SVM to be applied to MPM and FD A, and vice versa.
 The uni ed mo del will be of help in clarifying rela-tionships among existing mo dels and in nding new classi ers and new algorithms. That is, we migh t be able to devise a new classi er by nding a reasonable uncertain ty set for RCM. It will be imp ortan t to see how the learning algorithm, uncertain ty set, and pre-diction accuracy relate to eac h other.

