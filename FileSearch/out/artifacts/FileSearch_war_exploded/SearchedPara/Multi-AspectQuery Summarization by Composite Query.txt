 Conventional search engines usually return a ranked list of web pages in response to a query. Users have to visit sev-eral pages to locate the relevant parts. A promising future search scenario should involve: (1) understanding user in-tents; (2) providing relevant information directly to satisfy searchers X  needs, as opposed to relevant pages . In this pa-per, we present a paradigm for dealing with informational queries. We aim to summarize a query X  X  information from different aspects. Query aspects are aligned to user intents. The generated summaries for query aspects are expected to be both specific and informative, so that users can easily and quickly find relevant information. Specifically, we use a \Composite Query for Summarization" method, which lever-ages the search engine to proactively gather information by submitting multiple composite queries according to the o-riginal query and its aspects. In this way, we could get more relevant information for each query aspect and roughly clas-sify information. By comparative mining the search results of different composite queries, it is able to identify query (dependent) aspect words, which help to generate more spe-cific and informative summaries. The experimental results on two data sets, Wikipedia and TREC ClueWeb2009, are encouraging. Our method outperforms two baseline meth-ods on generating informative summaries.
 H.3.m [ Information Storage and Retrieval ]: Miscella-neous Algorithms, Experimentation  X 
Th is work was done when the first and third authors were visiting Microsoft Research Asia Query aspect, Query summarization, Composite query, Mix-ture Model
Nowadays, accessing information on the Internet through search engines has become a fundamental life activity. Cur-rent web search engines usually provide a ranked list of URL-s to answer a query. This type of information access does a good job for dealing with simple navigational queries by leading users to specific websites. However, it is becoming increasingly insufficient for queries with vague or complex information need. Many queries serve just as the start of an exploration of related information space. Users may want to know about a topic from multiple aspects. Organizing the web content relevant to a query according to user intents would benefit user exploration. In addition, a list of URLs couldn X  X  directly satisfy user information need. Users have to visit many pages and try to find relevant parts within long pages, since the information may be scattered across documents. The long-standing goal of search engines should be providing relevant information , as opposed to relevant documents , to directly satisfy searchers X  needs.
This paper presents a novel search paradigm that the sys-tem should automatically discover information and present an informative overview for a query from multiple aspects. We target on dealing with informational queries. A query represents a centric topic, and the query aspects are aligned to user intents covering diverse information needs. The query aspects could be specified explicitly by users through an interface or automatically mined from search logs or other resources [4, 18, 22, 25]. In this paper, we use simple meth-ods to do aspect mining and mainly focus on multi-aspect oriented query summarization : given a query and a set of aspects, generate a summary for each query aspect, which is expected to provide specific and informative content to users directly and helps for further exploration. Figure 1 shows an example of the system output.

We further formulate the multi-aspect oriented query sum-marization into 2 phases: information gathering and sum-mary generation . Different from traditional text summariza-tion where a set of documents to be summarized is given as a system input, we propose a \Composite Query for Sum-marization" method, which leverages the search engine to proactively gather related information. In addition to using the search result of the original query, we also composite a set of new queries and submit them to the search engine to Fi gure 1: An example output of multi-aspect orient-ed query summarization. collect query aspect related information. For example, by concatenating the original query and the keywords of an as-pect as a query, we are able to get query dependent aspect information; by submitting the aspect keywords only as a query, we could get query independent aspect information. Our motivations are:
First, the search result of the original query may not con-tain enough information for all aspects that users care about, because the search engine returns documents only consider-ing whether a document is relevant to the query keywords, rather than its aspects.

Second, for better aspect oriented exploration, the infor-mation for different query aspects should be as orthogonal as possible. It is important to distinguish the aspect specific information from the general information about the whole query. By using the composite queries, we could get more specific information for each aspect.

The flexible information gathering also helps for summary generation phase. By comparing the search results of differ-ent types of composite queries, query (dependent) aspect words can be identified without complex natural language processing, based on which more specific and informative summaries could be generated,
The contributions of this paper can be summarized as follows:
The rest of the paper is organized as follows. First, we dis-cuss related work in Section 2. In Section 3, we define the query aspect and briefly introduce optional approaches for query aspect mining. In Section 4, we detail the proposed  X  X omposite query X  based method for both information gath-ering and aspect oriented summary generation. After that, we report our experimental results in Section 5. Section 6 states our conclusions.
Exploratory search becomes a new frontier in the search domain, which aims to provide additional support for in-formation seeking beyond simple lookup [24]. Recent work has shown that well-organized search results are helpful for information exploration. For example, search result clus-tering [9, 11, 27], categorizing [1], facet based information exploration [6], representative queries [23] and tag cloud-s [10] are adopted for search result navigation. Clustering based approaches automatically group similar search result documents together [9, 11, 27]. Search result documents can also be classified into a manually constructed category tax-onomy [1]. But the fixed hierarchy often lacks of flexibility to describe various user information needs. Faceted search aims to offer the ability for searchers to filter search result-s by specifying desired attributes [6]. However, the facets are usually pre-defined for some specific domain so that it is difficult to apply it to web search. Though most of the above methods organize search result documents into vari-ous aspects and improve user experience for information ex-ploration, the content are still presented at document level, and users can X  X  get relevant information directly.
Single document summarization techniques have been suc-cessfully applied in web search engines (snippet generation) [19, 20]. A span of text gives users a first sight of the topics of a document. For efficiency, sentence extraction strategy is used for generating query dependent summaries [5].
Comparing with single document summarization, multi-document summarization is expected to generate a glob-al picture for a set of documents which is given as input [15, 26]. Recently researchers utilize latent topics for multi-ple document summarization. For example, subtopics from the narrative of a topic (a description of a topic, which is provided by the DUC summarization track) is used to en-hance summarization [17]. Wang uses topic model to extract subtopics and select sentences by topic words [21]. Howev-er, the latent topics used in these papers are usually mined unsupervised. As a result, the topics may fit to the data collection, rather than align to user intents.

Some work makes use of predefined aspects to provide (sentiment) summarization on reviews or comments [7, 14]. Our work is also inspired by [13], which incorporates user interaction into the summarization process. Given a corpus of documents, users predefine their interested facets and the h ttp://en.wikipedia.org/ Fi gure 2: A snipping of returned documents for query  X  X aving Private Ryan X  and two typical ser-vices provided by Bing Search. system provides summaries according to the facets. The au-thors evaluate it on online reviews and Gene corpus (which are relatively  X  X lean X  data sets). In contrast, we focus on summarizing user intents related to a query rather than a given corpus. They don X  X  consider the informativeness of the generated summaries, while one of our goals is to pro-vide direct information to users.

Our work is based on query aspects but differs from ex-isting work in several points. First, in our framework, query aspects could be mined from any resources but not limit-ed to a set of documents to be summarized. Second, the traditional summarization task treats the documents as a given input to the system. However, in our scenario, we separate the information gathering and summarization gen-eration phases. In this way, we view the whole web as a cor-pus and could proactively collect more related information for summarization. Third, we aim to generate both specific and informative content for each query aspect. Therefore, users could get relevant information directly.
Multi-aspect oriented query summarization depends on query aspects. In this section, we define the query aspect and briefly discuss query aspect mining methods both in literature and in realistic way.

An aspect represents a distinct information need relevant to the original query. Recently, various methods have been proposed for automatically discovering query intents [2, 4, 22, 25]. The NTCIR-9 Intent Task was organized to explore and evaluate the technologies of mining and satisfying dif-ferent user intents for a vague query [18]. In these work, a query aspect is represented in different ways, such as a set of search queries related to the original query [2, 25], a set of query qualifiers [22] or a single intent string [18]. These defi-nitions are in fact very similar. The main differences are: (1) Whether distinguish the original query and the query qual-ifier. (2) Whether select an exemplar (label) to represent a set of queries related to the same intent.

Inspired by previous work, we define an aspect as a query qualifier -keywords that are added to an original query to form a specific user intent. For example,  X  X eviews X  and  X  X c-tors X  could be seen as aspects for a movie. In this work, we mainly focus on multi-aspect oriented summary generation and use very simple method to mine query aspects. Howev-er, any existing method for mining query aspects could be incorporated.We can also use the services provided by search engines to get approximate query aspects. For example, search engines provide X  X uery suggestion X  X r X  X elated search-es X  features. Figure 2 shows a snipping of the search result from Bing Search page for query  X  X aving Private Ryan X , a famous movie. Thus, the aspects could be easily identified using simple rules from related searches. We could also pre-define some aspect templates for certain query classes, such as movie, travel, music, people, etc. We leave this as future work.
Now, we suppose the aspects are given and aim to summa-rize a query according to its different aspects. We expect to generate both speci c and informative summary for each as-pect instead of a set of documents so that the users could get relevant information directly. First, we explain the meaning of specific and informative by an example. Suppose that for the query  X  X aving Private Ryan X , one of the user infor-mation needs is to know the  X  X ctors X  of this movie. There are some candidate sentences: (i) \A movie page covers information about new Steven (ii) \Saving Private Ryan cast are listed here including the (iii) \The actors of Saving Private Ryan are Tom Hanks as
All the three sentences contain certain information about the aspect  X  X ctors X . The first one talks about the general in-formation about the query. It is Not specific to the desired aspect. The second sentence focuses on the desired aspect, however, it does not provide relevant information directly, only gives navigational information. We say it is specific but Not informative. The third sentence should be a good candidate which provides direct answers to the desired as-pect, i.e., the names of the actors. It is both specific and informative.

As the example shows, the challenges of this task in-clude: (1) Distinguish aspect specific information from gen-eral query information. (2) Identify informative content in-stead of navigational information only. We take the Com-posite Query for Summarization method to deal with above issue, which consists of 2 phases: information gathering and summary generation. First, we proactively get aspect spe-cific information using composite queries. Then a mixture model is used to model different types of words which present query common information or aspect specific information. Finally, we rank the candidate sentences based on the mix-ture model and the redundancy in search results for gener-ating summaries.
Existing work on text summarization doesn X  X  pay much attention on how to collect data. A natural way is to use query search result. However, there may be not enough information for certain query aspects, if we only use the search result of the original query. For example, some users wonder whether movie  X  X aving Private Ryan X  tells a true story, but few top documents in the search result of  X  X aving Private Ryan X  discuss this topic.

We present a composite query based method for informa-tion gathering. Formally, we denote the original query as Q and an aspect as A k . For example, Q refers to the o-riginal query  X  X aving Private Ryan X  and A k refers to one aspect  X  X ctors X . In information gathering phase, we com-posite a new query by concatenating the original query and the aspect words, denoted as Q + A k . The composite query is  X  X aving Private Ryan actors X . Therefore, we can submit the composite query to the search engine to get top ranked documents. Comparing with the search result of the original query, the search result of the composite query is much more specific for the query aspect. Also, we can submit the aspect A k itself to the search engine to get information about the aspect which is query independent.

For a query with K aspects, we have a set of composite queries { Q; Q + A 1 ; :::; Q + A K ; A 1 ; :::; A K } . We use the top returned documents for each composite query. The search result of Q (denoted as C Q ) provides overall information about the query; the search result of Q + A k (denoted as C
Q + A k ) provides the information about the aspect A k of the query Q . The search result of A k (denoted as C A k ) provides information about the aspect itself which is query indepen-dent. The idea of using composite queries is straightforward and the benefits are two folded: (1) We collect more aspect related data which may be not contained in original query X  X  search result. (2) The search engine helps us roughly classify information according to the query aspects.

Based on the collected data for query aspects, we identify aspect words by comparing the search results of different types of composite queries. These words are then used for assisting summary generation.
We assume the desired information for query aspect A k is embedded in collection C Q + A k , which consists of 3 kinds of information: query general information, aspect information, irrelevant information. Correspondingly, the words in search results could be divided into 3 categories:
Query Common Words : They tend to occur frequently across multiple aspects, such as  X  X ovie X ,  X  X V X ,  X  X MDB X  for  X  X aving Private Ryan X .

Query Aspect Words : These words provide information for an aspect, such as  X  X ast X ,  X  X ist X  and  X  X om Hanks X  for the aspect  X  X ctors X .

Global Background Words : These words distribute heavily on the Web. Mostly, they are stop words or high frequency non-discriminative words.

Figure 3 shows 3 types of words and their relationship in search results of the original query and the composite Fi gure 3: The illustration of the relationship be-tween the search results of different composite queries and different types of words. queries. We assume that the query aspect words describing the aspect A k of query Q will occur more in C Q + A k , while the query common words will occur frequently across multi-ple aspects. Based on the collected data by using composite queries, the observations support the assumption. There-fore, we adopt a mixture model to describe each type of words. Formally, k represents the query aspect words model for aspect A k . B represents the query common words mod-el. G represents the global background words model which is to draw globally high frequency terms. All these models are multinomial probability distributions over vocabulary.
The collection C Q + A k could be generated by the mixture model. Each word w in C Q + A k is generated according to: where p k ( w ) represents the probability of a term occurrence w in collection C Q + A k , G and B are fixed parameters. The generative process could be seen as 2 steps: first decide whether this word is from G , and then decide it comes from B or k . To estimate the aspect word model k , we first estimate G and B . G is estimated using maximum likelihood estimator based on document frequency which is computed on a large collection of web pages. B is estimated by combining the search results of the original query and all query aspects, i.e., C Q  X  X  C Q + A k } . We use C Q to catch the general content of the query and the unknown aspects which are not defined explicitly or mined already. B could be estimated according to: where tf ( w;  X  ) represents the term frequency in a collection. After deriving p ( w | B ) and p ( w | G ), p ( w | k ) could be esti-mated using the expectation maximization (EM) algorithm [3] by maximizing the log-likelihood of the collection C Q + A For each term w in C Q + A k , the updating formulas of the E-step and the M-step are shown below:
E-S tep:
M-Step: wh ere z is a latent variable introduced to represent which type a word is assigned to. p ( z = G ) and p ( z = k ) are corre-sponding probabilities. In this way, we distinguish the query aspect words from the query common words . The words with high probabilities in k represent the specific query aspect better.

Next, we consider to identify more informative aspect words. We divide the query aspect words into 2 categories: query dependent aspect words which provide direct informa-tion for the aspect, such as X  X om Hanks X  X nd X  X dward Burn-s X  for aspect  X  X ctors X ; query independent aspect words which are query independent and reflect the characteristics of the aspect itself, like X  X ctor X ,  X  X ctress X , and X  X ast X  X or aspect X  X c-tors X . We distinguish these 2 types of query aspect words by the assumption that query dependent aspect words oc-cur in C Q + A k , and query independent aspect words occur in both C Q + A k and C A k . The C A k is the search result of the aspect A k itself, which contains many words related to the aspect. However, these words can be used for any query with such aspect, but don X  X  bring direct information for a specific query. So we identify query dependent aspect words as QDW k = { t | t  X  C Q + A k and t  X  X  C A k } . The words occur in C Q + A k that suggests they are related to the query as-pect, but don X  X  occur in C A k that indicates they are query dependent. The relative importance of the query dependent aspect words could be read out from p ( w | k ).
To summarize aspect A k for query Q , we extract sentences from the content of the search result documents in C Q + A C
Q . The candidate sentences are then ranked based on their specificity, redundancy and informativeness. The top ranked sentences are used as a summary for the desired aspect. Candidate sentence filtering based on specificity . On-ly part of the sentences within the search result are related to the desired aspect. We select a candidate sentence for a desired aspect only if it is closer to the desired aspect than to any other aspects. To measure this, we classify each sen-tence to one of the aspects: where i is an estimated query aspect words model or the query common words model or the global background words model. A sentence within C Q + A k  X  C Q is chosen as a candi-date only if k  X  equals to k . Thus, all the selected candidate sentences are more specific to the desired aspect. Sentence clustering . The candidate sentences are selected from multi-documents. Redundancy is particular importan-t. On one hand, the same information conveyed by sen-tences from different documents indicates its importance. On the other hand, it is not good to show duplicate sen-tences to users. Due to the above reasons, the candidate sentences are grouped into clusters according to lexical fea-tures. We adopt a hierarchical clustering approach. Each single sentence is initiated as a cluster. If two clusters are close enough, they are merged. This procedure repeats until the smallest distance between all remaining clusters is larg-er than a threshold. Edit distance is used to measure the distance between two sentences. We use U ( s ) to represent the cluster, which the sentence s belong to. The size of this cluster U ( s ) :size indicates the popularity of this cluster or the redundancy of the information this cluster conveys. Measuring informativeness . Since informative summaries are expected, we measure the informativeness of a sentence based on: inf o ( s | k ) = (1  X  ) where QDW k represents the query dependent aspect words for aspect A k ; is a parameter to tune the impact of the query dependent aspect words.
 Sentence ranking . In each cluster, we select one sentence with highest inf o ( s | k ) as the exemplar to represent the cluster. The exemplars selected from all clusters are ranked according to W eight k ( s ):
In the experiments, we assume the query aspects are given and focus on evaluating the quality of generated summaries for query aspects. The data sets we used already contain aspects for each query. Our method and baseline methods take both query and aspects as input.
To the best of our knowledge, few public data set can be used to evaluate the multi-aspect oriented query summariza-tion. We constructed two data sets from well-known data sources, Wikepedia and TREC. We will introduce the data sets and experimental results in following sections.
Each topic page in Wikipedia is composed of a title and a list of sub sections, which describe the topic from different aspects. For example, the title of a page is  X  X aving Private Ryan X , and the page includes subheadings like X  X lot X , X  X ast X  and X  X roduction X . In our experiments, we treated the title of a page as a query, the meaningful subheadings (top level) as query aspects. We filtered out the meaningless subheadings like  X  X otes X ,  X  X eferences X  and  X  X urther Readings X  by rules. We also filtered out pages with less than 3 or larger than 10 aspects to avoid noise. We used the textual content under a subheading as the golden reference for the corresponding aspect. In all, we sampled 1000 pages (queries) from an En-glish Wikipedia dump which was collected in January 2011. The statistics of the sampled data is listed in Table 1. W e divided the sampled data into develop set and test set. The develop set containing 100 queries was used for param-eter tuning. While the test set, which contains 900 queries, was used for comparing performance of different systems. Note that, since our method uses the search results of a search engine which may give Wikipedia pages as returned documents, we removed Wikipedia pages from the search results when doing experiments.
The trec data is widely used for search related experiment evaluation. We use the public available query set of TREC 2009 Web track. One goal of TREC 2009 Web Track is eval-uating the search result diversity. The data set includes 50 topics and each topic has 3 to 8 manually edited subtopics to be covered. Each subtopic is a description of an information need. Figure 4 shows an example topic provided by TREC 2009 Web track.

We treated each topic as a query and derived query as-pects from its subtopic descriptions by simple rules. We first extracted all nouns from a description. Then we excluded those terms which occur in original query, then used the remaining terms as an aspect. For example, for the query  X  X bama family tree X ,  X  X other information X  was used as one aspect. In all, we got 50 queries and 4.9 aspects for each query on average.
The proposed algorithm is denoted as Q-Composite . We compare it with 2 baselines. Baseline 1 is based on Ling et al [13], denoted as Ling-2008 . This method first estimates an aspect prior distribution based on term co-occurrence in the corpus, then integrates the priors into a topic model, finally ranks sentences according to the distance between sentence language model and the aspect models. It is proved very ef-fective for mining faceted summaries on relatively clean and formal data sets, like Gene corpus. But it is not oriented to the web search. Like traditional text summarization tasks, they just use a collection of documents related to the centric topic for summarization. We implemented this method and applied it to the multiple aspect based query summarization as a baseline. The aspect model of Ling-2008 was estimated on the search result of each original query and the sentences for each aspect were extracted from the search results of both the original query and the composite query, which was the same as the input of our method.The second baseline is based on the top sentences in snippets, which are provided by a search engine for each composite query Q + A k , denoted as Snippet . The number of the top sentences depends on the total summarization length limit. Though it is simple, it is very strong. These snippets are selected from the top relevant documents of the composite query, so that they are more likely specific to the query aspect. In addition, most snippet generation algorithms are based on single document Fi gure 4: An example topic in TREC 2009 web track. summarization method, which tend to extract the sentences containing most relevant terms.
There are several parameters in our method. We tuned the parameters of our method and baselines on the develop set. In our experiments, G was set to 0.95 in order to get more discriminative words. B was set to 0.8 to balance query common information and aspect specific information. The threshold used in sentence merging procedure was set to 0.7. The parameter was set to 0.0, which means to rank sentences based on query dependent aspect words only. For each composite query, we used the top 50 documents from the search result. The words occurring in less than 3 documents were discarded.
Due to the different characteristics of the two data sets, we adopt different evaluation strategies and metrics.
For Wikipedia data, we generate the summaries based on real web data. We send both the original query and the composite queries to a commercial search engine and get the search result documents and snippets. For efficiency, we train the model using the snippets and extract sentences from the content of the documents. We use the ROUGE tool for evaluation on Wikipedia Data. ROUGE is a well-known tool for evaluating both single and multi-document summarization [12]. Basically, it is a recall-like metric. A higher ROUGE value means that more useful information is found. ROUGE-1 metric has been proved highly consistent with human judgements, so we take it for evaluation in our experiments. At evaluating time, the golden reference for each aspect is taken from the content of corresponding sub-heading in a Wikipedia page. Since the extracted sentences for summarization have different length, we let each system generate top sentences and the first 200, 400 and 600 words are used for evaluation.
For TREC data set, we generate summaries from the cor-pus provided by TREC rather than the whole Web, namely the ClueWeb09. Our method depends on the search engine X  X  search result, so we need index ClueWeb09 and build a s-mall search engine. We use a simple ranking function to give search result based on BM25 [16], anchor text and stat-
La bel Ga in Value
I nformative and spe-cific
I nformative but not specific
S pecific but not in-formative
No t about this aspect
No t about this query 0 Th e sentence does not talk about ic rank features. It generates snippets by selecting the top sentences which contain the most query terms.

Since the data does not provide golden reference at sen-tence level, we have to judge the quality of generated sen-tences manually. So it is necessary to clarify the standard for assessment. Ideally, a good query summary should make users get the desired information directly. In our scenario, we assess the summaries from two perspectives: specific and informative. First, we hope the summary can give specific information about an aspect rather than a general descrip-tion covering multiple aspects. Second, it should give more direct information in contrast to navigational information so that users spend less time to obtain information.
Based on this standard, we asked labelers to label the gen-erated sentences for 50 queries. For each system and each query aspect, the labelers had to evaluate the top 3 ranked sentences. Each sentence was assigned a gain value accord-ing to the guidelines shown in Table 2, which describes the labeling standard by using an example. Note that we skip the gain value 3, because we think that the  X  X nformative and specific X  and  X  X nformative but not specific X  sentences are useful to users for getting direct information, should be given higher bonus than other levels. The topic description-s, as shown in Figure 4, were also presented to labelers as reference.

The normalized Discounted Cumulative Gain (nDCG) [8] is used to evaluate the performance. The nDCG is a metric that gives higher weights to well ranked objects. The average nDCG over all the test query aspects is used to measure the overall performance.
In this session, we present the experimental results on two data sets and analyze the performance of different systems and the impacts of key factors. Fi gure 5: The average coverage of the search result-s of the original queries over the composite aspect queries.
Previous work focuses on organizing the search result of the original query into multiple aspects. We argue that the search result of an original query may not have enough infor-mation covering all query aspects. To verify this, we conduct a simple experiment to measure the coverage of the search results of the original queries on the corresponding compos-ite queries. We sampled 100 queries from the Wikipedia data set. For each original query Q , we retrieved the set of top N URLs from a search engine, denoted as S N Q . For each composite query Q + A k , we retrieved the set of top M URLs from the same search engine, denoted as S M Q + A k . We measured the coverage of S N Q over S M Q + A k , i.e., . Th e average coverage over all queries X  aspects is shown in Figure 5. Intuitively, the search result of Q + A k should de-sc ribe the query aspect better. However, the top documents in S M Q + A k rarely appear in S N Q . For example, more than 60% top 1 documents retrieved by composite queries are not in the top 100 returned documents for the corresponding o-riginal queries. When considering more top documents in S
Q + A k , the coverage is even smaller. These observations in-dicate that, at the document level, the search results of the original queries couldn X  X  cover most relevant information re-lated to query aspects. By using composite queries, we could get much more relevant information. Next, we evaluate the quality of the fine-grained information units generated by systems.
Figure 6 shows the performance comparisons of different systems on Wikipedia test set, varying the word number of summary length limit. We can see that Q-Composite outper-forms both Ling-2008 and Snippet . The results on TREC 2009 data have the similar trend, which are shown in Fig-ure 7. We have found favorable results for Q-Composite on both NDCG@1 and NDCG@3. This shows proposed method is effective to extract more informative and aspect specific sentences. Especially, Q-Composite gains great improvemen-t on NDCG@1, which is important for presenting condensing information on result pages.

To gain more insights, we analyze the label level distribu-tions of the generated summary sentences of the 3 systems on TREC 20009 data set, as shown in Figure 8. The X-axis are label levels. The Y-axis is the distribution. We can see that our method provides more informative sentences (level 5 and level 4) compared with baselines. However, all systems still generate less specific and informative sentences than navigational sentences. This indicates the task is really challenging.

Q-Composite performs better than Ling-2008 . The rea-sons may include: (1) Ling-2008 estimates the aspect model on the search result of the original query. There may be not enough information covering all query aspects as shown in section 5.5.1. Therefore, for difficult aspects, it is unable to estimate accurate models. (2) In the search result of the original query, information related to multiple aspects of-ten mixes together. It increases the difficulty to estimate discriminative aspect models. Therefore, it is more difficult to provide specific information for desired aspect. (3) The search result is so noisy that there are many navigational sentences. For example, sentences containing  X  X ctors X  may also contain words like  X  X ast X ,  X  X ist X  and  X  X ctress X . These words are very easy to have higher weights in aspect mod-els and the sentences are ranked high as well. However, such sentences may only contain navigational information but can X  X  provide direct information. Another reason af-fecting the performance of Ling-2008 may be that we did not implement the variation with regularization, which is more complex but reported having better performance than the basic algorithm with Dirichlet model priors. In con-trast, by using composite query based method, we are able to get more aspect specific information and roughly classify the information. By distinguishing query dependent aspect words and query independent aspect words, we give bonus to sentences that are aspect specific but also contain more information beyond aspect words.

Snippet performs well on Wikipedia data set. It is rea-sonable, since the snippet generation algorithm favorites the Fi gure 6: ROUGE-1 performance of Q-Composite and baseline systems on Wikipedia test data. Fi gure 7: Performance comparisons between sys-tems on TREC 2009 data set. sentences containing many query terms. Thus the generat-ed summaries match many query aspect terms, which ben-efits ROUGE-1 metric, especially when the length of sum-maries is short. However, the snippets don X  X  show much informative information. From the Figure 8, we can see that Snippet provides more level 2 sentences (specific but not informative), but very few level 4 and level 5 sentences. The generated sentences usually lack of detail description about the query aspect, mostly are just navigational sen-tences which often fail to satisfy user information need di-rectly. Our method could get more aspect specific informa-tion by comparing the search results of multiple composite queries. Highlighting query dependent aspect words also helps select more informative sentences. Snippet generates less irrelevant sentences. One reason is that most sentences in snippets contain original query terms, while other meth-ods don X  X  have such constraint. Another reason may be that using composite queries may lead to topic drift, if the search results of the composite queries contain much noise.
Our method distinguishes the query dependent aspect word-s and query independent aspect words. We examine the im-pact of these two types of words. We set the parameter to be 0.5 in Equation 5 , which means we do not distinguish query dependent and independent aspect words. We de-note it as Q-Composite-AVG . Since the human judgements on TREC 2009 data directly measure the informativeness of the generated summaries, we compare Q-Composite ( = 0 : 0) and Q-Composite-AVG on this data set. Figure 9 shows the level distributions of the generated top 1 sentences. We can Fi gure 8: Level distributions of systems on TREC 2009 data set. Fi gure 9: Level distributions of Q-Composite and Q-Composite-Avg on TREC 2009 data set. see that Q-Composite generates more informative sentences (level 4 and level 5). In contrast, the Q-Composite-AVG gen-erates more specific but non-informative sentences (level 2). That is because Q-Composite favorites the words not only related to the aspect but also related to the original query. The results show that distinguishing query dependent as-pect words and independent words is useful for identifying more informative sentences. However, we also see that Q-Composite selects slightly more irrelevant sentences. This is because some composite queries bring in more noise, which leads to topic drift.
Our method uses the search results returned by the search engine. In this section, we examine whether the quality of returned documents can affect system performance. We simulate some not very good results, by removing some doc-uments from the search results or randomly picking doc-uments. We test on the Wikipedia test set, since the e-valuation can be done automatically. In details, we evenly remove 5 documents from the top 50 search results, denoted as remove5 , namely the 1st, 11st, 21st, 31st and 41st doc-uments. We construct the remove15 in the same way. We also randomly sample 50 documents from the top 1000 re-sults (denoted as random ) and select the last 50 documents (denoted as tail ).

The experimental results are shown in Figure 10. When the search results are not so bad ( remove5 or remove15 ), where most of the documents are relevant, the results are comparable. However, as the relevant documents reduce and noisy data increases, the models may be not very accurate. It shows worse results on random and tail . The results indi-Fi gure 10: The impact of seach engine, on Wikipedia test set using ROUGE-1 performance. cate our method depends on the quality of the search engine search results. For difficult composite queries, there may be no enough relevant candidate sentences for summarization. More noise may lead to topic drift as well.
In this paper, we presented a multi-aspect oriented query summarization task. This task aims to summarize a query from multiple aspects which are aligned to user intents. Ide-ally, the users could get relevant information satisfying their information needs directly. Specifically, we formulated the task into 2 main phases: information gathering and sum-mary generation. In the information gathering phase, we proposed a composite query based strategy, which proac-tively gets information based on the search engine. This strategy differs from traditional search result organization and text summarization, where the set of documents to be deal with is seen as a given system input. In the summary generation phase, we took into consideration the specifici-ty, informativeness and redundancy for sentence selection. We conducted experiments on 2 data sets. Both automat-ic evaluation and manually judgements were explored. We emphasized that the quality of aspect oriented summaries should be evaluated according to their specificity and infor-mativeness. The experimental results showed that by using composite queries, much more aspect relevant information could be got and our method outperformed 2 baselines for generating informative summaries.

The proposed method attempts to directly provide well organized and relevant information to users, as opposed to relevant documents . We have several possible directions of future work. First, in this paper we assume the query as-pects are given. We would examine the system performance when using automatically mined query aspects. Second, more advanced methods could be exploited to integrate mul-tiple sources of information related to a query for generating more informative summaries. Third, the composite query s-trategy could be applied for search result diversification by retrieving more aspect related documents.
 The 1st, 4th and 5th authors are supported by the Nation-al Natural Science Foundation of China under Grant No. 60736044, by the National High Technology Research and Development Program of China No. 2011ZX01042-001-001, by Key Laboratory Opening Funding of MOE-Microsoft Key La boratory of Natural Language Processing and Speech, Harbin Institute of Technology, HIT.KLOF.2009020. [1] H. Chen and S. T. Dumais. Bringing order to the web: [2] V. Dang, X. Xue, and W. B. Croft. Inferring query [3] A. P. Dempster, N. M. Laird, and D. B. Rubin. [4] Z. Dou, S. Hu, K. Chen, R. Song, and J.-R. Wen. [5] J. Goldstein, V. Mittal, J. Carbonell, and [6] M. A. Hearst. Clustering versus faceted categories for [7] M. Hu and B. Liu. Mining and summarizing customer [8] K. J  X  arvelin and J. Kek  X  al  X  ainen. Ir evaluation methods [9] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and [10] B. Y.-L. Kuo, T. Hentrich, B. M. . Good, and M. D. [11] D. J. Lawrie and W. B. Croft. Generating hierarchical [12] C.-Y. Lin and E. Hovy. Automatic evaluation of [13] X. Ling, Q. Mei, C. Zhai, and B. Schatz. Mining [14] Q. Mei, X. Ling, M. Wondra, H. Su, and C. Zhai. [15] A. Nenkova, L. Vanderwende, and K. McKeown. A [16] S. Robertson and H. Zaragoza. The probabilistic [17] C. Shen, D. Wang, and T. Li. Topic aspect analysis [18] R. Song, M. Zhang, T. Sakai, M. Kato, Y. Liu, [19] A. Tombros and M. Sanderson. Advantages of query [20] C. Wang, F. Jing, L. Zhang, and H.-J. Zhang. [21] D. Wang, S. Zhu, T. Li, and Y. Gong. Multi-document [22] X. Wang, D. Chakrabarti, and K. Punera. Mining [23] X. Wang and C. Zhai. Learn from web search logs to [24] R. White and R. Roth. Exploratory search. beyond [25] F. Wu, J. Madhavan, and A. Halevy. Identifying [26] W.-t. Yih, J. Goodman, L. Vanderwende, and [27] H.-J. Zeng, Q.-C. He, Z. Chen, W.-Y. Ma, and J. Ma.
