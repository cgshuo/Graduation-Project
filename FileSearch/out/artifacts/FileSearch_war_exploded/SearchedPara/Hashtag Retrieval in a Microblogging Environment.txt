 Microblog services let users broadcast brief textual messages to people who  X  X ollow X  their activity. Often these posts con-tain terms called hashtags, markers of a post X  X  meaning, au-dience, etc. This poster treats the following problem: given a user X  X  stated topical interest, retrieve useful hashtags from microblog posts. Our premise is that a user interested in topic x might like to find hashtags that are often applied to posts about x . This poster proposes a language model-ing approach to hashtag retrieval. The main contribution is a novel method of relevance feedback based on hashtags. The approach is tested on a corpus of data harvested from twitter.com .
 H.3.3 [ Information Search and Retrieval ]: Relevance Feedback Experimentation, Performance, Theory microblog, twitter, hashtag, relevance feedback
Microblogging services allow users to post brief textual messages that are broadcasted to the user X  X   X  X ollowers. X  To-day, the most visible microblogging service is twitter.com where users post so-called tweets of no more than 140 char-acters. While many tweets are inconsequential, others con-tain information of broad interest, as well as links to external resources (e.g. photos or websites). This poster proposes an approach to one aspect of microblog IR: retrieving hashtags on a topic of interest to a searcher.

Many tweets are marked with so-called hashtags. A hash-tag is a character string preceded by a # sign. Hashtags often signal aspects of a tweet X  X  meaning such as its topic or its intended audience. Thus #sigir2010 ostensibly marks tweets related to the 2010 SIGIR conference. A person who is interested in a topic, say vegetarian recipes , might want to find hashtags that are often applied to posts about vege-tarian recipes, veganism, healthy eating, etc.

For a user X  X  topical query q , we wish to find a list of k tags that are relevant to the information need represented by q . The task involves accepting a keyword query and returning a ranked list of hashtags. We approach hashtag retrieval as a type of entity search [1, 2].

Finding useful hashtags offers benefits that are particular to the microblog setting: The task we address aims to support these actions. This poster explicitly treats only the last item: query expansion. More specifically, we are concerned with query expansion in service to hashtag retrieval.
Let C be a corpus containing n tweets. Among these n tweets we have m distinct hashtag types. We induce m lan-guage models, one per hashtag. To fit a tag t i  X  X  language model we analyze the set of tweets containing t i , fitting a multinomial over the vocabulary words, with probability vector  X  i . The maximum likelihood estimator for  X  wi , the probability of a word w given  X  i , is the number of times w occurs in the set of tweets containing t i divided by this set X  X  total word count. We smooth estimated models  X   X  by Bayesian updating with Dirichlet priors (  X  = 2000). Given a query q generated by the query model  X  q , we rank hashtags in decreasing order of the negative KL divergence between their models and  X  q [3]: where unless otherwise noted the calculation uses  X   X  ML q maximum likelihood estimator for  X  q .
We propose restricting the added query terms to those candidates that are hashtags, stripping candidates of their leading # . The topical nature of hashtags motivates this operation.
 Let r k be the set of the k top-ranked hashtags (by Eq. 1). Here we set k = 25. We define  X  r , a multinomial parameter vector that has non-zero probability over the tags in r k zero probability for all other terms. We derive a feedback query model:  X   X  fb = (1  X   X  )  X   X  ML q +  X   X   X  r where  X  is a tun-able parameter on (0 , 1) that we fix at 0.2 (a value chosen empirically). As for  X   X  r , we propose two variants: where IDF ( t i ) is the inverse document frequency for tag i and max IDF is the IDF for a tag with document frequency 1. Feedback information enters retrieval by using  X  the query model in Eq. 1.
Given the previous definition of r k , let X be the k  X  k matrix where for two tags t i and t j in r k , x ij gives the number of times t i occurs (across the corpus) in a tweet that also contains t j . Also let x ii = 1. We normalize X so that its columns (rows) are of unit length. Let a tag X  X  association with the retrieved tags r k be: Eq. 2 is large if a tag co-occurs with many other tags in r . Large values for Eq. 2 suggest that a tag has a strong presence in the  X  X eighborhood X  of the query. We combine Eq. 1 with Eq. 2, leading to the ranking score, r a ( t r ( t i ,q ) + log a ( t i , r k ). Runs using r a are designated HFB1a or HFB2a (depending on the HFB used).
We gathered data over a 24-hour period using Twitter X  X  streaming API 1 (cf. Table 1). 29 topical queries were cre-ated based on the author X  X  interaction with Twitter. Rele-vance judgments were obtained using the Amazon Mechan-ical Turk service 2 . For each query, we created a pool of tags to be judged using runs from three systems: simple KL divergence, KL divergence on a Porter-stemmed cor-pus, and Rocchio relevance feedback using a TF-IDF model. Each judger was shown a keyword query, a candidate tag, a paragraph-long description of what would make a tag  X  X se-ful, X  and a sample of recent tweets using the tag. Judgers ranked each query-tag pair on a four-point scale from 0 (not useful) to 3 (definitely useful). Each query-tag pair was judged by 5 judgers. Usefulness scores were obtained by tak-ing both the mean and median of these 5 scores (we report results based only on means). We take tags with usefulness &gt; 1 to be relevant (graded relevance is used for NDCG). http://api.twitter.com http://www.mturk.com
We tested four experimental conditions: 1. Baseline, no feedback: simple KL divergence 2. Baseline, with feedback: KLD retrieval with diver-3. Hashtag-based query expansion (HFB1 and HFB2) 4. Hashtag query expansion with association measure (HFB2a). For all feedback, five terms were added to the initial query, with a weight of 0.2. The baseline feedback model used the top 10 documents. No stemming or stoplists were applied. Runs returned the top 25 tags for each query.

We report three statistics (Table 2): Mean average preci-sion (MAP), normalized discounted cumulative gain (NDCG) at 15, and precision at 10 (P10). All runs using hashtag-based feedback gave results that were statistically significantly better than the baseline run using standard term-based feed-back .
 Table 2: Retrieval effectiveness. All HFB runs show statistically significant improvement over the base-line feedback run on all three measures ( p &lt; 0 . 05 on a randomization test).  X  indicates p &lt; 0 . 01 .

Table 2 suggests that hashtags provide useful informa-tion for relevance feedback. While the baseline (term-based) feedback run was only slightly more effective than the base-line run without feedback, all tag-based feedback runs per-formed better than the term-based baseline feedback model. HFB2 (using IDF weighting for feedback tags) gives marginal improvement over uniformly weighted expansion, while our association measure gives a bit more of an edge. However, the differences among the three test conditions are slight.
In future work we will identify additional features of hash-tags (e.g. from author-based statistics) for use in IR. We will also undertake a more thorough empirical evaluation. The main work, however, lies in defining the relevant problems, applications, and user needs in IR from microblogs. [1] K. Balog, L. Azzopardi, and M. de Rijke. A language [2] C. Macdonald and I. Ounis. Using relevance feedback [3] C. Zhai and J. Lafferty. Model-based feedback in the
