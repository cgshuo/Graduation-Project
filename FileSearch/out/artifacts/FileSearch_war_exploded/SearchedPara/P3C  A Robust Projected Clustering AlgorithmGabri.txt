
Projected clustering has emerged as a possible solu-tion to the challenges associated with clustering in high di-mensional data. A projected cluster is a subset of points together with a subset of attributes, such that the cluster points project onto a small range of values in each of these attributes, and are uniformly distributed in the remaining attributes. Existing algorithms for projected clustering rely on parameters whose appropriate values are difficult to set by the user, or are unable to identify projected clusters with few relevant attributes.

In this paper, we present a robust algorithm for projected clustering that can effectively discover projected clusters in the data while minimizing the number of parameters re-quired as input. In contrast to all previous approaches, our algorithm can discover, under very general conditions, the true number of projected clusters. We show through an extensive experimental evaluation that our algorithm: (1) significantly outperforms existing algorithms for projected clustering in terms of accuracy; (2) is effective in detect-ing very low-dimensional projected clusters embedded in high dimensional spaces; (3) is effective in detecting clus-ters with varying orientation in their relevant subspaces; (4) is scalable with respect to large data sets and high number of dimensions.
Projected clustering has been mainly motivated by sem-inal research showing that, as the dimensionality increases, the farthest neighbor of a point is expected to be almost as close as its nearest neighbor for a wide range of data dis-tributions and distance functions [6]. Due to this lack of contrast in distances, the concept of proximity, and subse-quently the concept of a  X  X luster X , are seriously challenged in high dimensional spaces. At the same time, irrelevant attributes are as important a motivation as the number of attributes for projected clustering. Even in data sets with moderate dimensionality, clusters may exist in subspaces , which are defined as subsets of attributes. The irrelevant attributes may in fact  X  X ide X  the clusters by making two ob-jects that belong to the same cluster look as dissimilar as an arbitrary pair of objects. Furthermore, data objects may cluster differently in varying subspaces.

Traditional feature selection techniques are not effective in this scenario, because they may remove attributes that are relevant for some clusters and it may not be possible to re-cover those clusters in the remaining attributes [10]. Global feature transformation techniques (e.g., PCA), preserve to some extent the information from irrelevant attributes, and they may thus be unable to identify clusters that exist in dif-ferent subspaces [10].

Projected clustering assumes that meaningful structure can be detected only when data is projected onto subspaces of lower dimensionality. Virtually all existing projected clustering algorithms (PROCLUS [1], DOC/FASTDOC [11], HARP [14], SSPC [15], EPCH [9]) assume, explicitly or implicitly, the following definition of a projected cluster.
Definition 1. Given a database D of d -dimensional points. A projected cluster is defined as a pair ( X where (1) X i is a subset of D , and (2) Y i is a subset of at-tributes so that the projection of the points in X i along each attribute a  X  Y i has a small variance, compared to the vari-ance of the whole data set on a , and (3) the points in X uniformly distributed along every other attribute not in Y
For a projected cluster ( X i ,Y i ) , the attributes in Y called the  X  X elevant X  attributes for X i , whereas the remain-ing attributes are called  X  X rrelevant X  attributes for X i data model in projected clustering assumes that the data consists of k projected clusters, { ( X i ,Y i ) } i = 1 ,k of outliers, O , where { X 1 ,...,X k ,O } form a partition of D . The subsets of attributes { Y i } i = 1 ,k may not be disjoint and they may have different cardinalities. The outliers O are assumed to be uniformly distributed throughout the space. The projected clustering problem is to detect k projected clusters in the data, plus possibly a set of outliers.
Definition 1 states that the relevant attributes Y i of a pro-jected cluster ( X i ,Y i ) are a subset of the data attributes. Such projected clusters are easily interpretable by the user because the original attributes of the data set have specific meaning in real-life applications. ORCLUS [2] generalizes projected clusters ( X i ,Y i ) by assuming that Y i is an arbi-trary set of orthogonal vectors.

Projected clustering is related to subspace clustering [3] in that both detect clusters of objects that exist in subspaces of a data set. In contrast to projected clustering, subspace clustering detects clusters of objects in all subspaces of a data set and tends to produce a large number of overlap-ping clusters. Related problems have been addressed in the bi-clustering community [8], where (sub)sets of objects are considered similar if they follow similar  X  X ise-and-fall X  pat-terns across a (sub)set of attributes.

The performance of existing projected clustering algo-rithms depends greatly on (1) a series of parameters whose appropriate values are difficult to anticipate by the users (e.g., the true number of projected clusters or the average dimensionality of subspaces where clusters exist), or (2) the computation of k initial clusters, which is typically per-formed in full dimensional space based on various heuris-tics. The performance of the algorithms that fall within the second category depends on how well the initial clusters ap-proximate projected clusters in the data. These algorithms are likely to be less effective in the practically most inter-esting case of projected clusters with very few relevant at-tributes, because the members of such clusters are likely to have low similarity in full dimensional space.

In this paper, we propose an algorithm for mining pro-jected clusters, called P3C ( P rojected C lustering via C luster C ores) with the following properties.  X  P3C effectively discovers the projected clusters in the  X  P3C effectively discovers very low-dimensional pro- X  P3C effectively discovers clusters with varying orien- X  P3C is scalable with respect to large data sets and high
P3C is comprised of several steps. First, regions corre-sponding to projections of clusters onto single attributes are computed. Second, cluster cores are identified by spatial ar-eas that (1) are described by a combination of the detected regions and (2) contain an unexpectedly large number of points. Third, cluster cores are iteratively refined into pro-jected clusters. Finally, the outliers are identified, and the relevant attributes for each cluster are determined.
The remainder of the paper is organized as follows. Sec-tion 2 introduces preliminary definitions. Section 3 de-scribes our algorithm. Section 4 presents an extensive ex-perimental evaluation of P3C. Section 5 reviews work rele-vant for the this paper. Section 6 concludes the paper.
To present our algorithm for finding projected clusters, we introduce the following notation and definitions.
Let D =( x ij ) i = 1 ,n,j = 1 ,d be a data set of nd -dimensional data objects. Let A = { a 1 ,...,a d } be the set of all attributes of the objects in D . We can assume, without restricting the generality, that all attributes have normalized
An interval S =[ v l ,v u ] on attribute a j is defined as all real values x on a j so that v l  X  x  X  v u . The width of interval S is defined as width ( S ):= v u  X  v l . The attribute of an interval S is denoted by attr ( S ) , i.e., attr ( S S  X  a a , S 4 , S 5 and S 6 are intervals on attribute a 2 , attr attr ( S 2 )= attr ( S 3 )= a 1 , and attr ( S 4 )= attr ( attr ( S 6 )= a 2 . To ease the presentation, we specify the attribute of an interval only when it is necessary.
Let S be an interval on attribute a j .The support set of S , denoted by SuppSet ( S ) , represents the set of database ob-jects that belong to S , i.e., SuppSet ( S ):= { x  X  D | x.a S } .The support of S , denoted by Supp ( S ) , is the cardinal-ity of its support set, i.e., Supp ( S ):= | SuppSet ( S ) |
A p -signature S is defined as a set S = { S 1 ,...,S p } of p intervals on some (sub)set of p distinct attributes { a is also called the projection of S onto attribute a j i , i For example, in figure 1, S = { S 3 ,S 4 } is a 2 -signature, where S 3 is the projection of S onto attribute a 1 , and S is the projection of S onto attribute a 2 . { S 3 ,S 1 } is not a 2 -signature, because S 3 and S 1 are intervals on the same attribute a 1 .

The support set of a p -signature S = { S 1 ,...,S p } , de-noted by SuppSet ( S ) , represents the set of database objects that are contained in the support sets of all intervals in S , i.e., SuppSet ( S ):= { x  X  D | x  X  p i =1 SuppSet ( S i ) } .The support of a p -signature S , denoted by Supp ( S ) , is the car-dinality of its support set, i.e., Supp ( S ):= | SuppSet
A true p -signature  X  S of a projected cluster ( X i ,Y Y S i is the smallest interval on attribute a i that contains the projections onto a i of all the points in X i , i = 1 ,p .Fig-ure 1 illustrates two projected clusters, C 1 , and C having a 1 and a 2 as the only relevant attributes. The true p -signature of C 1 is the 2 -signature { S 1 ,S 6 } , and the true p -signature of C 2 is the 2 -signature { S 2 ,S 4 } .
Since an attribute may be relevant to more than one pro-jected cluster, true p -signatures may overlap , i.e., they may contain overlapping intervals. In figure 1, C 1 and C 2 have overlapping true p -signatures, since intervals S 1 and S overlap on attribute a 1 . We assume that true p -signatures can overlap as long as they are not nested within each other. True p -signatures  X  S and  X  R are nested if for every interval S in  X 
S , there is an interval S j in  X  R so that S i  X  S j .
P3C is based on the idea that if the true p -signatures of projected clusters were known, then clusters can be imme-diately computed as the support sets of the true p -signatures. Since the true p -signatures are not known, P3C computes in two steps a set of p -signatures that match or approximate well the true p -signatures of projected clusters in the data. First, on every attribute, intervals that match or approxi-mate well projections of true p -signatures onto that attribute are computed (section 3.1). Second, the challenge is to de-termine which intervals actually represent the same true p -signature. P3C addresses this challenge by aggregating the computed intervals into cluster cores . Roughly speaking, a cluster core consists of a p -signature S and its support set SuppSet ( S ) , so that the p -signature S approximates a true p -signature  X  S of a projected cluster C , and a large fraction of the points in SuppSet ( S ) belongs to C (section 3.2).
For the example in figure 1, P3C first computes the in-terval S 3 on attribute a 1 that approximates the projections of the true 2 -signatures { S 1 ,S 6 } and { S 2 ,S 4 } onto attribute a , and intervals S 5 and S 4 that approximate/match the pro-jections of the same true 2 -signatures onto attribute a 2 ond, P3C aggregates these intervals into two cluster cores, i.e., { S 3 ,S 4 } and { S 3 ,S 5 } , which can be regarded as ap-proximations of the two projected clusters in the data.
Cluster cores may include in their support sets additional points that do not belong to the projected clusters that they approximate. This happens when the intervals are wider than the projections of true p -signatures that they approx-imate. In figure 1, interval S 3 is wider than interval S and thus, the support set of cluster core { S 3 ,S 4 } includes points that do not belong to cluster C 2 . On the other hand, cluster cores may not include completely in their support sets the projected clusters that they approximate. This is the case when the intervals are tighter than the projections of true p -signatures that they approximate. In figure 1, inter-val S 5 is tighter than interval S 6 , and thus the support set of cluster core { S 3 ,S 5 } does not include all points of cluster C . Thus, in order to compute the projected clusters, the supports sets of cluster cores are iteratively refined (section 3.3). Finally, outliers are detected (section 3.4), and relevant attributes for each cluster are determined (section 3.5). 3.1 Projections of true p -signatures
This section describes how P3C computes, for each at-tribute, intervals that match or approximate well projections of true p -signatures onto that attribute.

An attribute that is irrelevant for all projected clusters exhibits, by definition 1 , uniform distribution. In contrast, an attribute that is relevant for at least one projected cluster will exhibit in general a non-uniform distribution, because it contains one or more intervals with unusual high support corresponding to projections of clusters onto that attribute.
Note that theoretically an attribute could exhibit uniform distribution, even though it is relevant for several projected clusters. This is the case when projected clusters are con-structed in such a way that their projections on a specific attribute have equal support, and thus they form a uniform histogram. In such cases, it may still be possible to recover p -signatures of the involved clusters, which are incomplete, but can be later refined -unless projected clusters are con-structed in such a way that all their relevant attributes look uniform. However, it is assumed that these situations are not common in typical applications for projected clustering.
We need to identify attributes with uniform distribution, and, for the non-uniform attributes, to identify intervals with unusual high support. For this task, the Chi-square goodness-of-fit test [13] is employed. Each attribute is di-vided into the same number of equi-sized bins . Sturge X  X  rule [13] suggests that the number of bins should be equal to 1+ log 2 ( n ) , where n is the number of data objects. For every bin in every attribute, its support is computed. The Chi-square test statistic sums, over all bins in an attribute, the squared difference between the bin support and the av-erage bin support, normalized by the average bin support. Based on the Chi-square statistic, the uniform attributes are determined at a confidence level of  X  =0 . 001 . The confi-dence level  X  does not act as a parameter of our method.  X  is set to one of the standard values used in statistical hypoth-esis testing: the value 0 . 001 signifies that the probability of declaring an attribute non-uniform when in fact the attribute is uniform is very small, i.e., less than 0 . 001 .
On the attributes deemed non-uniform, the bin with the largest support is marked . The remaining un-marked bins are tested again using the Chi-square test for uniform distri-bution. If the Chi-square test indicates that the un-marked bins  X  X ook X  uniform, then we stop. Otherwise, the bin with the second-largest support is marked. Then, we repeat test-ing the remaining un-marked bins for the uniform distribu-tion and marking bins in decreasing order of support, until the current set of un-marked bins satisfies the Chi-square test for uniform distribution. At this point, intervals are computed by merging adjacent marked bins. The process of marking bins is linear in the number of bins.

The computed intervals may be wider or tighter than the projections of true p -signatures that they approximate. Overlapping true p -signatures may lead to the former case (e.g., intervals S 1 and S 2 are approximated by interval S An example of the latter case is an interval that approxi-mates the projection of a true p -signature onto an attribute where the cluster is normally distributed. In this case, the interval may only capture the most dense region of the pro-jection (e.g., interval S 5 on attribute a 2 ).
In figure 1, the computed intervals form only two pos-sible 2 -signatures, { S 3 ,S 5 } and { S 3 ,S 4 } , which actually represent the two projected clusters C 1 and C 2 . However, in practical applications, the number of possible p -signatures that can be constructed from the set of computed intervals is large. The challenge is to determine which p -signatures do in fact represent projected clusters. This section describes how P3C addresses this challenge.

Let S be a p -signature. Let R = S  X  X  S } be a ( p +1) -signature composed of S and an interval S that is not in S . Assuming that S is a subset of some true t -signature T ( t&gt;p ), we could ask the question whether S also belongs to T . When S does belong to T , the support Supp ( R ) of R is likely to have a larger value than in the case when S does not belong to T , because, in the former case, Supp ( should include a large fraction of the projected cluster with signature T . Clearly, the support Supp ( R ) of R = S  X  X  is equal to the number of points in SuppSet ( S ) that also belong to S . Therefore, we want to compute how many points in SuppSet ( S ) are expected to belong to S in the case when S does not belong to T .

The points in SuppSet ( S ) are mainly points of a pro-jected cluster with signature T , and interval S does not be-long to T . In this case, under the assumption that points in SuppSet ( S ) are uniformly distributed in the attribute of interval S , the expected number of points in SuppSet ( S that also belong to S is proportional to width ( S ) .Thefol-lowing definition formally introduces the notion of expected support of a ( p +1) -signature R = S  X  X  S } with respect to a p -signature S obtained by adding interval S to S .
Definition 2. Let S be a p -signature. Let R = S  X  X  S } be a ( p +1) -signature composed of S and interval S ( S not in S ). The expected support of the ( p +1) -signature R given the p -signature S , denoted by ESupp( R = S  X  X  S }| is defined as:
We consider that if the actual support Supp ( R ) of R is significantly larger than the expected support ESupp( R = S  X  X  S }| S )of R given S , then this is evidence that S belongs to the same true t -signature as S .

We need a quantitative way of deciding when the ob-served support Supp ( R ) of R = S  X  X  S } is significantly larger than the expected support ESupp( R = S  X  X  S }| S )of R given S . For this task, we employ the Poisson probability density function P oisson ( v, E ) of observing v occurrences of a certain event within a time interval/spatial region, given the expected number E of random occurrences per time in-terval/spatial region [13]: where exp stands for the exponential function. In our case, we measure the probability of observing a certain number of points (i.e., Supp ( R ) ) within a spatial region, given the expected number of points within this spatial region (i.e., ESupp( R = S  X  X  S }| S )), under a random process that uni-formly distributes the points in SuppSet ( S ) onto the at-tribute of S .

We call an observed support significantly larger than an expected support, if the observed support is larger than the expected support, and the Poisson probability of observing the support given the expected support is smaller than a cer-tain value, which we call the Poisson threshold .

The Poisson probability quantifies how likely the ob-served support Supp( R )of R is with respect to the expected support ESupp( R = S  X  X  S }| S )of R given S : the less likely the observed support, the stronger the evidence that S rep-resents the same projected cluster as S . The Poisson thresh-old is the only  X  X arameter X  required by P3C. The Poisson threshold is different from typical parameters used by clus-tering algorithms (such as the number of clusters) in that it requires little prior knowledge about the data. The Poisson threshold signifies the error probability that the user is will-ing to accept. Concretely, the value 1 . 0 E  X  20 for the Pois-son threshold signifies that the probability of declaring that S represents the same projected cluster as S , when in fact this is not true, is very small, i.e., less than 1 . 0 E  X  20 is why higher values for the Poisson threshold like 1 . 0 are not useful. On the other hand, a very small value for the Poisson threshold would result in failing to recognize that S represents the same projected cluster as S , when in fact this is true. The robustness of P3C to the Poisson threshold is studied empirically in section 4 (see figure 2).
Intuitively, a p -signature S = { S 1 ,...,S p } represents a projected cluster C if S consists of (1) only and (2) all intervals that represent cluster C . The first condition is equivalent to requesting that for any q -signature Q ( q = 1 ,p  X  1 ), and any interval S  X  S \ Q , there is ev-idence that S represents the same projected cluster as Q . The second condition is equivalent to requesting that S is maximal , i.e., for any interval S not in S , there is no ev-idence that S represents the same projected cluster as S . Formally, a cluster core can be defined as following. Definition 3. A p -signature S = { S 1 ,...,S p } together with its support set SuppSet ( S ) is called a cluster core ,if: 1. For any q -signature Q  X  S , q = 1 ,p  X  1 , and any in-2. For any interval S not in S , it holds that
Condition 1 in definition 3 is equivalent to requesting, for any q -signature Q  X  S ( q = 1 ,p  X  1 ), and any interval S  X  S \ Q , that Supp( Q  X  X  S } ) is significantly larger than ESupp ( Q  X  X  S }| Q ). Condition 2 in definition 3 is equiva-lent to requesting, for any interval S not in S , that Supp( S  X  X  S } )is not significantly larger than ESupp ( S  X  X  S }|
Condition 1 in definition 3 is anti-monotonic, in the sense that, given a p -signature S that satisfies condition any sub-signature of S also satisfies condition 1 . This fact motivates an Apriori-like generation of p -signatures that satisfy condition 1 . Condition 1 acts like the support test in frequent itemsets generation [4]: a signature consisting of ( q +1) intervals will not be generated if any of its sub-signatures consisting of q intervals does not satisfy condi-tion 1 . p -signatures that satisfy condition 1 are generated, and the ones that are  X  X aximal X  in the sense of condition are reported as cluster cores.
Let k be the number of cluster cores constructed accord-ing to section 3.2. The support sets of these cluster cores may not necessarily contain all and only the points of the projected clusters that the cluster cores approximate, de-pending on the accuracy of the intervals computed in sec-tion 3.1. In this section, we discuss how P3C refines the k cluster cores into k projected clusters.

The refinement of k cluster cores into k projected clus-ters is performed in a subspace of (reduced) dimension-ality d of the original d -dimensional data, containing all attributes that were deemed non-uniform according to the analysis presented in section 3.1.

The support sets of the k cluster cores are not necessar-ily disjoint, because they may contain, in addition to the members of the clusters approximated by the cluster cores, outlier objects and/or other clusters X  members that have the signatures of other cluster cores. The membership of data points to cluster cores can be described through a fuzzy membership matrix M =( m il ) i = 1 ,n,l = 1 ,k , where m notes the membership of object i to cluster core l ; it is de-fined as follows: m il =0 , if data point i does not belong to the support set of any cluster core; m il is equal to the fraction of clusters cores that contain data point i in their support set, if i is in the support set of cluster core l .
We want to compute for each data point its probability of belonging to each projected cluster using the Expectation Maximization (EM) algorithm [7]. For this purpose, we will initialize EM with the fuzzy membership matrix M . Since the fuzzy membership matrix M contains unassigned data points, i.e., data points with membership 0 everywhere, we first assign these points to one of the k cluster cores.
In the case of projected clusters, by definition 1 ,clus-ter members project closely to cluster means on the direc-tions with the least spread. Thus, cluster members have shorter Mahalanobis distances to cluster means than non-cluster members. Provided that the support set of a cluster core mainly consists of members of the projected cluster C approximated by the cluster core, data points with short Ma-halanobis distance to the mean of the support set are highly likely to be members of C . Based on these considerations, unassigned data points are assigned to the  X  X losest X  cluster core in terms of Mahalanobis distances to means of support sets of cluster cores.

Once all unassigned points have been assigned to clus-ter cores, the fuzzy membership matrix M is equivalent to a fuzzy partition of the data points into k projected clus-ters. EM computes data points X  probabilities of belonging to projected clusters based on Mahalanobis distances between data points and the means of projected clusters. Therefore, cluster members have higher probabilities of belonging to their clusters than non-cluster members. EM is considered to converge when the means of the projected clusters remain unchanged between two consecutive iterations. Typically, when starting with cluster cores, it takes only 5 to erations until convergence, since the cluster cores typically approximate well projected clusters in the data.

The output of EM is a matrix of probabilities that gives for each data point its probability of belonging to each pro-jected cluster. Since the data model in projected clustering assumes disjoint projected clusters, we convert the matrix of probabilities produced by EM into a hard membership ma-trix by assigning each data point to the most probable pro-jected cluster. Interestingly, our method can also be used to discover overlapping clusters. In this respect, P3C positions itself between projected and subspace clustering.
Although each data point has been assigned to a pro-jected cluster in section 3.3, the data set may contain out-lier points that need to be identified. We use a standard technique for multivariate outlier detection [12]. The Ma-halanobis distances between data points and the means of the projected clusters to which they belong are compared to the critical value of the Chi-square distribution with d de-grees of freedom at a confidence level of  X  =0 . 001 .The confidence level  X  signifies that the probability of failing to recognize a true outlier is less than 0 . 001 . Data points with Mahalanobis distances larger than this critical value are de-clared outliers.
Once the cluster members have been identified, the rele-vant attributes for each projected cluster can be determined. The relevant attributes of a projected cluster include the at-tributes of the intervals that make up the p -signature of the cluster core based on which this cluster has been computed. As discussed in section 3.1, an attribute may be considered uniform although it may be relevant for several projected clusters. To cover these rather rare cases too, we test, for each projected cluster, using the Chi-square test, whether its members are uniformly distributed in the attributes initially deemed uniform. When the members of a projected cluster are not uniformly distributed in one of the attributes initially considered uniform, then those attributes are included in the attributes considered relevant for the projected cluster. Fi-nally, the p -signatures of projected clusters can be refined by computing for each relevant attribute the smallest inter-val that the cluster members project onto.
The experiments reported in this section were conducted on a Linux machine with 3 GHz CPU and 2 GB RAM.

Synthetic Data. Synthetic data sets were generated as described in [1], [2], with n =10 , 000 data points, d = 100 attributes, 5 clusters with sizes 15% to 25% of n , and 5% n outliers. The performance of P3C is studied based on the following criteria: 1. Distribution of cluster points in the relevant subspace: 2. Projected clusters having an equal number of rele-3. Projected clusters with axis-parallel orientation versus Combining these three criteria results in 8 categories of data sets. A data set in the category  X  X niform Equal Parallel X  is a data set for which the cluster points are uniformly dis-tributed in the relevant subspace, the number of relevant at-tributes for each projected cluster is equal , and the eigen-vectors of each projected cluster X  X  covariance matrix are parallel to the coordinate axes. In each category, we gener-ated data sets with average cluster dimensionality 2% , 4% 6% , 8% , 10% , 15% , and 20% of the data dimensionality d . In total, 56 synthetic data sets have been generated.
For data sets where cluster points are normally dis-tributed in their relevant subspace, we ensured that the vari-ance of cluster members on individual relevant attributes is between 1% and 10% of the variance of all data points when uniformly distributed on an attribute. Various amounts of overlap were introduced among the signatures of projected clusters, i.e., the larger the average cluster dimensionality, the higher the chance for the overlap between signatures.
Real Data. We have tested P3C on two real-world data sets. The first data set is the colon cancer data set of Alon et al. [5] that measures the expression level of 40 tumor and 22 normal colon tissue samples in 2000 human genes. The task is to discover projected clusters using samples as data objects and genes as attributes. This task is challenging due to the data sparsity (i.e., 62 data points in 2000 attributes), but of practical importance. A relevant attribute of a pro-jected cluster represents a gene that has similar values in the samples that belong to the projected cluster. Provided that a projected cluster contains mainly tumor or normal sam-ples, the relevant attributes are potential indicators for the presence, respectively absence, of colon cancer.

Projected clusters may exist in data sets with moderate dimensionality when some of the attribute are irrelevant. The second data set is the Boston housing data 2 , which con-sists of 12 numerical attributes of 506 suburbs of Boston. Since this data set is not labeled, we apply clustering in an exploratory fashion, and report interesting findings. Experimental setup. We evaluate the performance of P3C against the following competing algorithms for pro-jected clustering 3 : PROCLUS [1], FASTOC [11], HARP [14], SSPC [15], and ORCLUS [2].
 P3C requires only one parameter setting, namely the Poisson threshold. P3C does not require the user to set the target number of clusters; instead, it discovers a cer-tain number of clusters by itself. In contrast, all competing algorithms require the user to specify the target number of clusters.

On synthetic data, we have run the competing algorithms with the target number of clusters equal to the true number of projected clusters. PROCLUS and ORCLUS require the average cluster dimensionality as a parameter, which was set to the true average cluster dimensionality. HARP re-quires the maximum percentage of outliers as a parameter, which was set to the true percentage of outliers. For FAST-DOC and SSPC, several reasonable values for their param-eters were tried, and we report results for the parameter set-tings that consistently produced the best accuracy. SSPC was run without any semi-supervision. Except HARP, all competing algorithms are non-deterministic; thus each of them is run five times, and the results are averaged.
On the colon cancer data, we have run the competing algorithms with the target number of clusters equal to the number of classes (i.e., 2 ). Multiple values were tried for the other parameters required by the competing algorithms, and the results with the best accuracy are reported. Since this data set contains no points labeled as outliers, the outlier removal option of all algorithms was disabled.

On the housing data, since it has no labels, the evaluation of the competing algorithms is cumbersome. The reason is that the performance of the competing algorithms is depen-dent on a large number of required parameters, including critical ones such as the number of clusters and the aver-age cluster dimensionality. Under these circumstances, we apply only P3C on the second real data set.

Performance measures. We refer to true clusters as in-put clusters, and to found clusters as output clusters. On synthetic data, cluster labels and relevant attributes for each cluster are known. On the colon cancer data, only the clus-ter labels are known. We use an F 1 value to measure the clustering accuracy. For each output cluster i , we determine the input cluster j i with which it shares the largest number of data points. The precision of output cluster i is defined as the number of data points common to i and j i divided by the total number of data points in i .The recall of output cluster i is defined as the number of data points common to i and j i divided by the total number of data points in j The F 1 value of output cluster i is the harmonic mean of its precision and recall. The F 1 value of a clustering solu-tion is obtained by averaging the F 1 values of all its output clusters. Similarly, we use an F 1 value to measure the ac-curacy of found relevant attributes based on the matching between output and input clusters.
 Sensitivity analysis. We have studied the sensitivity of P3C to the Poisson threshold. Figure 2 illustrates the ac-curacy of P3C measured using the two F 1 values intro-duced above for one of our synthetic data sets as the Pois-son threshold is progressively decreased from 1 . 0 E  X  10 1 . 0 E  X  100 . We observe that P3C is remarkably robust with respect to the Poisson threshold. Similar results have been obtained on all our synthetic data sets, but are omitted due to space limitations. Consequently, we have set the Poisson threshold at 1 . 0 E  X  20 .

Accuracy results. On synthetic data, in all the per-formed experiments, the number of clusters discovered by P3C equals the true number of projected clusters in the data.
Figures 3 to 10 show the accuracies of the compared al-gorithms as a function of increasing average cluster dimen-sionality for the 8 categories of data sets. We observe that P3C significantly and consistently outperforms the compet-ing projected clustering algorithms, both in terms of cluster-ing accuracy and in terms of accuracy of the found relevant attributes.

The difference in performance between P3C and previ-ous methods is particularly large for data sets that contain very low-dimensional projected clusters embedded in high dimensional spaces. Even in these difficult cases P3C shows very high accuracies, in contrast to the modest accuracies obtained by the competing algorithms. As the average clus-ter dimensionality increases, the accuracy of the competing algorithms increases as well.

Our experiments indicate that P3C effectively discovers projected clusters with varying orientation in their relevant subspaces. The accuracy of P3C on data sets where pro-jected clusters have axis-parallel orientation is as high as the accuracy of P3C on data sets where projected clusters have arbitrary orientation.

The accuracy of P3C on data sets where projected clus-ters are uniformly distributed in their relevant subspaces is slightly higher than the accuracy of P3C on data sets where projected clusters are normally distributed in their relevant subspaces. The reason is that projections of clusters onto their relevant attributes can be approximated more faithfully by the computed intervals for clusters in the former category than for clusters in the latter category.

The number of relevant attributes for projected clusters does not have an impact on the performance of P3C. This is to be expected, since P3C does not use in any way the average cluster dimensionality. Interestingly, the accuracy of the found relevant attributes is 100% in all experiments.
On the colon cancer data, P3C discovers 2 projected clusters. P3C obtains the highest clustering accuracy ( 67% followed by HARP ( 55% ) and SSPC ( 53% ), whereas the ac-curacies of the other projected clustering algorithms are sig-nificantly lower on this data set: FASTDOC and PROCLUS obtain 43% accuracy, and ORCLUS obtains 35% . The di-mensionality of these 2 projected clusters in 11 , which is much smaller than the dimensionality of the data set (i.e., 2000 ). This indicates that only a relatively small fraction of genes out of the total number of genes may be relevant for distinguishing between cancer and normal tissues, as also noted in previous work [5]. The biological significance of the genes selected as relevant is yet to be determined.
On the housing data, P3C discovers 2 projected clusters, which exist in subspaces of dimensionality 4 . The first pro-jected cluster contains suburbs that are similar in terms of residential land, crime rate, pollution and property tax. The second projected cluster contains suburbs that are similar in terms of business land, size, distance to employment cen-ters, and property tax. This data set illustrates that projected clusters can exist in data sets with a moderate number of at-tributes when some of these attributes are irrelevant. To ver-ify that the members of the 2 projected clusters are not close in full dimensional space, we have run KMeans ( k =2 ) several times. In all runs, the members of the projected clusters discovered by P3C are distributed between the clus-ters found by KMeans, which indicate that full dimensional clustering cannot reproduce the same clusters.

Robustness to outliers. Data sets with n =10 , 000 , d = 100 , 5 clusters, average cluster dimensionality 4 , and different percentages of outliers were generated. Figure 11 shows the accuracies of the compared algorithms as a function of increasing percentages of outliers. P3C, as well as the competing algorithms, are robust in the presence of outliers. The clustering accuracy of P3C decreases only slightly as more outliers are introduced. Even when the percentage of outliers in the data is as high as 25% , P3C still obtains a clustering accuracy of 86% . The accuracy of the found relevant attributes of P3C remains 100% with increasing percentages of outliers.

Scalability experiments. In all scalability figures, the time is represented on a log scale.

Figure 12 shows scalability results for data sets with d = 10 , 2 clusters, 5% outliers, average cluster dimensionality and increasing database sizes. The scalability of P3C with respect to database size is comparable to the scalability of the fastest projected clustering algorithms.

Figure 13 shows scalability results for data sets with n =10 , 000 , 2 clusters, 5% outliers, average cluster di-mensionality 2 , and increasing database dimensionalities. P3C is relatively unaffected by increasing data dimension-ality, because attributes with uniform distributions are not involved in the computation of cluster cores.

Figure 14 shows scalability results for data sets with n =10 , 000 , d = 100 , 5 clusters, 5% outliers, and in-creasing average cluster dimensionalities. The running time of P3C increases with increasing average cluster dimen-sionality, due to the increased complexity of p -signatures generation. However, as the average cluster dimension-ality increases, clusters become increasingly detectable in full dimensional space. P3C has comparable running times to the other projected clustering algorithms at low average cluster dimensionality, which is the critical cases that  X  X ull-dimensional X  clustering algorithms cannot deal with.
In summary, P3C consistently and significantly outper-forms existing projected clustering algorithms in terms of clustering accuracy and accuracy of the found relevant at-tributes, while being as efficient as the fastest of these algo-rithms on data sets with low-dimensional projected clusters.
PROCLUS [1] is essentially a k -medoid algorithm adapted to projected clustering. A main difference to the standard k -medoid algorithm is that initial clusters around the medoids have to be computed as basis for the simulta-neous selection of relevant attributes. The performance of PROCLUS crucially depends on 2 required input parame-ters ( k -the desired number of projected clusters, and l -the average cluster dimensionality), whose appropriate val-ues are difficult to guess. Another weakness is the strong dependency on the initial clustering which is hard to deter-mine since it is performed in full-dimensional space where the  X  X rue X  distances will be distorted by noisy attributes.
ORCLUS [2] is a generalization of PROCLUS that can discover clusters in arbitrary sets of orthogonal vectors. The quality of a projected cluster is defined as the sum of the variances of the cluster members along the projected at-tributes. Therefore, in order to identify the projection in which a set of points cluster  X  X est X  according to the qual-ity measure, ORCLUS selects the eigenvectors correspond-ing to the smallest eigenvalues of the covariance matrix of the given set of points. The parameter l is used to decide how many such eigenvectors to select. While ORCLUS can find significantly more general clusters, it inherits the weak-nesses of PROCLUS discussed above.

DOC [11] defines a projected cluster as a pair ( C, D ) , where C is a subset of points, and D is a subset of attributes, such that C contains at least a fraction  X  of the total num-ber of points, and D consists of all the attributes on which the projection of C is contained within a segment of length w . DOC defines the function  X  to measure the quality of a projected cluster as  X  ( | C | , | D | )= | C | X  (1 / X  ) | is a user-specified parameter that controls the trade-off be-tween the number of data points and the number of relevant attributes in a projected cluster. DOC computes one pro-jected cluster at a time, optimizing its quality using a ran-domized algorithm with certain quality guarantees. In order to reduce the time complexity of DOC, its authors introduce a variant, called FASTDOC, which uses three heuristics to reduce the search time. Similar to PROCLUS, the perfor-mance of DOC is sensitive to the choice of the input param-eters, whose values are difficult to determine for real-life data sets. In addition, the assumption that a projected clus-ter is a hyper-cube of same side length in all attributes may not be appropriate in real applications.

HARP [14] is an agglomerative, hierarchical clustering
Figure 14: Scalability with increasing avg. cluster dim algorithm that starts by placing each data object in a clus-ter. Two clusters are allowed to merge if the resulting clus-ter has d min or more relevant attributes, and an attribute is selected as relevant for the merged cluster if a given rele-vance score is greater than R min . d min and R min are two internal thresholds that start at some harsh values so that only objects belonging to the same real cluster are likely to be merged. Subsequently, as the clusters increase in size, and the relevant attributes are more reliably determined, the two thresholds are progressively decreased, until they reach some base values or a certain number of clusters has been obtained. HARP avoids some of the problems of the pre-vious approaches, such as the computation of initial clus-ters, or the usage of parameters whose values are difficult to set. However, HARP inherits the drawbacks of hierar-chical clustering algorithms, in particular the lack of back-tracking in the clustering process and the quadratic runtime complexity which makes it not scalable to large data sets.
Yip et al. proposes the algorithm SSPC [15], similar in structure to PROCLUS, and whose performance can be im-proved by the use of domain knowledge in the form of labeled objects and/or labeled attributes. The algorithm uses an objective function based on the relevance score of HARP [14]. The quality of a clustering solution is the sum of the qualities of each individual cluster, and the quality of an individual cluster is the sum of the relevance scores of the cluster X  X  relevant attributes. The performance of SSPC depends on a user-defined parameter that controls the relevance scores of attributes. SSPC can find projected clusters with moderately low dimensionality whereas most other methods fail due to an initialization based on the full-dimensional space.

EPCH [9] computes low-dimensional histograms ( 1 D or 2
D ), and  X  X ense X  regions are identified in each histogram, based on iteratively lowering a threshold that depends on a user-specified parameter. For each data object, a  X  X ig-nature X  is derived, which consists of the identifiers of the dense regions the data object belongs to. The similarity between two objects is measured by the matching coeffi-cient of their signatures in which zero entries in both sig-natures are ignored. Objects are grouped in decreasing or-der of similarity until at most a user-specified number of clusters is obtained. EPCH differs from our method both in how the computation of low-dimensional projections of projected clusters is performed, and in how these projec-tions are used to recover projected clusters. In particular, dense regions from different attributes are not combined into higher-dimensional regions, but used to measure the similarity of pairs of objects. In addition, the performance of EPCH is sensitive to the values of its parameters.
Projected clustering is motivated by data sets with a large number of attributes or with irrelevant attributes. Existing projected clustering algorithms crucially depend on user pa-rameters whose appropriate values are often difficult to an-ticipate, and are unable to discover low-dimensional pro-jected clusters. In this paper, we address these drawbacks through the novel, robust projected clustering algorithm P3C. P3C is based on the computation of so-called cluster cores. Cluster cores are defined as regions of the data space containing an unexpectedly high number of points, forming cores of actual projected clusters. Cluster cores are gen-erated in an Apriori-like fashion, and subsequently refined into projected clusters. Lastly, outliers are removed and the relevant cluster attributes are detected. Our experimental evaluation on numerous synthetic data sets and two real data sets demonstrates that P3C can indeed discover pro-jected clusters, including clusters in very low-dimensional subspaces, and clusters with varying orientation, distribu-tion or number of relevant attributes, while being robust to the only required parameter. P3C consistently outperforms the state-of-the-art methods in terms of accuracy, and it is robust to noise. In addition, our algorithm scales well with respect to large data sets and high number of dimensions.
As future work, we will investigate the extension of P3C for categorical data.

Acknowledgments. We would like to thank Kevin Yip from Yale University for providing us with the implemen-tation of the comparing algorithms for projected clustering. This research was supported by the Alberta Ingenuity Fund and the iCORE Circle of Research Excellence.

