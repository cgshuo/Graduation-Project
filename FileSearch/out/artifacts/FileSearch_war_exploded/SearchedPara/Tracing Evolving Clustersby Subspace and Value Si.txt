 Temporal properties of patterns and their analysis are under active research [5]. A well known type of pattern are clusters, corresponding to similarity-based groupings of data objects. A good example for clusters are customer groups. Clusters can change in the course of time and understanding this evolution can be used to guide future decisions [5], e.g . predicting whether a specific customer behavior will occur. The evolution can b e mined by cluster tracing algorithms that find mappings between clusters of consecutive time steps [8,13,14].
The existing algorithms have a severe limitation: Clusters are mapped if the corresponding object sets are similar, i.e. the algorithms check whether the pos-sibly matching clusters have a certain fraction of objects in common; they are unable to map clusters with different objects, even if the objects have similar attribute values. Our novel me thod, however, maps clusters only if their corre-sponding object values are similar, independently of object identities. That is, we trace similar behavior types, which is a fundamentally different concept. This is a relevant scenario, as the following two examples illustrate.

Consider scientific data of the earth X  X  surface with the attributes temperature and smoke degree. The latter correlates with forest fire probability. The attribute values are recorded over several months. In this dataset, at some point in time a high smoke degree and high temperatures occur in the northern hemisphere; sixth months later the same phenomenon occurs in the southern hemisphere, as the seasons on the hemispheres are shifted half-yearly to each other. Another example is the customer behavior of people in different countries. Often it is similar, but shifted in time. For example, the customer behavior in Europe is similar to the behavior in North America, but only some months later. Obviously, a cluster tracing algorithm should detect these phenomena; however, existing methods do not, since the observed populations, i.e. the environment and the people respectively, stay at the same pla ce, and thus there are no shared objects between clusters  X  only the behavior migrates.

With today X  X  complex data, patterns are often hidden in different subsets of the dimensions; for detecting these clusters with locally relevant dimensions, subspace clustering was introduced. However, despite that many temporal data sets are of this kind, e.g. gridded scientific data, subspace clustering has never been used in a cluster tracing scenario . The existing cluster tracing methods can only cope with fullspace clusters, and thus cannot exploit the information mined by subspace clustering algorithms. Our novel tracing method measures the subspace similarity of clusters and thus handles subspace clusters by design.
Summarized, we introduce a method for tracing behavior types in tempo-ral data; the types are represented by cl usters. The decision, which clusters of consecutive time steps are mapped is based on a novel distance function that tackles the challenges of object value similarity and subspace similarity. Our approach can handle the following developments: emerging or disappearing be-havior as well as distinct behaviors that converge into uniform behavior and uniform behavior that diverges into distinct behaviors. By using subspaces, we enable the following evolutions: Behavior can gain or lose characteristics; i.e., the representing subspace clusters can gain or lose dimensions over time, and clusters that have different relevant dimensions can be similar. Varying behavior can be detected; that is, to some extent the values of the representing clusters can change.

Clusterings of three time steps are illustrated in Fig. 1. The upper part shows the objects; the lower part abstracts fro m the objects and illustrates possible clusterings of the datasets and tracings between the corresponding clusters. Note that the three time steps do not share objects, i.e. each time step corresponds to a different database from the same attribute domain { d 1 ,d 2 } ; to illustrate the different objects, we used varying object symbols. An example for behavior that gains characteristics is the mapping of Cluster C 1 , 1 to C 2 , 1 , i.e. the cluster gains one dimension. Varying behavior is illustrated by the mapping from C 1 , 2 to C 2 , 2 ; the values of the cluster have changed. If the databases were spatial, this could be interpreted as a movement. A behavior divergence can be seen from time step Several temporal aspects of data are regar ded in the literature [5]. In stream clus-tering scenarios, clusters are adapted to reflect changes in the observed data, i.e. the distribution of incoming objects chang es [2]. A special case of stream cluster-ing is for moving objects [10], focusing on spatial attributes. Stream clustering in general, however, gives no information about the actual cluster evolution over time [5]. For this, cluster tracing algorit hms were introduced [8,13,14]; they rely on mapping clusters of consecutive time st eps. These tracing methods map clus-ters if the corresponding object sets are similar, i.e. they are based on shared objects. We, in contrast, map clusters o nly if their corresponding object values are similar, independently of shared objects. That is, we trace similar types of behavior, which is a fundamentally different concept.

Clustering of trajectories [7,15] can be seen as an even more limited variant of cluster tracing with similar object sets , as trajectory clusters have constant object sets that do not change over time.

The work in [1] analyzes multidimensional temporal data based on dense regions that can be interpreted as clusters. The approach is designed to detect substantial changes of dense regions; however, tracing of evolving clusters that slightly change their position or subspace is not possible, especially when several time steps are observed.

A further limitation of existing cluster tracing algorithms is that they can only cope with fullspace clusters. Fullspa ce clustering models use all dimensions in the data space [6]. For finding clusters hidden in individual dimensions, sub-space clustering was introduced [4]. An ov erview of different subspace clustering approaches can be found in [9], and the differences between subspace clustering approaches are evaluated in [11]. Until now, subspace clusters were only ap-plied in streaming scenarios [3], but neve r in cluster tracing scenarios; deciding whether subspace clusters of varying dimensionalities are similar is a challenging issue. Our algorithm is designed for this purpose. Our main objective is to trace behavior ty pes and their developments over time. First, some basic notations: For each time step t  X  X  1 ,...,T } of our temporal data we have a D -dim. database DB t  X  R D .Weassumethedatatobenormal-ized between [0 , 1]. A subspace cluster C t,i =( O t,i ,S t,i )attimestep t is a set of objects O t,i  X  DB t along with a set of relevant dimensions S t,i  X  X  1 ,...,D } . The objects are similar within these relevant dimensions. The set of all subspace clusters { C t, 1 ,...,C t,k } at time step t is denoted subspace clustering Clus t ,and each included subspace cluster represents a behavior type (e.g. a person group). 3.1 Tracing of Behavior Types In this section, we determine whether a typical behavior in time step t continues in t + 1. Customer behavior, for example, can be imitated by another population in the next time step. Other kinds of temporal developments are the disappear-ance of a behavior or a split-up into different behaviors. We have to identify these temporal developments for effect ive behavior tracing. Formally, we need a mapping function that maps each cluster at a given time step to a set of its successors in the next time step; we denote these successors as temporal contin-uations. Two clusters C t,i and C t +1 ,j are mapped if they are identified as similar behaviors. We use a cluster distance function, introduced in Sec. 3.2, to measure these similarities. If the distance is small enough, the mapping is performed. Definition 1. Mapping function. Given a distance function dist for two clus-ters, the mapping function M t : Clus t  X  X  ( Clus t +1 ) that maps a cluster to its temporal continuations is defined by A cluster can be mapped to zero, one, or several clusters (1:n), and several map-pings to the same cluster are possible (m:1), enabling detection of disappearance or convergence of behaviors. We describ e pairs of mapped clusters by a binary
Each tuple corresponds to one cluster mapping, i.e. for a behavior type in t we have a similar one in the next time step t + 1. These mappings and the clusters can be represented by a mapping graph . Reconsider that it is possible to map a behavior to several behaviors in the next time step (cf. Fig. 1, t +1  X  t +2). These mappings, however, are not equally important. We represent this by edge weights within the mapping graph; the weights indicate the strength of the temporal continuation. We measure similarity based on distances, and thus small weights denote a strong continuation and high weights reflect a weaker one. Formally, Definition 2. Mapping graph. A mapping graph G =( V, E, w ) is a directed and weighted graph with the following properties:  X  Nodes represent clusters, i.e. V = T i =1 Clus t  X  Edges represent cluster mappings, i.e. E = T  X  1 i =1 R t  X  Edge weights indicate the strength of the temporal continuations, i.e. Figure 2 illustrates an exemplary mapping graph with edge weights. A mapping graph allows us to categorize temporal developments: Definition 3. Kinds of temporal developments. Given a mapping graph G =( V, E, w ) , the behaviors represented by clusters C  X  V can be categorized:  X  a behavior disappears ,if outdegree ( C )=0  X  a behavior emerges ,if indegree ( C )=0  X  a behavior diverges ,if outdegree ( C ) &gt; 1  X  different behaviors converge into a single behavior, if indegree ( C ) &gt; 1 These categories show whether a behavior appears in similar ways in the sub-sequent time step. Since the characteristics of a behavior can naturally change over time, we also trace single behavior s over several time steps, denoted as an evolving cluster and described by a path in the mapping graph.

Evolving clusters are correctly identifie d if specific evolution criteria are ac-counted in our distance function. These are presented in the following section. 3.2 Cluster Distance Measure Our objective is to identify similar behaviors. Technically, a distance measure is needed that determines the similarity of two given clusters. Keep in mind that measuring the similarity simply based on the fraction of shared objects cannot satisfy our objective, since even totally different populations can show a similar behavior in consecutive time steps.

We have to distinguish two kinds of evolution: A cluster can gain or lose characteristics, i.e. the relevant dimensions of a subspace cluster can evolve, and within the relevant dimensions the values can change. Our distance function has to reflect both aspects for effective simila rity measurement of evolving clusters.
Similarity based on subspaces. Each cluster represents a behavior type, and because we are considering subspace clusters, the characteristics of a be-havior are restricted to a subset of the dimensions. If a behavior remains stable over time, its subspace remains also unc hanged. The relevant dimensions of the underlying clusters are identical. Consider the clusters C t,i =( O t,i ,S t,i )and C t +1 ,j =( O t +1 ,j ,S t +1 ,j )oftimesteps t and t + 1: the represented behaviors are very similar if the dimensions S t,i are also included in S t +1 ,j .
However, a behavior can lose some of its c haracteristics. In Fig. 1, for example, the dimension d 1 is no longer relevant in time step t +2 for the behavior depicted on the bottom. Accordingly, a distance measure is reasonable if behavior types are considered to be similar even if they lose some relevant dimensions. That is, This formula alone, however, would prevent an information gain: If a cluster C t,i evolves to C t +1 ,j by spanning more relevant dimensions, this would not be assessed positively. We would get the same distance for a cluster with the same shared dimensions as C t,i and without additional relevant dimensions as C t +1 ,j . Since more dimensions mean more information, we do consider this. Usually it is more important for tracing that we retain relevant dimensions. Few shared dimensions and many new ones normally do not indicate similar be-havior. Thus, we need a trade-off between retained dimensions and new (gained) dimensions. This is achieved by a linear combination of the two introduced terms: Definition 4. Distance w.r.t. subspaces. The similarity w.r.t. to subspaces with the trade-off factor  X   X  [0 , 1] .
 By choosing  X  1  X   X  we achieve that the similarity between two behaviors is primarily rated based on their shared dimensions.

Similarity based on statistical characteristics. Besides the subspace similarity, the actual values within these dimensions are important. E.g., al-though two clusters share a dimension like  X  X ncome X , they can differ in their values extremely (high vs. low income); these behaviors should not be mapped. A small change in the values, however, is possible for evolving behaviors. For a spatial dimension, this change would correspond to a slight cluster movement.
Given a cluster C =( O, S ), we denote the set of values in dimension d with v ( C, d )= { o [ d ] | o  X  O } . The similarity between two clusters C t,i and C t +1 ,j is thus achieved by analyzing the corresponding sets v ( C t,i ,d )and v ( C t +1 ,j ,d ). By deducing two normal distribution X d and Y d with means  X  x ,  X  y and variances  X  x ,  X  y from the two sets, the similarity can be measured by the information theoretic Kullback-Leibler divergence ( KL ). Informally, we calculate the expected number of bits required to encode a new distribution of values at time step t +1 ( Y d ) given the original distribution of the values at time step t ( X d ). Formally,
By using the KL , we do not just account for the absolute deviation of the means, but we have also the advantage of including the variances. A behavior with a high variance in a single dimension allows a higher evolution of the means for successive similar behaviors. A smal l variance of the values, however, only permits a smaller deviation of the means.
We use the KL for the similarity per dimension, and the overall similarity is attained by cumulating over several dimensions. Apparently, we just have to use dimensions that are in the intersection of both clusters. The remaining dimen-sions are non-relevant for at least one c luster and hence are already penalized by our subspace distance function. Our first approach for computing the similarity based on statistical characteristics is with I = S t,i  X  S t +1 ,j for averaging.

In a perfect scenario this distance is agoodwaytotracebehaviors.Inprac-tice, however, we face the following problem: Consider Fig. 3 (note the 7-dim. space) with the cluster C 1 , 2 at time step t and the cluster C 2 , 2 with the same relevant dimensions in t +1. However, C 2 , 2 is shifted in dimensions d 1 and d 2 ;the distance function proposed above (Eq. 1) would determine a very high value and hence the behaviors would not be mapped. A large part { d 3 , ..., d 7 } of the shared relevant dimensions { d 1 , ..., d 7 } , however, show nearly the same characteristics in both clusters. The core of the behaviors is completely identical, and thus a mapping is reasonable; as illustrated by the mapping in the right part of Fig. 3. Consider another example: the core of the customer behaviors of North Ameri-cans and Europeans is identical; however, North Americans and Europeans have further typical characteristics like their favorite sport (baseba ll vs. soccer). These additional, non-core , dimensions provide us with further informations about the single clusters at their current time step. They are mainly induced by the individ-ual populations. For the continuation of the behavior, however, these dimensions are not important. Note that non-core dimensions are a different concept than non-relevant ones; non-core dimensions are shared relevant ones with differing values. In other words, there are two different kinds of relevant dimensions: one for subspace clusters and one for t racing of subspace clusters.

An effective distance function between clusters has to identify the core of the behaviors and incorporate it into the distance. We achieve this by using a sub-set Core  X  S t,i  X  S t +1 ,j for comparing the values in Eq. 1 instead of the whole intersection. Unfortunately, this subset is not known in advance, and it is not reasonable to exclude dimensions from th e distance calculation by a fixed thresh-old if the corresponding dissimilarity is too large. Thus, we develop a variant to automatically determine the core. We choose the  X  X est X  core among all possible cores for the given two clusters. That is , for each possible core we determine the distance w.r.t. their value distributions, and we additionally penalize dimensions not included in the core. The core with the smallest overall distance is selected, i.e. we trade off the size of the core against the value V ( C t,i ,C t +1 ,j ,Core ): Definition 5. The core-based distance function w.r.t. values for two with the penalty factor  X   X  [0 , 1] for dimensions NonCore =( S t,i  X  S t +1 ,j ) \ Core . By selecting a smaller core, the first part of the distance formula enlarges. The second part, however, gains the possibility of determining a smaller value. The core must comprise at least one dimension; otherwise, we could map two clusters even if they have no dimensions with similar characteristics.

Overall distance function. To correctly identify the evolving clusters in our temporal data we have to consider evolutions in the relevant dimensions as well as in the value distributions. Thus, we have to use both distance measures simultaneously. Again, we require that two potentially mapped clusters share at least one dimension; otherwise, these clusters cannot represent similar behaviors. Definition 6. The Overall distance function for clusters C t,i =( O t,i ,S t,i ) with  X   X  [0 , 1] . In the case of | S t,i  X  S t +1 ,j | =0 , the distance is set to  X  . 3.3 Clustering for Improved Tracing Quality Until now, we assume a given clustering per time step such that we can de-termine the distances and the mapping graph. In general, our tracing model is independent of the used clustering method. However, since there are temporal relations between consecutive time step s, we develop a clustering method whose accuracy is improved by thes e relations and that avoids totally different clus-terings in consecutive time steps. A dir ect consequence is an improved tracing effectiveness. We adapt the effective cell-based clustering paradigm [12,16,11], where clusters are approximat ed by hypercubes with at least minSup many objects. The extent of a hypercube is restricted to w in its relevant dimensions. Definition 7. Hypercube and valid subspace cluster. A hypercube H S with the relevant dimensions S is defined by lower and upper bounds with up i  X  low i  X  w  X  i  X  S and low i =  X  X  X  ,up i =  X  X  X  i  X  S . The mean of H S is called m H S . The hypercube H S represents all objects Obj( H S )  X  DB with o  X  Obj ( H S )  X  X  X  d  X  X  1 ,...,D } : low d  X  o [ d ]  X  up d . A subspace cluster C =( O, S ) is valid  X  X  X  H S with Obj ( H S )= O and | Obj ( H S ) | X  minSup . We now introduce how temporal relations between time steps can be exploited.
Predecessor information. We assume an initial clustering at time step t = 1. (We discuss this later.) Caused by the temporal aspect of the data, clusters at a time step t occur with high probability in t + 1  X  not identical, but similar. Given a cluster and the corresponding hypercube H S at time step t , we try to find a cluster at the next time step in a similar region .Weusea Monte Carlo approach, i.e. we draw a random point m t +1  X  R D that represents the initiator of a new hypercube and that is nearby the mean m H S of H S .After inducing an hypercube by an initiator, the corresponding cluster X  X  validity is checked. The quantity of initiators is calculated by a formula introduced in [16]. Definition 8. Initiator of a hypercube. Apoint p  X  R D , called initiator, together with a width w and a subspace S induces a hypercube H w S ( p ) defined by  X  d  X  S : low d = p [ d ]  X  w 2 ,up d = p [ d ]+ w 2 and  X  i  X  S : low i =  X  X  X  ,up i =  X  . Formally, the initiator m t +1 is drawn from the region H 2 w S ( m H S ), permitting a change of the cluster. The new hypercube is then H w S ( m t +1 ). With this method we detect changes in the values; however, also the relevant dimensions can change: The initiator m t +1 can induce different hypercubes for different rele-vant dimensions S . Accordingly, beside the initi ator, we have to determine the relevant subspace of the new cluster. The next section discusses both issues.
Determining the best cluster. A first approach is to use a quality function [12,16]:  X  ( H S )= Obj ( H S )  X  k | S | . The more objects or the more relevant dimen-sions are covered by the cluster, the higher is its quality. These objectives are contrary: a trade-off is re alized with the parameter k .Intimestep t +1 we could choose the subspace S that maximizes  X  ( H w S ( m t +1 )).

This method, however, optimizes the quality of each single cluster; it is not intended to find good tracings. Possibly, the distance between each cluster from the previous clustering Clus t and our new cluster is large, and we would find no similar behaviors. Our solution is to directly integrate the distance function dist into the quality function. Consequently, we choose the subspace S such that the hypercube H w S ( m t +1 ) maximizes our novel distance based quality function. Definition 9. Distance based quality function. Given the hypercube H S in subspace S and a clustering Clus t , the distance based quality function is where C S indicates the induced subspace cluster of the hypercube H S . We enhance the quality of the clustering by selecting a set of possible initiators M from the specified region; this is also important as the direction of a cluster change is not known in advance. From the resulting set of potential clusters, we select the one that has the highest quality.

Overall we realize that for each cluster C  X  Clus t a potential temporal con-tinuation is identified in time step t + 1. Nonetheless it is also possible that no valid hypercube is identified for a single cluster C  X  Clus t . This indicates that a behavior type has disappeared in the current time step.

Uncovered objects and the initial clustering. When behavior emerges or disappears, there will be some objects of the current time step that are not part of any identified cluster: if we denote the set of clusters generated so far by and therefore clusters. Especially for the initial clustering at time step t =1 we have no predecessor information and hence Clus 1 =  X  .Todiscoverasmany patterns as possible, we have to check if the objects within Remain t +1 induce novel clusters. We draw a set of initiators M  X  Remain t +1 ,whereeach m  X  M induces a set of hypercubes H w S ( m ) in different subspaces. Finally, we choose the hypercube that maximizes our qualit y function. If this hypercube is a valid cluster, we add it to Clus t +1 , and thus Remain t +1 is reduced. This procedure is repeated until no valid cluster is identified or the set Remain t +1 is empty. Note that our method has the advantage of generating overlapping clusters. Setup. We use real world and synthetic data for evaluation. Real world data are scientific grid data reflecting oceanogr aphic characteristi cs as temperature and salinity of the oceans 1 . It contains 20 time steps, 8 dimensions, and 71,430 objects. The synthetic data cover 24 tim e steps and 20 dimensions. In average, each time step contains 10 clusters with 5-15 relevant dimensions. We hide de-velopments (emerge, conver ge, diverge, or disappear) and evolutions (subspace and value changes) within the data. In our experiments we concentrate on the quality of our approach. For synthetic data, the correct mappings between the clusters are given. Based on the detect ed mappings we calculate the precision and recall values: we check whether all but only the true mappings between clusters are detected. For tracing qua lity we use the F1 value corresponding to the harmonic mean of recall and precision. Our approach tackles the problem of tracing clusters with varying subspaces and is based on object-value-similarity. Even if we would constrain our approach to handle only full-space clusters as existing solutions, such a comparison i s only possible when we artificially add object ids to the data (to be used by these s olutions). Tracing clusters based on these artificial object ids, however, ca nnot reflect the ground truth in the data. Summarized, comparisons to other approaches are not performed since it would be unfair. We use Opteron 2.3GHz CPUs and Java6 64bit.

Tracing quality. First, we analyze how the par ameters affect the tracing effectiveness. For lack of space, we only present a selection of the experiments. For  X  , a default value of 0 . 1 was empirically determined.  X  isevaluatedinFig.4 for three different  X  values using synthetic data. By  X  we determine the trade-off between subspace similarity and value similarity in our overall distance function. Obviously we want to prevent extreme cas es for effective tracing, i.e. subspace similarity with no attribute similarity at all (  X   X  0), or vice versa. This is confirmed by the figure, as the tracing quality highly degrades, when  X  reaches 0or1forall  X  values. As  X  =0 . 3 enables a good tracing quality for all three  X  , we use this as default. Note that with the threshold  X  we can directly influence how many cluster mappings are created.  X  =0 . 1 is a good trade-off and is used as default. With a bigger  X  the tracing quality worsens: too many mappings are created and we cannot distinguish between meaningful or meaningless mappings. Thesameistruefor  X   X  0: no clusters are mapped and thus the clustering quality reaches zero; th us we excluded plots for  X   X  0.

The core dimension concept is evaluated in Fig. 5. We analyze the influence on the tracing quality (left axis) with a varying  X  on the x-axis; i.e., we change the penalty for non-core dimensions. Note, non-core dimensions are a different con-cept than non-relevant ones; non-core dimensions are shared relevant dimensions with differing values. The higher the penalty, the more dimensions are included in the dimension core; i.e., more shared dimensions are used for the value-based similarity. In a second curve, we show th e absolute number of non-core dimen-sions (right axis) for the different penalties: the number decreases with higher penalties. In this experiment the exact number of non-core dimensions in the synthetic data is 10. We can draw the following conclusions regarding tracing quality: A forced usage of a full core (  X   X  1) is a bad choice, as there can be some shared dimensions with different values. By lowering the penalty we allow some dimensions to be excluded from the core and thus we can increase the tracing quality. With  X  =0 . 1 the highest tracing quality is obtained; this is plausible as the number of non-core dimensions corresponds to the number that is existent in the data. A too low penalty , however, results in excluding nearly all dimensions from the core (many non-core dimensions,  X   X  0) and dropping quality. In the experiments, we use  X  =0 . 1asdefault.

Detection of behavior developments. Next we analyze whether our model is able to detect the different behavior developments. Up to now, we used our enhanced clustering method that utili zes the predecessor information and the distance based quality function. Now, we additionally compare this method with a variant that performs clustering of each step independently. In Fig. 6 we use the oceanographic dataset and we determine for each time step the number of disappeared behaviors for each clusterin g method. The experiment indicates that the number of unmapped clusters for the approach without any predecessor or distance information is larger than for our enhanced approach. By transferring the clustering information between the t ime steps, a larger number of clusters from one time step to the next can be mapped. We map clusters over a longer time period, yielding a more effective tracing of evolving clusters.

The aim of tracing is not just to map similar clusters but also to identify different kinds of evolution and development. In Fig. 7 we plot the number of clusters that gain or lose dimensions and the four kinds of development cumu-lated over all time steps. Beside the numbers our approach detects, we show the intended number based on this synthetic data. The first four bars indicate that our approach is able to handle dimension gains or losses; i.e., we enable sub-space cluster tracing, which is not cons idered by other models. The remaining bars show that also the developments can be accurately detected. Overall, the intended transitions are found by our tracing. In Fig. 8 we perform a similar experiment on real world data. We report only the detected number of patterns because exact values are not given. On th e left we cumulate over all time steps. Again, our approach traces clusters with varying dimensions. Accordingly, on real world data it is a relevant scenario that subspace clusters lose some of their characteristics, and it is mandatory to use a tracing model that handle these cases. The developments are also iden tified in this real world data. To show that the effectiveness is not restricted to single time steps, we analyze the de-tected patterns for each time step indivi dually on the right. Based on the almost constant slopes of all curves, we can see that our approach performs effectively. In this paper, we proposed a model for tracing evolving subspace clusters in high dimensional temporal data. In contrast to existing methods, we trace clusters based on their behavior; that is, clusters are not mapped based on the fraction of objects they have in common, but on the similarity of their corresponding object values. We enable effective tracing by introducing a novel distance measure that determines the similarity between cluste rs; this measure comprises subspace and value similarity, reflecting how much a cluster has evolved. In the experimental evaluation we showed that high quality tracings are generated.
 Acknowledgments. This work has been supported by the UMIC Research Centre, RWTH Aachen University.

