 We described a scenario of tag us e and an empirical study of tags as socio-cognitive artifacts providing information scent. We articulated a three-step use scenario of tags, and used it to conceptualize tag "quality X  as determined by use. We designed and conducted a user study to explor e what attributes of tags and taggers predict the user-rated "quality" of tags. We found that frequency best predicted tag quality, while information entropy provided further refinement. We found that people rated our identified quality tags higher in quality than general tags. But these identified quality tags were not perceived better than self-generated tags. We derived a re gression model for tag quality and discussed implications for social computing. H5.3 [ Information interfaces and presentation ]: Group and Organization Interfaces-collaborative computing . Keywords : Social bookmarking, sense-making, quality tags. Social tagging systems such as de l.icio.us and Flickr are popular among web users and have recently attracted attention from researchers as a focus of study in Human Computer Interaction and Computer Supported Cooperative Work (e.g., [1-4]). Tagging systems allow users to generate la beled hyperlinks (i.e., tags) to web content for the purposes of fu rther retrieval. These tags are typically keywords or short phrases assigned to any piece of information (e.g., website, photo, vi deo, document, etc). In this sense, tags serve as user-generated metadata, allowing web content to be browsed and searched later. However, compared with traditional metadata that is typically generated by experts, tags are assigned freely by a large number of end users. In fact, not all tags are of high quality because most users are not experts [4]. Further, end users have diffe rent tag vocabularies [3, 5] based on their own mental models a nd assumptions. Herein lies the problem: with the abundance of ta gs generated by a diverse and large number of users, how can quality tags be identified, and if they can be identified, how can these tags benefit the users? Our paper addresses the above-s tated problems. Viewing tagging system as socio-cognitive artifacts designed to aid user X  X  information foraging [6, 7] and sense-making [8] tasks, we articulate a scenario of tag use as a design representation [9]. The empirical part of our paper para llels the scenario, and explores how quality tags can be identified. As social tagging systems scale up, the vocabulary of tags increasingly stabilize to reach statistical regularity and form tag patterns [10]. There have been many studies on probability distribution of tags [11], their growth patterns [10], and attributes such as tag frequency and entropy [1], tag correlation [12], tag similarity [3], and tag non-obviousness [2]. These patterns and attributes were drawn from large-scale, macro-level statistics, which reflect collective thinking and other social trends. However, these studies of aggreg ate patterns and statistics provide little guidance on helping individual users better navigate social tagging systems and facilitate their information foraging behaviors. Few studies have discussed how these attributes at the macro level can be used at the micro level to help, for example, discriminate tags. In fact, there is no clear definition of  X  X uality X  for a tag. The statement by MacGregor and McCulloch X  X  X erms assigned to resources that are exhaustive w ill result in high recall at the expense of precision..., terms that are too specific will result in high precision, but lower recall X  [13] X  X mplies at least two aspects of quality: precision and reca ll. The characteristics of tags such as exhaustiveness and specificity may be implied or predicted by those statistical attr ibutes. For example, tags applied to too many items may be too gene ral; tags applied to fewer items may be more discriminate; and ta gs applied for too few times may be too obscure [2, 5, 14]. Although these studies imply the quality of tags, few studies discuss it explicitly. One exception is the paper by Sen and colleagues [4], which allowed users to rate the quality of tags for movies in MovieLens. Their resu lts suggested a trend that more frequently applied or searched ta gs were usually rated better. However, we have not yet seen any conclusive study that examines which and how tag attributes can predict tag quality with statistical significance. Quality tags can be recommended to users. Studies have found that as more tags exist, it is less likely that the next assigned tag is new [5]. Therefore, recommending tags can facilitate tag reuse, which is one goal of social tagging systems for achieving a converging vocabulary [2]. Recommending good tags may also improve the quality of the tagging vocabulary. Fu [15] suggested that the quality of tags influences the formation of user X  X  mental categories as well as information search efficiency. Furnas et al. [16] claimed that different users may use different te rms to describe the same thing. Supporting this  X  X ocabulary problem  X  [16, 17] is where social tagging systems outweigh c onventional taxonomies [18]. Recommending tags can  X  X nduce conforming behaviors X  by adapting users to better shared vocabulary [5]. Sen et al. [4] found that recommending tags changes th e user X  X  selection of tagging vocabulary and thus changes the distribution of tag classes. Since Vannevar Bush X  X  vision of Memex [19], researchers have been studying the use of information systems to facilitate information foraging and sense-ma king. In information foraging theory, user strategies and tec hnologies for information seeking, gathering, and consumption are ad apted to the flux of information in the environment according to their costs and benefits [6]. Just as animals rely on scents to forage, users rely on information scent provided by various cues in judging information sources and navigating through information spaces. Tags serve as proximal cues that provide information scent. They can be considered as an external representation of users X  mental concepts activated by web items. In later information fo raging tasks, by re-activating these concepts, they provide  X  X  he imperfect perception of the value, cost, or access path of information source X  [7]. Figure 1 represents a typical scenario of tag use for information foraging and sense-making. We use the example of web documents as typical web items th at can be tagged. First, when the user is viewing certain web content, mental concepts are activated. The user may instantiate salient concepts as tags. After these tags are generated, the user and others can use them for later information retrieval, sense-ma king, and information foraging. A visual web item uses symbo lic representations to convey information. Web documents cons ist of semantic terms, which can activate users X  cognitive processing of corresponding mental concepts. These concepts as internal representations can be represented with nodes in a knowledge network, with properties of the concepts represented as labeled relational links from the nodes to other concept nodes [20]. Reading and learning a new web document can be understood as changing the relationship between certain conceptual nodes, that is, the activations spread by tracing an expanding set of links in the network of these concepts. Such underlying cognitiv e mechanisms are used to explain how social tagging sy stem influences knowledge acquisition and adaptation [15]. Information acquisition behavior gives increment to or reconstructs human knowledge. Ther efore the information forager may have different (or different level of) activation of mental concepts on the same web content in different times. For example, when he or she gets access to the same document after a period of time, the forager may have d eeper understanding on the same problem; it is equally possible the forager has forgotten something where certain  X  X ognitive artifact X  [21] (e.g. tags) may be of help. Similarly, different informati on foragers may have different knowledge backgrounds and thus different activations on the same web content. Furthermore, if others X  understandings (as instantiated by tags) are provided in combination with the web content, the user X  X  mental activation may be influenced, where we call tags as  X  X ocio-cognitive artifact X . Tags, as cognitive and socio-cogn itive artifacts used to augment users X  cognitive capabilities in information foraging and sense-making tasks, are usually inten tionally generated by users for further use. The question is wh ich semantic terms would be assigned as tags. Theoretically, all semantic terms having certain activation can be assigned as tags for information scent. However, not all semantic terms or tags are of equal quality, given their varying levels of activation. It is reasonable that users will identify information of interests, and appropriately instantiate salient representations as tags for further reuse. Therefore, users X  interest, salient activation, and expected reuse are critical to determine such prospectiv e generation of tags. It is notable that this instantiation process is problematic due to bounded rationality and choices ba sed on satisficing [22]. When users are making decisions in assigning tags to a certain document, they may not always come up with the highest quality tags, given their limited knowledge, skill, and time. They may just choose  X  X ood enough X  options (see also  X  X aturalistic decision making X  in [23]). Weick [24] pointed out that in sense-making processes, people have inability to shift representati ons easily due to the inertia of their representations, and inability to find and use appropriate data. These inabilities can be reflected by individual user's preferences and bias of selecting tags, which can be complemented by social tags in a community level. Therefore, identifying and recommending others X  quality tags can alleviate such problems. Tags not only reflect the internal mental representations but also provide external information scent to reconstruct certain representation in later information processing tasks. They are usually generated intentionally and prospectively to reduce the cost of later operations. Therefore, tag quality can be defined as how effectively and efficiently it he lps later information retrieval, information foraging, and sense-making. Such quality can be examined in the reuse phase of tags. Broadly, there can be two different kinds of usage for tags: information retrieval and exploratory search [15]. For information retrieval, tags provide information scent to relocate the document (e.g. keywords to search) and restore activations that have decayed. For exploratory search, tags can give users main topics of a document, and give information scent to locate related interested documents. These tags provide  X  X mperfect information at intermediate location" [7] to decide on paths through online databases to target information. As moderated by prior knowledge , different users may gain different information scent from the same tags. The same user may also modify his or her tags with reconstructed knowledge in information foraging tasks. Furthe rmore, the tags become a part of the semantic terms of the doc ument, which provides different activation and information scent. We seek to answer three research questions based on the scenarios of tag use presented earlier. The fi rst research question is related to the identification of quality ta gs. Tags, as part of a language, come from practical use and natural understanding [25]; the underlying knowledge is also socially constructed [26]. Thus, the tags are ontologically subjective (semantic terms with different activation levels according to the scenario). Users may have different understandings and voca bularies for the same referred items. Therefore, users may choos e tags based on their personal tendency, preferences, and beliefs [5]. Indeed, this will result in individual differences when identifying which tags are of high quality. This creates the need to objectively identify quality tags by assessing the attributes of the semantic terms in a tag set. Our first research question (RQ) can be stated as: RQ1: Which attributes can obj ectively identify quality tags? Several attributes can be used to objectively assess the quality of tags. Based on our scenario and prior literature, we have identified three such attributes to explore as part of RQ1: centrality, frequency, and entropy. Each of these objective measures is explained below with an associated hypothesis. Reading a document activates the network of several mental concepts. Users select salient internal representations as tags. These selected tags are mentally linked, forming a network that depicts the main relationships in the original network of all activated mental concepts. Therefore, we can construct a network of tags representing their semantic relationships for each paper. Some researchers have tried to visualize tag correlation networks (e.g., [27]). This provides us opport unities to describe attributes of tags based on network analysis. Ce ntrality is one such measure to assess the relative importance of a node within a network. Tags with high centrality in the tag network are usually the most salient terms. Taking eigenvector centrality, for example, the centrality score of a node is proportional to the sum of the scores of all nodes that are connect ed to it. Therefore, a tag with high eigenvector centrality implies that, first, this tag is salient because it is related with many other tags; and second, related tags are also salient enough. Theref ore, tags with higher centrality may have higher activation, which leads us to articulate our first hypothesis: RQ1 (H1): Tags with higher centrality have higher quality. If a tag is frequently assigned to a document, this tag has salient activation for many users, which suggests that it is highly possible Recommender systems also recommend options with high usage to users [28]. This leads us to articulate our second hypothesis: RQ1 (H2): Tags with higher frequency have higher quality. Different documents may activate the same mental concepts, especially when these documents are from the same area. Consequently, same tags may be assigned to different documents. Given a tag X assigned to a set of documents, each of which occurred with probability p(x i ), the entropy H(X) is defined as: Here x i represents the event that tag X is assigned to certain document d i . Entropy measures uncerta inty about a particular event associated with a probability distribution. When a tag is assigned to only a few particular documents, that is, when the information entropy is low, th e tag has a high discriminating value for foraging these docum ents. Chi and Mytkowicz found social tagging systems become ha rder and harder to navigate because of the increasing entropy [14]. If a tag is dispersed in more documents, there is a higher uncertainty to determine which document is associated with that tag; therefore, this tag has higher entropy. Tags with high entropy may be  X  X oo general X  [5] without  X  X recision X  [13] and  X  X iscrimination value X  [2]. On the other hand, tags with low entropy may be too  X  X pecific X , too  X  X ard to recall X  X 13], and may be too  X  X bscur e X  [5]. From this perspective, both high and low entropy of tags imply lower information scent and thus lower quality. This leads us to articulate our third hypothesis: RQ1 (H3): Tags with medium entropy have higher quality.
 In addition to attributes of tags, attributes of users can also be used to identify quality tags. Studies found that experts usually gave more accurate descriptions of a task than novices [29]. Expertise recommendation systems he lp locate experts to provide recommendations [30]. Therefore, users with higher related expertise may also provide better ta gs, which are also descriptions at a meta-level. Our second research question is: RQ2: Can experts identify quality tags better than average users? Based on RQ1 and RQ2, we may identify quality tags with the attributes of tags and taggers. Ho wever, users may have different preferences of tags [5] due to their different understandings and vocabularies for the subjective semantic items. The question then becomes: is there value in identifying and recommending others X  quality tags? As stated earlier, users are typically not completely rational. In order to determine values of recommending tags, we need to empirically validate how people perceive the quality of their own and others X  tags, and most importantly, how our identified quality tags will be perceived. RQ3: How do people perceive our identified quality tags? In this paper, we selected the tagging of scholarly papers as the domain context for social bookmarking. Scholarly papers activate abundant meaningful mental concepts, which is a direct application of the spread activation theory in our scenario. To address the research questions, we conducted an online survey-based user study with two phases aligning with our scenario of use. In the first phase, participants were asked to read four scholarly papers (correspondi ng to the activation process in the scenario). Then they assigned tags to each of those papers (corresponding to the instantiation process in the scenario). In the second phase, participants were asked to evaluate how well the tags described each paper (corre sponding to the reuse process in the scenario). As researchers usually read and us e scholarly papers in their own areas, we chose HCI as our area of focus because of our familiarity with the literature. Th ree of the four papers ([31-33]) were selected from proceedings of the ACM Conference on Computer Supported Cooperativ e Work (ACM CSCW); the fourth paper [34] was selected from proceedings of the SIGCHI conference on Human factors in computing systems (ACM CHI). They were all popular papers that had been cited for more than two hundreds times. According to Google Scholar, the Dourish and Bellotti paper [31] had been cited 1345 times; the Gutwin et al. paper [32] had been cited 209 times; the Grudin paper [33] had been cited 618 times; and the Rodden and Wood paper [34] had been cited 221 times. These four papers were chosen based on the variance of their similarity. We chose  X  X SCW X  as one discriminator of similar papers, and  X  X roup awareness X  as a further discriminator. Accordingly, sixteen popular papers (cited more than 100 times) from the pool of all ACM CHI and ACM CSCW proceedings were selected. A panel of five HCI experts discussed their similarities and finally narrowed the set down to these four papers. According to our discu ssion, Dourish and Bellotti paper and Gutwin et al. paper were the most similar because they both discussed awareness in CSCW ; the Grudin paper had some similarity because it dealt with CSCW issues; the Rodden et al. paper was quite different but in the HCI area. The user study consisted of two online surveys corresponding to the two phases. Both surveys were conducted in Surveymonkey (www.surveymonkey.com ). Counterbalancing was done to ensure that presentation order of the papers did not affect user behavior in both phases. All participants entered a lottery to win a $100 gift card; two winners were randomly selected. As our tagging objects were scholarly papers in HCI, we selected HCI professionals as our participants. We provided participants titles, abstracts, and hyperlinks to full papers, asking them to assign between five to seven tags to each. We confined the number of tags to ensure we get a consistent and sufficient amount of data while avoiding burdening the participants by asking them to provide more than several tags. In addition, according to our experience in social tagging systems, users generally assign about five to seven tags to a particular document. We recruited participants by sending a recruitment email to the ACM CHI and AIS HCI mailing list. 90 participants completed our survey, providing 1,790 tags for all papers. 525 tags were distinct tags. Of these 90 par ticipants, 68 of them were from academia (with 24 graduate students, 12 research associates or post doc, and 32 professors), 17 of them were from industry or government; there were 5 missing values. 44 participants were male and 40 were female, with 6 missing values. The average time participants had been involved in the HCI area was 12 years with a standard deviation of 8.9 years. Two researchers independently performed data cleaning and consolidation. As the tags were ra ther subjective, we consolidated tags conservatively under the following five conditions: 1) abbreviation or acronyms (e.g ., HCI vs. human computer interactions); 2) capitalized letters (e.g., user interface vs. User Interface); 3) misspelling (e.g., collaboration vs. colaboration); 4) words with hyphens or dashes (e.g., human-computer interaction vs. human computer interaction); 5) plurality (e.g., widget vs. widgets). The data cleaning a nd consolidation was discussed between the two researchers to r each full agreement. After this process, 119 distinct tags for th e Dourish and Bellotti paper were consolidated to 107 tags; 119 distin ct tags for the Gutwin et al. paper were consolidated to 105 tags; 179 distinct tags for the Grudin paper were consolidated to 161 tags; and 197 distinct tags for the Rodden et al. paper were consolidated to 171 tags. To measure how well these tags provide information scents, we asked participants to evaluate these tags in the second phase. Specifically, we asked participan ts to evaluate how well the assigned tags described the papers. We sent a survey via email to 77 of the 90 participants from the first phase who agreed to take part in the second phase of the study by providing valid email addresses. The second phase survey was sent out one month after completion of the first phase. We thought one month would be long enough for certain decay of their memory, and short enough for certain re-activation. 46 participants completed the survey. 40 of them provided valid email addr esses so that we could match their data from the first phase. As the consolidated tags from the first phase followed a power law distribution, we categorized these tags according to their frequencies into high, medium, and low groups. Tags with the highest quartile of frequencies (i.e., larger than 8) were categorized as high frequency; tags with the second quartile of frequencies (i.e., less than 8 and la rger than 3) were categorized as medium frequency; tags with the third and fourth quartiles of frequencies (i.e., frequency of 1, 2, and 3) were categorized as low frequency. For each paper, from each of the three frequency groups, we randomly selected 9 tags to be ev aluated to be consistent across the four papers. In this way, we had 27 tags for each paper. We asked participants to rate the qua lity of these 108 tags on a 7-point Likert scale from  X  X ot good at all X  to  X  X ery good X . This rating scale was based on Sen et al. X  X  study [4]. In our scenario, we defined quality tags as those providing the most information scent for later information foraging task s. Thus, asking participants to evaluate the tags on a scale was a practical and feasible approach to assess perceived quality of tags. In this section, we first pres ent manipulation checks to confirm the similarity variance that was taken into account while selecting the four papers. We then address our research questions and hypotheses. Figure 2 illustrates a network analysis of tags. Due to space limitations, we selected tags ente red in the second phase of the are from the Dourish and Bellotti paper; the top right cluster represents tags from the Gutwin et al. paper; tags on the bottom left are from the Grudin paper; tags from the bottom right are from the Rodden et al. paper; tags in the center are the ones shared by more than one paper. As a manipulation check, we can see that the links between the first three ACM CSCW papers are denser than with the ACM CHI paper. As another manipulation check, we also explored the cosine similarity based on the assigned tags. Two papers A and B have n times in paper B. Given vectors A={a 1 , a 2 , ..., a could be measured by the Tanimoto coefficient T(A, B), which is defined as: Table 1 represents the cosine similarity between each of the four papers. The Dourish and Bellotti paper and Gutwin et al. paper has the highest similarity; the R odden et al. paper has much lower similarities with the three ACM CSCW papers. This result is consistent with our expectation. In the network of tags for each paper, we calculated the eigenvector centrality of each ta g as the measurement of their relevant importance in the c oncept network. We examined whether centrality identified tag quality (as rated by the participants) in the regression m odel. According to the result (see Figure 3), the centrality can identify the quality of tags. According to the Box-Cox method, the cubic regression model fits best. The equation is: Quality =29.62 X  centrality 3  X 27.23 X  centrality 2 +9.39 X  centrality +3.53 This regression model is significan t, p&lt;0.001. Therefore, H1 is supported. Tags having higher centrality within the paper have higher quality. Furthermore, we found that the quality of tags is a cubic function of centrality. The regression model explains 23% of the total variation. This result confirms our hypothesi s. Furthermore, as shown in Figure 3, the cubic model for centrality suggests that only tags with extreme centrality can be distinguished. Only those quality tags with high centrality can be id entified. We can see even when the centralities were between 0.4 and 0.5, the quality of some tags were still rated with scores below 4. Two examples were  X  X valuation X  and  X  X esign X  for th e Grudin paper. On the contrary, tags such as  X  X SCW X  and  X  X wa reness X  had high centrality (about 0.6), and their qualities were also rated high. This is reasonable because tags that best describe a specific paper are not necessarily salient in a whole knowledge netw ork. Therefore, only a small part of quality tags can be identified by centrality. This result suggests that centrality is not a very good predictor for identifying quality tags. We then examined whether frequency of tags within a paper identified tag quality in the regression model. According to the result, frequency can predict the quality of tags. According to the Box-Cox method, the logarithmic re gression model fits best. The equation is: Quality =3.39+0.46 X ln( frequency ) This regression model is significan t, p&lt;0.001. Therefore, H2 is supported. Tags having higher fre quency within the paper have higher quality. Furthermore, we found that the quality of tags is a logarithmic function of frequency. The regression model explains 27% of the total variation. As shown in Figure 4, because the quality of tags is a logarithmic function of frequency, tags with higher frequencies were all rated as good tags. Further, there was no tag in the bottom right quadrant, which implies that ta gs with higher frequency were never of low quality. Therefore, the frequency of tags identifies tag quality reasonably well. This result confirms our hypothesi s. Furthermore, the logarithmic model for frequency makes intuitive sense because some repetition gives us confidence that people agree the tag is appropriate for a resource, but lo ts of repetition gives only a little more confidence. We also examined whether information entropy identified tag quality in the regression model. According to the result, entropy can predict the quality of tags. A ccording to the Box-Cox method, the quadratic regression model fits best. The regression equation is: Quality=  X 10.22 X  entropy 2 +6.18 X  entropy +3.78 The regression model is significan t, p&lt;0.01. Therefore, H3 is supported. Tags with medium entropy have the highest quality. Furthermore, we found that the quality of tags is a quadratic function of entropy. The regression model explains 13% of the total variation. This result confirms our hypothesis. As suggested by the quadratic relationship in Figure 5, when the entropy is medium, the tags usually have the highest quality. When information entropy is 0.30, the mean quality reaches its highest value of 4.71. When the entropy is low and high, the mean quality is low. For example, tags such as  X  X REP X  and  X  X iniature view X  had the lowest entropies; their qualities we re rated low probably because they are too  X  X pecific X  [13] and  X  X bscure X  [5]. Tags such as  X  X CI X  and  X  X ser study X  had highest entropies; their qualities were also rated low probably because they were too general terms without much  X  X iscrimination value X  [2]. As the quality of tags is the cubic function of centrality, logarithmic function of frequenc y, and quadratic function of entropy, we put all of the (centrality) 3 , (centrality) ln(frequency), (entropy) 2 , and entropy variables into a linear regression model to examine their combination functions. We found ln(frequency), (entropy) 2 , and entropy entered into the regression model as significant with p&lt;0.001. The regression equation is: Quality =0.41 X ln( frequency )  X 7.54 X  entropy 2 +3.90 X  entropy +3.31 The regression model explains 31% of the total variation. Centrality was excluded from the model, which confirms our prior assessment that centrality is not a good predictor. Figure 6 shows their relationships more directly. We divided the frequency into low and high groups with its median. We divided entropy into three groups as it ha d a quadratic relationship. As 64.8% of tags had entropy of zero (i.e., tags only applied to one paper), we categorized these tags in the low entropy group. For the tags with entropy larger than zero (i.e., tags applied to more than one paper), we categorized them into the medium and high entropy groups with the median of their entropy. We can see that frequency is the main predictor of tag quality. The quality of tags with high frequencies (Mean=4.92, SD=0.88) were significantly higher than the quality of tags with low frequencies (Mean=3.75, SD=1. 01), t(106)=5.37, p&lt;0.001. Entropy provides a refinement for identifying quality tags. For tags with high frequencies, tags with medium entropy had mean quality of 5.18 (SD=0.86); tags with low entropy had mean quality of 4.73 (SD=0.93); tags with high entropy had the mean quality of 4.85 (SD=0.88). Howeve r, their differences did not reach statistical significance. 
Figure 6. Frequency and entropy together predict quality To determine participant X  X  expertise in each of the four papers, we provided ACM SIGCHI keywords in the survey and asked participants to check all keywords that could be applied to their expertise and professional interests. At the same time, two HCI experts decided which of these keywords could be applied to each of the four papers. We used the number of keywords checked by the participants as a percentage of the keywords identified by the two HCI experts as an index of expertise. Although only a subset of tags from the first phase was selected for the second phase, 87 of 90 participants had at least one tag evaluated. We calculated the average quality for each participant X  X  rated tags as an estimation of his or her assigned tags. We found that for each of the four papers, there was no correlation between particip ant X  X  expertise and their tag quality. The Pearson correlation coe fficients were -0.07, -0.05, 0.21, and -0.01 respectively. We also measured participant X  X  familiarity with each paper on a 5-point Likert scale. No corre lation with tag quality was found. The Pearson correlation coefficients were -0.21, -0.13, 0.02, and 0.03 respectively. Further, we examined participant X  X  tagging experience and years in the HCI area, which also reflect some of their expertise. They still had no correlation with the quality of their tags. All of the Pearson correlation coefficients were between -0.14 and 0.16. Users assigned several tags in th e first phase; in the second phase, they evaluated not only their own tags but also others X  tags. We compared the evaluated quality of self-assigned tags with the quality of tags assigned by others. For the Dourish and Bellotti paper, we calculated the mean quality of self-assigned tags (Mean=5.55, SD=1.39) for each participant, and the mean quality of tags assigned by others (Mean=4.19, SD=0.98). According to the paired-samples t test, these two had significant difference (t(39)=8.34, p&lt;0.001). The quality of self-assigned tags was significantly higher than quality of tags assigned by others. Similarly, for the Gutwin et al. paper, the quality of self-assigned tags (Mean=5.31, SD=1.38) was significantly higher than quality of tags assigned by others (M ean=4.03, SD=1.00), t(39)=7.38, p&lt;0.001. For the Grudin paper, th e quality of self-assigned tags (Mean=4.79, SD=1.63) was significantly higher than quality of tags assigned by others (Mean =3.36, SD=0.93), t(37)=6.21, p&lt;0.001. For the Rodden et al. pa per, the quality of self-assigned tags (Mean=5.18, SD=1.51) was significantly higher than quality of tags assigned by others (M ean=3.96, SD=0.82), t(35)=5.44, p&lt;0.001. Therefore, we conclude that self-assigned tags were generally perceived as higher quality than tags assigned by others. It is striking that, one month late r, participants still rated their self-generated tags as significantly better than other-generated tags. This suggests that the utility of tags in general is a highly personalized matter of designing effective scent. 
Figure 7. Comparison of qualities of other-assigned tags, self A subsequent question is whethe r our identified quality tags are perceived as better than other X  X  tags. Based on our regression model, we selected five tags w ith the highest predicted quality as  X  X dentified quality tags X , and examined how participants perceived these tags. Our  X  X dentified quality tags X  were perceived as much better than other-assigned tags in all of the f our papers, as presented in figure 7. According to the paired-sample t tests, all of the significance coefficients were smaller than 0. 01. This result suggests that our regression model can help identify tags that will be perceived as quality tags by users. When it comes to compare self-assigned tags and our identified quality tags, it is hard to say wh ich one is perceived better. For Dourish paper, the perceived quality of identified tags (Mean=5.07, SD=1.40) was lower th an that of self-assigned tags (Mean=5.55, SD=1.39), t(39)=2. 70, p&lt;0.01. Similarly, for Rodden paper, the perceived quality of identified tags (Mean=4.39, SD=1.08) was lower th an that of self-assigned tags (Mean=5.18, SD=1.51), t(34)=2.97, p&lt;0.01. However, for Gutwin paper, there was no significant difference between our identified quality tags(Mean=5.54, SD=1. 23) and self-assigned tags (Mean=5.31, SD=1.38). Similarly, our identified quality tags in Grudin paper (Mean=4.41, SD=1. 53) were not perceived as having different quality as se lf-assigned tags (Mean=4.79, SD=1.63). See Figure 7 for a boxplot of the comparisons. We further compared the contents of participants X  self-assigned tags with our identified quality tags. We found that 1) identified quality tags have overlaps with self-assigned tags. Therefore, recommending the identified tags could provide tags that users want to assign themselves. 2) identified quality tags can complement or improve users X  ow n tags. Table 2 represents an example of self-assigned tags and identified quality tags for a certain user i for the Dourish and Bellotti paper. We can see that user i also assigned the identified tag  X  X SCW X  and  X  X wareness X . Furthermore, user i did not cover tags such as  X  X hared workspace X  and  X  X roupware X  but rated them high in quality. So others X  tags may complement users X  own tags. In addition, this particular user rated the identified tag  X  X orkspace awareness X  better than self-assigned tag  X  X warene ss X  or  X  X assive awareness X , probably because he considered the former was more accurate. This suggests a user X  X  own tags may also be improved by recommending others X  tags. 
Table 2. An example of self-assigned tags and top five other-In this paper, we articulated a scenario of tag use. This three-step scenario resembles the three main processes in Russell et al's [8] learning loops in sense-making: generation loop, data coverage loop, and representational shift l oop. In the generation loop, users search for appropriate represen tations to capture important regularities, which corresponds with the process of spreading activation of semantic concepts in our scenario. In the data coverage loop, users identify in formation of interests, and appropriately instantiate their mental representations as "encodons" [8]. Similarly, in the tag assignment process of our scenario, users select salient repr esentations and instantiate them as tags. Representational shift loop is guided by the discovery of residue. Similarly, users X  tagging patterns evolve as they assign more and more tags. Tagging fi xes the vocabulary for sense making. Every tag and every poten tial tag narrows and sharpens the sense that people can easily make of documents. Moreover, online tagging is usually a social behavior, which helps the representational shift in a community level. This provides us a scenario to explore sense-making in group or community level, which has not been studied in detail. Based on the scenario, we conceptu alized "quality" as determined by use, and framed an investigati on of tags as folksonomy versus a priori or authoritative definitions of quality. Furthermore, we were investigating whether simple characteristics of the user experience can be linked to perceived quality in a schematization of using and retrieving documen ts via queues: activation of mental concepts by web items, inst antiation of mental concepts as tags, and later reuse of these ta gs for information foraging and sense-making. Our findings X  X or example, frequent tags are perceived as higher in quality X  X howed how tag sense-making consensus can work. This study provided applicable regression models to identify quality tags, which could be directly used by social tagging systems to recommend tags to us ers. Our study results not only confirmed our hypothesis, but also gave more information about the relationship between tag quality and tag attributes. Centrality has a cubic relationship with tag quality, because tags that best describe a specific paper are not necessarily salient in a whole knowledge network, which suggests that centrality is not a very good predictor to identify quality tags. Frequency has a logarithmic relationship with quality, because the marginal effect of quality decreases as the fre quency increase. Entropy has a quadratic relationship between ta gs with middle entropy has the highest value of discrimination. Some social tagging systems provided recommendation based on users X  collective evaluation of existing tags (e.g., [4]), where motivating their rating may be a problem. Our study used objective measures, in particular th e attributes of tags, to identify quality tags. These regression models can be used to predict the quality of tags over time if certain tag attributes are modified. These models can be used by de signers to understand how social bookmarking systems are evolving with respect to the quality of tagging vocabulary in their system and what they can do in terms of user recommendation to improve the quality. We utilized the collective statistics of tags to identify quality tags. There have been many studies describing the probability distribution of tags [11], their growth patterns [10], and attributes such as tag frequency and entropy [1]. However, few studies have explored how to use these data in practice. Our study demonstrated a way to use such aggregate patterns and statistics for recommending tags, which will facilitate individual information foraging behaviors [6] and improve the  X  X uality of a system X  X  vocabulary of tags X  X 4]. We also showed the value of id entifying quality tags. Participants perceived our identified quality tags based on our regression model as higher in quality than general tags. These tags could be recommended to predict, complement , and improve user X  X  tags. In our study, 90 participants assigned more than 100 tags for each of the four papers in the first pha se. Such a high number of tags seem to an inefficient model for information foraging. Just as our results showed, participants ra ted some tags high quality while rated others as low quality. Hence, there is value in discriminating higher quality tags from lower quality tags. This implies that given some attributes of tags that are deemed important for a social bookmarking system, lower quality tags may be allowed to decay after some time whereas higher quality tags can be sustained in the system (e.g., through tag recommendation). The identified quality tags are recomme ndable according to our results. In this study, we explored thr ee tag attributes to identify the quality of tags, which can explain 31% of the variance. The three tag attributes are not exhaustive as they were low hanging fruit identified from existing literature. Other attributes need to be folded into our regression model. For example,  X  X imilarity X  as suggested by [3] can be used to explore the vocabulary of tags. Another limitation is that we used only four documents X  from HCI/CSCW -as tagging resources. However, the kind of study we conducted (Internet-based survey) makes it difficult to use a larger sample of documents. Our materials were carefully selected, and we presented a manipulation check in our results, which is beyond current method standards for Internet survey research. In our study, participants X  evaluation of tag quality may have had subjective bias due to the generation effect [35]. Users may remember their assigned tags and rated them high. However, the interval of one month between the two phases may be long enough to alleviate this bias. Acco rding to our results, even with the generation effect, participants still rated a subset of others X  tags as good as or better than their own tags. Our definition of quality is rather general, which can be different in other scenarios of use. The term  X  X uality X  is rather vague, which implies many metrics such as recall, precision [13], discrimination value [2], etc. According to our study, we can build appropriate algorithms to identify and recommend quality tags depend on which aspect of  X  X uality X  is valued in later information foraging by particular social bookmarking system. For example, tags with high entr opy are good for recall; tags with low entropy have precision. If the goal of a social bookmarking system is to facilitate retrieval of all relevant documents based on searching for tags (i.e., recall is more important than precision), then high entropy tags are preferred. While we recognized users may have different preferences of tags, we did not discuss personalization issues in this paper. We found 102 of all 108 tags in the second phase had the maximum value of 7 (i.e., perceived as having highest quality); at the same time, 89 tags had the minimum value of 1 (i.e., perceived as having lowest quality). This suggests that people have diverse preferences of tags. However, when we examined the standard deviations of the perceived quality for each tag, we found that the perceived quality does not vary too much for different participants. For all of the 108 tags, the mean of their sta ndard deviations is 1.740; the standard deviation of the standard deviations is 0.277. This result suggests that although some of them may have quite different tag preferences for certain tags, users usually have a shared vocabulary. It is possible that us ers with similar previous tagging patterns may have similar tag pr eferences. However, we did not conduct such an analysis because the data was not sufficient. The measurement of tagging patterns would not be robust in the condition that every participant only had five to seven tags for each paper, and there were only four papers. We found that users X  expertise cannot identify quality tags, which is not consistent with other studies [29, 30]. This may be because the assignment of five to seven ta gs per paper was not an expert task. Further, only a subset of participants X  tags from the first phase was rated during the second phase, which may have skewed the results. Lastly, we found some participants with high expertise rated specific tags very well that were rated low quality by other participants. For example, some experts assigned tags such as  X  X REP X  and  X  X ellotti X , while many other participants rated these two tags as low quality. There are some studies (e.g., [36]) claiming that we should not use expert X  X  recommendation exclusively because they have di fferent backgrounds and different concerns. In sum, although we did not find a significant effect of expertise on identifying quality tags, we cannot arbitrarily claim experts do not generate better tags. One way to check this claim is to improve our study with larger user samples with more diverse expertise and backgrounds. We would like to thank Craig Ganoe for his help in designing the user study. We would like to thank Jing Wang for helping tag consolidation and Haibin Liu fo r helping data cleaning. We appreciate all participants X  efforts in this study, and the reviewers X  valuable suggestions. [1] Chi, E.H. and Mytkowicz, T. Understanding the efficiency [2] Farooq, U., Kannampallil, T. G., Song, Y., Ganoe, C.H., [3] Muller, M. J. Comparing ta gging vocabularies among four [4] Sen, S., Harper, F.M., LaPitz, A. and Riedl, J. The quest for [5] Sen, S., Lam, S.K., Cosley, D ., Frankowski, D., Osterhouse, [6] Pirolli, P. and Card, S. Inform ation foraging in information [7] Pirolli, P. and Card, S. Information Foraging. Psychological [8] Russell, D., Stefik, M., Pirolli, P. and Card, S. The cost [9] Carroll, J. M. Making use: a design representation. [10] Golder, S.A. and Huberman, B.A. Usage patterns of [11] Cattuto, C., Baldassarri, A., Servedio, V.D.P. and Loreto, V. [12] Cattuto, C., Loreto, V. and Pietronero, L. Collaborative [13] Macgregor, G. and McCulloch, E. Collaborative tagging as a [14] Chi, E.H. and Mytkowicz, T. Understanding navigability of [15] Fu, W. T. The Microstructures of Social Tagging: A Rational [16] Furnas, G.W., Landauer, T.K., Gomez, L.M. and Dumais, [17] Furnas, G.W., Fake, C., von Ahn, L., Schachter, J., Golder, [18] Shirky, C. Ontology is overrated . (2005). [19] Bush, V. As We May Think. Atlantic Monthly , 176, 1 [20] Collins, A. M. and Loftus, E. F. A spreading-activation [21] Norman, D. A. Cognitive Artifacts. In Carroll J.M. (eds) [22] Simon, H. A. A behavioral model of rational choice. [23] Klein, G. and Klinger, D. Naturalistic Decision Making. [24] Weick, K. E. Sense-making in organizations . Sage [25] Wittgenstein, L. Philosophical Investigations . Blackwell [26] Berger, P. L. and Luckmann, T. The Social Construction of [27] Halpin, H., Robu, V. and Shepherd, H. The complex [28] Resnick, P. and Varian, H. R. Recommender systems. [29] Vu, K. P. L., Hanley, G. L., Strybel, T. Z. and Proctor, R. W. [30] McDonald, D. W. and Acke rman, M. S. Expertise [31] Dourish, P. and Bellotti, V. Awareness and coordination in [32] Gutwin, C., Roseman, M. and Greenberg, S. A usability [33] Grudin, J. Why CSCW applications fail: problems in the [34] Rodden, K. and Wood, K. R. How do people manage their [35] Slamecka, N. J. and Graf, P. The generation effect: [36] Tory, M. and Moller, T. Ev aluating visualizations: do expert 
