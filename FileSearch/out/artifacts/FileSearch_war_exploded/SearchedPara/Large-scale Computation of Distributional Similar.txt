 Measuring the semantic similarity between queries or, more generally, between pairs of very short texts, is increasingly receiving attention due to its many applications. An accurate metric of query simi-larities is useful for query expansion , to improve recall in Information Retrieval systems; for query suggestion , to propose to the user related queries that might help reach the desired information more quickly; and for sponsored search , where advertisers bid for keywords that may be different but semanti-cally equivalent to user queries.

In this paper, we study the problem of measuring similarity between queries using corpus-based unsu-pervised methods. Given a query q , we would like to rank all other queries according to their similarity to q . The proposed approach compares favorably to a state-of-the-art unsupervised system. Distributional similarity methods model the similar-ity or relatedness of words using a metric defined over the set of contexts in which the words appear (Firth, 1957). One of the most common representa-tions for contexts is the vector space model (Salton et al., 1975). This is the basic idea of approaches such as (Grefenstette, 1992; Bordag, 2008; Lin, 1998; Riloff and Shepherd, 1997), with some varia-tions; e.g., whether syntactic information is used ex-plicitly, or which weight function is applied. Most of the existing work has focused on similarity between single words or syntactically-correct multiword ex-pressions. In this work, we adapt these techniques to calculate similarity metrics between pairs of com-plete queries, which may or may not be syntactically correct.

Other approaches for query similarity use sta-tistical translation models (Riezler et al., 2008), analysing search engine logs (Jones et al., 2006), looking for different anchor texts pointing to the same pages (Kraft and Zien, 2004), or replacing query words with other words that have the high-est pointwise mutual information (Terra and Clarke, 2004).

Sahami and Helman (Sahami and Heilman, 2006) define a web kernel function for semantic similarity based on the snippets of the search results returned by the queries. The algorithm used is the following: (a) Issue a query x to a search engine and collect the set of n snippets returned by the search engine; (b) Compute the tf  X  idf vector v i for each document snippet d i ; (c) Truncate each vector to include its m highest weighted terms; (d) Construct the centroid of the L 2 -normalized vectors v i ; (e) Calculate the similarity of two queries as the dot product of their L 2 -normalized vectors, i.e. as the cosine of both vectors.

This work was followed up by Yih and Meek (Yih and Meek, 2007), who combine the web kernel with other simple metrics of similarity between word vec-tors (Dice Coefficient, Jaccard Coefficient, Overlap, Cosine, KL Divergence) in a machine learning sys-tem to provide a ranking of similar queries. Using a search engine to collect snippets (Sahami and Heilman, 2006; Yih and Meek, 2007; Yih and Meek, 2008) takes advantage of all the optimizations performed by the retrieval engine (spelling correc-tion, relevance scores, etc.), but it has several disad-vantages: first, it is not repeatable, as the code un-derlying search engines is in a constant state of flux; secondly, it is usually very expensive to issue a large number of search requests; sometimes the APIs pro-vided limit the number of requests. In this section, we describe a method which overcomes these draw-backs. The distributional methods we propose for calculating similarities between words and multi-word expressions profit from the use of a large Web-based corpus.

The contextual vectors for a query can be col-lected by identifying the contexts in which the query appears. Queries such as [buy a book] and [buy some books] are supposed to appear close to simi-lar context words in a bag-of-words model, and they should have a high similarity. However, there are two reasons why this would yield poor results:
First, as the length of the queries grows, the prob-ability of finding exact queries in the corpus shrinks quickly. As an example, when issuing the queries [Lindsay Lohan pets] and [Britney Spears pets] to Google enclosed in double quotes, we obtain only 6 and 760 results, respectively. These are too few occurrences in order to collect meaningful statistics about the contexts of the queries.

Secondly, many user queries are simply a concate-nation of keywords with weak or no underlying syn-tax. Therefore, even if they are popular queries, they may not appear as such in well-formed text found in web documents. For example, queries like [hol-lywood dvd cheap] , enclosed in double quotes, re-trieve less than 10 results. Longer queries, such as [hotel cheap new york fares] , are still meaningful, but do not appear frequently in web documents.
In order to use of distributional similarities in the query setting, we propose the following method. Given a query of interest p =[ w 1 ,w 2 ,...,w n ] : 1. For each word w i collect all words that appear 2. Represent the query p with a vector of words, 4. Given two queries, use the cosine between their
The motivations for this approach are: the geo-metric mean is a way to approximate a boolean AND operation between the vectors, while at the same time keeping track of the magnitude of the frequen-cies. Therefore, if two queries only differ on a very general word, e.g. [books] and either [buy books] or [some books] , the vector associated to the general words ( buy or some in the example) will have non-zero values for most of the contextual features, be-cause they are not topically constrained; and the vec-tors for the queries will have similar sets of features with non-zero values. Equally relevant, terms that are closely related will appear in the proximity of a similar set of words and will have similar vectors. For example, if the two queries are Sir Arthur Co-nan Doyle books and Sir Arthur Conan Doyle nov-els , given that the vectors for books and novels are expected to have similar features, these two queries will receive a high similarity score.

On the other hand, this combination also helps in reducing word ambiguity. Consider the query bank account ; the bag-of-words vector for bank will con-tain words related to the various senses of the word, but when combining it to account only the terms that belong to the financial domain and are shared be-tween the two vectors will be included in the final query vector.

Finally, we note that the geometric mean provides a clean way to encode the pair-wise similarities of the individual words of the phrase. One can inter-pret the cosine similarity metric as the magnitude of the vector constructed by the scalar product of the individual vectors. Our approach scales this up by taking the scalar product of the vectors for all words in the phrase and then scaling them by the number of words (i.e., the geometric mean). Instead of comput-ing the magnitude of this vector, we use it to com-pute similarities for the entire phrase.

As an example of the proposed procedure, Table 1 shows a random sample of the contextual features collected for the words in the query [acid fast bac-teria] , and how the query X  X  vector is generated by using the geometric mean of the frequencies of the features in the vectors for the query words. 4.1 Experimental settings To collect the contextual features for words and phrases, we have used a corpus of hundreds of mil-lions of documents crawled from the Web in August 2008. An HTML parser is used to extract text and non-English documents are discarded. After pro-cess, the remaining corpus contains hundreds of bil-lions of words.

As a source of keywords, we have used the top one and a half million English queries sent to the Google search engine after being fully anonymized. We have calculated the pairwise similarity between all queries, which would potentially return 2.25 tril-lion similarity scores, but in practice returns a much smaller number as many pairs have non-overlapping contexts.

As a baseline, we have used a new implementa-tion of the Web Kernel similarity (Sahami and Heil-man, 2006). The parameters are set the same as re-ported in the paper with the exception of the snip-pet size; in their study, the size was limited to 1,000 characters and in our system, the normal snippet re-turned by Google is used (around 160 characters).
In order to evaluate our system, we prepared a goldstandard set of query similarities. We have ran-domly sampled 65 queries from our full dataset, and obtained the top 20 suggestions from both the Sa-hami system and the distributional similarities sys-tem. Two human raters have rated the original query and the union of the sets of suggestions, using the same 5-point Likert scale that Sahami used. Table 2 shows the confusion matrix of scores between the two raters. Most of the disagreements are between the scores 0 and 1, which means that probably it was not clear enough whether the queries were unrelated or only slightly related. It is also noteworthy that in this case, very few rewritten queries were clas-sified as being better than the original, which also suggests to us that probably we could remove the topmost score from the classifications scale.
We have evaluated inter-judge agreement in the following two ways: first, using the weighted Kappa score, which has a value of 0.7111. Second, by grouping the pairs judged as irrelevant or slightly relevant (scores 0 and 1) as a class containing nega-tive examples, and the pairs judged as very relevant, equal or better (scores 2 through 4) as a class con-taining positive examples. Using this two-class clas-sification, Cohen X  X  Kappa score becomes 0.6171. Both scores indicates substantial agreement amongst the raters.

The data set thus collected is a ranked list of sug-gestions for each query 1 , and can be used to evaluate any other suggestion-ranking system. 4.2 Experiments and results As an evolution of the distributional similarities approach, we also implemented a second version where the queries are chunked into phrases. The motivation for the second version is that, in some queries, like [new york cheap hotel] , it makes sense to handle new york as a single phrase with a sin-gle associated context vector collected from the web corpus. The list of valid n-grams is collected by combining several metrics, e.g. whether Wikipedia contains an entry with that name, or whether they appear quoted in query logs. The queries are then chunked greedily always preferring the longer n-gram from our list.

Table 3 shows the results of trying both systems on the same set of queries. The original system is the one called Unigrams , and the one that chunks the queries is the one called N-grams . The distri-butional similarity approaches outperform the web-based kernel on all the metrics, and chunking queries shows a good improvement over using unigrams. This paper extends the vector-space model of dis-tributional similarities to query-to-query similarities by combining different vectors using the geometric mean. We show that using n-grams to chunk the queries improves the results significantly. This out-performs the web-based kernel method, a state-of-the-art unsupervised query-to-query similarity tech-nique, which is particularly relevant as the corpus-based method does not benefit automatically from search engine features.

