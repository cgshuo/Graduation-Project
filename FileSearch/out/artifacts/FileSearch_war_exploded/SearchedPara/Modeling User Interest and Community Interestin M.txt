 In microblogging sites, users can publish short messages (called tweets ), as well as adopt a wide range of behaviors spontaneously. The behaviors include follow-ing other users, mentioning hashtags or other users in tweets, and forwarding (or retweeting ) messages received from other users, etc.. Empirical and user stud-ies have shown that both the tweets and behaviors of a microblogging user are determined by her personal interest or that of her community [ 14 , 29 , 34 , 38 ]. However, most existing works on modeling user personal interests and commu-nity interest in microbloggings consider only either user generated content or the relationship between user generated content and behavior, and thus learning the two type of interests can be inaccurate. It also cannot leverage user generated content to provide a semantic interpretation of the behaviors.
 that there is a dependent relationship between user and community interests. These works either determine a user X  X  interests purely based on her communities (e.g., the fact that a user X  X  personal interests may be different from that of the commu-nities she belongs to. For example, a user can belong to a political community at the same time expressing interest in entertainment topics. Second, it suffers from trivial topics . These are popular but not socially meaningful topics. One such topic may be about food and drinks, and another about daily activities. These trivial topics are shared by many microblogging users ([ 17 ]). These topics are also likely be modeled by existing models as community interest leading to multiple commu-nities sharing these common topics. While sharing trivial topics is reasonable for munities (e.g., political communities, and professional communities, etc.). These communities should be characterized by clear topics.
 users and that of their mutually exclusive communities considering both user generated content and behavior. Moreover, we want to differentiate between topical interests of each user and that of each community. For each user, we also want to learn the bias of the user towards her community in generating both content and behavior. Lastly, for each community, we want the community to be clearly distinguished by socially meaningful topics.
  X  We propose a generative model, called Community and Personal Interest model (abbreviated as CPI ), for modeling topics and user topical interest as well as modeling user community in microbloggings. Our model is designed to work with data consists of user communities that are mutually exclusive. The
CPI model encapsulates different types of user behaviors in a common frame-work, and associates the behaviors with the user generated content through a set of latent topics.  X  We develop a sampling method to infer the model X  X  parameters. We also develop a technique to regularize the sampling process so that: (1) trivial topics are less likely to be assigned as community interest topics; while (2) non-trivial topics shared mostly by users within a community are more likely be assigned to be interest of the community.  X  We apply CPI model on a Twitter dataset and show that it outperforms other state-of-the-art models in modeling content topics and user classification tasks.
We also conduct an empirical analysis of personal interest and community topics found in the dataset to demonstrate the efficacy of the CPI model. in Section 2 . We then present the CPI model in detail in Section 3 . The algorithm for learning parameters of the CPI model and the regularization technique are presented in Section 4 . Next, we describe the experimental dataset and report the results of experiments of applying the proposed model on the dataset in Section 5 . Finally, we give our conclusions and discuss future work in Section 6 . 2.1 Topic and Community Analyis Michelson et. al. first examined topical interests of Twitter users by analyzing the named entities mentioned in their tweets [ 25 ]. Hong et. al. [ 16 ], Mehrotra et. al. [ 24 ], and Ramage et. al [ 29 ] conducted empirical studies on different ways of performing topic modeling on tweets using the original LDA model [ 16 ], proposed TwitterLDA model in which: (a) each user has a topic distribution and they share a common background topic; and (b) a topic is assigned to each tweet (instead of to each word). Recently, Qiu et. al. proposed to use TwitterLDA for jointly modeling topics of tweets and their associated posting behaviors (i.e., tweet, retweet, or reply) [ 28 ]. These works however only consider user generated content in modeling user interest. Our work, in the other hand, considers both user generated content and user behaviors.
 Early works on community mining in social networks are purely based on [ 39 , 45 ]). Ding et. al. conducted an empirical studies showing that both social links and user generated content should be considered in community mining in order to find coherent user communities [ 10 ]. Our work considers the social links among users as part of user behaviors, e.g., users follow and retweet other users, and users mention other users in tweets. Moreover, most of existing works that consider both social link and user content are based on the assumption that users/documents within a community have similar interest and are densely con-context where users express interest in a vast variety of topics, and their interest is therefore not always determined by their communities only. Our model, on the other hand, seeks to differentiate between a user X  X  personal interest from that of her community. It is also important to note that, unlike works on mining over-lapping communities (e.g., [ 2 , 15 , 43 ]), our work aims to mine mutually exclusive communities where each user belongs to only one of the communities. Lastly, there are also existing works on finding community interest. However these works either (a) determine a community X  X  interest by aggregating interest first approach suffers from trivial topics which are shared by many users, and hence more likely be assigned to community interests. The second approach is not able to differentiate between user personal interest and community interest. In con-trast, our model differentiates between users X  personal interests and communities X  interests. We learn the two interests simultaneously with a regularization so that socially meaningful topics are more likely be assigned to community interests. 2.2 User Behavior Analyis There has been a number of works analyzing user behavior in microblogging studied the patterns of following behavior; Conover et. al. [ 7 ], Wu et. al. [ 38 ], and Suh et. al. [ 35 ] examined retweet behaviors; Hannon et. al. [ 12 ] and Yin et. al. [ 42 ] proposed models for recommending following behavior; and Yang retweet behavior. However, most of these works (i) only consider a single type of behaviors, or (ii) do not consider the user generated content when studying user behaviors. Our model, on the other hand, allows different types of user behaviors to be modeled simultaneously when modeling user generated content. In the CPI model, each tweet is a bags-of-words chosen from a vocabulary denoted by V t , and each behavior belongs to one of L behavior types. Each type-l behavior is drawn from a set of all possible values denoted by model has K latent topics, where each topic k has a multinomial distribution  X  over the vocabulary V t and a multinomial distribution  X  lk V bl for each behavior type-l .
 and U users. Each community c has a multinomial distribution  X  topics that represents interest of the community. The personal interest of each user u is also represented by a topic distribution  X  u over the K topics. Each user belongs to one of the C communities following a multinomial distribution  X  .We denote the community of user u by c u . Moreover, each user u has a dependence distribution  X  u which is a Bernoulli distribution indicating how likely the user behaves according to her own personal interest (  X  0 u ) or according to interest of her community (  X  1 u =1  X   X  0 u ). Lastly, we assume that  X  Dirichlet priors  X  ,  X  ,  X  ,  X  l ,and  X  respectively, while  X  model is shown in Figure 1 .To generate a tweet t for user u ,we first flip a biased coin y t (whose bias is  X  u ) to decide if the tweet will be based on u  X  X  personal interest or that of her commu-nity. If the coin is head up, (i.e., y = 0), we then choose the topic z t for the tweet according to u  X  X  topic distribution  X  u . Otherwise, (i.e., y t = 1), we choose z t accord-ing to her community X  X  topic dis-tribution  X  c u . As tweets are short with a limited number of charac-ters, we assume that each tweet has only one topic. Once the topic z words in t are then chosen according to the topic X  X  word distribution  X  ilarly, we assume the same process for all adopted behaviors, except that, for a behavior b of type l , once the topic z b is chosen, the behavior is then sampled according to the topic X  X  behavior distribution  X  lz b . In summary, the generative process is as follows. 4.1 Gibbs Sampling In learning the parameters of the CPI model, we use collapsed Gibbs sampler ([ 23 ]) to iteratively sample the latent variables (i.e., coins, topics, and communities) for tweets, behaviors, and users. Due to the space limitation and its similarity to the sampling for tweet, we do not present in the following the sampling for behaviors details about implementation and complexity of the learning procedure. We use W and T to denote the number of words in the tweet vocabulary V and the set of all tweets in the dataset respectively. For each user u denote her j -th tweet by t i j . Each tweet t i j is a bag-of-words with length N i.e., t V . Also, for each tweet t i j , we denote its topic and coin by z We use C to denote the bag-of-communities of all the users; and use to denote the bag-of-topics and bag-of-coins of all the tweets and behaviors. We use
Y  X  t i j and Z  X  t i j to denote the bag-of-coins and bag-of-topics, respectively, of all the behaviors and all other tweets in the dataset except the tweet t use
Z  X  u i to denote the bag-of-topics of the tweets behaviors posted/adopted by all other users except u i Sampling for Tweet t i j . The coin y i j is sampled according to Equations 1 and 2 , while the topic z i j is sampled according to Equations 3 and 4 . In these equations, n y ( y, u, Y ) records the number of times the coin y is observed in the set of tweets and behaviors of user u . Similarly, n zu ( z, u, of times the topic z is observed in the set of tweets and behaviors of user u (i.e., those tweets and behaviors currently have coins 0); n zc ( z, c, number of times the topic z is observed in the set of tweets and behaviors that are tweeted/adopted based on interest of community c and by any user; and n w ( w, z, T , Z ) records the number of times the word w is observed in the topic z for the set of tweets T and the bag-of-topics Z . Sampling for User u i . The community c u i is sampled according to Equa-tion 5 . In the equation, n c ( c, C ) records the number of times the community c is observed in the bag-of-communities C ,and n z ( z, u ) records the number of tweets/ behaviors of u are observed in the topic z and has coin 1. 4.2 Semi-supervised Learning The CPI model presented as above is totally unsupervised with two parame-ters, i.e., number of topics K and number of communities C . In some settings, however, we may have known the community labels for some users but not the others. For example, a subset of users may explicitly share their political and professional labels. By assigning users within the same known community labels with the same community label (i.e., a value of c ), and by fixing their community label assignments during the sampling process (i.e., do not sample community for those users), we can use CPI model as a semi-supervised model. On one hand, this helps to bias the CPI model to more socially meaningful communi-ties. On the other hand, this also helps to overcome the weakness of supervised methods that require large number of labeled users in user classification task [ 6 ]. 4.3 Sparsity Regularization Community Topic Regularization. To avoid learning trivial community top-ics, community topic regularization aims to make every topic covered by mostly one community. Trivial topics (see Section 1 ) are usually shared by almost all users and hence are likely covered by multiple communities. Such topics are less likely be clear community topics. In contrast, a community topic is pre-ferred to be more unique among users within the community. We thus apply the entropy based regularization technique [ 3 ] to obtain the sparsity in the distribu-tion p ( c | z ). We implement this regularization in each coin and topic sampling steps for tweets and behaviors since they are main steps to determine whether a topic is community topic or personal interest topic. Again, due to the space limitation, we do not present in the following the regularization in sampling for behaviors and leave it out to [ 37 ].
 When sampling coin for the tweet t i j , we multiply the right hand side of Equa-tions 1 and 2 with a corresponding regularization term R coin defined by Equation 6 . Similarly, when sampling topic for the tweet t ply the right hand side Equation 4 with regularization term which is defined by Equation 7 . Lastly, when sampling community for user u multiply the right hand side of Equation 5 with a corresponding regularization term R ( c ) which is defined by Equation 8 .

In Equations 6 , 7 ,and 8 , H y i when y i j = y ;and H z i respectively regards to p ( c u i | z )and p ( c | z ). The parameters  X  topicComm are the expected mean and variance of the entropy of p ( c tively. These are pre-defined parameters. Obviously, with a small expected mean E topComm (which is corresponding to a skewed distribution), these regulariza-tion terms (1) increase weight for values of y and z that give lower empirical butions; and (2) decrease weight for values of y and z that give higher empirical butions. The expected variance  X  topicComm can be used to adjust the strictness of the regularization: smaller  X  topicComm imposes stricter regularization. When  X  topicComm =  X  , the model has no regularization on p ( c | Community Distribution Regularization. Even with the above community topic regularization, we may still have an extreme case where there is a com-munity that (1) includes all if not most of the users, and (2) covers largely trivial topics. To avoid this extreme case, we need to achieve a balance of user populations among the communities, i.e., we need to regularize the community distribution so that it is not too skewed to a certain community. To achieve this, we again use entropy based regularization technique [ 3 ] to facilitate a balanced community distribution p ( c ). We implement this regularization in each commu-nity sampling step for users since it is the main step to determine the community distribution. That is, when sampling community for user u the right hand side of Equation 5 with the regularization term defined by the Equation 9 .

In Equation 9 , H c u lar to above, the pre-defined parameters E comm and  X  comm and variance of the entropy of p ( c ) respectively. With a high enough expected mean value of E comm (which corresponds to a balanced distribution), this regu-larization term (1) decreases the weight for values of c that give lower empirical entropies of p ( c ) (and hence increases the balance of the distribution); while (2) hence decreases the balance of these distributions). Similarly, the expected vari-ance  X  comm can be used to adjust the strictness of the regularization: smaller  X  topicComm imposes stricter regularization. When  X  comm =  X  regularization on p ( c ).
 where each topic is assigned to at most one community) and  X  and set E comm =ln( C ) where C is the number of the communities (this is corresponding to the case where the communities are perfectly balanced), and  X  comm =0 . 3. We also used symmetric Dirichlet hyperparameters with  X  = 50 /K ,  X  =0 . 01,  X  =2,  X  =1 /C ,  X  =50 /K ,and  X  l =0 . 01 for all l =1 , Given the input dataset, we train the model with 600 iterations of Gibbs sam-pling. We took 25 samples with a gap of 20 iterations in the last 500 iterations to estimate all the hidden variables. 5.1 Dataset We collected tweets from a set of Twitter users who are interested in software engineering for evaluating the CPI model. To construct this dataset, we first utilized the list of 100 most influential software developers in Twitter provided in [ 18 ] as seed users. These are highly-followed users who actively tweet about software engineering topics, e.g., Jeff Atwood 1 , Jason Fried We further expanded the user set by adding all users following at least five seed users so as to get more technology savvy users. Lastly, we took all tweets posted by these users in August to October 2011 to form the experimental dataset. In this work, we consider the following behavior types: (1) mention , and (2) hashtag , and (3) retweet . These are messaging behaviors beyond content generation that users may adopt multiple times.
 We employed the following preprocessing steps to clean the dataset. We first removed stopwords from the tweets. Then, we filtered out tweets with less than 3 non-stopwords. Next, we excluded users with less than 50 (remaining) tweets. Lastly, for each behavior, we filtered away the behaviors with less than 10 adopt-ing users; and for each user and each type of behaviors, we filtered out all the user X  X  behaviors if the user adopted less than 50 behaviors of the type. These minimum thesholds are necessary so that, for each behavior and each user, we have enough number of adoption observations for learning both influence of the user X  X  personal interest and that of her community on behavior adoption. Based on the biographies of the users, we were able to manually label 3,023 users, including 2,503 Developers and 520 Marketers . The labeling work is mostly unambiguous as the biographies are quite short and clear, and only users with explicit declaration of their professionals were labeled. We therefore used these labels as ground truth commu-nity labels in our experiments.
 Table 1 shows the statistics of the experimental dataset after the preprocess-ing steps. The statistics show that the dataset after the filtering is still large. This allows us to learn the parameters accurately. 5.2 Experimental Tasks Content Modeling. In this task, we compare CPI against TwitterLDA model [ 44 ] in modeling topics in the content. TwitterLDA is among state-of-the-art modeling methods for microblogging content. To evaluate the perfor-mance, we run both models with the number of topics varied from 10 to 100. User Classification. In this task, we evaluate the performance of the CPI model as a semi-supervised learner (see Section 4.2 ). The task is chosen since: (1) we have ground truth community labels ( Developer and Marketer ) for only a small fraction of users the dataset (20.7%); and (2) the supervised learning approach for user classification in microbloggings may not practical as shown in [ 6 ]. We compare CPI model against the state-of-the-art semi-supervised learning ( SSL ) methods provided in [ 36 ]. Those are label propagation based methods which iteratively update label for each (unknown label) user u based on labels of the other users who are most similar to u . Here, we use cosine similarity between pairs of users. We represent each user as a vector of features, which include: (a) tweet-based features, and (b) bags-of-behaviors of the users. The tweet-based features for each user are the components in topic distribution of the user X  X  tweets discovered by TwitterLDA model. For the CPI model, we set the communities to 3 since: (a) it is reasonable to have one more community in each of the two datasets since there are users who do not belong to any of the two manually identified communities; and (b) this is to ensure that the CPI model run with the same settings as the SSL baseline methods. 5.3 Evaluation Metrics We adopt likelihood and perplexity for evaluating the content modeling task. To do this, for each user, we randomly selected 90% of tweets of the user to form a training set, and use the remaining 10% of the tweets as the test set. Then for each method, we compute the likelihood of the training set and perplexity of the test set. The method with a higher likelihood, or lower perplexity is considered better for the task.
 metric. We first evenly distributed the set of labeled users in each dataset into 10 folds such that, for each user label, every fold has the same proportion of users having the label. Then, for each method, we run 10-fold cross validation. More precisely, for each method and each time, we chose 1 fold of labeled users as test set. We hide label of user in this fold and consider them as unlabeled users. Then, we use 9 remaining folds of labeled users and all unlabeled users as the (semi-) training set. We then compute the average F 1 score obtained by each method in both label classes (i.e., Developer and Marketer ). The method with a higher score is the winner in the task. 5.4 Results ContentModeling. Figures 2 (a) and (b) show the performance of TwitterLDA model and CPI model in content modeling task when varying the number of top-ics K . As expected, larger number of topics K gives larger likelihood and smaller perplexity, and the amount of improvement diminishes as K increases. The figures show that CPI model significantly outperforms TwitterLDA model in the task. Considering both time and space complexities, we set the number of topics to 80 for the remaining experiments.
 User Classification. Figure 2 (c) shows the performance of SSL methods and the CPI model in the user classification task. In the figure, the SSL bar shows the best performance obtained by methods provided in [ 36 ]. The figure clearly shows that the CPI model significantly outperforms the SSL baseline methods in the task. 5.5 Topic Analysis Community Topics. We now examine the representative topics for each com-munity as found by the CPI model and TwitterLDA in both the two datasets. As the TwitterLDA model does not identify community for each user, we first use the best user classifier among the learnt SSL classifiers to determine com-munity for all the users. We then compute topic distribution of each community by aggregating topic distributions of all users within the community. Table 2 shows the top topics for each ground truth community in the experi-mental dataset found by TwitterLDA+SSL method and CPI model. Note words 4 ) and top tweets. For each topic, the topic X  X  top words are the words having the highest likelihoods given the topic, and the topic X  X  top tweets are the tweets having the lowest perplexities given the topic. Table 2 clearly shows that the top topics found by TwitterLDA+SSL method are neither clear (as their proportions are small) nor socially meaningful (e.g., topic 32 ( Daily activities )or topic 64 ( Daily life )). On the other hand, the table also shows that the top topics for each community as found by the CPI model are both clear (as the commu-nities are extremely skewed to the topics) and socially meaningful (e.g., topic 46 ( Programming languages )for Developer community; and topic 7 ( Online marketing )for Marketer community). These top topics are also semantically reasonable. It is expected that the Developer community are mainly interested in programming related topics, and the Marketer community are mainly inter-ested in marketing related topics.
 Personal Interest Topics. Next, we exam-ine the representative personal interest topics found by CPI model. Table 3 shows the top topics in aggregated personal topic distribu-tions of all users in the dataset. The table clearly shows that these representative top-ics are reasonable. It is expected that the top personal interest topics include Enter-tainment (topic 34) and a trivial topic ( Daily life -topic 33). It is also expected that a technology related topic ( Smartphone -topic 39) is among the top personal interest topics of users in the experimental dataset as most of its users are working in IT industry. This also shows the effectiveness of our regularization technique in differentiating between trivially popular topics and socially meaningful ones so that to assign the formers to user personal interest, and assign the latter to community interest. 5.6 User Behaviors Analysis Lastly, we examine the user behaviors associated with the result topics. Table 4 show some of representative topics (shown in Tables 2 and 3 ) together with the topics X  top behaviors. For each topic, the topic X  X  top behaviors are the behaviors having the highest likelihoods given the topic. The table show that the extreme behaviors for each of the topics are reasonable. For example, it is expected that people use marketing and social media related hashtags ( #seo , #socialmedia , #marketing , etc.), mention online marketers and bloggers ( @jeffbullas , @leader-swest , @markwschaefer , etc.), and retweet from marketing magazines ( mashable , sengineland , marketingland ) for topic Online marketing (topic 7); people also use programming related hashtags ( #javascript , #programming , #java , ruby , etc.), mention big IT companies and hosting services ( @twitter , @github , etc.), and retweet from influential developers ( codinghorror , garybernhardt , steveklab-nik , etc.) for topic Programming languages (topic 46). A qualitatively similar result holds for the remaining topics as well as topics that are not shown in the two tables. We leave out these analysis due to the space limitation. In this paper, we propose a novel topic model for simultaneously modeling mutu-ally exclusive community and user topical interest in microblogging data. Our model is able to integrate both user generated content and multiple types of behaviors to determine user and community interests, as well as to derive the influence of each user X  X  community on her generated content and behaviors. We also report experiments on a Twitter dataset showing the improvement of the proposed model over other state-of-the-art models in content modeling and user classification tasks.
 social factors in studying user generate content and behavior. These factors include the users X  interaction, their social communities, and the temporal and spatial dynamics of the users and the communities.
