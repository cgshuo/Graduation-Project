 . This performance may become much worse when the training samples 1. Introduction
Breast cancer is the most common diagnosed cancer among woman. In the United Kingdom, every year there are about 45,000 cases are diagnosed, and more than 13,200 women die from this cancer, i.e. a death rate about 29.3% ( Cancer Research UK, 2009 ). In the United States, about 182,500 cases were diagnosed in 2008, and nearly 40,500 women die from this disease annually ( American Cancer Society, 2009 ). Since the reasons behind are still uncertain, early detection and diagnosis is the key for improving breast cancer prognosis ( Cheng et al., 2003, 2006 ). Among many available techniques, X-ray mammogram has been one of the most reliable methods for early detection of such disease ( Hassanien, 2007 ). In general, it can increase the survival ratio by 20% to about 80% for patients. In England alone, around 1400 lives are saved each year via the NHS breast screening ( Cancer Research UK, 2009 ). Other popular means for breast cancer detection include magnetic resonance imaging (MRI) ( Torheim et al., 2001 ), electrical impedance spectroscopy (EIS) ( Kerner et al., 2002 ), ultrasound ( Joo et al., 2004 ), and infrared imaging ( Tosteson et al., 1999 ).

Although mammogram contains useful information for the early detection of breast cancer, it is difficult for radiologists to make accurate and consistent judgmen ts due to the huge amount of data and widespread screening. Conseq uently, about 10 X 30% cases are missed during the routine check ( Cheng et al., 2003 ). With the assistance of computer-aided diagnosis (CAD), the overall sensitivity from human observers can be impr oved by 10% on average, which provides a promising solution in such a context.

Detection and classification of microclacification clusters (MCCs) from mammograms plays important ro les in early diagnosis of breast cancer. In early detected cases, MCCs can be found in 30 X 50% of the screened mammograms. This will increase to 60 X 80% if histological examinations of cancer cases are considered. The difficulty for the detection of MCCs is due to (i) small size but various shapes, (ii) low contrast and unclear boundary from surrounding normal tissue, etc. ( Cheng et al., 2003; Sentelle et al., 2002 ). To solve such problems, a typical CAD system contains at least four stages including preproces-sing, feature-based extraction of re gions of interest (ROI), detection of MCCs, and classification. The preprocessing covers noise suppres-sion and contrast enhancement, including histogram equalization, etc. ( Papadopoulos et al., 2008 ), which is useful for robust extrac-tion of features and ROIs. The features include local statistics and texture modeling ( Grim et al., 2009 ), wavelets ( Rashed et al., 2007; Soltanian-Zadeha et al., 2004; Heinlein et al., 2003; Sentelle et al., 2002 ), and morphological features ( Kallergi, 2004 ), and RIOs can be determined via segmentation followed by local thresholding, spatial filtering and active contours ( Cheng et al., 2003; H assanien, 2007;
Cheng et al., 2004; Wei et al., 2005 ). From the segmented ROIs, MCCs can be detected using heuristics ( Sentelle et al., 2002 ), fuzzy sets ( Hassanien, 2007; Cheng et al., 2004 ), sub-image decomposition and filtering ( Nakayama et al., 2006 ), and machine learning algorithms such as decision tree, neural network and support vector machine ( Wei et al., 2005; Sajda et al ., 2002; Bocchi et al., 2004 ), where shape features such as lin ear structure is widely used ( Zwiggelaar et al., 2004; Nakay ama et al., 2006; Ge et al., 2006; Wu et al., 2008 ).
 been presented using machine l earning approaches to classify samples as malignant and benign, and this is also the focus of this paper. Among these techniques, artificial neural networks (ANN) is particularly emphasized ( De Santo et al., 2003; Sajda et al., 2002; Hadjiiski et al., 1999; Verma and Zakos, 2001; Ge et al., 2006;
Papadopoulosab et al., 2005; Kallergi, 2004; Bocchi et al., 2004 ), along with other approaches like linear discriminant analysis (LDA) ( Ge et al., 2006 ),Bayesclassifiers( Nakayama et al., 2006 ), K -nearest-neighbor (KNN) clustering ( Soltanian-Zadeha et al., 2004 ), genetic algorithms (GA) ( Soltanian-Zadeha et al., 2004 ), decision-rules ( Hassanien, 2007; Papadopoulosab et al., 2005 ), and support vector machines (SVM) ( Wei et al., 2009, 2005; El-Naqa et al., 2004, 2002;
Papadopoulosab et al., 2005 ). According to the evaluation work in Wei et al. (2005) ,ingeneraltheareaundertheROCcurve A achieved by ANN is only about 0.80, which apparently has space for further improvement.
 not only because of the complexity of the problem, i.e. containing cases that cannot be judged even by radiologists as analyzed in Wei et al. (2005) , but also the difficulty in dealing with imbalanced training set in machine learning. The imbalance here refers to the fact that one class is more heavily represented than the other. This is a common problem in real-world domains in detecting rare but important cases from large suspiciously normal samples ( Hong et al., 2007 ). Most existing machine learning algorithms fail in dealing with imbalanced data set as their predictions are biased to the class of majority samples ( Huang et al., 2006 ). To solve such problems, an improved neural classifier is proposed in this paper for effective classification of MCCs. Using over-sampling based balanced learning strategy and optimized decision making, the proposed method is found effective in improving both the sensitivity and specificity rate while main-taining the computing complexity of the classifier.
 contains introductory concepts related to the ANN classifier.
In Section 3, the proposed balanced learning and optimized decision making are presented. Section 4 discusses the evaluation criteria and implementation details including the data set and extracted features.
Experimental results are given and analyzed in Section 5 to fully validate the proposed methodology . Finally, brief conclusions are drawninSection6. 2. Review of ANN learning techniques pattern classification problem, and the two classes are referred to as  X  X  X alignant X  X  and  X  X  X enign X  X . If we denote x A R d as an input vector or pattern to be classified, and let scalar y denote its class label, i.e. y A {0, 1} for ANN. The training set L contains M samples, i.e.
L  X  {( x i , y i )} and i A [1, M ]. The problem here is how to determine a classifier f ( x ) which can make correct decision and classify the input pattern into suitable classes. In this section, brief introduction to
ANN are presented, which forms the base of our improved classifier as presented in the next section. 2.1. The ANN classifier an information processing system which is composed of a network of interconnected simple processing elements, i.e. neurons. Determined by the connections between these neurons and the associated para-meters, ANN can exhibit complex global behavior to generate expected outputs via supervised or unsupervised learning. Inspired by the biological nervous system, the learning process is to adjust the connection strength or weights between the neurons. Each neuron forms a node in the whole network and after training each node is assigned with a determined bias or threshold. For each interconnec-tion between two nodes, a weight is also assigned to represent the link-strength between the neurons.
 determined as z  X  g  X  w T x b  X  X  g where g ( ) is namely an activation function to decide whether the perceptron should fire or not. The sigmoid function Sig  X  x  X  X   X  1  X  e x  X  1 is the most popular used activation function, and others include tanh and step functions, etc.
 neuron, the output of the whole network can also be calculated in a topological manner. This means that for each neuron its inputs from other neurons need to be computed before determining its output. As seen, the weight vector and the bias associated to each connection and each node will influence the outputted results, and they can be determined in training or learning process as follows. First of all, the topology of the ANN needs to be specified, and the feed-forward ANN is adopted as it has been widely applied for the classification of MCCs ( De Santo et al., 2003;
Sajda et al., 2002; Verma and Zakos, 2001; Kallergi, 2004 ). A feed-forward ANN is a multi-layer perceptron (MLPP) which contains three or more layers of neurons, i.e. one input layer, one output layer and at least one hidden layer. With a given training set, a specified activation function and a learning ratio g where g the learning process for supervised training using the well-known back-propagation algorithm can be described in the following three stages.
 [ 1, 1] to attain a group of outputs z ( t ) at t  X  1 referring to the first round of iteration. Then, an error function is decided as estimated output z and the target output y . Finally, the error signal at the output units is propagated backwards through the whole network to update the weights using the gradient descent rule
D w ij  X  t  X  X  g @ e  X  t  X  where w ij refers to a weight between the jth node in a given layer and the ith node in the following layer. With updated weights, we can set t  X  t+1 to start a new iteration until the network becomes convergence. This can be measured by using a small change ratio of e ( ) or a given number of iterations. 3. Improved neural classifier with balanced learning
In general, the performance on classification of MCCs remains unsatisfied at around 80% in terms of A z ( Wei et al., 2009, 2005; El-Naqa et al., 2004; Papadopoulosab et al., 2005 ), and this accuracy may degrade further if the distribution of the samples is severely imbalanced ( Wei et al., 2005 ). Unfortunately, such imbalance distribution is widely found for MCCs classification, as usually there are much more ( 4 4 times) benign samples than malignant ones in the training sets ( Wei et al., 2005; Wu et al., 2008 ). Therefore, the performance of the classifier may bias to the majority class and fails for correct detection of MCCs. For this purpose, we have proposed an improved strategy, namely balanced learning, to overcome this problem. 3.1. Strategy in balanced learning
To achieve balanced learning, there are two main technical streams, i.e. data level and algorithm level methods ( Kotsiantis et al., 2006; Liu et al., 2006 ). At the data level, balanced learning is achieved using various re-sampling solutions to balance the training data ( Chawla et al., 2002 ). On the other hand, algorithm level solutions intend to adjust the cost function, decision thresh-old or the learnt probability for refined learning, such as the work reported in Tang et al. (2009) , Hong et al. (2007) , and Huang et al. (2006) . Using Bayes optimal classifier theory, it is found that individual classifier has a fundamental performance limit which makes it little better than that of the majority class ( Kotsiantis et al., 2006; Drummond and Holte, 2005; Chawla et al., 2002 ). Consequently, data-level solutions are preferred for balanced training in our paper.

Regarding data level solutions, there are two strategies in data re-sampling which include over-sampling of the minority class or under-sampling of majority class. S traightforward over-and under-sampling refer to random replication in the minority class and discarding samples in the majority class. Although under-sampling may reduce the size of the training set for efficiency, it may lead to serious problems in accurate mode ling the majority class as most of data are ignored. On the contrary, random over-sampling seems to be a better solution despite of the increased training set.
Since random over-sampling may increase the likelihood of over-fitting in dealing with the duplicated samples, several smart sampling techniques have been presented such as synthetic over-sampling (SMOTE) ( Chawla et al., 2002 ). In SMOTE, synthetic minority samples are generated via interpolation of one random sample and its nearest neighbo rs. Some other smart sampling techniques include one-sided selection, cluster-based over-sampling, Wilson X  X  editing, etc., and details of which can be referred to the work in Hulse et al. (2007) . 3.2. Proposed balanced learning strategy
According to the exte nsive experiments in Hulse et al. (2007) ,it is found that random sampling outp erforms several smart sampling techniques and unaltered data set. However, the evaluation in Wei et al. (2005) indicates that random ov er-sampling seems not improving the performance in classification of MCCs, and similar finding is concluded in detecting sentence boundaries in Liu et al. (2006) . Besides, it is indicated that SMOTE may outperform down-sampling in certain cases ( Liu et al., 2006 ). These inconsistent results need to be further clarified before applying any sampling strategies to classify MCCs for improved performance.

Fig. 2 shows a typical two-class classification problem which contains combined linear decision boundaries. This is very com-mon in machine learning domain and the segment of the decision boundary can also be nonlinear. For the two classes marked as circle and star shapes, two pairs of same-class samples are extracted satisfying minimum neighboring distance and marked as A X  X  and C X  X . According to the rules of smart sampling in SMOTE, synthetic samples can be generated for balanced learning. Unfortunately, the generated samples in these cases are unreli-able noisy ones which may inevitably degrade the performance of training and classification.

To some degree, the analysis above can explain why smart sampling behaves well in some cases. The more complex the decision boundary is, the more noisy samples are introduced via smart sampling, and hence the wor se performance may be achieved. On the other hand, smart sampling like SMOTE may work well in simpler cases such as the linear pr oblem in detection of sentence boundary in Liu et al. (2006) . Since there are much more negative samples than positive ones, the strategy here is for each positive sample in the training set to introduce additional samples. These newly introduced samples are almost replications of the original one with minor changes (increasing o r decreasing at less than 1% after normalizing the range of the feature values within [ 1, 1]) to one item of the feature values which is randomly determined. This helps to keep consistency between generated samples and the original ones for balanced learning and avoiding the problem caused by smartsamplingasdiscussedabove.Pleasenotethatitisassumed that the samples in our test set contain no noise instances thus the over-fitting caused by over-sampl ing in training can be avoided. 3.3. Optimized decision making
In our implemented ANN classifier, the outputs are continuous values rather than binary symbols. Conventional methods use simple thresholding in decision making. If the outputs are larger than a chosen threshold, a positive sample is detected. Otherwise, it is decided as negative. However, this simple thresholding suffers uneven distribution of the training outputs and leads to poor performance. To overcome this drawback, on the contrary, optimized decision making using optimal thresholding is pro-posed and described as follows.

Optimal thresholding is achieved through statistical analysis of the classifier outputs as follows. Let z i denote the predicted output for a given input sample x i with a target label y i , y i A z i A ( a 0 , a 1 ) and the parameters a 0 and a 1 represent, respectively, the lowest and the highest boundary of the output from the classifier. Then, two conditional probabilities p ( z i 9 p ( z i 9 y i  X  1) are obtained. For a given threshold T A sum of error classification rate Err is determined as Err  X  T  X  X  w 1 where w 1 and w 0 are two non-negative weights satisfying w 0 + w 1  X  1. Then, an optimal threshold T opt is determined when the minimum cost of error classification is achieved. Conse-quently, this optimal threshold can be used to obtain another group of classification results. The effectiveness of the proposed optimized decision making has been fully validated by our experimental results as shown in Section 5:
T opt  X  argmin  X  Err  X  T  X  X  X  4  X  4. Implementation and evaluation study discussed in this section, along with some implementation details.
These are essential for consistent evaluation of our proposed methodology to compare with others. 4.1. Data set collected, which contain 633 benign and 115 malignant samples where the ratio between them is nearly 5.5. These MCCs are extracted from 295 full-field ma mmograms in the well-known
DDSM database, in which the mammography data from more than 2600 patients are scanned at 50 m musingLUMISYS( Heath et al., 1998; Heath et al., 2001 ). The collected MCCs are then randomly divided into two datasets for tra ining and testing, respectively. Examples of benign and malignant cases are shown in Fig. 3 . measurements is employed ( Wu et al., 2006, 2008 ). Firstly, some preprocessing is applied to remove the influence of background and several artifacts like white/black spots and scratches. Then, optimal filtering is employed using local frequency in terms of energy distri-bution extracted from mammograms. Finally, adaptive thresholding is utilized as post-processing for fur ther robustness. Relevant details can be found in Wu et al. (2006) . 4.2. Feature set various patterns on the mammogram ( Cheng et al., 2003 ).
Whether their clusters are malignant or benign depends on the size, shape and geographic distribution of all microcalcification regions in a cluster, i.e. if they are tightly clustered and have certain linear structure, etc. Therefore, the extracted features need to measure these properties accordingly which include the area, the scattered degree and brightness of the regions in the cluster. In total 23 features are extracted from each of the segmented microcalcification clusters, and a list of them is shown in Table 1 . As seen, except the three single measures #1 X 3, the other 20 features in the feature set are composed of the mean and standard deviation values of ten measures. These 20 features can be categorized into three classes including (i) intensity statistics (#4 X 5), (ii) shape features (#6 X 17), and (iii) linear structure features (#18 X 23). Introductions to most of these features can be found in Cheng et al. (2003) , Ge et al. (2006) , Papadopoulosab et al. (2005) , Nemoto et al. (2006) , Wu et al. (2008) , Jiang et al. (2010) , Kallergi (2004) , Papadopoulos et al. (2008) , and Bocchi et al. (2004) .

The three single measures are patient X  X  age, the number of regions in a cluster, and the average least inter-distance (LID) of all regions. Since about 80% of the diagnosed breast cancer cases are for women over 50 years old ( Cancer Research UK, 2009 ), age is a good indicator and has been widely used in the classification of MCCs ( Cheng et al., 2003; Wu et al., 2008; Kallergi, 2004 ). A MCC is defined as a group of at least three microcalcifications within 1 cm 2 , and the number of microcalcifications in a cluster is also an important feature ( Wei et al., 2009; Wu et al., 2008; Kallergi, 2004; Papadopoulos et al., 2008 ). The mean of LID refers to the average value of inter-distance between each region and its neighboring ones ( Wei et al., 2009 ), which can also be used to measure the scattered degree of the distribution of the microcalcifications in a cluster. In addition, the intensity measures are also useful as high intensity is expected for the white specks in MCCs ( Cheng et al., 2003; Papadopoulosab et al., 2005 ).

Shape features are very important indicators in this field, in which thearea(size),thecompactness, Fourier descriptors, moments, eccentricity, and the spread of these regions are commonly used. The definitions of these sha pe features can be referred to in Cheng et al. (2003) , Papadopoulosab et al. (2005) , Wu et al. (2008) , Kallergi (2004) ,and Papadopoulos et al. (2008) .Pleasenote that these measures can be extracted from each microclacification region within a candi date MCC, and the mean and standard deriva-tion values over all regions are then determined for classification purpose.

Linear structure features form another important feature set, which has been widely used in detection and classification of MCCs ( Zwiggelaar et al., 2004; Nakayama et al., 2006; Ge et al., 2006; Wu et al., 2008 ). Linear structure here means a string of pixels (representing a line) with similar intensity along a certain direction, which can be denoted as r ( y , l ) where y ; and l refer respectively to the direction and the length in the linear structure. In addition, the pixel intensities on the line are higher than that of their surrounding pixels, and also the length of the line should be larger than its width. To measure the consistency of the inten-sities along the linear structures in a MCC, six features are extracted as shown in Table 1 using the mean and standard deviation values of three measurements ( Wu et al., 2008 ). 4.3. Evaluation criteria
As mentioned above, all 748 MCC samples are randomly partitioned into two subsets for training and testing, respectively. All the positive samples in the training set are over-sampled for balanced learning. The trained model is then used to classify samples in the test set. This process is repeated 10 times to overcome any bias in data partition. The average performance over these 10 tests is taken as a final result for evaluations.
For a two-class problem, let us denote TP and TN as the numbers of correctly classified positive and negative samples, FP and FN for the numbers of incorrectly classified positive and negative samples, i.e. false alarms and missed positives. Several metrics can be determined below for quantitative evaluations: TP rate  X  Recall  X  TP =  X  TP  X  FN  X  X  5  X  Precision  X  TP =  X  TP  X  FP  X  X  6  X  FP rate  X  1 Specificity  X  FP =  X  TN  X  FP  X  X  7  X 
To enable a single measure of performance, the F 1 score is also obtained as follows: F  X 
Receiver operating characteristic (ROC) analysis and its var-iants are commonly used for quantitative evaluations of classi-fiers, especially for the detection and classification of MCCs ( Cheng et al., 2003 ). In ROC analysis, TP vs. FP rates are adopted. Under ROC analysis, the area under the ROC curve A z is also used as an important evaluation criterion ( Cheng et al., 2003 ), where A  X  1 indicates an ideal case with TP rate  X  100% and FP rate
For the ANN classifier, the number of nodes in the hidden layer is empirically set as 15 for the better results achieved. The training process stops when the training performance keeps unchanged over a long time, say more than 4000 iterations. The performance is measured using the F 1 , and the parameters which yield the highest F 1 value is stored and used for testing. 5. Results and discussions
In this section, comprehensive experiments are conducted and the results are presented. Quantitative evaluations are used to validate the effectiveness of our proposed method including balanced learning and optimal decision making. 5.1. Performance of balanced learning
First of all, the performance of balanced learning is compared with those training using the original data, and the training ratio is set as 80%, i.e. 80% of the samples for training and 20% for testing. The ROC curves are shown in Fig. 4 to show the performance of training and testing with or without balanced learning, respectively, where several facts can be summarized as follows. Firstly, training results are in general much better than testing ones. Secondly, balanced learning indeed can yield better results despite of a little higher false positive rate. Regarding training, it has generated slightly higher recall rate though its recall rate without balanced learning is already high enough. For testing, balanced learning produces much improved results. Finally, it is worth noting that balanced learning seems inferior to unbalanced learning only if the false positive rate is less than 3%. As a higher recall rate is always desirable in such applications, balanced learning still seems better than unbalanced ones.
Quantitative comparisons of the results are also reported and shown in Table 2 , in which no optimized decision making is Recall applied in the testing. First of all, with balanced learning, the testing performance in terms of F 1 score and A z are significantly improved, which has validated the effectiveness of balanced learning. In addition, it is found that the training performance is high in terms of all the five measures no matter balanced training is used or not. This has indicated that ANN is capable of model the problem accurately. However, the testing performance under balanced training is much better, in which an improvement of 12.8% is achieved in both F 1 and A z measurements. Although the testing data is severe imbalanced, a high Specificity value is still achieved to yield a higher A z measurement. Since the ratio between FP and TP is much larger than the ratio between FP to
TN , this has led to a lower F 1 score but a higher A z value. 5.2. Performance of optimized decision making making is presented. Again, we select 80% of samples for training and 20% for testing. By comparing the results in Tables 2 and 3 ,we can clearly find how optimized decision making helps to generate improved performance as summarized below: 5.3. Performance under various training ratios tests, the performance under various training ratios is compared.
Under various training ratios, the training results and testing results with or without optimized decision making are further evaluated in terms of F 1 and A z measurements. These results are shown in Fig. 5 , where Test 2 denotes results with op timal decision making. In total three pairs of curves are plotted in each figure, in which one pair is from training and the other two are from testing without or with optimal decision making. Each pair of curves is plotted using the training ratio (changed from 50% to 90%) vs. performance of F measurements and they are further discussed as follows. without balanced learning are very close to each other and appear insensitive to the training ratio, which again validates that ANN is capable of accurate modeling the problem. Secondly, the testing results with balanced learning are much better that those without balanced learning. Thirdly, in most cases optimized decision making produces better results in F 1 and A z measurements when balanced learning is employed, except the result at the training ratio of 60%. When balanced learning is not used, however, better F measurement can only be yielded when the training ratio is between 70% and 80%. In addition, better A z measurement can always be achieved from optimized decision making even without balanced learning. 5.4. Computational complexity learning and optimized decision making do need additional computations. As optimized decision making is excluded in the training iterations, it can be simply ignored. In the following, we will analyze the effect of balanced learning in such a context. costs additional time in learning the model. Let N  X  N 0  X  N total samples, where N 0 and N 1 denote, respectively, the number of negative and positive samples satisfying N 0  X  KN 1 , K 4 2. After over-sampling of positive samples,  X  K 1  X  N 1 new positive samples are produced and in total we have 2 N 0 training samples. This equals to 2 NK =  X  K  X  1  X  and less than 2 N , which has indicated that the number of samples under balanced learning is less than twice of the number of the original samples. Under same number of 0.7 0.75 0.8 0.85 0.9 0.95 1 iterations, the training time should be no more than double of the one without balanced learning.

In addition, it is found that balanced learning needs less number of iterations to converge, which is about 77% of the one required for training without balanced learning. This fast conver-ging might due to the improved distributions of training samples from our balanced learning. Consequently, the increased com-plexity under balanced learning becomes 2 K =  X  K  X  1  X  77 % 1  X  0 : 54 1 : 54 =  X  K  X  1  X  X  9  X 
This indicates a maximum of 54% additional computing burden, which is totally acceptable for the benefit of much improved performance. 6. Conclusions
In this paper, an improved neural classifier is proposed, where balanced learning and optimized decision making are introduced for classification of benign and malignant MCCs in mammograms. The experiments are conducted on 748 samples extracted from the well-known DDSM database, and the main findings can be summarized as follows.

Firstly, balanced learning indeed has significantly improved the classification accuracy, and an av erage gain of more than 10% can be achieved for ANN in terms of both F 1 score and A z measurement. Secondly, optimized decision making yields improved results in F score and A z no matter balanced learning is used or not. Thirdly, a training ratio between 70% and 80% is suggested due to the various performances under different training ratios. Finally, it is found that the suggested balanced training will only bring up to 54% of additional computation load, a tolerable cost for the much improved performance. Further investigations include applying feature selec-tion approaches for improved efficiency as well as reducing false alarms for more robustness.
 Acknowledgment
Firstly, we wish to thank anonymous reviewers and the editor for their insightful comments. The authors would also like to thank Dr. Z.-Q. Wu for his helps in data collection and feature extraction in this work.
 References
