 Background. The notion of a key is fundamental for understanding the struc-ture and semantics of data. For relational databases, keys were already intro-information extraction, and financial risk assessment produce large volumes of uncertain data from a variety of sources. For instance, RFID (radio frequency finite set of possible worlds, each being a relation.
 Keys address the consistency dimension of data quality in traditional data-bases. Due to the veracity inherent to probabilistic databases as well as the variety of sources the data originates from, the traditional concept of a key trivial key that is satisfied by all possible worlds: the key holds in the worlds W 1 and W 2 , k 2= k { rfid , time } holds in k 3= k { rfid , zone } holds in W in a probabilistic database. In our example, k 1, k 2, and of a key provides a control mechanism to balance consistency and completeness are unique. In our example, we may wonder about the chance that different wolverines are in the same zone at the same time, indicating potential mating behavior. We may ask
SELECT DISTINCT rfid FROM Tracking WHERE zone = X  X 2 X  AND time = X 2pm X  and using our p-keys enables us to derive a minimum probability of 0.65 that a most with probability 0.35. These bounds can be inferred without accessing any portion of a potentially big data source at all, only requiring that the key at least marginal probability 0.65 on the given data set.
 Contributions. The examples motivate us to stipulate lower bounds on the marginal probability of keys. The main inhibitor for the uptake of p-keys is even harder problem. Lower bounds appear to be a realistic compromise here. Our contributions can be summarized as follows. Modeling. We propose p-keys Their main target is to help organizations balance consistency and complete-meaningful from meaningless patterns in large volumes of uncertain data from zations to reduce the overhead of data quality management by p-keys to a min-imal level necessary. For example, enforcing k { rfid }  X  k { rfid,zone }  X  0 . 35 , would be redundant as the enforcement of we show how to visualize concisely any given system of p-keys in the form of an Armstrong PC-table. An Armstrong PC-table is a perfect semantic summary perceived to best represent the application domain. Any problems with such Figure 1 shows an Armstrong PC-table for { k 1  X  0 . 65 ,k k { rfid from Table 1 , which is represented by this PC-table. Profiling. For the data-driven acquisition of p-keys we compute the marginal probability of every key table from Figure 1 , then our algorithm would return the profile k { time }  X  0 , k { zone }  X  0 , k { rfid,time }  X  0 . 75 , k { and k { rfid,time,zone }  X  Our experiments demonstrate that our visualization and profiling techniques work efficiently in the context of our acquisition framework. Organization. We discuss related work in Section 2 . P-keys are introduced in future work in Section 7 .
 Poor data quality is arguably the biggest inhibitor to deriving value from big represent target constraints that avoid data redundancy with certain degrees there is empirical evidence that Armstrong databases help with the acquisition to conduct such empirical studies for p-keys in the future. is future work to investigate approximate versions of probabilistic keys. approach.
 unaware of any work concerning keys on probabilistic data. We introduce some preliminary concepts from probabilistic databases and the central notion of a probabilistic key.
 associated with a domain dom ( A ) of values. A tuple t over assigns to each attribute A of R an element t ( A ) from the domain dom ( A relation over R is a finite set of tuples over R . Relations over called possible worlds of R here. An expression kX over R called a key .Akey kX is said to hold in a possible world by W | = kX , if and only if there no two tuples t and t ( W ,P ) of a finite non-empty set W of possible worlds over distribution P : W X  (0 , 1] such that W  X  X  P ( W ) = 1 holds. Table 1 shows a probabilistic relation over relation schema Wolverine = { W , for example, satisfies the keys k { rfid , time } and k { the key k { rfid , zone } .The marginal probability of a key r =( W ,P ) over relation schema R is the sum of the probabilities of those possible worlds in r which satisfy the key. We will now introduce the central notion of a probabilistic key.
 Definition 1. A probabilistic key ,or p-key for short, over relation schema is an expression kX  X  p where X  X  R and p  X  [0 , 1] . The p-key probability of kX in r is not smaller than p .
 In our running example over relation schema Wolverine , the p-relation from Table 1 satisfies the p-keys k { rfid , time }  X  0 . violates the p-keys k { rfid , time }  X  0 . 9 and k { rfid When using sets of p-keys to manage the consistency and completeness targets probability by which a given key is implied from a given set of p-keys, and to optimize the efficiency of updates and query answers, for example. The results will also help us develop our acquisition framework later.
 Let  X   X  X   X  } denote a set of constraints over relation schema implies  X  , denoted by  X  | =  X  , if every p-relation r over satisfies  X  .Weuse  X   X  = {  X  :  X  | =  X  } to denote the semantic closure of relation schema R and a given set  X   X  X   X  } of constraints in  X  implies  X  . We will now characterize the C -implication problem for the class by a linear time algorithm.
 Axioms. We determine the semantic closure by applying inference rules of the form inference of  X  from  X  by R . That is, there is some sequence from an application of an inference rule in R to some premises in Let  X  + R = {  X  :  X  R  X  } be the syntactic closure of  X  under inferences by is sound ( complete ) if for every set  X  over every R we have implication of p-keys. Here, R denotes the underlying relation schema, Y form attribute subsets of R ,and p, q as well as p + q are probabilities. Theorem 1. P forms a finite axiomatization for p-keys.
 k { rfid , time }  X  0 . 25 , but not the p-key  X  = k { rfid , inferred from  X  by applying S to k { rfid }  X  0 . 3 to infer the more time we save by avoiding redundant validation checks. Algorithms. In practice, the semantic closure  X   X  of a finite set and even though it can always be represented finitely, it is often unnecessary as input  X   X  X   X  } and the question is whether  X  implies  X  time algorithm for computing the maximum probability p , such that for p-keys to a single scan of the input.
 Theorem 2. Let  X   X  X  kX  X  p } denote a set of p-keys over relation schema Then  X  implies kX  X  p if and only if X = R or p =0 or there is some such that Z  X  X and q  X  p .
 probability p by which a given key kX is implied by a given set R .If X = R , then we return probability 1. Otherwise, starting with algorithm scans all input keys kZ  X  q and sets p to q whenever the current p and X contains Z .Weuse |  X  | and R to denote the total number of attributes that occur in  X  and R , respectively.
 Theorem 3. On input ( R,  X , kX ) , Algorithm 1 returns in the maximum probability p with which kX  X  p is implied by Algorithm 1. Inference only if p  X  p .
 Corollary 1. The implication problem of p-keys is decidable in linear time. Given the p-key set  X  = { k { time }  X  0 . 2 ,k { rfid }  X  Algorithm 1 returns p =0 . 3. Consequently, the p-key k { by  X  , but k { rfid , time }  X  0 . 35 is not implied by  X  . Applications will benefit from the ability of analysts to acquire a good lower bound for the marginal probability by which keys hold in the domain of the application. For that purpose, analysts should communi-cate with domain experts. We establish two major tools that help analysts to commu-nicate effectively with domain experts. We follow the framework in Figure 2 . Here, analysts use our algorithm to visualize abstract sets  X  of p-keys in the form of some Armstrong PC-table, which is then inspected jointly with domain experts. In particular, the PC-table represents simul-taneously for every key kX the marginal probability that quality data sets in the tar-get domain should exhibit. Domain experts may change the PC-table or supply new PC-tables to the analysts. For that case we establish an algorithm that profiles p-keys.
 PC-table. Such profiles are also useful for query optimization, for example. 5.1 Visualizing Abstract Sets of p-keys as Armstrong PC-tables known as an Armstrong database , which we formally recall here [ 8 ]. Let a set of p-keys over a given relation schema R . A p-relation is Armstrong for  X  if and only if for all p-keys  X  it holds that and only if  X  implies  X  . The following theorem shows that every distribution of probabilities to keys, that follows the inference rules from Table 2 ,canbe marginal probabilities.
 Theorem 4. Let l : R  X  [0 , 1] be a function such that l ( X, Y  X  R , l ( XY )  X  l ( X ) holds. Then there is some p-relation that r satisfies kX  X  l ( X ) ,andforall X  X  R and for all p&gt;l ( X ) , r violates kX  X  p .
 Proof. Let { l 1 ,...,l n } = { l ( X ): X  X  R } such that let l 0 = 0. Define a probabilistic relation r =( { W 1 ,...,W For all i =1 ,...,n , the world W i is an Armstrong relation for the key set  X  i = { kY : l ( Y )  X  l i } ,and P ( W i )= l i  X  l i  X  1 . For all j  X  X  1 ,...,n } . Then, kX holds on W i if and only if i  X  j marginal probability l ( X ) with respect to r ,and kX  X  l r violates kX  X  p for every p&gt;l ( X ).
  X   X  X  kR  X  Now, let l ( X ):= p X . Then l ( R )= p R =1and l ( XY )= By Theorem 4 it follows that there is some Armstrong p-relation Z  X  R and all p  X  [0 , 1],  X  implies kZ  X  p if and only if concise representations of Armstrong p-relations. We call these Armstrong PC-A conditional table or c-table , is a tuple CD = r, W , where world identifiers of CD is the union of the sets W t for all tuples a world identifier i of CD , the possible world associated with r and i  X  W t } . The semantics of a c-table CD = r, W , called representation , is the set W of possible worlds W i where i denotes some world identifier of A probabilistic conditional database or PC-table , is a pair CD . The set of possible worlds of a PC-table CD,P is the representation of of its world identifier. For example, Figure 1 shows a PC-table Armstrong for the p-relation in Table 1 .
 Algorithm 2. Armstrong PC-table is determined by the number of distinct probabilities that occur in purpose, for every given set  X  of p-keys over R and every probability let  X  over R which have at least marginal probability p . It is possible that not contain any p-key kX  X  p where p = 1. In this case, Algorithm 2 computes an Armstrong PC-table for  X  that contains one more possible world than the number of distinct probabilities occurring in  X  . Processing the probabilities probability p i  X  p i  X  1 (line 5) a traditional Armstrong relation for the For this purpose, the anti-keys are computed for each p i W of those worlds i is recorded for which X is an anti-key with respect to which X is an anti-key and that has matching values with t columns of X (lines 14-18).
 Theorem 5. For every set  X  of p-keys over relation schema computes an Armstrong PC-table for  X  in which the number of possible worlds coincides with the number of distinct probabilities that occur in In our running example,  X  contains k { rfid , time }  X  0 . 75 , k { time , zone }  X  0 . 65 , and k { rfid , zone }  X  0 . 35 . Applying Algo-rithm 2 to Wolverine and  X  may result in the Armstrong PC-table of Figure 3 . Finally, we derive some bounds on the time complexity of finding Armstrong PC-tables. Addi-tional insight is given by our exper-iments in Section 6 .
 Theorem 6. The time complexity to find an Armstrong PC-table for a given set  X  of p-keys over relation schema R is precisely exponential in |  X  | .
 Proof. Given R and  X  as input, Algorithm 2 computes an Armstrong PC-table for  X  in time at most exponential in |  X  | . Indeed, an Armstrong relation for can be computed in time at most exponential in |  X  p i | X |  X  | more than |  X  | computations of such relations.
  X  over R is exponential in |  X  | . Such a case is given by R  X  PC-table requires 2 n + 1 tuples, and there is only one possible world. for  X  over R is logarithmic in |  X  | . Such a case is given by has n + 1 tuples that realize the n agree sets R  X  X  A 2 i  X  attributes on which some pair of distinct tuples have matching values. 5.2 Profiling of p-keys from PC-tables The profiling problem of p-keys from a given PC-table CD,P schema R is to determine for all X  X  R , the marginal probability the p-relation r =( W ,P ) that CD,P represents. The problem can be solved as follows: for each X  X  R , initialize p X  X  0 and for all worlds add the probability p W of W to p X ,if X contains some minimal key of see Algorithm 3 . The set of minimal keys of a world W is given by the set of minimal transversals over the disagree sets of W (the complements of agree keys k { time , zone }  X  0 . 65 , k { rfid , time }  X  0 . remaining X  X  R , as illustrated on the right of Figure 1 . Algorithm 3. Profiling In this section we report on some experiments regarding the computational com-6.1 Visualization p-keys, and outputs an Armstrong PC-table for  X  . The random generation of  X  was achieved by firstly sampling n probabilities p n from [0 attribute set X  X  R , we assign a probability randomly sampled from the set { 0 } X  X  p The left of Figure 4 shows the number of tuples in the Armstrong PC-table as of an Armstrong PC-table grows linearly in the input key size. The worst-case exponential growth occurs rarely on average. This demonstrates that Armstrong PC-tables exhibit small sizes on average, which makes them a practical tool to acquire meaningful p-keys in a joint effort with domain experts. The right of Figure 4 shows the time for computing Armstrong PC-tables from the given sets of randomly created p-keys. It shows that Armstrong PC-computation hardly ever exceeded 1 second. The left of Figure 6 shows the graphical user interface of our visualization tool, developed in R . The input interface is shown on the left, and the output PC-table on the right. 6.2 Profiling Figure 6 shows the time for profiling p-keys from the given Armstrong PC-Figure 2 . Large input sizes will require more sophisticated techniques. probability by which keys shall hold on large volumes of uncertain data. The trated in Figure 5 .
 rithmic tools to reason about probabilistic keys. This can minimize the overhead in using them for data quality management and query processing. These applications are effectively unlocked by developing support for identifying the right marginal probabilities by which keys should hold in a given application domain.
 For this challenging problem, we have devel-oped schema-and data-driven algorithms that can be used by analysts to communicate more effectively with domain experts. The schema-driven algorithm converts any input in the form of an abstract set of probabilis-tic keys into an Armstrong PC-table that sat-isfies the input and violates all probabilistic keys not implied by the input. Analysts and domain experts can jointly inspect the Armstrong PC-table which points out any Such PC-tables may represent some exemplary data sets or result from changes inspection. Experiments confirm that the computation of Armstrong PC-tables efficiently computed from PC-tables of reasonable size.
 In future research we will apply our algorithms to investigate empirically the usefulness of our framework for acquiring the right marginal probabilities ing is the question whether PC-tables or p-relations are more useful. We will stipulation of upper bounds or other features.

