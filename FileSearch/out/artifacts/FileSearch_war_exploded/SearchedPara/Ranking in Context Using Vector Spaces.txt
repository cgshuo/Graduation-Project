 Categories and Subject Descriptors: H.3.3 Information Search and Retrieval: Retrieval models General Terms: Theory.
 Keywords: Information Retrieval in Context.

This paper presents a principled approach to the prob-lem of retrieval in context. The notion of basis of a vector space was introduced previously to describe context. Any basis vectors represents a distinct piece of context, such as time, space, or word meaning. A vector is generated by a basis just as an informative object or an information need is generated in a context. As a consequence a different ba-sis generates a different vector as a different context would generate different information needs or informative objects. Also the Vector Space Model (VSM) describes information needs and informative objects as query vectors and docu-ment vectors, respectively. However the VSM assumes that there is a unique basis, which is the set of versors and always generates the same vector provided the same coefficients. Thus a drawback of the VSM is that the vectors are insen-sitive to context, i.e. they are generated and ranked in the same way independently of the context in which information need and informative objects are. This paper also proposes a function to rank documents in context. Since a basis spans a subspace, which includes all the vectors of object being in the same context, the ranking function is a distance mea-sure between the document vector and the subspace. Even though ranking is still based on an inner product between two vectors, the basic difference is that projection and dis-tance depend on the basis, i.e. on the pieces of context and then ultimately on context. Since an informative object can be produced by different contexts, different bases can arise and then ranking can change.

Mathematically, object vector generation is given by x = p b 1 +  X  X  X  p k b k = x = B  X  p ,where B is a n  X  k ( k  X  n ) complex matrix and p is a k  X  1 real vector. The b  X  X  are independent vectors and as such form a basis B of a subspace L (
B )of C n . The basis generates all the vectors in L ( B )and every vector in it describes an informative object produced within the context described by B . It should be clear that every vector x in L ( B )is entirely contained in the subspace spanned by B . Any other vector of the vector space may not be entirely contained in the subspace L ( B )andmaybe more or less close to it. A vector y being quite close to L ( B ) would mean that the object described by it would be the context given by the keywords of selected relevant units, and these units have been assessed as relevant in a given context. According to this analogy, it is not at all unreason-able to hypothesize that, if a model is effective to perform IR in Context, then it is likely effective to perform RF too. In other words, effective RF is a  X  X ecessary X  condition to effectively retrieve documents in context  X  the term  X  X ec-essary X  ought not be taken in a strict mathematical sense. Since RF algorithms have been successfully tested in labo-ratory, it is even more natural to think about laboratory as the environment where algorithms for IR in Context have to be tested. Such an approach was adopted, for instance, in [3, 9], and has been chosen in this paper too.
The aim of the experiments was to explore the potentiali-ties of the ranking function. To evaluate the model, RF was employed as the main task, i.e. a situation in which some relevant documents are used to estimate a context matrix and a test collection was ranked against that matrix. The initial list of 1000 of top-ranked retrieved documents, say I , was used as source to select the relevant document and to compute the statistics which were necessary to compute con-text matrices and term weights. The TREC-7 and TREC-8 subcollections were used for the experiments of this paper. Data were pre-processed through stopword removal, Porter X  X  stemming, and then one-word index terms were stored; the initial rankings of I were weighted by the TF  X  IDF formula proposed in [6]. The runs of our research were performed on FBIS, FR94, FT and LA sub-collections illustrated in [8]. To generate a query, the title of a topic was used by thus aligning to the common practice of TREC experiments  X  description and narrative title were not used. Mean Aver-age Precision (MAP)  X  no thresholds are required, it can be explained as the area underneath the recall-precision curve, it is robust. [8] MAP is also a single measure which makes presentation more effective.

The cooccurrence matrix was computed from the evidence observed from the relevance evidence being available for each topic by using Explicit Relevance Feedback: The n top-ranked relevant documents were selected from I accord-ingly to explicit relevance feedback algorithms. The fulltext of the n top-ranked relevant documents were used as they were a series of contiguous text windows. Then ten-words text windows centered around a keyword i were selected to compute the cooccurence frequencies among all the key-words of the n top-ranked relevant documents. This way a symmetric cooccurrence matrix was obtained. Then the k principal eigenvectors b i  X  X  of the cooccurrence matrix were computed.

Two baseline runs were defined. Both baselines were RF algorithms, yet they differed for the term weighting formula used to rank the index terms to be used to augment the query and did not compute any cooccurrence nor eigenvector matrix. In particular, (i) TFW (Term Frequency Weight-ing): The n top-ranked relevant documents were selected from I and the terms occurring within them were ranked by frequency of occurrence. (ii) RTW (Robertson X  X  Term Weighting), a slightly modified version of the query expan-sion algorithm proposed in [4].

The experimental results are reported and illustrated in the following. The results have been varied by number ( n =1 , 5 , 10) of documents from which relevance evidence was extracted, and number ( k =5 , 10 , 20 , 50 , 100) of words extracted for relevance feedback. Table 1 reports MAP and
