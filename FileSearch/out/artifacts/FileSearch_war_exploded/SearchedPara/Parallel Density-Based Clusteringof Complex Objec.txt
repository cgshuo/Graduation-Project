 Density-based clustering algorithms like DBSCAN [1] are based on  X  -range que-ries for each database object. Thereby, each range query requires a lot of distance calculations. When working with comp lex objects, e.g. trees, point sets, and graphs, often complex tim e-consuming distance functions are used to measure similarity accurately. As these distance calculations are the time-limiting factor of the clustering algorithm, the ultimate goal is to save as many as possible of these complex distance calculations.

Recently an approach was presented for th e efficient density-based clustering of complex objects [2]. The core idea of th is approach is to integrate the multi-step query processing para digm directly into the clustering algorithm rather than using it  X  X nly X  for a ccelerating range queries. In this paper, we present a sophisticated parallelization of this approach. Similar to the area of join process-ing where there is an increasing interest in algorithms which do not assume the existence of any index structure, we propose an approach for parallel DBSCAN which does not rely on the pre-clustering of index structures.

First, the data is partitioned according to the clustering result carried out on cheaply computable distance functions. The resulting approximated clustering conservatively approximat es the exact clustering. The objects of the conservative cluster approximations are then distributed onto the available slaves in such a way that each slave has to cluster the same amount of objects, and that the objects to be clustered are close to each other. Note that already at this early stage, we can detect some noise objects which do not have to be transmitted to the local clients. In addition to the objects to be clustered by a client, we send some filter merge points to this client. These filter merge points are also determined based on approximated distance functions. (cf. Figure 1a).

Second, each client carries out the clustering independently of all the other clients. No further communication is necessary throughout this second step. The presented local clustering approach also takes advantage of the approximating lower-bounding distance functions. The d etected clusters and the detected exact merge point sets are then transmitted to the server (cf. Figure 1b).

Finally, the server determines the correct clustering result by merging the locally detected clusters. This final m erging step is based on the exact merge points detected by the clients. Based on the se merge points, cluster connectivity graphs are created. In these graphs, the nodes represent the locally detected clusters. Two local clusters are conn ected by an edge if a merge point of one cluster is a core object in the other cluster (cf. Figure 1c).

The remainder of this paper is organized as follows. In Section 2, we shortly sketch the work from the literature related to our approach. In Sections 3, 4 and 5, we explain the server-side partitioning algorithm, the client-side clustering al-gorithm, and the server-side merging of the results from the clients, respectively. In Section 6, we present a detailed experimental evaluation based on real world test data sets. We close the paper in Section 7 with a short summary and a note on future work.
 Complex Object Representations. Complex object repres entations, like high-dimensional feature vectors [3], vector sets [4], trees or graphs [5], are helpful to model real world objects accurately . The similarity between these complex object representations is often measur ed by means of expensive distance func-tion, e.g. the edit distance. For a more detailed survey on this topic, we refer the interested reader to [6].
 Clustering. Given a set of objects with a distan ce function on them, an interest-ing data mining question is, whether these objects naturally form groups (called clusters) and what these groups look like. Data mining algorithms that try to answer this question are called clustering algorithms. For a detailed overview on clustering, we refer the interested reader to [7].
 Density-Based Clustering. Density based clustering algorithms apply a local cluster criterion to detect clusters. Clu sters are regarded as regions in the data space in which the objects are dense, and which are separated by regions of low object density (noise). One of the most prominent representatives of this clustering paradigm is DBSCAN [1].
 Density-Based Clustering of Complex Objects. In [2] a detailed overview can be found describing several approaches for the efficient density-based clustering of complex object. Furthermore, in [2] a new approach was introduced which performs expensive exact distance computations only when the information pro-vided by simple distance computations is not enough to compute the exact clus-tering. In Section 4, we will use an adaption of this approach for the efficient clustering on the various slaves.
 Parallel Density-Based Clustering of Complex Objects. To the b est of our knowl-edge there does not exist any work in this area. The key idea of density-based clusterin g is that for each object of a cluster the neighborhood of a given radius  X  has to contain at least a minimum number of MinPts objects, i.e. the cardinality of the neighborhood has to exceed a given threshold. An object p is called directly density-reachable from object q w.r.t.  X  and MinPts in a set of objects D ,if p  X  X   X  ( q )) and |N  X  ( q ) | X  MinPts , where N  X  ( q ) denotes the subset of D contained in the  X  -neighborhood of q .The condition |N  X  ( q ) | X  MinPts is called the core object condition . If this condition holds for an object q ,thenwecall q a core object . Other objects can be directly density-reachable only from core objects. An object p is called density-reachable from an object q w.r.t.  X  and MinPts in the set of objects D , if there is a chain of objects p 1 ,...,p n , p 1 = q , p n = p , such that p i  X  D and p i +1 is directly density-reachable from p i w.r.t.  X  and MinPts . Object p is density-connected to object q w.r.t.  X  and MinPts in the set of objects D , if there is an object o  X  D such that both p and q are density-reachable from o . Density-reachability is the transitive closure of direct density-reachability and is not necessarily symmetric. On the other hand, density-connectivity is a symmetric relation. DBSCAN. A flat density-based cluster is defined as a set of density-connected objects which is maximal w.r.t. density-reachability. Thus a cluster contains not only core objects but also border obj ects that do not satisfy the core object condition. The noise is the set of objects not contained in any cluster. OPTICS. While the partitioning density-based clustering algorithm DBSCAN can only identify a flat clustering, the newer algorithm OPTICS [8] computes an ordering of the points augmented by the so-called reachability-distance .The reachability-distance basically denotes the smallest distance of the current object q to any core object which belongs to the current cluster and which has already been processed. The clusters detected by DBSCAN can also be found in the OP-TICS ordering when using the same parametrization, i.e. the same  X  and MinPts values. For an initial clustering with OPTICS based on the lower-bounding filter distances the following two lemmas hold.
 Lemma 1. Let C exact 1 ,...,C exact n be the clusters detected by OPTICS based on the exact distances, and let C filter 1 ,...,C filter m be the clusters detected by OPTICS based on the lower-bounding filter distances. Then the following statement holds: Proof. Let N filter  X  ( o )denotethe  X  -neighborhood of o according to the filter dis-tances, and let N exact  X  ( o )denotethe  X  -neighborhood according to the exact dis-tances. Due to the lower-bounding filter property N exact  X  ( o )  X  N filter  X  ( o )holds. Therefore, each object o which is a core object based on the exact distances is also a core object based on the lower-bounding filter distances. Furthermore, each object p which is directly density-reachable from o according to the exact distances is also directly density-reachable according to the filter functions. In-duction on this property shows that if p is density-reachable from o based on the exact distances, it also holds for the filter distances. Therefore, all objects which are in one cluster according to the exact distances are also in one cluster according to the appr oximated distances.
 Lemma 2. Let noise exact denote the noise objects detected by OPTICS based on the exact distances and let noise filter denote the noise objects detected by OP-TICS based on the lower-bounding filter distances. Then the following statement holds: Proof. An object p is a noise object if it is not included in the  X  -neighborhood of any core object. Again, let N filter  X  ( o )and N exact  X  ( o )denotethe  X  -neighborhood of o according to the filter distances and the exact distances, respectively. Due p/  X  N filter  X  ( o ), it cannot be included in N exact  X  ( o ), proving the lemma. Both Lemma 1 and Lemma 2 are helpful to partition the data onto the different slaves. Lemma 1 shows that exact cluster s are conservatively approximated by the clusters resulting from a clustering on the lower-bounding distance functions. On the other hand, Lemma 2 shows that exact noise is progressively approxi-mated by the set of noise objects resulting from an approximated clustering. For this reason, noise objects according to the filter distances do not have to be transmitted to the slaves, as we already know that they are also noise ob-jects according to the exact distances. All other N objects have to be refined by the P available slave processors. Let C filter 1 ,...,C filter m be the approximated clusters resulting from an initial clustering with OPTICS. In this approach, we assign P slave = m i =1 | C filter i | /P objects to each of the P slaves. We do this partitioning online while carrying out the OPTICS algorithm. At each time dur-ing the clustering algorithm, OPTICS knows the slave j having received the smallest number L j of objects up to now, i.e. the client j has the highest free capacity C j = P slave  X  L j . OPTICS stops the current clustering at two different event points: In the first case, a cluster C filter i of cardinality | C filter i | X  C j was completely determined. This cluster is sent to the slave j . In the second case, OPTICS determined C j more points belonging to the current cluster C filter i . These points are grouped together to a filter cluster C filter i,j . Then, we transmit the cluster C filter i,j along with the filter merge points M filter i,j to the slave j .The set M filter i,j can be determined throughout the clustering of the set C filter i,j and can be defined as follows.
 Definition 1 (filter merge points). Let C filter i be a cluster which is split during an OPTICS run into n clusters C filter i, 1 ,...,C filter i,n .Then,the filter merge { Thefiltermergepoints M filter i,j are necessary in order to decide whether objects o merge exact clusters in the fina l merge step (cf. Section 5). Each of the filter clusters C filter i,j is clustered independently on the exact distances by the assigned slave j . For clustering these filter clusters, we adapt the approach presented in [2], so that it can also handle the additional merge points M filter i,j . The main idea of the client-side clustering approach is to carry out the range queries based on the lower-bounding filter distances instead of using the expen-sive exact distances. Thereto, we do not use the simple seedlist of the original DBSCAN algorithm, but we us e a list of lists, called Xseedlist .The Xseedlist consists of an ordered object list OL .Eachentry( o, T, PL )  X  OL contains a flag T indicating whether o  X  C filter i,j ( T =C)or o  X  M filter i,j ( T = M). Each entry of the predecessor list PL consists of the following information: a predecessor o p of o , which is a core object already added to the current cluster, and the predecessor distance , which is equal to the filter distance d f ( o, o p ) between the two objects. The result of the extended DBSCAN al gorithm is a set of exact clusters C To expand a cluster C exact i,j,l we take the first element ( o, T, PL )from OL and set o p to the nearest predecessor object in PL .

Let us first assume that T =Cholds.If PL =NILholds,weadd o to C exact i,j,l , delete o from OL , carry out a range query around o , and try to expand the proceed as in the case where PL = NIL holds. If d o ( o, o p ) &gt; X  and length of PL &gt; 1 hold, we delete the first entry from PL .If d o ( o, o p ) &gt; X  and length of PL = 1 hold, we delete o from OL . Iteratively, we try to expand the current cluster C exact i,j,l by examining the first entry of OL until OL is empty. Let us now assume that T =Mholds.If PL =NILholds,weadd o to M i,j,l , delete o from OL , and try to expand the exact merge point set M If PL = NIL holds, we compute d o ( o, o p ). If d o ( o, o p )  X   X  , we proceed as in the case where PL = NIL holds. If d o ( o, o p ) &gt; X  and length of PL &gt; 1 hold, we delete the first entry from PL .If d o ( o, o p ) &gt; X  and length of PL = 1 hold, we delete o from OL . Iteratively, we try to expand the current exact merge point set M exact i,j,l by examining the first entry of OL until OL is empty. Obviously, we only have to carry out the merge process for those clusters C filter i which were split in several clusters C filter i,j . The client detects that each of these equal to 0, i.e. no exact cluste r is contained in the cluster C filter i,j . For each of the t exact clusters C exact i,j,l there also exists a corresponding set of exact merge Definition 2 (exact merge points). Let C filter i,j be a cluster to be refined on an exact cluster determined during the client-side refinement clustering. Then, { q  X  M filter i,j | X  p  X  C exact i,j,l : q is directly density-reachable from p } . Based on these exact merge point sets and the exact clusters, we can define a  X  X luster connectivity graph X .
 Definition 3 (cluster connectivity graph). Let C filter i be a cluster which exact cluster determined by slave j along with the corresponding merge point sets M graph for C filter i iff the following statements hold: Note that two clusters C exact i,j,l and C exact i,j ,l from the same slave j = j are never connected by an edge. Such a connection o f the two clusters would already have taken place throughout the refinement clustering on the slave j . Based on the connectivity graphs G i for the approximated clusterings C filter i ,wecandetermine the database connectivity graph .
 Definition 4 (database connectivity graph). Let C filter i be one of n ap-proximated clusters along with the corresponding cluster connectivity graph G i = ( V i ,E i ) . Then we call G =( The database connectivity graph is nothing else but the union of the connectivity graphs of the approximated clusters. Based on the above definition, we state the central lemma of this paper.
 Lemma 3. Let G be the database connectivity graph. Then the determination of all maximal connected subgraphs of G is equivalent to a DBSCAN clustering carried out on the exact distances.
 Proof. For each object o the client-side clustering determines correctly, whether it is a core object, a border object, or a no ise object. Note, that we assign a border object which is directly density-reachabl e from core objects of different clusters redundantly to all of these clusters. Therefore, the only remaining issue is to show that two core objects which are directly density-reachable to each other are in the same maximal connected subgraph. By induction, according to the definition of density-reachability, two clusters then contain the same core objects. Obviously, two core objects o 1 and o 2 are directly density-reachable if they are either in the edge of the database connectivity graph. Therefore, depth-first traversals through all of the connectivity graphs G i corresponding to a filter cluster C filter i create the correct clustering result where each subgraph corresponds to one cluster. In this section, we present a detailed experimental evaluation based on real-world data sets. We used CAD data represented by 81-dimensional feature vectors [3] and vector sets where each element consists of 7 6D vectors [4]. Furthermore, we used graphs [5] to represent image data. The used distance functions can be characterized as follows: (i) The exact distance computations on the graphs are very expensive. On the other hand, the filter is rather selective and can efficiently be computed. (ii) The exact di stance computations on the feature vectors and vector sets are also very exp ensive as normalization aspects for the CAD objects are taken into account [4, 3] . As a filter for the feature vectors we use their Euclidean norms [9] which is not very selective, but can be computed very efficiently. The filter used for the vector sets is more selective than the filter for the feature vectors, but also computationally more expensive. If not otherwise stated, we used 3,000 complex objects from each data set.

The original OPTICS and DBSCAN algorithms, their extensions introduced in this paper, and the used filter and exact distances functions were implemented in Java 1.4. The experiments were run on a workstation with a Xeon 2.4 GHz processor and 2 GB main memory. All exp eriments were run sequentially on one computer. Thereby, the overall time for th e client-side clustering is determined by the slowest slave. If not otherwise stated, we chose an  X  -parameter yielding as many flat clusters as possible, and the MinPts -parameter was set to 5. Characteristics of the partitioning step. Figure 3 compares the number of merge points for different split techniques applied to filter clusters. As explained in Section 3, we split a filter cluster during the partitioning step along the order-ing produced by OPTICS. Note that OPTICS always walks through a cluster by visiting the densest areas first. Figure 3 shows that this kind of split strat-egy yields considerably less merge points than a split strategy which arbitrarily groups objects from a filter cluster together. Thus, the figure proves the good clustering properties of our metric space filling curve OPTICS.
 Dependency on the Number of Slaves. Figure 4 shows the absolute runtimes of our parallel DBSCAN approach dependent on the number of available slaves for the vector sets and for the graph dataset. The figure shows the accumulated times after the partitioning, client-side clustering, and the merge step. The partitioning times also include simulated communication times for the transfer of the objects to the slaves in a 100 Mbit LAN. No communication costs arise from the client-side clustering step, as each client alread y received all needed filter merge points. A growing number of slaves leads to a significant speedup of the client-side clustering. A lower bound of the achievable total runtime is given by the time needed for the initial partitioning step. It is worth to note the time needed for the final merging step is negligible even for a high number of slaves. Although the number of exact merge points grows with an increasing number of slaves (cf. Figure 3), the merge step remains cheap.
 Speedup. Finally, Figure 5 depicts the speedup achieved by our new parallel DB-SCAN approach based on a server-side partitioning with OPTICS. We compared this approach to a DBSCAN approach based on a full table scan and compared to a DBSCAN approach based on the traditional multi-step query processing paradigm. The figure shows that for the feature vectors we achieve a speedup of one order of magnitude already when only one slave is available. In the case of the graph dataset we have a speedup of 67 compared to DBSCAN based on a full table scan. These results demonstrate the suitability of the client-side clustering approach. For the vector sets the benefits of using several slaves can clearly be seen. For instance, our approach achieves a speedup of 4 for one slave and a speedup of 20 for eight slaves compared to DBSCAN based on traditional multi-step range queries. In this paper, we applied the novel concept of using efficiently computable lower-bounding distance functions for the parallelization of data mining algorithms to the density-based clustering algorithm DBSCAN. For partitioning the data, we used the hierarchical clustering algorithm OPTICS as a kind of space fill-ing curve for general metric objects, which provides the foundation for a fair and suitable partitioning strategy. We showed how the local clients can carry out their clustering efficiently by integrating the multi-step query processing paradigm directly into the clustering algorithm. Based on the concept of merge points, we constructed a global cluster connectivity graph from which the fi-nal clustering result can easily be derived. In the experimental evaluation, we demonstrated that our new approach is able to efficiently cluster metric objects. We showed that if several slaves are available, the benefits achieved by the full computational power of the slaves easily outweigh the additional costs of par-titioning and merging by the master. In our future work, we will demonstrate that also other data mining algorithms can beneficially be parallelized based on lower-bounding distance functions.

