 The Self-Organizing Map (SOM) [1] is a valuable tool in data analysis. It is a popular unsupervised neural network algorithm that has been used in a wide range of scientific and industrial applications [3], like Text Mining [6], natural language processing and monitoring of the condition of industrial plants and processes. It provides several beneficial properties, such as vector quantization and topology preserving mapping from a high-dimensional input space to a two-dimensional output space. This projection can be visualized in numerous ways in order to reveal the characteristics of the input data or to analyze the quality of the obtained mapping.
 which induces a concept of proximity on the map. For each map unit, we compute a vector pointing to the direction of the most similar region in output space. We propose two methods of visualizing the results, a vector field plot, which can be seen analogous to flow visualization and gradient visualization, and a plot that emphasizes on the cluster structure of the map. The SOMs used for demonstration and experiments are trained on Fisher X  X  well-known Iris data. sualization techniques for SOMs and related work. Section 3 gives an overview of neighborhood kernel functions and their parametrization. In Section 4, our visualization method is introduced, along with a description of its properties and interpretations. Section 5 presents experimental results, where the the influence of choices of neighborhood kernel, neighborhood radius and map size are inves-tigated. Finally, Section 6 gives a short summary of the findings presented in this paper. In this section, we briefly describe visualization concepts for SOMs related to our novel method. The most common ones are component planes and the U-Matrix. Both take only the prototype vectors and not the data vectors into account. Component planes show projections of singled out dimensions of the prototype vectors. If performed for each individual component, they are the most precise and complete representation available. However, cluster borders cannot be easily perceived, and high input space dimensions result in lots of plots, a problem that many visualization methods in multivariate statistics, like scatterplots, suffer from. The U-Matrix technique is a single plot that shows cluster borders according to dissimilarities between neighboring units. The dis-tance between each map unit and its neighbors is computed and visualized on the map lattice, usually through color coding. Recently, an extension to the U-Matrix has been proposed, the U*-Matrix [8], that relies on yet another vi-sualization method, the P-Matrix [7]. Other than the original, it is computed by taking both the prototype vectors and the data vectors into account and is based on density of data around the model vectors. Interestingly, both the U*-Matrix and our novel method, among other goals, aim at smoothing the fine-structured clusters that make the U-Matrix visualization for these large SOMs less comprehensible, although the techniques are conceptually totally different. Other visualization techniques include hit histograms and Smoothed Data His-tograms [4], which both take the distribution of data into account, and projec-tions of the SOM codebook with concepts like PCA or Sammon X  X  Mapping, and concepts that perform labeling of the SOM lattice [6]. For an in-depth discussion, see [9].
 SOMs trained on the Iris data set with 30  X  40 and 6  X  11 map units, respectively. The feature dimensions have been normalized to unit variance. The U-Matrix reveals that the upper third of the map is clearly separated from the rest of the map. The hit histogram shows the projection of the data samples onto the map lattice. It can be seen that this SOM is very sparsely populated, because the number of map units is higher than the number of data samples. When the two methods are compared, it can be observed that the fine cluster structures in the U-Matrix occur exactly between the map units that are occupied by data points. It is one of the goals of this work to create a representation that allows a more global perspective on these kinds of maps and visualize it such that the intended level of detail can be configured.
 next section has not been used for visualization purposes. Apart from the SOM training algorithm the neighborhood function is applied in the SOM Distortion Measure [2], which is the energy function of the SOM with fixed radius, where the neighborhood kernel is aggregated and serves as a weighting factor comparable to the one we use in this paper. A particularly important component of the Self-Organizing Map is the concept of adjacency in output space, i.e. the topology of the map lattice, and its definition of neighborhood that affects the training process. Our visualization technique heavily depends on this neighborhood kernel as a weighting factor. The neigh-borhood kernel is a parameterized function that takes the distance between two map units on the lattice as input and returns a scaling factor that determines by which amount the map unit is updated for each iteration. The parameter the kernel depends on is the neighborhood radius  X  ( t ), which is itself a monotoni-cally decreasing function over time t .  X  controls the width of the kernel function, such that high values lead to kernels that are stretched out and low values result in sharply peaked kernels. In this work, we will not consider the radius as a function of time as the training process does, but rather as a parameter that has to be specified before the visualization can be applied.
 with increasing distance d input . This distance will be formally defined in the next section, but can be roughly envisioned as the number of units that lie between two map units. Examples of neighborhood kernels are the Gaussian kernel, the bubble function, and the inverse function. The Gaussian kernel is the most frequently used kernel for the SOM. It has the well-known form of the Gaussian Bell-Shaped Curve, formally Since the returned value is exponentially decreasing for higher values of d input , the effects on the training process are neglegible. Thus, the kernel is frequently modified to cut off the function at input values greater than  X  : ciple of cutting off at radius  X  . It is a simple step function, formally which shows a sharper decrease than the Gaussian kernel.
 of the kernel for the map unit located in the center, indicated by a black dot. Figure 2(e) shows a plot of the kernels as function of the distance between the units and fixed neighborhood radius. All the graphics use the same value of 6 for parameter  X  . In this section, we introduce our visualization technique for the SOM. Similar to the U-Matrix, only the prototype vectors and their pairwise similarities are investigated. In the U-Matrix, only the differences between direct neighbors are considered. We aim to extend this concept to include the region around the units according to the neighborhood kernel. Furthermore, we wish to obtain the direction for each unit where the most similar units are located. The resulting visualization is analogous to gradient vector fields where units are repelled from or attracted to each other.
 will consider has a two-dimensional lattice, consisting of a number M of map units p i , where i is between 1 and M . Each of the map units is linked to a model vector m i of input dimension N .Eachofthe m i is linked to the output space by its position on the map. To distinguish between feature space and map lattice, we explicitly write p i for the position vector of map unit that represents prototype vector m i ; the index i connects input and output space representation. We denote the horizontal and vertical coordinates of the map unit as p u i and p v i , respectively. Thus, the distance between two prototype vectors m i and m j ,or p and p j , can be determined both in input and output space: where || . || input is a suitable distance metric and which is the Euclidean Distance.
 positions on the map lattice d output ( p i ,p j ) as its input. This kernel function com-putes how much the prototype vectors influence each other during the training process. We will use it as a weighting function that allows us to compute the similarity (in terms of input space distance) of map units that are close to each other on the map.
 izations. A unit X  X  arrow points to the region where the most similar prototype vectors are located on the map. The length of this arrow shows the degree of how much the area it is pointing to is more similar to it than the opposite direction. be decomposed in u and v coordinates, denoted as a u i and a v i . For each of the two axes, we compute the amount of dissimilarity along positive and negative directions. Our method determines these vectors in a two-step process: First, the computations for each map unit are performed separately for the positive and negative directions of axes u and v , and finally, these components are aggregated by a weighting scheme to calculate the coordinates of a i .
 is defined in basic trigonometry as computed as Here, the influence of the neighborhood kernel is distributed among the two axes according to the position of p i and p j on the map and serves as a weighting factor in the following steps. The neighborhood kernel relies on the width parameter  X  , which determines the influence of far-away map units.
 direction for both axes for each pair of map units p i ,p j : where con u + denotes the contribution of map unit p j  X  X  dissimilarity in positive direction along u ,and con u  X  in negative direction. The definition of con v + and con v  X  follows analogously. For example, a map unit p j that lies to the lower right weighted through the neighborhood kernel, and also its distance in input space, which is directly measured by the factor d input .
 Again, diss v + and diss v  X  are defined analogously. The variable diss u + ( p i ) indi-cates how much m i is dissimilar from its neighbors on the side in the positive u direction. In a gradient field analogy, this value shows how much it is repelled from the area on the right-hand side.
 vector a i . Normalization has to be performed, because units at the borders of the map lattice would have components pointing outside of the map equal to zero, which is not intended. The sums of the neighborhood kernel weights w i pointing in positive and negative directions are and likewise for the v direction. The weighting factor w u + is multiplied with the component in the other direction to negate the effects of units close to the border in which case the sum of the neighborhood kernel is greater on one side. If this normalization would be omitted, the vector a would be biased towards pointing to the side where units are missing. For map units in the center of the map X  X  u -axis, where w u + and w u  X  are approximately equal, Equation (16) can be approximated by this simpler formula where  X  is a constant factor equal to w all units in the middle of an axis.
 are briefly described:  X  If negative and positive dissimilarities are roughly equal, the resulting com- X  If the positive direction is higher than the negative one, a will point into the  X  If one side dominates, but the second side still has a high absolute value, the trained on the Iris data set with a Gaussian kernel with  X  = 5. If compared to the U-Matrix in Figure 1(a), it can be seen that the longest arrows are observed near the cluster borders, pointing to the interior of their cluster and away from these borders. Adjacent units, for which the arrow points in different directions, are clearly along a cluster border. The length of the arrows indicates how sharp the border is. In the middle of these transitions, arrows are sometimes drawn with almost no distinguishable length or direction. The corresponding prototype vectors are likely to be very far away from either cluster, and are referred to as interpolating units, since they do not represent any data vectors in a vec-tor quantization sense, but are only a link connecting two distant data clouds. Cluster centers also have small dot-like arrows pointing in no distinguishable direction, but the difference is that the surrounding arrows are pointing in their direction, and not away from them. Another property of this visu alization is that the units on the edges of the map never point outside of it, which is desired and stems from the normalization performed in (16).
 also be depicted to show the cluster borders themselves with a slight modification in the representation by depicting not the direction of the gradient, but rather the hyperplane obtained by rotation of 90 degrees in either direction. In our case of a two-dimensional map lattice, the hyperplane is a one-dimensional line. We choose to depict this line with length proportional to the original arrow. The result is visualized in Figure 3(e). The emphasis of this dual representation is stressing cluster borders, while information on directions is omitted. tion with other visualization techniques, such as hit histograms and component planes. What can be learned from comparing the positions of the different Iris species to our method is that the class membership of the data samples corre-lates with the cluster structure in case of the Setosa species, while Versicolor and Virginica do not show a distinguishable separation. This is of course a well-known fact about the Iris data set, and application of our technique to more complex data is subject to further research and is addressed in [5]. In this section, we will investigate the empirical results of our method applied to SOMs of different sizes, as well as how the choice of parameter  X  influences the visualization, and the effects of different kernel functions.
 vectors. The data vectors remain the same for both maps. The smaller version of the SOM consists of 6  X  11 units, and the larger one of 30  X  40 units. In the latter case, the number of data vectors (150) is much lower than the number of map units (1200). The visualization for the smaller version is depicted in Figure 1(e). U-Matrix and vector field plots for the larger map are shown in Figures 1(a) and 3, respectively. In the smaller SOM the gap between the upper third part representing the well-separated Setosa species and the lower two-thirds of the map can clearly be distinguished, as in the larger SOM. However, the larger version of the SOM gives more insight into the structure of the data. Transitions and gradual changes in directions and length can be distinguished more easily at this higher granularity.
 In Figure 3, the large Iris SOM is visualized with three different values of  X  . Fig-ures 3(a), (d) show the two methods for  X  = 1. The visualization with this width is the one most closely related to the U-Matrix technique, since only distances be-tween direct neighbors are regarded, while the influence of slightly more distant units is neglected. Of all the visualizations shown here, these two are chiseled the most and are least smooth. The frequent changes in direction of neighboring arrows is due to the very local nature of this kernel. In Figures 3(b), (e) the visu-alization is shown for  X  = 5, where the increased neighborhood radius produces a smoothing effect over the vector field. Here, changes in direction between close arrows can be better distinguished and result in a visually more comprehensible picture. The set of arrows is perceived as a whole and as less chaotic. It gives the impression of visualizing a somewhat more global structure. Finally, the vi-sualization for  X  = 15 is depicted in Figures 3(c), (f), where only big clusters can be perceived. The effect of  X  can be summarized as follows: For a value of 1, the cluster representation is very similar to the U-Matrix, which is the method relying mostly on local differences. With higher values of  X  , the kinds of per-ceived cluster structures gradually shift from local to global. The choice of  X  has a deep impact on this visualization method and is dependant on the map size. Further experiments have shown that good choices are close to one tenth of the number of map units in the axis of the map lattice with fewer map units, but it also depends on the desired level of granularity.
 on the visualization. The examples in this paper so far are all performed with Gaussian kernels. Surprisingly, the differences to the inverse function and cut-off Gaussian kernel are so minimal that they are hardly distinguishable. The only exception is the bubble function, which is actually a very unusual choice for a neighborhood kernel during training. Since all the map units are treated equally within the sphere of this radius, and nodes on the borders of this circle are not weighted less than near the center, the visualization is harder to interpret than the other kernels. During training, cluster structures are introduced that are not present in the data set. We find that the bubble function is not appropriate for this kind of visualization, and conclude that the neighborhood kernel should be a continuous function. In this paper, we have introduced a novel method of displaying the cluster struc-ture of Self-Organizing Maps. Our method is distantly related to the U-Matrix. It is based on the neighborhood kernel function and on aggregation of distances in the proximity of each codebook vector. It requires a parameter  X  that de-termines the smoothness and the level of detail of the visualization. It can be displayed either as a vector field as used in flow visualizations or as a plot that highlights the cluster borders of the map. In the former case, the direction of the most similar region is pointed to by an arrow. Our experiments have shown that this method is especially useful for maps with high numbers of units and that the choice of the neighborhood kernel is not important (as long as it is continuous), while the neighborhood radius  X  has a major impact on the outcome.
 Part of this work was supported by the European Union in the IST 6. Frame-work Program, MUSCLE NoE on Multimedia Understanding through Seman-tics, Computation and Learning, contract 507752.

