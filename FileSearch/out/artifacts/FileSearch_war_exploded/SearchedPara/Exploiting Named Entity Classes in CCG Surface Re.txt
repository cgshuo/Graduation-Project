 Hogan et al. (2007) have recently shown that better handling of named entities (NEs) in broad coverage surface realization with LFG can lead to substan-tial improvements in BLEU scores. In this paper, we confirm that better NE handling can likewise im-prove broad coverage surface realization with CCG, even when employing a more restrictive notion of named entities that better matches traditional real-ization practice. Going beyond Hogan et al. (2007), we additionally show that NE classes can be used to improve realization quality through better lan-guage models and better hypertagging (supertagging for realization) models, yielding a state-of-the-art BLEU score of 0.8173 on Section 23 of the CCG-bank.

A question addressed neither by Hogan et al. nor anyone else working on broad coverage surface realization recently is whether reported increases in BLEU scores actually correspond to observable improvements in quality. We view this situation as problematic, not only because Callison-Burch et al. (2006) have shown that BLEU does not al-ways rank competing systems in accord with hu-man judgments, but also because surface realiza-tion scores are typically much higher than those in MT X  X here BLEU X  X  performance has been repeat-edly assessed X  X ven when using just one reference. Thus, in this paper, we present a targeted manual evaluation confirming that our BLEU score increase corresponds to a significant rise in fluency, a practice we encourage others to adopt. CCG (Steedman, 2000) is a unification-based cat-egorial grammar formalism defined almost en-tirely in terms of lexical entries that encode sub-categorization as well as syntactic features (e.g. number and agreement). OpenCCG is a pars-ing/generation library which includes a hybrid symbolic-statistical chart realizer (White, 2006). A vital component of the realizer is the hypertagger (Espinosa et al., 2008), which predicts lexical cat-egory assignments using a maxent model trained on contexts within a directed graph structure represent-ing the logical form (LF) input; features and rela-tions in the graph as well as parent child relation-ships are the main features used to train the model. The realizer takes as input an LF description (see Figure 1 of Espinosa et al., 2008), but here we also use LFs with class information on some elementary predications (e.g. @ x :MONEY ( $ 10,000 ) ). Chart re-alization proceeds in iterative beta-best fashion, with a progressively wider hypertagger beam width. If no complete realization is found within the time limit, fragments are greedily assembled. Alternative real-izations are ranked using integrated n-gram scoring; n-gram models help in choosing word order and, to a lesser extent, making lexical choices. An error analysis of the OpenCCG baseline output reveals that out of 2331 NEs annotated by the BBN corpus, 238 are not realized correctly. For exam-ple, multi-word NPs like Texas Instruments Japan Ltd. are realized as Japan Texas Instruments Ltd. . Inspired by Hogan et al. X  X  (2007) X  X  Experiment 1, we decided to use the BBN corpus NE annotation (Weischedel and Brunstein, 2005) to collapse cer-tain classes of NEs. But unlike their experiment where all the NEs annotated by the BBN corpus are collapsed, we chose to collapse into single tokens only NEs whose exact form can be reasonably ex-pected to be specified in the input to the realizer. For example, while some quantificational or com-paratives phrases like more than $ 10,000 are anno-tated as MONEY in the BBN corpus, in our view only $ 10,000 should be collapsed into an atomic unit, with more than handled compositionally ac-cording to the semantics assigned to it by the gram-mar. Thus, after transferring the BBN annotations to the CCGbank corpus, we (partially) collapsed NEs which are CCGbank constituents according to the following rules: (1) completely collapse the PER-SON, ORGANIZATION, GPE, WORK OF ART major class type entitites; (2) ignore phrases like three decades later , which are annotated as DATE entities; and (3) collapse all phrases with POS tags CD or NNP(S) or lexical items % or $ , ensuring that all prototypical named entities are collapsed. Going beyond Hogan et al. (2007) and collaps-ing experiments, we also experiment with NE classes in language models and hypertagging mod-els. BBN annotates both major types and subtypes (DATE:AGE, DATE:DATE etc). For all our experi-ments, we use both of these. 4.1 Class replaced n-gram models For both the original CCGbank as well as the col-lapsed corpus, we created language model training data with semantic classes replacing actual words, in order to address data sparsity issues caused by rare words in the same semantic class. For exam-ple, in the collapsed corpus, the Section 00 sen-tence Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 . be-comes PERSON , DATE:AGE DATE:AGE old , will join the ORG DESC:OTHER as a nonexecutive PER DESC DATE:DATE DATE:DATE . During re-alization, word forms are generated, but are then re-placed by their semantic classes and scored using the semantic class replaced n-gram model, similar to (Oh and Rudnicky, 2002). As the specific words may still matter, the class replaced model is interpo-lated at the word level with an ordinary, word-based language model, as well as with a factored language model over POS tags and supertags. 4.2 Class features in hypertagging We also experimented with a hypertagging model trained over the collapsed corpus, where the seman-tic classes of the elementary lexical predications, along with the class features of their adjacent nodes, are added as features. 5.1 Hypertagger evaluation As Table 2 indicates, the hypertagging model does worse in terms of per-logical predication accuracy &amp; per-whole-graph accuracy on the collapsed cor-pus. To some extent this is not surprising, as collaps-ing eliminates many easy tagging cases; however, a full explanation is still under investigation. Note that class information does improve performance some-what on the collapsed corpus. 5.2 Realizer evaluation For a both the original CCGbank and the col-lapsed corpus, we extracted a section 02 X 21 lexico-grammars and used it to derive LFs for the devel-opment and test sections. We used the language models in Table 1 to score realizations and for the collapsed corpus, we also tried a class-based hyper-tagging model. Hypertagger  X  -values were set for each corpus and for each hypertagging model such that the predicted tags per pred was the same at each level. BLEU scores were calculated after removing the underscores between collapsed NEs. 5.3 Results Our baseline results are much better than those pre-viously reported with OpenCCG in large part due to improved grammar engineering efforts and bug fix-ing. Table 3 shows development set results which indicate that collapsing appears to improve realiza-tion on the whole, as evidenced by the small increase in BLEU scores. The class-replaced word model provides a big boost on the collapsed corpus, from 0.7917 to 0.7993, much more than 4-grams. Adding semantic classes to the hypertagger improves its ac-curacy and gives us another half BLEU point in-crease. Standard test set results, reported in Table 4, confirm the overall increase, from 0.7940 to 0.8173.
In analyzing the Section 00 results, we found that with the collapsed corpus, NE errors were reduced from 238 to 99, which explains why the BLEU score increases despite the drop in exact matches and grammatically complete realizations from the base-line. A semi-automatic analysis reveals that most of the corrections involve proper names that are no longer mangled. Correct adjective ordering is also achieved in some cases; for example, Dutch publish-ing group is enforced by the class-replaced models, while all the other models realize this as publishing Dutch group . Additionally, the class-replaced model sometimes helps with animacy marking on relative pronouns, as in Mr. Otero , who . . . instead of Mr. Otero , which . . . . (Note that our input LFs do not directly specify the choice of function words such as case-marking prepositions, relative pronouns and complementizers, and thus class-based scoring can help to select the correct surface word form.) 5.4 Targeted manual evaluation While the language models employing NE classes certainly improve some examples, others are made worse, and some are just changed to different, but equally acceptable paraphrases. For this reason, we carried out a targeted manual evaluation to confirm the BLEU results. 5.4.1 Procedure
Along the lines of (Callison-Burch et al., 2006), two native speakers (two of the authors) provided ratings for a random sample of 49 realizations that differed between the baseline and best conditions on the collapsed corpus. Note that the selection pro-cedure excludes exact matches and thus focuses on sentences whose realization quality may be lower on average than in an arbitrary sample. Sentences were rated in the context of the preceding sentence (if any) for both fluency and adequacy in compari-son to the original sentence. The judges were not aware of the condition (best/baseline) while doing the rating. Ratings of the two judges were averaged for each item. 5.4.2 Results
In the human evaluation, the best system X  X  mean scores were 4.4 for adequacy and 3.61 for fluency, compared with the baseline X  X  scores of 4.35 and 3.36 respectively. Figure 1 shows these results including the standard error for each measurement, with the BLEU scores for this specific test set. The sample size was sufficient to show that the increase in flu-ency from 3.36 to 3.61 represented a significant dif-ference (paired t-test, 1-tailed, p = 0.015), while the adequacy scores did not differ significantly. 5.4.3 Brief comparison to related systems
While direct comparisons cannot really be made when inputs vary in their semantic depth and speci-ficity, we observe that our all-sentences BLEU score of 0.8173 exceeds that of Hogan et al. (2007), who report a top score of 0.6882 (though with coverage near 100%). Nakanishi et al. (2005) and Langkilde-Geary (2002) report scores of 0.7733 and 0.7570, re-spectively, though the former is limited to sentences of length 20 or less, and the latter X  X  coverage is much lower. In this paper, we have shown how named entity classes can be used to improve the OpenCCG re-alizer X  X  language models and hypertagging models, helping to achieve a state-of-the-art BLEU score of 0.8173 on CCGbank Section 23. We have also con-firmed the increase in quality through a targeted manual evaluation, a practice we encourage others working on surface realization to adopt. In future work, we plan to investigate the unexpected drop in hypertagger performance on our NE-collapsed cor-pus, which we conjecture may be resolved by taking advantage of Vadas and Curran X  X  (2008) corrections to the CCGbank X  X  NP structures. This work was supported in part by NSF IIS-0812297 and by an allocation of computing time from the Ohio Supercomputer Center. Our thanks also to Josef Van Genabith, the OSU Clippers group and the anonymous reviewers for helpful comments and discussion.

