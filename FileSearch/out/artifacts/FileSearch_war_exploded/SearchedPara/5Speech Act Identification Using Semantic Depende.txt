 Spoken language understanding is one of the most core technologies in conversational dialogue systems and can be defined operationally as  X  X hen a computer we interact with understands our desires and delivers the goods X  [Huang et al. 2001]. Capturing the semantic meaning of speakers is one of the objectives in spoken language under-standing. For conversational dialogue systems, speech act classification schemes can facilitate identifying a speaker X  X  intent regarding an utterance. A speech act described in this article is thus approximately equivalent to that described by Searle [1979]. Prac-tical applications of speech act theory in spoken language processing have produced both insight into and a deeper understanding of verbal communication [Wu and Yan 2005]. Before initiating a detailed discussion on spoken language understanding, we must clarify our central conception of speech act identification. For a token to be an instance of communication, the audience must consider the token as being produced by a being with certain intentions that are relevant to the speaker X  X  intention. The core ob-jective of spoken language understanding is speech act identification. Previous studies have explored automatically inferring speech acts in various media, including message board posts [Qadir and Riloff 2011], Wikipedia talk pages [Ferschke et al. 2012], Twit-ter [Zhang et al. 2012], and situated social agents in games [Orkin and Roy 2010]. In the past decades, growing emphasis has been placed on research on the semantic rela-tionships between words within sentences or utterances, especially in computational linguistics and spoken language processing. Sag et al. [2002] reported that multiword expressions are used in natural language processing and machine translation. Wei et al. [2014] used a fusion approach that was developed based on psychological fac-tors to recognize Interaction Style (IS) for achieving harmonious interaction between computers and humans. They classified in terms of lexicalized phrases and institu-tionalized phrases. It achieved favorable performance in written language processing, whereas the absence of semantic relationships causes understanding to be difficult in spoken language processing. Calzolari et al. [2002] defined the term  X  X ultiword expression X  as the marginal role of idiosyncratic lexical information, and this term has been addressed using only specific types of word combinations including fixed or semifixed phrases, compounds, support verbs, idioms, phrasal verbs, and collocations. Because of the increasing usage of spoken language understanding, requirements for natural language processing have become more critical in multimedia human X  X achine interaction, especially in speech interfaces such as spoken dialogue systems. One of the key challenges in the field of conversational dialogue systems is enabling machines to understand natural spoken language [Huang et al. 2001; Allen et al. 2001]. Although the dialogue management frameworks in these systems can manage the dialogue flow effectively, these frameworks cannot be applied to complex applications such as dy-namic multiple domain switching dialogue systems. Users can switch directly from one ongoing task to another task, but, because the systems currently in operation lack pre-cise speech act identification and do not enable discourse analysis, such task switching fails.  X  X or semantic representation, semantic objects are used to represent real-word en-tities. Here, we assume that domain knowledge conforms to a relational or objected-oriented database, of which the schema is clearly defined X  [Huang et al. 2001]. The capability to identify speech acts and extract semantic objects by reasoning is thus a substantial objective for the next generation of dialogue systems. To facilitate achiev-ing this objective, this article proposes a semantic dependency-based discourse model to capture and share semantic objects among tasks that switch during a dialogue. In addition to acoustic speech recognition, natural language understanding is one of the most critical research topics in this field, because understanding and application re-stricted to a small scope are related to data structures that are used to capture and store meaningful items effectively. An example of this approach is the case in which Microsoft applied an object-oriented concept to provide a new semantic representation that included a semantic class and a learning algorithm for combining context-free grammars (CFGs) and N-grams [Wang and Acero 2003]. However, when the syntax and semantics are not analyzed and the system size is increased, systems, especially multidomain dialogue systems, do not operate favorably. The syntax and semantics pre-vent the system from operating favorably when the system size is increased, especially for multidomain dialogue systems.

Two key objectives concerning dialogue management in natural language processing are addressed in this article. The first objective is to determine how to develop a more effective speech act identification approach that facilitates semantic understanding, and the second objective is to determine how to obtain a semantic object from users X  utterances. Speech act identification featuring semantic interpretation is one of the most vital topics regarding the methods used to control dialogue with users. To capture the information among words within utterances to detect speech acts, dependency relationships among words are used here X  X xpressions that are composed of at least two words and can be syntactically and semantically idiosyncratic. Pinault and Lefevre [2011] applied semantic graph clustering in a partially observable Markov decision process (POMDP)-based approach and achieved favorable performance. Some research efforts were invested in their approaches, those involve integrating semantic dependency graphs and discourse information [Kudo and Matsumoto 2000; Hacioglu et al. 2003] to identify speech acts. Three major components, namely, semantic relationship, semantic class, and semantic role, are adopted in the semantic dependency graph [Henderson et al. 2013; Gildea and Jurafsky 2002; Hacioglu and Ward 2003; Gao and Suzuki 2003]. Wu et al. [2007a] presented a framework that integrated speech recognition confidence, prosody, word significance, word trigram, and semantic dependency scores for speech sentence compression and obtained excellent an compression ratio and kept the semantic and syntactic information in original speech. Semantic relationships constrain the word sense and provide a method for disambiguation. Semantic roles are assigned when relationships are established among semantic objects. Both semantic relationships and roles are defined in many knowledge resources and ontologies [Liu 2010], such as FrameNet [Baker et al. 1998] and HowNet (http://www.keenage.com/html/e_index.html). Semantic class is generally defined as a set containing elements that are usually words with the same semantic interpretation. Hypernyms that are superordinate concepts of the words, such as the hypernyms of synsets in WordNet (http://www.cogsci.princeton.edu/  X  wn/) and definitions of words X  primary features in HowNet, are generally used as semantic classes. In addition, this approach is used to identify implicit semantic dependency and the dependency structure between concepts in an utterance. Moreover, compared with a semantic frame or slot, a semantic dependency graph can provide more information that facilitates the understanding of dialogue.

As with the investments described previously, numerous attempts have been made by scholars to demonstrate the demand for spoken language understanding in spoken dialogue systems. Considering the practice of dialogue systems, three trends must be conducted here. For one thing, the development of a spoken dialogue system must aim at integrating multidomain applications. What is more, the knowledge-based system or ontology should be integrated in spoken language understanding to enhance the background knowledge. One final point is how to integrate the syntactic and semantic analysis and different level features such as the feature-extracted word, sentence, and discourse levels for grammatical ill-formed spontaneous utterances. Therefore, this article proposed a novel understanding approach based on semantic dependency graphs to identify speech acts by extracting the semantic relationships between words within utterances. For obtaining the syntactic structural information, HowNet and PCFGs are adopted to deal with the spoken utterance. Our contributions involved using a semantic dependency graph to identify speech acts and fill the values of semantic slots. Thus, the proposed approach enables a spoken language understanding to be achieved. The robust syntactic analysis and semantic interpretation are integrated. Actually, PCFGs were used to tokenize the phrases and chunk from ill-formed utterance [Wu et al. 2007b]. Based on the parsing results obtained by using PCFGs, we focused on extracting semantic dependence relationships from semantic graphs. Dialogue corpora were collected from multiple domains in practice, and Wizard of Oz was used to gather data used for evaluating the proposed approach.

The remainder of this article is organized as follows. Relevant literature is described in Section 2. Section 3 illustrates the framework of the proposed method, and Section 4 then describes the speech act identification performed using a semantic dependency graph combined with discourse information. Experiments conducted to evaluate semantic dependency graph approaches are presented in Section 5, and concluding remarks are provided in Section 6. For the development of spoken dialogue systems, a spoken language understanding module is one of the key components used to identify speech acts and fill domain-specific semantic frame slots from input utterances. Spoken dialogue systems specific to a single domain, such as those illustrated by Jur  X  c  X   X   X  cek et al. [2011] and Williams [2011], have been implemented in practice for assessments. Multidomain dialogue systems have recently been employed to support dynamic task switching. These systems generally use a distributed architecture that features extensibility and scalability. Pakucs [2003] introduced SesaME, a generic dialogue management framework that was designed specifically for supporting dynamic multidomain dialogue systems. SesaME supports numerous highly distributed applications and facilitates simultaneous content-based adaptation to provide dynamic multidomain dialogue processing. A multidomain spo-ken dialogue system must exhibit extensibility and robustness against speech recog-nition errors [Komatani et al. 2006]. Considering practical usage, Lee et al. [2009] proposed example-based dialogue modeling applicable in a multidomain dialogue sys-tem based on an object-oriented architecture of a situation-based dialogue system for services and car navigation domains. Nakano et al. [2011] investigated a two-stage domain selection framework for extensible multidomain spoken dialogue systems. Lee et al. [2013] developed an unsupervised spoken language understanding framework for a multidomain dialogue system and applied a nonparametric Bayesian approach to dialogue acts, intents, and slot entities, which are the components of a semantic frame. Petukhova and Bunt introduced an incremental understanding approach for spoken dialogue systems [2011]. For conversational dialogue systems that feature multiple tasking, the complexity of semantic object extraction is greater than that of tradi-tional systems. Williams [2010] used incremental partition recombination to solve the problem that occurs because the number of semantic slots grows exponentially as the dialogue length increases. Gasic and Young [2011] also provided an effective solution, the partially observable Markov decision process (POMDP), to predict the dialogue states by reward function obtained from the interactions between system and user and achieved a good performance. Crook and Lemon [2011] proposed lossless value-directed compression of complex user goal states to simplify the belief space for sta-tistical approaches. Besides, they also presented an automatic belief compression for POMDP-based approaches, and the evaluations by real users were illustrated in Crook and Lemon [2014]. Misu et al. [2012] applied reinforcement learning technologies to dialogue management optimization and applied it to museum navigation. Pietquin et al. [2011] proposed a sample-efficient, online, and off-policy learning algorithm for dialogue management policy optimization, namely, the Kalman temporal differences (KTD) algorithm. Furthermore, many efforts were invested in the optimization for dia-logue management. Schatzmann et al. [2006] and Ai and Litman [2011] both proposed statistical user simulation technique dialogue management strategies and obtained a significant improvement. For the aim toward self-improving dialogue systems, the user simulation was adopted in Lee and Eskenazi [2012]. Data-driven and adapta-tion methods for adaptive spoken dialogue systems are one of the core technologies for spoken language understanding. Data-driven grammar optimization was used for industry dialogue development in Hastie [2012]. Sun and Morency [2012] adopted an effectiveness of speaker adaptation for dialogue act recognition in multiparty meetings. Active learning was used for dialogue act classification [Gamb  X  ack et al. 2012]. Griol et al. [2014] developed a domain-independent statistical methodology to develop dialogue managers for spoken dialogue systems.

The understanding of spontaneous language is arguably the core technology of spo-ken dialogue systems, because the more accurate the information obtained by the machine is, the greater the possibility of successfully completing the dialogue task [Higashinaka et al. 2004; Selfridge and Heeman 2012]. A language understanding module typically performs segmentation, lexical analysis, parsing, semantic analysis, and pragmatic analysis. One of the concerns regarding language understanding is de-termining speaker intentions [Petukhova and Bunt 2011; Geertzen 2009; Dielmann and Renals 2008]. Petukhova and Bunt introduced an incremental dialogue act un-derstanding scheme and provided tagging and related experiments. Numerous recent studies have addressed speech act identification. When considering discourse as a whole, the relationship between the speech acts of dialogue turns is important. In the past decade, several practical dialogue systems [McTear 2002] have been developed to extract users X  semantic entities by using semantic frames or slots and conceptual graphs, providing various services such as air travel information systems (ATISs), weather forecast systems, automatic banking, and automatic train timetable informa-tion. Kim et al. [2012] conducted dialogue act classification for multiparty online chat services and determined that speech act identification is one of the most considered for spoken language understanding [Kang et al. 2010]. Webb et al. [2005] proposed an approach to dialogue act classification that uses intrautterance features derived from N-gram cue phrases, and the approach performed moderately well when applied to conversational utterances extracted from a switchboard corpus. Regarding disfluency in spontaneous speech, Wu and Yan [2005] developed a speech act classification and verification method. To address speech recognition errors, Yeh et al. [2008] applied a partial pattern tree to identify the speech acts of a speaker X  X  utterances in a spoken dialogue system, because speech act segmentation is a major process in speech act iden-tification. Guz et al. [2010] used lexical and prosodic features to segment both sentence and speech acts. Similarly, considering the understanding of spontaneous speech, Yeh and Yan [2012] adopted prosodic features to detect word fragments, and Cuayahuitl et al. [2013a, 2013b] considered the barge-in effect in Bayesian dialogue act recognition. Dielmann and Renals [2008] developed an approach for segmenting and classifying di-alogue acts in multiparty meetings by using a switching dynamic Bayesian network (BN) architecture with features related to lexical content and prosody. Laskowski and Shriberg [2010] compared the contributions of context and prosody in text-independent dialogue act recognition and observed that the prosodic features provided substantial improvements in dialogue act boundary detection, particularly when keywords were unavailable. Rossignol et al. [2011] trained a BN-based user model for simulating dialogue when data are missing.

Sridhar et al. [2008a] used prosodic features as vital cues for identifying dialogue acts by using the maximum entropy model and applied dialogue acts incorporating discourse context in spoken language translation. Pinault et al. [2009] proposed feature-based summary space for stochastic dialogue modeling with hierarchical semantic frames, and Wu et al. [2011] developed interrupt point detection using intersyllable boundary-based prosodic features. Kennington and Schlangen [2012] introduced Markov logic networks for situating incremental natural language understanding, and O X  X hea et al. [2012] proposed a multiclassifier approach to dialogue act classification using function words. Zhou and Zong [2009] recognized dialogue acts according to information on discourse and sentence structure. Peldszus et al. [2012] developed an approach that involves using syntactic and pragmatic constraints to improve incremental spoken language understanding [Blakemore 1992]. Kluwer et al. (2010) used syntactic-and semantic-based relationships to recognize dialogue acts. Discourse in speech acts has been extensively studied previously; specifically, Stolcke et al. [2000] applied hidden Markov models (HMMs) in dialogue act modeling to automatically tag and recognize conversational speech. Yong et al. [2013], Gasic and Yong [2014], and Crook et al. [2014] investigated dialogue management conducted by applying POMDP approaches. Li et al. [2014] adopted the probabilistic context-free grammars to capture user preferences. Table I summarizes the related works from the two aspects: difference unit and main contributions in feature extraction, model, and evaluation/assessment. The proposed system framework for identifying speech acts and extracting the semantic objects embedded in speakers X  utterances is shown in Figure 1. The framework consists of three procedures: a spoken interface, semantic and syntactic analysis, and knowledge extraction and understanding.
 A spoken utterance produced by a user is first fed into the speech recognition engine. According to the pretrained HMM-based acoustic models, the input speech signal is converted into phonetic symbols. The phonetic symbol sequences are then used to form words and word graphs by using statistical language models. Because possible word sequences are listed in the word graph after speech recognition is performed, syntactic and semantic analyses are used for speech act identification and semantic object ex-traction. Because the structural information in a sentence in natural language is rich, PCFGs are used to generate possible parse trees. By performing the parsing procedure, a hierarchical structure, such as the structure of phrases from a flat sentence, can be obtained. Based on the results of the syntactic analyzer, the semantic dependence analyzer extracts the relationships among words in the same word sequence. In this study, the ontology HowNet was employed as the knowledge base for semantic analy-sis. The semantic dependence analysis with PCFGs is used to generate the semantic dependence graph, which is then input into the knowledge extraction process and an understanding procedure that consists of speech act identification and semantic ob-ject extraction. Because historical information from discourse is important for speech act identification, the speech act sequence of previous utterances in the queue is re-tained. Based on the possible speech acts, the desired semantic objects embedded in the utterance are extracted. Because of the traditional slot filler structure, the values of the semantic slot are reset when the task switches in conventional spoken dialogue systems. By contrast, semantic objects can be shared among speech acts during the task switch in the proposed multidomain dialogue systems. Figure 2 illustrates an instance of the semantic object sharing in task switching. A partial transcription of spoken dialogue is shown on the left side and consists of three speech acts, namely,  X  X r. and clinic information, X   X  X r. X  X  information, X  and  X  X egistration, X  for different tasks, specifically medical FAQ mining and registration [Yeh et al. 2008]. The values of the semantic objects  X  X nternal medicine X  and  X  X r. Huang X  were respectively extracted dur-ing the speech acts  X  X r. and clinic information X  and  X  X r. X  X  information X  for task-medical FAQ mining. When the task is switched to  X  X egistration, X  these semantic objects can be shared among various tasks according to the discourse information retained in a semantic dependence graph. The status for semantic object extraction is shown on the lower right side of Figure 2.

The main contributions of this approach are speech act identification and semantic object extraction conducted by using semantic syntactic analyses in a conversational dialogue system. In the following sections, additional details are provided about the process of knowledge extraction and understanding regarding speakers X  utterances. Because speech act theory was developed to extract the functional meaning of an ut-terance in a dialogue [Searle 1979], in this study, discourse/history was defined as a act theory can be adopted for discourse modeling. According to this definition, discourse analysis is applied in semantics by using dependency graphs to identify the speech act sequence of the discourse. Discourse modeling using speech act identification consid-ering the history is shown in Equation (1). By introducing the hidden variable D i , representing the i th possible dependency graph derived from the word sequence W , the probability of hypothesis SA t given word sequence W and history SA t can be de-scribed in Equation (1). According to Bayes X  rule, the speech act identification model can be decomposed into two components, P ( SA t | D i , W , H t  X  1 )and P ( D i | W , H t  X  1 ), as described in the following equation: where SA  X  and SA t are the most probable speech act and the potential speech act at the t th dialogue turn, respectively. The term W ={ w 1 ,w 2 ,w 3 ,...,w m } denotes the word sequence extracted from the user X  X  utterance without stop words, and H t  X  1 is the history representing the previous t  X 1 turns. To combine semantic and syntactic structures, the relationships defined in HowNet were employed as the dependency relationships, and the hypernym was adopted as the semantic concept according to the primary features of the words defined in HowNet. The headwords were determined using an algorithm based on the part of speech (POS) proposed by Academia Sinica in Taiwan [Chen et al. 2001; Chen et al. 2010]. The probabilities of the headwords were estimated according to the PCFG trained on the Chinese Treebank developed by Academia Sinica (Chen et al. 2001). In other words, the headwords were extracted according to the syntactic structure, and the semantic dependency graphs were constructed based on the semantic relationships defined in HowNet.

Usually events of disfluency in spontaneous speech, such as repairs, indicate that a traditional parser without probability is not suitable for parsing a conversational spo-ken sentence. To obtain greater tolerance for ill forms in spoken utterances, the parser was adopted in this study to capture the partial structural information embedded in spontaneous speech. Because of the structural differences between utterances with the same speech act, especially at the syntax and phrase levels according to observations of spontaneous speech, PCFGs were adopted as the robust parser. A CFG is a quintuple CFG = (N, ,G,S) , where N is a finite set of nonterminal terms that specifies basic syntactic categories; for example, S represents  X  X entence or utterance, X  NP represents  X  X oun phrase, X  and VP represents  X  X erb phrase. X  The term represents a finite set of terminal symbols; in this article, words w within the spoken utterance W are the terminal symbols; G is a finite set of grammar rules; and S is a distinguished start symbol that usually denotes a spoken utterance or sentence.
 To construct the grammar rules and estimate the corresponding parameters in PCFG s, the Sinica Treebank (http://turing.iis.sinica.edu.tw/treesearch), which consists of 61,087 syntactic tree structures, was used as the training data. The structural representation in the training corpus contains semantic and syntactic tags provided by experts. According to the head-driven principle, one headword for each sentence and phrase represents the main syntactic component of each sentence and phrase. Relationships are then constructed between the headword and other words within the sentence and phrase according to the corresponding headword.
 To address grammatical ill form in spontaneous speech and speech recognition errors, PCFG s are used to capture syntactic meaning, especially structural information. As shown in Figure 3, the core component of the example sentence that contributes to the ill form,  X  X  want to have an internal medicine doctor X  X  diagnosis, X  the Chinese sentence is first decomposed as NP and VP . It is the event driven by the head, the verb phrase  X  X ant to. X  According to the definition of the part of speech (POS) in Chinese, a word that belongs to the word category VE2 should have two arguments: the agent and goal. The agent is the entity with the capability to perform an action; in this example,  X  (I) X  is the agent of the headword  X  (want to). X  The goal is the object or content of the headword  X  (want to), X  here  X  (have an internal medicine doctor X  X  diagnosis) X  An imperative clause is the goal of the entire sentence, which is driven by the headword classified as VF2  X  (have). X  This imperative clause contains two arguments: the goal and theme of the headword  X  (have). X  In Chinese, the possessive case of a noun is constructed by adding  X  (DE) X  after the noun. However,  X  (DE) X  is usually excluded in spoken language. Therefore, in the example, the goal is a noun phrase composed of  X  (internal medicine) X  and  X  (doctor X  X ). X  Finally, the verb phrase  X  (diagnosis) X  serves as the theme in the imperative clause. According to hierarchical structural information, one input utterance or sentence can be interpreted in multiple ways. The sentence can be interpreted as  X  X  want to have ... diagnosis, X  and  X  X  want to have ... . X  Thus, more useful expanded patterns are obtained. According to the definition in Chomsky normal form, the nonterminal terms are transformed into two cascading nonterminal terms or one terminal string in CFG s. The final parser tree is described as Figure 3 shows. The inside-outside algorithm [Baker 1979; Lari and Young 1990; Goodman 1998] is used to estimate the probabilities and, more specifically, to enable the word sequence within the spoken utterance, W = w 1 w 2 ...w n ,tobe generated by the grammar rules defined in PCFG s. To explore speech act identification further, semantic dependency relationships and discourse analysis is used to identify speech acts. Because D i is the i th possible depen-dency graph derived from word sequence W , speech act identification with semantic dependency can be simplified as shown in Equation (2): Because the history is defined as the speech act sequence, the joint probability of D i and H t  X  1 is given as the speech act SA t expressed as Equation (3). Because the data in the training corpus were sparse, the probability P ( SA t | D i , SA 1 , SA 2 ,..., SA t  X  1 )was difficult to estimate; thus, the speech act bigram model was conditionally independent and adopted to conduct approximation: Discourse information is accumulated in the semantic content from the first turn to the previous turn as the flow of information in dialogue incrementally. However, consider-ing the speech act analysis of current turn for backward dependency with discourse are known as the joint event D i and SA t  X  1 . Therefore, for obtaining the semantic depen-dency resulting from the words in current utterance and historical terms in discourse, we use the transpose of SA t  X  1 , SA t to yield the specific dependences between t th utter-ance and discourse that should be accompanied with a given consequence. By Bayes X  rule, the probability P ( SA t | D i , H t  X  1 ) can be formulated as Equation (4): Hence, the t th utterance to find the corresponding semantic dependences is estimated as the conditional probability from this aspect. SA t and D i mean the speech act of the t th turn and i th possible dependency graph derived from the t th utterance. Regarding the combination of semantic and syntactic structures, the relationships defined in HowNet were employed as the dependency relationships, and the hypernym was adopted as the semantic concept according to the primary features of the words defined in HowNet. The headwords were determined using an algorithm based on the POS proposed by Academia Sinica. The probabilities of the headwords were estimated according to the PCFGs trained using the Treebank developed by Academia Sinica (Chen et al. 2001). In other words, the headwords were extracted according to the syntactic structure, and the dependency graphs were constructed according to the semantic relationships defined in HowNet.

According to the word sense, some events are defined in HowNet to illustrate the relationships among words appearing in the event. Each word related to the event is annotated using the semantic dependence relationship for the concept defined in HowNet, as shown in Figure 4. For example,  X  X tethoscope X  and  X  X octor X  are the  X  X ool X  and  X  X gent, X  respectively, for  X  X ake a diagnosis. X  Some words with different relation-ships join events; for example,  X  X atient X  is the patient for  X  X ake a diagnosis, X  and  X  X uffer from X  is the experience of this patient. Finally, the dependence relationships constructed by combining the PCFG and the semantic dependency graph defined in HowNet are used to identify the speech act. The semantic dependence relationships among words in the input utterance are extracted by performing two steps. First, the headwords are estimated using PCFG s by performing a syntactic analysis. The depen-dence relationships are then determined according to the events defined in HowNet. The dependency relationship, r k , between word w k and headword w kh is extracted us-ing HowNet and denoted as DR ( w k ,w kh )  X  r k . The dependency graph, which is com-posed of a set of dependency relationships in the word sequence W , is defined as D vious definition involving the assumption of independence and bigram smoothing of the speech act model using the back-off procedure, Equation (4) can be rewritten into Equation (5): P ( D i , SA t  X  1 | SA t ) =  X  where  X  is the mixture factor for optimization.

Instead of the word, the corresponding superordinate according to semantic class based on a primary feature defined in Hownet is used for semantic graph constructing. According to the conceptual representation of the word, the transformation function, f (  X  ), transforms the word into its hypernym, which is defined as the semantic class by using HowNet. The dependency relationship between the semantic classes of two words is mapped to the conceptual space. In addition, the semantic roles among the dependency relationships are obtained. In the condition in which SA t , SA t  X  1 and the relationships are independent, and the equation becomes mated according to Equations (7) and (8), respectively: where C (  X  ) represents the number of the event in the training corpus. According to the definitions in Equations (7) and (8), Equation (6) becomes practicable. From another aspect, the discourse can be expressed as a bag of words H t  X  1  X  W t  X  1 1 = {
W 1 , W 2 ,... W t  X  1 } , and the probability that defines semantic dependency analysis using the word sequence and discourse can be rewritten as follows: and Because several dependency graphs can be generated from the word sequence W ,by introducing the hidden factor D i , the probability P ( W , W t  X  1 1 ) can be considered the sum of the probability P ( D i , W , W t  X  1 1 ), as shown in Equation (11): Because D i is generated from W , D i is sufficient for semantically representing W . The joint probability P ( D i , W , W t  X  1 1 ) can be estimated based on the dependency rela-tionships D i . That is, the dependency graph D i is determined mainly based on W including the related relations that resulted from W. Therefore, two relation categories should be considered in semantic dependency analysis: intrautterance and interutter-ance relations. For the intrautterance relation, the dependency relationships should be conditionally independent of each other given the headword w kh . The intrautterance dependence relations between the words w k and w h are considered. The probability P ( D i , W , W t  X  1 1 ) is used to model a semantic dependency graph based on the relation-ships and, therefore, the equation is simplified as follows: The probability of the dependency relationship between words is defined as that be-tween the concepts defined as the hypernyms of the words, and the dependency rules are and (14) by conducting maximum likelihood estimation: P DR i k ( w k ,w kh )  X  P DR i k ( f ( w k ) , f ( w kh )) = P ( r i | f ( w k ) , f ( w kh )) = P DR i k ( w k ,w h )  X  P DR i k ( f ( w k ) , f ( w h )) = P ( r i | f ( w k ) , f ( w h )) = where function f (  X  ) denotes the transformation from words into the corresponding semantic classes. Figure 5 shows an example for semantic dependency including the intrautterance and interutterance dependency.
 To evaluate the proposed method, a spoken dialogue system for the medical domain featuring three main services, registration information, clinical information, and FAQ information [Yeh et al. 2008], and online registration as the primary function was in-vestigated. Health education documents on the Internet were gathered and provided as the FAQ files in the system, and the inference engine for the clinical information based on the patients X  syndromes was constructed according to a medical encyclopedia. Relations such as  X  X esult in X  and  X  X esult from X  that describe the relationship between syndromes and diseases in the medical domain were expected to influence system performance. Data on syndromes and diseases was obtained from a medical encyclo-pedia that defines 1,213 axioms containing the relationships between diseases and syndromes in Mandarin [Wu et al. 2005]. Twelve speech acts were defined, as shown in Table II, and each service offered by the system corresponded to each of the 12 speech acts, which appeared with various frequencies. The instances for each speech act are illustrated in Table II.

To evaluate the performance of speech input, a speech recognition engine based on the HMM Toolkit (http://htk.eng.cam.ac.uk/) [Young et al. 2006] was developed. Each syllable model consists of eight states (three states for the initial part and five states for the final part in Mandarin syllable). A collection of microphone speech databases known as TCC-300 (http://www.aclclp.org.tw/use_mat.php#tcc300edu) is used as the training corpus. A syllable-based evaluation is adopted here to assess the performance of acoustic models. The accuracy of speech recognition is 90.38% for the top five outputs. The deletion, substitution, and insertion error rates are 0.15%, 8.82%, and 0.65%, respectively. N-gram models, a back-off trigram word model trained using the Academia Sinica Balanced Corpus of Modern Chinese (http://rocling.iis.sinica.edu.tw/CKIP/engversion/20corpus.htm), were used for postpro-cessing speech recognition to generate the corresponding word sequence. The training corpus was collected using online recordings from the National Cheng Kung University Hospital and the Wizard of Oz method. The corpus was used for both SDG construction and speech act identification. To evaluate the effectiveness of the proposed method, two transcriptions were used to assess the speech act identi-fication models: human-generated transcription (REF) and speech-to-text recognition output (STT). The reference transcriptions provided the optimal basis for evaluating the speech act identification rate, because they exhibit no word errors derived from speech recognition. To increase practicability, the syllable lattice from speech recog-nition was input into the proposed framework to assess the performance. The corpus comprises 1,862 dialogues with 13,986 sentences, and the frequencies of the speech acts used in the system are shown in Figure 6. The corpus is further divided into two parts: training and test corpus. Finally, there are 1,376 dialogues with 10,365 sentences and 486 dialogues with 3,621 sentences for training and testing separately.

The number of dialogue turns is one of the key issues in the success of the dialogue task. By observing the corpus, we determined that dialogues with more than 15 turns usually failed to be completed, indicating that common ground could not be achieved. These failed dialogues were filtered from the training corpus before conducting the following experiments. The distribution of the number of turns per dialogue is shown in Figure 7. The size of the training corpus is critical to the practicability of the proposed method. In this experiment, we analyzed the effect of the number of sentences according to the precision rate of the speech act by using semantic dependency graphs with and with-out discourse information. The 1,862 collected dialogues were randomly used to train the models that are proposed methods from divergence to convergence, as shown in Figure 8. The results indicated that the precision rates for speech act identification were 95.6% and 92.4% for the training corpus containing 10,036 and that containing 7,012 sentences when semantic dependency graphs with and without history, respectively, were used. Because obtaining a considerable improvement in precision is difficult when additional training data are provided, the models obviously achieved the conditions for convergence, indicating that semantic dependency graphs with discourse outperform those without discourse, but more training data are required to include the discourse used for speech act identification. Figure 8 shows the relationship between the speech act identification rate and the size of the training corpus, revealing that more training sentences are required for the semantic dependency graph with discourse analysis than for that without it; this implies that discourse analysis plays a crucial role in the iden-tification of speech acts. To optimize the semantic dependence graphs with discourse information, the weight factor  X  in Equation (5) should be estimated simultaneously. According to Figure 9, the maximum speech act identification rate was observed when  X  was 0.88. The test corpus, randomly selected from the remaining 486 dialogues with 3,950 sentences after the training phase, comprised 100 dialogues with 630 sentences. Three baseline systems were developed to evaluate the performance of the speech dependency graph in facilitating the identification of speech acts in utterances: the Bayes classifier [Grau et al. 2004], multiple BNs [Cuayahuit et al. 2013], and the partial pattern tree [Yeh et al. 2008; Chen et al. 2012]. The Bayes classifier is formulated as follows: where SA and SA i are the optimal and candidate speech acts, respectively, and W denotes an utterance that is composed of a word sequence w 1 w 2 w 3 ...w M . In the naive Bayes classifier [Grau et al. 2004], every word w k is assumed to be independent from all other words given the Markov assumption. To prevent the data from becoming sparse, add-one smoothing was used for probability estimation [Zhai and Lafferty 2004]. Bohus and Rudnicky [2013] adopted a BN to model a joint probability distribution over a set of random variables and their dependencies [Bishop 2006]. Bohus and Rudnicky considered speech acts, semantic slots and objects, and the value of the semantic slot as a joint event, &lt; SA, a, c &gt; . The term SA denotes a speech act, a denotes the attribute (or slot), and c is a slot value. They defined the formula as where pa x is a random parent variable defined in BN s, and Z is a normalizing factor. Because the dialogue discourse is defined as a sequence of speech acts, the prediction of speech acts in a new input utterance becomes the core concern for discourse modeling. Table III shows the average accuracies of speech act identification. Because speech recognition errors were introduced into the system, the accuracy of STT decreased dramatically from 95.6% to 77.4% according to the semantic dependence graph with discourse analysis compared with that of REF .

The result indicates that the proposed method SDG1 outperforms partial pattern tree, Bayes classifier, and multiple BN approaches with the corresponding improve-ment rates of 17.5%, 23%, and 7.7% in speech-to-text recognition and 10.1%, 16.4%, and 3.1% in human-generated transcription, respectively. The improvements of the proposed model in STT is more significant than that in REF. Discourse analysis by speech act sequence and relations among the keywords constrain the matching from syllable lattice to word lattice. The number of confusing words is increased due to the dependence; the computational complexity of the proposed model is significantly reduced compared to others. This result tells us that the proposed model is more practi-cal in speech input dialogue systems. Furthermore, a significant test, the paired-sample t-test procedure [Zimmerman 1997] with confidence interval 95%, is adopted to justify the performance difference between the proposed approach and other approaches, as shown in Table IV. The three, two, and one asterisk indicate a statistically significant difference between the proposed SDG and other approaches at a significance level of 0.001, 0.01, and 0.05 in the trials, respectively.

A more detailed analysis, the results of which are shown in Table V, indicated that the decrease in the speech act identification rate of the Bayes classifier was greater than those of the other approaches. Because numerous noisy words arising from speech recognition errors were observed in this study, we suggest that more constraints be placed in the speech act identification processes to filter out the noisy words. The par-tial pattern tree approach entails considering the patterns corresponding to speech acts to retain the semantic words and eliminate the functional and noisy words. Semantic-dependence-graph-based approaches use the relationships between words in the same event to exclude the effect of speech recognition errors. The performance of the approach based on a BN was not considerably different from that of the proposed approach. The accuracy rates of REF and STT were 92.7% and 71.9%, respectively. In this study, both partial patterns and relationships were used to tighten the constraints by considering the relationships among words. The average accuracies for speech act identification determined using a single measurement are shown in Table III, and the semantic dependency graph with discourse yielded the most favorable results, indicating that discourse information can facilitate the process of speech act identification. In addi-tion, the semantic dependency graph approach outperformed the traditional approach because the words were semantically analyzed with their corresponding relationships. Detailed results regarding the speech act identification of STT and REF are shown in Tables V and VI, respectively. Comparing the results revealed that the semantic depen-dency graph approach is clearly more effective than the other approaches. Table VII shows the significant test based on Tables V and VI. The approach based on a BN achieved acceptable performance. Regarding the accuracies for  X  X onfirmation (clinic) X  and  X  X onfirmation (others) X , the proposed semantic-dependency-graph-based approach outperformed other approaches because the related information is extracted from a previous discourse or history. However, the performance for  X  X onfirmation (clinic) X  was significantly more favorable than that for  X  X onfirmation (others) X  using a partial pat-tern tree, resulting in the variety of utterances for  X  X onfirmation (others). X  In addition, the approach based on a BN achieved the optimal performance for the speech acts  X  X r. and clinic X  and  X  X linic information X  separately, because these categories of speech act involve numerous causal relationships among semantic objects or slots. In summary, semantic and syntactic relationship analysis is necessary in speech act identification because the meanings of words or concepts as well as the structural information and the implicit semantic and syntactic relationships defined in a knowledge base are re-quired to identify speech acts. In addition, discourse can be considered to improve the prediction of speech acts in new or next utterances. Thus, the discourse model can im-prove the accuracy of speech act identification because discourse modeling can enable a user X  X  intention to be understood, especially when the response is short. For exam-ple, the user may say only  X  X es X  or  X  X o X  for confirmation, and misclassification may occur because of the limited information. However, the proposed system can achieve accurate interpretation by introducing the semantic dependency relations as well as discourse information. Compared with other speech acts, clinical specialty support for enrollment and registration exhibited more significant improvements in the speech act identification rate when the semantic-dependence-graph-based approach was applied. However, the partial-pattern-tree-based speech act identifier is the optimal method for the date and time, especially in STT.

The success of dialogue lies in the achievement of common ground between users and the machine, and this is ultimately the most crucial concern in dialogue management. To compare the semantic dependency graph with previous approaches, 150 people who were not involved in the development of this project used the dialogue system to measure the task success rate. To filter out incomplete tasks, 131 dialogues were employed as the data for analysis in this experiment, and the results are listed in Tables VIII and IX. The dialogue completion rate and the average length of the dia-logues obtained when the dependency graph was used were greater than those obtained when the Bayes classifier, partial pattern tree, and BN approaches were applied. This occurred for two main reasons. First, the dependency graph can retain the most vital information in a user X  X  utterance, whereas in the semantic slot/frame approach, se-mantic objects not matching the semantic slot or frame are generally filtered out. The dependency graph approach is thus able to skip repeated or similar utterances to fill the same information in different semantic slots. Second, the dependency graph ap-proach can provide the inferences required for interpreting the user X  X  intention. The task completion rate of STT was high when using the dependency graph method, which addresses REF, because of the convenience of the speech interface. Instead of typing tediously, users can talk naturally to the system. Although the average number of turns for STT is higher than for REF, users still prefer a natural interaction style. The increase in the average number of turns observed when using the semantic dependency graph approach was significantly more favorable than that observed when using either the Bayes classifier or partial pattern tree approaches in STT compared with REF. The average number of turns for the semantic dependence graph with discourse analysis was more favorable than that of the semantic dependence graph without discourse analysis, because the historical information is more useful with regard to predicting speech acts when the speech input contains noisy words.

In addition, correct speech act identification and correct extraction of the seman-tic objects are both crucial concerns for semantic understanding in spoken dialogue systems. We analyzed a system comprising five main categories: clinical information, doctor X  X  information, confirmation of clinical information, registration time, and clinical inferences.

Tables X and XI show that the least favorable results occurred in queries for doctor X  X  information when the partial pattern tree was used. The misidentification of speech acts resulted in the mismatching of semantic slots or frames; this problem does not occur when the semantic dependency graph is used, because it always retains the most vital semantic objects according to dependency relationships in the semantic depen-dency graph instead of semantic slots. Instead of filtering out unmatched semantic objects, the semantic dependency graph is constructed to retain semantic relationships in the utterance. Therefore, the system can preserve most of the user X  X  information by using the semantic dependency graph. We thus observed that the speech act iden-tification rate was higher when the semantic dependency graph was used than when the partial pattern tree, Bayes classifier, or BN was used. However, the proposed ap-proach outperformed that based on a BN only slightly, as shown in Tables X and XI. The semantic object extraction accuracy of the approach based on a BN is high when speech acts are identified correctly because they considered speech acts, semantic slots or ob-jects, and the values of semantic slots as a joint event. Table XII shows the significant test based on Tables VII and VIII. Speech acts and semantic object extraction are essential concerns in understanding spoken utterances. This article presents a semantic dependency graph that robustly and effectively addresses various types of conversational discourse information, thus enabling the speech acts in a speaker X  X  utterances in spoken dialogue systems to be identified. By modeling the dialogue discourse as a speech act sequence and semantic dependence relations among words that appear in users X  utterances, we developed a prediction method for speech act identification based on discourse analysis instead of on only keywords. The results of the corpus analysis revealed that the model proposed in this article is practicable and effective for REF and STT. The results of the experiments indicated that the semantic dependency graph outperformed the approaches based on Bayes X  rule, the partial pattern tree, and a BN. The results of this study indicated that incorporating discourse analysis improved the rate at which speech acts were identified as well as the performance of semantic object extraction.

