 Mining frequent patterns from data streams has drawn increasing attention in recent years. However, previous mining algorithms were all focused on a single data stream. In many emerging appli-cations, it is of critical importance to combine multiple data stream-s for analysis. For example, in real-time news topic analysis, it is necessary to combine multiple news report streams from di t media sources to discover collaborative frequent patterns which are reported frequently in all media, and comparative frequent pat-terns which are reported more frequently in a media than others. To address this problem, we propose a novel frequent pattern mining algorithm Hybrid-Streaming , H-Stream for short. H-Stream builds a new Hybrid-Frequent tree to maintain historical frequent and po-tential frequent itemsets from all data streams, and incrementally updates these itemsets for e ffi cient collaborative and comparative pattern mining. Theoretical and empirical studies demonstrate the utility of the proposed method.
 H.2.8 [ Database Management ]: Database Applications X  Data Min-ing Algorithm Multiple data streams, Data stream mining, Frequent pattern min-ing
Data stream mining has drawn increasing attention in recent years [1, 2, 3, 4, 5]. Mining frequent patterns from data streams repre-sents one of the most important directions in data stream mining community [1]. Due to the rapid growth of data volumes, frequent pattern mining on data streams needs to mine from large volumes of continuous stream data in limited memory. To this end, many da-ta stream frequent pattern mining algorithms have been proposed recently, such as the FP-Stream algorithm [1], Lossy Counting al-gorithm [2], Count Sketch algorithm [3], to name a few. Although these algorithms have demonstrated their e ff ectiveness, a limitation is that they were all designed for mining a single data stream. In many emerging applications, stream data are often generated and collected from multiple sources, and it is necessary to combine multiple data streams for mining.

E xample 1. Let us take a real-time news topic analysis system for example. The system collects news report streams from three public social media: BBC, New York Times, and The Globe And Mail. The essential goal of the system is to analyze some interesting and insightful patterns. Specifically, the following four tasks need to be addressed: (1) Which topics were reported frequently by all the three media in the last 12 hours? (2) Which topics were reported frequently by The Globe And Mail, while infrequently by BBC and New York Times in the last 12 hours? (3) Which topics were not only reported frequently by The Globe And Mail in the last 12 hours, but were also reported frequently by BBC and New York Times in the last 24 hours? (4) Which topics were reported frequently by all the three media, while more frequently by BBC in the last 12 hours?
Tasks (1) and (3) are defined as mining collaborative frequent patterns , as they aim to find the common frequent patterns across multiple data streams. Tasks (2) and (4) are defined as mining comparative frequent patterns , as they aim to find contradicting fre-quent patterns across multiple data streams.

In order to solve these tasks, it is necessary to combine multi-ple data streams for analysis. Combining multiple data streams for mining has been studied previously. For example, clustering mul-tiple streams [6], classifying multiple data streams [7], analyzing correlations among multiple streams [8], and privacy preserving mining across multiple data streams [9]. However, none of them considers the problem of mining collaborative and comparative fre-quent patterns across multiple data streams. On the other hand, although some previous work [10] considered mining frequent pat-terns across multiple databases, the method cannot be directly used in data streams.

In order to discover frequent patterns across multiple data stream-s, an intuitive method is to maintain and update the frequencies of all historical stream data into a summary structure, based on which all the collaborative and comparative patterns can be retrieved ac-cordingly. However, with this method, the following concerns need to be addressed: (1) How to e ffi ciently maintain historical frequent itemsets from all data streams? A simple solution is to extend the single data stream frequent mining structure (e.g., FP-Stream structure [1]) to multiple data streams. In other words, each data stream can be maintained in an independent FP-Stream structure, and the results from each FP-Stream can be combined for mining. Although this method is very easy to implement, a limitation is that its memory cost increases linearly with the number of streams. For example, in order to mine from one hundred data streams, we have to build one hundred FP-Stream structures. This is una ff ordable for many applications. A better alternative method is to construct a single global data structure to make full use of shared itemsets across all the data streams. (2) How to update historical frequent itemsets? In data stream-s, an infrequent itemset has the potential to grow into a frequent itemset. Thus, we cannot delete an infrequent itemset at each given time point. On the other hand, due to the huge number of infrequen-t itemsets in data streams, it is impractical to maintain them all in memory during mining. Thus, it is necessary to find a compromise solution between accuracy and memory cost. (3) Based on the historical itemsets, how to design an e ff method to discover collaborative and comparative patterns across all streams. The algorithm should be able to discover these com-plex patterns not only from the same time interval, but also from di ff erent time intervals.

In summary, in order to discover collaborative and comparative frequent patterns across multiple data streams, the following con-cerns need to be addressed:
To address these challenges, an novel Hybrid-Streaming algo-rithm (H-Stream for short) is proposed to discover collaborative and comparative frequent patterns across multiple data streams. H-Stream first employs a new Hybrid-Frequent Tree ( H-tree for short) to maintain historical frequent and potential frequent itemsets for all streams, and then it incrementally updates these itemsets un-der the Cherno ff bound. Based on the maintained itemsets in H-tree, the collaborative and comparative patterns can be e retrieved by traversing the tree. Theoretical and empirical studies demonstrate the utility of the proposed method.

The rest of this paper is organized as follows. Section 2 intro-duces the H-Stream algorithm and analyzes its performance. Sec-tion 3 reports experimental results and comparisons. We conclude the paper in Section 4.
In this section, we first introduce the H-tree structure in Sec-tion 2.1, based on which we design the H-Stream algorithm in Section 2.2. We analyze the performance of the algorithm in Sec-tion 2.3.
Structurally, H-tree consists of three components: (1) A Tree ( T ) that maintains frequent and potential frequent itemsets discovered from all streams. Itemsets in data streams are categorized into three types: frequent itemset whose current sup-port is larger than , where is the min-support; infrequent itemset whose current support is lower than (  X   X  ), where  X  is the error bound; and potential frequent itemset whose current support lies between parameters and ( - X  ). We will discuss this parameter later in Section 2.3.

Ideally, all these three types of itemsets can be maintained for accurate mining. But due to the limited memory, only the frequent and potential frequent itemsets will be maintained in the tree. (2) A Hybrid -window ( W ) that records the frequencies of the maintained itemsets in each time interval. Moreover, to further re-duce the memory cost of H-tree, a logarithmic tilted window [1] can be used here. (3) A Headertable ( H ) that consists of an array of pointers that point to the itemsets in distinct level of tree. By using table H , the frequent and potential frequent itemsets can be e ffi ciently retrieved from tree T . This will reduce the overall update time of H-tree.
Compared to maintain each data stream independently, H-Tree can achieve a more compacted memory cost by making full use of the overlapping structures among all maintained itemsets.
Algorithm 1 shows the whole procedure of the H-Stream algo-rithm. Obviously, H-Stream mainly consists of three steps. In the first step, it builds an H-tree structure to maintain the frequent and potential frequent itemsets from all streams. Note that the num-bers of these itemsets are controlled by parameters and  X  the second step, H-Stream continuously updates H-tree by inserting new emerging frequent and potential frequent itemsets, meanwhile deleting itemsets that have become infrequent. In the last step, H-Stream traverses the tree to retrieve collaborative and comparative frequent patterns.

There are five inputs of Algorithm 1. Parameter controls the number of frequent itemsets. The lower value of , the more fre-quent itemsets we can obtained. Parameter  X  controls the tree prune rate. The lower value of  X  , the more infrequent itemsets will be deleted. Besides, we assume the m data streams S are processed batch-by-batch, and data from the j th stream (1  X  j  X  m ) during time interval t i ( i  X  1) is denoted by B i j . The collaborative param-eter set  X  and the comparative parameter set  X  vary case by case. They can be manually set by end users.

In the first step of Algorithm 1, H-Stream uses the first batch data B 1 j (1  X  j  X  m ) to construct an H-tree. This can be further split into two sub-steps: (1) Discover frequent and potential frequent itemsets from B 1 j . Specifically, frequent itemsets U (i.e., sup ( U ) ), and potential frequent itemsets V (i.e., sup ( V )  X   X   X  extracted from all B 1 j (1  X  j  X  m ) using FP-growth algorithm [11]; (2) Initialize the three components of H-tree. Assume all items distribute uniformly in the streams, and all the distinct items in the first batch can be observed. We calculate the average frequencies for all the distinct items to get an item list in descending order. To Algorithm 1: The H-Stream Algorithm
Input : (1) min-support ,
Output : the collaborative and comparative patterns  X  // step 1. construct H-tree
U , V  X  X  X   X  ; i  X  X  X  1 ; for j = 1 to m do
T ree  X  X  X  Initialize _ H _ Tree ( U , V ) ; // step 2. update H-tree while S ,  X  do // step 3. mining across multiple streams  X   X  X  X  Collabrative _ Mining (  X  ) ;
 X   X  X  X   X 
Output  X  initialize the H-tree, we insert all elements e  X  { U tree T with it X  X  frequencies of all streams into the window W , the items in e are sorted by the item list. By doing so, we can obtain an H-tree with minimal size [1]. Besides, we add a pointer in the Headertable H pointing to the element e in the tree.

In the second step, H-Stream updates H-Tree based on new stream data. This can be categorized into four sub-steps: (1) Preprocess new stream data. For any item infrequent in all streams, if it is ei-ther not in the tree structure, it will be removed in the preprocessing stage; (2) Discover itemsets X . For each preprocessed stream, dis-cover itemsets X from it without any prune. (3) Insert itemsets X . For each itemset e , if e has already been maintained in the tree, we update its corresponding value in H-Window. Otherwise, if itemset e  X  U ponents of H-tree. Besides, for each itemset in H-tree, if it is not updated in the current time interval, we set its corresponding fre-quency to 0 in H-window. (4) Delete infrequent itemsets from H-tree. Call a tail prune function to tailor the H-window. A tail prune function will delete all the infrequent itemsets from H-window in a bottom-up manner.

In the last step, collaborative and comparative patterns can be retrieved by traversing H-tree. Given a query as in Example 1, H-Stream traverses H-Tree using a depth-first search until all the collaborative and comparative patterns are retrieved. Due to the limit of space, we omit the details here.
We study H-Stream X  X  performance from both accuracy loss and memory cost pointviews. As we discussed above, H-Stream uses parameters ( - X  ) to prune H-tree. Thus, we analyze H-Stream X  X  performance under this prune method.
 We use the Cherno ff bound to analyze H-Stream X  X  accuracy loss. Formally, each transaction in data streams can be considered as an observation. Suppose x is the expected support of an itemset X , is the observed support of X from n transactions, and  X  is the error bound that denotes the degree of e x deviating from x . According to the Cherno ff bound, we have the following Eq. (1),
Replace the right side of Eq. (1) with probability p , and let x equal to the min-support , we have Eq. (2) as follow,
Since the transactions number per batch in each stream may be di native method is to set a global satisfactory solution for above Cherno ff bound as in Eq. (3), where || B j || denotes transactions number per batch in each stream. From Eq. (3), we can come to the conclusion that all frequent item-sets will be output with the accuracy loss of (1  X  p ) at worst.
We use the downward closure property of the frequent itemsets to analyze H-Stream X  X  memory cost. Suppose streams are com-posed of L items, and the average length of transactions is S . Then we have Theorem 1. Due to the limit of space, we omit the proof. T heorem 1. The upper boundary of the number of tree nodes in
F or example, let = 0 . 2,  X   X  0 , L = 1000, m = 5, S = 3, the H-tree will maintain no more than 2 15  X  5 nodes. This is much more e ffi cient than the exact algorithm 1 that need to maintain a total number of 2 1000  X  1 nodes at most. So we can conclude that H-Stream scales well to multiple data streams.
News Report Streams were collected from three public social me-dia: BBC, The Globe And Mail, and New York Times from July 22 to July 29, 2011. The news titles span over politics, technology, entertainment, and sports. In our experiments, a news title is taken as a transaction.

The IBM Quest Market-Basket Synthetic Data Streams were gen-erated by IBM Quest Market-Basket Synthetic Data Generator [12]. The parameters of the data generator are listed in Table 1. We gen-erated seven data streams, each having a batch size of 10,000 trans-actions.
The exact algorithm maintains all the transactions in data streams for mining, which is supposed to be the best algorithm in term of mining accuracy.
Benchmark Method We compare H-Stream with another Ap-proximation Stream mining algorithm, A-Stream for short. A-Stream is a variation of H-Stream. Di ff erent from H-Stream that uses both  X  and to prune H-tree, A-Stream uses only  X  for pruning. In so doing, A-Stream can maintain more potential frequent itemsets.
Parameter Study on  X  The error rate  X  is an important parameter for pruning H-tree. The comparison results are shown in Table 2 and Fig. 2. From Table 2, we can observe that H-Stream and A-Stream perform almost equally well. From Fig. 2, we can observe that H-Stream maintains much fewer nodes than A-Stream. This also validates our claim in Theorem 1.
 parameter Stream A-Stream H-Stream  X  = 0 . 0005 s  X  = 0 . 001 s  X  = 0 . 0015 s P arameter Study on m From Table 3, we can observe that H-Stream outperforms A-Stream in terms of Precision rate, and per-forms almost as well as A-Stream in terms of Recall rate. parameter Stream A-Stream H-Stream
Mining results from the real world news report streams Stream news received in one day will be taken as a batch. A keyword will be taken as a frequent item if it is reported no less than three times a day. Due to space limitation, we only summarize four interesting patterns as follows. (1) The keyword Norway was reported frequently by all the three media from July 26 to July 27, 2011 ( collaborative frequent pat-tern ). (2) The keyword Olympic was reported frequently by BBC, while infrequently by New York Times and The Globe And Mail from July 25 to July 29, 2011 ( comparative frequent pattern ). (3) The keyword China was not only reported frequently by New York Times and BBC from July 26 to July 27, 2011, but was also reported frequently by The Globe And Mail from July 22 to July 25, 2011 ( collaborative frequent pattern ). (4) The keyword debt was reported frequently by all the three me-dia, while more frequently by New York Times from July 22 to July 29, 2011 ( comparative frequent pattern ).
In this paper, we study a novel problem of mining collaborative and comparative frequent patterns across multiple data streams. To achieve this goal, we propose a new frequent pattern mining algo-rithm called H-Stream. H-Stream uses a new H-tree to maintain historical frequent and potential frequent itemsets, and incremen-tally updates these itemsets for e ffi cient collaborative and compar-ative mining. Theoretical and empirical studies have demonstrated the utility of the proposed method.
This research was supported by the National Science Founda-tion of China (NSFC) under Grant No. 61003167, Basic Research Program of China (973 Program) under Grant No. 2007CB311100. [1] C. Giannella, J. Han, J. Pei, X. Yan, and P. S. Yu. Mining [2] G. Manku and R. Motwani. Approximate frequency counts [3] M. Charikar, K. Chen, and M. Colton. Finding frequent items [4] P. Zhang, J. Li, P. Wang, B. Gao, X. Zhu, and L. Guo. [5] P. Zhang, X.Zhu, Y. Shi, L.Guo, and X. Wu. Robust [6] M. Yeh and et al. Clustering over multiple evolving streams [7] P. Zhang, X. Zhu, J. Tan, and L. Guo. Skif: A data [8] V. Hristidis and et al. Information discovery across multiple [9] Y. Xu, K. Wang, A. Fu, R. She, and J. Pei.
 [10] X. Zhu and X. Wu. Discovering relational patterns across [11] J. Han, J.Pei, and Y. Yin. Mining frequent patterns without [12] http: // www.cs.indiana.edu / cgiannel / assoc_gen.html.
