 We present a new method, XORSample , for uniformly sampling from the solutions of hard combi-natorial problems. Although our method is quite general, we focus on problems expressed in the Boolean Satisability (SA T) frame work. Our work is moti vated by the fact that efcient sampling There has also been a gro wing interest in combining logical and probabilistic constraints as in the work of Koller , Russell, Domingos, Bacchus, Halpern, Darwiche, and man y others (see e.g. sta-tistical relational learning and Mark ov logic netw orks [ 1 ]), and a recently proposed Mark ov logic system for this task uses efcient SA T sampling as its core reasoning mechanism [ 2 ]. Typical approaches for sampling from combinatorial spaces are based on Mark ov Chain Monte Carlo (MCMC) methods, such as the Metropolis algorithm and simulated annealing [ 3 , 4 , 5 ]. These methods construct a Mark ov chain with a predened stationary distrib ution. One can dra w samples from the stationary distrib ution by running the Mark ov chain for a suf ciently long time. Unfortu-nately , on man y combinatorial problems, the time tak en by the Mark ov chain to reach its stationary distrib ution scales exponentially with the problem size.
 MCMC methods can also be used to nd (globally optimal) solutions to combinatorial problems. For example, simulated annealing (SA) uses the Boltzmann distrib ution as the stationary distrib ution. By lowering the temperature parameter to near zero, the distrib ution becomes highly concentrated around the minimum ener gy states, which correspond to the solutions of the combinatorial problem under consideration. SA has been successfully applied to a number of combinatorial search prob-lems. Ho we ver, man y combinatorial problems, especially those with intricate constraint structure, are beyond the reach of SA and related MCMC methods. Not only does problem structure mak e reaching the stationary distrib ution prohibiti vely long, even reaching a single (optimal) solution is often infeasible. Alternati ve combinatorial search techniques have been developed that are much more effecti ve at nding solutions. These methods generally exploit cle ver search space pruning techniques, which quickly focus the search on small, but promising, parts of the overall combina-torial space. As a consequence, these techniques tend to be highly biased, and sample the set of solutions in an extremely non-uniform way. (Man y are in fact deterministic and will only return one particular solution.) In this paper , we introduce a general probabilistic technique for obtaining near -uniform samples from the set of all (globally optimal) solutions of combinatorial problems. Our method can use any state-of-the-art specialized combinatorial solv er as a subroutine, without requiring any modications to the solv er. The solv er can even be deterministic. Most importantly , the quality of our sampling method is not affected by the possible bias of the underlying specialized solv er  X  all we need is a solv er that is good at nding some solution or pro ving that none exists. We pro vide theoretical guarantees for the sampling quality of our approach. We also demonstrate the practical feasibility of our approach by sampling near -uniformly from instances of hard combinatorial problems. As mentioned earlier , to mak e our discussion more concrete, we will discuss our method in the con-text of SA T. In the SA T problem, we have a set of logical constraints on a set of Boolean (True/F alse) variables. The challenge is to nd a setting of the variables such that all logical constraints are sat-ised. SA T is the prototypical NP-complete problem, and quite lik ely the most widely studied combinatorial problem in computer science. There have been dramatic adv ances in recent years in millions of variables and constraints. Man y practical combinatorial problems can be effecti vely translated into SA T. As a consequence, one of the current most successful approaches to solving hard computational problems, arising in, e.g. , hardw are and softw are verication and planning and scheduling, is to rst translate the problem into SA T, and then use a state-of-the-art SA T solv er to nd a solution (or sho w that it does not exist). As stated abo ve, these specialized solv ers deri ve much of their power from quickly focusing their search on a very small part of the combinatorial space. Man y SA T solv ers are deterministic, but even when the solv ers incorporate some randomization, solutions will be sampled in a highly non-uniform manner .
 The central idea behind our approach can be summarized as follo ws. Assume for simplicity that our original SA T instance on n Boolean variables has 2 s solutions or satisfying assignments. Ho w can we sample uniformly at random from the set of solutions? We add special randomly generated logical constraints to our SA T problem. Each random constraint is constructed in such a way that it rules out any given truth assignment exactly with probability 1 = 2 . Therefore, in expectation, after adding s such constraints, we will have a SA T instance with exactly one solution. 1 We then use a SA T solv er to nd the remaining satisfying assignment and output this as our rst sample. We can repeat this process with a new set of s randomly generated constraints and in this way obtain another random solution. Note that to output each sample, we can use whate ver off-the-shelf SA T solv er is available, because all it needs to do is nd the single remaining assignment. 2 The randomization in the added constraints will guarantee that the assignment is selected uniformly at random. Ho w do we implement this approach? For our added constraints, we use randomly generated par -ity or  X exclusi ve-or X  ( XOR ) constraints. In recent work, we introduced XOR constraints for the problem of counting the number of solutions using MBound [ 15 ]. Although the building blocks of MBound and XORSample are the same, this work relies much more hea vily on the properties of XOR constraints, namely , pairwise and even 3-wise independence. As we will discuss belo w, an XOR con-straint eliminates any given truth assignment with probability 1 = 2 , and therefore, in expectation, cuts the set of satisfying assignments in half. For this expected beha vior to happen often, the elimina-tion of each assignment should ideally be fully independent of the elimination of other assignments. Unfortunately , as far as is kno wn, there are no compact (polynomial size) logical constraints that can achie ve such complete independence. Ho we ver, XOR constraints guarantee at least pairwise independence, i.e., if we kno w that an XOR constraint C eliminates assignment s 1 , this pro vides no information as to whether C will remo ve any another assignment s 2 . Remarkably , as we will see, such pairwise independence already leads to near -uniform sampling.
 Our sampling approach is inspired by earlier work in computational comple xity theory by Valiant and Vazirani [ 16 ], who considered the question whether having one or more assignments affects the hardness of combinatorial problems. The y sho wed that, in essence, the number of solutions should not affect the hardness of the problem instances in the worst case [ 16 ]. This was recei ved as a negative result because it sho ws that nding a solution to a Unique SA T problem (a SA T instance that is guaranteed to have at most one solution) is not any easier than nding a solution to an arbitrary SA T instance. Our sampling strate gy turns this line of research into a positi ve direction by sho wing how a standard SA T solv er, tailored to nding just one solution of a SA T problem, can now be used to sample near -uniformly from the set of solutions of an arbitrary SA T problem.
 In addition to introducing XORSample and deri ving theoretical guarantees on the quality of the sam-ples it generates, we also pro vide an empirical validation of our approach. One question that arises is whether the state-of-the-art SA T solv ers will perform well on problem instances with added XOR (or parity) constraints. Fortunately , as our experiments sho w, a careful addition of such constraints does generally not degrade the performance of the solv ers. In fact, the addition of XOR constraints can be benecial since the constraints lead to additional propag ation that can be exploited by the solv ers. 3 Our experiments sho w that we can effecti vely sample near -uniformly from hard practical combinatorial problems. In comparison with the best current alternati ve method on such instances, our sampling quality is substantially better . For the rest of this paper , x the set of propositional variables in all formulas to be V , j V j = n . A We may think of the value 0 as FALSE and the value 1 as TRUE . We will often abuse notation and write s ( i ) for valuations of entities i 62 V when the intended meaning is either already dened or is i . For x 2 V , : x denotes the corresponding negated variable; s ( : x ) = 1 s ( x ) . Let F be a formula over variables V . s ( F ) denotes the valuation of F under s . If s satises F , i.e., s ( F ) = 1, then s is a model , solution , or satisfying assignment for F . Our goal in this paper is to sample uniformly from the set of all solutions of a given formula F .
 An XOR constr aint D over variables V is the logical  X xor X  or parity of a subset of V [ f 1 g ; s satises D if it satises an odd number of elements in D . The value 1 allo ws us to express even parity . For instance, D = f a ; b ; c ; 1 g represents the xor constraint a b c 1, which is TRUE when an even number of a ; b ; c are TRUE . Note that it suf ces to use only positi ve variables. E.g., : a b : c formulas which are a logical conjunction of a formula in Conjuncti ve Normal Form (CNF) and some XOR constraints. In all our experiments, XOR constraints are translated into CNF using additional variables so that the full formula can be fed directly to standard (CNF-based) SA T solv ers. We will need basic concepts from linear algebra. Let F 2 denote the eld of two elements, 0 and 1, and F n 2 the vector space of dimension n over F . An assignment s can be thought of as an element of F D has the parity constant 1. In this setting, we can talk about linear transformations of F n linear independence of s ; s 0 2 F n 2 (see standard texts for details). We will use two properties: every linear transformation maps the all-zeros vector to itself, and there exists a linear transformation that maps any k linearly independent vectors to any other k linearly independent vectors.
 Consider the set X of all XOR constraints over V . Since an XOR constraint is a subset of V [ f 1 g , j
X j = 2 n + 1 . Our method requires choosing XOR constraints from X at random. Let X ( n ; q ) denote the probability distrib ution over X dened as follo ws: select each v 2 V independently at random with probability q and include the constant 1 independently with probability 1 = 2 . This produces XOR s of average length nq . In particular , note that every two complementary XOR constraints involving the same subset of V (e.g., c d and c d 1) are chosen with the same probability irrespecti ve of q . Such complementary XOR constraints have the simple but useful property that any assignment s in X is chosen with probability 2 ( n + 1 ) . We will be interested in the random variables which are the sum of indicator random variables: Y =  X  s Y s . Linearity of expectation says that E [ Y ] =  X  s E [ Y s ] . When various Y s are pairwise independent , i.e., kno wing Y s  X  s Var [ Y s ] . We will also need conditional probabilities . Here, for a random event X , linearity of conditional expectation says that E [ Y j X ] =  X  s E [ Y s j X ] . Let X = Y s independent , i.e., kno wing Y s linearly : Var Y j Y s In this section, we describe and analyze two randomized algorithms, XORSample and XORSample' , for sampling solutions of a given Boolean formula F near -uniformly using streamlining with random XOR constraints. Both algorithms are parameterized by two quantities: a positi ve inte ger s and a real number q 2 ( 0 ; 1 ) , where s is the number of XOR s added to F and X ( n ; q ) is the distrib ution from which the y are dra wn. These parameters determine the degree of uniformity achie ved by the algorithms, which we formalize as Theorems 1 and 2 . The rst algorithm, XORSample , uses a SA T solv er as a subroutine on the randomly streamlined formula. It repeatedly performs the streamlining process until the resulting formula has a unique solution. When s is chosen appropriately , it tak es XORSample a small number of iterations (on average) to successfully produce a sample. The second algorithm, XORSample' , is non-iterati ve. Here s is chosen to be relati vely small so that a moderate number of solutions survi ve. XORSample' then uses stronger subroutines, namely a SA T model counter and a model selector , to output one of the survi ving solutions uniformly at random. 3.1 XOR -based sampling using SA T solv ers: XORSample Let F be a formula over n variables, and q and s be the parameters of XORSample . The algorithm works by adding to F , in each iteration, s random XOR constraints Q s dra wn independently from the solutions) are a subset of the solutions of F . If there is a unique survi ving solution s , XORSample outputs s and stops. Otherwise, it discards Q s and F q s , and iterates the process (rejection sampling). The check for uniqueness of s is done by adding the negation of s as a constraint to F q s and testing whether the resulting formula is still satisable. See Algorithm 1 for a full description. We now analyze how uniform the samples produced by XORSample are. For the rest of this section, each solution s of F to be sampled with probability 2 s . Let p one XORSample outputs s in one iter ation . This is typically much lower than 2 s , which is accounted for by rejection sampling. Nonetheless, we will sho w that when s is lar ger than s , the variation in p This, we will sho w, is very close to 2 s , where  X closeness X  is formalized as being within a factor of c ( a ) which approaches 1 very fast. The proof closely follo ws the argument used by Valiant and Vazirani [ 16 ] in their comple xity theory work on unique satisability . Ho we ver, we give a dif ferent, non-combinatorial argument for the pairwise independence property of XOR s needed in the proof, relying on linear algebra. This approach is insightful and will come handy in Section 3.2 . We describe the main idea belo w, deferring details to the full version of the paper .
 Lemma 1. Let a &gt; 0 ; c ( a ) = 1 2 a ; and s = s + a . Then c ( a ) 2 s &lt; p one Proof sketc h. We rst pro ve the upper bound on p one XOR s (e.g. c d and c d 1), s satises exactly one XOR . Hence, the probability that s satises an XOR chosen randomly from the distrib ution X ( n ; q ) is 1 = 2 . By independence of the s XOR s in Q s in XORSample , s survi ves with probability exactly 2 s , giving the desired upper bound on p one ; s ( s ) . For the lower bound, we resort to pairwise independence. Let s 6 = s 0 be two solutions of F . Let D be an XOR chosen randomly from X ( n ; 1 = 2 ) . We use linear algebra arguments to sho w that the probability that s ( D ) = 1 (i.e., s satises D ) is independent of the probability that s 0 ( D ) = 1. Recall the interpretation of variable assignments and XOR constraints in the vector space F n 2 (cf. from X ( n ; 1 = 2 ) , kno wing a 1 + b gives us no information about b , pro ving independence. A similar argument works when s is non-zero and s 0 = ( 0 ; 0 ; : : : ; 0 ) , and also when s and s 0 are linearly independent to begin with. We skip the details.
 Pr Pr [ s 0 ( Q s ) = 1 j s ( Q s ) = 1 ] = 2 s . No w, Ev aluating this using the union bound and pairwise independence sho ws p one Theor em 1. Let F be a formula with 2 s solutions. Let a &gt; 0 ; c ( a ) = 1 2 a ; and s = s + a . For any solution s of F , the probability p s ( s ) with whic h XORSample with par ameter s q = 1 = 2 and s outputs s satises Further , the number of iter ations needed to produce one sample has a geometric distrib ution with expectation between 2 a and 2 a = c ( a ) .
 Proof . Let  X  p denote the probability that XORSample nds some unique solution in any single it-eration. p one overall probability of sampling s , is given by the innite geometric series which sums to p one Lemma 1 says that for any two solutions s 1 and s 2 of F , p one within a factor of c ( a ) of each other , already pro ving the min vs. max part of the result. Further ,  X  s p s ( s ) = 1 because of rejection sampling.
 s , violating the claimed lower bound. By the abo ve argument, p s ( s ) is within a factor of c ( a ) of p s ( s 0 ) for every s , and would therefore be at most 2 s . This would mak e  X  s p s ( s ) strictly less than one, a contradiction. A similar argument pro ves the upper bound on p s ( s ) .
 Finally , the number of iterations needed to nd a unique solution (thereby successfully producing a sample) is a geometric random variable with success parameter  X  p =  X  s p one value 1 =  X  p . Using the bounds on p one This pro ves the claimed bounds on the expected number of iterations, 1 =  X  p . 3.2 XOR -based sampling using model counters and selectors: XORSample' We now discuss our second parameterized algorithm, XORSample' , which also works by adding to F s random XOR s Q s chosen independently from X ( n ; q ) . Ho we ver, now the resulting streamlined formula F q s is fed to an exact model counting subroutine to compute the number of survi ving solu-tions, mc . If mc &gt; 0, XORSample' succeeds and outputs the i th survi ving solution using a model to
XORSample , is non-iterati ve. Also, the model counting and selecting subroutines it uses are more comple x than SA T solv ers; these work well in practice only because F q s is highly streamlined. The sample-quality analysis of XORSample' requires some what more comple x ideas than that of XORSample . Let F have 2 s solutions as before. We again x q = 1 = 2 and pro ve that if the parameter s is suf ciently smaller than s , the sample-quality is pro vably good. The proof relies on the fact that the value of an XOR constraint on two variable assignments does not tell us anything about its value on a third assignment. We state this as the follo wing lemma, which can be pro ved by extending the linear algebra arguments we used in the proof of Lemma 1 (see the full version for details). Lemma 2 (3-wise independence). Let s 1 ; s 2 ; and s 3 be thr ee distinct assignments to n Boolean variables. Let D be an XOR constr aint chosen at random from X ( n ; 1 = 2 ) . Then for i 2 f 0 ; 1 g , Pr [ s 1 ( D ) = i j s 2 ( D ) ; s 3 ( D )] = Pr [ s 1 ( D ) = i ] .
 Recall the discussion of expectation, variance, pairwise independence, and 3-wise independence in Section 2 . In particular , when a number of random variables are 3-wise independent, the conditional variance of their sum (conditioned on one of these variables) equals the sum of their indi vidual conditional variances. We use this to compute bounds on the sampling probability of XORSample' . The idea is to sho w that the number of solutions survi ving, given that any x ed solution s survi ves, is independent of s in expectation and is highly lik ely to be very close to the expected value. As a result, the probability with which s is output, which is inversely proportional to the number of solutions survi ving along with s , will be very close to the uniform probability . Here  X closeness X  is one-sided and is measured as being within a factor of c 0 ( a ) which approaches 1 very quickly . Theor em 2. Let F be a formula with 2 s solutions. Let a &gt; 0 and s = s a . For any solution s of F , the probability p 0 s ( s ) with whic h XORSample' with par ameter s q = 1 = 2 and s outputs s satises Further , XORSample' succeeds with probability lar ger than c 0 ( a ) .
 Proof sketc h. See the full version for a detailed proof. We begin by setting up a frame work for Y constraint D to s independent XOR s Q s implies that the random variables Y s 0 are 3-wise independent. The variable mc (see Algorithm 2 ), which is the number of survi ving solutions, equals  X  s 0 Y s 0 . Consider the distrib ution of mc conditioned on the fact that s survi ves. Using pairwise indepen-dence, the corresponding conditional expectation can be sho ws to satisfy: m = E [ mc j s ( Q s ) = 1 ] = 1 + ( 2 s 1 ) 2 s . More interesting, using 3-wise independence, the corresponding conditional vari-ation can also be bounded: Var [ mc j s ( Q s ) = 1 ] &lt; E [ mc j s ( Q s ) = 1 ] .
 Since s = s a , 2 a &lt; m &lt; 1 + 2 a . We sho w that mc conditioned on s ( Q s ) = 1 indeed lies very close to m . Let b 0 be a parameter whose value we will x later . By Chebyche v's inequality , Therefore, conditioned on s ( Q s ) = 1, with probability more than 1 2 2 b = m , mc lies between ( 1 2 b ) m and ( 1 + 2 b ) m . Recall that p 0 s ( s ) is the probability that XORSample' outputs s . Lastly , the success probability of XORSample' is  X  s p 0 s ( s ) &gt; c 0 ( a ) .
 Remark 1. Theorems 1 and 2 sho w that both XORSample and XORSample' can be used to sample arbitrarily close to the uniform distrib ution when q = 1 = 2 . For example, as the number of XOR s used in XORSample is increased, a increases, the deviation c ( a ) from the truly uniform sampling probability p approaches 0 exponentially fast, and we get progressi vely smaller error bands around p . Ho we ver, for any x ed a , these algorithms, some what counter -intuiti vely , do not always sample truly uniformly (see the full version). As a result, we expect to see a uctuation around p , which, as we pro ved abo ve, will be exponentially small in a . To validate our XOR -sampling technique, we consider two kinds of formulas: a random 3-SA T in-stance generated near the SA T phase transition [ 18 ] and a structured instance deri ved from a logistics planning domain (data and code available from the authors). We used a complete model counter , Relsat [ 12 ], to nd all solutions of our problem instances. Our random instance with 75 variables has a total of 48 satisfying assignments, and our logistics formula with 352 variables has 512 sat-isfying assignments. (W e used formulas with a relati vely small number of assignments in order to evaluate the quality of our sampling. Note that we need to dra w man y samples for each assignment.) We used XORSample with MiniSat [ 14 ] as the underlying SA T solv er to generate samples from the set of solutions of each formula. Each sample took a fraction of a second to generate on a 4GHz pro-cessor . For comparison, we also ran the best alternati ve method for sampling from SA T problems, SampleSAT [ 19 , 2 ], allo wing it roughly the same cumulati ve runtime as XORSample . Figure 1 depicts our results. In the left panel, we consider the random SA T instance, generating 200,000 samples total. In pure uniform sampling, in expectation we have 200 ; 000 = 48 4 ; 167 samples for each solution. This level is indicated with the solid horizontal line. We see that the samples produced by XORSample all lie in a narro w band centered around this line. Contrast this with the results for SampleSAT : SampleSAT does sample quite uniformly from solutions that lie near each other in Hamming distance but dif ferent solution clusters are sampled with dif ferent fre-quencies. This SA T instance has two solution clusters: the rst 32 solutions are sampled around 2 ; 900 times each, i.e., not frequently enough, whereas the remaining 16 solutions are sampled too frequently , around 6,700 times each. (Although SampleSAT greatly impro ves on other sampling strate gies for SA T, the split into disjoint sampling bands appears inherent in the approach.) The Kullback-Leibler (KL) divergence between the XORSample data and the uniform distrib ution is 0.002. For SampleSAT the KL-di vergence from uniform is 0.085. It is clear that the XORSample approach leads to much more uniform sampling.
 The right panel in Figure 1 gives the results for our structured logistics planning instance. (To im-pro ve the readability of the gure, we plot the sample frequenc y only for every fth assignment.) In this case, the dif ference between XORSample and SampleSAT is even more dramatic. SampleSAT in fact only found 256 of the 512 solutions in a total of 100,000 samples. We also see that one of these solutions is sampled nearly 60,000 times, whereas man y other solutions are sampled less than Figure 1: Results of XORSample and SampleSAT on a random 3-SA T instance, the left panel, and a logistics ve times. The KL divergence from uniform is 4.16. (Technically the KL divergence is innite, but we assigned a count of one to the non-sampled solutions.) The expected number of samples for each assignment is 100 ; 000 = 512 195. The gure also sho ws that the sample counts from XORSample all lie around this value; their KL divergence from uniform is 0.013.
 These experiments sho w that XORSample is a promising practical technique (with theoretical guar -antees) for obtaining near -uniform samples from intricate combinatorial spaces.
 Refer ences
