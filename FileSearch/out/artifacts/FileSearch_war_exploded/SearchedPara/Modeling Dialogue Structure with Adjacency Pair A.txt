 Automatically learning dialogue structure from corpora is an active area of research driven by a re cognition of the value offered by data -driven a p-proaches ( e.g., Bangalore et al. , 2006). Di a logue structure information is of particular importance when the inte r action is centered around a learning task, such as in natural language tutoring, b e cause tech niques that support empirical identific a tion of dialogue strategies can inform not only the design of intelligent tuto r ing systems (Forbes -Riley et al. , 2007), but also contribute to our u n derstanding of the cognitive and affective pro c esses involved in le arning through tutoring (VanLehn et al. , 2007). Although traditional top -down approaches ( e.g., Cade et al. , 2008) and some empirical work on analyzing the stru c ture of tutorial dialogue (Forbes -Riley et al. , 2007) have yielded significant results, the field is limited by the lack of an aut o-matic, data -driven approach to identifying di a logue structure. An empirical approach to identif y ing tutorial dialogue strategies, or modes , could a d-dress this limitation by providing a mechanism for describing in succinct probabilistic terms the tut o-rial strategies that actually occur in a co r pus. Just as early work on dialogue act interpret a tion utilized hidden Markov models (HMMs) to capture linguistic structure (Stolcke et al. , 2000), we pr o-pose a system t hat uses HMMs to capture the stru c ture of tutorial dialogue implicit within s e-quences of already -tagged dialogue acts. This a p-proach ope r ates on the premise that at any given point in the tutorial dialogue, the collaborative i n-teraction is in a dialogue m ode that characterizes the nature of the exchanges between tutor and st u-dent. In our model, a di a logue mode is defined by a probability distribution over the observed sy m-bols ( e.g. , di a logue acts and adjacency pairs). Our previous work has noted some limitations of first -order HMMs as a p plied to sequences of individual dialogue acts (Boyer et al. , in press). Chief among these is that HMMs allow arbitrarily frequent transitions b e tween hidden states, which does not co n form well to human intuition abou t how tutoring strategies are applied. Training an HMM on a s e quence of adjacency pairs rather than individual dialogue acts is one way to generate a more d e scriptive model without increasing model complexity more than is required to accommodate the expan ded set of o b servation symbols. To this end, we apply the a p proach of Midgley et al. (2006) for empirically identifying significant adj a-cency pairs within dialogue, and proceed by trea t-ing adjacency pairs as atomic units for the purposes of training the H MM. Th is analysis uses a corpus of human -human tut o-rial dialogue collected in the domain of introdu c-tory computer science. Forty -three learners interacted remotely with a tutor through a ke y-board -to -keyboard r e mote learning environment y ielding 4,864 dialogue moves.

The tutoring corpus w as manually tagged with dia logue acts designed to capture the salient cha r-acteristics of the tutoring process (Table 1).
 The correspondence between utterances and di a-logue act tags is one -to -one. Compound utterances ( i.e. , a single utterance comprising more than one dial o gue act) were split by the primary annotator prior to the inter -rater rel i ability study. 1 The importance of adj a cency pairs is well -established in natural language dialogue ( e.g. , Schlegoff &amp; Sacks, 1973), and adjacency pair analysis has illuminated important phenomena in tuto r ing as well (Forbes -Riley et al. , 2007). For the current corpus, bigram anal y sis of dialogue acts yielded a set of commonly -occurring pairs. Ho w-ever, as noted in (Midgley et al. , 2006), in order to esta b lish that two dialogue acts are truly related as an adjacency pair, it is important to d e termine whether the pre s ence of the first member of the pair is associated with a significantly higher pro b-ability of the se c ond member occurring. For this analysis we uti l ize a  X  2 test for independence of the categorical var i ables act i and act i+1 for all two -way combinations of di a logue act tags. Only pairs in which speaker( act i )  X  speaker( act i+1 ) were consi d-ered. Other dialogue acts were treated as atomic elements in subsequent analysis, as discussed in Se c tion 3. Table 2 displays a list of the dependent pairs sorted by descending (unadjusted) statistical significance ; the subscript indicates tutor (t) or st u-dent ( s ). The keyboard -to -keyboard tutorial interaction r e-sulted in a sequence of utteran ces that were ann o-tated with di a logue acts. We have hypothesized that a higher -level dialogue structure , namely the tutorial dialogue mode , overlays the observed di a-logue acts. To build an HMM model of this stru c-ture w e treat dialogue mode as a hidden va riable and train a h idden Markov model to i n duce the dialogue modes and their associated di a logue act emission probability distr i butions.
 An adjacency pair joining algorithm (Figure 1) was a p plied to each sequence of dialogue acts. This algorithm joins pairs of dialogue acts into atomic units according to a priority determined by the strength of the adjacency pair depen d ency.
Figure 2 illustrates the application of the adj a-cency pair joining algorithm on a sequence of di a-logue acts. Any dialogue acts that were not grouped into adjacency pairs at the co m pletion of the algorithm are treated as atomic units in the HMM i analysis. 
The final set of ob served symbols consist s of 39 tags: 23 adjacency pairs (Table 2) plus all indivi d-ual di a logue acts augmented with a tag for the speaker (Table 1). It was desirable to learn n , the best number of hidden states, during modeling r ather than specif y-ing th is value a pr i ori . To this end , we trained and ten -fold cross -validated seven models (each featu r-ing ra n domly -initialized parameters) for each number of hidden states n from 2 to 15 , incl u sive . T he average log -likelihood was computed across all seven m odels for each n , and this ave r age log -likelihood l n was used to compute the Akaike I n-formation Criterion, a maximum -penalized likel i-hood estim a tor that penalizes more complex models (Scott, 2002). The best fit was obtained with n =4 (Figure 3). The trans ition probability distribution among hi d den states is depicted in Figure 4, with the size of the nodes indicating rel a-tive frequency of each hidden state; specif i cally, State 0 accounts for 63% of the corpus, States 1 and 3 account for approx i mately 15% ea ch, and State 2 accounts for 7%.
 This exploratory application of hidden Markov models involves training an HMM on a mixed i n-put sequence consisting of both indivi d ual dialogue acts and adjacency pairs. The best -fit HMM co n-sists of four hidden states whose emission symbol probability distributions lend themselves to inte r-pretation as tutorial dialogue modes. For example, Sta te 0 consists prima r ily of tutor statements and positive feedback, two of the most common di a-logue acts in our corpus. The transition pro b abili -ties also reveal that State 0 is highly stable; a self -transition is most likely with pro b ability 0.835. State 3 is an interactive state featuring student r e-flection in the form of questions, statements, and requests for feedback. The transition probabilit i e s show that nearly 60% of th e time the dialogue transitions from State 3 to State 0; this may ind i-cate that after establishing what the st u dent does or does not know in State 3, the tutoring switches to a less collaborative  X  X eaching X  mode repr e sented by State 0. will include comparison with other types of graphical models. An other important step is to correlate the dialogue profile of each tutoring se s-sion, as revealed by the HMM, to learning and a f-fective outcomes of the tutoring s ession. This type of inquiry can lead directly to design recommend a-tions for tutorial dialogue systems that aim to maximize particular learner outcomes. In add i tion, leveraging know l edge of the task state as well as surface -level utterance content below the di a logue act level are promising directions for refi n ing the descri p tive and predictive power of these models. This research was supported by the National Science Foundation under Grants REC -0632450, IIS -0812291, CNS -0540523, and GRFP . Any opinions, findings, and conclusions or recommendations e x-pressed in this mat e rial are those of the authors and do not necessarily reflect the views of the National Science Foundation.
 Boyer, K.E., Phillips, R., Wallis, M., Vouk, M., &amp; Boyer, K.E., Ha, E.Y., Wallis, M., Phillips, R., Vouk , Bangalore, S., DiFabbrizio, G., Stent, A. (2006). Cade, W., Copeland, J., Person, N., &amp; D'Mello, S. Forbes -Riley, K., Rotaru, M., Litman, D. J., &amp; Midgley, T. D., Harrison, S., &amp; MacNish, C. (2007). Schlegoff, E.A., Sacks, H. (1973). Opening up clo s-Scott, S. L. (2002). Bayesian methods for hidden Stolcke, A., Coccaro, N., Bates, R., Taylor, P., Van VanLehn, K., Graesser, A., Jackson, G.T., Jo r dan, P., 
