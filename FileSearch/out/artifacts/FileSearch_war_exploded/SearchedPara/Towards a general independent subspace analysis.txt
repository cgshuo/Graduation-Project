 A random vector Y is called an independent component of the random vector X , if there exists an invertible matrix A and a decomposition X = A ( Y , Z ) such that Y and Z are stochastically independent. The goal of a general independent subspace analysis (ISA) or multidimensional inde-pendent component analysis is the decomposition of an arbitrary random vector X into independent components. If X is to be decomposed into one-dimensional components, this c oincides with ordi-nary independent component analysis (ICA). Similarly, if t he independent components are required or simply k -ISA. So 1 -ISA is equivalent to ICA. 1.1 Why extend ICA? cannot be compared with other solutions, so for instance bli nd source separation (BSS) would be the only indeterminacies [11], given some additional rathe r weak restrictions to the model. Figure 1: Applying ICA to a random vector X = AS that does not fulfill the ICA model; here S is chosen to consist of a two-dimensional and a one-dimensio nal irreducible component. Shown are matrix using the three ICA-algorithms FastICA, JADE and Ext ended Infomax. Clearly, the original mixing matrix could not be reconstructed in any of the experi ments. However, interestingly, the separation model, which means that they have to be mixtures o f a random vector that consists of and the results may depend on the algorithm as illustrated in figure 1.
 ing previous approaches, we will provide such a model togeth er with a corresponding uniqueness result and a preliminary algorithm. 1.2 Previous approaches to ISA for dependent component anal ysis components have been studied for quite some time. ISA in the t erminology of multidimensional ICA extended from the ICA theory [11]. Algorithmic enhancement s in this setting have been recently structures, these may be used for the multidimensional sepa ration [13].
 density estimation. A related relaxation of the ICA assumpt ion is given by topographic ICA [8], where dependencies between all components are assumed and m odelled along a topographic struc-problem, which necessitates a model for inter-cluster inde pendence and intra-cluster dependence. 1.3 General ISA independent component. An invertible matrix W is called a (general) independent subspace analysis of X if WX = ( S 1 , . . . , S k ) with pairwise independent, irreducible random vectors S i . Note that in this case, the S that in contrast to ICA and k -ISA, we do not fix the size of the groups S as mentioned above, scalings i.e. invertible transformati ons within each S which extends previous results in the case of ICA [6] and k -ISA [11], where also the additional slight assumptions on square-integrability i.e. on existi ng covariance have been made. Theorem 1.2. Given a random vector X with existing covariance and no Gaussian independent component, then an ISA of X exists and is unique except for scaling and permutation. subspace extraction problem, and theorem 1.2 follows readi ly from Lemma 1.3. Let S = ( S independent components S ing two lemmata each give a simplification of lemma 1.3 by orde ring the components S to their dimensions. Some care has to be taken when showing th at lemma 1.5 implies lemma 1.4. Lemma 1.4. Let S and X be defined as in lemma 1.3. In addition assume that dim S i  X  l and dim S i &lt; dim X for i &gt; l . Then X  X  S i for some i  X  l .
 Lemma 1.5. Let S and X be defined as in lemma 1.4, and let l = 1 and k = 2 . Then X  X  S Lemma 1.6. Let S = ( S 1.4 Dealing with Gaussians avoid additional indeterminacies. Indeed, any orthogonal transformation of two decorrelated hence be possible.
 within the data, and no assumption of independence within th e subspace is made. More precisely, given a random vector X , a factorization X = AS with an invertible matrix A , S = ( S and S S decomposable . X is denoted to be minimally n -decomposable if X is not ( n  X  1) -decomposable. According to our previous notation, S shown that the subspaces of such decompositions are unique [ 12]: Theorem 1.7 (Uniqueness of NGSA) . The mixing matrix A of a minimal decomposition is unique except for transformations in each of the two subspaces.
 solutions of ISA.
 Theorem 1.8 (Existence and Uniqueness of ISA) . Given a random vector X with existing covari-ance, an ISA of X exists and is unique except for permutation of components of the same dimension and invertible transformations within each independent co mponent and within the Gaussian part. Proof. Existence is obvious. Uniqueness follows after first applyi ng theorem 1.7 to X and then theorem 1.2 to the non-Gaussian part. Joint diagonalization has become an important tool in ICA-b ased BSS (used for example in JADE) (JD) of a set of symmetric real n  X  n matrices M := { M E such that E  X  M where all off-diagonal elements of M have been set to zero, and k M k 2 squared Frobenius norm. The Frobenius norm is invariant und er conjugation by an orthogonal ma-trix, so minimizing f is equivalent to maximizing g (  X  E ) := P K of Givens rotation in two coordinates [5]. 2.1 Generalization to blocks diagonalizing all n  X  n matrices M determine E such that E  X  M with m m -block diagonal if it is of the form with arbitrary m As generalization of JD in the case of known the block structu re, we can formulate the joint m -block diagonalization ( m -JBD) problem as the minimization of f m (  X  E ) := P K m -block diagonal matrix by setting all other elements of M to zero. In practice due to estimation errors, such E will not exist, so we speak of approximate JBD and imply minim izing some error-by an m -block diagonal matrix from the right, and m -permutation defined by a permutation matrix that only swaps blocks of the same size.
 f m ( E ) &lt;  X  for some fixed constant  X  &gt; 0 instead of f m ( E ) = 0 . 2.2 JBD by JD A few algorithms to actually perform JBD have been proposed, see [1] and references therein. In the following we will simply perform joint diagonalization and then permute the columns of E to and we can prove the conjecture partially: We want to show that JD implies JBD up to permutation, i.e. if E is a minimum of f , then there exists a permutation P such that f m ( EP ) = 0 (given existence of a JBD of M ). But of course each block of size m such a JBD block-optimal in the following.
 Theorem 2.1. Any block-optimal JBD of M (zero of f m ) is a local minimum of f . minimum of f or equivalently a local maximum of the squared diagonal sum g . After substituting each M that E = I is a local maximum of g .
 Consider the elementary Givens rotation G manifold O ( n ) at I , simply by  X  (  X  locally maximal i.e. negative semi-definite at 0 in the direction  X  Now assume i and j are from different blocks. After possible permutation, we m ay assume that j = Then G on the diagonal other than at indices ( i, i ) and ( j, j ) are not changed, so Hence h (0 , . . . , 0 ,  X   X   X  maximum at 0 . 2.3 Recovering the permutation P by some combinatorial optimization. Figure 2: Performance of the proposed general JBD algorithm in the case of the (unknown) block-partition 40 = 1 + 2 + 2 + 3 + 3 + 5 +6 +6 + 6 +6 in the presence of noise with SNR of 5 dB . The diagonal matrix except for permutation within groups of the same sizes as claimed in section 2.2. In the case of unknown block-size, we propose to use the follo wing simple permutation-recovery algorithm: consider the mean diagonalized matrix D := K  X  1 P K tion that M is m -block-diagonalizable (with unknown m ), each E  X  M be m -block-diagonal except for a permutation P , so it must have the corresponding number of zeros whose choice is non-trivial.
 permuting columns and rows in order to guarantee that all non -zeros of D are clustered along the diagonal as closely as possible. This recovers the permutat ion as well as the partition m of n . Algorithm 1 : Block-diagonality permutation finder Input : ( n  X  n ) -matrix D Output : block-diagonal matrix P ( D ) := D  X  such that D  X  = PDP T for a permutation matrix P D  X   X  D for i  X  1 to n do We illustrate the performance of the proposed JBD algorithm as follows: we generate a set of K = They have been generated in blocks of size m with coefficients chosen randomly uniform from orthogonal mixing matrix E  X  O (40) , i.e. M the JBD algorithm from above to { M illustrated in figure 2. As usual by preprocessing of the observations X by whitening we may assume that Cov( X ) = I . scatter plots i.e. densities of the source components and th e mixing-separating map  X  A  X  1 A . also Cov( S ) = I . Then I = Cov( X ) = A Cov( S ) A  X  = AA  X  so A is orthogonal. Due to the ISA property, we follow the idea of the JADE algorithmbut now in t he ISA setting. We perform JBD of the (whitened) contracted quadricovariance matrices defined by C E One simple choice is to use n 2 matrices E resulting algorithm, subspace-JADE (SJADE) not only performs NGCA by grouping Gaussians as one-dimensional components with trivial C m using the general JBD algorithm from section 2.3. In a first example, we consider a general ISA problem in dimens ion n = 10 with the unknown shown in figure 3(a-d). Another 1-dimensional source follow ing a uniform distribution was con-structed. Altogether 10 4 samples were used. The sources S were mixed by a mixing matrix A with coefficients uniformly randomly sampled from [  X  1 , 1] to give mixtures X = AS . The recovered mixing matrix  X  A was then estimated using the above block-JADE algorithm wit h unknown block except for block permutation and scaling, which experiment ally confirms theorem 1.8. The algo-except for permutation and scaling within the sources  X  whic h in the higher-dimensional cases implies transformations such as rotation of the underlying images or shapes. When applying ICA ( 1 -ISA) to the above mixtures, we cannot expect to recover the o riginal sources as explained in figure 1; however, some algorithms might recover the sources up to permutation. Indeed, SJADE equals JADE with additional permutation recovery because t he joint block diagonalization is per-formed using joint diagonalization. This explains why JADE retrieves meaningful components even in this non-ICA setting as observed in [4].
 subspace JADE also includes NGCA. For this we consider the ca se n = 5 , m = (1 , 1 , 1 , 2) and sources with two Gaussians, one uniform and a 2-dimensional irreducible component as before; 10 A , and apply SJADE with  X  = 0 . 01 . The recovered mixing matrix  X  A is compared with A by taking the ad-hoc measure  X  ( P ) := P 3 from the other components, we compare the recovered source k urtoses. The median kurtoses are the algorithm in the general, noisy ISA setting. noisy case via NGCA). However in practice in the finite sample case, due to estimation errors the developed in future works.

