 Detecting inferences in documents is critical for ensuring privacy when sharing information. In this paper, we propose a refined and practical model of inference detection using a reference corpus. Our model is inspired by association rule mining: inferences are based on word co-occurrences. Using the model and taking the Web as the reference corpus, we can find inferences and measure their strength through web-mining algorithms that leverage search engines such as Google or Yahoo!.

Our model also includes the important case of private corpora, to model inference detection in enterprise settings in which there is a large private document repository. We find inferences in private corpora by using analogues of our Web-mining algorithms, relying on an index for the corpus rather than a Web search engine.
We present results from two experiments. The first experiment demonstrates the performance of our techniques in identifying all the keywords that allow for inference of a particular topic (e.g.  X  X IV") with confidence above a certain threshold. The second ex-periment uses the public Enron e-mail dataset. We postulate a sen-sitive topic and use the Enron corpus and the Web together to find inferences for the topic.

These experiments demonstrate that our techniques are practical, and that our model of inference based on word co-occurrence is well-suited to efficient inference detection.
 H.2.7 [ Database Management ]: Database Administration X  secu-rity, integrity, and protection ; I.2.6 [ Artificial Intelligence ]: Learn-ing X  knowledge acquisition ; I.2.3 [ Artificial Intelligence ]: De-duction and Theorem Proving X  inference engines ; K.4.1 [ Computers and Society ]: Public Policy Issues X  privacy Algorithms, Security, Performance, Experimentation Inference detection, inference control, association rule mining, web mining, search engine
Imagine a team of government employees tasked with preparing military documents for Web site publishing. The corpus of doc-uments is too large for anything more than cursory review by a human, and the topics covered by the documents include a broad array of sensitive topics (e.g. weapons development) as well as nonsensitive topics (e.g. purchase orders for office supplies). The team understands that it is not enough to look for known sensitive terms like  X  X issile X  and so they begin to look for other, seemingly innocuous, terms that might allow the government X  X  missile devel-opment activities to be inferred. That is, the team works on the inference detection problem in this semi-structured data set.
To detect sensitive inferences, they turn to association rule min-ing technology (e.g. [2, 1]). Given a reference corpus, association rule mining analyzes the contents of the corpus to identify words closely associated with  X  X issile X , thus potentially providing the team with an efficient way to identify the documents that might be sensitive and need human review. Since the team has no repository tailored to missiles, they decide to use the Web as their reference corpus.
 The team quickly runs into problems. The massive size of the Web makes existing association rule mining algorithms impracti-cal. In addition, attempts to work on smaller portions of the Web are still problematic because the well-known algorithms only de-tect associations with high support (e.g. [2]) or they depend on the user to lower-bound the support requirements (e.g. [24]) and the team simply doesn X  X  have that information. In the end, the associ-ation mining algorithms fail to detect the association between the set of terms  X  X nfrared, gyroscope, radar X  and  X  X issile X  due to the relatively small number of Web documents that contain any of the first three terms and  X  X issile X . Consequently, a sensitive document concerning missile development is released.
 Though this story is fictitious, such data leaks are commonplace. The Iraqi Freedom Document Portal and the Nuclear Regulatory Commission X  X  site, are two recent examples of government Web sites that were shut down (in the case of the Portal, [6]) or signif-icantly overhauled (the NRC X  X  site, [3]) when they were found to contain sensitive documents.

We present a model and algorithms for mining such sensitive as-sociations using large corpora such as the Web. This new model captures the essence of inference detection in a simple formalism inspired by association rule mining: inferences are based on word co-occurrences. Our model, however, captures a far larger vari-ety of inferences than basic association rule mining. While asso-ciation rule mining is restricted to  X  X onjunctive" inferences (co-occurrences of items), our model also supports  X  X isjunctive X  infer-ences that establish a relationship between a (conjunctive) set of precedents (e.g., attributes of a person), A 1 ,...,A n and a disjunc-tive set of consequents (e.g. suspected entities): A key challenge in inference detection that is not present in the tra-ditional association rule mining setting is modelling the knowledge of an  X  X dversary X  (i.e. any individual from which sensitive data should be withheld) in order to predict what the adversary can infer. Since there is no database or corpus of the adversary X  X  knowledge, we approximate such a corpus with the most appropriate data set available (e.g. the Web). Hence, our reference corpus is an approx-imation, and consequently there is the possibility of false positives and negatives amongst the associations. A significant contribution of this work is adapting the information retrieval notions of preci-sion and recall to this setting, thus providing metrics for assessing the accuracy of the discovered associations.

In [20] the notion of using the Web to represent the adversary X  X  knowledge is introduced. However, the model in [20] is incom-pletely specified and the algorithms proposed are computationally expensive and no mechanisms are provided for evaluating their suc-cess or failure. We build on this work with a fully-functional model that connects association rule mining and inference detection and supports far more efficient algorithms. For example, evaluating a single inference using the algorithm of [20] takes 150 seconds, while the fastest algorithm in this paper takes a mere 1 . 5 seconds.
We test our algorithms in two settings. First, we explore their use in healthcare privacy legislation compliance. Most U.S. states place restrictions on sharing information about the following sen-sitive topics: HIV/AIDS, genetic information, mental health, and communicable diseases [12]. Abiding by the intention of this leg-islation is very challenging as it requires protecting any informa-tion in medical records that can allow these sensitive topics to be inferred. It is not enough to protect obviously sensitive terms such as  X  X IV X ; any medications or symptoms from which an HIV diag-nosis can be inferred should be protected as well [22]. We provide experimental data demonstrating the performance of our algorithm in identifying all the keywords that allow for inference of a particu-lar topic ( X  X IV" in our experiment) with confidence above a certain threshold.

The second experiment explores the protection of a corporation X  X  sensitive information (e.g. intellectual property, client data, etc.) using an internal data set as the reference corpus. In particular, we use the public Enron e-mail dataset to demonstrate how sensitive topics can be protected with our algorithms. We divide the En-ron corpus into test and training sets and postulate a sensitive topic ( X  X harton" in our experiment). We then use the training part of the Enron corpus and the Web together to find inferences for the topic. We evaluate the inferences found using the test part of the Enron corpus.

To estimate the precision and recall of our experiments we em-ploy human review, lower bound calculations and a stability anal-ysis over different training sets. The sum total of this analysis is substantial evidence of the good precision and recall achieved by our algorithms.

Together with our model, our experiments demonstrate that it is possible to mine associations efficiently even with a reference corpus as large as the Web, and to use these associations to protect privacy.
 Overview. The rest of this paper is organized as follows. We re-view related work in the rest of this section. In Section 2, we give a precise definition of the problems of inference detection and in-ference control. In Section 3, we present our model of knowledge and inferences. We illustrate this model with two simple examples in Section 4. We discuss our algorithms for detecting inferences in Section 5, and present our experimental results in Sections 6 and 7. Finally, we conclude with a discussion of future work in Section 8.
Our inference detection algorithms can also be viewed as algo-rithms for finding association rules that are of a sensitive nature. In our setting, an association rule is an implication of the form A  X  B , where A and B are disjoint sets of words, and B is of a sensitive nature, e.g., a medical condition like HIV or a person X  X  identity. Recall that an association rule is said to have high confi-dence if Pr( B | A ) is large, and large support if Pr( A  X  B ) is large. Much of the association rule mining literature focuses on finding rules that have both high confidence and high support (see, for ex-ample, [2, 1]).

There are 3 important differences between our work on inference detection and conventional association rule mining.

The first difference is that in privacy applications, unlike data-mining applications, inferences need not have high support to be considered important or sensitive. Our goal is to find all associa-tions of a sensitive nature, and thus we cannot limit ourselves to rules with large support. Indeed, a feature that makes the privacy problem so challenging (and any automated algorithmic solution so valuable) is that sensitive inferences are almost certain to have low support when viewed in the context of a large corpus such as the Enron data set or the Web. Hence, algorithms such as Apriori and AprioriTid [2] which prune low-support item sets (sets of words, in our case) are not directly applicable to our setting.

Recent research explores the discovery of rules meeting certain support constraints, thus potentially allowing lower support rules to be discovered (e.g. [24]). However, this approach assumes an un-derstanding of minimum support constraints that is very difficult to achieve in the "needle in a hay stack" problem of finding sensitive inferences.

Second, conventional association rule mining assumes full ac-cess to a structured database (e.g. a supermarket database of trans-actions) or semi-structured corpus which, by definition of the task at hand, contains all the association rules of interest. Our goal is to detect the associations that may lead an adversary to make unde-sired inferences and unfortunately, there is no database of adversary knowledge from which to extract these associations. We approxi-mate such a database using the best publicly available corpus for the context. For example, we experiment with using the Web to de-tect sensitive medical inferences, and a corporate email repository (the Enron corpus, [9]) to demonstrate the detection of inferences pertaining to a sensitive topic (e.g. new product plans). To mine these corpora we use Web search engines and Lucene [4], respec-tively.

Innovative algorithms have been developed for mining semi-structured data for association rules (e.g. [19, 5]) but again this work assumes the corpus at hand contains all the wanted association rules, so any that are discovered (and meet the required confidence and support goals) are necessarily valid. Our necessary reliance on corpora that approximate the nonexistent "adversary knowledge corpus", may lead to the discovery of erroneous rules and the failure to discover accurate ones. To deal with this issue, we discuss ways to measure the precision and recall of our results (see Sections 6 and 7).
A third difference stems from the fact that association rules are typically mined in the  X  X arket basket X  setting with the goal of dis-covering when the purchase of a collection of goods is likely to be accompanied by the purchase of another good (e.g. milk is almost always purchased along with butter and bread). Implications stem-ming from a conjunction of items ( butter  X  bread  X  milk ) are typically enough to support the marketing recommendations (e.g. grocery store layout) that are the most common goal of associa-tion rule mining. Since our concern is implications that impact privacy, we are interested in more complex implications, beyond association rules, as well. For example, if A  X  B  X  C  X  D and A  X  B 0  X  C  X  E then in our setting it may be critical to notice that A  X  B  X  A 0  X  B 0  X  C , if, for example, C is the identity of an individual. Our model supports the detection of more complex inferences such as these.

Our approach to inference detection is in the same spirit as the use of the Web by Nakov and Hearst [18] to resolve language am-biguities. Their idea is to use co-occurrence on the Web to dis-ambiguate phrases. In this paper we also make use of the Web to side-step the issue of training data, but with a different application, the detection of potential privacy violations. We use co-occurrence on the Web to model adversary knowledge and consequently, to detect undesired inferences that may be drawn from text.
We also note that a huge body of work in information retrieval, natural language processing (NLP) and data mining is based on the powerful word co-occurrence feature. Indeed, co-occurrence is at the root of many techniques for detecting synonyms (e.g. [23]), interpreting search engine queries (e.g. [11]), automatic indexing and annotation [7, 8] and problems in structural linguistics like dis-covering conventional expressions (e.g. [17]). Here, we exploit co-occurrence for a new application, the identification of sensitive inferences to support privacy.

The notion of using the Web to detect inferences was introduced in [20]. Our approach is significantly different from the algorithms in [20]. Every algorithm in [20] relies on analysis of Web pages to identify inferences. For example, to determine if conditions A and B imply a diagnosis of HIV, [20] would issue a search engine query  X  AB  X  and examine the resulting hits for the appearance of the term  X  X IV X . In large part due to this analysis, the algorithms of [20] are slow and consequently, the algorithms contain shortcuts that lessen the depth of inference detection that is possible in order to keep running time at a reasonable duration. For example, in [20] a sensitive term such as  X  X IV X  is only found if it occurs in the first 5000 lines of the html of a Web site. For sites with significant graphics this restriction is likely to enable the analysis of only a small fraction of the actual text.

In contrast, we leverage the indexing power of the search en-gine to avoid this costly step while doing more thorough inference detection. In short, we use the fact that the search engine can mea-sure co-occurrence of terms for us to avoid doing any more content analysis than retrieving the hit counts. In addition, we develop the first rigorous model for corpora-based inference detection (with the Web and the Enron emails being our example corpora) and present approaches to measuring the precision and recall of our algorithms.
Finally we note that inference detection is a well-studied prob-lem in the database community (see, for example [10]), where the problem is to find ways in which classified or otherwise sensitive information can be learned through a sequence of database queries for unclassified or non-sensitive information. In addition, Sweeney has looked at the problem of using the Web to identify inferences based on regular expressions such as social security numbers and account numbers [21]. Inference detection in structured databases, while certainly difficult, is nevertheless a simpler problem than the problem we consider of detecting inferences in free-form text doc-uments.
Let D denote a document, or a collection of documents. Infor-mally, let K ( D ) denote the  X  X nowledge" (or facts, or axioms) that can be extracted from D . We assume the existence of knowledge composition rules, which specify how to derive new knowledge from the combination of existing pieces of knowledge. We write K ( D ) for the closure of K ( D ) under the knowledge composition rules, i.e. the closed set of all knowledge obtained from K ( D ) by repeated application of the composition rules.
 Inference detection. Now let C denote a private collection of doc-uments that is being considered for public release, and let R denote a collection of reference documents. Informally stated, the problem of inference control comes from the fact that the  X  X nowledge" that can be computed from the union of the private and reference collec-tions K ( C X  X  ) is typically greater than the union K ( C )  X  K ( R ) of what can be extracted separately from C and R . In its most general formulation, the inference detection problem is to understand the difference Inference control. In almost all applications, we have a set S of sensitive or secret knowledge that the publication of C should not expose. In that case, the problem of inference control can be stated more precisely as the problem of ensuring that the intersection S  X   X  ( C , R ) is empty. Inference control is closely tied with redaction , that is, the sanitization of a document by removal of some of the document X  X  content. When the intersection S  X   X  ( C , R ) is non-empty, an additional goal of inference control is to identify a subset C sub  X  X  such that S  X   X  ( C sub , R ) =  X  . While C sub =  X  trivially satisfies this condition, the goal might be to identify a subset C of maximum size or to preserve as much information on a certain topic while protecting S .

Controlling privacy leaks by identifying the set C sub that is ap-propriate to release is a highly challenging problem (particularly in light of attacks such as [16]) that is not the subject of this work. That said, since it is a crucial part of the content security problem, we outline some of the research issues around it when discussing open problems in Section 8.
 Adversarial model. This formulation of the problem implies as-sumptions about our adversarial model, which we now detail ex-plicitly. We assume that the adversary does not have any additional private knowledge beyond the reference collection R . We also as-sume that changes to the collection C prior to its publication (e.g. redaction, obfuscation) do not themselves allow the adversary to infer information about C . We also note that inference control may be accomplished in other ways than redaction; for example, words may be replaced or words may be added to provide inference con-trol. In this paper our focus is on identifying, as opposed to con-trolling, inferences. In this section, we define the  X  X nowledge extraction" function K , and the knowledge composition rules that allow us to compute K from K . Sophisticated formal languages have been developed in the NLP community to represent human knowledge (see [15] for a survey), but these heavy-weight languages would not allow for efficient computation of inferences. In this work, we adopt a simple formal representation of knowledge inspired by association rule mining: knowledge extraction is based on word co-occurrences and knowledge composition rules are the rules of Boolean logic. This model is well suited to efficient inference detection. Association rule mining. The goal of association rule mining is to discover elements that frequently co-occur in a given data set. The best-known use of association rule mining is in market-basket anal-ysis, where the data set consists of customers X  purchases and the goal is to discover products that are often purchased together (e.g. bread and milk). We use an approach similar to association rule mining to model the knowledge present in a collection, C , of docu-ments. We search for sets of keywords that frequently co-occur in the collection C , and model the knowledge in C with these sets. For example, if C is a collection of medical documents about HIV, we may learn the following sets of frequently co-occurring keywords: { HIV,AIDS } or { HIV,gp120 } . There is no question that this simple model captures only a fraction of the human knowledge embedded in C . Nevertheless, we will demonstrate the power of this model to efficiently detect inferences.
 Inference model. Adapting the framework of association rule min-ing to the detection of inferences in free form text documents raises both theoretical and algorithmic challenges. We address first the main theoretical differences between our model and association rule mining (algorithmic challenges are discussed in Section 5):
Let V (for vocabulary) denote the set of all the words that ap-pear in the collection C of documents. We assume that V contains, without restriction, all the words, numbers, dates, symbols, entity names, etc, which appear in the collection C . We could define the set of items to be V itself, but this ignores the lexical structure of the language and the semantic content of C . To take these into account, we let T denote the set of items and define a function f : V  X  T which maps any word v  X  X  to an item t  X  X  . The function f may take into account any (or all) of the following: Boolean Formulas. Given the set of items T , we can define boolean formulas of items. Let  X  denote the boolean operator AND, and  X  denote OR. If A  X  T , we let A denote the negation of A . The notation A  X  B is equivalent to A  X  B .
 Item Sets. Given a definition of items, we represent the collection, C , of documents as a collection of item sets. The simplest represen-tation is to create one item set for each document in the collection. The item set associated with a document consists of all the items contained in that document. More generally, we allow for finer grained definitions of item sets. We decompose every document in C into a collection of textual units, where a textual unit can be a sentence, a paragraph, a page, a section, etc. Let C 0 denote the re-sulting collection of item sets. We then associate one item set with every textual unit in C 0 , consisting of all the items in that textual unit.

Our algorithms search for inference precedents in each textual unit, so for efficiency reasons it is desirable that a textual unit be as-sociated with a single inference, that is, the topic of the textual unit. Our experimental results choose textual units with this in mind. Support of a formula. Let S  X  C 0 be an item set, and let F be a Boolean formula of terms. Viewing the set S as a conjunction of items, we say that S satisfies F if S  X  F . We define the support Supp ( F ) as the probability that S satisfies F for an item set S  X  X  In other words:
The knowledge extracted from C is represented as a list of infer-ences of the form A  X  B , where A and B are Boolean formulas of items. We adopt the terminology of association rule mining and call A the antecedent and B the consequent of the inference. We review briefly definitions used in association rule mining to deter-mine the importance of an inference, and discuss the relevance of these definitions to inference detection.

D EFINITION 1. The support of an inference A  X  B is the sup-port of A  X  B .

D EFINITION 2. The confidence of an inference A  X  B is the ratio Supp ( A  X  B ) / Supp ( A ) .

Association rule mining typically searches for inferences with high support and high confidence. High support is a much weaker indicator of the importance of an inference in our privacy applica-tion, since even an inference with low support may allow an adver-sary to draw damaging conclusions.
 Logical closure. Our algorithms will search for inferences with confidence above certain thresholds. These inferences will consti-tute the knowledge K ( C ) extracted from the collection C of docu-ments. Given this seed set of inferences, the closure K ( C ) is com-puted from the inferences by application of standard Boolean rules.
Before describing our inference detection algorithms, we illus-trate the model of Section 3 and the challenges of inference detec-tion using large corpora with two examples.
 Simple Inference. Let X  X  assume that the private collection C con-sists of the medical record of a single patient. We assume that the reference collection R consists of all Web pages indexed by a search engine, and we consider each Web page as a distinct textual unit.

In this example, items are medical keywords (we ignore other words). Assume that we have extracted the keyword gp120 from the collection C , and we want to measure the confidence of the in-ferences (gp120  X  HIV) and (gp120  X  Flu) using the knowledge extracted from R .

Since we have defined the textual units of R to be Web pages, the support Supp ( W ) of an item W is simply the fraction of Web pages which contain the keyword W . This fraction can be obtained from the search engine with a single query. Using Google for example, we learn that Supp ( gp120 ) = 991 , 000 and Supp ( gp120  X  HIV ) = 919 , 000 . It follows that Similarly, Supp ( gp120  X  Flu ) = 27 , 500 , and thus With a confidence threshold set at 0 . 2 , the inference (gp120  X  Flu) would (correctly) not be considered significant, but the inference (gp120  X  HIV) would (correctly) be considered very significant (gp120 is a glycoprotein that attaches to the HIV retrovirus). Complex Inferences. In the simple example above, the precedent (gp120) and consequent (HIV) of the inference consist of a single keyword. Our model, however, allows the precedent and conse-quent to be arbitrarily complex Boolean formulas of keywords. For example, we can define the confidence of the inference A  X  B  X  ( C  X  ( D  X  E )) , where A,B,C,D and E represent keywords. We can not only define these complex inferences, but also measure their confidence, since most search engines support both disjunc-tive and conjunctive queries. In practice, our ability to measure the confidence of complex inferences is limited only by the sparseness of the Web, which we discuss next.

Let F and G denote Boolean formulas of terms. The confidence of the inference F  X  G is the ratio of the support of F and the support of F  X  G . For complex formulas F and G , the support of F  X  G may be zero: there are no documents on the Web that satisfy both the formulas F and G . This makes it impossible to directly compute the confidence of F  X  G . Consider the following example: The support of F  X  G , as measured by Google, is zero. This is not to say that the inference F  X  G is invalid. In fact, mention of  X  X 5.0" and  X  X VM" in Las Vegas at the Lowes Lake Hotel, most likely implies the KDD-08 conference. But the confidence of this inference cannot be directly measured due to the sparseness of the Web.

The knowledge composition rules defined in our model can in theory help mitigate the problem of the sparseness of the Web. We offer the following made-up example as illustration. Suppose that we have found the following inferences to have high confidence: If  X  X DD" is the only term in the intersection of the consequents of these two inferences, Boolean composition rules allow us to infer F  X  G with high confidence.

In the rest of this paper, we focus on algorithms for measuring the support and confidence of fairly simple inferences. Our algo-rithms and experiments do not illustrate the logical combination of simple inferences to compute more complex inferences. Logical combination is nevertheless a powerful feature of our model, that we plan to explore in future work. Note that one way to improve the results of Section 6 would be to leverage logical combinations. In Section 3.1, we define the support of a formula as In this section, we discuss how to compute Supp ( A ) in practice. Recall that the definition of support depends on a collection of doc-uments C and on the textual sub-units used to define C 0 .
Given the collection of documents C and sufficient computa-tional resources, it is easy to compute Supp ( A ) for any formula A . Unfortunately, unrestricted access to C is not always possible. In what follows, we focus on the difficult but common case in which the collection C consists of all documents publicly available on the Web. Crawling the whole Web is an expensive operation, so we cannot assume direct knowledge of C . Instead, we rely on search engines to mediate access to C and discuss various techniques for estimating Supp ( A ) via queries to a search engine.
 Measuring support. For simplicity, we start with the assumption that the formula A is a conjunction of terms: A = V 1  X  ...  X  V (Some search engines also provide limited support for disjunctive queries). We issue a query to a Web search engine for the key-words V 1 ,...,V k (note that search engines interpret such queries conjunctively by default), and let n A denote the number of docu-ments found by the search engine to match this query. Let N denote the total number of documents indexed by the search engine. We estimate the support of A as Supp ( A )  X  n A /N . Note that the nor-malizing factor N serves to ensure Supp ( A )  X  [0 , 1] and need not be known precisely.
 Estimate of confidence. The technique above for estimating the support of a formula allows us to estimate the confidence of A  X  B as We call this estimate the PMI-IR estimate of confidence, after Tur-ney [23], who used the same technique on single terms to rate their similarity. Note that the PMI-IR estimate of confidence can be computed very efficiently: it requires only two search engine queries.
The inference enumeration problem is the problem of determin-ing all the precedents A which imply a given consequent B with confidence above a certain threshold. In our experiment, we chose the consequent  X  X IV" and searched for precedents that allow for in-ference of HIV. This experiment is motivated by the practical prob-lem of redacting from a medical record all information that might allow for inference of HIV infection. We limited our search for precedents to single keywords, but the same technique would allow us to test pairs, triplets, and more generally tuples of keywords.
We took a simple approach to generating candidate precedents that generalizes easily. We issued a query for  X  X IV" to a search engine (via Yahoo! X  X  web search API [25]) and retrieved the top 10 hits. Results included the Wikipedia article on HIV and  X  X IV In-Site", a site with information on HIV maintained by the University of California San Francisco. We discuss the choice of the number of documents to retrieve in Section 6.2.

We processed these 10 documents with the Apache Lucene in-dexer [4] and extracted 2349 distinct keywords from the docu-ments. For all these keywords, we measured the confidence of the were computed using the PMI-IR algorithm described in section 5. inference Keyword  X  HIV using the PMI-IR algorithm defined in Section 5.
The PMI-IR algorithm requires two queries to a search engine to estimate the confidence of an inference. Our implementation of the PMI-IR algorithm tested all 2349 keywords in approximately 70 seconds, whereas the algorithm of [20] reportedly took more than 6 hours to test 435 inferences.

The output of our algorithm was a list of all 2349 precedents, ranked in decreasing order of the confidence with which they imply HIV. Figure 1 shows the top precedents which imply HIV with con-fidence greater than 0 . 50 , among precedents with support greater or equal to 100,000. There were 70 of these precedents in all. The lower-bound of 100,000 on the support of the inference is arbitrary, and was chosen only to present the reader with a list of precedents that is not too obscure. In a specialized medical application, prece-dents with lower support would be equally important.
 Precision. We estimate the precision of our inference enumeration algorithm as the fraction of  X  X orrect" inferences among these 70 inferences (i.e. the inferences of Figure 1). A medical expert (a licensed physician in internal medicine with HIV-infected patients) evaluated the 70 inferences of Figure 1 manually. He classified 53 of the 70 inferences as correct, i.e. the precedent of these inferences is clearly related to HIV. Of the remainder, some precedents were not necessarily connected to HIV but did trigger the thought of HIV (for example,  X  X emophiliacs").

Interestingly, some of the precedents deemed by our medical ex-pert to not imply HIV were  X  X cr5" (an HIV protein),  X  X NAIDS" (the United Nations AIDS effort), and various AIDS web-sites such as  X  X idsinfo" and  X  X hebody.com". This suggests that a more com-plete review of our inferences would require a panel of HIV experts, including perhaps a molecular biologist or an HIV sociologist. The difficulty in obtaining a complete review of our results underscores our point that detecting inferences is a difficult, time-consuming and costly task. It also highlights the potential of our automated ap-proach for enumerating inferences, which uses the Web as a proxy for all human knowledge and thus draws from all disciplines. Recall. We define the recall of our algorithm as the fraction of precedents that allow for definite inference of HIV found by our al-gorithm. The recall of the algorithm is hard to estimate, since there exists no comprehensive list of precedents that allow for inference of HIV. To estimate recall, we resort to indirect evidence that our algorithm found  X  X ost" precedents that definitely imply HIV. Figure 2: Number of inferences found for HIV as a function of the number of source documents, for different values of the confidence cutoff.

Figure 2 shows the number of inferences found as a function of the number of source documents used to generate candidate prece-dents, for different values of the cutoff below which inferences are deemed insignificant. Not surprisingly, the graph shows that gen-erating more candidate precedents from more source documents yields more inferences. However, the graph suggests a sublinear re-lationship between the number of source documents and the num-ber of inferences found. The addition of new source documents yields few additional inferences after around 10 documents, imply-ing that at least for the  X  X IV" precedent, reasonable recall would be achieved when the algorithm uses 10 source documents.
In many applications the inference detection occurs in the con-text of a private corpus. In Section 7, we show that the use of a private corpus helps focus the inferences found and improves re-call.
For our second experiment, we describe the use of our inference detection algorithms in the presence of a private corpus. Potential application scenarios include: E-discovery. A corporation is sub-poenaed for all documents re-Data Leak Prevention. A corporation wants to ensure details on
In our experiment, we held part of the private corpus in reserve as a test set and used the remainder as input to our algorithms to generate inferences for a given sensitive topic. We then used these inferences on the test set to obtain a set of flagged documents. We analyzed the set of flagged documents in terms of precision and recall.

Of course, for these applications one can proceed as in Section 6 and simply use the Web to find inferences for a given topic. We argue, however, that this approach does not leverage the knowledge contained in the private corpus and better results will be obtained using the techniques that follow.
 Description of Enron e-mail corpus. The Enron e-mail corpus consists of e-mail from about 150 senior managers of Enron and contains over half a million messages. The corpus was made pub-lic as part of the Federal Energy Regulatory Commission Enron investigation. We use a cleaned version of the dataset available at [9]. The data leak prevention company InBoxer [13] uses the same Enron corpus to demonstrate their outbound e-mail scanning tech-nology [14].
 Generation of candidate inferences. In our experiment, we chose the topic of  X  X harton", the business school of the University of Pennsylvania. This is a well-known public entity, so that it is easy to evaluate the quality of the inferences detected. Almost 800 mes-sages in the Enron corpus contain the term  X  X harton". Note that there are a handful of e-mails that contain the term Wharton in ref-erence to Wharton, the county of Texas. We divided the Enron corpus into test and training sets by date, so that roughly each set contained half the e-mail with the term Wharton.

We generated candidate inferences for Wharton by taking all terms from all e-mails in the training set containing the term Whar-ton. Each candidate inference has two measures of confidence, the usual Web confidence, as in Section 5, and an analogous confidence computed using the private corpus: Note that we also generated candidate inferences by the method of Section 6, but we found that this additional step rarely generated anything of value. Indeed, the value of the private corpus is that it helps provide an efficient way to come up with relevant candidate inferences. The private corpus compensates for the sparseness of the Web and also acts as a counter balance to the all-encompassing nature of the Web.
 Estimation of Precision. Precision is the percentage of identified e-mails that are indeed about Wharton. To calculate precision, we approximate by assuming the e-mails about Wharton contain the Wharton keyword. Note that this is an underestimate of precision, neglecting the e-mails about Wharton, the county in Texas.
In Figure 3, precision is ( B + C ) / ( A + B + C ) . We approximate precision with B/ ( A + B + C ) . Figure 4: Precision and Recall curve of  X  X harton" inferences Estimation of Recall. Recall is the percentage of e-mails that are identified out of all e-mails about Wharton. Recall is difficult to practically evaluate, because it requires looking at several hundred thousand e-mails to see if they are about Wharton.

Hence, to calculate recall, we manually reviewed the e-mails containing the Wharton keyword to see if Wharton could be in-ferred even without the "Wharton" keyword. In this way, we ob-tained a set of e-mails that could be inferred to be about Wharton even without the Wharton term. Then the estimate for recall is the percentage of these e-mails identified where we do not use the triv-ial inference (wharton  X  wharton) .

In Figure 3, recall is ( B + C ) / ( B + C + D ) . We approximate recall by restricting to subset B.
Our results are shown in Figure 4. The figure shows the trade-off between precision and recall (as defined in the previous section) for emails identified as sensitive in the Enron corpus via different sets of inferences.

We use as a baseline a simple set of two inferences that was man-ually generated: (wharton  X  wharton), and (university of pennsyl-vania  X  wharton). These two inferences are a good proxy for in-ferences that a non-expert human would find manually in a short period of time. Identification of sensitive emails using this basic set of two inferences achieves high precision (75%) but mediocre recall (61%). This baseline data-point is shown as a square in Fig-ure 4.

Next, we used our inference detection technology to generate ad-ditional inferences, using the training portion of the Enron dataset. For each inference, we computed two (confidence, support)-pairs, corresponding to the two corpuses of the Web and Enron training set. For various cutoff values of confidence and support in the two corpuses, we obtained points in a precision-recall graph, as seen in Figure 4.
To summarize our results, our tools for generating additional in-ferences allow us to increase recall by 20% (from 61% to 81%) with only a barely noticeable degradation in precision (from 75% to 73%). We achieved recalls as high as 91%, but at the cost of a significant drop in precision (to 29%).

To illustrate the types of inferences that we were able to detect, we give in Figure 5 the list of the top 40 precedents that we found for the consequent  X  X harton", using both the Web and the private Enron corpus. This list of 40 inferences was generated with the fol-lowing parameters. For the Email corpus, we requested a support  X  2 and confidence &gt; 0 . 6 . For the Web corpus, we requested sup-port &gt; 5 and confidence &gt; 0 . 01 . The list was manually classified into categories for the sake of clarity. The number and diversity of inferences found automatically by our inference detection tool suggest that these tools would be very valuable in helping humans review and detect sensitive inferences.
We have given a general theoretical framework for describing inferences and using Web-based probabilities to rate their strength. We evaluated the strength of the identified inferences using known web-mining algorithms. We presented a case study of detecting HIV inferences: we generated precedents that implied HIV, rated their strength, and submitted the results for human review. We pre-sented another case study of detecting inferences for  X  X harton" using the Enron e-mail corpus.

Our techniques provide an efficient mechanism for detecting sen-sitive content. In practice, they might be used to identify documents that need human review before their release.

As mentioned in Section 2, the focus of this paper is detecting inferences rather than the equally challenging task of protecting against inferences. Protection options include selective removal of text (i.e. redaction), encryption of text, sanitization of text (includ-ing introducing noise) and quarantining of content. Each approach comes with security and usability concerns that are difficult to eval-uate. We highlight the need for a system that supports the user in making protection decisions as an open research problem.
An additional avenue for future work is leveraging the analysis of Web structure and document content to improve our inference detection algorithms. For instance, an analysis of the effect of sub-document item sets and the weighting of item sets according to some measure of authoritativeness may help reduce false positives and false negatives.
The authors gratefully acknowledge financial support for this project by Fujitsu Limited, Japan. We would also like to thank the anonymous reviewers of previous versions of this paper and Tomoyoshi Takebayashi and Toshihiro Sonoda of Fujitsu Labora-tories Ltd. for many helpful comments. [1] R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A. I. [2] R. Agrawal and R. Srikant. Fast algorithms for mining [3] M. Ahlers. Blueprints for terrorists? On the Web at [4] Apache Lucene project. On the Web at [5] M. Berardi, M. Lapi, P. Leo, and C. Loglisci. Mining [6] W. Broad. U. S. web archive is said to reveal a nuclear [7] P. Cimiano and S. Staab. Learning by googling. SIGKDD [8] M. Dowman, V. Tablan, H. Cunningham, and B. Popov.
 [9] Enron corpus. On the Web at [10] C. Farkas and S. Jajodia. The inference problem: a survey. [11] N. S. Glance. Community search assistant. In Intelligent [12] Health Privacy Project. On the Web at [13] Inboxer. On the Web at http://www.inboxer.com/ . [14] Inboxer X  X  Enron demonstration site. On the Web at [15] L. M. Iwanska and S. C. Shapiro. Natural Language [16] D. P. Lopresti and A. L. Spitz. Information leakage through [17] C. D. Manning and H. Schutze. Foundations of statistical [18] P. Nakov and M. Hearst. Using the web as an implicit [19] L. Singh, P. Scheuermann, and B. Chen. Generating [20] J. Staddon, P. Golle, and B. Zimny. Web-based inference [21] L. Sweeney. AI technologies to defeat identity theft [22] N. Terry and L. Francis. Ensuring the privacy and [23] P. D. Turney. Mining the web for synonyms: PMI-IR versus [24] K. Wang, Y. He, and J. Han. Pushing support constraints into [25] Yahoo! Web Search API. On the Web at
