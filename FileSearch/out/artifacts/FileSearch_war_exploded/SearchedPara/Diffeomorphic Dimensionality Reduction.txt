 The problem of visualizing high dimensional data often aris es in the context of exploratory data analysis. For many real world data sets this is a challenging task, as the spaces in which the data lie are often too high dimensional to be visualized directly . If the data themselves lie on a lower dimensional subspace however, dimensionality reduction t echniques may be employed, which aim to meaningfully represent the data as elements of this lower dimensional subspace.
 The earliest approaches to dimensionality reduction are th e linear methods known as principal com-ponents analysis (PCA) and factor analysis (Duda et al., 200 0). More recently however, the major-ity of research has focussed on non-linear methods, in order to overcome the limitations of linear approaches X  X or an overview and numerical comparison see e.g. (Venna, 2007; van der Maaten et al., 2008), respectively. In an effort to better understa nd the numerous methods which have been proposed, various categorizations have been proposed. In t he present case, it is pertinent to make the distinction between methods which focus on properties o f the mapping to the lower dimensional space, and methods which focus on properties of the mapped data , in that space. A canonical ex-ample of the latter is multidimensional scaling (MDS), whic h in its basic form finds the minimizer with respect to y 1 , y 2 ,..., y m of (Cox &amp; Cox, 1994) where here, as throughout the paper, the x i  X  R a are input or high dimensional points, and the y i  X  R b are output or low dimensional points, so that b &lt; a . Note that the above term is a function only of the input points and the corresponding mapp ed points, and is designed to preserve the pairwise distances of the data set.
 The methods which focus on the mapping itself (from the highe r to the lower dimensional space, which we refer to as the downward mapping, or the upward mappi ng which is the converse) are less common, and form a category into which the present work falls . Both auto-encoders (DeMers &amp; Cottrell, 1993) and the Gaussian process latent variable mo del (GP-LVM) (Lawrence, 2004) also fall into this category, but we focus on the latter as it provi des an appropriate transition into the main part the paper. The GP-LVM places a Gaussian process (GP ) prior over each high dimen-sional component of the upward mapping, and optimizes with r espect to the set of low dimensional points X  X hich can be thought of as hyper-parameters of the mod el X  X he likelihood of the high di-mensional points. Hence the GP-LVM constructs a regular (in the sense of regularization, i.e. likely under the GP prior) upward mapping. By doing so, the model gua rantees that nearby points in the low dimensional space should be mapped to nearby points i n the high dimensional space X  X n intuitive idea for dimensionality reduction which is also p resent in the MDS objective (1), above. The converse is not guaranteed in the original GP-LVM howeve r, and this has lead to the more re-cent development of the so-called back-constrained GP-LVM (Lawrence &amp; Candela, 2006), which essentially places an additional GP prior over the downward mapping. By guaranteeing in this way that (the modes of the posterior distributions over) both th e upward and downward mappings are regular, the back constrained GP-LVM induces something rem iniscent of a diffeomorphic mapping between the two spaces. This leads us to the present work, in w hich we derive our new algorithm, Diffeomap , by explicitly casting the dimensionality reduction probl em as one of constructing a dif-feomorphic mapping between the low dimensional space and th e subspace of the high dimensional space on which the data lie. In this paper we use the following definition: Definition 2.1. Let U and V be open subsets of R a and R b , respectively. The mapping F : U  X  V is said to be a diffeomorphism if it is bijective ( i.e. one to one), smooth ( i.e. belonging to C  X  ), and has a smooth inverse map F  X  1 .
 We note in passing the connection between this definition, ou r discussion of the GP-LVM, and di-mensionality reduction. The GP-LVM constructs a regular up ward mapping (analogous to F  X  1 ) which ensures that points nearby in R b will be mapped to points nearby in R a , a property referred to as similarity preservation in (Lawrence &amp; Candela, 2006). The back constrained GP-LVM s i-multaneously ensures that the downward mapping (analogous to F ) is regular, thereby additionally implementing what its authors refer to as dissimilarity preservation . Finally, the similarity between smoothness (required of F and F  X  1 in Definition 2.1) and regularity (imposed on the downward an d upward mappings by the GP prior in the back constrained GP-LV M) complete the analogy. There is also an alternative, more direct motivation for diffeomorp hic mappings in the context of dimension-ality reduction, however. In particular, a diffeomorphic m apping has the property that it does not lose any information. That is, given the mapping itself and t he lower dimensional representation of the data set, it is always possible to reconstruct the origin al data.
 There has been significant interest from within the image pro cessing community, in the construction of diffeomorphic mappings for the purpose of image warping ( Dupuis &amp; Grenander, 1998; Joshi &amp; Miller, 2000; Karac  X ali &amp; Davatzikos, 2003). The reason fo r this can be understood as follows. Let I : U  X  R 3 represent the RGB values of an image, where U  X  R 2 is the image plane. If we now define the warped version of I to be I  X  W , then we can guarantee that the warp is topology preserving, i.e. that it does not  X  X ear X  the image, by ensuring the W be a diffeomorphism U  X  U . The following two main approaches to constructing such diff eomorphisms have been taken by the image processing community, the first of which we mention for reference, while the second forms the basis of Diffeomap. It is a notable aside that there seem t o be no image warping algorithms analogous to the back constrained GP-LVM, in which regular f orward and inverse mappings are simultaneously constructed. x 0 s 1 Figure 1: The relationship between v ( , ) ,  X  ( , ) and  X  ( ) for the one dimensional case  X  : R  X  R . 2.1 Diffeomorphisms via Flow Fields The idea here is to indirectly define the mapping of interest, call it  X  : R a  X  R a , by way of a  X  X ime X  indexed velocity field v : R a  X  R  X  R a . In particular we write  X  ( x ) =  X  ( x , 1) , where This choice of  X  satisfies the following Eulerian transport equation with bo undary conditions: The role of v is to transport a given point x from its original location at time 0 to its mapped location  X  ( x , 1) by way of a trajectory whose position and tangent vector at ti me s are given by  X  ( x ,s ) and regularity properties, then the mapping  X  will be a diffeomorphism. This fact has been proven in a number of places X  X ne particularly accessible example is (Du puis &amp; Grenander, 1998), where the necessary conditions are provided for the three dimensiona l case along with a proof that the induced mapping is a diffeomorphism. Generalizing the result to hig her dimensions is straightforward X  X his fact is stated in (Dupuis &amp; Grenander, 1998) along with the ba sic idea of how to do so. We now offer an intuitive argument for the result. Consider F igure 1, and imagine adding a new starting point x  X  , along with its associated trajectory. It is clear that for t he mapping  X  to be a diffeomorphism, then for any such pair of points x and x  X  , the associated trajectories must not collide. This is because the two trajectories would be ident ical after the collision, x and x  X  would map to the same point, and hence the mapping would not be inver tible. But if v is sufficiently regular then such collisions cannot occur. The framework of Eulerian flow fields which we have just introd uced provides an elegant means of constructing diffeomorphic mappings R a  X  R a , but for dimensionality reduction we require additional ingredients, which we now introduce. The basic i dea is to construct a diffeomorphic mapping in such a way that it maps our data set near to a subspac e of R a , and then to project onto this subspace. The subspace we use, call it S b , is the b -dimensional one spanned by the first b canonical basis vectors of R a . Let P ( a  X  b ) : R a  X  R b be the projection operator which extracts the first b components of the vector it is applied to, i.e.
 where I  X  R a  X  a is the identity matrix and Z  X  R a  X  b  X  a is a matrix of zeros. We can now write the mapping  X  : R a  X  R b which we propose for dimensionality reduction as where  X  is given by (2). We choose each component of v at each time to belong to a reproducing kernel Hilbert Space (RKHS) H , so that v ( ,t )  X  X  a ,t  X  [0 , 1] . If we define the norm 1 morphism, provided that some technical conditions are sati sfied (Dupuis &amp; Grenander, 1998; Joshi &amp; Miller, 2000). In particular v need not be regular in its second argument. For dimensionali ty reduction we propose to construct v as the minimizer of where  X   X  R + is a regularization parameter. Here, L measures the squared distance to our b dimensional linear subspace of interest S b , i.e.
 Note that this places special importance on the first b dimensions of the input space of interest X  accordingly we make the natural and important preprocessin g step of applying PCA such that as much as possible of the variance of the data is captured in the se first b dimensions. 3.1 Implementation One can show that the minimizer in v of (7) takes the form where k is the reproducing kernel of H and  X  d is a function [0 , 1]  X  R m . This was proven directly for a similar specific case (Joshi &amp; Miller, 2000), but we note in passing that it follows immediately from the celebrated representer theorem of RKHS X  X  (Sch  X  olkopf et al., 2001), by considering a fixed time t . Hence, we have simplified the problem of determining v to one of determining m trajectories  X  ( x j , ) . This is because not only does (9) hold, but we can use standar d manipulations (in the context of kernel ridge regression, for example) to determi ne that for a given set of such trajectories, functions (including the Gaussian kernel which we employ in all our Experiments, see Section 4), provided that the set  X  ( x j ,t ) are distinct. Hence, one can verify using (9), (10) and the re producing property of k in H ( i.e. the fact that h f,k ( x , ) i H = f ( x ) ,  X  f  X  X  ), that for the optimal v , This allows us to write our objective (7) in terms of the m trajectories mentioned above: So far no approximations have been made, and we have construc ted an optimal finite dimensional basis for v ( ,t ) . The second argument of v is not so easily dealt with however, so as an approximate by discretizing the interval [0 , 1] . In particular, we let t k = k X ,k = 0 , 1 ,...,p , where  X  = 1 /p , Figure 2: Dimensionality reduction of motion capture data. (a) The data mapped from 102 to 2 dimensions using Diffeomap (the line shows the temporal or der in which the input data were recorded). (b)-(d) Three rendered input points correspond ing to the marked locations in (a). of our problem which is finite dimensional and hence readily o ptimized, i.e. the minimization of 3.2 A Practical Reduced Set Implementation A practical problem with (13) is the computationally expens ive matrix inverse. In practice we reduce this burden by employing a reduced set expansion which repla ces the sum over 1 , 2 ,...,m in (9) with a sum over a randomly selected subset I , thereby using |I| = n basis functions to represent objective function is identical to (13), but with the matrix K ( t k )  X  1 replaced by the expression where K m,n = K  X  n,m  X  R m  X  n is the sub-matrix of K ( t k ) formed by taking all of the rows, but only those columns given by I . Similarly, K n,n  X  R n  X  n is the square sub-matrix of K ( t k ) formed by taking a subset of both the rows and columns, namely those g iven by I . For optimization we also use the gradients of the above expression, the derivati on of which we have omitted for brevity. Note however that by factorizing appropriately, the comput ation of the objective function and its gradients can be performed with an asymptotic time complexi ty of n 2 ( m + a ) . It is difficult to objectively compare dimensionality reduc tion algorithms, as there is no universally agreed upon measure of performance. Algorithms which are ge neralizations or variations of older ones may be compared side by side with their predecessors, bu t this is not the case with our new algorithm, Diffeomap. Hence, in this section we attempt to c onvince the reader of the utility of our approach by visually presenting our results on as many and as varied realistic problems as space permits, while providing pointers to comparable results fr om other authors. For all experiments we fixed the parameters which trade off between computationa l speed and accuracy, i.e. we set the temporal resolution p = 20 , and the number of basis functions n = 300 . We used a Gaussian kernel function k ( x , y ) = exp  X  X  x  X  y k 2 / (2  X  2 ) , and tuned the  X  parameter manually along with the regularization parameter  X  . For optimization we used a conjugate gradient type method 2 fixed to 1000 iterations and with starting point [ X  k,d ] j = [ x j ] d ,k = 1 , 2 ,...p . Figure 3: Vowel data mapped from 24 to 2 dimensions using (a) P CA and (b)-(c) Diffeomap. Plots (b) and (c) differ only in the parameter settings of Diffeoma p, with (b) corresponding to minimal one nearest neighbor errors in the low dimensional space X  X ee Section 4.2 for details. 4.1 Motion Capture Data The first data set we consider consists of the coordinates in R 3 of a set of markers placed on a person breaking into a run, sampled at a constant frequency, result ing in m = 217 data points in a = 102 dimensions, which we mapped to b = 2 dimensions using Diffeomap (see Figure 2). This data set is freely available from http://accad.osu.edu/research/mocap/mocap_data.htm as Figure 1 Run , and was also considered in (Lawrence &amp; Candela, 2006), wher e it was shown that while the original GP-LVM fails to correctly discover t he periodic component of the sequence, the back constrained version maps poses in the same part of th e subject X  X  step cycle nearby to each other, while simultaneously capturing variations in t he inclination of the subject. Diffeomap also succeeded in this sense, and produced results which are competitive with those of the back constrained GP-LVM. 4.2 Vowel Data In this next example we consider a data set of a = 24 features (cepstral coefficients and delta cepstral coefficients) of a single speaker performing nine d ifferent vowels 300 times per vowel, acquired as training data for a vocal joystick system (Bilme s &amp; et.al., 2006), and publicly available in pre-processed form from http://www.dcs.shef.ac.uk/  X  neil/fgplvm/ . Once again we used Diffeomap to map the data to b = 2 dimensions, as depicted in Figure 3. We also depict the poor result of linear PCA, in order to rule out the hypothe sis that it is merely the PCA based initialization of Diffeomap (mentioned after equation (8) on page 4) which does most of the work. The results in Figure 3 are directly comparable to those prov ided in (Lawrence &amp; Candela, 2006) for the GP-LVM, back constrained GP-LVM, and Isomap (Tenenb aum et al., 2000). Visually, the Diffeomap result appears to be superior to those of the GP-LV M and Isomap, and comparable to the back constrained GP-LVM. We also measured the performance o f a one nearest neighbor classifier applied to the mapped data in R 2 . For the best choice of the parameters  X  and  X  , Diffeomap made 140 errors, which is favorable to the figures quoted for Isoma p (458), the GP-LVM (226) and the back constrained GP-LVM (155) in (Lawrence &amp; Candela, 2006) . We emphasize however that this measure of performance is at best a rough one, since by manual ly varying our choice of the param-eters  X  and  X  , we were able to obtain a result (Figure 3 (c)) which, althoug h leads to a significantly higher number of such errors (418), is arguably superior fro m a qualitative perspective to the result with minimal errors (Figure 3 (b)). 4.3 USPS Handwritten Digits We now consider the USPS database of handwritten digits (Hul l, 1994). Following the methodol-ogy of the stochastic neighbor embedding (SNE) and GP-LVM pa pers (Hinton &amp; Roweis, 2003; Lawrence, 2004), we take 600 images per class from the five cla sses corresponding to digits 0, 1, 2, 3, 4. Since the images are in gray scale and a resolution of 16 b y 16 pixels, this results in a data set of m = 3000 examples in a = 256 dimensions, which we again mapped to b = 2 dimensions as depicted in Figure 4. The figure shows the individual points c olor coded according to class, along Figure 4: USPS handwritten digits 0-4 mapped to 2 dimensions using Diffeomap. (a) Mapped points color coded by class label. (b) A composite image of the mappe d data X  X ee Section 4.3 for details. with a composite image formed by sequentially drawing each d igit in random order at its mapped location, but only if it would not obscure a previously drawn digit. Diffeomap manages to arrange the data in a manner which reveals such image properties as di git angle and stroke thickness. At the same time the classes are reasonably well separated, with th e exception of the ones which are split into two clusters depending on the angle. Although unfortun ate, we believe that this splitting can be explained by the fact that (a) the left-and right-pointin g ones are rather dissimilar in input space, and (b) the number of fairly vertical ones which could help to connect the left-and right-pointing ones is rather small. Diffeomap seems to produce a result whi ch is superior to that of the GP-LVM (Lawrence, 2004), for example, but may be inferior to that of the SNE (Hinton &amp; Roweis, 2003). We believe this is due to the fact that the nearest neighbor grap h used by SNE is highly appropriate to the USPS data set. This is indicated by the fact that a nearest nei ghbor classifier in the 256 dimensional input space is known to perform strongly, with numerous auth ors having reported error rates of less than 5% on the ten class classification problem. 4.4 NIPS Text Data Finally, we present results on the text data of papers from th e NIPS conference proceedings volumes 0-12, which can be obtained from http://www.cs.toronto.edu/  X  roweis/data.html .
 This experiment is intended to address the natural concern t hat by working in the input space rather than on a nearest neighbor graph, for example, Diffeomap may have difficulty with very high dimen-sional data. Following (Hinton &amp; Roweis, 2003; Song et al., 2 008) we represent the data as a word frequency vs. document matrix in which the author names are treated as word s but weighted up by a factor 20 ( i.e. an author name is worth 20 words). The result is a data set of m = 1740 papers represented in a = 13649 words + 2037 authors = 15686 dimensions. Note however that the input dimensionality is effectively reduced by the PCA preproces sing step to m  X  1 = 1739 , that being the rank of the centered covariance matrix of the data.
 As this data set is difficult to visualize without taking up la rge amounts of space, we have included the results in the supplementary material which accompanie s our NIPS submission. In particular, we provide a first figure which shows the data mapped to b = 2 dimensions, with certain authors (or groups of authors) color coded X  X he choice of authors and thei r corresponding color codes follows precisely those of (Song et al., 2008). A second figure shows a plain marker drawn at the mapped locations corresponding to each of the papers. This second fi gure also contains the paper title and authors of the corrsponding papers however, which are revea led when the user moves the mouse over the marked locations. Hence, this second figure allows o ne to browse the NIPS collection con-textually. Since the mapping may be hard to judge, we note in p assing that the correct classification rate of a one nearest neighbor classifier applied to the resul t of Diffeomap was 48%, which compares favorably to the rate of 33% achieved by linear PCA (which we u se for preprocessing). To compute this score we treated authors as classes, and considered onl y those authors who were color coded both in our supplementary figure and in (Song et al., 2008). We have presented an approach to dimensionality reduction w hich is based on the idea that the map-ping between the lower and higher dimensional spaces should be diffeomorphic. We provided a justification for this approach, by showing that the common i ntuition that dimensionality reduction algorithms should approximately preserve pairwise distan ces of a given data set is closely related to the idea that the mapping induced by the algorithm should be a diffeomorphism. This realization allowed us to take advantage of established mathematical ma chinery in order to convert the dimen-sionality reduction problem into a so called Eulerian flow pr oblem, the solution of which is guar-anteed to generate a diffeomorphism. Requiring that the map ping and its inverse both be smooth is reminiscent of the GP-LVM algorithm (Lawrence &amp; Candela, 20 06), but has the advantage in terms of statistical strength that we need not separately estimat e a mapping in each direction. We showed results of our algorithm, Diffeomap, on a relatively small m otion capture data set, a larger vowel data set, the USPS image data set, and finally the rather high d imensional data set derived from the text corpus of NIPS papers, with successes in all cases. Sinc e our new approach performs well in practice while being significantly different to all previou s approaches to dimensionality reduction, it has the potential to lead to a significant new direction in the field.

