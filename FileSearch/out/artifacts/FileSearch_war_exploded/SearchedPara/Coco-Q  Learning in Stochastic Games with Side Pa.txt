 Brown University, 115 Waterman Street, Providence, RI 02912-1910 The field of reinforcement learning (Sutton &amp; Barto, 1998) is concerned with agents that improve their behavior in sequential environments through inter-action. One of the best known and most ver-satile reinforcement-learning (RL) algorithms is Q -learning (Watkins &amp; Dayan, 1992), which is known to converge to optimal decisions in environments that can be characterized as Markov decision processes. Q -learning is best suited for single-agent environ-ments; nevertheless, it has been applied in multi-agent environments, with varying degrees of success (Sand-holm &amp; Crites, 1995; Gomes &amp; Kowalczyk, 2009). Minimax-Q (Littman, 1994) is a version of Q -learning for two-agent zero-sum games, which converges to op-timal (meaning, minimax) decisions for these games. In general-sum games, however, the learning problem has proven much more challenging. Nash-Q (Hu &amp; Wellman, 2003) is an attempt to use Q -learning in the general setting, but its update rule is inefficient and it lacks meaningful convergence guarantees (Bowl-ing, 2000; Littman, 2001). Correlated-Q (Greenwald &amp; Hall, 2003) is an improvement over Nash-Q in that, in exchange for access to a correlating device, its update rule is efficient. However, there are environments for which correlated-Q is unable to converge to stationary, optimal decisions (Zinkevich et al., 2005).
 As in the correlated-Q work, we consider what can be achieved if agents are afforded extra power. Instead of a correlating device, we consider the impact of binding agreements on our ability to design learning algorithms that converge to stationary, optimal decisions. In Section 2, we provide the necessary background on coco values as a solution concept for normal-form games. In Section 3, we generalize coco values to stochastic games, and prove that a variant of Q -learning based on this concept converges. Section 4 introduces a set of grid games and compares and con-trasts the solutions that result from the coco concept to the aforementioned multi-agent learning algorithms. Coco values were introduced by Kalai &amp; Kalai (2010) as a solution to two-player normal-form games where players have transferable utility (TU): a common cur-rency equally valued by both players. The coco -value solution concept takes advantage of the extra powers players have when making binding agreements that specify a joint action and include a side payment  X  X  transfer of utility from one player to the other. Coco gets its name from the way it is calculated: by decomposing the game into a co operative (or team) game and a co mpetitive (or zero-sum) game, and then combining the solutions to these games.
 Consider a scenario where you and a short, but strong, friend are picking bananas. Your friend cannot reach any bananas, and you can only reach two. If you try to climb on your friend to reach more bananas, you fail. However, if your friend is willing to give you a boost, you can climb and get two high bananas, as well as the two low ones. This game is depicted in Figure 1. If utility is not transferable, your friend has no in-centive to help you pick bananas. But, if it is, then offering her some bananas might encourage her to give you a boost. How many of the four bananas you pick should you offer her? One bite? All four? Coco values offer a focal point X  X ou should offer her one banana for her efforts. The coco values of this game are (1 , 3). 2.1. Operators on Values A two-player normal-form game  X  A, A,U, U  X  is played by a player, Ego , and another player, Alter . Here, A is Ego X  X  set of actions and A is Alter X  X . The values for the game are defined by U for Ego and U for Alter. For action a  X  A and a  X  A , U ( a, a ) is Ego X  X  value and U ( a, a ) is Alter X  X .
 We now define a few operators on values. The maxmax (or friend) operator finds the highest possible U value: This operation is computationally straightforward: simply maximize over all the entries in U .
 The minmax (or foe) operator finds the highest possi-ble worst-case value for Ego: Here,  X ( X ) represents the space of probability distributions over a discrete set X . Note that minmax(  X  U ) =  X  minmax( U ). This operator is also relatively straightforward to compute, as it can be cal-culated in polynomial time using linear programming. The Nash operator selects a value for Ego according to a Nash equilibrium. Specifically, where  X  and  X  are probability distributions that con-stitute some Nash equilibrium of the bimatrix game. This operator is more challenging to compute, as the problem of computing any Nash equilibrium is now known to be PPAD-complete (Chen &amp; Deng, 2006) and NP-hard for the welfare-maximizing equi-librium (Gilboa &amp; Zemel, 1989). Perhaps worse, the value of the operator is not well defined, as there can be multiple conflicting Nash equilibria.
 The CE operator is analogous, but it selects its value using a correlated equilibrium. Specifically, where  X  is a probability distribution over joint actions constituting a correlated equilibrium of the bimatrix game. In this paper, we restrict our attention to the correlated equilibrium operator that maximizes total welfare: that is, the sum of the players X  values. Like minmax, this operator can be computed in polynomial time using linear programming. It need not produce a unique value, however, as there can be multiple cor-related equilibria with the same total welfare but a different allotment of values for the two players. 2.2. Coco Value Operator With this notation established, we can write the coco value of a bimatrix game as an operator as follows:
Coco( U, U ) For all other operators, the players X  joint actions are chosen from the set that yields (one of) the operator X  X  values. In the case of coco , the players play a welfare-maximizing joint action: Then, to their respective values, they add a transfers (or side payment), which, for each player, amounts to the difference between its coco value and its share of the welfare-maximizing values: The side payments P , received by Ego, and P , received by Alter, are: A positive P indicates a payee, and a negative P indi-cates a payer. Payments balance because P =  X  P . To solidify how exactly coco values are computed, consider the following example (Figures 2 and 3). The original game f (Figure 2) decomposes into a team game and zero-sum game (Figure 3). We say  X  X ecom-poses X  because the sums of the team and zero-sum game values match those for Ego in the original game and the differences match those for Alter. The coco values are then the sum of the solutions of these two games, namely (2 , 2)+( . 5 ,  X  . 5) = (2 . 5 , 1 . 5). To achieve these values, the agents play ( a 2 , a ) and receive values (0 , 4), and then Alter pays Ego 2 . 5. Ego wields a great deal of power in this game, and only agrees to play a 2 because of the promise of substantial side payments. Note that it would be rational, and indeed a Nash equi-librium, for Ego to accept a smaller value, say . 5, or for Alter to offer a larger side payment, say 3 . 5. Coco val-ues offer a  X  X ecommended focal point X  in games where side payments and binding agreements support many Nash equilibria (Kalai &amp; Kalai, 2012).
 Because coco values are based on side payments, they also allow players to threaten each other. For example, if Ego X  X  values were reversed for a 1 and a 2 , making it rational for Ego to play a 2 , the coco values would be (2 . 5 , 2 . 5). Alter would still need to pay Ego to safeguard against Ego X  X  incredible threat of playing a 1 . 2.3. Coco Value Properties Though relatively simple, the coco operator has some remarkable properties. First, since it is a combination of the maxmax and minmax operators, both of which can be computed efficiently, coco too can be com-puted efficiently. Further, since each of their values is unique, so too is coco  X  X .
 Next, observe that the sum of the players X  coco values is the maximum joint value possible: Coco( U, U ) + Coco( U,U ) = maxmax(( U + U ) / 2) + minmax(( U  X  U ) / 2) + = maxmax( U + U ) . (1) On the other hand, the difference between the two coco values is the minmax of the difference: Coco( U, U )  X  Coco( U,U ) = (maxmax(( U + U ) / 2) + minmax(( U  X  U ) / 2))  X  = minmax( U  X  U ) . (2) The coco value is also known (Kalai &amp; Kalai, 2012) to be the only value that satisfies a set of five desirable axioms, which includes Pareto efficiency.
 Next, we show how the coco operator can be applied to stochastic games. A two-player stochastic game (Shapley, 1953)  X  S,A, A,T,R, R, X   X  is defined by a state space S , ac-tion spaces A and A for Ego and Alter, a transition function T mapping states and joint actions to proba-bility distributions over states, reward functions R and R mapping states and joint actions to rewards for the two players, and a discount factor  X  .
 If the number of actions available to Alter is one ( | A | = 1), the environment is a Markov decision pro-cess (MDP) (Puterman, 1994) and Ego X  X  objective is to maximize its expected discounted future reward. 3.1. Generalized Q -learning in Games Q -learning (Watkins &amp; Dayan, 1992) learns behavior from experience while acting in an MDP. We define a generalized Q -learning algorithm for 2-player games. Each player keeps an estimate ( Q values) of the ex-pected future discounted reward starting in state s and taking joint action ( a, a ). Ego X  X  estimate is writ-ten Q s ( a, a ) or Q s , and Alter X  X , Q s ( a, a ) or Q s estimate is updated based on the agent X  X  experience in its environment.
 Let  X  s,a, a,r, r,s 0  X  be an experience tuple that reflects that the players observe a transition from state s to state s 0 after Ego takes action a and Alter a . The players receive values r and r , respectively. General-ized Q -learning updates Q and Q as follows: Q 0 and Q 0 represent the updated versions of Q and Q ,  X  is a learning rate, and  X  a discount factor. In general, it is desirable for the Q values to converge to the solution of the following system of equations, which we call the solution of the game . Given operator  X  , for all s , a , and a ,
Q s ( a, a ) = R s ( a, a ) +  X  X and likewise for Q .
 Depending on the choice of  X  , different solutions, and correspondingly, different learning algorithms, re-sult. In an MDP,  X  = max recovers the original Q -learning algorithm, which converges to the corre-sponding solution of the game. In a zero-sum game ( R =  X  R ),  X  = minmax, resulting in an algorithm called minimax-Q or foe-Q . In a team game ( R = R ),  X  = maxmax, resulting in an algorithm called friend-Q (Littman, 2001). Each of friend-Q and foe-Q con-verge to the corresponding solution of the game. For general-sum games, the situation is more complex. Nash-Q uses  X  = Nash and CE-Q uses  X  = CE. While both definitions are plausible, the Q function alone does not contain enough information to identify a solution of the game (Zinkevich et al., 2005). Existing proofs of convergence of variants of Q -learning make use of the following result. If  X  is a non-expansion , then Equation 3 has a unique solution and Equation 3 converges to it (Littman &amp; Szepesv  X ari, 1996). For  X  to be a non-expansion, we need the fol-lowing to be true for all functions f and f 0 , That is, the summaries of two functions should be no further apart than the functions themselves. Op-erators max, maxmax, and minmax are all non-expansions, and convergence is guaranteed. Operators Nash and CE are not non-expansions and convergence is not guaranteed.
 The Coco operator is not a non-expansion. To see why, compare game f in Figure 2(Left) to game f 0 in Figure 2(Right). For this pair of games, we have maxmax | f  X  f 0 | = 1; the values between corresponding values in the two games differ by at most one. In Section 2.2, we show that Coco f = (2 . 5 , 1 . 5). By the same process, Coco f 0 = (3 , 3). The largest difference between coco values is 1.5, so the Coco operator is not a non-expansion.
 Although, the Coco operator is not a non-expansion, Coco-Q , the Q -learning algorithm that arises from defining  X  = Coco, still converges! We prove this re-sult next. Then, in the remainder of the paper, we describe experimental results that demonstrate that Coco-Q learns sound policies. 3.2. Convergence of Coco-Q Let Q and Q be a pair of initial functions. 1 We will maintain two auxiliary functions, Z (zero-sum) and C (common interest/team), defined as Z s = ( Q s  X  Q s ) / 2 and C s = ( Q s + Q s ) / 2.
 Let  X  s,a, a,r, r,s 0  X  be an experience tuple, which we will use to update the Q values as follows:
Z 0 s = Z s +  X  a, a (( r  X  r ) / 2 +  X  minmax( Z s 0 )  X  Z
C 0 s = C s +  X  a, a (( r + r ) / 2 +  X  maxmax( C s 0 )  X  C We claim that the following property holds. If Z s = ( Q s  X  Q s ) / 2 and C s = ( Q s + Q s ) / 2, then Z 0 s = ( Q Q s ) / 2 and C maintain this relationship between Q , Q , Z , and C . The relationship for Z 0 s holds because ( Q 0 s  X  Q 0 s ) / 2 (The second equality follows from Equation 2.) The relationship for C 0 holds by analogous reasoning. Given this relationship, it is not necessary to explicitly maintain all four functions. We could keep track of Q and Q and, at any time, produce Z s = ( Q s  X  Q s ) / 2 and C s = ( Q s + Q s ) / 2. Or, we could keep track of Z and C and, at any time, produce and Q s = C s + Z s and Q s = C s  X  Z s . Although the natural implementation is the former, the latter is the one for which convergence is evident. In particular, note that the Z update is pre-cisely minimax-Q on the reward function ( R s  X  R s ) / 2, which converges. Similarly, the C update is precisely friend-Q on the reward function ( R s + R s ) / 2, which also converges. Since the sum and difference of con-vergent sequences converge, Coco-Q converges.
 In fact, viewed from another perspective, it is not at all surprising that Coco-Q converges. A stochastic game is just a succinct representation of a normal-form game in which the two players select policies. As such, we have every reason to expect that the coco operator would apply to stochastic games and retain its formal properties. Indeed, the coco values of a general-sum stochastic game decompose into the sum and difference of the solutions of the corresponding team and zero-sum stochastic games, analogously to the normal-form case discussed by Kalai &amp; Kalai (2010). As shown above, Coco-Q converges to the set of values defined by Equation 3. We now introduce a set of sam-ple stochastic games and analyze their corresponding values. The results we present use value iteration in-stead of Q -learning to derive the Q functions, as the former converges faster and with less noise, making it a useful tool for understanding the behavior of Coco-Q . 4.1. Grid Game Specification A grid game is played on a grid of m  X  n squares. Each agent has associated with it a starting square on the grid and a (possibly empty) set of goal squares, where it receives rewards. Agents can observe their own and others X  positions in the grid, as well as walls and semi-walls, which impede movement to varying degrees. At each time step, all agents simultaneously choose an action from the set { up, down, left, right, stick } . Every action except stick incurs a step cost, even if the agent is unable to move as intended. If an agent X  X  selected move is unimpeded, the agent moves in the direction specified by that move. If an agent tries to move through a wall, or to a square that is already occupied by an agent who sticks, the agent remains in its current square. If an agent tries to move through a semi-wall, it will do so with probability p ; otherwise, it remains in its current square. If both agents try to move into the same square, including goals, at the same time, one of the agents is chosen X  uniformly at random X  X o do so; the other remains in its current square.
 The game ends once either agent reaches one of its goal squares. If multiple agents reach their respec-tive goal squares simultaneously, they all receive their respective rewards. In our experiments, unless other-wise specified, we set goal rewards = 100, step costs =  X  1, and p = 0 . 5. Initially, we set the discount factor  X  = 1, for ease of interpretation of the values; later, we set  X  = 0 . 95 to illustrate what can happen when agents prefer to reach their goal sooner than later. A policy  X  is tuple of mappings, one per player, from states to actions. A trajectory for a joint policy  X  is a possible sequence of states and actions that could arise when agents play  X  . We denote by V  X  i player i  X  X  expected discounted reward at its start state under joint policy  X  , dropping the superscript  X  when it is clear from context.
 Figure 4(a) shows a sample grid game with agent tra-jectories overlaid. In this and all subsequent games, agents are depicted on the grid by A and B . Agent goals are shown as squares with diagonal lines pass-ing through them (from the lower-left to upper-right corner for A  X  X , and from the upper-left to lower-right corner for B  X  X ). If both agents have a goal in the same square, both sets of diagonal lines are shown. Trajec-tories are shown as a sequence of arrows pointing from the agent X  X  current square to its next square. Any time an agent moves to a new square, the corresponding ar-row of the trajectory is labeled with the time step. A stick action is shown as a circle. If the agent attempts to move to a square but cannot because of an obstacle, an arrow is shown in the direction the agent attempted to move, but the arrow stops at the square at which the agent was stopped. Further, if side payments oc-cur, underneath the grid we show the payment that was made to A at each step, so that positive values mean A received a positive payment. ( B  X  X  side pay-ment is the negative of this number.) Concretely, in the game shown in Figure 4(a), each agent has a sin-gle goal two squares above its starting square. If both agents play up twice, then both agents will reach their goals at the same time, each receiving a goal reward of 100 minus a step cost of 2 (assuming  X  = 1). 4.2. Example Games We now present some specific grid games designed to illustrate properties of coco values. We compare the coco policies to those of Correlated-VI , the result of solving Equation 3 with  X  = CE. Specifically, we use the utilitarian variant of CE (Greenwald &amp; Hall, 2003), which, in the case of multiple correlated equi-libria, chooses an equilibrium that maximizes the sum of the agents X  rewards.
 Coordination In Coordination (Figure 4(b)), A and B have to cross paths without colliding to reach their goals, which are diagonally across the grid from their starting square. While Q -learning has been shown to have poor performance in this game, Nash-Q and CE-Q , which don X  X  use transfers, have been shown to coordinate to reach the efficient outcome (Hu &amp; Wellman, 2003; Greenwald &amp; Hall, 2003), in which V A = V B = 96 (assuming  X  = 1).
 Coco also takes actions that bring agents directly to their goals (and, hence, yield rewards of 96 for both), making interesting side payments along the way. In the particular trajectory shown in Figure 4(b), in step 1, both agents move up with no transfers, but in step 2, A pays B a small amount for B to move left while A moves up onto B  X  X  goal. B is being (temporarily) compensated for assuming a vulnerable position: In the resulting state, A can guarantee B never reaches its goal, while B cannot do the same to A . Once A reaches a square where it is no longer a threat to B , it gets its side payments back from B .
 Prisoner Figure 5 depicts Prisoner , a grid game ver-sion of the well-known normal-form game, the prison-ers X  dilemma. Each agent has its own goal at one end of the grid, and there is a shared goal in the center. This grid game resembles the prisoners X  dilemma be-cause moving to the shared goal ( X  X efect X ) is a dom-inant strategy for each agent: if A is moving to the shared goal, B prefers to also move to the shared goal, and possibly win the collision tiebreaker, since other-wise the game ends before B can reach either goal. If A moves toward its own goal ( X  X ooperate X ), B still prefers to move to the shared goal since that way it immediately reaches a goal without incurring any ad-ditional step costs or wasting any time. However, the agents each receive higher expected value when they both cooperate than when they both defect.
 Figure 5(a) depicts the Correlated-VI policy. Both agents play their dominant strategy, and each gets into the shared goal with probability 0 . 5, so that V A = V B = 49 = (100  X  2)(0 . 5) (assuming  X  = 1). Figure 5(b) shows the unique coco policy in this game. 2 Under this policy, B sticks in place for two steps while A proceeds to its goal, and then both agents walk into their goals simultaneously. The sum of the agents X  values is 196 instead of 98. Note that this solution is analogous to what is computed by FolkE-gal (Munoz de Cote &amp; Littman, 2008).
 The transfer payments made by the coco strategy are of particular interest. First, B pays A to take a step to the left while B sticks. B has to pay A so that A will move into a more vulnerable position. A is now two steps from a goal, while B is only one step away. Second, A pays B 49 to stick instead of moving into the shared goal, giving A time to move next to its own goal. Finally, when A and B are both able to step into their respective goals, no side payments are made. The final values are 98 for A and 98 for B .
 It is noteworthy that the players X  final values are equal in this game. Even though the two players adopt dif-ferent roles X  X ne approaching the near goal and one moving to a distant goal X  X heir expected discounted rewards end up the same. Values are not equal for the two players under FolkEgal or Correlated-VI . To achieve equal values under the policies they produce, it would be necessary to average across policies, with the two players switching roles. Although space does not permit a full proof here, this desirable property is general to coco values X  X layers in symmetric games have symmetric values.
 Turkey In Turkey 3 (Figure 6), A and B have their own goals, in the top left and top right corners, respec-tively, and a shared goal near the center of the grid. The thick dashed lines on the grid represent semi-walls. By way of comparison, Figure 6(a) shows a possible Correlated-VI trajectory for one possible Correlated-VI policy (with values V A = 43 . 20 and V B = 87 . 40). In this trajectory, B takes the shortcut around the semi-wall, while A tries to pass through the other, but fails. After one failed attempt, A sticks, since B will reach the shared goal and the game will end before A can reach its own goal. Had A been successful at passing through the semi-wall, it would have gone directly to its goal and reached it at the same time as B . Interestingly, there are exactly three possible trajecto-ries for coco . Figure 6(b) shows the first such trajec-tory (which occurs with probability p ). This trajectory is the same as what happens in Correlated-VI when A succeeds at passing through the semi-wall: both play-ers march to their goals. The difference between coco and Correlated-VI is that, in coco , a transfer payment is made from B to A to compensate A for the riskier route. When A happens to make it through the semi-wall, no further transfer payments are made, and A receives more total value than is possible in a game without transfer payments (at B  X  X  expense).
 Figure 6(c) shows what happens when A fails to make it through the semi-wall after the first attempt: unlike Correlated-VI , which sticks, under coco , A makes an-other attempt to pass through the semi-wall. In this trajectory, it succeeds the second time (this happens with probability p , so this trajectory occurs with prob-ability p 2 ), and then makes a transfer to B so that B waits one additional turn for A to catch up, at which point they walk into their goals simultaneously with-out further side payments. Note that B actually pays a small amount to A to make that second attempt at passing through the semi-wall. It does so because the alternative of A moving right on step two would neces-sitate that B pursue its own goal instead of the shared goal, which would incur additional step costs for B . In the third and final possible trajectory for coco (Figure 6(d)), A does not succeed at getting through the wall on the first or second attempt. At that point, B decides to pursue its own goal, and let A pursue the shared goal, rather than risk another block from the semi-wall. For B  X  X  added step cost, A pays B a slightly higher amount than it did when it passed through the semi-wall successfully on the second step. This trajec-tory occurs with probability 1  X  p  X  p 2 .
 Table 1 derives V A and V B for Turkey , by calculating an expected value of V A and V B across trajectories. The column labeled Probability gives the probability of each trajectory, and the columns labeled A and B , re-spectively, give the values of V A and V B for each player for each trajectory. Once again, we see a symmetric game, even in the face of asymmetric roles, resulting in symmetric values.
 Friend or Foe Friend or Foe (Figure 7) is an asym-metric game in which not all goals have equal value. Specifically, A  X  X  individual goal on the extreme left side of the grid has a much higher reward than the shared goal near the center (a reward of 1,000 versus 100). Friend or Foe is distinct from the other games in that, without considering transfer payments, the game has no deterministic equilibrium. In the first round, if A moves left, B  X  X  best response is to also move left, since B is then guaranteed to reach its goal as quickly as possible. But, if B moves left, A has no hope of get-ting its large-valued goal, so it would be better off immediately moving to the shared goal on the right. If A immediately moves right, B might as well stick and not waste step costs. But, if B sticks on the first round, A would be better off moving left in pursuit of its large-valued goal.
 Correlated-VI does not converge to a stationary policy in this instance, but rather it  X  X onverges X  to a pol-icy cycle of length two (Zinkevich et al., 2005). The agents X  joint action at the start state oscillates. Coco , in contrast, converges to a stationary policy. Under the coco policy (see Figure 7), A pays B a share of the large goal value to stick as A moves left. Both agents are then two steps from a goal, and they move to them without making further side payments. Incredible One potential issue with the coco solu-tion concept in normal-form games discussed by Kalai &amp; Kalai (2012) is that players may not be incentivized to abide by a coco policy. Figure 8 depicts the game Incredible , which illustrates this issue. In this game, B receives a larger value for reaching its goal than A receives for reaching its goal (a reward of 1,000 versus 100), but B  X  X  movement towards its goal is impeded by A . If A sticks, B is stuck with at most value 0. The coco policy prescribes that both players move left into their goals, but that B pay A to move left. However, even if B made no side payment, A should still move left towards its goal. B is essentially paying A over the incredible threat that A will stick. 4.3. Summary of Experiments Figure 9 shows each player X  X  values and the total values for the Nash-VI , Correlated-VI , and coco policies for all of the grid games we discussed. As coco values are welfare maximizing, no learning algorithm can achieve higher total values.
 Our experiments illustrate both the intelligent trans-fers made by coco agents, and the interesting proper-ties that arise from playing coco strategies. In Pris-oner and Turkey , we illustrate that coco agents ac-crue symmetric values in symmetric games. In Friend or Foe , we illustrate that, unlike coco , Nash-VI and Correlated-VI are not guaranteed to converge. In In-credible , we illustrate that the coco policy is not al-ways individually rational, in the sense that an agent can achieve a higher value by not abiding by it. We introduced a new algorithm, Coco-Q , that is con-vergent and produces interesting solutions to challeng-ing stochastic games when utility is transferable and binding agreements are possible.
 Coco-Q , like coco values in normal-form games, is not defined for games with three or more players. It is an open problem to generalize to the ideas discussed herein to a wider class of games.
 It remains to be seen whether it is reasonable to ex-pect agents (including people) to be rational about side payments in stochastic settings, but the strategies that Coco-Q exhibits appear sound.
 Bowling, Michael. Convergence problems of general-sum multiagent reinforcement learning. In Proceed-ings of the Seventeenth International Conference on Machine Learning , pp. 89 X 94, 2000.
 Chen, Xi and Deng, Xiaotie. Settling the complexity of two-player Nash equilibrium. In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS) , pp. 261 X 272, 2006. Gilboa, I. and Zemel, E. Nash and correlated equi-libria: Some complexity considerations. Games and Economic Behavior , 1:80 X 93, 1989.
 Gomes, Eduardo Rodrigues and Kowalczyk, Ryszard.
Dynamic analysis of multiagent Q-learning with e-greedy exploration. In Proceedings of the 2009 In-ternational Conference on Machine Learning , 2009. Greenwald, Amy and Hall, Keith. Correlated-Q learning. In Proceedings of the Twentieth Interna-tional Conference on Machine Learning , pp. 242 X  249, 2003.
 Hu, Junling and Wellman, Michael P. Nash q-learning for general-sum stochastic games. The Journal of Machine Learning Research , 4:1039 X 1069, 2003. Kalai, Adam and Kalai, Ehud. Cooperation in strate-gic games revisited*. The Quarterly Journal of Eco-nomics , 2012.
 Kalai, Adam Tauman and Kalai, Ehud. Cooperation and competition in strategic games with private in-formation. In Proceedings of the 11th ACM confer-ence on Electronic commerce , EC  X 10, pp. 345 X 346, New York, NY, USA, 2010. ACM.
 Littman, Michael L. Markov games as a framework for multi-agent reinforcement learning. In Proceed-ings of the Eleventh International Conference on Machine Learning , pp. 157 X 163, 1994.
 Littman, Michael L. Friend-or-foe Q-learning in general-sum games. In Proceedings of the Eighteenth
International Conference on Machine Learning , pp. 322 X 328. Morgan Kaufmann, 2001.
 Littman, Michael L. and Szepesv  X ari, Csaba. A gen-eralized reinforcement-learning model: Convergence and applications. In Saitta, Lorenza (ed.), Proceed-ings of the Thirteenth International Conference on Machine Learning , pp. 310 X 318, 1996.
 Munoz de Cote, Enrique and Littman, Michael L. A polynomial-time Nash equilibrium algorithm for re-peated stochastic games. In 24th Conference on Un-certainty in Artificial Intelligence (UAI X 08) , 2008. Puterman, Martin L. Markov Decision Processes X  Discrete Stochastic Dynamic Programming . John Wiley &amp; Sons, Inc., New York, NY, 1994.
 Sandholm, Tuomas W. and Crites, Robert H. Multia-gent reinforcement learning in the iterated prisoner X  X  dilemma. Biosystems , 37:144 X 166, 1995.
 Shapley, L.S. Stochastic games. Proceedings of the
National Academy of Sciences of the United States of America , 39:1095 X 1100, 1953.
 Sutton, Richard S. and Barto, Andrew G. Reinforce-ment Learning: An Introduction . The MIT Press, 1998.
 Watkins, Christopher J. C. H. and Dayan, Peter. Q-learning. Machine Learning , 8(3):279 X 292, 1992. Zinkevich, Martin, Greenwald, Amy R., and Littman, Michael L. Cyclic equilibria in Markov games. In
Advances in Neural Information Processing Systems
