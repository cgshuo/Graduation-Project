 Department of Statistics and Computer Science Division effectiveness.
 cally suboptimal at t =  X  ( t is the number of iterations).
 This paper provides a constructive answer to all of the mentioned issues: Here we describe the AdaBoost procedure formulated as a coordinate descent algorithm and in-S The quality of the classifier g n is given by the misclassification probability Of course we want this probability to be as small as possible and close to the Bayes risk ability The infimum above is achieved by the Bayes classifier g  X  ( x ) = g (2  X  ( x )  X  1) , where max | S | : S  X  X  , H | S = 2 | S | .
 Define Then the boosting procedure can be described as follows. We shall also use the convex hull of H scaled by  X   X  0 , as well as the set of k -combinations, k  X  N , of functions in H We shall also need to define the l  X  -norm: for any f  X  X  Define the squashing function  X  l (  X  ) to be Then the set of truncated functions is The set of classifiers based on class F is denoted by Define the derivative of an arbitrary function Q (  X  ) in the direction of h as The second derivative Q 00 ( f ; h ) is defined similarly. We shall need the following assumption.
 Assumption 1 Let the distribution P and class H be such that where R  X  = inf R ( f ) over all measurable functions.
 by successive univariate splits), where d is the dimensionality of X (see [4]). We begin with a simple lemma (see [1, Theorem 8] or [11, Theorem 6.1]): Lemma 1 For any t  X  N if d V C ( H )  X  2 the following holds: where d P ( F t ) is the pseudodimension of class F t .
 Koltchinskii and Panchenko [12] and resembles [6, Lemma 2].
 Lemma 2 For a continuous function  X  define the Lipschitz constant and maximum absolute value of  X  (  X  ) when argument is in [  X   X , X  ] Then for functions V = d V C ( H ) , c = 24 R 1 0 q ln 8 e 2 d and any n ,  X  &gt; 0 and t &gt; 0 , and Also, for any  X  &gt; 0 , with probability at least 1  X   X  , and We begin with symmetrization to get to the absolute value equal  X  , therefore we can rescale  X   X   X F t by (2  X  )  X  1 and get Next, we are going to use Dudley X  X  entropy integral [14] to bound the r.h.s above use Pollard X  X  bound [15] for F  X  [0 , 1] X where d P ( F ) is a pseudodimension, and obtain for  X  c = 12 R 1 0 q ln 8 e 2 d with constant c above being independent of H , t and  X  . To prove the second statement we use McDiarmid X  X  bounded difference inequality [16, Theorem 9.2, p. 136], since  X  i proof of the lemma.
 proof of [10, Theorem 1]  X  c 1 ,c 2 , s.t. Q  X  &lt; c 1 &lt; c 2 &lt;  X  , algorithm, the following bound holds  X  m s.t. Q ( f m ) &gt; Q (  X  f ) . B = sup { Q 00 ( f ; h ) : Q ( f ) &lt; Q ( f 0 ) ,h  X  X } .
 l -norm). Then from (7) and linearity of the derivative we have therefore Next, f m +1 we have the following bounds then by assumption of the theorem for  X  , that depends on Q (  X  f ) , we have On the other hand,
Q ( f m +  X  m h m ) = inf Therefore, combining (9) and (10) , we get Another Taylor expansion, this time around f m +1 , gives us | Q 0 ( f m ; h m ) | /B , then but by (10) therefore we conclude, by combining (11) and (8), that Using (12) we have Recall that therefore, combining with (14) and (13), since sequence i is decreasing, Since then Therefore and this completes the proof.
 function output by AdaBoost and the  X  -risk of the appropriate reference function. for n large enough with high probability the following holds Proof. This theorem follows directly from Theorem 1. Because in AdaBoost least 1  X   X  R Then, having all the ingredients at hand we can formulate the main result of the paper. Theorem 3 Assume V = d V C ( H ) &lt;  X  , L  X  &gt; 0 , t classifiers almost surely satisfying L ( g ( f t n ))  X  L  X  .
 have sufficiently large n when (17) holds. The  X  X  above are  X  n = n  X  2 , we have 1  X  0 , 2  X  0 , 3  X  0 and  X  (  X  n )  X  0 . Also, R ( use [17, Theorem 3] to conclude that But for  X  n &gt; 0 we have g (  X   X  n ( f t n )) = g ( f t n ) , therefore Hence AdaBoost is consistent if stopped after n  X  steps. O ( n 1  X   X  ) and O ( n 2 ln n ) . Lessening this gap is a subject of further research. future work to find an analog of Theorem 2 that will handle logit loss. Acknowledgments We gratefully acknowledge the support of NSF under award DMS-0434383. References [2] Leo Breiman. Bagging predictors. Machine Learning , 24(2):123 X 140, 1996. [5] Wenxin Jiang. On weak base hypotheses and their implications for boosting regression and [8] Tong Zhang and Bin Yu. Boosting with early stopping: convergence and consistency. The [13] Michel Ledoux and Michel Talagrand. Probability in Banach Spaces . Springer-Verlag, New [15] David Pollard. Empirical Processes: Theory and Applications . IMS, 1990.
