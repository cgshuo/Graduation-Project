 Kay (2000) points out that if a document is trans-lated once, it is likely to be translated again and again into other languages. This gives rise to an in-teresting idea: a human does the first translation by hand, then turns the rest over to machine translation (MT). The translation system now has two strings as input, which can reduce ambiguity via  X  X riangu-lation X  (Kay X  X  term). For example, the normally ambiguous English word  X  X ank X  may be more eas-ily translated into French in the presence of a sec-ond, German input string containing the word  X  X lus-sufer X  (river bank).
 Och and Ney (2001) describe such a multi-source MT system. They first train separate bilingual MT systems F  X  E , G  X  E , etc. At runtime, they sep-arately translate input strings f and g into candi-date target strings e 1 and e 2 , then select the best one of the two. A typical selection factor is the prod-uct of the system scores. Schwartz (2008) revisits such factors in the context of log-linear models and Bleu score, while Max et al. (2010) re-rank F  X  E n-best lists using n-gram precision with respect to G  X  E translations. Callison-Burch (2002) exploits
A B C &lt;EOS&gt; W X Y Z the framework of neural encoder-decoder models, where multi-target MT (Dong et al., 2015) and multi-source, cross-modal mappings have been ex-plored (Luong et al., 2015a). In the neural encoder-decoder framework for MT (Neco and Forcada, 1997; Casta  X  no and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b), we use a recurrent neural net-work ( encoder ) to convert a source sentence into a dense, fixed-length vector. We then use another re-current network ( decoder ) to convert that vector in a
In this paper, we use a four-layer encoder-decoder system (Figure 1) with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) trained for maximum likelihood (via a softmax layer) with back-propagation through time (Werbos, 1990). For our baseline single-source MT system we use two different models, one of which implements the local attention plus feed-input model from Lu-ong et al. (2015b).
 Figure 2 shows our approach to multi-source MT. Each source language has its own encoder. The question is how to combine the hidden states and cell states from each encoder, to pass on to the decoder. Black combiner blocks implement a function whose input is two hidden states ( h 1 and h 2 ) and two cell states ( c 1 and c 2 ), and whose output is a single hid-there are two forget gates indexed by the subscript i that serve as the forget gates for each of the incom-ing cells for each of the encoders. In equation 5, o represents the output gate of a normal LSTM. i , f , o , and u are all size-1000 vectors. 2.3 Multi-Source Attention Our single-source attention model is modeled off the local-p attention model with feed input from Luong et al. (2015b), where hidden states from the top de-coder layer can look back at the top hidden states from the encoder. The top decoder hidden state is combined with a weighted sum of the encoder hid-den states, to make a better hidden state vector (  X  h t ), which is passed to the softmax output layer. With input-feeding, the hidden state from the attention model is sent down to the bottom decoder layer at the next time step.

The local-p attention model from Luong et al. (2015b) works as follows. First, a position to look at in the source encoder is predicted by equation 9: S is the source sentence length, and v p and W p are learned parameters, with v p being a vector of di-mension 1000, and W p being a matrix of dimension 1000 x 1000.

After p t is computed, a window of size 2 D + 1 is looked at in the top layer of the source encoder cen-tered around p t ( D = 10 ). For each hidden state in this window, we compute an alignment score a t ( s ) , Word tokens 66.2m 59.4m 57.0m Word types 424,832 381,062 865,806 Segment pairs 2,378,112
Ave. segment 27.8 25.0 24.0 length (tokens) also have two separate sets of alignments and there-fore now have two c t values denoted by c 1 t and c 2 t as mentioned above. We also have distinct W a , v p , and W p parameters for each encoder. We use English, French, and German data from a subset of the WMT 2014 dataset (Bojar et al., 2014). Figure 3 shows statistics for our training set. For de-velopment, we use the 3000 sentences supplied by WMT. For testing, we use a 1503-line trilingual sub-set of the WMT test set.

For the single-source models, we follow the train-ing procedure used in Luong et al. (2015b), but with 15 epochs and halving the learning rate every full epoch after the 10th epoch. We also re-scale the normalized gradient when norm &gt; 5. For training, we use a minibatch size of 128, a hidden state size of 1000, and dropout as in Zaremba et al. (2014). The dropout rate is 0.2, the initial parameter range is [ -0.1, +0.1 ] , and the learning rate is 1.0. For the normal and multi-source attention models, we ad-just these parameters to 0.3, [ -0.08, +0.08 ] , and 0.7, respectively, to adjust for overfitting.

Figure 4 shows our results for target English, with source languages French and German. We see that the Basic combination method yields a +4.8 Bleu improvement over the strongest single-source, attention-based system. It also improves Bleu by +2.2 over the non-attention baseline. The Child-Sum method gives improvements of +4.4 and +1.4. We confirm that two copies of the same French input yields no BLEU improvement. Figure 5 shows the action of the multi-attention model during decoding. When our source languages are English and French (Figure 6), we observe smaller BLEU gains (up to +1.1). This is evidence that the more distinct the source languages, the better they disambiguate each other.
 This work was carried out with funding from DARPA (HR0011-15-C-0115) and ARL/ARO (W911NF-10-1-0533).

