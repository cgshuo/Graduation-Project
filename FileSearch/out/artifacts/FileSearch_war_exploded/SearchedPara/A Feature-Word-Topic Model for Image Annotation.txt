 Image annotation is to automatically associate semantic la-bels with images in order to obtain a more convenient way for indexing and searching images on the Web. This pa-per proposes a novel method for image annotation based on feature-word and word-topic distributions. The intro-duction of topics allows us to take word associations, such as { ocean, fish, coral } , into image annotation in an effi-cient way. Feature-word distributions are utilized to define weights in computation of topic distributions for annotation. By doing so, topic models in text mining can be applied di-rectly in our method. Experiments show that our method is able to obtain promising improvements over the state-of-the-art method -Supervised Multiclass Labeling (SML). H.3.1 [ Information Systems ]: Content Analysis and In-dexing Algorithms, Experimentation Image Annotation, Mixture Hierarchies, Topic Models
Image annotation has been an active topic of ongoing re-search for more than a decade and led to several noticeable methods [1, 2, 3, 8, 4, 9]. Among the existing methods, Supervised Multiclass Labeling (SML) is considered as the start-of-the-art thanks to its scalability, robustness and abil-itytoaddres sthe X  X eakl ylabeling  X  X roble m[2].Onedis-advantage of SML is the isolation of words in annotation. In other words, we are unable to exploit word relationships such as { beach, sand, sun } or { building, street, car, people } to obtain topic-consistent annotation.
 more adaptable to different feature extraction methods, or topic modeling.

The rest of this paper is organized in four sections. Section 2 presents the proposed method as well as its relationships with related works. Section 3 shows our experiments and result analysis on two datasets. Finally, some conclusions are given in Section 4
Given a vocabulary of size | V | : V = { w 1 , w 2 , ..., w | V | } , and a training dataset D = { I 1 , I 2 , ..., I N } of N images, in which each image I i is represented by a set of real feature learn a mapping function (model) from image space to word space so that it can be used to automatically annotate new images.
The feature-word distribution is estimated based on Mix-ture Hierarchies and Multiple Instance Learning (MIL), which is the same as SML [2]. Here, we suppose that feature vectors are generated according to conditional distributions p ( x | w ) (see Figure 2a). Let D w be the subset of D contain-ing all the images with w as an annotation, the distribution p ( x | w ) is estimated from D w using a two-stage procedure:
Like pLSA [5, 4, 9] for textual documents, we assume the existence of a latent aspect (topic) z k ( k  X  1 , ..., K ) in the generative process of each word w j ( j  X  1 , ..., T ) associated with an image I i ( i  X  1 , ..., N ). Here, we only care about annotated words, not visual features.

Parameters of pLSA are the conditional probability dis-tributions P ( w | z k ) and P ( z | I i ), which are multinomial. We can obtain the parameters by using EM algorithm [5].
The Feature-Word-Topic model (FWT) for annotation is represented in Figure 3, where p ( w | z ) and p ( x | w ) are esti-mated from the training process, N 0 is the number of test image J , and q ( X | w ) is calculated based on p ( x | w ). Suppose that X is the set of feature vectors of J , we select M words with highest values of feature-word distributions:
As aforementioned, feature-word distribution is estimated based on mixture hierarchies and MIL, which is the same as SML [2]. The difference of our approach compared with SML is the introduction of latent topics in the annotation. For annotating a new image J with SML, words are selected based on p ( w | X ) calculated as follows:
From the equations (3) and (6), we see that SML only integrates word frequencies into image annotation but our method considers word relationships (via topics).
Recently, there were a lot of applications of topic models, which originated from text mining, in image-related prob-lems. Most of current approaches model topics as the joint distribution of feature and words [1, 4, 9].

The difference of our method and previous approaches is that we model topics via words, not words and features (see Figure 4). As a result, we do not need to modify topic models for training, where captions are available. To infer topics for an unannotated image, we only need to consider weights based on p ( x | w ) instead of word occurrence in the original models.
Among the approaches that make use of word relation-ships in annotation [6, 7], our method falls into the post-refinement category. The difference of our method is that we make use of topic model to capture word relationship rather than word-to-word correlations or fine-constructed semantic structures like Wordnet. As a result, we are able to extend the vocabulary easier and take the current advances of topic modeling in text.
We performed experiments on 2 datasets namely UWDB and Corel5k. UWDB is a freely available benchmark for image retrieval 1 . Here, we obtained color images and resized them to 40% of original sizes, which results in 1490 images with size 300  X  200 annotated with 292 unique words. The maximum of words per image is 22, and the minimum is 1.
The Corel5k benchmark is commonly used for image an-notation [3, 2, 4]. It contains 5,000 images divided into a training set of 4,000 images, a validation set of 500 images, and a test set of 500 images. The validation set can be used to tune parameters such as the number of topics K . Each image is labeled with 1 to 5 captions from a vocabulary of 371 distinct words.
Each image I is represented in YBrCr color space. Using a sliding window, we extracted overlapping patches of size 8x8 from I (one patch has three color planes Y, Br, Cr). We next performed discrete cosine transform (DCT) for each color plane of each patch, kept coefficients of lower frequencies, www-i6.informatik.rwth-aachen.de/  X deselaers/uwdb Figure 6: mAPs on Corel dataset evaluated with SML, FWT with different values of K. Here, SML( n ) or FWT ( n ) means we obtained top n words per image for indexing (with SML or FWT).
 Table 1: mAPs of related methods and FWT re-ported on Corel5K words assigned by SML form a near-uniform distribution. Consequently, indexing less words per image reduces cross-word ambiguities, thus provides the better retrieval results for SML. Probabilities of top topic-consistent words of FWT, on the other hand, are significantly higher than the rest of candidate words. As a result, we obtain better results with FWT when increasing the number of words indexed per image. For FWT, further studies can be conducted to estimate the length of topic-consistent annotation instead of fixed length assignment.

Figure 7 shows how topics can help to improve annotation performance. Based on feature-word distributions, the top 20 candidates are selected and shown in the figure. Although the visual representation gives some wrong interpretation, which results in  X  X rch X , or  X  X lephant X  at higher ranks than  X  X rototype X , the reasonable interpretation makes the topic of the scene surpass the others. By taking topics into account, the more reasonable words occur at higher ranks than only based on features. Due to the  X  X emantic gap X , we need more  X  X emantic X  from scene settings to infer reasonable labels.
Table 1 summarizes significant results obtained in our im-plementation of SML, FWT-K models, and two pLSA-based models [4]. Note that these methods used DCT-based fea-ture extraction and were tested on Corel5k. In compari-son with these baselines, FWT shows promising improve-ment. The better implementation of feature-word distribu-tions, which leads to better results for SML in [2], is also expected to improve the performance of FWT.

About time complexity, for annotating one image J , SML requires O ( BL | V | ). Our method needs O ( BL | V | + MKe ) ( e is the number of EM iterations in Section 2). Since BL | V | (  X  6 . 000  X  32  X  292) is much larger than MKe (  X  20  X  50  X  30), the extra annotation time is relatively small.
This paper presented a new method for image annota-tion based on mixture hierarchies and topic modeling. This method has the advantages of SML but considers word re-
