 Community question answering (cQA) has become a popular ser-vice for users to ask and answer questions. In recent years, the efficiency of cQA service is hindered by a sharp increase of ques-tions in the community. This paper is concerned with the problem of question routing. Question routing in cQA aims to route new questions to the eligible answerers who can give high quality an-swers. However, the traditional methods suffer from the following two problems: (1) word mismatch between the new questions and the users X  answering history; (2) high variance in perceived answer quality.

To solve the above two problems, this paper proposes a novel joint learning method by taking both word mismatch and answer quality into a unified framework for question routing. We conduct experiments on large-scale real world data set from Yahoo! An-swers. Experimental results show that our proposed method signifi-cantly outperforms the traditional query likelihood language model (QLLM) as well as state-of-the-art cluster-based language model (CBLM) and category-sensitive query likelihood language model (TCSLM).
 H.3.3 [ Information Systems ]: Information Storage and Retrieval Algorithms, Experimentation, Performance Question Routing, Translation Model, Language Model, Answer Quality
Community question answering (cQA) provides an online ser-vice for users to share their knowledge in the form of questions and answers. cQA portals such as Yahoo! Answers 1 and Baidu Zhidao have attracted increasing number of users and accumulated a large number of questions over the last few years. For example in Yahoo! Answers, it has more than 200 million users worldwide and around 15 million visits daily [12].

Although cQA service has brought significant benefits for users to seek information online, there are still several drawbacks in cur-rent systems. The most important problem is the efficiency of solving a new question. Previous study [12] has shown that more than 80% new questions cannot be resolved efficiently within 48 hours. On the other hand, with the rapidly increasing number of new questions, users who know well the answers to a particular do-main are not easy to find their interested questions, which leads to the low participation rate (that is, most answers or knowledge in cQA comes from minority users) [4]. Besides, answer quality in cQA ranges from very high to low quality, sometimes abusive con-tent or even spam [1]. Although cQA provides many mechanisms for community feedback ("thumbs up" and "thumbs down" votes), such community feedback requires some time to accumulate, and often remains sparse for obscure or unpopular topics.

To address the above problems, several approaches have been proposed in both industry and academic communities. In industry community, Horowitz and Kamvar [5] developed a social search engine, called Aardvark 3 , which routed the question to the person in the user X  X  extended social network most likely to be able to an-swer that question. Recently, a new question answering social net-work called Quora 4 has gained increasing popular. Users in Quora can follow topics and experts as well as following people in Twit-ter, and then answer the questions of the specified topics or route the new questions to experts.

In academic community, question routing has been conducted to tackle the above problems. The task of question routing is to route new questions to the eligible answerers who can give high quality answers [13, 20]. The traditional methods include the query likelihood language model (QLLM) [12], the cluster-based lan-guage model (CBLM) [20], and state-of-the-art category-sensitive language model (TCSLM) [13]. However, two problems of apply-ing these methods to question routing are noted: http://answers.yahoo.com/ http://zhidao.baidu.com/ http://v ark.com http://quora.com/
An answerer profile usually consists of a small number of ques-
Figur e 1: User participation rate in our obtained data set.
To this end, we propose a joint relevance and answer quality learning method for question routing. First, we develop a gen-eral probabilistic model by taking both word mismatch and answer quality into a unified framework. We derive two scoring functions based on the framework: user interest score and answer quality score. Second, we estimate the user interest score by using the im-proved translation model, which models the exact matched words and the translated semantically related words using different ap-proaches. Then, we propose to estimate the answer quality score by taking into account the expertise of answerers and the non-textual features of answers. Finally, we propose a refined strategy to better rank the eligible answerers for a new question. To the best of our knowledge, little work has addressed both the word mismatch and the answer quality variance problems into a unified framework in studies of question routing, which remains an under-explored re-search area. This paper is thus designed to fill the gap.
The rest of this paper is organized as follows. Section 2 presents our proposed joint relevance and answer quality learning method for question routing. Experimental results are presented in Section 3. Finally, we conclude with ideas for future work.
In this paper, question routing is unified by means of a proba-bility model. Specifically, we will rank the users according to the probabilities that a candidate user is  X  X nterest" and  X  X nswer qual-ity" to a new question, and the key challenge is to compute these probabilities.

Formally, let U = f u 1 , u 2 , , u jUj g is the set of users in the community. Let I be a binary random variable to denote interest (1 for interest and 0 for non-interest). Let A be another binary random variable to denote answer quality (1 for high quality and 0 for low quality). Given a new question q and a candidate answerer u we are interested in estimating the conditional probability of a user u being an eligible answerer: tions. Also, the questions and answers themselves are always very short. Therefore, the insufficient word co-occurrence may lead to the word mismatch.

Here, we assume that A and I are independent with each other since the user X  X  interest and the user X  X  answer quality do not have directly relationship. Therefore, we have:
Based on the Bayes X  Theorem and some assumption, we have
Therefore, the question routing framework P ( I = 1 , A = 1 is decomposed into three components, an interest score P ( q 1) , an answer quality score P ( q j u, A = 1) , and a user prior ratio P ( u j I =0 ,A =0) . Following the literature [13, 20], we model the user prior ration as the number of answers u provided divided by the total number of answers. That is, where N ans ( u ) denotes the number of answers provided by u , N total denotes the total number of previously answered answers.
In this paper, we borrow the idea of statistical machine transla-tion and give a thorough analysis how to model the interest score estimation and learn the translation probabilities.
Formally, let D ( u ) = f ( u, q 1 , a ( u, q 1 )) , ( u, q denote the answerer profile of user u , which contains all questions previously answered by u . a ( u, q j ) denotes an answer of question q . For a new question q , u  X  X  interest on q is defined as follows:
Let q = t 1 t j q j and D ( u ) = w 1 w j D ( u ) j . The statistical machine translation model [7, 17] assumes that both q and D ( u ) are bag of words. However, the previous work [7, 17] cannot dif-ferentiate the importance between the exact matched terms and the translated semantically related words in ranking the relevancy of user profile to a new question q . Therefore, we define a improved model P s ( t j D ( u )) used for "seen" words that occur in the user pro-file D ( u ) (i.e., #( t, D ( u )) &gt; 0 ), and a model P u ( t j D ( u )) is used for "unseen" words that do not occur in the user profile (i.e., #( t, D ( u )) = 0 ). The improved translation model (ITR) can be rewritten as follows: = = The improved model P s ( t j D ( u )) can be computed like Jelinek-Mercer smoothed maximum likelihood estimation: The improved model P u ( t j D ( u )) can be computed with Jelinek-Mercer smoothed translation model: P ( t j D ( u )) = (1  X  ) where P ml ( w j D ( u )) is the unigram probability of word w in D ( u ) , and P ( t j w ) denotes the word-to-word translation probability.  X  [0 , 1] is the Jelinek-Mercer smoothing parameter [18]. C = is question-answer collection.

Previous work [7, 17] treat the exact matched words and the translated semantically related words equally, which may lead to non-optimal ranking performance because it is possible that a user profile that matches a new question word exactly ( P ( t j score contribution than a user profile that "matches" a new ques-tion word through translation ( P ( t j w ) ). On the contrary, the im-proved models defined in equation (8) and equation (9) treat the exact matched words and the translated semantically related words using different approaches. That is to say, the improved models only translate the unmatched words. It is thus reasonable to expect that using such improved models is likely to improve the perfor-mance of interest estimation, as we will show in our experiments. To learn the translation probabilities, we use the same way of [19].
In subsection 3.2, we propose a improved translation model for answerers X  interest estimation. This model assumes that a user has high interest on question q if he/she has previously answered some similar questions, but it does not consider the quality of the previ-ous answers. In cQA, a user may answer a great number of ques-tions which are relevant to q , but we cannot draw the conclusion that the user must be an eligible answerer if the previous answers are of low quality. We propose to estimate P ( q j u, A = 1) from previous answers X  qualities of user u i . Similar to [12], we assume that the user X  X  answer quality on the new question q is the weighted average answers X  qualities of similar questions he/she has previ-ously answered: where u  X  q i denotes question q i answered by u with answer previous answer a ( u, q i ) . sim ( q i , q ) is the relevance between q and q . To overcome the problem of word mismatch between the two questions, equation (7) can be used to measure the relevance sim ( q i , q ) , except that we change u  X  X  profile D ( u ) into a single question q i . Due to its symmetry, we thus define a symmetric met-ric to better capture the relevance:
Now we turn to calculate the answer quality score Q ( a ( u, q of u  X  X  previous answer a ( u, q i ) . In this paper, Q ( a ( u, q derived from the asking expertise ( Ask ) and answering expertise ( Ans ) of its answerer u : where u answers q i with answer a ( u, q i ) ,  X  controls the relative im-portance of asking expertise. Based on the question answering rela-tionships in cQA, the asking expertise and the answering expertise can be calculated using the HITS model [10] on the whole question-answer collections as proposed by Jurczyk and Agichtein [9].
However, Jurczyk and Agichtein [9] assumes that each user has the same level of expertise for different topics of questions . While in real applications, we assume that a user should have different levels of expertise when he/she answers different topics of ques-tions. Our claim sounds reasonable since users usually have diverse background knowledge. So the asking expertise and answering ex-pertise by taking the question topics into account at the ( t + 1) iteration are computed based on the answering expertise and asking expertise at the i th iteration as follow: where u ! q j represents u asking question q j , and u a ( u, q represents u answering q j with the answer a ( u, q j ) . Rel ( c reflects the topic relevance between q j and q i , where c egory of question q j , and c i is leaf category of question q
The difference between our method (equation (13) and equation (14)) and previous work [15] is that we leverage the leaf category to represent the question topic instead of representing each question with a vector space model. In this way, our proposed method can substantially alleviate the data sparseness problem and thus make a more accurate estimation, as we will shown in the experiments.
Jeon et al. [8] argued that answers X  goodness was also an impor-tant factor for answer quality prediction. Therefore, we incorporate answers X  goodness score w kj derived from the non-textual features of a ( u k , q j ) into equations (13) and (14):
To estimate the question topic similarity Rel ( c j , c i two categories, answerer-based and content-based methods described in Li et al. [13] can be employed. However, we observe that some leaf categories consist of only a small number of questions, which may lead to the data sparseness. In this paper, we use the widely studied topic model Latent Dirichlet Allocation (LDA) [3] to identify the latent topic information from the large scale question-answer collection. To identify the topics that each leaf category is about using LDA, we aggregate all questions under the same leaf category into a big document. Thus, each document essen-tially corresponds to a leaf category. After utilizing LDA, each leaf category c can be represented as a Z -dimension vector topic dis-tribution P ( z j c ) , where Z is the topic number. Thus, the task of
In cQA, questions are equipped with hierarchical and systematic categories. When an asker posts a question, he/she is required to choose a leaf category that the question belongs to. Therefore, it is reasonable to use the leaf categories to represent the question topics. topic similarity is converted to calculate the distance between two leaf category vectors. Here, we propose to use normalized Kull-back Leibler (KL) divergence [11]. The KL-divergence from c c is computed by KL ( c j jj c i ) = we calculate the similarity between leaf categories c j to c Jensen Shannon divergence, which shows the superior performance than others. Thus, we have We use logistic regression to measure each answer X  X  goodness. Jeon et al. [8] proposed to predict the goodness of answers using 13 features. Here, we use 8 of 13 features used in Jeon et al. [8] and two additional features marked by y sign. The other five features are not used because they are either not available or not provided in Yahoo! Answers (e.g., click, copy and print counts). The features we used for estimating the parameters are listed in Table 1.
The features listed in Table 1 are non-monotonic features. Fol-lowing [8], we convert these non-monotonic features into mono-tonic features using Kernel Density Estimation [6]. We use the getByCategory function from the Yahoo! Answers API 7 to obtain the questions for the evaluation. More specifically, the data set consists of 359,152 resolved questions crawled from March 20, 2011 to September 18, 2011 under the top-level cate-gory at Yahoo! Answers, namely "Cars &amp; Transportation". Under this category, there are 44 leaf categories. In this study, for all the resolved questions, the information of each question includes: (1) Texts of question and the associated answers, with stop words be-ing excluded 8 and the words being stemmed. 9 (2) Answerers X  IDs http://de veloper.yahoo.com/answers/ http://truereader.com/manuals/onix/stopwords1.html http://tartarus.org/martin/PorterStemmer/ Table 2: Performance of our proposed methods with traditional methods using two measures MRR and P@10. of each question. (3) Users X  rating information (e.g., thumbs up, thumbs down, the best answers and so on.). (4) Time information about the question posted and answered.

To demonstrate the effectiveness of our approach, we split all questions into two disjoint sets:
Test Set: questions posted after August 10, 2011, used as ques-tions to be routed.
 Archive Set: the remaining data set.

Finally, test set is made up of 32,405 questions, 91,399 answers and 17,315 answerers. Archive set consists of 326,747 questions, 926,904 answers and 170,906 answerers. Similar to the previous work [12, 13], the ground truth for each question in test set is the answerers who actually answer it. 10
Recall that we use a logistic regression to measure each answer X  X  goodness. To train the model, sufficient labeled instances are needed. Following the literature [12], we use the community and the askers X  choices to avoid manually labeling. For each question in archive set, the answer is labeled as  X  X ood" only if the following two con-ditions are met: (1) it is selected as the best answer; (2) it re-ceives more than 50% of thumbs up for all answers of the question. Also, one answer is labeled as  X  X ad" if it receives more than 50% of thumbs down for all answers of the question. Finally, 26,872  X  X ood" instances and 31,093  X  X ad" instances used as training data to estimate the parameters of the logistic regression model.
Evaluation Metrics: we adopt Mean Reciprocal Rank (MRR) and Precision at N (P@N) as evaluation metrics for question rout-ing, as they are widely used in evaluating the performance of ques-tion routing [13, 20].
The experiments use two parameters. The first is the smoothing parameter  X  ; and the second  X  , controls the relative importance of asking expertise and answering expertise in equation (12)Following the literature, we set  X  = 0 . 2 [18].

For parameter  X  , we conduct experiments on a small develop-ment set of 440 questions (10 questions from each leaf category) to determine the best value among 0.1, 0.2, , 0.9 in terms of MRR. This set is also extracted from Yahoo! Answers at the top-level cat-egory of  X  X ars &amp; Transportation", and it is not included in the test set. Finally, we set  X  = 0 . 2 empirically as these settings yield the best performance.
We first look into how well our proposed joint relevance and answer quality learning model performs as compared with the tra-ditional methods for question routing. Table 2 presents the results
In evaluation phase, we also remove the questions in test set whose answerers all do not appear in archive set. T able 3: Performance of the improved translation model with the query query likelihood language model using two measures MRR and P@10. for different traditional methods and our proposed joint learning method according to two measures MRR and P@10.
 In Table 2, row 1 is the query likelihood language model (QLLM). Row 2 is the answer quality smoothed query likelihood language model with user activity (LK2010) proposed by Li and King [12]. Row 3 is the category-sensitive QLLM (TCSLM) [13], which in-vestigates a ground-breaking incorporation of question category to filter the irrelevant answerers. Row 4 is the mixture of LDA and QLLM (LDALM) 11 proposed by Liu et al. [14], and row 5 is the cluster-based language model (CBLM) [20]. The last one is our proposed joint learning model with the improved translation model and category-sensitive answer quality method (ITR-CSAQ).
From this table, we can see that the proposed ITR-CSAQ signif-icantly outperforms all previous methods for question routing (row 1, row 3, row 4, and row 5 vs. row 6, all these comparisons are statistically significant at p &lt; 0 . 05 by using t -test).
We now look into how well the improved translation model (ITR) performs as compared with the query query likelihood language model (QLLM). Table 3 shows the results for QLLM and ITR methods according to two measures MRR and P@10. The compar-ison show that ITR significantly outperforms QLLM. Significant test using t -test show the difference between these two methods are statistically significant (row 1 vs. row 2).
In this paper, we propose a novel joint learning method by taking both word mismatch and answer quality into a unified framework for question routing. Experimental results conducted on cQA data set demonstrate that our proposed method significantly outperforms the traditional methods.
 There are some ways in which this research could be continued. First, user reputation should be considered, so it is necessary to combine our proposed approach with the semi-supervised coupled mutual reinforcement framework [2] for question routing. Second, we will try to investigate the use of external sources of social rela-tions between users to enhance the performance of question rout-ing, such as the method of Horowitz and Kamvar [5]. Third, in-spired by Wang and McCallum [16], we could take the temporal information of questions and answers into consideration. After this extension, we may find the interest change of users over time, and then route the questions by estimating the current interest of an-swerers.
This work was supported by the National Natural Science Foun-dation of China (No. 61070106), the National Basic Research Pro-gram of China (No. 2012CB316300), Tsinghua National Labo-W e use GibbsLDA++ to estimate the posterior probability of LDA. The default parameter setting is used and the number of top-ics is set to 100 because these settings give the best performance. ratory for Information Science and Technology (TNList), Cross-discipline Foundation and the Opening Project of Beijing Key Lab-oratory of Internet Culture and Digital Dissemination Research (No. 5026035403). We thank the anonymous reviewers for their insight-ful comments. [1] E. Agichtein, C. Castillo, and D. Donato. Finding [2] J. Bian, Y. Liu, D. Zhou, E. Agichtein, and H. Zha. Learning [3] D. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. [4] J. Guo, S. Xu, S. Bao, and Y. Yu. Tapping on the potential of [5] D. Horowitz and S. Kamvar. The anatomy of a large-scale [6] J. Hwang, S. Lay, and A. Lippman. Nonparametric [7] J. Jeon, W. Croft, and J. Lee. Finding similar questions in [8] J. Jeon, W. Croft, J. Lee, and S. Park. A framework to predict [9] P. Jurczyk and E. Agichtein. Discovering authorities in [10] J. Kleinberg. Authoritative sources in a hyperlinked [11] S. Kullback and R. A. Leibler. On information and [12] B. Li and I. King. Routing questions to approriate answerers [13] B. Li, I. King, and M. Lyu. Question routing in community [14] M. Liu, Y. Liu, and Q. Yang. Predicting best answerers for [15] M. Suryanto, E. Lim, A. Sun, and R. Chiang. Quality-aware [16] X. Wang and A. McCallum. Topic over time: a non-markov [17] X. Xue, J. Jeon, and W. Croft. Retrieval models for question [18] C. Zhai and J. Lafferty. A study of smooth methods for [19] G. Zhou, L. Cai, J. Zhao, and K. Liu. Phrase-based [20] Y. Zhou, G. Cong, B. Cui, C. Jensen, and J. Yao. Routing
