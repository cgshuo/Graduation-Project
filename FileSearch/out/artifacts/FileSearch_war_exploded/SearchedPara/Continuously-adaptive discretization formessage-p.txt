 Message passing algorithms such as Belief Propagation (BP) [1] exploit factorization to perform inference. Exact inference is only possible when the distri bution to be inferred can be represented by a tree and the model is either linear-Gaussian or fully dis crete [2, 3]. One attraction of BP is that algorithms developed for tree-structured models can b e applied analogously [4] to models with loops, such as Markov Random Fields.
 There is at present no general-purpose approximate algorit hm that is suitable for all problems, so the choice of algorithm is governed by the form of the model. M uch of the literature concentrates on problems from statistics or control where point measuremen ts are made (e.g. of an animal population or a chemical plant temperature), and where the state evolut ion is non-linear or the process noise is non-Gaussian [5, 6]. Some problems, notably those from co mputer vision, have more complex and so it is common to discretize the underlying continuous m odel to match the structure of the rapidly becomes intractable [8]. When models are complex fun ctions of the observations, sampling methods such as non-parametric belief propagation (NBP) [9 , 10], have been successful. Distributions of interest can often be represented by a fact or graph [11].  X  X essage passing X  is a class of algorithms for approximating these distributions , in which messages are iteratively updated between factors and variables. When a given message is to be up dated, all other messages in the graph are fixed and treated as though they were exact. The algo rithm proceeds by picking, from a family of approximate functions, the message that minimiz es a divergence to the local  X  X xact X  message. In some forms of the approach [12] this minimizatio n takes place over approximate belief distributions rather than approximate messages.
 A general recipe for producing message passing algorithms, summarized by Minka [13], is as fol-lows: (i) pick a family of approximating distributions; (ii ) pick a divergence measure to minimize; family. This paper makes contributions in all three steps of this recipe, resulting in a new algorithm termed Continuously-Adaptive Discretization for Message-Passi ng (CAD-MP).
 For step (i), we advocate an approximating family that has re ceived little attention in recent years: piecewise-constant probability densities with a bounded n umber of piecewise-constant regions. Al-though others have used this family in the past [14], it has no t to our knowledge been employed in a modern message-passing framework. We believe piecewise-c onstant probability densities are very well suited to some problem domains, and this constitutes th e chief contribution of the paper. For step (ii), we have chosen for our initial investigation the  X  inclusive X  KL-divergence [13] X  X  stan-dard choice which leads to the well known Belief Propagation message update equations. We show that for a special class of piecewise-constant probability densities (the so-called naturally-weighted densities), the minimal divergence is achieved by a distrib ution of minimum entropy, leading to by traversing axis-aligned binary-split kd-trees (explai ned in Section 3). The contribution here is an efficient algorithm called  X  X nformed splitting X  for perfor ming the necessary optimization in practice. As we show in Section 4, CAD-MP computes much more accurate ap proximations than competing approaches for a given computational budget. Let us consider what it means to discretize an inference problem represented by a factor graph with factors f uniform discretization of the factor graph by partitioning the state space of each variable x factors, which are now regarded as functions of discrete var iables x 0 { 1 , 2 , . . . , K } : according to the update equations for messages m and beliefs b : and | H k cretization { H k the variables according to (1), then using BP according to (2 ) X (4). The error in the approximation to the true marginals arises from (3) when f 0 Consider the task of selecting between discretizations of a continuous probability distribution p ( x ) over some subset U of Euclidean space. A discretization of p consists in partitioning U into K disjoint subsets V sponding discretized probability distribution q ( x ) assigns density w mal choice of the w Figure 1: Expanding a hypercube in two dimensions. Hypercube H (b), a subset of the full along each possible dimension. These sub-cubes are then re-combined to form two possible split candidates { H 1  X  , H 1+ } (d) and { H 2  X  , H 2+ } (e). Informed belief values are computed for the discretization decreases as the partitions become smaller . these the natural weights for p ( x ) , given the V of a naturally-weighted discretization and its entropy H (  X  ) : Theorem 1. Among any collection of naturally-weighted discretizatio ns of p ( x ) , the minimum KL divergence to p ( x ) is achieved by a discretization of minimal entropy.
 Proof. For a naturally-weighted discretization q , KL( p || q ) =  X  H ( q )  X  H ( p ) . H ( p ) is constant, so KL( p || q ) is minimized by minimizing H ( q ) . Suppose we are given a discretization { H k node using (2) X (4). The messages have not necessarily reach ed a fixed point, but we nevertheless have some current estimate for them. For any arbitrary hyper cube H at x receive if all other nodes and their incoming messages were l eft unaltered. To compute the informed belief, one first computes new discrete factor function valu es involving H using integrals like (1). These values are fed into (2), (3) to produce  X  X nformed X  mess ages m neighbor f The core of the CAD-MP algorithm is the procedure for passing a message to a variable x fixed approximations at every other node, any discretizatio n of  X  induces an approximate belief dis-tribution q shows, a good strategy for this selection is to look for a natu rally-weighted discretization that min-imizes the entropy of q is described next.
 CAD-MP employs an axis-aligned binary-split kd-tree [15] t o represent the discrete partitioning of a
D -dimensional continuous state space at each variable (the s ame representation was used in [14] each vertex is assigned a subset X  X ctually a hypercube X  X f the s tate space. The root is assigned the whole space, and any internal vertex splits its hypercube eq ually between its two children using an axis-aligned plane. The subsets assigned to all leaves part ition the state space into hypercubes. We build the kd-tree greedily by recursively splitting leaf vertices: at each step we must choose a hypercube H k Theorem 1, we should choose k and d to minimize the entropy of the resulting discretization X  using informed beliefs; we nevertheless proceed as though t hey were exact and choose the k -and d -values leading to lowest entropy. A subroutine of the algor ithm involves  X  X xpanding X  a hypercube generalizes to D dimensions by first expanding to 2 D subcubes and then re-combining these into 2 D candidate splits. Note that for all d  X  X  1 , . . . , D } Once we have expanded each hypercube in the current partitio ning and thereby computed values for  X   X   X  ( k, d ) =  X  Note that from (5) we can perform this minimization without n ormalizing the  X  b (  X  ) . We can now describe the CAD-MP algorithm using informed spli tting, which re-partitions a vari-able of the factor graph by producing a new kd-tree whose leav es are the hypercubes in the new partitioning: ables can be visited according to any standard message-pass ing schedule, where a  X  X isit X  consists of repartitioning according to the above algorithm. A simpl e example showing the evolution of the belief at one variable is shown in Figure 2.
 If the variable being repartitioned has T neighbors and we require a partitioning of K hypercubes, then a straightforward implementation of this algorithm re quires the computation of 2 K  X  2 D  X  KT message components. Roughly speaking, then, informed spli tting pays a factor of 2 D +1 over BP which must compute K 2 T message components. But CAD-MP trades this for an exponenti al factor in K since it can home in on interesting areas of the state space us ing binary search, so if BP requires K partitions for a given level of accuracy, CAD-MP (empirical ly) achieves the same accuracy with only O (log K ) partitions. Note that in special cases, including some low-level vision applications [16], classical BP can be performed in O ( KT ) time and space; however this is still prohibitive for large K . We would like to compare our candidate algorithms against th e marginal belief distributions that models. Instead, for each experiment we construct a fine-sca le uniform discretization D model and input data, and compute the marginal belief distri butions p ( x x can then compare the marginals p ( x by computing the KL-divergence KL ( p ( x While a  X  X ine-enough X  uniform discretization will tend to the true marginals, we do not a priori know how fine that is. We therefore construct a sequence of coa rser uniform discretizations D i the same model and data, and compute  X  ( D i discretization is a good approximation to the exact margina ls. Observation (local factor) (a) (b) (c) Figure 2: Evolution of discretization at a single variable. The left image is the local (single-variable) factor at the first node in a simple chain MRF whose n odes have 2-D state spaces. The titioning is informed simply by the local factor, but after m essages have been passed once along the chain and back (b), the posterior marginal estimate has shif ted and the discretization has adapted ac-cordingly. Subsequent iterations over the chain (c) do not s ubstantially alter the estimated marginal belief. For this toy example only 16 partitions are used, and the normalized log of the belief is displayed to make the structure of the distribution more app arent.
 We compare our adaptive discretization algorithm against n on-parametric belief propagation (NBP) [9, 10] which represents the marginal distribution at a variable by a particle set. We generate some importance samples directly from the observation dist ribution, both to initialize the algorithm of a distribution well, leading to zeros in the approximate m arginals and divergences that tend to infinity. We therefore regularize all divergence computati ons as follows: where { H k } are the partitions in the fine-scale discretization D which was found empirically to show good results for NBP.
 sequences is shown in Figure 3a, where time goes from bottom t o top. The measurement at a time-step consists in 240  X  X ixels X  (piecewise-constant regions of uniform width) generated by simulating a small one-dimensional target in clutter, with additive Ga ussian shot-noise. There are stationary clutter distractors, and also periodic  X  X orkings X  where a m oving clutter distractor emerges from the target and proceeds for a few time-steps before disappearin g. Each sequence contains 256 time-steps, and the  X  X xact X  marginals (Figure 3b) are computed us ing standard discrete BP with 15360 states per time-step. The modes of the marginals generated b y all the experiments are similar to those in Figure 3b, except for one run of NBP shown in Figure 3c that failed entirely to find the mode (red line) due to an unlucky random seed. However, the di stributions differ in fine structure, where CAD-MP approximates the tails of the distribution muc h better than NBP.
 at various degrees of coarseness, and adaptive discretizat ion using CAD-MP with varying numbers of partitions. Each data point shows the mean divergence  X  (  X  ) for one of the ten simulated one-dimensional datasets. As the number of adaptive partitions increases, the variance of  X  (  X  ) across trials increases, but the divergence stays small. Higher di vergences in CAD-MP trials correspond is a close approximation to the exact beliefs. The adaptive d iscretization provides a very faithful approximation to this  X  X xact X  distribution with vastly few er partitions.
 Figure 4b shows the divergences for the same ten one-dimensi onal trial sequences when the marginals are computed using NBP with varying numbers of par ticles. The NBP algorithm was run five times on each of the ten simulated one-dimensional da tasets with different random seeds each time, and the particle-set sizes were chosen to approxi mately match the computation time of the CAD-MP algorithm. The NBP algorithm does worse absolute ly (the divergences are much larger even after regularization, indicating that areas of high be lief are sometimes mis-estimated), and also Figure 3: One of the one-dimensional test sequences. The region of the white rectangle in (b) is expanded in (d) X (g), with beliefs now plotted on log intensi ty scale to expand their dynamic range. CAD-MP using only 16 partitions per time-step (e) already pr oduces a faithful approximation to the exact belief (d), and increasing to 128 partitions (f) fills i n more details. The NBP algorithm using 800 particles (g) does not approximate the tails of the distr ibution well. Figure 4: Adaptive discretization achieves the same accuracy as unif orm discretization using many fewer partitions, but non-parametric belief propagat ion is less effective. See Section 4 for details. varies greatly across different trial sequences, and when r e-run with different random seeds on the runs on which NBP incorrectly located the mode of the margina l belief distribution at some or all time-steps, as in Figure 3c.
 We performed a similar set of experiments using a simulated t wo-dimensional data-set. This time the input data is a 64  X  64 image grid, and the  X  X xact X  fine-scale discretization is at a resolution cretization still greatly outperforms NBP for an equivalen t computational cost. Again there is a straight-line trend in the log/log plots for both CAD-MP and uniform discretization, though as in the one-dimensional case the variance of the divergences in creases with more partitions. NBP again performs less accurately, and frequently fails to find the hi gh-weight regions of the belief at all at some time-steps, even with 3200 particles.
 Adaptive discretization seems to correct some of the well-k nown limitations of particle-based meth-bution, which leads to a more faithful approximation to the e xact beliefs. This also prevents the catastrophic failure case for NBP shown in Figure 3c, where t he mode of the distribution is lost entirely because no particles were placed nearby. Moreover , CAD-MP X  X  computational complexity scales linearly with the number of incoming messages at a fac tor. NBP has to resort to heuristics to sample from the product of incoming messages once the number of messages is greater than two. The work most closely related to CAD-MP is the 1997 algorithm of Kozlov and Koller [14]. We refer to this algorithm as  X  X K97 X ; its main differences to CA D-MP are: (i) KK97 is described in a junction tree setting and computes the marginal posterior o f just the root node, whereas CAD-MP computes beliefs everywhere in the graph; (ii) KK97 discret izes messages (on junction tree edges) rather than variables (in a factor graph), so multiplying incoming messages toget her requires the which the incoming messages share the same discretization. Difference (i) is the more serious, since it renders KK97 inapplicable to the type of early-vision pro blem we are motivated by, where the marginal at every variable must be estimated.
 Coarse-to-fine techniques can speed up the convergence of lo opy BP [16] but this does not address the discrete state-space explosion. One can also prune the s tate space based on local evidence [17, 18]. However, this approach is unsuitable when the data func tion has high entropy; moreover, it is very difficult to bring a state back into the model once it has b een pruned.
 Another interesting approach is to retain the uniform discr etization, but enforce sparsity on messages to reduce computational cost. This was done in both [19] (in w hich messages are approximated us-ing a using a mixture of delta functions, which in practice re sults in retaining the K largest message components) and [20] (which uses an additional uniform dist ribution in the approximating distri-bution to ensure non-zero weights for all states in the discr etization). However, these approaches appear to suffer when multiplying messages with disjoint pe aks whose tails have been truncated to enforce sparsity: such peaks are unable to fuse their eviden ce correctly. Also, [20] is not directly applicable when the state-space is multi-dimensional.
 Expectation Propagation [5] is a highly effective algorith m for inference in continuous-valued net-works, but is not valid for densities that are multimodal mix tures. We have demonstrated that our new algorithm, CAD-MP, perfor ms accurate approximate infer-ence with complex, multi-modal observation distributions and corresponding multi-modal posterior distributions. It substantially outperforms the two stand ard methods for inference in this setting: uniform-discretization and non-parametric belief propag ation. While we only report results here on simulated data, we have successfully used the method on low-level vision problems and are prepar-ing a companion publication to describe these results. We be lieve CAD-MP and variants on it may be applicable to other domains where complex distributions must be estimated in spaces of low to moderate dimension. The main challenge in applying the tech nique to an arbitrary factor graph is the tractability of the definite integrals (1).
 This paper describes a particular set of engineering choice s motivated by our problem domain. We use kd-trees to describe partitionings: other data structu res could certainly be used. Also, we employ a greedy heuristic to select a partitioning with low entropy rather than exhaustively computing a minimimum entropy over some family of discretizations. We h ave experimented with a Metropolis algorithm to augment this greedy search: a Metropolis move c onsists in  X  X ollapsing X  some sub-tree of the current partitioning and then re-expanding using a ra ndomized form of the minimum-entropy criterion. We have also tried tree-search heuristics that d o not need the O (2 D )  X  X xpansion X  step, and thus may be more effective when D is large. The choices reported here seem to give the best accuracy on our problems for a given computational budget, h owever many others are possible and variety of inference settings.

