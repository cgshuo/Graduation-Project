 over linguistic structures that can be characterized by a si mple grammar. models.
 is available from http://cog.brown.edu/  X mj/Software.ht m. to establish the formal devices we use to specify adaptor gra mmars. 2.1 Probabilistic context-free grammars associates with each symbol A  X  N  X  W a set T symbol then T A . The sets of trees associated with nonterminals are defined r ecursively as follows: where R subtree is a member of T by the CFG is the set { Y IELD ( t ) : t  X  X  Informally,  X  is used to define a distribution G then G for nonterminal symbols are defined recursively over T where T REE D IST That is, T REE D IST each subtree t relax. The distribution over trees generated by the PCFG is G sum of the probabilities of all trees with that string as thei r yields. 2.2 The Pitman-Yor process a numbered sequence of tables, and z and the n + 1 st customer chooses a table from the distribution where m is the number of different indices appearing in the sequence z = ( z number of times k appears in z , and  X  table counts n = ( n with the probability of z being unaffected by permutation of the indices of the z on any desired domain. Assume that every table in our restaur ant has a value x generator . Then, we can sample a sequence of variables y = ( y process to produce z and setting y drawn from G . When a = 0 , the distribution on the y parameter b and base distribution G .
 as that obtained via the Pitman-Yor process [2, 3]. 3.1 A general definition of adaptor grammars adaptors indexed by N . That is, C T , for each A  X  N . An adaptor grammar associates each symbol with two distrib utions G H the unit tree labeled A , while G The intuition here is that G choice of the adaptor, C practice, we integrate over H 3.2 Pitman-Yor adaptor grammars trees that cannot be expressed using PCFGs.
 the Pitman-Yor process. A Pitman-Yor adaptor C set of atoms from the distribution G Dirichlet distribution. A PYAG has an adaptor C expanded in the standard manner for a PCFG. Each adaptor C vectors, x grammar, these now have higher probability than other subtr ees. n the subtrees in x index of the entry in x of analyses u = ( u compute the adaptor state C ( u ) after generating u : the elements of x subtrees of u with root label A , and their frequencies n at q is encountered for the first time (i.e., when it is added to x A ). 4.1 Dirichlet processes and word segmentation bol for this grammar is Word . The parameters a a empty), so the first word s hence is s word generated by G those words and reserves mass b productions, changing the start label to Words and setting a 30 of [7] on the same data. 4.2 Hierarchical Dirichlet processes and morphological an alysis itself generates words either from x grammar are as follows, where Chars is  X  X onkeys at typewriters X  once again: probability than under the Chars PCFG subgrammar.
 as there are no entries in the Word adaptor. Sampling from H and perhaps also G word walking consist of the first entries for Word , Stem and Suffix . adaptor, or whether to generate the word using G sample from H jump from G G Section 4.2 after generating walking , jumping and walked . easing as eas plus ing . (MCMC) algorithm to sample from the posterior distribution over PYAG analyses u = ( u given strings s = ( s hyperparameters  X  over production probabilities  X  , i.e.: with  X ( x ) being the generalized factorial function, and  X  tegrating over the distributions H associated with each adaptor, is where f in (6) is the probability of generating the topmost node in ea ch analysis in adaptor C of generating a Pitman-Yor adaptor with counts n pler, proposing changes to the analysis u constructed to approximate the conditional distribution o ver u strings u gorithm for directly sampling from P ( u 5.1 The PCFG approximation G  X  ( u ing on its history. The PCFG approximation G  X  ( u adaptor grammar given the sentences s grammar H = ( N, W, R, S, C ) , let: where Y IELD ( x ) is the terminal string or yield of the tree x and m probability is the probability of the adaptor C the original productions R . The first term is the probability of adaptor C expansions of the trees x for u 5.2 A Metropolis-Hastings algorithm Hastings algorithm. If u from P ( U where u  X  is the same as u except that u  X  analyses for them at random. At each iteration we pick a strin g s parse for s over adaptor states C ( u ) and production probabilities  X  can be computed from them. the construction of more sophisticated models.
 Acknowledgments DC000314, NSF 9870676, 0631518 and 0631667, the DARPA CALO p roject and DARPA GALE contract HR0011-06-2-0001.

