 Rosa Karimi Adl  X  Seyed Mohammad Taghi Rouhani Rankoohi Abstract The Performance and the efficiency of a distributed database system depend two different types of data transmission across the network, i.e., data transmissions due to site-fragment dependencies and those caused by inter-fragment dependencies. We propose a new heuristic algorithm which is based on the ant colony optimization meta-heuristic, with is capable of producing near-optimal solutions within a reasonable time. The results also reveal the flexibility and scalability of the proposed algorithm.
 Keywords Distributed database system  X  Non-replicated data allocation  X  Site-fragment dependency  X  Inter-fragment dependency  X  Ant colony optimization 1 Introduction few decades. Distributed database systems are not only more compatible with the decen-tralized nature of organizations and their growing volume of required data, but they also help reduce costs (communication and equipment), increase efficiency by providing a higher degree of parallelism, and improve reliability as well as accessibility [ 6 ]. Consequently, in addition to utilization of a multitude of computer systems, the necessity of increasing parallelism and reliability have added to the complexities of data maintenance and manage-ment in a distributed database system. One of the fundamental and yet complicated problems in this area is the database design, 1 where in addition to the classic design stage for the global schema (using the same old techniques as in centralized databases) two new stages of fragmentation and allocation design should also be considered.

In order to come up with the appropriate allocation units at the distribution design stage, global relations are either decomposed into horizontal and vertical fragments, or a combina-will be decided subsequently at the allocation stage. Although fragmentation and allocation are two interconnected tasks, they are usually performed separately to make it possible to  X  X eal with the complexity of the problem X  [ 26 ]. Without considering their allocation, one group of algorithms provides relatively suitable units of allocation by fragmenting the base relations. The other group of algorithms takes these fragments as inputs and work out the proper placement for each (allocation scheme).

Furthermore, not only the environmental limitations are considered by good allocation scheme (e.g., memory capacity, processing power of each site, capacity of communication cated in such a way that firstly the volume of transferred data between sites would be low, secondly logically related fragments would be located in nearby sites, and finally the vol-ume of maintained data in every site would be less than its memory capacity (environmental the amount of provided parallelism should also be considered. Moreover, in order to choose an appropriate query optimization strategy some other environmental factors including the different processing power and disk drives X  speed of each site, workload on servers and the network traffic [ 30 ] should be taken into account. Therefore, the data allocation problem can be categorized within resource allocation problems which are defined as  X  X ptimization problems with constraints X  [ 18 ]. It is also possible to show that even without considering many other criteria, the allocation problem, is NP-hard and requires heuristic methods to problem within an acceptable computation time, the majority of the proposed algorithms (see response time among the other factors which include the processing time of a local request, the delay of data transfer between local storage hierarchy, etc.

In practice, there are two main causes for data transmission between sites: 1. Execution of a transaction in a site which lacks the required data for the transaction. 2. Execution of transactions in one site which depending on the fragments it contains
There are some similarities between the data allocation problem (DAP), the file allocation problem (FAP), and the quadratic assignment problem (QAP): simply by considering only the first cause of data transmissions, the problem becomes similar to the well known FAP. However, the FAP differs from the DAP in many ways, the most important of which is the logical and semantic relationship among fragments (the second cause of data transmission) to be located at nearby. This point has lead us to find similarities between the DAP and the QAP.
 The QAP is a well-known combinatorial optimization problem first formulated by Koopmans and Beckman [ 16 ]. The problem can be defined as follows: Consider a set of distance between their corresponding locations.

The dependence of fragments on each other can be expressed as a QAP, in which every two facilities represents the amount of data transmitted between two sites which contain the two corresponding fragments and the distance between two locations is considered as the cost of sending a data item (for example a block of data or a frame of data) between two sites. However, in most cases DAP is more complicated than QAP, because of the fact that in QAP the number of facilities and locations are equal, whereas in DAP there are usually more fragments than sites. Besides, unlike QAP, there are certain relationships between sites and fragments (the first reason of data transmission) in DAP, which should also be considered.
In this paper there is a solution for the allocation problem presented for the first time by combining some new heuristics with a number of ant colony optimization (ACO) algorithms proposed for the QAP.
 The rest of this paper is organized as follows: in Sect. 2 , some proposed algorithms for DAP are studied briefly. In Sect. 3 , we defined some of the new concepts that are widely is provided. In Sect. 6 , the QAP (which is very similar to the DAP) is defined briefly and the applied ACO algorithms are discussed. Sections 7 and 8 are devoted to our proposed ACO-DAP algorithm for DAP, which is explained in detail. In Sect. 9 , an alternate algorithm known as simulated evolutionary (SE) algorithm is introduced with some modifications so that the proposed ACO-DAP algorithm could be compared with the revised version. Sec-tion 10 explains the experiment environment and describes the way test data are generated. The achieved results are further studied in Sect. 11 . Finally, Sect. 12 provides a summary and conclusions. 2 Related work As mentioned in advance, the DAP in distributed databases (DAP) is a more complicated The proposed algorithms gradually considered some of the requirements of the DAP which do not exist in the FAP. For example, various authors have considered the DAP as a form of the replica placement problem with different types of underlying networks such as con-put emphasis on security considerations in their proposed data allocation process. Ram and Marsten [ 27 ] examined the problem from the concurrency mechanism point of view.
Existing algorithms for the DAP could define an allocation scheme either statically or dynamically. In a static data allocation algorithm, the data allocation scheme is designed based on the predefined execution pattern of transactions in the target environment. In con-of our suggested algorithm.

In 1982, Navathe et al. [ 24 ] proposed a data distribution algorithm in which data frag-mentation and data allocation were determined simultaneously. However, most of the data allocation algorithms were assumed to have predefined data fragments and tried to allocate greedy algorithm for both replicated and non-replicated data allocation design problem, how-ever since the proposed algorithm was not successful in considering the logical and semantic relationship between fragments, the achieved results were not satisfactory.
In 1984, Bell [ 2 ] showed that the DAP is NP-Hard. Later, Freider and Sieglemann [ 13 ] proved the NP-Completeness of multi processor document allocation problem (MDAP) by reducing it to a well known NP-Hard problem: the QAP.

Consequently, most of the algorithms introduced afterward, tried to use heuristic meth-ods to solve DAP. Although Sarathy et al. [ 29 ] and Menon [ 21 ] tried to present opti-mal approaches (based on mathematical programming) to the DAP, their algorithms suf-fered either from the high order time complexity or the complexity of the formulation itself.

In 1994, Corcoran and Hale [ 9 ] proposed a solution for DAP based on genetic algo-rithms (GA), in which the logical and semantic relation between fragments were not taken into account. Later, Frieder and Siegelmann [ 13 ] applied a different GA to the DAP which did consider the logical and semantic relationship between fragments, though in its sim-plest form using binary logic (i.e., the degree of dependencies were not mentioned) and without taking into account the important concept of dependencies between sites and frag-ments.

In 2002, Ahmad et al. [ 1 ] proposed another GA, a SE algorithm and an algorithm based on Mean Field Annealing [ 26 ] and showed that the SE algorithm provides better solutions in between fragments, it apparently assumes all fragments to have the same size, which is often not the case in real-world DAPs.

In short, most of the existing static data allocation algorithms proposed will fall into one of ACO algorithms is introduced. Our proposed algorithm is further compared with the SE of time. 3 Preliminaries Before going deep into our proposed solution for the DAP, it is necessary to define some of the concepts used in this work: Definition 1 direct transaction-fragment dependency The dependency between transaction t and fragment f j is said to be direct, if for each execution of the transaction t k there should be some data transmitted from site containing the fragment f j to the site executing the transaction. data items to be transmitted from s i 1 and s i 2 to the site s i .
 Definition 2 indirect transaction-fragment dependency The dependency between transac-one of the other fragments (not the originating site of the transaction).

This type of dependency is usually defined according to the strategies used for query optimization, integrity constraint checking, security support, etc. The query optimization problem itself has been the subject of several researches. With the ever growing complex-ity of queries in database systems, the proposed query optimization methods have become identifying and analyzing similar subqueries.
 necessary to send some data items from the site containing f j 1 to the site having f j 2 . Example 2.2 When transaction t k inserts some tuples into fragment f j , a vertical fragment of the inserted tuples to be sent to the other site containing the vertical fragments of R . Example 2.3 When transaction t k requests an update operation on attribute values involved from the site containing the updated data to the site containing the calculated attribute. Definition 3 site-fragment dependency The site s i is said to be dependent on fragment f j , dependent the site would be on the fragment.
 Example 3.1 Suppose the scenario defined in Example 1, the execution of transaction t k in site s i , causes the site s i dependent on fragments f j 1 and f j 2 .
 Definition 4 Inter-fragment dependency The fragment f j 1 is said to be dependent on fragment to the one containing fragment f j 2 . 2 Example 4.1 Suppose the scenarios defined in Examples 2.1 and 2.2 for every execution of on the fragment f j 2 .

It can be seen easily that one of the factors making fragments dependent on each other not been studied in the previous literature. 4 Problem definition 4.1 Data allocation in distributed database systems A distributed database consists of more than one site, each maintaining a portion of the global database. Different transactions with different execution frequencies are submitted to different sites, which may cause data transfer between the sites in the network (due to site-fragment and/or inter-fragment dependencies). The problem is to minimize the response transmission, we have only considered this cost in determining the overall response times of transactions. 4.2 Pertinent parameters Table 1 summarizes the key notations used in this paper.
 Parameters associated with the sites: nected to each other in a network with predefined topology. The distance between each two sending a unit data item from site s i 1 to the site s i 2 .
 Parameters associated with the fragments: If the global relations are decomposed into m fragments the j th fragment ( 1  X  j  X  m ) is denoted as f j and its size (in average 3 ) as fragSize j .
 Parameters associated with the transactions: The determining factor in placement of fragments in sites, are the access and the execution patterns of transactions. Since the number of transactions in a typical environment may be large, only 20% of the most active transactions (that do 80% of data accesses in the system) may be taken into account [ 25 ]. If the number of such transactions in the environment is l frequency of transaction t k at site s i .
 Parameters associated with the transaction dependencies on the fragments: The direct transaction-fragment dependency (explained in Sect. 3 ) is shown with matrix
One the other hand, the indirect transaction-fragment dependency is shown by a three
Regarding these two kinds of dependency between transactions and fragments, and the dependencies of sites on the transactions, a proper allocation can be defined. In Fig. 1 the dependency between sites, transactions and fragments is shown. 4.3 Cost and constraints evaluation As stated earlier, a proper allocation is one which minimizes the costs while considering the environmental constraints. Here, storage capacity is the only environmental constraint considered and because the most important cost and time delay is that of data transmission over the network, only the network data transmission cost is considered.

Assume an allocation scheme shown by the m -element vector in which  X  j specifies follows: Now the storage capacity constraint can be expressed as follows: The cost of data transmission in an allocation scheme includes COST1 and COST2: COST1 illustrates the cost resulting from direct transaction-fragment dependencies, whereas COST2 is the cost of data transmission between sites due to indirect transaction-fragment dependencies.

To calculate COST1 the amount of site-fragment dependencies must be calculated. This amount is expressed by matrix STFR n  X  m in which stfr ij indicates the volume of data items from fragment f j that are accessed by site s i in unit time. This matrix is defined as: Or: From this matrix, PARTIALCOST1 n  X  m can be calculated in which partialcost1 ij is the cost matrix can be calculated as follows: In other words: Having matrix PARTIALCOST1 n  X  m , the cost COST1 for an allocation scheme can be calculated from: To calculate COST2 it is necessary to define inter-fragment dependencies. First, according are calculated as: Using matrix QFR l  X  m  X  m , matrix FRDEP m  X  m can be determined. In this matrix, element fragment f j 2 in unit time due to the indirect transaction-fragment dependencies (according as: Now, according to matrix FRDEP m  X  m , COST2 of an allocation scheme, can be found: As explained in Sect. 4 , our proposed algorithm has some steps and in each step, the desir-ability of the achieved allocation scheme is calculated based on its COST using the formulae mentioned above. 5 General concepts of ACO algorithms Ant Colony Algorithms are used for solving many complicated problems such as routing [ 11 ], appropriate to be mimicked in order to explore many distributed control and optimization problems.

A group of ant algorithms that are based on ACO meta-heuristic are known as ACO algo-rithms [ 12 ]. These algorithms are used to solve discrete optimization problems and here we applied an algorithm of this type to the DAP. 6 The quadratic assignment problem (QAP) The QAP is an NP-Complete problem [ 28 ] to which ACO algorithms are applied with con-siderable success [ 32 ]. This problem can be defined formally as follows:
Assume n facilities with B n  X  n = b ij defined as the flow between facilities i , j and n is to find an allocation scheme in which each facility is assigned to exactly one location so that the objective function f () is minimized. The objective function is defined with the formula below: Here  X  i gives the location of facility i in the current allocation scheme .
Amongst proposed heuristic approaches, there have been some ACO algorithms suggested for the QAP, such as AS [ 20 ],  X  X  AS [ 31 ] and ANTS [ 19 ].

One can observe easily that the problem of minimizing COST2 (stated in Sect. 4.3 )is very similar to the QAP. Thus we have adopted some concepts from QAP in solving ACO algorithms. More precisely, we applied a heuristic similar to the one proposed in AS algo-rithm and some other methods used in the  X  X  AS algorithm such as  X  X ower pheromone trail limit X  and randomized the order of assignments. We have also applied a 2-opt local search procedure which is used in most of the proposed ACO algorithms for the QAP. 7 The proposed ACO algorithm for DAP In the proposed ACO algorithm for the DAP, as the ACO meta-heuristic suggests, each pair of s i , f j (couplings of sites and fragments) with 1  X  i  X  n and 1  X  j  X  m is associated heuristic desirability is computed based on the PARTIALCOST1 n  X  m matrix and a coupling matrix which will be discussed later in this section.

The generic ACO algorithm for the DAP is described below (this algorithm is designed based on ACO metaheuristic):
Ant colony optimization algortithm for DAP 1. Initialize all pheromone trails with  X  0 2. Calculate the heuristic desirability matrix  X  3. WHILE no_of_iterations &lt; MAXIMUM_ITERATIONS 4. WHILE no_of_ants &lt; ANTS_POPULATION 5. Sort the fragments randomly 6. Create a feasible allocation scheme probabilistically 7. Improve the assignment with local search 8. Calculate the cost of improved assignment 9. no_of_ants = no_of_ant s + 1 10. END WHILE 11. Evaporate pheromone trails 12. Update pheromone trails 13. Enforce the minimum pheromone trail to be  X  0 14. no_of_iterations = no_of_iteration s + 1 15. END WHILE 16. Output the best solution found so far 8 Description of our proposed ACO-DAP algorithm 8.1 Initialization site s i , X  ij is calculated as follows: to the heuristic desirability which is calculated according to COST2. Parameters  X  and  X  give weights to these two costs. For example, in an environment in which COST2 is much heavier than COST1, the designer can simply adjust  X  valuetobemorethan  X . The value  X  1 ij is computed as: The computation of  X  2 ij is a generalization of the heuristic proposed in AS_QAP [ 20 ]. We from other sites to the site containing the fragment f j .
 In the next step, two coupling matrixes E 1and E 2 are calculated as: by simply adding E 1and E 2: simply define FRDP matrix as follows: And compute vectors d and f with the following formula: Then the coupling matrix E = d T  X  f is calculated.

Now that the coupling matrix E is made (calculated either by the first formulas or by the second), the heuristic desirability  X  2 ij is defined as: 8.2 Ants X  solutions construction Similar to what is suggested in AS-QAP, a solution is constructed as follows:
For each ant in each iteration, fragments are sorted in a random order. At each step, the given by (this formulation is based on what AS-QAP suggests): areparametersdeterminingtherelativeimportanceofpheromonetrailsandtheheuristicinfor-mation. The term N k j shows the sites still having enough storage capacity to save fragment f . In fact, the closer a fragment f j is to the end of the fragment list, the fewer sites N k contain. 8.3 Local search This local search has two phases: Exchange and Change. 8.3.1 Exchange phase In the Exchange phase an ant follows these steps: 1. Set the_best_exchange_benefit = 0 2. If the terminating condition is not satisfied yet, chose next fragment f j (or the first 3. Examine the benefit of exchanging the location of fragment f j with the location of 4. If such a fragment f  X  exists and the benefit of exchanging the location of f j with the 5. Exchange the location of fragment f j with f  X  and set the_best_exchange_benefit to the or the number of accomplished exchanges equals to a parameter EX_Max. This parameter enables the designers to have trade-off between algorithm computation time and the optimal-where all the fragments are chosen from the list.

Using the variable  X  X he_best_exchange_benefit X  adds some random behavior to the local search method and reduces the possibility of frequent reaching a local optimum.
The benefit of exchanging the location of two fragments f j 1 and f j 2 in an allocation scheme is computed as follows: Where EXbenefit1 ( , j 1 , j 2 ) stands for the benefit achieved according to COST1 and is calculated as follows: This benefit is computed using the following equation [ 33 ]:
EXbenefit2 ( , j 1 , j 2 ) 8.3.2 Change phase 2. Examine the benefit of moving fragment f j to all other feasible sites (sites which have 3. If such s  X  site exists, change the location of fragment f j to be stored at site s  X  . 4. Go to step 1.
 The terminating condition is satisfied when either all of the fragments are chosen from the list or the number of changes exceeds the value of the parameter CH_Max. The reason of using CH_Max is identical to the one explained for EX_Max. Here we do not use a variable such as  X  X he_best_change_found X . This is because changing the location of a single fragment a local optimum of change phase is less than exchange one.

The benefit of moving fragment f j to the site s i in the allocation scheme is calculated by the following formula: is the change benefit according to the COST2. These benefits can be inferred from the for-mulas of EXbenefit . The procedure of moving fragment f j to site s i couldbeassumedas a special case of exchange procedure in which the second fragment is a virtual fragment features: 1) fragsize f v = 0 2)  X  k 1  X  k  X  l trfr kf v = 0 and thus  X  i 1  X  i  X  n partialcost1 if v = 0 3)  X  k , j 1  X  k  X  l , 1  X  j  X  mQ kj f v = Q kf v j = 0 And thus  X  j 1  X  j  X  m frdep jf v = frdep f v j = 0
With these properties defined for f v and assuming that  X  f v = s  X  , we can now calculate the CHbenefit1 as follows: Referring to the property2, partialcost1  X  And we have: On the other hand, CHbenefit2 is computed as: As property 3 implies: Therefore we have the following formula for CHbenefit2: 8.4 Pheromone updating the local search mechanism. After calculating the costs of each allocation, some pheromone trails are evaporated to avoid unlimited accumulation of trails and allow the algorithm to forget previously made improper choices [ 32 ]. Then, all ants should leave some pheromone of accumulated pheromone trail is updated as [ 20 ]: as follows [ 20 ]: where Q is a parameter representing the amount of pheromone deposited by an ant.
Updating the pheromone trails in this manner may cause the pheromone trail on some edges to become less than a lower bound  X  0 and thus avoid some couples s i , f j to be with the following formula: through all of the iterations is printed at the end of the algorithm. 9 The alternative algorithm for comparison In order to evaluate the performance of our algorithm, we have chosen a modified version of the SE data allocation algorithm [ 1 ] to be compared with our proposed method. This algorithm has been reported to outperform other algorithms such as genetic data allocation, mean field annealing, and random search algorithms in a reasonable time. 9.1 Simulated evolutionary algorithm (original version) The generic SE data allocation algorithm is as follows [ 1 ]:
Simulated Evolutionary Data Allocation Algorithm The chromosome structure is defined as follows:
The number of genes in part a is equal to the total allocation limit and this part specifies the storage capacity limit. On the other hand, each gene in part b corresponds to a fragment specifying the priority of that fragment to be considered in the allocation procedure.
In part a each gene is designed to be a single bit. A value of 1 indicates that the corre-sponding allocation space is allowed to be used for this chromosome; otherwise the space the total number of fragments.

After the creation of the first generation, the mapping heuristic is applied to extract the allocation scheme proposed by each chromosome. This mapping heuristic is explained in detail in [ 1 ].

At the end of each generation, the fitness of each chromosome is determined according to with a probability proportional to their fitness. Performing the crossover and mutation pro-cedures on each pair of parents yields two new children.

The process of creating a new generation and evaluating their corresponding allocations is done until the number of generations reaches a maximum limit. Then the best result achieved through all of the generations is given as the output of the algorithm. 9.2 Modified SE algorithm we have added and modified some parts in order to make the algorithm capable of dealing with our different test environments. The modified or added parts are listed below: 1. The original definition of this algorithm states that  X  X he number of genes in part a is 3. With the modified definition of part a , the first chromosome is made with all genes in 4. The process of checking whether the new effective allocation limit is enough for all 5. As the representation of part a of the chromosomes has been modified, the mutation Using these modifications, we have generated some test data and compared the modified SE algorithm with our proposed one. 10 Experiment environment To evaluate the proposed ACO-DAP algorithm we tested it through a number of experiments. In each experiment, we let one parameter vary while fixing others. Our test data generation process follows some structured rules which are explained in Sect. 10  X  2 . 10.1 The hardware/software configuration The experiments are all done in an environment using a 1.86GHz Intel Pentium M 750 pro-cessor with 1 Giga bytes of DDR2 RAM with Microsoft Windows XP Professional SP2 as the operating system. The ACO-DAP and SE algorithms were implemented in the programming environment provided by MATLAB 2006. The algorithms are then tested by the same test data generated by the rules which are defined in the next section. 10.2 Test data generation To compare our algorithm with the SE algorithm mentioned above, we have implemented a test data generator which gets number of fragments m , number of sites n and some other parameters (which will be defined later) as input and creates a random DAP instance as follows: Fragment sizes Where c is an input parameter defined as: In fact the c parameter is an approximation of the average of the fragment sizes. Site capacities We have designed our test data generator to create a rigorous site capacity constraint where site capacities are chosen to be very low. However, we have followed a specific strategy to still has enough memory capacity for storage. This condition should hold true regardless of the order in which fragments are considered to be allocated.

In this strategy, we first assume to have m fragments all being the same size of the largest fragment. Then for each site s i a random number p i is chosen which shows the number of the fragments mentioned above (big size fragments) which can be saved in it. The p i value is chosen randomly from 1 , 2  X  m n  X  1 . Note that p i  X  N . However, we want to have a somehow rigorous site capacity constraint in which n i = 1 p i = m . Thus whenever we assign p to a site s i , we should calculate the number of remaining fragments (fragments without any space to be stored in) as follows: We should make sure that the number of remaining sites (sites, s r without any p r defined overyet)islessthan rf i .

As long as this condition does not hold, we choose another random value for p i as men-formula: Transmission costs: The test data generator gets the parameter UCN (Unit transmission Cost between two Neigh-boringsites)asaninputandrandomlygeneratesthevaluefor UC i 1 i 2 from [ UCN , n  X  UCN ] , where n is the number of sites.
 Site-fragment dependency: To generate the STFR matrix, transaction frequencies and transaction-fragment dependency not require t k , the value for freq ik is simply set to zero.
Similarly,forthetransaction-fragmentdependency,thetestdatageneratorreceivesanother input parameter APF ( 0 &lt; APF  X  1 ) , which denotes the probability of a fragment being accessed by a transaction. With this parameter, a list of fragments being accessed by a action-fragment dependency trfr kj is chosen from a uniformly distributed random value in 0 , fragSize value of trfr kr is set to zero.

HavingFREQ n  X  l and TRFR l  X  m matrices,thesite-fragmentdependencymatrixSTFR n  X  m is calculated as: Inter-fragment dependency: To generate the FRDEP m  X  m matrix, the test data generator requires a parameter APFS which denotes the probability of a certain transaction causing some data transmission from the location of a specific fragment to the location of another given fragment. Increasing APFS yields more affinity and cohesion between fragments and intensifies the effect of COST2 on the final COST.
 from 0 , fragSize j are calculated using the equations mentioned in Sect. 4.3 . 11 Experimental results As stated earlier, we have compared our algorithm with a modified version of the SE data allocation algorithm. The comparison is done according to 210 different configurations of the DAP. In each problem instance, the number of fragments, m , and the number of sites, n , are chosen from [ 3 , 50 ] with the constraint that n should be less than or equal to m . 20 on the quality of achieved solutions.

For each test, the number of fragments and sites are defined and the test data generator is to the values shown in Table 2 .

The values for the probability parameters RPT and APF are chosen to be as close as possible to those in real problems. We have experimentally examined different values for the APFS parameter and chose the value which usually results in solutions with nearly equal values for COST1 and COST2.

The maximum number of iterations (or generations in the SE algorithm) is set to 200 experiments was to conduct a comparison, this value was deemed to be sufficient.
We have tested three versions of our ACO data allocation algorithm against the SE algo-rithm:
The  X  version : In this version we set CH_Max and EX_Max to zero which means no usage of the local search.

The  X  version: In this version we have CH_Max = m and EX_Max = 3 which causes a slightly lightweight local search.

The  X  version: Finally in this version we set CH_Max = EX_Max = m and thus used the maximum power of our local search method.

The results are organized into two types of diagrams: in type1 we have fixed the number of fragments and tested the algorithms with variable numbers of sites. On the other hand, in type2 the number of sites is fixed and the number of fragments varies. Thus a total num-ber of 40 diagrams are generated from which we have selected the following four diagrams (Figs. 2 , 3 , 4 , 5 ).

According to the above diagrams (Figs. 2 , 3 , 4 , 5 ) in most cases compared to the SE algorithm the proposed ACO-DAP algorithm (using local search procedure) provides better that the ACO data allocation algorithm is more scalable than the alternate SE data alloca-tion algorithm. However, the higher quality solutions appear only at the cost of higher time complexity of our algorithm. In order to address this problem the defined CH_Max and EX_Max parameters help the designer to achieve a desirable tradeoff between solution quality and algorithm X  X  running time. Consequently, the merit of our proposed ACO-DAP algorithm can be distinguished with its high quality solutions, scalability and flexibility.
By further analyzing the diagrams, it can be seen that the  X  version of the ACO data allo-cation algorithm outperforms the others. However, this algorithm is the most time consuming version. Thus  X  version is recommended whenever the quality of the solution is the main concern and not the computation time. One should note that in most cases these computations how close is the  X  curve to the  X ). Therefore if the main concern is the computation time, one can use customized values for the CH_Max and EX_Max in order to achieve a desirable of applying local search in the algorithm. 12 Conclusion database systems with memory capacity constraint. We took query optimization and integrity enforcement mechanisms in the formulation of the DAP into consideration. We also proposed a new data allocation algorithm called ACO-DAP which had been defined based on ACO meta-heuristics and combined it with a local search procedure.

The main contribution of this work is the clear definition provided for the DAP which not only considers the query optimization and integrity enforcement mechanism but also exploits the ACO concepts to solve the problem.

In our experimental studies, we have evaluated and compared the optimality of solutions and the execution time of three different versions of the ACO-DAP algorithm with that of a revised version of one of the most successful algorithms in this field known as SE algorithm DAP algorithm to have superior transaction response times to SE in DAPs of various sizes especially large ones. However, depending on the amount of applied local search, the time complexity of different versions of the ACO-DAP algorithm may become worse than that of the SE algorithm.

The impact and the influence of the local search mechanism which exist on the ACO-DAP in several experiments. Our experimental results demonstrated the merits of using the local search procedure in achieving appropriate data allocation schemes. These results also show the flexibility of our algorithm which provides a diverse range of solution quality and time complexity trade-offs.

In the future, we plan to extend our algorithm to provide solutions for the DAP with replication. Additionally, other plausible direct extensions would be completing the problem formulation, expansion of our experiments in order to cover a larger variety of values for other parameters and rearrangement of algorithms for DAP based on other meta-heuristics such as Particle Swarm Optimization or Neural Networks.
 References Author Biographies
