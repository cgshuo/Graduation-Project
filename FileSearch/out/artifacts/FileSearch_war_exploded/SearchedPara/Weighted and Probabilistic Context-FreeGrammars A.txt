 Carnegie Mellon University Brown University because PCFGs and HMMs are exactly as expressive as WCFGs and chain-structured CRFs, respectively. 1. Introduction
In recent years the field of computational linguistics has turned to machine learning
Adding weights to a formal grammar allows disambiguation (more generally, ranking wish to choose those weights empirically.
 (PCFG) X  X hat defined a distribution over the structures allowed by a grammar. Given a in the model.

Pereira 2001), maximum margin estimation (Taskar et al. 2004), and unsupervised con-
Weighted grammars learned in this way differ in two important ways from traditional, methods are widely documented and recognized.
 the same rules but where the weights of the rules expanding a nonterminal must sum to one.
 dom, unnormalized models should be more expressive than normalized, probabilistic bility distributions defined by WCFGs and PCFGs are the same (Abney, McAllester, and Pereira 1999; Chi 1999).
 extend the results of Chi and of Abney et al., and show that WCFGs and PCFGs both define the same class of conditional distribution. Moreover, we present an algorithm same rules as the WCFG and that defines the same conditional distribution over trees given strings.
 likelihood. 2. Weighted CFGs
ACFG G is a tuple N , S ,  X  , R where N is a finite set of nonterminal symbols, S production rules of the form X  X   X  where X  X  N and  X   X  ( N a positive number called the weight with each rule in R . attached to the rule X  X   X  , and the vector of rule weights by  X =
A weighted grammar is the pair G  X  = G ,  X  . 478
Let  X  ( G ) be the set of (finite) trees that G generates. For any  X   X  is defined as follows: distribution over  X  ( G ) by dividing by Z (  X  ): rules expanding each nonterminal is one: tight are given in several places, including Booth and Thompson (1973) and Wetherell (1980).
 Let G = { G  X  } denote the set of the WCFGs based on the CFG G (i.e., the WCFGs in G denote the set of PCFGs based on G . In general, G PCFG is a proper subset of is, every PCFG is also a WCFG, but because there are weight vectors  X  that don X  X  obey Equation 2, not all WCFGs are PCFGs.
 P Z &lt;  X  be the probability distributions over the trees  X  ( G ) defined by the WCFGs and let P PCFG be the probability distributions defined by the PCFGs sition 4) and Abney, McAllester, and Pereira (Lemma 5) showed that namely, that every WCFG probability distribution is in fact generated by some PCFG. 2.1 Chi X  X  Algorithm for Converting WCFGs to Equivalent PCFGs Chi (1999) describes an algorithm for converting a WCFG to an equivalent PCFG. Let G  X  be a WCFG in in X that can be built using G . Then define: For simplicity, let Z t (  X  ) = 1 for all t  X   X  . Chi demonstrated that G that Z X (  X  )isfiniteforall X  X  N  X   X  .
 where  X  i is the i th element of  X  and |  X  | is the length of  X  . Chi proved that G and that P  X  (  X  ) = s  X  (  X  ) / Z (  X  ) for all trees  X   X  Z (  X  ). The Z X (  X  ) are related by equations of the form which constitute a set of nonlinear polynomial equations in Z converges quickly when Z (  X  )isfinite. 3. Classifiers and Conditional Distributions
A common application of weighted grammars is parsing. One way to select a parse tree for a sentence x is to choose the maximum weighted parse that is consistent with the observation x : distribution of trees given sentence observations. A WCFG defines such a conditional distribution as follows: 480 tion (3) is equivalent to choosing  X  to maximize P  X  (  X  | G such that, for all n  X  0, (Note that, to be fully rigorous, we should quantify n in We use the abbreviated form to keep the notation crisp.) For any G follows that, for any x  X  L ( G ), Z x (  X  ) &lt;  X  ; the converse holds as well. tribution of trees given the sentence, for any sentence x need to normalize s  X  (  X  )by Z x (  X  ) (Equation (4)). Let some Z n (  X  ) diverge; this is a subset of G Z =  X  . 2 To s e e t h a t Example 1.

Example 1
This grammar produces binary structures over strings in a a distribution over all binary trees, given the string.

Example 2 gives a grammar in G Z  X  X ampen. X 
Example 2 set of trees for a n cannot be normalized into a distribution ( Z grammar in Example 2 is a simple example), then G Z
 X  expressed by G Z dampening cyclic derivations (WCFGs in G Z explanations depend crucially on arbitrary lengthening of cyclic derivations. Theorem 1
For a given CFG G , C Z Proof
Suppose we are given weights  X  for G such that G  X   X  G Z a transformation on  X  resulting in a new grammar G  X  that is in the same family of conditional distributions (i.e., P
 X  (  X  we construct  X  G  X   X  in CNF that preserves the total score for any x of  X 
G  X   X  was demonstrated by Goodman (1998, Section 2.6), who gives an algorithm for constructing the value-preserving weighted grammar  X  G  X  may include new nonterminals.
 mar is  X  G  X   X  = { S } , S ,  X  ,  X  R ,  X   X  .  X  R contains the rule S for a  X   X  . The weights of these rules are The grammar  X  G  X   X  will allow every tree allowed by  X  follows that, for all x  X  L ( G ): 482 Summing over all trees of any given yield length n , we have a given sentence x in L ( G ). Every tree generated by of bracketing a sequence of n items. The total number of unlabeled binary bracketings of an n -length sequence is the n th Catalan number C n (Graham, Knuth, and Patashnik length n is |  X  | n . Therefore let every n -length sentence having its score divided by (8 | G  X  defines the same conditional distribution over trees given sentences as G and (b) force Z n (  X  ) to converge. We have not found the minimum such value, but 8 is sufficiently large.
 The sequence of Z n (  X  ) now converges:
Hence Z (  X  ) =  X  n = 0 Z n (  X  )  X  2and G  X   X  G Z &lt;  X  Corollary 1
Given a CFG G , C Z Proof
By Theorem 1, C Z C apply the transformation in the proof of Theorem 1 to get a convergent WCFG, then apply Chi X  X  method (our Section 2.1). 4. HMMs and Related Models produce are labeled sequences, which are equivalent to right-branching trees. We can finite-state grammars that HMMs stochasticize as  X  X ight-linear grammars. X  Rather than using the production rule notation of PCFGs, we will use more traditional HMM nota-with parse trees).
 random fields (MRFs; Section 4.1), in which moves or transitions are associated with positive weights and which are globally normalized like a WCFG. two different types of dependency structures in these automata. Abusing the standard terminology somewhat, in a Mealy automaton arcs are labeled with output or terminal symbols, whereas in a Moore automaton the states emit terminal symbols. 484 length-n sequence x 1 , x 2 , ... , x n  X   X  n and  X  =  X  0 nonterminal) path. The distribution is given by  X  state transition emits a symbol (no arcs), an assumption made in typical tagging and chunking applications of HMMs. We can convert a Mealy HMM to a PCFG by including, for every tuple x ,  X  ,  X  ( x  X   X  and  X  ,  X   X  N ) such that p ( x ,  X  with the same probability as the corresponding HMM transition. For every  X  such that p ( STOP |  X  ), we include the rule  X   X  , with probability p (
HMM can be converted to a PCFG by adding a new nonterminal  X   X  for every state  X  and including the rules  X   X   X   X  (with probability p (  X  |
Moore HMMs are less probabilistically expressive than Mealy HMMs, though we can convert between the two with a change in the number of states.
 bution over paths given words, we conditionalize This is how scores are assigned when selecting the best path given a sequence. (right-linear) grammars G HMM , the set of probability distributions define. 6 4.1 Mealy Markov Random Fields
When the probabilities in Mealy HMMs are replaced by arbitrary positive weights, the production rules can be seen as features in a Gibbs distribution. The resulting model defined a conditional distribution over paths given sequences by normalizing for each sequence x : states and paths: linear grammar G . We call these weighted grammars  X  X ealy MRFs. X  As in the WCFG case, we can add the constraint Z n (  X  ) &lt;  X  (for all n ), giving the class models to natural language processing, there are no rules of the form X G &lt;  X  is empty and emissions.

This means that any random field using Mealy HMM features (Mealy MRF) such that  X  distribution of tags given words. 7 Corollary 2 For a given right-linear grammar G , C HMM = C Z &lt;  X  = imperfections in the numerical search for parameter values. 4.2 Maximum-Entropy Markov Models
While HMMs and chain MRFs represent the same set of conditional distributions, we can show that the maximum-entropy Markov models (MEMMs) of McCallum, Freitag, and Pereira (2000) represent a strictly smaller class of distributions. bution over paths given words as: Unlike an HMM, the MEMM does not define a distribution over output sequences x .
The name  X  X aximum entropy Markov model X  comes from the fact that the conditional 486 and are trained to maximize entropy.
 Lemma 1
For every MEMM, there is a Mealy MRF that represents the same conditional distribu-tion over paths given symbols.
 Proof
By definition, the features of the MRF include triples  X  weight  X   X  P can be expressed by MEMMs is strictly smaller than those expressible by HMMs (and by extension, Mealy MRFs).
 Theorem 2 For a given right-linear grammar G , C MEMM  X  C HMM .
 Proof
We give an example of a Mealy HMM whose conditional distribution over paths (trees) pointing out to us the existence of examples like this one. Define a Mealy HMM with three states named 0, 1, and 2, over an alphabet { a, b, c state.
 Example 3 tributions cannot both be met by any MEMM. To see why, consider
This implies that the distribution p (  X |  X  , x ) (e.g., multinomial or log-linear). and the latter are less expressive.
 models with the same dependencies among random variables. If the HMM X  X  distribu-tion p ( x i ,  X  i |  X  i  X  1 ) is factored into p ( x i | distributions that the Moore HMM cannot. 8
MEMM to  X  X ook ahead X  n words, factoring its distribution into p (  X  x Corollary 3 lookahead can represent.
 Proof Consider again Example 3. Note that, for all m  X  1, it sets
Suppose we wish to capture this in an MEMM with n symbols of look-ahead. Letting m = n + 1, 488
MEMM to  X  X ook back X  and condition on earlier symbols (or states), it cannot represent the distribution in Example 3.
 with the conditional structure of the model. That some model structures work better than others at real NLP tasks was discussed by Johnson (2001) and Klein and Manning of those allowed by Mealy HMMs X  X akes this unsurprising. 5. Practical Implications and HMMs) are no more powerful than the probabilistic models. This means that, inso-the model, (b) improved estimation procedures (e.g., maximum conditional likelihood differently than PCFGs and HMMs, respectively, unless they are augmented with more features. 6. Related Work
Abney, McAllester, and Pereira (1999) addressed the relationship between PCFGs and probabilistic models based on push-down automaton operations (e.g., the structured may not be simple (indeed, a blow-up in the automaton X  X  size may be incurred), given G , P automata are weakly equivalent. Importantly, the standard conversion of a CFG into a work on the relationship between weighted CFGs and weighted PDAs is described in Nederhof and Satta (2004).
 that is essentially the same as Moore MRFs) express the same set of distributions as
Moore HMMs, under the condition that the Boltzmann chain has a single specific end state. MacKay avoided the divergence problem by defining the Boltzmann chain always G 7. Conclusion
We have shown that weighted CFGs that define finite scores for all sentences in their languages have no greater expressivity than PCFGs, when used to define distributions no more powerful than Mealy HMMs, for instance. We have also related  X  X aximum entropy Markov models X  to Mealy Markov random fields, showing that the former is a strictly less expressive weighted formalism.
 Acknowledgments References 490
