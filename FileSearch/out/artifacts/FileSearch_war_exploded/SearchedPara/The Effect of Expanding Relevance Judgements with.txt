 We examine the effects of expanding a judged set of sen-tences with their duplicates from a corpus. Including new sentences that are exact duplicates of the previously judged sentences may allow for better estimation of performance metrics and enhance the reusability of a test collection. We perform experiments in context of the Temporal Summariza-tion Track at TREC 2013. We find that adding duplicate sentences to the judged set does not significantly affect rel-ative system performance. However, we do find statistically significant changes in the performance of nearly half the sys-tems that participated in the Track. We recommend adding exact duplicate sentences to the set of relevance judgements in order to obtain a more accurate estimate of system per-formance.
 Categories and Subject Descriptors: H.3.4 [Systems and Software Performance evaluation]: Efficiency and Effec-tiveness Keywords: Duplicate Detection; Evaluation; Pooling
The Temporal Summarization Track (TST) at TREC 2013 [2], required returning information relevant to topics, from a web-scale time-ordered document stream (the TREC KBA 2013 Stream Corpus [1]). The Sequential Update Sum-marization (SUS) task in the TST, called for participating systems (runs), to return updates (sentences) about events (topics), with the goal that new updates should contain in-formation that is new to the user.

For the SUS task, the Stream corpus is effectively a collec-tion of documents (along with their timestamps), spanning the time period between October 2011 to January 2013, with an average of 93,037 documents per hour. Each participat-ing system was tasked to return updates from a duration spanning 240 hours for each topic. In all, the TST received 28 runs from the participants of the SUS task. The number of updates returned in the runs varied from 110 to 2,815,808. For evaluation of runs, the pool of updates judged by track assessors contains 9,113 sentences for 9 task topics.
We found that there are a very large number of duplicate sentences in the corpus. Indeed, the track organizers found duplicates amongst the sentences sampled from runs, while constructing the pool for assessment. Accordingly, they cre-ated an evaluation framework designed to accommodate for duplicates within the judged set of sentences. In effect, the identifier of a duplicate sentence (within the judged set of sentences) is mapped to the identifier of the designated orig-inal sentence (prototype). However, the duplicates of judged sentences from the corpus, were not mapped to the proto-types. The track X  X  evaluation is also designed to omit sen-tences that are not in the judged set. Such omission is con-sistent with similar approaches [7, 3], and works very well for evaluating relative system performance (Section 3.1).
Including exact duplicates of judged sentences from the corpus may have ramifications for a system X  X  evaluation as well as for the re-usability of the TST test collection. For example, if sentence s is an exact duplicate of a sentence p in the judged set, a system would neither be rewarded nor penalized for returning s , since s is not in the judged set of sentences. This may lead to an unfair evaluation for the system.

In this work we investigate the effect of expanding the set of judged sentences with their exact duplicates from the corpus, where, an exact duplicate is a sentence that exactly matches the text of the judged sentence for each character in the sentence string. We find that:
We briefly describe the original set of judged sentences of the SUS task and set a context for subsequent sections.
Each topic in the SUS task corresponds to an event (of type earthquake, storm, accident, bombing or shooting) that occurred within the corpus duration. The query for the topic included query terms, and query start and end times. The query was considered active for the time interval between query start and end times (the query duration, typically 240 hours).

The assessors initially identified a set of nuggets (short snippets of text containing information relevant to the topic), from the edit history of the Wikipedia 1 article for an event. A pool of sentences for evaluation was created by sampling 60 updates per topic per submitted run with highest confi-dence scores (a confidence score was required for each update in a run). Some close duplicates were identified from this ini-tial sample which allowed more updates to be included in the pool [2]. This created a pool of sentences totalling 9,113 for 9 topics. A pooled sentence was matched against nuggets and was considered relevant if it contained a nugget.
Given the original set of judged sentences, for each topic, for each sentence in the judged set, we found the exact dupli-cates within the query duration of the topic, from the Stream corpus. For each duplicate found, we added it to the origi-nal judged set and mapped its identifier to the identifier of its prototype. Table 1 lists the number of judged sentences, the number of duplicates known to exist within the original judged set, the number of sentences for which the duplicates were found within the query duration. Also shown are the number of relevant sentences in the original judged set for each topic, and the number of relevant sentences for which duplicates were found in the query duration from the corpus.
We see that the original judged set expands nearly a 1000 times from 9,113 to 9,034,179, when duplicates are added. However, the number of relevant duplicates found is ex-tremely less at 97,256 (about 10 times the size of the original set). Table 2 shows the top occurring duplicate sentences. In fact, the top 3 most occurring duplicate sentences ac-count for 67% of the total duplicates found. We observed that most of the duplicates that occur with high frequency are duplicates of non relevant sentences. They appear to be boilerplate sentences found in web-site navigation menus or at the end of news articles. In contrast the most frequent rel-evant duplicate,  X  X ational Hurricane Center in Miami said Isaac became a Category 1 hurricane Tuesday with winds of 75 mph. X , was found to occur just 5,403 times, in the query duration for topic 5 ( X  X urricane Isaac X ).

We feel that the high number of duplicates found overall could be because of news/web syndication services. News wire documents form the second largest component of docu-ments in the Stream corpus [1], with social documents (blogs and forums) forming the bulk of the corpus, and documents sourced from links submitted to bitly.com forming the re-mainder. One would expect a high number of news/web ar-ticles to be generated after the occurrence of a catastrophic event.
The TST introduces new measures to evaluate tempo-ral summarization. The measures are analogous to preci-sion/gain and recall and they are also designed to account http://www.wikipedia.org/ for latency and verbosity of updates (sentences). Partici-pant systems are tasked to return a timestamp along with each update about an event. Latency discounts are applied to sentences that are returned later than the first known occurrence of the nugget of information that they contain. The nuggets were identified, and their time of first occur-rence noted, by the track X  X  assessors, using the edit histo-ries of the Wikipedia articles for the topics (Section 2.1). Verbosity discounts are applied based on the length of the returned sentences. Longer sentences are penalized more than shorter sentences by the verbosity discount, which es-sentially forms an aspect of  X  X ser friendliness X  for a system. The track introduces two precision-like measures, Expected Latency Gain (E[LG]) and Expected Gain (E[G]), as well as two recall-like measures, Latency Comprehensiveness (LC) and Comprehensiveness (C). The E[LG] and E[G] measures use the relevance score (0/1 for binary relevance) as a mea-sure of gain and potentially discount the gain for update-latency. The C and LC measures attempt to capture how well a system performed at returning all identified nuggets and how quickly it emitted updates containing these nuggets from the canonical time of first occurrence for the nuggets.
The track coordinators consider E[LG] and LC to be the official metrics of the track and we report our analyses for these metrics only. Detailed descriptions of all metrics can be found in the 2013 Temporal Summarization Track Over-iew [2]. We note here that in the track X  X  evaluation frame-work, a system returning sentences not present in the judged set of sentences is neither penalized nor rewarded. On the other hand, returning duplicate sentences present in the judged set will result in the verbosity discount being applied for each.
Table 3 provides Kendall X  X   X  correlation between the rank-ings given by the original judged and expanded set for each of the four task measures. We can see that the correla-tion tends to be high indicating that relative performance is typically maintained regardless of assessment pool. This is consistent with other works conducting similar research [7, 10, 11, 3, 9]. Accordingly, we can see that not including all exact duplicates in the judged sentences was a reasonable and effective method of relative system performance evalu-ation. However, there may still be benefit to expanding the judged set because there are changes to the absolute per-formance scores of the systems (Section 3.2) which may be indicative of a change in the user experience.
Overall, 13 submitted runs showed a statistically signifi-cant change in E[LG] and 12 submitted runs showed a sta-tistically significant change in LC, for a paired t-test with p-value  X  0.05. The average difference across topics (and standard deviation) between the original judged set and the expanded set are presented in Tables 4 and 5 for E[LG] and LC, respectively. Runs for which there was no difference in score are not listed in the tables. Runs are listed in sorted order based upon the run names.

There exist several runs with no E[LG] change which is primarily due to the fact that they do not contain any newly identified duplicates and so would not be penalized for them. The majority of the the other runs, do experience a general decrease in the performance with respect to the expanded Table 2: Examples of Duplicate Sentences with high number of occurrences across all topics
Judged Sentences Kendall X  X   X  for Ranking Metric expanded with E[LG] E[G] LC C exact duplicates 0.899 0.894 0.942 0.937 lowercase duplicates 0.899 0.894 0.942 0.937 Table 3: Rank correlation between Original Judged sentences vs Expanded set, for TST measures set; though, the affect on such runs is not consistent across topics. Of particular note is run 8 in Table 4, which has a general increase in performance indicating that run 8 did re-turn relevant sentences which were not in the original judged set, but were duplicates of relevant sentences from the orig-inal set. This increase was found not to be statistically sig-nificant with a p-value &gt; 0.05 for a paired t-test. However, with more topics, we may find more statistically significant positive improvements [8].

Furthermore, we can see that the duplicate detection in the expanded set does not hurt but improves LC perfor-mance on average. This makes sense since systems may have returned relevant sentences duplicate to those in the origi-nal set of judgements. By expanding the original judged set with exact duplicates, we argue that a more accurate assessment of absolute performance is being achieved since runs are now being rewarded or penalized for new sentences which were not present in the original set.
We also tried duplicate detection with simple transforma-tions (to ensure minimal information loss) like lowercase -ing, whitespace -collapsing (reducing sequences of whitespace to a single space) and whitelower (lowercase + whitespace). The Table 4: Average Difference and Standard Deviation of E[LG] between the Original Set and the Expanded Set of judged sentences lowercase transformation is a common normalization tech-nique employed in search engine indexing, and may intro-duce errors (e.g. US, the country, vs us, the pronoun). It did increase the total number of duplicates found to 10,872,223 but found only 44 new duplicates for relevant updates. Both whitespace transformations did not produce different sets of sentences than their basis transformations.

The Kendall X  X   X  between the original judged set and the expanded set, using exact duplicates and lowercase dupli-cates, are identical (Table 3), due to the fact they both produced identical rankings, and in fact scores, for all sys-tems. The lowercase transformation found additional du-plicates overall but very few relevant duplicates and hence there are insignificant changes in the scores when averaged across topics and runs. With more topics and more par-ticipant systems, we might expect to see the effect of such transformations to be more pronounced.
We believe that much of the negative effect of the original judged set expansion (on the gain-based measures) would be subsumed, in the majority of cases, if the verbosity penaliza-tion were applied to all sentences retrieved by a system. Cur-rently, such penalization only occurs if a retrieved sentence is also present in the judged set. While beneficial for eval-uating system performance for finding relevant sentences, ignoring unjudged sentences does not accurately reflect the user experience. One would expect a large difference in per-formance scores between submitting 1,000 sentences and 1 Table 5: Average Difference and Standard Deviation of LC between the Original Set and the Expanded Set of judged sentences million sentences; a difference which has the potential to overwhelm the user. The official set of judgements averages around 1,000 sentences per topic which results in an upper limit on verbosity penalization.

However, the current evaluation methology is consistent with that of [7], who found that removing unjudged docu-ments from evaluation of ranked lists works well and does not affect the relative system performance. Indeed, the new TST metrics are stable for measuring relative system perfor-mance, even after processing an expanded set of judgements that is 1000 times the size of the original.

Nugget-based evaluation [4, 6] -where identified relevant material is representative of relevance -is aimed towards automatic identification of nuggets in the whole collection. However, tracking duplicates of the retrievable unit (e.g. documents, sentences) may be useful depending on the eval-uation metrics for the specific task at hand (such as Tempo-ral Summarization).
As an immediate future work, we plan to investigate the effect of including unjudged updates for verbosity discounts. Accounting for unjudged sentences in runs may not be a straightforward task due to the potential quantity of them. A simple but potentially inefficient mechanism would be to store information (e.g. length, timestamp, duplicates) about every sentence in the corpus which would facilitate apply-ing the necessary discount. Alternatively, it may be possible to produce an estimate of unjudged sentence lengths. This may require the use of some form of sampling and the use of inclusion probabilities (e.g. [5]). Determining a reason-able method for applying verbosity penalization to unjudged sentences is an area of research that we intend to pursue.
The TST at TREC 2013 had only 9 topics. As per [8, 12], even though there are statistically significant changes in systems X  performance, we cannot currently presume that the effects would be reproducible for a different/larger set of topics. We definitely need to test for the effect of duplicates on more number of topics.
We experimented with expanding the set of judged sen-tences with exact duplicates from the corpus and investi-gated its effects on the evaluation of temporal summariza-tion. We found that adding exact duplicate sentences to the set of relevance judgements, does not affect relative order-ing of temporal summarization systems. It does however, induce a change in the performance scores of the systems. 13 out of 28 systems that participated in the Temporal Sum-marization Track at TREC 2013 experienced a statistically significant change in performance scores with respect to the track X  X  metrics. With more topics from the TREC 2014 ver-sion of the track, we expect to get a more accurate estimate of changes in performance when evaluating with a large num-ber of duplicates. Expansion of relevance judgements with exact duplicates is simple and not only does it help produce more accurate performance scores but also potentially aids in reusability of the test collection for the development of new temporal summarization systems.
We thank Rakesh Guttikonda for helpful discussions and ideas regarding this work. This work was made possible by the facilities of SHARCNET (www.sharcnet.ca) and Com-pute/Calcul Canada, and was supported in part by NSERC, and in part by the Google Founders Grant, and in part by the University of Waterloo. [1] KBA Stream Corpus 2013. http: [2] J. Aslam, F. Diaz, M. Ekstrand-Abueg, V. Pavlu, and [3] C. Buckley and E. M. Voorhees. Retrieval evaluation [4] G. Marton and A. Radul. Nuggeteer: Automatic [5] V. Pavlu and J. Aslam. A practical sampling strategy [6] V. Pavlu, S. Rajput, P. B. Golbus, and J. A. Aslam. [7] T. Sakai and N. Kando. On information retrieval [8] M. Sanderson and J. Zobel. Information retrieval [9] I. Soboroff, C. Nicholas, and P. Cahan. Ranking [10] A. Trotman and D. Jenkinson. IR evaluation using [11] E. M. Voorhees. Variations in relevance judgments and [12] E. M. Voorhees and C. Buckley. The effect of topic set
