 The Netflix Prize is a collaborative filtering problem. This subfield of machine learning became popular in the late 1990s with the spread of online services that used recom-mendation systems (e.g. Amazon, Yahoo! Music, and of course Netflix). The aim of such a system is to predict what items a user might like based on his/her and other users X  previous ratings. The Netflix Prize dataset is much larger than former benchmark datasets, therefore the scala -bility of the algorithms is a must. This paper describes the major components of our blending based solution, called the Gravity Recommendation System (GRS). In the Net-flix Prize contest, it attained RMSE 0.8743 as of November 2007. We now compare the effectiveness of some selected individual and combined approaches on a particular subset of the Prize dataset, and discuss their important features and drawbacks. Collaborative filtering (CF) is a subfield of machine learnin g that aims at creating algorithms to predict user preference s based on known user ratings or user behavior in the selec-tion/purchasing of items. Recommendation systems provid-ing personalized suggestions greatly increase the likelih ood of a customer making a purchase compared to systems pro-viding unpersonalized ones. This is especially important i n markets where the variety of choices is large, the taste of customers is important, and last but not least, the price of items is modest. Typical areas of such services are mostly related to art (esp. books, movies, music), fashion, food &amp; restaurants, gaming &amp; humor, etc. Clearly, the online DVD rental service operated by Netflix fits into this list. The Netflix Prize problem (see details in [1]) belongs to the social-filtering CF framework. In this case the user first provides ratings of some items, titles or artifacts -usuall y on a discrete numerical scale -, and then the system recom-mends other items based on ratings the virtual community using the system has already provided [5]. The underlying assumption of such a system is that people who had similar  X 
Domonkos Tikk was supported by the J  X anos Bolyai Re-search Scholarship of the Hungarian Academy of Science. tastes in the past may also agree in the future.
 The first works on the field of CF have been published in the early 1990s. GroupLens systems in [7] was one of the pioneer applications in the field, where users could rate ar-ticles after having read them on a 1 X 5 scale, and were then offered suggestions. The underlying techniques of predict-ing user preferences can be divided into two main groups [2]. Memory based approaches operate on the entire rating database. On the other hand, model based approaches use the database to estimate or learn a model, and then apply it for prediction.
 Over the last broad decade many CF algorithms have been proposed that approach the problem from different points of view, including similarity based approaches [7; 9], Bayesi an networks [2], various matrix factorization techniques [3; 6; 11], and very recently restricted Boltzman machines [8]. We use the following notation in this paper. The rating ma-trix is denoted by X  X  X  1 , . . . , 5 } I  X  J , where an element x stores the rating of the j th movie provided by i th customer. I and J denote the total number of users and movies, resp. We refer to the set of all known ( i, j ) pairs in X as R . At the prediction of a given rating we refer to the user as active user , and to the movie as active movie . Circumflex accents denote the prediction of given quantities, i.e.  X  x for x . We refer the Reader to [1] for the description of Netflix Prize datasets. The idea behind matrix factorization (MF) techniques is very simple. Suppose we want to approximate the matrix X as the product of two matrices: where U is an I  X  K and M is a K  X  J matrix. Values u ik and m kj can be considered the k th feature of the i th user and the j th movie, respectively. If we consider the matrices as linear transformations, the approximation can be interpreted as follows: the M matrix transforms from R into R K , and U transforms from R K into R I . Thus, the R acts as a bottleneck when predicting R I from R J . The number of parameters to describe X is reduced from |R| to IK + KJ . Note that X contains integers in [1 , 5], while the elements of M and U are real numbers.
 Several matrix factorization techniques being able to hand le missing data have been successfully applied to CF (see e.g. [4; 6]). Note that missing data cannot be treated as zero. For the given problem, our goal is to find U and M such that the elements of  X  X = UM are as close to the elements of X as possible, but only for known ratings. Formally: Here  X  x ij denotes how the i th user would rate the j th movie, according to the model, e ij the training error on the ( i, j )th example, and SE the total squared training error. Eq. (3) states that optimal U and M matrices minimize the sum of squared errors only over the known elements of X . To minimize SE, we applied a simple gradient descent method to find a local minimum. The gradient of e 2 ij is: We updated the weights in the direction opposite of the gradient: Here  X  is the learning rate. To better generalize on unseen examples, we applied regularization with factor  X  to prevent large weights: Initially, we set the weights in U and M randomly, and set  X  and  X  to some small positive value. Steps (5) X (6) were repeated until the (RM)SE improved.
 Our approach is similar to Simon Funk X  X  SVD, 1 but we update each factor simultaneously, and initialize the ma-trix randomly. Consequently, our approach converges much faster, since it iterates much less on X .
 We remark that after the learning phase, each value of X can be computed easily by (2), even the  X  X nseen X  values. In other words, the model ( U and M ) can say something about how an arbitrary user would rate any movie. We now describe some important MF variants that are used in GRS. Besides the (user ID, movie ID, date, rating) quadruples, Netflix has also provided the title and year of release of each movie. This information can be incorporated in the MF model as follows: we can extend M with rows that indicate the occurrence of words in the movie title, or the year of release. For example, the k 1 th row of M is 1 or 0 depending on whether or not  X  X eason X  occurs in the title of the movies. This row of M is fixed to be a constant, thus we do not apply equ. (6) for k = k 1 . If the occurrence of  X  X eason X  in a title increases the i th user X  X  rating (i.e. the user likes TV-series), the model will contain a positive weight for u Other constant values can also be inserted. For example, we can increase the size of matrices, K , by 2, by inserting the average rating of the user and the movie, resp. We found http://sifter.org/~simon/journal/20061211.html that the most effective approach was to insert constant 1s into the matrix (increasing K by 2 again). The more parameters a matrix factorization has, the harder it is to set them well, but the greater is the chance of getting a more precise or different MF. Our goal is to find many different and precise MFs. We experimented mainly with the following parameters: We subsampled the matrix for faster evaluation of parameter settings. We experienced that movie-subsampling substan-tially increased the error, in contrast to user-subsamplin g. It is interesting that the larger the subsample was, the fewe r iterations were required to achieve the optimal model. In the spirit of creating many different MFs we experimented with the following: We arranged the movie and user fea-tures (recall that these are provided by MF) to form a 2D lattice and defined a simple neighborhood relation between the features. If the difference of the horizontal and vertica l positions of two features are small (they are neighbors) the  X  X eaning X  of those features should also be close. To achieve this, we proceed as follows. After each usual learning step, we modify the feature values towards the direction of the neighboring features by smoothing the calculated changes of feature values. In practice, in order to prevent slow trai n-ing, we take only those features into account which are next to each other: the modified, and  X  u the 2D array of the original changes in user features. The indices of  X  u denote the user index, and the two coordinates of the feature in the feature array, resp . We applied an analogous formula on the movies as well. The 2D features of a movie (or user) can be nicely visualized. Meanings can be associated with the regions of the image. The labels on Figure 1 are assigned to sets of features and not to single ones. The labels have been determined based on movies having extreme values at the given feature. Such feature maps are useful to detect main differences be-tween movies or episodes of the same movie. Figure 2 rep-resents the three episodes of The Matrix movie. One can observe that the main characteristic of the feature maps are the same, but there are noticeable changes between the first and the other episodes. In the first episode the feature value s are higher in the area of  X  X olitical protest X  and  X  X bsurd X  and lower around  X  X egendary X .
 In neighbor based approaches, a similarity value and a uni-variate predictor function is assigned to each movie pair or to each user pair. The first variant can be referred to as the movie neighbor and the second as the user neighbor method. Assuming the movie neighbor method, the unknown matrix element x ij can be estimated as where s jk is the similarity between the j th and the k th movie, and f jk is the function that predicts the rating of the j th movie based on the rating of the k th. The answer of the system is the similarity-weighted average of movie-t o-movie sub-predictions.
 A common approach is to use a correlation based movie-to-movie prediction and similarity. If we introduce the notati on R jk = { i : ( i, j ) , ( i, k )  X  X } , then the formula of the empir-ical Pearson correlation coefficient [10] between two movies is the following: where  X  j and  X  k are the empirical means,  X  j and  X  k are the empirical standard deviations of the j th and the k th movie, resp.
 If there are only a few common ratings between two movies then the empirical correlation is not a good estimator of the true one. Therefore we regularize the estimate with the a priori assumption of zero correlation. This can be per-formed in a theoretically grounded way by applying Fisher X  X  z-transformation: z jk = tanh  X  1 ( r jk ). The distribution of z jk is approximately normal [10] with standard deviation 1 / p |R jk | X  3. We can now compute a confidence interval around the z value and use the lower bound as the regu-is the regularization factor. Our experiments showed that  X  = 2 . 3 is a reasonable choice for the Netflix problem. The regularized correlation coefficient can be obtained from the inverse of the z -transformation: r  X  jk = tanh z  X  jk . We can now define the similarity metric and the movie-to-movie predictor functions: where  X  can be called the amplification factor. Large  X   X  1 values increase the weight of similar movies in eq. (7). An advantageous property of neighbor based methods is that they can be implemented without training phases. Having said this, it is useful to precompute the correlation coeffi-cients and keep them in the memory, because this drasti-cally decreases testing time. It is sufficient to store only th e correlations between the 6000 most rated movies, because these values are much more frequently needed than other ones.
 Additional improvements can be achieved using the follow-ing modifications: Theoretically, we could also use the user based  X  X ual X  of thi s method but the user neighbor approach does not fit well to the Netflix problem, because The approaches were evaluated on the probe subset of the training set in [1]. Tables 1 and 2 present some results of MF and neighbor based methods, respectively, and Table 3 tab-ulates the best results of single methods and their combina-tions. Due to the lack of space, we present here results that were obtained without the use of any performance boosting technique. However, the results themselves indicate the effi -ciency of various techniques, and we also report on the effect of certain previously mentioned modifications.
 Table 1: RMSE of the basic matrix factorization algorithm for various  X  and  X  values ( K = 40)  X  Table 3: Best results of single and combined approaches As shown also in Table 3, MF is the most effective approach of the three. The appropriate selection of its parameters ca n boost MF X  X  performance significantly (see also Table 1). In our MF experiments we initialized the weights of U and M uniformly between  X  0 . 01 and 0 . 01. We realized that increas-ing K yields better RMSE, however, the memory consump-tion increases linearly, and the training time almost linea rly. The increase of  X  , and analogously, the decrease of  X  have the same effect: it improves RMSE but slows down the training. In case of  X  = 0 . 005 ,  X  = 0 . 02 37 iterations were required, as opposed to  X  = 0 . 02, where 12 iterations were enough. The running times was ranged from 15 mins to 2 hours depend-ing on the parameters. We can state in general that MF with better parameters requires more time to converge, but it also depends on the details of implementation. Figure 3 illustrates the effect of K (when  X  = 0 . 01 ,  X  = 0 . 01) and some learning curves of 2D MFs. Figure 3: MFs with different values of K (on the left), learn-ing curves of two 2D MF algorithm (on the right) The results of Table 2 were achieved with the correlation coefficients computed only between the 6.000 most rated movies based on the top 75.000 users. When predicting other ratings we simply answered a combination of the user and the movie mean.
 Observing the combinations of the different approaches on Table 3, one can see that they outperform the single ones significantly.
 The effect of using only a fixed number of neighbors im-proved RMSE by 0 . 0098. Taking the movie average into account yielded an improvement of 0 . 0024. Regularizing the correlation coefficient using Fisher X  X  z-transformatio n re-sulted in a 0 . 0043 change. When all of the three enhance-ments were applied, the improvement was 0 . 0173, which means that combination of these modifications amplify their own beneficial effects. [1] J. Bennett and S. Lanning. The Netflix Prize. In Proc. [2] J. S. Breese, D. Heckerman, and C. Kadie. Empirical [3] J. Canny. Collaborative filtering with privacy via facto r [4] N. Del Buono and T. Politi. A Continuous Technique [5] W. Hill, L. Stead, M. Rosenstein, and G. Furnas. Rec-[6] J. D. M. Rennie and N. Srebro. Fast maximum mar-[7] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and [8] R. Salakhutdinov, A. Mnih, and G. Hinton. Restricted [9] B. M. Sarwar, G. Karypis, J. A. Konstan, and J. Riedl. [10] G. W. Snedecor and W. G. Cochran. Statistical Meth-[11] N. Srebro, J. D. M. Rennie, and T. S. Jaakkola.
