 Continuous latent factor models, such as factor analysis (FA) and probabilistic principal components analysis (PPCA), are very commonly used density models for continuous-valued data. They have many applications including latent factor discovery, dimensionality reduction, and missing data im-putation. The factor analysis model asserts that a low-dimensional continuous latent factor z n  X  R L underlies each high-dimensional observed data vector y n  X  R D . Standard factor analysis models and  X  is a D  X  D diagonal matrix specifying the marginal noise variances. If we set  X  =  X  2 I and require W to be orthogonal, we recover probabilistic principal components analysis (PPCA). Such models can be easily fit using the expectation-maximization (EM) algorithm [Row97, TB99]. The FA model can be extended to other members of the exponential family by requiring that the natural (canonical) parameters have the form Wz n +  X  [WK01, CDS02, MHG08, LT10]. This is the unsupervised version of a generalized linear model (GLM), and is extremely useful since it allows for non-trivial dependencies between data variables with mixed types.
 The principal difficulty with the general FA model is computational tractability, both at training and test time. A problem arises because the Gaussian prior on p ( z n ) is not conjugate to the likeli-hood except when y n also has a Gaussian distribution (the standard FA model). There are several approaches one can take to this problem. The simplest is to approximate the posterior p ( z n | y n ) using a point estimate, which is equivalent to viewing the latent variables as parameters and esti-mating them by maximum likelihood. This approach is known as exponential family PCA (ePCA) [CDS02]. We refer to it as the  X  X M X  approach to fitting the general FA model since we maximize over z n in the E-step, as well as W in the M-step. The main drawback of the MM approach is that it ignores posterior uncertainty in z n , which can result in over-fitting unless the model is carefully regularized [WCS08]. This is a particular concern when we have missing data.
 The opposite end of the model estimation spectrum is to integrate out both z n and W using Markov chain Monte Carlo methods. This approach has recently been studied under the name  X  X ayesian exponential family PCA X  [MHG08] using a Hamiltonian Monte Carlo (HMC) sampling approach. We will refer to this as the  X  X S X  approach to indicate that we are integrating out both z n and W by sampling. The SS approach preserves posterior uncertainty about z n (unlike the MM approach) and is robust to missing data, but can have a significantly higher computational cost.
 In this work, we study a variational EM model fitting approach that preserves posterior uncertainty about z n , is robust to missing data, and is more computationally efficient than SS. We refer to this as the  X  X M X  approach to indicate that we integrate over z n in the E-step after applying a variational bound, and maximize over W in the M-step. We focus on the case of continuous (Gaussian) and categorical data. Our main contribution is the development of variational EM algorithms for factor analysis and mixtures of factor analyzers based on a simple quadratic lower bound to the multinomial likelihood (which subsumes the Bernoulli case) [Boh92]. This bound results in an EM iteration that is computationally more efficient than the bound previously proposed by Jaakkola for binary PCA when the training data is fully observed [JJ96], but is less tight. The proposed bound has advantages relative to other previously introduced bounds, as we discuss in the following sections. In this section, we describe a model for mixed continuous and discrete data that we call the gen-eralized mixture of factor analyzers model. This model has two important special cases: mixture models and factor analysis, both for mixed continuous and discrete data. We use the general model as well as both special cases in subsequent experiments. In this work, we focus on Gaussian dis-tributed continuous data and multinomially distributed discrete data. The graphical model is given in Figure 1 while the probabilistic model is given in Equations 1 to 4. We begin with a description of the the general model and then highlight the two special cases.
 We let n  X  X  1 ...N } index data cases, d  X  X  1 ...D d } index discrete data dimensions and k  X  { 1 ...K } index mixture components. Superscripts C and D indicate variables associated with y nd  X  X  1 ...M +1 } denote the d th discrete data variable. 1 We use a 1 -of-( M +1) encoding for the discrete variables where a variable y D nd = m is represented by a ( M + 1) -dimensional vector y D nd in which m  X  X h element is set to 1, and all remaining elements equal 0. We denote the complete data vector by y n = y C n , y D n 1 ,..., y D nD The generative process begins by sampling a state of the mixture indicator variable q n for each data case n from a K -state multinomial distribution with parameters  X  . Simultaneously, a length L latent factor vector z n  X  R L is sampled from a zero-mean Gaussian distribution with precision parameter  X  . Both steps are given in Equation 1. The natural parameters of the distribution over the data variables is obtained by passing the latent factor vector z n through a linear function defined by a factor loading matrix and an offset term, both of which depend on the setting of the mixture indicator variable q n . Assuming that q n = k , the continuous data vector y C n is Gaussian distributed with mean W C k z n +  X  k and covariance  X  parameters  X  ndk = W D dk z n +  X  D dk , as seen in Equation 2. Here, N (  X | m , V ) denotes a Gaussian distribution with mean m and covariance V , while M (  X |  X  ) denotes a multinomial distribution with parameter vector  X  such that P i  X  i = 1 and  X  i  X  0 . For the discrete data variables, the natural parameter vector is converted into the standard mean parameter vector through the softmax function S m (  X  ) is itself defined in terms of the log-sum-exp (LSE) function, which we give in Equation 5. We note that the factor loading matrices for the k th mixture component are W C k  X  R D c  X  L and semble of factor loading matrices and offsets to be W k = [ W C k , W D 1 k , W D 2 k ,..., W D D  X  k = [  X  prior on these parameters. For each row of each factor loading matrix W k , we use a Gaussian prior of the form N ( 0 , X   X  1 w I ) . We use vague conjugate priors for the remaining parameters. As mentioned at the start of this section, this general model has two important special cases: general-ized factor analysis and mixture models for mixed continuous and discrete data. The factor analysis model is obtained by using one mixture component and at least one latent factor ( K = 1 ,L &gt; 1 ). The mixture model is obtained by using no latent factors and at least one mixture component ( K &gt; 1 ,L = 0 ). In the mixture model case where L = 0 , the distribution is modeled through the offset parameters  X  k only. We will compare these three models in Section 5.
 Before concluding this section, we point out one key difference between the current model and other latent factor models for discrete data like multinomial PCA [BJ04] and latent Dirichlet allo-cation (LDA) [BNJ03]. In our model, the natural parameters for discrete data are defined on a low-dimensional linear subspace and are mapped to the mean parameters via the softmax function. In multinomial PCA and LDA, the mean parameters are instead directly defined on a low-dimensional linear subspace. The latter approach can also be extended to the mixed-data case [BDdF + 03]. How-ever, model fitting is even more computationally challenging than in our approach. In fact, the bounds we propose can be used in this alternative setting, but we leave this to future work. In the standard expectation-maximization (EM) algorithm for mixtures of factor analyzers, the E-step consists of marginalizing over the complete-data log likelihood with respect to the posterior over the mixture indicator variable q n and latent factors z n . The M-step consists of maximizing the expected complete log likelihood with respect to the parameters  X  . In the case of Gaussian observations, this posterior is available in closed form because of conjugacy. Introduction of discrete observations, however, makes it intractable to compute the posterior as the likelihood for these observations is not conjugate to the Gaussian prior on the latent factors.
 To overcome these problems, we propose to use a quadratic bound on the LSE function. This allows us to obtain closed form updates for both the E and M steps. We use the quadratic bound described in [Boh92]. In rest of the paper, we will refer to it as the  X  X ohning bound X . For simplicity, we describe the bound only for one discrete measurement with K = 1 and  X  k = 0 in order to suppress the n , k and d subscripts. To ensure identifiability, we assume that the last element of  X  is zero (this can be enforced by setting the last row of W to zero).
 The key idea behind the Bohning bound is to take a second order Taylor series expansion of the LSE function around a point  X  . An upper bound to the LSE function is found by replacing the Hessian matrix H (  X  ) , which appears in the second order term, with a fixed matrix A such that A  X  H (  X  ) is positive definite for all  X  [Boh92]. Bohning gives one such matrix A , which we define below. The expansion point  X  is a free variational parameter that must be optimized.  X   X  R M is the vector of variational parameters, I M is the identity matrix of size M  X  M and 1 M is a vector of ones of length M . By substituting this bound in to the log-likelihood, completing the square and exponentiating, we obtain the Gaussian lower bound described below. We obtain a Gaussian-like  X  X seudo X  observation  X  y  X  corresponding to the discrete observation y D . We use this result to obtain a lower bound for each mixed data vector y n . We will suppress the  X  subscripts, which differ for each data point n and each discrete variable d for clarity. Let  X  y n = [ y observation gives the following lower bound on the joint likelihood, p (  X  y n | z n ) = N (  X  y n |  X  Wz n ,  X   X  ) ,  X  W = W C , W D 1 ,..., w D D Given this pseudo observation, the computation of the posterior means m n and covariances V n is similar to the Gaussian FA model as seen below. This result can be generalized to the mixture case in a straightforward way. The M-step is the same as in mixtures of Gaussian factor analyzers [GH96]. The only question remaining is how to obtain the value of  X  . By maximizing the lower bound, one can show that the optimal value is  X  n =  X  Wm n . This follows from the fact that the Bohning bound is tight for lse (  X  ) when  X  =  X  , and that the curvature is independent of  X  [Boh92]. We iterate this update until convergence. In practice, we find that the method usually converges in five or fewer iterations.
 The most attractive feature of the bound described above is its computational efficiency. To see this, note that the posterior covariance V n does not in fact depend on n if the data vector is fully observed, since A is a constant matrix. Consequently we need only invert V n once outside the EM loop instead of N times, once for each data point. We will see in the next section that the other existing quadratic bounds do not have this property. To derive the overall computational cost of our EM algorithm, let us define the total dimension of  X  y n to be D and assume K = 1 . Computing V n takes O ( L 3 + L 2 D ) time, and computing each m n takes O ( L 2 + LD ) time. So the total cost of one E-step is O ( L 3 + L 2 D + NI ( L 2 + LD )) , where I is the number of variational updates. If there is missing data, V n will change across data cases, so the total cost will be O ( NI ( L 3 + L 2 D )) . 3.1 Comparison with Other Bounding Methods In the binary case, the Bohning bound reduces to the following: log(1 + e  X  )  X  1 2 A X  2  X  b  X   X  + c  X  , interesting to compare this bound to Jaakkola X  X  bound [JJ96] used in [Tip98, YT04]. This bound can  X  c Although the Jaakkola bound is tighter than the Bohning bound, it has higher computational com-plexity. The reason is that the  X  A  X  parameter depends on  X  and hence on n , which means we need to compute a different posterior covariance matrix for each n . Consequently, the cost of an E-step is O ( NI ( L 3 + L 2 D )) , even if there is no missing data (note the L 3 term inside the NI loop). To explore the speed vs accuracy trade-off, we use the synthetic binary data described in [MHG08] with N = 600 , D = 16 , and 10% missing data. We learn a binary FA model with L = 10 ,  X  z = 1 , and  X  w = 0 . We learn on the observed entries in the data matrix and compute the mean squared error (MSE) on the held out missing entries as in [MHG08]. We average the results over 20 repetitions of the experiment. We see in Figure 2 (top left) that the Jaakkola bound gives a lower MSE than Bohning X  X  bound in less time on this data. Next, we consider the case where the training data is fully observed using a modified version of the data generating procedure described in [MHG08]. We vary D from 16 to 128 while setting L = 0 . 25 D and N = 10 D . We sample L different binary prototypes at random, assign each data case to a prototype, and add 10% random binary noise. We measure the average time per iteration over 40 iterations of each method. Figure 2 (bottom left) shows that the Bohning bound exhibits much better scalability per iteration than the Jaakkola bound in this regime. The speed issue becomes more serious when combining binary variables with categorical variables. Firstly, there is no direct extension of the Jaakkola bound to the general categorical case. Hence, to combine categorical variables with binary variables, we can use the Jaakkola bound for binary and the Bohning for the rest. However, this is not computationally efficient as we need to compute the posterior covariance for each data point because of the Jaakkola bound. For computational simplicity, we use Bohning X  X  bound for both binary and categorical data.
 Various other bounds and approximations to the multinomial likelihood also exist; however, they are all more computationally intensive, and do not give an efficient variational algorithm. To the best of our knowledge these methods have not been applied to the FA model, but we describe them briefly for completeness. An extension of the Jaakkola bound to the multinomial case was given in [Bou07]. However, this tends to be less accurate than the Bohning bound. Another approach [BL06]  X  is a variational parameter. This bound does not give closed form updates for the E and M steps so a numerical optimizer needs to be used (see [BL06] for details).
 Instead of using a bound, an alternative approach is to apply a quadratic approximation derived from a Taylor series expansion of the LSE function [AX07]. This provides a tighter approximation that could perform better than a bound, but one cannot make convergence guarantees when using it inside of EM. In practice we found this alternative approach to be very slow on the datasets that we consider. In view of its speed and simplicity, we will only consider the Bohning method for the remainder of the paper. In this section, we discuss several alternative methods for fitting the generalized FA model in the case K = 1 , which we compare to the VM method. We defer comparisons of FA to mixture models to Section 5. 4.1 Maximize-Maximize (MM) Method The simplest approach to fit the FA model is to maximize log p ( Y , Z , W |  X  w , X  z ) with respect to Z and W , the matrix of latent factor values and the factor loading matrix. It is straightforward to compute the gradient of the log posterior and apply a generic optimizer (we use the limited-memory quasi-newton method). Alternatively, one can use coordinate descent [CDS02]. We set the hyper-parameters  X  w and  X  z by cross validation. To handle missing data, we simply evaluate the gradients by only summing over the observed entries of Y . At test time, consider a data vector consisting of missing and observed components, y  X  = [ y  X  m , y  X  o ] . To fill in the missing entries, we compute  X  z  X  = arg max p ( z  X  , y  X  o |  X  W ) and use it with  X   X  to predict y  X  m .
 The MM approach is simple and widely applicable, but these benefits come at the expense of ignor-ing the posterior variance of Z [WCS08]. This has negative consequences for the method in terms using D = 10 , L = 5 , and N = 200 data cases by sampling from the FA model. We set  X  w = 1 ,  X  z = 1 , and  X  c = 0 . 1 . We standardize each data dimension to have unit variance and zero mean. We consider the case of 10% and 50% missing data. We evaluate the sensitivity of the methods to the setting of the posterior precision parameter  X  w by varying it over the range 10  X  2 to 10 2 . We fix  X  z = 1 , since this is the standard assumption when fitting FA models. We run the methods on a random 50 / 50 train/test split. We train on the observed entries in the training set, and then compute MSE on the missing entries in the training and test sets. We average the results over 20 repetitions of the experiment.
 Figure 2 (top right) shows that the test MSE of the MM method is extremely sensitive to the prior precision  X  w . We can see that this sensitivity increases as a function of the missing data rate. We hypothesize that this is a result of the MM method ignoring the posterior uncertainty in Z . This is supported by looking at the MSE on the training set, Figure 2 (bottom right). We see that the MM method overfits when  X  w is small. Consequently, MM requires a careful discrete search over the values of  X  w , which is slow, since the quality of each such value must be estimated by cross-validation. By contrast, the VM method takes the posterior uncertainty about Z into account, resulting in almost no sensitivity to  X  w over this range. Henceforth we set  X  w = 0 for VM, meaning we are performing (approximate) maximum likelihood parameter estimation. 4.2 Sample-Sample (SS) Method An alternative to the MM approach is to sample both Z and W from their posteriors using Hamil-tonian Monte Carlo (HMC) [MHG08]. We call this the  X  X S X  method, since we sample both Z and W . HMC leverages the fact that we can compute the gradient of the log posterior in closed form. However, it has several important parameters that must be set including the step size, the momentum distribution, the number of leapfrog steps, etc.
 To handle missing data, we can simply evaluate the gradients by only summing over the observed entries of Y . We do not need to impute the missing entries on the training set. At test time, we have a collection of samples of W . For each sample of W and each test case, we sample a set of z , and compute an averaged prediction for y m . In Figure 2 (right), we see that SS is insensitive to  X  w , just like VM, since it also models posterior uncertainty in Z (note that the absolute MSE values are higher for SS than VM since for continuous data, VM corresponds to EM with an exact posterior). However, in Figure 2 (top left), we see that SS can be much slower than VM. In the remainder of the paper we focus on deterministic fitting methods only. In this section, we evaluate the performance of our model on real data with mixed continuous and discrete variables. We consider the following three cases of our model: (1) a model with latent factors but no mixtures (FA) (2) a model with mixtures but no latent factors (Mix) and (3) the general mixture of factor analyzers model (MixFA). To learn the FA model, we consider the FA-MM and FA-VM approaches. For the Mix model, we use the standard EM algorithm. In the Mix model, continuous variables can be modeled with either a diagonal or a full covariance matrix. We refer to these two variants as Mix-Diag and Mix-Full. For MixFA model, we use the VM approach. This gives us five methods: FA-MM, FA-VM, MixFA, Mix-Full and Mix-Diag.
 70% for training, 10% for validation and 20% for testing. We consider 20 splits for each dataset. We use the validation set to determine the number of latent factors and the number of mixtures (ranges shown in the table) with imputation error (described below) as our performance objective. For the FA-MM method, we set the values of the regularization parameters  X  z and  X  w by cross validation. parameters, we set  X  z = 1 and  X  w = 0 .
 One way to assess the performance of a generative model is to see how well it can impute missing data. We do this by randomly introducing missing values in the test data with a missing data rate of 0.3. For continuous variables, we compute the imputation MSE averaged over all the missing values (these variables are standardized beforehand). For discrete variables, we report the cross-entropy (averaged over missing values) defined as y T log  X  p , where  X  p m is the estimated probability that y = m and y uses the one-of-( M + 1) encoding.
 These errors are shown in Figure 3 along with the running time for ASES dataset in the bottom right subfigure. We see that FA-VM consistently performs better than FA-MM for all the datasets. Moreover, because of the need for cross-validation, FA-MM takes more time than FA-VM. We also see that the Mix model, although faster, performs worse than FA-VM. Finally, as expected, MixFA generally performs slightly better than FA, but takes longer to run. In this work we have proposed a new variational EM algorithm for fitting factor analysis models with mixed data. The algorithm is based on the Bohning bound, a simple quadratic bound to the log-sum-exp function. In the special case of fully observed binary data, the Bohning bound itera-tion is theoretically faster than Jaakkola X  X  bound iteration and we have demonstrated this advantage empirically. More importantly, the Bohning bound also easily extends to the categorical case. This enables, for the first time, an efficient variational method for fitting a factor analysis model to mixed continuous, binary, and categorical observations.
 In comparison to the maximize-maximize (MM) method, which forms the basis of ePCA and other matrix factorization methods, our variational EM method accounts for posterior uncertainty in the latent factors, leading to reduced sensitivity to hyper parameters. This has important practical con-sequences as the MM method requires extensive cross validation while our approach does not. We have compared a range of models and algorithms in terms of imputation performance on real data. This analysis shows that the cost of the cross validation search for MM is higher than the cost of fitting the FA model using our method. It also shows that standard alternatives to FA, such as finite mixture models, do not perform as well as FA. Finally, we show that the MixFA model can yield a performance improvement over a single FA model, although at a higher computational cost. We note that the quadratic bound that we study can be used in a variety of other models, such as linear-Gaussian state-space models with categorical observations [SH03]. It might be an interesting alternative to a Laplace approximation to the posterior, which is used in [KPBSK10, RMC09]. The bound might also be useful in the context of the correlated topic model [BL06, AX07], where similar variational EM methods have been applied.
 In the Bayesian statistics literature, it is common to use latent factor models combined with a pro-bit observation model; this allows one to perform inference for the latent states using efficient auxiliary-variable MCMC techniques (see e.g., [HSC09, Dun07]). Additionally, the recently pro-posed Riemannian Manifold Hamiltonian Monte Carlo sampler [GCC09] may significantly speed-up sampling-based approaches for mixed-data factor analysis models. We leave a comparison to these approaches to future work.
 Acknowledgments We would like to thank the reviewers for their helpful coments. This work was completed in part at the Xerox Research Center Europe and was supported by the Pacific Institute for the Mathematical Sciences and the Killam Trusts at the University of British Columbia. [AX07] A. Ahmed and E. Xing. On tight approximate inference of the logistic-normal topic [BDdF + 03] Kobus Barnard, Pinar Duygulu, Nando de Freitas, David Forsyth, David Blei, and [BJ04] W. Buntine and A. Jakulin. Applying Discrete PCA in Data Analysis. In UAI , 2004. [BL06] D. Blei and J. Lafferty. Correlated topic models. In NIPS , 2006. [BNJ03] D. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. J. of Machine Learning [Boh92] D. Bohning. Multinomial logistic regression algorithm. Annals of the Inst. of Statistical [Bou07] G. Bouchard. Efficient bounds for the softmax and applications to approximate infer-[CDS02] M. Collins, S. Dasgupta, and R. E. Schapire. A generalization of principal components [Dun07] D. Dunson. Bayesian methods for latent trait modelling of longitudinal data. Stat. [GCC09] M. Girolami, B. Calderhead, and S.A. Chin. Riemannian manifold hamiltonian monte [GH96] Z. Ghahramani and G. Hinton. The EM algorithm for mixtures of factor analyzers. [HSC09] P. R. Hahn, J. Scott, and C. Carvahlo. Sparse Factor-Analytic Probit Models. Technical [JJ96] T. Jaakkola and M. Jordan. A variational approach to Bayesian logistic regression [KPBSK10] S. Koyama, L. Perez-Bolde, C. Shalizi, and R. Kass. Approximate methods for state-[LT10] J. Li and D. Tao. Simple exponential family PCA. In AI/Statistics , 2010. [MHG08] S. Mohamed, K. Heller, and Z. Ghahramani. Bayesian Exponential Family PCA. In [RMC09] H. Rue, S. Martino, and N. Chopin. Approximate Bayesian Inference for Latent Gaus-[Row97] S. Roweis. EM algorithms for PCA and SPCA. In NIPS , 1997. [SH03] V. Siivola and A. Honkela. A state-space method for language modeling. In Proc. [TB99] M. Tipping and C. Bishop. Probabilistic principal component analysis. J. of Royal [Tip98] M. Tipping. Probabilistic visualization of high-dimensional binary data. In NIPS , [WCS08] Max Welling, Chaitanya Chemudugunta, and Nathan Sutter. Deterministic latent vari-[WK01] Michel Wedel and Wagner Kamakura. Factor analysis with (mixed) observed and [YT04] K. Yu and V. Tresp. Heterogenous data fusion via a probabilistic latent-variable model.
