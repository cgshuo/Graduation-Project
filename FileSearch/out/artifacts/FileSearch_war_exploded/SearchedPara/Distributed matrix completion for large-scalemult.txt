 Computer Engineering Department, Iran University of Science and Technology, Narmak, Tehran, Iran 1. Introduction
Processing large data matrices with partially unknown entries, has recently gained a lot of interest due to its applications in many fields, such as recommender systems [1], music rating prediction [2], interdependent from one another, while in multi-class classification they are mutually exclusive.
Large-scale classification has many applications in the fields of computer vision, machine learning, security and etc. Many algorithms tackle the large-scale classification problem on a single computer (such as LIBLINEAR [10] and PEGASOS [11]), but training and testing with large amount of instances on a single machine becomes very slow due to the disk I/O rather than CPU usage. Some other works Alternating Direction Method (ADM), where they distribute the samples across the network nodes and labels.
 makes the optimization process a non-convex and time consuming one. But the computational complex-matrix completion algorithm could be of great interest.

The problem of matrix completion and rank minimization is initially a non-convex optimization prob-lem [14,15], which is simply based on factorizing the matrix into two matrices of rank at most r .Re-cently, rank minimization has gained attention, due to the simple, effective success in solving many problems. As denoted by [8] the minimization of the rank function can be achieved using the minimizer vex rank minimization problem, many approaches are developed, such as Fixed Point Continuation [6], Augmented Lagrangian Multipliers method [9] and Alternating Direction Method [16].

Our goal is to develop a distributed algorithm for large-scale matrix completion for the purpose of matrix completion/factorization use parallel/distributed programming models, such as MapReduce and Hadoop. For instance, [17,18] are designed for MapReduce and [19] for the second version of Hadoop. One of the drawbacks of these models is that they are limited to the restrictive programming models and therefore most of them suffer from run-time overheads. Furthermore, the cluster management is hard and the optimal configuration of the nodes is not obvious. Other approaches in this area include These approaches use Alternating Direction Method or Stochastic Gradient Descent approaches for matrix into two lower-rank matrices suggest non-convex objectives. Recently, another method [22] has solving their optimization problem.
 In this paper, we propose an iterative algorithm for nuclear norm minimization based on consensus. problem into a matrix completion framework, with a convex optimization process to minimize the rank of the matrix. A consensus algorithm based on singular value thresholding is proposed to minimize the nuclear norm. The minimization problem is solved via Alternating Direction Method of Multipliers, by [25,26], consensus algorithms for convex optimization problems converge to the optimal solution in a satisfactory amount of time.
 (such as multiple cameras looking at a same scene), this distribution could be the most appropriate model.

Our contributions could be summarized as two fold: (1) Developing a distributed convex approach for multi-label classification problems.
 the results on some synthetic and real datasets. Finally, Section 5 concludes the paper. 2. Matrix completion for classification
Matrix Completion is the process of recovering a matrix from a sampling of its entries. We are inter-ested in recovering a data matrix D from a matrix D 0 , in which we only get to observe a number of its entries, which is comparably much smaller than the total number of elements in the matrix. Let  X  optimization problem would be
Unfortunately, this is an NP-hard optimization problem and all known algorithms which provide exact function could be simply defined as the number of non-vanishing singular values. Therefore, a simple which is called the nuclear norm:
To model the noise and the outliers, an error term is added to the formulation coupled with a loss function, l ( . ) , to avoid trivial solutions: Let m be the number of different classes (the number of labels), n the dimensionality of the feature b , such that a loss l ( . ) of the class predictions for the training data is minimized: As noted by Goldberg et al. [6] the problem of classifying N tst test entries can be cast as a Matrix Completion task. To this end, we can concatenate all labels and features into a single matrix: X D entries the error term is zeroed out: the best Y tst and the error matrix, E mc such that the rank of D = D 0 + E mc is minimized [6]. This would be equivalent to [5]: parameters  X  and  X  1 are positive trade-off weights [5,6].
 3. Distributed matrix completion be of great interest.

In order to recover the correct labels for each test instance, we need to let the matrix completion So, we first develop a distributed algorithm for nuclear norm minimization of a data matrix.
For modeling the distributed environment, let X  X  assume that the network of our processing nodes is E X  X  X V represents the nodes that can communicate wit h each other. With this definition, each node i can have some neighbors denoted by N i = { j  X  X  :( i, j )  X  X } and the degree d i = |N i | . 3.1. Distributed nuclear norm minimization
As long as the error matrix E is sufficiently sparse, we can exactly recover the low-rank matrix D from D 0 = D + E by solving the following convex optimization problem [27]: By introducing a Lagrangian multiplier, Y , the Lagrangian function would be:
Using the iterative thresholding (IT) or the singular value thresholding (SVT) algorithm [27,28] and the Alternating Direction Method, problem Eq. (8) could be solved by updating each variable while tion, D 0  X  D  X  E , is used to update Y .
 A shrinkage operator, S [ . ] , as a proximal operator for the nuclear norm could be defined as: Having the singular value decomposition of a matrix, USV , we use the Alternating Direction Method for Low-Rank Matrix Recovery (ADM-LRMR) algorithm, which is shown in Algorithm 1 [27].
In order to parallelize this algorithm, we need to distribute the entries present in D 0 between the processing nodes. We will have separate E matrices for each node, and accordingly we will require to use corresponding Lagrangian multipliers.
 We can assume that the original data matrix is formed as
The Lagrangian multipliers Y and the error matrix E arealsosplitinthesamemanner.Now,we need to calculate the SVD of the J = D 0  X  E +  X   X  1 k Y k matrix (as could be seen in Algorithm 1). Algorithm 1 Alternating Direction Method for Low-Rank Matrix Recovery (ADM-LRMR), for solving optimization prolem Eq. (8) Require: Initial data matrix D 0 , parameter  X  .
 Ensure: Completed matrix D k and Error matrix E k
Initialize Y 0 =0 , X  0 &gt; 0 , E 0 = 0 , X &gt; 1 . k =0 while not converged do end while First, suppose we want to compute 1 N own by: grow, C can still be computed accurately, possibly in different number of iterations.
In order to compute the SVD of J (as required in Algorithm 1, J = U  X  V ), we need to calculate For this purpose, we can first compute the SVD of C :
Therefore, after the distributed averaging each node can recover V and if they know N p ,theyalsocan recover  X  . These two matrices will be common for all the nodes and easy to calculate. They can then compute their own share of the matrix U as:
As a result, the SVD operation in Algorithm 1 could be calculated in a distributed manner and each Algorithm 2 Distributed Alternating Direction Method for Low-Rank Matrix Recovery (DADM-LRMR) algorithm for solving Eq. (8) on the i th node, in a network of N p nodes.
 Require: Initial data matrix D i 0 , parameters  X  ,and N p .
 Ensure: Completed matrix D i k and Error matrix E i k
Initialize C i = J i 0 =0 , X  k &gt; 0 , E i 0 = 0 , X &gt; 1 . k =0 while not converged do end while This algorithm consists of two stages: (1) Calculating C via consensus over the network and (2) and would satisfy the convergence properties of the whole Algorithm [9]. Although, the main loop of Algorithm 3. The performance of the two algorithms are discussed and evaluated in the experiments section. 3.2. Distributed multi-label classification
In many applications, none of the nodes have a complete instance of the data. Whereas, they can only smart camera network setup, where each camera observes the scene from its viewpoint and an instance could be defined as a concatenation of all the features extracted form the scene of all cameras. We can model the distribution of the data matrix as shown in Fig. 2. The data matrix (as shown in Algorithm 3 Inexact Distributed Alternating Direction Method for Low-Rank Matrix Recovery (IDADM-LRMR) algorithm for solving Eq. (8) on the i th node, in a network of N p nodes. Require: Initial data matrix D i 0 , parameters  X  ,and N p .
 Ensure: Completed matrix D i k and Error matrix E i k
Initialize Y i 0 =0 , X  k &gt; 0 , E i 0 = 0 , X &gt; 1 . k =1 while not converged do end while node.

For the classification task, we need to solve problem Eq. (7), which is a convex problem. Let us, for g ( node i , which has a portion of the data matrix:
The constraint D 1 = 1 is also enforced by keeping the last row of E mc i for each single node equal to 0 [9]. The only shared part between the nodes is the minimization of the nuclear norm of the whole data matrix, which was outlined in the previous section. This problem could be solved using an ADM algorithm. The Lagrangian function is: rithm 4. The nuclear norm minimization is done via the algorithm outlined in Algorithm 2. Algorithm 4 Distributed Matrix Completion Algorithm, on the i th processing node.
 Require: Initial portion of the data matrix for the i th node, D i = D 0 i , and parameter  X  . Ensure: i th portion of the completed matrix D i
L k =0 while not converged do end while 4. Experiments
To evaluate the proposed technique for large-scale classification, we set up several experiments on We first need to test the DADM-LRMR algorithm (Algorithms 2 and 3), and then we will evaluate the distributed classification algorithm presented in Algorithm 4.

The distributed algorithms are simulated with 8 processing nodes, each with a Core i7-3610 processor and 8 GB of RAM. The data is split and distributed among these nodes. The nodes are considered to be connected with a ring topology.

We have also conducted experiments on several different number of machines to analyze the effect of different numbers of processing nodes on the performance of the algorithm. 4.1. Nuclear norm minimization In order to test the DADM-LRMR algorithm proposed in Section 3.1, we create some artificial data. evaluation of the algorithm. The rank-r matrix D  X  0 is generated by the product LR ,where L and R matrix, E  X  . So, the data matrix to be used for the low-rank matrix recovery would be calculated as an indicator of the level of sparseness for the error matrix.
 Figure 3 shows the convergence speed in the number of iterations of both DADM-LRMR and IDADM-LRMR algorithms for a randomly generated matrix of rank r = 200 ,and m = 5000 with E  X  0 = 0 . 1 m 2 added noise. Figure 4 also gives the running times for this execution compared to the non-distributed IT algorithm. As can be seen, the amount of the required time for communication in DADM-LRMR is more, compared to IDADM-LRMR and that is because it needs to come into a complete consensus while the latter only uses the inexact formulation. 4.2. Classification a random rotation matrix and U 1 is a random orthogonal matrix of dimension d  X  r . Therefore, each subspace has a dimension of r . Then, we form an r  X  s i.i.d. N (0 , 1) matrix, Q i ,andsample s data vectors from each subspace by X i = U i Q i , 1 i 10 .
For the experiments, we set d = 1000 , r =50 , s = 200 , t = 2000 . So, we have 200  X  10 data vectors find the labels for the test instances. Since we do not add any noise, we get almost 100% accuracy. Figure 5 shows execution time for finding the correct labels for all the instances.

We also test the algorithm with some real world datasets. For our purpose, we employ a music emo-tions dataset, a document dataset (webspam) and an education dataset (kddcup2010). The detials of a dataset of records of interactions between students and computer-aided-tutoring systems. We use the pre-processed KDD Cup 2010, where each feature is normalized to have unit norm. The whole dataset of the datasets. Each of these sub-datasets contain fewer number of instances, with the same number distributed algorithms.

To compare the execution time of the proposed algorithm (Distributed Matrix Completion or D-MC), we use the well-known Block LIBLINEAR [10]. Block LIBLINEAR (B-LIBLINEAR) runs on a single machine, but to deal with the huge number of instance for training, it loads and processes the data block-wise. Figure 6 shows the performance comparisons between the algorithms. It shows the overall execution time, data loading time, and communication overhead for both train and test phases of the above algorithms.
 as a function of running time.

As shown D-MC converges faster due to the smaller amount of data it has to deal with in each node, while B-LIBLINEAR is run on a single machine, which causes each iteration to deal with a huge amount of data. This is also why B-LIBLINEAR has larger loading time as an overhead.
 time on a single machine is calculated as the speedup of the algorithm. Figure 8 shows the amount of the speedups as a function of the number of the processing nodes, for the large-scale kddcup2010 and webspam datasets. As could be seen, the speedup of the proposed distributed algorithm for larger datasets is rather larger, while the number of the processing nodes increases. 5. Discussions and conclusion
In this paper, we have developed a distributed multi-label classification framework based on matrix completion. We have first proposed a simple, yet effective algorithm to minimize the nuclear norm of matrices for low rank recovery of matrices, which can solely have many applications, such as Robust PCA (RPCA), Low-Rank Representation (LRR) and etc. Then, using this algorithm, the matrix comple-tion framework was extended for distributed large-scale classification.

Matrix completion does not have separate train and test phases and models the data set accurately and algorithm presented in this paper, the same matrix completion framework could also be executed on large-scale matrices.

Originally, the SVD operation is the bottle neck of processing in the centralized algorithm. In our will be dealing with the whole matrix. For extremely large data sets, the amount of communication and processing may put large overheads on the algorithm. One simple solution could be to perform the matrix completion in an incremental way or by grouping the feature vectors for each label. the averaged value will still stay close to the desired value. On the other hand, if one of the nodes the process of deducing the labels.
 References
