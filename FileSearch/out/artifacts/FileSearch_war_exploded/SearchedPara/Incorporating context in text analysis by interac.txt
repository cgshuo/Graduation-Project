 1. Introduction Part of science is the discovery of associations and patterns in the world around us. According to John of lists of observational data into abbreviated form by the recognition of patterns X  X , all with the goal in mind of  X  X  X lgorithmic compression X  X . ( Barrow, 1991, pp. 10 X 11 ) This can be applied to the social sciences where the goal is to make sense of the diversity of human activities. Of particular interest to scholars of communication and informatics are the patterns and associations that occur in texts, such as newspaper articles, works of literature, conversations or email messages. These patterns are studied to learn about so-cial structures and interactions, individual thought processes and the entire range of cognitive and commu-nicative processes between. The study of patterns in text has been applied to a variety of domains including 1997 ), philosophy ( Pasquale &amp; Meunier, 2001 ) and others.

In the applied realm, individuals and organizations are interested in accurate and robust means to track opinions and attitudes for marketing, political, and social reasons ( Lasswell, 1931 ). Personal attitudes and opinions are expressed and shared through a variety of communication modalities such as email. Email messages in public forums, such as listserv lists, can be a steady and plentiful source of personal expression in machine readable form. List email is created at an estimated rate of 36.5 billion messages or 675terabytes ods to analyze. Analysis of these texts, therefore, requires automated techniques that can deal with natural language. Large corporations, for instance, employ automated text analysis methods to help them discover information in focus group transcripts ( Woelfel &amp; Styanoff, 1993 ).

This paper reports on modifications of an interactive activation with competition (IAC) artificial neural network (ANN) algorithm to incorporate the notion of context, defined here as the words in the sentences preceding a sentence being processed. The results from applying two approaches to external activation of words during self-organizing were compared. In the first (sentence) approach the ANN set the external acti-vations of all words to zero after each learning cycle took place, i.e. after each sentence was processed. In the second (message) approach the ANN reduced (without going below zero) the external activations of the words by a reduction factor after the learning cycle had been run X  X he external activations were set to zero only when the beginning of a new message was encountered.

The rationale for this second approach comes from the observation that the specific meaning of each word in a text depends on the context within which the text is set. The context is set, in part, by the other words in the text. Therefore, taking these words into account adds a degree of context to an analysis. There are, of course, other contextual clues that assist the reader in assigning specific meaning to specific words.
These include, for instance, the source of the text (i.e. where it was published and by whom), the references to other texts made within the work, etc. This research focuses on the immediate contextual clues, those that can be derived from the surrounding text.

After a brief discussion of text classification and artificial neural networks, I will present the methodol-ogy used, the results obtained using two samples and some conclusions that are suggested. Finally, some suggestions for future research will be made. 2. Text classification
There are a variety of methods of text analysis including: content analysis ( Breen, 1997; McKinnon, 1989; McMillan, 2000; Palmquist, 2002 ; Rubenstein, 1995; Schneider, 1997; Shi-Xu, 2000 ; and others); text mining ( Dworman, 1996; Hearst, 1999; Witten, 2001 ); natural language processing ( Liddy, 2001; Mani, 1999 ); latent semantic analysis ( Dumais, Furnas, Landauer, Deerwester, &amp; Harshman, 1988 ); probabilistic latent semantic indexing ( Hofmann, 1999 ); and the topic of this study, artificial neural networks ( Belew, analysis. The interested reader should refer, for instance, to Belew (1996) . Current methods have two lim-itations which this study addresses. First, in order to disambiguate terms they often rely on a multistage processes to implement contextual awareness. Second, they usually employ stop word lists which, again, require pre-processing and preclude real-time processing of text streams. Additional pre-processing steps are often needed to conflate plurals and other variants of words to a single term. 2.1. Limitations of current methods 2.1.1. Lack of contextual awareness
Current methods of text analysis generally do not take into consideration  X  X  X ontext X  X . By this I mean words are assigned meanings based on a syntactic, rather than a semantic, level of analysis. Therefore, there is no disambiguation of homographs. The  X  X  X ank X  X  of a river is taken as the same as the  X  X  X ank X  X  in which we save our money. Within the neural network approach some attempts have been made to address this issue. Other methods, e.g. sublanguage approaches ( Liddy, 2004 ), have also attempted to deal with this issue.
They will not be considered further here. The approaches the ANN methods generally use rely on multi-stage analyses (for example, to identify nouns, noun-phrases and attached modifiers, etc.). Grefenstette (1994) describes one such method which involves five pre-processing steps before a weighted Jaccard mea-discounted probabilistic creation and selection of microclusters. Ohgaya, Simmura, and Takagi (2003) incorporate the method reported by Aizawa (2002) in their conceptual fuzzy set method which also uses term co-occurrence in neighboring web pages to improve context sensitivity. Another approach to the prob-lem of homography is the use of multiword terms and noun phrases ( Grefenstette, 1994 ). While these ap-proaches have shown some success, the extra complications and steps they introduce is a drawback. This competition artificial neural network. 2.1.2. The use of stop word lists
Current text analysis methods often rely on the use of  X  X  X top word X  X  lists to filter out (generally) small words that are assumed to occur with a high frequency and to add little or no meaning to the text. This commonly used technique  X  X  X n some cases may hope to improve retrieval X  X  (emphasis added) ( Wilbur &amp;
Sirotkin, 1992, p. 45 ). The increasing use of short acronyms (such as IT , WWW , IS , IR , KM )( Kennedy, 2001 ), further calls into question the value of eliminating stop words as commonly implemented, i.e. words containing fewer than some arbitrary small number of letters. Additionally stop word lists suffer from the same problem that all controlled vocabularies do, namely the slowness with which they are able to respond to changes in the vocabulary of a domain. The method presented here does not rely on the use of stop word lists, but, nonetheless, reduces the significance of these words. 2.2. Artificial neural networks
Artificial neural networks implement algorithms inspired by how the neurons of the brain (and in some cases sensory organs) are thought to act together to perceive, learn, and recall patterns. The fundamental concept is that the nodes (also called units or neurons) are highly interconnected by links whose strengths (or weights) are dynamically adjusted by a learning algorithm when patterns of stimuli are presented. Stim-uli may be words, pixels or anything else that can be differentiated in input to the system. Each stimulus is represented by a node in the network. 1 Thus when a pattern (of words, in this case) is presented to the system the nodes that represent these words are externally activated and, through the learning algorithm, their connections to each other are strengthened. Depending on the exact model and algorithms used, connections to unactivated nodes may also be weakened during the learning step. In this way ANNs be-come sensitized to patterns of stimuli which can be recalled at a later time when a fragment or an inexact copy of the original pattern is presented. ANNs used in this way are a type of associative memory. Thus,
ANN software can be used to discover relationships between words in bodies of natural language text ( Merkl &amp; Rauber, 2000; Woelfel &amp; Styanoff, 1993 ) by developing associations between words based on exposure to those words as they occur together in the text. However, ANN systems typically do not differ-entiate between homographs because they operate on the syntactic rather than the semantic level. This makes them less than ideal for real time text analysis ( Elman, 1990 ). ANN tools would, therefore, be better able to deal with homography if they could be made to take context into account.

Generally speaking, ANNs have two modes of operation, self-organizing and probing. The first, or self-organizing phase, is when the network is  X  X  X earning X  X  about the texts. The second, or recall phase is used to discover what the network has  X  X  X earned X  X . The connection values (which are set to zero when a node is cre-ated) are refined by a learning algorithm applied to the links as the software encounters the features that these nodes represent in the self-organizing mode. Nodes representing words that are encountered within a scanning  X  X  X indow X  X , in this case defined as a sentence but in many implementations an arbitrary or user-define number of adjacent words, are co-activated. A learning step is performed next in which the activa-tions spread through the network via the links. Each node is activated to a degree determined by the weights of the links from nodes that have fired. Connections between highly activated nodes are strength-ened and those between less activated nodes are weakened by the application of a Hebbian learning rule resulting in an average weight of zero and a range of 1 to +1.

Repeatedly presenting similar patterns causes the link weights to gradually assume values that allow pat-terns to be recalled given partial or inexact pattern presentation. Recall is performed by exciting one or more (probe) nodes and allowing the activation to spread. This causes each node to become activated to the degree to which it has become associated with the probe(s) during the learning phase. During recall the learning step is not performed, i.e. connection weights are not modified. The interested reader can find thorough reviews of the development of artificial neural networks in Garson (1998) and Simpson (1990) . 3. Methodology 3.1. Definitions
For the purpose of this study, text is defined as email messages exclusive of smtp headers, signature  X  X  X ines X  X  and quoted text. Email was chosen because it is a readily available machine readable example of natural language expression of ideas and opinions typical of those of interest to text analysts. A sentence
A word is any series of characters delimited by a space or punctuation mark and not containing spaces or punctuation marks. (All characters in words are converted to uppercase.) The network is an N  X  N matrix of link weights where N = the number of nodes each of which represents a unique word that has been encoun-tered in the text being processed. No attempt is made to conflate plurals and other variants to single terms nor to filter out  X  X  X top words X  X . Nodes are added dynamically as new words are encountered. At any given time each node has two activation levels which are real numbers. The external activation of all nodes is ini-tially zero. When a word is encountered in the sentence currently being processed the external activation of its node is set to 1. Computed activations are calculated during the spreading activation phase. In this net-work model each node is connected to all of the others nodes, including itself through links , whose weights form the N  X  N matrix of the network. When a node  X  s level of activation reaches or exceeds a programmat-ically determined threshold it will fire sending a unit signal to all other nodes. The strength of the signal always +1) and the value, or weight, stored in the link. An activation function , in this case a sigmoid func-the function is the computed activation of the node which determines whether or not the node will fire, and what its final activation level will be.

During self-organizing, when the activation cycle described above is complete (at the end of each sen-tence) the learning cycle takes place during which a modified Hebbian learning rule is implemented to adjust the weights of the connections ( Klerfors &amp; Huston, 1998 ). It reinforces the connections between nodes which are externally excited, thereby strengthening their associations. The modified IAC ANN used in this at as a good starting point during the development and extensive use of that program, was used here as well. The modified Hebbian learning rule can be expressed mathematically as where Win i cycle t 1, x the average activation levels of all nodes and h is the learning rate.

After a learning cycle takes place, the external excitations of the nodes are reduced before the next sen-tence is processed. Finally, in both algorithms, the network connection weight matrix is normalized, i.e. centered on zero and limited to the range of 1to+1( Jordan, 1997 ).
 During recall one or more nodes (corresponding to one or more probe words ) are externally activated.
The network is then cycled without a learning step. The activation levels that result are known as a case which can be output as a list of all of the nodes, in network entry order, with the activation levels produced by the probe. 3.2. Software adaptation
This research is investigating a method of introducing context into the learning phase of an interactive activation with competition (IAC) network ( McClelland &amp; Rumelhart, 1981 ). To this end the core engine of ( Jorgensen, 2003 ). The software, originally written in FORTRAN77 as an interactive command-line driven program to run under MS-DOS, was ported to FORTRAN95 and modified to run under Darwin (a BSD derivative) as a cgi. 2 A context (sentence or message) selector was added to the original parameters of threshold, learning rate, decay rate, activation function, input file name, network file name and output file name. The context selector was used to determine which of two external excitation reduction algorithms would be used as described above and thus forms the basis of this experiment. The two excitation reduction algorithms apply only in learning mode. In the first (sentence) algorithm, the external excitations of all nodes are reduced to zero after a sentence is processed. This is the standard behavior for this type of net-work. In the second (message) algorithm the external excitations are reduced by a decay function after a sentence is processed and reduced to zero at the end of the message. The decay function in this case is a the  X  X  X essage X  X  algorithm. The rationale behind using the decay function in the second algorithm is based on the observation that when humans process the text of an email they do so in a context that includes, among other things, the prior sentences of the email. The words in any sentence are most strongly related to each other, but they are also related to words in sentences preceding and following them, i.e. their context. 3.3. Data collection
Two sets of email messages were collected to serve as test data. Messages were manually selected by inspection of the subject lines of incoming messages posted to the topical internet mailing list, Open Lib/
Info Sci Education Forum ( L-Soft, 2002 ) and saved as text files. Twenty-three messages concerning the use of web citations in evaluation of scholarship were selected and will be called the web citation set .
Twenty-six messages concerning the writing ability of students were collected and will be called the writing set . Email headers, signature lines and quoted text were removed from the messages when they were saved as these elements were not relevant to the research question. 3.4. Network manipulation
The parameter that was manipulated was the context selector, as described above. Descriptive statistics such as the number of unique words and word frequencies were generated. Processing the messages from each thread under the two algorithms (sentence and message) produced two networks. Although the system allows multiple simultaneous probe words, this experiment used just one probe word at a time for each probe cycle. All of the keywords (defined below) were used individually as probe words with networks gen-erated by both algorithms producing two cases per keyword (one for each treatment), for a total of 12 cases for the web citation set of data and 10 cases for the writing set.

For the purpose of choosing keywords, the messages were analyzed by a professor who teaches indexing two sets of messages. The indexer extracted 17(10) web citation (writing) keywords that she felt represented cluster. 6 (5) web citation (writing) words identified by the indexer also appeared (along with their plurals) writing set. These words are shown in italics in Tables 1 and 2 and were chosen as keywords to be used as probes and indicators of performance. They are: citation, counts, engine, links, search , and web for the web citation set and: English , papers , student , students , and write for the writing set.
 4. Results 4.1. Summary statistics
Summary statistics are shown in Table 3 . 4.1.1. Web-citation set
The 23 messages in the web-citation set consist of 3214 words of which 1121 are unique and therefore formed the 1121 nodes of the network. The longest message had 387 words and the shortest 21. As would be expected without the use of a stopword list, the most frequently used word was  X  X  X he X  X  occurring more than twice as often (198 times) as the next most frequent word ( X  X  X nd X  X , 98 occurrences). The next most fre-quently occurring words, in order of frequency, were  X  X  X eb X  X ,  X  X  X hat X  X ,  X  X  X or X  X ,  X  X  X esearch X  X  and,  X  X  X his X  X . 4.1.2. Writing set
The 26 messages of the writing set consist of 3055 words of which 1262 are unique creating a 1262 node network. The longest message contained 329 words and the shortest 17. The most frequently used word again was,  X  X  X he X  X  followed by  X  X  X nd X  X ,  X  X  X hat X  X ,  X  X  X riting X  X ,  X  X  X tudents X  X ,  X  X  X or X  X  and,  X  X  X ot X  X . 4.2. Data analysis
The question of whether one text analysis algorithm produces better or more useful results than another can be explored by looking at three indicators: the differentiation between words in the networks; the clus-tering of keywords; and, the amount of noise associated with the keywords. This study  X  s findings ( Table 4 ) can be summarized by the following generalizations: the message algorithm produced networks with a greater term differentiation, as measured by total activation spread; tighter keyword clustering, as measured by (smaller) keyword activation spread and; better rejection of noise words as measured by fewer highly-activated noise strings. It should be noted that all of these values were obtained with no attempt to adjust parameters to improve performance. Each of these findings will be discussed next. 4.2.1. Term differentiation
Term differentiation is measured by the activation spread which is the difference between the highest and lowest activation levels for nodes in the network after probing. A network (or portion thereof) with a small spread suggests that the nodes are more equally related to the probe word(s) than does a larger spread.
Activation spreads for all words (total activation spread or TAS) and for keywords (keyword activation spread or KAS) were calculated for all cases ( Tables 5 and 6 ). As would be expected, the activation spread for keywords is generally less then that for the entire network because the keywords are each more closely related to the probe word than are all the words in general. This is true for both the sentence and message algorithms. 4.2.2. Total activation spread
Comparisons of the mean TAS values for the entire network across the two self-organizing algorithms reveals one of the differences that the message algorithm makes. For the web-citation data the mean TAS was 0.27975 3 for the sentence algorithm and 0.89397 for the message algorithm. For the writing data the figures are 0.09438 for the sentence algorithm and 0.57542 for the message algorithm. In both cases the mes-sage algorithm created a more highly differentiated network ( Table 7 ).
 4.2.3. Keyword clustering
Comparing the means of the TAS and KAS shows that the keywords have a smaller activation spread than the whole network and therefore the software is, indeed, clustering the keywords. For the web-citation data the mean TAS (across all probes) for the sentence algorithm is 0.27975 while the mean KAS is 0.06922 for this treatment. 4 Similarly, for the writing set sentence algorithm the mean TAS is 0.09438 while the mean KAS is 0.03717. For the web-citation set message algorithm the mean TAS is 0.89397 while the mean KAS is 0.03915 and for the writing set/message algorithm the values are 0.57542 and 0.42720 respectively. Table 8 summarizes these results.

Comparing the KAS values for the two algorithms suggests the conclusion that the message algorithm creates a tighter cluster of keywords, just the opposite of the effect it has on the TAS, i.e. creating a more diffuse overall network. A tighter cluster of keywords is desirable because it means that the relatedness of the keywords is being discovered to a greater degree ( Tables 5 and 6 ).
 4.2.4. Stop words
The exclusion of noise is a desirable feature of a text analysis system and is usually accomplished by the use of stop word lists. The words in these lists can be considered noise in the information processing par-
Fox (1989) in the 20 most highly activated nodes for every case was tabulated ( Tables 9 and 10 ). In every case, the message algorithm included fewer noise strings in the 20 most highly activated nodes. The mean number of noise strings was 14.7 (15.8) in the sentence algorithm and 6.5 (7.4) in the message algorithm. 5. Conclusion
This study has explored the effect of modifying the way external activation levels of intra-message words are reduced between sentences during the learning phase of an interactive activation with competition arti-ficial neural network (IAC ANN) processing email text. The results from this initial investigation suggest that IAC ANN analysis of email text is improved by the use of message context during learning. Message context can be implemented by allowing the external activations of the words already processed from the current message to decay gradually rather than go to zero at the end of each learning cycle. The results dem-onstrate that by employing a context oriented model several benefits may be realized; keywords are more closely related; better discrimination between related and unrelated words is achieved and; high frequency  X  X  X top words X  X  are effectively ignored. 6. Future research
This research has explored a potential method of improving the performance of an interactive activation with competition artificial neural network in the analysis of email text. The development of this technology for text analysis has many potential applications including tracking public opinion, identifying shifts in consumer attitudes, detecting and following the adoption of new ideas and, monitoring the attitudes and thereby helping to predict the behavior of well defined groups. Before these capabilities are fully realized the method needs to be tested with other sets of messages and there needs to be additional research in sev-eral areas. The ANN model that was used employs several parameters (threshold, decay rate, learning rate and others) that were not manipulated. The effect of altering these, and other parameters that could be added to this model such as the size of the message or the activation decay function, should also be ex-plored. For instance, words encountered in short messages might warrant a greater effect on the network than words in long messages. Words at the beginning (introduction) and/or end (summary/conclusion) of a message may warrant greater weighting.

Two additional areas that need further research are the handling of stop words and noise in general and appropriate treatment of sense shifters such as  X  X  X ot X  X . Several techniques for allowing the net-work to learn to ignore noise should be explored including limiting the activation level of words that are frequently encountered and desensitizing noise word nodes by altering their activation func-tion.

Disambiguation is another hotly debated topic in the field of automated text analysis. After sufficient text has been processed it may possible that the network will be able to discriminate between homonyms such as  X  X  X anks X  X  of a river and savings  X  X  X anks X  X .

The model presented here treats all sentences equally, yet that does not reflect how humans communicate ( Liddy, 2004 ). Therefore, another avenue of refinement which requires more research is the inclusion of additional discourse-level information in the model. The subject line was used to manually select the mes-sages to be processed in this study. A network that recognizes subject lines could eliminate this manual step.
One that treats the subject lines of messages differently could potentially process unrelated messages allow-ing intra-and inter-thread relationships to be explored. This would move such a system closer to the text mining model. Yet another discourse-level concept that should be researched is sequence. In other words, should words and/or sentences that occur early, in the middle, or late in a message be given more or less weight in the self-organizing process?
The purpose of this research was to test the essential viability of this novel method to reduce the need for preprocessing of text before analysis by ANN. Future trials will test the basic model with other sets of messages and explore the effects of altering network parameters that were not manipulated in this study.
 In the dendrogram above the main cluster words have been formatted in bold type by the author, not by In the dendrogram above the main cluster words have been formatted in bold type by the author, not by References
