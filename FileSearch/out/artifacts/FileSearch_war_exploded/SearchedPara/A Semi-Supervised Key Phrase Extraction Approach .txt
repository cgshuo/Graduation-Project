 Key phrases are defined as the phrases that e x-press the main content of a document. Guided by the given key phrases, people can easily unde r-stand what a document describes , sav ing a great amount of time reading the whole text. Cons e-quently , a utomatic key phrase extraction is in high demand . Meanwhile, it is also fundament al to many other natural language processing appl i-cations, such as information retrieval, text clu s-tering and so on. 
Key phrase extraction can be normally cast as a ranking pr oblem solved by either supervised or unsupervised methods. Supervised learning r e-quire s a large amount of expensive training data, wh ereas unsupervised learning totally ignore s human knowledge . To overcome the deficiencies of these two kinds of methods, we propose a novel semi -supervised key phrase extraction a p-proach in this paper, which explores title phrases as the source of knowl edge .

It is well agreed that the title has a similar role to the key phrases . They are both elaborated to reflect the content of a document. Therefore, phrases in the titles are often appropriate to be key phrases. Th at is why position ha s been a quite ef fective feature in the feature -based key phrase e x traction methods ( Witten , 1999) , i.e., if a phrase is located in the title, it is ranked higher. 
However , one can only include a couple of most important phrases in the title prudently due to the limitatio n of the title length, even though many other key phrases are all pivotal to the u n-derstanding of the document. For example, when we read the title  X  X  hina T ightens G rip on the W eb  X  , we can o n ly ha ve a glimpse of what the document says. On the other hand , t he key phrases , such as  X  X hina X  ,  X  Censorship  X  ,  X  Web  X  ,  X  X  o main n ame X  ,  X  X nternet X  , and  X  X NNIC X  , etc. can tell more details about the main topic s of the document. In this regard, title phrases are often good key phrases but they are far from enough.
If we rev iew the above example again , we will find that the key phrase  X  I nternet  X  can be i n-ferred from the title phrase  X  W eb  X  . As a matter of fact, k ey phrases o f ten have close semantics to title phrases . Then a question c omes to our minds: can we make use of these title phrases to infer the other key phrases? 
To provide a foundation of inference , a sema n-tic network that captures the relatio n ships among phrases is required . In the previous work s ( Tu r-dakov and Velikhov , 2008 ) , semantic network s are co n struc t ed based on the binary relations , and the s emantic relatedness between a pair of phra s-es is formulated by the weighted edges that co n-nects them . The deficiency of these approaches is the incapability to capture the n -ary relations among multiple phrases. For examp le, a group of phra s es may collectively describe an entity or an event. 
In this study , we propose to model a s e mantic network as a hyper -graph, where vertices represent phrases and weighted hyper -edges measure the semantic rel a tedness of both binary relat ions and n -ary relations among phrases. W e explor e a universal knowledge base  X  Wikipedia  X  to co m pute the semantic relatedness . Yet o ur major co n tribution is to develop a novel semi -supervised key phrase extraction approach by computing the phrase importa nce in the semantic network, through which the influence of title phrases is propagated to the other phrases iter a-tively.

The goal of the semi -supervised learning is to design a function that is sufficiently smooth with respect to the intrinsic structure revealed by title phrases and other phrases . Based on the assum p-tion that semantically related phrases are likely to have similar score s , the function to be est i-mated is required to assign title phrases a higher score and meanwhile locally smooth on the co n-structed hyper -graph. Zhou et al. X  s work (Zhou 200 5 ) lay s down a foun dation f o r our semi -supervised phras e ranking algorithm introduced in Section 3 . Experimental results presented in Section 4 de m onstrate the effectiveness of this approach. Wikipedia 1 is a free online encyclopedia , which has unarguably become the world  X  s largest co l-lection of encyclopedic knowledge . Articles are the basic entries in the Wikipedia, with each a r-ticle explaining one Wikipedia te rm. Articles contain link s pointing from one article to another. Currently , there are over 3 million articles and 90 million links in English Wikipedia. I n addition to pr o v id ing a large vocabulary, Wikipedia articles also contain a rich body of lexical semantic i n-formation expressed via the e x tensive number of links . During recent years, Wikipedia has been used as a powerful tool to compute semantic r e-l a tedness b e tween terms in a good few of works ( Turdakov 2008 ) . 
We consider a document composed of the phras es that describ e various aspects of entit ies or event s with different s emantic relationships . We then model a document a s a semantic ne t-work formulated by a weighted hyp er -graph G =( V , E , W ) , where each vertex v i  X  V (1  X  i  X  represent s a phrase , each hyper -edge e j  X  E (1  X  j  X  m ) is a subset of V , represent ing binary r e-lations or n -ary rel a tions among phrases, and the weight w ( e j ) measures the semantic relatedness of e j . 
By apply ing the WSD technique proposed by (Turdakov and Velikhov , 2008), each phrase is assigned with a single Wikipedia article that d e-scribes its mea n ing. Intuitively, if the fraction of the links that the two articles have in common to the total number of the l inks in both articles is high, the two phrases corresponding to the two articles are more semantically related. Also, an article co n tains different types of links , which are relevant to the computation of semantic relate d-ness to different extent . Hence we adopt the weighted Dice metric proposed by (Turdakov 2008) to compute the semantic relatedness of each binary relation, resulting in the edge weight w ( e ij ) , where e i j is an edge connecting the phrase s v and v j .

To define the n -ary relations in the semant ic network, a proper graph clustering technique is needed . We adopt t he weighted Gi r van -Newman algorithm (Newman 2004) to cluster phrases (i n-cluding title phrases) by computing their be t-weenness centrality . The advantage of this alg o-rithm is that it need n ot specify a pre -defined number of clusters. Then the phrases , within e ach cluster , are connected by a n -ary relation. n -ary relations among the phrases in the same clu s-ter are then measured based on binary relatio n s . The weight of a hyper -edge e is define d as: where | e | is the number of the vertices in e , e an edge with two vertices included in e a nd  X   X  0 i s a parameter balancing the relative importance of n -ary hyper -edges compared with bina ry ones. Given the document semantic network represented as a phrase hyper -graph, one way to mak e better use of the semantic in formation is to ran k phrases with a semi -supervised learn ing strategy, where the title phrases are regarded as labeled samples , while the other phrases as unl a-beled ones . That is, the information we have at the beginning about how to rank phrases is that t he title phrases are the most impo r tant phrases . Initially, t he title phras es are assigned with a po s-itive score of 1 indicating its importance and ot h-er phrases are assigned zero. Then the impo r-tance scores of the phrases are learned it e ratively from the title phrases through the hype r -graph. The key idea be hind hyper -graph based semi -supervised ranking is that the vertices which usually belong to the same hyper -edges should be assigned with similar scores. Then, we have the following two constraints : 1. T he phras es w hich have many incident h y-per -edges in common should be assigned sim i lar scores . 2. The give n initial scores of the title phrases should be changed as little as possible .

Given a weighted hyper -graph G , assume a ranking function f over V , which assigns each vertex v an importance score f ( v ). f can be thought as a ve c tor in Euclid space R | V | . For the convenience of computation, we use an inc i-dence matrix H to represent the hypergraph, d e-fine d as :
B ased on the incidence matrix , we define the d e gree s of the vertex v and the hyper -edge e as a nd
Then, to formulate the above -mentioned co n-straints, l et denote the initial score vector, then the importance score s of the phrases are learned iteratively by solving the following optimization problem: w here  X  &gt; 0 is the parameter specifying the tradeoff between the two competitive items . Let D v a nd D e denote the diagonal matrices contai n-ing th e vertex and the hyper -edge degrees r e-spe c tively, W denote the diagonal matrix co n-tai n ing the hyper -edge weights , f * denote the s o-lution of (6) . Z hou h a s given the solution ( Zhou , 2005 ) a s . Using an approximation algorithm (e.g. Alg o-rithm 1 ) , we can finally get a vector f representing the approximate phrase scores .

Finally we rank phrases in descending order of the calculated importance scores and select those high est ranked phrases as key phrases. Accor d-ing to the number of all the candidate phrases, we choose an appropriate proportion , i.e. 10%, of all the phrases as key phrases. 4.1 Experiment Set -up We first collect all the Wikipedia terms to co m-pose of a dictionary. The word sequences that occur in the d ictionary are identified as phrases. Here we use a finite -state automaton to acco m-plish this task to avoid the impr e cision of pre -processing by POS tagging or chunking. Then, we adopt the WSD technique proposed by ( Tu r-dakov and Velikhov 2008 ) to find the c orre s-ponding W i kipedia article for each phrase. As mentioned in Section 2, a document semantic network in the form of a hyper -graph is co n-structed, on which Alg o rithm 1 is applied to rank the phrases. 
To evaluate our proposed approach , we s e lect 200 piece s of news from well -known English media. 5 to 10 key phrases are manually labeled in each news document and the average number of the key phrases is 7.2 per document . Due to the abbreviation and synonymy phenomena, we co n struct a thesaurus and convert all manual and automatic phrases in to their canonical form s when evaluated. The traditional Recal l, Precision and F 1 -measure metrics are adopted for evalu a-t ion. This section conduct s two sets of exper i-ment: (1) to examine the influence of two par a-meters:  X  and  X  , on the key phrase extraction performance; (2) to compare with other well known state -of -art key phrase extraction a p-proache s . 4.2 Parameter tuning The approach involves two pa rameters:  X  (  X   X  0) is a relation factor balanc ing the influence of n -ary relation s and binary relations ;  X  ( 0  X   X   X  1) is a learning factor tuning the influence from the title phrase s . It is hard to find a global optimized s o-l u tion for the combination of these two factors. So we apply a gradient search strategy. At first, the learning factor is set to  X  =0.8. Different va l-ues of  X  ranging from 0 to 3 are examined . Then, given that  X  is set to the value with the best pe r-formance, we conduct experiments to find an appropriate value for  X  . 4.2.1  X  : Relation F actor First, w e fi x the learning factor  X  as 0.8 rando m-ly and evaluate the performance by varying  X  value from 0 to 3. When  X  =0 , it means that the weight of n -ary relations is zero and only b i nary relations are considered. As we can see from Figure 1 , the performance is imp roved in most cases in terms of F1 -measure and reaches a peak at  X  = 1.8. This justifies the rational to incorp o-rate n -ary relations with binary relations in the document semantic network .
 4.2.2  X  : L earning factor
Next, we set the relation factor  X  =1.8, we i n-spect the performance with the learning factor  X  ran g ing from 0 to 1.  X  = 1 mea ns that the ranking scores learn from the semantic network without any consideration of title phrases . As shown in Figure 2, we find that the p erfo r mance almost keep a smooth fluctuation as  X  increases from 0 to 0.9, and then a diving when  X  =1. This proves that title phrases indeed provide valuable info r-mation for learning. 4.3 Comparison with Other Approaches Our approach aims at inferring important key phrases from title phrases through a semantic network . Here w e take a method of synonym expansion as the baseline , called WordNet e x-pansion here. The WordNet 2 expansion approach selects all the synonyms of the t itle phrases in the document as key phrases. Afterwards, our a p-proach is evaluated against two existing a p-proaches , which rely on the conventional sema n-tic network and are able to capture binary rel a-tions only . O n e approach combines the title i n-formation i nto the Grineva  X  s community -based method ( Grineva et al . , 2 009) , called title -community approach . The title -community a p-proach uses the Gi r van -Newman algorithm to cluster phrases into communities and selects those phrases in the co m munities containing the title phrases as key phrases. We do n ot limit the number of key phrases selected. T he other one is based on topic -sensitive L e x Rank ( Otterbacher et al . , 2005) , called title -sensitive P ageR ank here . The t itle -sensitive P a geRank approach makes use of title p hrases to re -weight the transition s b e-tween vertices and pick s u p 1 0% top -ranked phrases as key phrases.
  X 
Table 1 summarizes the performance on the test data. The results presented in the table show that our approach exhibits the best performance among all the four approaches. It follows that the key phrases inferred from a d ocument semantic network are not limited to the synonyms of title phrases. As t he t itle -sensitive P age R ank a p-proach totally ignores the n -ary relations , its pe r-formance is the worst . Bas ed on b i nary relations , t he title -community approach cluste rs phrases into co m munities and each community can be considered as an n -ary relation . However, th is approach lacks of an importance prop a gation process. Consequently, it has the highest recall val ue but the lowest prec i sion. In contrast , our approach achieves the hig h est precision, due to it s ability to infer many correct key phrases using impo r tance propagation among n -ary relations . This work is based on the belief that key phrases t end to ha ve close s e mantics to the title phrases. In order to make better use of phrase r e lations in key phrase extraction, we explore the Wikipedia knowledge to model one document as a semantic network in the form of hyper -graph , through which the other p hrases learned their importance scores from the title phrases iteratively. Exper i-mental result s demonstrate the effectiveness and robustness of our approach .
 Acknowledgments The work described in this p a per was partially supported by NSFC programs (No: 60 773173, 60875042 and 90920011 ) , and Hong Kong RGC Projects (No : PolyU5217/07E) . We thank the anonymous reviewers for the ir insightful co m-ments.
 Da vid Milne, Ian H. Witten. 2008. An Effective, Low -Cost Measure of Semantic Relatedness 
Obtained f rom Wikipedia Links . In Wikipedia 
Sch  X  lkopf . 2005. Beyond Pairwise Classific a-tion and Cluste r ing Using Hypergraphs . MPI D enis Turdakov and P avel Velikhov. 2008. Semantic relatedness metric for wikipedia concepts based on link analysis and its application to word sense disambiguation . In Colloquium on 
Gutwin , Craig G. Nevill -Manning . 1999. KEA: practical automatic keyphrase extraction , In 
Radev . 2005. Using Random Walks for Que s-tion -focused Sentence Retrieval . In Proceedings 2009. Extracting key terms from noisy and multitheme documents , In Proceedings of the 2006. WikiRelate! Computing Semantic R el a-tedness using Wikipedia. In Proceedings of the M. E. J. Newman . 2004. Analysis of Weighted Ne t-works . Physical Review E 70 , 056131.
